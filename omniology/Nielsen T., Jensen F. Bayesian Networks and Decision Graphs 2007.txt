
Information Science and Statistics
Series Editors:
M. Jordan
J. Kleinberg
B. Schölkopf

Information Science and Statistics
Akaike and Kitagawa: The Practice of Time Series Analysis.
Bishop: Pattern Recognition and Machine Learning.
Cowell, Dawid, Lauritzen, and Spiegelhalter: Probabilistic Networks and Expert 
Systems.
Doucet, de Freitas, and Gordon: Sequential Monte Carlo Methods in Practice.
Fine: Feedforward Neural Network Methodology.
Hawkins and Olwell: Cumulative Sum Charts and Charting for Quality 
Improvement.
Jensen and Nielsen: Bayesian Networks and Decision Graphs, Second Edition.
Lee and Verleysen: Nonlinear Dimensionality Reduction.
Marchette: Computer Intrusion Detection and Network Monitoring: A Statistical 
Viewpoint.
Rissanen: Information and Complexity in Statistical Modeling.
Rubinstein and Kroese: The Cross-Entropy Method: A Unified Approach to 
Combinatorial Optimization, Monte Carlo Simulation, and Machine Learning.
Studen´y: Probabilistic Conditional Independence Structures.
Vapnik: The Nature of Statistical Learning Theory, Second Edition.
Wallace: Statistical and Inductive Inference by Minimum Massage Length.

Finn V. Jensen and Thomas D. Nielsen
Bayesian Networks and Decision
Graphs
February 8, 2007
Springer
Berlin Heidelberg NewYork
Hong Kong London
Milan Paris Tokyo

Finn V. Jensen
Department of Computer Science
Aalborg University
Fredrik Bajers Vej 7, building E
DK-9220 Aalborg EAST,
Denmark
fvj@cs.aau.dk
Thomas D. Nielsen
Department of Computer Science
Aalborg University
Fredrik Bajers Vej 7, building E
DK-9220 Aalborg EAST,
Denmark
tdn@cs.aau.dk
Library of Congress Control Number: 2006938666
ISBN-10: 0-387-68281-3
eISBN-10: 0-387-68282-1
ISBN-13: 978-0-387-68281-5
eISBN-13: 978-0-387-68282-2
Printed on acid-free paper.
© 2007 Springer Science +Business Media, LLC
All rights reserved. This work may not be translated or copied in whole or in part without the
written permission of the publisher (Springer Science +Business Media, LLC, 233 Spring Street,
New York, NY 10013, USA), except for brief excerpts in connection with reviews or scholarly
analysis. Use in connection with any form of information storage and retrieval, electronic
adaptation, computer software, or by similar or dissimilar methodology now known or hereafter
developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if
they are not identified as such, is not to be taken as an expression of opinion as to whether or
not they are subject to proprietary rights.
9 8 7 6 5 4 3 2 1
springer.com
Series Editors:
Michael Jordan
Division of Computer
Science and Department 
of Statistics
University of California,
Berkeley
Berkeley, CA 94720
USA
Jon Kleinberg
Department of Computer
Science
Cornell University
Ithaca, NY 14853
USA
Bernhard Schölkopf
Max Planck Institute for
Biological Cybernetics
Spemannstrasse 38
72076 Tübingen
Germany

Preface
Ever since the ﬁrst machines were constructed, artists and scientists have
shared a vision of a human-like machine: an autonomous self-moving machine
that acts and reasons like a human being. Much eﬀort has been put into this
dream, but we are still very far from having androids with even the tiniest
similarity to humans.
This does not mean that all of these eﬀorts have been wasted. As a spin-
oﬀ, we have seen a long series of inventions that can take over very specialized
sections of human work. These inventions fall into two categories: machines
that can make physical changes in the world and thereby substitute human
labor, and machines that can perform activities usually thought of as requiring
intellectual skills.
In contemporary science and engineering, we still have this split into two
categories. The activity of the ﬁrst category is mainly concentrated on the con-
struction of robots. The aim is to construct autonomous machines performing
“sophisticated” actions such as searching for a cup, ﬁnding a way from the
oﬃce to a lavatory, driving a vehicle in a deserted landscape, or walking on
two legs. Construction of such robots requires computers to perform certain
kinds of artiﬁcial intelligence. Basically, it is the kind of intelligence that hu-
mans share with most mammals. It involves skills such as visual recognition
of items, sound recognition, learning to abstract crucial items from a scene,
or control of balance and position in 3-D space. Although they are very chal-
lenging research tasks, and they certainly require enormous computing power
and very sophisticated algorithms, you would not say that these skills are in-
tellectual, and the basis for the activity is the physical appearance of a device
that moves. To put it another way: the success criterion is how the algorithms
work when controlling a physical machine in real time.
The activity in the second category is basically concerned with reasoning
and human activities that we presumably do not share with other animals.
The activity is separated from matter. When performed, no changes in the
physical world need to take place. The ﬁrst real success was the automated
calculator: a machine that can perform very large and complicated arithmetic

vi
calculations. Automated calculation skill is nowadays hardly considered artiﬁ-
cial intelligence, and we are now acquainted with computers performing tasks
that decades ago were considered highly intellectual (e.g. taking derivatives
of functions or performing reduction of mathematical expressions). When an
activity has been so well understood that it can be formalized, it will soon
be performed by computers, and gradually we acknowledge that this activity
does not really require intelligence.
A branch of research in the second category has to do with reasoning. The
ﬁrst successes were in logical reasoning. Propositional logic is fully formalized,
and although some tasks are NP-complete and therefore in some situations
intractable for a computer, we have for propositional logic completed the
transition from “intellectual task” to “we have computers to do this for us.”
Unfortunately, logical reasoning is very limited in scope. It deals with how
to infer from propositions that you know are true. Very often you do not know
the truth of a proposition for certain, but you still need to perform inference
from your incomplete and uncertain knowledge. Actually, this is the most
common situation for human reasoning. Reasoning under uncertainty is not
yet so well understood that it can be formalized entirely for computers. There
are several approaches to reasoning under uncertainty. The approach taken
in this book is (subjective) probability theory. When the reasoning ends up
in a conclusion on a decision, we use utilities, and we assume that the deci-
sion taken is the one that maximizes the expected utility. In other words, the
approach prescribes a certain behavior. We may not always expect this behav-
ior from human beings, and therefore the approach is also termed normative.
There are alternative approaches to reasoning under uncertainty. Most promi-
nent is possibility theory, which in certain contexts is called fuzzy logic. The
interested reader may consult the wide literature on these approaches.
The aim of normative systems can in short be termed human wisdom:
to take decisions on the basis of accumulated and processed experience. The
tasks are of the following types:
−using observations to interpret a situation;
−focusing a search for more information;
−choosing an appropriate intervening action;
−adapting to changing environments;
−learning from experience.
A damping factor for properly exploiting the advances in artiﬁcial intel-
ligence has for a long time been the lack of successes in robotics. An au-
tonomous agent that moves, observes, and changes the world must carry a
not easily controllable body.
Therefore, the advances have been exploited mainly in decision support
systems, computer systems that provide advice for humans on highly special-
ized tasks. With the Internet, the scope of artiﬁcial intelligence has widened
considerably. The Internet is an ideal nonphysical world for intelligent agents,

vii
which are pure spirits without bodies. In the years to come, we will experi-
ence a ﬂood of intelligent agents on the Internet, and companies as well as
private persons will be able to launch their own agents to explore and collect
information on the Internet. Also, we will experience the dark sides of human
endeavor. Some agents will destroy, intrude, tell lies and so on, and we will
have to defend ourselves against them. Agents will meet agents, and they will
have to decide how to treat each other, they will have to learn from past
experience, and they will have to adapt to changing environments.
During the 1990s, Bayesian networks and decision graphs attracted a great
deal of attention as a framework for building normative systems, not only in
research institutions but also in industry. Contrary to most other frameworks
for handling uncertainty, a good deal of theoretical insight as well as practi-
cal experience is required in order to exploit the opportunities provided by
Bayesian networks and decision graphs.
On the other hand, many scientists and engineers wish to exploit the pos-
sibilities of normative systems without being experts in the ﬁeld. This book
should meet that demand. It is intended for both classroom use and self-study,
and it addresses persons who are interested in exploiting the approach for the
construction of decision support systems or bodyless agents.
The theoretical exposition in the book is self-contained, and the mathe-
matical prerequisite is some prior exposure to calculus and elementary graph
theory. Throughout the book we alternate between theoretical exposition and
practical examples for gaining experience with the use of Bayesian networks
and decision graphs, and we have assumed that the reader has access to a
computer system for handling Bayesian networks and inﬂuence diagrams (the
exercises marked with an E require such a system). There are many sys-
tems, academic as well as commercial. A comprehensive list of systems can be
found at www.cs.berkeley.edu/∼murphyk/Bayes/bnsoft.html. Several of the
commercial systems have an academic version, which can be downloaded free
of charge. In several chapters the presentation is based on examples, and for
overview purposes there is a summary section at the end of each chapter.
A hands-on course could cover Sections 1.1–1.3, Chapter 2, Chapter 3,
Sections 6.1–6.2, 7.1, 8.1–8.3, 9.1–9.4, and Sections 11.1–11.2. A ﬁrst-year
graduate course could cover Chapters 1–3, Sections 4.1–4.6, 5.2–5.3, 5.5, 5.7,
6.1–6.3, 7.1–7.3, Chapters 8–9, Sections 10.1–10.2, and Chapter 11.
The book is an introduction to Bayesian networks and decision graphs.
Many results are not mentioned or just treated superﬁcially. The following
textbooks and monographs can be used for further study:
−Judea Pearl, Probabilistic Reasoning in Intelligent Systems, Morgan Kauf-
mann Publishers, 1988.
−Russell Almond, Graphical Belief Modelling, Chapman & Hall, 1995.
−Steﬀen L. Lauritzen, Graphical Models, Oxford University Press, 1996.
−Enrique Castillo, Jos´e M. Guti´errez, and Ali S. Hadi, Expert Systems and
Probabilistic Network Models, Springer-Verlag, 1997.

viii
−Robert G. Cowell, A. Philip Dawid, and Steﬀen L. Lauritzen, Probabilistic
Networks and Expert Systems, Springer-Verlag, 1999.
−Kevin B. Korb and Ann E. Nicholson, Bayesian Artiﬁcial Intelligence,
Chapman & Hall 2004.
−Richard E. Neapolitan, Learning Bayesian Networks, Pearson Prentice
Hall, 2004.
The annual Conference on Uncertainty in Artiﬁcial Intelligence (www.auai
.org) is the main forum for researchers working with Bayesian networks and
decision graphs, so the best general references for further reading are the
proceedings from these conferences.
Another relevant conference is the biannual European Conference on
Symbolic and Quantitative Approaches to Reasoning with Uncertainty (EC-
SQARU). The conference deals with various approaches to uncertainty calcu-
lus, and the proceedings are published in the Springer-Verlag series Lecture
Notes in Artiﬁcial Intelligence.
The book is supported by a web site, bndg.cs.aau.dk, which provides read-
ers with solutions and models for selected exercises, a list of errata, special
exercises, and other links relevant to the issues in the book.
Changes from the First Edition
In the second edition, we have added several subjects. Primarily, we have
included chapters presenting commonly used methods for learning graphical
models, and we have extended the treatment of graphical languages for mod-
eling decision problems. We have also reorganized the material such that Part
I is devoted to Bayesian networks and Part II deals with decision graphs.
The mathematical treatment is intended to be at the same level as in
the ﬁrst edition. However, many of the new issues in the book are mathe-
matically rather demanding, particularly learning. Some of the sections are
marked with an asterisk to indicate that they are not required for reading any
of the unmarked sections.
Acknowledgments
We wish to express our gratitude to several people for ideas and comments
during the preparation of the book. First, we thank present and previous
colleagues in the Machine Intelligence group, Olav Bangsø, Søren L. Dittmer,
Uﬀe Kjærulﬀ, Tom´aˇs Koˇcka, Anders L. Madsen, Dennis Nilsson, Kristian G.
Olesen, Jose Pe˜na, Jiˇr´ı Vomlel, and Marta Vomlelov´a. We also thank the
many academic colleagues around the world with whom we have had the
pleasure of exchanging ideas, in particular Poul S. Eriksen, Linda van der
Gaag, Helge Langseth, Steﬀen L. Lauritzen, Seraf´ın Moral, Prakash Shenoy,

ix
Antonio Salmer´on, Claus Skaanning, Marco Valtorta, Yang Xiang, and Nevin
Zhang. Special thanks to Søren Holbech Nielsen for assistance with ﬁgures,
bibliography, and exercises.
We also thank several years’ worth of undergraduate students who have
had to cope with unﬁnished drafts of notes for parts of their course on decision
support systems.
Aalborg, February 2007
Finn V. Jensen and Thomas D. Nielsen

Table of Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
1
Prerequisites on Probability Theory . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Two Perspectives on Probability Theory. . . . . . . . . . . . . . . . . . . .
1
1.2
Fundamentals of Probability Theory . . . . . . . . . . . . . . . . . . . . . . .
2
1.2.1
Conditional Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2.2
Probability Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2.3
Conditional Independence . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3
Probability Calculus for Variables . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.1
Calculations with Probability Tables: An Example . . . . . 11
1.4
An Algebra of Potentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.5
Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.5.1
Continuous Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Part I Probabilistic Graphical Models
2
Causal and Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.1
Reasoning Under Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.1.1
Car Start Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.1.2
A Causal Perspective on the Car Start Problem . . . . . . . 24
2.2
Causal Networks and d-Separation . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.2.1
d-separation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.3
Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.3.1
Deﬁnition of Bayesian Networks . . . . . . . . . . . . . . . . . . . . . 32
2.3.2
The Chain Rule for Bayesian Networks . . . . . . . . . . . . . . . 35
2.3.3
Inserting Evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2.3.4
Calculating Probabilities in Practice . . . . . . . . . . . . . . . . . 41
2.4
Graphical Models – Formal Languages for Model Speciﬁcation
42
2.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

xii
Table of Contents
2.6
Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
2.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3
Building Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.1
Catching the Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.1.1
Milk Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.1.2
Cold or Angina? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
3.1.3
Insemination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.1.4
A Simpliﬁed Poker Game . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
3.1.5
Naive Bayes Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.1.6
Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3.2
Determining the Conditional Probabilities . . . . . . . . . . . . . . . . . . 60
3.2.1
Milk Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
3.2.2
Stud Farm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.2.3
Poker Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.2.4
Transmission of Symbol Strings . . . . . . . . . . . . . . . . . . . . . 68
3.2.5
Cold or Angina? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
3.2.6
Why Causal Networks? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
3.3
Modeling Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.3.1
Undirected Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
3.3.2
Noisy-Or . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.3.3
Divorcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
3.3.4
Noisy Functional Dependence . . . . . . . . . . . . . . . . . . . . . . . 80
3.3.5
Expert Disagreements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
3.3.6
Object-Oriented Bayesian Networks. . . . . . . . . . . . . . . . . . 84
3.3.7
Dynamic Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . 91
3.3.8
How to Deal with Continuous Variables . . . . . . . . . . . . . . 93
3.3.9
Interventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
3.4
Special Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
3.4.1
Joint Probability Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.4.2
Most-Probable Explanation . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.4.3
Data Conﬂict . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.4.4
Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
3.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.6
Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
3.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
4
Belief Updating in Bayesian Networks . . . . . . . . . . . . . . . . . . . . . 109
4.1
Introductory Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
4.1.1
A Single Marginal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
4.1.2
Diﬀerent Evidence Scenarios . . . . . . . . . . . . . . . . . . . . . . . . 111
4.1.3
All Marginals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
4.2
Graph-Theoretic Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
4.2.1
Task and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
4.2.2
Domain Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

Table of Contents
xiii
4.3
Triangulated Graphs and Join Trees . . . . . . . . . . . . . . . . . . . . . . . 119
4.3.1
Join Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.4
Propagation in Junction Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
4.4.1
Lazy Propagation in Junction Trees . . . . . . . . . . . . . . . . . 127
4.5
Exploiting the Information Scenario . . . . . . . . . . . . . . . . . . . . . . . 130
4.5.1
Barren Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.5.2
d-Separation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.6
Nontriangulated Domain Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.6.1
Triangulation of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
4.6.2
Triangulation of Dynamic Bayesian Networks . . . . . . . . . 137
4.7
Exact Propagation with Bounded Space . . . . . . . . . . . . . . . . . . . . 140
4.7.1
Recursive Conditioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.8
Stochastic Simulation in Bayesian Networks. . . . . . . . . . . . . . . . . 145
4.8.1
Probabilistic Logic Sampling . . . . . . . . . . . . . . . . . . . . . . . . 146
4.8.2
Likelihood Weighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
4.8.3
Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
4.9
Loopy Belief Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
4.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4.11 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
4.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5
Analysis Tools for Bayesian Networks . . . . . . . . . . . . . . . . . . . . . 167
5.1
IEJ Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
5.2
Joint Probabilities and A-Saturated Junction Trees . . . . . . . . . . 169
5.2.1
A-Saturated Junction Trees . . . . . . . . . . . . . . . . . . . . . . . . . 169
5.3
Conﬁguration of Maximal Probability . . . . . . . . . . . . . . . . . . . . . . 171
5.4
Axioms for Propagation in Junction Trees . . . . . . . . . . . . . . . . . . 173
5.5
Data Conﬂict . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.5.1
Insemination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
5.5.2
The Conﬂict Measure conf . . . . . . . . . . . . . . . . . . . . . . . . . . 175
5.5.3
Conﬂict or Rare Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
5.5.4
Tracing of Conﬂicts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
5.5.5
Other Approaches to Conﬂict Detection . . . . . . . . . . . . . . 179
5.6
SE Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
5.6.1
Example and Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
5.6.2
h-Saturated Junction Trees and SE Analysis . . . . . . . . . . 182
5.7
Sensitivity to Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
5.7.1
One-Way Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . 187
5.7.2
Two-Way Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . 188
5.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
5.9
Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
5.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191

xiv
Table of Contents
6
Parameter Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
6.1
Complete Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
6.1.1
Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . . 196
6.1.2
Bayesian Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
6.2
Incomplete Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
6.2.1
Approximate Parameter Estimation: The EM Algorithm201
6.2.2
*Why We Cannot Perform Exact Parameter Estimation 207
6.3
Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
6.3.1
Fractional Updating . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
6.3.2
Fading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
6.3.3
*Speciﬁcation of an Initial Sample Size . . . . . . . . . . . . . . . 212
6.3.4
Example: Strings of Symbols . . . . . . . . . . . . . . . . . . . . . . . . 213
6.3.5
Adaptation to Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
6.3.6
*Fractional Updating as an Approximation . . . . . . . . . . . 215
6.4
Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
6.4.1
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
6.4.2
Determining grad dist(x, y) as a Function of t . . . . . . . . 222
6.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
6.6
Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
6.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
7
Learning the Structure of Bayesian Networks . . . . . . . . . . . . . . 229
7.1
Constraint-Based Learning Methods . . . . . . . . . . . . . . . . . . . . . . . 230
7.1.1
From Skeleton to DAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
7.1.2
From Independence Tests to Skeleton . . . . . . . . . . . . . . . . 234
7.1.3
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
7.1.4
Constraint-Based Learning on Data Sets . . . . . . . . . . . . . 237
7.2
Ockham’s Razor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
7.3
Score-Based Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
7.3.1
Score Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
7.3.2
Search Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
7.3.3
Chow–Liu Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
7.3.4
*Bayesian Score Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 253
7.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
7.5
Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
7.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
8
Bayesian Networks as Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
8.1
Naive Bayes Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
8.2
Evaluation of Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
8.3
Extensions of Naive Bayes Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . 270
8.4
Classiﬁcation Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
8.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
8.6
Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
8.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276

Table of Contents
xv
Part II Decision Graphs
9
Graphical Languages for Speciﬁcation of Decision Problems279
9.1
One-Shot Decision Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
9.1.1
Fold or Call? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
9.1.2
Mildew. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
9.1.3
One Decision in General. . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
9.2
Utilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
9.2.1
Instrumental Rationality . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
9.3
Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
9.3.1
A Couple of Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
9.3.2
Coalesced Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
9.3.3
Solving Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
9.4
Inﬂuence Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
9.4.1
Extended Poker Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
9.4.2
Deﬁnition of Inﬂuence Diagrams . . . . . . . . . . . . . . . . . . . . 305
9.4.3
Repetitive Decision Problems . . . . . . . . . . . . . . . . . . . . . . . 308
9.5
Asymmetric Decision Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
9.5.1
Diﬀerent Sources of Asymmetry . . . . . . . . . . . . . . . . . . . . . 314
9.5.2
Unconstrained Inﬂuence Diagrams . . . . . . . . . . . . . . . . . . . 316
9.5.3
Sequential Inﬂuence Diagrams . . . . . . . . . . . . . . . . . . . . . . 322
9.6
Decision Problems with Unbounded Time Horizons . . . . . . . . . . 324
9.6.1
Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . 324
9.6.2
Partially Observable Markov Decision Processes . . . . . . . 330
9.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
9.8
Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
9.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
10
Solution Methods for Decision Graphs . . . . . . . . . . . . . . . . . . . . . 343
10.1 Solutions to Inﬂuence Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
10.1.1 The Chain Rule for Inﬂuence Diagrams . . . . . . . . . . . . . . 345
10.1.2 Strategies and Expected Utilities . . . . . . . . . . . . . . . . . . . . 346
10.1.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
10.2 Variable Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
10.2.1 Strong Junction Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
10.2.2 Required Past . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
10.2.3 Policy Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
10.3 Node Removal and Arc Reversal. . . . . . . . . . . . . . . . . . . . . . . . . . . 362
10.3.1 Node Removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
10.3.2 Arc Reversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
10.3.3 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
10.4 Solutions to Unconstrained Inﬂuence Diagrams . . . . . . . . . . . . . . 367
10.4.1 Minimizing the S-DAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
10.4.2 Determining Policies and Step Functions . . . . . . . . . . . . . 371

xvi
Table of Contents
10.5 Decision Problems Without a Temporal Ordering:
Troubleshooting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
10.5.1 Action Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
10.5.2 A Greedy Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
10.5.3 Call Service. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
10.5.4 Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
10.6 Solutions to Decision Problems with Unbounded Time Horizon 380
10.6.1 A Basic Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
10.6.2 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
10.6.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
10.6.4 Solving Partially Observable Markov Decision Processes*388
10.7 Limited Memory Inﬂuence Diagrams . . . . . . . . . . . . . . . . . . . . . . 392
10.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
10.9 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
10.10Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
11
Methods for Analyzing Decision Problems . . . . . . . . . . . . . . . . . 407
11.1 Value of Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
11.1.1 Test for Infected Milk? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
11.1.2 Myopic Hypothesis-Driven Data Request . . . . . . . . . . . . . 409
11.1.3 Non-Utility-Based Value Functions . . . . . . . . . . . . . . . . . . 411
11.2 Finding the Relevant Past and Future of a Decision Problem . . 413
11.2.1 Identifying the Required Past . . . . . . . . . . . . . . . . . . . . . . . 415
11.3 Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
11.3.1 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
11.3.2 One-Way Sensitivity Analysis in General . . . . . . . . . . . . . 423
11.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
11.5 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
11.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
List of Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441

1
Prerequisites on Probability Theory
In this chapter we review some standard results and deﬁnitions from probabil-
ity theory. The reader is assumed to have had some contact with probability
theory before, and the purpose of this section is simply to brush up on some
of the basic concepts and to introduce some of the notation used in the later
chapters. Sections 1.1–1.3 are prerequisites for Section 2.3 and thereafter, Sec-
tion 1.4 is a prerequisite for Chapter 4, and Section 1.5 is a prerequisite for
Chapter 6 and Chapter 7.
1.1 Two Perspectives on Probability Theory
In many domains, the probability of seeing a certain outcome of an experiment
can be interpreted as the relative frequency of seeing this particular outcome
in all of the experiments performed. For instance, if you throw a six-sided die,
then you would say that the probability of obtaining a three is 1/6, because
if we throw this die a large number of times we would expect to see a three in
approximately 1/6 of the throws. Along the same line of reasoning, we would
also say that if we randomly draw a card from a deck consisting of 52 cards,
then the probability that it will be a spade is 13/52. This interpretation of
probability rests on the assumption that there is some stochastic process that
can be repeated several times and from which the relative frequencies can be
counted. On the other hand, we often talk about the probability of seeing
a certain event although we cannot specify a frequency for it. For example,
I may estimate that the probability that the Danish soccer team will win
the World Cup in 2010 is p. This probability is my own personal judgment
of how likely it is that the Danish team will actually win, and it is based
on my belief, experience, and current state of information. However, another
person may specify another probability for the same event, and it has no
meaning to look for ways of determining which of us is right, if either. These
probabilities are referred to as subjective probabilities. One way to interpret

2
1 Prerequisites on Probability Theory
my subjective probability of Denmark winning the world cup in 2010 is to
imagine the following two wagers:
1. If the Danish soccer team wins the world cup in 2010, I will receive $100.
2. I will draw a ball from an urn containing 100 balls out of which n are
white and 100 −n are black. If the ball drawn is white then I will receive
$100 in 2010.
If all the balls are white then I will prefer the second wager, and if all the
balls are black then I will prefer the ﬁrst. However, for a certain n between 0
and 100 I will be indiﬀerent about the two wagers, and for this n, n/100 will
be my subjective probability that the Danish soccer team will win the World
Cup.
1.2 Fundamentals of Probability Theory
For both views on probability described above, we will refer to the set of
possible outcomes of an experiment as the sample space of the experiment.
Here we use the somewhat abstract term “experiment” to refer to any type
of process for which the outcome is uncertain, e.g., the throw of a die and the
winner of the World Cup. We shall also assume that the sample space of an
experiment contains all possible outcomes of the experiment, and that each
pair of outcomes are mutually exclusive. These assumptions ensure that the
experiment is guaranteed to end up in exactly one of the speciﬁed outcomes
in the sample space. For instance, for the die example above, the sample space
would be S = {1, 2, 3, 4, 5, 6}, and for the soccer example the sample space
would be S = {yes, no}, assuming that I am interested only in whether the
Danish team will win; both of the sample spaces satisfy the assumptions above.
A subset of a sample space is called an event. For example, the event that
we will get a value of three or higher with a six-sided die corresponds to the
subset {3, 4, 5, 6} ⊆{1, 2, 3, 4, 5, 6}, and the event will occur if the outcome
of the throw is an element in the set. In general, we say that an event A is
true for an experiment if the outcome of the experiment is an element of A.
When an event contains only one element, we will also refer to the event as
an outcome.
To measure our degree of uncertainty about an experiment we assign a
probability P(A) to each event A ⊆S. These probabilities must obey the
following three axioms:
The event S that we will get an outcome in the sample space is certain to
occur and is therefore assigned the probability 1.
Axiom 1 P(S) = 1.
Any event A must have a nonnegative probability.
Axiom 2 For all A ⊆S it holds that P(A) ≥0.

1.2 Fundamentals of Probability Theory
3
If two events A and B are disjoint (see Figure 1.1(a)), then the probability
of the combined event is the sum of the probabilities for the two individual
events:
Axiom 3 If A ⊆S, B ⊆S and A ∩B = ∅, then P(A ∪B) = P(A) + P(B).
For example, the event that a die will turn up 3, B = {3}, and the event that
the die will have an even number, A = {2, 4, 6}, are two disjoint events, and
the probability that one of these two events will occur is therefore
P(A ∪B) = P(A) + P(B) = 1
6 + 3
6 = 4
6.
A
S
S
(a)
(b)
B
A
B
Fig. 1.1. In ﬁgure (a) the two events A and B are disjoint, whereas in ﬁgure (b),
A ∩B ̸= ∅.
On the other hand, if A and B are not disjoint (see Figure 1.1(b)), then
it can easily be shown that
P(A ∪B) = P(A) + P(B) −P(A ∩B),
where A ∩B is the intersection between A and B and it represents the event
that both A and B will occur. Consider again a deck with 52 cards. The event
A that I will draw a spade and the event B that I will draw a king are clearly
not disjoint events; their intersection speciﬁes the event that I will draw the
king of spades, A ∩B = {king of spades}. Thus, the probability that I will
draw either a king or a spade is
P(A ∪B) = P(A) + P(B) −P(A ∩B) = 13
52 + 4
52 −1
52 = 16
52.
Notation: Sometimes we will emphasize that a probability is based on a
frequency (rather than being a subjective probability), in which case we will
use the notation P #. If the event A contains only one outcome a, we write
P(a) rather than P({a}).

4
1 Prerequisites on Probability Theory
1.2.1 Conditional Probabilities
Whenever a statement about the probability P(A) of an event A is given,
then it is implicitly given conditioned on other known factors. For example, a
statement such as “the probability of the die turning up 6 is 1
6” usually has
the unsaid prerequisite that it is a fair die, or rather, as long as I know nothing
further, I assume it to be a fair die. This means that the statement should be
“given that it is a fair die, the probability . . . .” In this way, any statement on
probabilities is a statement conditioned on what else is known. These types of
probabilities are called conditional probabilities and are generally statements
of the following kind:
“Given the event B, the probability of the event A is p.”
The notation for the preceding statement is P(A|B) = p. It should be
stressed that P(A|B) = p does not mean that whenever B is true, then the
probability of A is p. It means that if B is true, and everything else is irrelevant
for A, then the probability of A is p.
Assume that we have assigned probabilities to all subsets of the sample
space S, and let A and B be subsets of S (Figure 1.1(b)). The question is
whether the probability assignment for S can be used to calculate P(A|B). If
we know the event B, then all possible outcomes are elements of B, and the
outcomes for which A can be true are A ∩B. So, we look for the probability
assignment for A ∩B given that we know B. Knowing B does not change the
proportion between the probabilities of A ∩B and another set C ∩B (if, for
example, I will bet twice as much on A ∩B as on C ∩B, then after knowing
B, I will still bet twice as much on A ∩B as on C ∩B). We can conclude that
the proportions P(A ∩B)/P(C ∩B) and P(A|B)/P(C|B) must be the same.
Setting C = B, and since we know from Axiom 1 that P(B|B) = 1, we have
justiﬁed the following property, which should be considered an axiom.
Property 1.1 (Conditional probability). For two events A and B, with P(B) >
0, the conditional probability for A given B is
P(A | B) = P(A ∩B)
P(B)
.
For example, the conditional probability that a die will come up 4 given
that we get an even number is P(A = {4} | B = {2, 4, 6}) = P({4})/P({2, 4,
6}), and by assuming that the die is fair we get 1/6
3/6 = 1
3.
Obviously, when working with conditional probabilities we can also con-
dition on more than one event, in which case the deﬁnition of a conditional
probability generalizes as
P(A | B ∩C) = P(A ∩B ∩C)
P(B ∩C)
.

1.2 Fundamentals of Probability Theory
5
1.2.2 Probability Calculus
The expression in Property 1.1 can be rewritten so that we obtain the so-called
fundamental rule for probability calculus:
Theorem 1.1 (The fundamental rule).
P(A | B)P(B) = P(A ∩B).
(1.1)
That is, the fundamental rule tells us how to calculate the probability of
seeing both A and B when we know the probability of A given B and the
probability of B.
By conditioning on another event C, the fundamental rule can also be
written as
P(A | B ∩C)P(B | C) = P(A ∩B | C).
Since P(A ∩B) = P(B ∩A) (and also P(A ∩B | C) = P(B ∩A | C)), we get
that P(A | B)P(B) = P(A ∩B) = P(B | A)P(A) from the fundamental rule.
This yields the well-known Bayes’ rule:
Theorem 1.2 (Bayes’ rule).
P(A | B) = P(B | A)P(A)
P(B)
.
Bayes’ rule provides us with a method for updating our beliefs about an
event A given that we get information about another event B. For this reason
P(A) is usually called the prior probability of A, whereas P(A | B) is called
the posterior probability of A given B; the probability P(B | A) is called the
likelihood of A given B. For an explanation of this strange use of the term,
see Example 1.1.
Finally, as for the fundamental rule, we can also state Bayes’ rule in a
context C:
P(A | B, C) = P(B | A, C)P(A | C)
P(B | C)
.
Example 1.1. We have two diseases a1 and a2, both of which can cause the
symptom b. Let P(b | a1) = 0.9 and P(b | a2) = 0.3. Assume that the prior
probabilities for a1 and a2 are the same (P(a1) = P(a2)). Now, if b occurs,
Bayes’ rule gives
P(a1 | b) = P(b | a1)P(a1)
P(b)
= 0.9 · P(a1)
P(b) ;
P(a2 | b) = P(b | a2)P(a2)
P(b)
= 0.3 · P(a2)
P(b) .
Even though we cannot calculate the posterior probabilities, we can conclude
that a1 is three times as likely as a2 given the symptom b.

6
1 Prerequisites on Probability Theory
If we furthermore know that a1 and a2 are the only possible causes of b,
we can go even further (assuming that the probability of having both diseases
is 0). Then P(a1 | b) + P(a2 | b) = 1, and we get
P(a1)
P(b) = P(a2)
P(b) =
1
0.9 + 0.3 = 1
1.2,
P(a1 | b) = 0.9/1.2 = 0.75, and P(a2 | b) = 0.3/1.2 = 0.25.
1.2.3 Conditional Independence
Sometimes information on one event B does not change our belief about the
occurrence of another event A, and in this case we say that A and B are
independent.
Deﬁnition 1.1 (Independence). The events A and B are independent if
P(A | B) = P(A).
For example, if we throw two fair dice, then seeing that the ﬁrst die turns
up 2 will not change our beliefs about the outcome of the second die.
This notion of independence is symmetric, so that if A is independent of
B, then B is independent of A:
P(B | A) = P(A ∩B)
P(A)
= P(A | B)P(B)
P(A)
= P(A)P(B)
P(A)
= P(B).
The proof requires that P(A) > 0, so if P(A) = 0, the calculations are not
valid. However, for our considerations it does not matter; if A is impossible
why bother considering it?
When two events are independent, then the fundamental rule can be
rewritten as
P(A ∩B) = P(A | B)P(B) = P(A) · P(B).
That is, we can calculate the probability that both events will occur by mul-
tiplying the probabilities for the individual events.
The concept of independence also appears when we are conditioning on
several events. Speciﬁcally, if information about the event B does not change
our belief about the event A when we already know the event C, then we say
that A and B are conditionally independent given C.
Deﬁnition 1.2 (Conditional independence). The events A and B are
conditionally independent given the event C if
P(A | B ∩C) = P(A | C).

1.3 Probability Calculus for Variables
7
Similar to the situation above, the conditional independence statement is
symmetric. If A is conditionally independent of B given C, then B is condi-
tionally independent of A given C:
P(B | A ∩C) = P(A ∩B | C)P(C)
P(A | C)P(C)
= P(A | B ∩C)P(B | C)
P(A | C)
= P(A | C)P(B | C)
P(A | C)
= P(B | C).
Furthermore, when two events are conditionally independent, then we can
use a multiplication rule similar to the one above when calculating the prob-
ability that both of the events will occur:
P(A ∩B | C) = P(A | C) · P(B | C).
Note that when two events are independent it is actually a special case of
conditional independence but with C = ∅.
1.3 Probability Calculus for Variables
So far we have talked about probabilities of simple events and outcomes with
respect to a certain sample space. In this book, however, we will be working
with a collection of sample spaces, also called variables, and we will now extend
the concepts above to probabilities over variables. A variable can be considered
an experiment, and for each outcome of the experiment the variable has a
corresponding state. The set of states associated with a variable A is denoted
by sp(A) = (a1, a2, . . . , an), and similar to the sample space these states
should be mutually exclusive and exhaustive. The last assumption ensures that
the variable is in one of its states (although we may not know which one), and
the ﬁrst assumption ensures that the variable is in only one state. For example,
if we let D be a variable representing the outcome of rolling a die, then its
state space would be sp(D) = (1, 2, 3, 4, 5, 6). We will use uppercase letters
for variables and lowercase letters for states, and unless otherwise stated, a
variable has a ﬁnite number of states.
For a variable A with states a1, . . . , an, we express our uncertainty about
its state through a probability distribution P(A) over these states:
P(A) = (x1, . . . , xn);
xi ≥0;
n

i=1
xi = x1 + · · · + xn = 1,
where xi is the probability of A being in state ai. A distribution is called
uniform (or even) if all probabilities are equal.
Notation: In general, the probability of A being in state ai is denoted by
P(A = ai), and denoted by P(ai) if the variable is obvious from the context.

8
1 Prerequisites on Probability Theory
As we talked about conditional probabilities for events, we can also talk
about conditional probabilities for variables: If the variable B has states
b1, . . . , bm, then P(A | B) contains n · m conditional probabilities P(ai | bj)
that specify the probability of seeing ai given bj. That is, the conditional
probability for a variable given another variable is a set of probabilities (usu-
ally organized in an n×m table) with one probability for each conﬁguration of
the states of the variables involved (see Table 1.1 for an example). Moreover,
since P(A | B) speciﬁes a probability distribution for each event B = bj, we
know from Axiom 1 that the probabilities over A should sum to 1 for each
state of B:
n

i=1
P(A = ai | B = bj) = 1 for each bj.
b1 b2 b3
a1 0.4 0.3 0.6
a2 0.6 0.7 0.4
Table 1.1. An example of a conditional probability table P(A | B) for the binary
variable A given the ternary variable B. Note that for each state of B the probabil-
ities of A sum up to 1.
The probability of seeing joint outcomes for diﬀerent experiments can be
expressed by the joint probability for two or more variables: For each conﬁg-
uration (ai, bj) of the variables A and B, P(A, B) speciﬁes the probability of
seeing both A = ai and B = bj. Hence, P(A, B) consists of n · m numbers,
and, similar to P(A | B), P(A, B) is usually represented in an n×m table (see
Table 1.2 for an example). Note that since the state spaces of both A and B
are mutually exclusive and exhaustive, it follows that all combinations of their
states (the Cartesian product) are also mutually exclusive and exhaustive, and
they can therefore be considered a sample space. Hence, by Axiom 1,
P(A, B) =
n

i=1
m

j=1
P(A = ai, B = bj) = 1.
b1
b2
b3
a1 0.16 0.12 0.12
a2 0.24 0.28 0.08
Table 1.2. An example of a joint probability table P(A, B) for the binary variable
A and the ternary variable B. Note that the sum of all entries is 1.

1.3 Probability Calculus for Variables
9
When the fundamental rule (equation (1.1)) is used on variables A and B,
the procedure is to apply the rule to each of the n · m conﬁgurations (ai, bj)
of the two variables:
P(ai | bj)P(bj) = P(ai, bj).
This means that in the table P(A | B), each probability in P(A | bj) is multi-
plied by P(bj) to obtain the table P(A, bj), and by doing this for each bj we
get P(A, B). If P(B) = (0.4, 0.4, 0.2), then Table 1.2 is the result of using the
fundamental rule on Table 1.1 (see also Table 1.3).
P(A, B) =
b1
b2
b3
a1 0.4 · 0.4 0.3 · 0.4 0.6 · 0.2
a2 0.6 · 0.4 0.7 · 0.4 0.4 · 0.2
=
b1
b2
b3
a1 0.16 0.12 0.12
a2 0.24 0.28 0.08
Table 1.3. The joint probability table P(A, B) in Table 1.2 can be found by mul-
tiplying P(B) = (0.4, 0.4, 0.2) by P(A | B) in Table 1.1.
When applied to variables, the fundamental rule is expressed as follows:
Theorem 1.3 (The fundamental rule for variables).
P(A, B) = P(A | B)P(B),
and conditioned on another variable C we have
P(A, B | C) = P(A | B, C)P(B | C).
From a joint probability table P(A, B), the probability distribution P(A)
can be calculated by considering the outcomes of B that can occur together
with each state ai of A. There are exactly m diﬀerent outcomes for which A
is in state ai, namely the mutually exclusive outcomes (ai, b1), . . . , (ai, bm).
Therefore, by Axiom 3,
P(ai) =
m

j=1
P(ai, bj).
This calculation is called marginalization, and we say that the variable B is
marginalized out of P(A, B) (resulting in P(A)). The notation is
P(A) =

B
P(A, B).
By marginalizing B out of Table 1.2, we get
P(A) = (0.16 + 0.12 + 0.12, 0.24 + 0.28 + 0.08) = (0.4, 0.6),

10
1 Prerequisites on Probability Theory
and by marginalizing out A we get
P(B) = (0.16 + 0.24, 0.12 + 0.28, 0.12 + 0.08) = (0.4, 0.4, 0.2).
That is, the marginalization operation allows us to remove variables from a
joint probability distribution.
Bayes’ rule for events (Theorem 1.2) can also be extended to variables, by
treating the division in the same way as we treated multiplication above.
Theorem 1.4 (Bayes’ rule for variables).
P(B | A) = P(A | B)P(B)
P(A)
=
P(A, B)

B P(A, B),
and conditioned on another variable C we have
P(B | A, C) = P(A | B, C)P(B | C)
P(A | C)
=
P(A, B | C)

B P(A, B | C).
Note that the two equalities in the equations follow from (1) the fundamental
rule and (2) the marginalization operator described above.
By applying Bayes’ rule using P(A), P(B), and P(A | B) as speciﬁed
above, we get P(B | A) shown in Table 1.4.
P(B | A) = P (A | B)P (B)
P (A)
=
a1
a2
b1
0.4·0.4
0.4
0.6·0.4
0.6
b2
0.3·0.4
0.4
0.7·0.4
0.6
b3
0.6·0.2
0.4
0.4·0.2
0.6
=
a1
a2
b1 0.4 0.4
b2 0.3 0.47
b3 0.3 0.13
Table 1.4. The conditional probability P(B | A) obtained by applying Bayes’ rule
to P(A | B) in Table 1.1, P(A) = (0.4, 0.6), and P(B) = (0.4, 0.4, 0.2). Note that the
probabilities over B sum to 1 for each state of A.
The concept of (conditional) independence is also deﬁned for variables.
Deﬁnition 1.3 (Conditional independence for variables). Two vari-
ables A and C are said to be conditionally independent given the variable
B if
P(ai | ck, bj) = P(ai | bj)
for each ai ∈sp(A), bj ∈sp(B), and ck ∈sp(C).
As a shorthand notation we will sometimes write P(A | C, B) = P(A | B).
This means that when the state of B is known, then no knowledge of
C will alter the probability of A. Observe that we require the independence
statement to hold for each state of B; if the conditioning set is empty then we

1.3 Probability Calculus for Variables
11
say that A and C are marginally independent or just independent (written as
P(A | C) = P(A)).
When two variables A and C are conditionally independent given B, then
the fundamental rule (Theorem 1.3) can be simpliﬁed:
P(A, C | B) = P(A | B, C)P(C | B) = P(A | B)P(C | B).
In the expression above, we multiply two conditional probability tables over
diﬀerent domains. Fortunately, the method for doing this multiplication is a
straightforward extension of what we have done so far:
P(ai, ck | bj) = P(ai | bj)P(ck | bj).
For example, by multiplying P(A | B) and P(C | B) (speciﬁed in Table 1.1 and
Table 1.5, respectively) we get the joint probability P(A, C | B) in Table 1.6.
b1
b2
b3
c1 0.2
0.9 0.3
c2 0.05 0.05 0.2
c3 0.75 0.05 0.5
Table 1.5. The conditional probability table P(C | B) for the ternary variable C
given the ternary variable B.
P(A, C | B) = P(A | B)P(C | B)
=
b1
b2
b3
c1
(0.2 · 0.4, 0.2 · 0.6)
(0.9 · 0.3, 0.9 · 0.7)
(0.3 · 0.6, 0.3 · 0.4)
c2 (0.05 · 0.4, 0.05 · 0.6) (0.05 · 0.3, 0.05 · 0.7) (0.2 · 0.6, 0.2 · 0.4)
c3 (0.75 · 0.4, 0.75 · 0.6) (0.05 · 0.3, 0.05 · 0.7) (0.5 · 0.6, 0.5 · 0.4)
=
b1
b2
b3
c1 (0.08, 0.12)
(0.27, 0.63)
(0.18, 0.12)
c2 (0.02, 0.03) (0.015, 0.035) (0.12, 0.08)
c3 (0.3, 0.45) (0.015, 0.035)
(0.3, 0.2)
Table 1.6. If A and C are conditionally independent given B, then P(A, C | B)
can be found by multiplying P(A | B) and P(C | B) as speciﬁed in Table 1.1 and
Table 1.5, respectively.
1.3.1 Calculations with Probability Tables: An Example
To illustrate the theorems above, assume that we have three variables, A, B,
and C, with the probabilities as in Table 1.7. We receive evidence A = a2 and

12
1 Prerequisites on Probability Theory
C = c1 and we would now like to calculate the conditional probability table
P(B | a2, c1).
b1
b2
b3
a1 (0, 0.05, 0.05) (0.05, 0.05, 0) (0.05, 0.05, 0.05)
a2
(0.1, 0.1, 0)
(0.1, 0, 0.1)
(0.2, 0, 0.05)
Table 1.7. A joint probability table for the variables A, B, and C. The three
numbers in each entry correspond to the states c1, c2, and c3.
First, we focus on the part of the table corresponding to A = a2 and
C = c1, and we get
P(a2, B, c1) = (0.1, 0.1, 0.2).
(1.2)
To calculate P(B | a2, c1), we can use Theorem 1.4:
P(B | a2, c1) = P(a2, B, c1)
P(a2, c1)
=
P(a2, B, c1)

B P(a2, B, c1).
(1.3)
By marginalizing B out of equation (1.2) we get
P(a2, c1) = 0.1 + 0.1 + 0.2 = 0.4.
Finally, by performing the division in equation (1.3) we get
P(B | a2, c1) =
0.1
0.4, 0.1
0.4, 0.2
0.4

= (0.25, 0.25, 0.5).
Another way of doing the same is to say that we wish to transform P(a2, B, c1)
into a probability distribution. Because the numbers do not add up to one,
we normalize the distribution by dividing each number by the sum of all the
numbers.
Suppose now that we were given only the evidence A = a2, and we want
to calculate P(B | a2, C). The calculation of this probability table follows the
same steps as above, except that we now work with tables during the calcula-
tions. As before, we start by focusing on the part of P(A, B, C) corresponding
to A = a2 and we get the result in Table 1.8.
To calculate P(B | a2, C) we use
P(B | a2, C) = P(a2, B, C)
P(a2, C)
=
P(a2, B, C)

B P(a2, B, C).
(1.4)
The probability P(a2, C) is found by marginalizing B out of Table 1.8:
P(a2, C) = (0.1 + 0.1 + 0.2, 0.1 +0 + 0, 0 + 0.1 + 0.05) = (0.4, 0.1, 0.15), (1.5)
and by inserting this in equation (1.4) we get the result shown in Table 1.2.

1.4 An Algebra of Potentials
13
b1 b2
b3
c1 0.1 0.1 0.2
c2 0.1 0
0
c2
0 0.1 0.05
Table 1.8. The probability table P(a2, B, C) that corresponds to the part of the
probability table in Table 1.8 restricted to A = a2.
P(B | a2, C) =
b1
b2
b3
c1
0.1
0.4
0.1
0.4
0.2
0.4
c2
0.1
0.1
0
0.1
0
0.1
c2
0
0.15
0.1
0.15
0.05
0.15
=
b1
b2
b3
c1 0.25 0.25 0.5
c2
1
0
0
c2
0
2/3 1/3
Table 1.9. The calculation of P(B | a2, C) using P(a2, B, C) (Table 1.1) and
P(a2, C) (equation (1.5)).
1.4 An Algebra of Potentials
Below we list some properties of the algebra of multiplication and marginal-
ization of tables. The tables need not be (conditional) probabilities, and they
are generally called potentials.
A potential φ is a real-valued function over a domain of ﬁnite variables X:
φ : sp(X) →R
The domain of a potential is denoted by dom (φ). For example, the domain
of the potential P(A, B | C) is dom (P(A, B | C)) = {A, B, C}.
Two potentials can be multiplied, denoted by an (often suppressed) dot.
Multiplication has the following properties:
1. dom (φ1φ2) = dom (φ1) ∪dom (φ2).
2. The commutative law: φ1φ2 = φ2φ1.
3. The associative law: (φ1φ2)φ3 = φ1(φ2φ3).
4. Existence of unit: The unit potential 1 is a potential that contains only
1’s and is deﬁned over any domain such that 1·φ = φ, for all potentials φ.
The marginalization operator deﬁned in Section 1.3 can be generalized to po-
tentials so that 
A φ is a potential over dom(φ)\{A}. Furthermore, marginal-
ization is commutative:

A

B
φ =

B

A
φ.
For potentials of the form P(A | V), where V is a set of variables, we have
5. The unit potential property: 
A P(A | V) = 1.

14
1 Prerequisites on Probability Theory
For marginalization of a product, the following holds
6. The distributive law: If A /∈dom(φ1), then 
A φ1φ2 = φ1

A φ2.
The distributive law is usually known as ab + ac = a(b + c), and the
preceding formula is actually the same law applied to tables. To verify it,
consider the calculations in Tables 1.10–1.14. Here we see that Table 1.12 and
Table 1.14 are equal and correspond to the left-hand and right-hand sides of
the distributive law.
B \ A a1 a2
b1
x1 x2
b2
x3 x4
B \ C c1 c2
b1
y1 y2
b2
y3 y4
Table 1.10. φ1(A, B) and φ2(C, B).
B \ A
a1
a2
b1
(x1y1, x1y2) (x2y1, x2y2)
b2
(x3y3, x3y4) (x4y3, x4y4)
Table 1.11. φ1(A, B) · φ2(C, B). The two numbers in each entry correspond to the
states c1 and c2.
B \ A
a1
a2
b1
x1y1 + x1y2 x2y1 + x2y2
b2
x3y3 + x3y4 x4y3 + x4y4
Table 1.12. P
C φ1(A, B) · φ2(C, B).
B
b1 y1 + y2
b2 y3 + y4
Table 1.13. P
C φ2(C, B).
We also use the term projection for marginalization. For example, if A
and B are marginalized out of φ(A, B, C), we may say that φ is projected
down to C, and we use the notation φ↓C. With this notation, the properties
of marginalization look as follows (V and W denote sets of variables):

1.5 Random Variables
15
B \ A
a1
a2
b1
x1(y1 + y2) x2(y1 + y2)
b2
x3(y3 + y4) x4(y3 + y4)
Table 1.14. φ1(A, B) P
C φ2(C, B).
7. The commutative law: (φ↓V)↓W = (φ↓W)↓V.
8. The distributive law: If dom(φ1) ⊆V, then (φ1φ2)↓V = φ1(φ↓V
2 ).
1.5 Random Variables
Let S be a sample space. A random variable is a real-valued function on S;
V : S →R. If, for example, you throw a die, and you win $1 if you get 4 or
above, and you lose $1 if you get 3 or below, then the corresponding random
variable is a function with value −1 on {1, 2, 3} and 1 on {4, 5, 6}.
The mean value of a random variable V on S is deﬁned as
μ(V ) =

s∈S
V (s)P(s).
(1.6)
For the example above, the mean value is −1 1
6 +−1 1
6 +−1 1
6 + 1
6 + 1
6 + 1
6 = 0
(provided that the die is fair). The mean value is also called the expected value.
A measure of how much a random variable varies between its values is the
variance, σ2. It is deﬁned as the mean of the square of the diﬀerence between
value and mean:
σ2(V ) =

s∈S
(V (s) −μ(V ))2P(s).
(1.7)
For the example above we have
σ2 = 3(−1 −0)2 1
6 + 3(1 −0)2 1
6 = 1.
1.5.1 Continuous Distributions
Consider an experiment, where an arrow is thrown at the [0, 1]×[0, 1] square.
The possible outcomes are the points (x, y) in the unit square. Since the
probability is zero for any particular outcome, the probability distribution
is assigned to subsets of the unit square. We may think of this assignment
as a process of distributing a probability mass of 1 over the sample space.
We may, for example, assign a probability for landing in the small square
[x, x+ϵ]×[y, y+ϵ]. To be more systematic, let n be a natural number, then the
unit square can be partitioned into small squares of the type [ i
n, i+1
n ]×[ j
n, j+1
n ],
and we can assign probabilities P([ i
n, i+1
n ]×[ j
n, j+1
n ]) to these squares with area

16
1 Prerequisites on Probability Theory
1
n2 . Now, if P([ i
n, i+1
n ] × [ j
n, j+1
n ]) = x, then you can say that the probability
mass x is distributed over the small square with an average density of n2x,
and we deﬁne the density function (also called the frequency function) f(x, y)
as
f(x, y) = lim
n→∞n2P

x, x + 1
n

×

y, y + 1
n

.
In general, if S is a continuous sample space, the density function is a
nonnegative real-valued function f on S, for which it holds that for any subset
A of S,

A
f(s)ds = P(A).
In particular,

S
f(s)ds = 1.
When S is an interval [a, b] (possibly inﬁnite), the outcomes are real num-
bers (such as height or weight), and you may be interested in the mean (height
or weight). It is deﬁned as
μ =
 b
a
xf(x)dx,
and the variance is given by
σ2 =
 b
a
(μ −x)2f(x)dx.
Mathematically, the mean and variance are the mean and variance of the
identity function I(x) = x, but we use the term “mean and variance of the
distribution.”
1.6 Exercises
Exercise 1.1. Given Axioms 1 to 3, prove that
P(A ∪B) = P(A) + P(B) −P(A ∩B) .
Exercise 1.2. Consider the experiment of rolling a red and a blue fair six-
sided die. Give an example of a sample space for the experiment along with
probabilities for each outcome. Suppose then that we are interested only in
the sum of the dice (that is, the experiment consists in rolling the dice and
adding up the numbers). Give another example of a sample space for this
experiment and probabilities for the outcomes.

1.6 Exercises
17
Exercise 1.3. Consider the experiment of ﬂipping a fair coin, and if it lands
heads, rolling a fair four-sided die, and if it lands tails, rolling a fair six-sided
die. Suppose that we are interested only in the number rolled by the die,
and a sample space SA for the experiment could thus be the numbers from
1 to 6. Another sample space could be SB = {t1, . . ., t6, h1, . . . , h4}, with for
example t2 meaning “tails and a roll of 2” and h4 meaning “heads and a roll
of 4.” Choose either SA or SB and associate probabilities with it. According
to your sample space and probability distribution, what is the probability of
rolling either 3 or 5.
Exercise 1.4. Draw a Venn diagram (like that in Figure 1.1) over SB deﬁned
in Exercise 1.3. The diagram should show the events corresponding to “rolling
a 3,” “ﬂipping tails,” and “ﬂipping tails and rolling a 3.”
Exercise 1.5. Let SB be deﬁned as in Exercise 1.3, but with a loaded coin
and loaded dice. A probability distribution is given in Table 1.15. What is
the probability that the loaded coin lands “tails”? What is the conditional
probability of rolling a 4, given that the coin lands tails? Which of the loaded
dice has the highest chance of rolling 4 or more?
t1
5
18 t6
1
18
t2
1
9 h1
1
24
t3
1
9 h2
1
24
t4
1
18 h3
1
8
t5
1
18 h4
1
8
Table 1.15. Probabilities for SB in Exercise 1.5.
Exercise 1.6. Prove that
P(A | B ∪C)P(B | C) = P(A ∩B | C) .
Exercise 1.7. A farmer has a cow, which he suspects is pregnant. He admin-
isters a test to the urine of the cow to determine whether it is pregnant. There
are four outcomes in this experiment:
1. The cow is pregnant and the test is positive.
2. The cow is pregnant, but the test is negative.
3. The cow is not pregnant, but the test is positive.
4. The cow is not pregnant, and the test is negative.
The prior probability of the event that the cow is pregnant is 0.05, the prob-
ability of the event that the test is positive, when the cow indeed is pregnant,
is 0.98 and the probability that the test is negative, when the cow is not
pregnant, is 0.999. The test turns out to be positive. What is the posterior
probability of the cow being pregnant?

18
1 Prerequisites on Probability Theory
Exercise 1.8. Consider the following two experiments: One consists in throw-
ing a red six-sided die, and one consists in throwing a blue six-sided die.
We let R be a variable representing the roll of the red die, having a set of
states {r1, r2, r3, r4, r5, r6}, and B be a variable representing the roll of the
blue die (states {b1, b2, b3, b4, b5, b6}). Assume that the red die is fair so that
P(R = r1) = · · · = P(R = r6) =
1
6, and that the variable for the blue
die has probabilities P(B = b1) = P(B = b2) = P(B = b3) =
1
12 and
P(B = b4) = P(B = b5) = P(B = b6) = 1
4. Give an example of a sample
space for an experiment consisting of throwing both the red and the blue die.
Using P(R) and P(B), what is the probability distribution for your sample
space?
Exercise 1.9. Consider the sample space SB from Exercise 1.3, with probabil-
ity distribution as deﬁned in Table 1.15. Recast the sample space as variables.
What is the probability distribution for each variable?
Exercise 1.10. Prove the fundamental rule for variables:
P(A, B) = P(A | B)P(B) .
Exercise 1.11. Calculate P(A), P(B), P(A | B), and P(B | A) from the table
for P(A, B) (Table 1.16).
b1
b2
b3
a1 0.05 0.10 0.05
a2 0.15 0.00 0.25
a3 0.10 0.20 0.10
Table 1.16. P(A, B) for Exercise 1.11.
Exercise 1.12. Table 1.17 describes a test T for an event A. The number
0.01 is the frequency of false negatives, and the number 0.001 is the frequency
of false positives.
(i) The police can order a blood test on drivers under the suspicion of having
consumed too much alcohol. The test has the above characteristics. Expe-
rience says that 20% of the drivers under suspicion do in fact drive with
too much alcohol in their blood. A suspicious driver has a positive blood
test. What is the probability that the driver is guilty of driving under the
inﬂuence of alcohol?
(ii)The police block a road, take blood samples of all drivers, and use the same
test. It is estimated that one out of 1,000 drivers have too much alcohol
in their blood. A driver has a positive test result. What is the probability
that the driver is guilty of driving under the inﬂuence of alcohol?

1.6 Exercises
19
A = yes A = no
T = yes
0.99
0.001
T = no
0.01
0.999
Table 1.17. Table for Exercise 1.12. Conditional probabilities P(T | A) character-
izing test T for A.
Exercise 1.13. In Table 1.18, a joint probability table for the binary variables
A, B, and C is given.
•
Calculate P(B, C) and P(B).
•
Are A and C independent given B?
b1
b2
a1 (0.006, 0.054) (0.048, 0.432)
a2 (0.014, 0.126) (0.032, 0.288)
Table 1.18. P(A, B, C) for Exercise 1.13.
Exercise 1.14. Write a short algorithm that given an n×m potential φ(A, B)
calculates 
A φ. Use your algorithm on the joint probability table P(A, B)
in Table 1.2 and on the conditional probability table P(A|B) in Table 1.1.
Exercise 1.15. Prove that the associative, commutative, and distributive
laws hold for potentials.
Exercise 1.16. Let φ(x) = ax be a distribution on [0, 1]. Determine a. What
are the mean and the variance of φ?
Exercise 1.17. Let φ(x) = a sin(x) be a distribution on [0, π]. Determine a
and the mean of φ.

Part I
Probabilistic Graphical Models

2
Causal and Bayesian Networks
In this chapter we introduce causal networks, which are the basic graphi-
cal feature for (almost) everything in this book. We give rules for reasoning
about relevance in causal networks; is knowledge of A relevant for my belief
about B? These sections deal with reasoning under uncertainty in general.
Next, Bayesian networks are deﬁned as causal networks with the strength of
the causal links represented as conditional probabilities. Finally, the chain
rule for Bayesian networks is presented. The chain rule is the property that
makes Bayesian networks a very powerful tool for representing domains with
inherent uncertainty. The sections on Bayesian networks assume knowledge of
probability calculus as laid out in Sections 1.1–1.4.
2.1 Reasoning Under Uncertainty
2.1.1 Car Start Problem
The following is an example of the type of reasoning that humans do daily.
“In the morning, my car will not start. I can hear the starter turn, but
nothing happens. There may be several reasons for my problem. I can hear
the starter roll, so there must be power from the battery. Therefore, the most-
probable causes are that the fuel has been stolen overnight or that the spark
plugs are dirty. It may also be due to dirt in the carburetor, a loose connection
in the ignition system, or something more serious. To ﬁnd out, I ﬁrst look at
the fuel meter. It shows half full, so I decide to clean the spark plugs.”
To have a computer do the same kind of reasoning, we need answers to
questions such as, “What made me conclude that among the probable causes
“stolen fuel”, and “dirty spark plugs” are the two most-probable causes?” or
“What made me decide to look at the fuel meter, and how can an observation
concerning fuel make me conclude on the seemingly unrelated spark plugs?”
To be more precise, we need ways of representing the problem and ways of

24
2 Causal and Bayesian Networks
performing inference in this representation such that a computer can simulate
this kind of reasoning and perhaps do it better and faster than humans.
For propositional logic, Boolean logic is the representation framework, and
various derived structures, such as truth tables and binary decision diagrams,
have been invented together with eﬃcient algorithms for inference.
In logical reasoning, we use four kinds of logical connectives: conjunction,
disjunction, implication, and negation. In other words, simple logical state-
ments are of the kind, “if it rains, then the lawn is wet,” “both John and Mary
have caught the ﬂu,” “either they stay at home or they go to the cinema,” or
“the lawn is not wet.” From a set of logical statements, we can deduce new
statements. From the two statements “if it rains, then the lawn is wet” and
“the lawn is not wet,” we can infer that it is not raining.
When we are dealing with uncertain events, it would be nice if we could
use similar connectives with certainties rather than truth values attached, so
we may extend the truth values of propositional logic to “certainties,” which
are numbers between 0 and 1. A certainty 0 means “certainly not true,” and
the higher the number, the higher the certainty. Certainty 1 means “certainly
true.”
We could then work with statements such as, “if I take a cup of coﬀee
while on break, I will with certainty 0.5 stay awake during the next lecture”or
“if I take a short walk during the break, I will with certainty 0.8 stay awake
during the next lecture.” Now, suppose I take a walk as well as have a cup
of coﬀee. How certain can I be to stay awake? To answer this, I need a rule
for how to combine certainties. In other words, I need a function that takes
the two certainties 0.5 and 0.8 and returns a number, which should be the
certainty resulting from combining the certainty from the two statements.
The same is needed for chaining: “if a then b with certainty x,” and “if b
then c with certainty y.” I know a, so what is the certainty of c?
It has turned out that any function for combination and chaining will in
some situations lead to wrong conclusions.
Another problem, which is also a problem for logical reasoning, is abduc-
tion: I have the rule “a woman has long hair with certainty 0.7.” I see a
long-haired person. What can I infer about the person’s sex?
2.1.2 A Causal Perspective on the Car Start Problem
A way of structuring a situation for reasoning under uncertainty is to construct
a graph representing causal relations between events.
Example 2.1 (A reduced Car Start Problem).
To simplify the situation, assume that we have the events {yes, no} for
Fuel?, {yes, no} for Clean Spark Plugs?, {full, 1
2, empty} for Fuel Meter, and
{yes, no} for Start?. In other words, the events are clustered around vari-
ables, each with a set of outcomes, also called states. We know that the
state of Fuel? and the state of Clean Spark Plugs? have a causal impact on

2.1 Reasoning Under Uncertainty
25
the state of Start?. Also, the state of Fuel? has an impact on the state of
Fuel Meter Standing. This is represented by the graph in Figure 2.1.
Fuel Meter Standing
Fuel?
Start?
Clean Spark Plugs
Fig. 2.1. A causal network for the reduced Car Start Problem.
If we add a direction from no to yes inside each variable (and from empty to
full), we can also represent directions of the impact. For the present situation,
we can say that all the impacts are positive (with the direction); that is, the
more the certainty of the cause is moved in a positive direction, the more the
certainty of the aﬀected variable will also be moved in a positive direction. To
indicate this, we can label the links with the sign “+” as is done in Figure 2.2.
Fuel Meter Standing
Fuel?
Start?
Clean Spark Plugs
+
+
+
Fig. 2.2. A causal network for the reduced Car Start Problem with a sign indicating
direction of impact.
We can use the graph in Figure 2.2 to perform some reasoning. Obviously,
if I know that the spark plugs are not clean, then the certainty for no start
will increase. However, my situation is the opposite. I realize that I have a
start problem. As my certainty on Start? is moved in a negative direction, I
ﬁnd the possible causes (Clean Spark Plugs? and Fuel?) for such a move more
certain; that is, the sign “+” is valid for both directions. Now, because the
certainty on for Fuel? = no has increased, I will have a higher expectation
that Fuel Meter Standing is in state empty.
The movement of the certainty for Fuel Meter Standing tells me that by
reading the fuel meter I will get information related to the start problem. I
read the fuel meter, it says 1
2, and reasoning backward yields that the certainty
on Fuel? is moved in a negative direction.
So far, the reasoning has been governed by simple rules that can easily
be formalized. The conclusion is harder: “Lack of fuel does not seem to be
the reason for my start problem, so most probably the spark plugs are not
clean.” Is there a formalized rule that allows this kind of reasoning on a causal

26
2 Causal and Bayesian Networks
network to be computerized? We will return to this problem in Section 2.2.
Note: The reasoning has focused on changes of certainty. In certainty calculus,
if the actual certainty of a speciﬁc event must be calculated, then knowledge
of certainties prior to any information is also needed. In particular, prior
certainties are required for the events that are not eﬀects of causes in the
network. If, for example, my car cannot start, the actual certainty that the
fuel has been stolen depends on my neighborhood.
2.2 Causal Networks and d-Separation
A causal network consists of a set of variables and a set of directed links
(also called arcs) between variables. Mathematically, the structure is called a
directed graph. When talking about the relations in a directed graph, we use
the wording of family relations: if there is a link from A to B, we say that B
is a child of A, and A is a parent of B.
The variables represent propositions (or sample spaces), see also Sec-
tion 1.3. A variable can have any number of states (or outcomes). A vari-
able may, for example, be the color of a car (states blue, green, red, brown),
the number of children in a speciﬁc family (states 0, 1, 2, 3, 4, 5, 6, > 6), or
a disease (states bronchitis, tuberculosis, lung cancer). Variables may have a
countable or a continuous state set, but we consider only variables with a
ﬁnite number of states (we shall return to the issue of continuous state spaces
in Section 3.3.8).
In a causal network, a variable represents a set of possible states of aﬀairs.
A variable is in exactly one of its states; which one may be unknown to us.
As illustrated in Section 2.1.2, causal networks can be used to follow how
a change of certainty in one variable may change the certainty for other vari-
ables. We present in this section a set of rules for that kind of reasoning. The
rules are independent of the particular calculus for uncertainty.
Serial Connections
Consider the situation in Figure 2.3. Here A has an inﬂuence on B, which in
turn has an inﬂuence on C. Obviously, evidence about A will inﬂuence the
certainty of B, which then inﬂuences the certainty of C. Similarly, evidence
about C will inﬂuence the certainty of A through B. On the other hand, if
the state of B is known, then the channel is blocked, and A and C become
independent; we say that A and C are d-separated given B. When the state
of a variable is known, we say that the variable is instantiated.
We conclude that evidence may be transmitted through a serial connection
unless the state of the variable in the connection is known.

2.2 Causal Networks and d-Separation
27
A
B
C
Fig. 2.3. Serial connection. When B is instantiated, it blocks communication be-
tween A and C.
Example 2.2. Figure 2.4 shows a causal model for the relations between
Rainfall (no, light, medium, heavy), Water level (low, medium, high), and
Flooding (yes, no). If I have not observed the water level, then knowing that
there has been a ﬂooding will increase my belief that the water level is high,
which in turn will tell me something about the rainfall. The same line of
reasoning holds in the other direction. On the other hand, if I already know
the water level, then knowing that there has been ﬂooding will not tell me
anything new about rainfall.
Rainfall
Water level
Flooding
Fig. 2.4. A causal model for Rainfall, Water level, and Flooding.
Diverging Connections
The situation in Figure 2.5 is called a diverging connection. Inﬂuence can
pass between all the children of A unless the state of A is known. That is,
B, C, . . . , E are d-separated given A.
Evidence may be transmitted through a diverging connection unless it is
instantiated.
...
A
B
C
E
Fig. 2.5. Diverging connection. If A is instantiated, it blocks communication be-
tween its children.
Example 2.3. Figure 2.6 shows the causal relations between Sex (male, female),
length of hair (long, short), and stature (<168 cm, ≥168 cm).

28
2 Causal and Bayesian Networks
Hair length
Sex
Stature
Fig. 2.6. Sex has an impact on length of hair as well as stature.
If we do not know the sex of a person, seeing the length of his/her hair will
tell us more about the sex, and this in turn will focus our belief on his/her
stature. On the other hand, if we know that the person is a man, then the
length of his hair gives us no extra clue on his stature.
Converging Connections
A description of the situation in Figure 2.7 requires a little more care. If
nothing is known about A except what may be inferred from knowledge of
its parents B, . . . , E, then the parents are independent: evidence about one
of them cannot inﬂuence the certainties of the others through A. Knowledge
of one possible cause of an event does not tell us anything about the other
possible causes. However, if anything is known about the consequences, then
information on one possible cause may tell us something about the other
causes. This is the explaining away eﬀect illustrated in the car start problem:
the car cannot start, and the potential causes include dirty spark plugs and
an empty fuel tank. If we now get the information that there is fuel in the
tank, then our certainty in the spark plugs being dirty will increase (since this
will explain why the car cannot start). Conversely, if we get the information
that there is no fuel on the car, then our certainty in the spark plugs being
dirty will decrease (since the lack of fuel explains why the car cannot start).
In Figure 2.8, two examples are shown. Observe that in the second example
we observe only A indirectly through information about F; knowing the state
of F tells us something about the state of E, which in turn tells us something
about A.
A
B
C
E
Fig. 2.7. Converging connection. If A changes certainty, it opens communication
between its parents.

2.2 Causal Networks and d-Separation
29
B
A
C
A
B
C
F
E
e
e
Fig. 2.8. Examples in which the parents of A are dependent. The dotted lines
indicate insertion of evidence.
The conclusion is that evidence may be transmitted through a converging
connection only if either the variable in the connection or one of its descen-
dants has received evidence.
Remark: Evidence about a variable is a statement of the certainties of its
states. If the variable is instantiated, we call it hard evidence; otherwise, it is
called soft. In the example above, we can say that hard evidence about the
variable F provides soft evidence about the variable A. Blocking in the case
of serial and diverging connections requires hard evidence, whereas opening
in the case of converging connections holds for all kinds of evidence.
Example 2.4. Figure 2.9 shows the causal relations among Salmonella infec-
tion, ﬂu, nausea, and pallor.
Salmonella
Nausea
Pallor
Flu
Fig. 2.9. Salmonella and ﬂu may cause nausea, which in turn causes pallor.
If we know nothing of nausea or pallor, then the information on whether
the person has a Salmonella infection will not tell us anything about ﬂu.
However, if we have noticed that the person is pale, then the information
that he/she does not have a Salmonella infection will make us more ready to
believe that he/she has the ﬂu.

30
2 Causal and Bayesian Networks
2.2.1 d-separation
The three preceding cases cover all ways in which evidence may be transmitted
through a variable, and following the rules it is possible to decide for any
pair of variables in a causal network whether they are independent given the
evidence entered into the network. The rules are formulated in the following
deﬁnition.
Deﬁnition 2.1 (d-separation). Two distinct variables A and B in a causal
network are d-separated (“d” for “directed graph”) if for all paths between A
and B, there is an intermediate variable V (distinct from A and B) such that
either
−the connection is serial or diverging and V is instantiated
or
−the connection is converging, and neither V nor any of V ’s descendants
have received evidence.
If A and B are not d-separated, we call them d-connected.
Figure 2.10 gives an example of a larger network. The evidence entered
at B and M represents instantiations. If evidence is entered at A, it may
be transmitted to D. The variable B is blocked, so the evidence cannot pass
through B to E. However, it may be passed to H and K. Since the child M
of K has received evidence, evidence from H may pass to I and further to
E, C, F, J, and L, so the path A −D −H −K −I −E −C −F −J −L is a
d-connecting path. Figure 2.11 gives two other examples.
Note that although A and B are d-connected, changes in the belief in
A will not necessarily change the belief in B. To stress this diﬀerence, we
will sometimes say that A and B are structurally independent if they are
d-separated (see also Exercise 2.23).
In connection to d-separation, a special set of nodes for a node A is the
so-called Markov blanket for A:
Deﬁnition 2.2. The Markov blanket of a variable A is the set consisting of
the parents of A, the children of A, and the variables sharing a child with A.
The Markov blanket has the property that when instantiated, A is d-
separated from the rest of the network (see Figure 2.12).
You may wonder why we have introduced d-separation as a deﬁnition
rather than as a theorem. A theorem should be as follows.
Claim: If A and B are d-separated, then changes in the certainty of A have
no impact on the certainty of B.
However, the claim cannot be established as a theorem without a more-
precise description of the concept of “certainty.” You can take d-separation as
a property of human reasoning and require that any certainty calculus should
comply with the claim.

2.2 Causal Networks and d-Separation
31
A
B
C
D
E
F
G
H
I
J
K
L
M
e
e
Fig. 2.10. A causal network with M and B instantiated. The node A is d-separated
from G only.
A
B
C
D
E
F
G
H
A
B
C
D
E
F
G
e
e
e
e
e
e
e
(a)
(b)
Fig. 2.11. Causal networks with hard evidence entered (the variables are instan-
tiated). (a) Although all neighbors of E are instantiated, it is d-connected to F, B,
and A. (b) F is d-separated from the remaining uninstantiated variables.

32
2 Causal and Bayesian Networks
A
B
C
D
E
F
G
H
I
J
K
L
M
N
Fig. 2.12. The Markov blanket for I is {C, E, H, K, L}. Note that if only I’s neigh-
bors are instantiated, then J is not d-separated from I.
From the deﬁnition of d-separation we see that in order to test whether
two variables, say A and B, are d-separated given hard evidence on a set
of variables C you would have to check whether all paths connecting A and
B are d-separating paths. An easier way of performing this test, without
having to consider the various types of connections, is as follows: First you
construct the so-called ancestral graph consisting of A, B, and C together
with all nodes from which there is a directed path to either A, B, or C (see
Figure 2.13(a)). Next, you insert an undirected link between each pair of nodes
with a common child and then you make all links undirected. The resulting
graph (see Figure 2.13(b)) is known as the moral graph for Figure 2.13(a). The
moral graph can now be used to check whether A and B are d-separated given
C: if all paths connecting A and B intersect C, then A and B are d-separated
given C.
The above procedure generalizes straightforwardly to the case in which we
work with sets of variables rather than single variables: you just construct the
ancestral graph using these sets of variables and perform the same steps as
above: A and B are then d-separated given C if all paths connecting a variable
in A with a variable in B intersect a variable in C.
2.3 Bayesian Networks
2.3.1 Deﬁnition of Bayesian Networks
Causal relations also have a quantitative side, namely their strength. This can
be expressed by attaching numbers to the links.

2.3 Bayesian Networks
33
A
A
B
B
C
C
D
D
E
E
F
F
H
H
I
I
K
K
M
M
(a)
(b)
Fig. 2.13. To test whether A is d-separated from F given evidence on B and M
in Figure 2.10, we ﬁrst construct the ancestral graph for {A, B, F, M} (ﬁgure (a)).
Next we add an undirected link between pairs of nodes with a common child and
then the direction is dropped on all links (ﬁgure (b)). In the resulting graph we have
that the path A −D −H −K −I −E −C −F does not intersect B and M, hence
A and F are d-connected given B and M.
Let A be a parent of B. Using probability calculus, it would be natural to
let P(B | A) be the strength of the link. However, if C is also a parent of B,
then the two conditional probabilities P(B | A) and P(B | C) alone do not give
any clue about how the impacts from A and C interact. They may cooperate
or counteract in various ways, so we need a speciﬁcation of P(B | A, C).
It may happen that the domain to be modeled contains causal feedback
cycles (see Figure 2.14).
Feedback cycles are diﬃcult to model quantitatively. For causal networks,
no calculus has been developed that can cope with feedback cycles, but certain
noncausal models have been proposed to deal with this issue. For Bayesian
networks we require that the network does not contain cycles.
Deﬁnition 2.3. A Bayesian network consists of the following:
−A set of variables1 and a set of directed edges between variables.
−Each variable has a ﬁnite set of mutually exclusive states.
−The variables together with the directed edges form an acyclic directed
graph (traditionally abbreviated DAG); a directed graph is acyclic if there
is no directed path A1 →· · · →An so that A1 = An.
1 When we wish to emphasize that this kind of variable represents a sample space
we call it a chance variable.

34
2 Causal and Bayesian Networks
A
B
C
D
E
F
G
Fig. 2.14. A directed graph with a feedback cycle. This is not allowed in Bayesian
networks.
−To each variable A with parents B1, . . . , Bn, a conditional probability table
P(A | B1, . . . , Bn) is attached.
Note that if A has no parents, then the table reduces to the unconditional
probability table P(A). For the DAG in Figure 2.15, the prior probabilities
P(A) and P(B) must be speciﬁed. It has been claimed that prior probabilities
are an unwanted introduction of bias to the model, and calculi have been
invented in order to avoid it. However, as discussed in Section 2.1.2, prior
probabilities are necessary not for mathematical reasons but because prior
certainty assessments are an integral part of human reasoning about certainty
(see also Exercise 1.12).
A
B
C
D
E
F
G
Fig. 2.15. A directed acyclic graph (DAG). The probabilities to specify are P(A),
P(B), P(C | A, B), P(E | C), P(D | C), P(F | E), and P(G | D, E, F).
The deﬁnition of Bayesian networks does not refer to causality, and there is
no requirement that the links represent causal impact. That is, when building
the structure of a Bayesian network model, we need not insist on having the

2.3 Bayesian Networks
35
links go in a causal direction. However, we then need to check the model’s
d-separation properties and ensure that they correspond to our perception
of the world’s conditional independence properties. The model should not
include conditional independences that do not hold in the real world.
This also means that if A and B are d-separated given evidence e, then
the probability calculus used for Bayesian networks must yield P(A | e) =
P(A | B, e) (see Section 2.3.2).
Example 2.5 (A Bayesian network for the Car Start Problem).
The Bayesian network for the reduced Car Start Problem is the one in
Figure 2.16.
FM
Fu
St
SP
Fig. 2.16. The causal network for the reduced car start problem. We have used
the abbreviations Fu (Fuel?), SP (Clean Spark Plugs?), St (Start?), and FM (Fuel
Meter Standing).
For the quantitative modeling, we need the probability assessments P(Fu),
P(SP), P(St | Fu, SP), P(FM | Fu). To avoid having to deal with numbers that
are too small, let P(Fu) = (0.98, 0.02) and P(SP) = (0.96, 0.04). The re-
maining tables are given in Table 2.1. Note that the table for P(FM | Fu)
reﬂects the fact that the fuel meter may be malfunctioning, and the table for
P(St | Fu, SP) leaves room for causes other than no fuel and dirty spark plugs
by assigning P(St = no | Fu = yes, SP = yes) > 0.
2.3.2 The Chain Rule for Bayesian Networks
Let U = {A1, . . . , An} be a universe of variables. If we have access to the joint
probability table P(U) = P(A1, . . . , An), then we can also calculate P(Ai)
as well as P(Ai | e), where e is evidence about some of the variables in the
Bayesian network (see, e.g., Section 1.3.1). However, P(U) grows exponen-
tially with the number of variables, and U need not be very large before the
table becomes intractably large. Therefore, we look for a more compact rep-
resentation of P(U), i.e., a way of storing information from which P(U) can
be calculated if needed.
Let BN be a Bayesian network over U, and let P(U) be a probability dis-
tribution reﬂecting the properties speciﬁed by BN: (i) the conditional prob-
abilities for a variable given its parents in P(U) must be as speciﬁed in BN,
and (ii) if the variables A and B are d-separated in BN given the set C, then
A and B are independent given C in P(U).

36
2 Causal and Bayesian Networks
Fu = yes Fu = no
FM = full
0.39
0.001
FM = 1
2
0.60
0.001
FM = empty
0.01
0.998
P(FM | Fu)
Fu = yes Fu = no
Sp = yes (0.99, 0.01)
(0,1)
Sp = no (0.01, 0.99)
(0,1)
P(St | Fu, Sp)
Table 2.1. Conditional probabilities for the model in Figure 2.16. The numbers
(x, y) in the lower table represent (St = yes, St = no).
Based on these two properties, what other properties can be deduced about
P(U)? If the universe consists of only one variable A, then BN speciﬁes P(A),
and P(U) is uniquely determined. We shall show that this holds in general.
For probability distributions over sets of variables, we have an equation
called the chain rule. For Bayesian networks this equation has a special form.
First we state the general chain rule:
Proposition 2.1 (The general chain rule).
Let U = {A1, . . . , An} be a
set of variables. Then for any probability distribution P(U) we have
P(U) = P(An | A1, . . . , An−1)P(An−1 | A1, . . . , An−2) . . . P(A2 | A1)P(A1).
Proof.
Iterative use of the fundamental rule:
P(U) = P(An | A1, . . . , An−1)P(A1, . . . , An−1),
P(A1, . . . , An−1) = P(An−1 | A1, . . . , An−2)P(A1, . . . , An−2),
...
P(A1, A2) = P(A2 | A1)P(A1).
⊓⊔
Theorem 2.1 (The chain rule for Bayesian networks). Let BN be a
Bayesian network over U = {A1, . . . , An}. Then BN speciﬁes a unique joint
probability distribution P(U) given by the product of all conditional probability
tables speciﬁed in BN:
P(U) =
n
	
i=1
P(Ai | pa(Ai)),

2.3 Bayesian Networks
37
where pa(Ai) are the parents of Ai in BN, and P(U) reﬂects the properties
of BN.
Proof.
First we should show that P(U) is indeed a probability distribution.
That is, we need to show that Axioms 1–3 hold. This is left as an exercise (see
Exercise 2.15).
Next we prove that the speciﬁcation of BN is consistent, so that P(U)
reﬂects the properties of BN. It is not hard to prove that the probability
distribution speciﬁed by the product in the chain rule reﬂects the conditional
probabilities from BN (see Exercise 2.16). We also need to prove that the
product reﬂects the d-separation properties. This is done through induction
in the number of variables in BN.
When BN has one variable, it is obvious that the d-separation properties
speciﬁed by BN hold for the product of all speciﬁed conditional probabilities.
Assume that for any Bayesian network with n −1 variables and a distri-
bution P(U) speciﬁed as the product of all conditional probabilities, it holds
that if A and B are d-separated given C, then P(A | B, C) = P(A | C). Let
BN be a Bayesian network with n variables {A1, . . . , An}. Assume that An
has no children and let BN ′ be the result of removing An from BN. Clearly
BN ′ is a Bayesian network with the same conditional probability distribu-
tions as BN (except for An) and with the same d-separation properties over
{A1, . . . , An−1} as BN. Moreover,
P(U \ {An}) =

An
P(U) =

An
n
	
i=1
P(Ai | pa(Ai))
=
n−1
	
i=1
P(Ai | pa(Ai))

An
P(An | pa(An))
=
n−1
	
i=1
P(Ai | pa(Ai))1 =
n−1
	
i=1
P(Ai | pa(Ai)),
and by the induction hypothesis P(U \ {An}) reﬂects the properties of BN ′.
Now, if A and B are d-separated given C in BN, then they are also d-separated
in BN ′, and therefore P(A | B, C) = P(A | C). To prove that it also holds for
d-separation properties involving An, we consider the case in which An ∈C
and the case in which A = An. For the ﬁrst case we have that since An
participates only in a converging connection, it holds that if A and B are
d-separated given C, then they are also d-separated given C \{An} and we get
the situation above. For the second case, we ﬁrst note that
P(An | B, C) =

pa(An)
P(An | B, C, pa(An))P(pa(An) | B, C).
Now, if An and B are d-separated given C, then pa(An) and B are also d-
separated given C, and since An is not involved, we have P(pa(An) | B, C) =

38
2 Causal and Bayesian Networks
P(pa(An) | C). So we need to prove only that P(An | B, C, pa(An)) = P(An |
pa(An)). Using the fundamental rule and the chain rule, we get
P(An | B, C, pa(An)) = P(An, B, C, pa(An))
P(B, C, pa(An))
=

U\{An,B,C,pa(An)} P(U)

U\{B,C,pa(An)} P(U)
=

U\{An,B,C,pa(An)}

n
i=1 P(Ai | pa(Ai))

U\{B,C,pa(An)}

n
i=1 P(Ai | pa(Ai))
=
P(An | pa(An)) 
U\{An,B,C,pa(An)}

n−1
i=1 P(Ai | pa(Ai))

U\{An,B,C,pa(An)}

n−1
i=1 P(Ai | pa(Ai)) 
An P(An | pa(An))
=
P(An | pa(An)) 
U\{An,B,C,pa(An)}

n−1
i=1 P(Ai | pa(Ai))

U\{An,B,C,pa(An)}

n−1
i=1 P(Ai | pa(Ai))1
= P(An | pa(An)).
To prove uniqueness, let {A1, . . . , An} be a topological ordering of the
variables. Then, for each variable Ai with parents pa(Ai) we have that Ai
is d-separated from {A1, . . . , Ai−1} \ pa(Ai) given pa(Ai) (see Exercise 2.11).
This means that for any distribution P reﬂecting the speciﬁcations by BN
we must have P(Ai | A1, . . . , Ai−1) = P(Ai | pa(Ai). Substituting this in the
general chain rule yields that any distribution reﬂecting the speciﬁcations by
BN must be the product of the conditional probabilities speciﬁed in BN. ⊓⊔
The chain rule yields that a Bayesian network is a compact representation
of a joint probability distribution. The following example illustrates how to
exploit that for reasoning under uncertainty.
Example 2.6 (The Car Start Problem revisited).
In this example, we apply the rules of probability calculus to the Car
Start Problem. This is done to illustrate that probability calculus can be used
to perform the reasoning in the example, in particular, explaining away. In
Chapter 4, we give general algorithms for probability updating in Bayesian
networks. We will use the Bayesian network from Example 2.5 to perform the
reasoning in Section 2.1.1.
We will use the joint probability table for the reasoning. The joint proba-
bility table is calculated from the chain rule for Bayesian networks,
P(Fu, FM, SP, St) = P(Fu)P(SP)P(FM | Fu)P(St | Fu, SP).
The result is given in Tables 2.2 and 2.3.
The evidence St = no tells us that we are in the context of Table 2.3. By
marginalizing FM and Fu out of Table 2.3 (summing each row), we get
P(SP, St = no) = (0.02864, 0.03965).

2.3 Bayesian Networks
39
FM = full
FM = 1
2
FM = empty
Sp = yes
(0.363, 0)
(0.559, 0)
(0.0093, 0)
Sp = no (0.00015, 0) (0.00024, 0) (3.9 · 10−6, 0)
Table 2.2. The joint probability table for P(Fu, FM, SP, St = yes).
FM = full
FM = 1
2
FM = empty
Sp = yes (0.00367, 1.9 · 10−5) (0.00564, 1.9 · 10−5) (9.4 · 10−5, 0.0192)
Sp = no
(0.01514, 8 · 10−7)
(0.0233, 8 · 10−7)
(0.000388, 0.000798)
Table 2.3. The joint probability table for P(Fu, FM, SP, St = no). The numbers
(x, y) in the table represent (Fu = yes, Fu = no).
We get the conditional probability P(SP | St = no) by dividing by P(St =
no). This is easy, since P(St = no) = P(SP = yes, St = no)+P(SP = no, St =
no) = 0.02864 + 0.03965 = 0.06829, and we get
P(SP | St = no) =
0.02864
0.06829, 0.03965
0.06829

= (0.42, 0.58).
Another way of saying this is that the distribution we end up with will be a
set of numbers that sum to 1. If they do not, normalize by dividing by the
sum.
In the same way, we get P(Fu | St = no) = (0.71, 0.29).
Next, we get the information that FM = 1
2, and the context for calculation
is limited to the part with FM = 1
2 and St = no. The numbers are given in
Table 2.4.
Fu = yes
Fu = no
Sp = yes
0.00564 1.9 · 10−5
Sp = no
0.0233
8 · 10−7
Table 2.4. P(Fu, SP, St = no, FM = 1
2).
By marginalizing Sp out and normalizing, we get P(Fu|St = no, FM =
1
2) = (0.999, 0.001), and by marginalizing Fu out and normalizing we get
P(SP|St = no, FM = 1
2) = (0.196, 0.804). The probability of SP = yes in-
creased by observing FM = 1
2, so the calculus did catch the explaining away
eﬀect.
2.3.3 Inserting Evidence
Bayesian networks are used for calculating new probabilities when you get new
information. The information so far has been of the type “A = a,” where A is

40
2 Causal and Bayesian Networks
a variable and a is a state of A. Let A have n states with P(A) = (x1, . . . , xn),
and assume that we get the information e that A can be only in state i or j.
This statement expresses that all states except i and j are impossible, and we
have the probability distribution P(A, e) = (0, . . . , 0, xi, 0, . . . , 0, xj, 0, . . . , 0).
Note that P(e), the prior probability of e, is obtained by marginalizing A
out of P(A, e). Note also that P(A, e) is the result of multiplying P(A) by
(0, . . . , 0, 1, 0, . . ., 0, 1, 0, . . ., 0), where the 1’s are at the i’th and j’th places.
Deﬁnition 2.4. Let A be a variable with n states. A ﬁnding on A is an n-
dimensional table of zeros and ones.
To distinguish between the statement e, “A is in either state i or j,” and
the corresponding 0/1-ﬁnding vector, we sometimes use the boldface notation
e for the ﬁnding. Semantically, a ﬁnding is a statement that certain states of
A are impossible.
Now, assume that you have a joint probability table, P(U), and let e be
the preceding ﬁnding. The joint probability table P(U, e) is the table obtained
from P(U) by replacing all entries with A not in state i or j by the value zero
and leaving the other entries unchanged. This is the same as multiplying P(U)
by e,
P(U, e) = P(U) · e.
Note that P(e) = 
U P(U, e) = 
U(P(U) · e). Using the chain rule for
Bayesian networks, we have the following theorem.
Theorem 2.2. Let BN be a Bayesian network over the universe U, and let
e1, . . . , em be ﬁndings. Then
P(U, e) =
	
A∈U
P(A | pa(A)) ·
m
	
i=1
ei,
and for A ∈U we have
P(A | e) =

U\{A} P(U, e)
P(e)
.
Some types of evidence cannot be represented as ﬁndings. You may, for
example, receive a statement from someone that the chance of A being in
state a1 is twice as high as for a2. This type of evidence is called likelihood
evidence. It is possible to treat this kind of evidence in Bayesian networks.
The preceding statement is then represented by the distribution (0.67, 0.33),
and Theorem 2.2 still holds. However, because it is unclear what it means that
a likelihood statement is true, P(e) cannot be interpreted as the probability
of the evidence, and P(U, e) therefore has an unclear semantics. We will not
deal further with likelihood evidence.

2.3 Bayesian Networks
41
2.3.4 Calculating Probabilities in Practice
As described in Section 2.3.3 and illustrated in Example 2.6, probability up-
dating in Bayesian networks can be performed using the chain rule to calculate
P(U), the joint probability table of the universe. However, U need not be large
before P(U) becomes intractably large. In this section, we illustrate how the
calculations can be performed without having to deal with the full joint ta-
ble. In Chapter 4, we give a detailed treatment of algorithms for probability
updating.
Consider the Bayesian network in Figure 2.17, and assume that all vari-
ables have ten states. Assume that we have the evidence e = {D = d, F = f},
and we wish to calculate P(A | e).
A
B
C
D
F
G
H
Fig. 2.17. A Bayesian network.
From the chain rule we have
P(U, e) = P(A, B, C, d, f, G, H)
= P(A)P(H)P(B | A, H)P(C | A)P(d | B, H)P(f | B, C)P(G | C),
where for example P(d | B, H) denotes the table over B and H resulting from
ﬁxing the D-entry to the state d. We say that the conditional probability table
has been instantiated to D = d. Notice that we need not calculate the full
table P(U) with 107 entries. If we wait until evidence is entered, we will in
this case need to work with a table with only 105 entries. Later, we see that
we need not work with tables larger than 1000 entries.
To calculate P(A, e), we marginalize the variables B, C, G, and H out of
P(A, B, C, d, f, G, H). The order in which we marginalize does not aﬀect the
result (Section 1.4), so let us start with G; that is, we wish to calculate

G
P(A, B, C, d, f, G, H)
=

G
P(A)P(H)P(B | A, H)P(C | A)P(d | B, H)P(f | B, C)P(G | C).

42
2 Causal and Bayesian Networks
In the right-hand product, only the last table contains G in its domain,
and due to the distributive law (Section 1.4) we have

G
P(A, B, C, d, f, G, H)
= P(A)P(H)P(B | A, H)P(C | A)P(d | B, H)P(f | B, C)

G
P(G | C),
and we need only calculate 
G P(G | C). Actually, for each state c of C, we
have 
G P(G | c) = 1; hence no calculations are necessary. We therefore get
P(A, B, C, d, f, H) =

G
P(A, B, C, d, f, G, H)
= P(A)P(H)P(B | A, H)P(C | A)P(d | B, H)P(f | B, C).
Next, we marginalize H out. Using the distributive law again, we get

H
P(A, B, C,d, f, H)
= P(A)P(C | A)P(f | B, C)

H
P(H)P(B | A, H)P(d | B, H).
We multiply the three tables P(H), P(B | A, H), and P(d | B, H), and we
marginalize H out of the product. The result is a table T (d, B, A), and we
have
P(A, B, C, d, f) = P(A)P(C | A)P(f | B, C)T (d, B, A).
Finally, we calculate this product and marginalize B and C out of it.
Notice that we never work with a table of more than three variables (the
table produced by multiplying P(H), P(B | A, H), and P(d | B, H)) compared
to the ﬁve variables in P(A, B, C, d, f, G, H).
The method we just used is called variable elimination and can be de-
scribed in the following way: we start with a set T of tables, and whenever we
wish to marginalize a variable X, we take from T all tables with X in their
domains, calculate the product of them, marginalize X out of it, and place
the resulting table in T .
2.4 Graphical Models – Formal Languages for Model
Speciﬁcation
From a mathematical point of view, the basic property of Bayesian networks
is the chain rule: a Bayesian network is a compact representation of the joint

2.4 Graphical Models – Formal Languages for Model Speciﬁcation
43
probability table over its universe. In this respect, a Bayesian network is one
type of compact representation among many others. However, there is more to
it than this: From a knowledge engineering point of view, a Bayesian network
is a type of graphical model. The structure of the network is formulated in
a graphical communication language for which the language features have a
very simple semantics, namely causality. This does not mean that “causality”
is an easy concept. It may be very diﬃcult to experience causality, and philo-
sophically the concept is not fully understood. However, most often humans
can communicate sensibly about causal relations in a knowledge domain. Fur-
thermore, the graphical speciﬁcation also speciﬁes the requirements for the
quantitative part of the model (the conditional probabilities). In Chapter 3,
we extend the modeling language, and in Part II we present other types of
graphical models.
As mentioned, graphical models are communication languages. They con-
sist of a qualitative part, where features from graph theory are used, and a
quantitative part consisting of potentials, which are real-valued functions over
sets of nodes from the graph; in Bayesian networks the potentials are condi-
tional probability tables. The graphical part speciﬁes the kind of potentials
and their domains.
Graphical models can be used for interpersonal communication: The
graphical speciﬁcation is easy for humans to read, and it helps focus attention,
for example in a group working jointly on building a model. For interpersonal
communication, the semantics of the various graph-theoretic features must be
rather welldeﬁned if misunderstandings are to be avoided.
The next step in the use of graphical models has to do with communication
to a computer. You wish to communicate a graphical model to a computer,
and the computer should be able to process the model and give answers to
various queries. In order to achieve this, the speciﬁcation language must be
formally deﬁned with a well-deﬁned syntax and semantics.
The ﬁrst concern in constructing a graphical modeling language is to en-
sure that it is suﬃciently welldeﬁned so that it can be communicated to a
computer. This covers the graphical part as well as the speciﬁcation of po-
tentials. The next concern is the scope of the language: what is the range of
domains and tasks that you will be able to model with this language? The
ﬁnal concern is tractability: do you have algorithms such that in reasonable
time the computer can process a model and query to provide answers?
The Bayesian network is a suﬃciently welldeﬁned language, and behind
the graphical speciﬁcation in the user interface, the computer systems for
processing Bayesian networks have an alphanumeric speciﬁcation language,
which for some systems is open to the user. Actually, the language for Bayesian
networks is a context-free language with a single context-sensitive aspect (no
directed cycles).
The scope of the Bayesian network language is hard to deﬁne, but the
examples in the next chapter show that it has a very broad scope.

44
2 Causal and Bayesian Networks
Tractability is not a yes or no issue. As described in Chapter 4, there are
algorithms for probability updating in Bayesian networks, but basically prob-
ability updating is NP-hard. This means that some models have an updating
time exponential in the number of nodes.
On the other hand, the running times of the algorithms can be easily
calculated without actually running them. In Chapter 4 and Part II, we treat
complexity issues for the various graphical languages presented.
2.5 Summary
d-Separation in Causal Networks
Two distinct variables A and B in a causal network are d-separated if for all
paths between A and B, there is an intermediate variable V (distinct from A
and B) such that either
•
the connection is serial or diverging, and V is instantiated, or
•
the connection is converging, and neither V nor any of V ’s descendants
have received evidence.
Deﬁnition of Bayesian Networks
A Bayesian network consists of the following:
•
There is a set of variables and a set of directed edges between variables.
•
Each variable has a ﬁnite set of mutually exclusive states.
•
The variables together with the directed edges form an acyclic directed
graph (DAG).
•
To each variable A with parents B1, . . . , Bn there is attached a conditional
probability table P(A | B1, . . . , Bn).
The Chain Rule for Bayesian Networks
Let BN be a Bayesian network over U = {A1, . . . , An}. Then BN speciﬁes a
unique joint probability distribution P(U) given by the product of all condi-
tional probability tables speciﬁed in BN:
P(U) =
n
	
i=1
P(Ai | pa(Ai)),
where pa(Ai) are the parents of Ai in BN, and P(U) reﬂects the properties
of BN.
Admittance of d-Separation in Bayesian Networks
If A and B are d-separated in a Bayesian network with evidence e entered,
then P(A | B, e) = P(A | e).

Exercises
45
Inserting Evidence
Let e1, . . . , em be ﬁndings, and then
P(U, e) =
n
	
i=1
P(Ai | pa(Ai))
m
	
j=1
ej
and
P(A | e) =

U\{A} P(U, e)
P(e)
.
2.6 Bibliographical Notes
The connection between causation and conditional independence was studied
by Spohn (1980), and later investigated with special focus on Bayesian net-
works in (Pearl, 2000). The concepts of causal network, d-connection, and the
deﬁnition in Section 2.2.1 are due to Pearl (1986) and Verma (1987). A proof
that Bayesian networks admit d-separation can be found in (Pearl, 1988) or
in (Lauritzen, 1996). Geiger and Pearl (1988) proved that d-separation is the
correct criterion for directed graphical models, in the sense that for any DAG,
a probability distribution can be found for which the d-separation criterion is
sound and complete. Meek (1995) furthermore proved that for a given DAG,
the set of discrete probability distributions for which the d-separation cri-
terion is not complete has measure zero. That is, given a random Bayesian
network, there is almost no chance that it contains conditionally independent
variables that cannot be read oﬀthe graph by d-separation. The method for
discovering d-separation properties using ancestral graphs was ﬁrst presented
in (Lauritzen et al., 1990).
Bayesian networks have a long history in statistics, and can be traced
back at least to the work in (Minsky, 1963). In the ﬁrst half of the 1980s
they were introduced to the ﬁeld of expert systems through work by Pearl
(1982) and Spiegelhalter and Knill-Jones (1984). Some of the ﬁrst real-world
applications of Bayesian networks were Munin (Andreassen et al., 1989, 1992)
and Pathﬁnder (Heckerman et al., 1992). The basis for the inference method
presented in Section 2.3.4 originates from (D’Ambrosio, 1991) and was mod-
iﬁed to the presented variable elimination in (Dechter, 1996). The fact that
inference is NP-hard was proved in (Cooper, 1987).
2.7 Exercises
Exercise 2.1. To illustrate that simple rules cannot cope with uncertainty
reasoning, consider the following two cases:

46
2 Causal and Bayesian Networks
(i) I have an urn with a red ball and a white ball in it. If I add a red ball
and shake it, what is the certainty of drawing a red ball in one draw? If I
add a white ball instead, what is the certainty of drawing a red ball? If I
combine the two actions, what is the certainty of drawing a red ball?
(ii) When shooting, I am more certain to hit the target if I close the left eye.
I am also more certain to hit the target if I close the right eye. What is
the combined certainty if I do both?
Exercise 2.2. Construct a causal network and follow the reasoning in the
following story. Mr. Holmes is working in his oﬃce when he receives a phone
call from his neighbor, who tells him that Holmes’ burglar alarm has gone oﬀ.
Convinced that a burglar has broken into his house, Holmes rushes to his car
and heads for home. On his way, he listens to the radio, and in the news it
is reported that there has been a small earthquake in the area. Knowing that
earthquakes have a tendency to turn on burglar alarms, he returns to work.
Exercise 2.3. Consider the Car Start Problem in Section 2.1.1 with the
causal network in Figure 2.1, and the following twist on the story: “I dis-
tinctly remember visiting the pump last night, so the fuel meter should be
reading full. Since this is not the case, either there must be a leak in the tank,
someone has stolen gasoline during the night, or the fuel meter is malfunc-
tioning. Sniﬃng the air I smell no gasoline, so I conclude that a thief has been
visiting last night or that the fuel meter is malfunctioning.” Alter the causal
network in Figure 2.1 to incorporate the above twist on the story.
Exercise 2.4. In the graphs in Figures 2.18 and 2.19, determine which vari-
ables are d-separated from A.
A
B
C
D
E
F
G
H
I
e
Fig. 2.18. Figure for Exercise 2.4.
Exercise 2.5. For each pair of variables in the causal network in Figure 2.1,
state whether the variables can be d-separated, and if so which set(s) of vari-
ables that allow this.

Exercises
47
A
B
C
D
E
F
G
H
I
J
e
Fig. 2.19. Figure for Exercise 2.4.
Exercise 2.6. Consider the network in Figure 2.20. What are the minimal
set(s) of variables required to d-separate C and E (that is, sets of variables
for which no proper subset d-separates C and E)? What are the minimal
set(s) of variables required to d-separate A and B? What are the maximal
set(s) of variables that d-separate C and E (that is, sets of variables for which
no proper superset d-separates C and E)? What are the maximal set(s) of
variables that d-separate A and B?
A
B
C
D
E
F
Fig. 2.20. A causal network for Exercise 2.6.
Exercise 2.7. Consider the network in Figure 2.20. What is the Markov blan-
ket of each variable?
Exercise 2.8. Let A be a variable in a DAG. Assume that all variables in
A’s Markov blanket are instantiated. Show that A is d-separated from the
remaining uninstantiated variables.
Exercise 2.9. Apply the procedure using the ancestral graph given in Sec-
tion 2.2.1 to determine whether A is d-separated from C given B in the
network in Figure 2.19.

48
2 Causal and Bayesian Networks
Exercise 2.10. Let D1 and D2 be DAGs over the same variables. The graph
D1 is an I-submap of D2 if all d-separation properties of D1 also hold for D2.
If D2 is also an I-submap of D1, they are said to be I-equivalent. Which of
the four DAGs in Figure 2.21 are I-equivalent?
A
A
A
A
B
B
B
B
C
C
C
C
Fig. 2.21. Figure for Exercise 2.10.
Exercise 2.11. Let {A1, . . . , An} be a topological ordering of the variables
in a Bayesian network, and consider variable Ai with parents pa(Ai). Prove
that Ai is d-separated from {A1, . . . , Ai−1} \ pa(Ai) given pa(Ai).
Exercise 2.12. Consider the network in Figure 2.20. Which conditional prob-
ability tables must be speciﬁed to turn the graph into a Bayesian network?
Exercise 2.13. In Figure 2.22 the structure of a simple Bayesian network
is shown. The accompanying conditional probability tables are shown in Ta-
bles 2.5 and 2.6, and the prior probabilities for A are 0.9 and 0.1. Are A and
C d-separated given B? Are A and C conditionally independent given B?
A
B
C
Fig. 2.22. A simple Bayesian network for Exercise 2.13.
A = a1 A = a2
B = b1
0.3
0.6
B = b2
0.7
0.4
Table 2.5. P(B | A).

Exercises
49
A = a1
A = a2
B = b1 (0.1 ; 0.9) (0.1 ; 0.9)
B = b2 (0.2 ; 0.8) (0.2 ; 0.8)
Table 2.6. P(C | A, B).
Exercise 2.14. Consider the network in Figure 2.20. Using the chain rule, es-
tablish an expression for the joint distribution over the universe {A, B, C, D, E,
F}. Use this expression to show that B and D are conditionally independent
given A and C.
Exercise 2.15. Prove that the probability distribution P(U) deﬁned by the
chain rule for Bayesian networks is indeed a probability distribution.
Exercise 2.16. Prove that the probability distribution P(U) deﬁned by the
chain rule for a Bayesian network BN reﬂects the conditional probabilities
speciﬁed in BN.
Exercise 2.17. Consider the Bayesian network from Exercise 2.13 and the
ﬁnding e = (0, 1) over A. What is P(B, C, e)?
Exercise 2.18. What steps would be taken if variable elimination were used
to calculate the probability table P(F | C = c1) for the network in Figure 2.20?
Assuming that each variable has ten states, what is the maximum size of a
table during the procedure?
Exercise 2.19. Consider the DAG (a) in Exercise 2.10.
•
Show that P(B | A, C) = P(B | A).
•
We have P(A) = (0.1, 0.9) and the conditional probability tables in Ta-
ble 2.7. Calculate P(A, B, C).
a1 a2
b1 0.2 0.3
b2 0.8 0.7
a1 a2
c1 0.5 0.6
c2 0.5 0.4
P(B | A)
P(C | A)
Table 2.7. Conditional probability tables for Exercise 2.19.
Exercise 2.20. E
Install an editor for Bayesian networks (a reference to a
list of systems can be found in the preface).
Exercise 2.21. E Construct a Bayesian network for Exercise 1.12.
Exercise 2.22. E Construct a Bayesian network to follow the reasoning from
Exercise 2.2. Use your own estimates of probabilities for the network.

50
2 Causal and Bayesian Networks
Exercise 2.23. E Consider the Bayesian network in Figure 2.23 with condi-
tional probabilities given in Table 2.8. Use your system to investigate whether
A and C are independent.
A
B
C
Fig. 2.23. Figure for Exercise 2.23.
A = yes A = no
b1
0.6
0.2
b2
0.1
0.5
b3
0.2
0.1
b4
0.1
0.2
b1 b2 b3 b4
C = yes 0.8 0.8 0.2 0.2
C = no 0.2 0.2 0.8 0.8
P(B | A)
P(C | B)
Table 2.8. Tables for Exercise 2.23.
Exercise 2.24. E Use your system and Section 2.5 to perform the reasoning
in Section 2.1.2.

3
Building Models
The framework of Bayesian networks is a very eﬃcient language for building
models of domains with inherent uncertainty. However, as can be seen from the
calculations in Section 2.6, it is a tedious job to perform evidence transmission
even for very simple Bayesian networks. Fortunately, software tools that can
do the calculational job for us are available. In the rest of this book, we
assume that the reader has access to such a system (some URLs are given in
the preface). Therefore, we can start by concentrating on how to use Bayesian
networks in model building and defer a presentation of methods for probability
updating to Chapter 4.
In Section 3.1, we examine through examples the considerations you may
go through when determining the structure of a Bayesian network model.
Section 3.2 gives examples of estimation of conditional probabilities. The ex-
amples cover theoretically well-founded probabilities as well as probabilities
taken from databases and purely subjective estimates. Section 3.3 introduces
various modeling tricks to use when the quantity of numbers to acquire is
overwhelming. Finally, Section 3.4 considers other types of queries that can
be answered by Bayesian networks besides standard probability updating.
3.1 Catching the Structure
The ﬁrst thing to have in mind when organizing a Bayesian network model
is that its purpose is to give estimates of certainties for events that are not
directly observable (or observable only at an unacceptable cost), and the pri-
mary task in model building is to identify these events. We call them hypothesis
events. The hypothesis events detected are then grouped into sets of mutually
exclusive and exhaustive events to form hypothesis variable.
The next thing to have in mind is that in order to come up with a cer-
tainty estimate, we should provide some information channels, and the task
is to identify the types of achievable information that may reveal something
about the hypothesis variables. These types of information are grouped into

52
3 Building Models
information variablesinformation variable, and a typical piece of information
is a statement that a certain variable is in a particular state, but softer state-
ments are also allowed.
Having identiﬁed the variables for the model, the next thing will be to
establish the directed links for a causal network.
3.1.1 Milk Test
Milk from a cow may be infected. To detect whether the milk is in-
fected, you have a test, which may give either a positive or a negative
test result. The test is not perfect. It may give a positive result on
clean milk as well as a negative result on infected milk.
We have two hypothesis events: milk infected and milk not infected, and be-
cause they are mutually exclusive and exhaustive, they are grouped into the
variable Infected? with the states yes and no. A possible information source is
the test results, which can be either positive or negative. For this, we establish
the variable Test with states pos and neg.
The causal direction between the two variables is from Infected? to Test
(see Figure 3.1).
Infected?
Test
Fig. 3.1. The Bayesian network for the milk test.
Warning: Certainly, no sensible person will claim that a positive test result
may infect the milk. However, our reasoning is often performed in the diagnos-
tic direction, and in more complex situations you may therefore be tempted
to wrongly direct the link from “symptom” to “disease.”
From one day to another, the state of the milk can change. Cows with
infected milk will heal over time, and a clean cow has a risk of having infected
milk the next day. Now, imagine that the farmer performs the test each day.
After a week, he has not only the current test result but also the six previous
test results. For each day, we have a model like the one in Figure 3.1. These
seven models should be connected such that past knowledge can be used
for the current conclusion. A natural way would be to let the state of the
milk yesterday have an impact on the state today. This yields the model in
Figure 3.2.
The model in Figure 3.2 contains a set of hidden assumptions, which can
be read from the d-separation properties.
First, the model assumes the Markov property: if we know the present,
then the past has no inﬂuence on the future. In the language of d-separation,
the assumption is that, for example, Infi−1 is d-separated from Infi+1 given

3.1 Catching the Structure
53
Test7
Test1
Inf1
Inf2
Inf3
Inf4
Inf5
Inf6
Inf7
Test2
Test3
Test4
Test5
Test6
Fig. 3.2. A seven-day model for the milk test.
Infi. If we know that the milk on day four is infected, then this can be used
to forecast the probability that the milk will be infected on day ﬁve. This
forecast will not be improved by knowing that the milk was not infected on
day three. For various diseases, such an assumption will not be valid. Some
diseases have a natural span of time. For example, if I have the ﬂu today but
was healthy yesterday, then I will most probably have the ﬂu the day after
tomorrow. On the other hand, if I have had the ﬂu for four days, then there
is a good chance that I will be cured the day after tomorrow. If the Markov
property of Figure 3.2 does not reﬂect reality, the model should be changed.
For example, it may be argued that you also need to go an extra day back,
and the model will be as in Figure 3.3.
Inf7
Test1
Test2
Test3
Test4
Test5
Test6
Test7
Inf1
Inf2
Inf3
Inf4
Inf5
Inf6
Fig. 3.3. A seven-day model with a two-day memory of infection.
Notice that although we in practice will never know the state of the in-
fection nodes, it makes a diﬀerence whether the memory links are included.
In the reasoning, we cannot exploit knowledge of the exact state of the previ-
ous infection node, but we may use a probability distribution based on a test
result.
The second hidden assumption has to do with the test. Any two test
nodes are d-separated given any infection node on the path. This means that
the fault probability of the test is independent of whether it was previously
correct. In other words, the fact that the test was wrong yesterday has no
inﬂuence on whether the test will be correct today. If this does not reﬂect the
behavior of the test, you may, for example, include its performance yesterday
in the model. This is done in Figure 3.4.
A minor digression on modeling of tests: It is good to have as a rule that
no test is perfect. Unless you explicitly know otherwise, a test should always

54
3 Building Models
Inf1
Test1
Inf2
Inf7
Test2
Test3
Test4
Test5
Test6
Test7
Inf6
Inf5
Inf4
Inf3
Fig. 3.4. A seven-day model with two-day memory for infection and a one-day
memory of correctness of test.
be given a positive probability of false positives as well as false negatives.
This is not all, though. You should also take the mechanism for false test
results into account. Consider for example an HIV test with a probability of
false positives of 10−5, and assume that a person has received a positive test
result. Now, you may have the option of repeating the test, but will this be
of any help? It will depend on the mechanisms that cause the test to give
a wrong result. If a test is positive because this particular person’s blood is
composed so that it will produce a positive test result regardless of a positive
HIV infection, then a repeated test will not provide new information. If, on
the other hand, the experiment is such that it now and then goes wrong, then
a repeated test may be worthwhile and it will be advisable to repeat the test
before the “verdict” is passed (in case the second test result is negative, a third
test may be advisable). Models for these two types of failure mechanisms are
shown in Figure 3.5.
HIV?
HIV?
Test1
Test1
Test2
Test2
(a)
(b)
Fig. 3.5. Model (a) illustrates the scenario in which a repeated test may provide
new information, and model (b) shows the situation in which repeating a test always
produces the same result.
3.1.2 Cold or Angina?
I wake up in the morning with a sore throat. It may be the beginning
of a cold or I may suﬀer from angina (inﬂammation of the throat). If
it is severe angina, I will not go to work. To gain more insight, I can
take my temperature, and I can look down my throat for yellow spots.

3.1 Catching the Structure
55
Here we have ﬁve hypothesis events Cold? {no, yes} and Angina? {no, mild,
severe}. The hypothesis events must be organized into a set of variables with
mutually exclusive and exhaustive states. We may use the variables indicated
previously, but we may also use only one variable Sick? with states {no, cold,
mild angina, severe angina}. In the latter case, suﬀering from both cold and
angina is excluded as a possibility. We choose to use the two variables Cold?
and Angina?.
The information variables are Sore Throat? {no, yes}, See Spots? {no,
yes}, and Fever? {no, low, high}. The variable Fever? causes a problem be-
cause it really is continuous. In Section 3.3.8, we give methods on how to deal
with continuous variables.
Now it is time to consider the causal structure between the variables. We
need not worry about how information is transmitted through the network.
The only thing to worry about is which variables have a direct causal impact
on other variables.
In this example, we have that Cold? has a causal impact on Sore Throat?
and Fever? while Angina? has an impact on all information variables. The
model is given in Figure 3.6.
Fever?
Sore Throat?
See Spots?
Cold?
Angina?
Fig. 3.6. A model for Cold? or Angina?.
The next thing to check is whether the conditional independences laid
down in the model correspond to reality. For example, the model in Figure 3.6
yields that if we know the state of Angina?, then seeing spots will not have
an impact on the expectation either for Fever? or for Sore Throat?. If we do
not agree, we may introduce a link from See Spots? to, for example, Fever?.
For now, we accept the conditional independences given by the model.
3.1.3 Insemination
Six weeks after insemination of a cow, you can perform two tests to
determine whether the cow is pregnant: a blood test and a urine test.
Following the method from Section 3.1.1, we construct a model as in Fig-
ure 3.7. The variable Pr {yes,no} represents a possible pregnancy, and BT
{pos,neg} and UT {pos,neg} represent the results of the blood test and the
urine test, respectively.
Next, we will analyze the conditional independences stated by the model.
We ask the expert whether it is correct that the outcomes of the two tests

56
3 Building Models
Pr
BT
UT
Fig. 3.7. A model for pregnancy.
are independent given Pr. More speciﬁcally, assume that we know the cow is
pregnant. From this, we infer some expectations for the test results. Now, if we
get a negative test result from the blood test, will this change our expectation
for the urine test? The experts say that it will, and we must conclude that
the model is not a proper reﬂection of reality.
There are several ways to change the model. You might, for example,
introduce a link between the two test nodes, but there is no natural direction.
To ﬁnd out what to do, you must study the process more carefully, and it turns
out that what the two tests actually do is to trace indications of hormonal
changes in the cow. A more-reﬁned model will involve a variable Ho, reﬂecting
whether hormonal changes have taken place in the cow, and the model will
be as in Figure 3.8.
Pr
BT
UT
Ho
Fig. 3.8. A more correct model for pregnancy. Both the blood test (BT) and the
urine test (UT) measure the hormonal state (Ho).
For the model in Figure 3.8, it does not hold that BT and UT are indepen-
dent given Pr. The model states that BT and UT are independent given Ho
(which should be checked). If the model in Figure 3.7 is used for diagnosing a
possible pregnancy, a negative outcome of both the blood test and the urine
test will be counted as two independent pieces of evidence and therefore over-
estimate the probability for the insemination to have failed (see Exercise 3.8).
In the model in Figure 3.8, we have introduced the variable Ho, which is
neither a hypothesis variable nor an information variable. Such variables are
called mediating variables. Mediating variables are often introduced when two

3.1 Catching the Structure
57
variables are not (conditionally) independent as opposed to the situation in
the current model. Some standard situations are illustrated in Figure 3.9.
e
A
A
A
A
C
C
C
C
B
B
B
B
D
D
Fig. 3.9. Examples in which an intermediate variable C “resolves” undirected de-
pendencies. In examples (a) and (b), A and B are not independent, whereas A and
B are not independent given D in examples (c) and (d).
3.1.4 A Simpliﬁed Poker Game
In this poker game, each player receives three cards and is allowed
two rounds of changing cards. In the ﬁrst round, you may discard any
number of cards from your hand and get replacements from the pack
of cards. In the second round, you may discard at most two cards.
After the two rounds of card changing, I am interested in an estimate
of my opponent’s hand.
The hypothesis events are the various types of hands in the game. They may
be classiﬁed in the following way (in increasing rank): nothing special, 1 ace,
2 of the same value, 2 aces, ﬂush (3 of a suit), straight (3 of consecutive
value), 3 of the same value, straight ﬂush. Ambiguities are resolved according
to rank. This is, of course, a simpliﬁcation, but it is often necessary to do so
in modeling. The hypothesis events are collected into one hypothesis variable
OH (opponent’s hand) with the preceding classes as states.
The only information to acquire is the number of cards the player discards
in the two rounds. Therefore, the information variables are FC (ﬁrst change)
with states 0, 1, 2, 3 and SC (second change) with states 0, 1, 2. By saying
this, we are making an approximation again. The information on the cards
you have seen is relevant for your opponent’s hand. If, for example, you have
seen three aces, then he cannot have two aces.
A causal structure for the information variables and the hypothesis variable
could be as in Figure 3.10. However, this structure will leave us with no clue
as to how to specify the probabilities.

58
3 Building Models
FC
SC
OH
Fig. 3.10. An oversimpliﬁed structure for the poker game. The variables are FC
(ﬁrst change), SC (second change), and OH (opponent’s hand).
What we need are mediating variables describing the opponent’s hands in
the process: the initial hand OH0 and the hand OH1 after the ﬁrst change of
cards. The causal structure will then be as in Figure 3.11.
FC
SC
OH0
OH1
OH2
Fig. 3.11. A structure for the poker game. The two mediating variables OH0 and
OH1 are introduced. OH2 is the variable for my opponent’s ﬁnal hand.
To determine the states of OH0 and OH1, we must produce a classiﬁcation
that is relevant for determining the states of the children (FC and OH1,
say). We may let OH0 and OH1 have the states nothing special, 1 ace, 2
of consecutive value, 2 of a suit, 2 of the same value, 2 of a suit and 2 of
consecutive value, 2 of a suit and 2 of the same value, 2 of consecutive value
and 2 of the same value, ﬂush, straight, 3 of the same value, straight ﬂush.
We defer further discussion of the classiﬁcation to the section on specifying
the probabilities (Section 3.2).
3.1.5 Naive Bayes Models
In the previous sections we saw examples of Bayesian networks that were
designed to capture the independence properties in the domains being mod-
eled. However, the ﬁrst Bayesian diagnostic systems were actually constructed

3.1 Catching the Structure
59
based on much simpler models, namely so-called naive Bayes models. In a
naive Bayes model the information variables are assumed to be independent
given the hypothesis variable (see Figure 3.12).
H
I1
In
· · ·
Fig. 3.12. A naive Bayes model.
Using this assumption, the conditional probability distribution for the hy-
pothesis variable given the information variables is very easy to calculate, and
the overall process (from model speciﬁcation to probability updating) can be
summarized as follows:
-
Let the possible diseases be collected into one hypothesis variable H with
prior probability P(H).
-
For all information variables I, acquire the conditional probability distri-
bution P(I | H) (the likelihood of H given I).
-
For any set of observations f1, . . . , fn on the variables I1, . . . , In, calculate
the product P(f1, . . . , fn | H) = P(f1 | H) · P(f2 | H) · · · P(fn | H). This
product is also called the likelihood for H given f1, . . . , fn. The posterior
probability for H is then calculated as
P(H | f1, . . . , fn) = μP(H)P(f1, . . . , fn | H)
= μP(H)
n
	
i=1
P(fi | H),
(3.1)
where μ = 1/P(f1, . . . , fn) is a normalization constant.
What is particularly attractive with the calculation in equation (3.1) is that
the time complexity is linear in the number of information variables, and that
each term in the product involves only two numbers (assuming that the hy-
pothesis variable is binary), one for P(fi | H = y) and one for P(fi | H = n).
On the other hand, as we also saw from the insemination example, the in-
dependence assumption need not hold, and if the model is used anyway, the
conclusions may be misleading. However, in certain application areas (such as
diagnosis) the naive Bayes model has been shown to provide very good per-
formance, even when the independence assumption is violated. This is partly
due to the fact that for many diagnostic problems we are interested only in
identifying the most probable disease. In other words, if the conditional inde-
pendence assumption does not change which state has the highest probability,
then the naive Bayes model can be used without aﬀecting the performance of
the system. We shall return to these models in Section 8.1.

60
3 Building Models
3.1.6 Causality
In the examples presented in the previous section, there was no problem in
establishing the links and their directions. However, you cannot expect this
part of the modeling always to go smoothly.
First, causal relations are not always obvious – recall the debates on
whether smoking causes lung cancer or whether a person’s sex has an im-
pact on his/her ability in the technical sciences. Furthermore, causality is not
a well-understood concept. Is a causal relation a property of the real world,
or rather, is it a concept in our minds helping us to organize our perception of
the world? For now, we make only one point about this issue, namely that in
some situations you may be able to infer information about causality based on
actions that change the state of the world. For example, assume that you are
confronted with two correlated variables A and B, but you cannot determine
a direction. If you observe the state of A, you will change your belief of B
and vice versa. A good test then is to imagine that some outside agent ﬁxes
the state of A. If this does not make you change your belief of B, then A is
not a cause of B. On the other hand, if this imagined test indicates no causal
arrow in any direction, then you should look for an event that has a causal
impact on both A and B. If C is such a candidate, then check whether A and
B become independent given C (see Figure 3.9). We shall brieﬂy return to the
issue of discovering causal relations in Section 7.1, where we discuss methods
for learning Bayesian networks from data.
3.2 Determining the Conditional Probabilities
The numbers (conditional probabilities) that you need to specify for a Bayesian
network are called the parameters of the network. The basis for the conditional
probabilities can have an epistemological status ranging from well-founded
theory over frequencies in a database to subjective estimates. We will give
examples of each type.
3.2.1 Milk Test
For the milk test in Figure 3.1, we need P(Infected?) and P(Test | Infected?).
The retailer of the test should provide P(Test | Infected?). Any producer of
such kinds of tests is supposed to have performed a series of tests yield-
ing the relevant numbers, namely the frequency of false positives, P(Test =
pos | Infected? = no), and the frequency of false negatives, P(Test = neg | In-
fected? = yes). Let both numbers be 0.01.
The numbers provided by the retailer are not suﬃcient for the user of the
test. In the case of a positive test result, the milk may still be clean, and to
come up with a probability we need the prior probabilities P(Infected?).

3.2 Determining the Conditional Probabilities
61
An estimate of the prior probability would in this case be the daily fre-
quency λ of infected milk for each cow at the particular farm. Estimating λ
may be a bit tricky because the farmer may have no experience with actually
testing the milk from each speciﬁc cow with a perfect test. Assume that this
particular farm has 50 cows, and that the milk from all cows is poured into
a container and transported to the dairy, which tests the milk with a very
precise test. The farmer’s experience is that on average the dairy reports his
milk to be infected once a month.
Now we must make various assumptions. The ﬁrst assumption could be
that the daily λ is the same for all cows. The next assumption could be that
outbreaks of infected milk for the cows in the farm are independent. This
yields a coin-tossing model with P(Infected? = yes) = λ. The information we
have is that if we toss ﬁfty coins at the same time, the frequency of at least
one of them coming up with Infected? = yes is 1 out of 30. That is, in 29 days
out of 30, none of the cows are infected and the probability that all the cows
are clean on a given day is therefore 29/30. Moreover, from the assumption of
the outbreaks being independent we also have that the probability of all 50
cows being clean on a given day is (1 −λ)50:
P(Inf1, . . . , Inf50) = (1 −λ1) · · · (1 −λ50) = (1 −λ)50.
Combining all this, we now have
(1 −λ)50 = 29
30,
which yields the estimate
λ = 1 −
29
30
0.02
≈0.0007.
This completes the model, and next you can use a computer system to
calculate posterior probabilities. The interesting question for this situation
is, if we get a positive test result, what is the probability that the milk is
infected? This is left as an exercise (see Exercise 3.5).
For the seven-day model in Figure 3.2, we also need P(Infi+1 | Infi). There
are two numbers to estimate: the risk of becoming infected and the chance of
being cured. These numbers must be based on experience. For the sake of the
example, let the risk of becoming infected be 0.0002 and the chance of being
cured 0.3. This gives the numbers in Table 3.1.
For the seven-day model with a two-day memory of infection (Figure 3.3),
we need P(Infi+1 | Infi, Infi−1). If we assume that the risk of being infected is
the same as before, that the infection always lasts at least two days, and that
after this the chance of being cured is 0.4 each of the following days, then the
numbers are as in Table 3.2 (see Exercise 3.10).
For the seven-day model with two-day memory of infection as well as
correctness of test (Figure 3.4), we furthermore need P(Testi+1 | Infi, Infi+1,

62
3 Building Models
Infi
yes
no
Infi+1
yes 0.7 0.0002
no 0.3 0.9998
Table 3.1. P(Infi+1 | Infi).
Infi−1
yes
no
Infi
yes
0.6
1
no 0.0002 0.0002
Table 3.2. P(Infi+1 = yes | Infi, Infi−1).
Testi). If we assume that a correct test has a 99.9% chance of being correct
next time, and an incorrect test has a 90% risk of also being incorrect next
time, we can calculate all required numbers for the four-dimensional table.
However, by introducing mediating variables, Cori, the speciﬁcation of num-
bers could be easier, and the tables would be smaller. Figure 3.13 shows how
the model could be simpliﬁed.
Inf1
Cor1
Test1
Inf2
Cor2
Test2
Inf3
Cor3
Test3
Inf4
Cor4
Test4
Inf5
Cor5
Test5
Inf6
Cor6
Test6
Inf7
Test7
Fig. 3.13. A seven-day model with a two-day memory for infection and a one-day
memory of correctness of test.
With the preceding assumptions, the required tables are as in Table 3.3.
3.2.2 Stud Farm
The stallion Brian has sired Dorothy on the mare Ann and sired Eric
on the mare Cecily. Dorothy and Fred are the parents of Henry, and
Eric has sired Irene on Gwenn. Ann is the mother of both Fred and
Gwenn, but their fathers are in no way related. The colt John with

3.2 Determining the Conditional Probabilities
63
Infi
yes no
Testi
pos
1
0
neg
0
1
Cori−1
yes
no
Infi
yes 0.999 0.1
no 0.001 0.9
Table 3.3. The conditional probability distributions P(Cori = yes | Infi,
Testi) and P(Testi = pos | Infi, Cori−1).
the parents Henry and Irene has been born recently; unfortunately, it
turns out that John suﬀers from a life-threatening hereditary disease
carried by a recessive gene. The disease is so serious that John is
displaced instantly, and since the stud farm wants the gene out of
production, Henry and Irene are taken out of breeding. What are the
probabilities for the remaining horses to be carriers of the unwanted
gene?
The genealogical structure for the horses is given in Figure 3.14.
Ann
Brian
Cecily
Fred
Dorothy
Eric
Gwenn
Henry
Irene
John
Fig. 3.14. Genealogical structure for the horses in the stud farm.
The only information variable is John. Before the information on John is
acquired, he may have three genotypes: he may be sick (aa), a carrier (aA),
or he may be pure (AA). The hypothesis events are the genotypes of all other
horses in the stud farm.
The conditional probabilities for inheritance are both empirically and the-
oretically wellstudied, and the probabilities are as shown in Table 3.4.
The inheritance tables could be as in Table 3.4. However, for all horses
except John, we have additional knowledge. Since they are in production,
they cannot be of type aa. A way to incorporate this would be to build a

64
3 Building Models
aa
aA
AA
aa
(1, 0, 0)
(0.5, 0.5, 0)
(0, 1, 0)
aA (0.5, 0.5, 0) (0.25, 0.5, 0.25) (0, 0.5, 0.5)
AA
(0, 1, 0)
(0, 0.5, 0.5)
(0, 0, 1)
Table 3.4. P(Child | Father, Mother) for genetic inheritance. The numbers (α, β, γ)
are the child’s probabilities for (aa, aA, AA).
Bayesian network in which all inheritance is modeled in the same way and
afterward enter the ﬁndings that all horses but John are not aa. It is also
possible to calculate the conditional probabilities directly. If we ﬁrst consider
inheritance from parents that may be only of genotype AA or aA, we get
Table 3.5.
aA
AA
aA (0.25, 0.5, 0.25) (0, 0.5, 0.5)
AA
(0, 0.5, 0.5)
(0, 0, 1)
Table 3.5. P(Child | Father, Mother) when the parents are not sick.
The table for John is as in Table 3.5. For the other horses, we know
that aa is impossible. This is taken care of by removing the state aa from
the distribution and normalizing the remaining distribution. For example,
P(Child | aA, aA) = (0.25, 0.5, 0.25), but since aa is impossible, we get the dis-
tribution (0, 0.5, 0.25), which is normalized to (0, 0.67, 0.33). The ﬁnal result
is shown in Table 3.6.
aA
AA
aA (0.67, 0.33) (0.5, 0.5)
AA
(0.5, 0.5)
(0, 1)
Table 3.6. P(Child| Father, Mother) with aa removed.
In order to deal with Fred and Gwenn, we introduce the two unknown
fathers I and K as mediating variables and assume that they are not sick.
For the horses at the top of the network, we specify prior probabilities. This
will be an estimate of the frequency of the unwanted gene, and there is no
theoretical way to derive it. Let us assume that the frequency is such that the
prior belief of a horse being a carrier is 0.01.
In Figure 3.15, the ﬁnal model with initial probabilities is shown; Fig-
ure 3.16 gives the posterior probabilities given that John is aa; and in Fig-

3.2 Determining the Conditional Probabilities
65
ure 3.17 you can see the posterior probabilities with the prior beliefs at the
top changed to 0.0001. Note that the sensitivity to the prior beliefs is very
small for the horses whose posterior probability for carrier is much greater
than 0, for instance in the cases of Ann and Brian.
Fig. 3.15. The stud farm model with initial probabilities.
Fig. 3.16. Stud farm probabilities given that John is sick.

66
3 Building Models
Fig. 3.17. Stud farm probabilities with prior probabilities for top variables changed
to (0.0001, 0.9999).
3.2.3 Poker Game
In the stud farm example, the conditional probabilities were established
mainly through theoretical considerations. This should also be attempted for
the model of the poker game developed in Section 3.1.4, but it cannot be
carried through entirely.
Consider for example P(FC | OH0). It is not possible to give probabilities
that are valid for any opponent. It is heavily dependent on the opponent’s
insight, psychology, and game strategy. We will assume the following strategy:
•
If nothing special (no), then change 3.
•
If 1 ace (1 a), then keep the ace.
•
If 2 of consecutive value (2 cons), 2 of a suit (2 s), or 2 of the same value
(2 v), then discard the third card.
•
If 2 of a suit and 2 of consecutive value, then keep 2 of a suit (this strategy
could be substituted by a random strategy for keeping either 2 of a suit
or 2 of consecutive value).
•
If 2 of a suit and 2 of the same value or 2 of consecutive value and 2 of
the same value, then keep the 2 of the same value.
•
If ﬂush (ﬂ), straight (st), 3 of the same value (3 v), or straight ﬂush (sﬂ),
then keep it.
Based on the preceding strategy, a logical link between FC and OH0 is estab-
lished. Note that the strategy makes the states for combined hands redundant.
They play no role, and therefore we remove them.
The strategy for P(SC | OH1) is the same except that in the case of no,
only 2 cards are discarded.

3.2 Determining the Conditional Probabilities
67
These strategies seem to be the most rational. However, deterministic
strategies in games do not always work, since they give your opponent valuable
information about your hand. A good strategy should therefore be random
rather than deterministic. Sometimes you may, for example, change nothing
although you have a weak hand. Some people call it bluﬀ, but it is really a
way of increasing your opponent’s uncertainty no matter what you do.
The remaining probabilities to specify are P(OH0), P(OH1 | OH0, FC),
and P(OH2 | OH1, SC).
The Probability Distribution P (OH0)
The states are (no, 1 a, 2 cons, 2 s, 2 v, ﬂ, st, 3 v, sﬂ), and through various
(approximated) combinatorial calculations, the prior probability distribution
is found to be P(OH0) = (0.1569, 0.0765, 0.0635, 0.4447, 0.1694, 0.0494, 0.0353,
0.0024, 0.0024). For example, in order to determine the probability P(OH0 =
st) we ﬁrst calculate the number of diﬀerent ways in which we can obtain a
straight: by disregarding permutations of the three cards, we get 52 · 4 · 4 by
letting ka2 be a straight. However, since we do not want to include straight
ﬂushes, we subtract the number of ways (52) in which we can obtain a straight
ﬂush (again disregarding permutations), and ﬁnally we divide by the number
of ways to draw three cards out of 52 cards (the latter is equal to the binomial
coeﬃcient
52
3

):
P(OH0 = st) = 52 · 4 · 4 −52
52
3

= 0.0353.
The Probability Distribution P (OH1 | OH0, FC)
Due to the logical links between OH0 and FC, it is suﬃcient to consider only
nine out of the possible 36 parent conﬁgurations, namely (no, 3), (1 a, 2),
(2 cons, 1), (2 s, 1), (2 v, 1), (ﬂ, 0), (st, 0), (3 v, 0), (sﬂ, 0). The last four
are obvious. In Table 3.7, the results of the approximate combinatorial calcu-
lations are given.
The probabilities for the remaining parent conﬁgurations may be whatever
is convenient, so put, for example, P(OH1 | 3 v, 1) = (1, 0, . . . , 0).
The Probability Distribution P (OH2 | OH1, SC)
First, a table P(OH2′ | OH1, SC) similar (but not identical in the numbers)
to Table 3.7 can be calculated. However, the states of OH2′ are not the ones
we are interested in. We are interested in the value of the hand, and a state
such as 2 cons is of no value unless one of them is an ace. Therefore, the
probabilities for the states of OH2′ are transformed to probabilities for OH2.
For the transformation, the following rules are used:

68
3 Building Models
(OH0, FC)
(no, 3) (1 a, 2) (2 cons, 1) (2 s, 1) (2 v, 1)
no
0.1583
0
0
0
0
1 a
0.0534
0.1814
0
0
0
2 cons 0.0635
0.0681
0.3470
0
0
2 s
0.4659
0.4796
0.3674
0.6224
0
OH1 2 v
0.1694
0.1738
0.1224
0.1224
0.9592
ﬂ
0.0494
0.0536
0 0.2143
0
st
0.0353
0.0383
0.1632
0.0307
0
3 v
0.0024
0.0026
0
0
0.0408
sﬂ
0.0024
0.0026
0 0.0102
0
Table 3.7. P(OH1 | OH0, FC) for the nonobvious parent conﬁgurations.
1 a = 1 a + 1
6(2 cons + 2 s),
no = no + 5
6(2 cons + 2 s).
The probabilities of 2 a are calculated speciﬁcally. The resulting probabilities
are given in Table 3.8.
(OH1, Sc)
(no, 2) (1 a, 2) (2 cons, 1) (2 s, 1) (2 v, 1)
no
0.5613
0
0.5903
0.5121
0
1 a 0.1570
0.2425
0.1181
0.1024
0
2 v 0.1757
0.0667
0.1154
0.1154
0.8838
OH2 2 a 0.0055
0.1145
0.0096
0.0096
0.0736
ﬂ
0.0559
0.0559
0 0.2188
0
st
0.0392
0.0392
0.1666
0.0313
0
3 v 0.0027
0.0027
0
0
0.0426
sﬂ
0.0027
0.0027
0 0.0104
0
Table 3.8. P(OH2 | OH1, SC) for the nonobvious conﬁgurations.
Using a model such as the one in Figure 3.11 and with the conditional
probability tables speciﬁed in this section, we have established a model for
assisting a (novice) poker player. However, if my opponent knows that I use
the system, he can change cards in such a way that aﬀects my estimate of his
hand.
3.2.4 Transmission of Symbol Strings
A language L over 2 symbols (a, b) is transmitted through a channel.
Each word is surrounded by the delimiter symbol c. In the transmis-

3.2 Determining the Conditional Probabilities
69
sion some characters may be corrupted by noise and be confused with
others.
A ﬁve-letter word is transmitted. Give a model that can determine the
probabilities for the transmitted symbols given the received symbols.
There are ﬁve hypothesis variables T1, . . . , T5 with states a, b and ﬁve informa-
tion variables R1, . . . , R5 with states a, b, c. There is a causal relation from Ti
to Ri. Furthermore, there may also be a relation from Ti to Ti+1(i = 1, . . . , 4)
encoding that certain pairs of symbols are more likely to occur than oth-
ers. You could also consider more-involved relations from pairs of symbols to
symbols, but for now we refrain from doing that. The structure is given in
Figure 3.18.
T1
T2
T3
T4
T5
R1
R2
R3
R4
R5
Fig. 3.18. A model for symbol transmission. Ti are the symbols transmitted; Ri
are the symbols received.
The conditional probabilities can be established through experience. The
probabilities P(Ri | Ti) will be based on statistics describing the frequencies
of confusion. Let Table 3.9 be the result.
T = a T = b
R = a 0.80
0.15
R = b 0.10
0.80
R = c 0.10
0.05
Table 3.9. P(R | T) under transmission.
You may obtain the probabilities P(Ti+1 | Ti) by investigating the ﬁve-
letter words in L. What is the frequency of the ﬁrst letter? What is the
frequency of the second letter given that the ﬁrst letter is a? You continue to
do this for each letter. You can reﬁne this frequency analysis by also taking
the frequencies of the words into consideration. Let Table 3.10 be the result
of a frequency analysis.
You can calculate the required probabilities from Table 3.10 using the
fundamental rule. The prior probabilities for T1 are (0.5, 0.5), and P(T2, T1) is

70
3 Building Models
First 2
Last 3 letters
letters
aaa
aab
aba
abb
baa
bab bba bbb
aa
0.017 0.021 0.019 0.019 0.045 0.068 0.045 0.068
ab
0.033 0.040 0.037 0.038 0.011 0.016 0.010 0.015
ba
0.011 0.014 0.010 0.010 0.031 0.046 0.031 0.045
bb
0.050 0.060 0.056 0.057 0.016 0.023 0.015 0.023
Table 3.10. Frequencies of ﬁve-letter words in L. The word abaab, for example,
has frequency 0.040.
achieved by adding the elements in each row. Table 3.11 gives two conditional
probabilities.
a
b
a 0.6 0.4
b 0.4 0.6
a
b
a 0.24 0.74
b 0.76 0.26
P(T2 | T1)
P(T3 | T2)
Table 3.11. Two conditional probabilities for ﬁve-letter words in L.
An alternative model would be to have a hypothesis variable, Word, with
32 states and with Table 3.10 as prior probabilities (see Figure 3.19).
Word
R1
R2
R3
R4
R5
Fig. 3.19. An alternative model for symbol transmission. Word is the set of possible
transmitted words.
This is manageable because of the small number of ﬁve-letter words over
{a, b}; but if the alphabet had 24 symbols, and if six-letter words were con-
sidered, the number of states in Word would become intractably large. On
the other hand, the model of Figure 3.18 may be too simple to catch the
dependencies in Table 3.10, so the task really is to analyze the table in order
to ﬁnd the simplest structure describing it. There are methods for doing this,
and we return to this topic in Chapter 7.

3.2 Determining the Conditional Probabilities
71
3.2.5 Cold or Angina?
The estimation of the conditional probabilities for the example introduced
in Section 3.1.2 has a very subjective ﬂavor based on my own experience
with colds and anginas. I estimate the following probabilities: P(Cold?),
P(Angina?), P(See Spots? | Angina?), P(Fever? | Cold?, Angina?), P(Sore Th
roat? | Cold?, Angina?).
Because in the morning I do not recall having been chilly yesterday, the
prior probabilities P(Cold?) and P(Angina?) are my subjective recollections of
how often I wake up in the morning with a cold or with an angina. Because cold
is more frequent than angina, I put P(Cold?) = (0.97, 0.03) and P(Angina?) =
(0.993, 0.005, 0.002); the order of the states are taken from Section 3.1.2.
Without angina or with mild angina, I will not see spots. With severe
angina, I would expect to see spots, but I may not. I put P(See Spots? | An
gina? = severe) = (0.1, 0.9).
The Probability Distribution P (Sore Throat? | Cold?, Angina?)
If I suﬀer from neither a cold nor angina, I have a background probability
of 0.05 of having a sore throat in the morning; this background probability
covers everything other than cold and angina that may result in a sore throat.
A cold as well as angina may give me a sore throat. If I only have a cold, the
probability of a sore throat is 0.4. If I have mild angina, the probability of a
sore throat is 0.7, and in the case of severe angina, I will certainly have a sore
throat. What if I have both a cold and mild angina? I do not have suﬃcient
experience to come up with a reliable estimate. Instead, I can use the two
conditional probabilities from before: out of 100 mornings, I will wake up ﬁve
mornings with a “background produced” sore throat. Out of the remaining 95
mornings, the cold yields a sore throat in 40% of them, that is, 38 mornings.
Out of the remaining 57 mornings, mild angina will cause a sore throat in
70% of them: 39.9 mornings. In total, if I have both mild angina and a cold, I
will have a sore throat in 82.9 mornings out of 100. The number 82.9 indicates
an unjustiﬁed precision, and for psychological reasons we set the probability
to 0.85. In Section 3.3.2 on “noisy-or,” we give a systematic treatment of this
method of estimating probabilities. The full table for P(SoreThroat? | Cold?,
Angina?) is given in Table 3.12. It is left as an exercise to complete the model.
Angina? = no Angina? = mild Angina? = severe
Cold? = no
0.05
0.7
1
Cold? = yes
0.4
0.85
1
Table 3.12. P(Sore Throat? = yes | Cold?, Angina?).

72
3 Building Models
3.2.6 Why Causal Networks?
As mentioned previously, the structure of a Bayesian network need not reﬂect
cause–eﬀect relations. The only requirement is that the d-separation proper-
ties of the network hold for the domain modeled. There are, however, good
reasons to strive for causal networks. The model in Figure 3.20 can be used
to illustrate some of the points. We have a disease Dis and two tests, Ts and
Tt.
Dis
Tt
Ts
Fig. 3.20. A model for a disease with two tests.
When diagnosing, you usually reason opposite to the directions of the
arrows in Figure 3.20, and trained physicians are usually inclined to provide
conditional probabilities in the diagnostic direction. A model reﬂecting this
might look like the one in Figure 3.21 a).
Dis
Dis
Tt
Tt
Ts
Ts
(a)
(b)
Fig. 3.21. Diagnostic models for the situation in Figure 3.20: (a) with a wrong
independence, (b) with no (conditional) independence.
The model in Figure 3.21(a) is not correct. According to this model, Ts
and Tt are independent (which is not the case in Figure 3.20), and there is
no way to correct it by specifying the potentials in a sophisticated manner.
To correct the model, you must add some extra structure making Ts and Tt
dependent. You may, for example, introduce a link from Ts to Tt, as is done
in Figure 3.21(b). Therefore, to get a correct model, it is not suﬃcient to
acquire P(Dis | Ts, Tt) together with the “priors” P(Ts) and P(Tt). This also
illustrates another point, namely that a correct model of a causal domain is
minimal with respect to links. In other words, if for some reason you wish to

3.3 Modeling Methods
73
represent a causal relation with a link directed opposite to the causal direction,
then the total number of links can not decrease, and most likely it will increase.
The model in Figure 3.20 has another advantage over the models in Fig-
ure 3.21, namely that the conditional probabilities P(Ts | Dis) and P(Tt | Dis)
are more stable than the conditional probabilities speciﬁed for the models in
Figure 3.21. The conditional probabilities for Figure 3.20 reﬂect general prop-
erties of the relation between diseases and tests, and they are the ones that
a manufacturer of tests can publish, whereas the conditional probabilities for
Figure 3.21 are a mixture of disease–test relations and prior frequencies of the
disease.
It may happen that it is not possible to acquire the conditional probabili-
ties for a correct model, but instead, other types of conditional probabilities
are available. Assume, for example, that for the model in Figure 3.20, we can
acquire only the potentials P(Dis | Ts), P(Dis | Tt), P(Ts), and P(Tt). Us-
ing Bayes’ rule on P(Dis | Ts) and P(Ts), we get P(Dis) and P(Ts | Dis). The
same can be done with P(Dis | Tt) and P(Tt). If the two calculations of P(Dis)
give the same result, we have the required potentials. If, on the other hand,
the two calculations disagree, there is no safe way to solve the conﬂict. It can
happen in many diﬀerent situations that you have a set of potentials, but the
model requires another set and there is no safe way of inferring the needed
potentials. It is a lively area of research to construct engineering methods for
getting the best out of what you have.
In Chapter 9, we deal with interventions. They provide another good rea-
son for constructing causal models. An intervention is an action that has an
impact on the state of certain variables. The impact of an intervention will
spread in the causal direction, but not opposite to the causal direction. If the
model does not reﬂect causal directions, it cannot be used to simulate the
impact of interventions.
3.3 Modeling Methods
Much skepticism of Bayesian networks stems from the question of where the
numbers come from. As shown in the previous section, they come from many
diﬀerent sources. If you are building a model over a domain in which experts
actually do take decisions based on estimates, why should you not be able
to make your Bayesian network estimate at least as well as the experts? You
can, for example, use the technique described in Section 1.1 to acquire the
probabilities from the experts. The acquisition of numbers is, of course, not
without problems, and in this section we give some methods that can help
you in this job. Also, we provide some modeling tricks.
3.3.1 Undirected Relations
It may happen that the model must contain dependence relations among vari-
ables A, B, C, say, but it is neither desirable nor possible to attach directions

74
3 Building Models
to them.1 The relation may, for example, be a description of possible conﬁgu-
rations. This diﬃculty may be overcome by using conditional dependence as
described in Section 2.2.1 (converging inﬂuence).
Let R(A, B, C) describe the relation using the values 0 and 1; R(A, B, C) =
1 for all valid conﬁgurations of A, B, and C. Add a new variable D with two
states y and n and let A, B, and C be parents of D (see Figure 3.22). Assign D
the deterministic conditional probability table given as P(D = y | A, B, C) =
R(A, B, C) (and P(D = n | A, B, C) = 1−R(A, B, C)) and enter the evidence
D = y. The variable D is called a constraint variable, and by entering D = y
we are basically forcing the relation/constraint to hold.
A
B
C
D
D = y
Fig. 3.22. A way to introduce undirected relations among A, B, and C.
Example 3.1. If we want to model that A, B, and C are always in the same
state, then we can assign D the conditional probability table given in Ta-
ble 3.13 (assuming that A, B, and C are binary).
C = y
C = n
B = y B = n B = y B = n
A
y
1
0
0
0
n
0
0
0
1
Table 3.13. The conditional probability distribution P(D = y | A, B, C) for the
constraint variable D modeling that A, B, and C are always in the same state.
Example 3.2. I have washed two pairs of socks in the washing machine. The
washing has been rather hard on them, so they are now diﬃcult to distin-
guish. However, it is important for me to pair them correctly. To classify the
socks, I have pattern and color. A classiﬁcation model may be like the one
in Figure 3.23. The variables Si have states t1 and t2 for the two types, the
1 In that case, the model is called a chain graph. A chain graph is an acyclic graph
with both directed and nondirected links, where acyclic means that all cycles
consist of only nondirected links.

3.3 Modeling Methods
75
variables Pi have two pattern types, and the variables Ci have two color types.
The constraint that there are exactly two socks of each type is described in
Table 3.14.
S1
S2
S3
S4
P1
P2
P3
P4
C1
C2
C3
C4
Constraint
Constraint = y
Fig. 3.23. A model for classifying pairs of socks.
S1 t1 t1 t1 t1 t1 t1 t1 t1 t2 t2 t2 t2 t2 t2 t2 t2
S2 t1 t1 t1 t1 t2 t2 t2 t2 t1 t1 t1 t1 t2 t2 t2 t2
S3 t1 t1 t2 t2 t1 t1 t2 t2 t1 t1 t2 t2 t1 t1 t2 t2
S4 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2 t1 t2
P 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0
Table 3.14. The table for P = P(Constraint = y | S1, S2, S3, S4); t1 and t2 are the
two states of S1, S2, S3, S4.
The situation is more subtle if the relation R(A, B, C) is of probabilistic
nature. If A, B, and C have no parents, R(A, B, C) can be a joint probability
table. On the other hand, if A has a parent, then R(A, B, C) may be considered
as representing a feedback cycle. We shall not deal with this problem but refer
the reader to the literature on chain graphs.
3.3.2 Noisy-Or
When a variable A has several parents, you must specify P(A | c) for each
conﬁguration c of the parents. If you take the distributions from a database,
the number of cases for each conﬁguration may become too small. Also, the
conﬁgurations may be too speciﬁc for any expert. You may also be in the
situation that you have reasonable estimates of P(A | B) and P(A | C), but
you require P(A | B, C). Then, you should look for assumptions that reduce
the number of distributions to specify.
Consider in Section 3.2.5 the conditional probability table for P(Sore Th
roat? | Cold?, Angina?). It was possible to get estimates of P(Sore Throat? |

76
3 Building Models
Cold?) and P(Sore Throat? | Angina?), but is there a general way to describe
how they then combine into P(Sore Throat? | Cold?, Angina?)? The following
is a way of describing it.
There are three events causing me to have a sore throat in the morning:
•
the “background event,” which in 5% of the mornings yields a sore throat;
•
cold, which causes a sore throat with probability 0.4;
•
angina, which when mild causes a sore throat with probability 0.7, and
when it is severe it certainly causes a sore throat.
The preceding uncertainty can be interpreted as follows. If any of the
causes are present, then I have a sore throat unless something has prevented
it. In other words, if I have mild angina, then I have a sore throat unless some
other circumstances prevent it, and there is a 30% chance that it is prevented.
In the same way, there is a 60% chance that some inhibitor prevents me from
having a sore throat although I have a cold, and the background event is
prevented with probability 0.95.
Now, if we assume that the preventing factors are independent, then the
combined probabilities are easy to calculate as one minus the product of the
appropriate probabilities for the inhibitors (note that the background event
is always a fact). The probabilities are given in Table 3.15.
Angina? = no Angina? = mild Angina? = severe
Cold? = no
0.05
1 −0.95 · 0.3
1
Cold? = yes 1 −0.95 · 0.6 1 −0.95 · 0.3 · 0.6
1
Table 3.15. Calculation of P(Sore Throat? = yes | Cold?, Angina?). Note that some
numbers are slightly diﬀerent from the corresponding numbers in Table 3.12.
Another way to view the calculations above is to make the independence
assumptions explicit in the model. Consider the model shown in Figure 3.24(a)
and introduce an intermediate node STC between Sore Throat (ST ) and Cold?
(C) as well as an intermediate node STA between Sore Throat? and Angina?
(A). The node STC captures the eﬀect that Cold? has on Sore Throat? (i.e., it
represents a “cold-induced” sore throat), whereas STA represent an “angina-
induced” sore throat. In order to model the “background event” we introduce
two additional nodes B and STB, where B represent the “background event,”
and STB plays the same role as STC and STA above. The three nodes STA,
STB, and STC also represent the inhibitors, and they are assigned the condi-
tional probability tables shown in Table 3.16; the numbers have been deduced
from the itemized list above. Finally, since we will have a sore throat no mat-
ter whether it is induced by cold, angina, or something else, we assign ST
a conditional probability distribution that corresponds to a logical-or. The
resulting model is shown in Figure 3.24(b), where the variables STA, STB,
and STC are independent, reﬂecting the assumption that the inhibitors are

3.3 Modeling Methods
77
independent. Moreover, if we marginalize out the variables STA, STB, and
STC, we end up with the conditional probability table in Table 3.15 (see also
Exercise 3.20).
STB
B
A
C
ST
A
C
(a)
(b)
ST
STC
STA
Fig. 3.24. Figure (a) shows the model structure for P(ST | C, A), and ﬁgure (b)
shows the model structure that explicitly represent the independence assumption
about the inhibitors.
A
no
mild
severe
STA yes 0 1 −0.3
1
no
1
0.3
0
B
yes
STB yes 1 −0.95
no
0.95
P(STA | A)
P(STB | B)
C
no
yes
STC yes 0 1 −0.6
no
1
0.6
P(STC | C)
Table 3.16. The conditional probability tables P(STA | A), P(STB | B), and
P(STC | C).
The preceding construction is an example of the simplifying assumption
called a noisy-or. In what follows we put this assumption into a more general
context, albeit only with binary variables.
Let A1, . . . , An be binary variables listing all the causes of the binary
variable B. Each event Ai = y causes B = y unless an inhibitor prevents it,
and the probability for that is qi (see Figure 3.25).

78
3 Building Models
A1
A2
An
B
1 −q1
1 −q2
1 −qn
Fig. 3.25. The general situation for noisy-or. Here qi is the probability that the
impact of Ai is inhibited.
In other words, P(B = n | Ai = y) = qi. We assume that all inhibitors are
independent. Then P(B = n | A1, A2, . . . , An) = 
j∈Y qj, where Y is the set
of indices for variables in the state y. For example,
P(B = y | A1 = y, A2 = y, A3 = · · · = An = n)
= 1 −P(B = n | A1 = y, A2 = y, A3 = · · · = An = n)
= 1 −q1 · q2.
By assuming “noisy-or,” the number of probabilities to estimate grows lin-
early with the number of parents.
Note 1. We require P(B = y | A1 = · · · = An = n) to be 0. This may seem
to restrict the applicability of the approach. However, as in the preceding ex-
ample, if P(B = y) > 0 when none of the causal events in the model are on,
then introduce a background event that is always on.
Note 2. The complementary construction to noisy-or is called noisy-and. A
set of causes should all be “on” in order to have an eﬀect. However, the causes
have random inhibitors, which are mutually independent.
Note 3. As in Figure 3.24(b), noisy-or can be modeled directly without per-
forming the calculations (see Figure 3.26). This highlights the assumptions
behind the noisy-or gate. If a cause is on, then its eﬀect may be prevented by
an inhibitor, and the probabilities for the inhibitors to be present are inde-
pendent.
Note 4. The noisy-or model has been generalized to variables having more
than two states, and in this form it is called a noisy-max; in this model we
assume that the states of B are ordered.
3.3.3 Divorcing
Let A1, . . . , An be a list of variables all of which are causes of B. If you wish to
specify P(B | A1, . . . , An), you might have a very large knowledge acquisition

3.3 Modeling Methods
79
A1
A2
An
B
B1
B2
Bn
Fig. 3.26. Direct modeling of a noisy-or gate. Here P(Bi | Ai) is the original
P(B | Ai), and P(B | B1, . . . , Bn) is logical or.
task ahead of you. Either you need to ask the experts on the distribution of
B given very speciﬁc parent conﬁgurations or, if the table must be extracted
from a database, you need a very large set of cases. The following example
illustrates the problem.
Example 3.3 (Granting a loan). A bank will decide on a mortgage loan for a
customer who wishes to purchase a house. The customer is asked to ﬁll in a
form giving information on various ﬁnancial and personal matters together
with various key information on the house. The answers are used to estimate
the probability that the bank will get its money back.
The information can be the following: type of job, yearly income, other
ﬁnancial commitments, number and types of cars in the family, number of
previous addresses during the last ﬁve years, number of children in the family,
number of divorces, size and age of the house, price of the house, and type of
environment.
In principle, each slot in the form represents a variable with a causal im-
pact on the variable Money back?. If we assume that each parent variable
has ﬁve states, we have already listed a parent space with 511 ≈5,000,000
conﬁgurations. For each conﬁguration, we request a distribution for A. No
person can estimate that number of distributions, nor can he or she estimate
a distribution for a divorced businesswoman with a yearly income of $50,000,
having loans of $70,000 already, one car, three previous addresses, two chil-
dren, wanting to purchase a twenty-year-old house of 150 m2 at the price of
$200,000 in a farming area. Also, if the distributions are to be taken from a
database, the bank will need at least 50,000,000 cases that may not be more
than 10 years old.
To handle this kind of task, we divorce the parents. The set of parents
A1, . . . , Ai for B is divorced from the parents Ai+1, . . . , An by introducing a
mediating variable C, making C a child of A1, . . . , Ai and a parent of B (see
Figure 3.27).

80
3 Building Models
A1
A1
A2
A2
A3
A3
A4
A4
B
B
C
Fig. 3.27. Parents A1 and A2 are divorced from A3 and A4 by introducing the
variable C.
The assumption behind divorcing is the following (with reference to Fig-
ure 3.27).
The set of conﬁgurations (A1, A2) can be partitioned into the sets c1, . . . ,
cm such that whenever two conﬁgurations (a1, a2) and (a′
1, a′
2) are elements in
the same ci, then P(B | a′
1, a′
2, A3, A4) = P(B | a1, a2, A3, A4). The divorcing
variable then has c1, . . . , cm as states.
In the example of granting a loan, it is impossible to perform an analysis as
before, and you will group the variables based on another type of insight into
the domain. For example, the variables about the house can be grouped and
given a common child variable describing how safe the mortgage will be, the
ﬁnancial variables may be grouped for a variable describing the applicant’s
ﬁnancial abilities; and the remaining variables may describe the applicant’s
stability.
In connection to the example of granting a loan, it should be noted that if
we only want to perform a classiﬁcation, then we need not build a Bayesian
network. Other techniques such as statistical classiﬁers and classiﬁcation trees
(see Section 8.4) may be more adequate. However, if we also wish to calculate
decision recommendations, we will need the posterior probabilities provided
by a Bayesian network. We will deal further with this in Chapter 9.
3.3.4 Noisy Functional Dependence
There are ways of directing the divorcing. “Noisy-or” and “noisy-and” are
examples of a general method called noisy functional dependence.
Example 3.4 (Headache). Headache (Ha) may be caused by fever (Fe), hang-
over (Ho), ﬁbrositis (Fb), brain tumor (Bt), and other causes (Ot), and you
may choose to soothe it with aspirin (As) (we ignore the eﬀect aspirin has on
fever). Let Ha have the states no, mild, moderate, severe. The various causes
support each other in the eﬀect. If, for example, Ho = y or Fb = y is present,
then it may yield a mild Ha, but if both are present, then the Ha would be
moderate. Furthermore, if also As = y, then Ha may drop to no or mild. Al-
though the various parents of Ha combine in a rather involved manner, we
still have the feeling that the impacts of the causes are independent. This kind

3.3 Modeling Methods
81
of independence can be described as follows: if the headache is at level l, and
we add an extra cause for headache, then the result is a headache at level q
independent of how the initial state has been caused.
Assume that we can estimate conditional probabilities of type P(Ha | C),
and we want to combine the eﬀects of the various causes. For this, we can imag-
ine that we attach a number to the states of Ha: no →0, mild →1, moderate →
2, severe →4, and the “adding up” of the eﬀects consists in adding the num-
bers. A model could be similar to the one in Figure 3.28.
Ha-Ot
Ha-Fe
Ha-Ho
Ha-Fb
Ha-Bt
Ot
Fe
Ho
Fb
Bt
Ha
Fig. 3.28. A model for causes of headache. The bottom node adds up the eﬀects.
The hidden assumption behind this method of adding up is that the eﬀect
from any cause is independent of the current state of headache, and it is
faithfully reﬂected in the numbers attached to the headache states. To make
it explicit in the model, we can give each headache node a child with numbers
as states, these nodes are given a common child that adds the numbers, and
a new node translates the numbers to Ha states (see Figure 3.29).
Now, for P(Nu-Ha | Nu-Ot, Nu-Fe, Nu-Ho, Nu-Fb, Nu-Bt) we can perform
divorcing, we can add one number at a time (see Figures 3.30 and 3.31), or
we can represent the function in any other kind of compact way.
The eﬀect of aspirin can be included in two diﬀerent ways. Either it sub-
tracts a number from the sum or it has a direct eﬀect on the headache state.
3.3.5 Expert Disagreements
It may happen that we are in a situation in which the experts disagree on
the conditional probabilities for a model. Consider the model in Figure 3.32,
and assume that we have three experts who agree on P(B) and P(C | A), but
they disagree on P(A) and P(D | B, C). For the three experts, we have P(A =
y) = (0.1, 0.3, 0.4), and the table for P(D | B, C) can be seen in Table 3.17.
If you have equal conﬁdence in the three experts, you can take the mean of
the three numbers. If your conﬁdence in the experts varies, you may incorpo-
rate this and calculate a weighted average. For example, you may give the ﬁrst

82
3 Building Models
Ha-Ot
Ha-Fe
Ha-Ho
Ha-Fb
Ha-Bt
Ot
Fe
Ho
Fb
Bt
Ha
Nu-Ot
Nu-Fe
Nu-Ho
Nu-Fb
Nu-Bt
Nu-Ha
Fig. 3.29. A model that adds the headache states by transforming to numbers,
adding, and transforming back to headache states again.
Nu-x
Nu-y
Nu-z
Nu-Ot
Nu-Fe
Nu-Ho
Nu-Fb
Nu-Bt
Nu-Ha
Fig. 3.30. The adder represented through divorcing.
Nu-1
Nu-2
Nu-3
Nu-Ot
Nu-Fe
Nu-Ho
Nu-Fb
Nu-Bt
Nu-Ha
Fig. 3.31. The adder represented through adding one number at a time.

3.3 Modeling Methods
83
A
B
C
D
Fig. 3.32. A model with expert disagreements. All variables are binary.
B
y
n
C
y (0.4, 0.4, 0.6) (0.7, 0.9, 0.7)
n (0.6, 0.4, 0.5) (0.9, 0.7, 0.9)
Table 3.17. P(D = y | B, C) for the three diﬀerent experts s1, s2, s3.
two experts a conﬁdence weight 1 and the third expert a conﬁdence weight
2. Because the total conﬁdence weight is 4, you get a conﬁdence distribution
(0.25, 0.25, 0.5), and for A you have P(A = y) = 0.25·0.1+0.25·0.3+0.5·0.4 =
0.3. The probability P(D | B, C) is shown in Table 3.18.
B
y
n
C
y 0.5 0.75
n 0.5 0.85
Table 3.18. P(D = y | B, C) weighted with conﬁdence distribution (0.25, 0.25, 0.5).
The experts can be represented explicitly in the model by introducing a
variable S with states s1, s2, and s3. The variable S has a link to the nodes,
about whose tables the three experts disagree (see Figure 3.33).
The variable S is given the conﬁdence distribution (0.25, 0.25, 0.5) as
before, and the child variables have a conditional probability table for each
expert. The table P(D = y | B, C, S) is as in Table 3.17.
By modeling the diﬀerent expert opinions explicitly, you have prepared
the model for adaptation. Whenever you have a case with evidence e entered
into the model, you will get P(S | e), which is an updated indication of which
expert to believe. That is, you get a new conﬁdence distribution that can be
used for the next case, see also Section 6.3.

84
3 Building Models
A
B
C
D
S
Fig. 3.33. The model from Figure 3.32 with the experts represented explicitly by
the node S.
3.3.6 Object-Oriented Bayesian Networks
Complex Bayesian network models often include copies of almost-identical
network fragments. Consider, for example, the Bayesian network shown in
Figure 3.34, and assume that X1 and X2 have the same state space (sp(X1) =
sp(X2)), and that the conditional probability tables associated with the nodes
labeled A are identical; similarly for the nodes labeled B, C, D, and E. Given
these two assumptions we see that the network contains four identical copies
of the same network fragment deﬁned by the ﬁve nodes A, B, C, D, E.
X1
X2
Y1
Y2
Y
A
A
A
A
B
B
B
B
C
C
C
C
D
D
D
D
E
E
E
E
Fig. 3.34. A Bayesian network containing repetitive substructures.

3.3 Modeling Methods
85
The occurrence of such repetitive structures can be exploited during model
construction. For example, instead of explicitly specifying the same network
fragment multiple times, we could instead construct a generic network frag-
ment that can be instantiated the required number of times. By borrowing
terminology from the object-oriented programming paradigm, we call such
a generic network fragment a class, and each network fragment that is pro-
duced by instantiating the class is called an object. Figure 3.35 shows a class
description (called Class-name) for the duplicated network fragment in Fig-
ure 3.34. In order for the class to support the speciﬁcation of the conditional
probability distribution for A, the class includes an artiﬁcial node X (drawn
as a dashed node) having the same state space as X1 and X2. Note that this
node does not correspond to an actual variable, but should rather be seen as
a “placeholder” that simply allows us to specify the probability distribution
for A. The shaded nodes in Figure 3.35 indicate the part of the class/object
that is accessible outside the object; they may be parents of nodes outside
the object. Nodes that are neither dashed nor shaded are encapsulated within
the object, and they may therefore be considered invisible to the rest of the
model.
X
A
B
C
D
E
Class-name
Fig. 3.35. A class model for the duplicated network fragment in Figure 3.34. Class-
name is the name of the class.
Given such a class description, we can make an equivalent representation of
the model in Figure 3.34 by instantiating the class four times and connecting
X1, X2, Y1, and Y2 to the objects (labeled Inst. 1, Inst. 2, Inst. 3, Inst. 4)
as appropriate. The resulting model is shown in Figure 3.36 and is called an
object-oriented Bayesian network model (OOBN). The dashed arcs indicate
which node X is a placeholder for in the various objects.
As implied by the discussion above, an object (or a class) can be seen as a
function that given a certain input provides a probability distribution over a

86
3 Building Models
X1
X2
Y1
Y2
Y
A
A
A
A
B
B
B
B
C
C
C
C
D
D
D
D
E
E
E
E
X
X
X
X
Inst. 1
Inst. 2
Inst. 3
Inst. 4
Fig. 3.36. An object-oriented Bayesian network representation of Figure 3.34.
set of variables. For example, the class shown in Figure 3.35 speciﬁes a proba-
bility distribution over D and E given a state for X. Based on this perspective,
we can partition the elements in an object into three sets: input attributes,
output attributes, and encapsulated attributes. In the example above, X is an
input attribute, D and E are output attributes, and A, B, and C are encap-
sulated attributes. Following standard programming terminology, the input
attributes in the class description can be seen as the formal parameters of the
corresponding function, whereas the actual parameters passed to an object
are identiﬁed as the parents of the input attributes in the surrounding model.
Thus, X can be considered a formal parameter, and X1 is the actual param-
eter passed to the left-most object in Figure 3.36. In general, we also allow
encapsulated attributes and output attributes to be objects themselves. How-
ever, input attributes must correspond to variables, since they serve as the
parameters passed to the object. Note that the simplest type of class/object
consists of a single variable, where the input attributes correspond to the
parents of that variable.
The speciﬁcation of encapsulated attributes is closely related to the con-
cept of information hiding in the object-oriented programming paradigm. By
taking this idea one step further, we obtain a straightforward mechanism for
simplifying the visual representation of a model by abstracting away irrelevant
details. For example, by abstracting away the encapsulated attributes in Fig-
ure 3.36 we obtain the OOBN shown in Figure 3.37. In general, when objects

3.3 Modeling Methods
87
are encapsulated within other objects this approach provides us a method for
obtaining a hierarchical representation of the model; each level corresponds
to a particular level of abstraction revealing the encapsulated attributes for
the current layer of objects.
X
X
X
X
Y
D
D
D
D
E
E
E
E
X1
X2
Y1
Y2
Inst. 1
Inst. 2
Inst. 3
Inst. 4
Fig. 3.37. An object-oriented Bayesian network model corresponding to the model
shown in Figure 3.36. The encapsulated attributes have been hidden to simplify the
representation.
Top-Down Construction of OOBNs
The input attributes and the output attributes are also referred to as the
interface of the object, since instantiating these nodes will d-separate the
internal part of the object (the encapsulated attributes) from the rest of the
network (the proof is left as an exercise. This property supports a top-down
model construction process: you may start constructing the model at a high
level of abstraction by including only the interfaces of the objects without
specifying their internal details. Later you can change the abstraction level
and start specifying/reﬁning the internal class description.
For example, assume that you should construct a Bayesian network model
for the safety characteristics of a car. We know that the type of car and its
maintenance level inﬂuence both the general steering characteristics of the
car as well as its braking capabilities. In turn, these two aspects inﬂuence the
steering safety and the braking power of the car.
We also know that the steering safety and the braking power are inﬂuenced
by the grip of the car, and the grip is mainly determined by the tire type
and the tire mileage. However, it may happen that at the time of model
speciﬁcation we do not know (or do not want to specify) the relationship
between the grip of the car and tire type and mileage. See Figure 3.38 for a

88
3 Building Models
Car type
Maintenance
Brakes
Steering
Steering
Braking
power
safety
Fig. 3.38. A partial Bayesian network model for the safety characteristics of a car.
The dashed arrows indicate unspeciﬁed parent and child relations.
partial Bayesian network representation. We could instead construct a class
representing the grip of the car with a rudimentary internal structure and
simply include the interface of the class in the model. An example is shown in
Figure 3.39. Figure 3.40 shows two possible speciﬁcations of a class modeling
the tire grip. The leftmost class could serve as an initial approximation to the
more detailed speciﬁcation shown at the right-hand side of Figure 3.40.
Car type
Maintenance
Tire mileage
Tire type
Tire type
Grip
Brakes
Steering
Steering
Braking
power
safety
Tire
mileage
Tire-grip instance
Fig. 3.39. An object-oriented Bayesian network model of the driving characteristics
of a car.
Subclassing and Inheritance
A powerful property of object-oriented modeling is the use of subclassing (or
inheritance) between classes. When a class C′ is a subclass of another class

3.3 Modeling Methods
89
Tire type
Tire type
grip
grip
grip
grip
grip
grip
grip
grip
Front left
Front right
Back left
Back right
Front
Back
Tire-grip 1
Tire-grip 2
Tire
Tire
mileage
mileage
Fig. 3.40. Two possible reﬁnements of the interface for the grip class illustrated in
Figure 3.39. In the rightmost reﬁnement, we model the grip on each of the tires.
C (also called the superclass for C′), then an instance of C can always be
substituted with an instance of class C′. For example, consider again the two
classes in Figure 3.40. We wish for the class Tire grip 2 to be viewed as a
subclass of Tire-grip 1, which means that any instance of Tire-grip 1 can be
substituted with an instance of Tire-grip 2. This example is quite obvious,
since the two classes have the same interface connecting them to the rest of
the model. However, suppose now that we should reﬁne our grip model so
that it also covers the car type; we assume that for a car with front-wheel
drive there is a tendency for the front tires to be more worn than for a car
with rear-wheel drive (conversely for cars with rear-wheel drive). One way
to include these considerations into the model is to construct a class as in
Figure 3.41.
Tire type
grip
grip
grip
Front
Back
Tire-grip-reﬁnement
Tire
mileage
Car type
Fig. 3.41. The class Tire-grip-reﬁnement taking the car type into account.
We would now like to be able to replace the instance in Figure 3.39 with an
instance of class Tire-grip-reﬁnement. However, this raises a technical question:

90
3 Building Models
If we simply replace the instance in Figure 3.39 without connecting the input
node Car type to an actual node in the model, then both Back Grip and
Front Grip would have a parent with an unspeciﬁed probability distribution
(see Figure 3.42). In order to avoid this problem, we associate a so-called
default potential with each input node in the class; a default potential is
simply a probability distribution that will be used when an input node is not
connected to a node in the surrounding model. For the example above, we
could specify the default potential P(Car type) = (0.5, 0.5), assuming that
the node is binary. Based on these considerations we require that if a class C′
should be a subclass of another class C, then it should hold that:
•
the set of input variables for C is a subset of the input variables for C′,
and
•
the set of output variables for C is a subset of the output variables for C′.
Car type
Car type
Maintenance
Tire mileage
Tire type
Tire type
Grip
Brakes
Steering
Steering
Breaking
power
safety
Tire
mileage
Tire-grip-reﬁnement instance
Fig. 3.42. An object-oriented Bayesian network model of the driving character-
istics of a car. The input node Car type is associated with the default potential
P(Car type) = (0.5, 0.5).
We can construct additional subclasses of Tire-Grip representing diﬀerent
aspects of the grip of the car. The classes can be organized in a hierarchy
according to their subclass/superclass relationship. In turn we can view this
class hierarchy as a model repository that facilitates a quick top-down model
construction, and for more general settings, we can construct generic reposi-
tories of classes representing common modeling problems.
When we subsequently use the object-oriented Bayesian network model
for answering queries (i.e., doing belief updating), we ﬁrst observe that an
object-oriented Bayesian network can be seen as a standard Bayesian network
with some extra features for simplifying the model speciﬁcation. This also
implies that inference in an OOBN can be performed by ﬁrst transforming
the model into a standard Bayesian network, and then applying any inference

3.3 Modeling Methods
91
algorithm on the produced network (see Chapter 4). Transforming an OOBN
into a BN is basically a matter of recursively merging each input node with its
parent in the surrounding model. Methods have also been developed whereby
you keep the OOBN structure and respect the privacy of the encapsulated
attributes. The inference method transmits probability distributions only over
the interface nodes between the objects.
3.3.7 Dynamic Bayesian Networks
When working with domains that evolve over time, you can introduce a dis-
crete time stamp and have a model for each unit of time. We call such a
local model a time slice. Consider, for example, the model for infected milk
in Figure 3.43.
Inf1
Cor1
Test1
Inf2
Cor2
Test2
Inf3
Cor3
Test3
Inf4
Cor4
Test4
Inf5
Cor5
Test5
Inf6
Cor6
Test6
Inf7
Test7
Fig. 3.43. A seven-day model with a two-day memory for infection as well as
correctness of test.
For each time slice i, you have three variables Infi, Testi, and Cori. The
three variables are connected in a time slice, as shown in Figure 3.44.
Cori
Infi
Testi
Fig. 3.44. A time slice for infected milk.

92
3 Building Models
The time slices are connected through temporal links to constitute a full
model. If the structures of the time slices are identical, and if the temporal
links are the same, we say that the model is a repetitive temporal model. If
the conditional probabilities are also identical, we call the model a dynamic
Bayesian network model.
The model for transmission of symbols in Section 3.2.4 can be considered a
temporal repetitive model, but it is not a dynamic Bayesian network because
the conditional probabilities are not identical. On the other hand, the seven
day model in Figure 3.2 is a dynamic Bayesian network.
A special category of time-stamped model is that of the hidden Markov
models. They are strictly repetitive models with an extra assumption (the
Markov property): the past has no impact on the future given the present.
The model in Figure 3.2 is an example of a hidden Markov model, but in Fig-
ure 3.43 inﬂuence from Infi−1 may ﬂow to Infi+1 regardless of our knowledge
of time slice i. The latter model can, however, be transformed to a hidden
Markov model by introducing a copy Inf ∗
i of Infi−1 in the ith time slice (see
Figure 3.45).
Cori−1
Infi−1
Testi−1
Cori
Infi
Testi
Cori+1
Infi+1
Testi+1
Inf ∗
i−1
Inf ∗
i
Inf ∗
i+1
Fig. 3.45. The model of Figure 3.43 transformed into a hidden Markov model.
The reason for the term hidden Markov model is that under the surface
(the test results) there is a hidden activity that cannot be observed (the
infections).
A Kalman ﬁlter is a hidden Markov model in which exactly one variable
has relatives outside the time slice. The model in Figure 3.2 is a Kalman ﬁlter.
A Markov chain is a Kalman ﬁlter consisting of exactly one variable in each
time slice. Note that a hidden Markov model can be transformed to a Markov
chain by taking the cross product of all variables in each time slice.
In modeling domains that are evolving over time, there is a distinction be-
tween ﬁnite-horizon and inﬁnite-horizon domains. The infected milk problem
is an inﬁnite-horizon domain, and a typical ﬁnite-horizon domain is a cornﬁeld
from sowing to harvest.

3.3 Modeling Methods
93
Specifying a repetitive temporal model can be eased by introducing a
couple of new features to the speciﬁcation language. Apart from the structure
of a time slice, you must specify the number of time slices and the temporal
links. The number of slices can be written in a special box, and you can
introduce a special kind of arrow to specify temporal links. A number attached
to a temporal link can specify the number of time steps to jump (if no number
is speciﬁed, the link goes from slice i to slice i + 1). In Figure 3.46, we have
used an extended speciﬁcation language for the model in Figure 3.43.
Cori
Infi
Testi
2
7
Fig. 3.46. A compact speciﬁcation of the model in Figure 3.43 (an extension of
Figure 3.44). The ⇒indicates a temporal link. The number “2” attached to one
of them speciﬁes that it jumps two time steps (no number attached means a jump
from slice i to slice i + 1).
Dynamic Bayesian networks are easily modeled through the object-oriented
approach: the output variables are the variables with a child in later time
slices, and the input variables are parents from earlier time slices. In Fig-
ure 3.46 the output variables are Infi and Cori, and the input variables are
Infi−1, Infi−2, and Cori−1.
So from a modeling point of view, it is quite straightforward to work with
time-stamped models. However, they will often yield calculational problems
(see Exercise 3.25 and Chapter 4).
3.3.8 How to Deal with Continuous Variables
Consider the Cold or Angina? example from Section 3.1.2, in which the vari-
able Fever? was given a discrete state space with three states (chosen a bit
arbitrarily). A more natural way of representing fever would be to use a con-
tinuous variable (typically drawn using a double circle as in Figure 3.47(a)).
With a continuous variable we can no longer encode the uncertainty using a
conditional probability table. Instead we will have to specify a density function
for each combination of states for the parent variables for Fever?. A typical

94
3 Building Models
Fever?
Fever?
SoreThroat?
SoreThroat?
SeeSpots?
SeeSpots?
Therm
Cold
Cold
Angina
Angina
(a)
(b)
Fig. 3.47. Figure (a) shows the cold and angina model in which Fever? is repre-
sented by a continuous variable (drawn as a double circle). In Figure (b) the model
is extended with another continuous variable Therm that models the accuracy of
the thermometer.
density function is the normal distribution (or Gaussian distribution), which
is deﬁned by a mean μ and a variance σ2 (see Figure 3.48 for examples):
f(x) =
1
√
2πσ2 exp

−(x −μ)2
2σ2

.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
-10
-5
 0
 5
 10
 15
 20
f(x)
x
f(x) : μ = 1, σ = 0
f(x) : μ = −3, σ = 2
f(x) : μ = 5, σ = 0.5
Fig. 3.48. Example of normal distributions with diﬀerent values for the mean and
the variance.
For the example above, we should therefore specify a μ and a σ2 for each
state combination of the variables Cold and Angina (the resulting function is

3.3 Modeling Methods
95
also called a conditional Gaussian distribution). A possible speciﬁcation could
be as in Table 3.19.
Cold?
no
yes
Angina?
no
(37◦C, 0.25) (37.5◦C, 0.75)
mild
(38◦C, 0.5)
(38.5◦C, 1)
severe (39◦C, 0.75) (39.5◦C, 1.25)
Table 3.19. Means and variances for the Fever? variable.
The model in Figure 3.47(a) can be extended to also represent the accu-
racy of the thermometer. Speciﬁcally, the thermometer that I use is rather
old with an accuracy corresponding to a variance of 0.25. In addition to this
it has a peculiar tendency of showing 1◦C plus 5% more than the actual tem-
perature. This situation is modeled in Figure 3.47(b). The continuous variable
Therm represents the thermometer, and it is assigned a conditional Gaussian
distribution, where the variance is set to 0.25 and the mean is speciﬁed as a
linear function of Fever?:
μTherm = 1.0 + 1.05 · xFever?.
Given this model, we can now answer queries such as P(Cold | Therm =
39.2◦C, SoreThroat? = yes, SeeSpots? = no) and f(Fever | Therm = 39.2◦C,
SoreThroat? = yes, SeeSpots? = no); the latter density is a linear combination
of conditional Gaussian distributions. For example, if we use the probabil-
ities speciﬁed in Section 3.2.5 together with the conditional Gaussian dis-
tributions described above we get P(Cold | Therm = 39.2◦C, SoreThroat? =
yes, SeeSpots? = no) = (0.13(y), 0, 87(n)), and for f(Fever | Therm = 39.2◦C,
SoreThroat? = yes, SeeSpots? = no) we get a mean and a variance of 36.67◦C
and 0.127, respectively. We will not present the methods for calculating pos-
terior probabilities in networks with continuous variables.
Bayesian networks containing both discrete and continuous variables are
also called hybrid Bayesian networkshybrid. Unfortunately, in order to per-
form exact probability updating in these types of networks we need to put
some rather severe constraints on the networks. In general, we require that:
•
Each continuous variable be assigned a (linear) conditional Gaussian dis-
tribution. That is, for each conﬁguration c of the discrete parents, the
variance σ2
c is a constant (independent of the continuous parents) and the
mean μc is a linear function of the continuous parents Y1, . . . , Ym:
μc = ac +
m

i=1
ai
cyi.

96
3 Building Models
•
No discrete variable have continuous parents.
Note that if a continuous variable does not have any parents, then it is assigned
an unconditional normal distribution.
Whether these two constraints can be met is strongly dependent on the
domain being modeled. For example, you may argue that it is inappropriate
to assign a conditional Gaussian distribution to the Fever? variable, since
the distribution is deﬁned over the entire real line and it will therefore also
assign a nonzero probability mass to impossible temperature intervals. On
the other hand, when specifying probabilities you are almost always making
some kinds of approximations, and the question is then whether the speciﬁed
Gaussian distribution is within an acceptable distance from what you deem the
“correct” distribution. If it is not, you have to look for other ways of specifying
the probabilities (an example of this is given below). The second constraint
is more serious, since it puts restrictions on the structure of the domains
that can be modeled. For instance, if we were to extend the model with a
child, Headache? (having states yes and no), of Fever?, then the structural
constraint would be violated.
If it is not possible to meet the two constraints above, then one possibility
would be to approximate by discretizing the continuous variables. Assume that
we have the speciﬁcation in Table 3.19, and we should now specify intervals for
a ﬁnite set of states. For the three states no, low, and high, it would be natural
to use knowledge of fever. In other situations, you would try to determine
intervals such that for each parent conﬁguration most of the probability mass
is concentrated in a few intervals. This may not be possible, and it will often be
a delicate matter to establish a good set of intervals. In the current situation,
we deﬁne low fever to be in the interval (37.5◦C, 38.5◦C). Consequently, no is
(−∞, 37.5◦C) and high is (38.5◦C, ∞). Next, you use Table 3.19 to calculate
the probability mass for each interval. The result is given in Table 3.20.
Cold?
no
yes
Angina?
no
(0.834, 0.165, 0.01)
(0.5, 0.376, 0.124)
mild
(0.24, 0.52, 0.24)
(0.159, 0.341, 0.5)
severe (0.042, 0.24, 0.718) (0.037, 0.149, 0.814)
Table 3.20. The result of sampling Table 3.19 to the intervals for no, low, and high.
3.3.9 Interventions
You may wish to incorporate actions that change the state of some vari-
ables. You may, for example, wish to model the result of cleaning the spark
plugs in the car start problem. If you use the model in Figure 2.16 directly

3.4 Special Features
97
and enter your cleaning of the spark plugs by entering SP = yes, you get
incorrect results. The problem is that you may no longer have a start prob-
lem, and the state of St may be changed due to your action. The problem is
called persistence. You may extend the model in Figure 2.16 with a variable
Clean?, but then you also must introduce new nodes for the variables that
may change state. Because you have a causal model, the nonpersistent nodes
are the descendants of the nodes aﬀected by the intervention (see Fig. 3.49).
The variable Clean? has a special status in the model. It is not meaningful to
give it prior probabilities, and the descendants of the nodes have no meaning
before a decision on Clean? has been taken. Therefore, it is customary to give
this kind of node a rectangular shape.
SP
SP-C
Fu
FM
St
St-C
Clean
Fig. 3.49. A network modeling the eﬀect of cleaning the spark plugs.
The conditional probabilities for new nodes are natural. If Clean? = no,
then SP-C is in the same state as SP, and if Clean? = yes and SP = yes,
then the probability that SP-C = no is the probability that you can clean the
spark plugs properly. For St-C, you still have a start problem unless it was
due to dirty spark plugs and they have been properly cleaned.
3.4 Special Features
A Bayesian network model is primarily used for belief updating. However, you
may request other kinds of information from a model. This section outlines
some types of requests. Chapter 5 gives a more detailed presentation. To
illustrate the features in this section, we use the sore throat example from
Section 3.1.2 (see Figure 3.50). However, we change the potentials slightly:
when I suﬀer from mild angina, I will see yellow spots with probability 0.01,
and it also happens with probability 0.001 that I have severe angina without
a sore throat, provided that I do not have a cold. The rest of the potentials
can be found in Sections 3.2.5 and Section 3.3.8.
We use the evidence e = {Fever? = no, SoreThroat? = no, See Spots? =
yes} (do not ask why I looked down my throat).

98
3 Building Models
Fever?
Sore Throat?
See Spots?
Cold?
Angina?
Fig. 3.50. The sore throat model.
3.4.1 Joint Probability Tables
Because it is not unusual to suﬀer from both cold and angina, it may be of
interest to use the model in Figure 3.50 to calculate the joint probability table
P(Angina?, Cold? | e). This can be done by use of the fundamental rule
P(Angina?, Cold? | e) = P(Angina? | Cold?, e)P(Cold? | e).
Read P(Cold? | e) from the system; then enter and propagate ﬁrst Cold? = yes
and then Cold? = no to get P(Angina? | Cold?, e).
This method is conceptually easy, but if you request the joint table for
many variables, it is computationally very time-consuming. Other methods
are presented in Chapter 5.
3.4.2 Most-Probable Explanation
Instead of requesting the full joint probability table, I may request the most-
probable conﬁguration of Cold? and Angina?. This can be achieved much
faster than by calculating P(Cold?, Angina? | e) and picking the state with
highest probability.
In general, you have a set of instantiated variables and you request the
most-probable conﬁguration of the remaining variables. This is also called the
most-probable explanation, MPE. MPE can be calculated similarly to proba-
bility updating (see Section 2.3.4 and Chapter 4). The only diﬀerence is that
instead of marginalizing by summing out, you take the maximum. The dis-
tributive law for max reads max(ab, ac) = a max(b, c). In the general form, it
says
If A ̸∈dom (φ1) , then max
A φ1φ2 = φ1 max
A φ2.
Most Bayesian network systems have a special feature for calculating MPE.
3.4.3 Data Conﬂict
Although the evidence e yields posterior probabilities for Cold? as well as for
Angina?, it is more likely that I have misinterpreted what I saw in the throat.

3.4 Special Features
99
In other words, in the light of neither fever nor sore throat, it is very likely
that the evidence See Spots? = yes is faulty. It would be nice if the system by
itself could raise a ﬂag indicating that the evidence does not seem coherent.
To investigate coherence of the evidence, a conﬂict measure is deﬁned. The
idea behind the measure is that correct ﬁndings from a coherent case covered
by the model support each other, and therefore we will expect them to be pos-
itively correlated. For example, if e1 and e2 are two pieces of evidence, then we
would expect P(e1 | e2) > P(e1) and therefore P(e1, e2) = P(e1 | e2)P(e2) >
P(e1)P(e2). Let e = {e1, . . . , em} be a set of ﬁndings. Based on the intuition
above, the conﬂict measure on e is deﬁned as
conf(e) = log2
P(e1) · · · P(em)
P(e)
.
The conﬂict measure is easy to calculate because P(e) is communicated
by the system (see Example 3.9) and P(ei) can be read from the model in its
initial state. If conf(e) is positive, the ﬁndings are not positively correlated,
and we can take this as an indication that the evidence is conﬂicting. To be
quite accurate, a high conﬂict measure is an indication that there is discrep-
ancy between model and evidence. This may be due to ﬂawed ﬁndings, it may
be because we are faced with a very rare case, or the situation may not be
covered by the model. This is discussed in more detail in Section 5.5.
3.4.4 Sensitivity Analysis
Sensitivity analysis refers to analyzing how sensitive the conclusions (the prob-
abilities of the hypothesis variables) are to minor changes. The changes may
be variations of the parameters of the model or may be changes of the evi-
dence (SE analysis). In general, sensitivity analysis is rather technical and in
this section we only give some hints. It is treated in more detail in Chapter 5.
Consider the angina example. The conclusion is P(Angina? | e) = (0, 0.98,
0.02). SE analysis consists in answering questions such as, “what are the
crucial ﬁndings?”, “what if one of the ﬁndings was changed or removed?” or
“what set of ﬁndings would be suﬃcient for the conclusion?” If we consider
the conclusion to be that I suﬀer from mild angina, we see that the ﬁnding
See Spots? = yes is not suﬃcient in itself because it indicates severe angina,
nor is any of the other ﬁndings. Instead, See Spots? = yes together with
SoreThroat = no is suﬃcient, and with these two ﬁndings ﬁxed, the conclusion
is insensitive to any ﬁnding on Fever?.
Now consider the parameters t = P(SoreThroat? = no | Angina? =
severe, Cold? = no) and s = P(See Spots)yes | Angina? = mild). The ini-
tial values of t and s are 0.001 and 0.01, respectively. What we might look
for is a functional expression for P(Angina? = mild | e)(t) and P(Angina? =
mild | e)(s). This is called one-way sensitivity analysis. We might also request
two-way sensitivity analysis by establishing P(Angina? = mild | e)(t, s).

100
3 Building Models
It follows from a general theorem that P(e)(t) as well as P(Angina? =
mild, e)(t) are linear expressions (see Section 5.7), and hence P(Angina? =
mild | e)(t) is a quotient of two linear expressions. From the initial propa-
gation, we can acquire P(e)(0.001) and P(Angina? = mild | e)(0.001). By
changing t to 0.002 and propagating, we get P(e)(0.002) and P(Angina? =
mild | e)(0.002). These four values are suﬃcient for determining the four con-
stants in the functional expression for P(Angina? = mild | e)(t).
3.5 Summary
Types of Variables in Building a Bayesian Network Model
Hypothesis variables: Variables with a state that is asked for. They are, how-
ever, either impossible or too costly to observe directly.
Information variables: Variables that can be observed.
Mediating variables: Variables introduced for a special purpose. It may be
to properly reﬂect the independence properties in the domain, to facili-
tate the acquisition of conditional probabilities, to reduce the number of
distributions to acquire for the network, or for other purposes.
Warning: It is tempting to introduce mediating variables in order to have a
more reﬁned model of the domain; however, if they do not serve any other
purpose you should get rid of them. They jeopardize performance.
Acquiring Conditional Probabilities
Theoretically well founded probabilities as well as frequencies and purely sub-
jective estimates can be used in the same network.
If the number of distributions is too large for a reasonable estimation, a
simplifying assumption can reduce it.
Noisy-or: Let B have the parents A1, . . . , An (all variables binary). Suppose
that Ai = y causes B = y unless it is inhibited by an inhibitor Qi that is
active with probability qi. Assume that the inhibitors are independent. Then,
P(B = n | a1, . . . , an) =
	
j∈Y
qj,
where Y is the set of indices for the states y.
Divorcing: Let B have the parents A1, . . . , An. Assume that the set of conﬁg-
urations of (A1, . . . , Ai) can be partitioned into the sets c1, . . . , cm such that
whenever two conﬁgurations a∗
1 and a∗
2 of (A1, . . . , Ai) are elements in the
same cj, then
P(B | a∗
1, Ai+1, . . . , An) = P(B | a∗
2, Ai+1, . . . , An).

3.6 Bibliographical Notes
101
Then, A1, . . . , Ai can be divorced from Ai+1, . . . , An by introducing a medi-
ating variable C with states c1, . . . , cm, making C a child of A1, . . . , Ai and a
parent of B.
Other Tricks
Undirected relations – in particular, logical constraints – can be modeled by
introducing a dummy child of the constrained variables and letting its poten-
tial reﬂect the relation.
For a speciﬁcation language for repeating structures, see Figure 3.51.
Cori
Infi
Testi
2
7
Fig. 3.51. A compact speciﬁcation of a repeating structure with 7 slices. The ⇒
indicates a temporal link. The number “2” attached to one of them speciﬁes that it
jumps two time steps (no number attached means a jump from slice i to slice i + 1).
Expert disagreements on potentials can be represented in a model by intro-
ducing a node representing the experts.
Continuous variables can be represented in the model if:
•
they do not have any discrete children, and
•
they are assigned a linear conditional Gaussian distribution.
If these two conditions cannot be met, an alternative is to transform them
into variables with a ﬁnite number of states.
3.6 Bibliographical Notes
Naive Bayes was used by de Dombal et al. (1972) and can be traced back
at least to Minsky (1963). Noisy-or was ﬁrst described by Pearl (1986); di-
vorcing was used in MUNIN (Andreassen et al., 1989). Exercise 3.27 is based

102
3 Building Models
on (Cooper, 1990). Chain graphs are treated in depth in (Lauritzen, 1996).
Dynamic Bayesian networks are described in (Kjærulﬀ, 1992). The compact
representation of repetitive structures was suggested by Bangsø and Wuillemin
(2000). Andreassen (1992) discusses various ways of transforming conditional
Gaussian variables into ﬁnite variables. A method not described in this chap-
ter is similarity networks (Heckerman, 1990). The method helps in elicit-
ing the conditional probabilities. Other elicitation methods can be found in
(Druzdzel and van der Gaag, 1995). Object oriented Bayesian networks were
introduced in (Koller and Pfeﬀer, 1997); the version presented here is the one
from (Bangsø and Wuillemin, 2000). References for the special features in
Section 3.4 are given in Section 5.9.
3.7 Exercises
Exercise 3.1. Peter is currently taking three courses on the topics of proba-
bility theory, linguistics, and algorithmics. At the end of the term he has to
take an exam in two of the courses, but he has yet to be told which ones. Pre-
viously he has passed a mathematics and an English course, with good grades
in the mathematics course and outstanding grades in the English course. At
the moment, the workload from all three courses combined is getting too big,
so Peter is considering dropping one of the courses, but he is unsure how this
will aﬀect his chances of getting good grades in the remaining ones. What are
reasonable variables of interest in assessing Peter’s situation? How do they
group into information, hypothesis, and mediating variables?
Exercise 3.2. Assume that three mornings in a row I wonder whether my
sore throat is due to cold or angina. Construct a model.
Exercise 3.3. Construct a model extending the model in Section 3.1.3 with
a scanning test.
Exercise 3.4. Consider the following variables relating to a single household
consisting of a couple and possibly some children:
•
Illness at the moment, with states severe illness, minor illness, and no ill-
ness.
•
History of illness, with states cases of severe illness, often minor illness-
es, and rarely minor illness.
•
Number of children, with states none, one, two, three, and four and up,
•
Working parents, with states both, father, mother, and none.
•
Religion, with states Christianity, Judaism, Islam, Buddhism, Atheism,
and other.
•
Household income, with states $0–$50000, $50000–$100000, and $100000–
and up.
•
Fish-eating habits, with states often ﬁsh and rarely ﬁsh.

Exercises
103
•
Fiber-eating habits, with states lots of ﬁber and not much ﬁber.
•
Drinking habits, with states never alcohol, wine once in a while, often wi-
ne, and wine every day.
Try to construct a Bayesian network incorporating the above variables ac-
cording to your perception of the world. What are the d-separation properties
of the network you constructed?
Exercise 3.5. E
Construct a model for a single milk test (Section 3.2.1).
What is the probability of infected milk given a positive test result?
Exercise 3.6. E
Ground meat purchased in the supermarket may be in-
fected. On average, it happens once out of 600 times. A test with results
positive and negative can be used. If the meat is clean, the test result will be
negative in 499 out of 500 cases, and if the meat is infected, the test result will
be positive in 499 out of 500 cases.
Construct a Bayesian network and use a software system to calculate the
probability of infected for meat with a positive test result.
Exercise 3.7. E
Complete the Bayesian network for Cold or angina? and
perform a self-diagnosis.
Exercise 3.8. E Consider the insemination example from Section 3.1.3. Let
the probabilities be as in Table 3.21 (Ho = y means that hormonal changes
have taken place) P(Pr) = (0.87, 0.13).
Pr = y Pr = n
Ho = y
0.9
0.01
Ho = n
0.1
0.99
Ho = y Ho = n
BT = y
0.7
0.1
BT = n
0.3
0.9
Ho = y Ho = n
UT = y
0.8
0.1
UT = n
0.2
0.9
Table 3.21. Tables for Exercise 3.8.
(i) What is P(Pr | BT = n, UT = n)?
(ii) Construct a naive Bayes model. Determine the conditional probabilities
for the model using the model above. What is P(Pr | BT = n, UT = n) in
this model?
Exercise 3.9. E Use the model from Exercise 3.8 to calculate P(UT =
y, BT = y). Enter the two pieces of evidence into the model and prompt your
system to update probabilities. As a side eﬀect, the system computes P(e),
the probability of the evidence entered. Find out how your system provides
it.

104
3 Building Models
Exercise 3.10. E
(i) Implement the seven-day model in Figure 3.13. Are the initial probabilities
stable over time?
(ii) Consider the conditional probability tables P(Inf2 | Inf1) and P(Inf1) =
(0.0007, 0.9993) and assume that the risk of becoming infected is 0.0002.
We require that the initial probabilities be stable: P(Inf2) = P(Inf1) =
(0.0007, 0.9993). Show that the chance of being cured must be 2/7.
(iii) Consider the conditional probabilities P(Infi+2 | Infi, Infi+1), and assume
that the risk of being infected is the same as above. We require stable
initial probabilities. Show that the chance of being cured for a more than
one day infection must be 0.4.
Exercise 3.11. Show that the procedure described in Section 3.1.5 is equiv-
alent to updating in the model in Figure 3.12.
Exercise 3.12. E
Consider the stud farm example in Section 3.2.2 and let
the prior probability for aA be 0.005.
(i) Enter the model into your Bayesian network system.
(ii) Add to the model the frequency 0.001 for mutation of the gene from A to
a.
(iii) Construct a model for the situation in part (ii), but for a recessive gene
borne by the female sex chromosome. (Note that horses with the disease
are taken out of production.)
Exercise 3.13. E Consider the transmission example from Section 3.2.4.
(i) From Table 3.10, calculate the remaining conditional probabilities for the
model in Figure 3.18.
(ii) Implement the model.
(iii) The sequence baaca is received. What is the most-probable symbol trans-
mitted according to the model in Figure 3.18? What is the most-probable
word?
(iv) What is the most-probable word according to the model in Figure 3.19?
Exercise 3.14. E
Consider the simpliﬁed poker game in Sections 3.1.4
and 3.2.3.
(i) Implement the system.
(ii) Extend the system with a facility giving the chances that your hand is
better than your opponent’s hand.
Exercise 3.15. E Construct a naive Bayes model of the simpliﬁed poker game
example in Sections 3.1.4 and 3.2.3 with OH2 being the class variable. Use your
implemented model from Exercise 3.14 to calculate the needed probabilities

Exercises
105
for the naive Bayes model. What is P(OH2 | FC1 = 1, FC2 = 2) using the
model from Exercise 3.14? What is P(OH2 | FC1 = 1, FC2 = 2) using the
naive Bayes model?
Exercise 3.16. You are confronted with three doors, A, B, and C. Behind
exactly one of the doors there is $10,000. When you have pointed at a door,
an oﬃcial will open another door with nothing behind it. After he has done
so, you are allowed to alter your choice. Should you do that?
Exercise 3.17. Extend the model in Figure 3.23 to incorporate constraints
on color and pattern for the same sock.
Exercise 3.18. The drive in golf is the ﬁrst shot in playing a hole. If you
drive with a 3-wood (a particular type of golf club), there is a 2% risk of a
miss (a bad drive), and 1
4 of the good drives have a length of 180 m, 1
2 are 200
m, and 1
4 have a length of 220 m. You may also use a driver (another type of
golf club). This will on average increase the length by 10%, but you will also
have 3 times as high a risk of a miss. Both wind and the slope of the hole may
aﬀect the result of the drive. Wind doubles the risk of a miss, and the length
is aﬀected by 10% (longer if the wind is from behind and shorter otherwise).
A downhill slope yields 10% longer drives, and an uphill slope decreases the
length of the drive by 10%.
Estimate the probabilities for miss and length given the various factors.
Exercise 3.19. The putt is (usually) the last shot on a golf hole. My ball is
lying 1 m away from the hole, and under normal circumstances I will miss 1
putt out of 10. However, when it rains, I miss 1 out of 7; if it is windy, I miss
1 out of 4; if the green is curved, I miss 1 out of 3; and if I am putting for a
birdie (one under par), I miss 1 out of 2.
Estimate the probabilities for success and failure given the various factors.
Exercise 3.20. Show that the model in Figure 3.26 corresponds to the one
in Figure 3.25.
Exercise 3.21. E
Show that noisy or may be modeled as described in Fig-
ures 3.30 and 3.31. Apply this model to the putting problem of Exercise 3.19,
and compare the number of quantities to specify.
Exercise 3.22.
(i) Complete the model in Section 3.3.4.
P(Ha) = P(Ha | Ot = y) = (0.93, 0.04, 0.02, 0.01),
P(Ha | Fe = y) = P(Ha | Ho = y) = P(Ha | Fb = y) = (0.1, 0.8, 0.1, 0),
P(Ha | Bt = y) = (0.3, 0.2, 0.2, 0.3).

106
3 Building Models
As \ Ha1
no
mild
moderate
severe
y
(1, 0, 0, 0) (0.7, 0.3, 0, 0) (0.1, 0.7, 0.2, 0) (0, 0.1, 0.7, 0.2)
n
(1, 0, 0, 0)
(0, 1, 0, 0)
(0, 0, 1, 0)
(0, 0, 0, 1)
Table 3.22. P(Ha | Ha1, As) for Exercise 3.22.
(ii) Include aspirin in the basis of Table 3.22.
Exercise 3.23. Specify the model in Figure 3.4 as an OOBN.
Exercise 3.24. Construct an OOBN model for the stud farm in Section 3.2.2.
Use default potentials for horses with parents outside the model.
Exercise 3.25. E Consider the model in Figure 3.52. All variables have ten
states.
A
B
C
D
F
G
E
n
Fig. 3.52. A compact representation of a dynamic Bayesian network for Exer-
cise 3.25.
(i) Implement one time slice (with any set of potentials).
(ii) Implement three time slices.
(iii) How many time slices can you implement before your system reports that
it requires extra memory?
Exercise 3.26. E Consider a soccer tournament with 8 teams. Teams 1 to
4 are poor ones, and Teams 5 to 8 are good ones. Each match is between
two teams drawn at random from those that have played the same number of
matches previously in the tournament. The loser of each match is eliminated
from the tournament. The probability of a good team winning a match against
another team is 0.8 if the other team is a poor one, and 0.5 if the other team is
a good one. The probability of a poor team winning a match against another

Exercises
107
poor team is 0.5. What is the probability of a poor team making it to the ﬁnal?
(Hint: For each match, generate a variable that represents the winner (with
states poor team and good team), and variables that represent each contestant
in the opening matches (with states poor team and good team). Finally, use
constraint nodes to ensure compliance with the restrictions in the exercise.)
Exercise 3.27. E The following relations hold for the Boolean variables
A, B, C, D, E, and F:
(A ∨¬B ∨C) ∧(B ∨C ∨¬D) ∧(¬C ∨E ∨¬F) ∧(¬A ∨D ∨F) ∧
(A ∨B ∨¬C) ∧(¬B ∨¬C ∨D) ∧(C ∨¬E ∨¬F) ∧(A ∨¬D ∨F).
(i) Is there a truth value assignment to the variables making the expression
true? (Hint: Represent the expression as a Bayesian network.)
(ii) We receive the evidence that A is false and B is true. Is there a truth
value assignment to the other variables making the expression true?
(iii) The satisﬁability problem for propositional calculus is, given a Boolean
expression E (over n Boolean variables), is there a truth-value assignment
to the variables that makes E true?
Show that a method for calculation of probabilities in Bayesian networks
yields a method for solving the satisﬁability problem for propositional
calculus. (Hint: Assume that E is in conjunctive normal form.)
(iv) Show that probability calculation in Bayesian networks is NP-hard.
Exercise 3.28. You have the model A →B and P(A) = (0.7, 0.3). Two
experts give the tables in Table 3.23, and you have no reason to believe more
in one expert than in the other.
You receive the evidence A = y. What are the posterior probabilities for
B and the experts?
B \ A y
n
y
0.9 0.4
n
0.1 0.6
B \ A y
n
y
0.6 0.4
n
0.4 0.6
P1(B | A)
P2(B | A)
Table 3.23. Table for Exercise 3.28.

108
3 Building Models
Exercise 3.29. E
(i) Take your model from Exercise 3.7 and enter the evidence e = {Fever? =
no, Sore Throat? = no, See Spots? = yes}. How does your system react?
Change the potentials such that P(Sore Throat? = no | Angina? = severe,
Cold? = no) = 0.001, and P(See Spots? | Angina? = mild) = 0.01.
(ii) Calculate P(Cold?, Angina? | e).
(iii) Calculate MPE(e).
(iv) Calculate conf(e).
(v) Determine P(Angina? = mild | e)(s), where s = P(See Spots? = yes |
Angina? = mild).

4
Belief Updating in Bayesian Networks
In this chapter, we present algorithms for probability updating. An eﬃcient
updating algorithm is fundamental to the applicability of Bayesian networks.
As shown in Chapter 2, access to P(U, e) is suﬃcient for the calculations.
However, because the joint probability table increases exponentially with the
number of variables, we look for more-eﬃcient methods. Unfortunately, no
method guarantees a tractable calculational task. However, the method pre-
sented here represents a substantial improvement, and it is among the most-
eﬃcient methods known.
We shall use the framework of potentials. A conditional probability table
P(A | pa(A)) is a function φ : pa(A)∪{A} →[0 : 1], and we call it a potential.
For the algebra of probability tables we shall for notational convenience use
functional notation. That is, the product P(A | pa(A)) · P(B | pa(B)) is con-
sidered as a product of two functions φ1(A, pa(A))φ2(B, pa(B)). The reader
is expected to be familiar with Section 1.4.
Sections 4.1–4.6 present the junction tree algorithm, a version of the vari-
able elimination method. Section 4.7 presents an alternative method with
any-space properties, recursive conditioning, and in Sections 4.8 and 4.9 we
outline diﬀerent approximation methods.
4.1 Introductory Examples
To repeat the fundamentals from Chapter 2 and for pinpointing the issues in
belief updating for Bayesian networks, we consider in this section a simple
example. Consider the Bayesian network in Figure 4.1 over the universe U.
The potentials speciﬁed for BN are φ1 = P(A1), φ2 = P(A2 | A1), φ3 =
P(A3 | A1), φ4 = P(A4 | A2), φ5 = P(A5 | A2, A3), and φ6 = P(A6 | A3).

110
4 Belief Updating in Bayesian Networks
A1
A2
A3
A4
A5
A6
Fig. 4.1. A simple Bayesian network, BN.
4.1.1 A Single Marginal
Let us ﬁrst assume that we wish to calculate P(A4). From the chain rule, we
have
P(U) = φ1φ2φ3φ4φ5φ6 and P(A4) =

A1,A2,A3,A5,A6
φ1φ2φ3φ4φ5φ6.
To avoid calculating P(U), we use the distributive law (Section 1.4):
P(A4) =

A1
φ1(A1)

A2
φ2(A2, A1)φ4(A4, A2)

A3
φ3(A3, A1)

A5
φ5(A5, A2, A3)

A6
φ6(A6, A3).
First, calculate φ′
6(A3) = 
A6 φ6(A6, A3), then multiply φ′
6(A3) by
φ5(A5, A2, A3) and calculate φ′
5(A2, A3) = 
A5 φ5 (A5, A2, A3) φ′
6(A3); φ′
5(A2,
A3) is multiplied by φ3(A3, A1), and so forth. Notice that in the calculation
of φ′
5(A2, A3) you can apply the distributive law again; that is, you need not
multiply by φ′
6(A3) before you marginalize A3 out. The calculation is sketched
graphically in Figure 4.2.
The reason for using the distributive law is to reduce the size of the tables
to handle. The full joint, P(U), requires a space incorporating all six vari-
ables. For the process illustrated in Figure 4.2, the largest potential to handle
contains three variables. In Figure 4.3, the structure is repeated, but in each
bucket (drawn as an ellipse) we have indicated the variables to handle, and
the variables in a mailbox (drawn as a rectangle) indicate the domain of the
potential communicated.
In the preceding calculations, we performed the marginalizations in a par-
ticular order, namely A6, A5, A3, A2, A1, and this is reﬂected in the structure
of Figure 4.2. Because marginalization is commutative (Section 1.4), it can
be done in any order. It is standard to use the term elimination order rather
than marginalization order. If we use the reversed elimination order, we get
the structure in Figure 4.4.

4.1 Introductory Examples
111
φ5(A5, A2, A3)
φ′
6(A3)
P
A3
φ2(A2, A1)
φ4(A4, A2)
P
A2
φ′
2(A1, A4)
P
A1
P (A4) = P
A1 φ1 · φ′
2
φ6(A6, A3)
P
A6
φ′
5(A2, A3)
φ′
3(A1, A2)
φ1(A1)
φ3(A3, A1)
P
A5
Fig. 4.2. An illustration of the process of marginalizing down to A4. The elliptic
nodes are buckets containing potentials. In a bucket, the potentials are multiplied
by the incoming potentials, a variable is marginalized out, and the result is placed
in a mailbox (a rectangular node) for a neighboring bucket.
Figure 4.5 illustrates the domains to handle for the last elimination order.
As can be seen, the domains for the ﬁrst order are smaller than the domains
for the last order.
Because the size of the domains to handle is a good measure of complexity,
we will address the task of ﬁnding an elimination order yielding the smallest
domains to handle.
4.1.2 Diﬀerent Evidence Scenarios
In the preceding calculations, we assumed that no evidence was entered into
the network. By analyzing the process illustrated in Figure 4.2, we realize
some simpliﬁcations. Because φ5 = P(A5 | A2, A3) and φ6 = P(A6 | A3), we
have that φ′
5 = 
A5 P(A5 | A2, A3) = 1 and φ′
6 = 
A6 P(A6 | A3) = 1, where
1 is the unit potential. Also,
φ′
3 =

A3
φ3φ′
5φ′
6 =

A3
φ31 · 1 =

A3
φ3 =

A3
P(A3 | A1) = 1.

112
4 Belief Updating in Bayesian Networks
φ6
A3
A2, A3
A5, A2, A3
φ3
A1, A2, A3
A1, A2, A4
A1, A4
A1, A4
A6, A3
φ5
A1, A2
φ2, φ4
φ1
Fig. 4.3. A structure indicating the domains of the various potentials to handle.
We note that φ′
3 is void, and the entire process is reduced to calculating

A1 P(A1) 
A2 P(A2 | A1)P(A4 | A2).
The nodes A3, A5, and A6 are examples of so-called barren nodes.1 A node
A is barren if neither A nor any of A’s descendants have received evidence.
The conditional probability potential attached to a barren node has an impact
only on descendant nodes.
If we have the evidence A5 = a5 and A6 = a6, the evidence is represented
as two 0-1 ﬁndings, e5 and e6 (Section 2.3.3). The formula is
P(A4, e) =

A1,A2,A3,A5,A6
φ1φ2φ3φ4φ5φ6e5e6,
and we have (Section 2.3.3)
P(A4 | e) =
P(A4, e)

A4 P(A4, e).
To calculate P(A4, e), the eﬀect on the frame in Figure 4.3 is that the two
evidence potentials are added in the buckets with φ5 and φ6 attached to them
(see Figure 4.6).
1 This term was ﬁrst used in connection with inﬂuence diagrams (Section 9.4),
where barren nodes have no inﬂuence on the decisions.

4.1 Introductory Examples
113
P
A6
P
A5
P
A3
φ6(A6, A3)
φ5(A5, A2, A3)
φ2(A2, A1)
φ3(A3, A1)
φ1(A1)
P
A1
φ′
1(A2, A3)
φ4(A4, A2)
P
A2
φ′
2(A3, A4, A5)
P (A4)
Fig. 4.4. The structure resulting from eliminating in an order that is the reverse of
that from Figure 4.2.
φ4, φ5
A2, A3
A1, A2, A3
φ1, φ2, φ3
A3, A4, A5
A2, A3, A4, A5
φ6
A3, A4, A5, A6
Fig. 4.5. The domains for the elimination order A1, A2, A3, A5, A6.

114
4 Belief Updating in Bayesian Networks
φ5, e5
P (A4, e)
P
A6
φ′
6
φ3
P
A3
φ′
5
P
A5
φ′
3
φ2, φ4
φ′
2
P
A1
φ1
P
A2
φ6, e6
Fig. 4.6. The frame from Figure 4.2 incorporating the evidence e5 : A5 = a5 and
e6 : A6 = a6.
The eﬀect of e is that the variables A5 and A6 are instantiated in the
potentials φ5 and φ6, and the marginalizations of A5 and A6 are redundant,
that is, φ′
5 = P(A5 = a5 | A2, A3) and φ′
6 = P(A6 = a6 | A3).
The process in Section 4.1.1 is suﬃciently general to encompass all types
of evidence scenarios. The task is to supplement this general process with
methods taking advantage of simpliﬁcations due to the particular evidence
scenario, such as identiﬁcation of barren nodes.
4.1.3 All Marginals
Assume that we wish to compute P(Ai, e) for all i. Without taking advantage
of the special evidence scenario, we can for each node use the method from
Section 4.1.1. Assume that we calculate P(A2, e) through the elimination order
A6, A5, A3, A1, A2. Then, the frame of potentials looks as in Figure 4.7.
As can be seen, the frame in Figure 4.7 is very similar to the frame in
Figure 4.3. Only one arrow is reversed, and many calculations from the cal-
culation of P(A4, e) can be reused. In this chapter, we present a systematic
way of exploiting reuse in calculating all marginals. The resulting method has
a complexity equivalent to two single-variable marginalizations.

4.2 Graph-Theoretic Representation
115
A6, A3
A3
A2, A3
A5, A2, A3
A1, A2, A3
A1, A2, A4
A1, A4
φ6
φ5
φ3
A1, A2
φ2, φ4
A1, A4
φ1
Fig.
4.7.
A frame
for
computing P(A2, e) through the elimination
order
A6, A5, A3, A1, A2.
4.2 Graph-Theoretic Representation
As illustrated in Section 4.1, belief updating for Bayesian networks consists
basically in calculating sums of products. In this section, we deal system-
atically with this task without explicit reference to Bayesian networks. The
methods presented are general and can be applied to a large variety of tasks.
4.2.1 Task and Notation
We will work with a set of real-valued potentials Φ = {φ1, . . . , φm} over ﬁnite
variables from the universe U = {A1, . . . , An}.
Let Ψ be any set of potentials. The product of all potentials ψ in Ψ is de-
noted by 
 ψ. We will also use the notation 
k
i=1 ψi for the product ψ1 · · · ψk,
and if the boundaries are apparent from the context, we write 
 ψi.
The potential 
X φ(X, Y, . . . , Z) is the sum φ(x1, Y, . . . , Z)+· · ·+φ(xk, Y,
. . . , Z), and it is deﬁned over (Y, . . . , Z). We say that X has been marginalized
out of φ(X, Y, . . . , Z). If V is a set of variables, then 
V is a notation for
marginalizing out all variables in V. Because marginalization is commutative
(Section 1.4), this notation is unambiguous.
Instead of sum notation, we may also use projection notation. We let
φ↓X(X, Y, . . . , Z) denote the potential resulting from marginalizing out (Y, . . . ,
Z); the potential is projected down to X. If W is a set of variables, then φ↓W

116
4 Belief Updating in Bayesian Networks
denotes the result of marginalizing out all variables except the members of W.
Task: Compute (
 Φ)↓Ai for all Ai.
Deﬁnition 4.1. Let Φ be a set of potentials, and let X be a variable. Then X
is eliminated from Φ through the following procedure:
1. Remove all potentials in Φ with X in their domains. Call the set of re-
moved potentials ΦX.
2. Calculate φ−X = 
X

 ΦX.
3. Add φ−X to Φ. Call the result Φ−X; Φ−X = {Φ \ ΦX, φ−X}.
Note that elimination of the variable X corresponds to using the distribu-
tive law on the product. Instead of calculating the product, we keep the factors
in a bucket and do not multiply before we are forced to do so.
Proposition 4.1. The task (
 Φ)↓X is solved by repeatedly eliminating the
variables except for X.
It remains to establish an elimination order.
4.2.2 Domain Graphs
To get an overview of the consequences of various elimination orders, the task
is represented graphically.
Deﬁnition 4.2. Let Φ = {φ1, . . . , φm} be potentials over U = {A1, . . . , An}
with dom (φi) = Di. The domain graph for Φ is the undirected graph with the
variables of U as nodes and with a link between each pair of variables that are
members of the same Di.
For the sake of exposition, we assume throughout the chapter that the
graphs considered are connected.
Example 4.1. In Section 4.1.1, we dealt with a Bayesian network over the
potentials Φ = {φ1(A1), φ2(A2, A1), φ4(A4, A2), φ3(A3, A1), φ5(A5, A2, A3),
φ6(A6, A3)}. The domain graph for Φ is given in Figure 4.8.
Compared to the initial Bayesian network in Figure 4.1, we see that direc-
tions on the links have been dropped and that a new link (A2, A3) has been
inserted. It is often called a moral link because it connects two nodes with a
common child. The domain graph for a Bayesian network is called the moral
graph.
When we eliminate a variable X, we work with the product of all poten-
tials with X in the domain. The domain of this product consists of X and
its neighbors in the domain graph, and when X is eliminated, the resulting
potential has all X’s neighbors in its domain. This means that in the domain

4.2 Graph-Theoretic Representation
117
A1
A2
A3
A4
A5
A6
Fig. 4.8. The domain graph for Φ = {φ1(A1), φ2(A2, A1), φ3(A3, A1), φ4(A4, A2),
φ5(A5, A2, A3), φ6(A6, A3)}.
A1
A2
A4
A5
A6
Fig. 4.9. The domain graph for Φ−A3 from Figure 4.8.
graph for Φ−X all neighbors of X are pairwise linked. In Figure 4.9, we show
the domain graph for the example in Figure 4.8 with A3 eliminated.
Note that the graph in Figure 4.9 has several new links. These new links
are called ﬁll-ins. The introduction of ﬁll-ins highlights the fact that when
eliminating A3 you work with a potential over a domain that was not present
initially. In order to avoid working with new domains, you try to avoid ﬁll-ins.
To put it another way, an elimination sequence that does not introduce ﬁll-ins
requires less space than an elimination sequence that introduces ﬁll-ins.
In Section 4.1, we considered calculation of P(A4). In the graph-theoretic
framework, it corresponds to constructing an elimination sequence ending with
A4. For the domain graph in Figure 4.8, it is possible to eliminate down to A4
without introducing ﬁll-ins: A6, A5, A3, A1, A2, A4. Such a sequence is called a
perfect elimination sequence. There are several perfect elimination sequences
ending with A4, and an optimal elimination sequence will be found among
them. In Figure 4.8, we see that the sequence A5, A6, A3, A1, A2, A4 as well
as A1, A5, A6, A3, A2, A4 and A6, A1, A3, A5, A2, A4 are perfect elimination se-
quences.

118
4 Belief Updating in Bayesian Networks
Proposition 4.2. Let X1, . . . , Xk be a perfect elimination sequence, and let
Xj be a node with a complete neighbor set.1 Then, the sequence Xj, X1, . . . ,
Xj−1, Xj+1, . . . , Xk is also a perfect elimination sequence.
Proof.
If you start by eliminating Xj, you do not introduce ﬁll-ins. Consider
variable Xi. When you eliminate Xi, you look at the uneliminated neighbors,
and if a pair of them is not linked, you introduce a ﬁll-in. Eliminating Xj
before Xi does not give Xi new neighbors, and it will not enforce new ﬁll-ins
when Xi is eliminated.
⊓⊔
The complexity of using a particular elimination sequence is character-
ized by the set of domains for the potentials used. For the elimination or-
der A6, A5, A3, A1, A2, A4, the set of domains is {{A6, A3}, {A2, A3, A5},
{A1, A2, A3}, {A1, A2}, {A2, A4}}. If a domain is a subset of another do-
main, then it does not require extra space and we need not consider it. For
example, the set {A1, A2} is removed from the preceding domain set.
Deﬁnition 4.3. The domain set of an elimination sequence is the set of do-
mains of potentials produced during the elimination in which potentials that
are subsets of other potentials are removed.
Unfortunately, it does not hold that if you eliminate without introducing
ﬁll-ins, then the domain set consists only of domains from the initial set of
potentials. For the preceding perfect elimination sequence, we have that when
A3 is eliminated, you work with a potential with domain {A1, A2, A3}, which
is not one of the initial domains. However, there is no way to avoid this.
No matter which of the three variables you eliminate ﬁrst, you will produce a
potential with all three variables in the domain. In general, it holds that if the
set V of variables is a complete set in the domain graph, then any elimination
sequence will contain a potential with a domain including V.
Proposition 4.3. All perfect elimination sequences produce the same domain
set, namely the set of cliques of the domain graph; a complete set is a clique
if it is not a subset of another complete set (a maximal complete set).
Proof.
First we show that a clique V corresponds to the domain of a potential
produced during the elimination. Let X be the ﬁrst variable from V to be
eliminated. When X is eliminated, we produce a domain D consisting of X
and all its neighbors. Because all elements of V are neighbors of X, D must
contain V . Let Y be a member of D. After elimination of X, there is a link
between Y and all members of V . The elimination does not produce ﬁll-ins,
so the links must have been present initially, and because V is a maximal
complete set, Y must be a member of V . Hence, the cliques must be members
of the domain set.
1 A set of nodes is complete if all nodes are pairwise linked.

4.3 Triangulated Graphs and Join Trees
119
Finally we show that each member W of the domain set is a clique. Be-
cause the elimination does not produce ﬁll-ins, W must be a complete set in
the domain graph. If W is not maximal, it is a subset of a clique V , and V is
a member of the domain set, so W cannot be a member.
⊓⊔
From Proposition 4.3, we can conclude that any perfect elimination se-
quence ending with the variable A is optimal with respect to calculating
P(A). The full task is to compute the marginals down to each variable, so
the task can be solved by establishing an optimal elimination sequence for
each variable.
4.3 Triangulated Graphs and Join Trees
Before continuing with the belief-updating task, we deal in detail with some
purely graph-theoretic concepts. They will be used for the belief updating
task in the next section.
Deﬁnition 4.4. An undirected graph with a perfect elimination sequence is
called a triangulated graph.
Note that the term “triangulated” may be misleading. The graph (b) in
Figure 4.10 is not triangulated.
A1
A2
A3
A4
A5
(a)
A1
A2
A3
A4
A5
(b)
Fig. 4.10. (a) A triangulated graph; (b) a nontriangulated graph.
Notation: Let X be a node in an undirected graph. The set of neighbors of
X we denote by nb(X), and the set of neighbors plus X we denote by fa(X),
the family of X. If the nodes of the graph are enumerated, we use the index
to write Ni rather than NXi. Nodes with a complete neighbor set are called
simplicial nodes. A neighbor to a node X is said to be adjacent to X. Note
that X is simplicial if and only if fa(X) is a clique.
Proposition 4.4. Let G be a triangulated graph, and let X be a simplicial
node. Let G′ be the graph resulting from eliminating X from G (see Fig-
ure 4.11). Then G′ is a triangulated graph.

120
4 Belief Updating in Bayesian Networks
X
A
D
B
C
E
A
D
B
C
E
G
G′
Fig. 4.11. If fa(X) is a complete set, you eliminate X from G by simply removing
X together with its links.
Proof.
Follows from Proposition 4.2.
⊓⊔
Note that a triangulated graph always has at least one simplicial node,
namely the ﬁrst one in the elimination sequence. Actually, there are at least
two.
Theorem 4.1. A triangulated graph with at least two nodes has at least two
simplicial nodes.
Proof.
We prove by induction a slightly stronger statement: let G be an
incomplete triangulated graph with at least three nodes. Then, it has at least
two nonadjacent simplicial nodes.
Certainly, any incomplete triangulated graph with three nodes has two
nonadjacent simplicial nodes (see Figure 4.12).
A
B
C
Fig. 4.12. A connected incomplete triangulated graph with three nodes.
Assume the statement to be true for all graphs with fewer than n nodes,
and let G be an incomplete triangulated graph with n nodes. The ﬁrst node,
X, in the elimination sequence is simplicial, and we must ﬁnd another one
not adjacent to X. Let G′ be the graph resulting from removing X from G.
The graph G′ is triangulated, and any simplicial node in G′ is either sim-
plicial in G or a member of nb(X).
Because G is not complete, it must contain nodes that are not members
of nb(X). If G′ is complete, any of these nodes can do. If G′ is not complete,
we know from the induction hypothesis that it has at least two nonadjacent
simplicial nodes. If both were neighbors of X, they would be adjacent.
⊓⊔

4.3 Triangulated Graphs and Join Trees
121
Corollary 4.1. In a triangulated graph, each variable A has a perfect elimi-
nation sequence ending with A.
Proof.
Let A be any node in the triangulated graph G. Eliminate a simplicial
node X(X ̸= A); Theorem 4.1 ensures that such a node exists. Proposition 4.4
yields that the resulting graph is triangulated, and you can repeatedly apply
Theorem 4.1 until only A is left.
⊓⊔
From Corollary 4.1, we see that if you have established one perfect elimi-
nation sequence, then you can easily establish a perfect elimination sequence
down to any variable. In other words, you can for each variable A establish
an optimal sequence of marginalizations for calculating P(A). We give the
details in Section 4.4.
Unfortunately, it does not hold that all domain graphs are triangulated.
The following theorem gives an easy way of checking whether a graph is tri-
angulated, and if it is, it also gives a simple way of establishing an elimination
sequence.
Theorem 4.2. An undirected graph is triangulated if and only if all nodes
can be eliminated by successively eliminating a simplicial node X.
Proof.
If all nodes can be eliminated by successively eliminating simpli-
cial nodes, then we produce a perfect elimination sequence, and the graph is
triangulated.
Now assume that the undirected graph is triangulated. Let us eliminate
any simplicial node. Proposition 4.4 yields that the resulting graph is trian-
gulated, and we can continue the procedure.
⊓⊔
To check whether a graph is triangulated, you repeatedly eliminate sim-
plicial nodes. At some stage, you run into a situation in which you cannot
eliminate more nodes. If the node set is empty, then the graph is triangu-
lated; if not, then the graph is not triangulated.
In general, it is NP-hard to determine the set of cliques in a graph. For
triangulated graphs, Proposition 4.3 and Theorem 4.2 yield an easy procedure.
Algorithm 4.1 To determine the set of cliques in a triangulated graph, you
can do as follows
1. Eliminate a simplicial node X; fa(X) is a clique candidate.
2. If fa(X) does not include all remaining nodes, go to 1.
3. Prune the set of clique candidates by removing sets that are subsets of
other clique candidates.
⊓⊔

122
4 Belief Updating in Bayesian Networks
4.3.1 Join Trees
Deﬁnition 4.5. Let G be the set of cliques from an undirected graph, and let
the cliques of G be organized in a tree T . Then T is a join tree if for any pair
of nodes V , W all nodes on the path between V and W contain the intersection
V ∩W.
BCDE
ABCD
BCDG
CHGJ
BCDE
BCDG
CHGJ
DEF I
ABCD
DEF I
(a)
(b)
Fig. 4.13. (a) A join tree; (b) not a join tree.
Theorem 4.3. If the cliques of an undirected graph G can be organized into
a join tree, then G is triangulated.
Proof.
Let the cliques be organized in a join tree, and let V be a leaf clique
with unique neighbor clique W. Any member of V that is a member of an-
other clique must – due to the join tree condition – also be a member of W.
Therefore, V must contain at least one variable X not contained in any other
clique (otherwise V would be a subset of W). Then fa(X) must be complete,
and X can be eliminated without creating ﬁll-ins. We can repeat eliminating
variables that are only members of V , and when all these have been elimi-
nated, we have a graph G′ with the same cliques as G except for V . Then,
the join tree for G with the node V removed is a join tree for G′, and we can
continue by eliminating a variable from a leaf in G′.
⊓⊔
Theorem 4.4. If the undirected graph G is triangulated, then the cliques of
G can be organized into a join tree.
The proof is a construction of a join tree from a triangulated graph. To
illustrate the construction, we use the graph in Figure 4.14.
Construction: Establish an elimination sequence in the following way. Start
with a simplicial node X. Then fa(X) is a clique. Continue eliminating nodes

4.3 Triangulated Graphs and Join Trees
123
A
B
E
F
D
C
H
G
J
I
Fig. 4.14. A triangulated graph.
from fa(X) that have neighbors only in fa(X). Give fa(X) an index i according
to the number of nodes eliminated, and denote the set of the remaining nodes
by Si. This set is called a separator. Choose a new clique in the graph G′ with
the eliminated nodes removed, and repeat the process with the index starting
at i. Continue to do so until all cliques have been eliminated. Figure 4.15
shows the result of this process on the graph in Figure 4.14.
BCDE
DEF I
DE
CG
BCD
S1
V3
V1
S3
V5
S5
V6
BCD
S6
V10
CGHJ
ABCD
BCDG
Fig. 4.15. The cliques, separators, and indices resulting from the graph in Fig-
ure 4.14. The elimination sequence used is A, F, I, H, J, G, B, C, D, E.
When the parts have been established as indicated in Figure 4.15, each
separator Si is connected to a clique Vj (j > i) such that Si ⊂Vj (see
Figure 4.16). This is always possible because Si is a complete set, and when
the ﬁrst node from Si is eliminated, it must be when dealing with a clique of
higher index than i, and it must contain all of Si. For convenience, we talk of
the direction from Vi over Si to Vj as upward, and we call Vj a parent of Vi.
We must prove that the structure constructed is a tree and has the join
tree property. Each clique has at most one parent, so there cannot be multiple
paths, and the structure is a tree.

124
4 Belief Updating in Bayesian Networks
ABCD
DEF I
DE
CG
BCD
S1
V3
V1
S3
V5
S5
V6
BCD
S6
V10
BCDG
BCDE
CGHJ
Fig. 4.16. A join tree (expanded with separators) resulting from the construction
applied to the graph in Figure 4.14.
To prove the join tree condition, consider the cliques Vi and Vj (i < j),
and let X be a member of both. There is a unique path between Vi and Vj,
and we will prove that X is a member of all cliques on that path. Because X
is not eliminated when dealing with Vi, it must be a member of Si, and from
the construction, X must be a member of Vi’s parent Vk. If k = j, we are
ﬁnished; otherwise we continue the argument for the smallest of the two.
Remark: The separators are so called because any separator S divides the
graph into two parts, and all paths connecting the two parts must pass through
S. If the join tree is constructed from a Bayesian network, the two parts are
d-separated given S.
A join tree provides the framework for constructing perfect elimination
sequences. Namely, notice that the simplicial nodes are those with all une-
liminated neighbors in one clique, and two nodes are neighbors if they are
members of the same clique. Hence, all perfect elimination sequences can be
constructed from a join tree by repeatedly eliminating simplicial nodes.
4.4 Propagation in Junction Trees
In the literature you often see the terms “join tree” and “junction tree” used
interchangeably. In this book we introduce a distinction.
Deﬁnition 4.6. Let Φ be a set of potentials with a triangulated domain graph,
G. A junction tree for Φ is a join tree for G with the following addition: each
potential φ in Φ is attached to a clique containing dom (φ); each link has the
appropriate separator attached; each separator contains two mailboxes, one for
each direction.
If Φ is a set of conditional probabilities for a Bayesian network BN to-
gether with evidence potentials for the evidence e, we say that the junction

4.4 Propagation in Junction Trees
125
tree represents BN with evidence e.
Notation: The propagation algorithm presented here deals with sets of po-
tentials. A set of potentials is a representation of the product of the member
potentials. Let Φ be a set of potentials whose domains are subsets of V , and
let W be a subset of V . Then, Φ↓W is a set of potentials resulting from suc-
cessively eliminating the variables in V \ W as described in Deﬁnition 4.1.
Because the elimination order is arbitrary, this notation seems to introduce
some ambiguity with respect to the functions in the resulting set. Because we
treat the sets as representations of products, and the product is independent
of the elimination order, we will not deal with this apparent ambiguity.
Example 4.2. Consider the set ψ = {φ1(A), φ2(A, B), φ3(A, C), φ4(C,D),
φ5(C)}, and let W = {B, C}. Then, ψ↓W = {
A φ1(A)φ2(A, B)φ3(A, C),

D φ4(C, D), φ5(C)}.
Before giving a general description of the propagation algorithm, we will
go through an example.
Example 4.3. Consider the Bayesian network in Section 4.1 with potentials
φ1 = P(A1), φ2 = P(A2 | A1), φ3 = P(A3 | A1), φ4 = P(A4 | A2), φ5 =
P(A5 | A2, A3), φ6 = P(A6 | A3) and with the domain graph in Figure 4.8. We
know that the elimination sequence A6, A5, A3, A1, A2, A4 is perfect. The do-
main graph has a join tree over the cliques V1 = {A3, A6}, V2 = {A2, A3, A5},
V4 = {A1, A2, A3}, V6 = {A2, A4} and the separators S1 = {A3}, S2 =
{A2, A3}, S4 = {A2}. The junction tree is shown in Figure 4.17.
V4 : A1, A2, A3
↓
↓
S4 : A2
S2 : A2, A3
S1 : A3
V6 : A2, A4
V2 : A2, A3, A5
V1 : A3, A6
φ1, φ2, φ3
↓
φ4
φ5
φ6
↑
↑
↑
Fig. 4.17. A junction tree for the Bayesian network in Figure 4.8.
To calculate P(A4), we ﬁnd a clique containing A4(V6). It is made a tem-
porary root, and we send messages in the direction of V6 starting from the

126
4 Belief Updating in Bayesian Networks
leaf cliques. The message ψ1 = φ↓A3
6
= φ↓S1
6
is placed in the appropriate S1
mailbox, and the message ψ2 = φ↓{A2,A3}
5
= φ↓S2
5
is placed in the appropri-
ate S2 mailbox. Next, V4 assembles the incoming messages and the poten-
tials held form the set Φ4 = {ψ1, ψ2, φ1, φ2, φ3}. The variables A1 and A3
are eliminated from Φ4, and the result, ψ4 = (φ1φ2(φ3ψ2ψ1)↓{A1,A2})↓A2 =

A1 φ1φ2

A3 φ3ψ2ψ1, is placed in the appropriate mailbox (see Figure 4.18).
V4 : A1, A2, A3
↓
↓
S4 : A2
S2 : A2, A3
S1 : A3
V6 : A2, A4
V2 : A2, A3, A5
V1 : A3, A6
φ1, φ2, φ3
↓
φ4
φ5
φ6
↑
↑
↑
ψ4
ψ2 = φ↓S2
5
ψ1 = φ↓S1
6
Fig. 4.18. The cliques V1 and V2 have sent messages to their separators, and V4
has sent the message P
A1 φ1φ2
P
A3 φ3ψ2ψ1 to S4.
Now V6 can collect its message, multiply it by φ4, and marginalize A2 out
to get P(A4).
The process just described is called collecting evidence to V6. To calculate
the marginal for another variable X, we can collect to a clique containing X.
If, for example, we wish to calculate P(A6), we can collect to V1. We can also
prepare the junction tree for the calculation of all marginals: send messages
in the direction away from V6. This process is called distributing evidence.
First, V6 sends the message ψ4 = φ↓A2
4
to S4, and V4 sends a message to
S2 as well as S1 (see Figure 4.19). When the message for S2 is calculated,
the set {ψ4, φ1, φ2, φ3, ψ1} is assembled, and A1 is marginalized out. Here, we
multiply only the potentials that have A1 in the domain, and the message
becomes a set of potentials: {ψ4, 
A1 φ1φ2φ3, ψ1}.
When both collecting and distributing evidence have been performed, we
have performed a full propagation, and to calculate a marginal P(X) we ﬁnd
a clique V containing X. Take, for example, A3. The clique V1 contains A3.
The incoming message to V1 is the message for collecting evidence to V1, and
therefore it corresponds to a perfect elimination sequence ending with the
nodes A6 and A3. This means that the product φ6ψ1 is the projection of the

4.4 Propagation in Junction Trees
127
V4 : A1, A2, A3
↓
↓
S4 : A2
S2 : A2, A3
S1 : A3
V6 : A2, A4
V2 : A2, A3, A5
V1 : A3, A6
φ1, φ2, φ3
↓
φ4
φ5
φ6
↑
↑
↑
ψ2
ψ2
ψ4
ψ4
ψ1
ψ1
Fig.
4.19.
The
junction
tree
after
a
full
propagation:
ψ2
=
{ψ4, P
A1 φ1φ2φ3, ψ1}, ψ1 = P
A2 ψ2ψ4
P
A1 φ1φ2φ3.
entire product down to {A3, A6}, and we can easily calculate P(A3) as well
as P(A6).
There is a slightly easier way of calculating A3. Consider the separator S1.
It consists of A3 alone. For the product of the two messages of S1, we have
ψ1ψ1 = (

A2
ψ2ψ4

A1
φ1φ2φ3)(

A6
φ6)
=

A2

A5
φ5

A4
φ4

A1
φ1φ2φ3

A6
φ6
= (φ5φ4φ1φ2φ3φ6)↓A3 = P(A3).
Next, assume that you have the evidence e = {e5 : A5 = a5, e6 : A6 = a6}.
The evidence e is represented as two 0−1 potentials e5 and e6. To calculate
the probabilities P(X, e), you place the two evidence potentials in appropriate
cliques (V2 and V1) and perform a full propagation.
4.4.1 Lazy Propagation in Junction Trees
Each clique V holds a set of potentials denoted by ΦV . Each separator has
two mailboxes, one for each direction of the link. The messages stored in
the mailboxes are sets of potentials. The messages are denoted by ψS or ψS,
depending on the direction.
The basic operation in the lazy propagation procedure is message passing.
Deﬁnition 4.7. Let V be a clique with set of potentials ΦV , and let S be a
neighboring separator. Let S1, . . . , Sk be the other neighboring separators of
V . Assume that each Si has received a message Ψi for V .

128
4 Belief Updating in Bayesian Networks
Then V can pass the message (ΦV ∪Ψ1 ∪· · ·∪Ψk)↓S to S, and we say that
the direction V to S is triggered.
The propagation method consists in repeatedly passing messages along
triggered directions.
Proposition 4.5. If you repeatedly pass messages along triggered directions
in a junction tree, then you need not stop before a message has been passed in
both directions over each link. In that situation, we say that the junction tree
is full.
Proof.
See Exercise 4.27.
⊓⊔
As shown in Example 5.3, you can start oﬀby directing all messages
toward a chosen temporary root R. In other words, the junction tree is given
a direction from R and outward, and the messages are passed in the opposite
direction from leaves and inward (see Figure 4.20). This procedure is called
CollectEvidence(R).
R
1
1
1
1
2
1
2
Fig. 4.20. The message passing in CollectEvidence(R).
Notice that the message passing in CollectEvidence(R) corresponds to
a perfect elimination sequence ending with the nodes of R.
To ﬁll the junction tree after a CollectEvidence(R), you need only
to place messages in the opposite directions. First, R passes a message to
its neighbors, they in turn pass messages further outward, and so forth out
to the leaves (see Figure 4.21). This procedure is called DistributeEvi-
dence(R). Note that messages are passed along triggered directions only
if DistributeEvidence(R) is performed after CollectEvidence(R) has
been performed.

4.4 Propagation in Junction Trees
129
R
2
2
1
1
1
2
2
Fig. 4.21. The messages passing in DistributeEvidence(R).
Theorem 4.5. Let the junction tree T represent the Bayesian network BN
over the universe U and with evidence e. Assume that T is full.
1. Let V be a clique with set of potentials ΦV , and let S1, . . . , Sk be V ’s
neighboring separators and with V -directed messages Ψ1, . . . , Ψk. Then
P(V, e) =
	
ΦV
	
Ψ1 · . . . ·
	
Ψk.
2. Let S be a separator with the sets ΨS and Ψ S in the mailboxes. Then
P(S, e) =
	
ΨS
	
Ψ S.
Proof.
1. Consider the messages passed in the direction of V . They correspond to a
CollectEvidence(V ), and the message passing corresponds to a perfect
elimination sequence ending with the nodes of V . Therefore,
P(V, e) = P(U, e)↓V =
	
ΦV
	
Ψ1 · · ·
	
Ψk.
2. Consider Sk as before. Because
	
Ψ k =
	
ΦV
	
Ψ1 · · ·
	
Ψk−1
↓Sk ,
we have
P(Sk, e) = P(V, e)↓Sk =
	
ΦV
	
Ψ1 · · ·
	
Ψk
↓Sk
=
	
ΦV
	
Ψ1 · · ·
	
Ψk−1
↓Sk 	
Ψk
=
	
Ψ k 	
Ψk.
⊓⊔

130
4 Belief Updating in Bayesian Networks
4.5 Exploiting the Information Scenario
As mentioned at the beginning of this chapter, the actual information scenar-
ios can provide simpliﬁcations of the calculations. This is one of the reasons
why we let lazy propagation work with sets of potentials rather than multi-
plied potentials.
4.5.1 Barren Nodes
Barren nodes (see Section 4.1.2) do not contribute to the probabilities of non-
barren nodes, and therefore we need not take their potentials into account
when calculating marginals of nonbarren nodes. This is illustrated in Fig-
ure 4.22.
eG
B
A
C
D
E
F
G
Fig. 4.22. The nodes A, B, C, D, and E are barren.
In the calculation of P(F | eG), the part of the network with barren nodes
can be discarded. Figure 4.23 shows a junction tree for the network.
To calculate P(F | eG), you can collect to the clique (F, G). We see that
all marginalizations to perform are of the form 
X P(X | pa(X)), and from
the unit potential property (Section 1.4) they are all 1.
Now assume that there is also evidence eA for the variable A. Because
A is d-separated from F, eA does not aﬀect P(F | eG). In the junction
tree propagation, the message ψ(B) from the clique (A, B) is no longer 1.
When the clique (B, C, D) produces a message for (D, F), the calculation is
{P(C | B, D), P(B), P(a | B)}↓D. If we start marginalizing C out, we apply
the unit potential property, and marginalizing B will result in a constant.
The handling of barren nodes can be taken care of using the following rule.
Barren node rule: Let Ψ be a set of potentials, and assume that we calculate
Ψ ↓V . If A /∈V , and the only potential in Ψ with A in the domain is of the
form P(A | W), then A is marginalized by discarding P(A | W).

4.5 Exploiting the Information Scenario
131
BCD
F G
↓
DF
↓
D
D
DE
F
↑
P (D | F )
↑
↓
↑
P (E | D)
↓
↑
B
P (A | B)
AB
P (C | B, D)
P (G | F ), P (F )
eG
P (B)
Fig. 4.23. A junction tree for the network in Figure 4.22.
4.5.2 d-Separation
When evidence is of the form that it instantiates a variable (hard evidence),
then the domains to handle will be reduced with this variable. There are
other simpliﬁcations due to instantiation: new pairs of variables may become
d-separated, reducing the domains of the messages to communicate. We illus-
trate this with the example in Figure 4.24.
A
B
C
D
E
F
Fig. 4.24. A Bayesian network.

132
4 Belief Updating in Bayesian Networks
We will be interested in P(E | e), and therefore we only consider collecting
evidence to the clique (D, E). A junction tree for the network is shown in
Figure 4.25.
P (E | D)
DE
↑
D
B
BD
P (C | B), P (D | C)
BCD
↑
↑
P (F | B, D)
BDF
P (B | A), P (A)
AB
Fig. 4.25. A junction tree for the Bayesian network in Figure 4.24. Only the upward
mailboxes are indicated.
First, let A be instantiated to a. The messages are given in Figure 4.26,
and we see that the evidence has an impact on P(E, e) through the message
ψ1(D): P(E | a) = 
D P(E | D)P(D | a).
Next, let C be instantiated to c. Then A and E are d-separated. Figure 4.27
shows how this is reﬂected in the messages: P(E | c) = 
D P(E | D) P(D | c).
Finally, let F be instantiated to f. Then A and E are no longer d-separated.
This is shown in Figure 4.28.
Note: In the examples, we have entered evidence on a variable X by instan-
tiating the potentials including X. In general, evidence can be entered by
adding the corresponding evidence potential to a clique containing X, and
the instantiation is eﬀected when X must be marginalized. This means that
the evidence potential is passed to separators containing X.
4.6 Nontriangulated Domain Graphs
So far, we have considered propagation methods only for potentials with a
triangulated graph. For these methods, we know that the junction tree is a

4.6 Nontriangulated Domain Graphs
133
D
↑
{ψ1(D), P (a)}
DE
P (E | D)
P (C | B), P (D | C)
BCD
↑
1
BD
P (F | B, D)
↑
{P (B | a), P (a)}
P (B | a), P (a)
AB
BDF
B
Fig. 4.26. The messages on collecting to (D, E) for A instantiated. Here ψ1 =
P
C P(D | C) P
B P(C | B)P(B | a) = P
C P(D | C)P(C | a) = P(D | a).
D
↑{k, P (D | c), P (a)}
DE
P (E | D)
P (c | B), P (D | c)
BCD
1
{P (B | a), P (a)}
↑
↑
P (F | B, D)
BDF
P (B | a), P (a)
AB
Fig. 4.27. The messages on collecting to (D, E) for A and C instantiated. Here
k = P
B P(c | B)P(B | a) = P(c | a).

134
4 Belief Updating in Bayesian Networks
D
↑{ψ2(D), P (D | c), P (a)}
DE
P (E | D)
P (c | B), P (D | c)
BCD
↑
{P (f | B, D)}
↑
{P (B | a), P (a)}
BD
B
P (B | a), P (a)
AB
P (f | B, D)
BCF
Fig. 4.28. The messages on collecting to (D, E) for A, C, and F instantiated. Here
ψ2(D) = P
B P(f | B, D)(c | B)P(B | a).
propagation framework having the smallest possible domains with which to
work.
If the domain graph is not triangulated, we embed it in a triangulated
graph and use its junction tree. In fact, we did so in Section 4.5.2 when we
handled evidence.
Example 4.4. Consider the Bayesian network in Figure 4.29. After having elim-
inated the variables A, C, H, I, and J, we cannot eliminate any node without
adding ﬁll-ins, and the graph is not triangulated.
The graph in Figure 4.30 is a triangulated graph extending the moral graph
in Figure 4.29. We can use a junction tree for that graph (see Figure 4.31).
4.6.1 Triangulation of Graphs
It is quite easy to ﬁnd a triangulated graph extending a graph G. You eliminate
the variables in some order, and if you wish to eliminate a node with an incom-
plete neighbor set, you make it complete by adding ﬁll-ins (the graph in Fig-
ure 4.30 is the result of eliminating in the order A, C, H, I, J, B, G, D, E, F).
The resulting graph has a perfect elimination sequence, and it is therefore
triangulated.
There are several diﬀerent elimination orders, and many of them produce
diﬀerent triangulated graphs. We aim to work with the one yielding the small-
est domains.

4.6 Nontriangulated Domain Graphs
135
A
A
B
B
C
C
D
D
E
E
F
F
G
G
H
H
I
I
J
J
(a)
(b)
Fig. 4.29. A Bayesian network (a) with a nontriangulated moral graph (b).
A
B
C
D
E
F
G
H
I
J
Fig. 4.30. A triangulated graph extending the moral graph in Figure 4.29.
Deﬁnition 4.8. Let V be a set of variables. For X ∈V, |sp(X)| denotes the
number of states of X. The size of V, sz(V), is the product 
X∈V |sp(X)|.
Let BN be a Bayesian network, let G be a triangulated graph extending BN’s
moral graph, and let V1, . . . , Vn be the cliques of G. The size of G is the sum
size(G) = 
i sz(Vi).
Unfortunately, it is NP-hard to determine an elimination sequence yielding
a triangulation of minimal size. However, there are heuristic algorithms that
have proven to give fairly good results. One example is the following:
Heuristic: Repeatedly eliminate a simplicial node, and if this is not possible,
eliminate a node X of minimal sz(fa(X)).
Example 4.5. Let the number of states for the variables in Figure 4.29 be
as follows: A, B, C, H, I, and J have two states, D has four states, E has
ﬁve states, F has six states, and G has seven states. After having elimi-
nated the variables A, C, H, I, and J, we eliminate a nonsimplicial node. We
have sz(fa(B)) = 40, sz(fa(D)) = 48, sz(fa(E)) = 70, sz(fa(F)) = 168, and

136
4 Belief Updating in Bayesian Networks
GJ
F
G
EF G
F G
EF
DEF
DE
BDE
BD
BE
ABD
BCE
F H
F GI
φJ
φH
↑
↑
↑
↑
↑
↑
↑
φI
↓
↓
↓
φG
↓
φF
↓
↓
↓
φA, φB, φD
φC, φE
Fig. 4.31. A junction tree with potentials from the Bayesian network in Figure 4.29.
Notation: φX = P(X | pa(X)).

4.6 Nontriangulated Domain Graphs
137
sz(fa(G)) = 210. We choose to eliminate B, creating the ﬁll-in (D, E). With
this new link, we have new sizes sz(fa(D)) = 120 and sz(fa(E)) = 140. We
eliminate D and add the ﬁll-in (E, F). Now the graph is triangulated. How-
ever, in this case the triangulation is not optimal (see Exercise 4.32).
For later use, we establish the following proposition.
Proposition 4.6. Let A1, . . . , An be an elimination sequence triangulating
the graph G, and let Ai and Aj be two nonneighbors in G(i < j). Then
the elimination sequence introduces the ﬁll-in (Ai, Aj) if and only if there is a
path Ai −X −· · · −Aj such that all intermediate nodes are eliminated before
Ai.
Proof.
Assume that ﬁll-ins may be introduced that violate the proposition,
and let (Ai, Aj) be such a ﬁll-in with i as small as possible. Let the link
be introduced on eliminating the node Ak. Because new ﬁll-ins cannot be
attached to Ai when it has been eliminated, we must have k < i. One of the
links (Ak, Aj) and (Ai, Ak) on eliminating Ak must be a ﬁll-in (if not, the
(Ai, Aj) ﬁll-in does not violate the proposition). Let it be (Ai, Ak). Due to
the choice of (Ai, Aj) the proposition holds for (Ai, Ak), hence there is a path
Ak −X −· · · −Ai such that all intermediate nodes are eliminated before Ak
(see Figure 4.32). If also (Ak, Aj) is a ﬁll-in, the same must hold. Connecting
these two paths yields a path Ai −X −· · · −Aj such that all intermediate
nodes are eliminated before Ai, a contradiction.
Ak
Ai
Aj
X
Fig. 4.32. A path connecting Ai and Aj through nodes eliminated before Ai.
Next, assume that we have a path Ai −X −. . . −Aj such that all inter-
mediate nodes are eliminated before Ai. Let Ak be any node on the path to
be eliminated, and let Y and Z be the neighbors on the path. After the elimi-
nation of Ak, there is a link (Y, Z), and there is still a path Ai −X −. . . −Aj
such that all intermediate nodes are eliminated before Ai, so the property is
invariant under elimination. When all the nodes before Ai are eliminated, the
path must be the link (Ai, Aj).
⊓⊔
4.6.2 Triangulation of Dynamic Bayesian Networks
Return to Exercise 3.25 and consider the model in Figure 3.52. In Figure 4.33,
we have folded it out to three time slices.

138
4 Belief Updating in Bayesian Networks
B1
D1
C1
E1
F1
G1
A1
A2
B2
D2
C2
E2
F2
G2
A3
B3
D3
C3
E3
F3
G3
Fig. 4.33. Three time slices of the model in Figure 3.52.
As you have probably experienced when solving Exercise 3.25, your com-
puter ran out of memory when you tried to compile the model folded out to
four or ﬁve time slices. The reason is that the cliques become too large.
A conceptually simple way of considering propagation in dynamic Bayesian
network models is that information is transmitted from one time slice to the
next (if the task is forecasting) or to the previous time slice (if the task is to
ﬁnd out what happened in the past). In other words, probability potentials
describing time slice i are transmitted from time slice i to time slice i + 1 or
to time slice i −1.
Let us consider forward passing from time slice i to time slice i+1, and let
W be the set of variables with a child in slice i+1. We wish to pass potentials
representing the joint probability of W. For the model in Figure 4.33, we pass
the information from slice 1 to slice 2 by eliminating all nodes in slice 1 before
any node from slice 2 is eliminated. Now consider any pair of nodes (X2, Y2).
If there is a path in slice 1 connecting them, then they will be linked after the
elimination of slice 1 (Proposition 4.6). Because the moral graph for slice 1 is
connected, and all nodes in slice 2 have a parent in slice 1, the entire slice 2
will be a subset of a clique if slice 1 is eliminated before any node from slice
2. If you process only two time slices, you may avoid this clique explosion by
using another elimination sequence. However, it will inevitably arrive when
you extend the number of time slices to process. Some cliques will contain
all variables with a child in the next slice or will contain all variables with a
parent in the previous slice.
This situation is not reserved for models with connected time slices. Con-
sider the model in Figure 4.34. If the model is folded out to four time slices,
and the ﬁrst three slices are eliminated before any node from slice four, then
slice four becomes a complete set. Figure 4.35 shows the moral graph for four

4.6 Nontriangulated Domain Graphs
139
slices of the model. The reader can check that all pairs of nodes in slice four
have a connecting path through the past slices.
A
B
C
D
E
n
Fig. 4.34. A dynamic Bayesian network model with very sparse connection inside
the time slices.
A1
A2
A3
A4
B1
B2
B3
B4
C1
C2
C3
C4
D1
D2
D3
D4
E1
E2
E3
E4
Fig. 4.35. The moral graph for four time slices of the model in Figure 4.34.
As indicated above, you may think of propagation in dynamic Bayesian
networks as a way of passing probabilities of output nodes forward in time.
The problem is that most often, the required probability distribution is the
joint distribution over all output variables. If this is intractable, you can ap-
proximate the joint distribution by partitioning the set of output variables.
If O is partitioned into {O1, O2, O3}, then instead of passing P(O) you pass
{P(O1), P(O2), P(O3)}. It has been proven that the error introduced does
not accumulate over time, but converges to a ﬁnite error (in Kullback-Leibler
divergence; see Deﬁnition 6.2).

140
4 Belief Updating in Bayesian Networks
4.7 Exact Propagation with Bounded Space
One of the biggest problems with exact propagation algorithms such as the
junction tree based approach described in Section 4.4 is that the probability
tables can become intractably large. In this section we will investigate an
exact propagation algorithm in which space can be traded for time. For this
particular propagation algorithm, we will consider calculation of probabilities
only of the form P(x, e), since P(x | e) can subsequently be found by P(x | e) =
P(x, e)/(
x P(x, e)).
4.7.1 Recursive Conditioning
Consider the Bayesian network in Figure 4.36 and assume that we are inter-
ested in the probability P(f).
A
B
C
D
E
F
Fig. 4.36. A Bayesian network.
By calculating P(f) using, for example, variable elimination (Section 2.3.4)
or lazy propagation (Section 4.4.1), we basically ﬁrst establish an elimination
sequence and then use the distributive law. For example, by using the elimi-
nation sequence F, E, D, C, B, A we would get
P(f)
=

A

B

C

D

E
P(A, B, C, D, E, f)
=

A
P(A)

B
P(B | A)

C
P(C | B)

D
P(D | C)

E
P(E | D)P(f | D, E).
(4.1)

4.7 Exact Propagation with Bounded Space
141
The sequence in which the calculations are performed can be encoded
in a computation tree. The leaf nodes represent the conditional probability
distributions in the model, and for each internal node the potentials deﬁned
by the two subtrees are multiplied and the variables indicated by the label of
the node are marginalized out (see Figure 4.37).
P(A)
P(B | A)
P(C | B)
P(D | B)
P(E | C)
P(F | D, E)
P
A
P
B
P
C
P
D
P
E
Fig. 4.37. The computation tree for the calculation of P(f) in Figure 4.36 using
the elimination sequence E, D, C, B, A.
Based on the computation tree in Figure 4.37 we can easily specify an
algorithm that calculates P(f) and performs the same operations as in equa-
tion (4.1): evaluate the computation tree from the leaves toward the root.
When an internal node is reached, multiply the two potentials calculated in
the two subtrees for that node and marginalize out the appropriate variables.
Another way of doing the calculations would be to start at the root 
A
and recursively evaluate the subtrees for each state of A; when the recursive
calls return, the results are added up. Assuming that A is binary, for the
calculations in equation (4.1) this would correspond to
P(f) = P(a1)

B
P(B | a1)

C
P(C | B)

D
P(D | C)

E
P(E | D)P(f | D, E)
+ P(a2)

B
P(B | a2)

C
P(C | B)

D
P(D | C)

E
P(E | D)P(f | D, E),
(4.2)
where, for example, the ﬁrst term is the result of the recursive calls made at
node 
B:

142
4 Belief Updating in Bayesian Networks

B
P(B | a1)

C
P(C | B)

D
P(D | C)

E
P(E | D)P(f | D, E)
=P(b1 | a1)

C
P(C | b1)

D
P(D | C)

E
P(E | D)P(f | D, E)
+ P(b2 | a1)

C
P(C | b2)

D
P(D | C)

E
P(E | D)P(f | D, E).
Compared to equation (4.1) we can say that when the computation tree is
“read” from the root toward the leaves, we condition in the internal nodes,
and when it is “read” from the leaves towards the root, we marginalize out in
the internal nodes.
By continuing the “recursive conditioning” above, we see that the storage
requirements are considerably reduced. Speciﬁcally, for handling the interme-
diate results we have to store only the initial conditional probability distribu-
tions together with a single number for each internal node in the computation
tree, i.e., the space complexity is linear in the number of nodes. Unfortu-
nately, this reduction in space comes at a price. In this particular example,
the number of recursive calls corresponds to the size of the state space of
all the variables involved. Assuming that the variables are binary, this would
amount to 32 recursive calls. Note, however, that the size of the call stack is
proportional to the depth of the tree.
In general, the number of recursive calls increases exponentially with the
height of the computation tree, so to reduce the time complexity we should aim
for a more balanced tree structure. For example, consider again the Bayesian
network in Figure 4.36, but assume now that we have the elimination ordering
B, A, E, C, D:
P(f) =

B

A

E

C

D
P(A, B, C, D, E, f)
=

B

A
P(A)P(B | A)

×

E

C
P(C | B)P(E | C)
 
D
P(D | B)P(f | D, E)

.
(4.3)
The corresponding computation tree is shown in Figure 4.38. In this tree
the calculation of P(f) requires only 2 · (2 + 2 · (2 + 2)) = 20 recursive calls.
In the two examples above, we condition on only one variable at a time. The
reason is that both elimination sequences ensure that each time we condition
on a variable, the remaining variables can be partitioned into two d-separated
sets. This, however, is not the case in general. For example, for the elimination
sequence D, C, E, B, A, neither D nor C can alone partition the variables into
independent sets; hence a node in the tree is labeled with both variables (see
Figure 4.39):

4.7 Exact Propagation with Bounded Space
143
P(A)
P(B | A)
P(C | B)
P(D | B)
P(E | C)
P(f | D, E)
P
A
P
B
P
C
P
D
P
E
Fig. 4.38. The computation tree for the calculation of P(f) in Figure 4.36 using
the elimination sequence B, A, E, C, A.
P(f) =

D

C

E

B

A
P(A, B, C, D, E, f)
=

D

C

E
P(E | C)P(f | D, E)

×

B
P(C | B)P(D | B)

A
P(A)P(B | A)

.
It should also be noted that the computation graph is not required to be
binary; for example, if conditioning on a variable partitions the remaining
variables into three or more d-separated sets, then the corresponding node
may have more than two children in the computation tree.
P(A)
P(B | A)
P(C | B)
P(D | B)
P(E | C)
P(f | D, E)
P
A
P
B
P
C,D
P
E
Fig. 4.39. The computation tree for the calculation of P(f) in Figure 4.36 using
the elimination sequence D, C, E, B, A.
In general, the set of variables attached to a node T corresponds to the
set of noninstantiated variables shared by its two subtrees Tl and Tr. This set
is also called the cutset for the node:

144
4 Belief Updating in Bayesian Networks
cutset(T ) = (dom (Tl) ∩dom (Tr)) \ a-cutset(T ),
where dom (Ti) are the variables that appear in the conditional probability
tables in the subtree Ti and a-cutset(T ) is the union of the cutsets associ-
ated with the ancestral nodes for T in the tree (if T is the root node, then
a-cutset(T ) = ∅). Thus, the a-cutset is the set of nodes already instantiated.
For example, in the tree in Figure 4.39, the cutset for the root node is {C, D}
and the a-cutset for the node labeled 
A is {B, C, A}. In particular, the a-
cutset for the unlabeled node is {B, C, D}, which covers all variables in the
subtree, hence this node is given the empty cutset.
Before we present a more-formal speciﬁcation of the algorithm it should be
noted that in the above examples we incorporated the evidence f directly in
the computation tree, indicating that a new computation tree is constructed
for each piece of evidence. A more-eﬃcient approach would be to ﬁrst con-
struct a single computation tree with no evidence inserted. Then, when ev-
idence arrives we simply “record” the variables that should be instantiated
such that no summations are performed for these variables.
Algorithm 4.2 reﬂects this approach for calculating the probability of a
conﬁguration e based on a computation tree for a Bayesian network. Observe
that at each recursive call we record the corresponding instantiation and un-
record it when the call returns.
Algorithm 4.2 [RecursiveConditioning] In order to calculate P(e) using
recursive conditioning on the tree T , do:
1. If T is the root, then record instantiation e.
2. If T is a leaf, then:
a) Return LookUp(T ).
3. Else
a) Set p := 0.
b) For each noninstantiated conﬁguration c of cutset(T ) do:
i. Record instantiation c.
ii. Set
p := p +
m
	
i=1
RecursiveConditioning(Ti),
where T1, . . . , Tm are the children of T .
iii. Unrecord instantiation c.
c) Return p.
⊓⊔
Algorithm 4.3 [LookUp] To ﬁnd the value of the leaf node T under the
recorded instantiations, do:
1. Let X be the variable associated with T and let P(X | pa(X)) be the con-
ditional probability table assigned to X.
2. If X is instantiated, then:

4.8 Stochastic Simulation in Bayesian Networks
145
a) Let x be the recorded instantiation for X and let π be the recorded
instantiation for pa(X).
b) Return P(x | π).
3. else
a) Return 1.
⊓⊔
Clearly, this algorithm requires only as much space as is needed to store
the computation tree, and this is linear in the number of variables (hence for
this aspect the shape of the tree is of no importance). However, the situation
is diﬀerent if we consider the time complexity. The time complexity can be
estimated by counting the number of recursive calls, and it can be shown (see
Exercise 4.38) that for a balanced tree it is O(nw+1) and for an unbalanced
tree it is O(n · exp(w · n))), where w is the size of the largest cutset.
This also indicates that it is important to ﬁnd a good computation tree
representation of the Bayesian network, and as we also indicated above this is
closely connected with ﬁnding a good elimination sequence (see Section 4.6.1).
In fact, given an elimination sequence that produces a maximum clique size
of w, there are algorithms that will return a computation tree in which the
cutset is not larger than w. The idea is to build the tree from the leaves to
the root, where appropriate subtrees are joined according to the sequence in
which the variables are marginalized out.
As for the tree in Figure 4.37, the algorithm above may perform redun-
dant recursive calls to a subtree. This may happen when the a-cutset for
a node/subtree includes a variable that is not in the domain of any of the
probability tables associated with the subtree in question; we shall call all
nonredundant nodes in a-cutset(T ) the context for T . A way of controlling
the number of redundant recursive calls is to cache previous calculations.
Since we assume that we do not have enough memory to cache all values,
the trick is therefore to ﬁnd a good strategy for selecting the values to cache.
If cache?T (x) is a function that determines whether to cache the value for
subtree T evaluated in the context x, we can directly control how much mem-
ory the algorithm is allowed to use. Algorithm 4.2 can easily be modiﬁed to
support such a caching strategy: before a recursive call is made in context x
we check whether a value for that context is already stored in the cache; if
this is the case we simply return that value; otherwise, the call is completed
and the result is cached if this is in accordance with cache?T (x).
4.8 Stochastic Simulation in Bayesian Networks
The junction tree based propagation methods described in the beginning of
this chapter require tables for the cliques in the triangulated graph. These
cliques may be very large, and it may happen that the space requirements of
the tables cannot be met by the hardware available. When this is the case

146
4 Belief Updating in Bayesian Networks
either you can make a tradeoﬀbetween time and space (using, for example,
recursive conditioning as described in Section 4.7) or you can trade space for
accuracy by using an approximate inference method.
In this section, we give a ﬂavor of a class of approximate methods that are
based on a technique called stochastic simulation. To illustrate the methods,
consider the Bayesian network in Figure 4.40, with the conditional probabili-
ties speciﬁed in Table 4.1, and assume that we want to estimate the probabil-
ity of E = y. Now suppose also that we have access to a database containing
conﬁgurations over the ﬁve variables and for which the distribution of the
conﬁgurations follows the probability distribution speciﬁed by the Bayesian
network. Given such a database, we can estimate the probability of E = y by
counting the number of cases that contain E = y and divide it by the total
number of cases:
P(E) ≈N(E = y)
N
.
Since we (usually) do not have access to such a database, stochastic simulation
instead tries to simulate such an access. This is done by drawing a large num-
ber of random conﬁgurations over (A, B, C, D, E) using the Bayesian network.
There are several diﬀerent algorithms for performing this type of sampling,
and their main diﬀerences lie in how the samples are generated and how the
probabilities are estimated from the sampled conﬁgurations.
A
B
C
D
E
Fig. 4.40. An example network. All variables have the states y and n.
4.8.1 Probabilistic Logic Sampling
Probabilistic logic sampling is one of the simplest sampling procedures. To il-
lustrate the approach, consider again the Bayesian network in Figure 4.40 and
assume for simplicity that we have not received any evidence. A conﬁguration
can now be sampled by iteratively sampling a state of each of the variables.
First a state of variable A is sampled. A random generator (with an even
distribution) is asked to give a real number between 0 and 1. If the number
is less than 0.4 (the prior probability of A = y), the state is y; otherwise, the

4.8 Stochastic Simulation in Bayesian Networks
147
A
B
y
n
y 0.3 0.8
n 0.7 0.2
A
C
y
n
y 0.7 0.4
n 0.3 0.6
B
D y
n
y 0.5 0.1
n 0.5 0.9
P(B | A)
P(C | A)
P(D | B)
C
D
y
n
y
(0.9, 0.1)
(0.999, 0.001)
n (0.999, 0.001) (0.999, 0.001)
P(E | C, D)
Table 4.1. The conditional probabilities for the example network. P(A) = (0.4, 0.6).
state is n. Assume that the result is y. From the conditional probability table
P(B | A), we have that P(B | y) = (0.3, 0.7). The random generator is asked
again, and if the number is less than 0.3, the state of B is y. This procedure
continues until we also have a state for C, D, and E. Observe that the se-
quence in which we generate the sample follows the topological ordering of
the nodes in the network: we start at the nodes without parents and work
ourselves toward the nodes without children; when visiting a variable we sam-
ple a state for that variable using its associated probability table conditioned
on the conﬁguration of the parent variables that have already been sampled.
The next conﬁguration is sampled through the same procedure, and this is
repeated until N conﬁgurations have been sampled. In Table 4.2, an example
set of conﬁgurations is given.
CDE
AB yyy yyn yny ynn nyy nyn nny nnn
yy
4
0
5
0
1
0
2
0
yn
2
0
16
0
1
0
8
0
ny
9
1
10
0
14
0
16
0
nn
0
0
4
0
0
0
7
0
Table 4.2. A set of 100 conﬁgurations of (A, B, C, D, E) sampled from the network
in Figure 4.40 and Table 4.1.
The probability distributions for the variables can now be calculated by
counting in the sample set (see Exercise 4.39). For example, for 99 of the sam-
ples in Table 4.2, the state of E is y, and this gives an estimated probability:
P(E) ≈
N(E = y)
N
, N(E = n)
N

=
 99
100, 1
100

= (0.99, 0.01).

148
4 Belief Updating in Bayesian Networks
So far, only marginal probabilities have been calculated. However, a
straightforward approach to handle evidence is simply to discard the conﬁg-
urations that do not conform to it. In other words, a new series of stochastic
simulations is started, and whenever a state of an observed variable is drawn,
you stop simulating if the state drawn is not the one observed. In general,
if we have evidence e and we are interested in estimating P(Xk | e) using N
samples, then probabilistic logic sampling can be performed as follows:
1. Let (X1, . . . , Xn) be a topological ordering of the variables.
2. For j = 1 to N:
a) For i = 1 to n:
- Sample a state xi for Xi using P(Xi | pa(Xi) = π), where π is the
conﬁguration already sampled for pa(Xi).
b) If x = (x1, . . . , xn) is consistent with e, then
N(Xk = xk) := N(Xk = xk) + 1,
where xk is the state that was sampled for Xk.
3. Return:
P(Xk = xk | e) ≈
N(Xk = xk)

x∈sp(Xk) N(Xk = x).
The preceding method does not require a triangulation of the network, nor
is it necessary to store the sampled conﬁgurations (as we did in Table 4.2).
It is enough to store the counts for each variable of interest. Whenever a
sampled conﬁguration has been determined, the counts of all variables are
updated, and the sample can be discarded. The method therefore saves much
space, and each conﬁguration is determined in time linear in the number of
variables. These beneﬁts, however, come at the expense of accuracy and time.
In particular, this method has a serious drawback when the probability of
the evidence is small. For instance, assume that for the preceding example we
have the observations B = n and E = n. The probability for (B = n, E = n)
is 0.00282, which means that in order to get 100 conﬁgurations, you should for
this tiny example expect to perform more than 35,000 stochastic simulations.
In general, since the probability of the evidence drops oﬀexponentially fast,
this method can be hopelessly time-consuming even when we have only a few
pieces of evidence.
4.8.2 Likelihood Weighting
You might be tempted to overcome the shortcoming of probabilistic logic
sampling by simply ﬁxing the evidence variables E to their observed states
and sample only from the nonevidence variables; in this way no samples need
to be discarded. However, since a sample is generated by going from the root
nodes down to the leaves, this naive procedure would result in a sample in
which the value for a given variable takes only the evidence from its ancestors

4.8 Stochastic Simulation in Bayesian Networks
149
into account and not the evidence pertaining to the variables further down in
the network. For example, if we should try to estimate P(A | B = n, E = n)
using this modiﬁed sampling procedure we would actually estimate P(A). The
problem is that instead of sampling from the distribution P(U, e) speciﬁed
by the evidence and the Bayesian network, we are in fact sampling from a
probability distribution somewhere in between the prior distribution P(U)
and the posterior distribution P(U | e). To be more precise, if pa(X)′′ are the
parents of X that have received evidence (pa(X) = pa(X)′∪pa(X)′′), then the
joint distribution P(U, e) that we would like to sample from can be expressed
as
P(U, e) =
	
X∈U\E
P(X | pa(X)′, pa(X)′′ = e)



Part 1
×
	
X∈E
P(X = e | pa(X)′, pa(X)′′ = e)



Part 2
.
(4.4)
However, the distribution that we are actually sampling from is
Sampling distribution =
	
X∈U\E
P(X | pa(X)′, pa(X)′′ = e),
which corresponds only to Part 1 of equation (4.4).
Fortunately, this also points to a simple way of compensating for the esti-
mation problem above: weigh each of the generated samples x with a weight
corresponding to Part 2 of equation (4.4). That is, instead of adding 1 to
the count N(Xi) (as we did for probabilistic logic sampling) we add a weight
w(x, e):
w(x, e) =
	
E∈E
P(E = e | pa(X) = π),
where π is the conﬁguration of pa(X) speciﬁed by x and e.
This updating approach, called likelihood weighting, ensures that we get
the correct counts for estimating the probabilities. This can also be seen by
combining the weight calculation and the sampling distribution, which to-
gether correspond to the distribution P(U, e).
Now consider again the example network above and assume that we want
to estimate P(A | B = n, E = n). As before, we start by sampling a state of A
using a random generator (let the resulting state be y). Since B has received
the evidence B = n, no state is sampled, and instead we continue to C and
sample a state using P(C | A = y) = (0.7, 0.3); assume that the sampled state
is n. Next we sample a state for D using P(D | B = n) = (0.5, 0.5) (assume
that we get D = y). Since E has received evidence, E = n, we now have a
complete conﬁguration over all ﬁve variables and the sampling stops. Next we
calculate the weight associated with the sampled conﬁguration:

150
4 Belief Updating in Bayesian Networks
w((A = y, B = n, C = n, D = y, E = n), (B = n, E = n))
= P(B = n | A = y)P(E = n | C = n, D = y) = 0.7 · 0.001 = 0.0007.
This value is then added to N(A = y) (and to N(C = n) and N(D = y)
as well if we are also interested in the probabilities for these two variables).
We then continue to generate samples (and weights) as above, and when
a suﬃcient number of samples has been generated we return the estimate
P(A | B = n, E = n) ≈N(A = y)/(N(A = y) + N(A = n)).
In general, if we are interested in estimating P(Xk | E = e) using N sam-
ples, then the likelihood weighting algorithm can be summarized as follows:
1. Let (X1, . . . , Xn) be a topological ordering of the variables.
2. For j = 1 to N:
a) w:=1.
b) For i = 1 to n:
- Let x′ be the conﬁguration of (X1, . . . , Xi−1) speciﬁed by e and the
previous samples.
- If Xi ̸∈E, then:
- Sample a state xi for Xi using P(Xi | pa(Xi) = π), where
pa(Xi) = π is consistent with x′.
- else
w := w · P(Xi = ei | pa(Xi) = π), where pa(Xi) = π is consistent
with x′.
c) N(Xk = xk) := N(Xk = xk) + w, where xk is the sampled state for
Xk.
3. Return:
P(Xk = xk | e) ≈
N(Xk = xk)

x∈sp(Xk) N(Xk = x).
Although likelihood sampling is an improvement over probabilistic logic
sampling it may still require a large number of samples. This is typically the
case when there is a large diﬀerence between the sampling distribution and
P(U, e) and, again, this is often the case when the evidence is unlikely.
4.8.3 Gibbs Sampling
Other methods have been constructed for dealing with this problem. A widely
used method is Gibbs sampling. In Gibbs sampling, you start with some conﬁg-
uration consistent with the evidence (for example determined by probabilistic
logic sampling), and then you randomly change the state of the variables in
topological order. In one sweep through the variables, you determine a new
conﬁguration, and then you use this conﬁguration for a new sweep, and so on.
From this perspective, Gibbs sampling diﬀers from the above two procedures
by generating a new sample based on the current one.
Consider again the example above and let the evidence be B = n and
E = n. Assume also that we are given the starting conﬁguration ynyyn. Now,

4.8 Stochastic Simulation in Bayesian Networks
151
to generate a sample we ﬁrst calculate the probability of A given the other
states of that conﬁguration, that is, P(A | B = n, C = y, D = y, E = n). From
the network, we see that the Markov boundary for A includes only B and C;
hence it is suﬃcient to calculate P(A | B = n, C = y). It is easily done by
Bayes’ rule, which gives (0.8, 0.2). We then draw a number from the random
generator, and let us assume that the number is 0.456, resulting in A = y.
The next free variable is C. We calculate
P(C | A = y, B = n, D = y, E = n) = P(C | A = y, D = y, E = n)
= (0.996, 0.04),
and draw a number from the random generator; assume that it results in
C = y.
In general, the calculation proceeds as follows. Let A be a variable in a
Bayesian network BN, let B1, . . . , Bn be the remaining variables, and let b =
(b1, . . . , bn) be a conﬁguration of (B1, . . . , Bn). Then, P(A, b) is the product
of all conditional probabilities in BN with Bi instantiated to bi. Therefore,
P(A, b) is proportional to the product of the potentials involving A, and
P(A | b) is the result of normalizing this product. Note that the calculation
of P(A | b) is a local task.
To return to the example, the next variable is D, and we follow the same
procedure. Assume that the result is D = n. Then the conﬁguration from the
ﬁrst sweep is ynynn. The next sweep follows the same procedure. Assume that
the state of A changes to n. Then we calculate P(C | A = n, D = n, E = n)
and so forth.
In this way, a large sample of conﬁgurations consistent with the observa-
tions is produced. The question is whether the sample is representative of the
probability distribution. It is not always so. It may be that the initial conﬁg-
uration is rather improbable, and therefore the ﬁrst samples likewise are out
of the mainstream. For this reason you usually discard the ﬁrst 5-10% of the
samples. It is called the burn-in. A related problem is the dependence among
the samples: two successive samples will in general not be independent, since
the second sample is generated by altering the ﬁrst sample. In this way, these
samples are also not representative of the probability distribution, and you
therefore typically try to compensate for this by recording samples only at
certain intervals.
Another problem is that you may be stuck in certain “areas” of the con-
ﬁgurations. Perhaps there is a set of very likely conﬁgurations, but in order to
reach them from the one you are in, a variable should change to a state that
is highly improbable given the remaining conﬁguration (see Exercise 4.43).
Finally, the method relies on an initial starting conﬁguration. Unfortu-
nately, it may be very hard to ﬁnd such a conﬁguration, and in fact this
problem is NP-hard (see Exercise 4.44).

152
4 Belief Updating in Bayesian Networks
4.9 Loopy Belief Propagation
There is a popular approximate method that is not a version of sampling. It
is called loopy belief propagation (LBP). LPB has been extremely successful
in a setting not directly connected to Bayesian networks, namely in error-
correcting codes; the so-called turbo codes.
LBP is a message passing algorithm similar to the junction tree algorithm
in Section 4.4. However, instead of having cliques in a junction tree for passing
messages, it uses the nodes in the Bayesian network directly.
The message passing structure consists of one node for each variable in the
Bayesian network. A node representing the variable A holds the conditional
probability table P(A | pa(A)), and it can process potentials over fa(A) (the
variables involved in the table). The neighbors of a node representing A are the
neighbors of A in the Bayesian network, and the messages being passed over
the links are potentials over the shared variables. We shall stick to the term
separator for the domains of the potentials being passed over links, though
these domains need not separate any variables from others. The structure is
illustrated in Figure 4.41.
(a)
(b)
A
A
A
B
B
B
B
C
C
C
D
D
D
E
E
P(A)
P(B | A)
P(C | B)
P(D | B)
P(E | C, D)
Fig. 4.41. (a) A Bayesian network. (b) The corresponding message-passing struc-
ture for LPB. Each node holds the corresponding variable’s conditional probability
table; the domain of a node is the variable’s family. The square box on a link indi-
cates the separator (the domain for the potentials to be passed over that link).
Note that all separators consist of one variable. If B is a child of A then
the separator is A.
The processing of messages is similar to the one for junction trees: a mes-
sage is sent to a neighbor by multiplying the incoming messages from all other

4.9 Loopy Belief Propagation
153
neighbors to the potential it holds and marginalizing the result down to the
separator. This is illustrated in Figure 4.42.
A
A
B
B
C
C
C
D
E
P(C | A, B)
φA
φB
φD
φE
λC(A)
πE(C)
λC(A) = P
B,C P(C | A, B)φBφDφE
πE(C) = φD
P
A,B P(C | A, B)φAφB
Fig. 4.42. The node C holding P(C | A, B) has received all messages (the φs). It
sends a λ-message to its parent A and a π-message to its child E.
A message from a parent variable to a child variable is called a π-message
(because it is in fact a probability distribution), and a message from a child
to a parent is called a λ-message (for likelihood).
Since the structure may not be a tree, you cannot use the rule that a
node can send to a neighbor when it has received a message from all its other
neighbors. In Figure 4.41, only the node A can send a message. All other nodes
wait for a message that never comes. Instead, you have a marching regime; at
each step all nodes send messages to each neighbor using the messages they
have received so far from the other neighbors. After each step, any node A
can calculate an estimate of its own probability distribution: take the product
of P(A| pa(A)) and all incoming messages, marginalize it down to A, and
normalize.
Now you let the method march step by step, monitor the development
of the probability distributions, and use some stopping criterion. There is
no guarantee that the method will converge, nor is there any guarantee that
in case of convergence it will converge to the correct posterior distributions.
On the other hand, very much experience has been gained, and the method
converges to the correct posteriors surprisingly often.
However, sometimes the method is guaranteed to converge correctly, for
example, if the network is singly connected (there are no multiple paths in the
network). In that case, the junction tree will be exactly the structure for LBP
(see Exercise 4.23), and when the method has marched twice the number of
links in the network, the messages will be the same as the messages in the
junction tree algorithm.

154
4 Belief Updating in Bayesian Networks
Unfortunately, this result is not of any use. If the Bayesian network is
singly connected, the cliques are small, and exact junction tree propagation
is no problem. As mentioned above, LBP does very often give good results,
and much research is now directed at understanding why and characterizing
situations in which you are guaranteed a result within a reasonable margin of
tolerance.
4.10 Summary
Exact Belief Updating
Exact belief updating can be performed by message passing in a junction tree
representation of the Bayesian network. The junction tree is obtained after
triangulating the moral graph of the Bayesian network.
Moral graph: The moral graph of a Bayesian network is obtained by inserting
a link between all pairs of variables with a common child, and dropping the
direction on all arcs.
Triangulated graph: An undirected graph with a perfect elimination sequence
is called a triangulated graph. If a graph is not triangulated, you can insert
additional links (determined by, for example, node elimination), making it
triangulated.
Node elimination: A node is eliminated by inserting a link between each pair
of its noneliminated neighboring nodes.
Perfect elimination sequence: An elimination sequence is perfect if all nodes
can be eliminated according to that sequence without inserting a link between
a pair of noneliminated variables.
Clique: A complete set is a clique if it is not a subset of another complete set
(a maximal complete set).
Join tree: Let G be the set of cliques from an undirected graph, and let the
cliques of G be organized in a tree T . Then T is a join tree if for any pair of
nodes V , W all nodes on the path between V and W contain the intersection
V ∩W.
Junction tree: Let Φ be a set of potentials with a triangulated domain graph,
G. A junction tree for Φ is a join tree for G with the following addition: each
potential φ in Φ is attached to a clique containing dom (φ); each link has the
appropriate separator attached; each separator contains two mailboxes, one

4.10 Summary
155
for each direction.
Message passing: Let V be a clique with set of potentials ΦV , and let S be a
neighboring separator. Let S1, . . . , Sk be the other neighboring separators of
V . Assume that each Si has received a message Ψi for V . Then V can pass
the message (ΦV ∪Ψ1 ∪· · · ∪Ψk)↓S to S.
Belief updating (calculating marginals): Let the junction tree T represent the
Bayesian network BN over the universe U and with evidence e. Assume that
each mailbox contains a message.
1. Let V be a clique with set of potentials ΦV , and let S1, . . . , Sk be V ’s
neighboring separators and with V -directed messages Ψ1, . . . , Ψk. Then,
P(V, e) =
	
ΦV
	
Ψ1 · · ·
	
Ψk.
2. Let S be a separator with the sets ΨS and Ψ S in the mailboxes. Then,
P(S, e) =
	
ΨS
	
Ψ S.
Belief Updating with Bounded Space
If there is not enough space to perform junction tree propagation, you may
reduce the space complexity by applying a divide-and-conquer strategy: re-
cursively condition on a variable (or subset of the variables) to be eliminated,
solve the new smaller problems, and add up the results. A cache may be
introduced to trade space for time.
Approximate Belief Updating
Stochastic simulation: Estimate P(X | e) by sampling a large number of ran-
dom conﬁguration over the variables in the Bayesian network. Throw away
the conﬁgurations that are inconsistent with e, and let N ′ be the resulting
number of cases. Then
P(X | e) ≈N ′(X)
N ′
.
Likelihood weighting: Estimate P(X | e) by sampling a large number of random
conﬁgurations over the noninstantiated variables in the Bayesian network.
Weigh each conﬁguration (x, e) with
w(x, e) =
	
E∈E
P(E = e | pa(X) = π),
where E are the evidence variables, and π is the conﬁguration of pa(X) spec-
iﬁed by x and e.

156
4 Belief Updating in Bayesian Networks
Gibbs sampling: Estimate P(X | e) by sampling a large number of random
conﬁgurations over the noninstantiated variables in the Bayesian network. A
sample is generated by starting with a conﬁguration consistent with the evi-
dence, and randomly changing the state of a variable by following the topo-
logical order.
Loopy belief updating (LBP): LBP is a message-passing algorithm that works
directly on the Bayesian network. Messages are similar to those in junction
trees, but in LBP they are passed between the families of variables in the
Bayesian network.
4.11 Bibliographical Notes
Loopy belief propagation is rooted in a version of probability updating for
singly connected DAGs through message passing presented by Kim and Pearl
(1983). In (Pearl, 1986), cutset-conditioning was used to reduce propagation
in multiply connected networks to propagation in singly connected networks.
Shachter (1986) introduced arc reversal and uses it for a probability updating
procedure in the bucket elimination style. Two versions of join tree propa-
gation were presented in the late 1980s. Shafer and Shenoy (1990) proposed
the method presented in this book. They did not exploit lazy evaluation but
worked with multiplied potentials. Lauritzen and Spiegelhalter (1988) and
Jensen et al. (1990b) proposed what is now called the Hugin method. It also
works with multiplied potentials, but the potentials in the cliques are changed
dynamically. This, together with a division operation in the separators, re-
duced the calculation substantially for join trees with branching higher than
three. A detailed study of the similarities and diﬀerences of the two methods
is reported in (Shafer, 1996). Lazy propagation (Madsen and Jensen, 1999b)
dissolves the diﬀerence between Shafer-Shenoy and Hugin propagation.
The concepts of triangulated graphs and join trees have been discovered
and rediscovered with various names. In (Bertele and Brioschi, 1972), they
are used for dynamic programming, and Beeri et al. (1983) uses them for
database management. A good reference on triangulated graphs is (Golumbic,
1980). The heuristic for triangulating nontriangulated domain graphs given
in this chapter is due to Kjærulﬀ(1990), and more can be found in (Cano
and Moral, 1995). The problem of inference in dynamic Bayesian networks
has been treated in (Boyen and Koller, 1998).
Recursive conditioning was introduced in (Darwiche, 2001). Probabilis-
tic logic sampling was proposed by Henrion (1988), and Fung and Chang
(1990) and Shachter and Peot (1990) introduced likelihood-weighted sampling
for Bayesian networks. Gibbs sampling was originally introduced for image
restoration by Geman and Geman (1984). Gilks et al. (1994) have developed
a system, BUGS, for Gibbs sampling in Bayesian networks.

Exercises
157
4.12 Exercises
Exercise 4.1. BN has the potentials in Table 4.3.
A
B
y
n
y 0.2 0.6
n 0.8 0.4
B
C
y
n
y 0.3 0.2
n 0.7 0.8
C
D y
n
y 0.9 0.6
n 0.1 0.4
P(B | A)
P(C | B)
P(D | C)
Table 4.3. Potentials for Exercise 4.1. P(A) = (0.2, 0.8).
(i) Calculate P(A | D = y).
(ii) Calculate P(C | D = y).
Exercise 4.2. BN has the potentials in Table 4.4.
A
B
y
n
y 0.2 0.6
n 0.8 0.4
B
C
y
n
y 0.1 0.5
n 0.9 0.5
B
D y
n
y 0.7 0.4
n 0.3 0.6
P(B | A)
P(C | B)
P(D | B)
Table 4.4. Potentials for Exercise 4.2. P(A) = (0.2, 0.8).
(i) Calculate P(A | C = y, D = y).
(ii) Calculate P(A | D = y).
Exercise 4.3. BN has the potentials in Table 4.5.
A
B
y
n
y 0.2 0.6
n 0.8 0.4
A
C
y
n
y 0.1 0.5
n 0.9 0.5
B
C
y
n
y (0.3, 0.7) (0.2, 0.8)
n (0.9, 0.1) (0.6, 0.4)
P(B | A)
P(C | A)
P(D | B, C)
Table 4.5. Potentials for Exercise 4.3. P(A) = (0.2, 0.8).
(i) Calculate P(A | D = y), P(B | D = y), P(C | D = y).

158
4 Belief Updating in Bayesian Networks
(ii) Calculate P(B | C = y).
Exercise 4.4. Consider the Bayesian network in Figure 4.43. All variables
have three states.
A
B
C
D
E
F
G
H
Fig. 4.43. The network for Exercise 4.4.
(i) Calculate the size of the table P(A, B, C, D, E, F, G = g1, H = h1).
(ii) In the calculation of P(A | G = g1, H = h1), the variables have been
marginalized in the following order: B, F, D, E, C. Calculate the size of
each table produced in the process, and compare the sum with the result
of (i).
(iii) Determine an elimination order yielding a sum smaller than the one from
(ii).
Exercise 4.5. We have the potentials φ1(A1, A2, A3), φ2(A2, A3, A5), φ3(A1,
A3, A4), φ4(A5, A6) over the universe {A1, A2, A3, A4, A5, A6}.
(i) Determine the domain graph.
(ii) Eliminate A3.
(iii) Determine the domain graph for the resulting set of potentials.
Exercise 4.6. We have the potentials φ1(A1, A2, A3), φ2(A2, A4, A5), φ3
(A4, A6, A7), φ4(A1, A6, A8) over the universe {A1, A2, A3, A4, A5, A6,
A7, A8}.
(i) Determine the domain graph.
(ii) Eliminate A1.
(iii) Determine the domain graph for the resulting set of potentials.

Exercises
159
Exercise 4.7. Write a short algorithm that takes as input a Bayesian network
over nodes X1, . . . , Xn and an elimination sequence for all nodes but Xi, and
which outputs the maximum table size that would be used during computation
of P(Xi) using this elimination sequence.
Exercise 4.8. Consider the Bayesian network given in Figure 4.44. What
would the elimination trees (such as those in Figures 4.2 to 4.7) look like for
the two elimination orders C, F, G, B, E, D and F, E, G, D, C, B?
A
B
C
D
E
F
G
Fig. 4.44. A Bayesian network.
Exercise 4.9. Prove Proposition 4.1.
Exercise 4.10. What is (
 φ)↓A for the Bayesian network in Figure 4.44?
Exercise 4.11. What are the domains encoded by the domain graph in Fig-
ure 4.45? Give an example of an elimination sequence ending with C. What
do the intermediate domain graphs look like as you apply the elimination
sequence? Is the sequence perfect?
Exercise 4.12. Consider the domain graph for the potentials in Exercise 4.5.
Determine a perfect elimination sequence ending with A1.
Exercise 4.13. Consider the domain graph for the potentials in Exercise 4.6.
Does the graph have a perfect elimination sequence?
Exercise 4.14. Consider the Bayesian network in Figure 4.43.
(i) Determine the domain graph.
(ii) Does the domain graph have a perfect elimination sequence?

160
4 Belief Updating in Bayesian Networks
A
B
C
D
E
F
G
Fig. 4.45. A domain graph.
G
A
B
C
D
F
E
Fig. 4.46. The graph for Exercise 4.15.
Exercise 4.15. Consider the graph in Figure 4.46.
(i) Determine the simplicial nodes.
(ii) Is the graph triangulated?
Exercise 4.16. Consider the graph in Figure 4.47.
F
A
B
C
D
E
Fig. 4.47. The graph for Exercise 4.16.
(i) Determine the simplicial nodes.
(ii) Is the graph triangulated?

Exercises
161
Exercise 4.17. Deﬁnition Let G be an undirected graph with node set U.
A path in G is a sequence A1, . . . , An of distinct nodes; where Ai and Ai+1 are
neighbors. A cycle is a path except A1 = An, and all other nodes are distinct.
A chord in a cycle A1, . . . , An is a link between two nodes Ai and Aj that
are not neighbors on the path. The graph G is chord-saturated if any cycle of
length > 3 has a chord.
(i) Prove that any triangulated graph is chord-saturated. (Hint: Use induction
and the fact that any cycle through a simplicial node must have a chord.)
(ii) Prove the following decomposition lemma. Let G be a incomplete chord-
saturated graph with at least three nodes and with node set U. Then
there is a complete subset S of U such that G \ S is disconnected. (Hint:
Let A and B be two nonadjacent nodes, and let S be a minimal set of
nodes such that any path connecting A and B contains a node from S.
Use chord saturation and minimality of S to prove that S is complete.)
(iii) Prove that any chord-saturated graph is triangulated. (Hint: Use (ii) to
prove that any incomplete chord-saturated graph with at least two nodes
has at least two simplicial nodes.)
Exercise 4.18. Prove that the moral graph of the graph in Figure 4.48 is
triangulated. Give an example of a join tree for the graph.
A
B
C
D
E
F
G
Fig. 4.48. A Bayesian network.
Exercise 4.19. Consider the domain graph from Exercise 4.5.
(i) Determine the cliques.
(ii) Construct a join tree for the graph.
Exercise 4.20. Consider the graph in Figure 4.47.
(i) Determine the cliques.

162
4 Belief Updating in Bayesian Networks
(ii) Construct a join tree for the graph.
Exercise 4.21. Consider the Bayesian network in Figure 4.49. Construct a
join tree.
A
B
C
D
E
F
G
H
I
J
K
Fig. 4.49. The Bayesian network for Exercise 4.21.
Exercise 4.22. Let A and B be any two adjacent nodes in a join tree for a
Bayesian network M with separator S = A ∩B. Furthermore, let UA be the
variables in the nodes found in the part of the join tree on A’s side of the link,
and UB those found in nodes on B’s side of the link. Prove that for any two
nodes A ∈UA \ S and B ∈UB \ S, we have that A and B are d-separated by
S.
Exercise 4.23. A directed acyclic graph is singly connected if the graph you
get by dropping the directions of the links is a tree (the graph in Figure 4.49
is singly connected).
(i) Prove that the moral graph of a singly connected graph is triangulated.
(Hint: If you successively eliminate a node with exactly one parent and
no children or with no parents and exactly one child, then the result is a
moral graph for a singly connected graph.)
(ii) Prove that the separators in a join tree for a singly connected graph consist
of exactly one node. (Hint: If the neighbors A and B share the neighbors
C and D, then C and D are neighbors.)
Exercise 4.24. Consider the Bayesian network in Exercise 4.21.
Indicate the potentials to communicate in a full lazy propagation with
evidence F = f, I = i, E = e.
Exercise 4.25. Expand the join tree in Figure 4.16 to a junction tree, and
add the potentials deﬁned by the domain graph in Figure 4.14 to suitable
cliques. Which messages are sent if evidence is collected to node CG?
Exercise 4.26. Consider the Bayesian network in Figure 4.50.

Exercises
163
A
B
C
D
E
F
G
H
Fig. 4.50. The Bayesian network for Exercise 4.26.
(i) Construct a junction tree.
(ii) Indicate the potentials to communicate in a full lazy propagation without
evidence.
(iii) Indicate the potentials to communicate with evidence D = d and H = h.
Exercise 4.27. Prove Proposition 4.5. (Hint: Assume a deadlock (no trig-
gered nodes).)
Exercise 4.28. Show that any asynchronous full order of message pass-
ing corresponds to a CollectEvidence(R) followed by a DistributeEvi-
dence(R) for some node R. (Hint: Look at the ﬁrst node that receives all its
messages.)
Exercise 4.29. Triangulate the domain graph from Exercise 4.6.
Exercise 4.30.
(i) Construct a junction tree for the Bayesian network in Figure 4.51 by using
the elimination order F, J, B, A, I, K, E.
(ii) The numbers inside the nodes indicate the number of states. Use the
heuristics from Section 4.6.1 to construct a junction tree.
Exercise 4.31. What is the moral graph of the Bayesian network in Fig-
ure 4.44? Assuming that each node has 10 states, use the heuristics following
Deﬁnition 4.8 to triangulate the graph. Would the result be the same if each
node had 2 states instead?
Exercise 4.32. Consider the Bayesian network in Figure 4.29, and let the
number of states be as listed in Section 4.6.1. Find a better triangulation
than the one obtained by using the heuristics from Section 4.6.1.

164
4 Belief Updating in Bayesian Networks
A 2
B 2
C 2
D 3
E 3
F 5
G 2
H 3
I 2
J 5
K 4
Fig. 4.51. The Bayesian network for Exercise 4.30.
Exercise 4.33. (Conditioning) Propagation methods for DAGs without mul-
tiple paths have existed for a long time. A propagation method for multiply
connected DAGs consists in reducing a DAG to a set of singly connected
DAGs.
(i) Consider the DAG (a) in Figure 4.52 with P(A), P(B | A), P(C | A), and
P(D | B, C) given. Assume that A = a. Show that the DAG is reduced to
the DAG (b) with P(B, a), P(C, a), and P(D | B, C) given. (Hint: Use the
chain rule.. Calculate P(B, a) and P(C, a).
A
B
C
D
(a)
B
C
D
(b)
A
C
D
(c)
Fig. 4.52. Figures for Exercise 4.33(i)–(v).
(ii) Show that P(D, a) = 
B,C P(D | B, C)P(B, a)P(C, a).
(iii) Assume that for all states a of A we have a reduced DAG as in (i). Let
evidence e be entered and propagated in all the reduced DAGs, yielding
P(B, a, e), P(C, a, e), P(D, a, e) for all a. Calculate P(B, e) and P(A, e).
This procedure is called conditioning on A.

Exercises
165
(iv) Reduce the DAG by conditioning on B. Show that the tables are P(A, b),
P(C | A), and P(D | C, b).
(v) Show that conditioning on D does not result in a singly connected DAG.
Conditioning over several variables can be performed stepwise.
(vi) Determine a minimal set of conditioning variables for the DAG in Fig-
ure 4.53 to reduce it to singly connected DAGs.
A 2
B 2
C 4
D 3
E 5
F 2
G 2
Fig. 4.53. Figure for Exercise 4.33 (vi)–(vii).
(vii) The numbers attached to the variables indicate the number of states. De-
termine a conditioning resulting in a minimal number of singly connected
DAGs.
Exercise 4.34. Let C be the set of cliques from a triangulated graph. A pre-
J -tree is a tree over C with separators S = V ∩W for adjacent cliques V, W.
The weight of a pre-J -tree is the sum of the number of variables in the
separators.
(i) Prove that a join tree is a pre-J -tree of maximal weight.
(ii) Prove that any pre-J -tree of maximal weight is a join tree.
Exercise 4.35. (i) Consider the graph in Figure 4.35. Determine a triangu-
lation such that no clique contains more than four nodes.
(ii) Expand the model in Figure 4.34 to six time slices. Can this model be
triangulated such that no clique contains more than four nodes?
Exercise 4.36. Consider the Bayesian network in Figure 4.54, where each
variable is binary, with probabilities deﬁned as P(A = a1) = 0.1, P(B =
b1 | a1) = 0.1, P(B = b1 | a2) = 0.9, P(C = c1 | b1) = 0.1, P(C = c1 | b2) = 0.9,
P(D = d1 | c1) = 0.1, and P(D = d1 | c2) = 0.9. Using recursive conditioning,
calculate P(a1 | d1).
Exercise 4.37. Construct two time slices of the model in Figure 3.52. Using
recursive conditioning, what would a computation tree for calculating P(C2)
look like?

166
4 Belief Updating in Bayesian Networks
A
B
C
D
Fig. 4.54. A simple Bayesian network.
Exercise 4.38. Show that the worst case complexity of Algorithm 4.2 is O(n·
exp (wn)), and that the complexity for a balanced tree is O(nw+1).
Exercise 4.39. Calculate the marginals from the sample in Table 4.2 and
compare the result with the exact marginals.
Exercise 4.40. From the conﬁgurations in Table 4.2, estimate the following
probability distributions: P(A), P(A | D = n), and P(C, D | B = y, E = n).
Exercise 4.41. Does your software tool allow for sampling from a Bayesian
network model? Which kind of sampling technique is used?
Exercise 4.42. Using the sequence of random numbers in Table 4.41 generate
as many full samples as you can for the Bayesian network model given in
Figure 4.46, with conditional probabilities as deﬁned in Table 4.1 and evidence
B = n, using ﬁrst probabilistic logic sampling, then likelihood weighting,
then Gibbs sampling using sampling sequence A, C, D, E, and ﬁnally Gibbs
sampling using sampling sequence A, D, E, C.
1 0.80 5 0.33 9
0.55 13 0.14
2 0.19 6 0.08 10 0.71 14 0.42
3 0.85 7 0.52 11 0.06 15 0.32
4 0.28 8 0.65 12 0.78 16 0.11
Table 4.6. A sequence of random numbers in the interval [0, 1].
Exercise 4.43. The binary variables A and B are parents of the binary vari-
able C. We have P(A) = P(B) = (0.5, 0.5), and the conditional probability
table is an exclusive OR table (C = y if and only if exactly one of A and B
is in the state y). Show that Gibbs sampling on this structure will give either
P(C = y) = 1 or P(C = n) = 1.
Exercise 4.44. Given a Bayesian network over U with evidence e entered,
show that it is NP-hard to ﬁnd a conﬁguration U ∗such that P(U ∗, e) > 0.
(Hint: Look at Exercise 3.27.)

5
Analysis Tools for Bayesian Networks
The main reason for building a Bayesian network is to estimate the state of
certain variables given some evidence. In Chapter 4, we gave methods that
made it easy to access P(A | e) for any variable A. However, this may not
be suﬃcient. It may be crucial to establish the joint probability for a set of
variables. Section 5.2 gives a general method for calculating P(V | e) for any
set V of variables.
Another typical request is to ask for the most-probable conﬁguration. We
give a method for this in Section 5.3. Section 5.5 deals with methods for
analyzing whether the evidence entered to the network is coherent; for example
to trace ﬂawed data.
A very important tool for a decision support system is explanation: a tool
to explain to the user how the system came to its conclusions. A part of ex-
planation is sensitivity to evidence: how sensitive is the conclusion to (small)
changes in the evidence? Which parts of the evidence are crucial and/or suf-
ﬁcient for the conclusion? This is the subject of Section 5.6.
Finally, we present methods for analyzing how sensitive posterior proba-
bilities are to changes in the numbers speciﬁed in the model.
The procedures in this chapter are based on lazy propagation as presented
in Chapter 4, but most of them are also valid using other propagation methods.
In lazy propagation, you work with sets of potentials representing the product.
Often you will perform the product of the union of two sets of potentials. We
shall call this operation to “take the product of the two sets” and unless
necessary for the exposition, we do not bother whether this is done by taking
the union of the two sets or by actually taking all potentials in the two sets
and multiplying them together.

168
5 Analysis Tools for Bayesian Networks
5.1 IEJ Trees
Let eX be a ﬁnding of the form “only the states x′
1, . . . , x′
q of the variable X
are possible”. If you know P(X) then P(eX) is easy to calculate, namely as
the sum of the probabilities for the states declared possible.
We shall in several situations need P(e) for a set of ﬁndings e and therefore
we repeat Theorem 4.5 in condensed form.
Proposition 5.1. Let BN be a Bayesian network and let e = {e1, . . . , em} be
evidence. When e has been entered and a full propagation has been performed,
then P(e) can be calculated in the following way: take any separator S, mul-
tiply the two messages in the mail boxes (to get P(S, e)) and marginalize all
variables out of the product.
Proposition 5.1 can be used for more than calculation of probabilities of
evidence. Assume that some Bayesian network has received evidence e, and
we want to calculate the probability of the conﬁguration c = (A = a, B =
b, C = c) given e. That is, we want P(c|e). Proposition 5.1 yields P(e). If
now we enter c as further evidence and perform an extra propagation, then
Proposition 5.1 yields P(c, e), and the fundamental rule gives
P(c|e) = P(c, e)
P(e) .
This technique can for example solve the question from Section 3.2.4 with
the model in Figure 3.18: the sequence baaca is received; what is the proba-
bility that the transmitted word is baaba?
Sometimes we may want to calculate P(e′) for various subsets e′ ⊆e. To
do this we can work with two copies of the junction tree. In the ﬁrst copy
we have performed an initial propagation, and the appropriate messages are
placed in the mailboxes. In the second copy we have entered and propagated
evidence, and the messages from this propagation are stored in the mailboxes.
To be precise, we can work with junction trees in which the separators have
four mailboxes (See Figure 5.1). We call this kind of junction tree an IEJ tree
(for Initial-Evidence Junction).
The separator S in Figure 5.1 divides the evidence into two sets: the ev-
idence eV entered at the left of S and eW entered at the right of S. From
Proposition 5.1 we have that P(S) is the product of ΦV and ΦW , and P(S, e)
is the product of ΦV
e and ΦW
e . Now look at the pair (ΦV , ΦW
e ). This pair
is the pair of messages we would have, had we entered eW only. Therefore,
the product must be P(S, eW ), and we can easily calculate P(eW ) as well as
P(S | eW ). Similarly for P(S, eV ) and P(S | eV ).

5.2 Joint Probabilities and A-Saturated Junction Trees
169
V
W
eV
eW
ΦV
ΦW
ΦV
e
ΦW
e
S
→
←
Fig. 5.1. The separators in an IEJ tree contain four mailboxes, two for each direc-
tion. One of the mailboxes contains the message from the initial propagation, and
the other contains the message from a propagation of evidence.
5.2 Joint Probabilities and A-Saturated Junction Trees
When dealing with utility functions (see Chapter 9) over several variables
and in various other connections, we will be faced with a request for the joint
probability of several variables.
Take for example the stud farm example from Section 3.2 and the situation
in Figure 3.16, and assume that the farmer has to decide on a new mating
among the horses Fred, Dorothy, Eric and Gwenn. Which pair should be
chosen to minimize the risk of getting a carrier as oﬀspring?
If the set requested is a subset of a node in the junction tree, then you
have the joint distribution directly. If not, the technique from Section 5.1 can
be used by entering and propagating all conﬁgurations, but it is troublesome.
A better technique is to perform propagation without eliminating variables
from the requested set. This technique is called variable propagation.
Example 5.1. Assume that we request P(A, B, C, D, E) from the junction tree
in Figure 5.2.
Then collect to (DEH) and in the operations, do not marginalize A, B, and
C. In Figure 5.3, the functions communicated in the operation are indicated.
Note that the “sending” of functions does not mean that the functions are
moved. What is sent is a pointer to a table for the function, and since variable
propagation involves fewer marginalizations than normal propagation, it may
be faster. However, when ﬁnally the incoming messages are multiplied, we
have to work with a considerably larger domain.
5.2.1 A-Saturated Junction Trees
Sometimes a variable A may be of particular interest. It may be a hypoth-
esis variable, and you may be interested in investigating P(A | X) for many
diﬀerent variables X. You may enter each state of X and propagate, but it
requires one propagation for each state of each variable. Instead, you can make

170
5 Analysis Tools for Bayesian Networks
BF
AJK
JFI
GHI
CG
DEH
F
J
I
G
H
Fig. 5.2. A junction tree from which we request P(A, B, C, D, E).
GHI
h = P
F f1
P
J(g · f2)
f4
CG
f5
AJK
JFI
f1
P
I h P
G(f5 · f4)
g = P
K f3
f5
f3
DEH
f6
f1
BF
f2
Fig. 5.3. The messages passed in performing variable propagation for the calculation
of P(A, B, C, D, E). We assume that each clique holds one function (over its domain).
A present in the entire junction tree: perform a full propagation but do not
eliminate A. The result is called an A-saturated junction tree.
If W is a set of variables, we can do the same, and the result is a W-
saturated junction tree. The propagation in the example above is the Col-
lectEvidence part of establishing an (A, B, C, D, E)-saturated junction
tree. Notice that the work requested for establishing a W-saturated junc-
tion tree does not exceed the work required for a normal propagation. More
space may be required, though.
Proposition 5.2. Let T be a W-saturated junction tree with evidence e, and
let X be any variable. Then P(W | X, e) is calculated through the following
procedure
1. Choose any node V or separator S in T containing X.
2. P(V ∪W, e) is the product of V ’s set of potentials with the incoming
messages (P(S ∪W, e) is the product of the two messages in S).
3. P(W, X, e) = 
V \(W∪{X}) P(V ∪W, e).
4. P(X, e) = 
W P(W, X, e).

5.3 Conﬁguration of Maximal Probability
171
5. P(W | X, e) = P (W,X,e)
P (X,e) .
Note that in a W-saturated junction tree you can get P(W | X, e) for each
X through one local calculation. On the other hand, this local calculation is
more complex than in the case of a normal junction tree. In the extreme, W
may be very close to the universe, and the “local” calculation is extremely
demanding.
We shall later deal with W-saturated IEJ trees. They contain four messages
in each separator, and they can be used for easy calculations of P(W|e′) for
various subsets of the evidence.
5.3 Conﬁguration of Maximal Probability
In the example in Section 3.2.4 concerning transmission of symbol strings,
the immediate task is to ﬁnd out which symbol string most probably has
been transmitted. Using propagation of variables, the joint probabilities for
all possible strings can be calculated, and thereby the most-probable string
can be found. This may require an intractably large table. There is, however,
a much more eﬃcient method.
Example 5.2. Consider a small system consisting of the variables A, B, and C
with the joint probability determined by the conditional probabilities speciﬁed
in Table 5.1, and suppose that we want to ﬁnd out which conﬁguration of
(A, B, C) has maximal probability.
a1 a2
b1 0.6 0.2
b2 0.4 0.8
b1 b2
c1 0.2 0.7
c2 0.8 0.3
P(B|A)
P(C|B)
Table 5.1. Probability tables for a small system, P(A) = (0.4, 0.6).
Let us start calculating the probability α of the most-probable conﬁgura-
tion; α is the largest number in the joint probability table P(A, B, C):
α = max
A,B,C P(A, B, C) = max
A,B,C P(A)P(B|A)P(C|B)
= max
A

max
B (max
C (P(A)P(B|A)P(C|B)))

= max
A

max
B (P(A)P(B|A) max
C
P(C|B))

= max
A

P(A) max
B (P(B|A) max
C
P(C|B))

.

172
5 Analysis Tools for Bayesian Networks
In the equations above, we ﬁrst used the chain rule for Bayesian networks
and next the distributive law for the max operation.
So ﬁrst we determine maxC P(C|B). It is a potential over B, and from
Table 5.1 we get the potential (0.8, 0.7). Next, this potential is multiplied by
P(B|A) (see Table 5.2).
B \ A a1
a2
b1
0.48 0.16
b2
0.28 0.56
Table 5.2. P(B|A) maxC P(C|B).
When maximizing Table 5.2 over B, we get the potential (0.48, 0.56) over
A. It is multiplied by the prior distribution (0.4, 0.6), and we get (0.192, 0.336).
From this, we can conclude that the most-probable conﬁguration has proba-
bility 0.336, and the A-component of it must be a2.
To get the B-component, return to Table 5.2. Since we know that A = a2,
we have that the state of maximal value for B is b2. Actually, when the value
0.56 in Table 5.2 is multiplied by the prior, 0.6, for a2, we get the maximal
value 0.336. In the same way, the C-state is determined from P(C | b2) to c1.
Let U be the universe for a Bayesian network. The general task of de-
termining the conﬁguration of maximal probability is to determine the X-
component for each X ∈U. In fact, there may be several conﬁgurations of
maximal probability. We will leave this problem for a short while and assume
that there is exactly one conﬁguration of maximal probability. So the general
task is for each variable X to get the distribution resulting from maximizing
the remaining variables out. To help in the calculation, we have the following
result:
Proposition 5.3 (The distributive law for max).
max
Z
f(X, Y )g(Y, Z) = f(X, Y ) max
Z
g(Y, Z).
So the task is very similar to the task from Chapter 4 except that the
operation is “max” instead of “.” Since the distributive law holds for max
too, the propagation methods from Chapter 4 can be applied by substituting
“max” for “.” This is called max-propagation, and accordingly we may
use the term sum-propagation for the methods in Chapter 4. The result of
maximizing variables out of a function f is called a max-marginal of f.
Theorem 5.1. Let BN be a Bayesian network representing P(U), and let
T be a junction tree corresponding to BN. Let e be the evidence represented
by the functions {e1, . . . , em}, and assume that the evidence functions are
attached to appropriate nodes in the junction tree.
After a full round of (lazy) max-propagation in T we have

5.4 Axioms for Propagation in Junction Trees
173
i)
for each separator S, maxU\S P(U, e) is the product of the two messages
in S’s mailboxes;
ii) for each node V , maxU\V P(U, e) is the product of the potential set attached
to V and the incoming messages.
Proof.
Repeat the consideration from Chapter 5 with “max” instead of
“.” Since the potential sets attached to the nodes in the junction tree are
never changed, you can always change between max-propagation and sum-
propagation.
⊓⊔
Several Conﬁgurations of Maximal Probability
When there is exactly one conﬁguration of maximal probability, then for each
variable X we can read the component by taking the state of maximal proba-
bility in the max-marginalized distribution for X. However, if there are several
conﬁgurations of maximal probability, then for some variables {Y1, . . . , Ym}
there are several states of maximal probability in their max-marginalized dis-
tribution. Unfortunately, it does not hold that all combinations of these max-
probable states form a conﬁguration of maximal probability. If you request
one of them, you can enter a max-probable state as evidence and perform a
new max-propagation. If still there are several max-probable states in some of
the remaining variables, you can repeat this operation until all variables have
only one max-probable conﬁguration.
Working with Subsets of Variables
If the evidence variables E and the query variables Q do not constitute all
the variables in the Bayesian network, then the above procedure cannot be
applied. The problem is that since we are interested only in Q, the remaining
variables U \ (Q, E) should be marginalized out by summation before we do
the maximization:
max
Q P(Q | e) = max
Q

U\(Q,E)
P(U | e).
The result is the maximum posterior probability (MAP) over the query
variables and the associated conﬁguration is called a MAP conﬁguration. Un-
fortunately, the constraint on the elimination ordering makes it much more
diﬃcult to work with MAP problems than MPE problems.
5.4 Axioms for Propagation in Junction Trees
As shown in Section 5.3, the propagation algorithm can be used for other types
of tasks than probability updating. Therefore, a general framework and a set of

174
5 Analysis Tools for Bayesian Networks
axioms for propagation in junction trees have been established. The framework
and the axioms look very much like the properties listed in Section 1.4, and
we shall state them in a more general form here.
We have a set ϑ of valuations. Each v ∈ϑ has a set dom (v) ⊆U attached.
The set U is called the universe. Valuations can be combined through a binary
operation ⊗, and for each V ⊆U there is a projection operator v↓V .
Axioms
1. dom (v1 ⊗v2) ⊆dom (v1) ∪dom (v2),
2. dom

v↓V 
⊆V ,
3. Combination is associative: (v1 ⊗v2) ⊗v3 = v1 ⊗(v2 ⊗v3),
4. Combination is commutative: v1 ⊗v2 = v2 ⊗v1,
5. (v↓V )↓W = v↓V ∩W ,
6. The distributive law: If dom (v1) ⊆V then (v1 ⊗v2)↓V = v1 ⊗(v2)↓V ,
7. v↓∅is a neutral element with respect to combination, and it is denoted by
1.
The axiom 7 is not needed, but it is customary to include it as an assump-
tion.
With respect to probability updating in Bayesian networks, combination
corresponds to multiplication, and projection corresponds to marginalizing
out (see Section 1.4). Since the expression v↓V∩W is symmetric in V and W,
axiom 5 includes the property that marginalization is commutative.
In the case of determining the most-probable conﬁguration, projection
corresponds to maximizing out.
If you have a valuation framework satisfying the axioms above, you can
calculate (
i vi)↓X for all X ∈U through junction tree propagation. We shall
not prove it here, but the interested reader may reread Chapter 4 and check
that only the axioms above are used.
5.5 Data Conﬂict
A Bayesian network represents a closed world with a ﬁnite set of variables
and causal relations. The causal relations are not universal but reﬂect rela-
tions under certain constraints. Take for example a diagnostic system that on
the basis of blood analysis monitors pregnancy. Only diseases and relations
relevant for pregnant women are represented in the model. So if the blood
originates from a man, the case is not covered by the model. It may happen
that ﬁndings from male blood are impossible given the model. If so, the in-
consistency is easy to detect: the probability of the evidence is 0. However,
most often a set of ﬁndings is possible in the given model, and the system
will not object to it. It will yield posterior probability distributions that may
look rather harmless. The same also happens if test results are ﬂawed. In a
diagnostic situation, a single ﬂawed test result may turn the investigation in

5.5 Data Conﬂict
175
a completely wrong direction (such ﬂawed pieces of information are called red
herrings.
5.5.1 Insemination
Consider the insemination example from Section 3.1.3, and assume that the
farmer also has a scanning test. The model is given in Figure 5.4. To make
things easy, assume that all tests have 2% false positives as well as false
negatives (the prior for Pr is (0.87, 0.13)).
Pr
BT
UT
Sc
Ho
Fig. 5.4. Insemination extended with a scanning test.
Assume that we get the evidence UT = n and Sc = n but BT = y.
From our knowledge of the network model, we would say that the ﬁndings
are in conﬂict. However, a propagation of the evidence does not disclose it.
The posterior probabilities for Pr are (0.12, 0.88). Since the test results can
coexist, we may be facing a rare case, but it may also be the case that the
blood test is ﬂawed or that the case is not covered by the model (a bull may
have sneaked into the laboratory). We do not really have tools to distinguish
between these situations, but it would be good to have a tool that gives a
warning, “it seems that the evidence is conﬂicting.”
5.5.2 The Conﬂict Measure conf
Several approaches for analyzing data for conﬂicts have been developed. We
shall in this section present a measure that requires only two propagations
and that gives an indication of a possible conﬂict. The idea behind the mea-
sure is that correct ﬁndings originating from a coherent case covered by the
model conform to certain expected patterns laid down in the model. In other
words, the ﬁndings should be positively correlated (see also Section 3.4.3). If
e = {e1, . . . , em} is a set of ﬁndings, we would expect P(e) to exceed the prob-
ability for independent ﬁndings: P(e1) · · · P(em). Hence we deﬁne the conﬂict
measure as

176
5 Analysis Tools for Bayesian Networks
conf({e1, . . . , em}) = log2
P(e1) · · · P(em)
P(e)
.
The reason for the log2 is sheer convenience; some formulas look nicer.
A positive conf(e) is an indicator of a possible conﬂict. For the insemination
case, the conf-value is 3.1.
To get the required probabilities, you start performing a propagation with-
out evidence entered. From this, you get P(X) for all X in U. If ei is a ﬁnding
on X, then P(ei) can be calculated from P(X) as explained in Section 5.1.
To compute P(e), you use Proposition 5.1.
5.5.3 Conﬂict or Rare Case
It may happen that typical data from a very rare case causes a high conf-
value. In the insemination case, a very rare blood type may have the eﬀect of
always causing BT to give a positive result.
Pr
BT
UT
Sc
Ho
B-t
B-tT
Fig. 5.5. A rare blood type (frequency 0.001) causes BT to always give a positive
test result. Here B-tT is a test for blood-type with 0.1% false positives and negatives.
By extending the model in Figure 5.4 to take the blood type into account,
we get the model in Figure 5.5. For this extended model we still get conf(UT =
n, Sc = n, BT = y) = 3.1, indicating a possible conﬂict. The reason is that
though the evidence is perfectly coherent for a cow with this particular blood
type, it is very rare. Now assume that the blood-type test gives the result y.
This resolves the conﬂict, since conf of the new set of evidence is -1.34.
The problem above calls for a method for pointing out whether a positive
conf-value may be explained as a rare case covered by the model.
Let e = {e1, . . . , em} be ﬁndings for which conf(e) > 0, and let h be a
hypothesis that could explain the ﬁndings, conf({e1, . . . , em, h}) ≤0.
We have
conf({e1, . . . , em, h}) = log2
P(e1) · · · P(em)P(h)
P(e, h)
= conf(e) + log2
P(h)
P(h | e).

5.5 Data Conﬂict
177
This means that if
log2
P(h | e)
P(h)
≥conf(e),
(5.1)
then h can explain away the conﬂict. In the blood example the value of the
left hand side of (5.1) is 5.4 with h = “B-t = y”.
The fraction P (h | e)
P (h)
is used in various ways, and it is called the normalized
likelihood. Note that by the fundamental rule
P(h | e)
P(h)
= P(e | h)
P(e) .
Normalized likelihoods can be monitored automatically for all variables.
Therefore, in analyzing for conﬂict/rare case, it is easy to detect whether a
conﬂict may be due to a particular variable being in a very rare state.
5.5.4 Tracing of Conﬂicts
After the conﬂict measure has been found positive, a further task would then
be to ﬁnd out whether a possible conﬂict is due to ﬂawed ﬁndings, and if so,
to trace them.
Let us return to the insemination problem with evidence e = {eS = “Sc =
n”, eU = “UT = n”, eB = “BT = y”}. We have conf(e) = 3.1. We want to
trace the origin of the conﬂict.
The evidence e is in the network communicated to Pr in two sets, e′ =
{eB, eU} and e′′ = {eS}. A further investigation could therefore be to see
whether e′ contains an internal conﬂict. To do that we need P(e′), which is
0.0196. We get
conf(e′) = 3.16,
and not surprisingly a conﬂict is detected in e′.
Another possibility could be that the two sets e′ and e′′ are conﬂicting.
We deﬁne
conf(e′, e′′) = log2
P(e′)P(e′′)
P(e)
= −0.001,
which indicates that the two sets of ﬁndings are not conﬂicting, and we con-
clude that e′ is ﬂawed.
To deal with tracing of conﬂicts, we can use IEJ trees. Using an IEJ tree,
we can easily calculate the local conﬂict (see Figure 5.1)
conf(eV , eW ) = log2
P(eV )P(eW )
P(e)
.
The local conﬂict is a measure of whether the two sets of evidence eV and
eW are in conﬂict.
We can also calculate the partial conﬂicts conf(eV ) and conf(eW ). The
partial conﬂicts give an indication of possible internal conﬂicts in the sets eV
and eW .

178
5 Analysis Tools for Bayesian Networks
For each separator, we can get the probability of the evidence entered
to the left and the probability of the evidence entered to the right. We can
calculate the local and partial conﬂicts, and they are used for tracing the
origin of the global conﬂict.
Using the IEJ tree in Figure 5.6, we calculate the following local and inter-
nal conﬂicts: conf(eB, eU) = 3.16, conf(eB, eS) = 2.55, conf(eS, eU) = −1.93,
conf({eB, eU}, eS) = −0.001, conf({eB, eS}, eU) = 0.615, conf({eS, eU}, eB) =
5.1.
∅
Ho, UT
Ho, BT
Pr, Sc
Pr, Ho
↑
↑
↓
↓
{eS, eU }
eS
eB
↓
↑
{eS, eB}
eU
{eB, eU }
∅
∅
∅
∅
∅
Fig. 5.6. The IEJ tree for the insemination example. The various sets of evidence
held are indicated in the mailboxes.
These conﬂict measures point clearly at eB as the dubious ﬁnding.
To round oﬀthis section, we give the following proposition, which relates
the three kinds of conﬂicts.
Proposition 5.4. The global conﬂict conf(e) is the sum of a local conﬂict and
partial conﬂicts.
conf(e) = conf(eV , eW ) + conf(eV ) + conf(eW ).
Proof.
Exercise 5.8.
⊓⊔

5.6 SE Analysis
179
5.5.5 Other Approaches to Conﬂict Detection
The conf measure is not the only way of dealing with conﬂict detection. An-
other approach to the problem would be to incorporate sources of surprise
directly in the model. This can be done by entering variables modeling prob-
abilities for malfunctioning of sensors, and to extend causal variables such
as disease variables with the state other. This approach, however, has the
problem that it is diﬃcult to model malfunctions or other unless the types
of malfunction and other are known. Also, with other you can handle only
discrepancies that are local in the network.
Another approach is to calculate a so-called surprise index for the set
of ﬁndings. If the ﬁndings e are statements on the variables A, . . . , B, the
surprise index is the sum of probabilities for all conﬁgurations of (A, . . . , B)
with a probability no higher than P(e). If the surprise index is less than 0.1,
this should be an indication of a possible conﬂict. In the insemination case,
the surprise index is 0.06. Unfortunately, the calculation of a surprise index is
exponential in the number of ﬁndings, and it must be considered intractable
in general.
5.6 SE Analysis
Evidence e has been entered into a Bayesian network, and some hypotheses
h1, . . . , hn are the focus of interest. Sensitivity analysis to evidence will give
answers to questions like
−what evidence is in favor of/against/irrelevant for hi?
−what evidence discriminates hi from hj?
We shall call this kind of analysis SE analysis.
5.6.1 Example and Deﬁnitions
The following example is used for illustration.
In the morning when Mr Holmes leaves his house, he realizes that
his lawn is wet. He wonders whether it has rained during the night or
whether he has forgotten to turn oﬀhis sprinkler. He looks at the lawn
of his neighbors, Dr Watson and Mrs Gibbon. Both lawns are dry, and
he concludes that he must have forgotten to turn oﬀhis sprinkler.
The network for Holmes’ reasoning is shown in Figure 5.7, and the initial
probabilities are given in Table 5.3.
The evidence e consists of the three ﬁndings eH, eW , eG, and the hypothesis
in focus is hs : “S = y”. We have P(hs) = 0.1 and P(hs | e) = 0.9999.

180
5 Analysis Tools for Bayesian Networks
R
S
G
W
H
Fig. 5.7. Network for the wet lawn example. Holmes can inspect both Watson’s
and Mrs Gibbon’s lawns.
R = y R = n
G = y
0.99
0.1
G = n
0.01
0.9
R = y
R = n
S = y
(1, 0)
(0.9, 0.1)
S = n (0.99, 0.01)
(0, 1)
P(G | R) = P(W | R)
P(H | R, S)
Table 5.3. Tables for the wet lawn example. P(R) = (0.1, 0.9) = P(S).
We have P(hs | eH) = 0.51, P(hs | eW ) = 0.1 = P(hs | eG).1 So neither eW
nor eG alone has any impact on the hypothesis, but eH is also not suﬃcient
for the conclusion. Therefore, the immediate conclusion that eW and eG are
irrelevant for the hypothesis is not correct, and we must conclude that evidence
in combination may have a larger impact than the “sum” of the individual
impacts.
To investigate further, we must consider the impact of subsets of the evi-
dence. We have
P(hs | eW , eG) = 0.1, P(hs | eH, eG) = 0.988 = P(hs | eW , eH).
To relate the probabilities above to their impact on the hypothesis, we can
divide them by the prior probability P(hs) to get the normalized likelihood.
Other measures can be used, for example Bayes’ factor
P(e | h)
P(e | ¬h),
or the fraction of achieved probability
P(h | e′)
P(h | e) .
The various normalized likelihoods are given in Table 5.4.
From Table 5.4 we can conclude that no single ﬁnding is suﬃcient for
the conclusion. Also, though (eW , eG) alone has no impact on hs, these two
1 A d-separation analysis could yield some of the results. However, this is not the
point here.

5.6 SE Analysis
181
W = n G = n H = y P (hs | e)
P (hs)
1
1
1
9.999
1
1
0
1
1
0
1
9.88
1
0
0
1
0
1
1
9.88
0
1
0
1
0
0
1
5.1
0
0
0
1
Table 5.4. Normalized likelihoods for the subsets in the example. A ”1” in the table
indicates that the ﬁnding is an element of e′.
ﬁndings cannot both be removed. Moreover, we see that the subsets (eH, eG)
and (eH, eW ) can account for almost all the change in the probability for hs.
Deﬁnition 5.1. Let e be evidence and h a hypothesis. Suppose that we want
to investigate how sensitive the result P(h | e) is to the particular set e. We
have that e′ ⊆e is suﬃcient if P(h | e) is almost equal to P(h | e′). We then
also say that e \ e′ is redundant evidence.
The term almost equal can be made precise by selecting a threshold θ1 and
require that
 P (h | e′)
P (h | e) −1
 < θ1. Note that P (h | e′)
P (h | e) is the fraction between the
two normalized likelihood ratios.
•
e′ is minimal suﬃcient if it is suﬃcient, but no proper subset of e′ is so.
•
e′ is crucial evidence if it is a subset of any suﬃcient set.
•
e′ is important evidence if the probability of h changes too much without
it, to be more precise, if
 P (h | e\e′)
P (h | e)
−1
 > θ2, where θ2 is some chosen
threshold.
In the example above put θ2 = 0.2, θ1 = 0.05. Then (eH, eG) and (eH, eW )
are minimal suﬃcient, (eW , eG) is important, and eH is crucial.
In Holmes’s universe, there is another possible hypothesis, namely hr :“R =
y”. To ﬁnd out which ﬁndings discriminate between the two hypotheses, an
analysis of hr can be performed. The probability P(hr | e′) is calculated for
each subset of e′, and the ratio between the two (normalized) likelihoods is
used. The ratios are shown in Table 5.5.
Table 5.5 shows that eW and eG are good discriminators between the two
hypotheses.
As illustrated above, the heart of sensitivity analysis is the calculation of
P(h | e′) for each e′ ⊆e. Since the number of subsets grows exponentially
with the number of ﬁndings, the job may become very heavy, particularly
when P(h | e′) has to be calculated through a propagation in a large network.

182
5 Analysis Tools for Bayesian Networks
W = n G = n H = y P (e′ | hs)
P (e′ | hr)
1
1
1
6622
1
1
0
7300
1
0
1
74
1
0
0
81
0
1
1
74
0
1
0
81
0
0
1
0.92
0
0
0
1
Table 5.5. Likelihood ratios for the hypotheses hs and hr.
Note that when P(h | e′) and P(h) are available, then also Bayes’ factors
can be calculated:
P(e′ | h)
P(e′ | ¬h) = P(h | e′)P(¬h)
P(h)P(¬h | e′) = P(h | e′)(1 −P(h))
P(h)(1 −P(h | e′)).
5.6.2 h-Saturated Junction Trees and SE Analysis
A-saturated junction trees – sometimes extended to IEJ trees – can be of
great help for SE analysis. If a particular state h of the hypothesis variable H
is a focus of interest, another type of junction tree will suﬃce. Let e be the
evidence. After propagating e we insert H = h in an appropriate node R and
perform a DistributeEvidence from R. The messages from this propagation
are stored in the separators too (see Figure 5.8). We call this type of junction
tree an h-saturated junction tree.
V
W
h, eW
eV
eW
→
←
Fig. 5.8. Part of an h-saturated junction tree, where the hypothesis H = h is
entered to the right. The evidence handled is indicated.
The speciﬁc approach to SE analysis is much dependent on the type of
hypothesis, the type and size of the evidence, the topology of the network, etc.,
and below we shall only give some hints on how the tasks may be approached.

5.6 SE Analysis
183
What-If?
Assume that we want to investigate the impact on H if the ﬁnding eX is
removed or changed to e′
X.
If you have a single state h in focus, you can use an h-saturated junction
tree. Go to the node V , where eX is placed. Local to V you have messages
for all evidence, and substituting eX with e′
X (e′
X may be empty) will give
you P(e \ {eX} ∪{e′
X}). You also have messages involving e together with
“H = h”. Substituting eX with e′
X will give you P(e \ {eX} ∪{e′
X}, h). From
this you get P(H = h | e \ {eX} ∪{e′
X}). The same H-saturated junction
tree can be used for all ﬁndings. What-if? analysis can, for example, sort out
redundant ﬁndings, and it can also be used to determine the ﬁndings acting
for or against h.
Note that this technique also allows you to investigate the eﬀect of evidence
on a variable for which you have not yet received evidence.
Crucial Findings
Assume that P(h | e) is high, and we want to determine the set of crucial
ﬁndings.
Use an h-saturated junction tree. It may happen that some ﬁndings are
evidence against h, but they are overwritten by the entire set. We assume that
ﬁndings acting against h have been sorted out (for example through What-if?
analysis as above).
For the remaining evidence we assume monotonicity: no insuﬃcient set
contains a suﬃcient subset.
Then eX is crucial if and only if e \ {eX} is not suﬃcient. Using an h-
saturated junction tree, it is easy to determine the crucial ﬁndings.
Minimal Suﬃcient Sets
It will be natural to continue the procedure above and repeatedly remove
ﬁndings from suﬃcient sets. However, h-saturated junction trees only allow
you to remove ﬁndings inserted in the same node in the junction tree. If they
are inserted in diﬀerent nodes, new propagations are required.
An h-saturated IEJ tree can speed up the search using ﬁve mailboxes for
each separator (see Figure 5.9). An h-saturated junction tree gives you access
to P(h | e′) for a large family of subsets e′ ⊂e (see Table 5.6). From this
family you choose the minimal suﬃcient subsets and continue the search for
each of them by establishing a new h-saturated IEJ tree.
As described in Section 5.2.1, the separators can be used to obtain P(h | e′)
for the sets e′ “communicated” to them. A similar procedure can be used for
the nodes in the junction tree. Take for example the node V in Figure 5.9.
By selecting appropriate messages from the neighbors, we can handle any
union of sets communicated to a separator. This yields a way of calculating
for example P(h | q, t). A full list is given in Table 5.6. Note that some subsets
are not in the list, for example {t, y}.

184
5 Analysis Tools for Bayesian Networks
z
h
q
h, e \ {q}
e \ {q}
V
s
h, e \ {s}
s
x, y
x
y
h, e \ {y}
t
e \ {t}
h, e \ {t}
t
→
←
→
←
→
←
→
←
→
e \ {s}
h, e \ {x, y}
←
y
q
∅
∅
∅
∅
∅
∅
e \ {x, y}
∅
∅
∅
∅
e \ {y}
Fig. 5.9. An h-saturated IEJ tree. The evidence “communicated” is indicated in
the separators. It is assumed that h is inserted in V . The subsets of the evidence
accessed are listed in Table 5.6.
e
∅
{t}
e \ {t}
{s}
e \ {s}
{q}
e \ {q}
{y}
e \ {y}
{x}
e \ {x}
{z}
e \ {z}
{x, y}
e \ {x, y} {z, t}
e \ {z, t}
{z, s}
e \ {z, s} {z, q}
e \ {z, q} {t, s}
e \ {t, s}
{t, q}
e \ {t, q} {s, q}
e \ {s, q} {x, y}
e \ {x, y}
{z, x, y} {t, s, q}
{x, y, q} {z, t, s}
{x, y, s} {z, t, q}
{x, y, t} {z, s, q}
Table 5.6. A list of sets of evidence e′ for which the h-saturated IEJ tree in Fig-
ure 5.8 yields P(h | e′) through a local computation.
5.7 Sensitivity to Parameters
We have a Bayesian network BN with evidence e. Assume that we have a
single hypothesis variable H, and let a particular state h of H be in focus of
interest. Let t be a set of parameters for BN (a parameter is an entry in a
conditional probability table). We are interested in how P(h | e) varies with t.
We must make clear what is meant by “the probabilities are functions of
the parameters.” Let A be a binary variable, and let π be a conﬁguration of
A’s parents pa(A). Then, t = P(A = a | π) is a parameter, but consequently
we have P(A = ¬a | π) = 1 −t, and it covaries with t. If A has more than two

5.7 Sensitivity to Parameters
185
states, we assume proportional scaling: the remaining probabilities are scaled
by the same factor. If A has n states, and a1 is a parameterized state, we
assume that P(A | π) = (t, (1 −t)x2, . . . , (1 −t)xn), where  xi = 1.
It is possible to deal with several parameters in the same distribution. If, for
example, the ﬁrst two states are parameterized, we would require P(A | π) =
(t, s, (1−t−s)x3, . . . , (1−t−s)xn). Then, s does not scale when t is changed.
In the following, we assume proportional scaling, and we also assume that
there is at most one parameter per distribution.
Theorem 5.2. Let BN be a Bayesian network over the universe U. Let t be a
parameter and let e be evidence entered in BN. Then, assuming proportional
scaling, we have
P(e)(t) = αt + β,
where α and β are real numbers.
Before proving Theorem 5.2 we need a lemma.
Lemma 5.1. Let φ(V ) be a potential over the variables V. Let A ∈V and let
v∗be a conﬁguration over V \ {A}. Let all entries be real-valued except for
φ(A, v∗), which has the form (α1t + β1, . . . , αkt + βk). Then

V
φ(V ) = αt + β,
where α and β are real numbers.
Proof.
Let us ﬁrst look at the example in Table 5.7. To calculate 
V φ(A, B,
C), take ﬁrst the sum of all numbers in the entries with B ̸= b2 and C ̸= c2.
The result is 56. Finally, add the expressions in the (b2, c2)-entry, and you get
4t + 57.
B
b1
b2
b3
C c1 (1, 2, 3)
(2, 4, 7)
(4, 1, 2)
c2 (5, 2, 1) (t + 1, −2t + 2, 5t −2) (1, 1, 1)
c3 (2, 2, 1)
(3, 1, 4)
(2, 2, 2)
Table 5.7. φ(A, B, C).
In general, let V ∗be the set of all conﬁgurations in sp(V) except for the
(A, v∗)-conﬁgurations. Then

V
φ(V ) =

V ∗
φ(V ) +

A
φ(A, v∗).

186
5 Analysis Tools for Bayesian Networks
The ﬁrst term is a real number β∗, and the second is (α1t + β1) + · · · +
(αkt + βk). Hence

V
φ(V ) =

i
αi

t +

i
βi

+ β∗.
⊓⊔
Proof.
We prove Theorem 5.2. Let U = {A} ∪{A1, . . . , An}, fa(A) = {A} ∪
pa(A) and let π be a parent conﬁguration for which
P(A | π) = (t, γ2(1 −t), . . . , γk(1 −t)).
Without loss of generality, assume that the parameter t is attached to the
ﬁrst state of A. Let the evidence potentials be e1, . . . , em.
Now
P(e) =

U
P(U, e) =

U
P(A | pa(A))
	
i
P(Ai | pa(Ai))
	
j
ej
=

fa(A)
P(A | pa(A))

U\fa(A)
	
i
P(Ai | pa(Ai))
	
j
ej.
The factor 
U\fa(A)

i P(Ai | pa(Ai)) 
j ej is a potential, φ(fa(A)), with
only real-numbered values, and we have
P(e) =

fa(A)
P(A | pa(A))φ(fa(A)).
The product P(A | pa(A))φ(fa(A)) is a potential satisfying the conditions
in Lemma 5.1, and we can conclude that
P(e) = αt + β.
⊓⊔
Notation: Let t = (t1, . . . , tm) be a set of parameters, and let pol(t) be
a polynomial over t. The polynomial pol(t) is said to be multilinear if all
exponents in the expression are of degree at most 1. If so, it has a term for
each subset of t.
Corollary 5.1. Let BN be a Bayesian network over the universe U. Let t be
a set of parameters for diﬀerent distributions and let e be evidence entered to
BN. Then, assuming proportional scaling, P(e)(t) is a multilinear polynomial
over t.

5.7 Sensitivity to Parameters
187
Proof.
For the sake of notational convenience, let t = (x, y). From Theo-
rem 5.2 we have
P(e)(x, y) = αx(y)x + βx(y) = αy(x)y + βy(x).
Inserting x = 0 yields
β0(y) = αy(0)y + βy(0).
(5.2)
That is, βx(y) is a linear function.
Inserting x = 1 yields
α1(y) + β1(y) = αy(1)y + βy(1).
Using Formula 5.2 we get
αx(y) = αy(1)y + βy(1) −αy(0)y −βy(0)
(5.3)
= (αy(1) −αy(0))y + βy(1) −βy(0).
That is, αx(y) is a linear function. Combining Formula 5.2 and Formula 5.3
we get
P(e)(x, y) = ((αy(1) −αy(0))y + βy(1) −βy(0))x + αy(0)y + βy(0),
which is of the form αxy + βx + γy + δ.
If we have more than two parameters, we let t = (x, y), where y is a set of
parameters. The reasoning above then yields that βx(y) and αx(y) are multi-
linear polynomials over y, and we repeat the arguments on βx(y) and αx(y). ⊓⊔
Corollary 5.2. Let BN be a Bayesian network over the universe U. Let t be
a set of parameters for diﬀerent distributions. Let a be a state of A ∈U and
let e be evidence. Then P(a | e)(t) is a fraction of two multilinear polynomials
over t.
Proof.
Follows from Corollary 5.1 and P(a | e) = P (a,e)
P (e) .
⊓⊔
5.7.1 One-Way Sensitivity Analysis
Let e be evidence, h a state of a hypothesis variable H, and s a parameter
for the Bayesian network. We wish to establish P(h | e) as a function of s.
Corollary 5.2 yields that P(h | e) has the form
P(h | e) = αs + β
as + b = P(h, e)
P(e) .

188
5 Analysis Tools for Bayesian Networks
To determine the four constants you can enter two diﬀerent values, s0 and s1,
for the parameter and then propagate. You then get P(h, e)(s1), P(h, e)(s0),
P(e)(s1), and P(e)(s0):
αs0 + β = P(h, e)(s0),
αs1 + β = P(h, e)(s1),
as0 + b = P(e)(s0),
as1 + b = P(e)(s1),
and from the four equations you can determine P(h | e) as a function of s.
Another way of determining the coeﬃcients is to establish an h-saturated
junction tree. Now, consider a clique with the table for the parameter attached.
In this clique (and its neighboring separators) you have all the information
needed to calculate P(h, e) and P(e) for diﬀerent values of s. This method
can easily be extended to one-way sensitivity analysis for several variables.
Note that when you have established the h-saturated junction tree, then
you can perform one-way sensitivity analysis for any parameter you wish, by
looking at a clique and its neighbor separators.
5.7.2 Two-Way Sensitivity Analysis
Let s and u be two parameters. Then P(e)(s, r) = αsr+βs+γr+δ, and we wish
to determine the coeﬃcients. From the propagation described in Section 5.7.1,
we have the value of P(e)(s0, r0). By working locally in the clique containing s,
we get the values of P(e)(0, r0) and P(e)(1, r0), and by working locally in the
clique containing r, we get P(e)(s0, 0) and P(e)(s0, 1). That is, we have ﬁve
equations with four unknowns, and we can determine (α, β, γ, δ), provided we
can pick four equations with an invertible coeﬃcient matrix. Unfortunately,
the equations are of rank 3, and we need extra information. By entering a new
value s1 and propagating, we get suﬃcient information to locally compute all
two-way sensitivity analyses involving s.
To calculate three-way sensitivity analysis is much more demanding, and
the number of propagations grows exponentially with the number of param-
eters considered. The complexity of the local computations also increases ex-
ponentially. We shall not treat this further.
5.8 Summary
W-Saturated Junction Trees
To calculate the joint probability over a set of variables W, you can either
perform a propagation for each conﬁguration of W or you can establish a
W-saturated junction tree (the junction tree obtained by performing a full
propagation without eliminating the variables in W).
Let T be a W-saturated junction tree with evidence e, and let X be any
variable. Then P(W | X, e) is calculated through the following procedure:

5.8 Summary
189
1. Choose any node V or separator S in T containing X.
2. P(V ∪W, e) is the product of V ’s set of potentials with the incoming
messages (P(S ∪W, e) is the product of the two messages).
3. P(W, X, e) = 
V \(W∪{X}) P(V ∪W, e).
4. P(X, e) = 
W P(W, X, e).
5. P(W | X, e) = P (W,X,e)
P (X,e) .
Finding the Most Probable Explanation (MPE)
The distributive law for max:
max
Z
f(X, Y )g(Y, Z) = f(X, Y ) max
Z
g(Y, Z).
Max propagation corresponds to standard (lazy) junction tree propagation,
where marginalizations are performed using the max-operator rather than
the -operator.
Let BN be a Bayesian network representing P(U), and let T be a junction
tree corresponding to BN. Let e be the evidence represented by the func-
tions {e1, . . . , em}, and assume that the evidence functions are attached to
appropriate nodes in the junction tree.
After a full round of (lazy) max-propagation in T we have
1. for each separator S, maxU\S P(U, e) is the product of the two messages
in S’s mailboxes;
2. for each node V , maxU\V P(U, e) is the product of the potential set at-
tached to V and the incoming messages.
Axioms for probability updating
1. dom (v1 ⊗v2) ⊆dom (v1) ∪dom (v2),
2. dom

v↓V 
⊆V ,
3. Combination is associative: (v1 ⊗v2) ⊗v3 = v1 ⊗(v2 ⊗v3),
4. Combination is commutative: v1 ⊗v2 = v2 ⊗v1,
5. (v↓V )↓W = v↓V ∩W ,
6. The distributive law: If dom (v1) ⊆V then (v1 ⊗v2)↓V = v1 ⊗(v2)↓V ,
7. v↓∅is a neutral element with respect to combination, and it is denoted by
1.
Data conﬂict
Conﬂict measure: To measure how well the evidence ﬁts the model, you can
use the conﬂict measure
conf({e1, . . . , em}) = log2
P(e1) · · · P(em)
P(e)
.

190
5 Analysis Tools for Bayesian Networks
Conﬂict or rare case: If
log2
P(h | e)
P(h)
≥conf(e),
then the hypothesis h can explain the conﬂict (the conﬂict is due to e being
a rare conﬁguration).
Sensitivity to Evidence
Let e be evidence and h a hypothesis. Suppose that we want to investigate
how sensitive the result P(h | e) is to the particular set e. We say that e′ ⊆e
is suﬃcient if P(h | e) is almost equal to P(h | e′). We then also say that e \ e′
is redundant.
The term almost equal can be made precise by selecting a threshold θ1
and require that
 P (h | e′)
P (h | e) −1
 < θ1. Note that P (h | e′)
P (h | e) is the fraction between
the two likelihood ratios.
•
e′ is minimal suﬃcient if it is suﬃcient, but no proper subset of e′ is so.
•
e′ is crucial evidence if it is a subset of any suﬃcient set.
•
e′ is important evidence if the probability of h changes too much without
it, to be more precise, if
 P (h | e\e′)
P (h | e)
−1
 > θ2, where θ2 is some chosen
threshold.
One can use h-saturated junction trees to ﬁnd the minimal suﬃcient sets
as well as the crucial ﬁndings.
Sensitivity to Parameters
Probability of evidence, P(e)(t): Let BN be a Bayesian network over the
universe U. Let t be a parameter and let e be evidence entered in BN. Then,
assuming proportional scaling, we have
P(e)(t) = αt + β,
where α and β are real numbers.
Functional expression for P(X | e)(t): Let BN be a Bayesian network over
the universe U. Let t be a set of parameters for diﬀerent distributions. Let a
be a state of A ∈U and let e be evidence. Then P(a | e)(t) is a fraction of two
multilinear polynomials over t.
5.9 Bibliographical Notes
Max-propagation was proposed by Dawid (1992). The axioms for propaga-
tion were formulated by Shafer and Shenoy (1990), and Lauritzen and Jensen

Exercises
191
(1997) extended them to cover Hugin propagation. A measure for calculat-
ing data conﬂict (surprise index) was ﬁrst proposed by Habbema (1976). The
method presented here is due to Jensen et al. (1990a). See also (Laskey, 1991),
(Kim and Valtorta, 1995), and (Laskey, 1995). SE analysis is part of explana-
tion, which was systematically studied by Suermondt (1992). The presentation
here is an extension of (Jensen et al., 1995). Theorem 5.2 establishing the lin-
earity of P(e)(t) was independently proved by Castillo et al. (1997) and Coup´e
and van der Gaag (1998), and the method described here is based on (Kjærulﬀ
and van der Gaag, 2000).
5.10 Exercises
Exercise 5.1. Construct the IEJ tree for the Bayesian network from Exer-
cise 4.2 with evidence “D = y”.
Exercise 5.2. Construct the IEJ tree for the Bayesian network from Exer-
cise 4.3 with the evidence “C = y”.
Exercise 5.3. Based on the join tree in Figure 4.16, draw the following:
•
A junction tree with the evidence e = {A = a, F = f}.
•
An IEJ tree for the evidence e = {A = a, F = f}.
•
An {A, F}-saturated junction tree. Which messages need to be sent for
obtaining this?
•
A b-saturated junction tree (b is a state of B) with evidence e = {A =
a, F = f}.
Exercise 5.4. Consider a Bayesian network with two variables A and B, each
having two states, and probability distributions deﬁned by P(A = a1) = 0.1,
P(B = b1 | A = a1) = 0.2, and P(B = b1 | A = a2) = 0.3. What is the
most-probable explanation for B = b2?
Exercise 5.5. E Using your implemented model from Exercise 3.14 for the
simpliﬁed poker game in Sections 3.1.4 and 3.2.3, what is the most-probable
explanation for observing FC = 2 and SC = 0? What is the conﬂict measure
of observing FC = 0 and SC = 2? What is the conﬂict measure of observing
FC = 0, SC = 2, and OH2 = sﬂ? Which of the three observations seems to
be ﬂawed?
Exercise 5.6. E Using your implemented model from Exercise 3.14 for the
simpliﬁed poker game in Sections 3.1.4 and 3.2.3, let e be the observations
FC = 2 and SC = 0? For hypothesis OH2 = sﬂand sensitivity parameters
θ1 = θ2 = 0.01, what are the crucial ﬁndings? Are the two observations
important individually?
Exercise 5.7. E This exercise concerns the stud farm from Section 3.2 and
the situation in Figure 3.16.

192
5 Analysis Tools for Bayesian Networks
(i) The farmer has to decide on a new mating among the horses Fred, Dorothy,
Eric, and Gwenn. Which pair should be chosen to minimize the risk of
getting a carrier as oﬀspring?
(ii) What is the most-probable conﬁguration of genotypes of all horses? Does
this correspond to the most-probable genotype for each horse?
(iii) The prior frequencies λL and λK of the a-gene for the outside horses L
and K are parameters. Determine intervals for both parameters for which
Dorothy as well as Gwenn have a risk above 0.70 of being a carrier.
(iv) Assume that the farmer gets the evidence that Ann is pure, Brian is pure,
and Cecily is a carrier. Perform a data conﬂict analysis.
(v) Assume that a horse is taken out if the probability of it being a carrier
is above 0.60. The evidence “John = aa” is double checked and consid-
ered certain. Perform an SE analysis of the evidence from (iv) for the
grandparents of John.
Exercise 5.8. E
This exercise concerns the transmission of symbol strings
from Section 3.2.4 and Exercise 3.12 (i).
(i) The sequence baaca is received. What is the most-probable word trans-
mitted?
(ii) Perform a data conﬂict analysis of the evidence.
(iii) Consider the parameters t = P(T4 = a | T3 = a), s = P(R4 = c | T4 = a),
and u = P(R4 = c | T4 = b). Perform an analysis of the sensitivity of the
conclusion “the word transmitted is baaba.” A one-way analysis could, for
example, determine the minimal distance to a value where the conclusion
changes.
(iv) The parameters s and u are common for all R-variables. Perform a sensi-
tivity analysis as in (iii).
Exercise 5.9. Fill in the intermediate steps of
P(e′ | h)
P(e′ | ¬h) = P(h | e′)P(¬h)
P(h)P(¬h | e′) = P(h | e′)(1 −P(h))
P(h)(1 −P(h | e′)).
Exercise 5.10. E Consider the poker model from Exercise 3.13 (ii). Assume
that you have seen your opponent change two cards ﬁrst and then no cards.
You have a ﬂush. You know that your opponent sometimes changes no cards
in the second round, no matter her hand. Let the frequency of this be t, and
let your initial estimate be t0 = 0.1. Analyze the sensitivity of the conclusion
with respect to t and determine the value for which you have the best hand
with probability 0.67.
Exercise 5.11. Consider the Bayesian network with two variables A and B,
each having two states, and probability distributions deﬁned by P(A = a1) =
t, P(B = b1 | A = a1) = t, and P(B = b1 | A = A2) = 0.3. For what range of
values for t is a2 the most probable explanation for B = b1?

Exercises
193
Exercise 5.12. E Investigate whether your Bayesian network tool allows for
either automated SE analysis or sensitivity analysis of parameters. If so, verify
your results from Exercises 5.6 and 5.11.
Exercise 5.13. Prove Proposition 5.4.

6
Parameter Estimation
Assume that you know the structure of a Bayesian network model over the
variables U, but you do not have any estimates for the conditional proba-
bilities. On the other hand, you have access to a database of cases, i.e., a
set of simultaneous values for some of the variables in U. You can now use
these cases to estimate the parameters of the model, namely the conditional
probabilities. In this chapter we consider two approaches for handling this
problem: First we show how a database of cases can be used to estimate the
parameters once and for all (so-called batch learning). After that, we shall
investigate the situation in which the cases are accumulated sequentially and
we wish to adapt the model as each new case arrives. The reader is expected
to be familiar with Section 1.5.
6.1 Complete Data
Let M = (S, θ) be a Bayesian network with structure S and parameters θ,
and let U be the variables in M. Moreover, let D be a data set of cases, where
each case is a conﬁguration over all the variables in U. Such a case is said to
be complete case. In the learning community, a parameter is typically denoted
by θ (rather than t as we have done previously), and in this chapter we shall
follow the same convention. Moreover, to ensure that the parameters can be
learned independently we shall make the following two assumptions:
•
Global independence says that the parameters for the various variables are
independent. This means that we can modify the tables for the variables
independently.
•
Local independence says that the uncertainties of the parameters for dif-
ferent parent conﬁgurations are independent. To be more precise, let (b, c)
and (b′, c′) be diﬀerent conﬁgurations; then the uncertainty on P(A | b, c)
is independent of the uncertainty on P(A | b′, c′), and the parameters for
the two distributions can be modiﬁed independently.

196
6 Parameter Estimation
6.1.1 Maximum Likelihood Estimation
For each case d ∈D, the probability P(d|M) is called the likelihood of M
given d. If we assume that the cases in D are independent given the model,
then the likelihood of M given D is
L(M | D) =
	
d ∈D
P(d|M).
Often the log is taken, and it is then called the log-likelihood:
LL(M | D) =

d ∈D
log2 P(d|M).
If we have to choose among several models for describing the data, then
the principle of maximum likelihood advises us to choose a model of maximal
likelihood given the data. This means that if we want to estimate the condi-
tional probabilities, then our possible models Mθ agree on the structure but
diﬀer with respect to the parameters θ. So we choose a parameter estimate ˆθ
that maximizes the likelihood:
ˆθ = arg max
θ
L(Mθ | D) = arg max
θ
LL(Mθ | D).
In what follows we shall use ˆθ to denote a maximum likelihood estimate for
the parameters θ.
Example 6.1. We have tossed a thumbtack 100 times. It has landed pin up 80
times, and we look for the best estimate of the probability for pin up.
The situation is that we have a family of models, one for each possible value
of θ, the probability of pin up. Let Mθ denote the model with P(pin up) = θ,
then by assuming independent tosses, the likelihood of Mθ given the data is
P(D | Mθ) =
	
d ∈D
P(d | Mθ) = μ · θ80(1 −θ)20,
where μ is a binomial factor independent of θ. By setting the derivative
d
dθP(D | Mθ) equal to zero it is easy to see that the likelihood is maximal
for θ = 0.8, so ˆθ = 0.8.
In general, you get a maximum likelihood estimate as the fraction of posi-
tive counts over the total number of counts. This also holds for variables with
more than two states. If you want to ﬁnd a maximum likelihood estimate
for the parameters in a Bayesian network model, then this can be done by
ﬁnding maximum likelihood estimates for each conditional probability distri-
bution in the model. That is, for each conditional probability distribution,
e.g., P(A = a | B = b, C = c), you simply calculate

6.1 Complete Data
197
N(A = a, B = b, C = c)
N(B = b, C = c)
,
where N(A = a, B = b, C = c) is the number of cases in the database for
which A = a, B = b, C = c.
The principle of maximal likelihood therefore supports the intuition of
using frequencies as estimates, and to achieve a maximum likelihood estimate
you just count. We did so in Section 3.2.4, where Table 3.10 was the result of
10, 000 words transmitted.
6.1.2 Bayesian Estimation
When you have a sparse database, maximum likelihood estimation has some
drawbacks. Consider Table 6.1, which is the result of collecting 100 trans-
mitted words. If you do maximum likelihood parameter estimation using this
table, the outcomes with zero counts would be given zero probability and they
are thereby doomed impossible, a rather strong assumption based on only 100
cases.
Last three letters
aaa aab aba abb baa bba bab bbb
First
two
letters
aa
2
2
2
2
5
7
5
7
ab
3
4
4
4
1
2
0
2
ba
0
1
0
0
3
5
3
5
bb
5
6
6
6
2
2
2
2
Table 6.1. The table shows the number of ﬁve-letter words (T1T2T3T4T5) transmit-
ted over a channel. For example, the word abaab has appeared four times, whereas
bbabb has appeared six times.
An alternative to the principle of maximum likelihood is Bayesian estima-
tion: start with a prior distribution, and use experience to update the distri-
bution. The approach can be illustrated with a Bayesian network, where each
parameter for estimation is made explicit through a node. For the thumbtack
experiment, a model for three tosses would be as in Figure 6.1. The condi-
tional probabilities P(pin up|θ) are θ, and the prior distribution f(θ) is (as
always) up to you. If you have no idea at all, a common approach is to use
the uniform distribution f(θ) = 1, 0 ≤θ ≤1.
Now assume that we have performed one experiment with the result
pin up. Using Bayes’ rule we get
fp(θ|pin up) = P(pin up|θ)f(θ)
P(1 up)
=
θf(θ)
P(pin up)
for the posterior frequency function fp. If we let f(θ) = 1, we get

198
6 Parameter Estimation
T3
θ
T2
T1
Fig. 6.1. A Bayesian network model for estimating the parameter given the outcome
of three tosses.
fp(θ|pin up) =
θ
P(pin up).
As usual, P(pin up) is calculated as the normalization factor:
 1
0
θ dθ = 1
2,
so
fp(θ|pin up) = 2θ.
This yields a distribution of the posterior for θ given pin up, and the best
single estimate is the mean value of this distribution:
 1
0
θ(2θ) dθ = 2
3.
Next, assume that we get a toss with pin down. Then we have for the new
posterior distribution
fp2(θ|pin down, pin up) = P(pin down, pin up|θ)f(θ)
P(pin up, pin down)
= μP(pin down|θ)P(pin up|θ)f(θ)
= μP(pin down|θ)θ · 1 = μ(1 −θ)θ,
where μ is the normalization constant
1
μ = P(pin down, pin up) =
 1
0
(1 −θ)θ dθ = 1
6.
The posterior distribution fp2(θ|pin down, pin up) can now be written as
fp2(θ|pin down, pin up) = 6(1 −θ)θ,
and the single best estimate for θ is
 1
0
θ6(1 −θ)θ dθ = 1
2.

6.1 Complete Data
199
Theorem 6.1. Let X be a binary variable (yes, no), and assume that we have
performed a number of independent experiments out of which n turned up yes
and m turned up no. Let θ be the probability for yes. Then, starting with the
even prior distribution for θ, the posterior distribution is
fp(θ) = μθn(1 −θ)m,
where μ is a normalization constant. The Bayesian estimate for θ is
n+1
n+m+2.
Parameters estimated through Bayesian estimation are called maximum a
posteriori parameters.
The theorem can be proved by induction along the lines described above.
Moreover, the theorem can be interpreted so that an even prior distribution
corresponds to adding two virtual experiments to the data (one for yes and
one for no) and then counting frequencies.
This procedure also holds for distributions over more than two states. To
pursue the Bayesian approach, assume for example that you wish to estimate
P(T2 | T1) from Table 6.1. First you marginalize out the other variables to
obtain Table 6.2(a).
T1
a
b
T2 a 32 17
b 20 31
T1
a
b
T2 a 33 18
b 21 32
(a)
(b)
Table 6.2. (a) Counts of the ﬁrst two letters from Table 6.1. (b) The table obtained
by adding 1 to all counts in (a).
Next, add 1 to all cells (Table 6.2(b)), and you get the conditional proba-
bility table in Table 6.3.
T1
a
b
T2 a
` 33
54
´ ` 18
50
´
b
` 21
54
´ ` 32
50
´
Table 6.3. The result of a Bayesian approach for estimating the conditional prob-
ability table P(T2 | T1)

200
6 Parameter Estimation
6.2 Incomplete Data
In the previous section we saw how the probability parameters in a Bayesian
network can be estimated from a complete data set, i.e., a data set in which
each case speciﬁes a value for each of the variables. In practice, however, we
are often faced with situations in which the data is incomplete. For example,
some values may be accidentally missing (for example due to faulty sensor
readings), some values may have been intentionally removed, and, in the more
extreme case, some variables may simply not be observable (such variables are
also called latent variables or hidden variables). If only some of the cases in
the database contain missing values, then you could be tempted to simply
throw these cases away and estimate the probability parameters using the
remaining (complete) database. This approach, however, may have a serious
drawback: Besides the risk of ending up with a very small database, we may
unintentionally bias the parameter estimates. For example, assume that we
have two binary variables A and B, and we are given a database with 20 cases
over A and B. Assume also that the database contains an equal number of
cases with A = a1 and A = a2, but when A = a2, then the value of B is
missing in 5 of the cases (B is not missing in any of the other cases). Now if
we want to ﬁnd the maximum likelihood estimate for θ, the probability of a1,
using the entire database, then (recall that P # is the notation for frequency
counts)
P #(a1) = ˆθ =
N(a1)
N(a1) + N(a2) =
10
10 + 10 = 1
2.
However, if we throw away the cases that contain missing values, then the
maximum likelihood estimate would be
P #(a1) = ˆθ′ =
N ′(a1)
N ′(a1) + N ′(a2) =
10
10 + 5 = 2
3.
The diﬀerence in the two estimates is caused by A’s inﬂuence on B’s “miss-
ingness.” On the other hand, if A does not have an inﬂuence on whether the
value of B is missing in the database, then we can (if the database is large
enough) safely throw away the cases with missing values without aﬀecting the
maximum likelihood estimate of A.
The example above illustrates that in order to deal with missing data we
need to take into account how the data is missing. Consider the incomplete
data set as having been produced from a complete data set by a process that
hides some of the data.
•
If the probability that a particular value is missing depends only on the
observed values, then the data is said to be missing at random (MAR).
•
If this probability is also independent of the observed values, then the data
is said to be missing completely at random (MCAR).
•
If the data is neither MAR nor MCAR, then the process that generated
the missing data is said to be nonignorable.

6.2 Incomplete Data
201
In the deﬁnitions of MAR and MCAR, the probability that a value is missing is
independent of that speciﬁc value. In particular, when we have hidden/latent
variables, then the data is MCAR, since the variables are unobserved regard-
less of the values of any of the variables.
To give a few examples. Consider ﬁrst an exit poll performed during an
election, where an extreme right-wing party, ER, is running for parliament. If
we expect people who vote for ER to be more likely than others to refuse to
answer how they have voted, then the data is neither MCAR nor MAR. This
also means that when estimating the parameters, we cannot disregard the un-
derlying process causing the missing data. As another example, assume that
we have a database containing the results of two tests. The results of both
tests can be either positive or negative, but whereas the ﬁrst test is always
performed, the second test is performed only as a “backup test” when the re-
sult of the ﬁrst test is negative. In this situation the pattern of “missingness”
is dependent only on the observed values, hence the data is MAR. Finally,
consider a monitoring system equipped with sensors whose values are contin-
uously recorded and stored in a database. The recording system, however, is
not completely stable, and sometimes a sensor value is not stored properly
(i.e., it will be missing in the database). In this situation, the process causing
the data to become missing is independent of all the sensor values, and the
data is MCAR.
Today, the majority of the methods used for parameter estimation assume
the data to be MAR, and in the remainder of this chapter we shall make the
same assumption.
One approach to ﬁnding the maximum likelihood parameters could be
to simply solve the corresponding likelihood equations. Unfortunately, this
approach is not feasible in practice, since an incomplete case may cause the
parameters to become dependent. The same holds if we were to consider the
maximum a posteriori parameters θ∗:
θ∗= arg max
θ
P(θ | D).
(6.1)
Instead, researchers have focused on approximative methods for doing pa-
rameter estimation.
6.2.1 Approximate Parameter Estimation: The EM Algorithm
One of the most popular algorithms for doing parameter estimation is the
Expectation-Maximization (EM) algorithm. The EM algorithm is a general
algorithm for ﬁnding maximum likelihood estimates for a set of parameters θ
when one is faced with an incomplete data set. The algorithm basically alter-
nates between a so-called expectation step and a maximization step: loosely
speaking, in the expectation step we “complete” the data set by using the
current parameter estimates ˆθ to calculate expectations for the missing val-
ues, and in the maximization step we use the “completed” data set to ﬁnd

202
6 Parameter Estimation
a new maximum likelihood estimate ˆθ
′ for the parameters. This estimate is
then used to complete the data set in the next iteration of the algorithm. The
algorithm continues either for a predetermined number of iterations or until
the algorithm has converged.
Example 6.2. Consider the Bayesian network representation M of the simpli-
ﬁed insemination problem described in Section 3.1.3 (page 55), and assume
that we have the database in Table 6.4.
Cases Pr Bt Ut
1.
? pos pos
2.
yes neg pos
3.
yes pos
?
4.
yes pos neg
5.
? neg
?
Table 6.4. A database consisting of ﬁve cases covering the variables Pr, Bt, and
Ut. The ? indicates that the value of the corresponding variable is missing.
When using the EM algorithm for learning the probability parameters
based on this database, we ﬁrst specify some initial “guesses” for the prob-
ability distributions for M, i.e., P0(Pr), P0(Bt | Pr) and P0(Ut | Pr). For the
sake of simplicity we let all three probability distributions be even although
you would usually start oﬀwith random distributions. Now, had the database
been complete, then in order to ﬁnd a new estimate for, say, the distribution
P(Pr = yes), we would count the number of cases N(Pr = yes) with Pr = yes:
P #
1 (Pr = yes) = N(Pr = yes)
N
.
From the database we see that cases 2, 3, and 4 contain Pr = yes, and they
therefore contribute with the value 1 to N(Pr = yes). However, for cases 1
and 5 the value for Pr is missing. So to ﬁnd the contribution from these two
cases we use the probability of seeing Pr = yes: case 1 therefore contributes
with P0(Pr = y | Bt = Ut = pos) = 0.5 and case 5 contributes with P0(Pr =
y | Bt = neg) = 0.5. What we are actually calculating here is the expected
value for N(Pr = yes), denoted by E[N(Pr = yes)]:
E[N(Pr = y)] =P0(Pr = y | Bt = Ut = pos) + 1 + 1 + 1
+ P0(Pr = y | Bt = neg) = 1
2 + 1 + 1 + 1 + 1
2 = 4;
E[N(Pr = n)] =P0(Pr = n | Bt = Ut = pos) + 0 + 0 + 0
+ P0(Pr = n | Bt = neg) = 1
2 + 0 + 0 + 0 + 1
2 = 1.

6.2 Incomplete Data
203
In general, the expected value of N(Pr = yes) is given by
E[N(Pr = yes)] =
N

i=1
P0(Pr = yes | di).
We can now use the expected counts to calculate a new estimate for P(Pr),
but before we come that far we should also calculate the counts necessary
for ﬁnding new estimates for the remaining probabilities. To estimate, say,
P(Ut = pos | Pr = yes), we need estimates for P(Ut = pos, Pr = yes) and
P(Pr = yes):
P #
1 (Ut = pos | Pr = yes) = P #(Ut = pos, Pr = yes)
P #(Pr = yes)
=

N(Ut = pos, Pr = yes)
N


N(Pr = yes)
N

= N(Ut = pos, Pr = yes)
N(Pr = yes)
.
Here N(Ut = pos, Pr = yes) denotes the number of cases containing both
Ut = pos and Pr = yes. However, as for Pr, we cannot ﬁnd N(Ut = pos, Pr =
yes) when there are missing values, so again we use the expected value/count
E[N(Ut = pos, Pr = yes)] =
N

i=1
P(Ut = pos, Pr = yes | di).
For the database above we get
E[N(Ut = pos, Pr = yes)] = P(Ut = pos, Pr = yes | Bt = pos, Ut = pos) + 1
+ P(Ut = pos, Pr = yes | Bt = pos, Pr = yes)
+ 0 + P(Ut = pos, Pr = yes | Bt = neg)
= 1
2 + 1 + 1
2 + 0 + 1
4 = 2.25.
These counts are suﬃcient for ﬁnding new estimates for the probability
parameters in the network (see Section 6.1). For example,
P #
1 (Pr = yes) = E[N(Pr = yes)]
N
= 4
5 = 0.8,
P #
1 (Ut = pos | Pr = yes) = E[N(Ut = pos, Pr = yes)]
E[N(Pr = yes)]
= 2.25
4
= 0.5625.
When a new estimate has been found for all the probabilities, the proce-
dure starts over again, but this time you should use the newly found probabil-
ity estimates when calculating the expected counts. The procedure continues

204
6 Parameter Estimation
until the probabilities no longer change or until another termination criterion
is met. In the special case that the database is complete, the algorithm con-
verges after one iteration and returns the maximum likelihood estimates for
the parameters.
Calculation of Family Counts
In the example above, we saw that in order to ﬁnd a new estimate for a condi-
tional probability distribution P(X | pa(X)) we should calculate the expected
counts for the family {X} ∪pa(X) of variables. That is, for a speciﬁc conﬁg-
uration of the family we calculate the expected number of cases that contain
this conﬁguration. Intuitively, we can consider the following three situations:
1. If a case is inconsistent with the conﬁguration (i.e., the case and the
conﬁguration disagrees on at least one value), then it counts as 0.
2. If a case contains the entire conﬁguration, then it counts as 1.
3. If the value for a variable is missing in a case, then it contributes with a
fractional count corresponding to the conditional probability of seeing the
conﬁguration.
The situations 1 and 2 are in fact special cases of situation 3.
From a computational point of view, the calculation of the expected counts
is the main diﬃculty of the EM-algorithm: when a case does not contain a
value for all the variables in question, then we need to calculate the condi-
tional probability distribution for these variables given that particular case.
We shall consider two situations: First, assume that we are interested in a spe-
ciﬁc conﬁguration fa(A) = a for a family of variables, and let d be a case with
a missing value for exactly one variable, X, in fa(A). If a speciﬁes X = x, then
the probability for a given d is equal to the probability P(X = x|d), which
in turn can be calculated by a single propagation in the Bayesian network.
Second, and more generally, assume that d contains missing values for a set of
variables X ⊆fa(A) in the family. In this situation the probability for a can
be read directly from the joint probability P(X | d), but this is not immedi-
ately provided by the Bayesian network. Fortunately, in order to calculate this
probability we can exploit the junction tree architecture (see Section 4.4). In
particular, the construction of the underlying junction tree ensures that each
family of variables is contained in at least one clique, say V , having variables
V. Hence, after a single propagation of the evidence corresponding to case d,
all the required probabilities can be read directly from the potentials associ-
ated with V and its neighboring separators. Speciﬁcally, from Theorem 4.5 we
see that if V is a clique with the set of potentials ΦV and with k neighboring
separators containing the V -directed sets of potentials Φ1, . . . , Φk, then
P(V, d) =
	
φV ∈ΦV
φV
	
φ1∈Φ1
φ1 · · ·
	
φk∈Φk
φk.

6.2 Incomplete Data
205
From this joint probability we can ﬁnd the required probability P(X, d) by
marginalizing out the irrelevant variables:
P(X, d) =

V\X
P(V, e).
We return to our previous example. In order to calculate all the expected
counts, we use the junction tree structure shown in Figure 6.2.
Pr
Bt,Pr
Ut,Pr
{P(Ut | Pr)}
{P(Pr), P(Bt | Pr)}
→
←
Fig. 6.2. A junction tree representation of the simpliﬁed insemination problem.
In particular, when calculating the contribution from case 5, we perform
a full propagation with the evidence Bt = neg, and we get the annotated
junction tree in Figure 6.3.
{P(Pr), P(Bt = neg | Pr)}
Bt,Pr
Ut,Pr
{P(Ut | Pr)}
→
←
Pr
{P(Pr, Bt = neg)}
{1Pr}
Fig. 6.3. A junction tree representation of the simpliﬁed insemination problem after
inserting and propagating the evidence Bt = neg.
The required probability, e.g., P(Ut, Pr | Bt = neg), can now be calculated
directly from the potential in the clique containing Ut and the potential in
the separator directed toward that clique:
P(Ut, Pr, Bt = neg) = P(Ut | Pr)P(Pr, Bt = neg),
P(Bt = neg) =

Ut,Pr
P(Ut, Pr, Bt = neg),
P(Ut, Pr | Bt = neg) = P(Ut, Pr, Bt = neg)
P(Bt = neg)
.
Similarly, if we use the junction tree to calculate the contribution from case
5 to the expected counts for the family {Bt, Pr}, then we need P(Bt, Pr | Bt =
neg) = P(Pr | Bt = neg). This probability can be found using the same method
as above:

206
6 Parameter Estimation
P(Pr, Bt = neg) = P(Bt = neg | Pr)P(Pr)1Pr,
P(Bt = neg) =

Pr
P(Pr, Bt = neg),
P(Pr | Bt = neg) = P(Pr, Bt = neg)
P(Bt = neg)
.
The EM-Algorithm for Bayesian Networks
We describe the algorithm more formally. Assume that we have a model struc-
ture B over the variables U = {X1, . . . , Xn}, and let θijk denote the param-
eter corresponding to the conditional probability P(Xi = k | pa(Xi) = j),
i.e., the conditional probability for variable Xi being in its kth state given
the jth conﬁguration of the parents of Xi. Using this notation we can ﬁnd a
maximum likelihood estimate, ˆθijk, for the parameters θijk given a data set
D = {d1, . . . , dm} with m cases as follows:
Algorithm 6.1 [The EM algorithm]
1. Choose an ϵ > 0 to regulate the stopping criterion.
2. Let θ0 = {θijk}, where 1 ≤i ≤n, 1 ≤k ≤|sp(Xi)| −1, and
1 ≤j ≤|sp(pa(Xi))|, be some initial estimates of the parameters (cho-
sen arbitrarily).
3. Set t := 0.
4. Repeat:
E-step: For each 1 ≤i ≤n calculate the table of expected counts:
E
θ
t[N(Xi, pa(Xi)) | D] =

d ∈D
P(Xi, pa(Xi) | d, θt).
M-step: Use the expected counts as if they were actual counts to calculate
a new maximum likelihood estimate for all θijk:
ˆθijk =
Eθ
t[N(Xi = k, pa(Xi) = j) | D]
|sp(Xi)|
h=1
Eθ
t[N(Xi = h, pa(Xi) = j) | D]
.
Set θt+1 := ˆθ and t := t + 1.
Until | log2 P(D | θt) −log2 P(D | θt−1)| ≤ϵ.
⊓⊔
The EM-algorithm has been generalized for estimating the maximum a poste-
riori parameters (or penalized likelihood) instead of the maximum likelihood
parameters. In this approach, virtual counts are added to both the denomi-
nator and numerator in the M-step, hence the method follows the idea of the
Bayesian estimation method for complete data (see Section 6.1.2). As before,
the virtual values can be interpreted as counts from a virtual database.

6.3 Adaptation
207
6.2.2 *Why We Cannot Perform Exact Parameter Estimation
When we have access to a complete database we can ﬁnd the exact maximum
likelihood parameters by simply counting frequencies in the database, or we
can express the posterior probability distribution of the parameters in closed
form. However, we are not that lucky when working with incomplete data. For
example, assume that we have a probability distribution P(U | θ) and that we
get a single case d that speciﬁes a conﬁguration x over X ⊂U; the variables
Y = U \ X are therefore not observed. In order to ﬁnd an estimate for the
maximum likelihood parameters we should maximize the following expression
with respect to θ:
P(x | θ) =

Y
P(x | Y, θ)P(Y | θ).
That is, we maximize a sum having one term for each conﬁguration of the
unobserved variables. When performing the maximization we cannot consider
the terms independently, since P(Y | θ) will, in general, depend on all the
parameters involved. Moreover, we have such a weighted sum for each case in
the database; hence the number of terms may become intractably large.
6.3 Adaptation
When constructing a Bayesian network, you will almost always be uncertain
of the correctness of the conditional probabilities speciﬁed, whether they are
speciﬁed manually or learned from data. Usually you would allow each proba-
bility to range within an interval, and a number in this interval is then chosen.
This type of uncertainty is called second-order uncertainty.
Second-order uncertainty raises two questions:
•
Does the second-order uncertainty have an impact on the conclusions from
the model?
•
Are there systematic ways of reducing the second-order uncertainty?
The ﬁrst question was discussed in Section 3.4 and was addressed in Sec-
tion 5.6. In this section, we address the second question. We will look at a
situation in which certain parameters are open for modiﬁcation.
When a system is at work, you repeatedly get new cases, and you would
like to learn from these cases. The situation may be that you are fairly certain
of the structure of the network. However, the conditional probabilities are
dependent on a context that varies from place to place, and you want to build
a system that automatically adapts to the particular context in which it is
placed.
In Figure 6.4(a), the variable A is directly inﬂuenced by B and C, and the
strength is modeled by P(A | B, C). The uncertainty in P(A | B, C) may be

208
6 Parameter Estimation
modeled explicitly by introducing an extra parent, T , for A (Figure 6.4(b)).
The variable T can be considered as a type variable. To reﬂect the frequencies
of the context types, a prior distribution P(T ) is given.
B
A
C
T
B
C
A
Case n
B
C
T
B
C
A
A
Case 1
(a)
(b)
(c)
Fig. 6.4. Adaptation through a type variable T. The distribution of T is updated
by Case 1 and used in the next case.
When a case, e, is entered into the network, the propagation will yield a
new distribution P ∗(T ) = P(T | e), and we may say that the change of the
distribution for T reﬂects what has been learned from the case. Now P ∗(T )
can be used as a new prior distribution when we get the next case. All vari-
ables whose tables are dependent on the context will be children of T . The
way P(T ) is updated can also be made explicit in the network structure as
shown in Figure 6.4(c). The network contains a copy of the variables for each
case that will be considered, and when the ith case arrives, the correspond-
ing variables are instantiated and P ∗(T ) = P(T | e1, . . . , ei−1) is updated to
P(T | e1, . . . , ei).
Example 6.3. Consider again the milk test problem described in Section 3.2.1,
and assume that the farmer is not always as careful as he ought to be when
performing the test. When this is the case, the risk of getting a false positive
or a false negative is ten times as high as it otherwise would have been. Let
us initially assume that there is an 80% chance that the farmer performs the
test carefully.
One way of modeling this situation is to introduce a type variable Type
(with states careful and careless) representing how the farmer performs the
test (see Figure 6.5).
The probability P(Inf) is as before, and the conditional probability distri-
bution P(Test | Inf, Type = careful) is as speciﬁed in Section 3.2.1. The proba-
bility distributions P(Test | Inf, Type = careless) and P(Type) can be derived
from the description above, i.e., P(Type) = (0.8, 0.2) and P(Test | Inf, Type =
careless) are as speciﬁed in Table 6.5.
Now assume that a test is performed and the result is negative. When
updating the probabilities with this piece of evidence you get P ∗(Type) =
P(Type | Test = neg) = (0.815, 0.185). This probability distribution represents
our updated belief in how the farmer performs the test. That is, the next time

6.3 Adaptation
209
Inf
Test
Type
Fig. 6.5. The type variable Type models whether the farmer performs the milk test
properly.
Inf = yes Inf = no
Test = pos
0.9
0.1
Test = neg
0.1
0.9
Table
6.5.
The
table
shows
the
conditional
probability
distribution
P(Test | Inf, Type = careless).
you get new evidence you should use this conditional probability distribution
(i.e., P ∗(Type)) as the prior distribution for the variable Type.
Finally, it should be noted that you have to be a bit careful when working
with several type variables. To illustrate the problem, assume that we get the
case A = a for the Bayesian network shown in Figure 6.6. When inserting this
piece of evidence, we see, from d-separation, that S and T become dependent.
Hence we cannot use their updated marginal distributions as prior distribu-
tions for the next case (by doing so we would have to assume that they are
independent, which we have just seen is not the case). That is, in order for the
above procedure to work correctly with several type variables, the evidence
from a case should d-separate the type variables.
T
A
B
S
Fig. 6.6. A Bayesian network augmented with two type variables, S and T.

210
6 Parameter Estimation
6.3.1 Fractional Updating
If the uncertainty of the conditional probabilities cannot be modeled explic-
itly through type variables, statistical methods can be used. The statistical
task is ﬁrst to specify a prior probability distribution over the parameters,
and then iteratively update this distribution as new cases are entered. The
correct approach for updating this distribution is basically the same as the
task of learning exact parameter estimates from a database, but as we also
saw in Section 6.2.2, this is infeasible in practice when we have missing values.
Instead, approximative techniques are usually applied.
Consider P(A | B, C), and let all variables be ternary. Under the assump-
tions of global and local independence, we may now think of P(A | bi, cj) =
(x1, x2, x3) as a distribution established through a number of past cases in
which (B, C) was in state (bi, cj). We can then express our certainty of the
distribution by a ﬁctitious sample size s. The larger the sample size, the
smaller the second-order uncertainty, so we work with a sample size s, a set
of counts (n1, n2, n3) such that s = n1 + n2 + n3, and
P(A | bi, cj) =
n1
s , n2
s , n3
s

.
That is, s represents the number of cases with (bi, cj), and n1 is the number
of these cases that also include a1.
Let us ﬁrst consider a couple of simple cases before we take the general
case.
1. We get a new case e with B = bi and C = cj and with A = a1. Then
n1 := n1 + 1 and s := s + 1, and the probabilities are updated as follows:
x1 := (n1 + 1)
(s + 1) ;
x2 :=
n2
(s + 1);
x3 :=
n3
(s + 1).
2. We get a new case e with B = bi and C = cj, but for A we have only
a distribution P(A | e) = P(A | bi, cj, e) = (y1, y2, y3). Then we cannot
work with integer counts, and we update nk := nk + yk and s := s + 1.
Accordingly, we get
x1 := (n1 + y1)
(s + 1) ;
x2 := (n2 + y2)
(s + 1) ;
x3 := (n3 + y3)
(s + 1) .
3. We get a new case e with A = a1, but for B and C we have only P(B =
bi, C = cj | e) = z. As before, we cannot work with integer counts, so
instead we update with a fractional count:
x1 := (n1 + z)
(s + z) ;
x2 :=
n2
(s + z);
x3 :=
n3
(s + z).

6.3 Adaptation
211
In general, we may get a case with P(bi, cj | e) = z and P(A | bi, cj, e) =
(y1, y2, y3). To update the counts, we use these distributions; because the
sample size is increased only with z we take nk := nk + zyk, and we get
xk := (nk + zyk)
(s + z)
= nk + P(ak, bi, cj | e)
s + P(bi, cj | e)
.
This scheme is known as fractional updating. Unfortunately, the scheme
has a serious drawback, namely that it tends to overestimate the count of s,
thereby overestimating our certainty of the distribution. Assume for example
that e = {B = bi, C = cj}. Then the case tells us nothing about P(A | bi, cj),
but nevertheless fractional updating will add a count of 1 to s and take it as
a conﬁrmation of the present distribution:
xk := nk + P(ak | bi, cj)
s + 1
= nk + nk
s
s + 1
= nk
s .
In Section 6.3.6 we shall return to this issue and consider another approxima-
tive updating method, which does not have the same drawback as mentioned
above.
6.3.2 Fading
It is often a problem for fractional updating that the initial counts are kept
when the system is trying to adapt to the environment. Particularly, when the
conditional probabilities in the environment change over time, the accumu-
lated counts will prevent the system from following the changes. Also, because
fractional updating has a tendency to overestimate counts, vacuous counts will
build up and make the parameters too resistant to change. Therefore, to keep
the ﬂexibility of parameters, it may be a good policy to prevent the sample
size from growing unbounded.
An idea for solving this problem is the following: For example, let a ternary
variable X have sample size s and counts (n1, n2, n3), and assume that we get
a count of 1 for x1. Now, instead of increasing n1 by one, we ﬁrst multiply
the counts by a fading factor, q ∈(0, 1). Hence, we get
s := sq + 1; n1 := n1q + 1; n2 := n2q; n3 := n3q.
If we assume that all counts are of value 1, the inﬂuence from the past will
fade away exponentially. In the limit where s →∞, we get a sample size s∗,
where
s∗=
1
(1 −q).
The number s∗is called the eﬀective sample size, and it represents a steady-
state situation. If s = s∗and we get a new count, we have
s := s∗q + 1 =
q
(1 −q) + 1 =
1
(1 −q) = s∗.

212
6 Parameter Estimation
Instead of declaring a fading factor, you may declare an eﬀective sample
size s∗, and the fading factor is then
q∗= (s∗−1)
s∗
.
This idea can be used for each distribution P(X | pa(X) = π) that we
wish to adapt to the evidence. The eﬀective sample size need not be the same
for all distributions. The eﬀective sample size to declare is dependent on how
resistant to change you wish the distribution to be. The higher the resistance,
the higher the eﬀective sample size.
Fading can be implemented such that the eﬀective sample size is preserved.
In other words, if the sample size for a distribution is equal to the declared
eﬀective sample size s∗, then it will not be changed in adapting to a new case.
Let P(X | π) be declared with an eﬀective sample size s∗, and assume we
have P(π | e) = y for a case. Then fractional updating yields a new count of
y. To preserve the sample size in the steady-state situation we have to adapt
the fading factor q to the count y:
s∗q + y = s∗.
Hence
q = (s∗−y)
s∗
.
Note that if P(π | e) = 1, then q = q∗, and if P(π | e) = 0, then q = 1.
6.3.3 *Speciﬁcation of an Initial Sample Size
Frequently, the uncertainty of a parameter is expressed as an interval [x, y]. To
exploit the technique for adaptation, the second-order uncertainty expressed
by this interval will be translated to an initial sample size and a set of counts.
The speciﬁcation of the interval [x, y] for t = P(A = a) can be interpreted as,
“I expect the value of t to be somewhere in the middle of the interval, and
I am 90% sure that the value is in the interval.” In other words, you have
a distribution of t with mean close to 1
2(x + y) and with 90% of the density
mass inside [x, y].
As an example, take the interval [0.3, 0.4] for the state a of the binary vari-
able A. We interpret the interval as before, and assume that the distribution
is the result of s samples out of which n were in state a. The distribution for
t is a beta distribution, Beta(n1, n2), with mean μ = n1
s and with variance
σ2 = μ(1−μ)
(s+1) , where s = n1 + n2 (see Figure 6.7 for examples). It holds that
at least 90% of the probability mass lies in the interval [μ −3σ, μ + 3σ], so
we seek values for s and n such that μ ≈0.35 and σ ≈0.0167, and we get
n1 = 285.16 and s = 814.73.

6.3 Adaptation
213
2
0.6
1.5
1
0.4
0.5
0
0.2
0
x
1
0.8
0.6
0.4
0.2
0
1.4
1.2
1
0.8
0.6
0.4
0.2
0
x
1
0.8
0.6
0.4
0.8
0.2
0
0
1
2.5
x
2
1.5
1
0.5
Fig. 6.7. The ﬁgure shows the density functions for the three beta distributions
Beta(1, 1), Beta(2, 2), and Beta(2, 5).
6.3.4 Example: Strings of Symbols
Consider the transmission of symbols example from Section 3.2.4 with the
model from Figure 3.18. Assume that every tenth word is sent through an
error-correcting code, so that you know for certain the word transmitted. You
wish to adapt the parameters of the model to the words actually transmitted
and received.
First, you can use the coded words to adapt the distribution of the error
rates: P(Ri | Ti). Choose the eﬀective sample size 100 for all parameters. This
gives the fading factor 0.99. Also, let the initial sample be 100. The counts
are given in Table 6.6.
T = a T = b
R = a
80
15
R = b
10
80
R = c
10
5
Table 6.6. Initial counts for P(R | T).
Whenever a coded word is received, you have ﬁve cases (excluding the
redundancy bits in the code). Assume that baaba was sent but baaca re-
ceived. This means that the distribution P(R | a) is modiﬁed three times and
the distribution P(R | b) is changed twice. For P(R | a) we get the faded counts
((80·0.99+1)·0.99+1)·0.99+1, 10·0.993, 10·0.993) = (80.6, 9.7, 9.7), and for
P(R | b) we get the faded counts (15·0.992, (80 ·0.99 +1)·0.99, 5 ·0.992+1) =
(14.7, 79.4, 5.9).
The noncoded words cannot be used for adaptation of P(R | T ), but they
can be used for modifying P(T1) as well as P(Ti+1 | Ti). Assume that we
receive the word e = baaca. Let us concentrate on modifying P(T2 | T1). Let
the initial sample size be 50 for T1 = a and 150 for T1 = b. From Table 3.11,
we infer the count table as given in Table 6.7.

214
6 Parameter Estimation
T1 = a T1 = b
T2 = a
30
60
T2 = b
20
90
Table 6.7. Initial counts for P(T2 | T1).
The model from Exercise 3.13 yields P(T1 | e) = (0.13, 0.87), P(T2 | T1 =
a, e) = (0.81, 0.19), and P(T2 | T1 = b, e) = (0.66, 0.34). The fading factors are
(100−0.13)/100 = 0.9987 and (100−0.87)/100 = 0.9913. We get for P(T2 | a)
the counts (30·0.9987+0.13·0.81, 20·0.9987+0.13·0.19) = (30.07, 20.00) and for
P(T2 | b) we get (60·0.9913+0.87·0.66, 90·0.9913+0.87·0.34) = (60.05, 89.5).
Note that the sample size increases for the part with initial sample size
smaller than the eﬀective sample size and decreases for the part with initial
sample size larger than the eﬀective sample size.
6.3.5 Adaptation to Structure
As for the parameters in a model, it may happen that the structure of the
model does not ﬁt the cases you meet. If you use incremental adaptation of
parameters, you will often experience that the changes in parameter values to
a large degree will compensate for a slightly incorrect structure. Anyway, the
structural inaccuracy may be so substantial that parameter adjustments can-
not compensate. Unfortunately, no handy method for incremental adaptation
of structure has been constructed. The reason is that structural changes are
performed in jumps, and the justiﬁcation for a jump is based on accumulated
experience rather than a single case.
Basically, there are two ways out: you can accumulate the cases and run
a batch learning algorithm (see Chapter 7) now and then, or you can work
concurrently with several models. The second way is similar to the “expert
disagreement approach.”
Assume that you have three alternative models M1, M2, M3 with initial
normalized weights w1, w2, w3; these weights can be interpreted as the prob-
abilities for the models, P(M1), P(M2), and P(M3). A case with evidence
e is entered into all models, and propagation yields P(A | Mi, e) as well as
P(e | Mi), where A is any variable. Then we can calculate new weights for the
models
wi := P(Mi | e) = P(e | Mi)P(Mi)
P(e)
=
P(e | Mi)wi

j wjP(e | Mj),
as well as the probability for the variable A:
P(A | e) = w1P(A | M1, e) + w2P2(A | M2, e) + w3P3(A | M3, e).

6.3 Adaptation
215
6.3.6 *Fractional Updating as an Approximation
As we saw in Section 6.3.1, fractional updating has a serious drawback, namely
that it tends to overestimate the sample size. To overcome this problem an
alternative updating method (called incremental updating) has been proposed.
Both fractional updating and incremental updating have their origins in the
same problem: exact updating of the probability parameters is intractable,
since it requires us to keep track of a mixture of Dirichlet distributions, where
the number of mixture components may grow exponentially in the number
of cases. More speciﬁcally, given evidence e, both updating methods look for
an approximation of the posterior distribution P(θ|e), which determines the
conditional probability distributions in the network.
In order to illustrate the updating method, we will ﬁrst revisit the initial
problem and show some of the derivations that underlie both fractional up-
dating and incremental updating. Based on this, we will consider where the
two updating methods diﬀer.
Consider again the conditional probability distribution P(A | B, C), where
all variables are ternary. We set P(A = ak | B = bi, C = cj, θ) = θijk (see
Figure 6.8 for a graphical representation) such that θ = {θijk} and 1 ≤i ≤3,
1 ≤j ≤3, and 2 ≤k ≤3; the parameter θij1 is given by 1 −(θij2 + θij3).
We will sometimes use the shorthand notation θij = {θij1, θij2, θij3}, and we
also assume that the prior distribution for θij follows a Dirichlet distribution
with hyperparameters (n1, n2, n3), denoted by Dir[θij|n1, n2, n3].
θ
A
B
C
Fig. 6.8. An explicit representation of the parameter θ, which determines the con-
ditional probability distribution P(A = ak | B = bi, C = cj).
Now assume that we have the simple case with evidence e = {A = a2, B =
bi, C = cj} ∪e′. Since A, B, and C constitute the Markov blanket for θ,
we can disregard e′ when updating the distribution for the parameters θij,
i.e., f(θij|a2, bi, cj, e′) = f(θij|a2, bi, cj). Moreover, due to the choice of prior
distribution for θ, we have
f(θij|a2, bi, cj) = Dir[θij|n1, n2 + 1, n3].
As we did in the thumbtack problem (Section 6.1.2), we can similarly ﬁnd
a single point estimate for P(A = ak | B = bi, C = cj) by calculating the
expectation of θijk given e = {A = a2, B = bi, C = cj} ∪e′:

216
6 Parameter Estimation
P ′(A = ak | B = bi, C = cj) =

θij
θijkDir[θij|n1, n2 + 1, n3]dθij
=

nk+1
n1+n2+n3+1
for k = 2,
nk
n1+n2+n3+1
otherwise.
These updating rules are identical to those for fractional updating.
Consider now the more general situation in which the evidence does not
necessarily include A, B, and C. In this case, we ﬁrst express the posterior
distribution f(θij | e) as follows (recall that f(θ | A, B, C) = f(θ | A, B, C, e)):
f(θ | e) =

A

B

C
f(θ | A, B, C)P(A, B, C | e).
From the assumption of local parameter independence (see Section 6.3.1) we
can derive that f(θij) = f(θij | A, B = bi′, C = cj′), for i′ ̸= i or j′ ̸= j. This
allows us to decompose the above expression into two parts, one with j′ = j
and i′ = i and the other with j′ ̸= j and i′ ̸= i:
f(θij | e) =

A
f(θij | A, B = bi, C = cj)P(A, B = bi, C = cj | e)
+

j′̸=j

i′̸=i

A
f(θij)P(A, B = bi′, C = cj′ | e)
=

A
f(θij | A, B = bi, C = cj)P(A, B = bi, C = cj | e)
+ f(θij)(1 −P(B = bi, C = cj | e)).
As we also used above, we have that, for example,f(θij | A = a2, B = bi, C =
cj) = Dir[θij|n1, n2 + 1, n3]; hence the above expression can be rewritten as
f(θij | e) = Dir[θij|n1 + 1, n2, n3]P(A = a1, B = bi, C = cj | e)
+ Dir[θij|n1, n2 + 1, n3]P(A = a2, B = bi, C = cj | e)
+ Dir[θij|n1, n2, n3 + 1]P(A = a3, B = bi, C = cj | e)
+ Dir[θij|n1, n2, n3](1 −P(B = bi, C = cj | e)).
(6.2)
Note that the last term models the situation in which the speciﬁed parent
conﬁguration is not observed, and if it is observed then the term contributes
with zero.
This equation readily generalizes to a variable A with r states and parent
conﬁguration π:
f(θπ | e) =
r

k=1
Dir[θπ|n1, . . . , nk + 1, . . . , nr]P(A = ak, pa(A) = π | e)
+ Dir[θπ|n1, . . . , nr](1 −P(pa(A) = π | e)).
(6.3)

6.3 Adaptation
217
Unfortunately, there is a computational problem with this expression, namely
that the number of mixture components may grow exponentially in the num-
ber of cases that we process. This problem has led to the development of
approximate updating methods such as fractional updating and incremental
updating. Both of these methods approximate the mixture above using a sin-
gle Dirichlet distribution, but there is a diﬀerence in how they estimate the
parameters.
Fractional Updating Revisited
In fractional updating, equation (6.3) is approximated with a single Dirich-
let distribution. The hyperparameters for this approximate distribution are
formed by taking the linear combination (as deﬁned by the mixture) of the
corresponding hyperparameters in the mixture (disregarding the last term).
For example, for the ﬁrst hyperparameter n′
1 in equation (6.2) we get
n′
1 = (n1 + 1)P(A = a1, B = bi, C = cj|e)
+ n1P(A = a2, B = bi, C = cj|e)
+ n1P(A = a3, B = bi, C = cj|e)
= n1 + P(A = a1, B = bi, C = cj|e).
That is, the mixture in equation (6.2) is approximated by
f ′(θij | e) = Dir[n1 + P(A = a1, B = bi, C = cj | e),
n2 + P(A = a2, B = bi, C = cj | e),
n3 + P(A = a3, B = bi, C = cj | e)],
and the new estimate for P(A = ak | , B = bi, C = cj) is then given by the
mean value of θijk:
P ′(A = ak | , B = bi, C = cj) =

ij
θijkDir[n1 + P(A = a1, , B = bi, C = cj | e),
n2 + P(A = a2, , B = bi, C = cj | e),
n3 + P(A = a3, , B = bi, C = cj | e)]dθij.
Hence
P ′(A = ak | , B = bi, C = cj) = nk + P(A = ak, , B = bi, C = cj | e)
n1 + n2 + n3 + P(B = bi, C = cj | e)
= nk + P(A = ak, , B = bi, C = cj | e)
s + P(B = bi, C = cj | e)
,
and by comparing this result with the updating rule presented in Section 6.3.1
we see that they are identical. Thus, the intuitive appeal of fractional updating
that we saw in Section 6.3.1 rests on a mathematical foundation.

218
6 Parameter Estimation
The Incremental Updating Rule
Analogously to fractional updating, when doing incremental updating we also
estimate the mixture of Dirichlet distributions in equation (6.3) with a sin-
gle Dirichlet distribution. However, the hyperparameters for the approximate
Dirichlet distribution are determined by equating the means and average vari-
ance of the mixture to the means and the average variance of the approximat-
ing distribution. To be more speciﬁc, let θ∗
ik denote the mean of θ··k in the ith
component in equation (6.3). The mean for the kth parameter in the mixture
is then (θ∗
0k denotes the mean of θ··k in the last term)
θ∗
k =
r

i=1
θ∗
ikP(A = ai, π | e) + θ∗
0k(1 −P(B = bj, C = cj|e)).
Thus estimating the mixture with a single Dirichlet distribution having the
parameters (sθ∗
1, . . . , sθ∗
r) will provide the correct means for the parameters θk.
The value for s is found by setting the average variance of the approximating
distribution
˜v =
r

i=1
θ∗
i
θ∗
i (1 −θ∗
i )
s + 1
,
equal to the average-mean-weighted variance of the mixture. This gives the
following updating value for s:
s =
r
k=1 θ∗
k
2(1 −θ∗
k)
r
k=1 θ∗
kvk
−1,
where vk is the variance of θ∗
k in the mixture. Although this updating rule does
not have the same intuitive appeal as fractional updating, it has the property
that the sample size will not increase when no relevant evidence is entered.
In fact, it is actually possible for the sample size to decrease if the evidence
does not reﬂect an event with high prior probability.
6.4 Tuning
We have a Bayesian network BN. For this network, we have some evidence
e, and for a particular variable A we have x = P(A | e) = (x1, . . . , xn). We
may have a prior request y = (y1, . . . , yn) for P(A | e), so we want to tune the
network such that P(A | e) = y. Assume that the structure of BN is ﬁxed,
but for the conditional probabilities we have some freedom described by a set
of modiﬁable parameters t = (t1, . . . , tm) with an initial set of values t0; to
emphasize that we consider a subset of the parameters we use ti to represent
a parameter rather than θijk as we previously have used. We want to set the
parameters so that P(A | e) is suﬃciently close to y. One way to measure how
close the two distributions are would be to use the Euclidean distance:

6.4 Tuning
219
Deﬁnition 6.1. Let x = (x1, . . . , xn) and y = (y1, . . . , yn) be two probability
distributions. Then the Euclidean distance between x and y is (although we
do not take the square root):
dist (x, y) =
n

i=1
(xi −yi)2.
The Euclidean distance measure is a metric, meaning that:
1. dist(x, y) = 0 if and only if x = y.
2. dist(x, y) ≤dist(x, z) + dist(z, y).
3. dist(x, y) = dist(y, x).
Another distance measure frequently used is the Kullback-Leibler diver-
gence:
Deﬁnition 6.2. Let x = (x1, . . . , xn) and y = (y1, . . . , yn) be two probability
distributions. Then the Kullback-Leibler divergence between x and y is:
KL(x, y) =
n

i=1
xi log2
xi
yi

,
where 0 log2(0/yi) = 0 and xi log2(xi/0) = ∞.
Note that the Kullback-Leibler divergence does not satisfy property 3 above
so it is not a metric. In the remainder of this section we shall consider only
the Euclidean distance.
If (t1, . . . , tn) are parameters in the Bayesian network BN (parameters are
entries in conditional probability tables, see also Section 5.7) over the universe
U, then P(U) is a function of (t1, . . . , tn), as are also P(A | e) and P(e). In
the following, we assume proportional scaling, and we also assume that there
is at most one parameter per distribution.
The task is to set the parameters such that the distance is as small as
possible. If the parameters cannot be set in such a way that the distance is
close to zero, then it is an indication of an incorrect structure.
If it is possible to determine dist(x, y) as a function of t, you might be
so fortunate that the problem can be solved directly. However, usually the
problem cannot be solved directly even when the function is known, and a
gradient descent method can be used:
1. Calculate grad dist(x, y) with respect to the parameters t.
2. Give t0 a displacement △t in the direction opposite to the direction of
the gradient grad dist (x, y) (t0); that is, choose a step size α > 0 and let
△t = −α grad dist (x, y) (t0).
3. Iterate this procedure until the gradient is close to 0.

220
6 Parameter Estimation
From the deﬁnition of the Euclidean distance measure, we see that
∂
∂t dist (x, y) =

i
2(xi −yi)∂xi
∂t .
The yi’s are known, and the xi’s are available through updating in BN,
so what we need are grad xi(t) for all i. If the variable A is binary, we have
x = (x, 1 −x), y = (y, 1 −y), and
dist (x, y) = 2(x −y)2
and
grad dist (x, y) = 4(x −y) grad x,
From these formulas, we see that the gradient is 0 if and only if either x
is independent of all the parameters or x = y.
6.4.1 Example
Let BN be the Bayesian network in Figure 6.9 with initial probabilities from
Table 6.8. Let C be the information variable and A the variable of interest.
Assume also that the parameters are t = P(¬a) and s = P(¬c | ¬b). Initially,
we have t0 = (0.5, 0.4).
C
A
B
Fig. 6.9. A small Bayesian network for illustration.
B \ A a ¬a
b
1 0.3
¬b
0 0.7
C \ B b ¬b
c
1 0.6
¬c
0 0.4
Table 6.8. Parameters for the network in Figure 6.9, P(A) = (0.5, 0.5).
Assume that we require P(A | c) = (0.4, 0.6) = (y, 1 −y). Through up-
dating, we get x = P(a | c) = 0.58. We calculate P(a | c) as a function of
t:

6.4 Tuning
221
P(A, c) =

B
P(A)P(B | A)P(c | B) = (1 −t, t −0.7ts),
P(a | c) =
P(a, c)

A P(A, c).
We get
P(a | c) = x(t, s) =
(1 −t)
(1 −0.7ts).
The request is
1 −t
1 −0.7ts = 0.4,
which yields
s = t −0.6
0.28t = 25
7 −15
7t .
The set of parameters t meeting the request is shown in Figure 6.10.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
t
s
to = (0.5, 0.4)
s(t)
Fig. 6.10. The graph of s(t) consists of the parameter pairs (t, s) meeting the
request P(a | c) = 0.4.
Out of the inﬁnite number of parameter pairs (t, s(t)), we choose one. If
we do not wish to choose either of the extremes (0.6, 0) and ( 5
6, 1), it would be
natural to choose the point closest to t0 = (0.5, 0.4). This point is character-
ized by the property that the normal contains t0 (see Figure 6.10). Through
standard calculations, we get the following equation in t:
t4 −1
2t3 + 666
98 t −225
49 = 0.
A root is t = 0.668, and we get s = 0.364. For this very simple example,
it was possible to calculate the closest parameter setting meeting the request.

222
6 Parameter Estimation
The situation need not be much more complex before a direct calculation
becomes intractable.
The gradient descent method will in this example go as follows:
grad x(t) =
1
(1 −0.7ts)2 (0.7s −1, (1 −t)0.7t),
grad x(t0) = (−0.97, 0.24).
Formula (6.4) yields
grad dist (x, y) = 4(0.58 −0.4)(−0.97, 0.24) = (−0.70, 0.18).
Using a step size of 0.2, we get
△t = (0.14, −0.036)
and
t1 = (0.640, 0.364);
P 1(a | c) = 0.43.
The process is repeated:
grad x(t1) = (−1.06, 0.23),
grad dist (x, y) = (−0.13, 0.03),
t2 = (0.686, 0.358);
P 2(a | c) = 0.380.
Repeating once more yields
t3 = (0.672, 0.361);
P 3(a | c) = 0.395.
6.4.2 Determining grad dist(x, y) as a Function of t
The gradient descent method seems to require that we be able to calculate x
and grad x as a function of the parameters t. It was possible for the preceding
small example, but the method used will in general be intractable.
Instead, the results form Section 5.7 can be used. By using proportional
scaling we have
x = αt + β
at + b .
This yields
∂x
∂t = α(at + b) −a(αt + β)
(at + b)2
= αb −aβ
(at + b)2 ,
where the constants can be found as described in Section 5.7.

6.5 Summary
223
6.5 Summary
Maximum Likelihood Estimation
For each case d ∈D, the probability P(d|M) is called the likelihood of M
given d. If we assume that the cases in D are independent given the model,
then the likelihood of M given D is
L(M | D) =
	
d ∈D
P(d|M).
The parameters θ maximizing the likelihood are called the maximum like-
lihood parameters (and denoted by ˆθ):
ˆθ = arg max
θ
L(Mθ | D) = arg max
θ
LL(Mθ | D),
where
LL(M | D) =

d ∈D
log2 P(d|M).
If the database does not contain missing values, then the likelihood of a
Bayesian network is maximized by the (local) maximum likelihood estimates
for the conditional probability tables, say P(A | pa(A)), in the network:
N(A, pa(A))
N(pa(A)) .
Bayesian Estimation
Let X be a binary variable (yes, no), and assume that we have performed
a number of independent experiments out of which n turned up yes and m
turned up no. Let θ be the probability for yes. Then, starting with the even
prior distribution for θ, the posterior distribution is
fp(θ) = μθn(1 −θ)m,
where μ is a normalization constant. The Bayesian estimate for θ is
n+1
n+m+2.
This result can be interpreted so that an even prior distribution corre-
sponds to adding two virtual experiments to the data (one for yes and one for
no) and then counting frequencies. The procedure generalizes to distributions
over variables with more than two states.
Incomplete Data
•
If the probability that a particular value is missing depends only on the
observed values, then the data is said to be missing at random (MAR).
•
If this probability is also independent of the observed values, then the data
is said to be missing completely at random (MCAR).
•
If the data is neither MAR nor MCAR, then the process that generated
the missing data is said to be nonignorable.

224
6 Parameter Estimation
The EM algorithm
To ﬁnd an estimate for the maximum likelihood parameters when the data
is incomplete, you may run the EM algorithm; note that you are guaranteed
only to ﬁnd a local maximum likelihood estimate.
1. Choose an ϵ > 0 to regulate the stopping criterion.
2. Let θ0 = {θijk}, where 1 ≤i ≤n, 1 ≤k ≤|sp(Xi)| −1, and
1 ≤j ≤|sp(pa(Xi))|, be some initial estimates of the parameters (chosen
arbitrarily).
3. Set t := 0.
4. Repeat:
E-step: For each 1 ≤i ≤n calculate the table of expected counts:
E
θ
t[N(Xi, pa(Xi)) | D] =

d ∈D
P(Xi, pa(Xi) | d, θt).
M-step: Use the expected counts as if they were actual counts to calculate
a new maximum likelihood estimate for all θijk:
ˆθijk =
Eθ
t[N(Xi = k, pa(Xi) = j) | D]
|sp(Xi)|
h=1
Eθ
t[N(Xi = h, pa(Xi) = j) | D]
.
Set θt+1 := ˆθ and t := t + 1.
Until | log2 P(D | θt) −log2 P(D | θt−1)| ≤ϵ.
The probabilities required in the E-step are easily calculated using junction
tree propagation.
Adaptation
Adaptation through type variables: The second-order uncertainty can be char-
acterized as uncertainty about which table out of t1, . . . , tm is the correct one
for P(A | pa(A)).
Add a type variable T with states t1, . . . , tm and with A as child. The
prior probability P(t1, . . . , tm) reﬂects your belief in the various tables. Put
P(A | pa(A), ti) = ti.
Whenever a case e has been processed, the probability P(t1, . . . , tn | e) is
used as the new prior for the next case.
Fractional updating: Assume that the second-order uncertainty obeys both the
global and local independence requirements. For each parent conﬁguration π,
choose a ﬁctitious sample size n expressing the present certainty of P(A | π).
This yields a ﬁctitious sample size na = nP(a | π) for the conﬁguration (a, π).
When a case has been processed, it yields P(a, π | e). Add P(a, π | e) to na.
Thereby the sample is increased by P(π | e).

6.6 Bibliographical Notes
225
Warning: fractional updating reduces the second-order uncertainty too
quickly.
Fading: Instead of counting up with na, ﬁrst multiply the counts for π by a
fading factor. A fading factor q can be established from an eﬀective sample
size s∗
q = (s∗−P(π | e))
s∗
.
The alternative model approach: If there is explicit uncertainty in the model
– that is, if there are alternative models M1, . . . , Mm – they can be weighted
initially and run in parallel. After each case, the weights are modiﬁed.
Tuning
The set of parameters t open for modiﬁcation; x(t) the current distribution
in the model; y the target distribution.
1. Calculate grad dist(x, y) with respect to the parameters t.
2. Give t0 a displacement △t in the direction opposite to the direction of
the gradient grad dist (x, y) (t0); that is, choose a step size α > 0 and let
△t = −α grad dist (x, y) (t0).
3. Iterate this procedure until the gradient is close to 0.
We have
∂
∂t dist (x, y) =

i
2(xi −yi)∂xi
∂t .
Because P(e)(t) = αt + β, we know that xi(t) is the ratio of two linear
functions, and the partial derivatives can be calculated for all parameters
through two propagations (Chapter 4).
6.6 Bibliographical Notes
The characterization of the diﬀerent ways in which data may be miss-
ing/incomplete was suggested by Rubin (1976). With outset in incomplete
data, the EM algorithm was proposed by Dempster et al. (1977) for learning
maximum likelihood parameter estimates. Green (1990) described how the
EM-algorithm can be used to ﬁnd penalized maximum likelihood estimates,
and Lauritzen (1995) showed how the junction tree architecture can be ex-
ploited in calculating the expected counts in the E-step of the algorithm.
When data arrives sequentially, the probability parameters can be adapted
using fractional updating (Titterington, 1976). In some cases, however, frac-
tional updating may overestimate the sample size and an improved version of
the algorithm (known as incremental updating) was proposed by Spiegelhal-
ter and Lauritzen (1990). Later this algorithm was extended by Olesen et al.
(1992) to also allow for fading.

226
6 Parameter Estimation
The tuning method was proposed by Jensen (1999), based on work by
Russell et al. (1995) and Castillo et al. (1996).
6.7 Exercises
Exercise 6.1. Consider Example 6.1. Prove that the maximum likelihood
estimate for the model given the data is θ = 0.8.
Exercise 6.2. In the thumbtack experiment, let the nonnormalized prior dis-
tribution for θ be
f(θ) =

θ if θ ≤1/2
(1 −θ) if 1/2 ≤θ ≤1
(i) What is the normalization constant?
We have performed one experiment resulting in up.
(ii) What is the functional part of fp, the posterior distribution for θ?
(iii) What is normalization constant for fp?
(iv) What is the posterior Bayesian estimate?
Exercise 6.3. Consider the data in Table 6.1 and a Bayesian network con-
sisting of two nodes T1 and T2, with T1 being a parent of T2. What are the
maximum likelihood parameter estimates for the model given the data? What
are the Bayesian parameter estimates for the model given the data?
Exercise 6.4. Prove the distribution part of Theorem 6.1.
Exercise 6.5. Establish a Bayesian estimate of the conditional probability
P(a|b) from the counts in Table 6.1.
Exercise 6.6. Characterize the type (MAR, MCAR, or nonignorable) of
missingness that underlies the database for the variables A and B described
in the beginning of Section 6.2.
Exercise 6.7. Without taking the size of the database into account, when
would it be safe to throw away cases with missing values, i.e., should the data
be MAR, MCAR, or neither of the two?
Exercise 6.8. E
(i) Update the remaining probabilities in Example 6.2.
(ii) Use the updated probabilities to perform another iteration of the EM-
algorithm.

6.7 Exercises
227
Exercise 6.9. Refer back to the example of EM parameter estimation in Ex-
ample 6.2. What are the estimated parameters after a full iteration? And after
two full iterations? What are the maximum likelihood parameter estimates us-
ing only the complete cases? What are the Bayesian parameter estimates using
only the complete cases?
Exercise 6.10. E Consider the model in Exercise 3.28.
(i) What happens when you adapt to the following sequence of (A, B) states:
⟨(n, y)(n, y)(y, n)⟩?
(ii) Process a sequence of cases with A = y in which the states of B are
(n, y, n, y, y, n, n, y, y, y).
What are your beliefs in the experts now, and what is P(B | A)?
Exercise 6.11. You have the same model as in Exercise 6.10, but P(B | A)
is the one in Table 6.9.
B \ A
y
n
y
0.75 0.4
n
0.25 0.6
Table 6.9. Table for Exercise 6.11.
For P(B | A = y), you have an initial sample size of 12.
(i) Perform fractional updating from the sequence in Exercise 6.10 (iii).
(ii) Perform fractional updating on the same sequence but with fading factor
0.9.
Exercise 6.12. The network from Example 6.4.1 in its initial state has sample
sizes st = 25, ss = 10, and su = 25 for the three parameters. It now receives
20 cases with C = c out of which 10 have A = a (the rest have A = ¬a). For
the cases with A = a, all cases have B = b, and in the rest, 4 had B = ¬b.
1. Adapt the network without fading.
2. Adapt the network with eﬀective sample sizes 25, 10, and 25 for t, s, and
u, respectively.
3. Adapt the network to the same cases but without the information on B.
Exercise 6.13. Perform the calculations of Example 6.4.1 by use of a direct
representation of the parameters t, s.
Exercise 6.14. Assume that in Example 6.4.1 we require P(A | c) = (0.5, 0.5),
and assume that t = 0.6 is ﬁxed. Use the technique from Example 6.4.1 to
tune the parameters s and u.

228
6 Parameter Estimation
Exercise 6.15. Let D be a child of C, and let C have parents A and B, all
variables being binary. P(A) and P(B) have even distributions; P(D | c) =
(0.1, 0.9), P(D | ¬c) = (0.6, 0.4), and P(c | A, B) are as speciﬁed in Table 6.10.
Tune the parameters t, s to the prescribed behavior P(a | d) = 0.8.
B \ A
a
¬a
b
1 −ts 1 −s
¬b
1 −t
0
Table 6.10. The conditional probability table P(C = c | A, B) for Exercise 6.15.

7
Learning the Structure of Bayesian Networks
Consider the following situation. Some agent produces samples of cases from
a Bayesian network N over the universe U. The cases are handed over to you,
and you are asked to reconstruct the Bayesian network from the cases. This
is the general setting for structural learning of Bayesian networks. In the real
world you cannot be sure that the cases are actually sampled from a “true”
network, but this we will assume. We will also assume that the sample is fair.
That is, the set D of cases reﬂects the distribution PN(U) determined by N.
In other words, the distribution P #
D (U) of the cases is very close to PN(U).
Furthermore, we assume that all links in N are essential, i.e., if you remove
a link, then the resulting network cannot represent P(U). Mathematically, it
can be expressed as follows: if pa(A) are the parents of A, and B is any of
them, then there are two states b1 and b2 of B and a conﬁguration c of the
other parents such that P(A|b1, c) ̸= P(A|b2, c).
The task is now to ﬁnd a Bayesian network, M, close to N. In principle
this can be done by performing parameter learning for all possible structures,
and then selecting as candidates those models for which PM(U) is close to
P #
D (U) . However, by following this very simple approach we are faced with
three problems, which are fundamental for learning Bayesian networks. First
of all, the space of all Bayesian network structures is extremely large. In fact,
it has been shown that the number of diﬀerent structures, f(n), grows more
than exponentially in the number n of nodes (some example calculations can
be found in Table 7.1):
f(n) =
n

i=1
(−1)i+1
n!
(n −i)!n!2i(n−i)f(n −1).
(7.1)
Secondly, when searching through the network structures, we may end
up with several equally good candidate structures. Since a Bayesian network
over a complete graph can represent any distribution over its universe, we
know that we will always have several candidates, but a Bayesian network

230
7 Learning the Structure of Bayesian Networks
Nodes Number of DAGs Nodes Number of DAGs
1
1
13
1.9 · 1031
2
3
14
1.4 · 1036
3
25
15
2.4 · 1041
4
543
16
8.4 · 1046
5
29281
17
6.3 · 1052
6
3.8 · 106
18
9.9 · 1058
7
1.1 · 109
19
3.3 · 1065
8
7.8 · 1011
20
2.35 · 1072
9
1.2 · 1015
21
3.5 · 1079
10
4.2 · 1018
22
1.1 · 1087
11
3.2 · 1022
23
7.0 · 1094
12
5.2 · 1026
24
9.4 · 10102
Table 7.1. The table shows the number of diﬀerent DAGs that can be generated
for a given number of nodes. For example, there exist 4.2 · 1018 diﬀerent DAGs with
10 nodes.
over a complete graph will hardly be the correct answer. If so, it is a very
disappointing answer.
Thirdly, we have the problem of overﬁtting: the selected model is so close
to P #
D (U) that it also covers the smallest deviances from PN(U). Again, a
complete graph can represent P #
D (U) exactly, but D may have been sampled
from a sparse network.
There are basically two methods used for learning the structure of Bayesian
networks; constraint-based and score-based. The constraint based methods es-
tablish a set of conditional independence statements holding for the data, and
use this set to build a network with d-separation properties corresponding to
the conditional independence properties determined. The score-based meth-
ods produce a series of candidate Bayesian networks, calculate a score for each
candidate, and return a candidate of highest score.
To emphasize the focus on structural learning we shall use the following
convention: A Bayesian network M = (S, θS) consists of a network struc-
ture, S, and a set of parameters, θS, where the parameters determine the
conditional probabilities of the model. The structure S consists of an acyclic
directed graph, G = (U, E), together with a speciﬁcation of the state space
for each node/variable in the graph.
7.1 Constraint-Based Learning Methods
We shall ﬁrst consider the following problem: we have to determine the struc-
ture of a Bayesian network, and the only source of information is an oracle that
correctly answers queries of the type, “is the variable A d-separated from the
variable B given the set X?”, later we shall replace the oracle with a database

7.1 Constraint-Based Learning Methods
231
for answering the queries. We let I(A, B, X) denote that A is d-separated from
B given X. We use I(A, B) as shorthand for I(A, B, ∅), and if X consists of
only one element C, we write I(A, B, C).
The method consists in ﬁrst determining the skeleton of the network, and
afterward directing the links.
Deﬁnition 7.1. The skeleton of a Bayesian network N is the undirected
graph obtained by removing directions from all arcs in N.
The skeleton can quite easily be established through a series of questions to
the oracle: if there is a link between A and B, then they cannot be d-separated.
That is, the link A −B is part of the skeleton if and only if ¬I(A, B, X) for
all X not containing A or B. As a starting point, let us assume that we have
the skeleton.
7.1.1 From Skeleton to DAG
Consider the skeleton in Figure 7.1(a), and assume that the only conditional
independence found is I(A, B). This means that A and B are not independent
given C, and therefore Figure 7.1(b) is the only possible directed graph with
d-separation properties corresponding to the conditional independences found.
(b)
C
B
A
A
B
C
(a)
Fig. 7.1. (a) A skeleton for the set {I(A, B)}. (b) the corresponding DAG.
This observation can be generalized as illustrated in Figure 7.2, where C
must be a child of A and B if I(A, B) or I(A, B, D).
Rule 1 [introduction of v-structures]: If you have three nodes, A, B, C,
such that A −C and B −C, but not A −B, then introduce the v-structure
A →C ←B if there exists an X (possibly empty) such that I(A, B, X) and
C ̸∈X.
As an example, consider the skeleton in Figure 7.3(a) with independences
I(A, B), I(B, C), I(A, B, C), I(B, C, A), I(C, D, A), I(B, C, {D, A}), I(C, D,
{A, B}), I(B, E, {C, D}), I(A, E, {C, D}), I(B, C, {A, D, E}), I(A, E, {B, C,
D}), I(B, E, {A, C, D}). Consider the chain C −E −D. Since E is not a
member of any conditioning set yielding C and D independent, we introduce
the v-structure C →E ←D. In the same way we introduce the v-structure

232
7 Learning the Structure of Bayesian Networks
D
A
B
C
Fig. 7.2. {A, C, B} are connected in an undirected chain, but there is another path
between A and B. If also I(A, B) or I(A, B, D), then C must be a child of A and
B.
A →D ←B (see Figure 7.3(b)) With these two v-structures, there cannot be
more of them. This is also conﬁrmed by the conditional independences, and
since they give no clue as to the remaining link, A −C, it may be oriented in
any direction (see Figure 7.3(c)).
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
(a)
(b)
(c)
Fig. 7.3. (a) A skeleton. (b) Two v-structures introduced through rule 1. (c) A full
DAG
After the v-structures have been introduced, we give a direction to the
remaining links using the following rules:
Rule 2 [Avoid new v-structures]: When Rule 1 has been exhausted, and
you have A →C −B (and no link between A and B), then direct C →B.
Rule 3 [Avoid cycles]: If A →B introduces a directed cycle in the graph,
then do A ←B.
Rule 4 [Choose randomly]: If none of the rules 1–3 can be applied anywhere
in the graph, choose an undirected link and give it an arbitrary direction.

7.1 Constraint-Based Learning Methods
233
For example, having found the v-structures in Figure 7.3 (b), we can choose
any direction for A −C (Figure 7.3 (c)).
Example 7.1. Consider the graph in Figure 7.4(a). The only v-structure found
is C →F ←D. Rule 2 yields the direction F →G (Figure 7.4(b)). None of the
Rules 1–3 can be applied, and we choose the direction D ←E (Figure 7.4(c)).
Now Rule 2 yields D →A and D →B (Figure 7.4(d)), and in turn, Rule 2
yields A →C (Figure 7.4(e)). Now none of the Rules 1–3 can be applied. We
use rule 4 and choose A →B (Figure 7.4(f)).
A
C
D
F
G
E
B
A
C
D
F
G
E
B
(a)
(b)
A
C
D
F
G
E
B
A
C
D
F
G
E
B
(c)
(d)
A
C
D
F
G
E
B
A
C
D
F
G
E
B
(e)
(f)
Fig. 7.4. A sequence of applications of Rules 2 and 4.
The application of Rules 2–4 raises various questions. For example, Rule 4
opens up for several solutions. If in the example above we had chosen D →E
rather than D ←E, the solution could have been the DAG in Figure 7.5.

234
7 Learning the Structure of Bayesian Networks
A
C
D
F
G
E
B
Fig. 7.5. A resulting DAG if we had chosen D →E rather than D ←E.
A solution represents a family of probability distributions over the universe
U; a distribution for each setting of the parameters. From a statistical point
of view, the various solutions are equivalent; they have the same d-separation
properties. Equivalently, they represent the same family of distributions. Fur-
thermore, a maximal likelihood setting of the parameters in one graph will
have a corresponding parameter setting in any other graph, and this param-
eter setting is also of maximal likelihood.
A more fundamental problem is whether there in fact is a solution. If the
oracle is reliable, then the skeleton and the v-structures are correct, and there-
fore there must be a way to direct the remaining links so that the generative
model is established. Moreover, any other solution will also be valid (see Sec-
tion 7.3.2). Finally, you might fear that the choice in Rule 4 may lead us into
a blind alley. However, it has been proven that this will not happen.
7.1.2 From Independence Tests to Skeleton
Since consulting the oracle has a price, we wish to reduce the number of
questions. We use the answers from the oracle when establishing the skeleton
and when introducing v-structures. The following theorem helps to reduce the
number of questions.
Theorem 7.1. The nodes A and B are not linked in N if and only if I(A, B,
pa(A)) or I(A, B, pa(B)).
Proof.
Clearly, if I(A, B, X) for any X, then A and B are not linked.
Assume now that A and B are not linked in N, and construct the ancestral
graph for {A, B} (see Section 2.2.1). If there is a path in this graph from B
to A not passing through pa(B), then B is an ancestor of A, and all paths
from B to A must pass through pa(A).
⊓⊔
The theorem ensures that it is suﬃcient to ask questions of the form
I(A, B, X), where X is a subset of A’s or B’s neighbors. It is used in the
PC algorithm to focus on local independence questions.

7.1 Constraint-Based Learning Methods
235
Algorithm 7.1 [The PC algorithm: test sequence]
1. Start with the complete graph;
2. i := 0;
3. while a node has at least i + 1 neighbors
-
for all nodes A with at least i + 1 neighbors
-
for all neighbors B of A
-
for all neighbor sets X such that |X| = i and X ⊆(nb(A)\{B})
- if I(A, B, X) then remove the link A −B and store
”I(A, B, X)”
-
i := i + 1
⊓⊔
7.1.3 Example
Assume that the cases are a faithful sample of the Bayesian network in Fig-
ure 7.6(a). We start with the complete graph in Figure 7.6(b) and ask the ques-
tions I(A, B)?, I(A, C)?, I(A, D)?, I(A, E)?, I(B, C)?, I(B, D)?, I(B, E)?,
I(C, D)?, I(C, E)?, I(D, E)?.
B
C
D
E
A
B
C
D
E
A
(a)
(b)
Fig. 7.6. (a) The Bayesian network from which the cases have been sampled.(b)
The starting graph for the PC algorithm.
We get “yes” for I(A, B)? and I(B, C)?; the links A −B and B −C are
removed (see Figure 7.7(a)), and i is set to 1.
We now ask I(A, C, E)?, I(B, C, D)?, I(B, C, E)?, I(B, D, C)?, I(B, D,
E)?, I(B, E, C)?, I(B, E, D)?, I(C, B, A)?, I(C, D, B)?, I(C, D, A)?. The last
question has the answer ”yes”; we remove the link C −D and continue;
I(C, E, A)?, I(C, E, B)?, I(D, B, E)?, I(D, E, B)?, I(E, A, B)?, I(E, A, D)?,
I(E, B, A)?, I(E, C, B)?, I(E, C, D)?, I(E, D, A)?, I(E, D, C)?.
Next, for i = 2 we ask questions like I(A, C, {D, E})?, and we get aﬃrma-
tive answers for I(B, E, {C, D})? and I(A, E, {C, D})?. The result is shown

236
7 Learning the Structure of Bayesian Networks
B
C
D
E
A
B
C
D
E
A
(a)
(b)
B
C
D
E
A
(c)
Fig. 7.7. (a) The result after testing all unconditional independences (i = 0).
(b) After testing with a single conditioning variable. (c) After testing with two
conditioning variables.
in Figure 7.7(c). Setting i = 3 we realize that no node has four neighbors, and
the algorithm terminates.
To sum up, we get the skeleton in Figure 7.7(c) together with the con-
ditional independences I(A, B), I(B, C), I(C, D, A), I(A, E, {C, D}), and
I(B, E, {C, D}). They are suﬃcient for applying Rules 1–4.
The PC-algorithm has the following property, which is easily seen from
the construction and Theorem 7.1.
Property 1: If the case set is a faithful sample from a Bayesian network, N,
then the graph resulting from the PC-algorithm is the skeleton of N.
We also have the following property, which allows us to establish the di-
rection of the arcs.
Property 2: The conditional independences found by the PC-algorithm are
suﬃcient for determining the v-structures.
Let namely A−C−B be a chain, and assume that the PC-algorithm found
I(A, B, X). We know that the two links are part of the skeleton, and if C ̸∈X
then the only way to direct the links will be to introduce the v-structure
A →C ←B. On the other hand, if C ∈X we cannot have a v-structure.

7.1 Constraint-Based Learning Methods
237
The Necessary Path Condition
The number of queries to the oracle can be further reduced. Consider the
situation in Figure 7.8, where the links A −D and A −C have been removed.
Then we need not ask for I(A, B, D) (or I(A, B, C)), since no path between
A and B passes D (similar for C). This is called the necessary path condition:
only ask I(A, B, X) for sets X, where all members of X occur on a path
between A and B.
A
C
D
B
Fig. 7.8. D cannot block any path between A and B.
7.1.4 Constraint-Based Learning on Data Sets
When learning structure, you do not have an oracle for queries of the type
I(A, B, X). Instead, you have a data set D, which you may analyze for condi-
tional independences. We shall use the notation ID(A, B, X) for conditional
independence in the distribution determined by D. We shall assume that D
is sampled from a Bayesian network N.
Deﬁnition 7.2. D is a faithful sample from N if the following holds: A and
B are d-separated in N given X if and only if ID(A, B, X).
If D is faithful to N, we can use a test for independence in D as oracle.
For this, conditional mutual information can be used.
CMI(A, B|X) =

X
P #(X)

A,B
P #(A, B|X) log2
P #(A, B|X)
P #(A|X)P #(B|X). (7.2)
It holds (Exercise 7.5) that
ID(A, B, X) ⇔CMI(A, B|X) = 0.
Based on the data set, the oracle will calculate an estimate of CMI(A, B|X),
and then it performs a χ2-test on the hypothesis CMI(A, B|X) = 0, and the
user decides on a signiﬁcance level. A high signiﬁcance level means that fewer

238
7 Learning the Structure of Bayesian Networks
links are removed. Because any test has false positives as well as false nega-
tives, there is a risk that links that should have been removed are not removed,
and vice versa. The error rate is closely related to the sample size. The smaller
the sample size, the more independences will be accepted and the fewer links
inserted.
In a real-world learning situation, you may, for example, get into the sit-
uation illustrated in Figure 7.9: you have ¬I(A, B), ¬I(A, C), ¬I(B, C), but
I(A, B, C), I(A, C, B), I(B, C, A); hence you cannot direct the links without
violating the independences found by the test. Then a solution may be to
remove one link and direct the remaining as a chain.
A
C
B
Fig. 7.9. The tests yield all pairs dependent, but all pairs independent given the
third variable.
This is called an uncertain region: removal of a link is dependent on how
you treat the other links. Note that for this example, the PC algorithm will
stop after I(A, B, C) and I(A, C, B) and removal of the links A−B and A−C.
If the necessary path condition is used, the process will stop after I(A, B, C)
and removal of the link A −B.
There may be other reasons why it is not possible to direct links without
violating some of the independences returned by the tests. Assume you have
the four variables A, B, C, D, and you get the independences I(A, C), I(A, D),
and I(B, D) for i = 0. Then the PC algorithm extended with the necessary
path condition will stop, and you have the skeleton in Figure 7.10. Now there
is no proper way of directing the links.
A
D
B
C
Fig. 7.10. A skeleton that cannot be directed.

7.1 Constraint-Based Learning Methods
239
Rule 1 grants the introduction of two v-structures, A →B ←C and
B →C ←D; but then the link B −C receives two directions. For this
particular case, the inconsistency need not be due to the test, but it can be
caused by a hidden variable as illustrated in Figure 7.11.
E
D
B
C
A
Fig. 7.11. The problem illustrated in Figure 7.10 may be caused by a hidden
variable (E).
You cannot always assume that a problem related to directing the skeleton
is due to erroneous tests or hidden variables. It may happen that the cases
have not been sampled from a Bayesian network. Anyhow, you have to enforce
directions inconsistent with the test results. Beware that violating dependence
results makes it impossible to represent the joint probability distribution of
the case set.
It is tempting to conclude that the PC algorithm discovers causality from
observed data. for a century it has been a commonly accepted view that
causality can be discovered only through controlled experiments, where an
outside agent ﬁxes some variables to certain states. The new algorithms for
learning Bayesian network structures have questioned this view. The PC al-
gorithm (and other preceding constraint-based algorithms) works on observed
nonmanipulated data, and it allows you to introduce v-structures. However,
you can conclude that you have discovered a causal relation only if you can
be sure that there are no hidden variables obscuring the picture.
For example, consider the structure in Figure 7.12 with D and E hidden.
The PC algorithm will yield I(A, C), ¬I(A, B), ¬I(B, C) and stop. However,
A and C are not causes of B. We shall not go deeper into this very lively and
interesting discussion.
Finally, it shall be mentioned that even a completely correct statistical test
for independence may not provide the correct d-separation properties (even if
you have a very large database); the conditional probabilities in the network
may hide dependencies. Take for example two switches A and B for the light
C. The light is on if and only if A and B are in the same position. The
prior probabilities for A and B are even. Although both links in this example

240
7 Learning the Structure of Bayesian Networks
E
B
C
A
D
Fig. 7.12. A structure with confounding variables:: D and E are hidden and obscure
the learning of causality.
are essential, then for a fair sample D we have ID(A, C) and ID(B, C); the
problem is that the faithfulness assumption is violated.
7.2 Ockham’s Razor
When learning structure from experiments, there is a general principle of
inductive learning, called Ockham’s razor (after William of Ockham, 1285–
1349). It recommends that one choose the simplest hypothesis consistent with
the observations.
In the case of learning Bayesian networks, this principle has a justiﬁcation
of its own. The complexity of a Bayesian network can be measured in number
of links or in number of independent parameters.
Proposition 7.1. Let M be a Bayesian network over the variables U, and
assume that the parameters θM are both locally and globally independent (see
Section 6.3.1). Then the number of independent parameters (or the size of M)
is given by:
size(M) =

X∈U
|pa(X)| · (|sp(X)| −1).
(7.3)
For example, assuming that all variables are binary, then the size of the model
in Figure 7.12 is 1+2+1+4+2 = 10. On the other hand, when the assumption
about either local and global parameter independence is violated, then the
number of independent parameters is usually lower.
Proposition 7.2. Let N be a Bayesian network over U with only essential
links. Then no other Bayesian network M representing PN(U) can have fewer
links or a smaller size than N.
Proof.
Let M represent P(U). Since all links are essential, it must hold that
whenever A and B are linked in N they are also in M. If there is a chance

7.3 Score-Based Learning
241
for M to have smaller size than N, then it must be because some links in M
have the direction opposite to that of the corresponding links in N.
Let L be a link from A to C, which is reversed. For simplicity we assume
that C has only one parent more, and that A has a single parent (See Fig-
ure 7.13(a)). Figure 7.13(b) depicts the situation in which the link has been
reversed.
In Figure 7.13(b) we have that C and D are independent, and A and B
are independent given C. To compensate for this, M must have extra links.
The cheapest will be to add a link from C to D and from B to A (See Fig-
ure 7.13(c)). Elementary arithmetic (see Exercise 7.6) now yields that the size
of the Bayesian network in Figure 7.13(c) is larger than for the Bayesian net-
work in Figure 7.13(a). Equality is only possible if A has no parents, and A
is the only parent of C.
⊓⊔
C
B
A
D
C
B
A
D
C
B
A
D
(a)
(b)
(c)
Fig. 7.13. In (a), C and D are dependent. If the link between A and C is reversed,
then C and D become independent (b), and to compensate for this you can insert
extra links (c).
Note that the proposition does not hold if we count probability parameters
rather than size. Note also that we use conditional independence properties
rather than probabilities in the proof.
The proposition justiﬁes a search for minimal models: If the real world is
a Bayesian network (with all links essential), and if the sample set is faith-
ful, then among all the models representing the distribution, the true one is
minimal with respect to links as well as size.
7.3 Score-Based Learning
When doing structural learning, we look for a Bayesian network structure that
on the one hand can represent our database suﬃciently well (when augmented
with a set of probabilities) and on the other hand is not overly complex. In
Section 7.1 we saw how to perform structural learning based on independence
tests, and in this section we shall focus on another type of learning, called
score-based learning. Score-based learning assigns a number (a score) to each

242
7 Learning the Structure of Bayesian Networks
Bayesian network structure. The score reﬂects the “usefulness” of a structure,
where the term “usefulness” can for example cover how likely it is that the
structure could have been used to generate the database at hand.
If we have a score function that takes a Bayesian network structure as
argument and returns a value, then the task of score-based learning can be
considered a search problem: we simply look for the model structure with the
highest score. This also means that a score based learning algorithm can in
principle be completely described by specifying two components, (1) a score
function, and (2) a search procedure.
7.3.1 Score Functions
When specifying a score for a network structure S with respect to a database
D, your ﬁrst attempt might be to consider the Euclidean distance (see Def-
inition 6.1) between the probability distribution, P #
D (U), represented by the
database D and your “best shot” at the probability distribution that can be
encoded in S over the same set of variables. By “best shot” we mean the
conditional probabilities for S that bring PS(U) closest to P #
D (U). An im-
mediate attempt might be to use the maximum likelihood estimates ˆθS (see
Section 6.1), in which case the distance measure can be speciﬁed as
dist

PD(U), PS(U | ˆθ)

=

x∈sp(U)

PD(x) −PS(x | ˆθS)
2
.
Unfortunately, there are (at least) two rather severe problems in using
this distance as the score of a structure. First of all, since a complete network
structure can encode any probability distribution, we know that in order to
minimize the distance above, we should simply select any complete network
structure; this is obviously not satisfactory. To avoid this problem you could
augment the score with a term penalizing model complexity (see Proposi-
tion 7.2). This means that the score of a structure should be deﬁned as a
trade-oﬀbetween how good it is at representing the distribution encoded by
the database and the complexity/size of the structure. A possible suggestion
for such a score could then be
score

PD(U), PS(U | ˆθ)

= dist

PD(U), PS(U | ˆθ)

+ c · size(S),
where c is a (user speciﬁed) constant used to control the trade-oﬀbetween
model accuracy and model complexity.
However, even though we may have found a suggestion for a score function
that reliably reﬂects the usefulness of a structure, we still have another prob-
lem to address: from a computational perspective, the Euclidean distance can
be extremely diﬃcult to work with, since it is a function of P #
D (U). That is, it
basically requires us to deal with a table over the joint state space of all the
variables, and we are therefore faced with the same combinatorial explosion,
which we again and again try to avoid.

7.3 Score-Based Learning
243
To summarize the discussion above, we look for a score function that should
(at least) have the following properties:
•
It should balance the accuracy of a structure with the complexity of the
structure.
•
It should be computationally tractable to evaluate.
The Bayesian Information Criterion
An example of a score function satisfying the above two properties is the
Bayesian information criterion (BIC), which contains a term measuring how
well the data ﬁts the model as well as a term that accounts for model com-
plexity:1
BIC(S | D) = log2 P(D | ˆθS, S) −size(S)
2
log2(N),
(7.4)
where ˆθ is an estimate of the maximum likelihood parameters for the structure
S. If we furthermore assume that the cases are independent given the model,
then
BIC(S | D) =
N

i=1
log2 P(di | ˆθS, S) −size(S)
2
log2(N).
In order to score a model using BIC, you start oﬀby estimating the max-
imum likelihood parameters for the model. If the database is complete, then
this is just a matter of frequency counting, but if some of the cases contain
missing values you may run the EM algorithm. Based on these estimates you
calculate the probability for each case in the database. This can be done by
simply inserting the case as evidence in the Bayesian network and performing
a propagation; the probability of the case is then the probability of the evi-
dence. If all the cases are complete, then this task is even simpler, you just
multiply the appropriate entries in the conditional probability tables, which,
in turn, are frequency counts derived from the database! This also means that
the calculation of the BIC score has been reduced to a counting problem: let
ri denote the number of states for variable Xi, and let qi = 
Xl∈Πi rl denote
the number of conﬁgurations over the parents for Xi in S (if Xi does not
have any parents then we let qi = 1). With this notation we now have (the
derivation is left as an exercise)
BIC(S | D) =
n

i=1
qi

j=1
ri

k=1
Nijk log2
Nijk
Nij

−log2 N
2
n

i=1
qi(ri −1),
(7.5)
where Nijk denotes the number of cases in the database with Xi in its kth
conﬁguration and pa(Xi) in the jth conﬁguration.
1 The exact form of the BIC score can be derived from a Taylor expansion of
P(D | S).

244
7 Learning the Structure of Bayesian Networks
Example 7.2. Consider the two Bayesian network structures over the two bi-
nary variables X1 and X2 shown in Figure 7.14 (we shall refer to them as
Ba and Bb, respectively), and assume that we have the database shown in
Table 7.2.
X1
X2
X1
X2
(a) Complete dependence;
(b) complete independence.
Fig. 7.14. Two BN model structures for the domain X = (X1, X2).
Case X1
X2
1.
yes positive
2.
yes positive
3.
yes positive
4.
yes positive
5.
yes positive
6.
yes positive
7.
yes negative
8.
yes negative
9.
no negative
10.
no negative
yes no
X1
8
2
X1
yes no
X2 pos
6
0
neg
2
2
Table 7.2. A database for the two binary variables X1 and X2 as well as the counts
N11k and N2jk derived from the database.
In order to calculate the BIC score for Ba we ﬁrst calculate the counts
(the states yes and positive correspond to state number 1) shown in Table 7.2.
By substituting these values into equation (7.5), we get
BIC(S | D)
=

8 · log

8
8 + 2

+ 2 · log

2
8 + 2

+ 6 · log

6
6 + 2

+ 2 · log

2
6 + 2

+ 0 · log

0
0 + 2

+ 2 · log

2
0 + 2

−1 + 2
2
log (10)
= −18.69.
For the network BNb we calculate the following counts, which can be read
and derived from the counts in Table 7.2: N ′
111 = 8, N ′
112 = 2, N ′
211 =
N211 + N221 = 6 and N ′
212 = N212 + N222 = 4. This gives us

7.3 Score-Based Learning
245
BIC(S | D)
=

8 · log

8
8 + 2

+ 2 · log

2
8 + 2

+ 6 · log

6
6 + 4

+ 6 · log

4
6 + 4

−1 + 1
2
log(10)
= −20.25.
That is, according to the BIC score we should choose Ba rather than Bb.
7.3.2 Search Procedures
Given a score function, the task is to ﬁnd the highest-scoring Bayesian network
structure among the set of all possible network structures. That is, the task
of structural learning has been reduced to a search problem. The challenging
part of this problem is that the size of the space of all structures is super-
exponential in the number of nodes (see equation (7.1)) so an exhaustive
enumeration of all the structures is not possible.
Instead, researchers have considered heuristic search strategies that move
around in the search space by iteratively performing small changes to the
current structure. Most commonly, these search methods work directly on the
space of Bayesian network structures; hence each point in such a search space
corresponds to a particular DAG; in the remainder of this section we shall use
the terms structure and DAG interchangeably, since the state spaces of the
variables in the structure is ﬁxed.
The deﬁnition of the search space determines the deﬁnition of the search
operators used to move from one structure to another. In turn, these operators
determine the neighborhood of a DAG, namely the DAGs that can be reached
in one step from the current DAG. Typically, the operators consist of:
•
arc addition: insert a single arc between two nonadjacent nodes.
•
arc deletion: remove a single arc between two nodes.
•
arc reversal: reverse the direction of a single arc.
In what follows we let op(S, A) represent the result of performing the arc
operation A on the structure S, i.e., op(S, A) is a DAG that diﬀers from S
with respect to one arc.
One important property of these operators is that they result only in local
changes to the current structure; for example, if an arc is inserted from node
Xi to Xj, then only the family of node Xj is changed, and similarly if an arc is
deleted; if an arc is reversed, then the families of both Xi and Xj are changed.
This property can be exploited when we have a so-called decomposable score
function.
Deﬁnition 7.3. A score function is said to be decomposable if it can be ex-
pressed as a sum of local scores, one for each family of nodes in the structure:

246
7 Learning the Structure of Bayesian Networks
score(D, S) =
n

i=1
score(Xi, pa(Xi), D).
The BIC score is an example of a decomposable score function for complete
data, since it can be written as
BIC(S | D) =
n

i=1
⎡
⎣
qi

j=1
ri

k=1
Nijk log
Nijk
Nij

−1
2qi(ri −1) log N
⎤
⎦.
This decomposition property can be used when we evaluate the beneﬁt of
making an arc change. For example, if we insert an arc from Xi to Xj, then
only the local score for Xj will change, i.e., when evaluating whether such a
move is beneﬁcial we need to evaluate only the score diﬀerence (or gain)
Δ(Xi →Xj) = score(Xj, pa(Xj) ∪{Xi}, D) −score(Xj, pa(Xj), D).
(7.6)
Greedy Search
A simple heuristic search procedure is greedy search: choose some initial struc-
ture (usually the empty structure, a randomly chosen structure, or a prior
structure speciﬁed by the user) and calculate the gain for each legal arc op-
eration; by legal we mean that the resulting graph should be acyclic. Next,
perform the arc operation A with highest gain (if positive) and use the result-
ing model as your current model. More formally:
Algorithm 7.2 [Greedy search]
1. Let S be an initial structure.
2. Repeat
a) Calculate Δ(A) for each legal arc operation A
- Let Δ∗= maxA Δ(A) and A∗= arg maxA Δ(A).
b) If Δ∗> 0, then
- Set S = op(S, A∗).
3. Until Δ∗≤0.
⊓⊔
It should be noted that in the greedy algorithm above you can further
exploit the decomposition property of the score function: If the parents sets
of two nodes, say Xi and Xj, do not change from one iteration to another,
then the gain (equation (7.6)) of any arc operation involving Xi and Xj will
remain unchanged. This gain can therefore be cached for subsequent iterations
so that the calculations can be reused.
Obviously, when we work with heuristic search algorithms we are not guar-
anteed to ﬁnd a global optimal structure but only a local optimal structure.
Several methods have been proposed to escape local maxima. An example of

7.3 Score-Based Learning
247
this is greedy search with multiple restarts: after a local maximum is found
the search is reinitialized with a random structure. This reinitialization is
then repeated for a ﬁxed number of iterations, and the best structure found
throughout the entire process is selected.
Prior Information
A way of reducing the search space (and thereby also the risk of ending up in
a local maximum) is to incorporate prior information, thus constraining the
models under investigation.
There are various standard ways of constraining the models to consider.
First, causality can be exploited. If possible, the nodes are clustered in a causal
hierarchy. You may, for example, consider a medical domain in which you have
disease nodes D, symptom nodes S, risk factor nodes R, and treatment nodes
T. Then, you need not consider links from a node in S to a node in T. The
full hierarchy is shown in Figure 7.15.
R
T
D
S
Fig. 7.15. A causal hierarchy for clusters of nodes. Directed links are allowed only
inside a cluster or downward in the hierarchy.
If R and T have two nodes, and D and S have three nodes, then the
hierarchy allows approximately 1015 diﬀerent DAGs. This is a considerable
reduction compared to 4.2 · 1018, but still it is extremely many. This prior
knowledge could then be included directly in the search algorithm by consid-
ering an arc operation as being legal only if it adheres to the causal hierarchy.
A more general approach could be to specify a partial ordering, ⪯, over
the variables, such that we allow an arc from Xi to Xj only if Xi ⪯Xj. In
the special case that we have a linear ordering, then the i’th node can have
at most i −1 parents producing 2i−1 diﬀerent parent sets. The number of
structures consistent with the ordering is therefore
n
	
i=1
2n−1 = 2
Pn−1
i=1 i = 2n(n−1)/2.
Although the number of structures is still exponential, specifying a linear or-
dering provides a substantial reduction. For example, with 10 nodes we have

248
7 Learning the Structure of Bayesian Networks
3.5 · 1013 diﬀerent structures as opposed to 4.2 · 1018 in the unrestricted case.
Whether it is reasonable to specify such an ordering is heavily dependent on
the domain in question. However, you could imagine diﬀerent rules of thumb.
For instance, if the variables represent events that manifest themselves at dif-
ferent points in time, then you may be able to order the variables according
to these time points. An example of this could be variables representing com-
ponents in a physical production process, where there is a time delay for an
item to move from one component to another.
Finally, you could also use more speciﬁc expert statements when reducing
the model space. All positive as well as negative statements on the presence
of links reduce the number by a factor between 2 and 3. Consider again the
medical domain above. If, for example, the expert states that the nodes in D
are independent given R and T (three links missing), then the model space
is reduced by a factor of 25.
*Equivalence Class Search
It can sometimes be advantageous to deﬁne the search space using a more
abstract representation than DAGs. An example is a procedure called greedy
equivalence search.
The search is based on the observation that data alone cannot be used
to discriminate among structures with the same d-separation properties (see
also Section 7.1.1).
Deﬁnition 7.4. Two network structures B1 and B2 are said to be equivalent
if they have the same d-separation properties.
The equivalence relation is reﬂexive, symmetric, and transitive; hence the
relation deﬁnes a collection of equivalence classes.
A score function assigning the same score to equivalent structures is said to
be score equivalent; the BIC score is an example of a score-equivalent function
(see Figure 7.16). This also means that if we have identiﬁed a particular
structure using a score equivalent function, then we could just as well pick
any other structure equivalent to the one identiﬁed. A way of making this
observation explicit is to deﬁne the search space such that each point in the
search space corresponds to an equivalence class.
In order to move around in the space of equivalence classes, we should
also specify a set of search operators, but due to the nature of the search
space, these operators are a bit more complex than the ones used in DAG
spaces. Instead we shall only deﬁne the neighborhood for an equivalence class:
the set of structures reachable by a single change to the current structure
or one of its equivalents. Since the equivalence classes are deﬁned in terms
of independence statements, we deﬁne the neighborhood of an equivalence
class in this way. We have an upper neighborhood consisting of equivalence
classes with fewer dependence statements, and a lower neighborhood with more
dependence statements. The two neighborhoods are deﬁned as the equivalence

7.3 Score-Based Learning
249
X1
X2
X3
X1
X2
X3
X1
X2
X3
Fig. 7.16. The three DAGs constitute an equivalence class, and any score equivalent
scoring function will assign the same score to all three structures.
classes that can be obtained by either adding or deleting a single arc from a
DAG in the current equivalence class. Figure 7.17 illustrates the diﬀerent
equivalence classes for three variables; the arcs attached to an equivalence
class identify the upper and lower neighborhoods.
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
X1
X2
X3
Fig. 7.17. The equivalence class hierarchy for the possible structures over three
variables.

250
7 Learning the Structure of Bayesian Networks
Based on this speciﬁcation of the search space, the greedy equivalence
search algorithm consists of two steps: First, start with the equivalence class
representing no dependencies among the variables (the bottom equivalence
class in Figure 7.17), and perform a greedy search upward until a local max-
imum is reached. Next, starting from the equivalence class just identiﬁed,
perform a greedy search downward until a local maximum is reached. It has
been proved that if the database is suﬃciently large, then the resulting equiv-
alence class is guaranteed to include the Bayesian network from which the
data was generated.
Finally, it should be emphasized that even though we have made another
speciﬁcation of the search space, we have unfortunately not solved the general
complexity problem that we faced in DAG spaces: the number of equivalence
classes also grows super-exponentially in the number of variables.
7.3.3 Chow–Liu Trees
The BIC score function incorporates a penalty term to control model com-
plexity. Another way of dealing with this issue is to put restrictions on the
allowable network structures so that overly complex structures are not con-
sidered. A particular simple class of Bayesian network structures is the set of
tree-shaped structures, where each node is allowed at most one parent (see
Figure 7.18). Not only is probability updating very easy in these networks,
but Chow and Liu also showed that a network of maximal likelihood can be
learned eﬃciently from a database; due to this result, these tree structures
are also called Chow–Liu trees.
X1
X2
X3
X4
X5
X6
Fig. 7.18. An example of a Chow–Liu tree structure.
Theorem 7.2 (Learning of Chow-Liu trees).
Let D be a data set over
the variables {X1, . . . , Xn}. A Chow-Liu tree of maximal likelihood can be
constructed as follows:
1. Calculate the mutual information MI(Xi, Xj) for each pair (Xi, Xj).
2. Consider the complete MI-weighted graph: the complete undirected graph
over {X, . . . , Xn}, where the links (Xi, Xj) have the weight MI(Xi, Xj).

7.3 Score-Based Learning
251
3. Build a maximal-weight spanning tree for the complete MI-weighted graph.
4. Direct the resulting tree by choosing any variable as a root and setting the
directions of the links to be outward from it.
5. Learn the parameters.
Notes:
•
The likelihood of a Bayesian network B given a data set D is the same as
described in Section 7.3.1: P(D|B).
•
The formula for mutual information is
MI(X, Y ) =

X,Y
P(X, Y ) log2
 P(X, Y )
P(X)P(Y )

.
•
A maximal-weight spanning tree can be constructed through Kruskal’s
algorithm: choose repeatedly a link of maximal weight not producing a
cycle.
•
Calculating the mutual information for a pair of variables requires one
sweep through the data. If the database consists of N cases, then this can
be done in time O(N), and since we need to perform this calculation for all
pairs of variables, the overall time complexity of the Chow–Liu algorithm
becomes O(n2 · N).
Example 7.3. Consider the Cold or Angina problem described in Section 3.1.2
and assume that we have a database of cases from this domain. For simplicity
we assume that the cases are a faithful sample from the model in Figure 3.6,
with the probabilities speciﬁed as in Section 3.2.5, Table 3.15 (Page 76) and
Table 3.20 (Page 96).
In order to learn a Chow–Liu tree for this domain, we start by calcu-
lating the mutual information between each pair of variables (the following
calculations are based on the speciﬁed model). For the variables Cold and
SoreThroat? (Sore) we get
MI(Cold, Sore) =

Cold,Sore
P(Cold, Sore) log2
 P(Cold, Sore)
P(Cold)P(Sore)

= 0.02101216.
The mutual information for all pairs of variables is given in Table 7.3.
Based on these calculations we can construct a maximal-weight spanning
tree by starting from the empty graph, and iteratively adding an edge with
maximum weight as long as no cycle is created. The resulting structure is
shown in Figure 7.19(a), and by picking Fever? as a root and directing the
edges away from Fever? we obtain the Chow–Liu tree in Figure 7.19(b).
Since the learned model has a tree structure, the model may specify (con-
ditional) independences that are not reﬂected in the data. On the other hand,

252
7 Learning the Structure of Bayesian Networks
MI(Cold, Angina) = 0
MI(Fever?, Angina) = 0.015076
MI(SoreThroat?, Angina) = 0.018016 MI(SeeSpots?, Angina) = 0.0180588
MI(Cold, Fever?) = 0.014392
MI(Cold, SoreThroat?) = 0.0210122
MI(Cold, SeeSpots?) = 0
MI(SoreThroat, Fever?) = 0.0015214
MI(Fever?, SeeSpots?) = 0.0017066
MI(SeeSpots?, SoreThroat?) = 0.0070697
Table 7.3. The mutual information for each pairs of variables.
Fever?
Fever?
SoreThroat?
SoreThroat?
SeeSpots?
SeeSpots?
Cold
Cold
Angina
Angina
(a)
(b)
Fig. 7.19. Figure (a) shows a maximal weight spanning tree based on the MI-
calculations in Table 7.3. Figure (b) shows the Chow-Liu tree obtained by selecting
the variable Fever? as root and directing the edges away from Fever?.
even though the independence properties are inaccurate, it has turned out
that the model may still provide a good approximation. We shall return to
this issue in Chapter 8, where we will use Chow–Liu trees in a classiﬁcation
context.
Proof. [Learning of Chow–Liu trees, Theorem 7.2 ]
The proof involves some pencil pushing. First we rewrite the log-likelihood
of a Bayesian network B given the data D = (d1, . . . , dN):
log2 P(D | B) = log2
N
	
j=1
P(dj | B) =
N

j=1
log2 P(dj | B)
=
N

j=1
n

i=1
log2 P(Xi = dj | pa(Xi) = dj, B).
The number of cases in D that agree on a particular conﬁguration of Xi and
pa(Xi) is given by N(Xi, pa(Xi)) = N · P #(Xi, pa(Xi)). Hence instead of
summing over all the cases, we can write
log2 P(D | B) = N ·
n

i=1

Xi,pa(Xi)
P #(Xi, pa(Xi)) · log2 P(Xi | pa(Xi), B).
Since we are looking for a Bayesian network of maximal likelihood, we can
assume that the parameters of B are maximum likelihood parameters (see
Section 6.1.1), i.e., P(X | pa(X), B) = P #(X | pa(X)), and therefore

7.3 Score-Based Learning
253
log2 P(D | B) = N ·
n

i=1

Xi,pa(Xi)
P #(Xi, pa(Xi)) · log2 P #(Xi | pa(Xi)).
This equation can be rewritten as
log2 P(D | B) = N ·
n

i=1

Xi,pa(Xi)
P #(Xi, pa(Xi)) ·

log2
P #(Xi, pa(Xi))
P #(Xi)P #(pa(Xi))
+ log2 P #(Xi)

,
and since the parent sets contain at most one variable, we get
log2 P(D | B) = N ·
n

i=1
MI(Xi, pa(Xi)) +
n

i=1

Xi
P #(Xi) · log2 P #(Xi).
This expression is maximized by choosing parents such that the sum of the
MI terms is maximized. Since B should be a tree and each parent set contains
at most one variable, step three in the theorem is guaranteed to maximize the
log-likelihood. Finally, by choosing an arbitrary root and directing the arcs
away from the root, we ensure that each node will get at most one parent,
and from the d-separation properties we also see that we get the same inde-
pendence properties regardless of the choice of root.
⊓⊔
7.3.4 *Bayesian Score Functions
The BIC score is an example of a score function combining a maximum likeli-
hood term with a term measuring complexity. Another approach for measuring
the ﬁtness of a Bayesian network model structure, S, is to calculate the poste-
rior probability that the data was generated by a distribution with the same
independence properties as S. If we abuse the notation slightly, and also use
S to denote the hypothesis that the data is sampled from a distribution with
the same independence properties as S, then we have:
P(S | D) = P(D, S)
P(D)
= P(S)P(D|S)
P(D)
= μP(S)P(D|S),
(7.7)
where μ = P(D) is the normalization constant. This constant does not
depend on S, and it is therefore not necessary to calculate it when we
compare two network structures. Actually, if you were to calculate P(D)
you would be faced with a computational problem, because the calculation
of this constant involves summing over all possible model structures, i.e.,
P(D) = 
B P(B)P(D|B).
From equation (7.7) we see that in order to score a structure based on its
posterior probability given the data, we only need two terms, namely the prior

254
7 Learning the Structure of Bayesian Networks
probability of the structure (P(S)) and the marginal likelihood of the structure
given the data (P(D | S)). Typically you would choose a prior probability
distribution for the structures that is relatively easy to calculate, and the main
computational problem is therefore the calculation of the marginal likelihood,
where we will have to deal with the parameters of the model θS (we shall
return to the speciﬁcation of structure priors in Section 7.3.4):
P(D|S) =

θS
P(D|S, θS)f(θS|S)dθS,
(7.8)
where f(θS | S) is a prior probability distribution over the parameters (con-
ditional probabilities) for S. The integral in the above equation is over all
parameters, and, in eﬀect, over all possible Bayesian networks with the same
structure but with diﬀerent conditional probability distributions. Intuitively,
the marginal likelihood can therefore be interpreted as the probability that we
could generate the database D if we were to randomly select the parameters
for S according to the parameter prior f(θS | S).
As hinted above, the hard part in the calculation of P(S | D) is the eval-
uation of the integral in equation (7.8). Fortunately, it has been shown that
the evaluation of this integral can be reduced to a simple counting problem
based on the following six assumptions:
1. the database D is a faithful sample from some Bayesian network;
2. the cases in the database D are independent given the BN model;
3. the database is complete;
4. the prior distribution of the parameters in every Bayesian network is uni-
form;
5. [local independence] for any two conﬁgurations over the parents for a vari-
able Xi, the parameters for the conditional probability distributions asso-
ciated with Xi are independent; and
6. [global independence] the densities of the parameters for the conditional
probability distributions for Xi and Xj are independent for i ̸= j.
Now let us again use Nijk to denote the number of cases in the database that
include the conﬁguration (Xi = k, pa(Xi) = j). Based on the assumptions
above, the following theorem has been proved.
Theorem 7.3. Let D be a database over the variables X1, X2, . . . , Xn, and
consider the Bayesian network structure Bs over the same set of variables.
Given the six assumptions above, it holds that
P(D | S) =
n
	
i=1
qi
	
j=1
(ri −1)!
(Nij + r1 −1)!
ri
	
k=1
(Nijk)! ,
(7.9)
where Nij = ri
k=1 Nijk.
This means that the evaluation of the integral in equation (7.8) is reduced to
a counting problem, which can be carried out in polynomial time.

7.3 Score-Based Learning
255
Example 7.4. Consider again the two Bayesian network structures from Fig.
7.14, and assume that we have the database from Table 7.2 for the two binary
variables X1 and X2.
Let us also assume that we have a priori the same belief in the two network
structures, P(BNa) = P(BNb). In order to select between Ba and Bb, the
task then reduces to the calculation of
P(D|S) =
n
	
i=1
qi
	
j=1
(ri −1)!
(Nij + r1 −1)!
ri
	
k=1
(Nijk)!
for both networks. To start oﬀ, consider network Ba (X1 →X2). As for the
calculation of the BIC score (see Example 7.2), we also need the following
counts, which can be found from the database: N111 = 8, N112 = 2, N211 = 6,
N212 = 2, N221 = 0, and N222 = 2. By using these values we get
P(D|BNa) =
(2 −1)!8!2!(2 −1)!6!2!(2 −1)!2!0!
(10 + 2 −1)!(8 + 2 −1)!(2 + 2 −1)! = 2.67 · 10−6.
For the network BNb we have N111 = 8, N112 = 2, N211 = 6, and N212 = 4.
This gives us
P(D|BNb) =
(2 −1)!8!2!(2 −1)!6!4!
(10 + 2 −1)!(10 + 2 −1)! = 8.75 · 10−7.
So with a uniform prior distribution over both structure and parameters we
should prefer Ba over Bb.
Although the metric provides a simple expression for calculating the like-
lihood of a structure, it also rests on assumptions that may not always be
appropriate. Most notably, it requires that the database be complete. When
these assumptions are not fulﬁlled you will have to resort to other methods,
such as the BIC score or constraint-based algorithms.
Prior Distribution over Structures
In order to score a network structure using the metric above, you have to
specify a prior distribution P(S) over the network structures. The speciﬁcation
of this prior can be used to guide the subsequent structure search, although the
contribution from the prior distribution is usually dominated by the likelihood
term P(D | S) when the database gets large (P(D | S) decreases exponentially
fast as cases are added to the database). One exception, however, occurs when
some of the network structures are given zero probability a priori, in which
case the data cannot change that belief.
Common to most (if not all) prior distributions over structures currently
used is that they can be expressed as a product (or sum) with one term for
each family of nodes in the network:

256
7 Learning the Structure of Bayesian Networks
P(S) = c ·
n
	
i=1
ρ(Xi, pa(Xi)),
where c is a normalization constant that does not depend on S. These types
of prior probability distributions are decomposable, which means that equa-
tion (7.9) is also decomposable (see Deﬁnition 7.3).
The simplest prior distribution is the one that encodes complete ignorance,
i.e., we use an even distribution over the possible network structures:
ρ(Xi, pa(Xi)) = 1.
A more informative prior that has been suggested is based on the diﬀerence
between the families in the current network S and the families in a user-
speciﬁed prior network Bp. Speciﬁcally, let δi denote the number of parents
that BP and S disagree on for Xi:
δi = |(pa(Xi)S ∪pa(Xi)BP ) \ (pa(Xi)S ∩pa(Xi)BP )| .
(7.10)
Then we can give a low prior probability to structures that are far away from
the prior network structure, BP , by setting
ρ(Xi, pa(Xi)) = κ
Pn
i=1 δi,
where 0 < κ ≤1 is a user-speciﬁed constant.
Example 7.5. Consider the four Bayesian network structures depicted in Fig.
7.20, and assume that Figure 7.20(BP ) is a prior network speciﬁed by the
user.
X3
X1
X2
X3
X1
X2
BP
B1
X3
X1
X2
X3
X1
X2
B2
B3
Fig. 7.20. The candidate structures B1, B2, and B3 are assigned a prior distribution
based on the number of arcs they have in common with the prior network BP .

7.3 Score-Based Learning
257
When using equation (7.10) to calculate the unnormalized prior probability
for the candidate structure B1, we ﬁrst calculate the diﬀerences in the node
families:
δB1
1
= |({X2} ∪∅) \ ({X2} ∩∅)| = 1;
δB1
2
= |(∅∪{X1}) \ (∅∩{X1})| = 1;
δB1
3
= |({X2} ∪{X2}) \ ({X2} ∩{X2})| = 0.
Hence, the total diﬀerence between the two structures is measured as δB1 =
δB1
1
+ δB2
1
+ δB3
1
= 2, which gives the prior probability
P(B1) = c · κδ = c · κ2.
(7.11)
For B2 we get δB2
1
= 1, δB2
2
= 2, and δB2
3
= 1, and therefore δ = 4 and
P(B2) = c · κ4. Finally, for B3 we have δB3
1
= 0 , δB3
2
= 1 and δB3
3
=
1, and therefore P(B3) = c · κ2. That is, a priori, Bp would be given the
highest probability, then B1 and B3, and ﬁnally, B2 would be given the lowest
probability; observe that the normalization constant is of no importance when
comparing structures.
Finally, it should be emphasized that although we can easily come up with
elaborate prior distributions, there is also a caveat: the prior distribution does
not necessarily assign the same score to equivalent network structures (as in
Example 7.5). When this is the case, then if it is used to deﬁne, say, the score
function in equation (7.9), the resulting score function is not score equivalent.
As an example, consider equation (7.10), and use any prior network structure
that is diﬀerent from the empty graph.
Regulating Model Complexity
An attractive property of the BD score (and likelihood based scoring functions
in general) is that it has an intrinsic property that no extra term is needed
for penalizing complexity.
The intuition why the BD score is less likely to pick out an overﬁtted
network structure is closely related to the Bayesian version of Ockham’s razor:
A complex structure with few conditional independences can generate many
possible data sets, so it is unlikely that it has generated this particular data
set at hand (see Figure 7.21 for an illustration). Obviously, models that are
too simple are also unlikely to have generated the data.
To provide a speciﬁc example, consider again the Bayesian network struc-
tures depicted in Figure 7.14. From the model in Figure 7.14(a) you can sample
a database, and then use it to score the model structure S in Figure 7.14(b),
where X1 and X2 are independent. Speciﬁcally, let the databases be generated
according to the following probability distributions: P(X1) = (0.5, 0.5),

258
7 Learning the Structure of Bayesian Networks
D
P(· | BS)
Diﬀerent datasets
Fig. 7.21. The ﬁgure illustrates the marginal likelihood for three diﬀerent struc-
tures; the dotted line represents a structure that is too complex, the dashed line
represents a structure that is too simple, and the solid line represents an appropri-
ate structure.
P(X2 = 1|X1 = 0) = 0.5(1 −ϵ),
P(X2 = 1|X1 = 1) = 0.5(1 + ϵ),
where the parameter ϵ varies between 0 and 1 and is used to control the
strength of the dependency between X1 and X2; the larger the value of ϵ, the
stronger the dependency. A plot of P(S | D) for four diﬀerent database sizes
(generated for various values of ϵ) is depicted in Figure 7.22. In particular,
we can see that when the database is not too large, S is acceptable even for
relatively large values of ϵ. For example, with ϵ = 0.2 we have that P(S | D) ≈
0.6 for a database with 100 cases.
7.4 Summary
Constraint-Based Methods
The structure of a Bayesian network can be learned from independence state-
ments of the form, “A independent of B given C” (denoted by I(A, B, C)):
1. Find the skeleton of the Bayesian network: the link A −B is part of the
skeleton if and only if ¬I(A, B, X) for all X not containing A or B.
2. Direct the links:
Introduction of v-structures: If you have three nodes, A, B, C, such that
A −C and B −C, but not A −B, then introduce the v-structure
A →C ←B if there exists an X (possibly empty) such that I(A, B, X)
and C ̸∈X.
Avoid new v-structures: When Rule 1 has been exhausted, and you have
A →C −B (and no link between A and B), then direct C →B.
Avoid cycles: If A →B introduces a directed cycle in the graph, then do
A ←B.
Choose randomly: If none of the rules 1–3 can be applied anywhere in the
graph, choose an undirected link and give it an arbitrary direction.

7.4 Summary
259
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
N=10
N=100
N=1000
N=10000
ϵ
P(S | D)
Fig. 7.22. A plot of P(complete independence | D) for four diﬀerent database sizes.
The databases were sampled from a model with two variables, where the level of
dependency between the variables was controlled by the value of ϵ (a high value
implies a high dependency).
The independence statements can be established through statistical tests on
a database.
The PC algorithm: The initial skeleton can be found using the PC-algorithm:
1. Start with the complete graph;
2. i := 0;
3. while a node has at least i + 1 neighbors
-
for all nodes A with at least i + 1 neighbors
-
for all neighbors B of A
-
for all neighbor sets X such that |X| = i and X ⊆(nb(A) \
{B})
- if I(A, B, X) then remove the link A −B and store
“I(A, B, X)”
-
i := i + 1
Score-Based Methods
A Bayesian network can be learned from a database by performing a search
in the space of all DAGs and selecting the one with the highest score.
The BIC Score:

260
7 Learning the Structure of Bayesian Networks
BIC(S | D) =
N

i=1
log2 P(d | ˆθS, S) −size(S)
2
log2(N).
Chow–Liu trees
A tree-shaped Bayesian network of maximal likelihood can be learned in poly-
nomial time using the Chow–Liu algorithm:
1. Calculate the mutual information MI(Xi, Xj) for each pair (Xi, Xj).
2. Consider the complete MI-weighted graph: the complete undirected graph
over {X, . . . , Xn}, where the links (Xi, Xj) have the weight MI(Xi, Xj).
3. Build a maximal-weight spanning tree for the complete MI-weighted
graph.
4. Direct the resulting tree by choosing any variable as a root and setting
the directions of the links to be outward from it.
5. Learn the parameters.
Mutual information:
MI(X, Y ) =

X,Y
P(X, Y ) log2
 P(X, Y )
P(X)P(Y )

.
7.5 Bibliographical Notes
The ﬁrst method for automated learning of Bayesian networks was the method
of Chow and Liu (1968), which learned tree-structured models. A statistical
approach for learning Bayesian networks through manually selected indepen-
dence tests was given by Edwards and Havranek (1985). The PC algorithm
was developed by Spirtes et al. (1993); see also (Spirtes et al., 2000). It is an
extension of work by Wermuth and Lauritzen (1990) and Verma and Pearl
(1991). The necessary path condition and uncertain areas are due to Steck
(2001). Improved algorithms, which theoretically should be more robust in
face of ﬂawed independence tests, are given by Margaritis and Thrun (1999)
and Cheng et al. (2002). A discussion on observing causality can be found in
(Pearl, 2000).
The dimensionality of models with hidden variables has been explored by
Geiger et al. (1996) in the context of model selection. Here the BIC score
(Schwarz, 1978) was extended to Bayesian networks with hidden variables.
The BIC score of a model is an asymptotic approximation to the marginal
likelihood of that model, and it is equivalent to the minimum description
length proposed by Rissanen (1987), and adopted to a decomposable consis-
tent score for Bayesian networks by Lam and Bacchus (1994) and Friedman
and Goldszmidt (1998). A Bayesian metric for scoring models was proposed
by Cooper and Herskovits (1991) and generalized in (Cooper and Herskovits,
1992). Cooper and Herskovits (1992) also proposed a search algorithm (known

7.6 Exercises
261
as the K2 algorithm) that performs a greedy search conditioned on a linear
ordering of the variables (for literature on search in general, see (Michalewicz
and Fogel, 2000)). Heckerman et al. (1995b) considered the speciﬁcation of
prior information such that equivalent network structures (Chickering, 1995)
are given the same score. In the context of equivalent structures, greedy search
procedures have been proposed by Chickering (2002); Chickering and Meek
(2002) that are guaranteed to identify the correct structure when the amount
of data grows large. (Chickering et al., 2004) is one of the latest in a line
of results that show that the task of learning Bayesian network structures
is NP-hard, and Cowell (2001) has shown that, under often-quoted assump-
tions, constraint-based learning and score based learning are equivalent. In
the context of missing data, Friedman (1998) has proposed a structural learn-
ing method that follows the intuition of the EM-algorithm. Finally, Friedman
and Koller (2003) provide a method for calculating the posterior probability
of absence or presence of individual arcs in the generating net given the data.
Other sources of literature that can be recommended for further reading in-
clude (Buntine, 1996), (Heckerman, 1998), (Jordan, 1998), and (Cowell et al.,
1999).
7.6 Exercises
Exercise 7.1. Apply the PC algorithm to learn a skeleton over the six vari-
ables A, B, C, D, E, and F (use the network structure in Figure 7.23 as an
oracle). Using rules 1–4, exploit the identiﬁed independence statements to set
directions on the links in the skeleton.
F
A
B
C
D
E
Fig. 7.23. Use the Bayesian network structure as an oracle for Exercise 7.1.
Exercise 7.2. Use rules 1–4 to set the directions on the remaining links in
the structure in Figure 7.24.
Exercise 7.3. Assume that the PC-algorithm is run on ﬁve variables A,
B, C, D, and E. During its running time, the algorithm gets positive

262
7 Learning the Structure of Bayesian Networks
I
A
B
C
D
E
F
G
H
Fig. 7.24. A partial Bayesian network structure found by the PC algorithm and
rule 1.
replies only to the following oracle queries: I(A, B, E), I(A, C, E), I(A, D, E),
I(B, E, {C, D}), and I(C, D). The result of the run is a Bayesian network M.
What does the skeleton of M look like? Which graphs can M be?
Exercise 7.4.
(i) Find a (tight) upper bound on the number of independence tests per-
formed by the PC algorithm.
(ii) Discuss an implementation strategy for the PC algorithm with focus on
the time used to perform the independence tests required by the algo-
rithm (that is, calculating the conditional mutual independence expres-
sion, equation (7.2)).
Exercise 7.5. Prove that
I(A, B, X) ⇔CMI(A, B|X) = 0.
Exercise 7.6. Show that the size of the BN in Figure 7.13(c) is larger than
the size of the BN in Figure 7.13(a).
Exercise 7.7. What is the size (see Proposition 7.1) of the BN shown in
Figure 7.3(c) assuming that all variables are ternary.
Exercise 7.8. What is the BIC score, based on the data in Table 7.4, for the
structure in Figure 7.25? What is the score for the structure in Figure 7.26?
Exercise 7.9. E Calculate the BIC score for the model of the simpliﬁed in-
semination problem described in Section 3.1.3, based on the (incomplete)
database in Example 6.2.

7.6 Exercises
263
C B A C B A
1 1 1
1 1 2
1 1 1
1 1 2
1 1 2
2 2 1
2 1 2
1 1 2
1 1 1
2 1 2
1 1 2
1 1 1
2 1 2
1 1 1
1 1 1
2 1 2
1 1 1
1 1 2
1 1 2
1 1 1
1 1 1
1 1 1
1 1 1
2 1 2
1 1 2
1 1 2
2 2 1
1 1 2
1 1 2
2 2 1
2 1 2
1 1 2
Table 7.4. A number of conﬁgurations over binary variables A, B, and C.
A
B
Fig. 7.25. A Bayesian network for Exercise 7.8.
A
B
C
Fig. 7.26. A Bayesian network Exercise 7.8.
Exercise 7.10. What is the result of running greedy search based on the BIC
score and the data in Table 7.4 starting from the empty graph?
Exercise 7.11. Show that the two expressions (in equation (7.4) and equa-
tion (7.5)) for the BIC score are identical.
Exercise 7.12. Tabu search is a general search technique based on greedy
search. The technique tries to avoid getting stuck in local minima by pro-
hibiting moves that involve aspects that were changed by a recent move. How
could Algorithm 7.2 be modiﬁed to incorporate this behavior?
Exercise 7.13. Simulated annealing is a general search technique based on
greedy search. The technique tries to explore more parts of the search space
by making totally random moves at ﬁrst, ignoring the score of the parts of
the search space it moves to. Gradually it starts letting the scores inﬂuence
the search, and ﬁnally ends up moving only if the score improves (as greedy
search always behaves). How could Algorithm 7.2 be modiﬁed into a simulated

264
7 Learning the Structure of Bayesian Networks
annealing search algorithm? (Hint: Use a counter i that is decreased at each
iteration, and an error term like e−i to modify scores.)
Exercise 7.14. What network structures are equivalent to the network in
Figure 7.27?
A
B
C
D
Fig. 7.27. A Bayesian network for Exercise 7.14.
Exercise 7.15. Learn a Chow–Liu tree from the data in Table 7.4.
Exercise 7.16. Complete Example 7.4 by calculating the BD score for the
Bayesian network structure shown in Figure 7.28 based on the database in
Table 7.2. As in Example 7.4 we assume that all network structures are a
priori equally probable and that we have a uniform prior over all the possible
parameters.
X1
X2
Fig. 7.28. Together with the network structures shown in Figure 7.14, this BN
structure deﬁnes the space of model structures covering two variables.
(i) Calculate P(D), the prior probability of the data.
(ii) Calculate the conditional probabilities for the three network structures
given the database.
(iii) What should the prior probability for the empty graph (at least) have
been for it to be picked by the BD score? Give an intuitive reason.
Exercise 7.17. Consider the database in Table 7.2 and a prior network struc-
ture consisting of an arc from X2 to X1. What is the result of learning with
a greedy search and the BD score introduced in (7.7)?
Exercise 7.18. Show that when using a nonempty prior network structure
together with equation (7.10), the resulting prior distribution cannot be score
equivalent.

8
Bayesian Networks as Classiﬁers
You receive an email and wish to determine whether it is spam; you see a
bird and wish to determine its species; you examine a patient and wish to
diagnose him. These are only a few examples of the very common human task
of classiﬁcation.
Formally, you have a set of variables, {F1, . . . , Fn}, called features (or at-
tributes) and a class variable, C, where the states of C correspond to the
possible classes. For the bird example above, the feature variables would en-
code various characteristics of the bird, and the class variable would represent
the possible species. Since it often happens that some feature values are not
known, feature variables are often extended with state “?” for unknown (or
“missing value”). A case is said to be complete if there are no missing values.
A case set is said to be consistent if two complete cases with the same values
on the features are of the same class.
A classiﬁer is a function from F1 × · · · × Fn to C. We shall deal only with
classiﬁcation tasks over a ﬁnite set of classes and with discrete features.
If you have a Bayesian network model, it can be used for classiﬁcation.
In fact, if there is only one hypothesis variable, the network is a model for
classiﬁcation. In the pregnancy model (Section 3.1.3), for example, test infor-
mation is used to classify the state of the cow, the class being the state of
highest probability.
In this chapter we consider learning of classiﬁers. Let D be a data set
of cases over features {F1, . . . , Fm} and class variable C; we do not require
the data set to be consistent. We wish to use the data set for constructing
a classiﬁer. If the space of feature conﬁgurations is small and the amount of
data is relatively large, you may use the data set to establish a look-up table:
given a complete case f of features, look up f in the data set. If there are cases
in D with feature values f, then return the majority class value. If f is not
present in D, then return the most frequent class value in D. However, this
method is tractable only for small conﬁguration sets; even with a moderate
number of feature variables you will need a more compact representation of

266
8 Bayesian Networks as Classiﬁers
the classiﬁcation function. Any other method for learning classiﬁers should
predominantly produce better classiﬁers.
8.1 Naive Bayes Classiﬁers
Consider the poker game model introduced in Section 3.2.3, and extend the
model with a variable for my hand (MH) and for best hand (BH) (see Exer-
cise 3.14). A Bayesian network model would be like the one in Figure 8.1.
OH0
OH1
OH2
FC
SC
BH
MH
Fig. 8.1. A Bayesian network for the poker game extended with a node for my hand
and best hand.
Assume that you have a set of cases over the observable features MH,
opponent’s change of cards, FC, SC), and the class (BH). Exploiting structural
learning will most likely result in the model in Figure 8.2. The reader may
test this by a manual run of the PC algorithm on Figure 8.1 with the variables
(OHi) hidden.
FC
SC
BH
MH
Fig. 8.2. A Bayesian network learned from a case set of poker games.
The model in Figure 8.2 does not provide a compact representation of the
classiﬁcation function, since the class variable has all features as parents, and
therefore the conditional probability table for the class variable is as large as
a look-up table for the classiﬁcation problem. Unfortunately, it is often seen

8.1 Naive Bayes Classiﬁers
267
in connection to Bayesian network classiﬁers that in the correct model, the
class variable has (almost) all feature variables as parents, and the network
therefore becomes intractably large. Instead, you can insist on working with
a class of simpler structures and search for the model that best approximates
the correct structure.
One such class of models could be naive Bayes networks (see Section 3.1.5),
and for the poker game, the structure will be the one in Figure 8.3.
FC
SC
BH
MH
Fig. 8.3. Naive Bayes structure for the poker game.
In general, in a naive Bayes classiﬁer (NBC) each feature variable has the
class variable as its only parent. This means that the structure is ﬁxed, and
the only task involved in learning is to estimate the parameters.
The parameters for an NBC are easily determined by the methods pre-
sented in Chapter 6. If the cases are complete, you can determine a maximal
likelihood model through simple counting. If a case contains missing values,
the EM algorithm can be used; equivalently, disregard that case for the at-
tributes that are missing.
All methods for learning classiﬁers from data have a problem with very rare
cases, which may not be represented in the data set. Assume, for example,
that the data set for learning a poker classiﬁer does not contain a case in
which I have lost with a hand with 3v. If one is not careful, the classiﬁer
would deem this impossible regardless of the pattern of card changes. For a
Bayesian network classiﬁer, this problem corresponds to incorrectly setting
a parameter to zero. To avoid zero values for parameters, you may simulate
Bayesian learning by introducing virtual cases. An easy way of handling this
is initially to give all parameters a small positive count.
Since NBCs are easy to learn, and easy to use as classiﬁers, and since they
are very ﬂexible with respect to missing values, they are very widespread.
As mentioned in Section 3.1.5, NBCs assume the features to be independent
given the class, and even though this is rarely the case, NBCs have proved
surprisingly precise. A reason for this is that when doing classiﬁcation we
are interested only in the class of maximal probability and not in the exact
probability distribution over the classes.

268
8 Bayesian Networks as Classiﬁers
8.2 Evaluation of Classiﬁers
Assume that you have a classiﬁer, Clsf, and a data set of cases covering the
feature variables and the class variable. We wish to characterize the quality
of Clsf. A way of characterizing Clsf is to calculate its classiﬁcation accuracy:
the fraction of correctly classiﬁed cases.
A more detailed description of a classiﬁer would be to calculate the confu-
sion matrix, P #(Classiﬁed value, Correct value). In addition to the confusion
matrix you can also introduce a value for how bad a misclassiﬁcation is, and
thereby establish a loss matrix, describing a punishment for the various kinds
of misclassiﬁcation.
To illustrate this, consider again the poker game. Assume that you have
established a classiﬁer Pcl, and you have the set of cases in Table 8.1. Since
12 out of 20 cases are classiﬁed correctly, the classiﬁcation accuracy is 0.6.
Case number: BH MH FC SC Pcl
1
op
no
3
1
op
2
op
1a
2
1
op
3
draw 2 v
1
1
op
4
me
2 a
1
1
me
5
draw
ﬂ
1
1
me
6
me
st
3
2
me
7
me
3 v
1
1
me
8
me
sﬂ
1
0
me
9
op
no
0
0
op
10
op
1 a
3
2
me
11
draw 2 v
2
1
op
12
me
2 v
3
2 draw
13
op
2 v
1
1 draw
14
op
2 v
3
0
op
15
me
2 v
3
2
me
16
draw no
3
2 draw
17
draw 2 v
1
1 draw
18
op
ﬂ
1
1
me
19
op
no
3
2
op
20
me
1 a
3
2
op
Table 8.1. Test cases for a poker classiﬁer. The entry Pcl is the class value provided
by the classiﬁer.
The confusion matrix is given in Table 8.2, but it does not consider the
stakes involved in the poker game. Let the situation be that both players ini-
tially have bet a euro, and you have to decide whether to fold (your opponent
takes the pot) or to call. To simplify, assume that you place a euro when you
call, and your opponent is forced to place a euro. The winner takes the pot,

8.2 Evaluation of Classiﬁers
269
BH
me draw op
Plc
me
0.25 0.05 0.1
draw 0.05 0.1 0.05
op
0.05 0.1 0.25
Table 8.2. Confusion matrix for the poker classiﬁer. The sum of the diagonal ele-
ments is the classiﬁcation accuracy.
and in the case of a draw you share the pot. The wins and losses in the various
situations are given in Table 8.3.
BH
me draw op
Action fold 0
0
0
call
3
1
−1
Table 8.3. Wins and losses in the poker game.
Based on Table 8.3, you decide on the strategy to call if and only if the
classiﬁer says m or draw. The loss matrix tells you what you lose by following
the classiﬁer compared to a situation with certainty on BH. It is given in
Table 8.4.
BH
me draw op
Plc
me
0
0
−1
draw 0
0
−1
op
−3 −1
0
Table 8.4. Loss matrix for the poker classiﬁer.
The confusion matrix and the cost matrix can now be used to calculate
the expected loss of a strategy following the classiﬁer (based on the data set
D):

270
8 Bayesian Networks as Classiﬁers
Expected loss
=

Classiﬁed,Correct
P #(Classiﬁed | Correct)P #(Correct)
× Loss(Classiﬁed, Correct)
=

Classiﬁed,Correct
P #(Classiﬁed, Correct)Loss(Classiﬁed, Correct).
That is, you ﬁrst multiply the confusion matrix and the loss matrix term
by term, and then you take the sum of all these elements.
The expected loss for the poker classiﬁer is

Plc,BH
P #(Plc, BH)Loss(Plc, BH) = −3.0.05 −1.0.1 −1.0.1 −1.0.05 = −0.4.
A general problem in connection to machine learning is overﬁtting. What
we are looking for is a classiﬁer that can classify not-yet-seen cases. However,
it may happen that the learned classiﬁer is very accurate on the training
data, but it is very poor when confronted with cases not represented there.
To monitor overﬁtting, you usually divide the set data into training and test
data, and you measure the classiﬁcation accuracy on the test data set rather
than on the training data set. A way of addressing overﬁtting in the choice of
model is to reserve a part of the training set for validation and comparison of
models only and not for establishing the models.
8.3 Extensions of Naive Bayes Classiﬁers
NBCs assume that the feature variables are independent given the class. Even
though this assumption seldom holds, NBCs are surprisingly good with re-
spect to classiﬁcation accuracy. However, as described in the previous section,
classiﬁcation accuracy does not tell the full story. Often you are particularly
interested in detecting a rare class. The class being rare also means that clas-
siﬁcation accuracy does not drop signiﬁcantly if your classiﬁer never identiﬁes
these cases.
A rare class is often identiﬁed through a set of feature values appearing
together, where each value by itself does not point in that direction. NBCs
cannot cope with that, since they assume the features to be independent given
the class. Therefore, you may wish to extend NBCs to allow more elaborate
dependency structure among feature variables. A simple extension of this kind
is the tree augmented naive Bayes classiﬁer (TAN): each feature variable has
at most one feature variable as parent.
As opposed to the situation for NBCs, the structure is not given, and we
have to look for a structure that with optimal parameter setting has maximal
likelihood: out of the possible links between feature nodes we have to choose

8.3 Extensions of Naive Bayes Classiﬁers
271
a set forming a tree. This is similar to the situation described in Section 7.3.3,
and not surprisingly, the problem is solved through a slight modiﬁcation of
the Chow–Liu algorithm using conditional mutual information rather than
mutual information (see equation (7.2), Page 237).
We give the construction without proof.
Theorem 8.1 (Learning TANs). Let D be a data set over the variables
{F1, . . . , Fm, C}. A TAN of maximal likelihood can be constructed as follows:
1. Calculate the conditional mutual information MI(Fi, Fj | C) for each pair
(Fi, Fj).
2. Consider the complete MI-weighted graph: the complete undirected graph
over {F1, . . . , Fn}, where the links Fi −Fj have the weight MI(Fi, Fj | C).
3. Build a maximal-weight spanning tree for the complete MI-weighted graph.
4. Direct the resulting tree by choosing any variable as a root and setting the
directions of the links to be outward from it.
5. Add the node C and a directed link from C to each feature node.
6. Learn the parameters.
Running the TAN algorithm on the data for the poker domain resulted in
the TAN in Figure 8.4.
FC
SC
BH
MH
Fig. 8.4. A TAN for classifying poker.
Another extension is to introduce intermediate variables. For the poker
example, the dependence between FC and SC can be mediated through a
hidden variable C, as illustrated in Figure 8.5.
A problem with hidden variables is that even if you know how to connect
the hidden variables introduced, you have to determine the number of states
of the hidden variables. Let H be a hidden variable with n states and with
children ch(H). If n is equal to the product of the number of states of the
children, then H can represent any conﬁguration of ch(H), and you cannot
hope for a better ﬁt. On the other hand, in that case, you should represent the
product of ch(H) directly without a hidden variable. For the poker example
it means that the number of states of C should be between 2 and 11. Now use
the EM algorithm for these ten possible numbers of states. Since the likelihood
increases with the number of states of C, the model of maximal likelihood has
eleven hidden states, and that is not really what you are after. Therefore, you
have to balance likelihood with size as described in Section 7.3.1.

272
8 Bayesian Networks as Classiﬁers
C
FC
SC
BH
MH
Fig. 8.5. The dependence between FC and SC is mediated by the hidden variable
C.
8.4 Classiﬁcation Trees
For the sake of completeness we shall in this section present a very popular
method for doing classiﬁcation. In the data mining literature the method is
called a decision tree. However, since in this book we use this term diﬀerently
(see Section 9.3), we shall call it a classiﬁcation tree.
A classiﬁcation tree is a directed tree whose internal nodes are feature
variables. The links are labeled with values of the feature in question, and the
leaves are labeled with class values (see Figure 8.6).
no
1a
2v
2a
ﬂ
st,3v,sﬂ
0
0
0
0
0
0
1
1
1
1
2
2
2
2
3
0,1
1,2
1,2
2,3
1,2,3
op
op
op
op
op
op
op
m
m
m
m
m
dr
dr
dr
dr
op,dr
op,m
FC
FC
FC
SC
SC
SC
SC
SC
MH
Fig. 8.6. A classiﬁcation tree for poker
The tree in Figure 8.6 can be used to classify the situation with respect
to BH. Classiﬁcation is performed through processing the tree from the root
toward the leaves. First you branch out based on the value of MH. Depending
on the answer, you branch out according to the value of either FC or SC,
and sometimes you also ask for the value of the other card change. When you
reach a leaf, you read the classiﬁcation.

8.4 Classiﬁcation Trees
273
To learn a classiﬁcation tree, you ﬁrst determine which feature variable to
use as the root. Let C be the class variable with states {c1, . . . , cn}, and let
F be a feature variable with states {f1, . . . , fk}. We wish to characterize how
good a classiﬁer F alone would be. That is, if we know the state of F, how
close will we be at knowing the class value?
The values of F partitions D into the data sets D1
1, . . . , D1
k, and for each
data set D1
i we have a distribution P #(C|fi). One way of measuring how close
we are to knowing C in the data set D1
i is to calculate the entropy for C. In
general, for a variable X with distribution P(X) (or P #(X)), the entropy is
deﬁned as
Ent(P(X)) = −

x∈sp(X)
P(x) log2(P(x)),
(8.1)
where we let 0 log2(0) = 0. If the probability of X being in a particular state
approaches 1, then the entropy goes toward 0. On the other hand, the more
dispersed the probability mass, the higher the entropy; in case we have a
uniform distribution, the entropy attains its maximum value, log2(|sp(X)|).
Now, if the entropy of each distribution P #(C|fi) is small, then knowing
F brings us close to knowing C, but if the entropies are large, then knowing F
does not give us much information about C. There are various ways of using
the entropies as a score for ranking the variables. A method called ID3 uses
the expected entropy as a measure of how good a feature is at predicting the
class:
E[Ent(F)] =

F
P #(F) Ent(P #(C | F)).
Actually, the algorithm uses information gain,
Ent(P #(C)) −E[Ent(F)],
but since Ent(P #(C)) is independent of F, you look for a variable giving the
lowest expected entropy.
Having chosen the feature F as the root, you continue recursively on the
data sets D1
1, . . . , D1
k.
As an illustration, the ID3 algorithm applied to the data set in Table 8.1
would ﬁrst partition the data set for each variable. For the variable SC we
have the sets {8, 9, 14}, {1, 2, 3, 4, 5, 7, 11, 13, 17, 18}, and {6, 10, 12, 15, 16, 19,
20} corresponding to the states 0, 1, and 2, respectively. The set for state 0
has two cases with state op, and one with state m. This distribution has the
entropy
−1
3 log2
1
3

−2
3 log2
2
3

= −1
3 (2 −2 log2 3 −log2 3) = 0.918,
yielding a contribution of 3/20 · 0.918 = 0.138 to the expected entropy.
The following expected entropies are calculated (note that the maximal
entropy for a distribution over three states is log2 3 = 1.585):

274
8 Bayesian Networks as Classiﬁers
E[Ent(MH)] = 0.735,
E[Ent(FC)] = 1.351,
E[Ent(SC)] = 1.403.
Since MH has the lowest expected entropy, it is chosen as root. For each
value of MH you now have a small data set, and you choose the best root for
each. For MH = no you have four cases, and since SC separates these cases
better than FC, SC is chosen. The full tree is given in Figure 8.7; the ? indi-
cates that no case covers the speciﬁed conﬁguration, and for these situations
you may take the majority class.
?
?
no
1a
2v
2a
ﬂ
st,3v,sﬂ
0
1
1
2
2
2
2
3
3
0,1
0,1
op
op
op
m
m
m
m
dr
op,dr
op,dr
op,dr
op,dr
FC
FC
SC
SC
MH
Fig. 8.7. The result of applying the ID3 algorithm on the data set in Table 8.1.
8.5 Summary
The Naive Bayes Classiﬁer
In a naive Bayes classiﬁer, each feature variable has the class variable as its
only parent. This means that the structure is ﬁxed, and learning a classiﬁer
therefore amounts to estimating the parameters.
Evaluating Classiﬁers
Two approaches for evaluating a classiﬁer:
Classiﬁcation accuracy: the fraction of correctly classiﬁed cases.
Expected loss:
Expected loss
=

Classiﬁed,Correct
P #(Classiﬁed, Correct)Loss(Classiﬁed, Correct).

8.6 Bibliographical Notes
275
The Tree-Augmented Naive Bayes Classiﬁer
In the tree-augmented naive Bayes classiﬁer (TAN classiﬁer), each feature
variable has at most one other feature variable as parent in addition to the
class variable.
Learning TANs: Let D be a dataset over the variables {F1, . . . , Fm, C}. A
TAN of maximal likelihood can be constructed as follows:
1. Calculate the conditional mutual information MI(Fi, Fj | C) for each pair
(Fi, Fj).
2. Consider the complete MI-weighted graph: the complete undirected graph
over {F1, . . . , Fn}, where the links Fi −Fj have the weight MI(Fi, Fj | C).
3. Build a maximal-weight spanning tree for the complete MI-weighted
graph.
4. Direct the resulting tree by choosing any variable as a root and setting
the directions of the links to be outward from it.
5. Add the node C and a directed link from C to each feature node.
6. Learn the parameters.
Classiﬁcation Trees
A classiﬁcation tree is a directed tree whose internal nodes are feature vari-
ables. The links are labeled with values of the feature in question, and the
leaves are labeled with class values.
To learn a classiﬁcation tree, you start with the empty tree and iteratively
insert the node X that tells you the most about the class variable C. One
possible measure is the expected entropy:
E[Ent(X)] =

X
P #(X) Ent(P #(C | X)),
where
Ent(P(X)) = −

x∈sp(X)
P(x) log2(P(x)).
8.6 Bibliographical Notes
As mentioned, naive Bayes was used by de Dombal et al. (1972) and can be
traced back at least to Minsky (1963). It was introduced to classiﬁcation by
Duda and Hart (1973). Its role in classiﬁcation has been thoroughly studied in
the last decade or so, with Domingos and Pazzani (1997) providing theoretical
results on concepts that naive Bayes can classify better than any other classi-
ﬁer, and with empirical results that show how violations of the independence
assumptions of the model are often of no consequence. Jaeger (2003) further

276
8 Bayesian Networks as Classiﬁers
clariﬁes the distinction between the concepts they can recognize, and the theo-
retical limits on the concepts that can be learned from data. Tree-augmented
naive Bayes classiﬁers were introduced by Friedman et al. (1997). The ID3
algorithm for inferring classiﬁcation trees was introduced by Quinlan (1979)
and later improved in (Quinlan, 1986). For a general overview over classiﬁers,
see (Mitchell, 1997).
8.7 Exercises
Exercise 8.1. Verify that the PC-algorithm results in the network in Fig-
ure 8.2 (or one of its equivalents) when run with an oracle based on the
d-separation properties of the network in Figure 8.1, and with the variables
OH1 and OH2 hidden.
Exercise 8.2. Learn the maximum likelihood parameters for the classiﬁer in
Figure 8.3 from the cases in Table 8.1. What class does your classiﬁer assign
to a case with MH=1a, FC=1, and SC=1?
Exercise 8.3. Verify that the TAN-algorithm constructs the classiﬁer in Fig-
ure 8.4 and complete the classiﬁer by learning the maximum likelihood param-
eters. What class does the classiﬁer assign to the case with MH=1a, FC=1,
and SC=1? What would the result be if you instead of maximum likelihood
estimates used Bayesian parameter estimates?
Exercise 8.4. Consider the classiﬁcation tree in Figure 8.6. How would this
classiﬁer classify the case with MH=1a, FC=1, and SC=1?
Exercise 8.5. Using the data in Table 7.4, construct a classiﬁcation tree for
classifying A. What class is assigned to (B = 1, C = 2)?

Part II
Decision Graphs

9
Graphical Languages for Speciﬁcation of
Decision Problems
A Bayesian network serves as a model for a part of the world, and the relations
in the model reﬂect causal impact between events. The reason for building
these computer models is to use them in taking decisions. In other words,
the probabilities provided by the network are used to support some kind of
decision making. In principle, there are two kinds of decisions, namely test
decisions and action decisions.
A test decision is a decision to look for more evidence to be entered into the
model, and an action decision is a decision to change the state of the world.
In real life, this distinction is not very sharp; tests may have side eﬀects,
and by performing a treatment against a disease, evidence on the diagnosis
may be acquired. In order to be precise, we should say that decisions have
two aspects, namely a test aspect and an action aspect. The two aspects are
handled diﬀerently in connection with Bayesian networks, and accordingly we
treat them separately.
Although both observations and actions may change the probability dis-
tributions in the model, they are fundamentally diﬀerent. To highlight this,
consider the example in Figure 9.1.
A wheat type may be genetically resistant to mildew. If so, there will be
no attack, and this has an impact on the quality of the crop. If you observe
that there is no attack, the probabilities for Resistance and Crop are changed.
If you, on the other hand, prevent an attack through spraying and thereby
ﬁx the state of Attack to no, then it has no impact on your belief about
Resistance. That is, the impact of actions can only follow the direction of the
causal links.
The example stresses the important point already made in Section 3.2.6
concerning the use of Bayesian networks. Using Bayes’ theorem, it is easy to
establish the model in Figure 9.2, which reﬂects a kind of diagnostic reasoning.
From the point of view of entering evidence and propagating probabilities,
the two Bayesian networks in Figure 9.1 and Figure 9.2 represent the same
joint probability distribution, so why bother emphasizing that the links in the
network should be causal links? The diﬀerence becomes apparent when one

280
9 Graphical Languages for Speciﬁcation of Decision Problems
P
Resistance
Attack
Crop
T
Fig. 9.1. A simple Bayesian network with an action and a test attached. The
decision (Prevention) can by spraying ﬁx the state of Attack to no. The test T can
determine the state of Attack.
Resistance
Attack
Crop
Fig. 9.2. A Bayesian network equivalent to the one in Figure 9.1.
sprays. In Figure 9.2, spraying will change the probability of resistance but it
will have no impact on the crop.
In Section 9.1 we show how to extend a Bayesian network to cope with
a single decision, and in Section 9.2 we describe fundamentals of rational
decision making. Sections 9.3–9.5 present various graphical frameworks for
modeling decision problems with several decisions involved, and in Section 9.6
we deal with problems that have an unbounded time horizon.
9.1 One-Shot Decision Problems
A Bayesian network provides a model of the world that can be used in making
decisions. The typical situation is that we have observed some of the variables
in the domain and based on these observations we make an inquiry to the
Bayesian network about some other set of variables (probability updating).
The result of the inquiry is in turn used in the subsequent decision-making
process.
This type of application of Bayesian networks can be taken one step fur-
ther, so that rather than keeping the model separated from the decision-
making process, you could combine these two parts. That is, not only does
the ﬁnal model reveal the structure of the decision problem, but it can also be
used to give advice about the decisions. In the simple situation in which only
a single decision is to be made, the Bayesian network can readily be extended
to reﬂect the structure of the decision problem.

9.1 One-Shot Decision Problems
281
9.1.1 Fold or Call?
Consider the poker example in Section 3.1.4 as extended in Exercise 3.13
with the variables MH (“my hand” having the same states as OH2) and BH
(“best hand” with the states me, opponent, and draw), see Figure 9.3. The
conditional probability distribution for BH is a deterministic function of OH2
and MH.
OH0
OH1
OH2
FC
SC
BH
MH
Fig. 9.3. The poker model extended with variables for my hand and best hand.
The reason I am interested in knowing which hand is best is that I shall
take a decision on an action. For this game, the rules are that we both placed
$1 on the table to get the initial hand, and after the rounds of card changing,
my opponent places $1 extra (in this game she is forced to place $1 regardless
of her hand). Now, I may either fold or call. If I fold, my opponent takes the
pot, and if I call, I place $1 on the table, and we compare the hands. The
player with the best hand takes the pot (in case of a draw we share).
My decision problem in deciding to fold or to call can be represented
graphically by extending the Bayesian network with a couple of extra nodes.
The decision options are represented by a rectangular node D with states
fold and call. Another type of node, U, represents the possible outcomes in
dollars. The node U is called a utility node, and the outcomes are called
utilities. The variables determining the outcomes are BH and D, and this is
shown graphically through directed links from BH and D to the diamond-
shaped node U. See Figure 9.4. Note that in this example the utilities also
include the initial $1 that I was forced to put on the table.
When I have extended the Bayesian network to the model in Figure 9.4,
I can use the model to give advice on the decision D. I have observed my
opponent’s change of cards (for example, two cards and one card), and I know
my own hand (for example, a ﬂush). The probability for BH (best hand) is
calculated, and it is used to calculate EU(call), the expected utility of calling:
the sum of the various wins and losses weighted by their probability. The
formula for EU(call) is

282
9 Graphical Languages for Speciﬁcation of Decision Problems
OH0
OH1
OH2
FC
SC
BH
MH
U
D
D
fold call
BH
me
−1
2
opponent −1 −2
draw
−1
0
Fig. 9.4. Graphical representation of my decision problem of whether to fold or
call. The variable D is a decision variable. The variable U represents the outcome in
$ (shown in the table), and the links into U indicate that the outcomes of the game
(only) depend on D and BH.
EU(call) =

BH
U(BH, call)P(BH | evidence)
= P(BH = me|FC = two, SC = one, MH = ﬂush)U(BH = me, call)
+ P(BH = draw|FC = two, SC = one, MH = ﬂush)
U(BH = draw, call)
+ P(BH = opponent|FC = two, SC = one, MH = ﬂush)
U(BH = opponent, call).
If you use the probabilities found in Section 3.2.3, the expected utility of
calling is
EU(call) = 0.4 · 2 + 0.054 · (−2) + 0.546 · 0 = 0.692,
and since the expected utility of folding is −1, I should call.
9.1.2 Mildew
Two months before the harvest of a wheat ﬁeld, the farmer observes the state
Q of the crop, and he observes whether it has been attacked by mildew, M. If
there is an attack, he will decide on a treatment with fungicides.
There are ﬁve variables:
•
Q with states fair (f), not too bad (n), average (a), and good (g);
•
M with states no, little (l), moderate (m), and severe (s);
•
H (state of the crop at time of harvest) with the states from Q plus rotten
(r), bad (b), and poor (p) (farmers in all countries tend to describe their
harvests in pessimistic terms);
•
OQ (observation of Q) with the same states as Q;
•
OM (observation of M) with the same states as M.

9.1 One-Shot Decision Problems
283
Furthermore, there is a decision node A with decision options no, light (l),
moderate (m), and heavy (h) and a variable M′ describing the mildew attack
after the decision. We deﬁne a utility function U(H) giving the utility of the
outcome of the harvest for each state of the crop. The cost of the decisions
is modeled as a utility function C attached to A (the values of C are either
negative or zero). The total utility is U + C. Figure 9.5 gives a model.
Q
OQ
H
U
M′
M
OM
A
C
Fig. 9.5. A decision model for mildew.
With evidence e (statements on OQ and OM), the farmer wishes to deter-
mine an optimal decision (a decision of maximal expected utility). To do this,
he needs to calculate the expected utility of the various options. That is, for
each state a of A, we ﬁrst calculate P(H | A = a, e), and then
EU(A | e) = C(A) +

H
U(H)P(H | A, e).
9.1.3 One Decision in General
The general situation with one decision variable is as described in Figure 9.6.
There is a Bayesian network structure with chance nodes and directed links.
The network is extended with a single decision node D that may have an
impact on the variables in the structure. In other words, there may be a link
from D to some chance nodes. Furthermore, there is a set of utility functions,
U1, . . . , Un, over domains X1, . . . , Xn.
The task is to determine the decision that yields the highest expected
utility. Thus, if none of the utility nodes contain D in the domain, then with
evidence e we calculate
EU(D | e) =

X1
U1(X1)P(X1 | D, e) + · · · +

Xn
Un(Xn)P(Xn | D, e),

284
9 Graphical Languages for Speciﬁcation of Decision Problems
.
.
.
...
...
...
.
.
. . .
.
D
U1
U2
U3
Fig. 9.6. A graphical representation of a one-action decision scenario.
and a state d maximizing EU(D = d | e) is chosen as an optimal decision. When
D is contained in the domain of a utility node, such as U1 in Figure 9.6, then
we should perform the summation only over X1 \ {D}, and accordingly, we
should use the probability distribution P(X1 \ {D} | D, e).
A requirement of the method described above is that the decision problem
contains only a single decision. When one is working with decision problems
involving several decisions, things become a bit more complicated (we shall
return to this issue in Sections 9.3 and 9.4).
9.2 Utilities
We treat decision problems in the framework of theory. Decisions are made
because they may be of use in some way. Therefore, the various decisions
should be evaluated on the basis of the usefulness of their consequences. We
assume that “usefulness” is measured on a numerical scale called a utility scale,
and if several kinds of utilities are involved in the same decision problem, then
the scales have a common unit.
Management of Eﬀort
In your computer science studies you attend two courses, Graph Al-
gorithms and Machine Intelligence. In the middle of the term, you
realize that you cannot keep pace. You can either reduce your eﬀort
in both courses slightly or you can decide to attend one of the courses
superﬁcially. What is the best decision?

9.2 Utilities
285
You have three possible actions:
Gm: Keep pace in Graph Algorithms and follow Machine Intelligence super-
ﬁcially.
SB: Slow down in both courses.
Mg: Keep pace in Machine Intelligence and follow Graph Algorithms super-
ﬁcially.
The results of the actions are your ﬁnal marks for the courses. The marks
are integers between 0 and 5, where 0 and 1 are failing marks. You have certain
expectations for the marks given your eﬀort in the rest of the term. They are
shown in Table 9.1.
kp sd
fs
0 0
0 0.1
1 0.1 0.2 0.1
2 0.1 0.1 0.4
3 0.2 0.4 0.2
4 0.4 0.2 0.2
5 0.2 0.1 0
kp sd
fs
0 0
0 0.1
1 0 0.1 0.2
2 0.1 0.2 0.2
3 0.2 0.2 0.3
4 0.4 0.4 0.2
5 0.3 0.1 0
P(GA | eﬀort) P(MI | eﬀort)
Table 9.1. The conditional probabilities of the ﬁnal marks in Graph Algorithms
(GA) and Machine Intelligence (MI ) given the eﬀorts keep pace (kp), slow down
(sd), and follow superﬁcially (fs).
A way of solving your decision problem would be to say that the numeric
value of the mark is a utility, and you want to maximize the sum of the
expected marks. The calculations would then be
EU(Gm) =

m∈GA
P(m | kp)m +

m∈MI
P(m | fs)m = 3.5 + 2.3 = 5.8,
EU(SB) =

m∈GA
P(m | sd)m +

m∈MI
P(m | sd)m = 2.9 + 3.2 = 6.1,
EU(Mg) =

m∈GA
P(m | fs)m +

m∈MI
P(m | kp)m = 2.3 + 3.9 = 6.2.
From this, you would conclude that you should follow Graph Algorithms su-
perﬁcially but keep pace in Machine Intelligence.
However, do the marks really reﬂect your utilities? If, for example, you
had the same number of marks but the numeric values were 0, 5, 6, 8, 9, 10,
you would have come to another conclusion. The problem is that you cannot
expect that a diﬀerence of 1 in mark number always represents the same
diﬀerence in utility. Actually, in this case your subjective utility is probably
not increasing in the numeric value of the mark: the rule at your university

286
9 Graphical Languages for Speciﬁcation of Decision Problems
is that if you fail, you are given another chance, but if you pass, you are not
allowed to try again to get a better mark. Therefore, you ﬁnd that the worst
mark to get is a 2 rather than a 0!
To overcome this problem, the mark scale is mapped into a utility scale
going from 0 to 1. The best possible mark (5) is given the utility 1, and the
worst possible mark (2) gets the utility 0.
The intermediate marks are given utilities between 0 and 1 by imagining
that you have a choice between two games:
Game 1: You get for certain the mark x;
Game 2: You get mark 5 with probability p, and you get mark 2 with
probability 1 −p.
Which game would you prefer?
If p = 0, you would prefer Game 1, and for p = 1, Game 2 would be best.
For some p between 0 and 1, you would be indiﬀerent, and this p is the utility
for the mark x. Speciﬁcally, if you should ﬁnd a value for p that would make
you indiﬀerent between games 1 and 2, then it should hold that
EU(Game 1 ) = EU(Game 2 ).
This can be rewritten as 1 · U(x) = (1 −p) · U(2) + p · U(5), and by exploiting
that U(2) = 0 and U(5) = 1 we get U(x) = p.
In Table 9.2, we have performed the utility assessment for you. The utilities
assessed are for only one course. We will now assume that the utility of marks
for several courses is the sum of the individual utilities. Note that this is
not evident (it might, for example, be that you prefer two 2’s to failing both
courses, which would delay your studies considerably), and an alternative
could be to construct a single utility function for both courses.
Mark
0
1 2 3
4 5
Utility 0.05 0.1 0 0.6 0.8 1
Table 9.2. Utilities for the various marks (the same for both courses).
In Figure 9.7, the decision model is illustrated. To ﬁnd an optimal decision,
the calculations are
EU(action) =

m∈GA
P(m | action)UGA(m) +

m∈MI
P(m | action)UMI (m).
We get EU(Gm) = 1.015, EU(SB) = 1.07, EU(Mg) = 1.045, and the optimal
decision is therefore SB.

9.2 Utilities
287
+
Action
MI
UMI
GA
UGA
Fig. 9.7. A decision model for eﬀort.
9.2.1 Instrumental Rationality
Beneath the principle of maximal expected utility there is a normative claim
that rational decision making shall be represented as a task of calculating
expected utilities and to choose an option of maximal expected utility. The
question is whether this claim includes all kinds of human choice (private
decisions, company decisions, political decisions, etc.). Does it cover choosing
the dinner for tomorrow as well as whether to kill your husband or leave him?
Does it include setting of tax rates and building dams for ﬂood protection?
It is not claimed that humans/companies/politicians act in accordance
with the principle of maximal expected utility (which can easily be disproved).
The claim is that if the decision maker takes his time to analyze the situation
to ﬁnd out which choice seems the best, then it is irrational not to choose one
of maximal expected utility.
In order not to enter into a circular argument, you need to be precise
about the term rational without referring to utilities, and a way of doing so
is to put up a set of rules that characterize rational choice. The rules need
not be exhaustive or independent, but they should have the character that
everybody agrees that it is irrational not to obey them.
Below we present the ﬁrst such set of rules, presented by von Neuman and
Morgenstern in 1947. The rules have been called axioms of instrumentally
rational choice, and they are formulated in terms of preferences over lotteries.
Formally, a lottery is a probability distribution over a set of outcomes/prices,
denoted by X, where an outcome X = x can be a bundle of commodities,
services, resources, etc. The lottery with a certain outcome of the price x is
denoted by [x]. The decision maker is supposed to rank the lotteries by prefer-
ence. The notation A ⪰B denotes that B is not preferred to lottery A, A ≻B
denotes that A is (strictly) preferred to B; and A ∼B denotes that the deci-
sion maker is indiﬀerent between A and B (shorthand for A ⪰B and B ⪰A).

288
9 Graphical Languages for Speciﬁcation of Decision Problems
Construction of mixed lotteries. From two lotteries A and B we can construct
compound lotteries. Let α ∈[0, 1]. Then αA + (1 −α)B is a new lottery: with
probability α, A is drawn, else B.
Axioms of instrumentally rational choice:
1. Reﬂexivity. For any lottery A, A ⪰A.
2. Completeness. For any pair (A, B) of lotteries, A ⪰B or B ⪰A.
3. Transitivity. If A ⪰B and B ⪰C, then A ⪰C.
4. Preference increasing with probability. If A ⪰B then αA + (1 −α)B ⪰
βA + (1 −β)B if and only if α ≥β.
5. Continuity. If A ⪰B ⪰C then there exists α ∈[0, 1] such that B ∼
αA + (1 −α)C.
6. Independence. If C = αA+(1−α)B and A ∼D, then C ∼(αD+(1−α)B).
Theorem 9.1. For an individual who acts according to a preference ordering
satisfying rules 1–6 above, there exists a utility function over the outcomes so
that the expected utility is maximized.
Proof.
Since the set of prices X is ﬁnite, there is a best price, xB, and a worst
price, xW . Without loss of generality we set U(xB) = 1 and U(xW ) = 0. The
continuity axiom [5] then yields that for any price x there is an α ∈[0, 1] such
that [x] ∼α[xB] + (1 −α)[xW ]. We set U(x) = α.
Now let xi denote prices and let ti be probabilities. From standard prob-
ability calculus we have that if A = αB + (1 −α)C, B = 
i tB
i [xi], and
C = 
i tC
i [xi], then A = 
i(αtB
i + (1 −α)tC
i )[xi]. That is, any lottery A can
be written in the form
A =

i
ti[xi],
and
EU(A) =

i
tiU(xi).
Since [xi] ∼U(xi)[xB] + (1 −U(xi))[xW ], we get (axiom [6])
A ∼

i
ti(U(xi)[xB] + (1 −U(xi))[xW ]).
Since U(xi) is independent of ti, we have
A ∼

i
tiU(xi)

[xB] +

i
ti(1 −U(xi))

[xW ].
Hence, for all lotteries A we have (axiom [3])
A ∼α[xB] + (1 −α)[xW ],

9.2 Utilities
289
where α = EU(A). Now let A ∼α[xB] + (1 −α)[xW ] and B ∼β[xB] + (1 −
β)[xW ]. By axiom [4] we have that A ⪰B if and only if α ≥β if and only if
EU(A) ≥EU(B).
⊓⊔
The theorem says that if you agree that rules 1–6 apply for your decision
problem, then you have to choose a decision that maximizes your expected
utility. If you do not wish to follow the recommendation of a perfect max-EU
analysis of your problem, your only way out is to attack the rules.
To illustrate this point, consider the following example (Allais’ paradox).
You have a choice between two lotteries:
•
Lottery A = [$1mill.],
•
Lottery B = 0.1[$5mill.] + 0.89[$1mill.] + 0.01[$0].
Most probably you would strictly prefer A to B because your life would
be completely changed if you got $1 million, and in B there is a risk of this
not happening. This reasoning is perfectly rational. It reﬂects only that your
subjective utility of $1 million is very close to your utility of $5 million. This
must also be the case in other situations. Assume that you are faced with a
new choice between two lotteries:
•
Lottery C = 0.11[$1mill.] + 0.89[$0],
•
Lottery D = 0.1[$5mill.] + 0.9[$0].
In turns out that if you chose D (as many people would do) you would not
maximize expected utility. In other words, if you seriously mean that the
diﬀerence in utility between $1 million and $5 million is very small, you must
take the extra 1% chance of winning $1 million.
The following calculations show that choosing D does not maximize your
expected utility. Let U($5mill.) = 1, U(0) = 0, U($1mill.) = u. If you prefer A
to B, you have
u > 0.1 + 0.89u.
Hence
u > 10
11
and now
EU(C) = 0.11u > 0.1110
11 = 0.1 = EU(D).
The rules presented here cover a simple type of decision problem. There is
an extensive scientiﬁc debate about how wide the scope is for the principle of
maximizing expected utilities in a world assigned with subjective probabilities.
Axioms similar to the axioms presented here have been devised, and theorems
similar to Theorem 9.1 have been proved.

290
9 Graphical Languages for Speciﬁcation of Decision Problems
9.3 Decision Trees
A classical way of representing decision problems with several decisions is
with decision trees. A decision tree is a model that encodes the structure of
the decision problem by representing all possible sequences of decisions and
observations explicitly in the model.
The nonleaf nodes in a decision tree are decision nodes (rectangular boxes)
or chance nodes (circles or ellipses), and the leaves are utility nodes (diamond
shaped). The links in the tree have labels. A link from a decision node is
labeled with the action chosen, and a link from a chance node is labeled by a
state.
Example 9.1 (The Two-Test Milk Problem). Consider the infected milk sce-
nario from Figure 3.1 and Section 3.2.1 (to keep things simple, we assume
that the infections and tests are independent between the days). The farmer
has 50 cows, and the milk from each cow is poured into a common container
and transported to the dairy. The value of the milk is $2 per cow. The dairy
checks the milk carefully, and if it is infected it is thrown away. After having
milked a cow, the farmer may perform two diﬀerent tests of the milk, TA and
TB, before pouring it into the container. The price of the ﬁrst test is 6 cents
and it has a false positive/negative rate of 0.01, and the price of the second
test is 20 cents and it has a false positive/negative rate of 0.001.
To establish the utilities, let us assume that the farmer has clean milk
from the 49 other cows. If the farmer pours the milk into the container, he
will gain $100 if it is not infected, and he will gain nothing if it is infected. If
he throws the milk away, he will gain $98 regardless of the state of the milk.
The question is whether he should perform the tests and in which order.
Figures 9.8 and 9.9 show the graphical part of a decision tree for the milk
example with two tests.
A decision tree is read from the root downward. When you pass a decision
node, the label tells you what the decision is, and when you pass a chance
node, the label tells you the state of the node. If a decision node follows a
chance node, then the chance node is observed before the decision is made.
Hence the sequence in which we visit the nodes corresponds to the sequence
of observations and decisions. We assume no-forgetting: when a decision is to
be taken, the decision maker knows all the labels on the path from the root
down to the current position in the decision tree. We adopt the shorthand
past for the set of labels from the root to a position in the tree.
Each path from the root to a leaf speciﬁes a complete sequence of obser-
vations and decisions, and we call such a sequence a decision scenario. Fur-
thermore, we require decision trees to be complete: from a chance node there
must be a link for each possible state, and from a decision node there must be
a link for each possible decision option. This also means that a decision tree
speciﬁes all the possible scenarios in the decision problem.

9.3 Decision Trees
291
D1
TA
TA
D2
pos
TB
TB
D3
pos
97.74
discard
Inf
pour
99.74
clean
−0.26
infected
D3
neg
97.74
discard
Inf
pour
99.74
clean
−0.26
infected
97.94
discard
Inf
pour
99.74
clean
−0.26
infected
D2
pos
TB
TB
D3
pos
97.74
discard
Inf
pour
99.74
clean
−0.26
infected
D3
neg
97.74
discard
Inf
pour
99.74
clean
−0.26
infected
97.94
discard
Inf
pour
99.74
clean
−0.26
infected
98
discard
Inf
pour
100
clean
0
infected
continued in succeeding ﬁgure
TB
Fig. 9.8. The graphical part of a decision tree for the milk problem from Exam-
ple 9.1. The tree reﬂects that no test is performed when the milk has been poured
or discarded. Note that nodes in a decision tree may share names.

292
9 Graphical Languages for Speciﬁcation of Decision Problems
TB
TB
D2
pos
TA
TA
D3
pos
97.74
discard
Inf
pour
99.74
clean
−0.26
infected
D3
neg
97.74
discard
Inf
pour
99.74
clean
−0.26
infected
97.80
discard
Inf
pour
99.80
clean
−0.20
infected
D2
pos
TA
TA
D3
pos
97.80
discard
Inf
pour
99.74
clean
−0.26
infected
D3
neg
97.80
discard
Inf
pour
99.74
clean
−0.26
infected
97.80
discard
Inf
pour
99.80
clean
−0.20
infected
Fig. 9.9. Continuation of diagram in Figure 9.8.
The quantitative part of a decision tree consists of utilities and probabili-
ties. Each leaf has a utility value attached to it. This utility reﬂects the utility
of the decision scenario identiﬁed by the path from the root to the leaf in
question. For the chance nodes, we associate a probability with each of the
links emanating from them. See Figure 9.11 for an example. Let A be a chance
node at a particular position in the tree with past o, and let l be an outgoing
link labeled with a. We then associate P(A = a | o) with this link. Either you
can have the probabilities explicitly attached to the links (which can be rather
impractical to work with), or you can use your Bayesian network model as a

9.3 Decision Trees
293
reference. You can, for example, complement the graphical part in Figures 9.8
and 9.9 with the Bayesian network in Figure 9.10 and then use the Bayesian
network to calculate the required probabilities.
Test1
Inf
Test2
Fig. 9.10. A Bayesian network for calculating the probabilities for the decision tree
in Figures 9.8 and 9.9.
9.3.1 A Couple of Examples
We now give two other examples of decision problems involving a sequence of
decisions.
Example 9.2 (The Car Start Problem). In the morning, my car will not start.
There are three possible faults: the spark plugs may be dirty, with probability
0.3; the ignition system may be malfunctioning, with probability 0.2; or there
is some other cause, with probability 0.5. I can perform two repair actions
myself: SP, which at the cost of 4 minutes always ﬁxes spark plugs; and IS,
which takes 2 minutes and ﬁxes the ignition system with probability 0.5. I
can also perform a test T , namely to check the charge on the spark plugs
when starting. It takes half a minute, and it says ok if and only if the ignition
system is okay. Finally, I can call road service RS, which at the cost of 15
minutes ﬁxes everything. The car was okay yesterday evening, so I assume
that there is at most one fault.
To work with utilities rather than costs, let us say that I have 30 minutes to
ﬁx the car and arrive at work, and I want to ﬁnd a test−repair sequence that
expectedly gives me as much time as possible for getting to work. Therefore,
the utility of a test−repair sequence is the remaining time for getting to work.
A decision tree for this Car Start Problem is shown in Figure 9.11. The
probabilities for the decision tree are calculated from the model in Figure 9.12,
where the technique from Section 3.3.9 is used.
Example 9.3 (The Reactor Problem).
An electric utility ﬁrm must decide whether to build (B) a reactor of
advanced design (a), a reactor of conventional design (c), or no reactor (n) at
all. If the reactor is successful, an advanced reactor is more proﬁtable, but it
is also more risky.
If the ﬁrm builds a conventional reactor, the proﬁts are $8B if it is a
success (cs), and −$4B if there is a failure (cf). If the ﬁrm builds an advanced

294
9 Graphical Languages for Speciﬁcation of Decision Problems
T
ok
0.8
14.5
RS
SP
25.5
ok
0.38
¬ok
0.62
10.5
RS
¬ok
0.2
14.5
RS
IS
27.5
ok
0.5
¬ok
0.5
12.5
RS
SP
26
ok
0.3
¬ok
0.7
11
RS
IS
24
ok
0.14
¬ok
0.86
9
RS
T
ok
10.5
RS
¬ok
10.5
RS
IS
23.5
ok
0.5
¬ok
0.5
8.5
RS
15
RS
IS
28
ok
0.1
¬ok
0.9
13
RS
T
¬ok
0.11
12.5
RS
ok
0.89
12.5
RS
SP
23.5
ok
0.38
¬ok
0.62
8.5
RS
SP
24
ok
0.33
¬ok
0.67
9
RS
Fig. 9.11. A decision tree for the Car Start Problem in Example 9.2.

9.3 Decision Trees
295
T
IS
Fault
Fault-I
Fig. 9.12. A model for calculating the probabilities for a decision tree for the Car
Start Problem in Example 9.2. Due to the assumption of exactly one fault, the faults
are collected in the node Fault with states is, sp, and other.
reactor, the proﬁts are $12B if it is a success (as), −$6B if there is a limited
accident (al), and −$10B if there is a major accident (am). The ﬁrm’s utility is
assumed to be linear in dollars. Before making the decision to build, the ﬁrm
has the option to conduct a test (T = t) or not (nt) of the components (Cp)
of the advanced reactor. The test results (R) can be classiﬁed as either bad
(b), good (g), or excellent (e). The cost of the test is $1B. If the test results
are bad, then the Nuclear Regulatory Commission (NRC) will not permit the
construction of an advanced reactor.
Figure 9.17 shows a decision tree representation of the problem, where the
probabilities can be found from the Bayesian network in Figure 9.14.
The speciﬁcation of the quantitative part (Figure 9.14) can be extended
with decision nodes and utility nodes as shown in Figure 9.15, which can also
be considered a model of the relevant world.
9.3.2 Coalesced Decision Trees
The main drawback of decision trees is that they grow exponentially with
the number of decision and chance variables, and – as illustrated in the two
examples – even very small decision problems require a relatively large decision
tree. There are, however, methods for reducing the complexity by exploiting
symmetries in the decision problem.
The idea is that when a decision tree contains identical subtrees, they can
be collapsed. In the milk problem, if both tests are negative, the situations
will be the same regardless of the order in which the tests are performed. The
succeeding parts of the decision tree must therefore be the same, both in terms
of structure and numerical information (probabilities and utilities); hence we
can have the links to these parts meet in a common decision node. Figure 9.16
shows the structure of a coalesced decision tree for the milk problem, and
Figure 9.17 shows the coalesced decision tree for the reactor problem.
The procedure for solving a coalesced decision tree is the same as the
procedure for normal decision trees (see the next section).

296
9 Graphical Languages for Speciﬁcation of Decision Problems
T
a
as
al
am
nt,0
t,−1
B
B
A
0
0
0
e
g
b
n
0
cs
cf
8
−4
C
12
−6
−10
c
cs
cf
8
−4
C
12
−6
−10
c
B
as
al
am
A
a
as
al
am
A
B
12
a
−6
−10
c
cs
cf
8
−4
C
c
cs
cf
8
−4
C
R
Fig. 9.13. A decision tree for the Reactor Problem. Note that the cost of the test is
attached to the link T = t, indicating that the cost will be the same for all ensuing
scenarios.
9.3.3 Solving Decision Trees
A solution to a decision tree is a strategy that speciﬁes how we should act at the
various decision nodes. An example of a strategy is illustrated in Figure 9.18 by
the boldfaced links. Strategies are compared based on their expected utilities,
and ﬁnding an optimal strategy amounts to ﬁnding a strategy with highest
expected utility; such a strategy is not necessarily unique.
By assigning to each node in the decision tree a value corresponding to
the maximum expected utility achievable at that node, an optimal strategy
will pick an action leading to a child of maximum value. Looking at the end

9.3 Decision Trees
297
C
A
Cp
R
Fig. 9.14. A Bayesian network providing probabilities for the decision tree repre-
sentation of the Reactor Problem shown in Figure 9.13.
C
A
Cp
R
T
B
U1
U2
U3
U4
Fig. 9.15. A model of the world relevant for the reactor problem.
of the decision tree, one sees that the value of a leaf node is simply the utility
assigned to that node. If we go one step further up the tree, then the value of
a decision node D is the maximum value associated with its children/leaves,
since D is under our full control. For a chance node, its value corresponds to
the utility you can expect to achieve from that point in the decision tree: the
value is the sum of the utilities of the leaves weighted with the probabilities
of their outcomes. When all children of a node N have been assigned a value,
we can calculate the value to assign to N. If N is a decision node, we assign
it the maximum of the children’s values, and if N is a chance node, we assign
the weighted sum.
These observations form the basis for a procedure known as “average-out
and fold-back” for calculating an optimal strategy and the maximum expected
utility: start with nodes that have only leaves as children. If the node is a

298
9 Graphical Languages for Speciﬁcation of Decision Problems
discard
TA
pour
pos
neg
pour
clean
infected
pos
neg
pour
infected
clean
TB
pour
clean
infected
pos
neg
pour
clean
pos
neg
TA
pour
clean
infected
pos
neg
pour
clean
infected
TA
pour
clean
infected
neg
pos
pour
clean
infected
clean
TB
infected
TB
infected
discard
discard
discard
discard
discard
discard
discard
discard
Fig. 9.16. The structure of a coalesced decision tree for the milk problem.

9.3 Decision Trees
299
a
n
c
a
n
c
n
c
c
a
as
al
am
cs
cf
as
al
am
as
al
am
8
−4
0
nt,0
t,−1
R
B
B
B
A
A
C
A
n
12
B
−6
−10
12
−6
−10
12
−6
−10
T
g
b
e
Fig. 9.17. A coalesced decision tree for the reactor problem. If we decide to build
a conventional reactor the resulting subtrees will be the same regardless of our
previous decisions and observations.
chance node A, the expected utility for A is calculated. Each child of A is
an outcome o and has a utility U(o) attached, and the link has a probability
P(A = a). We calculate the product U(o) · P(A = a) from each child, and
their sum is attached to A. If the node is a decision node D, each child of
D has an (expected) utility attached. Choose a child with maximal expected
utility, highlight the link, and attach the value to D.
This is done repeatedly until the root is reached. The resulting value for the
root is the expected utility if you adhere to the strategy of always maximizing
the expected utility, and the paths from root to leaves following highlighted
links when possible represent an optimal strategy for the decision problem.
Example 9.4 (The Car Start Problem, continued).
Figure 9.18 illustrates the calculations for solving the troubleshooting
problem.

300
9 Graphical Languages for Speciﬁcation of Decision Problems
T
16.96
ok
12.96
14.5
RS
SP
16.2
25.5
ok
9.69
¬ok
6.51
10.5
RS
¬ok
4
14.5
RS
IS
20
27.5
ok
13.75
¬ok
6.25
12.5
RS
SP
16.27
26
ok
7.8
¬ok
8.47
11
RS
IS
11.1
24
ok
3.36
¬ok
7.74
9
RS
T
12.1
ok
7.46
10.5
RS
¬ok
4.64
10.5
RS
IS
16
23.5
ok
11.75
¬ok
4.25
8.5
RS
15
RS
IS
15.43
28
ok
2.8
¬ok
12.63
13
RS
T
14.03
¬ok
1.38
12.5
RS
ok
12.66
12.5
RS
SP
14.22
23.5
ok
8.93
¬ok
5.29
8.5
RS
SP
14
24
ok
8
¬ok
6
9
RS
Fig. 9.18. Results when solving the decision tree from Figure 9.11. The boldfaced
links indicate the optimal strategy.

9.3 Decision Trees
301
As can be seen from Figure 9.18, the maximum expected utility is 16.96.
A strategy close to the optimal one (in terms of expected utility) is to start
performing SP and if unsuccessful to follow with T .
More formally, if we use N(X = x) to denote the node following X by
the link labeled x, then the “average-out and fold-back” algorithm can be
speciﬁed recursively as follows.
Algorithm 9.1 [Expected-Utility (EU)] Let X be a node in a decision
tree T . To calculate an optimal strategy and the maximum expected utility for
the subtree rooted at X, do:
1. If X is a utility node, then return U(X).
2. If X is a chance node, then return
EU(X) =

x∈sp(X)
P(X = x | past(X)) EU(N(X = x)).
3. If X is a decision node, then return
EU(X) =
max
x∈sp(X) EU(N(X = x)),
and mark the arc labeled:
x′ = arg max
x∈sp(X) EU(N(X = x)).
⊓⊔
By unfolding the calculations in the algorithm, we see that the expected
utility of an optimal strategy Δ is the sum of the utilities of the possible
outcomes o (the leaves in the decision tree) weighted by the probability of the
path down to o under the strategy Δ:
EU(Δ) =

o
U(o)P(o | Δ).
The probability P(o | Δ) is the product of the probabilities attached to the
arcs on the path from the root to o, where arcs emanating from decision nodes
contribute 1 if they are part of Δ and 0 otherwise. For example, the strategy
in Figure 9.18 is ﬁrst to perform the test T, and if it says ok then follow with
SP and possibly RS. If T says ¬ok, then follow with IS and possibly RS. The
strategy has four possible outcomes, and the expected utility is
EU(Δ) = 25.5 · P(T = ok, SP = ok | Δ) + 10.5 · P(T = ok, SP = ¬ok | Δ)+
12.5 · P(T = ¬ok, IS = ¬ok | Δ) + 27.5 · P(T = ¬ok, IS = ok | Δ)
= 25.5 · 0.8 · 0.38 + 10.5 · 0.8 · 0.62 + 12.5 · 0.2 · 0.5 + 27.5 · 0.2 · 0.5
= 16.96.

302
9 Graphical Languages for Speciﬁcation of Decision Problems
In general, this procedure can be used for calculating the expected utility
of any strategy; hence the identiﬁcation of an optimal strategy could also be
formulated as
Δ = arg max
Δ′ EU(Δ′).
This approach, however, clearly has a complexity problem, since we should
explore all possible strategies. The reason that this problem is not as apparent
in the algorithm above is that it exploits a general principle known as dynamic
programming. The idea is that the contribution from, say, the subtree rooted
at T = ¬ok is independent of the subtree rooted at T = ok; hence a strategy
that is optimal for the subtree at T = ¬ok will be part of an optimal strategy
for the full decision tree.
9.4 Inﬂuence Diagrams
Decision trees are very easy to use, but they have a serious drawback: the
number of decisions and observations need not be large before it becomes
an inhuman task to specify the problem. We therefore look for other model-
ing frameworks that in a much more compact way can be used to represent
decision problems with several decisions and observations.
In this section we present the inﬂuence diagram framework. It is particu-
larly well suited for so-called symmetric decision problems.
In the decision tree framework, we used two models for describing a deci-
sion problem: a Bayesian network for calculating probabilities and a decision
tree for representing the sequence of decisions and observations. In the inﬂu-
ence diagram framework the approach is diﬀerent: the Bayesian network is
extended with syntactic features that will allow it to encode the probability
model as well as the structure of the decision problem.
9.4.1 Extended Poker Model
In the poker problem described in Section 3.2.3, the ﬁnal decision is whether
to call or fold. When taking this decision I have information about my own
hand (MH) as well as the number of cards my opponent has discarded in the
ﬁrst and in the second round of changing cards. However, before I come that
far I would also have had to decide on my ﬁrst change of cards (MFC) and my
second change of cards (MSC). In order to make these two decisions explicit
in the representation, you can extend the model in Figure 9.3 with MFC and
MSC as well as two variables representing my initial hand (MH0) and my
hand after the ﬁrst change of cards (MH1). The resulting model is shown in
Figure 9.19.
Looking at Figure 9.19 we see that even though all relevant variables are
included in the model, it does not convey the order in which the decisions

9.4 Inﬂuence Diagrams
303
OH0
OH1
OH2
OFC
OSC
BH
MH2
MH0
MH1
MFC
MSC
U
D
Fig. 9.19. The poker model in Figure 9.3 extended with variables for my initial
hand (MH0), my ﬁrst change of cards (MFC), my second hand (MH1), and my
second change of cards (MSC).
are taken; nor does it specify the variables that are observed before a par-
ticular decision: before deciding on the ﬁrst decision MFC I observe MH0;
then I observe my opponent’s ﬁrst change of cards OFC as well as my second
hand MH1 before I decide on MSC; and ﬁnally, I observe both MH2 and my
opponent’s second change of cards OSC prior to deciding on D.
An immediate way to encode this information directly in the model is to
extend the model with so-called information arcs. An information arc is a
directed arc X →D going into a decision node D from either a chance node
or another decision X. Semantically it speciﬁes that X is either observed (if
it is a chance node) or decided on (if it is a decision node) before we decide
on D. By extending the model in Figure 9.19 with information arcs we get
the model in Figure 9.20, where we can see, for example, that when deciding
on MSC we know the state of OFC, MH0, MFC, and MH1.
Now assume that we adopt the no-forgetting assumption from the decision
tree framework, i.e., the decision maker remembers all previous observations
and decisions. Given this assumption, we see that the model in Figure 9.20
contains redundant information arcs. For example, the arc MFC →MSC indi-
cates that we decide on MFC before deciding on MSC, and the two arcs from
MH0 into MFC and MSC specify that the state of MH0 is known when we
decide on both MFC and MSC. However, under the no-forgetting assumption
the link from MH0 to MSC is redundant and it can therefore be removed. Sim-
ilarly, MFC has an impact on MH1, which is observed before MSC. Therefore,
MFC must precede MSC, and the link from MFC to MSC can be removed.
By iteratively removing all redundant information arcs we obtain the model
in Figure 9.21.
A model such as the one shown in Figure 9.21 is also called an inﬂuence
diagram, and it encodes information about the probability model as well as the
relevant information about the structure of the decision problem: the directed

304
9 Graphical Languages for Speciﬁcation of Decision Problems
OH0
OH1
OH2
OFC
OSC
BH
MH2
MH0
MH1
MFC
MSC
U
D
Fig. 9.20. The poker model in Figure 9.19 extended with information arcs into the
decision variables.
OH0
OH1
OH2
OFC
OSC
BH
MH2
MH0
MH1
MFC
MSC
U
D
Fig. 9.21. The poker model in Figure 9.19, where the redundant information arcs
have been removed.
path going through all the decision variables speciﬁes the sequence in which
the decisions are made, and the chance variables appearing as parents of a
decision variable are the set of chance variables observed immediately before
that decision. For example, since MH2 and OSC are parents of D, they are
observed immediately before D but after the decisions MFC and MSC. Note
that we do not specify the sequence in which MH2 and OSC are revealed, but
their ordering will not aﬀect the solution of the inﬂuence diagram (see also
Section 9.3.3 and Section 10.1). In summary, the sequence of observations and
decisions can be described as follows:
{MH0} ≺MFC ≺{MH1, OFC} ≺MSC ≺{MH2, OSC} ≺D
≺{OH0, OH1, OH2, BH}.
For the last set of variables it should be noted that whether a variable will
eventually be observed depends on the semantics of that variable and cannot
be deduced from the syntax of the inﬂuence diagram. Finally, we also see that
due to the no-forgetting assumption we can read that at the time of deciding

9.4 Inﬂuence Diagrams
305
on D, I will know the states of the parents MH2 and OSC, and by assuming
that I do not forget my past, I will also know the states of MH0, MFC, MH1,
OFC, and MSC.
9.4.2 Deﬁnition of Inﬂuence Diagrams
In the previous section we exempliﬁed the inﬂuence diagram framework as
an alternative to the decision tree framework. Historically, inﬂuence diagrams
were invented as a compact representation of decision trees for symmetric
decision problems (see Section 9.5). Now they are seen more as a decision tool
extending Bayesian networks, and below we formally introduce the inﬂuence
diagram framework in this way.
Syntax
An inﬂuence diagram consists of a directed acyclic graph over chance nodes,
decision nodes, and utility nodes with the following structural properties:
•
there is a directed path comprising all decision nodes;
•
the utility nodes have no children;
•
the decision nodes and the chance nodes have a ﬁnite set of states;
•
the utility nodes have no states.
An inﬂuence diagram is realized when the following quantities have been spec-
iﬁed:
•
a conditional probability table P(A | pa(A)) is attached to each chance
node A;
•
a real-valued function over pa(U) is attached to each utility node U.
Unless the context requires a distinction we let the term “inﬂuence dia-
gram” include a speciﬁcation of probabilities and utilities.
Figure 9.22 shows an example of an inﬂuence diagram (the states of the
variables are not speciﬁed).
Semantics
Links into a decision node yield no quantitative requirements. They are called
information links, and they indicate that the states of the parents are known
prior to taking the decision. On the other hand, links into chance nodes or
utility nodes represent functional relations.
The structural requirement that there be a path comprising all decision
nodes ensures that the inﬂuence diagram deﬁnes a temporal sequence of de-
cisions. This yields a partitioning of the chance variables into disjoint subsets
according to the time of observation. The set I0 is the set of variables observed
before any decision is taken. The set I1 is the set of variables observed after

306
9 Graphical Languages for Speciﬁcation of Decision Problems
L
D1
V1
D2
D3
V3
D4
V4
A
B
D
C
E
F
H
G
V2
K
J
I
Fig. 9.22. An example of an inﬂuence diagram.
the ﬁrst decision D1 is taken but before the second decision D2, and the set
Ii is the set of chance variables observed after decision Di but before decision
Di+1. If there are n decisions, In is the set of variables that are observed after
Dn or not at all:
I0 ≺D1 ≺I1 ≺. . . ≺In−1 ≺Dn ≺In.
For example, in Figure 9.22 we have I0 = {B}, I1 = {E, F}, I2 is empty,
I3 = {G}, and I4 = {A, C, D, H, I, J, K, L}. The ordering ≺therefore speciﬁes
a partial temporal ordering over the variables in the inﬂuence diagram; the
ordering is partial since we do not have an ordering over the variables in each
of the sets Ii.
There is a hidden assumption behind the semantics of inﬂuence diagrams,
namely no-forgetting: the decision maker remembers the past observations and
decisions. Thus, at Di we know the state of the variables appearing before Di
under ≺.
In some decision problems, two decisions may be independent in the sense
that they can be taken in any order without changing the expected utilities.
In Figure 9.22, the two decisions D2 and D3 are independent. Therefore, the
link from D2 to D3 puts an unnecessary restriction on the decision maker. It
could be removed and the representation would still be meaningful, although
the ﬁrst structural requirement would be violated. Unfortunately, it is not
always easy to characterize situations in which decisions are independent, and
we will keep the ﬁrst structural requirement, which ensures a well-speciﬁed

9.4 Inﬂuence Diagrams
307
decision problem. We shall, however, return to this issue in Section 9.5.2 and
Section 11.2.
If there is more than one utility node, then the entire utility can be either
the sum or the product of the individual utilities. Due to the intuitive appeal,
local utilities are usually treated as components in a sum. For instance, in
the mildew example (Section 9.1.2) we have two local utility functions: C,
which represents the cost of the various treatments, and U, which represents
the utility of the harvest for each state of the crops. The total utility is the
sum of C and U, and if we assume that both C and U are the actual costs
and payoﬀs, then the sum simply encodes the overall monetary value of the
diﬀerent scenarios as described by the parents of C and U. Should it happen
that the total utility is the product rather than the sum of the local utilities,
then taking the logarithm of the utilities will transform the problem into an
inﬂuence diagram in which the total utility is the sum of the transformed
utilities.
Solving an Inﬂuence Diagram
An inﬂuence diagram provides a description of a decision problem and should
subsequently be used to aid the decision maker in the decision process. This
amounts to prescribing an action for each decision variable conditioned on
the previous observations and decisions. A way of doing the prescription is
to transform the inﬂuence diagram into a decision tree and then apply the
“average-out and fold-back” algorithm. The inﬂuence diagram’s decision tree
representation has the property that each node representing a decision D has
the same variables in the past. Let past(D) denote the variables in D’s past.
Thus, if in the decision tree we have an action for each such decision node,
these actions will collectively specify an action for each possible conﬁguration
past(D). Such a speciﬁcation is called a policy (denoted by δ) for D:
δD : sp(past(D)) →sp(D) .
If we have a policy for each decision variable in an inﬂuence diagram, we call
it a strategy. For example, a strategy for the inﬂuence diagram in Figure 9.21
will consist of three policies:
δMFC : sp(MH0) →sp(MFC);
δMSC : sp(MH0, MFC, MH1, OFC) →sp(MSC);
δD : sp(MH0, MFC, MH1, OFC, MH2, OSC) →sp(D) .
If the strategy encodes the solution of the “average-out and fold-back” algo-
rithm (i.e., the strategy maximizes the expected utility), then the strategy is
called an optimal strategy and each of its policies is called an optimal policy.
Deﬁnition 9.1. A policy for decision Di is a mapping δi that for any con-
ﬁguration of the past of Di yields a decision for Di. That is,

308
9 Graphical Languages for Speciﬁcation of Decision Problems
δi(I0, D1, . . . , Di−1, Ii−1) ∈sp(Di) .
A strategy for an inﬂuence diagram is a set of policies, one for each deci-
sion. A solution to an inﬂuence diagram is a strategy maximizing the expected
utility.
By transforming the inﬂuence diagram into a decision tree in order to solve
it, the complexity problem inherent in the decision tree framework is still
present in the solution phase. However, solution methods working directly on
the inﬂuence diagram have also been developed (see Chapter 10).
9.4.3 Repetitive Decision Problems
Fishing in the North Sea
Every year, the European Union undertakes very delicate political and bio-
logical negotiations to determine a volume of ﬁshing for most kinds of ﬁsh in
the North Sea. Simpliﬁed, you can say that each year the EU has a test for
the volume of ﬁsh, and based on this test the volume of allowable catch is
decided. This decision has an impact on the volume for next year (note that
the decision on volume does not mean that only this volume is actually caught
– quotas have a status similar to speed limits on highways). Figure 9.23 gives
an inﬂuence diagram for a ﬁve-year strategy, where each variable is given ten
states.1
F V1
F V2
F V4
U2
U4
U5
V2
V1
V3
V4
V5
T2
T1
T3
F V3
T4
T5
F V5
U1
U3
Fig. 9.23. An inﬂuence diagram for a ﬁve-year strategy for ﬁshing volumes of
herring in the North Sea.
The ﬁshing model above has a complexity problem. For the ﬁfth decision,
all the past is relevant. Because there are nine ten-state variables in the past,
the domain of the policy function for FV5 has 109 elements.
1 The model in Figure 9.23 is an example of a partially observable Markov decision
process (POMDP), which we shall consider further in Section 9.6.2.

9.4 Inﬂuence Diagrams
309
This does not mean that whenever the past is intractably large, the com-
puter must give up. It fortunately often happens that not all information from
the past is relevant (see Section 11.2).
Sometimes solving even fairly small inﬂuence diagrams represents an in-
tractable task, and then you must use various approximation methods. One
method is blocking. The principle in information blocking is to introduce vari-
ables that when observed, d-separate most of the past from the present deci-
sion.
Fishing Again
The problem with the model in Figure 9.23 is that all information from the
past has an impact on how we will estimate the current volume of ﬁsh. We
can make an approximation by allowing only this year’s test and ﬁshing vol-
ume to be used for estimating next year’s volume of ﬁsh. In the model, we
delete the arrows Vi →Vi+1 and instead introduce the arrows Ti →Vi+1 (see
Figure 9.24).
U3
F V2
F V4
U2
U4
U5
V2
V1
V3
V4
V5
F V1
T2
T1
T3
F V3
T4
T5
F V5
U1
Fig. 9.24. The inﬂuence diagram from Figure 9.23 approximated through informa-
tion blocking.
To establish the potential P(Vi+1 | Ti, FVi), we can use the model in Fig-
ure 9.23.
P(V2, T1 | FV1) =

V1
P(V1)P(T1 | V1)P(V2 | V1, FV1),
P(T1 | FV1) =

V2
P(V2, T1 | FV1),
P(V2 | T1, FV1) = P(V2, T1 | FV1)
P(T1 | FV1)
.
This last potential is used for all time slices.
The trick just shown is an example of a general information-blocking tech-
nique whereby you abstract the past into a history variable and allow only

310
9 Graphical Languages for Speciﬁcation of Decision Problems
temporal links from observed variables and from the history variable (see
Figure 9.25 for another example).
A1
A2
An
B1
B2
Bn
C1
C2
Cn
D1
D2
Dn
E1
E2
En
U1
U2
Un
A1
A2
An
B1
B2
Bn
C1
C2
Cn
D1
D2
Dn
E1
E2
En
H2
H3
Hn
U1
U2
Un
Fig. 9.25. In the top ﬁgure we have to take the entire past into account when de-
ciding on Dn. In the lower ﬁgure, history variables have been introduced to perform
information blocking.
9.5 Asymmetric Decision Problems
From the speciﬁcation of the syntax for the inﬂuence diagram we see that the
sequence in which the nodes are observed and decided on is the same in all
possible scenarios (up to a permutation of the chance nodes in the sets Ii).
For instance, in the poker example we always start by observing MH0, and
regardless of the outcome we then decide on MFC, etc. These types of decision
problems are also called symmetric decision problems, because they can be
represented by a decision tree that is completely symmetric (see Figure 9.26
for an example). If a decision problem is not symmetric we call it asymmetric.
Deﬁnition 9.2. A decision problem is said to be symmetric if:
•
in all of its decision tree representations, the number of scenarios is the
same as the cardinality of the Cartesian product of the state spaces of all
chance and decision variables, and
•
in at least one decision tree representation, the sequence of chance and
decision variables is the same in all scenarios.

9.5 Asymmetric Decision Problems
311
In particular, the ﬁrst requirement ensures that the possible outcomes and
decision options for a variable do not depend on previous observations and
decisions. Moreover, the reason why the deﬁnition deals with several decision
tree representations for a decision problem is that two consecutive observa-
tions (without intermediate decisions) or two consecutive decisions can be
swapped without aﬀecting the solution to the decision problem. For example,
in Figure 9.26 the cardinality of the product of the state spaces of all variables
is 2·2·2·2 = 16. This is also the number of scenarios in the decision tree, and
since the decision tree also adheres to the second condition in the deﬁnition
above, the underlying decision problem is symmetric.
A
C
C
C
C
C
C
C
C
D2
D2
D2
D2
A
D1
2
y
n
n
y
1
2
y
n
1
2
y
y
n
n
1
2
y
1
y
n
n
2
y
y
y
n
n
n
1
B
D1
D2
A
C
Fig. 9.26. A symmetric decision tree and the associated probability model.
The inﬂuence diagram corresponding to the decision problem shown in
Figure 9.26 is given in Figure 9.27. From this example we see that the inﬂu-
ence diagram provides a much more compact representation of the decision

312
9 Graphical Languages for Speciﬁcation of Decision Problems
problem than does the decision tree. However, this holds only for symmetric
problems: in the (asymmetric) decision tree shown in Figures 9.8 and 9.9 we
observe only the result of the ﬁrst test Test1 if we decide to actually perform
the test (D1 = T1 or D2 = T1). That is, the sequence in which we make
observations and decisions may vary in the diﬀerent scenarios, but the inﬂu-
ence diagram does not provide an immediate mechanism for representing such
types of conditional orderings.
B
D1
D2
A
C
U
Fig. 9.27. An inﬂuence diagram representation corresponding to the decision tree
from Figure 9.26.
The use of test decisions (like the ones in Figure 9.8 and 9.9 and in Exam-
ple 9.1) is a frequent causes of asymmetry in decision problems: if you decide
to perform a test, you will eventually observe the test result, but if you decide
not to perform the test then a result will never be observed. Inﬂuence dia-
grams do not contain a special representation of test decisions. However, there
is a general way of representing test decisions as ordinary decision variables.
Assume, in the crop example in Figure 9.1, that I am in the situation that I
can test the severity of the mildew attack before I decide on whether to spray.
The node Attack represents the severity of the attack before spraying, so to
model the impact of spraying we introduce a new chance node, A-Attack, rep-
resenting the attack after the spray decision P. The decision is connected to
the model by inserting a link from P to A-Attack. To model the test decision
we insert a decision node T. This decision is basically a decision on whether
the state of the chance node Attack should be revealed before deciding on
P (we assume the test to be accurate). One way to model this situation is
to introduce an additional node Attack′ with the same states as Attack and
with the additional state, unobserved, for handling the situation in which we
decide not to perform the test. Next we add an arc from T and Attack to
Attack′ and an informational arc from Attack′ to P. The ﬁnal model is shown
in Figure 9.28.

9.5 Asymmetric Decision Problems
313
P
T
Resistance
Attack
Attack′
A-Attack
Crop
Fig. 9.28. An inﬂuence diagram representation (without utility nodes) of the crop
problem: should you investigate the severity of the mildew attack before deciding
on spraying against mildew?
The table for Attack′ given T and Attack is speciﬁed so that the state is
unobserved if T is no, and if T is yes, then Attack′ is in the same state as
Attack (see Table 9.3 and 9.4).
Attack
y
n
T
y (1, 0, 0) (0, 1, 0)
n (0, 0, 1) (0, 0, 1)
Table 9.3. The probability table P(Attack′ = (y, n, unobserved) | Attack, T) associ-
ated with Attack′ in Figure 9.28.
This construction is general, and it is illustrated in Figure 9.29 and Ta-
ble 9.4. In this way, methods developed for computing decision strategies can
also be used for decision scenarios containing test decisions.
T
A
A′
D
Fig. 9.29. A general way to model a decision on whether to observe A before
deciding on D.
The construction can be made a bit simpler by extending the node A with
the extra state unobserved and thereby avoiding the extra node A′. However,
usually it is preferable not to change the nodes of the initial (causal) model.

314
9 Graphical Languages for Speciﬁcation of Decision Problems
A
a1
. . .
an
T
y (1, . . . , 0, 0) . . . (0, . . . , 1, 0)
n (0, . . . , 0, 1) . . . (0, . . . , 0, 1)
Table 9.4. The probability table P(A′ = (a1, . . . , an, unobserved) | A, T) associated
with A′ in Figure 9.29.
As the modeling technique illustrates, inﬂuence diagrams can be used to
model decision problems even when the decision problem is not completely
symmetric. However, this comes at a cost since we need to introduce artiﬁcial
states (e.g. the state unobserved) and in some situations it may also be nec-
essary to introduce artiﬁcial nodes. In the extreme case in which the decision
problem does not contain any symmetric substructures, the decision tree will
provide a more compact representation than the inﬂuence diagram.
9.5.1 Diﬀerent Sources of Asymmetry
As we have discussed above, inﬂuence diagrams are not really suitable for
modeling asymmetric decision problems. However, decision trees are not re-
ally an alternative either when there are many observations and decisions.
Therefore, much research has been directed at ﬁnding speciﬁcation languages
that much more compactly can represent the information needed for describ-
ing the decision problem. The following two examples shed additional light on
some of the problems we face when constructing such languages.
Example 9.5 (The Diagnosis Problem). Consider a two-test problem like the
one in Example 9.1, Page 290; after an initial observation I you have two
tests, TA and TB, and a decision Pour?. The decision on pouring is the last
decision, but the two tests can be performed in any order.
To represent this problem by an inﬂuence diagram we have to represent the
unspeciﬁed ordering of the tests as a linear ordering of decisions. Introduce two
decision nodes, Test1 and Test2, with options, tA, tB, and no-test; introduce
two chance nodes, O1 and O2, as children of Inf? with states posA, negB, posA,
negB, and no-test. To specify that two consecutive tests of the same type will
give the same results, you introduce a link from O1 to O2 (See Figure 9.30).
Example 9.6 (The Dating Problem). Joe needs to decide whether he should
ask (Ask) Emily for a date for Friday evening. He is not sure whether Emily
likes him (LikesMe). If he decides not to ask Emily or if he decides to ask and
she turns him down, he will then decide whether to go to a nightclub or watch
a movie on TV at home (NClub?). Before making this decision, he will consult
the TV guide to see whether there are any movies he would like to see (TV). If
he decides to go to a nightclub, he will have to pay a cover charge and pay for
drinks. His overall nightclub experience (NCExp) will depend on whether he

9.5 Asymmetric Decision Problems
315
Test1
Test2
Pour?
I
Inf
O1
O2
C1
C2
CB
U
Fig. 9.30. An inﬂuence diagram representation of two tests and a decision on
pouring. The Test nodes have three options, tA, tB, and no-test. The O nodes have
ﬁve states, posA, posB, negA, negB, no-test. The arc O1 →O2 indicates that repeating
a test will give identical results.
meets his friends (MeetFr), the quality of the live music, etc (Club). If Emily
accepts (Accept), then he will ask her whether she wishes to go to a restaurant
or to a movie (ToDo); Joe cannot aﬀord to do both. If Emily decides on a
movie, Joe will have to decide (Movie) whether to see an action movie he
likes or a romantic movie that he does not really care for, but which may
put Emily in the right mood (mMood) to enhance his post movie experience
with Emily (mExp). If Emily decides on a restaurant, he will have to decide
(Rest) whether to select a cheap restaurant or an expensive restaurant. He
knows that his choice will have an impact on his wallet and on Emily’s mood
(rMood), which in turn will aﬀect his post restaurant experience with Emily
(rExp).
From the examples above we can identify three types of asymmetry:
Functional asymmetry: The possible outcomes or decision options of a
variable may vary depending on the past. We saw this in the reactor
problem, where the options of the build decision are dependent on the
result of a test.
Structural asymmetry: The very occurrence of an observation or a deci-
sion depends on the past. In the Dating Problem, for example, the restau-
rant options exist only if Emily accepts the invitation.
Order asymmetry: The ordering of the decisions and observations is not
settled at the time the model is speciﬁed. For instance, in the Diagnosis
Problem the ordering of the two tests is unspeciﬁed.

316
9 Graphical Languages for Speciﬁcation of Decision Problems
9.5.2 Unconstrained Inﬂuence Diagrams
In this section we shall look at a particular class of decision problems in which
only order asymmetry is present.
Example
Consider again the two-test problem from Example 9.1 (Page 290) and its
inﬂuence diagram representation shown in Figure 9.30. A much more direct
speciﬁcation would be to use decision nodes representing each test explicitly.
If we knew, for example, that TestA comes before TestB, it can done with an
inﬂuence diagram (see Figure 9.31(a)). However, in practice this is rarely the
case.
TestA?
TestB?
Pour?
Inf
I
OA
OB
Cα
Cβ
U
(a)
TestA?
TestB?
Pour?
Inf
I
OA
OB
Cα
Cβ
U
(b)
Fig. 9.31. (a) An ID representing the scenario in which you ﬁrst decide on TestA?
and next on TestB?. (b) An attempt to remove the temporal constraint on the test
decisions.
To relax the temporal constraint on the test decisions, you may remove
the link from OA to TestB? (Figure 9.31(b)). However, now there is no spec-
iﬁcation that the result of the ﬁrst test is known when deciding on the next
test. To specify this we introduce a new type of chance variables, observables.
They are drawn as double circles, and they are observed when all preceding
decision nodes have been decided (Figure 9.32). In that case we say that the
observable is free and that the last preceding decision released the observable.

9.5 Asymmetric Decision Problems
317
TestA?
TestB?
Pour?
Inf
I
OA
OB
Cα
Cβ
U
Fig. 9.32. A graphical representation of two tests and a decision of pouring. Here
I is observed prior to any decision, OA is observed when TestA? has been decided,
and OB is observed when TestB? has been decided.
Looking at Figure 9.32 it may seem that we have not speciﬁed that OA
is actually observed immediately after deciding on T estA?. However, since
the expected utility cannot increase by delaying an observation free of cost,
we can safely introduce the rule that an observable chance node is observed
immediately after it has been released. This means that the decision problem
has been uniquely speciﬁed, and the rest can be left to a computer. The spec-
iﬁcation in Figure 9.32 yields that solving the decision problem boils down
to solving two inﬂuence diagrams (one for each order of the test decisions)
and choosing the order and strategy from the one giving the highest expected
utility. This also means that while the inﬂuence diagram encoded the possi-
ble sequences of observations and decisions at the graphical level, this new
framework has postponed it to the solution phase.
Next, consider a more complex situation. A patient may suﬀer from two
diﬀerent diseases. After an initial observation OI, there are two possible tests,
TA and TB, and each disease has a speciﬁc treatment, T r1 and T r2. After each
treatment, the new state of the disease is observed (cost free). In Figure 9.33
the problem is speciﬁed graphically.
Even for a simple problem like the one above it is extremely cumbersome to
draw a decision tree, and it is rather tricky to squeeze the scenario into the ID
straightjacket; the problem is that all possible sequences must be represented
explicitly.

318
9 Graphical Languages for Speciﬁcation of Decision Problems
TA
TB
OA
OB
OT
D
T r1
T r2
D2
D1
D′
O1
O2
U
Fig. 9.33. A graphical representation of a situation with two tests and two treat-
ments.
Deﬁnition of UIDs
As the examples above illustrate, we can meaningfully relax the linear tem-
poral order constraint for inﬂuence diagrams without getting an ambiguous
representation.
Deﬁnition 9.3. An unconstrained inﬂuence diagram (UID) is an acyclic di-
rected graph over decision variables (rectangular shaped), chance variables
(circular shaped), and utility variables (diamond shaped). Utility variables
have no children. There are two types of chance variables, observables (doubly
circled) and nonobservables (singly circled). A nonobservable cannot have a
decision as a child.
Let U be a UID. The set of decision variables is denoted by DU, and the
set of observables is denoted by OU. The partial temporal order induced by U
is denoted by ≺U. When obvious from the context we avoid the subscript.
The quantitative speciﬁcation required is similar to the speciﬁcation for
inﬂuence diagrams: conditional probabilities and utility functions. We add the
convention that each decision variable D has a cost. If this cost depends only
on D, it is not represented graphically. We say that a UID is realized when
the structure has been extended with the required quantitative speciﬁcations.
The semantics of a UID are similar to the semantics of an ID. A link into a
decision variable represents temporal precedence; a link into a chance variable
represents causal inﬂuence; a link into a utility variable represents functional
dependence. We assume no-forgetting: at each point of the decision process
the decision maker knows all previous decisions and observations.

9.5 Asymmetric Decision Problems
319
An observable can be observed when all its antecedent decision variables
have been decided on. In that case we say that the observable is free, and we
release an observable when the last decision in its ancestral set is taken.
The structural speciﬁcation of a UID yields a partial temporal ordering of
the decisions and observations. An extension to a linear ordering is called an
admissible order. Any admissible order yields an inﬂuence diagram.
S-DAGs and Strategies
As for decision trees and inﬂuence diagrams, the graphical language and its
suitability as a language supporting human modeling are the most important
properties. Having constructed an adequate model, you can hand it over to a
computer, which may then unfold the model to a decision tree and compute
an optimal strategy.
In dealing with UIDs, the concept of strategy is more complex than in the
case of IDs (see Section 9.4.2). In principle we look for a set of rules telling
us what to do given the current information, where “what to do” is to choose
the next action as well as to choose a decision option if the next action is a
decision. That is, a strategy consists of a function prescribing the next step
and a set of functions for choosing decisions. The structure of the step function
can be represented in a graphical structure, called an S-DAG (strategy DAG).
Deﬁnition 9.4. Let U be a UID. An S-DAG is a directed acyclic graph G.
The nodes are labeled with variables from DU ∪OU such that each maximal
directed path in G represents an admissible ordering of DU ∪OU. For nota-
tional convenience we add two unary nodes Source, and Sink. Source is the
only node with no parents and Sink is the only node with no children.
Note that an S-DAG need not contain all admissible orderings. Figure 9.34
gives an example of an S-DAG for the two-tests-two-treatments problem.
Source
OI
TA
TA
TA
OA
OA
OA
T r1
T r1
T r1
T r1
D1
D1
D1
D1
T r2
T r2
T r2
D2
D2
D2
TB
TB
TB
OB
OB
OB
Sink
Fig. 9.34. An example of an S-DAG for the UID in Figure 9.33.
For a node N in an S-DAG G, the history of N is deﬁned as the union of
the labels of N and its ancestors, denoted by hstG(N). When the S-DAG is

320
9 Graphical Languages for Speciﬁcation of Decision Problems
obvious from the context we drop the subscript. For example, the OB-node at
the bottom path in Figure 9.34 has the history {OI, TB, OB} and the children
{TA, T r1}; the set of labels of N’s children is denoted by ch(N). A step policy
for node N is now deﬁned as a function
σ : sp(hst(N)) →ch(N).
Recall that sp(hst(N)) denotes all possible conﬁgurations of the variables in
hst(N).
A step strategy for a UID U is a pair (G, S), where G is an S-DAG for U
and S is a set of step policies, one for each node in G (except for Sink); when
a node has only one child, the step policy is trivial. For a decision node N a
decision policy is a function
δ : sp(past(N)) →sp(N) .
A strategy for U is a step strategy together with a decision policy for each
decision node.
Example
Consider the UID in Figure 9.35. A strategy may have the structure illustrated
by the S-DAG and the simple policy rules in Figure 9.36. Note that the policies
combine step policies and decision policies.
D1
A
B
U1
D2
D3
C
E
F
U2
D4
Fig. 9.35. An example UID.
The strategy represented in Figure 9.36 can be unfolded to the strategy
tree in Figure 9.37. The expected utility from following the strategy can be
calculated in the same way as for decision trees, where the UID is used for
calculating the probabilities.

9.5 Asymmetric Decision Problems
321
D1
B
D2
D2
D3
D3
C
C
E
E
D4
∅: choose option d1
1
D1, B : choose
(
d3
2
if B = b1,
d2
1
if B = b2.
D1, B, D3, E : choose
(
d2
2
if E = e1,
d2
1
if E = e2.
D1, B, D2, C : choose
(
d3
1
if C = c1,
d3
2
if C = c2.
Fig. 9.36. The structure of a strategy for the UID in Figure 9.35.
D1
d1
1
B
b1
b2
D2
D2
D2
d1
2
d1
2
d2
2
C
C
C
c1
c1
c1
c2
c2
c2
D3
D3
D3
d1
3
d2
3
d2
3
E
E
E
e1
e1
e1
e2
e2
e2
F
F
F
F
F
F
F
F
f1
f1
f1
f1
f1
f1
f1
f1
f2
f2
f2
f2
f2
f2
f2
f2
U1
U2
U3
U4
U5
U6
U7
U8
U9
U10
U11
U12
U13
U14
U15
U16
Fig. 9.37. The strategy from Figure 9.36 unfolded to a strategy tree.

322
9 Graphical Languages for Speciﬁcation of Decision Problems
Deﬁnition 9.5. Let Δ be a strategy for the UID U. The expected utility of
Δ is the expected utility of the corresponding unfolded strategy tree for Δ with
respect to U. A solution to U is a strategy of maximal expected utility. Such
a strategy is called an optimal strategy. The S-DAG for an optimal strategy
is called optimal, and the step policies as well as the decision policies are also
called optimal.
Rather than trying out all possible strategy trees in looking for an op-
timal strategy, there are eﬃcient solution algorithms that exploit dynamic
programming and work on a (single) S-DAG representation of the UID (see
Section 10.4).
9.5.3 Sequential Inﬂuence Diagrams
There is no widely recognized graphical language that compactly can cope
with all types of asymmetry. Here we shall indicate only one attempt, called
sequential inﬂuence diagrams (SIDs). The SID framework has its source in a
(causal) world model like the one in Figure 9.15. To extend this world model
to also represent the structure of the decision problem we need to specify the
order of the decisions and observations as well as any asymmetry constraints.
There are various ways of doing so. In the case of inﬂuence diagrams, the
order is speciﬁed in the same graph through information links, but you may
also have a separate speciﬁcation (as in decision trees).
The SID framework takes the former approach by extending the world
model with features specifying order and asymmetry constraints. This is done
in Figure 9.38. The world model is extended with dashed arrows (structural
links) indicating informational precedence. A label on a link is a guard re-
ﬂecting asymmetry constraints. A guard consists of two parts. The ﬁrst part
takes care of structural asymmetry, and the second part describes functional
asymmetry. That is, the ﬁrst part describes the condition for following the
link. If the condition is satisﬁed we say that the link is open. For example, if
we decide to perform the test T = t in Figure 9.38, then the next node will
be R. If there are constraints on the choices at a decision node, then this is
speciﬁed in the second part of the guard (this part is empty when there are
no constraints). In Figure 9.38 the choice a in B can be taken only if a test is
not performed or a test is performed and the result is either good or excellent
(i.e., the scenario satisﬁes (T = nt) ∨(T = t ∧(R = e ∨R = g))).
The speciﬁcation can be unfolded to a decision tree by iteratively following
the open arcs from a source node (a node with no incoming structural arcs)
until a node is reached with no open outgoing arcs.
An SID speciﬁcation of the Dating Problem is shown in Figure 9.39. The
framework partly adopts the UID method of representing order asymmetry
by introducing clusters of nodes (encapsulated in a dashed ellipse). In terms
of information precedence, we can think of a cluster C of nodes as a single
node in the sense that a structural arc going into C from a node X indicates

9.5 Asymmetric Decision Problems
323
Cp.
R
b, g, e
A
as, al, am
T
t, nt
B
a, c, n
C
cs, cf
t
nt
a|∗
c
U1
U2|a
U3|c
U4|n
Fig. 9.38. A graphical representation of the Reactor Problem; the ∗denotes that
the choice B = a is allowed only in scenarios that satisfy (T = nt) ∨(T = t ∧(R =
e ∨R = g)).
U6
U5
acy,acn
asy,asn
LikesMe
Accept
Ask?
ToDO
ly,ln
m,r
mg,mb
Movie
TVExp
teg,teb
ncy,ncn
fy,fn
U1
U2
U3
U4
TV
tg,tb
NClub?
Club
cg,cb
MeetFr
NCExp
neg,neb
mMood
mExp
rExp
rMood
rg,rb
Rest.
meg,meb
reg,reb
ro,ac
rc,re
acn
asn
asy
acy
m
r
ncy
ncn
Fig. 9.39. An SID representation of the Dating Problem.

324
9 Graphical Languages for Speciﬁcation of Decision Problems
that when X has been observed or decided on, the next node is a node in
C. A structural arc from C to a node Y indicates that Y will be the next
node in the ordering on leaving C. Figure 9.39 illustrates the use of clusters
for representing the partial temporal ordering over the chance nodes Club and
MeetFr. From the model we see that these two nodes will be observed only
after a decision on NClub? but before NCExp is observed.
A sequential inﬂuence diagram can be solved by unfolding it into a deci-
sion tree. There are, however, more eﬃcient ways, which identify symmetric
subtrees and solve them as inﬂuence diagrams, but that is outside the scope
of this book.
9.6 Decision Problems with Unbounded Time Horizons
Consider a problem of robot navigation in which a robot is placed in some
environment and its task is to ﬁnd a path from its current position to a certain
goal position. Each time the robot moves from one position to another it incurs
a loss (fuel expenditure), but when it reaches the goal state it receives a reward
and the navigation task ends. The aim is now to ﬁnd a sequence of moves that
will maximize the robot’s expected reward (and minimize its expected loss):
The problem above is an example of a general type of problem called
planning under uncertainty:
•
at each step we are faced with the same type of decision,
•
at each step we are given a certain reward (possibly negative) determined
by the chosen decision and the state of the world,
•
the outcome of a decision may be uncertain,
•
the time horizon of the decision problem is unbounded.
Examples of other problems of this type include factory process control and
transportation logistics.
In Section 9.4.3 we discussed a related type of decision problem, namely
repetitive decision problems with a bounded time horizon. In what follows we
extend this discussion to unbounded time horizons.
9.6.1 Markov Decision Processes
In the robot navigation problem above, the robot’s process can roughly be
described as an unbounded loop over the following events:
1. observe the state of the world (for example the robot’s position in the
world),
2. decide on the next action and collect the reward (possibly negative),
3. perform the action.

9.6 Decision Problems with Unbounded Time Horizons
325
Using the inﬂuence diagram modeling language, we can represent the qual-
itative part of this problem by the structure in Figure 9.40. The node Si rep-
resents the state of the world at step i; Di is the ith decision of the robot;
and Ri is the reward received when action Di is performed in state Si. The
dashed arcs indicate that the future time horizon may be unbounded.
S0
Si−1
Si
Si+1
D0
Di−1
Di
Di+1
R0
Ri−1
Ri
Ri+1
Fig. 9.40. A snapshot of a model of a Markov decision process. Si represents the
state at step i; Di represents the ith decision; and Ri represents the reward of taking
decision Di in state Si.
In order to specify the quantitative part of the model we need some ad-
ditional information about the problem domain. Speciﬁcally, we shall assume
that the robot is placed in the 3 × 3 grid environment shown in Figure 9.41.
The robot can move north, east, south, and west, and for each move it incurs a
loss of 0.1. If the robot decides to move, say, north, then this move will succeed
with probability 0.7, and with probability 0.3 the robot will “slip” and move
in one of the other three directions with equal probability; if the robot moves
into a wall it will remain at its current position. At any point in time the robot
can observe its exact position, and the aim is now to ﬁnd a sequence of moves
that will take it to the goal state at position (3, 1) in the upper right corner.
At the goal state it will receive a reward of 10, and from this state it cannot
exit. Such a state is called a terminal state. At positions (2, 2) and (3, 2) two
obstacles are placed that will incur a loss of 5 and 1, respectively. Although
the environment is bounded, the decision problem is in principle unbounded.
The robot may, for example, cycle between two positions an indeﬁnite number
of times before entering the goal state.
Returning to the model in Figure 9.40, we see that the variable Si has
a state for each possible position of the robot (a total of nine), and based
on the description above, the associated transition function can (for Di =
north) be speciﬁed as in Table 9.5; the structure of the transition function is
similar for the other actions. For this particular example, the reward function
is independent of the chosen decision, and R(Si, D) (= R(Si)) speciﬁes a value
for each position.

326
9 Graphical Languages for Speciﬁcation of Decision Problems
1
1
2
2
3
3
−1
−5
10
Fig. 9.41. A 3 × 3 grid world.
Si
(1, 1) (1, 2) (1, 3) (2, 1) (2, 2) (2, 3) (3, 1) (3, 2) (3, 3)
Si+1
(1, 1) 0, 8
0, 7
0
0, 1
0
0
0
0
0
(1, 2) 0, 1
0, 1
0, 7
0
0, 1
0
0
0
0
(1, 3)
0
0.1
0.2
0
0
0.1
0
0
0
(2, 1)
0.1
0
0
0.7
0.7
0
0
0
0
(2, 2)
0
0.1
0
0.1
0
0.7
0
0.1
0
(2, 3)
0
0
0.1
0
0.1
0.1
0
0
0.1
(3, 1)
0
0
0
0.1
0
0
1
0.7
0
(3, 2)
0
0
0
0
0.1
0
0
0.1
0.7
(3, 3)
0
0
0
0
0
0.1
0
0.1
0.2
Table 9.5. The transition function P(Si+1 | north, Si) for the robot navigation prob-
lem.
The robot navigation problem is an example of a Markov decision process
(MDP). In general, in a Markov decision process:
•
the world is fully observable, i.e., the agent can observe the true state of
the world at any point in time,
•
the uncertainty in the system is a result of the consequences of the ac-
tions being nondeterministic (when performing an action we make a state
transition with a certain probability), and
•
for each decision we get a reward (which may be negative) that may depend
on the current world state.
More formally:
Deﬁnition 9.6 (Markov decision Processes). An MDP consists of an
unbounded set of identical time steps. Each time step i consists of:
1. A ﬁnite set of states of the world represented by the chance variable Si.
2. A ﬁnite set of actions represented by the decision variable Di.
3. A transition function P(Si+1 = s′ | Si = s, Di = a) specifying the proba-
bility that the next state is s′ when action a is taken in state s.
4. A reward function R(Si = s, Di = a) specifying the reward of taking action
a in state s, for each a ∈sp(Di) and s ∈sp(Si).

9.6 Decision Problems with Unbounded Time Horizons
327
5. An initial state s0 ∈S0.
The transition function and the reward function are the same for all time
steps.
In the deﬁnition above, we require that at any given point in time the world
state be represented by a single variable. This means that when specifying
the transition function we need to elicit |sp(S) × sp(S)| probabilities for each
decision. In order to make this elicitation task easier, you may exploit the
internal structure of the world and represent S as a Bayesian network.
Types of Strategies
A policy for a decision variable is in general a function that returns a decision
option for each possible conﬁguration of the variables previously observed and
decided on. In dealing with MDPs, however, the past is irrelevant in deter-
mining the optimal decision. More precisely, from the d-separation properties
of the MDP model in Figure 9.40 we see that the future is independent of the
past given the current state of the world Si (this is also called the Markov
property). Hence, instead of considering the past for decision Di, it is suﬃcient
to include only Si:
δDi : sp(Si) →sp(Di) .
In decision problems with a bounded time horizon we have previously
deﬁned an optimal strategy as a collection of optimal polices, one for each de-
cision. However, in dealing with unbounded time horizons the situation is a bit
diﬀerent. To illustrate the diﬀerence, consider again the model in Figure 9.24
approximating the ﬁshing in the North Sea decision problem (described in
Section 9.4.3). Strictly speaking, according to Deﬁnition 9.6, this model is not
an MDP, but by marginalizing out the unobserved variables we obtain the
equivalent MDP structure in Figure 9.42.
In looking for an optimal strategy for this model it is obvious that the
optimal policy for FV1, say, is not necessarily the same as the optimal policy
for FV5 (δF V1(T1) ̸= δF V5(T5)); even though the tests conducted at year 1 and
year 5 produce the same results, the decisions at these two points in time will
in general be diﬀerent. For example, at year 1 the optimal policy may set the
allowable catch to a conservative number to ensure that there will be enough
ﬁsh in the forthcoming years. On the other hand, at year 5 these concerns are
irrelevant, since the time horizon stops at that year, and the optimal policy
may set the allowable catch to a higher volume. To take another example,
in the robot navigation problem we look for a strategy for arriving at the
goal state from some starting position, say (2, 3). Suppose now that we have
a ﬁnite time horizon and require that the robot should reach the goal state
within 4 steps. With this constraint we do not have time to follow the route
left around the center state (2, 2) corresponding to the relative sequence of
positions (west, north, north, east, east). Instead we would have to pass either
(2, 2) or (3, 2), both of which incur a loss.

328
9 Graphical Languages for Speciﬁcation of Decision Problems
T1
T2
T3
T4
T5
FV1
FV2
FV3
FV4
FV5
U1
U2
U3
U4
U5
Fig. 9.42. The approximated model for ﬁshing in the North Sea obtained from the
original model in Figure 9.24 by marginalizing out the unobserved state variables
Vi.
In general, we can say that optimal decisions at the end will be diﬀerent
from the ones at the beginning. For these situations we say that the optimal
strategy is a nonstationary strategy.
Consider now the case in which we have an unbounded time horizon. At
any time step, the optimal decision can depend only on the current state and
what may happen in the future. If two time steps are in the same state, then
they also have the same possibilities in the future, and therefore the optimal
decision must be the same.
In the ﬁshing in the North Sea example with unbounded time horizon, the
optimal policy for deciding on the allowable catch at year 1 will not be any
diﬀerent from the policy at year 5. Similarly, in the robot example the optimal
policy at state (2, 3) does not depend on the point in time at which the robot
entered that state. That is, when there is no ﬁxed time horizon there is no
reason to change the optimal policy for a given state at diﬀerent points in
time. For the robot example, this allows us to represent the optimal policy as
in Figure 9.43.
More formally, an optimal strategy Δ consists of a set of identical policies,
which are functions of only the current state. Such a strategy is called sta-
tionary, since it can be completely described by a single policy. We will not
distinguish between a stationary strategy and a policy, and these terms will
also be used interchangeably.
Optimality in Markov Decision Process
When evaluating a strategy for a decision problem with an unbounded
time horizon, you might be tempted to simply consider the expected utili-
ties/rewards for each time step and sum them up over time. However, if the
process never stops, the sum may not be bounded, and you cannot compare
two strategies with an expected reward of +∞. This is not a problem for the

9.6 Decision Problems with Unbounded Time Horizons
329
1
1
2
2
3
3
→
→
↑
↑
↑
↑
←
←
Fig. 9.43. A strategy for the robot in the 3 × 3 grid world.
robot example, since it has a terminal state in which the robot will eventually
end up. However, in the ﬁshing example, any catching policy that at each
time step gives a positive reward will have an inﬁnite sum. An immediate
approach for handling this problem could be to specify some ﬁxed horizon k
so that the utility of a state sequence s0, s1, s2, . . . is simply the sum of the
rewards obtained at the ﬁrst k states. For notational convenience we shall in
this section assume that the reward is independent of the chosen action:
U(s0, s1, s2, . . .) = R(s0) + R(s1) + · · · + R(sk).
However, this raises the question of how to choose k, and, more importantly,
it has the eﬀect of postponing unpleasant decisions to after the horizon; in
the extreme case in which k = 0 we care only about the immediate reward.
The bounded ﬁshing model in Figure 9.42 illustrates this point. With a ﬁxed
time horizon, you will be very greedy, in the end not caring about the volume
of ﬁsh in later years.
Another approach is to weigh rewards in the immediate future higher than
rewards in the distant future. This can be done by introducing a discounting
factor γ, 0 ≤γ ≤1, so that the utility of a state sequence s0, s1, s2, . . . is the
accumulated discounted reward of each of the states:
U(s0, s1, s2, . . .) = R(s0) + γR(s1) + γ2R(s2) + · · · .
In the extreme case that γ = 0, the agent considers all future rewards as
being insigniﬁcant (corresponding to k = 0 above), and if γ = 1 then the
discounted utility corresponds to having additive rewards as in the robot
navigation problem. When γ < 1 the utility of an inﬁnite sequence is always
ﬁnite:
U(s0, s1, s2, . . .) =
∞

i=0
γiR(si) ≤
∞

i=0
γimaxR = maxR
1 −γ ,
(9.1)
where maxR is the maximum reward we can achieve in any state. A problem
domain in which the discounted reward model has been applied is economics;
here the discounting factor has been used, for example, to represent inﬂation or
an interest rate. Discounted rewards have also been used to model unbounded

330
9 Graphical Languages for Speciﬁcation of Decision Problems
decision problems, in which the decision process may terminate at any point
in time with probability (1 −γ). This could, for example, be used to model
that there is a risk of (1 −γ) that the robot will break down after it has
performed a move.
Some decision problems cannot naturally be modeled using discounted
rewards. The robot navigation problem with no terminal state is an example of
such a decision problem: the navigation task is not only to reach the goal state
but also to avoid the obstacles, and if we disregard the potential problem of the
robot breaking down, then there is no real justiﬁcation for using discounted
rewards (why should it be worse to hit an obstacle now than in the future?).
In this situation, the average reward may be a more appropriate model:
U(s0, s1, s2, . . .) = lim
N→∞
1
N
N−1

i=0
R(si) ≤maxR.
No matter whether we use discounted reward or average reward, we should
take into account that each strategy Δ corresponds to a set of diﬀerent state
sequences due to the actions being nondeterministic. For example, if the robot
starts at (1, 3), then a performed action sequence (north, north, east, east)
will result in the state sequence [(1, 2), (1, 1), (2, 1), (3, 1)] with probability
0.74 = 0.2401. Thus, we evaluate strategies based on their expected reward.
Let P(Si | Δ, s0) be the probability distribution for Si given that we start in
s0 and follow the strategy Δ. Then

Si
R(Si)P(Si | Δ, s0)
is the expected reward at step i, and γi 
Si R(Si)P(Si | Δ, s0) is the dis-
counted expected reward. The expected reward of Δ is deﬁned as
U ∗(s, Δ) = lim
N→∞
N

i=0
γi

Si
R(Si)P(Si | Δ, s0)

.
A standard notation for U ∗(s0, Δ) is also
E
 ∞

i=0
γiR(si)
 Δ, s

.
In Section 10.6 we shall return to the actual calculation of these expecta-
tions.
9.6.2 Partially Observable Markov Decision Processes
In many decision problems, the assumption that the environment is fully ob-
servable is not realistic. For example, the sensors used by a robot for position-
ing may be inaccurate, and they will therefore provide only a blurred picture

9.6 Decision Problems with Unbounded Time Horizons
331
of the state of the world. We call such an environment partially observable,
and in the Bayesian framework we can encode this uncertainty with a prob-
ability distribution over the possible world states. For bounded horizons, we
have actually encountered such a decision problem before, namely in the form
of the more exact model for the ﬁshing in the North Sea decision problem
speciﬁed in Figure 9.23.
In general, we can model that type of decision problem as a so-called par-
tially observable Markov decision process (POMDP) illustrated in Figure 9.44.
In the POMDP model the node Oi represents the observation at step i, and
the conditional probability distribution attached to this node encodes the
uncertainty associated with the observation; the information arc from Oi to
Di speciﬁes that only Oi is observed immediately before decision Di. More
formally, a POMDP consists of:
1. A set of states and actions as in the MDP framework.
2. A transition function and a reward function as speciﬁed for the MDP.
3. A set of possible observations represented by the chance variable Oi at
time step i.
4. An observation function P(Oi | Si, Di−1) that speciﬁes the probability of
the possible observations conditioned on the current state of the world
and the last decision.
Observe that as for the MDP we use a single variable to represent the
observation and the state at the ith time step. However, as for the MDP, we
can consider these variables as being the products of several variables, so that
both the transition function and the observation function can be speciﬁed
more compactly using a Bayesian network. To simplify the model, we will
stick to the single-variable representations.
Si−1
Si
Si+1
Di−1
Di
Di+1
Oi−1
Oi
Oi+1
Ri−1
Ri
Ri+1
Fig. 9.44. A snapshot of a model of a partially observable Markov decision process.
The state of the world Si is observed only indirectly through the observation node
Oi.

332
9 Graphical Languages for Speciﬁcation of Decision Problems
When the world is only partially observable we can no longer execute
an action based on the current state of the world. In fact, based on the d-
separation properties of the model in Figure 9.44, we see that when decision
Di is taken, all previous observations and decisions are d-connected to the
current and future state variables, hence the entire past is relevant when the
decision is taken. Another way of interpreting this situation is that all our
previous observations and decisions have an impact on our current beliefs
about the state of the world, and our ensuing action is based on these beliefs.
This also means that while for MDPs we speciﬁed a policy conditionally on
the observed state of the world, we should now specify a policy conditionally
on our belief of the state of the world. Since the actual state of the world is
not observed, our belief will in general not point to any speciﬁc state but will
rather be a probability distribution over the possible states. That is, our belief
can be expressed as a probability distribution P(Si | D1, O1, . . . , Di−1, Oi),
and an optimal policy for step i will therefore specify an action for each
possible probability distribution over Si. This implies that if P(Si|pasti) =
P(Sj|pastj), then the optimal decisions for Di and Dj are the same.
9.7 Summary
One Action
Decision D, utility functions U1, . . . , Un over domains X1, . . . , Xn, evidence e.
The expected utility is
EU(D | e) =

X1
U1(X1)P(X1 | D, e) + · · · +

Xn
Un(Xn)P(Xn | D, e),
and a state d maximizing EU(D | e) is chosen as an optimal action.
Instrumental Rationality
For an individual who acts according to a preference ordering satisfying the
rules below, there exists a utility function so that the individual maximizes
the expected utility.
1. Reﬂexivity. For any lottery A, A ⪰A.
2. Completeness. For any pair (A, B) of lotteries, A ⪰B or B ⪰A.
3. Transitivity. If A ⪰B and B ⪰C, then A ⪰C.
4. Preference increasing with probability. If A ⪰B then αA + (1 −α)B ⪰
βA + (1 −β)B if and only if α ≥β.
5. Continuity. If A ⪰B ⪰C then there exists α ∈[0, 1] such that B ∼
αA + (1 −α)C.
6. Independence. If C = αA+(1−α)B and A ∼D, then C ∼(αD+(1−α)B).

9.7 Summary
333
T
16.96
ok
12.96
14.5
RS
SP
16.2
25.5
ok
9.69
¬ok
6.51
10.5
RS
¬ok
4
14.5
RS
IS
20
27.5
ok
13.75
¬ok
6.25
12.5
RS
SP
16.27
26
ok
7.8
¬ok
8.47
11
RS
IS
11.1
24
ok
3.36
¬ok
7.74
9
RS
T
12.1
ok
7.46
10.5
RS
¬ok
4.64
10.5
RS
IS
16
23.5
ok
11.75
¬ok
4.25
8.5
RS
15
RS
IS
15.43
28
ok
2.8
¬ok
12.63
13
RS
T
14.03
¬ok
1.38
12.5
RS
ok
12.66
12.5
RS
SP
14.22
23.5
ok
8.93
¬ok
5.29
8.5
RS
SP
14
24
ok
8
¬ok
6
9
RS
Fig. 9.45. An example of a decision tree. The probabilities may be taken from a
Bayesian network. The bold links indicate an optimal strategy.

334
9 Graphical Languages for Speciﬁcation of Decision Problems
Decision Trees
An example is shown in Figure 9.45.
To calculate an optimal strategy and the maximum expected utility for the
subtree rooted at node X, do:
1. If X is a utility node, then return U(X).
2. If X is a chance node, then return
EU(X) =

x∈sp(X)
P(X = x | past(X)) EU(N(X = x)).
3. If X is a decision node, then return
EU(X) =
max
x∈sp(X) EU(N(X = x)),
and mark the arc labeled
x′ = arg max
x∈sp(X) EU(N(X = x)).
Inﬂuence Diagrams
An inﬂuence diagram consists of a directed acyclic graph over chance nodes,
decision nodes, and utility nodes with the following structural properties:
−there is a directed path comprising all decision nodes;
−the utility nodes have no children.
For the quantitative speciﬁcation, we require that:
−the decision nodes and the chance nodes have a ﬁnite set of mutually
exclusive states;
−the utility nodes have no states;
−to each chance node A there be attached a conditional probability table
P(A | pa(A));
−to each utility node V there be attached a real-valued function over pa(V ).
Figure 9.46 gives an example of the structural part of an inﬂuence diagram.
A policy for decision Di is a mapping δi that for any conﬁguration of the past
of Di yields a decision for Di. That is
δi(I0, D1, . . . , Di−1, Ii−1) ∈sp(Di) .
A strategy for an inﬂuence diagram is a set of policies, one for each decision. A
solution to an inﬂuence diagram is a strategy maximizing the expected utility.
Methods for determining optimal strategies from inﬂuence diagrams are
given in Chapter 10.

9.7 Summary
335
L
D1
V1
D2
D3
V3
D4
V4
A
B
D
C
E
F
H
G
V2
K
J
I
Fig. 9.46. An example of the structure of an inﬂuence diagram. We have I0 =
{B}, I1 = {E, F}, I2 is empty, I3 = {G}, I4 = {A, C, D, H, I, J, K, L}.
Asymmetric Decision Problems
A decision problem is said to be symmetric if:
•
in all of its decision tree representations, the number of scenarios is the
same as the cardinality of the Cartesian product of the state spaces of all
chance and decision variables, and
•
in at least one decision tree representation, the sequence of chance and
decision variables is the same in all scenarios.
There are three types of asymmetry:
Functional asymmetry: The possible outcomes or decision options of a vari-
able may vary depending on the past.
Structural asymmetry: The very occurrence of an observation or a decision
depends on the past.
Order asymmetry: The ordering of the decisions and observations is not set-
tled at the time the model is speciﬁed.
Unconstrained Inﬂuence Diagrams
Unconstrained inﬂuence diagrams are used to model order asymmetry. Com-
pared to inﬂuence diagrams there need not be a total ordering of the decisions,
and the chance variables are partitioned into two sets: observable chance vari-
ables and nonobservable chance variables. An observable chance variable is

336
9 Graphical Languages for Speciﬁcation of Decision Problems
released (for observation) when all its antecedent decision variables have been
decided on.
Solving an unconstrained inﬂuence diagram involves ﬁnding the next ac-
tion as well as ﬁnding an optimal policy if the next action is a decision (that
is, ﬁnding the conditional sequence of action and observations maximizing the
expected utility). The solution is speciﬁed in terms of an S-DAG:
An S-DAG is a directed acyclic graph G. The nodes are labeled with variables
from DU ∪OU such that each maximal directed path in G represents an ad-
missible ordering of DU ∪OU.
A step policy for a node N in an S-DAG G is a function
σ : sp(hst(N)) →ch(N).
A step strategy for U is a pair (G, S), where G is an S-DAG for U and S is a
set of step policies, one for each node in G (except for Sink). A policy for N
is a function
δ : sp(past(N)) →ch(N).
A strategy for U is a step strategy together with a policy for each node.
Decision Problems with an Unbounded Time Horizon
An MDP consists of an unbounded set of identical time steps. Each time step
i consists of:
1. A ﬁnite set of states of the world (represented by the chance variable Si).
2. A ﬁnite set of actions (represented by the decision variable Di).
3. A transition function P(Si+1 = s′ | Si = s, Di = a) specifying the proba-
bility that the next state is s′ when taking action a in state s.
4. A reward function R(Si = s, Di = a) specifying the reward of taking
action a in state s, for each a ∈sp(Di) and s ∈sp(Si).
5. An initial state s0 ∈S0.
The transition function and the reward function are the same for all time
steps.
There are three standard ways to ensure that the utility of an unbounded
state sequence s0, s1, s2, . . . is bounded:
Fixed time horizon: The sum of the rewards obtained at the ﬁrst k states:
U(s0, s1, s2, . . .) = R(s0) + R(s1) + · · · + R(sk).
Discounted reward: The accumulated discounted reward of each of the states:
U(s0, s1, s2, . . .) = R(s0) + γR(s1) + γ2R(s2) + · · · ,
where 0 ≥γ < 1.

9.9 Exercises
337
Average expected reward: The accumulated average reward at each of the
states:
U(s0, s1, s2, . . .) = lim
N→∞
1
N
N−1

i=0
R(si).
A POMDP consists of:
1. A set of states and actions as in the MDP framework.
2. A transition function and a reward function as speciﬁed for the MDP.
3. A set of possible observations (represented by the chance variable Oi at
time step i).
4. An observation function P(Oi | Si, Di−1) that speciﬁes the probability of
the possible observations conditioned on the current state of the world
and the last decision.
9.8 Bibliographical Notes
Decision theory has a long history but achieved a breakthrough in the work
of von Neumann and Morgenstern (1944), who laid down the axioms for in-
strumental rationality. Decision trees were introduced by Raiﬀa and Schlaifer
(1961). Inﬂuence diagrams were proposed by Howard and Matheson (1981),
and were adapted to allow for additive decompositions of utility functions
in (Tatman and Shachter, 1990). Unconstrained inﬂuence diagrams were in-
troduced in (Jensen and Vomlelova, 2002), and sequential inﬂuence diagrams
in (Jensen et al., 2006). The latter is a fusion of the valuation networks of
Shenoy (1996) and the asymmetric inﬂuence diagrams of Nielsen and Jensen
(2003a). The study of Markov decision processes can be traced back at least to
Howard (1960). A good starting point for further reading is (Puterman, 1994).
Partially observed Markov decision processes originate with Drake (1962) and
˚Astr¨om (1965). The reactor problem, as presented here, is due to Covaliu and
Oliver (1995).
9.9 Exercises
Exercise 9.1. Consider the management of eﬀort example in Section 9.2.
(i) Let the marks be 0, 5, 6, 8, 9, 10. What is the optimal decision if the nu-
merical values are used as utilities?
(ii) Consider the approach in which the marks are given subjective utilities.
Show that action Gd can be optimal only if the mark 0 is given higher
utility than mark 3.
Exercise 9.2. Prove that if U is a utility function for a decision maker and if
a (a > 0) and b are real numbers, then aU +b is an equivalent utility function.

338
9 Graphical Languages for Speciﬁcation of Decision Problems
Exercise 9.3. E Extend the model from Exercise 3.14 to a model for folding
or calling.
Exercise 9.4. E Extend Exercise 3.18 with the following:
In golf, the task is to use as few strokes as possible at each hole. I am
driving at a hole 260 m long. If the drive is 265 m, I will on average
use 1.8 strokes to ﬁnish the hole. If the drive is 240 m, on average
2 extra strokes are needed; 220 m requires 2.5 extra strokes; 200 m
requires 2.7; 180 m 2.9 extra strokes; 160 m 3.1; 145 m 3.3; a drive of
290 m will carry the ball into a sand trap, requiring 3.5 extra strokes;
if the drive is misshit, the ball will drop into a lake, and it will require
4.5 extra strokes to ﬁnish the hole.
Construct a system that helps me decide whether to use the 3-wood or the
driver in the drive.
Exercise 9.5. E Consider the stud farm example from Section 3.2.2. Extend
the model to be an aid for deciding for each horse whether it should be taken
out of breeding. Table 9.6 gives the utilities.
Carrier Pure
Out
−10 −10
In
−40
100
Carrier Pure
Out
−3
−3
In
−10
40
Stallions
Mares
Table 9.6. Tables for Exercise 9.5.
Exercise 9.6. Let the hypothesis variable H have n states. Introduce an ac-
tion variable A with the same states as H; let the utility table be as follows:
U(h, a) =

1 if h and a are the same,
0 otherwise.
Show that a value function based on U corresponds to selecting a hypoth-
esis state of highest probability.
Exercise 9.7. Construct a decision tree for the mildew decision problem in
Section 9.1.2. How many numbers would you need to specify to render it
complete?
Exercise 9.8. Solve the decision tree in Figure 9.47.
Exercise 9.9. Consider an altered version of the poker decision problem in
which each player is now allowed three rounds of changing hands. What would
an inﬂuence diagram look like for this altered problem? What is the past for
each decision variable in the diagram?

9.9 Exercises
339
0.3
2
0.6
0
2
1
3
2
0.2
0.5
0.5
1
0.6
0.3
0.4
0.3
0
3
0.45
0.55
5
2
0.1
0.9
0.9
0.1
1
3
3
a1
a2
a3
0.7
b1
b2
0.4
0.8
c1
c2
0.4
d1
d2
e1
e2
0
4
A
B
C
D
E
4
Fig. 9.47. Figure for Exercise 9.8.
Exercise 9.10. What is the partial temporal ordering of observations and
decisions in the inﬂuence diagrams in Figures 9.23 and 9.24?
Exercise 9.11. E (The oil wildcatter’s problem)
An oil wildcatter must decide whether to drill or not to drill. The cost
of drilling is $70,000. If he decides to drill, the hole may be soaking
(with a return of $270,000), wet (with a return of $120,000), or dry
(with a return of $0). The prior probabilities for soaking, wet, and
dry are (0.2, 0.3, 0.5). At the cost of $10,000, the oil wildcatter could

340
9 Graphical Languages for Speciﬁcation of Decision Problems
decide to take seismic soundings of the geological structure at the site.
The speciﬁcs of the test are given in Table 9.7.
T \ S dr wt so
n
0.6 0.3 0.1
o
0.3 0.4 0.4
c
0.1 0.3 0.5
P(T est| Structure)
Table 9.7. Table for Exercise 9.11. The states n, o, and c are the outcomes of the
test.
(i) Solve the problem with a decision tree.
(ii) Solve the problem with an inﬂuence diagram.
Exercise 9.12. (The used car buyer’s problem)
Joe is considering buying a used car from a dealer for $1,000. The
market price of similar cars with no defects is $1,100. Joe is uncertain
whether the particular car he is considering is a “peach” or a “lemon.”
Of the ten major subsystems in the car, a peach has a serious defect
in only one subsystem, whereas a lemon has a serious defect in six
subsystems. The probability that the used car under consideration is
a lemon is 0.2. The cost of repairing one defect is $40, and the cost of
repairing six defects is $200.
For an additional $60, Joe can buy the car from the dealer with an
“antilemon guarantee.” The antilemon guarantee will normally pay for
50% of the repair cost, but if the car is a lemon, then the guarantee
will pay 100% of the repair cost.
Before buying the car, Joe has the option of having the car examined
by a mechanic for an hour. In this period, the mechanic oﬀers three
alternatives t1, t2, t3 as follows:
t1: test the steering subsystem alone at a cost of $9,
t2: test the fuel and electrical subsystems for a total cost of $13,
t3: do a two-test sequence in which Joe can authorize a second test
after the result of the ﬁrst test is known. In this alternative, the
mechanic will ﬁrst test the transmission subsystem at a cost of $10
and report the results to Joe. If Joe approves, the mechanic will
then proceed to test the diﬀerential subsystem at an additional
cost of $4.
All tests are guaranteed to ﬁnd a defect in the subsystem if a defect
exists. We assume that Joe’s utility for proﬁt is linear in dollars.

9.9 Exercises
341
(i) Solve the problem with a decision tree.
(ii) Consider how to represent the problem as an inﬂuence diagram (you may
add dummy states and variables as you wish).
Exercise 9.13. Draw an inﬂuence diagram for the decision problem in Sec-
tion 9.1.2.
Exercise 9.14. Solve the decision tree in Figures 9.8 and 9.9 (the probabili-
ties can be taken from the model in Figure 9.10).
Exercise 9.15. Complete the reduced decision tree from Figure 9.16 and
solve it.
Exercise 9.16. E Solve Exercise 3.16 as a decision problem.
Exercise 9.17. E
Solve the example in Section 11.1.1 as an inﬂuence dia-
gram.
Exercise 9.18. E Extend the poker model from Exercise 9.3 to the inﬂuence
diagram in Figure 9.21.
Exercise 9.19. E
Represent the Car Start Problem in Section 9.3.1 as an
inﬂuence diagram. (What are the decision options at each step?)
Exercise 9.20. Unfold the sequential inﬂuence diagram in Figure 9.38 with
the following probabilities: A conventional reactor (C) has probability 0.980
of being successful (cs), and a probability 0.020 of a failure (cf). An advanced
reactor (A) has probability 0.660 of being successful (as), probability 0.244 of
a limited accident (al), and probability 0.096 of a major accident (am).
Exercise 9.21. Consider the Dating Problem in Example 9.6. What are the
asymmetries in the decision problem? Which of them are functional asymme-
tries/structural asymmetries/order asymmetries?
Exercise 9.22. Construct an S-DAG for the UID in Figure 9.48.
Exercise 9.23. Consider the two-player turn-taking game of tic-tac-toe in
which each player has three game pieces, and the objective is to place all your
pieces in a straight line on a 3 × 3 board. The players take turns placing a
piece in one of the free slots on the board, and when a player has no more
pieces oﬀthe board, he must take one of his pieces already on the board and
place it somewhere else. Formalize the game as a Markov decision process,
seen from the point of view of one of the players.
Exercise 9.24. Consider the example of the possibly infected milk from a
single cow introduced in Sections 3.1.1 and 3.2.1. Add to that the daily deci-
sion of throwing the milk out or pouring it into the tank, and associate the
utility of

342
9 Graphical Languages for Speciﬁcation of Decision Problems
D1
D2
D3
A
B
C
D
E
F
U1
U2
U3
Fig. 9.48. A UID.
•
0 with pouring infected milk into the tank,
•
98 with throwing the milk out, and
•
100 with pouring noninfected milk into the tank.
Formalize the setting as a POMDP.

10
Solution Methods for Decision Graphs
In Chapter 9 we presented graphical languages for modeling decision prob-
lems. The languages ease the burden of specifying the problem and transfer
the complexity of the problem to the computer. For problems with a ﬁnite time
horizon, the computer may fold out the speciﬁcation to a decision tree and
determine an optimal strategy by averaging out and folding back as described
in Section 9.3.3. However, the calculations may be intractable, and in this
chapter we present alternative methods exploiting symmetries in the decision
problem. Sections 10.1–10.3 are devoted to solution methods for inﬂuence dia-
grams. Section 10.4 presents a method for solving unconstrained inﬂuence di-
agrams. In Section 10.5 we consider decision theoretic troubleshooting, which
has next to no temporal ordering, and for which the decision trees tend to be
intractably large. In Section 10.6 we present two methods for solving MDPs,
and a method for solving POMDPs is indicated. The last section presents
LIMIDs, which is a way of approximating inﬂuence diagrams by limiting the
memory of the decision maker.
10.1 Solutions to Inﬂuence Diagrams
An inﬂuence diagram has three types of nodes: chance nodes, decision nodes,
and utility nodes. The set of chance nodes is denoted by UC, the set of decision
nodes is denoted by UD and the set of utility nodes is denoted by UV . The
universe is U = UC ∪UD. We shall also refer to the members of U as the
variables of the inﬂuence diagram.
The decision nodes have a temporal order, D1, . . . , Dn, and the chance
nodes are partitioned according to when they are observed: I0 is the set of
chance nodes observed prior to any decision, . . . , Ii is the set of chance nodes
observed after Di is taken and before the decision Di+1 is taken. Finally, In
is the set of chance nodes never observed or observed after the last decision.
That is, we have a partial temporal ordering I0 ≺D1 ≺I1 ≺. . . ≺Dn ≺In.

344
10 Solution Methods for Decision Graphs
Recall that an inﬂuence diagram is constructed so that if A ≺Di, then there
is a directed path from A to Di.
We shall in this chapter use the inﬂuence diagram DI in Figure 10.1 as a
standard example, where I0 = ∅, I1 = {T }, and I2 = {A, B, C}. In order not
to make things unnecessarily complicated, all variables in DI are binary.
D1
D2
A
B
C
T
V1
V2
Fig. 10.1. The example inﬂuence diagram, DI.
As in Bayesian networks, the graphical representation of inﬂuence dia-
grams supports an analysis of conditional independence. However, d-separation
for inﬂuence diagrams is performed slightly diﬀerently from the way it is done
for Bayesian networks: ignore the utility nodes, and since the links into deci-
sion nodes encode only information precedence, they shall also be ignored.
For the DI example, we can perform d-separation analysis on Figure 10.1.
We get, for example, that C is d-separated from T given B (note that you
need not condition on D2, since the link from T to D2 is ignored). Also, A
and T are d-separated from D2. This means that if I perform an action from
D2, then this action has no impact on T . Note that this is diﬀerent from: if
I am told what action from D2 was performed, what can I infer about T ? If,
for example, I know that the decision maker maximizes expected utilities, I
may be able to infer a great deal about T .
Decision variables play a diﬀerent role from that played by chance vari-
ables. For chance variables you ask the question, may information about node
A change my belief about node B? For decision variables the question is, may
an action from D have consequences for node B? Although the two concepts
are diﬀerent, they are in the case of inﬂuence diagrams not in conﬂict. In
general, eﬀects of decisions cannot “go back in time”:
Proposition 10.1. Let A ∈Ii and let Dj be a decision variable with i < j.
Then
(i) A and Dj are d-separated and hence
P(A | Dj) = P(A).

10.1 Solutions to Inﬂuence Diagrams
345
(ii) Let W be any set of variables prior to Dj in the temporal ordering. Then
A and Dj are d-separated given W and hence
P(A | Dj, W) = P(A | W).
Proof.
(i) Since Dj has no parents, any impact from Dj must follow the direction of
a link from Dj. The only way the impact can start going in the opposite
direction from that of a link is if it meets a converging connection at
a chance variable B, and then it can do so only if either B or one of
its children C has received evidence. Since Dj is the only variable we
condition on, this cannot happen. Hence if Dj and A are not d-separated,
there must be a directed path from Dj to A. Since A ≺Dj in the temporal
ordering, there is a directed path from A to Dj, and since the graph is
acyclic, there cannot be a directed path from Dj to A.
(ii) We argue in the same way as for (i). By following directions of links from
Dj, we can only start going opposite to the direction by meeting evidence.
Since all evidence is prior in the temporal ordering, we know from (i) that
we cannot meet it.
⊓⊔
10.1.1 The Chain Rule for Inﬂuence Diagrams
For Bayesian networks we have that P(U) is the product of all probability
potentials attached to the variables in the network. For inﬂuence diagrams we
have a similar theorem. Again, decision variables act diﬀerently from chance
variables. since a decision variable eventually will come under my control, it
requires no prior probabilities. Also, it has no meaning to attach a probability
distribution to a chance variable A eﬀected by a decision variable D, unless
a decision has been taken and the action performed. So in Figure 10.1 it has
no meaning to consider P(A) or P(A, D). What is meaningful is P(A | d) for
all d ∈D, and we may lump the probabilities for all decisions of D together
in the expression P(A | D).
Theorem 10.1 (The chain rule for inﬂuence diagrams). Let ID be an
inﬂuence diagram with universe U = UC ∪UD. Then
P(UC | UD) =
	
X∈UC
P(X | pa(X)).
Proof.
Let us ﬁrst look at the inﬂuence diagram DI. From the fundamental
rule we have

346
10 Solution Methods for Decision Graphs
P(C, T, B, A | D1, D2) = P(C | T, B, A, D1, D2)P(T, B, A | D1, D2)
= P(C | T, B, A, D1, D2)P(T | B, A, D1, D2)
× P(B | A, D1, D2)P(A | D1, D2).
(10.1)
Since C is d-separated from A, T , and D1 given B and D2, we have
P(C | T, B, A, D1, D2) = P(C | B, D2).
We also have
P(T | B, A, D1, D2) = P(T | B, A),
P(B | A, D1, D2) = P(B | A),
P(A | D1, D2) = P(A | D1).
Substituting in equation (10.1) yields
P(C, B, T, A | D1, D2) = P(C | B, D2)P(T | B, A)P(B | A)P(A | D1, ),
which is the product of the probability potentials for DI.
A general proof can follow another line of reasoning. Let d be a particular
conﬁguration of decisions. By inserting them in the inﬂuence diagram ID,
you get a Bayesian network representing P(UC | d), the joint probability of
UC, under the condition that the decisions d are taken. Using the chain rule
for Bayesian networks, you infer that P(UC | d) is the product of all probabil-
ity potentials attached to the decision variables instantiated to d. Since this
holds for all instantiations of UD, you get the result.
⊓⊔
10.1.2 Strategies and Expected Utilities
To solve an inﬂuence diagram, you may unfold it into a decision tree and solve
it. In Figure 10.2 we have unfolded DI from Figure 10.1.
When solving the decision tree in Figure 10.2, we start at the leaves and
work toward the root (see Section 9.3.3). Consider the path (d1
1, t1). We wish
to compute the expected utility of performing action d2
1 given (d1
1, t1). We
have
EU(d2
1 | d1
1, t1) =

A,C
P(A, C | d1
1, t1, d2
1)(V1(A, d2
1) + V2(C)).
For the action d2
2, we have
EU(d2
2 | d1
1, t1) =

A,C
P(A, C | d1
1, t1, d2
2)(V1(A, d2
2) + V2(C)).
Taken together, we write

10.1 Solutions to Inﬂuence Diagrams
347
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
D1
V1 + V2
V1 + V2
V1 + V2
V1 + V2
D2
D2
T
d2
1
t1
t2
d2
2
d2
2
a1c2
a2c1
a1c1
a2c2
A × C
A × C
A × C
A × C
d1
2
d2
1
D2
D2
T
t1
t2
d2
1
d2
2
a1c2
d2
2
V1 + V2
V1 + V2
V1 + V2
V1 + V2
a2c1
a1c1
d1
1
a2c2
A × C
A × C
A × C
A × C
d2
1
Fig. 10.2. DI from Figure 10.1 unfolded into a decision tree. Note that to reduce
the size of the decision tree the last chance node in each path is deﬁned as the
Cartesian product of A and C, and that the utilities in the leaves are the sums of
V1 and V2.

348
10 Solution Methods for Decision Graphs
EU(D2 | d1
1, t1) =

A,C
P(A, C | d1
1, t1, D2)(V1(A, D2) + V2(C)).
We choose the action of maximal expected utility, and we get a decision
rule for D2 with D1 = d1
1 and T = t1
δ2(d1
1, t1) = arg max
D2 EU(D2 | d1
1, t1).
If there are several decisions yielding the maximum, either of them will do.
The maximal expected utility from D2 given (d1
1, t1) is
ρ2(d1
1, t1) = max
D2

A,C
P(A, C | d1
1, t1, D2)(V1(A, D2) + V2(C)).
Generalizing these two formulas to any path over D1, T , we get a policy
for D2
δ2(D1, T ) = arg max
D2 EU(D2 | D1, T )
= arg max
D2

A,C
P(A, C | D1, T, D2)(V1(A, D2) + V2(C)),
and a new utility function
ρ2(D1, T ) = max
D2

A,C
P(A, C | D1, T, D2)(V1(A, D2) + V2(C)),
(10.2)
which gives the expected utilities when we know the values of (D1, T ). The
decision tree in Figure 10.2 can now be reduced to the one in Figure 10.3.
T
D1
t1
t2
t1
t2
d1
1
d2
1
ρ2(d1
1, t1)
ρ2(d1
1, t2)
ρ2(d2
1, t1)
ρ2(d2
1, t2)
T
Fig. 10.3. The decision tree from Figure 10.2 with D2 replaced by a utility function
reﬂecting that the policy δ2 for D2 is followed.

10.1 Solutions to Inﬂuence Diagrams
349
Next, look at the decision D1 as in Figure 10.3. If we take the action d1
1,
we get the expected utility
EU(d1
1) = P(t1 | d1
1)ρ2(d1
1, t1) + P(t2 | d1
1)ρ2(d1
1, t2),
which can also be written
EU(D1) =

T
P(T | D1)ρ2(D1, T ).
The policy for D1 is
δ1 = arg max
D1

T
P(T | D1)ρ2(D1, T ),
and the expected utility of performing optimal decisions is
ρ1 = max
D1

T
P(T | D1)ρ2(D1, T ).
(10.3)
So far we have written various expressions without really connecting them
to the potentials from the inﬂuence diagram. In principle, all probabilities in
the expressions can be calculated from the inﬂuence diagram by inserting and
propagating evidence. However, by taking a closer look at equation (10.3) we
can make a much tighter connection between the speciﬁcation of the inﬂuence
diagram and its solution: By combining equation (10.2) and equation (10.3),
we get
ρ1 = max
D1

T
P(T | D1) max
D2

A,C
P(A, C | D1, T, D2)(V1(A, D2) + V2(C))
= max
D1

T
max
D2

A,C
P(T | D1)P(A, C | D1, T, D2)(V1(A, D2) + V2(C))
= max
D1

T
max
D2

A,C
P(T | D1, D2)P(A, C | D1, T, D2)(V1(A, D2) + V2(C))
= max
D1

T
max
D2

A,C
P(A, C, T | D1, D2)(V1(A, D2) + V2(C))
= max
D1

T
max
D2

A,B,C
P(A, B, C, T | D1, D2)(V1(A, D2) + V2(C))
= max
D1

T
max
D2

A,B,C
P(UC | UD)(V1(A, D2) + V2(C)).
The formula for δ1 is
δ1 = arg max
D1

T
max
D2

A,B,C
P(UC | UD)(V1(A, D2) + V2(C)).
For the policy δ2 we have

350
10 Solution Methods for Decision Graphs
δ2(D1, T ) = arg max
D2

A,C
P(A, C | D1, T, D2)(V1(A, D2) + V2(C)).
We can multiply inside “arg maxD2” with anything not varying with D2:
δ2(D1, T ) = arg max
D2 P(T | D1)

A,C
P(A, C | D1, T, D2)(V1(A, D2) + V2(C))
= arg max
D2

A,C
P(T | D1, D2)P(A, C | D1, T, D2)(V1(A, D2) + V2(C))
= arg max
D2

A,C
P(A, T, C | D1, D2)(V1(A, D2) + V2(C))
= arg max
D2

A,B,C
P(UC | UD)(V1(A, D2) + V2(C)),
and the similarity with the formula for δ1 is transparent. Similar calculations
yield for ρ2,
ρ2(D1, T ) =
1
P(T | D1) max
D2

A,B,C
P(UC | UD)(V1(A, D2) + V2(C)).
Theorem 10.2. Let ID be an inﬂuence diagram over U = UC ∪UD and
UV = {Vi}. Let the temporal order of the variables be described as I0 ≺D1 ≺
I1 ≺· · · ≺Dn ≺In and let V = 
i Vi. Then:
(i) An optimal policy for Di is
δi(I0, D1, . . . , Ii−1) = arg max
Di

Ii
max
Di+1 · · · max
Dn

In
P(UC | UD)V.
(ii)The expected utility from following the policy δi (and acting optimally in
the future) is
ρi(I0, D1, . . . , Ii−1) =
1
P(I0, . . . , Ii−1 | D1, . . . , Di−1)
max
Di

Ii
max
Di+1 · · · max
Dn

In
P(UC | UD)V,
and the strategy for ID consisting of an optimal policy for each decision yields
the maximum expected utility:
MEU(ID) =

I0
max
D1

I1
max
D2 · · · max
Dn

In
P(UC | UD)V.
Proof.
We start with the last decision Dn. We have for the expected utility
given the past

10.1 Solutions to Inﬂuence Diagrams
351
EU(Dn | I0, D1, . . . , Dn−1, In−1)
=

In
P(In | I0, D1, . . . , Dn−1, In−1, Dn)V
=

In
1
P(I0, . . . , In−1 | D1, . . . , Dn)P(In, I0, . . . , In−1 | D1, . . . , Dn)V
=
1
P(I0, . . . , In−1 | D1, . . . , Dn−1)

In
P(UC | UD)V.
In the last expression we used that P(I0, . . . , In−1 | D1, . . . , Dn) = P(I0, . . . ,
In−1 | D1, . . . , Dn−1). We now get
ρn(I0,D1, . . . , In−1)
=
1
P(I0, . . . , Ii−1 | D1, . . . , Dn−1) max
Dn

In
P(UC | UD)V,
and
δn(I0,D1, . . . , In−1)
= arg max
Dn
1
P(I0, . . . , In−1 | D1, . . . , Dn−1)

In
P(UC | UD)V
= arg max
Dn

In
P(UC | UD)V.
Next, assume the theorem to hold for i + 1, . . . , n and consider decision
Di. We have
EU(Di | I0, D1, . . . , Dn−1, Ii−1)
=

Ii
P(Ii | I0, D1, . . . , Di−1, Ii−1, Di)ρi+1(I0, D1, . . . , Ii)
=

Ii
1
P(I0, . . . , Ii−1 | D1, . . . , Di)P(Ii, I0, . . . , Ii−1 | D1, . . . , Di)
1
P(I0, . . . , Ii | D1, . . . , Di) max
Di+1

Ii+1
· · · max
Dn

In
P(UC | UD)V
=

Ii
1
P(I0, . . . , Ii−1 | D1, . . . , Di) max
Di+1

Ii+1
· · · max
Dn

In
P(UC | UD)V
=
1
P(I0, . . . , Ii−1 | D1, . . . , Di−1)

Ii
max
Di+1

Ii+1
· · · max
Dn

In
P(UC | UD)V,
and we get the formulas in (i) and (ii).
Since we have repeatedly determined a policy maximizing the expected
utility regardless of the past, no other set of policies can give a higher ex-
pected utility. The formula for MEU(ID) is the formula from (ii) for ρ0. It is
calculated by taking ρ1(D1), multiplying by P(I0), and marginalizing I0 out:

352
10 Solution Methods for Decision Graphs
MEU(ID) =

I0
P(I0)ρ1(I0)
=

Io
P(I0)
1
P(I0) max
D1

I1
max
D2 · · · max
Dn

In
P(UC | UD)V
=

I0
max
D1

I1
max
D2 · · · max
Dn

In
P(UC | UD)V.
⊓⊔
Since P(UC | UD) is the product of all probability distributions attached
to ID, we have a method for calculating ρi as well as δi. The method speciﬁes
that you may start with the product of all probability potentials and then
marginalize out in reverse temporal order where chance variables are sum-
marginalized and decision variables are max-marginalized. Each time an Ii is
marginalized out, the result is used to determine a policy for Di.
The method has the same problem as the method for Bayesian networks,
namely that P(UC | UD) may be an intractably large table, and we therefore
have to look for methods that reduce the size of the domains to deal with. We
shall consider this task in detail in Section 10.2.
10.1.3 An Example
The inﬂuence diagram DI in Figure 10.1 has the potentials in Table 10.1.
Using Theorem 10.2 we get δ2(D1, T ) and ρ2(D1, T ) as listed in Table 10.2.
A \ D1 d1
1 d1
2
y 0.2 0.8
n 0.8 0.2
B \ A y
n
y 0.8 0.2
n 0.2 0.8
P(A | D1)
P(B | A)
A \ B
y
n
y (0.9, 0.1) (0.5, 0.5)
n (0.5, 0.5) (0.1, 0.9)
B \ D2
d2
1
d2
2
y (0.9, 0.1) (0.5, 0.5)
n (0.5, 0.5) (0.9, 0.1)
P(T | A, B)
P(C | D2, B)
A \ D2 d2
1 d2
2
y 3
0
n 0
2
V (A, D2)
V2(C) = (10, 0)
Table 10.1. Potentials for DI.

10.2 Variable Elimination
353
T \ D1 d1
1 d1
2
y d2
1 d2
1
n d2
2 d2
2
T \ D1
d1
1
d1
2
y 9.51 11.29
n 10.34 8.97
δ2(D1, T )
ρ1(D1, T )
Table 10.2. δ2(D1, T ) and ρ2(D1, T ) for DI.
Finally, we get δ1 = d1
2 and MEU(DI) = 10.58. Note that δ2(D1, T ) has
the property that the state of T alone determines the decision to choose, hence
we can remove D1 from the domain of δ2. This phenomenon can sometimes
be determined from the d-separation properties of the inﬂuence diagram (see
Figure 10.4 and Section 11.2), and we say that this part of the past is not
required for the decision in question. For DI it cannot be deduced from the
structure; the potentials happened to cause it.
A
B
C
D1
D2
V1
V2
Fig. 10.4. An inﬂuence diagram in which D1 is not required for D2.
10.2 Variable Elimination
The method for solving inﬂuence diagrams has many similarities with the junc-
tion tree propagation algorithm for Bayesian networks: you start oﬀwith a set
of potentials, and you eliminate one variable at a time. There are, however,
diﬀerences. First of all, the elimination order is constrained by the temporal
order. Since max-marginalization and sum-marginalization do not commute,
you have to do it in an order whereby you ﬁrst sum-marginalize In, then
max-marginalize Dn, sum-marginalize Ii−1, etc. This type of elimination or-
der is called a strong elimination order. Furthermore, you have two types of
potentials to deal with. Also, you need to eliminate in only one direction; this
corresponds to CollectEvidence.
We shall ﬁrst analyze the calculations in eliminating a variable. Let Φ be
a set of probability potentials and Ψ a set of utility potentials. The two sets
represent the expression 
 Φ( Ψ), the product of all probability potentials
multiplied by the sum of all utility potentials.

354
10 Solution Methods for Decision Graphs
Now assume that we shall calculate 
X

 Φ( Ψ) for some chance vari-
able X. To do that we partition Φ into two sets: ΦX, which is the set of
potentials with X in the domain, and Φ∗= Φ \ ΦX. The set Ψ is in the
same way divided up in the two sets ΨX and Ψ ∗. Set φX = 
X

 ΦX and
ψX = 
X

 ΦX( ΨX). Using the distributive law we get

X
	
Φ

Ψ

=
	
Φ∗
X
	
ΦX

Ψ ∗+

ΨX

=
	
Φ∗

Ψ ∗ 
X
	
ΦX

+

X
	
ΦX

ΨX

=
	
Φ∗
Ψ ∗
φX + ψX

=
	
Φ∗φX

Ψ ∗+ ψX
φX

.
We see that the result of eliminating the chance variable X is that ΦX is
removed from the set of probability potentials and substituted with φX. For
the set of utility potentials, ΨX is removed and ψX
φX is added.
Let D be a decision variable. We again divide the potentials into ΦD, Φ∗
and ΨD, Ψ ∗. Since all variables coming after D in the temporal ordering have
been eliminated when we are about to eliminate D, it follows that 
 ΦD does
not vary with D (See Exercise 10.3). So taking maxD of 
 ΦD is an almost
empty operation; it only removes D from the domain. Using the distributive
law for max, setting φD = maxD

 ΦD and ψD = maxD

 ΦD( ΨD), and
exploiting that 
 ΦD( Ψ ∗) does not vary with D, we get
max
D
	
Φ

Ψ

=
	
Φ∗max
D
	
ΦD

Ψ ∗+

ΨD

=
	
Φ∗
max
D
	
ΦD

Ψ ∗
+ max
D
	
ΦD

ΨD

=
	
Φ∗
φD

Ψ ∗
+ ψD

=
	
Φ∗φD

Ψ ∗+ ψD
φD

.
The result is similar to the result for sum-elimination. To sum up:
Algorithm 10.1 [Variable elimination for inﬂuence diagrams] You
work with two sets of potentials: Φ, the set of probability potentials; Ψ, the
set of utility potentials. When a variable X is eliminated, the potential sets
are modiﬁed in the following way:
1.
ΦX : = {φ ∈Φ | X ∈dom (φ)};
ψX : = {ψ ∈Ψ | X ∈dom (ψ)}.

10.2 Variable Elimination
355
2. If X is a chance variable, then
φX : =

X
	
ΦX;
ψX : =

X
	
ΦX

ΨX

.
If X is a decision variable, then
φX : = max
X
	
ΦX;
ψX : = max
X
	
ΦX

ΨX

.
3.
Φ : = (Φ \ ΦX) ∪{φX}
Ψ : = (Ψ \ ΨX) ∪
ψX
φX

.
⊓⊔
The inﬂuence diagram is solved by repeatedly eliminating variables according
to a strong elimination order.
10.2.1 Strong Junction Trees
The considerations on triangulated graphs and junction trees (see Section 4.4)
can be applied when the method above is used for solving inﬂuence diagrams.
The considerations shall not be repeated here. Consider now the inﬂuence
diagram in Figure 10.5
When solving the inﬂuence diagram, you ﬁrst establish the moral graph:
for each potential you link all variables in the domain. For the graph it means
that you remove information links, add a link for each pair of nodes with
a common child (including a common utility node), and ﬁnally remove the
directions and the utility nodes. It is done in Figure 10.6 for the inﬂuence
diagram in Figure 10.5.
As opposed to Bayesian networks, we cannot choose any elimination order
for the triangulation. We have to follow a strong elimination order: ﬁrst elim-
inate In (in any order), then eliminate Dn, then In−1 and so on (if some Ii is
empty, we may permute the elimination of Di+1 and Di). The resulting trian-
gulation is called a strong triangulation. Figure 10.7 shows the strong triangu-
lation resulting from eliminating the nodes in the moral graph in Figure 10.5
through the strong elimination order A, L, I, J, K, H, C, D, D4, G, D3, D2, E,
F, D1, B.
If you use the method for constructing the join trees from Section 4.3.1,
the result of a strong triangulation is called a strong junction tree with the

356
10 Solution Methods for Decision Graphs
L
D1
V1
D2
D3
V3
D4
V4
A
B
D
C
E
F
H
G
V2
K
J
I
Fig. 10.5. The inﬂuence diagram from Figure 9.22.
D3
A
B
C
D
E
F
G
L
I
H
J
K
D1
D2
D4
Fig. 10.6. The moral graph for the inﬂuence diagram in Figure 10.5.

10.2 Variable Elimination
357
A
B
C
D
E
F
G
L
I
H
J
K
D1
D2
D4
D3
Fig. 10.7. A strong triangulation of the graph in Figure 10.6.
last clique constructed in the strong elimination order serving as a strong root.
A junction tree with a strong root R has the following property: for any two
neighboring cliques C, C′ with separator S and C′ closest to R, it holds that
the variables in S do not appear after the variables in C \ S in the tempo-
ral order. This property ensures that when CollectEvidence(R) is called,
then whenever a variable is eliminated the appropriate potentials are present.
Figure 10.8 shows a strong junction tree for the graph in Figure 10.7.
Note: Although the inﬂuence diagram prescribes a speciﬁc order of the de-
cisions, it happens that some decisions are independent such that the order
may be altered without changing the strategy or the MEU. This is sometimes
detected in constructing a strong junction tree. That is, if you follow the
method from Section 4.3.1, you may get a tree in which the decision nodes
are eliminated in two diﬀerent branches (as is the case in Figure 10.8, where
the elimination of D3 can be done independently of D2 and D4).
From the strong junction tree, you can construct elimination sequences
that do not meet the temporal constraints (the elimination sequence J, K, H,
D3, A, C, L, I, D4, G, D2, D, E, F, D1, B is a perfect elimination sequence end-
ing with B, but it does not follow the temporal order). Since the result of
CollectEvidence(R) is independent of the actual order of messages sent,
all elimination sequences allowed by the strong junction tree give the same
result (as long as the elimination order inside each clique obeys the temporal
constraints). This means that the strong junction tree in Figure 10.8 discloses
that D3 and D4 are independent, and the temporal order can be relaxed to a
partial ordering of the decision nodes.

358
10 Solution Methods for Decision Graphs
BD1EF D
E
F
BED
F D3H
D2G
D3H
ED2G
D2GD4I
D3HK
D4I
HK
BC
D4IL
BEDC
HKJ
BCA
Fig. 10.8. A strong junction tree for the graph in Figure 10.7.
It may also happen that the strong junction tree does not allow for a strong
elimination sequence when CollectEvidence(R) is called. An example is
given in Figure 10.9, where C and F are the ﬁrst variables to be eliminated
according to the temporal ordering, but in the strong junction tree, C is
eliminated after D4. However, this is not a problem, since C cannot aﬀect the
policy for D4 (see Section 11.2).
10.2.2 Required Past
As noted previously, the domain of a policy for a decision variable D1 is in
general (I0, D1, . . . , Ii−1), but a strong elimination order can reveal reductions
of the domain: whenever Di is eliminated, you consider only the potentials
with Di in the domain. The required past must therefore be a subset of the
union of these domains, and thus part of the clique closest to the strong root
containing Di.
With the strong elimination ordering A, L, I, J, K, H, C, D, D4, G, D3, D2,
E, F, D1, B for the inﬂuence diagram in Figure 10.5, we get the following
policies for the decision variables: δ4(G, D2), δ3(F), δ2(E), and δ1(B). Here
we see that the policy for D4, say, contains only two variables as opposed to
the seven variables that constitute the past for D4.

10.2 Variable Elimination
359
A
B
C
E
F
V
BD1A
B
CED3D2B
E
F D4E
D1
D2
D3
D4
(a)
(b)
Fig. 10.9. An inﬂuence diagram (a) with a strong junction tree (b) for which
CollectEvidence(R) does not initiate a strong elimination sequence meeting the
temporal constraints: C should be eliminated before D4.
This analysis does not guarantee minimal policy domains, as can be seen
from the inﬂuence diagram in Figure 10.10. We shall return to this issue in
Section 11.2.
A
E
B
C
D1
D2
V2
V3
V2
Fig. 10.10. The minimal domain of the policy for D1 contains only the variable E,
but a strong elimination ordering would produce a policy over E and B.

360
10 Solution Methods for Decision Graphs
10.2.3 Policy Networks
When a strategy for an inﬂuence diagram has been determined, we have a
policy δi for each decision node Di. The domain of δi is (I0, D1, . . . , Ii−1),
but as shown above (and explained in Section 11.2) we may be able to reduce
it so that it contains only the required variables, denoted by req(Di).
A decision variable can together with its policy be represented in a
Bayesian network.
Deﬁnition 10.1. Let D be a decision variable with policy δD. The chance-
variable representation of D is the result of the following construction: Sub-
stitute D with a chance variable D∗having parents req(D), and assign D∗the
conditional probability distribution P(D∗| req(D)):
P(d|r) =

1
if δD(r) = d,
0
otherwise.
If all decision variables are substituted with their chance-variable repre-
sentations, we obtain a so-called policy network for the inﬂuence diagram.
Deﬁnition 10.2. Let I be an inﬂuence diagram over U = UC ∪UD. A policy
network for ID (denoted by I∗) is a Bayesian network over U = UC ∪U∗
D in
which all decision variables Di have been substituted with their chance-variable
representations. The probability potentials from I are kept (with Djs replaced
by D∗
j ).
Figure 10.11 shows the policy network for the inﬂuence diagram in Fig-
ure 10.5 with the policy domains determined in Section 10.2.2.
Example 10.1. A farmer has a wheat ﬁeld. Twice during the season, he ob-
serves the state of the ﬁeld and decides on a possible treatment with fungicides.
Later, he observes the state of the ﬁeld to decide on the booking of machin-
ery for the harvest. Figure 10.12 shows an inﬂuence diagram for his decision
problem.
To make an advance booking of machinery and for booking plane tickets
for his summer vacation, he wishes to know the time of harvest on which he
may eventually decide.
Based on the inﬂuence diagram an optimal strategy is determined, and
the policy network is constructed (see Figure 10.13).
From the policy network he can read the probabilities of his future decision
as to the time of harvest. After the ﬁrst observation and decision, he may enter
this as evidence and now get a new probability distribution for the optimal
time of harvesting.
Policy networks can be used in other ways. Assume that you know the
farmer’s inﬂuence diagram and observe some of his actions. Then the policy
network can give you estimates on what he may have observed or done in

10.2 Variable Elimination
361
D∗
1
A
C
B
D
E
F
G
I
L
H
L
J
D∗
2
D∗
3
D∗
4
Fig. 10.11. A policy network for the inﬂuence diagram in Figure 10.5.
S4
S1
S2
V3
V1
V2
T1
T2
H
S3
Fig. 10.12. An inﬂuence diagram for treatment and time of harvest.
H∗
S1
S2
S3
S4
T ∗
1
T ∗
2
Fig. 10.13. A policy network for the inﬂuence diagram in Figure 10.12.

362
10 Solution Methods for Decision Graphs
the past. Furthermore, policy networks can be used for analyzing the strategy
proposed by the system: risk proﬁle (what is the probability of losing $X or
going bankrupt?), probability of success (winning at least $X), variance of the
expected utility, etc.
10.3 Node Removal and Arc Reversal
In this section we present a method for solving inﬂuence diagrams by succes-
sively removing the nodes from the diagram. That is, the inﬂuence diagram
is solved through the construction of a sequence of simpler and simpler inﬂu-
ence diagrams. Actually, this method was historically the ﬁrst, which worked
directly on the inﬂuence diagram rather than unfolding it to a decision tree.
10.3.1 Node Removal
The method has four operations: removal of barren nodes, removal of chance
nodes, removal of decision nodes, and arc reversal. The ﬁrst three operations
are rather straightforward.
Removal of barren nodes: A chance or decision node is barren if it has no
children or if all its children are barren. Since a barren node plays no role for
any decision, it can safely be removed.
Removal of chance nodes: Let the only children of the chance node C be
the utility nodes U1, . . . , Uk. Then C and the utility nodes can be removed
by integrating them into one utility node (see Figure 10.14) with the utility
potential
U ∗=

C
P(C| pa(C))
 k

i=1
Ui

.
Removal of decision nodes: Let the only children of the decision node D be
the utility nodes U1, . . . , Uk. Assume that all parents of U1, . . . , Uk are known
at the time of deciding on D. Then the optimal policy for D is
δD = arg max
D
 k

i=1
Ui

,
and D and U1, . . . , Uk can be removed by substituting them with a new utility
node having the potential
U ∗= max
D
 k

i=1
Ui

.

10.3 Node Removal and Arc Reversal
363
...
pa(C)
pa(C)
C
U1
Uk
U ∗
(a)
(b)
Fig. 10.14. (a) C has only utilities as children. (b) The result of removing C.
10.3.2 Arc Reversal
Consider now the inﬂuence diagram in Figure 10.15 (a). None of the removal
operations can be applied. However, by applying Bayes’ rule we can reverse
the arrow from A to B, and now A can be removed.
A
A
B
B
D
D
U
U
(a)
(b)
Fig. 10.15. (a) An inﬂuence diagram, where no nodes can be removed. (b) The arc
has been reversed, and A can now be removed.
To generalize this operation, consider the node A with parents C, . . . , D,
and B with parents A and E, . . . , F (see Figure 10.16 (a)). Assume further
that there is no other directed path between A and B.
Now, if the arc from A to B is reversed and the two nodes are given the
same parents (see Figure 10.16 (b)), then all d-separation properties in the
resulting Bayesian network also hold in the initial network (see Exercise 10.12).
Therefore, the resulting network can represent the probability distri-
bution from the initial network. It is just a question of determining the
new conditional probabilities. We substitute the potentials P(A | C, . . . , D)
and P(B | A, E, . . . , F) with the potentials P(A | B, C, . . . , D, E, . . . , F) and
P(B | C, . . . , D, E, . . . , F), and if the product of the new potentials is equal to
the product of the old potentials, then the chain rule for Bayesian networks

364
10 Solution Methods for Decision Graphs
...
...
...
...
A
A
B
B
C
C
D
D
E
E
F
F
(a)
(b)
Fig. 10.16. (a) A part of a Bayesian network. (b) The arc from A to B has been
reversed, and the two variables are given the same parents.
grants that the two networks represent the same probability distribution. Fur-
thermore, we wish to use only the old potentials for the computation of the
new. For this purpose we ﬁrst establish the following proposition.
Proposition 10.2. Let A be a node with parents pa(A) in a Bayesian net-
work, and let X be a nonparent ancestor of A. Then X and A are d-separated
given pa(A).
Proof.
An active path from A to X not containing parents of A must go
from A to a child of A. Since there cannot be converging connections on this
path, the path must be a directed path from A to X. Since X is an ances-
tor of A, this would create a directed cycle; hence the path cannot be active. ⊓⊔
To establish the new potentials we look at P(A, B | C, . . . , D, E, . . . , F).
From the fundamental rule we have
P(A, B | C, . . . , D, E, . . . , F) =P(B | A, C, . . . , D, E, . . . , F)
P(A | C, . . . , D, E, . . . , F).
The proposition yields that B is independent of C, . . . , D given A, E, . . . , F.
Since there is no directed path between A and B (other than the directed
link), A is independent of E, . . . , F given C, . . . , D. Hence
P(A, B | C, . . . , D, E, . . . , F) = P(B | A, E, . . . , F)P(A | C, . . . , D),
and this can be calculated from the potentials in the Bayesian network. Then
P(B | C, . . . , D, E, . . . , F) =

A
P(A, B | C, . . . , D, E, . . . , F),
and
P(A | B, C, . . . , D, E, . . . , F) = P(A, B | C, . . . , D, E, . . . , F)
P(B | C, . . . , D, E, . . . , F) .
Note that the product of the new potentials is equal to the product of the old
potentials.

10.3 Node Removal and Arc Reversal
365
Arc reversal: Let A and B be chance nodes so that A is a parent of B and
there are no other directed path from A to B. Let C, . . . , D be the parents of
A and let A, E, . . . , F be the parents of B. Then the arc from A to B can be
reversed by assigning A and B the conditional probability distributions
P(B | C, . . . , D, E, . . . , F) =

A
P(B | A, E, . . . , F)P(A | C, . . . , D),
P(A | B, C, . . . , D, E, . . . , F) = P(B | A, E, . . . , F)P(A | C, . . . , D)
P(B | C, . . . , D, E, . . . , F)
,
respectively.
10.3.3 An Example
Consider the inﬂuence diagram in Figure 10.17(a). First we remove the barren
node E, and we get the inﬂuence diagram in Figure 10.17(b).
A
A
B
B
C
C
E
D1
D1
D2
D2
U
U
(a)
(b)
Fig. 10.17. (a) An example inﬂuence diagram. (b) The same inﬂuence diagram
without barren nodes.
Next we can remove C, which has only the utility node as a child, and we
get the new potential
U 1(D2, B) =

C
U(C)P(C | D2, B).
The resulting inﬂuence diagram is shown in Figure 10.18(a). Next, no node
can be removed, and we look for an application of arc reversal. The node B
cannot be removed since it has a chance variable as a child, and we ﬁx this
by arc reversal. The result is shown in Figure 10.18(b).
The new potentials are
P(A | D1) =

B
P(A | B, D1)P(B),
P(B | D1, A) = P(A | B, D1)P(B)/P(A | D1).

366
10 Solution Methods for Decision Graphs
A
A
B
B
U1
U1
D1
D1
D2
D2
(a)
(b)
Fig. 10.18. (a) C has been removed from Figure 10.17(a). (b) The arc from B to
A has been reversed.
Now we can remove B, and we get the new utility (see Figure 10.19(a))
U 2(D1, A, D2) =

B
U 1(B, D2)P(B | A, D1).
D1
D1
D2
A
A
U2
U3
(a)
(b)
Fig. 10.19. (a) B has been removed from Figure 10.18(b). (b) D2 has been removed.
In the inﬂuence diagram in Figure 10.19(a) we can determine the policy
for D2. We have a potential, which directly gives us the utility for each con-
ﬁguration of the relevant past and for each decision option. Hence the policy
is achieved by determining the max-value
δ2(D1, A) = arg max
D2 U 2(D1, A, D2),
and the maximum expected utility is
U 3(D1, A) = max
D2 U 2(D1, A, D2).
The result is the inﬂuence diagram in Figure 10.19(b). A chance-node
removal followed by a decision-node removal does the rest.
Finally, we need to show that the four rules above are complete: all inﬂu-
ence diagrams can be solved by successive application of the four rules. What

10.4 Solutions to Unconstrained Inﬂuence Diagrams
367
we need to show is that if no variables can be removed, then arc reversal will
bring us further. See Exercise 10.9.
10.4 Solutions to Unconstrained Inﬂuence Diagrams
A solution to an unconstrained inﬂuence diagram is an S-DAG together with
optimal policies. An S-DAG containing all admissible orderings and all pos-
sible branchings after each observation can support all policies, and it could
therefore be a candidate for a computational structure for the solution algo-
rithm. However, this full S-DAG grows exponentially in the number of “holes”
in the ordering, and there is a risk that it will become intractably large. Also,
some nodes in the full S-DAG may never be visited by an optimal strategy, and
the corresponding policy is superﬂuous. Therefore it is worthwhile to reduce
the S-DAG under investigation.
Before presenting an algorithm for calculating optimal policies, we shall
illustrate various ways of reducing the full S-DAG, while keeping it an S-DAG
for an optimal strategy.
10.4.1 Minimizing the S-DAG
Consider the UID in Figure 10.20 with the full S-DAG shown in Figure 10.21;
since nothing is gained by delaying a cost-free observation the observables are
placed immediately after they have been released.
A
B
O1
O2
O3
D1
D2
D3
U1
U2
U3
Fig. 10.20. An example UID.
In order to reduce the size of the S-DAG, you can merge paths at points
where they have the same history. For example, the upper path in Figure 10.21
D1−O1−D2 · · · shares history with the path D2−D1−O1 · · · , and from that

368
10 Solution Methods for Decision Graphs
Source
Sink
D1
D1
D1
D1
D1
D2
D2
D2
D2
D2
D3
D3
D3
D3
D3
O1
O1
O1
O1
O1
O2
O2
O2
O2
O2
O2
O3
O3
O3
O3
O3
O3
Fig. 10.21. The full S-DAG for the UID in Figure 10.20.
Source
Sink
D1
D1
D1
D1
D2
D2
D2
D2
D3
D3
D3
D3
O1
O1
O1
O1
O2
O2
O2
O3
O3
O3
Fig. 10.22. The result of merging paths in the S-DAG from Figure 10.21.
point on, they follow the same routes. The result of merging paths according
to this principle is shown in Figure 10.22.
Next, consider the path D2 −D1 −O1 · · · . Since the two decisions D2
and D1 can be swapped without changing the expected utilities, the path
D1 −D2 −O1 · · · will have the same expected utility as D2 −D1 −O1 · · · .
However, on this path, the observation O1 is not taken as soon as it has been
released, and we say that O1 is misplaced. Moving O1 to the other side of D2

10.4 Solutions to Unconstrained Inﬂuence Diagrams
369
cannot decrease the expected utilities, and we get the path D1 −O1 −D2 · · · .
The conclusion is that the path D2 −D1 −O1 · · · can never be better than
D1 −O1 −D2 · · · , and it can therefore be removed from the S-DAG. We say
that the path D1 −O1 −D2 · · · dominates the path D2 −D1 −O1 · · · .
For the same reasons D1 −O1 −D3 · · · dominates D3 −D1 −O1 · · · ,
D2 −D3 −O2 · · · is the same as D3 −D2 −O2 · · · , D1 −O1 −D2 −O3 −D3 · · ·
dominates D1 −O1 −D3 −D2 −O3 · · · . We end up with the S-DAG in
Figure 10.23, and for this particular example the job is reduced to solving
two diﬀerent inﬂuence diagrams. The solution for the UID is then the optimal
strategy of the one with highest expected utility.
Source
Sink
D1
D1
D2
D2
D3
D3
O1
O1
O2
O2
O3
O3
Fig. 10.23. The result of removing dominated paths from the S-DAG in Fig-
ure 10.22.
The reduction of the full S-DAG as performed above has the drawback
that you start out with the full S-DAG, which may be intractably large. To
avoid that, you can start from behind and build up a reduced S-DAG. The
procedure is like a breadth-ﬁrst search in which you go stepwise backward
over the “cross section” of the S-DAG constructed so far. For the UID in
Figure 10.20 you start with the sink, add all decisions that may come last,
and ﬁnally add the observables released by each last decision (see Figure 10.24
(a)).
Consider the path with D2 as the last decision. Then D3 must be placed at
some stage before (see Figure 10.24(b)). If the child of D3 is a decision node,
you can swap until you reach an observable, O. If O is not released by D3, O
is misplaced and it can be swapped with D3. Since O2 is the only observable
released by D3, you can move D3 until it meets O2, and then D3 has passed
D2. To conclude, you can avoid D2 as the last decision.
In general you have the following:
Proposition 10.3. Let D be a decision node (or Sink) in an S-DAG, and let
D1 and D2 be parents of D. If the set of observables released by D1 is a subset
of the set of observables released by D2, then the path with D2 as a parent of
D can be removed without reducing the maximal expected utility.
The proof goes along the same lines as the reasoning above in removing
the path with D2 as a parent of Sink.
To continue the construction of the reduced S-DAG, expand backward
from D1 and D3. The result is shown Figure 10.25.

370
10 Solution Methods for Decision Graphs
...
...
Sink
Sink
D1
D1
D2
D2
D3
D3
D3
O1
O1
O2
O2
O2
O2
O3
O3
O3
O3
D
O
(a)
(b)
Fig. 10.24. (a) The ﬁrst step in a roll-back construction of a reduced S-DAG. (b)
An illustration showing why D2 can be avoided as the last decision.
Sink
D1
D1
D2
D2
D3
D3
O1
O1
O2
O2
O2
O3
O3
O3
Fig. 10.25. The nodes D1 and D3 in Figure 10.24 are expanded backward.

10.4 Solutions to Unconstrained Inﬂuence Diagrams
371
Due to the proposition above, we can remove D2 as a parent of D1 as well
as D1 as a parent of D3. The last expansions yield the S-DAG in Figure 10.23.
10.4.2 Determining Policies and Step Functions
A solution for a reduced S-DAG is determined in almost the same manner
as for inﬂuence diagrams. We eliminate variables in reverse order; when a
branching point is met, the elimination is branched out; when several paths
meet, the probability potentials are the same, and the utility potentials are
uniﬁed through maximization. To illustrate the method we use the UID in
Figure 10.26 with the reduced S-DAG in Figure 10.27.
A
B
C
E
F
D1
D2
D3
D4
U2
U4
Fig. 10.26. A UID. Recall that each decision node has a hidden utility function.
B
C
C
E
E
D1
D2
D2
D3
D3
D4
Fig. 10.27. A reduced S-DAG for the UID in Figure 10.26 (Sink and Source are
ignored).
We start oﬀwith the two sets:
Φ = {P(A | D1), P(B | A), P(C | D2), P(E | D3), P(F | C, E)},
Ψ = {U1(D1), U2(A, D2), U3(D3), U4(F, D4)}.

372
10 Solution Methods for Decision Graphs
First the nonobservables are eliminated. The actual variable elimination fol-
lows the same procedure as for inﬂuence diagrams (see Section 10.2). When
A and F are eliminated, we get the sets
Φ′ = {P(B | D1), P(C | D2), P(E | D3)},
Ψ ′ = {U1(D1), U ′
2(B, D1, D2), U3(D3), U ′
4(C, E, D4)},
where
P(B | D1) =

A
P(A | D1)P(B | A);
U ′
2(B, D1, D2) =
1
P(B | D1)

A
P(A | D1)P(B | A)U2(A, D2);
U ′
4(C, E, D4) =

F
P(F | C, E)U4(F, D4).
Note that 
F P(F | C, E) = 1. When D4 has been eliminated we have
Ψ 4 = {U1(D1), U ′
2(B, D1, D2), U3(D3), U ′′
4 (C, E)},
where
U ′′
4 (C, E) = max
D4 U ′
4(C, E, D4),
δD4(C, E) = arg max
D4 U ′
4(C, E, D4).
Next we branch and produce one set of potentials after elimination of C and
another set after eliminating E:
ΦC = {P(B | D1), P(E | D3)},
Ψ C = {U1(D1), U ′
2(B, D1, D2), U3(D3), U C
4 (E, D2)},
where U C
4 (E, D2) = 
C P(C | D2)U ′′
4 (C, E), and
ΦE = {P(B | D1), P(C | D2)},
Ψ E = {U1(D1), U ′
2(B, D1, D2), U3(D3), U E
4 (C, D3)},
where U E
4 (C, D3) = 
E P(E | D3)U ′′
4 (C, E).
When eventually D3 has been eliminated in the C-branch, and D2 is elim-
inated in the E-branch, we have the two potential sets
ΦCe = {P(B | D1)},
Ψ Ce = {U1(D1), U C(B, D1)};
ΦEc = {P(B | D1)},
Ψ Ec = {U1(D1), U E(B, D1)}.

10.5 Decision Problems Without a Temporal Ordering: Troubleshooting
373
It is no coincidence that the two probability potential sets are identical.
They are both the result of sum-marginalizing the same set of variables from
the same set of potentials. Since sum-marginalizations can be commuted, the
two branches must give the same result. Before marginalizing B we unify the
utility function sets by taking the max for each entry in the utility functions:
Ψ = {U1(D1), max(U C(B, D1), U E(B, D1))}.
The step function is
σ(b, d1) =
D3 if U C(b, d1) ≥U E(b, d1),
D2 otherwise.
Finally, the eliminations of B and D1 are standard.
10.5 Decision Problems Without a Temporal Ordering:
Troubleshooting
A special subclass of decision problems is that of decision problems with no
temporal ordering imposed on the decisions (an extreme type of order asym-
metry). An important example is troubleshooting, whereby a fault causing a
device to malfunction should be identiﬁed and eliminated through a sequence
of troubleshooting steps. Some steps are repair steps, which may or may not
ﬁx the problem; some steps are observation steps, which cannot ﬁx the prob-
lem but may give indications of the causes of the problem; and some steps
have repair aspects as well as observation aspects. The task is to ﬁnd the
cheapest strategy for sequencing the troubleshooting steps. As a ﬁrst attempt
you might try to model the decision problem using the unconstrained inﬂu-
ence diagram framework, but the lack of temporal constraints will quickly
cause the S-DAG to become intractably large, thereby making inference pro-
hibitive. The car start problems of Sections 2.1.1 and 9.3.1 are examples of
troubleshooting tasks.
In this section, we shall consider a solution method for decision problems
with no temporal ordering by focusing solely on troubleshooting problems. In
addition we will deal with pure repair steps and pure observation steps only,
and we will call them actions and questions, respectively.
10.5.1 Action Sequences
First we consider a set of steps consisting of actions only. An action Ai has two
possible outcomes, namely “Ai = yes” (the problem was ﬁxed) and “Ai = no”
(the action failed to ﬁx the problem). Each action Ai has a cost CAi(e), which
may depend on evidence e. We sometimes use Ci(e) (or Ci) as shorthand
for CAi(e). Because there are no questions, a troubleshooting strategy
is a

374
10 Solution Methods for Decision Graphs
sequence of actions s = ⟨A1, . . . , An⟩prescribing the process of repeatedly
performing the next action until an action ﬁxes the problem or the last action
has been performed.
When solving a troubleshooting problem, we have some initial evidence e
and in the course of executing actions in the troubleshooting sequence s =
⟨A1, . . . , An⟩we collect further evidence, namely that the previous actions
have failed. We let ei denote the evidence that the ﬁrst i actions have failed,
and we refer to a set of failed actions as simple evidence. In the following, we
will not mention the initial evidence explicitly.
Deﬁnition 10.3. The expected cost of repair (ECR) of a troubleshooting se-
quence s = ⟨A1, . . . , An⟩with costs Ci is the mean of the costs until an action
succeeds or all actions have been performed:
ECR(s) ≡

i
ECR
i
(s),
where
ECR
i
(s) = Ci(ei−1)P(ei−1).
Note that the term “expected cost of repair” may be misleading because
we allow a situation in which all actions have been performed without having
ﬁxed the problem. If this happens, it will happen with the same probability
regardless of the sequence, and therefore we need not estimate a cost for it.
We may also extend the set of actions with a call service action, CS, that will
ﬁx the problem for sure. We return to this in Section 10.5.3.
Now consider two neighboring actions Ai and Ai+1 in s, and let s′ be
obtained from s by swapping the two actions. The contribution to ECR(s)
from the two actions is
Ci(ei−1)P(ei−1) + Ci+1(ei)P(Ai = no, ei−1),
(10.4)
and the contribution to ECR(s′) from the two actions is
Ci+1(ei−1)P(ei−1) + Ci(ei−1, Ai+1 = no)P(Ai+1 = no, ei−1).
(10.5)
The diﬀerence between (10.5) and (10.4) equals ECR(s′) −ECR(s), so we get
ECR(s′) −ECR(s) = P(ei−1) ·
 
Ci+1(ei−1) −Ci(ei−1)
+Ci(ei−1, Ai+1 = no) P(Ai+1 = no | ei−1) −Ci+1(ei) P(Ai = no | ei−1)
!
.
If s is an optimal troubleshooting sequence, we must have ECR(s) ≤
ECR(s′), and therefore
Ci(ei−1)+Ci+1(ei)P(Ai = no | ei−1)
(10.6)
≤Ci+1(ei−1) + Ci(ei−1, Ai+1 = no)P(Ai+1 = no | ei−1).

10.5 Decision Problems Without a Temporal Ordering: Troubleshooting
375
If it holds that the costs are independent of the actions taken previously, (10.6)
can be rewritten as
P(Ai = yes | ei−1)
Ci
≥P(Ai+1 = yes | ei−1)
Ci+1
.
(10.7)
Deﬁnition 10.4. Let A be a repair action and e be the evidence collected so
far. The eﬃciency of A is deﬁned as
ef(A | e) = P(A = yes | e)
CA(e)
.
The considerations above yield the following result:
Proposition 10.4. Let s be an optimal sequence of actions for which the costs
are independent of the actions taken previously. Then it must hold that
ef(Ai | ei−1) ≥ef(Ai+1 | ei−1), for all i.
10.5.2 A Greedy Approach
As remarked initially, it is not feasible to solve the troubleshooting problem
using, for example, the decision tree framework or the unconstrained inﬂuence
diagram framework. Alternatively, you might try to solve the troubleshooting
problem by doing the sequencing in a greedy fashion: always choose an action
with the highest eﬃciency. However, Proposition 10.4 does not guarantee that
this approach will yield an optimal troubleshooting sequence.
As an example, consider Figure 10.28, where there are four possible causes,
C1, C2, C3, and C4, for a malfunctioning device; we assume that exactly one
of the causes is present, and that the prior probabilities are 0.2, 0.25, 0.40,
and 0.15, respectively. Assume that all actions are perfect and have cost 1.
Then, action A2 has the highest eﬃciency, and if A2 fails, then A1 has higher
eﬃciency than A3. The sequence ⟨A2, A1, A3⟩has ECR = 1.50. However, the
sequence ⟨A3, A1⟩has ECR = 1.45.
To analyze why the decreasing-eﬃciency approach does not guarantee an
optimal sequence, let ⟨A1, . . . , An⟩be a sequence ordered by decreasing eﬃ-
ciency. If the sequence is not optimal, there must be two actions Ai and Aj,
i < j, that in the optimal sequence are taken in reverse order. At the time at
which Ai is chosen, we have
P(Ai = yes | e)
Ci
≥P(Aj = yes | e)
Cj
.
In the optimal sequence, in which Aj is chosen before Ai, we have
P(Ai = yes | e′)
Ci
< P(Aj = yes | e′)
Cj
,

376
10 Solution Methods for Decision Graphs
A1
0.25
0.20
C3
A2
A3
0.15
0.40
C4
C2
C1
Fig. 10.28. An example of dependent actions. The C’s are causes for the device
failing. The A variables represent actions. An action will repair a parent if faulty. A
single fault is assumed.
where e and e′ are simple evidence (not involving Ai and Aj). From this we
can infer that an action sequence ⟨A1, . . . , An⟩is optimal if for all i < j it
holds that
ef(Aj | e) ≤ef(Ai | e),
where e is simple evidence (not involving Ai and Aj).
Proposition 10.5. Consider the following assumptions:
•
The device has n diﬀerent faults F1, . . . , Fn and n diﬀerent repair actions
A1, . . . , An.
•
Exactly one of the faults is present.
•
Each action has a speciﬁc probability of repair, pi = P(Ai = yes | Fi), and
P(Ai = yes | Fj) = 0 for i ̸= j.
•
The cost Ci of a repair action does not depend on the performance of any
previous actions.
If these assumptions hold, then ef(Aj) ≤ef(Ai) implies that ef(Aj | e) ≤
ef(Ai | e), where e is simple evidence (not involving Ai and Aj).
Note that we do not assume the repair actions to be perfect. They may
fail to ﬁx a fault that they are supposed to ﬁx.
Proof.
Let Am be an action that has failed. We calculate P(Ai = yes | Am =
no) (for notational convenience, we omit mention of the current evidence e).
Due to the single-fault assumption, we have P(Am = no | Ai = yes) = 1.
Using Bayes’ rule, we get
P(Ai = yes | Am = no) = P(Am = no | Ai = yes)P(Ai = yes)
P(Am = no)
= P(Ai = yes)
P(Am = no).

10.5 Decision Problems Without a Temporal Ordering: Troubleshooting
377
In other words, P(Am = no) is a normalizing constant for the remaining ac-
tions, and the relative order of eﬃciencies is preserved.
⊓⊔
Example 10.2 (Expansion of Example 9.2). On a cold and wet morning, my
car will not start. Moisture may have aﬀected the ignition system or the
carburetor, the spark plugs may be dirty, there may be a lack of fuel, or there
may be some other fault that I cannot ﬁx myself.
Table 10.3 gives the initial probabilities and costs for the various causes.
Because my car started yesterday evening, I assume that exactly one of the
causes is present. I have one repair action for each possible cause, but the
actions may not be perfect. The measure of precision is the probability of
success given that the cause is present. Table 10.3 gives the precision and
time requirement of the various actions.
SP
IS
Carb
Fu
Other
Cost
4 min. 2 min. 3 min. 1 min. n.a.
Prob.
0.3
0.1
0.1
0.1
0.4
Prec.
0.8
0.7
0.6
0.95
n.a.
Table 10.3. Initial probabilities of the causes, precision, and cost in terms of minutes
for the various repair actions.
The eﬃciencies are calculated as
ef(SP) = 0.3 · 0.8
4
= 0.060,
ef(IS) = 0.1 · 0.7
2
= 0.035,
ef(Carb) = 0.1 · 0.6
3
= 0.02,
ef(Fu) = 0.1 · 0.95
1
= 0.095;
hence I should start with Fu. Assume now that Fu did not solve the problem.
By updating the eﬃciencies of the remaining repair actions (as in the proof
above), we get
ef(SP) =
0.3 · 0.8
(1 −0.1 · 0.95) · 4 = 0.066;
ef(IS) =
0.1 · 0.7
(1 −0.1 · 0.95) · 2 = 0.039;
ef(Carb) =
0.1 · 0.6
(1 −0.1 · 0.95) · 3 = 0.022,
which specify the same sequence of the remaining actions as before the update.

378
10 Solution Methods for Decision Graphs
The following theorem concludes the considerations.
Theorem 10.3. Let s = ⟨A1, . . . , An⟩be an action sequence for a trou-
bleshooting problem fulﬁlling the conditions in Proposition 10.5. Assume that
s is ordered according to decreasing initial eﬃciencies. Then s is an optimal
action sequence and
ECR(s) =
n

i=1
Ci
⎛
⎝1 −
i−1

j=1
pj
⎞
⎠.
(10.8)
Proof.
From the proof of Proposition 10.5, we have that the relative order of
the eﬃciencies of the actions is preserved. For any action sequence s′ that is
not ordered according to ef(Ai), there will be a j such that ef(Aj) < ef(Aj+1)
and therefore ef(Aj | ej−1) < ef(Aj+1 | ej−1). Hence, s′ can be improved by
swapping Aj and Aj+1. From the deﬁnition, we have
ECR(s) =
n

i=1
CiP(ei−1).
Due to the single fault assumption, we have P(ei−1) = 1 −i−1
j=1 pj.
⊓⊔
10.5.3 Call Service
The action call service (CS) will always solve the problem. The cost of CS is
not the unknown price of ﬁxing the device but the possible overhead of having
outsiders ﬁxing a problem you could have ﬁxed yourself. The eﬃciency of CS
is 1/CCS no matter what set of actions has been performed so far.
Let s = ⟨A1, . . . , An⟩be an optimal action sequence resulting from a situa-
tion meeting the assumptions in Proposition 10.5. It may be that the sequence
should be broken before An and service called. According to Proposition 10.4,
CS should be performed only after an action of higher eﬃciency. It is a good
idea to perform the CS action as soon as it has maximal eﬃciency. However,
this is not guaranteed to be optimal. The question of ﬁnding an optimal action
sequence including CS is of higher combinatorial complexity: instead of look-
ing for a sequencing of actions each of which must eventually be performed if
the other actions fail, we now look for a subset of actions and a sequencing of
them. We will not go further into this problem.
10.5.4 Questions
The outcome of a question may shed light on any of the possible faults, or it
may be focused on a particular fault.

10.5 Decision Problems Without a Temporal Ordering: Troubleshooting
379
The troubleshooting task is to interleave actions and questions such that
the expected cost is minimal. To do so, we must analyze the value of answers
to questions.
Imagine that we are in the middle of a troubleshooting sequence; we have
so far gained the evidence e, and now we have the option to ask the question
Q with cost CQ. For simplicity, we assume that Q has only two outcomes,
“yes” and “no.” Assume that regardless of the outcome of Q, we are able
to calculate the minimal expected cost of repair for the remaining sequence.
Therefore, let ECR be the minimal expected cost if Q is not performed, and
let ECRQ=yes and ECRQ=no denote the same for the outcomes “yes” and
“no,” respectively.
Then, the value of observing Q is
V (Q) = ECR −

P(Q = yes | e) ECR
Q=yes +P(Q = no | e) ECR
Q=no

,
(10.9)
and Q is performed if and only if V (Q) > CQ.
In order to determine whether to ask a question prior to an action, we must
analyze all possible succeeding sequences, and if there are several actions and
questions, it is in general intractable. In the future, we will also have question
options to interleave.
A workable approximation is the myopic strategy, where it is assumed at
any stage of troubleshooting that we allow questions to be asked, but in the
future we allow only repair actions. In that case, the task reduces to calculating
expected costs given the various outcomes of the possible questions, and the
approaches from the previous sections can be used.
The Myopic Repair–Observation Strategy
The following strategy is a workable approximation to the general trou-
bleshooting task.
Algorithm 10.2 [Myopic repair–observation strategy] To ﬁnd a my-
opic repair-observation strategy, do:
1. Let e := “the device is not working properly”.
2. While the device is not working properly do
a) Calculate EGC (the expected cost of the greedy observationless repair
sequence).
b) For all O do
i. For all states s of O do
A. Calculate P(O = s | e).
B. For all a do
- Calculate ps
a = P(a solves the problem | O = s, e).
C. Calculate EGCs, the expected cost of the greedy observation-
less repair given O = s.

380
10 Solution Methods for Decision Graphs
ii. Calculate
EGCO = cO +

s
P(O = s | e)EGCs.
c) Choose the observation or action with lowest expected greedy cost; up-
date e according to the choice and result.
⊓⊔
10.6 Solutions to Decision Problems with Unbounded
Time Horizon
When solving a decision problem with an unbounded time horizon, we are
looking for an optimal strategy for the decisions involved. However, as opposed
to optimal strategies for bounded decision problems, an optimal strategy for
an unbounded decision problem will specify the same optimal policy for all
the decisions (see also Section 9.6.1). In what follows we will look at solution
methods for unbounded decision problems. To keep things simple we will
focus on the discounted reward model, and to simplify the exposition we shall
assume that the reward received in a state is independent of the chosen action.
10.6.1 A Basic Solution
As described in Section 9.6.1 we look for a utility function U ∗that speciﬁes
the value of any state s assuming that all subsequent actions maximize the
expected discounted reward:
U ∗(s) = max
Δ U ∗(s, Δ) = max
Δ E
 ∞

i=0
γiR(si)
 Δ, s

.
Instead of calculating U ∗(s, Δ) directly, it can be determined from its “step
wise” speciﬁcation: According to the principle of maximum expected utility
we should always choose the action δ(s) that maximizes the expected utility
of the subsequent states:
δ(s) = arg max
a

s′∈sp(S)
P(s′ | s, a)U ∗(s′).
(10.10)
Hence, the value U ∗of the current state s is the immediate reward collected
at that state plus the maximum expected discounted reward of the subsequent
states:
U ∗(s) = R(s) + γ max
a

s′∈sp(S)
P(s′ | s, a)U ∗(s′).
(10.11)

10.6 Solutions to Decision Problems with Unbounded Time Horizon
381
From equation (10.10) we see that if we can calculate the maximum ex-
pected utility U ∗for each state, then we can also ﬁnd the optimal policy. A
way of calculating U ∗is to consider the equations deﬁned by equation (10.11)
as a system of |sp(S)| nonlinear equations with |sp(S)| unknowns (correspond-
ing to the utility of each state); the nonlinearity is due to the max operator.
A solution to these equations then corresponds to the utility function U ∗.
Unfortunately, solving such a set of equations can be a very diﬃcult task,
and instead, iterative methods are usually applied. The two most commonly
applied iterative methods are called value iteration and policy iteration.
10.6.2 Value Iteration
The idea of value iteration is to start out with an initial guess at the utility
U ∗for each state s, and then iteratively reﬁne this guess. How this reﬁnement
could be done is suggested by equation (10.11): the utility of a state is de-
termined by the immediate reward received at that state plus the maximum
expected utility of all the neighboring states according to our current best
guess at the utility function. To be more precise, if we let U j denote our esti-
mate of the utility function at step j, then we can deﬁne an updating function
as
U j+1(s) = R(s) + max
a

s′
P(s′ | a, s)U j(s′).
(10.12)
The process of updating the utilities is continued for perhaps a ﬁxed number
of iterations or until the largest change is below a certain threshold value.
Example 10.3. In the robot navigation problem in Section 9.6.1, we may set
the initial guess U 0 to 0. Then the ﬁrst iteration sets the utilities U 1 equal to
the rewards at the corresponding positions (see Figure 10.29(a)). During the
next iteration we update, say position (2, 1), as
U 2(2, 1) = R(2, 1) + γ max

s
P(s | north, (2, 1))U 1(s) ,

s
P(s | east, (2, 1))U 1(s),

s
P(s | south, (2, 1))U 1(s),

s
P(s | west, (2, 1))U 1(s)
&
.
By setting the discount factor γ to 0.9 we get

382
10 Solution Methods for Decision Graphs
U 2(2, 1) = −0.1 + 0.9 · max{0.7 · −0.1 + 0.1 · 10 + 0.1 · −5 + 0.1 · −0.1,
0.7 · 10 + 0.1 · −5 + 0.1 · −0.1 + 0.1 · −0.1,
0.7 · −5 + 0.1 · −0.1 + 0.1 · −0.1 + 0.1 · 10,
0.7 · 0.1 + 0.1 · −0.1 + 0.1 · 10 + 0.1 · −5}
= −0.1 + 0.9 · max{0.42, 6.48, −2.52, 0.56}
= 5.73,
and the maximal value corresponds to going east. Similarly, for position (2, 2)
we get
U 2(2, 2) = −5 + 0.9 · max{0.7 · −0.1 + 0.1 · −1 + 0.1 · −0.1 + 0.1 · −0.1,
0.7 · −1 + 0.1 · −0.1 + 0.1 · −0.1 + 0.1 · −0.1,
0.7 · −0.1 + 0.1 · −0.1 + 0.1 · −0.1 + 0.1 · −1,
0.1 · 0.1 + 0.1 · −1 + 0.1 · −0.1 + 0.1 · −0.1}
= −5 + 0.9 · max{−0.19, −0.73, −0.19, −0.19}
= −5.171,
and the optimal action is then either north, south, or west (ties are resolved
according to the sequence west, south, east, and north).
By updating the remaining utilities in this fashion we get the utility func-
tion U 2 shown in Figure 10.29(b). Based on this utility function we can con-
tinue with the third iteration (the result is shown in Figure 10.29(c)) and so
forth; the optimal strategies corresponding to U 2 and U 3 (according to equa-
tion (10.10)) are shown in Figures 10.29(d)–(e); the optimal policy for U 1 is
completely random.
If we continue updating the utilities according to the procedure above, the
method will eventually converge to the utility function and the strategy shown
in Figure 10.30(a) and Figure 10.30(c), respectively. To see the eﬀect of the
discounting factor, Figures 10.30(b) and 10.30(d) show the utility function
and the optimal strategy obtained for γ = 0.1. Observe that when the value
of the discounting factor is reduced (the future becomes less signiﬁcant) the
robot cares less about the goal state and instead focuses on avoiding the
immediate obstacles. Finally, Figure 10.31 shows the maximum log2-diﬀerence
in the utilities after each iteration (using γ = 0.9), which indicates that the
procedure converges exponentially fast.
The fact that value iteration converged to a solution for this particular
problem is no coincidence. It can be shown that value iteration is guaranteed
to converge, and the utility function that it converges to is the maximum
expected discounted reward. Before we give an indication as to why value
iteration exhibits these properties, we shall ﬁrst state the algorithm in its
general form.
Algorithm 10.3 [Value Iteration] Let γ be the discounting factor, R the
reward function, and P the transition function:

10.6 Solutions to Decision Problems with Unbounded Time Horizon
383
1
1
2
2
3
3
−0.1
−0.1
10
−0.1
−5
−1
−0.1
−0.1
−0.1
1
1
2
2
3
3
−0.19
5.73
10
−0.63 −5.17
4.75
−0.19 −0.63 −0.27
1
1
2
2
3
3
3.42
6.23
10
−0.76 −1.07
5.24
−0.35 −0.77
2.79
(a)
(b)
(c)
1
1
2
2
3
3
←
→
←
←
↑
←
←
←
1
1
2
2
3
3
→
→
↓
↑
↑
←
←
↑
(d)
(e)
Fig. 10.29. Figures (a), (b), and (c) show the utility functions produced during the
ﬁrst three updates. Figures (d) and (e) show the corresponding optimal strategies;
the arrows point in the direction of maximum expected discounted reward.
1. Choose an ϵ > 0 to regulate the stopping criterion.
2. Let U 0 be an initial estimate of the utility function (for example, initialized
to zero for all states).
3. Set i := 0.
4. Repeat
a) Let i := i + 1.
b) For each s ∈sp(S),
U i(s) := R(s) + γ · max
a

s′∈sp(S)
P(s′ | a, s)U i−1(s′).
5. Until U i(s) −U i−1(s) < ϵ, for all s ∈sp(S).
⊓⊔
It can be shown that the updating step of the algorithm ensures that the
diﬀerence between any two utility functions is guaranteed to get smaller after
each update. To be more speciﬁc, if we measure the diﬀerence between two
utility functions as the maximum distance between two components in the
functions

384
10 Solution Methods for Decision Graphs
1
1
2
2
3
3
6.18
7.53
10
4.66
1.11
6.46
3.90
4.04
5.28
1
1
2
2
3
3
−0.06
0.56
10
−0.16 −4.97 −0.35
−0.11 −0.16 −0.11
(a)
(b)
1
1
2
2
3
3
→
→
↑
↑
↑
↑
→
↑
1
1
2
2
3
3
→
→
↑
↑
↑
←
←
↓
(c)
(d)
Fig. 10.30. Figures (a) and (c) show the utility function and the optimal strat-
egy obtained upon convergence with discounting factor γ = 0.9. Convergence was
achieved after 75 iterations. Figures (b) and (d) show the situation for γ = 0.1,
where convergence was achieved after 18 iterations.
dist (U1, U2)max = max
s∈sp(S) |U1(s) −U2(s)|,
then for two utility function U1 and U2 we have1
dist

U i+1
1
, U i+1
2

max ≤γ · dist

U i
1, U i
2

max .
In particular, if we set U1 equal to the true utility function U ∗(the solution
to equation (10.11), which does not change during updates), we have
dist

U ∗, U i+1
max ≤γ · dist

U ∗, U i
max .
This behavior allows us to derive two important properties of the updating
function:
•
There is only one true utility function (see Exercise 10.19).
•
The value iteration algorithm is guaranteed to converge to the true utility
function.
1 The updating function is a contraction of a metric space with contraction constant
γ.

10.6 Solutions to Decision Problems with Unbounded Time Horizon
385
-50
-40
-30
-20
-10
 0
 10
 0
 10
 20
 30
 40
 50
 60
 70
Iterations
log2 of the largest change in utility value
Fig. 10.31. The greatest (log2) diﬀerence in the utility values produced by a value
iteration in the robot navigation problem. The solid line corresponds to the dis-
counting factor γ = 0.9 and the dashed line corresponds to γ = 0.1.
In addition to these properties we can also ﬁnd an upper bound on the
number of iterations required for the distance between the true utility func-
tion and a candidate utility function to be less than ϵ. First of all, from
equation (9.1) (page 329) we see that the utility of any state is bounded by
Rmax/(1−γ) (Rmax is maximum absolute reward, Rmax = maxs |R(s)|). Thus,
for the initial iteration we have dist

U ∗, U 0
max ≤2Rmax/(1 −γ), and for
the mth iteration we have dist (U ∗, U m)max ≤γm · 2Rmax/(1 −γ). From the
latter inequality we get
dist (U ∗, U m)max ≤γm · 2Rmax/(1 −γ) ≤ϵ.
By taking the logarithm and isolating m we have m = log(ϵ(1 −γ)/2Rmax)/
log(γ), which speciﬁes an upper bound on the number of iterations required
to achieve an error less than or equal to ϵ. In practice, however, this upper
bound has a tendency to be overly conservative, and other methods have been
devised to provide tighter bounds. Finally, from the equation above we can
also see that the error fades away exponentially fast, but at the same time i
will also quickly increase as γ approaches 1. These eﬀects are demonstrated
in Figures 10.30 and 10.31.
10.6.3 Policy Iteration
In value iteration you might say that we look for the true utility function as
a means of ﬁnding an optimal policy. Another (more direct) approach, called
policy iteration, is to perform an iterative reﬁnement of the current best guess

386
10 Solution Methods for Decision Graphs
at an optimal policy. This method basically consists of two parts: calculate
the utility function UΔi corresponding to the current best guess Δi at an
optimal policy [policy evaluation], and update Δi according to UΔi, thereby
producing an updated policy Δi+1 [policy improvement]. See Figure 10.32.
Δ0
Δ1
Δm
UΔ0
UΔ1
Policy evaluation
Policy evaluation
Policy improvement
Policy improvement
Policy improvement
Fig. 10.32. Policy iteration alternates between two steps: policy evaluation and
policy improvement.
The idea of policy improvement is to improve our current best guess at
the optimal policy Δi by beginning in a single state s and ﬁnding the action
that maximizes the expected utility for that state assuming that the current
policy is optimal for all the other states:
Δi+1(s) := arg max
a

s′∈sp(S)
P(s′ | a, s)UΔi(s′).
That is, we can think of policy improvement as an updating procedure for Δi
based on a one step look-a-head according to the utility function for Δi:
The utility function UΔi used in policy improvement is found during policy
evaluation, where the basic task is to calculate the expected discounted reward
of following the strategy Δi for each state s:
UΔi(s) = R(s) + γ

s′∈sp(S)
P(s′ | Δi(s), s)UΔi(s′).
Since we are working with a ﬁxed strategy, this equation does not involve a
max-operator (as opposed to our initial speciﬁcation of the problem in equa-
tion (10.11)) and the expression is therefore linear in the utilities. This also
means that we can calculate the utility function for a speciﬁc strategy by
treating it as a linear programming problem:

10.6 Solutions to Decision Problems with Unbounded Time Horizon
387
UΔ(s1) = R(s1) + γ
n

j=1
P(sj | Δ(s1), s1)U(sj),
UΔ(s2) = R(s2) + γ
n

j=1
P(sj | Δ(s2), s2)U(sj),
...
UΔ(sn) = R(sn) + γ
n

j=1
P(sj | Δ(sn), sn)U(sj),
consisting of n linear equations and n unknowns. For our robot navigation
problem, n corresponds to the number of possible world positions. When the
state space is small, this programming problem does not introduce any dif-
ﬁculties, but for larger state spaces it may be too time-consuming. Instead,
we can go for an approximate solution to this problem using value iteration.
In this case the time complexity can be controlled by specifying a suitable
termination criterion (a value for ϵ) and then using the upper bound on the
number of value iterations required to reach ϵ.
In general, the policy iteration method can be stated as follows:
Algorithm 10.4 [Policy iteration]
1. Let Δ0 be an initial randomly chosen policy.
2. Set i := 0.
3. Repeat
a) Find the utility function UΔi corresponding to the policy Δi [policy
evaluation].
b) Let i := i + 1.
c) For each s ∈sp(S)
Δi(s) := arg max
a

s′∈sp(S)
P(s′ | a, s)UΔi−1(s′) [policy improvement].
4. Until Δi = Δi−1.
⊓⊔
The algorithm terminates when the current policy is not changed during
an iteration. This also implies that the utility function UΔm for the ﬁnal
policy Δm is the same as the utility function for the policy Δm−1 found in
the previous iteration, since they are both solutions to the same system of
linear equations. Hence, UΔm is a solution to equation (10.11):

388
10 Solution Methods for Decision Graphs
UΔm(S) = R(S) + γ

S′
P(S′ | Δm(S), s0)UΔm(S′)
= R(S) + γ

S′
P(S′ | Δm(S), s0)UΔm−1(S′)
= R(S) + γ max
a

S′
P(S′ | a, s0)UΔm−1(S′)
= R(S) + γ max
a

S′
P(S′ | a, s0)UΔm(S′).
Since this solution is unique (see Section 10.6.2), we know that the policy
returned by policy iteration is also an optimal policy.
10.6.4 Solving Partially Observable Markov Decision Processes*
As stated in Section 9.6.2, there is a fundamental diﬀerence between an op-
timal policy for an MDP and an optimal policy for a POMDP: an optimal
policy for an MDP speciﬁes an action for each possible state of the world, but
an optimal policy for a POMDP speciﬁes an action for each possible belief
that we may have about the state of the world. A belief at step i corresponds
to a probability distribution P(Si | d1, o1, . . . , di−1, oi), which summarizes the
relevant information from the past (lowercase letters are used to denote spe-
ciﬁc observations and decisions). This means that P(Si | d1, o1, . . . , di−1, oi),
our belief at step i, plays the same role as a state in an MDP, and this is
also the reason why P(Si | d1, o1, . . . , di−1, oi) is called the belief state at time
i (denoted by b(Si) or just bi). Thus, if Bi denotes the set of all possible belief
states (of which there are inﬁnitely many), then an optimal policy for decision
Di is a function
δDi : Bi →sp(Di) .
Since both value iteration and policy iteration for MDPs require a ﬁnite
number of states, we cannot directly adopt these methods when working with
POMDPs. Instead you might try to transform the POMDP into an “equiva-
lent” MDP (see Figure 10.34), so that by solving the MDP we also obtain a
solution to the original POMDP.
One possibility might be to simply construct a new ﬁnite belief space B′
representing the original belief space B. For example, in a POMDP with two
world states, sp(S) = {s1, s2}, we have a belief state for each probability of
s1; see Figure 10.33(a). This belief space can be partitioned into, for example,
10 equally wide intervals, B′ = {[0, 0.1), [0.1, 0.2), . . ., [0.9, 1]}, which can be
used as the world states in an MDP representation. To complete the spec-
iﬁcation you also need P(B′
i | B′
i−1, Di−1) and U(Di, B′
i), both of which can
be derived from the original POMDP speciﬁcation. An approximate solution
to the POMDP can now be found by solving the MDP representation using
either value iteration or policy iteration.

10.6 Solutions to Decision Problems with Unbounded Time Horizon
389
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
P(s1 | ·)
P(s2 | ·)
1 −P(s1 | ·) + P(s2 | ·)
 0
 0.2
 0.4
 0.6
 0.8
 1  0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
P(s3 | ·)
P(s1 | ·)
P(s2 | ·)
1 −P(s1 | ·) + P(s2 | ·) + P(s3 | ·)
(a)
(b)
Fig. 10.33. The belief space for a POMDP with two and three world states, re-
spectively.
Unfortunately, this partitioning/discretization procedure is infeasible for
all but the smallest POMDPs, since the number of states in the MDP repre-
sentation grows exponentially in the number of world states in the POMDP.
Figure 10.33(b) shows a partitioning of the belief space for a POMDP with
three states; with four states the belief space would be a hypercube in 4-
dimensional space.
Rather than discretizing the belief space, a more common approach is to
extend the MDP algorithms to inﬁnite state spaces (see Figure 10.34). To give
an idea of the procedure, let us ﬁrst look at how a POMDP can be transformed
into an MDP without dwelling on the potential complications of inﬁnite state
spaces.
Bi−1
Bi
Bi+1
Di−1
Di
Di+1
R′
i−1
R′
i
R′
i+1
Fig. 10.34. The MDP representation of a POMDP. The state variable Bi contains
one state for each possible belief state at step i (of which there are inﬁnitely many).

390
10 Solution Methods for Decision Graphs
First of all, let us start by establishing the fact that the belief state at step
i −1 summarizes all the relevant information about the previous observations
and decisions. This will also help us establish the conditional probabilities
used in the MDP representation. Thus, we look for an independence relation
formed by conditioning on a continuous variable. This type of conditioning
is not an issue we have touched upon previously, but for the purpose of the
subsequent derivations you may treat it as conditioning on a discrete variable.
That is,
bi = P(Si | di−1, oi, past(Di−1)) = P(Si | di−1, oi, bi−1),
where past(Di−1) = (o1, d1, . . . , di−2, oi−1) denotes all observations and deci-
sions prior to decision Di−1. By Bayes’ rule we have that
P(Si | di−1, oi, past(Di−1))
= P(oi | Si, di−1, past(Di−1))P(Si | di−1, past(Di−1))
P(oi | di−1, past(Di−1))
,
(10.13)
and since P(oi | di−1, past(Di−1)) is just a normalization constant, we get
P(Si | di−1, oi, past(Di−1))
∝P(oi | Si, di−1, past(Di−1))P(Si | di−1, past(Di−1)).
The third probability can also be expressed as
P(Si | di−1, past(Di−1))
=

Si−1
P(Si | di−1, past(Di−1), Si−1)P(Si−1 | di−1, past(Di−1)).
Since Si−1 is independent of di−1, and Si is independent of past(Di−1) given
Si−1 (check the d-separation properties in the model) the above expression
simpliﬁes to
P(Si | di−1, past(Di−1)) =

Si−1
P(Si | di−1, Si−1)P(Si−1 | past(Di−1)).
By also exploiting that P(oi | Si, di−1, past(Di−1)) = P(oi | Si, di−1), equa-
tion (10.13) can now be expressed as
P(Si | di−1, oi, past(Di−1))
∝P(oi | Si, di−1)

Si−1
P(Si | di−1, Si−1)P(Si−1 | past(Di−1)).
Since P(Si−1 | past(Di−1)) is the belief state, bi−1, for step i −1 we end up
with

10.6 Solutions to Decision Problems with Unbounded Time Horizon
391
bi = P(Si | di−1, oi, past(Di−1))
∝P(oi | Si, di−1)

Si−1
P(Si | di−1, Si−1)b(Si−1),
(10.14)
where the right-hand side of the expression does not depend on the past
observations and decisions given the previous belief state b(Si−1). We can
therefore write
b(Si) = P(Si | di−1, oi, past(Di−1)) = P(Si | di−1, oi, b(Si−1)).
It should also be noted that in equation (10.14) we have that P(oi | Si, di−1)
is the observation function and P(Si | di−1, Si−1) is the transition function.
Hence, equation (10.14) also provides a way to update our belief state based on
the prior belief state, the decision di−1, and the observation oi. This updated
belief state corresponds to the observation of bi.
Now, going back to our initial goal of describing the transformation of the
POMDP model to the MDP model in Figure 10.34, we need to specify the
transition function P(bi | bi−1, di−1) and the reward function R′
i(bi, Di). The
speciﬁcation should ensure that the two models become equivalent, meaning
that an optimal solution for one of the models is also an optimal solution for
the other model.
The transition function P(bi | di−1, bi−1) can be expressed as
P(bi | di−1, bi−1) =

Oi
P(bi | di−1, bi−1, Oi)P(Oi | di−1, bi−1),
(10.15)
where the probability P(Oi | di−1, bi−1) corresponds to the normalization con-
stant in equation (10.13) and can be calculated as
P(Oi | di−1, bi−1) =

Si
P(Oi | Si, di−1)

Si−1
P(Si | di−1, Si−1)b(Si−1).
Again, the expression depends only on the observation function, the transition
function, and the previous belief state. The function P(bi | di−1, bi−1, oi) is
simply an indicator function deﬁned as
P(bi | di−1, bi−1, oi) =

1
if b(Si) = P(Si | di−1, oi, bi−1),
0
otherwise.
Next, we also have to specify the reward function R′(b(Si, di). Fortunately,
this function can simply be calculated as
R′(bi, Di) =

Si
R(Si, di)b(Si).
(10.16)
Thus, equation (10.15) together with equation (10.16) provides a complete
speciﬁcation of the transformed POMDP, and equation (10.14) describes how
to ﬁnd the observed belief state at each time step.

392
10 Solution Methods for Decision Graphs
The ﬁnal part is now to solve the MDP. However, we cannot immediately
apply the algorithms described in the previous sections, since they work only
on MDPs with ﬁnite state spaces. Instead these algorithms have to be modiﬁed
to work with continuous MDPs. The overall approach is to partition the space
of belief functions into regions, where each region is associated with a par-
ticular strategy and a corresponding linear utility function. A more thorough
description of the algorithm is outside the scope of the present book.
10.7 Limited Memory Inﬂuence Diagrams
The major complexity problem for inﬂuence diagrams is that the relevant
past for a policy may be intractably large. A way of addressing this problem
is to restrict memory. This restriction can be introduced in the form of history
variables or information blocking as described in Section 10.1. Another way is
to pinpoint explicitly what is remembered when a decision is taken. That is,
the no-forgetting assumption in interpreting an inﬂuence diagram is dropped,
and instead memory is represented directly by information links.
Assume that for the ﬁshing example in Figure 9.23 we add the restriction
that we (the EU politicians) remember only last year’s decision, but we can
recall the T -observations up to two years back. This can be represented by
the model in Figure 10.35.
V1
T1
FV1
U1
V2
T2
FV2
U2
V3
T3
FV3
U3
V4
T4
FV4
U4
V5
T5
FV5
U5
Fig. 10.35. Figure 9.23 modiﬁed to represent limited memory. Absent information
arcs mean that the information is not remembered.
An inﬂuence diagram with direct representation of memory is called a lim-
ited memory inﬂuence diagram (LIMID). To stress the diﬀerence, inﬂuence
diagrams can be called perfect recall inﬂuence diagrams. The advantage of
LIMIDs is that they allow you to work with decision policies with small do-
mains. If the domain of a policy does not include all the variables relevant for
the associated decision, then the solution to the LIMID is an approximation
to a solution for the corresponding perfect recall inﬂuence diagram.

10.7 Limited Memory Inﬂuence Diagrams
393
The strong junction tree method automatically constructs cliques contain-
ing domains for perfect recall policies, and it is therefore not well suited for
taking advantage of the space reduction oﬀered by LIMIDs. Instead, a policy
network can be used (see Section 10.2.3): substitute each decision variable D
with a chance variable D∗having the same parents and children as D (we
ignore that some informational parents may turn up nonrequired; see Sec-
tion 11.2). A policy network representation of the LIMID in Figure 10.35 is
shown in Figure 10.36.
V1
T1
FV ∗
1
V2
T2
FV ∗
2
V3
T3
FV ∗
3
V4
T4
FV ∗
4
V5
T5
FV ∗
5
Fig. 10.36. The policy network for the LIMID in Figure 10.35.
We attach a set of initial conditional probability distributions P0(D∗|
pa(D∗)) to the D∗variables. These distributions represent our initial guess
at the optimal policies of the decisions. The distributions need not be deter-
ministic and could be chosen at random. Next, you change the policy network
to a series of one-action networks and solve them as described in Section 9.1.
It is natural to start with the last decision. The single-action network for the
last decision in the ﬁshing network is shown in Figure 10.37.
V1
T1
FV ∗
1
U1
V2
T2
FV ∗
2
U2
V3
T3
FV ∗
3
U3
V4
T4
FV ∗
4
U4
V5
T5
FV5
U5
Fig. 10.37. The single-action network for the last decision in Figure 10.36.

394
10 Solution Methods for Decision Graphs
To establish an optimal policy for FV5 you need P(V5 | FV ∗
4 , T5, T4, T3).
To ﬁnd this probability you can use any inference method for the underlying
Bayesian network; there are no constraints on the elimination order.
Next, having found a new policy δF V5(FV ∗
4 , T5, T4, T3) for FV5 you substi-
tute the initially speciﬁed potential P0(FV ∗
5 | FV ∗
4 , T5, T4, T3) with a chance
variable representation of δF V5:
P1(FV ∗
5 = v | FV ∗
4 , T5, T4, T3) =

1
if δF V5(FV ∗
4 , T5, T4, T3) = v,
0
otherwise,
and construct the single-action network for FV4. See Figure 10.38.
V1
T1
FV ∗
1
U1
V2
T2
FV ∗
2
U2
V3
T3
FV ∗
3
U3
V4
T4
FV4
U4
V5
T5
FV ∗
5
U5
Fig. 10.38. A single-action network for FV4.
To ﬁnd a new policy for FV4 we look for EU(FV4 | FV3, T4, T3, T2), which
is the sum of the expectations for U4 and U5. This requires the calculation of
P(FV ∗
5 , V5 | FV4, FV ∗
3 , T4, T3, T2) and P(V4 | FV4, FV ∗
3 , T4, T3, T2), where the
former joint probability can be found using, for example, variable propagation
(see Section 5.2). Continue to FV3 and down to FV1.
Now, the initial policies for FV1, FV2, FV3, and FV4 were used in de-
termining a new policy for FV5. These initial policies also had an impact
on P(FV ∗
5 , V5 | FV4, FV ∗
3 , T4, T3, T2) and P(V4 | FV4, FV ∗
3 , T4, T3, T2), and you
need to repeat the process based on the new policies. That is, the procedure,
called single policy updating, is iterative, and from the description above we
see that it is closely related to policy iteration for MDPs. It can be shown that
the procedure converges, and that it converges to an optimal strategy for the
LIMID. However, this need not be an optimal strategy for the perfect recall
inﬂuence diagram, and it is an issue of research to establish bounds on the
distance between the LIMID optimal strategy and the perfect recall optimal
strategy.
Algorithm 10.5 [Single policy updating] Let I be a LIMID with decision
variables D1, . . . , Dn, and let I′ be a policy network for I, where the decision
variables are represented by the chance variables D∗
1, . . . , D∗
n.

10.8 Summary
395
1. Let P0(D∗
j | pa(D∗
j )) be a randomly chosen initial probability distribution
for D∗
j , 1 ≤j ≤n, in I′.
2. Let i := 1.
3. Repeat
a) For j := n to 1
i. Let UDj be the utility descendants of Dj.2
ii. Calculate a policy for Dj:
δDj(pa(Dj))i = arg max
Dj

U∈UDj

pa(U)\fa(Dj)
P(pa(U) \ fa(Dj) | fa(Dj))U(pa(U)).
iii. Replace Pi−1(D∗
j | pa(D∗
j )) in I′ with
Pi(D∗
j = d | pa(D∗
j )) =

1
if δi
Dj(pa(D∗
j )) = d,
0
otherwise.
b) Set i := i + 1.
4. Until convergence.
⊓⊔
The repeated construction of single-action networks and variable propa-
gation can be performed in a uniﬁed framework saving a large number of
repetitions of the same calculations. We shall not treat this further but refer
the interested reader to the literature.
10.8 Summary
The Chain Rule for Inﬂuence Diagrams
Let ID be an inﬂuence diagram with universe U = UC ∪UD. Then
P(UC | UD) =
	
X∈UC
P(X | pa(X)).
The Expected Utility and an Optimal Strategy
Let the temporal order of the variables in U be described as I0 ≺D1 ≺I1 ≺
· · · ≺Dn ≺In and let V = 
i Vi. Then
(i) an optimal policy for Di is
δi(I0, D1, . . . , Ii−1) = arg max
Di

Ii
max
Di+1 . . . max
Dn

In
P(UC | UD)V,
2 No other utility nodes can inﬂuence the policy for Dj. See Section 11.2.

396
10 Solution Methods for Decision Graphs
(ii)the expected utility from following the policy δi (and acting optimally in
the future) is
ρi(I0, D1, . . . , Ii−1) =
1
P(I0, . . . , Ii−1 | D1, . . . , Di−1)
max
Di

Ii
max
Di+1 · · · max
Dn

In
P(UC | UD)V,
and the strategy for ID consisting of an optimal policy for each decision yields
the maximum expected utility
MEU(ID) =

I0
max
D1

I1
max
D2 · · · max
Dn

In
P(UC | UD)V.
Variable Elimination for Inﬂuence Diagrams
The inﬂuence diagram is solved by repeatedly eliminating the variables in re-
verse temporal order. When eliminating a variable, you work with two sets
of potentials: Φ, the set of probability potentials; Ψ, the set of utility poten-
tials. When a variable X is eliminated, the potential sets are modiﬁed in the
following way:
1.
ΦX : = {φ ∈Φ | X ∈dom (φ)};
ψX : = {φ ∈Ψ | X ∈dom (φ)}.
2. If X is a chance variable, then
φX : =

X
	
ΦX;
ψX : =

X
	
ΦX

ΨX

.
If X is a decision variable, then3
φX : = max
X
	
ΦX;
ψX : = max
X
	
ΦX

ΨX

.
3.
Φ : =

Φ \ ΦX

∪{φX},
Ψ : =

Ψ \ ΨX

∪
ψX
φX

.
These calculations can also be organized in a strong junction tree for the inﬂu-
ence diagram. A strong junction tree is produced by eliminating the variables
in reverse temporal order.
3 When X is a decision variable, Φx is a constant function over X.

10.8 Summary
397
Policy Networks
Let D be a decision variable with policy δD(req(D)). The chance-variable
representation of D is the result of the following construction: Substitute D
with a chance variable D∗with parents req(D). The conditional probability
potential P(D∗| req(D)) is
P(d|¯r) =

1
if δD(¯r) = d,
0
otherwise.
Let ID be an inﬂuence diagram over U = UC ∪UD. A policy network for ID
(denoted by ID∗) is a Bayesian network over U = UC∪U∗
D in which all decision
variables Di have been substituted with their chance-variable representations.
The probability potentials from ID are kept (with Djs replaced by D∗
j ).
Node Removal and Arc Reversal
The inﬂuence diagram is solved by iteratively removing nodes and reversing
arcs according to the following rules:
Removal of barren nodes: A chance or decision node is barren if it has no
children or all its children are barren. Since a barren node plays no role for
any decision, it can safely be removed.
Removal of chance nodes: Let the only children of the chance node C be the
utility nodes U1, . . . , Uk. Then C and the utility nodes can be removed by
integrating them into one utility node with the utility potential
U ∗=

C
P(C| pa(C))
 k

i=1
Ui

.
Removal of decision nodes. Let the only children of the decision node D be
the utility nodes U1, . . . , Uk. Assume that all parents of U1, . . . , Uk are known
at the time of deciding on D. Then the optimal policy for D is
δD = arg max
D
 k

i=1
Ui

,
and D and U1, . . . , Uk can be removed by substituting them with a new utility
node having the potential
U ∗= max
D
 k

i=1
Ui

.
If no nodes can be removed, then arc reversals can be performed to obtain
another (EU-equivalent) inﬂuence diagram in which one of the rules above

398
10 Solution Methods for Decision Graphs
can be applied.
Arc reversal: Let A and B be chance nodes such that A is a parent of B and
there are no other directed paths from A to B. Let C, . . . , D be the parents
of A and let A, E, . . . , F be the parents of B. Then the arc from A to B can
be reversed by assigning A and B the conditional probability distributions
P(B | C, . . . , D, E, . . . , F) =

A
P(B | A, E, . . . , F)P(A | C, . . . , D),
P(A | B, C, . . . , D, E, . . . , F) = P(B | A, E, . . . , F)P(A | C, . . . , D)
P(B | C, . . . , D, E, . . . , F)
,
respectively.
Unconstrained Inﬂuence Diagrams
An S-DAG can be constructed from a breadth-ﬁrst procedure starting at the
sink: add all the decisions that may come last, and after that you add the ob-
servables released by the decisions. By exploiting the following rule we need
not construct the full S-DAG:
Let D be a decision node (or Sink) in an S-DAG, and let D1 and D2 be par-
ents of D. If the set of observables released by D1 is a subset of the set of
observables released by D2, then the path with D2 as a parent of D can be
removed without reducing the maximal expected utility.
A solution to the UID is found using variable elimination based on the S-DAG
structure.
Troubleshooting
The expected cost of repair of a troubleshooting sequence s = ⟨A1, . . . , An⟩of
repair actions is
ECR(s) =

i
Ci(ei−1)P(ei−1),
where ej denotes the statement that the ﬁrst j actions have failed.
For an optimal repair sequence, it holds that
Ci(ei−1)+Ci+1(ei)P(Ai = n | ei−1)
≤Ci+1(ei−1) + Ci(ei−1, Ai+1 = n)P(Ai+1 = n | ei−1).
The eﬃciency of a repair action is
ef(A | e) = P(A = y | e)
CA(e)
.

10.8 Summary
399
If costs are independent of evidence, then for an optimal repair sequence it
must hold that
ef(Ai | ei−1) ≥ef(Ai+1 | ei−1),
and if for all i < j it holds that
ef(Aj | e) ≤ef(Ai | e)
for all simple evidence e (not involving Ai and Aj) of the type “actions
A, . . . , B have failed,” then the repair sequence ⟨A1, . . . , An⟩is optimal (this
does not necessarily hold when call service is an option).
Questions: The value of getting an answer of Q is
V (Q) = ECR −

s∈Q
P(Q = s | e) ECR
s
,
where ECRs is the expected cost of repair for an optimal sequence given
evidence e and “Q = s,” and ECR is the expected cost of repair for an optimal
sequence not starting with Q. Because neither ECR nor ECRs is tractable, a
myopic approach is often used.
Unbounded Decision Problems
Let γ be the discounting factor, R the reward function, and P the transition
function.
Value iteration:
1. Choose an ϵ > 0 to regulate the stopping criterion.
2. Let U 0 be an initial estimate of the utility function (for example, initialized
to zero for all states).
3. Set i := 0.
4. Repeat
a) Let i := i + 1.
b) For each s ∈sp(S)
U i(s) := R(s) + γ · max
a

s′ sp(S)
P(s′ | a, s)U i−1(s′).
5. Until U i(s) −U i−1(s) < ϵ, for all s ∈sp(S).
Policy iteration:
1. Let Δ0 be some initial (randomly chosen) policy.
2. Set i := 0.
3. Repeat
a) Find the utility function UΔi corresponding to the policy Δi [policy
evaluation].

400
10 Solution Methods for Decision Graphs
b) Let i := i + 1.
c) For each s ∈sp(S)
Δi(s) := arg max
a

s′∈sp(S)
P(s′ | a, s)UΔi−1(s′) [policy improvement].
4. Until Δi = Δi−1.
Limited Memory Inﬂuence Diagrams (LIMIDs)
The no-forgetting assumption is dropped and instead, the informational arcs
specify the variables observed before a particular decision (thereby control-
ling the size of the policy functions). A solution can be found using the single
policy updating algorithm:
Single policy updating: Let I be a LIMID with decision variables D1, . . . , Dn,
and let I′ be a policy network for I in which the decision variables are repre-
sented by the chance variables D∗
1, . . . , D∗
n.
1. Let P0(D∗
j | pa(D∗
j )) be an initial probability distribution (chosen at ran-
dom) for D∗
j , 1 ≤j ≤n, in I′.
2. Let i := 1.
3. Repeat
a) For j := n to 1
i. Let UDj be the utility descendants of Dj.
ii. Calculate a policy for Dj:
δDj(pa(Dj))i = arg max
Dj

U∈UDj

pa(U)\fa(Dj)
P(pa(U) \ fa(Dj) | fa(Dj))U(pa(U)).
iii. Replace Pi−1(D∗
j | pa(D∗
j )) in I′ with
Pi(D∗
j = d | pa(D∗
j )) =

1
if δi
Dj(pa(D∗
j )) = d,
0
otherwise.
b) Set i := i + 1.
4. Until convergence.
10.9 Bibliographical Notes
Various methods for solving inﬂuence diagrams have been constructed. Olm-
sted (1983) and Shachter (1986) introduced arc-reversal, and Shenoy (1992),
Jensen et al. (1994), Cowell (1994), Ndilikilikesha (1994), and Madsen and

10.10 Exercises
401
Jensen (1999a) used elimination and direct manipulation of potentials. Cooper
(1988) presents a method that works well for scenarios with one decision
variable. It substitutes the decision variable and the utility variables with
chance variables and uses Bayesian network propagation. Zhang (1998) ex-
ploits Cooper’s method to full inﬂuence diagrams.
The solution strategy for unconstrained inﬂuence diagrams was proposed
in (Jensen and Vomlelova, 2002). A solution algorithm for sequential inﬂuence
diagrams can be found in (Jensen et al., 2006).
Troubleshooting based on decision theory was introduced by Kalagnanam
and Henrion (1990), and it was further analyzed by Heckerman et al. (1995a).
Section 10.5 is an extension of this work. Proofs that various versions of trou-
bleshooting are NP-complete can be found in (Vomlelov´a, 2003).
The main ideas involved with solving Markov decision processes through
value iteration originates with Shapley (1953). Policy iteration originates with
Howard (1960).
LIMIDs were proposed in (Nilsson and Lauritzen, 2000).
10.10 Exercises
Exercise 10.1. Consider the inﬂuence diagram in Figure 9.22. Is L d-separa-
ted from E given I? Find a minimal set of nodes that d-separate A from
D3.
Exercise 10.2. Consider the inﬂuence diagram DI from Figure 10.1 but
without the utility node V1. Derive the formulas for an optimal strategy.
Exercise 10.3. Prove that during variable elimination, the potential 
 ΦD
is constant over D if all the variables following D in the partial ordering have
already been eliminated.
Exercise 10.4. Construct a strong junction tree for the inﬂuence diagram in
Figure 9.21 and determine the domains of the policies.
Exercise 10.5. Construct strong junction trees for the inﬂuence diagrams
in Figures 9.23 and 9.24. Compare the clique sizes and the domains of the
policies.
Exercise 10.6. Show that any strong triangulation of the inﬂuence diagram
in Figure 10.10 will place E and B in the clique where D1 is eliminated.
Exercise 10.7. Construct a strong junction tree for the inﬂuence diagram in
Figure 10.39
(i) Is D2 required for D3?
(ii) Is B required for D3?

402
10 Solution Methods for Decision Graphs
A
B
C
E
F
G
H
I
D1
D2
D3
U1
U2
U3
Fig. 10.39. Figure for Exercise 10.7.
(iii) Construct a join tree for the policy network and compare the size with
the size of the strong junction tree.
Exercise 10.8. (i) Let {aij} be an n × m matrix of reals. Prove that
maxi

j
aij ≤

j
maxiaij.
(ii) Use (i) to show that the MEU of an inﬂuence diagram will not increase
by delaying an observation. (Hint: Look at the formulas for the two elim-
ination orders.)
Exercise 10.9. Consider the arc-reversal solution method for inﬂuence dia-
grams, and a point where no node can be removed (because the only nodes
with only utility nodes as children are decision nodes and these utility nodes
have nonobservables as parents as well). To show that we can always ﬁnd an
arc to reverse, prove that there is at least one pair of chance nodes A and B
such that A is a parent of B and there is no other directed path from A to B.
Exercise 10.10. Consider the simple inﬂuence diagram in Figure 10.40,
where all variables are binary, and the probabilities for C1 are given in Ta-
ble 10.4, the probability of C2 = c2 is 0.8, and the utility functions U1 and
U2 are given in Tables 10.5 and 10.6. Solve the inﬂuence diagram using node
removal and arc reversal.
D1 \ C2 c2 ¬c2
d1
0.2 0.7
¬d1
0.5 0.5
Table 10.4. P(C1 = c1 | D1, C2).

10.10 Exercises
403
D1
D2
U1
U2
C1
C2
Fig. 10.40. A simple inﬂuence diagram.
D1 \ C1 c1 ¬c1
d1
5
−2
¬d1
3 −10
Table 10.5. U1(D1, C1).
D1 \ C2
c2
¬c2
d1
(0, 0) (8, −5)
¬d1
(5, −1) (1, 12)
Table 10.6. U2(D1, C2, D2). Entries should be interpreted as (d2, ¬d2).
Exercise 10.11. Which steps would be carried out if the inﬂuence diagram in
Figure 9.22 were solved using node removal and arc reversal? Assuming that
each node has two states, what is the largest potential constructed during the
solution process?
Exercise 10.12. Let I be an inﬂuence diagram, and I′ be the inﬂuence dia-
gram obtained by reversing an arc in I. Prove that if X and Y are variables
d-separated by a set of variables Z in I′, then X and Y are also d-separated
given Z in I.
Exercise 10.13. Prove that when the node removal and arc reversal solution
method is applied to an inﬂuence diagram, it eliminates the decision variables
in an order consistent with the partial temporal ordering of the nodes in the
diagram.
Exercise 10.14. Consider the UID in Figure 9.48. Construct the full S-DAG
for the UID, and then reduce it as much as possible. Is the result the same
when you do a roll-back construction of the S-DAG?
Exercise 10.15. Use the algorithm in Section 10.5.4 to solve the start prob-
lem in Example 9.2 (Page 293).

404
10 Solution Methods for Decision Graphs
Exercise 10.16. E You are experiencing irregularities using your computer.
There are several reasons why this can be: ﬁrst, one of the programs you are
running can be malfunctioning and interfering with your operating system;
second, you can have attracted a virus; and third, you can have a hardware
problem. Assuming that only one problem exists, the probabilities of the three
problems are 0.8, 0.15, and 0.05, respectively. Your possible actions for ﬁxing
the problem are
1. Reboot the computer.
2. Run a virus removal tool.
3. Reformat your hard disk and reinstall your operating system.
4. Buy a new computer.
The costs of each option as an overall index of frustration, time usage, and
money spent are 1, 2, 25, and 500, respectively. The probability of action 4
solving the problem is 1 no matter what the problem is and which other
attempts to solve the problem have failed so far. Action 3 has a probability
of 0.99 of ﬁxing the problem if it is a nonhardware problem, and 0 if it is a
hardware problem, no matter which other solutions that have failed previously.
Action 2 solves the problem with probability 0.95 if it is a virus problem,
and with probability 0 otherwise, again no matter what other solutions have
unsuccessfully been tried. Finally, action 1 solves the problem with probability
1 if it is due to a malfunctioning program, and 0 otherwise, no matter what
previous unsuccessful attempts at solving the problem were tried.
Formulate the above setting as a troubleshooting problem, and give an
optimal sequence of repair actions. What is the expected cost of repair for the
sequence?
Exercise 10.17. E Consider again the computer problem in Exercise 10.16,
and assume further that you are given the option of buying a computer pro-
gram that can scan the computer for hardware errors. The overall eﬀort in-
volved in doing this is 4. If there is a hardware error, the program has a
0.999 chance of discovering it, and there is no risk of false positives. More-
over, you are given the choice of having your computer scanned remotely on
the Internet by some company for a price of 0.25. The scanning discovers a
virus with a probability of 0.99 if there is one, but the scanner cannot remove
it. For that you are given the option of downloading a special virus-removal
program, which has a cost of 2 and which removes the identiﬁed virus with a
probability of 1. Are the two oﬀers individually worth the asking price? Are
they worth the price in combination?
Exercise 10.18. Continue Example 10.3 and perform one more iteration of
value iteration starting with the utility function shown in Figure 10.29(c).
Exercise 10.19. Show that there is only one true utility function representing
the maximum expected discounted reward of a Markov decision process with
an unbounded time horizon.

10.10 Exercises
405
Exercise 10.20. E Consider the inﬂuence diagram in Example 10.10, but
interpreted as a LIMID. Using the policies D1 = ¬d1 and D2 = d2, regardless
of the state of C1, run two iterations of policy updating.
Exercise 10.21. E Consider the LIMID in Figure 10.41, with its realization
speciﬁed as in Example 10.10. Using the policies D1 = ¬d1 and D2 = d2,
regardless of the states of C1 and D1, run two iterations of policy updating.
D1
D2
U1
U2
C1
C2
Fig. 10.41. A LIMID for Exercise 10.21.

11
Methods for Analyzing Decision Problems
The primary issue in dealing with a decision problem is to determine an
optimal strategy, but other issues may be relevant. This chapter deals with
value of information, the relevant past and future for a decision, and the
sensitivity of decisions with respect to parameters.
11.1 Value of Information
As mentioned previously, there is a diﬀerence between action decisions and
test decisions; action decisions may result in a state change for some of the
variables, whereas test decisions are decisions to look for more evidence. A
typical situation is that you may choose among some actions, but before de-
ciding on the action you also have the option to perform some tests. The
question is which test to perform, if any.
These types of decision problems can be characterized as asymmetric deci-
sion problems, since they contain at least two types of asymmetry: structural
asymmetry (if you decide not to perform a test, the result is never observed),
and order asymmetry (the sequence of tests may be unspeciﬁed). However,
rather than looking at this as a general asymmetric decision problem we shall
in this section deal directly with the problem by considering the actual value
of information.
11.1.1 Test for Infected Milk?
Consider again the infected milk problem described in Example 9.1, where
we assume that the farmer only has one test, which costs 6 cents and has
a false positive/negative frequency of 0.01. The test situation corresponds
to choosing between the two inﬂuence diagrams in Figure 11.1, where the
leftmost inﬂuence diagram incurs an additional cost of 6 cents.
To establish the utilities, let us assume that the farmer has clean milk
from the 49 other cows. If the farmer pours the milk into the container, he

408
11 Methods for Analyzing Decision Problems
Gain
Gain
Inf
Inf
Action
Action
Test
Test
Fig. 11.1. The test scenario for infected milk corresponds to choosing between
the inﬂuence diagrams, but by choosing the rightmost model you have to pay an
additional 6 cents for the test.
will gain $100 if it is not infected, and he will gain nothing if it is infected. If
he throws the milk away, he will gain $98 regardless of the state of the milk.
If the farmer does not perform a test, the probability of the milk being
infected is 0.0007. The expected utility of pouring the milk into the container
is
EU(pour) = P(Inf = no)U(Inf = no) + P(Inf = yes)U(Inf = yes)
= 0.9993 · 100 + 0.0007 · 0 = 99.93.
Because the expected utility of pouring the milk into the container is larger
than 98, he will do this.
The reason for performing the test is that some outcome will make the
farmer change the decision. To put it in another way, if the decision is the
same regardless of the outcome of the test, then it is not worth the bother to
perform it. Only a positive test result may change the current decision. An
easy calculation yields P(clean | pos) = 0.935. The expected utility of pouring
given a positive test result is
EU(pour | Test = pos) = P(Inf = no | Test = pos)U(Inf = no)
+ P(Inf = yes | Test = pos)U(Inf = yes)
= 0.935 · 100 + 0.065 · 0 = 93.5,
so if the test is positive, the farmer changes his decision. The next concern
is whether the test is worth its price. There are two possibilities: the test
is negative and the milk is poured, or the test is positive and the milk is
thrown away. The probability of the ﬁrst possibility can be calculated from
the speciﬁed probabilities and is 0.9893, and the second possibility has the
probability 0.0107. Hence, the expected beneﬁt of performing the test is
EU(Test) = 0.9893 · 100 + 0.0107 · 98 = 99.98.
The farmer has an increase in expected utility only from 99.93 to 99.98 at
the price of $0.06, so it is not worth while to perform the test.

11.1 Value of Information
409
11.1.2 Myopic Hypothesis-Driven Data Request
In the preceding example, we attached a value to the various information
scenarios, namely the expected utility of the optimal action. The driving force
for evaluating the information scenario was how the distribution of the variable
Infected? was aﬀected by the test. We call this kind of data request hypothesis-
driven: the distribution of a hypothesis variable H is the target of the analysis.
To formulate it in more general terms, there is a value function V attached
to the distribution P(H). Usually, the value function is a maximal utility for
a decision variable D:
V (P(H)) = max
d∈D

h∈H
U(d, h)P(h | d).
Note that here we use V (P(H)) rather than EU(D) to emphasize that we
are looking at the decision problem in a value-of-information context. If test
T with cost CT yields the outcome t, then the value of the new information
scenario is
V (P(H | t)) = max
d∈D

h∈H
U(d, h)P(h | t, d).
Since the outcome of T is not known, we can calculate only the expected value:
EV (T ) =

t∈T
V (P(H | t)) · P(t | d).
The expected beneﬁt of performing test T is
EB(T ) = EV (T ) −V (P(H)).
The expected proﬁt is
EP(T ) = EB(T ) −CT .
The hard part in the calculations is the calculation of P(H | T, D). This
will usually require one propagation per state of T and D. Very often, the
action has no impact on the hypothesis, and this reduces the work.
If there are several possible tests to perform, we are faced with a new
problem. We may calculate the expected proﬁt of each test, but we cannot be
sure that the best choice is the one with the highest expected positive proﬁt.
A proper analysis of the data-request situation should consist in an analysis of
all possible sequences of tests (including the empty sequence). To avoid such
an intractable analysis, the so-called myopic approximation is often used: If
you are allowed to perform at most one test, which one will you choose? The
answer is the one with the highest expected proﬁt if it is positive.
The myopic approach does not guarantee an optimal sequence (see also
Section 10.5.4 in a troubleshooting context). Sometimes a single test does not
yield anything by itself, whereas its outcome may be crucial for selecting a
second very informative test.

410
11 Methods for Analyzing Decision Problems
Now, assume you have the tests T1, . . . , Tm, let H be the hypothesis vari-
able, and assume that the action has no impact on H. To calculate the ex-
pected proﬁt for all tests, you need P(H | Ti) for each Ti. This can be achieved
by propagating each possible outcome of each possible test. It can also be
achieved in a simpler way. By propagating the states of H rather than the
states of the tests, we get P(Ti | H) for all Ti. Bayes’ rule yields
P(H | Ti) = P(Ti | H)P(H)
P(Ti).
Because P(Ti) and P(H) are available initially, we do not need more propa-
gations than there are states in H.
The junction tree framework can also be used to perform some types of
value of information analysis. For example, consider the inﬂuence diagram in
Figure 11.2, where the variable C is observed prior to D3.
A
B
C
D
E
D1
D2
D3
D4
V
Fig. 11.2. An inﬂuence diagram.
The observation may improve the decision D3 and yield a higher expected
utility. The observation has a cost, though, but since it does not aﬀect the
strategy, it is not part of the model. Assume now that we wish to analyze how
much the observation actually improves the expected utility. The situation in
which C is not observed is reﬂected in the inﬂuence diagram in Figure 11.3.
If the diﬀerence in MEU between the two inﬂuence diagrams is smaller than
the cost of observing, then it does not pay to perform the test.
A
B
C
D
E
D1
D2
D3
D4
V
Fig. 11.3. An inﬂuence diagram for the scenario from Figure 11.2 but with C not
observed.

11.1 Value of Information
411
If we assume that the cost of observing is not dependent on the timing,
the MEU cannot get higher by delaying an observation that must eventually
be performed. Therefore, the only option we have is either to observe as soon
as possible or never to observe.
Using a method similar to propagation of variables as described in Sec-
tion 5.2, the calculation of the various MEUs can be joined in one strong
junction tree. Perform a strong triangulation for the inﬂuence diagram mod-
eling that the observations have not been performed (that is, with the chance
variables under analysis as members of In) and construct the strong junction
tree. When solving the inﬂuence diagram corresponding to an observation
of the chance node C just before deciding on Di, you use the same strong
junction tree. However, you defer the elimination of C until Di has been elim-
inated. Figure 11.4 shows the inﬂuence diagram from Figure 10.5, where an
observation is optional for several variables as indicated by the dashed arrows.
The reader may check that you can solve all inﬂuence diagrams correspond-
ing to all combinations of possible observations through delayed elimination
in the strong junction tree in Figure 10.8.
L
D1
V1
D2
D3
V3
D4
V4
A
B
D
C
E
F
H
G
V2
K
J
I
Fig. 11.4. An inﬂuence diagram with the option of not observing A, H, and I.
11.1.3 Non-Utility-Based Value Functions
If there is no proper model for actions and utilities, the reason for acquiring
more information is to decrease the uncertainty of the hypothesis. This means
that you will give high values to probabilities close to zero and one, while

412
11 Methods for Analyzing Decision Problems
probabilities in the middle area should have low values. A classical function
with this property is entropy (see Section 8.4).
The formula for the entropy of a distribution over H is in Section 8.4
deﬁned as
Ent(P(H)) = −

h∈H
P(h) log2(P(h)),
where p log2 p = 0 if p = 0.
Because we want the value function to increase with preference, we let an
entropy-based value function be
V (P(H)) = −Ent(P(H)) =

h∈H
P(h) log2(P(h)).
Variance
If the states of H are numeric, another classical measure can be used, namely
the variance. Again, since small variances are preferred, the value function
becomes
V (P(H)) = −

h∈H
(h −μ)2P(h),
where μ = 
h∈H hP(h).
It is up to the modeler to specify the value function. If decisions with
known utilities are attached to the hypothesis variable, then the utility value
function should be preferred. If this is not the case, the user will mainly be
interested in the precision of a diagnosis.
In the case of a Boolean hypothesis with states 0 and 1, the entropy func-
tion is log pp(1 −p)1−p, and the variance function is −p(1 −p). These two
functions reﬂect that the value of p increases as it approaches its bounds 0
and 1. The entropy function is rather drastic in the way that the slope is
inﬁnite for 0 and 1. Therefore, small changes of p close to 0 and 1 will be
highly valued. On the other hand, the variance is of polynomial degree 2, and
the slope close to the bounds is 1 and −1, giving changes almost even value
no matter how close they are to the bound.
Other Value Functions
In principle, any value function may be used. However, a particular class of
functions called convex functions are best suited for the purpose.
Deﬁnition 11.1. A function f : Rn →R is convex if for any two points
P1, P2 on the graph of f, the line segment P1P2 lies above the graph (see
Figure 11.5). Mathematically, the property is expressed as follows:
∀t ∈[0, 1], ∀x, y ∈Rn : tf(x) + (1 −t)f(y) ≥f(tx + (1 −t)y).
The reason why a convex function is well suited is due to the following
theorem, which we will not prove.

11.2 Finding the Relevant Past and Future of a Decision Problem
413
Theorem 11.1. If the value function is a convex function, then the expected
beneﬁt of performing a test is never negative.
P1
P2
f(tx + (1 −t)y)
tf(x) + (1 −t)f(y)
x
y
tx + (1 −t)y
Fig. 11.5. A convex function. The line segment between two points of the graph
lies above the graph.
Utility based functions are convex and so are entropy and variance.
11.2 Finding the Relevant Past and Future of a Decision
Problem
When solving a decision problem we look for an optimal policy for each of
the decisions. The optimal policy for a decision is in principle a function that
for each possible conﬁguration of the past, prescribes how to act in order
to maximize the expected utility. Thus, for the poker domain modeled in
Figure 11.6, a policy for the decision node D is a function over the entire past
of D:
δD : sp(MH0, MFC, MH1, OFC, MSC, OSC) →sp(D) .
In general, if we represent such a policy function as a table, then the size
of the policy increases exponentially in the number of variables in the past,
and the policy can therefore quickly become intractable to handle.
However, when analyzing the decision problem above, we ﬁnd that not
all variables can provide information inﬂuencing decision D. For example,
if I know my current hand MH2, then knowledge about how many cards I
discarded in the second round, MSC, will not aﬀect my decision at D: at D
I will try to maximize my proﬁt represented by the utility function U. This
utility function depends only on D and BH, and with knowledge of the state
of MH2, the decision MSC becomes d-separated from BH. Hence MSC cannot
tell me anything about BH (and therefore U), and it can therefore not aﬀect
my decision at D. By performing this type of analysis for the remainder of
the variables in the past of D, we ﬁnd that the only variables that can have

414
11 Methods for Analyzing Decision Problems
OH0
OH1
OH2
OFC
OSC
BH
MH2
MH0
MH1
MFC
MSC
U
D
Fig. 11.6. An inﬂuence diagram representation of the poker domain described in
Section 9.4.1. An optimal policy for decision D is a function over the past of D,
namely MH0, MFC, MH1, OFC, MSC, and OSC.
an impact on D are OFC, OSC, and MH2. Hence, the optimal policy for D
reduces to
δD : sp(OFC, OSC, MH2) →sp(D) .
This policy contains only 96 conﬁgurations, as opposed to the full policy
function containing 165888 conﬁgurations. By doing the same exercise for the
two remaining decisions we ﬁnd that only MH1 and OFC are relevant for
MSC, and MH0 is relevant for MFC.
Deﬁnition 11.2 (Required variables). Let I be an inﬂuence diagram and
let D be a decision variable in I. The variable X ∈past(D) is said to be
required for D if there exist a realization R of I, a conﬁguration ¯y over
dom (δD) \ {X}, and states x1 and x2 of X such that δD(x1, ¯y) ̸= δD(x2, ¯y),
where δD is an optimal policy for D with respect to R. The set of variables
required for D is denoted by req(D).
To take another example, consider the inﬂuence diagram in Figure 11.7,
which speciﬁes the partial ordering
{B} ≺D1 ≺{E, F} ≺D2 ≺D3 ≺{G} ≺D4 ≺C4;
C4 denotes the variables not observed before the last decision.
When looking for an optimal policy for D4 we should in principle consider
all the variables in the past of D4, i.e., B, D1, E, F, D2, D3, and G. However,
when analyzing the inﬂuence diagram, we see that deciding on D4 has an
impact only on V4, and from the d-separation properties of the model we have
that by conditioning on G and D2, all the other variables in the past of D4
become d-separated from V4. Hence, only G and D2 are required for D4.

11.2 Finding the Relevant Past and Future of a Decision Problem
415
L
D1
V1
D2
D3
V3
D4
V4
A
B
D
C
E
F
H
G
V2
K
J
I
Fig. 11.7. The ﬁgure illustrates an inﬂuence diagram that speciﬁes the partial order
{B} ≺D1 ≺{E, F} ≺D2 ≺D3 ≺{G} ≺D4 ≺C4 (C4 denotes the chance variables
observed after deciding on all the decisions.
11.2.1 Identifying the Required Past
In the examples above we informally characterized a variable as being re-
quired for D if it can provide information about the utility functions that we
are trying to maximize when deciding on D. To test whether a variable X can
provide information about these utility functions, we used the d-separation
criterion. The question is then how to identify the utility functions that can
inﬂuence D. To be on the safe side you might simply include all utility func-
tions, but this may result in variables that are falsely identiﬁed as required
for D. So we would like to identify the minimal set of utility functions to take
into account when deciding on a particular decision.
Deﬁnition 11.3 (Relevant utility nodes). The utility function U is rele-
vant for decision D if there exists two realizations R1 and R2 of I that diﬀer
only on U such that the optimal policies for D are diﬀerent in R1 and R2.
Luckily, it turns out that this semantic deﬁnition also supports a simple
syntactic characterization. For the last decision we have the following speciﬁ-
cation:
Proposition 11.1. Let Dn be the last decision variable in the inﬂuence dia-
gram I, and let U be a utility node in I. Then U is relevant for Dn if and
only if there is a directed path from Dn to U.

416
11 Methods for Analyzing Decision Problems
Proof.
For the last decision Dn we know that the optimal policy is
δDn(past(Dn)) = arg max
Dn

Cn
P(Cn | past(Dn), Dn)

U(pa(U))
+
m

i=1
Ui(pa(Ui))

= arg max
Dn
 
Cn
P(Cn | past(Dn), Dn)U(pa(U))
+

Cn
P(Cn | past(Dn), Dn)
m

i=1
Ui(pa(Ui))

.
Since

Cn
P(Cn | past(Dn), Dn)U(pa(U))
=

Cn∩pa(U)
P(Cn ∩pa(U) | past(Dn), Dn)U(pa(U)),
we have that U is relevant for Dn if and only if Dn is either a parent of U
or Dn is d-connected to a variable in Cn ∩pa(U) given past(Dn); otherwise,
the above expression would be independent of Dn. In order for Dn to be d-
connected to a variable X ∈Cn ∩pa(U) given past(Dn), there must be an
active path between pa(U) and Dn. Since a converging connection on such a
path cannot be opened by evidence (a descendant of Dn cannot be observed),
the path must be directed from Dn to a node in pa(U).
⊓⊔
Based on this proposition, we now have a full syntactic characterization
of the variables required for the last decision.
Proposition 11.2. Let D be the last decision variable in the inﬂuence dia-
gram I and let X be a variable in past(D). Then X is required for D if and
only if X is d-connected to a utility node relevant for D given past(D) \ {X}.
Proof.
Follows the proof above.
⊓⊔
For example, if we go back to the inﬂuence diagram shown in Figure 11.7,
we see that V4 is the only utility node to which there exists a directed path
from D4; hence V4 is the only utility node relevant for D4. Moreover, using
Proposition 11.2 we ﬁnd that only G and D2 are required for D4, req(D4) =
{G, D2}.
Suppose now that we also want to identify the required variables for D3.
This can be done by substituting D4 with its chance-variable representation
(actually, we need not calculate the policy). This is done in Figure 11.8.

11.2 Finding the Relevant Past and Future of a Decision Problem
417
L
D1
V1
D2
D3
V3
D′
4
V4
A
B
D
C
E
F
H
G
V2
K
J
I
Fig. 11.8. The inﬂuence diagram obtained from the inﬂuence diagram in Figure 11.7
by substituting D4 with its chance-variable representation. Since D3 is the last
decision, we see from Proposition 11.2 that F is the only variable required for D3.
In this transformed inﬂuence diagram, D3 appears as the last decision,
and by applying the propositions we ﬁnd that V2 and V3 are relevant for D3
and that F is the only variable required for D3.
By replacing D3 in Figure 11.8 with its chance-variable representation we
obtain the inﬂuence diagram in Figure 11.9, where D2 is the last decision.
From this model we ﬁnd that V4 is the only utility function relevant for D2,
and E is therefore the only variable required for D2.
Finally, we can ﬁnd the required variables for D1 by substituting D2 with
its chance-variable representation . The resulting model is shown in Fig-
ure 11.10, where we see that all four utility functions are relevant for D1,
and since B is d-connected to V2, V3, V4 we have that B is required for D1.
More generally, we can specify an algorithm for ﬁnding the required vari-
ables for the decisions in an inﬂuence diagram as follows.
Algorithm 11.1 [Identify required variables] Let I be an inﬂuence dia-
gram and let D1, D2, . . . , Dn be the decision variables in I ordered by index.
To determine req(Di), the variables required for Di (∀1 ≤i ≤n), do:
1. Set i := n.
2. For each decision variable Di not considered (i > 0)
a) Let Vi be the set of utility nodes to which there exists a directed path
from Di in I.
b) Let req(Di) be the set of nodes X such that X ∈past(Di) and X is
d-connected to a node in Vi given past(Di) \ {X}.

418
11 Methods for Analyzing Decision Problems
L
D1
V1
D2
D′
3
V3
D′
4
V4
A
B
D
C
E
F
H
G
V2
K
J
I
Fig. 11.9. The inﬂuence diagram obtained form the inﬂuence diagram in Figure 11.8
by substituting D3 with its chance-variable representation.
L
D1
V1
D′
2
D′
3
V3
D′
4
V4
A
B
D
C
E
F
H
G
V2
K
J
I
Fig. 11.10. The inﬂuence diagram obtained from Figure 11.9 by replacing D2 with
its chance variable policy.

11.2 Finding the Relevant Past and Future of a Decision Problem
419
c) Replace Di with a chance-variable representation of the policy for Di,
and let I be the resulting model.
d) Set i := i −1.
⊓⊔
Identifying the Relevant Future
Analogously to the idea of identifying the required variables in the past of a
decision, we can also identify the future variables that are relevant for that
decision. By relevant variables we mean the variables whose probability distri-
butions (or policies) should be taken into account when deciding on D. Having
such a characterization will not reduce the complexity of the policies, but it
may provide insight into the overall structure of the decision problem. For
example, if some decision variable is of particular interest, then the relevant
variables may pinpoint the part of the model that we should focus on when
specifying the probabilities.
Deﬁnition 11.4. Let I be an ID and let D be a decision variable in I. The
future variable X is said to be relevant for D if either:
•
X is a chance node and there exist two realizations R1 and R2 of I that
diﬀer only on the probability distribution associated with X such that the
optimal policies for D are diﬀerent in R1 and R2, or
•
X is a decision variable and there exist a realization of I and two diﬀerent
policies δ1
X and δ2
X for X such that the optimal policies for D are diﬀerent
with respect to δ1
X and δ2
X.
Together with the required past, the relevant variables describe the part of a
decision problem that is suﬃcient to take into account when one is focusing
on a particular decision.
To complete the characterization, we need an algorithm for identifying the
variables that are relevant for a decision D. The ﬁrst thing to notice is that
by using the chance-variable representation of a decision node, we again need
to consider only the situation in which D is the last decision variable in the
inﬂuence diagram. Hence we can identify the relevant future decisions as the
decision variables whose chance-variable representations are relevant for D.
This also means that in order to identify all the relevant variables we just
need a method for identifying the relevant chance variables.
Theorem 11.2. Let I be an ID and let D be the last decision variable in I.
Then the future chance variable X is relevant for D if and only if
•
X is not barren in the ID formed from I by removing all utility nodes that
are not relevant for D, and1
•
there exists a utility node U relevant for D such that X is d-connected to
U in I given {D} ∪past(D).
1 If X is barren, then it does not aﬀect any decisions and it can simply be removed.

420
11 Methods for Analyzing Decision Problems
By going back to the inﬂuence diagram in Figure 11.7, we see that I and
L are the only future variables d-connected to the relevant utility function,
V4, for D4. Hence, no other future chance variables are relevant, and the
decision problem for D4 can therefore be described by the utility node V4, the
required variables G and D2, and the relevant chance variables I and L, see
Figure 11.11(a). To determine the relevant variables for D3 we substitute D4
with a chance variable and apply the same procedure as above. That is, from
Figure 11.8 we see that H, I, and K are relevant for D3, and together with the
relevant utility nodes and the required variables we can identify the part of the
decision problem relevant for D3. See Figure 11.11(b). By continuing to D2,
we use the inﬂuence digram in Figure 11.9. When performing the analysis,
we identify the variable D′
4 as relevant for D2, which in turn means that
the decision node D4 is relevant for D2 (the identiﬁcation of the remaining
variables is left as an exercise).
(a)
D3
V2
D4
D2
L
G
I
V4
F
H
K
J
V3
(b)
Fig. 11.11. The ﬁgures illustrate the parts of the inﬂuence diagram in Figure 11.7
relevant for D4 and D3, respectively.
11.3 Sensitivity Analysis
One of the main diﬃculties in modeling a decision problem is the elicitation of
utilities and probabilities. This makes it desirable to be able to investigate how
sensitive the solution is to variations in some utility or probability parameter,
and how robust the solution is to joint variations over a set of parameters.
We distinguish between value sensitivity and decision sensitivity. Value
sensitivity concerns variations in the maximum expected utility when a set
of parameters is changed, and decision sensitivity refers to changes in the
optimal strategy.

11.3 Sensitivity Analysis
421
11.3.1 Example
Consider the following simpliﬁed binary version of the Oil Wildcatter Problem
from Exercise 9.11. The inﬂuence diagram is shown in Figure 11.12. The hole
can be good or bad. If the hole is good, the gain is $260,000, and if the hole is
bad, the gain is $0. The test has no false negatives, and the probability of a
false positive is 0.05. The prior probability for the hole being good is 0.2. The
cost of drilling is $60,000, and the cost of the test is $5,000.
Hole
Gain
Test?
T-Res
Drill?
Cost
Fig. 11.12. An inﬂuence diagram for the Oil Wildcatter Problem.
The optimal strategy, Δ, is to test and then to drill if and only if the test is
positive. However, although the oil wildcatter is quite certain of the speciﬁcs
of the test, he is rather uncertain of the gain of a good hole as well as of the
prior probability for this particular hole being good. If the gain and the prior
for a good hole are large, he need not test, because he will drill regardless of
the result of the test, and if the prior and the gain for a good hole are low, he
will just leave the hole.
To be precise, the optimal strategy consists of two optimal policies,
δTest? = y for Test?, and δDrill?(Test?, T ′) for Drill?, where δDrill?(y, pos) = y,
δDrill?(y, neg) = n, δDrill?(n, no-test) = n, and the values for other conﬁgura-
tions are of no importance, since they will never be realized.
Let t denote P(Hole = good) and let s denote Gain(Hole = good) −60000.
Then δDrill? is optimal for (t, s) = (0.2, 200000), and the wildcatter would
like to know which parameter values support this policy. To determine the
support, we calculate the expected utilities of the various options. The relevant
utilities are only the utilities on which Drill? has an impact, namely Gain;
the descendant of Drill?. We now get

422
11 Methods for Analyzing Decision Problems
EU(Drill? | n, no-test) = (P(good | no-test)s −P(bad | no-test)60000, 0)
= (ts −(1 −t)6000, 0),
EU(Drill? | y, pos) = (P(good | pos)s −P(bad | pos)60000, 0)
=
ts −0.05(1 −t)60000
0.95t + 0.05
, 0

,
EU(Drill? | y, neg) = (P(good | neg)s −P(bad | neg)60000, 0)
= (−60000, 0).
The policy δDrill? is optimal if
EU(Drill? = n | n, no-test) ≥EU(Drill? = y | n, no-test),
EU(Drill? = y | y, pos) ≥EU(Drill? = n | y, pos),
EU(Drill? = n | y, neg) ≥EU(Drill? = y | y, neg).
This gives the following inequalities:
0 ≥ts −(1 −t)60000,
0 ≤ts −0.05(1 −t)60000,
0 ≥−6000.
That is,
ts + 3000t −3000 ≥0 ≥ts + 60000t −60000.
(11.1)
For s = 200000 we get that δDrill? is optimal for
3
203 ≤t ≤
3
13, and for t = 0.2
it is optimal for 12000 ≤s ≤240000. These intervals are called the admissible
domains for the parameters in δDrill?.
Next we analyze the ﬁrst decision. The decision node Drill? is substituted
with the chance node D (Figure 11.13), and P(D | T ′) reﬂects the optimal
policy (see Section 10.2.3).
Using the model in Figure 11.13 we calculate
EU(Test? = y) = −5000 + P(pos)(EU(Drill? = y | pos)
= −5000 + (0.95t + 0.05)ts −0.05(1 −t)60000
0.95t + 0.05
= −5000 + ts −0.05(1 −t)60000
= ts + 3000t −8000,
EU(Test? = n) = 0.
This yields that testing is optimal if
ts + 3000t −8000 ≥0.
(11.2)

11.3 Sensitivity Analysis
423
Hole
Gain
Test?
T-Res
Drill?
Cost
Fig. 11.13. The decision node Drill? is substituted by its chance-node representa-
tion.
For s = 200000 it holds for t ≥
8
203 and for t = 0.2 it holds for s ≥37000.
The strategy is optimal in the intersection of the admissible domains of the
two policies. That is, for s = 200000 the admissible domain for t is [ 8
203, 3
13].
For t = 0.2, the admissible domain for s is [37000, 240000].
11.3.2 One-Way Sensitivity Analysis in General
Let t be a parameter with initial value t0 in an inﬂuence diagram, and let Δ
be an optimal strategy for the value t0. We wish to determine the admissible
interval for t. The method starts determining the admissible interval for the
policy δDrill? for the last decision D. Then D is substituted by its chance-
variable representation, and the admissible interval for t is determined for
the last decision in this inﬂuence diagram. The procedure is repeated until
the ﬁrst decision has been analyzed. The admissible interval for t in Δ is the
intersection of the admissible intervals for all the policies. Since t0 is a member
of all intervals, we know that the intersection is nonempty.
In the example above it turned out that the expected utilities were simple
expressions in the parameters. This holds in general.
Theorem 11.3. Let s be a utility parameter in the inﬂuence diagram ID, let
D be the last decision in ID, and let π be any conﬁguration of the required
past of D. Then for any d in D, the expected utility of d given π is a linear
function in s.
Let t be probability parameter in the inﬂuence diagram ID, let D be the
last decision in ID, and let π be any conﬁguration of the required past of D.
Then for any d in D, the expected utility of d given π is a fraction of two
linear functions in t.
Proof. [Sketch] The expected utility is calculated as

424
11 Methods for Analyzing Decision Problems

Parents
P(Parents| past)U(Parents).
For utility parameters, this expression is linear. A probability parameter
has an eﬀect on P(Parents| past), and from Corollary 5.2, it can be expressed
as a fraction of two linear functions.
⊓⊔
As for sensitivity analysis for Bayesian networks, this theorem can be
exploited to establish a functional expression for the expected utilities. Assume
that we analyze a utility parameter s with initial value s0. We have a solution
for ID with value s0. That is, we have a value of the expected utility for the
last decision Dn for each conﬁguration of the required past. Next, substitute
s0 with s1 and solve the inﬂuence digram. Again, we get the expected utility
for each option and any conﬁguration of the required past. Now, for each
option and for each parent conﬁguration we have two values of the expected
utility, and the two coeﬃcients in the linear expression can be determined.
The next step is to establish a new inﬂuence diagram, and do the same
with Dn−1 as the last decision. However, if the value s1 lies in the admissible
interval for the policy for Dn, the solution from before can be reused. The
optimal policy for Dn is guaranteed, also for the value s1, to be the same as
the conditional probability for its chance-node representation. This holds for
the next decisions too, so by careful choice of the new value, one extra solution
of the inﬂuence diagram is suﬃcient for the calculation of all the expected
utilities required for determining the admissible domain for the parameter. In
the case of probability parameters, three extra solutions are suﬃcient.
We shall illustrate the method for the parameter s in the oil wildcatter
example above.
Solving the inﬂuence diagram with s = 200000 we get the following ex-
pected utilities:
EU(Drill? | pos) = (156666, 0),
EU(Drill? | neg) = (−60000, 0),
EU(Drill? | no-test) = (−8000, 0),
EU(Test?) = (32600, 0).
Changing s to 150000 we get
EU(Drill? | pos) = (115000, 0),
EU(Drill? | neg) = (−60000, 0),
EU(Drill? | no-test) = (−18000, 0),
EU(Test?) = (22600, 0).
This yields the following expressions:

11.3 Sensitivity Analysis
425
EU(Drill? = y | pos) = 0.833s + 10000,
EU(Drill? = y | neg) = −60000,
EU(Drill? = y | no-test) = 0.2s −48000,
EU(Test? = y) = 0.2s −7400,
which are the same as the result of the expressions in Section 11.3.1.
If you wish to ﬁnd out how stable the strategy is to joint variations of
several the parameters, one-way sensitivity analysis for each parameter may
not provide the full picture and you may need to resort to n-way sensitivity
analysis. However, the work becomes much harder. For example, in the case
of a probability parameter t and a utility parameter s, the expected utilities
have the form αs + β, where α and β are fractions of linear expressions over
t. This means that there are eight coeﬃcients to determine. For illustration,
the admissible area for (t, s) in the strategy from Section 11.3.1 is shown in
Figure 11.14.
 0
 50
 100
 150
 200
 250
 300
 0
 0.2
 0.4
 0.6
 0.8
 1
t = P(Hole = good)
s = Gain(Hole = good) −60000
(0.2,200)•
ts + 3000t −8000 = 0
ts + 60000t −60000 = 0
Fig. 11.14. The admissible area for (t, s) in the strategy for the the oil wildcatter.
The y-axis is scaled by a factor of 1000.
If all parameters are utility parameters, s1, . . . , sn, then the situation is
much simpler. Since utilities are never multiplied, the expected utilities are
linear expressions over s1, . . . , sn. Therefore, there are only n + 1 coeﬃcients
to determine, and n extra solutions are suﬃcient.

426
11 Methods for Analyzing Decision Problems
11.4 Summary
Value of Information
Value function (one utility function U, one decision D):
V (P(H)) = max
d∈D

h∈H
U(d, h)P(h | d).
Expected value of performing test T :
EV (T ) =

t∈T
P(t) max
d∈D

h∈H
U(d, h)P(h | t, d).
Expected proﬁt:
EP(T ) = EV (T ) −V (P(H)) −CT .
The value EV (T ) can be calculated for all tests T by entering the states
of h as evidence and using Bayes’ rule.
Myopic approach: Choose repeatedly a test with the highest positive expected
proﬁt, if any.
Nonutility value functions:
−Entropy: V (P(H)) = 
h∈H P(h) log2(P(h));
−Variance: V (P(H) = −
h∈H(h −μ)2P(h), where μ = 
h∈H hP(h).
The Required Past for a Decision
Let I be an inﬂuence diagram and let D be a decision variable in I. The
variable X ∈past(D) is said to be required for D if there exist a realization
of I, a conﬁguration ¯y over dom (δD) \ {X}, and states x1 and x2 of X such
that δD(x1, ¯y) ̸= δD(x2, ¯y). The set of variables required for D is denoted by
req(D).
To determine req(Di) (∀1 ≤i ≤n) do:
1. Set i := n.
2. For each decision variable not considered (i > 0)
a) Let Vi be the set of value nodes to which there exists a directed path
from Di in I.
b) Let req(Di) be the set of nodes X such that X ∈past(Di) and X is
d-connected to a node in Vi given past(Di) \ {X}.
c) Replace Di with a chance-variable representation of the policy for Di,
and let I be the resulting model.
d) Set i := i −1.

11.6 Exercises
427
Sensitivity Analysis
Value sensitivity: How much can the utility and probability parameters be
varied without changing the optimal strategy? This question can be answered
by performing an analysis of the expected utility as a function of these pa-
rameters.
Utility parameters: Let s be a utility parameter in the inﬂuence diagram ID,
let D be the last decision in ID, and let π be any conﬁguration of the required
past of D. Then for any d in D, the expected utility of d given π is a linear
function in s.
Probability parameters: Let t be probability parameter in the inﬂuence dia-
gram ID, let D be the last decision in ID, and let π be any conﬁguration of
the required past of D. Then for any d in D, the expected utility of d given π
is a fraction of two linear functions in t.
Calculating the coeﬃcients: If there are only utility parameters to investigate,
then all coeﬃcients can be found by performing only one extra propagation for
each parameter. This will also give all the information necessary for performing
n-way sensitivity analysis (that is, sensitivity analysis in which you consider
joint variations of the parameters).
11.5 Bibliographical Notes
Value of information is formally treated in (Howard, 1966) and (Lindley, 1971),
where utilities are guiding the test selection. The myopic approximation was
introduced by Gorry and Barnett (1968). In (Ben-Bassat, 1978), entropy and
variance are used. Value of information for inﬂuence diagrams has been treated
by Dittmer and Jensen (1997) and Shachter (1999). The required past of deci-
sions in inﬂuence diagrams was introduced independently by Shachter (1999)
and Nielsen and Jensen (1999). The relevant future of decisions was described
in (Nielsen, 2002). Sensitivity analysis for multiple parameters in decision
problems was investigated in (Felli and Hazen, 1999a). A method using value
of information was given in (Felli and Hazen, 1999b). Sensitivity analysis for
inﬂuence diagrams in particular was treated in (Nielsen and Jensen, 2003b).
The oil wildcatter’s problem is due to Raiﬀa (1968). The used car buyer’s
problem is due to Howard (1962).
11.6 Exercises
Exercise 11.1. E
Consider the insemination model from Exercise 3.8. As-
sume that you have the options to repeat the insemination or to wait another

428
11 Methods for Analyzing Decision Problems
six-week period. The cost of repeating the insemination is 65 regardless of the
pregnancy state of the cow. If the cow is pregnant and you wait, it will cost
you nothing, but if the cow is not pregnant and you wait, it will cost you
further 30 (that makes a total of 95 for waiting plus the eventual repeated
insemination). The cost of BT is 1 and the cost of UT is 2. Perform a myopic
value of information analysis.
Exercise 11.2. Solve the problem in Exercise 9.11 as a value of information
problem.
Exercise 11.3. E Consider the inﬂuence diagram obtained by adding arcs
from FC, SC, and MH to D in the network in Figure 9.3, using the probabilities
found in Section 3.2.3 and the utilities found in Section 9.1.1. Assume that
prior to the game, a shady-looking person at the table next to me oﬀers to
tell me the ﬁrst hand of my opponent (OH0) for the price of $0.1. Ignoring
ethical issues, should I take the oﬀer?
Exercise 11.4. Consider the inﬂuence diagrams in Figures 9.23 and 9.24.
What is the required past of decision FV4 in the two diagrams?
Exercise 11.5. What are the relevant futures of decisions D1 and D2 in the
inﬂuence diagram in Figure 11.7?
Exercise 11.6. Consider again the inﬂuence diagram in Example 10.10 and
the strategy Δ, stating that one should always choose d1 and d2 if and only
if C1 is in state c1. Denoting by t the utility parameter U(d1, c2, ¬d2), what
is the support of Δ?

List of Notation
Variables have their names capitalized (X, A, Fuel Meter, . . . ). A state of
a variable is written in lowercase. Sets are represented by caligraphic letters
(X, A, . . . ) and boldface is used for vectors (a, X, grad). When we want to
emphasize that we are working with a probability distribution we use, e.g.,
P(A | B) or P #(A | B); otherwise, we use φ(A, B) or ψ(A, B) for probability
potentials and utility potentials, respectively.
arg maxD ρ
A decision from D maximizing ρ.
BIC
Bayesian information criterion.
ch(A)
The children set for variable A.
CMI(X, Y )
The conditional mutual information for variables X and Y .
conf(e)
Measure of conﬂict of evidence e.
D
A database of cases.
Dir
Dirichlet distribution.
dist
Euclidean distance.
dom (φ)
The domain of potential φ.
e
Evidence e represented as a ﬁnding.
ECR
Expected cost of repair.
ef(A)
Eﬃciency of action a.
Ent(X)
The entropy of variable X.
EU
Expected utility.
E
Expectation.
fa(A)
The family set for variable A.
hstG(N)
The history of node N in the S-DAG G.
grad
Gradient vector.
I(A, B, C)
A and B are conditionally independent given C.
KL
Kullbach-Leibler divergence.
L(B | A)
The likelihood of B given A, P(A | B).
LL(B | A)
The log-likelihood of B given A.
maxA
Max-marginal over variable A.
MEU
Maximal expected utility.
MI(X, Y )
The mutual information between variables X and Y .

430
11 Methods for Analyzing Decision Problems
MPE
Most probable explanation.
N(A = a, B = b)
The number of cases containing A = a and B = b.
nb(A)
The neighbor set for variable A.
O
Big-O notation.
P(A)
Probability of event A.
P(A | B)
Probability of event A given B.
P(A)
Probability distribution for variable A.
P(A | B)
Probability distributions for variable A given the
states of variable B.
P(A, B)
Joint probability distribution for variables A and B.
P(A = a)
The probability of variable A being in state a.
P(a)
The probability of outcome/state a (shorthand for
P(A = a)).
P(e)(t)
P(e) as a function of the parameter t.
P #(X | Y )
The frequency based conditional probability distribu-
tion for X given Y .
pa(A)
The parent set for variable A.
past(D)
The variable appearing in the past of variable D.
R
The set of real numbers.
req(D)
The required past of variable D.
s∗
The eﬀective sample size.
score
The score of a Bayesian network.
size(M)
The size of the Bayesian network M.
sp(X)
The state space of variable X.
U
The set of all variables.
1
The unit potential.

A
Summation over variable A.
φ↓V
Projection of potential φ down to domain V.
Φ↓V
Projection of set of potentials Φ down to domain V.

i ψi
The product of the potentials φi.

 Φ
The product of all potentials in set Φ.
Φ−X
The potentials resulting from elimination of variable
X from the set of potentials Φ.
⊗
Combination operator.
∧
Logical and.
∨
Logical or.
¬
Logical negation.
|X|
The number of elements in the set X.
μ
The mean value of a distribution.
ρD
The expected utility for decision variable D.
δD
A policy for decision variable D.
σ2
The variance of a distribution.
ˆθ
A maximum likelihood estimate of the parameter θ.

References
Andreassen, S. (1992). Knowledge representation by extended linear models. In
E. Keravnou, editor, Deep Models for Medical Knowledge Engineering, pages
129–145. Elsevier Science Publishers B. V., Amsterdam.
Andreassen, S., Jensen, F. V., Andersen, S. K., Falck, B., Kjærulﬀ, U., Woldbye,
M., Sørensen, A. R., Rosenfalck, A., and Jensen, F. (1989). MUNIN - an expert
EMG assistant, chapter 21, pages 255–277. Elsevier Science Publishers B. V.
(North-Holland).
Andreassen, S., Falck, B., and Olesen, K. G. (1992).
Diagnostic function of the
microhuman prototype of the expert system munin. Electroencephalography and
Clinical Neurophysiology, 85, 143–157.
˚Astr¨om, K. J. (1965). Optimal control of Markov decision processes with incomplete
state estimation. Journal of Mathematical Analysis and Applications, 10, 174–
205.
Bangsø, O. and Wuillemin, P.-H. (2000). Top-down speciﬁcation and compact rep-
resentation of repetitive structures in Bayesian networks.
In Proceedings of
the Thirteenth International Florida Artiﬁcial Intelligence Research Symposium
Conference.
Beeri, C., Fagin, R., Maier, D., and Yannakakis, M. (1983). On the desirability of
acyclic database schemes. Journal of the Association for Computing Machinery,
30(3), 479–513.
Ben-Bassat, M. (1978). Myopic policies in sequential classiﬁcation. IEEE Transac-
tions of Computing, 27, 170–74.
Bertele, U. and Brioschi, F. (1972). Nonserial Dynamic Programming. Academic
Press, London.
Boyen, X. and Koller, D. (1998). Tractable inference for complex stochastic pro-
cesses. In Proceedings of the 14th Annual Conference on Uncertainty in Artiﬁcial
Intelligence (UAI-98), pages 33–42, San Francisco, CA. Morgan Kaufmann.
Buntine, W. L. (1996). A guide to the literature on learning probabilistic networks
from data. IEEE Transactions on Knowledge and Data Engineering, 8, 195–210.
Cano, A. and Moral, S. (1995). Heuristic algorithms for the triangulation of graphs.
In IPMU’94: Selected papers from the 5th International Conference on Process-
ing and Management of Uncertainty in Knowledge-Based Systems, Advances in
Intelligent Computing, pages 98–107, London, UK. Springer-Verlag.

432
References
Castillo, E., Guti´errez, J. M., and Hadi, A. S. (1996). A new method for eﬃcient
symbolic propagation in discrete Bayesian networks. Networks, 28, 31–43.
Castillo, E., Guti´errez, J. M., and Hadi, A. S. (1997). Sensitivity analysis in dis-
crete Bayesian networks. IEEE Transactions on Systems, Man and Cybernetics,
27(4), 412–423.
Cheng, J., Greiner, R., Kelly, J., Bell, D., and Liu, W. (2002). Learning Bayesian
networks from data: An information-theory based approach. Artiﬁcial Intelli-
gence, 137, 43–90.
Chickering, D. M. (1995). A transformational characterization of Bayesian networks.
In P. Besnard and S. Hanks, editors, Proceedings of the Eleventh Conference on
Uncertainty in Artiﬁcial Intelligence, pages 87–98. Morgan Kaufmann Publish-
ers.
Chickering, D. M. (2002). Optimal structure identiﬁcation with greedy search. Jour-
nal of Machine Learning Research, 3, 507–554.
Chickering, D. M. and Meek, C. (2002). Finding optimal Bayesian networks. In
A. Darwiche and N. Friedman, editors, Proceedings of the Eighteenth Confer-
ence on Uncertainty in Artiﬁcial Intelligence, pages 94–102. Morgan Kaufmann
Publishers.
Chickering, D. M., Heckerman, D., and Meek, C. (2004). Large-sample learning of
Bayesian networks is NP-hard. The Journal of Machine Learning Research, 5,
1287–1330.
Chow, C. and Liu, C. (1968). Approximating discrete probability distributions with
dependence trees. IEEE Transactions on Information Theory, 14(3), 462–467.
Cooper, G. F. (1987).
Probabilistic inference using belief networks is NP-hard.
Artiﬁcial Intelligence, 42, 393–405.
Cooper, G. F. (1988). A method for using belief networks as inﬂuence diagrams.
In G. F. Cooper and S. Moral, editors, Proceedings of the Fourth Conference on
Uncertainty in Artiﬁcial Intelligence, pages 55–63.
Cooper, G. F. (March 1990). The computational complexity of probabilistic infer-
ence using Bayesian belief networks. Artiﬁcial Intelligence, 42(2–3), 393–405.
Cooper, G. F. and Herskovits, E. (1991).
A Bayesian method for constructing
Bayesian belief networks from databases. In B. D. D’Ambrosio, P. Smets, and
P. P. Bonissone, editors, Proceedings of the Seventh Conference on Uncertainty
in Artiﬁcial Intelligence, pages 86–94. Morgan Kaufmann Publishers.
Cooper, G. F. and Herskovits, E. (1992).
A Bayesian Method for Constructing
Bayesian Belief Networks from Databases. Machine Learning, 9, 309–347.
Coup´e, V. M. H. and van der Gaag, L. C. (1998). Practicable sensitivity analysis
of Bayesian belief networks. In M. Huˇskov´a, P. Lachout, and J. V´ıˇsek, editors,
Prague Stochastics ’98 −Proceedings of the Joint Session of the 6th Prague Sym-
posium of Asymptotic Statistics and the 13th Prague Conference on Information
Theory, Statistical Decision Functions and Random Processes, Union of Czech
Mathematicians and Physicists, Prague, pages 81–86.
Covaliu, Z. and Oliver, R. M. (1995). Representation and solution of decision prob-
lems using sequential decision diagrams. Management Science, 41(12), 1860–
1881.
Cowell, R. G. (1994). Decision networks: A new formulation for multistage decision
problems. Research Report 132, Department of Statistical Science, University
College London, London.

References
433
Cowell, R. G. (2001). Conditions under which conditional independence and scoring
methods lead to identical selection of Bayesian network models. In J. Breese
and D. Koller, editors, Proceedings of the Seventeenth International Conference
on Uncertainty in Artiﬁcial Intelligence, pages 91–97. Morgan Kaufmann.
Cowell, R. G., Dawid, A. P., Lauritzen, S. L., and Spiegelhalter, D. J. (1999). Proba-
bilistic Networks and Expert Systems. Statistics for engineering and information
science. Springer-Verlag New York, Inc. ISBN 0-387-98767-3.
D’Ambrosio, B. (1991). Local expression language for probabilistic dependence: a
preliminary report. In B. D. D’Ambrosio, P. Smets, and P. P. Bonissone, editors,
Proceedings of the Seventh Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), pages 95–102. Morgan Kaufmann Publishers.
Darwiche, A. (2001). Recursive conditioning. Artiﬁcial Intelligence, 126(1–2), 5–41.
Dawid, A. P. (1992). Applications of a general propagation algorithm for a proba-
bilistic expert system. Statistics and Computing, 2, 25–36.
de Dombal, F., Leaper, D., Staniland, J., McCann, A., and Harrocks, J. (1972).
Computer-aided diagnosis of acute abdominal pain. British Medical Journal, 2,
9–13.
Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic infer-
ence. In E. Horvitz and F. V. Jensen, editors, Proceedings of the Twelfth Confer-
ence on Uncertainty in Artiﬁcial Intelligence, pages 211–219. Morgan Kaufmann
Publishers.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society,
Series B, 39, 1–38.
Dittmer, S. L. and Jensen, F. V. (1997). Myopic value of information in inﬂuence
diagrams.
In D. Geiger and P. P. Shenoy, editors, Proceedings of the Thir-
teenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 142–149. Mor-
gan Kaufmann Publishers.
Domingos, P. and Pazzani, M. J. (1997). On the optimality of the simple Bayesian
classiﬁer under zero-one loss. Machine Learning, 29(2–3), 103–130.
Drake, A. W. (1962). Observation of a Markov process through a noisy channel.
Ph.D. thesis, Massachusetts Institute of Technology. Dept. of Electrical Engi-
neering.
Druzdzel, M. and van der Gaag, L. (1995).
Elicitation of probabilities for belief
networks: Combining qualitative and quantitative information. In P. Besnard
and S. Hanks, editors, Proceedings of the Eleventh Conference on Uncertainty in
Artiﬁcial Intelligence, pages 141–148. Morgan Kaufmann Publishers.
Duda, R. O. and Hart, P. E. (1973).
Pattern Classiﬁcation and Scene Analysis.
John Wiley & Sons, New York.
Edwards, D. and Havranek, T. (1985). A fast procedure for model search in multi-
dimensional contingency tables. Biometrika, 72(2), 339–351.
Felli, J. C. and Hazen, G. B. (1999a). Do sensitivity analysis really capture problem
sensitivity? an empirical analysis based on information value.
Risk, Decision
and Policy, 4(2), 79–98.
Felli, J. C. and Hazen, G. B. (1999b). Sensitivity analysis and the expected value
of perfect information. Medical Decision Making, 18, 95–109.
Friedman, N. (1998). The Bayesian Structural EM Algorithm. In G. F. Cooper and
S. Moral, editors, Proceedings of the Fourteenth Conference on Uncertainty in
Artiﬁcial Intelligence. Morgan Kaufmann Publishers.

434
References
Friedman, N. and Goldszmidt, M. (1998). Learning Bayesian networks with local
structure. In M. Jordan, editor, Learning in Graphical Models, pages 421–459.
Kluwer.
Friedman, N. and Koller, D. (2003). Being Bayesian about network structure. Ma-
chine learning, 50(1–2), 95–125.
Friedman, N., Geiger, D., and Goldszmidt, M. (1997). Bayesian network classiﬁers.
Machine Learning, 29(2–3), 131–163.
Fung, R. M. and Chang, K.-C. (1990).
Weighing and integrating evidence for
stochastic simulation in Bayesian networks.
In M. Henrion, R. Shachter,
L. Kanal, and J. Lemmer, editors, Proceedings of the Fifth Annual Conference
on Uncertainty in Artiﬁcial Intelligence, pages 209–220. North-Holland.
Geiger, D. and Pearl, J. (1988). On the logic of causal models. In Proceedings of
the 4th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-88),
pages 3–14, New York, NY. Elsevier Science Publishing.
Geiger, D., Heckerman, D., and Meek, C. (1996). Asymptotic model selection for
directed networks with hidden variables. In E. Horvitz and F. V. Jensen, editors,
Proceedings of the Twelfth Conference on Uncertainty in Artiﬁcial Intelligence,
pages 283–290. Morgan Kaufmann Publishers.
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and
the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6(6), 721–741.
Gilks, W. R., Thomas, A., and Spiegelhalter, D. J. (1994). A language and a program
for complex Bayesian modelling. The Statistician, 43, 169–178.
Golumbic, M. C. (1980). Algorithmic Graph Theory and Perfect Graphs. Academic
Press, London.
Gorry, G. A. and Barnett, G. O. (1968). Experience with a model of sequential
diagnosis. Computers and Biomedical Research, 1, 490–507.
Green, P. J. (1990). On use of the EM algorithm for penalized likelihood estimation.
Journal of the Royal Statistical Society, Series B, 52(3), 443–452.
Habbema, J. D. F. (1976). Models diagnosis and detection of diseases. In de Dom-
bal et al., editors, Decision Making and Medical Care, pages 399–411. Elsevier
Science Publishers, Amsterdam.
Heckerman, D. (1990). Probabilistic similarity networks. Networks, 20, 607–636.
Heckerman, D. (1998). A turorial on learning with Bayesian networks. In M. I.
Jordan, editor, Learning in Graphical Models, pages 301–354. Kluwer Academic
Publishers.
Heckerman, D., Horwitz, E., and Nathwani, B. (1992). Toward normative expert
systems: Part i. the pathﬁnder project. Methods of Information in Medicine,
31, 90–105.
Heckerman, D., Breese, J., and Rommelse, K. (1995a).
Decision-theoretic trou-
bleshooting. Communications of the ACM , 38(3), 49–56.
Heckerman, D., Geiger, D., and Chickering, D. M. (1995b).
Learning Bayesian
networks: The combination of knowledge and statistical data. Machine Learning,
20(3), 197–243.
Henrion, M. (1988). Propagating uncertainty in Bayesian networks by probabilis-
tic logic sampling. In J. F. Lemmer and L. M. Kanal, editors, Uncertainty in
Artiﬁcial Intelligence 2, pages 149–163. Elsevier Science Publishers, Amsterdam.
Howard, R. A. (1960). Dynamic Programming and Markov Process. MIT Press.

References
435
Howard, R. A. (1962). The used car buyer. In R. A. Howard and J. E. Mathe-
son, editors, The Principles and Applications of Decision Analysis, volume 2,
chapter 36, pages 691–718. Strategic Decision Group.
Howard, R. A. (1966). Information value theory. IEEE Transactions on Systems
Science and Cybernetics, pages 22–26.
Howard, R. A. and Matheson, J. E. (1981). Inﬂuence diagrams. In R. A. Howard and
J. E. Matheson, editors, The Principles and Applications of Decision Analysis,
volume 2, chapter 37, pages 721–762. Strategic Decision Group.
Jaeger, M. (2003).
Probabilistic classiﬁers and the concepts they recognize.
In
T. Fawcett and N. Mishra, editors, Proceedings of the Twentieth International
Conference on Machine Learning, pages 266–273. AAAI Press.
Jensen, F., Jensen, F. V., and Dittmer, S. L. (1994).
From inﬂuence diagrams
to junction trees. In R. L. de Mantaras and D. Poole, editors, Proceedings of
the Tenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 367–373.
Morgan Kaufmann Publishers.
Jensen, F. V. (1999). Gradient descent training of Bayesian networks. In A. Hunter
and S. Parsons, editors, Proceedings of the Fifth European Conference on Sym-
bolic and Quantitative Approaches to Reasoning with Uncertainty, Lecture Notes
in Artiﬁcial Intelligence, pages 190–200. Springer-Verlag.
Jensen, F. V. and Vomlelova, M. (2002).
Unconstrained inﬂuence diagrams.
In
A. Darwiche and N. Friedman, editors, Proceedings of the Eighteenth Confer-
ence on Uncertainty in Artiﬁcial Intelligence, pages 234–241. Morgan Kaufmann
Publishers.
Jensen, F. V., Chamberlain, B., Nordahl, T., and Jensen, F. (1990a).
Analysis
in HUGIN of data conﬂict. In Uncertainty in Artiﬁcial Intelligence 6, pages
519–528. Elsevier Science Publishers, Amsterdam.
Jensen, F. V., Lauritzen, S. L., and Olesen, K. G. (1990b). Bayesian updating in
causal probabilistic networks by local computations. Computational Statistics
Quarterly, 4, 269–282.
Jensen, F. V., Aldenryd, S. H., and Jensen, K. B. (1995). Sensitivity analysis in
Bayesian networks.
In C. Froidevaux and J. Kohlas, editors, Proceedings of
ECSQARU’95, volume 946 of Lecture Notes in Artiﬁcial Intelligence, pages 243–
250, Fribourg, Switzerland. Springer, Berlin.
Jensen, F. V., Nielsen, T. D., and Shenoy, P. P. (2006). Sequential inﬂuence dia-
grams: A uniﬁed asymmetry framework. International Journal of Approximate
Reasoning, 42(1–2), 101–118.
Jordan, M., editor (1998). Learning in Graphical Models. Kluwer.
Kalagnanam, J. and Henrion, M. (1990). A comparison of decision analysis and
expert rules for sequential analysis. In P. Besnard and S. Hanks, editors, Uncer-
tainty in Artiﬁcial Intelligence 4, pages 271–281. North-Holland, New York.
Kim, J. H. and Pearl, J. (1983). A computational model for causal and diagnostic
reasoning in inference systems. In Proceedings of the Eight International Joint
Conference on Artiﬁcial Intelligence, pages 190–193. William Kaufmann, Los
Altos, CA.
Kim, Y.-G. and Valtorta, M. (1995).
On the detection of conﬂicts in diagnostic
Bayesian networks using abstraction. In P. Besnard and S. Hanks, editors, Pro-
ceedings of the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence,
pages 362–367. Morgan Kaufmann Publishers.

436
References
Kjærulﬀ, U. (1990). Triangulation of graphs — algorithms giving small total space.
Technical Report R 90-09, Department of Mathematics and Computer Science,
Aalborg University.
Kjærulﬀ, U. (1992). A computational scheme for reasoning in dynamic probabilistic
networks. In D. Dubois, M. P. Wellman, B. D’Ambrosio, and P. Smets, editors,
Proceedings of the Eighth Conference on Uncertainty in Artiﬁcial Intelligence,
pages 121–129. Morgan Kaufmann Publishers.
Kjærulﬀ, U. and van der Gaag, L. C. (2000). Making sensitivity analysis computa-
tionally eﬃcient. In C. Boutilier and M. Goldszmidt, editors, Proceedings of the
Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 317–325.
Morgan Kaufmann Publishers.
Koller, D. and Pfeﬀer, A. (1997). Object-oriented Bayesian networks. In Proceedings
of the Thirteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI-97),
pages 302–313.
Lam, W. and Bacchus, F. (1994). Learning Bayesian belief networks. An approach
based on the MDL principle. Computational Intelligence, 10, 269–293.
Laskey, K. B. (1991). Conﬂict and surprise: Heuristics for model revision. In B. D.
D’Ambrosio, P. Smets, and P. P. Bonissone, editors, Proceedings of the Sev-
enth Conference on Uncertainty in Artiﬁcial Intelligence, pages 197–204. Mor-
gan Kaufmann Publishers.
Laskey, K. B. (1995). Sensitivity analysis for probability assessments in Bayesian
networks. IEEE Transactions on Systems, Man and Cybernetics, 25, 901–909.
Lauritzen, S. L. (1995). The EM algorithm for graphical association models with
missing data. Computational Statistics and Data Analysis, 19, 191–201.
Lauritzen, S. L. (1996). Graphical Models. Oxford University Press. ISBN: 0-19-
852219-3.
Lauritzen, S. L. and Jensen, F. V. (1997). Local computation with valuations from
a commutative semigroup. Annals of Mathematics and Artiﬁcial Intelligence,
21(1), 51–69.
Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Local computations with probabil-
ities on graphical structures and their application to expert systems. Journal of
the Royal Statistical Society, Series B, 50(2), 157–224.
Lauritzen, S. L., Dawid, A. P., Larsen, B. N., and Leimer, H.-G. (1990). Indepen-
dence properties of directed Markov ﬁelds. Networks, 20(5), 491–505.
Lindley, D. V. (1971). Making Decisions. John Wiley & Sons, New York.
Madsen, A. L. and Jensen, F. V. (1999a). Lazy evaluation of symmetric Bayesian
decision problems. In K. B. Laskey and H. Prade, editors, Proceedings of the
Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 382–390.
Morgan Kaufmann Publishers.
Madsen, A. L. and Jensen, F. V. (1999b). Lazy propagation: A junction tree infer-
ence algorithm based on lazy evaluation. Artiﬁcial Intelligence, 113, 203–245.
Margaritis, D. and Thrun, S. (1999). Bayesian network induction via local neigh-
borhoods.
In Advances in Neural Information Processing Systems 12, pages
505–511. MIT Press.
Meek, C. (1995). Strong completeness and faithfulness in Bayesian networks. In Pro-
ceedings of the 11th Annual Conference on Uncertainty in Artiﬁcial Intelligence
(UAI-95), pages 411–41, San Francisco, CA. Morgan Kaufmann.
Michalewicz, Z. and Fogel, D. B. (2000).
How to Solve It: Modern Heuristics.
Springer Verlag.

References
437
Minsky, M. (1963). Steps toward artiﬁcial intelligence. In E. A. Feigenbaum and
J. Feldman, editors, Computers and Thoughts, pages 406–450. McGraw-Hill.
Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
Ndilikilikesha, P. C. (1994). Potential inﬂuence diagrams. International Journal of
Approximate Reasoning, 10, 251–285.
Nielsen, T. D. (2002).
Decomposition of inﬂuence diagrams.
Journal of Applied
Non-Classical Logics – Symbolic and Quantitative Approaches to Reasoning with
Uncertainty, 12(2), 135–150.
Nielsen, T. D. and Jensen, F. V. (1999). Welldeﬁned decision scenarios. In Pro-
ceedings of the 15th Annual Conference on Uncertainty in Artiﬁcial Intelligence
(UAI-99), pages 502–551, San Francisco, CA. Morgan Kaufmann.
Nielsen, T. D. and Jensen, F. V. (2003a).
Representing and solving asymmetric
decision problems. International Journal of Information Technology & Decision
Making, 2(2), 217–263.
Nielsen, T. D. and Jensen, F. V. (2003b). Sensitivity analysis in inﬂuence diagrams.
IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and
Humans, 33(2), 223–234.
Nilsson, D. and Lauritzen, S. L. (2000). Evaluating inﬂuence diagrams using LIMIDs.
In C. Boutilier and M. Goldszmidt, editors, Proceedings of the Sixteenth Confer-
ence on Uncertainty in Artiﬁcial Intelligence, pages 436–345. Morgan Kaufmann
Publishers.
Olesen, K. G., Lauritzen, S. L., and Jensen, F. V. (1992). ahugin: A system creating
adaptive causal probabilistic networks. In Proceedings of the Eighth Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), pages 223–229.
Olmsted, S. M. (1983). On representing and solving decision problems. Ph.D. thesis,
Department of Engineering–Economic Systems, Stanford University.
Pearl, J. (1982). Reverend Bayes on inference engines: A distributed hierarchical
approach. In Proceedings of the First National Conference on Artiﬁcial Intelli-
gence, pages 133–136. The AAAI Press.
Pearl, J. (1986). Fusion, propagation, and structuring in belief networks. Artiﬁcial
Intelligence, 29(3), 241–288.
Pearl, J. (1988).
Probabilistic Reasoning in Intelligent Systems.
Representation
and Reasoning. Morgan Kaufmann Publishers, San Mateo California.
ISBN
0-934613-73-7.
Pearl, J. (2000). Causality: Models, Reasoning and Inference. Cambridge University
Press. ISBN 0-521-77362-8.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic
Programming. John Wiley & Sons, Chichester, UK.
Quinlan, J. R. (1979). Discovering rules by induction from large collections of ex-
amples. In D. Michie, editor, Expert Systems in the Micro Electronic Age. Ed-
inburgh University Press.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81–106.
Raiﬀa, H. (1968). Decision Analysis, Introductory Lectures on Choices under Un-
certainty. Addison-Wesley.
Raiﬀa, H. and Schlaifer, R. (1961). Applied Statistical Decision Theory. MIT press,
Cambridge.
Rissanen, J. (1987). Stochastic complexity. Journal of the Royal Statistical Society,
Series B, 49(3), 223–239. With discussions.
Rubin, D. B. (1976). Inference and missing data. Biometrika, 63(3), 581–592.

438
References
Russell, S. J., Binder, J., Koller, D., and Kanazawa, K. (1995). Local learning in
probabilistic networks with hidden variables. In Proceedings of the Fourteenth
International Joint Conference on Artiﬁcial Intelligence, pages 1146–1152.
Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics, 6,
461–464.
Shachter, R. D. (1986). Evaluating inﬂuence diagrams. Operations Research, 34(6),
871–882.
Shachter, R. D. (1999). Eﬃcient value of information computation. In K. B. Laskey
and H. Prade, editors, Proceedings of the Fifteenth Conference on Uncertainty
in Artiﬁcial Intelligence, pages 594–601. Morgan Kaufmann Publishers.
Shachter, R. D. and Peot, M. A. (1990). Simulation approaches to general proba-
bilistic inference on belief networks. In M. Henrion, R. Shachter, L. Kanal, and
J. Lemmer, editors, Proceedings of the Fifth Annual Conference on Uncertainty
in Artiﬁcial Intelligence, pages 221–234. North-Holland.
Shafer, G. (1996). Probabilistic Expert Systems. Society for Industrial and Applied
Mathematics, Philadelphia.
Shafer, G. R. and Shenoy, P. P. (1990). Probability Propagation. Annals of Mathe-
matics and Artiﬁcial Intelligence, 2, 327–352.
Shapley, L. S. (1953). Stochastic games. Proceedings of the National Academy of
Sciences, 39, 1095–1100.
Shenoy, P. P. (1992). Valuation-based systems for Bayesian decision analysis. Op-
erations Research, 40(3), 463–484.
Shenoy, P. P. (1996). Representing and solving asymmetric decision problems using
valuation networks. In D. Fisher and H.-J. Lenz, editors, Learning from Data:
Artiﬁcial Intelligence and Statistics V , volume 112 of Lecture Notes in Statistics,
pages 99–108. Springer-Verlag.
Spiegelhalter, D. J. and Knill-Jones, R. P. (1984). Statistical and knowledge-based
approaches to clinical decision-support systems. Journal of the Royal Statistical
Society, Series A, 147(1), 35–77.
Spiegelhalter, D. J. and Lauritzen, S. L. (1990). Sequential updating of conditional
probabilities on directed graphical structures. Networks, 20, 579–605.
Spirtes, P., Glymour, C., and Sheines, R. (1993). Causation, Prediction and Search.
Lecture Notes in Statistics. Springer-Verlag.
Spirtes, P., Glymour, C., and Sheines, R. (2000). Causation, Prediction and Search.
MIT Press, Cambridge, Massachusetts, second edition.
Spohn, W. (1980). Stochastic independence, causal independence, and shieldability.
Journal of Philosophical Logic, 9, 73–99.
Steck, H. (2001). Constrained-based structural learning in Bayesian networks using
ﬁnite data sets. Ph.D. thesis, Institut f¨ur Informatik der Technischen Universit¨at
M¨unchen.
Suermondt, H. J. (1992). Explanation in Bayesian Belief Networks. Ph.D. thesis,
Knowledge Systems Laboratory, Medical Computer Science, Stanford University,
California. Report No. STAN-CS-92-1417.
Tatman, J. A. and Shachter, R. D. (1990). Dynamic Programming and Inﬂuence
Diagrams. IEEE Transactions on Systems, Man and Cybernetics, 20(2), 365–
379.
Titterington, D. M. (1976). Updating a diagnostic system using unconﬁrmed cases.
Applied Statistics, 25(3), 238–247.

References
439
Verma, T. (1987). Causal networks: Semantics and expressiveness. In Proceedings
of the Third Workshop on Uncertainty in Artiﬁcial Intelligence, pages 352–359.
Elsevier Science Publishers, New York.
Verma, T. and Pearl, J. (1991). Equivalence and synthesis of causal models. In Un-
certainty in Artiﬁcial Intelligence 6, pages 255–268. Elsevier Science Publishers
B.V.
Vomlelov´a, M. (2003). Complexity of decision-theoretic troubleshooting. Interna-
tional Journal of Intelligent Systems, 18(2), 267–277.
von Neumann, J. and Morgenstern, O. (1944).
Theory of Games and Economic
Behavior. John Wiley & Sons, New York, ﬁrst edition.
Wermuth, N. and Lauritzen, S. L. (1990).
On substantive research hypotheses,
conditional independence graphs and graphical chain models (with discussion).
Journal of the Royal Statistical Society, 52, 21–72.
Zhang, N. L. (1998). Probabilistic Inference in Inﬂuence Diagrams. In G. F. Cooper
and S. Moral, editors, Proceedings of the Fourteenth Conference on Uncertainty
in Artiﬁcial Intelligence, pages 514–522. Morgan Kaufmann Publishers.

Index
λ-message
153
π-message
153
A-saturated junction tree
169, 170
action sequence
373
acyclic directed graph
33
adaptation
83, 207
to structure
214
adjacent node
119
algebra of potentials
13
Allais’ paradox
289
analysis
data conﬂict
98
relevant future
419
required past
415
SE
99, 179
sensitivity
99, 184, 420
value of information
407
ancestral graph
32
associative law
13
asymmetric decision problems
310
functional asymmetry
315
order asymmetry
315
structural asymmetry
315
attribute
encapsulated
86
input
86
output
86
barren node
112, 130
rule
130
batch learning
195
Bayes’ factor
180
Bayes’ rule
5
for variables
10
Bayesian estimation
197
Bayesian information criterion
243
Bayesian network
33
dynamic
91
hybrid
95
object-oriented
84, 85
parameters
60
size
240
Bayesian score function
253
belief state
388
BIC
243
bucket elimination
41
BUGS
156
burn-in
151
call service
378
causal network
26, 26
causality
60
chain graph
74
chain rule
35
for Bayesian networks
36
for inﬂuence diagrams
345
general
36
chaining
24
chance node
305
chance variable
33
chance-variable representation
360
chord
161
Chow–Liu tree
250
learning of
250
class variable
265
classiﬁcation accuracy
268

442
Index
classiﬁcation tree
272
classiﬁer
accuracy
268
classiﬁcation tree
272
confusion matrix
268
evaluation of
268
naive Bayes
266
tree augmented naive Bayes
270
clique
118
collect evidence
126
commutative law
13
complete case
195
complete set of nodes
118
computation tree
141
conditional Gaussian distribution
95
conditional independence
6
for variables
10
conditional probability
4
for variables
8
conditioning
164
conﬁguration of maximal probability
171
conﬂict
data
98, 174
local
177
measure
99, 175
partial
177
confounding variable
240
confusion matrix
268
connected graph
singly c. graph
162
connection
converging
28
diverging
27
serial
26
constraint variable
74
constraint-based learning
230
continuous variable
93
converging connection
28
convex function
412, see value
function
crucial evidence
181
crucial ﬁnding
183
cycle
161
d-connected
30
d-separation
26, 30, 131
DAG
33
neighborhood
245
data conﬂict
98, 174
decision
action
279
node
305
scenario
290
test
279
tree
see decision tree
decision tree
290
coalesced
295
no-forgetting
290
strategy
296
decision variable
283
chance -variable representation
360
decision/classiﬁcation tree
272
default potential
90
density function
16
directed graph
26
distribute evidence
126
distributive law
14, 174
for max
172
diverging connection
27
divorcing
78
domain
ﬁnite-horizon
92
inﬁnite-horizon
92
domain graph
116
domain of variable
13
domain set
118
dynamic Bayesian network
91
time slice
91
ECR
see troubleshooting
eﬀective sample size
211
elimination
bucket
41
of variable
116
order
110
variable
353
elimination order
strong
353
elimination sequence
perfect
117
EM algorithm
201, 206
entropy
412
equivalence class search
248
Euclidean distance
219
evaluation of classiﬁers
268
event
2
hypothesis
51

Index
443
evidence
39
collect
126
crucial
181
distribute
126
hard
131
important
181
likelihood
40
minimal suﬃcient
181
redundant
181, 190
sensitivity to
167
simple
374
suﬃcient
181, 190
expectation step
201
expected beneﬁt
409
expected proﬁt
409
expected utility
281, 346
maximal
350, 396
expected value
15, 409
expert disagreements
81
explaining away
28
explanation
167
most-probable
98
fading
211
faithful sample
237
false negative
18, 60
false positive
18, 60
ﬁll-in
117
ﬁnding
40
crucial
183
ﬁnite-horizon domain
92
fractional updating
210
frequency function
16
full junction tree
128
fundamental rule
5
for variables
9
Gaussian distribution
94
general chain rule
36
Gibbs sampling
150
burn-in
151
global independence
195
gradient descent
219
graph
acyclic directed
33
ancestral
32
chain
74
domain
116
moral
116
nontriangulated
132
singly connected
162
triangulated
119
triangulation of
134
graphical model
43
greedy approach
375
greedy equivalence search
248
h-saturated junction tree
182
hard evidence
131
hidden Markov model
92
hidden variable
200
history
319
history variable
309
horizon
ﬁnite h. domain
92
inﬁnite h. domain
92
hybrid Bayesian networks
95
hypothesis event
51
hypothesis variable
51
I-equivalence
48
I-submap
48
IEJ tree
168
important evidence
181
incremental updating
215
independence
6
conditional
see conditional
independence
global
195
local
195
marginal
11
structural
30
inﬁnite-horizon domain
92
inﬂuence diagram
302, 305
chain rule for
345
limited memory
392
no-forgetting
306
optimal policy
307
optimal strategy
307
partially observable Markov decision
process
308
policy
307, 307
policy network
360
realized
305
relevant future
419
required past
358, 415
solution
308
strategy
307, 308

444
Index
information
blocking
309
hiding
86
link
305
variable
52
inheritance
88
inhibitor
77
initial sample size
212
instantiated potential
41
instantiated variable
26
instrumental rationality
287
axioms
287
intervention
96
join tree
122
joint probability
8
joint probability table
8, 98
junction tree
124
A-saturated
169, 170
full
128
h-saturated
182
strong
355
Kalman ﬁlter
92
Kullback-Leibler divergence
219
latent variable
200
law
associative
13
commutative
13
distributive
14, 174
distributive for max
172
lazy propagation
127
LBP
152
likelihood
59, 196
evidence
40
marginal
254
normalized
177
weighting
148
LIMID
see limited memory inﬂuence
diagram
limited memory inﬂuence diagram
392
single policy updating
394
link
information
305
moral
116
temporal
92
local independence
195
local conﬂict
177
loopy belief propagation
152
λ-message
153
π-message
153
lottery
287
lower neighborhood
248
mailbox
124
MAR
200
marginal likelihood
254
marginalization
9
marginalize
115
marginally independent
11
Markov
blanket
30
chain
92
hidden model
92
property
52
Markov decision process
324, 326
average reward
330
discounting factor
329, 329
nonstationary strategy
328
policy iteration
see policy iteration,
387
stationary strategy
328
terminal state
325
value iteration
381, 382
max-marginal
172
max-propagation
172
maximal expected utility
350, 396
maximization step
201
maximum a posteriori parameters
199, 201
maximum likelihood estimation
196
MCAR
200
MDP
see see Markov decision process
mean value
15
mediating variable
56
message passing
127
metric
219
minimal suﬃcient evidence
181
missing at random
200
missing completely at random
200
moral graph
32, 116
moral link
116
most-probable explanation
98
MPE
98
multilinear polynomial function
186
myopic repair strategy
379

Index
445
myopic value of information
409
naive Bayes
58
classiﬁer
266
tree augmented
270
NBC
267
necessary path condition
237
negative
false
18, 60
neighborhood
245, 248
lower
248
upper
248
network
Bayesian
33
causal
26
fragment
84
network fragment
84
instantiate
85
no-forgetting
290, 306
node
adjacent
119
barren
112, 130, 362
barren n. rule
130
chance
305
decision
305
misplaced
368
simplicial
119
utility
281, 305
node removal and arc reversal
362
arc reversal
365
removal of barren nodes
362
removal of chance nodes
362
removal of decision nodes
362
noisy functional dependence
80
noisy-and
78
noisy-or
75
nontriangulated graph
132
normal distribution
94
normalized likelihood
177, 180
normative approach
vi
object-oriented Bayesian network
84,
85
attribute
see attribute
interface
87
Ockham’s razor
240
OOBN
see object-oriented Bayesian
network
order
elimination
110
perfect elimination
117
overﬁtting
230, 257
parameters
60
partial conﬂict
177
partially observable Markov decision
process
330
observation function
331
path
161
PC algorithm
235
perfect elimination sequence
117
policy
307, 307
decision
320
optimal
307
step
320
policy iteration
385, 387
policy evaluation
386
policy improvement
386
policy network
360
POMDP
see partially observable
Markov decision process
positive
false
18, 60
potential
13, 43
default
90
instantiated
41
pre-J -tree
165
principle of maximum likelihood
196
probabilistic logic sampling
146
projection operator
174
propagation
lazy
127
loopy belief
152
variable
169
proportional scaling
185
question
378
random variable
15
recursive conditioning
140
cutset
143
red herring
175
redundant evidence
181, 190
relevant future
419
relevant utility node
415
repetitive temporal model
92
required past
358, 415
required variable
353, 414

446
Index
rule
barren node
130
chain
35
S-DAG
319
decision policy
320
dominating path
369
history
319
misplaced node
368
optimal
322
step policy
320
step strategy
320
strategy
320
sample size
210
eﬀective
211
initial
212
satisﬁability problem
107
scaling
proportional
185
score equivalent
248
score function
242
Bayesian
253
SE analysis
99, 179
search
equivalence
248
greedy
246
operator
245
second-order uncertainty
207
sensitivity analysis
99, 184, 420
decision sensitivity
420
value sensitivity
420
sensitivity to evidence
167
separator
123
sequential inﬂuence diagram
322
guard
322
open link
322
structural link
322
serial connection
26
SID
see sequential inﬂuence diagram
simplicial node
119
single fault assumption
376
singly connected graph
162
size
Bayesian network
240
eﬀective sample
211
initial sample
212
sample
210
skeleton
231
solution
308, 334
stochastic simulation
145
strategy
296, 307, 308, 320, 334, 346
myopic repair
379
optimal
296, 307, 322
step
320
strictly repetitive model
92
strong elimination order
353
strong junction tree
355
strong root
357
strong triangulation
355
structural independence
30
subclass
88, 90
subjective probabilities
1
suﬃcient evidence
181, 190
sum-propagation
172
superclass
89
surprise index
179
TAN
270
temporal link
92
temporal model
92
time slice
91
time-stamped models
137
tree
A-saturated junction
170
augmented naive Bayes classiﬁer
270
Chow–Liu
250
classiﬁcation
272
decision
see decision tree
full junction
128
IEJ
168
join
122
junction
124
triangulated graph
119
triangulation
of graphs
134
strong
355
triggered direction
128
troubleshooting
373
call service
378
expected cost of repair
374
greedy approach
375
observation step
373
question
378
repair step
373
simple evidence
374
single fault assumption
376

Index
447
strategy
see troubleshooting
strategy
troubleshooting strategy
373
eﬃciency
375
myopic
379
tuning
218
UID
see unconstrained inﬂuence
diagram
uncertain region
238
unconstrained inﬂuence diagram
316,
318
admissible order
319
free
316
no-forgetting
318
observable
316
optimal strategy
322
realized
318
released
316
S-DAG
319
strategy
see S-DAG
undirected relations
73
unit potential
13
property
13
upper neighborhood
248
utility
284
expected
346
maximal expected
350, 396
node
305
theory
284
utility node
relevant
415
v-structure
231
valuation
174
valuation axiom
174
value
expected
409
value function
409
convex
413
entropy-based
412
non-utility-based
411
utility based
409
variance-based
412
value iteration
381, 382
value of information
407
expected beneﬁt
409
expected proﬁt
409
hypothesis-driven
409
myopic
409
value function
see value function
variable
7
chance
33
class
265
confounding
240
constraint
74
continuous
93
decision
282, 283
domain of
13
elimination
42
elimination of
116
hidden
200
history
309
hypothesis
51
information
52
instantiated
26
latent
200
mediating
56
random
15
required
353, 414
variable elimination
42, 353
variable propagation
169
variance
15, 412


Studies in Computational Intelligence 1137
Roger Lee   Editor
Software 
Engineering and 
Management: 
Theory and 
Application
Volume 16

Studies in Computational Intelligence 
Volume 1137 
Series Editor 
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland

The series “Studies in Computational Intelligence” (SCI) publishes new develop-
ments and advances in the various areas of computational intelligence—quickly and 
with a high quality. The intent is to cover the theory, applications, and design methods 
of computational intelligence, as embedded in the ﬁelds of engineering, computer 
science, physics and life sciences, as well as the methodologies behind them. The 
series contains monographs, lecture notes and edited volumes in computational 
intelligence spanning the areas of neural networks, connectionist systems, genetic 
algorithms, evolutionary computation, artiﬁcial intelligence, cellular automata, self-
organizing systems, soft computing, fuzzy systems, and hybrid intelligent systems. 
Of particular value to both the contributors and the readership are the short publica-
tion timeframe and the world-wide distribution, which enable both wide and rapid 
dissemination of research output. 
Indexed by SCOPUS, DBLP, WTI Frankfurt eG, zbMATH, SCImago. 
All books published in the series are submitted for consideration in Web of Science.

Roger Lee 
Editor 
Software Engineering 
and Management: Theory 
and Application 
Volume 16

Editor 
Roger Lee 
Software Engineering and Information 
Technology Institute 
Central Michigan University 
Mount Pleasant, MI, USA 
ISSN 1860-949X
ISSN 1860-9503 (electronic) 
Studies in Computational Intelligence 
ISBN 978-3-031-55173-4
ISBN 978-3-031-55174-1 (eBook) 
https://doi.org/10.1007/978-3-031-55174-1 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
Paper in this product is recyclable.

Contents 
Changing Beauty Industry’s Strategic: Focused on Metaverse 
and Generation MZ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 
Youngju Kim and Hyeil Jung 
ESG News Analysis Using News Big Data: Focusing on Topic 
Modeling Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15 
Hang Ju Seo, Dong Hyuk Jo, and Zhi Pan 
Effects of the Perceived Beneﬁt and Innovativeness of Live 
Commerce on Intention to Use: Focus on Chinese Consumers . . . . . . . . .
29 
Zhi Pan, Dong Hyuk Jo, and Su Young Kim 
A Study on the Effect of Customer Experience on the Intention 
to Continuously Use Coffee Store Mobile App Based on the Uniﬁed 
Theory of Acceptance and Use of Technology (UTAUT) Model . . . . . . . . .
41 
Sumin Han, Myeongsook Park, Hyojin Yook, and Sungtaek Lee 
A Study on Parallel Recommender System with Stream Data 
Using Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55 
Thin Nguyen Si, Trong Van Hung, Dat Vo Ngoc, and Quan Ngo Le 
Using Incremental Algorithm in Hybrid Recommender System 
Combined Sentiment Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69 
Thin Nguyen Si and Trong Van Hung 
A Bottom-Up Generic Probabilistic Building and Enriching 
Approach for Knowledge Graph Using the LDA-Based Clustering 
Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81 
Amani Mechergui, Sami Zghal, and Wahiba Ben Abdessalem Karaa 
Measuring the Effects of Signal-To-Noise in EEG Emotion 
Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103 
Zachary Estreito, Vinh Le, Jr. Frederick Harris, and Sergiu Dascalu
v

vi
Contents
Phishy? Detecting Phishing Emails Using Machine Learning 
and Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119 
Md. Fazle Rabbi, Arifa I. Champa, and Minhaz F. Zibran 
Are We Aware? An Empirical Study on the Privacy and Security 
Awareness of Smartphone Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139 
Arifa I. Champa, Md. Fazle Rabbi, Farjana Z. Eishita, 
and Minhaz F. Zibran 
Using Effective Dummy Locations and Routes to Conceal User 
Locations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159 
Sanjaikanth E Vadakkethil Somanathan Pillai and Wen-Chen Hu 
Anomalous Node Detection in a Graph Considering Text Features . . . . .
173 
Yeonju Song and Ki Yong Lee 
Robust Federated Learning: A Heterogeneity Index Based 
Clustering Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185 
Papa Pene, Pu Tian, Weixian Liao, Qianlong Wang, and Wei Yu 
Evaluation of Improvement Plans to Increase the Efﬁciency 
of Performance Data Collection/Transfer for Server Systems . . . . . . . . . .
203 
Chika Iiyama, Akira Hirai, Mari Yamaoka, Naoto Fukumoto, 
and Masato Oguchi 
A Learning Enhancement System for Learner’s Community . . . . . . . . . . .
219 
Yuta Ishii, Ayako Sugiyama, Kosuke Fukushima, Ryotaro Okada, 
and Takafumi Nakanishi 
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237

Changing Beauty Industry’s Strategic: 
Focused on Metaverse and Generation 
MZ 
Youngju Kim and Hyeil Jung 
Abstract The South Korean cosmetics industry is loved by the whole world in the 
name of ‘K-beauty’ together with the Hallyu culture, and has been established as 
one of South Korea’s key export items. In particular, lots of changes occur rapidly 
due to the development of telecommunication and the introduction of new tech-
nologies, and also changes caused by COVID-19 have brought about great changes 
in all the areas of distribution and sales. In addition, the consumption of the MZ 
generation shows different patterns from the existing consumer generations, and a 
new change in distribution and sales start in earnest according as their inﬂuence 
grows. While metaverse platform services, which are yet in their early stages, are 
now used mostly by entertainment area and global large companies, it is deemed that 
continuous investment and technological innovation will accelerate the expansion 
of metaverse platforms. Currently, response to cosmetics industry in the metaverse 
platform market is trivial, but cosmetics industry will also have to provide services 
through the market of this new type before long. For this, it should understand the 
consumption tendency of the main consumers, and grope for marketing strategies 
responding to it. The strategies are as follows: First, the consumption patterns of 
the MZ generation, which will grow as a main consumer group in the future though 
they are not a current main consumption group, should be understood, and marketing 
methods should be developed in agreement with consumption patterns pursued by 
them. Second, PR and marketing embracing the values of the MZ generation, such 
as environment and value-based consumption should be carried out, and services of 
real-time communication through stories sympathized by the MZ generation should 
be provided. Third, while ofﬂine experience is important, the marketing of expe-
rience in metaverse platforms should be provided, and goods that enable two-way 
communication and simultaneous enjoyment should be provided by planning and 
developing goods that allow linkage between metaverse and ofﬂine, Lastly, it is
Y. Kim envelope symbol
SookMyung Woman’s University Seoul, Seoul, South Korea 
e-mail: zomein@sookmyung.ac.kr 
H. Jung 
Cosmedia.Co.Ltd, Seoul, South Korea 
e-mail: cosmedia@kcosmedia.com 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_1 
1

2
Y. Kim and H. Jung
deemed that strategies for attracting consumers by providing contents of amusement 
and fun should be used. 
Keywords Metaverse · Beauty industry · Cosmetics industry · Generation MZ ·
Introduction 
1 Introduction 
Korean beauty market has been continuously developing and growing since 1945 via 
diverse ways and means. Also, coupled with the evolving communication technology, 
continual technological shifts and advancement in the country’s beauty industry help 
scale up production and sales. 
Currently, Korean beauty industry has garnered international popularity on the 
back of the Korean Wave and ‘K-beauty’, playing pivotal roles in the country’s export. 
Thus, the beauty industry seeks to change in many aspects including production, 
distribution and sales. Most of all, distribution and sales have gone through rapid 
changes resulting from the advanced communication technology and the adoption 
of new technologies, while the global COVID-19 pandemic has brought substantial 
changes across the board including distribution and sales. The door-to-door sales 
as the primary distribution channel of beauty products in the early days contributed 
to the popularity of beauty products and the diversiﬁcation of distribution channels 
into discount stores and specialty stores, before evolving into online virtual channels 
and the metaverse. Many companies and consumers make use of new distribution 
channels for sales and consumption. Also, consumption patterns in those channels 
have evolved and developed. Currently, as the main consumers the generation MZ 
show the consumption pattern different from that of older generations. With the 
generation MZ becoming more inﬂuential, new changes in distribution and sales 
have begun. 
Hence, this paper reviews the literature and previous studies to revisit the evolution 
and advancement of beauty product distribution into the metaverse, examines the 
distribution and sales strategies focused on the generation MZ and speciﬁcally digital 
consumers, who are the main consumers in this age, and ﬁnally makes suggestions 
concerning the ideal future courses that companies need to take.

Changing Beauty Industry’s Strategic: Focused on Metaverse …
3
2 Theoretical Background 
2.1 
Cosmetics Industry 
2.1.1 
Deﬁnition of Beauty Products 
Beauty products collectively refer to the cream, powder and perfume products used 
to put on make-up. The Article 2, Sect. 1, of the Korean Cosmetics Act deﬁnes beauty 
products as the ‘items that are applied to, rubbed or sprayed on the body or used in 
equivalent ways in order to keep the body clean, beautiful and attractive, brighten 
the complexion, and keep or improve the healthy skin and hair, and that hardly cause 
side effects on the body.’ That is, beauty products refer to such items as used to keep 
the body clean, beautiful and attractive, brighten the complexion, and keep/improve 
the health of the skin and hair. Beauty products and relevant industries are currently 
included in the ﬁve big promising consumer goods and valued by the government 
as important contributors to the country’s international trade and as indispensable 
consumer staples. 
2.1.2 
State of Beauty Market 
Beauty products meet the needs of consumers for beauty, have direct/indirect effects 
on our body unlike other consumer goods, provide special beneﬁts, and are char-
acterized by the organically connected process from production to consumption 
[1]. Beauty products used to be classiﬁed as luxury items, but now as consumer 
staples, and the changing consumer awareness has fueled the rapid growth of their 
share in the consumer goods market. Also, the extensive research on new materials, 
formulations, functionality and technology, the increasing participation of women in 
economic activities and their improved social status, and the emergence of males who 
choose to put on make-up have accelerated the advancement of the beauty industry. 
Such changes have also led to the roll-outs of conceptual and personalized products 
that meet the diversiﬁed needs of consumers. 
In Korea, riding on the coattails of internationally popular Korean Wave, K-
Beauty have exerted signiﬁcant effects on the advancement of the beauty industry. 
Hence, beauty products have become consumer staples that contribute to the country’s 
economic and industrial development, increase the quality of life, meet the needs of 
consumers for beauty, and enrich people’s life. 
2.1.3 
Distribution of Beauty Products 
Distribution channels in the beauty industry have been well-documented. According 
to Oh (2003), the distribution channels of beauty products have developed from 
wholesalers to door-to-door sales and then specialty stores in the order named, and

4
Y. Kim and H. Jung
consumers, distributors and manufacturers affect the distribution channels. That is, 
in the evolution from the manufacturer-centric door-to-door sales to the consumer-
centric specialty stores, consumers not manufacturers have become central to the 
beauty product distribution channels. 
Also, Lee [1] described the four phases of the 60-year development of beauty 
product distribution channels since 1945: the ﬁrst phase was driven by wholesalers 
on behalf of manufacturers that lacked in capital, which was followed by the second 
phase when the door-to-door sales was ﬁrst introduced and grew, the third phase 
when discount stores and specialty stores boomed, and the fourth phase when the 
transition to multi-channels has engendered online stores and nonstore media and 
triggered the ongoing development in diverse ways (Table 1). 
Following the 1980s when the door-to-door sales boomed, the vendors dealing in 
multiple beauty brands developed into what is now the specialty stores in the 1990s 
touting discounts, wider ranges of assortments and convenience, and settled as a 
huge distribution channel taking up around 55% of the beauty market in 1997 [2], 
before the end of the 1990s when the popularity, market share and growth trend of 
specialty stores fell due to the intensifying price competition, distrust in discounts 
and changing customer needs [3]. Furthermore, the foreign exchange crisis aka. 
the IMF crisis, divided consumers into those prioritizing the product quality and 
those sensitive to price tags, which caused the distribution channels to diversify into 
pharmacies, large discount supermarkets and other retail channels [1]. In addition, 
with the rise of low-priced beauty products since 2002, e.g. Able Sea &’s launch of 
‘MISSHA’ advocating reasonable consumption, brand shops became well-received. 
In 2018, to meet the needs of hectic modern consumers, online and select shops 
emerged to overshadow the existing brand shops in sales, bringing another trend in 
the distribution channels.
Table 1 Beauty product distribution channels by period 
Phase Period
Description 
1st
1945~1963 Wholesalers led distribution channels on behalf of manufacturers lacking 
in capital 
2nd
1964~1985 Door-to-door sales was ﬁrst introduced and successfully grew in Korea 
3rd
1986~1995 Discount cosmetics stores and specialty cosmetics stores grew and boomed 
on the back of consumers’ rational decision making 
4th
1996~
Transition to multi-channels engendered online stores and nonstore media, 
which have developed until now in diverse ways 
Adapted from Lee [1] 

Changing Beauty Industry’s Strategic: Focused on Metaverse …
5
2.2 
Metaverse 
The term ‘metaverse’ ﬁrst appeared in Neal Stephenson’s novel ‘Snow Crash’ 
published in 1992. It is a compound word with ‘meta’ and ‘verse’ meaning ‘beyond’ 
and ‘universe,’ respectively. It is a new 3D virtual world transcending the reality, 
where avatars engage in activities on behalf of users. In the early 2000s, with the 
advent of the 3D virtual world game ‘Second Life’ and Google’s map service ‘Google 
earth’, 3D web drew much attention to the extent that the discussion on the metaverse 
commenced as a new future space where users can engage in social and economic 
activities [4]. 
As the concept of the metaverse has been applied in further depth, many different 
industries have been developing technologies and platforms using the metaverse 
(Table 2) , which has taken business operations to the next level across the board. 
According to Vibe Company specializing in analyzing the trends based on social 
big data, there has been the most explosive increase in the mentioning of the word 
‘metaverse’ this year [5]. Still, there exists a generation gap in the perspectives 
on the metaverse. That is, older generations view the metaverse as experiences or 
games, whereas younger generations take the metaverse as new daily routines of 
doing real activities including education, socializing and shopping through avatars 
in the pandemic-stricken world. This is referred to as digital terraforming (creating a 
digital earth), and the younger generation is called the metaverse-natives. According 
to Univ Tomorrow Research Laboratory for the Twenties, ‘older generations make 
use of online activities while living ofﬂine, whereas the gen Z live online as they 
do ofﬂine.’ During the COVID-19 pandemic, the metaverse emerged as a virtual 
space that could replace the ofﬂine space and became the new normal to the younger 
generation [6].
Table 2 Metaverse evolution 
Trigger
Transition
Convergence 
Pre~2010s
2010~2020s
Post~2020s 
−Virtual reality games 
emerged to allow users to share 
experiences that appear to be 
real 
−Presenting the concept of 
metaverse 
Development & 
commercialization of 
immersive technology and 
products enabled the 
eco-system for sharing 
‘real-virtual’ experiences 
Virtual reality converging with 
reality combines with AI, 
immersive technology and 
networks to coexist with reality 
in real time and with continuity 
−Virtual simulation 
−Avatar 
−3D 
−Augmented reality on 
smartphones 
−AR glasses 
−VR/XR 
−5G/6G 
−AI IoT 
−Blockchain 
−3D virtual reality 
−GoogleEarth (Google) 
−Second Life (Linden Lab) 
−PokeMonGO (Niantic) 
−VR training for new staff 
(Walmart) 
−Zepeto (Naver) 
−Roblox (Roblox) 
−eFriend (SKT) 
Source Korea Creative Content Agency 

6
Y. Kim and H. Jung
Based on the attributes of games, the metaverse has emerged as a new content 
model for music and fashion. The COVID-19 pandemic has caused the metaverse 
to serve as a means of social connectedness for strengthening the social solidarity 
and the sense of belonging, not just for enjoying contents in the age of untactness. In 
Korea, Naver’s ‘Zepeto’ is a leading metaverse platform. Launched in August 2018 in 
165 countries around the world, ‘Zepeto’ is a global AR avatar platform and a leading 
metaverse platform in Korea, rapidly growing through relentless collaborations with 
the entertainment industry. As of December 2020, ‘Zepeto’ boasts 200 million global 
subscribers, 80% of whom are teens, with overseas users accounting for 90% [7]. 
2.3 
Generation MZ 
The M for generation MZ refers to millennials, or those who were born in the new 
twenty-ﬁrst century as mentioned ﬁrst by William Strauss and Neil Howe in their 
writing [8]. 
Millennials are the digital pioneers having gone through the transition to the 
digital age and experienced both analog and digital eras, which distinguishes them 
from the generation Z, who were born and have grown up in digital environment. The 
generation Z are different from the generation M in that the former have grown up in 
digital environment for life as digital natives skillful at using online platforms and IT 
devices in every aspect of life including shopping and consumption [9]. Also, their 
consumption pattern evolves in a different direction from that of the older generation, 
and the space where they consume objects varies and evolves as well (Fig. 1). 
Fig. 1 Metaverse roadmap. Source ASF, Metaverse Roadmap Overview, p. 5, 2007

Changing Beauty Industry’s Strategic: Focused on Metaverse …
7
3 Result and Discussion 
3.1 
Characteristics of Current Beauty Industry 
3.1.1 
Characteristics of Domestic Beauty Industry 
Struggling with challenges including sluggish consumption in domestic market 
resulting from the prolonged pandemic, Korean beauty industry needs to develop 
proper strategies to adapt their operations to the new normal, to turn the crisis into a 
breakthrough in line with the new environment and changing trends, to reinvigorate 
domestic demand and to reinforce the competitive advantage of K-beauty. Above all, 
with the untact culture widely adopted, retail channels have seen the untact consump-
tion increasing on online and mobile platforms. At the same time, as people have to 
wear face masks for long hours to prevent any viral infection, the demand for color 
makeup products has decreased whereas the demand for skin care and protection and 
thus the importance of eco-friendly materials have increased. Also, the global value 
chain that used to pursue efﬁciency via international division of labor encountered 
structural limitations including border closures, which has reafﬁrmed the beneﬁts of 
building onshore value chains and localizing the raw materials of beauty products 
that used to rely on imports. Refraining from going and dining out and preferring 
food delivery services have substantially increased the use of disposable products 
and plastic wares, raising concerns over environmental contamination. Particularly, 
the environmental impact of chemical ingredients and hard-to-recycle packages of 
beauty products has raised the consumer awareness of eco-friendly beauty products, 
and increased green consumers, who value the clean beauty and the philosophy and 
ethics of manufacturers, which highlights the importance of value consumption [10]. 
3.1.2 
Characteristics of Overseas Beauty Industry 
Beauty brands in the US are turning to Tik Tok from Facebook, YouTube and Insta-
gram for marketing campaigns. Notably, video-based SNS platform Tik Tok has 
been successfully appealing to the generation Z in the US, playing pivotal roles as a 
leading platform for beauty information. To address the challenges of attracting 
customers to their ofﬂine stores due to the pandemic, companies has opted for 
e-commerce platforms and third-party logistics (outsourcing logistics partially or 
entirely to specialized logistics providers). Small-scale beauty brands take advan-
tage of e-commerce platforms and/or third-party logistics services to sell products 
directly to consumers online. The effective targeting using the algorithm-based social 
media ads has facilitated such online retail trends [11].

8
Y. Kim and H. Jung
3.2 
Characteristics of Beauty Platforms 
3.2.1 
Evolving Beauty Platforms 
Noting the increasing importance of digital platforms amid the substantial growth of 
online shopping and consumption, companies have been focusing their investment 
in developing platforms using AI (Artiﬁcial Intelligence), AR (Augmented Reality) 
and VR (Virtual Reality) to allow users to have vicarious experiences of their beauty 
items, e.g. Lullulaeb’s untact AI skin test smart mirror and Sephora’s AR app Virtual 
Artist. Leading OEM/ODM companies in Korea draw on state-of-the-art digital tech-
nologies including AI and big data to build new beauty platforms and extend their 
operations to OBM (Original Brand Manufacturing) services supporting branding, 
marketing and overseas market entries, not just focusing on product development 
and production. 
3.2.2 
Characteristics of Successful Platforms 
The advancement of digital technology has given rise to the digital beauty consumers 
who shop beauty products online and produce and sell information via digital media. 
The ever-evolving digital platforms and social media add to the volume of online 
information transactions, while reducing the transaction cost and thereby enhancing 
the availability and convenience of information for the beneﬁt of consumers. 
Under such conditions, consumers accumulate their consumption experiences of 
a wide range of beauty brands and products, transforming themselves into digital 
beauty consumers who willingly share and utilize such experiences. Subsequently, 
consumers jump onto digital platforms, where they search different products, 
compare the prices thereof, stay informed of products and have vicarious experiences 
prior to making ﬁnal decisions on beauty products [12]. 
3.3 
State of the Metaverse 
3.3.1 
Metaverse Applications 
According to the ‘Trends of Content Use in 2021 Digital Transformation Era’ 
published by the Korea Creative Content Agency, 60.6% of metaverse users encoun-
tered the metaverse within the preceding year, and in 2021 Korean metaverse users 
spent an average of around 87.1 min a day. Also, Animal Forest was found to be the 
most popular metaverse platform among Korean users (56.3%) followed by Zepeto 
(47.6%), Minecraft (37.9%) and Roblox (30%) (1,000 respondents, select-all-that-
apply lists). Moreover, by age group, teens spent an average of around 94 min a day

Changing Beauty Industry’s Strategic: Focused on Metaverse …
9
Fig. 2 Reasons to join the metaverse. Source Korea Creative Content Agency 
and those in their twenties spent an average of 90 min a day on metaverse platform 
services [13] (Fig. 2). 
3.3.2 
Shopping Experiences in the Metaverse 
According to the Korea Creative Content Agency’s survey results (242 respondents), 
around 34.6% of those respondents shopped in the metaverse (Fig. 3), spending 
an average of around KRW 10,670.3(71.1%) on game items followed by KRW 
7,725.9(52.9%) on accessorizing or growing their avatars. As for the satisfaction with 
products offered in collaboration with businesses among the experienced metaverse 
shoppers, they were satisﬁed most with the content-related items (51.6%), followed 
by fashion items (47.9%) and artistic items (34.5%) [13] (Table 3). 
Table 3 Domestic beauty platform services 
In metaverse purchasing experience
Purchase details(%)
One-time spending an average 
cost 
YES 
34.6 
NO 
65.4 
Game item(71.1)
₩10,670.3 
Related avatar(52.9)
₩7,725.9 
Unrelated avatar(15.3)
₩19,140.5 
Source Korea Creative Content Agency (2021)

10
Y. Kim and H. Jung
3.4 
Inﬂuence of the Metaverse on Beauty Industry 
3.4.1 
Beauty Companies Entering the Metaverse 
Many beauty companies have been making a foray into the newly emerging meta-
verse, e.g. AMORE and LG H&H. Some have opened virtual stores on metaverse plat-
forms and others have ﬁled patent and trademark registrations for their virtual prod-
ucts as a means of broadening their spheres while simultaneously waging marketing 
campaigns. 
Amore 
AMORE has launched its HERA, Etude and Sulwhasoo on Naver’s metaverse Zepeto. 
‘HERA’ has opened its pop-up store including a VR hall presenting its limited edition 
‘Wish Rocket Collection’ since Jan. 7 2022 on Zepeto in collaboration with an 
illustrator and artist Sangho Bang. The Wish Rocket Collection provides a makeup 
zone, a photo zone and a weightless library, and holds diverse events. Currently, 
HERA Village is operating. AMORE’s ‘Etude’ launched diverse items including 
makeup look and clothing in the item shop on Zepeto, and opened ‘Etude Virtual 
Flagship Store,’ a virtual version of its ofﬂine store in World Map and a photo booth. 
Users can try on the virtual Etude products and use the brand’s pink items to create 
their own avatars, while diverse games and events provide users with fun factors and 
enhance the marketing effects by promoting products. Also, ‘Sulwhasoo’ presents 
diverse items and try-ons via the items created in collaboration with Zepeto. 
Lg H&H 
LG H&H has opened the ‘belif Universe’ on the global metaverse platform Zepeto, 
implementing the world view of its brand belif. In line with the digital competency 
initiative set forth by the company earlier this year, LG H&H has brought its brand 
‘belif’ targeted at the generation MZ at the forefront of its new digital story-telling 
marketing revolving around the brand’s world view. In March, LG H&H issued the 
NFTs for the ﬁrst time in the domestic beauty industry, releasing the ‘belif Universe 
Collection’ presenting the brand’s world view in the form of story-telling. The belif 
brand manager at LG H&H says, “Fun factors and story telling connecting the value 
of diversiﬁed experiences matter in digital marketing targeted at the generation MZ, 
who constitute the major consumer layer of the brand,” adding “The belif Universe 
is not just another brand and product PR but also a new opportunity for potential 
customers to experience the brand via diversiﬁed digital contact points [15] (Fig. 3).

Changing Beauty Industry’s Strategic: Focused on Metaverse …
11
Fig. 3 Beauty stores on Zepeto (ex: AMORE, LG H&H) [14] 
3.5 
Inﬂuence of Generation MZ on Beauty Industry 
3.5.1 
Generation MZ’s Status and Pattern as Consumers 
Growing up with the advancement of digital technology, the generation MZ have 
emerged as the main force of economic activities including consumption, and thus 
the effects of their characteristic lifestyle and changing consumption behavior on 
economy have been drawing much attention Fig. 4 [17]. However, the economic 
signiﬁcance of the generation MZ has not been well-documented in Korea [16]. 
The generation M constitute the digital natives, characterized by their consump-
tion propensity distinct from older generations including their emphasis on personal 
preferences and tastes [18]. The generation M shop online mainly by means of digital 
devices and SNS, actively express personal tastes and preferences, and respond sensi-
tively to other consumers’ experiences. The generation M tend to ﬁnd the electronic
Fig. 4 Global spend power outlook by generation 

12
Y. Kim and H. Jung
word-of-mouth more reliable than traditional marketing ads [19]. Also, the generation 
M have evolved into prosumers who are directly and indirectly involved in product 
development, production and retail processes in digital economy, far from remaining 
the consumers focusing only on consumption in analog economy. In addition, the 
generation M are sensitive to new technologies and changing political, economic and 
cultural trends, and thus less loyal to brands than older generations [20]. 
3.5.2 
State and Pattern of Beauty Product Consumption 
With the untact consumption being part of our daily life due to the pandemic, the 
domestic beauty industry opts for online distribution channels and combined on-/off-
line distribution channels while doubling down on the digital marketing strategies. In 
particular, e-commerce channels grew by 26.4% per annum over the past ﬁve years, 
accounting for 34.7% of all retail channels dwarﬁng the other channels [10]. 
4 Conclusion 
Currently, the content industry involving the games such as Roblox, Fortnite and 
Zepeto, performances and social media platforms plays pivotal roles in spreading 
the metaverse. Concerts and fan meetings that used to be held only ofﬂine have now 
turned to the metaverse platforms, where users themselves create and sell contents 
and most of all engage in actual economic activities in combination with the block-
chain technology. Still in its infancy, the block-chain technology market is primarily 
used in the entertainment industry but projected to be utilized extensively across 
diverse industries. Continual investment and technical innovation by global enter-
prises and increasing platforms that meet the needs of users will add to the spread of 
the metaverse. As the market is starting to develop, it is crucial to make use of the 
metaverse to lay the foundation for the industry and facilitate the market. Despite the 
metaverse platform market’s lukewarm response to the beauty industry’s distribution 
and sales, the beauty industry needs to enter the market so as to provide consumers 
with new services, given that a wide range of businesses tap into the metaverse plat-
form market. To that end, it is necessary to take the following strategic approaches 
to the metaverse: 
First, it is crucial to identify the consumption patterns of the generation MZ, who 
are not the major consumer layer as of now but will become one in near future, and 
to develop marketing strategies appealing to the generation MZ in accordance with 
their consumption pattern. 
Second, it is necessary to roll out promotional and marketing campaigns consistent 
with the value of the generation MZ such as the environmental awareness and value 
consumption and provide real-time interactive services via stories that the generation 
MZ empathize with.

Changing Beauty Industry’s Strategic: Focused on Metaverse …
13
Third, although the ofﬂine experience is important, it is necessary to give 
consumers the opportunity to experience the metaverse platforms and the experi-
ential marketing, so as to plan and develop some products that link the metaverse 
with the ofﬂine world, support the two-way communication and carry some fun 
factors [21]. 
Finally, it is desirable to provide contents that are worth playing with and add fun 
factors to strategically attract consumers. 
Some may take a skeptical stance against the metaverse. Yet, once the generation 
who have experienced the virtual reality since birth come to the fore, a range of 
unprecedented activities and businesses may emerge. It is time to go beyond online 
and delve into how to implement the real world in the metaverse [22]. 
References 
1. Lee JH (2009) A study on the historical evolution of channel structure of the cosmetics industry 
in Korea. Korean Acad Bus History 24(4):157–175 
2. Oh SJ (2013) Building-up strategies for competitiveness of specialty stores distribution 
channels in the cosmetics industry. Lead Futur Agendas Bus & Society 7(1):27–45 
3. Korea Health Industry Development Institute Act (2006) 
4. Han SY (2021) Current status of metaverse platform and its prospctive. Future Horizon:19–24 
5. Shin SJ et al. (2022) Trend note, BOOKSTONE, 2021.10 
6. ‘Millennials Generation Z Trends. Univ Tomorrow Research Laboratory for the Twenties, 
2021.10 
7. Chae DH, Lee SH, Song J, Lee YH (2021) Metaverse and contents. Korean Creative Content 
Agency. KOCCA Focus 13 
8. Sin YR (2021) A study on identity in metaverse discourse: focusing on ZEPETO. J Korean 
Lang Culture 76:249–278 
9. Yu JM (2020) Classiﬁcation of generation MZ utility for function of mobile ﬁnancial services: 
Based on KANO model. Kookmin University, Graduate School of Techno Design 
10. Kim MH, Kim GS (2021) Analysis of changes by value chain in the cosmetic industry in the 
post-corona era. Health industry brief. Korea Health Industry Development Institute. 336 
11. 2021 Global cosmetics focus no. 2(USA, FRANCE), Korea Cosmetic Industry Institute 
12. Digital beauty consumers and strategies. Global Mmarket Aanalysis 2020, 002 
13. Yoon HY Trends of content use in 2021 digital transformation Era. Korean Creative Content 
Agency. 2021/11/15 
14. One Beauty, [Metaverse meets beauty] The beginning of new beauty! Experience another world, 
the metaverse, 2022. 02.28. https://m.post.naver.com/viewer/postView.naver?volumeNo=333 
67565&memberNo=25590338 
15. ‘LGH&H Co., bilif (bilif herb shop) open store in Metaverse’, Maeil Business News Korea, 
2022.05.23. https://www.mk.co.kr/news/business/view/2022/05/454425 
16. Choi YJ (2022) Current status of generation MZ and its characteristics. Bank of Korea economic 
research Institute. BOK Issue Note. 2022(13) 
17. Wan lee J (2018) Become the millennial moment. Hana Bank Bi-Wkly Hana Financ Focus 
8(13) 
18. Moreno FM, Lafuente JG, Carreón FÁ, Moreno SM (2017) The characterization of the 
millennials and their buying behavior. Int J Mark Studies 9(5) 
19. Allsop DT, Bassett BR, Hoskins JA (2007) Word-of-mouth research: principles and applica-
tions. J Advert Res 47(4):398–411

14
Y. Kim and H. Jung
20. Bilgihan A (2016) Gen Y customer loyalty in online shopping. An integrated model of trust. 
user experience and branding. Comput Hum Behav 61:103–113 
21. Park JH (2021) The approaching metaverse era, the direction and implications of the next-
generation content industry. KIET Mon Ind Econ. 2021(5) 
22. Moon SJ, Hwang HJ (2021) A new generation in the post-pandemic era LG Business Research

ESG News Analysis Using News Big 
Data: Focusing on Topic Modeling 
Analysis 
Hang Ju Seo, Dong Hyuk Jo, and Zhi Pan 
Abstract Recently, the rapidly changing global economy and sustainable develop-
ment of society have been requiring the practice of ESG management. Therefore, the 
main purpose of this study was to suggest policy implications for the introduction, 
spread and implementation of ESG through news big data analysis regarding ESG 
management. To that end, this study collected news data on ESG management and 
analyzed key keywords and topics of ESG management by period using the topic 
modeling method to derive major agendas and implications for ESG management. 
Through this study, we intend to suggest strategic directions for successful ESG 
management of companies in the rapidly changing business environment. 
Keywords ESG · News big data · Topic modeling · LDA 
1 
Introduction 
ESG refers to environment, social responsibility, and governance, and each element 
consists of various sub-factors [1]. ESG is non-ﬁnancial elements that are emerging as 
a topic in the management of businesses and states, evaluation, and investments, and 
is a concept that sustainable development is possible only when transparent manage-
ment such as environment-friendly management, social responsibility management, 
and governance improvement are considered. As corporate and national social 
responsibilities for sustainable development become more important, many compa-
nies around the world are using ESG evaluation information. Starting with the United
H. J. Seo · Z. Pan 
Department of Business Administration, Graduate School, Soongsil University, Seoul, Korea 
e-mail: sb22332@gmail.com 
Z. Pan 
e-mail: panzhiinkorea@naver.com 
D. H. Jo envelope symbol
Department of Business Administration, Soongsil University, Seoul, Korea 
e-mail: joe@ssu.ac.kr 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_2 
15

16
H. J. Seo et al.
Kingdom in 2000, many countries such as Sweden, Germany, Canada, Belgium, and 
France introduced the mandatory ESG information disclosure system centering on 
pension funds, and the United Nations is encouraging socially responsible invest-
ments considering ESG issues through the United Nations Principles of Responsible 
Investment (UN PRI) launched in 2006. In the case of South Korea, on January 
14, 2021, the Financial Services Commission announced that ESG disclosure will 
become mandatory for companies listed on the KOSPI market with total assets of 2 
trillion won or more from 2025, and the foregoing will be expanded to all KOSPI 
listed companies from 2030. This is the beginning of the momentum for non-ﬁnancial 
environment-friendly social responsibility activities to become a major indicator to 
evaluate corporate value. In addition, Ahn et al. [2] stated that at present when social 
problems have become brimful due to the climate change that has become serious 
globally, environmental pollution and garbage problems due to plastics, and opaque 
governance, companies must ﬁnd a solution to ensure a sustainable future, under-
stand the essence of the ﬁelds of ‘society and environment’, and should be interested 
in and make investments in corporate social responsibilities to ﬁnd out how social 
responsibilities should be grafted on management to utilize them in investments. 
As such, each company’ expectations from and interest in ESG management 
are increasing drastically along with national policies, and investor interest in ESG 
investments considering corporate ESG elements comprehensively and scales of 
the investments are also rapidly expanding but the analysis and discussion of ESG 
trends based on data are lacking. Therefore, in this study, major keywords, topics, 
and changes in topics that appeared in ESG media reports will be analyzed by period 
through the LDA algorithm using news big data and the topic modeling analysis 
method, and the implications of ESG trends will be suggested based on the analysis 
result data. To that end, the following research ques-ions were set. 
<Research Question 1> What are the main keywords by period that appeared in 
the domestic ESG media reports? 
<Research Question 2> What are the topics by period that appeared in the do-
mestic ESG media reports, and how do they change? 
2 
Theoretical Background 
Domestic studies on ESG are very sluggish. Papers that analyzed ESG news articles 
were searched in domestic and foreign academic journals, and as a result, no related 
papers were found. This is a result contrary to the reality where various companies 
and institutions are interested in national policies on ESG and ESG management. 
Although the research trend analysis through academic papers is somewhat different 
from the news ESG and topic modeling analysis methods that this study intends to 
use, the results of studies through academic papers are suitable for understanding the 
overall trend of ESG. 
Domestic papers related to ESG can be divided as follows. The ﬁrst type includes 
studies on the effects of ESG activities and the ﬁnancial characteristics of companies.

ESG News Analysis Using News Big Data: Focusing on Topic Modeling …
17
Kang and Jung [3] stated that corporate ESG activities can achieve great effects in 
companies with high transparency of disclosure because of the fact that the prof-
itability is supported or due to the presence of advanced normative investors, that 
trust in the companies in the market is important to maximize the effects of ESG 
activities, and that to that end, it is necessary to focus on ﬁnancial efforts such as 
improving proﬁtability and attracting foreign investments. 
The second type includes studies on the improvement of ESG governance and 
corporate value. Previous studies are divided into studies indicating that ESG 
management and governance improvement affect corporate value and studies indi-
cating that the foregoing do not have any signiﬁcant effect. First, among the studies 
that presented study ﬁndings indicating that ESG management such as corporate 
governance improvement positively affect corporate value, Fama [4] and Fama and 
Jensen [5] studied the ratio of outside directors and corporate value and reported that, 
according to the results of analysis, outside directors provide expertise and moni-
toring functions and that it was shown that the higher the ratio of outsider directors, 
the higher the corporate value. Oh and Lee [6] argued that the results of empir-
ical analysis of the effects of ESG evaluation elements on corporate value indicated 
that governance have the largest signiﬁcant effects followed by ESG on return on 
asset and price-earnings ratio, which are corporate performance. On the other hand, 
Yermack [7] who presented a study ﬁnding indicating that ESG management, such 
as improvement of corporate governance, has no effect on corporate value, analyzed 
the relation-ship between the size of the board of directors and corporate perfor-
mance, and according to the results of the study, he reported that, unlike the general 
knowledge that the larger the size of a company, the higher the corporate value of the 
company, larger sizes of the board of directors were not always associated with higher 
corporate value and that it was shown that the smaller the size of the board of direc-
tors, the higher the corporate performance, and the more effectively agency problems 
were alleviated on the contrary. Gong and Choi [8] empirically analyzed corporate 
governance and agency costs of large corporate groups and stated that there was no 
difference in agency costs between large corporate groups, which received excellent 
ratings from the Korean Corporate Governance Service, and the comparison group. 
The third type include studies related to ESG management and consumers. Kang 
and Kim [9] related that, despite that ESG management is an activity that indicates 
the creation of positive social values, such as solving social problems and making 
efforts for the public interest, the overall level of consumers’ awareness of, interest in, 
and understanding of ESG management at normal times is not very high compared to 
the importance, and that consumers were highly aware of the need for corporate PR 
and communication activities for ESG management, and their intention to purchase 
products and services from companies that carry out ESG management was also 
high. 
As summarized above, although studies on ESG exist among some scholars, these 
studies present future directivity, but did neither show what changes occurred by 
detailed agenda, nor explore related improvement measures. There-fore, the purpose 
of this study is to analyze ESG-related domestic news big data using the topic 
modeling analysis technique thereby deriving major ESG-related keywords that

18
H. J. Seo et al.
appeared in news big data and to identify ESG-related agendas by period based 
on the foregoing with a view to identifying changes in topics. 
3 
Research Method 
The media to be analyzed are all media that provide news to BIGKinds operated by 
the Korea Press Foundation as of December 31, 2021. As shown in (Table 1), there 
are 16 media including 11 comprehensive magazines and 5 broad-casting companies. 
The analysis period was set from May 24, 2006, to December 31, 2021. In this study, 
to understand what the ESG-related agenda is and how it changes, starting with May 
24, 2006, when ESG was ﬁrst reported in the domestic media, and until December 
31, 2021, when the current analysis was conducted, were set as the period to be 
analyzed. 
The purpose of this thesis is to analyze overall what topics have changed due to the 
COVID-19 in the existing concept of ESG, and to suggest implications. Accordingly, 
the ﬁrst period was set from May 24, 2006, to December 31, 2018, from the time when 
the concept of ESG was introduced to the time when the National Human Rights 
Commission of Korea distributed the ‘Public Institutions Human Rights Management 
Manual’ and urged the establishment of a human rights management system. The 
second period, from January 1, 2019, to December 31, 2020, is the time to announce 
the promotion of the Financial Supervisory Service’s ESG disclosure legislation and 
the use of the National Pension’s ESG evaluation index in investment standards and 
shareholder activities. The third period was set from January 1, 2021, to December 
31, 2021, as a turning point in which the corporate management paradigm shifts 
rapidly and accelerates the introduction of ESG due to COVID-19. 
In order to collect ESG-related news, the search term was set to ‘ESG’, and data 
was collected using BigKinds, a news big data analysis service operated by the Korea 
Press Foundation. As shown in (Table 2), the analysis was conducted on 7,049 news 
items after ﬁltering 1,720 news items such as simple announcements, identiﬁcation of 
people, and duplicate reports among the ﬁrst 8,769 news items collected. Through Big 
Kinds, an Excel ﬁle with morpheme extraction and keyword reﬁnement of atypical 
text was provided and used for analysis (Fig. 1).
As can be seen from the graph above, the frequency of exposure to ESG-related 
media reports has shown a trend of increasing overall since 2019. Among them,
Table 1 Analysis target media status 
Type
Media 
Central 
media 
Kyunghyang Shinmun, Kookmin Ilbo, Naeil Newspaper, Donga Ilbo, Munhwa 
Ilbo, Seoul Newspaper, Segye Ilbo, Chosun Ilbo, Joongang Ilbo, Hankyoreh, 
Hankook Ilbo 
Broadcast 
media 
KBS, MBC, OBS, SBS, YTN 

ESG News Analysis Using News Big Data: Focusing on Topic Modeling …
19
Table 2 Number of news cases analyzed 
Category
1st period
2nd period
3rd period
Total 
First collected news
340
1,139
7,290
8,769 
Filtered news
14
57
1,649
1,720 
Last analyzed news
326
1,082
5,641
7,049 
Fig. 1 The current state of data of ESG social responsibility article
newspaper articles are rapidly increasing in 2021 as companies’ ESG management 
has become a necessity, not an option, due to the Ministry of Justice’s legislative 
notice of the Framework Act on Human Rights Policy and the mandatory considera-
tion of ESG factors in the national pension investment management. Therefore, since 
the purpose of this study is to identify major keywords and topics for each period 
shown in ESG-related news, a useful topic modeling analysis method was used to 
derive topics. The topic modeling analysis method is a method of automatically 
extracting keywords and topics based on probability distribution by using machine 
learning algorithms for unstructured data such as text [10]. This is useful in deriving 
topics latent in large amounts of text [11]. In particular, the LDA technique is widely 
used in news topic analysis as an analysis technique that groups and classiﬁes major 
keywords by topic according to their occurrence probability in a large document set 
[12-14]. The analysis pro-gram utilized NetMiner 4.5v, a domestic program useful 
for morpheme extraction and analysis of Hangeul. In order to determine the appro-
priate number of topics for the study, topic modeling analysis was performed by 
setting the number of topics from 2 to 20, and then the degree of keyword duplica-
tion and the appropriateness of classiﬁcation were compared for each topic (α = 0.2, 
β = 0.02) [13]. As a result of the analysis, it was found that the degree of overlap 
of keywords was the lowest, and the optimal number of topics in which keywords 
were meaningfully classiﬁed by subject was 4. Based on this, after reviewing the 
keywords for each topic and the context in which the keywords were used in the 
actual news, the analysis was performed by assigning a topic name.

20
H. J. Seo et al.
The purpose of this study is to identify ESG trends by deriving key keywords 
appearing in ESG-related news big data, and a topic modeling analysis method useful 
in deriving major trends from a large amount of text was utilized. The topic modeling 
analysis method is a method of automatically extracting keywords and topics based 
on probability distribution using machine learning algorithms for unstructured data 
and is useful for identifying key keywords and major trends latent in large amounts 
of text. In particular, the LDA technique is an analysis technique that groups and 
classiﬁes by topic according to the probability of occurrence of key keywords in a 
large document set, and is widely used in analysis targeting news big data [13, 15]. 
The topic modeling analysis program utilized NetMiner 4.5v, a program useful 
for morpheme extraction and analysis of Hangeul. To determine the ap-propriate 
number of topics for analysis, the Silhouette Coefﬁcient was measured by setting 
the number of topics from 4 to 10 and setting the α values to 0.1, 0.5, and 0.1. The 
silhouette coefﬁcient is a method developed through research by A. Panichella et al. 
[16] and is an index indicating how closely the corresponding data are clustered with 
the data of the same cluster. In other words, the silhouette coefﬁcient is an index that 
can quantitatively analyze the similarity inside the topic and the difference between 
the topics, and it has a range of –1 to + 1, and the closer it is to +1, the more 
appropriate clustering is. As a result of measuring the silhouette coefﬁcient, when 
the α value was 0.2 and the number of topics was 4, the silhouette coefﬁcient was 
closest to +1. Based on the set value, topic modeling analysis was performed. 
4 
Analysis and Result 
4.1 
The First Period: Introduction of ESG Environmental 
Management 
The ﬁrst period is the result of topic modeling analysis of 326 ESG-related news 
items collected from May 24, 2006, to December 31, 2018, as shown in Table 3.
[Topic 1] derived keywords such as “Council”, “Environmental Management”, 
“Listed Companies”, “Korea Corporate Governance Institute”, and “Comprehensive 
Assessment”, deﬁning the topic name as “Introduction of Environ-mental Manage-
ment and the emergence of ESG Comprehensive Evaluation of Listed Companies”. 
The number of news is 48, and the proportion of topics is 14.72%. 
[Topic 2] derived keywords such as “Governance”, “Management”, “Society”, 
“Environment”, and “responsibility”, deﬁning the topic name as “Ambiguity in the 
Selection and Judgment Criteria of Social Responsibility Investment”, with 40 news 
cases, accounting for 12.27%. 
[Topic 3] deﬁnes the name of the topic as “Disclosure of Non-ﬁnancial In-
formation, Environmental and Social Governance Information and Activation of

ESG News Analysis Using News Big Data: Focusing on Topic Modeling …
21
Table 3 1st LDA topic modeling results 
Period 
Topic 
name 
(n, %) 
1 Keyword  
(%) 
2 Keyword  
(%) 
3 Keyword  
(%) 
4 Keyword  
(%) 
5 Keyword  
(%) 
1st
Topic 1 
(48, 
14.72%) 
Council 
(0.054) 
Environmental 
management 
(0.043) 
Listed 
companies 
(0.039) 
Korea 
corporate 
governance 
institute 
(0.028) 
Comprehensive 
assessment 
(0.016) 
Topic 2 
(40, 
12.27%) 
Governance 
(0.074) 
Management 
(0.060) 
Society 
(0.048) 
Environment 
(0.041) 
Responsibility 
(0.034) 
Topic 3 
(34, 
10.43%) 
Governance 
(0.031) 
Exchange 
(0.030) 
Enterprise 
(0.022) 
Institutional 
investors 
(0.021) 
Kospi 
(0.020) 
Topic 4 
(204, 
65.58%) 
SRI (0.092)
Investor 
(0.080) 
Operator 
(0.066) 
Governance 
(0.065) 
National 
pension (0.064)
ESG Investment” by deriving keywords such as “Governance”, “Exchange”, “Enter-
prise”, and “Kospi”. The number of news is 34, and the proportion of the topic is 
10.43%. 
[Topic 4] derived keywords such as “Social Responsibility Investment”, 
“Investor”, “Operator”, “Governance” and “National Pension”, deﬁning the topic 
name as “Introduction of Stewardship Codes and Active Participation in Corporate 
Decision-making”, with 204 news cases, accounting for 62.58%. 
The ﬁrst period was named the “Introduction of ESG Environmental Manage-
ment” as topics such as “Introduction of Environmental Management and the emer-
gence of ESG Comprehensive Evaluation of Listed Companies”, “Ambiguity in the 
Selection and Judgment Criteria of Social Responsibility Investment”, “Disclosure 
of Non-ﬁnancial Information, Environmental and Social Governance Information 
and Activation of ESG Investment” and "Introduction of Stewardship Codes and 
Active Participation in Corporate Decision-making" appeared. 
4.2 
The Second Period: Spread of ESG Eco-Friendly 
Management 
The second period is the result of topic modeling analysis of 1,082 ESG-related news 
collected from January 1, 2019, to December 31, 2020, and is shown in Table 4.
[Topic 1] deﬁned the name of the topic as “Coal Phase-Out and Expansion 
of Investments in New and Renewable Energy” by deriving keywords such as 
“Eco-friendly”, “Consumer”, “Coal phase-out”, “Emissions”, “Renewable” and the 
number of news was 141, accounting for 13.03%.

22
H. J. Seo et al.
Table 4 2nd LDA topic modeling results 
Period
Topic 
name 
(n, %) 
1 Keyword  
(%) 
2 Keyword  
(%)) 
3 Keyword  
(%) 
4 Keyword  
(%) 
5 Keyword  
(%) 
2nd
Topic 1 
(141, 
13.03%) 
Eco-friendly 
(0.047) 
Consumer 
(0.031) 
Coal 
phase-out 
(0.029) 
Emissions 
(0.029) 
Renewable 
(0.024) 
Topic 2 
(157, 
14.51%) 
Investor 
(0.075) 
Stakeholders 
(0.036) 
COVID-19 
(0.034) 
Capitalism 
(0.019) 
Social Bond 
(0.019) 
Topic 3 
(174, 
16.08%) 
Society 
(0.045) 
SDGs 
(0.043) 
UN 
(0.037) 
Environment 
(0.032) 
Korea 
corporate 
governance 
Institute 
(0.030) 
Topic 4 
(610, 
56.38%) 
Chairman 
(0.031) 
Investor 
(0.030) 
Council 
(0.029) 
National 
pension 
(0.022) 
Rate of 
return on 
investment 
(0.020)
[Topic 2] derived keywords such as “Investor”, “Stakeholder”, “COVID-19”, 
“Capitalism”, and “Social Bond”, deﬁning the topic name as “Economic crisis caused 
by COVID-19 and strengthening the capitalist system of stakeholders”. The number 
of news is 157, and the proportion of the topic is 14.51%. 
In [Topic 3], keywords such as “Social”, “SDGs”, “UN”, “Environment”, and 
“Korean Corporate Governance Institution” were derived, and the name of the 
topic was deﬁned as “Sustainable Development Goals (SDGs) Index and corpo-
rate survival”. The number of relevant news is 174, and the proportion of topics is 
16.08%. 
In [Topic 4], keywords such as “Chairman”, “Investor”, “Council”, “National 
Pension”, and “Rate of return on investment” were derived, and the topic name was 
deﬁned as “Responsible Investment Risk Management and Proﬁtability Enhance-
ment of Fund Management”. The number of relevant news is 610, and the proportion 
of topics is 56.38%. 
The second period was named “The spread of ESG eco-friendly management” as 
topics such as “Coal Phase-Out and Expansion of Investments in New and Renew-
able Energy”, “Economic crisis caused by COVID-19 and strengthening the capi-
talist system of stakeholders”, “Sustainable Development Goals (SDGs) Index and 
corporate survival”, and “Responsible Investment Risk Management and Proﬁtability 
Enhancement of Fund Management” appeared.

ESG News Analysis Using News Big Data: Focusing on Topic Modeling …
23
4.3 
The Third Period: Spread of ESG Eco-Friendly 
Management 
The third period is the result of topic modeling analysis of 5,641 ESG-related news 
collected from January 1, 2021, to December 31, 2021, as shown in Table 5. 
In [Topic 1], keywords such as “Council”, “Chairman”, “Sustainability”, “Stake-
holders”, and “External Directors” were derived, and the topic name was deﬁned as 
“The Trend of ESG Management and the Improvement of the Governance Structure 
of Fame”. The number of news is 1,090, and the proportion of topics is 19.32%. 
[Topic 2] derived keywords such as “Management”, “Society”, “Environment”, 
“Enterprise”, and “Domination” and deﬁned the name of the topic as “ESG Manage-
ment Performance and Return to Shareholders”. The number of news is 833, and the 
proportion of topics is 14.77%. 
[Topic 3] derived keywords such as “Consumer”, “Investor”, “Eco-friendly”, 
“COVID-1” and “Competitiveness”, deﬁning the topic name as “The Spread of Non-
face-to-face Service and the Implementation of Low-Carbon Economy”, with 1,198 
news cases, accounting for 21.24%. 
[Topic 4] derived keywords such as “Eco-friendly”, “Partners”, “Emissions”, 
“ESG management”, “Low carbon”, deﬁned the topic name as “Support for Partner 
ESG Management and Accelerate Low-Carbon Innovation” with 2,520 news cases, 
44.67%. 
The third phase was named “The Maturity of ESG Management and Low Carbon 
Growth” as topics such as “The Trend of ESG Management and the Improvement of 
the Governance Structure of Fame”, “ESG Management Performance and Return to 
Shareholders”, “The Spread of Non-face-to-face Service and the Implementation of 
Low-Carbon Economy” and “Support for Partner ESG Management and Accelerate 
Low-Carbon Innovation” appeared.
Table 5 3rd LDA topic modeling results 
Period 
Topic 
name (n, 
%) 
1 Keyword  
(%) 
2 
Keyword 
(%) 
3 Keyword  
(%) 
4 Keyword  
(%) 
5 Keyword  (%)  
3rd
Topic 1 
(1,090, 
19.32%) 
Council 
(0.026) 
Chairman 
(0.013) 
Sustainability 
(0.012) 
Stakeholders 
(0.012) 
External 
directors 
(0.011) 
Topic 2 
(157, 
14.51%) 
Management 
(0.037) 
Society 
(0.032) 
Environment 
(0.031) 
Enterprise 
(0.018) 
Domination 
(0.017) 
Topic 3 
(174, 
16.08%) 
Consumer 
(0.021) 
Investor 
(0.019) 
Eco-friendly 
(0.013) 
COVID-19 
(0.010) 
Competitiveness 
(0.010) 
Topic 4 
(610, 
56.38%) 
Eco-friendly 
(0.015) 
Partners 
(0.013) 
Emissions 
(0.012) 
ESG 
management 
(0.012) 
Low carbon 
(0.011) 

24
H. J. Seo et al.
5 
Conclusions 
This study was conducted to derive major ESG agendas and implications by identi-
fying key ESG keywords and topics by period using ESG-related news big data. To 
that end, 8,769 news articles reported from May 24, 2006, to December 31, 2021, 
were divided into three periods and analyzed with topic modeling (LDA), and the 
following topics were derived by period. 
In the ﬁrst period (Introduction of ESG Environmental Management), topics such 
as ➀Introduction of Environmental Management and the emergence of ESG Compre-
hensive Evaluation of Listed Companies ➁ Ambiguity in the Selection and Judg-
ment Criteria of Social Responsibility Investment ➂ Disclosure of Non-ﬁnancial 
Information, Environmental and Social Governance Information and Activation of 
ESG Investment ➃ Introduction of Stewardship Codes and Active Participation in 
Corporate Decision-making appeared. In the second period (The spread of ESG eco-
friendly management), topics such as ➀ Coal Phase-Out and Expansion of Invest-
ments in New and Renewable Energy ➁ Economic crisis caused by COVID-19 and 
strengthening the capitalist system of stakeholders ➂Sustainable Development Goals 
(SDGs) Index and corporate survival ➃ Responsible Investment Risk Management 
and Proﬁtability Enhancement of Fund Management, and in the third period (The 
Maturity of ESG Management and Low Carbon Growth), topics such as ➀ The Trend 
of ESG Management and the Improvement of the Governance Structure of Fame ➁ 
ESG Management Performance and Return to Shareholders ➂ The Spread of Non-
face-to-face Service and the Implementation of Low-Carbon Economy ➃ Support 
for Partner ESG Management and Accelerate Low-Carbon Innovation appeared. 
As described above, the following implications can be found through ESG-related 
topics. To review changes in the topic, in the ﬁrst period, reports dealing with issues 
of large corporations and ﬁnancial institutions related to the introduction of ESG 
environmental management and the evaluation of corporate ESG ratings mainly 
increased. In the second period, the promotion of ESG disclosure legislation and 
the announcement by the National Pension that it would actively utilize the ESG 
evaluation index in investment standards and shareholder activities, enhance the 
transparency of disclosure of ESG information by related companies, and expand 
their ESG investments were mainly reported. The third period became turning point in 
accelerating the introduction of ESG due to the increase in the number of consumers 
who value corporate sustainability or environmental value due to the COVID-19 
outbreak and changes in management paradigm and reports related to inter-national 
society’s demand for carbon neutrality and the implementation of carbon neutrality 
increased signiﬁcantly. 
In this regard, when the topics were compared by period, similarities and differ-
ences were shown at the same time, and the following implications appeared. First, 
three of the four topics derived in the ﬁrst period (“Ambiguity in the Selection 
and Judgment Criteria of Social Responsibility Investment”, “Disclosure of Non-
ﬁnancial Information, Environmental and Social Governance Information and Acti-
vation of ESG Investment” and “Introduction of Stewardship Codes and Active

ESG News Analysis Using News Big Data: Focusing on Topic Modeling …
25
Participation in Corporate Decision-making”) mainly focus on ﬁnancial, corporate 
governance, environmental and social aspects. In general, ESG performance indexes 
clarify the relationship be-tween sustainable investments and ﬁnancial performance 
[17]. It is important to evaluate corporate social responsibilities (indices or evaluation 
systems) as they can inﬂuence investors’ investment decisions. As ESG management 
is highlighted in external credibility and investment attraction of companies, it can 
be seen that ESG evaluation is emerging as a new normal in corporate evaluation. 
Second, one of the four topics derived in the second period (Coal Phase-Out 
and Expansion of Investments in New and Renewable Energy) is consistent with the 
study ﬁndings in relation to the environmental (E) dimension in a study conducted by 
Flammer [18] indicating that active participation of companies in the environmental 
aspect can create new competitive re-sources in the companies and can play the role of 
insurance. Another topic (“Economic crisis caused by COVID-19 and strengthening 
the capitalist system of stakeholders”) shows the trend of expansion of global ESG 
investments to overcome the economic crisis caused by the COVID-19 pandemic 
and global climate change. This suggests that companies should actively respond 
to ESG environmental issues, which will become a new standard for survival and 
management for companies after climate change and COVID-19 and strive for sound 
and transparent disclosure and smooth communication with consumers. 
Third, in relation to two topics (“ESG carbon neutral implementation”, “Support 
for Partner ESG Management and Accelerate Low-Carbon Innovation”) among the 
topics derived in the third period, Matsumura et al. [19] stated that carbon emissions 
were found to be inversely proportional to corporate value and managers’ decision 
and level to disclose carbon emissions were found to be inversely proportional to 
corporate value. This suggests that companies can secure future competitiveness in 
the industrial sector by accelerating low-carbon innovations such as carbon emission 
regulation, environment-friendly investment, and new industry growth in the process 
of implementing carbon neutrality. 
This study is a paper that analyzed ESG press reports and has academic signiﬁ-
cance as a study that empirically and systematically analyzed what agendas are set 
and reported by the press in relation to ESG utilizing news big data and the topic 
modeling analysis technique. In addition, this study has policy signiﬁcance because 
it proposed major topics and implications by period such as the introduction of ESG 
environmental management, spread of ESG environment-friendly management, and 
implementation of ESG carbon neutrality. 
The limitations and suggestions of this study are as follows. First, this study tried 
to temporally explore changes in keywords and agenda for ESG based on new articles 
from May 2006 when the concept of ESG was introduced to the domestic press media 
to December 2021 and divided the new articles into three periods to analyze them 
separately. In 2020 and 2021, when reports on ESG began to increase signiﬁcantly, 
a huge amount of ESG-related news articles was poured out every day. Therefore, it 
is expected that in future studies, the period can be further subdivided so that more 
abundant im-plications can be derived. 
Second, in this study, when determining the number of topics, the re-searcher’s 
interpretability was set as the standard. However, in future studies, it is expected that

26
H. J. Seo et al.
more systematic and in-depth analysis results will be obtained by applying empirical 
methods such as perplexity index. 
Third, ESG has three core elements: Environment, Social Responsibility, and 
Governance. However, in this study, the change in key factors was not the issue, 
but an attempt was made to see the large range termed ESG as a whole. Therefore, 
‘ESG’ was set as a search word to conduct the study. How-ever, in future studies, 
it seems necessary to additionally study the issues of wide changes by referring to 
news articles related to each core element. 
Lastly, this study was conducted with only domestic ESG-related news. However, 
after the introduction of the Stewardship Code, which allows institutional investors to 
actively exercise their voting rights, socially responsible investments in consideration 
of issues related to ESG management and ESG investment are being emphasized. 
Among the environmental elements of ESG, the demands of countries around the 
world for carbon neutrality should be responded. In overseas countries, too, great 
interest in ESG is shown and a lot of news on ESG are released. In addition, interest 
in ESG is increasing rap-idly around the world. Therefore, in future studies, it is 
expected that more meaningful study ﬁndings will be derived if it is understood what 
differences are in agendas in comparison with overseas news. In addition, as there 
are not enough previous studies related to ESG at present, the rationale and rationale 
for this study may be lacking. Therefore, it is expected that ESG related studies are 
activated further through this study. 
References 
1. Ben-Amar W, Chang M, Mcllkenny P (2017) Board gender diversity and corporate response 
to sustainability initiatives: Evidence from the car-bon disclosure project. J Bus Ethics 
142(2):369–383 
2. Ahn TW, Lee HS, Yi JS (2021) A study on the keyword extraction for ESG controversies 
through association rule mining. J Inf Systems 30(1):123–149 
3. Kang W, Jung MK (2020) Effect of ESG activities and ﬁrm’s ﬁnancial characteristics. Korean 
Secur Association 49(5):681–707 
4. Fama EF (1980) Agency problems and the theory of the ﬁrm. J Polit Econ 88(2):288–307 
5. Fama EF, Jensen MC (1983) Separation of ownership and control. J Law Economics 26(2):301– 
325 
6. Oh SH, Lee ST (2019) A study on the relationship between ESG evaluation factors and corporate 
value. Korean Comput Account Rev 17(2):205–223 
7. Yermack D (1996) Higher market valuation of companies with a small board of directors. J 
Financ Econ 40(2):185–211 
8. Gong KT, Choi JS (2013) Does the superior corporate governance mitigate agency costs? Korea 
Int Account Rev 49:251–272 
9. Kang YJ, Kim SH (2022) ESG Management, Strategies for corporate sustainability growth: 
KT’s company-wide goals and strategies. J Korea Converg Soc 13(4):233–244 
10. Lee SS (2018) Network analysis methods applications and limitations. CheongRam, Seoul, pp 
105–120 
11. Hwang YS (2017) Journalism studies in the data age. Communication Books, Seoul, pp 331– 
377

ESG News Analysis Using News Big Data: Focusing on Topic Modeling …
27
12. Kim TJ (2020) COVID-19 news analysis using news big data: Focusing on topic modeling 
analysis. J Korea Contents Association 20(5):457–466 
13. Lee SY, Kim TJ (2020) News big data analysis of tap water larvae using topic modeling analysis. 
J Korea Contents Association 20(11):28–37 
14. Han S, Kim T (2021) News big data analysis of media literacy using topic modeling analysis. 
J Korea Contents Association 21(4):26–37 
15. Blei DM (2012) Probabilistic topic models. Commun ACM 55(4):77–84 
16. Panichella A, Dit B, Oliveto R, Di Penta M, Poshynanyk D, De Lucia A (2013) How to 
effectively use topic models for software engineering tasks? an approach based on genetic 
algorithms. In: 2013 35th International conference on software engineering (ICSE). IEEE, pp 
522–531 
17. Khan M, Serafeim G, Yoon A (2016) Corporate sustainability: First evidence on materiality. 
Account Rev 91(6):1697–1724 
18. Flammer C (2013) Corporate social responsibility and shareholder reaction: The environmental 
awareness of investors. Acad Manag J 56(3):758–781 
19. Matsumura EM, Prakash R, Vera-Munoz SC (2014) Firm-value effects of carbon emissions 
and carbon disclosures. Account Rev 89(2):695–724

Effects of the Perceived Beneﬁt 
and Innovativeness of Live Commerce 
on Intention to Use: Focus on Chinese 
Consumers 
Zhi Pan, Dong Hyuk Jo, and Su Young Kim 
Abstract While the frequency of ofﬂine product purchases has sharply decreased 
due to the decrease in the number of times consumers go out due to COVID-19, which 
occurred in 2019, non-face-to-face online purchases, that is, non-contact consump-
tion, are becoming common around the world. In the COVID-19 environment where 
direct product access is impossible, there is a growing interest in Live Commerce, 
which delivers real-time product information vividly to fashion consumers who 
are accustomed to interactive SNS-based distribution and immediately responds to 
consumer needs. Live commerce in China has grown rapidly since it began to become 
popular in 2015 and is expected to grow signiﬁcantly in the future. In this study, the 
effect of perceived beneﬁts of live commerce on intention to use was veriﬁed. First, 
as a result of analyzing the effect on the perceived beneﬁt and perceived value of 
live commerce, it was found that among the four perceived beneﬁts, playfulness, 
economic feasibility, and usefulness had a positive (+) effect in the order. Also, it 
was found that technicality had no signiﬁcant effect on perceived value. Second, 
as a result of analyzing the effect on perceived value and intention to use, it was 
found that perceived value had a positive (+) effect on intention to use. Finally, the 
moderating effect of innovative-ness was veriﬁed. It was found that innovativeness 
has a moderating effect. 
Keywords Live commerce · Perceived beneﬁt · Perceived value · Value-based 
adoption model · Introduction
Z. Pan · S. Y. Kim 
Department of Business Administration, Graduate School, Soongsil University Seoul, Seoul, 
Korea 
e-mail: panzhiinkorea@naver.com 
S. Y. Kim 
e-mail: aliospec@gmail.com 
D. H. Jo envelope symbol
Department of Business Administration, Soongsil University Seoul, Seoul, Korea 
e-mail: joe@ssu.ac.kr 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_3 
29

30
Z. Pan et al.
1 
Introduction 
The development of networks and mobile technologies has brought many changes 
to the lifestyles of modern people. With the high distribution rate and popularization 
of smartphones, consumers shared and quickly acquired information with each other 
through social network services and media without the limitation of time and place, 
and as a result, consumption patterns began to change [1]. While the frequency of 
ofﬂine product purchases has decreased sharply due to a reduction in the number 
of times consumers go out because of COVID-19 which has occurred in 2019, non-
face-to-face online purchases, or non-contact consumption, are becoming common 
around the world [2]. In a COVID-19 environment where people cannot contact 
directly with products, attention is being paid to Live Commerce, which vividly 
delivers real-time product information to consumers who are becoming proﬁcient in 
a mutual SNS-based distribution, and which can immediately reﬂect the demands 
of viewers that are customers. In fact, the sales volume of products through Live 
Commerce is increasing, and this study wants to focus on Live Commerce, a new 
form of product distribution, especially the Live Commerce market in China [3]. 
Live commerce in China has grown rapidly since 2015 and is expected to grow 
signiﬁcantly in the future. However, research on consumer behavior using new media 
such as Live Commerce is insufﬁcient. Live commerce is popular as a new shopping 
method and has a necessity to research as a new technology acceptance method. Users 
should also recognize it from the perspective of consumers, not just new technology 
users. VAM model research provides information on new technology industries such 
as Live Commerce. Also, it can provide a new perspective for subsequent model 
studies. 
Therefore, this research aims to study the factors that promote the use of Live 
Commerce. To this end, this study attempts to examine the relationship by deriving 
factors based on both Kim et al.’s [4] Value-based Adoption Model and studies on 
the use of new information technology. In addition, we would like to scrutinize how 
personal innovativeness affects the relationship between value and intention to use in 
the process of using new technologies. The main perspective of this study was limited 
to the perceived beneﬁts because there are more perceived beneﬁts than the sacriﬁcial 
factors of the Live Commerce service. The results of this study are expected to afford 
useful information to related companies by providing a fundamental that helps them 
understand factors that users value regarding Live Commerce, which is developing 
rapidly in the Chinese market.

Effects of the Perceived Beneﬁt and Innovativeness of Live Commerce …
31
Table 1 Comparison of the main characteristics of conventional Internet-based commerce and Live 
Commerce 
Conventional internet-based commerce
Live commerce 
Not simultaneous information delivery
Delivering product information through 
real-time video 
Delivery of product information through 
images, text, etc 
Real-time and various interactions between 
shopping hosts and users or between users 
Non-simultaneous and restrictive interactions 
between sellers and consumers 
Relative freedom from content regulation due 
to the nature of online services 
Relative freedom from content regulation due to 
the nature of online services 
There is no cost for broadcasting, so the fee 
burden is low compared to home shopping 
2 
Theoretical Background 
2.1 
Live Commerce 
Live Commerce is a compound word of e-commerce and live streaming and is a 
platform for sellers and consumers to promote, sell, and purchase products through 
bilateral communication on online real-time broadcasting. With the development of 
mobile communication technology, Live Commerce can be broadcasted and watched 
directly on smartphones without a limitation of time and place. Also, it has both 
characteristics of live streaming which is capable of real-time communication, and e-
commerce which can order goods in real-time [5, 6]. Live Commerce is e-commerce 
in which sellers promote products using live streaming technology and sell them to 
consumers by using electronic documents. Shu [7] explains that Live Commerce is 
generally used for marketing purposes and that it is a platform that can increase brand 
reputation, brand pro-motion, brand awareness directly and indirectly by interacting 
with viewers in real-time video. Yin [8] deﬁned Live Commerce as a new type 
of media marketing based on communication and media among online marketing 
methods and explained that it is based on consumer needs and interaction with 
viewers through a real-time online broadcast. The characteristics of Live Commerce 
com-pared to conventional internet-based commerce are shown in Table 1. 
2.2 
Value-Based Adoption Model 
Kim et al. [4] proposed a Value-based Adoption Model (VAM) and said that the 
existing Technology Acceptance Model (TAM) proposed by Davis et al. [9] is limited 
in explaining the acceptance of new ICT, and also explained that new ICT users 
should be recognized as consumers, not just new technology users. TAM assumed 
that the main interests of technology users in the organization were ease of use and 
usefulness, but VAM assumed that consumers were focused on maximizing value.

32
Z. Pan et al.
VAM which describes an individual’s technology acceptance behavior in terms of 
maximizing value presented that the personal perceived value is formed through a 
comparison of beneﬁts including usefulness and enjoyment and sacriﬁces including 
technicality and perceived fee [4]. 
2.3 
The Relationship Between Perceived Beneﬁt 
and Perceived Value, Perceived Value and Intention 
to Use 
In this study, four variables related to perceived beneﬁts by users from Live 
Commerce are proposed: usefulness, playfulness, technicality, and economic feasi-
bility. First, usefulness refers to the degree to which it is believed that using Live 
Commerce will further improve the user’s experience and give help in daily life [4, 
9]. In VAM, usefulness was also proposed as a major variable related to beneﬁts 
affecting perceived value, and as a result of empirical analysis, it was proved to be a 
variable that positively affects perceived value [10, 11]. Second, playfulness refers to 
the degree to which you believe that using Live Commerce will cause pleasure and 
interest [12]. Agarwal and Karahanna [12] explained that when recognizing interest 
and fun in the process of using technology, users become immersed in the use of 
new technologies and lose track of time, and that playfulness is an important factor 
in their user behavior. In VAM, usefulness and playfulness were also presented as 
major variables related to perceived beneﬁts and proved to be factors that positively 
affect perceived value, and later studies on various technologies conﬁrmed that they 
are variables that form value and positively evaluate information technology [11]. 
Technicality refers to the degree to believe that the use and understanding of Live 
Commerce are easy and the use is not complicated [11]. VAM explains technicality 
as the degree of physical and mental effort required of users when using new infor-
mation technology [4]. Finally, economic feasibility refers to the degree to which 
goods or services can be purchased at low prices when using Live Commerce [13]. 
In VAM, the cost was proposed as one of the variables that had a signiﬁcant impact 
on shaping value and said that if users perceived that the cost they pay to use the new 
technology was unreasonable and too high, it negatively affected value formation 
[4]. 
Perceived value in business administration research has been mainly explained 
by the subjective evaluation felt by consumers in the process of selecting a product 
or service and has been considered one of the important variables that determine 
behavior or attitude [13]. It refers to the consumer’s cognitive evaluation of the 
trade-off relationship between ‘giving’ and ‘receiving’ in relation to acquiring a new 
product or service [14]. Blackwell et al. [15] explained the perceived value that the 
difference between advantages that customers gain from using services or speciﬁc 
goods and costs to be paid and deﬁned as the viewpoint of the offset between beneﬁts 
and sacriﬁces. Through follow-up studies, this approach was deﬁned as the perception

Effects of the Perceived Beneﬁt and Innovativeness of Live Commerce …
33
of the quality that customers acquire compared to their monetary costs (paid price) 
and non-monetary costs (effort, time) and developed into a strategic point of view 
[15–18]. Based on the pre-ceding studies above, this study deﬁned perceived value 
in Live Commerce as "the degree of perceived value that compared to the beneﬁts 
customers gains in the process of using Live Commerce". This can be formed in 
terms of economy, in relation to product quality and characteristics of new products. 
In addition, it has a positive effect on the acceptance behavior of new technologies 
[11, 19]. 
In the case of Live Commerce, it will have a positive effect if the user perceives 
that the new technology and service will help his or her work or daily life, or if they 
perceive it as causing pleasure. Lastly, if users feel easy to use and understand Live 
Commerce and feel that they have advantages in terms of cost, it will have a positive 
effect on value formation. In addition, in the case of users who perceive value for 
Live Commerce, the intention to use can be expected to increase. Therefore, the 
hypothesis is built as follows. 
Hypothesis 1–1 (H1–1): Usefulness has a positive inﬂuence on perceived value. 
Hypothesis 1–2 (H1–2): Playfulness has a positive inﬂuence on perceived value. 
Hypothesis 1–3 (H1–3): Technicality has a positive inﬂuence on perceived value. 
Hypothesis 1–4 (H1–4): Economic feasibility has a positive inﬂuence on perceived 
value. 
Hypothesis 2 (H2): Perceived value has a positive inﬂuence on intention to use. 
2.4 
The Moderating Effect of Innovativeness 
Innovativeness means the degree to which people try to accept and try new things 
before others without hesitating about new technologies or changes [12, 19, 20]. 
Users with innovativeness tendencies take active actions in accepting new technolo-
gies and are less reluctant to uncertainty or perceived risks [20, 21]. It was conﬁrmed 
that the groups with higher individual innovativeness have a higher inﬂuence rela-
tionship than those with lower individual innovativeness in the relationship between 
perceived value and continuous intention to use [19]. The study of Kim et al. [22] 
con-ﬁrmed the behavior types of online shoppers from the perspective of individual 
innovativeness. It showed that customers with high innovativeness had a positive 
attitude change in online shopping. Chang et al. [23] said that if the group has an 
innovative tendency, it has a positive effect on the relative advantages and intention to 
use. Kim et al. [24] showed that in the mobile payment market, innovative propensity 
has a positive effect on usefulness. In the early 2010s, studies related to smartphones, 
which were once in the spotlight as innovative products, said that innovativeness has 
a positive effect on continuous use intention, recommendation intention, loyalty, and 
user satisfaction [25-28]. Based on these preceding studies above, this study expects 
that innovativeness will affect the relationship between perceived value and inten-
tion to use new technologies such as Live Commerce. In this sense, the hypothesis 
is suggested as follows.

34
Z. Pan et al.
Fig. 1 Research model 
Hypothesis 3 (H3): Innovativeness will strengthen the relationship between 
perceived value and intention to use (Fig. 1). 
3 
Research Method 
This study conducted a survey of customers using live commerce in China through 
WeChat or online links. A survey was conducted for two weeks and a total of 
157 surveys were collected. Of these, 27 cases of insincere responses and missing 
values were removed, and 130 cases were used as the ﬁnal effective sample. The 
characteristics of the samples collected in this study are shown in (Table 2).
The measurement tool for empirical analysis of the research model was 
constructed according to the purpose of this study based on previous studies. First of 
all, the questionnaire was supplemented according to the context of this study based 
on the questionnaire whose validity was veriﬁed in previous studies. In addition, only 
valid measurement items were ﬁnally used in this study by determining the accuracy 
of variables measurement. 
4 
Analysis and Result 
Reliability and validity were veriﬁed to verify the measurement tool in this study. 
First of all, reliability veriﬁcation was based on Cronbach’s Alpha, which determines 
the internal consistency of the scale, and if it is 0.6 or higher, it can be said that the

Effects of the Perceived Beneﬁt and Innovativeness of Live Commerce …
35
Table 2 Characteristics of sample (n = 130) 
Category and items
Sample size
Ratio (%) 
Gender
Male
36
27.7 
Female
94
72.3 
Age
20~30
94
72.3 
31~40
23
17.7 
41~50
11
8.5 
51~60
2
1.5 
Educational background
Under high school graduation
12
9.2 
Graduation from junior college
20
15.4 
Graduation from college
66
50.8 
Graduate school or higher
32
24.6 
Frequency of use
0–1 time a month
64
49.2 
2–3 times a month
42
32.3 
Once a week
15
11.5 
2–3 times a week
8
6.2 
Every day
1
0.8 
Period of use
Less than 6 months
19
14.6 
Less than 6–12 months
40
30.8 
Less than 12–18 months
29
22.3 
Less than 18–24 months
26
20 
More than 24 months
16
12.3
reliability of the scale was secured. Exploratory factor analysis was conducted to 
verify the construct validity. In the case of factor analysis, principal component 
analysis and Varimax were adopted to extract construct variables and simplify factor 
loading values. As a result of the analysis, if the eigenvalue was 1.0 or more, and if 
the factor loading value was 0.4 or more, it was judged to be appropriate. Detailed 
results are presented in (Table 3).
Next, the correlation between the constituent concepts was analyzed. As a result 
of the analysis, the correlation between each variable was found to be a positive (+) 
correlation, and the empirical analysis was judged to be valid. (Table 4) summarizes 
the results of the correlation analysis.
In this study, regression analysis was conducted to conﬁrm the effect of the 
perceived beneﬁts of Live Commerce on the intention to use, and the results of 
verifying the hypothesis are as follows. First, as a result of analyzing the rela-
tionship between perceived beneﬁts and perceived values of Live Commerce, it is 
found that usefulness (t = 3.140, p = 0.002), playfulness (t = 5.887, p = 0.000), 
and economy feasibility (t = 3.661, p = 0.000) appear to have a positive effect 
on per-ceived values, and technicality (t = 0.881, p = 0.380) appears to have no 
signiﬁcant effect on perceived value. Second, perceived value (t = 13.358, p =

36
Z. Pan et al.
Table 3 Conﬁrmatory factor analysis based on reliability 
Variable
Measurement items
Factor L.D 
Eigenvalue 
Crb. alpha 
Usefulness
Using live shopping is handy
0.725
3.115
0.864 
Saving time searching for product 
information 
0.729 
Providing a variety of information
0.676 
Generally useful
0.729 
Playfulness
Live shopping is fun
0.685
2.812
0.859 
Spending time on live shopping is not 
wasteful 
0.750 
Felt positive when using live shopping 0.595 
Lost track of time when using live 
shopping 
0.826 
Technicality
Not require a lot of mental effort
0.720
2.733
0.843 
Live shopping makes it easy to do 
what I want 
0.893 
The purchase product procedure is 
simple 
0.825 
It is convenient to use
0.560 
Economic 
feasibility 
Products are not expensive
0.768
2.793
0.852 
More economical than ofﬂine 
purchasing 
0.696 
The price is reasonable
0.824 
Satisﬁed with the price
0.644 
Perceived value 
More time saving than ofﬂine 
purchasing 
0.625
2.954
0.900 
More money-saving than ofﬂine 
purchasing 
0.791 
No restrictions
0.876 
Reﬂect on the latest trends
0.804 
Intention to use
Will use live shopping again
0.811
3.568
0.936 
Will continue to use live shopping
0.853 
The use of live shopping will increase 
0.842 
I want to continue using live shopping 
0.859
0.000) is found to have a positive effect on the intention to use. Lastly, a moder-
ating effect anal-ysis was conducted to verify that innovativeness plays a moderating 
role. As a result of the analysis, in the case of moderated innovativeness (perceived 
value*innovativeness), the signiﬁcance probability (p = 0.009) is found to be a 
meaning-ful result value. Therefore, innovativeness has a moderating effect on the 
rela-tionship between perceived value and intention to use. Tables 5 and 6 sum-marize 
the analysis results.

Effects of the Perceived Beneﬁt and Innovativeness of Live Commerce …
37
Table 4 Results of correlation analysis 
Variable
Correlation 
Usefulness
Technicality
Economics 
feasibility 
Playfulness
Perceived 
value 
Intention to 
use 
Usefulness
1 
Technicality
0.538**
1 
Economic 
feasibility 
0.633**
0.539**
1 
Playfulness
0.693**
0.496**
0.577**
1 
Perceived 
value 
0.721**
0.529**
0.676**
0.760**
1 
Intention to 
use 
0.762**
0.468**
0.627**
0.764**
0.763**
1 
** The correlation is signiﬁcant at level 0.01 (both sides)
Table 5 Results of hypotheses tests 
Hypotheses
β
T
Result 
H1–1
Usefulness has a positive 
inﬂuence on perceived 
value 
0.24
3.14(0.002**)
Supported 
H1–2
Playfulness has a positive 
inﬂuence on perceived 
value 
0.422
5.877(0.000**)
Supported 
H1–3
Technicality has a 
positive inﬂuence on 
perceived value 
0.055
0.881(0.380)
Not supported 
H1–4
Economic feasibility has 
a positive inﬂuence on 
perceived value 
0.251
3.661(0.000**)
Supported 
R2 = 0.692, Durbin-Watson = 2.100, F = 70.347, P = 0.000 
H2
Perceived value has a 
positive inﬂuence on 
intention to use 
0.763
13.359(0.000**)
Supported 
R2 = 0.582, Durbin-Watson = 1.709, F = 178.471, P = 0.000 
*** p < 0.001, ** p < 0.01, * p < 0.05 
Table 6 Results of verifying the moderating effect of innovativeness 
Independent variable
Moderating variable
Dependent variable
β
T
p 
Perceived value
Innovativeness
Intention to use
−0.147
−2.642a
0.009 
Note *p < 0.05, ** p < 0.01, *** < 0.001 
a: perceived value * innovativeness

38
Z. Pan et al.
5 
Conclusions 
In this study, the effect of the perceived value and innovativeness of Live Commerce 
on the intention to use was veriﬁed. First, as a result of analyzing the effect of 
perceived beneﬁts on the perceived value of Live Commerce, it was found that among 
the factors of perceived beneﬁts, playfulness, economic feasibility, and usefulness 
had a positive effect. Interpreting this means that the more consumers believe that 
using Live Commerce will help them in their daily lives, and the greater the degree 
of interest and enjoyment, the higher the perception of the value of Live Commerce. 
Also, if consumers buy products at a low price by using Live Commerce, it means 
that they appreciate the value of Live Commerce. These results are consistent with 
the ﬁndings of Kim et al. [4], Lee et al. [10], Oh Jong-Chul. [11], Yu et al. [21], 
Wang [29]. Technicality was found to have not a signiﬁcant effect on perceived 
value. These results mean that users do not appreciate Live Commerce even if they 
often perceive it as easy to use and understand. This is inconsistent with the results 
of previous studies by Lin et al. [30] and Wang et al. [31]. Second, according to a 
result of analyzing the effect of perceived value on intention to use, it was found that 
perceived value had a positive effect on the intention to use. This is consistent with 
previous studies [4, 11]. References [19, 21, 32] that show that perceived value has a 
signiﬁcant effect on the intention to use. Finally, it was found that innovativeness has 
a moderating effect. If the interested parties actively try to promote live commerce 
to people with high innovation tendencies, they will be able to help them use Live 
Commerce services. 
The academic implications of this study are as follows. First, the point is that 
it revealed the factors inﬂuencing the intention to use Live Commerce at a time 
when Live Commerce is becoming common and high market growth is expected 
in the future. Second, this study examined the factors of perceived beneﬁts from 
the perspective of perceived value of Live Commerce users by applying a Value-
based Adoption Model. This has theoretical signiﬁcance in that it presented the 
results of empirical analysis of the perceived value of the user for Live Commerce. 
The practical implications of this study are as follows. First of all, considering that 
the playfulness and usefulness de-scribed as the main variables of beneﬁts in the 
Value-based Adoption Model have a positive effect on perceived value even in the 
case of Live Commerce, it is necessary to emphasize and inform consumers of the 
useful aspects of Live Commerce and the points that can induce pleasure. Second, 
this study conﬁrmed that innovativeness can strengthen the relationship between 
perceived value and intention to use. Therefore, it can help spread Live Commerce 
by actively informing Live Commerce by ﬁnding people who are less reluctant to 
new technologies and enjoy various experiences. Finally, by providing empirical 
information on the importance of values perceived by individuals to Live Commerce 
developers and companies, they can prioritize service and technology development. 
This study has the following limitations. First, this study did not take into account 
other factors while measuring perceived beneﬁts in four aspects: usefulness, playful-
ness, technicality, and economic feasibility. Other factors that may affect the intention

Effects of the Perceived Beneﬁt and Innovativeness of Live Commerce …
39
to use need to be considered through additional analysis in the future. Second, this 
study was conducted on users of Live Commerce and the Live Commerce market 
only in China. Therefore, it is proposed to conduct research on Live Commerce 
consumers in each country as the global Live Commerce market develops and the 
number of users in-creases in the future. 
for the library’s digital material. More customized digital materials will increase 
the number of being used, reduce production and increase efﬁciency. In addition, 
benchmarking also can be an effective solution for inefﬁcient DMUs. For each inef-
ﬁcient DMU, it is able to target any DMU for best practice based on Ref value and 
investigate how to operate aspects with slacks in efﬁcient way. 
References 
1. Silver L, Taylor K (2019) Smartphone ownership is growing rapidly around the world, but not 
always equally. Pew Research Center 
2. La KW, Oh KW (2021) Effects of Wanghong marketing in live commerce on Chinese 
consumers’ purchase intention toward fashion products-Focusing on the mediating effect 
of Wanghong’s characteristics and consumers’ co-experience. J Korea Fash Costume Des 
Association 23(1):19–36 
3. Sun Y, Shao X, Li X, Guo Y, Nie K (2020) A perspective on How live streaming inﬂuences 
purchase intentions in social commerce: An IT affordance perspective. Electron Commer Res 
Appl 40:2020 
4. Kim HW, Chan HC, Gupta S (2007) Value-based adoption of mobile internet: an empirical 
investigation. Decis Support Syst 43(1):111–126 
5. Chen L (2017) The dilemma and countermeasures of internet live commerce service 
supervision. New Media Res 3(3):106–107 
6. Tan C, Jia H, Du G, Jiang D (2018) Analysis of the deﬁnition characteristics, development 
process and business model of webcast. Modern Business. 19:165–168 
7. Shu S, Chen J (2017) Thinking and prospects of online live marketing in publishing industry. 
Sci-Technol & Publication 36(6):33–37 
8. Yin Z (2017) A study on the live marketing of tourism destinations with internet plus 
background. Econ & Trade 17:129–129 
9. Davis FD, Bagozzi RP, Warshaw PR (1989) User acceptance of computer technology: a 
comparison of two theoretical models. Manag Sci 35(8):982–1003 
10. Lee C, Yun H, Lee C, Lee CC (2015) Factors affecting continuous intention to use mobile 
wallet: Based on value-based adoption model. J Soc E-Bus Stud., 20(1) 
11. Oh JC (2017) An empirical study on use-diffusion of AR technology based on VAM: the 
moderating effects of positive TRI. The E-Business Studies. 18(5):225–244 
12. Agarwal R, Karahanna E (2000) Time ﬂies when you’re having fun: Cognitive absorption and 
beliefs about information technology usage. MIS Quarterly:665–694 
13. Dodds WB, Monroe KB, Grewal D (1991) Effects of price, brand, and store information on 
buyers’ product evaluations. J Mark Res 28(3):307–319 
14. Zeithaml VA (1988) Consumer perceptions of price, quality, and value: a means-end model 
and synthesis of evidence. J Mark 52(3):2–22 
15. Blackwell RD, Miniard PW, Engel JF (2001) Consumer behavior, 9th edn. Harcourt, New York 
16. Bolton RN, Lemon KN (1999) A dynamic model of customers’ usage of services: Usage as 
and antecedent and consequence of satisfaction. J Mark Res 36(2):171–186 
17. Duman T, Mattila AS (2005) The role of affective factors on perceived cruise vacation value. 
Tour Manage 26(3):311–323

40
Z. Pan et al.
18. Johnson MD, Herrmann A, Huber F (2006) The evolution of loyalty in-tentions. J Mark 
70(2):122–132 
19. Han J, Kang S, Moon T (2013) An empirical study on perceived value and continuous intention 
to use of smart phone, and the moderating effect of personal innovativeness. Asia Pac J Inf 
Systems 23(4):53–84 
20. Rogers EM (1995) Diffusion of innovations: modiﬁcations of a model for telecommunications. 
Die diffusion von innovationen in der telekom-munikation. Springer, Berlin, Heidelberg, pp 
25–38 
21. Yu J, Lee H, Ha I, Zo H (2017) User acceptance of media tablets: An empirical examination 
of perceived value. Telematics Inform 34(4):206–223 
22. Kim J, Fiore AM, Lee HH (2007) Inﬂuences of online store perception, shopping enjoyment, 
and shopping involvement on consumer patron-age behavior towards an online retailer. J Retail 
Consum Serv 14(2):95–107 
23. Chang YH, Park JG (2010) An exploratory study on barriers of the diffusion of smartphone: 
focusing on the determinants of postponers’ in-novation resistance. Stud. Broadcast. Cult, 
22(2):37–62 
24. Kim C, Mirusmonov M, Lee I (2010) An empirical examination of factors inﬂuencing the 
intention to use mobile payment. Comput Hum Behav 26(3):310–322 
25. Kim YJ, Jung JM, Lee EJ (2011) What drives the adoption and usage of smartphone applica-
tions?: Factors affecting degree of use, continuous use, and recommendation. Korean J Journal 
Commun Stud 55(6):227–252 
26. Kim HJ, Kim DY (2011) Perceptions and usages of smartphone users in the different phases 
of adoption. Korean J Journal Commun Stud 55(4):382–405 
27. Sohn SH, Choi YJ, Hwang HS (2011) Understanding acceptance of smartphone among 
early adopters using extended technology acceptance model. Korean J Journal Commun Stud 
55(2):227–251 
28. Nam ST, Kim DG, Jin CY (2013) A study on the continuous intention to use for Smartphone 
based on the innovation diffusion theory: Considered on the loyalty between users of iOS and 
Android platform. J Korea Inst Inf Commun Engineering 17(5):1219–1226 
29. Wang C (2014) Antecedents and consequences of perceived value in Mobile Government 
continuance use: An empirical research in China. Comput Hum Behav 34:140–147 
30. Lin TC, Wu S, Hsu JSC, Chou YC (2012) The integration of value-based adoption and expec-
tation–conﬁrmation models: An example of IPTV continuance intention. Decis Support Syst 
54(1):63–75 
31. Wang HY, Wang SH (2010) Predicting mobile hotel reservation adoption: Insight from a 
perceived value standpoint. Int J Hosp Manag 29(4):598–608 
32. Hsiao KL, Chen CC (2017) Value-based adoption of e-book subscription services: The roles 
of environmental concerns and reading habits. Telematics Inform 34(5):434–448

A Study on the Effect of Customer 
Experience on the Intention 
to Continuously Use Coffee Store Mobile 
App Based on the Uniﬁed Theory 
of Acceptance and Use of Technology 
(UTAUT) Model 
Sumin Han, Myeongsook Park, Hyojin Yook, and Sungtaek Lee 
Abstract Due to the spread of COVID-19, the use of mobile applications is 
increasing in coffee shops as activities that have been conducted face-to-face are 
being carried out non-face-to-face. The purpose of this paper is to make strategic 
suggestions to positively improve the continuous use intention of coffee shop mobile 
applications by checking the effect of expanded integrated technology acceptance 
model and online review after using the mobile app on perceived value and user 
satisfaction. A non-face-to-face online survey was conducted on mobile application 
users of coffee shops from March 13 to April 10, 2021 to secure a total of 341 
data, and statistical analysis used the PLS structural equation model. As a result, 
it is expected that the satisfaction with the use of coffee shop applications and the 
inﬂuence of online reviews in customer experience will recognize positive values, 
affect the intention to continue using them, and help in planning and marketing. 
Keywords Mobile applications · The Uniﬁed Theory of Acceptance and Use of 
Technology (UTAUT) model · Customer experience · Online reviews · Continuous 
use intention
S. Han envelope symbol · M. Park 
Department of Business Administration, Soongsil University Seoul, Seoul, South Korea 
e-mail: sumin2214@naver.com 
M. Park 
e-mail: myeongsookpark@ssu.ac.kr 
H. Yook · S. Lee 
Department of Art and Technology/Mgt, Soongsil University Seoul, Seoul, South Korea 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_4 
41

42
S. Han et al.
1 
Introduction 
The social change from COVID-19 has led to the non-face-to-face progress of activ-
ities that have been made face-to-face, so the presence of various non-contact related 
online systems, platforms, and mobile-based applications has brought new conve-
nience in our lives. Mobile apps are used as innovative and convenient channels that 
can be accessed online and ofﬂine [1, 2]. Online mobile app services have the conve-
nience of providing accurate information such as menu search and order, delivery, 
and payment [1]. 
According to the “2021 Korea’s Consumer Life Indicators” survey, since the 
spread of COVID-19, digital consumption has doubled from 44.0% in 2019 to 82.1% 
in 2021 (8,207 people out of 10,000 people). The most frequently used types were 
Internet and mobile shopping (65.8%), TV home shopping (40.3%), and SNS plat-
form home shopping (20.8%). Likewise, the use of mobile coffee brands has soared 
[3]. According to Mobile Index, a mobile big data platform company, Starbucks in 
Korea has increased its mobile app subscribers by 1.5 million in 2021 and has over-
come the recession of COVID-19 and continued to expand various IT services linked 
to applications [4]. 
As the number of coffee shops aiming for mobile shopping increases, research on 
what factors or practices should be applied to maintain a continuous mobile shopping 
experience is insufﬁcient. In particular, it is very important to understand what factors 
affect users’ intention to continue using from the perspective of online coffee shops. 
Therefore, based on the Uniﬁed Theory of Acceptance and Use of Technology 
(UTAUT2), this study aims to examine the effect on the continuous use intention of 
coffee mobile app users [5]. 
This study considers it important to improve and develop applications in the 
direction of securing mobile application users of coffee shops and expanding the 
entire market, as well as increasing customer groups’ satisfaction with applications 
of their brands and making them reuse it. The purpose of the study is to enable 
application planning, design, and marketing to reﬂect customer satisfaction due to 
customer group experiences and online reviews. 
2 
Theoretical Background 
2.1 
Coffee Shop Mobile Applications 
The mobile app is deﬁned as an ’innovative and convenient channel for menu search, 
ordering and payment without physical interaction with restaurant staff’, and both 
customers and restaurants are constructed with a variety of innovative characteris-
tics to help redeﬁne issues such as long waiting times, trafﬁc, miscommunication, 
delivery delays, and handling customer complaints [1, 6]. Restaurant operators are

A Study on the Effect of Customer Experience on the Intention …
43
increasingly aware of the importance of apps. This is because mobile applications 
can help improve service quality, customer satisfaction, and loyalty [1]. 
The purpose of this study is to make it convenient for companies or private stores 
to promote their products and brands by using mobile applications, sell products 
and services directly, and use various services provided ofﬂine online. A coffee shop 
mobile application is one of application software running on mobile devices such 
as smartphones and tablet PCs, and is deﬁned as software capable of performing 
location information, product order, and payment services through mobile devices. 
2.2 
Uniﬁed Theory of Acceptance and Use of Technology 
Model (UTAUT2) 
The Uniﬁed Theory of Acceptance and Use of Technology (UTAUT) helps to under-
stand the acceptance factors for actively designing training, and marketing for groups 
that adopt and use new systems. The Uniﬁed Theory of Acceptance and Use of Tech-
nology model integrates and expands three structures into the Uniﬁed Theory of 
Acceptance and Use of Technology (UTAUT), including hedonic motivation, price 
value, and habit, name, age, gender, experience. [5, 7] 
Taqwa Hariguna et al., identiﬁes and empirically tests the main factors predicting 
the customer’s intention to reuse electronic satisfaction and mobile food application 
services (MPAS). The study incorporated customer experience, several variables 
from the Uniﬁed Theory of Acceptance and Use of Technology (UTAUT), online 
reviews, online tracking, and online rating variables, and contributed to verifying the 
role of online reviews in focusing more on e-satisfaction and enabling customers to 
continue to reuse [8]. 
(MFOA) as an innovative and convenient mobile app that allows smartphone users 
to access restaurants, view food menus, order and pay without physical interaction 
with restaurant staff, and studies the impact of mobile food applications (MFOA) on 
customer satisfaction and consistent reuse [1, 5, 6]. 
2.3 
Online Review 
Customer reviews play a positive role for consumers in the decision-making process 
to purchase a variety of products and services, and help customers make important, 
better decisions and improve satisfaction when purchasing products or evaluating 
alternatives [25, 10]. 
The signiﬁcant impact of online reviews predicts the customer’s intention to adopt 
the platform, and inﬂuences consumer decisions in which online reviews lead to 
satisfaction [11]. Consumers who generate online reviews provide critical infor-
mation services to purchasing customers, which have a signiﬁcant impact on all

44
S. Han et al.
purchasing decisions, and help them engage in online communities [12]. Online 
customer reviews have a positive relationship with the features of online reviews 
(perceived usefulness, ease of use, and pleasure) and have a positive impact on sales. 
2.4 
Customer Experience 
Consumers try to measure the value gained from the service with the perception they 
feel after interacting with the system through customer experience (CE) [14, 15]. 
This value is the perceived result of the subjective customer determined through plea-
sure. It also refers to the sensory, emotional, and cognitive effects of the customer’s 
experiences and observations, especially on the customer’s interests, motives, and 
recognition [16]. Customer experience is a perception created after the experience, 
and satisfaction appears when the customer feels positive perception and value. In 
addition, it refers to a positive interrelationship between customers and their experi-
ences and has a correlation with satisfaction [17, 18]. Schmitt deﬁned the experience 
as a stimulus provided by marketing efforts before and after purchase as a personal 
response, arising from direct observation and participation [19]. Schmitt developed 
ﬁve types of strategic experience modules (SEMs) to present marketing factors in a 
total of ﬁve experiences: SENSE, FEEL, THINK, ACT, and RELATE [19–21]. 
3 
Research Method 
3.1 
Research Design 
This paper used the Uniﬁed Theory of Acceptance and Use of Technology in the 
study of performance expectations, effort expectations, social impact, facilitation 
conditions, hedonic motivation from Venkatesh et al., Okumus et al. and the study 
on price difference from Venkatesh et al. Also, Ajzen & Fishbein’s study on habits and 
measurement tools with veriﬁed internal consistency in a study on online reviews by 
Mudambi & Schuff were used [5, 22–25]. For perceived value, Zeithaml’s measure-
ment items, which were revised and supplemented by previous researchers, were 
used, and user satisfaction was measured by measuring methods by Bhattacharjee 
[26, 27]. 
Lastly, the measurement tool of the mobile application user group of coffee shops 
was modiﬁed according to the application usage environment using the measurement 
items presented in Wang and Bhattacharjee’s study [26, 28] (Fig. 1).

A Study on the Effect of Customer Experience on the Intention …
45
Fig. 1 Research model 
3.2 
Hypothesis Setting 
As the number of mobile app users increases due to the social change from COVID-
19, the use of mobile applications in coffee shops is drawing attention To understand 
the continuous use intention of the application, the research was established by setting 
seven factors (performance expectation, effort expectation, social inﬂuence, promo-
tion conditions, price value, pleasure motivation, and habit) presented by Venkatesh 
et al. as independent variables, and setting perceived value and user satisfaction as 
dependent variables (Table 1).
4 
Data Analysis 
4.1 
Demographic Characteristics 
The survey was conducted from February 10th to March 10th, 2022 for a month. Data 
from 341 of the 360 respondents who responded to the survey on a 7-point Likert 
scale, excluding data from 19 unfaithful respondents, were used and the causation

46
S. Han et al.
Table 1 Hypothesis 
Variable
Hypothesis 
Performance 
expectancy 
H1
Performance expectancy will have a positive (+) effect on perceived 
value 
H2
Performance expectancy will have a positive (+) effect on user 
satisfaction 
Effort 
expectancy 
H3
Expectancy on effort will have a positive (+) effect on perceived 
value 
H4
Expectancy on effort will have a positive (+) effect on user 
satisfaction 
Social inﬂuence H5
Social inﬂuence will have a positive (+) effect on perceived value 
H6
Social inﬂuence will have a positive (+) effect on user satisfaction 
Facilitating 
condition 
H7
Facilitating condition will have a positive (+) effect on the perceived 
value 
H8
Facilitating condition will have a positive (+) effect on user 
satisfaction 
Hedonic 
motivation 
H9
Hedonic motivation will have a positive (+) effect on perceived value 
H10
Hedonic motivation will have a positive (+) effect on user satisfaction 
Price value
H11
Price value will have a positive (+) effect on the perceived value 
H12
Price value will have a positive (+) effect on user satisfaction 
Habit
H13
Habit will have a positive (+) effect on perceived value 
H14
Habit will have a positive (+) effect on user satisfaction 
Online review
H15
Online review will have a positive (+) effect on perceived value 
H16
Online review will have a positive (+) effect on user satisfaction 
Perceived value 
H17
Perceived value will have a positive (+) effect on user satisfaction 
H18
Perceived value will have a positive (+) effect on the intention to 
continue using 
User 
satisfaction 
H19
User satisfaction will have a positive (+) effect on the intention to 
continue using
relationship of the variables were analyzed using structural equation from Smart PLS 
3.0. 
The gender of the survey respondents was 46.6% male (n=159) and 53.4% female 
(n = 182). Those in their 50s (36.4%·n = 124) took the most of the respondents, 
followed by those in their 30s (23.5%·n = 80), those in their 40s (17.9%·n = 61), 
those in their 20s (16.1%·n = 55), and those in their 60s (6.2%·n = 21). The frequency 
of use was the highest with 126 people 37.0% once a week, and the average cost of 
using a coffee shop mobile application was less than 5,000 won to 10,000 won, with 
156 people and 45.7% using an individual order, and 44.3% of 151 people used the 
application. As for occupations, general ofﬁce workers were the highest at 30.5% 
with 104 people, while Starbucks was the highest at 73.0% with 249 people.

A Study on the Effect of Customer Experience on the Intention …
47
4.2 
Reﬂective Measurement Model Fit 
This study attempted to conﬁrm the causal relationship by applying the Smart PLS 
structural equation model to check the signiﬁcant path coefﬁcient between each 
variable presented in the research model. The study checked the appropriate model 
composition and the convergent validity and discriminative validity of each item. 
4.2.1
Convergent Validity 
If the outer loading is above 0.7, it can be maintained [29]. The indicator reliability 
and average variance extracted (AVE) should be at least 0.5 [30]. Hence, the analysis 
of the paper eliminated metrics that do not exceed this value. The result is shown in 
(Table 2).
4.2.2
Measurement Model Reliability Veriﬁcation 
Conﬁrmatory factor analysis conﬁrmed that the suitability of the research model 
was satisfactory. The reliability analysis showed that the Construct Reliability (CR) 
was 0.7 or higher and Cronbach’s Alpha was 0.7 or higher which shows that the 
measurement model of this study was reliable [31]. The reliability analysis results 
for the measurement model are shown in (Table 3).
4.2.3
Determination Validity Check Result 
The discriminative validity can be evaluated that if the square root of the average 
variance extracted (AVE) is greater than the correlation coefﬁcient of each latent 
variable, validity is secured between the two latent variables [31]. It was conﬁrmed 
through the Fornell-Larker criterion. 
The results of conﬁrming the discriminative validity of the factors presented in 
the research model of this study are shown in (Table 4).
4.2.4
Path Coefﬁcient Veriﬁcation Results and Hypothesis Veriﬁcation 
Results 
This study conﬁrmed the effect of online review of coffee shop mobile applica-
tion users’ performance expectations, effort expectations, social impact, promotion 
conditions, hedonic motivation, price value, usage habits, and customer experience 
on perceived value and user satisfaction. The overall path coefﬁcient and signiﬁcance 
ﬁgures are shown in (Table 5).

48
S. Han et al.
Table 2 Convergent validity result 
Variable
Measurement 
item 
Outer loading
Indicator reliability
AVE 
Price value
price value1, 
price value2, 
price value3 
0.83,0.833,0.787
0.689,0.695,0.619
0.667 
Effort 
expectancy 
Effort 
expectancy1, 
Effort 
expectancy3, 
Effort 
expectancy4 
0.792,0.867,0.826
0.627,0.752,0.682
0.687 
User 
satisfaction 
User 
satisfaction1 
User 
satisfaction2, 
User 
satisfaction3, 
User 
satisfaction4, 
User 
satisfaction5, 
User 
satisfaction6 
0.826,0.803,0.884, 
0.848,0.767,0.854 
0.682,0.645,0.781, 
0.719,0.588,0.729 
0.691 
Social inﬂuence 
Social inﬂuence2 
Social 
inﬂuence3, 
Social 
inﬂuence4, 
Social 
inﬂuence5, 
Social inﬂuence6 
0.844,0.819,0.809, 
0.883,0.841 
0.712,0.671,0.654, 
0.780,0.707 
0.664 
Performance 
expectancy 
Performance 
expectancy1, 
Performance 
expectancy2, 
Performance 
expectancy3, 
Performance 
expectancy4, 
Performance 
expectancy5 
0.793,0.837,0.821 
,0.754,0.872 
0.629,0.701,0.674, 
0.569,0.760 
0.666 
Habit
Habit1, Habit2, 
Habit3, 
Habit4, Habit5 
0.89,0.9,0.929, 
0.929,0.907 
0.792,0.810,0.863, 
0.863,0.823 
0.830
(continued)

A Study on the Effect of Customer Experience on the Intention …
49
Table 2 (continued)
Variable
Measurement
item
Outer loading
Indicator reliability
AVE
Online review
Online review1, 
Online review2, 
Online review3, 
Online review4, 
Online review5 
0.865,0.88,0.909, 
0.869,0.744 
0.748,0.774,0.826, 
0.755,0.554 
0.731 
Perceived value
Perceived 
value2, 
Perceived 
value4, 
Perceived 
value5, 
Perceived value6 
0.810,0.909, 
0.882,0.864 
0.656,0.826, 
0.777,0.746 
0.752 
Continued 
intention 
Continued 
intention1, 
Continued 
intention2 
Continued 
intention3 
0.780.929,0.864
0.608,0.863,0.746
0.739 
Facilitating 
condition 
Facilitating 
condition2 
Facilitating 
condition3 
Facilitating 
condition5 
0.8410.746, 
0.818 
0.707,0.557,0.669
0.644 
Hedonic 
motivation 
Hedonic 
motivation1, 
Hedonic 
motivation2, 
Hedonic 
motivation3, 
Hedonic 
motivation4, 
Hedonic 
motivation5 
Hedonic 
motivation6 
0.837,0.866, 
0.902,0.708, 
0.853,0.873 
0.701,0.750,0.814, 
0.501,0.728,0.762 
0.709
5 
Conclusion 
This study attempted to examine the effect of consumers’ customer experience of 
coffee shop mobile apps based on the expanded integrated technology acceptance 
model on the intention to continue use as interest in mobile applications increases 
in the coffee shop industry due to the social change from COVID-19. As a result, it 
was conﬁrmed that the mobile application of coffee shops is convenient, easy to use, 
socially inﬂuenced, and high values the aspect of usage habits. The above customer

50
S. Han et al.
Table 3 The Reliability Analysis Results for the Measurement Model 
Factors
Cronbach’s alpha
rho_A
Composite reliability 
Price value
0.752
0.759
0.759 
Effort expectancy
0.776
0.789
0.868 
User satisfaction
0.91
0.914
0.93 
Social inﬂuence
0.898
0.914
0.922 
Performance expectancy
0.875
0.884
0.909 
Habit
0.949
0.952
0.961 
Online review
0.908
0.923
0.931 
Perceived value
0.889
0.894
0.924 
Continued intention
0.823
0.864
0.894 
Facilitating condition
0.727
0.743
0.844 
Hedonic motivation
0.916
0.920
0.936
Table 4 Checking the discriminative validity based on fornell-larcker standard 
1
2
3
4
5
6
7
8
9
10
11 
1
0.817 
2
0.298
0.829 
3
0.489
0.378
0.831 
4
0.427
0.242
0.644
0.815 
5
0.459
0.423
0.595
0.542
0.816 
6
0.342
0.157
0.524
0.432
0.465
0.911 
7
0.411
0.295
0.514
0.553
0.289
0.341
0.855 
8
0.418
0.367
0.640
0.62
0.602
0.595
0.506
0.867 
9
0.508
0.277
0.690
0.578
0.543
0.733
0.428
0.647
0.860 
10
0.319
0.228
0.307
0.336
0.405
0.310
0.336
0.429
0.165
0.803 
11
0.521
0.083
0.54
0.636
0.471
0.449
0.462
0.553
0.603
0.158
0.842 
1 = price value, 2 = Effort Expectancy, 3 = User Satisfaction, 4 = Social Inﬂuence, 5 = Performance 
Expectancy, 6 = Habit, 7 = Online Review, 8 = Perceived Value, 9 = Continued Intention, 10 = 
Facilitating Condition, 11 = Hedonic Motivation
experience is more directly effective in customer satisfaction, but user satisfaction 
is also a variable inﬂuenced by perceived value, so an experience that can increase 
together should be organized. Here, the price value was rejected for perceived value, 
and the promotion conditions and hedonic motives were also rejected for user satis-
faction. Therefore, I think that the coffee shop mobile app should be planned and 
designed to enhance the enjoyment and pleasure of consumers. In addition, various 
service experiences such as membership registration and accumulation of reserves 
are provided to increase higher usability. Not only that, the fact that their shopping 
experiences are connected through mobile app platforms to social networks and

A Study on the Effect of Customer Experience on the Intention …
51
Table 5 Hypothesis veriﬁcation results 
Hypothesis
Weight
P Values
Result 
Price value —> User satisfaction
0.104
0.027*
Accept 
Price value —> Perceived value
–0.057
0.253
Reject 
Effort expectancy_ —> User satisfaction
0.105
0.013*
Accept 
Effort expectancy_ —> Perceived value
0.145
0***
Accept 
User satisfaction —> Continued intention
0.467
0***
Accept 
Social inﬂuence —> User satisfaction
0.246
0***
Accept 
Social inﬂuence —> Perceived value
0.171
0***
Accept 
Performance expectancy —> User satisfaction
0.183
0***
Accept 
Performance expectancy —> Perceived value
0.174
0***
Accept 
Habit —> User satisfaction
0.154
0.002**
Accept 
Habit —> Perceived value
0.275
0***
Accept 
Online review —> User satisfaction
0.132
0.017*
Accept 
Online review —> Perceived value
0.121
0.004**
Accept 
Perceived value —> User satisfaction
0.148
0.018*
Accept 
Perceived value —> Continued intention
0.348
0***
Accept 
Facilitating condition —> User satisfaction
–0.068
0.068
Reject 
Facilitating condition —> Perceived value
0.132
0***
Accept 
Hedonic motivation —> User satisfaction
0.033
0.603
Reject 
Hedonic motivation —>Perceived value
0.179
0***
Accept 
* p < 0.05, ** p < 0.01, *** p < 0.001
online reviews are shared based on customer trust can be seen as an important factor 
in marketing. 
In an academic sense, the sub-factors of the inﬂuence of the coffee shop mobile 
application user group on the customer experience were veriﬁed through a new 
extended integrated technology acceptance model. Also, this study found that online 
reviews on the mobile app have an effect on the continued use of customers. 
As for future study, it is necessary to study the value of online reviews by dividing 
the customer experience into direct and indirect experiences. Moreover, a further 
study that combines customer experience and UTAUT-based innovation technology 
acceptance models is needed. 
References 
1. Okumus B, Bilgihan A (2014) Proposing a model to test smartphone users’ intention to use 
smart applications when ordering food in restaurants. J Hosp Tour Technol 5(1):31–49 
2. Wang YS, Tseng TH, Wang WT, Shih YW, Chan PY (2019). Developing and validating a 
mobile catering app success mode. 77:19–30

52
S. Han et al.
3. Korea Consumer Resources and Consumer Life Indicators Survey Results (2021) See Policy 
Report 
4. Statistics Korea, January 2022 and annual online shopping trends (January 2022) References 
5. Venkatesh V, Thong JYL, Xu X (2012) Consumer acceptance and use of information 
technology: Extending the uniﬁed theory of acceptance and use of technology. MIS Q 
36(1):157–178 
6. Alalwan AA (2020) Mobile food ordering apps: An empirical study of the factors affecting 
customer e-satisfaction and continued intention to reuse. J Inf Management:28–44 
7. Venkatesh V, Morris MG, Davis GB, Davis FD (2003) User acceptance of information 
technology: toward a uniﬁed view. MIS Q 27(3):425–478 
8. Harigunaa T, Ruangkanjanasesb A (2020) Elucidating E-satisfaction and sustainable intention 
to reuse mobile food application service, integrating customer experiences. Online Track, 
Online Rev. 122–138 
10. Kohli R, Devaraj S, Mahmood MA (2004) Understanding determinants of online consumer 
satisfaction: a decision process perspective. J Manag Inf Syst 21(1):115–135 
11. Cheung CM, Lee MK, Rabjohn N (2008) The impact of electronic word-of-mouth: The 
adoption of online opinions in online customer communities. Internet Res 18(3):229–247 
12. Mathwick C, Mosteller J (2017) Online reviewer engagement: A typology based on reviewer 
motivations. J Serv Res, 20(2):204–218. J Manag Inf Syst, 21(3):111–147 
13. Elwalda A, Lü K, Ali M (2016) Perceived derived attributes of online customer reviews. Comput 
Hum Behav 56:306–319 
14. Math wick C, Malhotra N, Rigdon E (2001) Experiential value: conceptualize-toon, measure-
ment and application in the catalog and Internet shopping environment. J Retail 77:39–56 
15. Vera J, Trujillo A (2013) Service quality dimensions and superior customer perceived value in 
retail banks: an empirical study on Mexican consumers. J Retail Consum Serv 20:579–586 
16. von Wallpach S, Kreuzer M (2013) Multisensory sculpting (MSS): eliciting embodied brand 
knowledge via multisensory metaphors. J Bus Res 66:1325–1331 
17. Kim HR (2005) De Int J Hosp veloping an index of online customer satisfaction. J Financ Serv 
Mark 10:49–64 
18. Torres EN, Fu X, Lehto X (2014) Examining key drivers of customer delight in a hotel 
experience: a cross-cultural perspective. Int J Hosp Manag 36:255–262 
19. Schmitt B (1999) Experiential marketing: how to get customers to sense feel think act relate 
to your company and brands. The Free Press, New York 
20. Schmitt BH (2003) Customer experience management. John Wiley & Sons, NY 
21. Kim G-Y, Cho Y-B (2019) Customer experience marketing using coffee shop impact on 
customer satisfaction and loyalty. Restaur Manag Soc, 22(2):7–24 
22. Okumus B, Ali F, Bilgihan A, Ozturk AB (2018) Psychological factors inﬂuencing customers’ 
acceptance of smartphone diet apps when ordering food at restaurants. Int J Hosp Manag 
72:67–77 
23. Ajzen I, Fishbein M (2005) The inﬂuence of attitudes on behavior. In: Albarracín D, Johnson 
BT, Zanna MP (eds) The handbook of attitudes. Erlbaum, Mahwah, NJ, pp 173–221 
24. Filieri R (2015) What makes online reviews helpful? A diagnostic city-adoption framework to 
explain informational and normative inﬂuences in e-WOM. J Bus Res 68(6):1261–1270 
25. Mudambi SM, Schuff D (2010) Research note: What makes a helpful online review? A study 
of customer reviews on Amazon.com. MIS Q, 34(1):185–200 
26. Bhattacherjee A (2001) Understanding information system continuance: an expectation 
conﬁrmation model. MIS Q 25:351–370 
27. Zeithaml VA (1988) Consumer perceptions of price, quality, and value: A means-end model 
and synthesis of evidence. J Mark 52(3):2–22 
28. Wang YS (2008) Assessing e-commerce systems success: a respeciﬁcation and valid ation of 
the DeLone and McLean model of IS success. Inf Syst J 18(5):529–557 
29. Hair JF, Ringle CM, Sarstedt M (2011) PLS-SEM: Indeed a silver bullet. J Mark Theory Pract 
19(2):139–152

A Study on the Effect of Customer Experience on the Intention …
53
30. Chin, WW (1998) The partial least squares approach to structural equation modeling. Mod 
Methods Bus Res 295(2), 295–336 
31. Shin G-K (2013) Follow Amos 20 statistical analysis centered on master’s and doctorate degrees 
and academic thesis writing. A book publication hearing

A Study on Parallel Recommender 
System with Stream Data Using 
Stochastic Gradient Descent 
Thin Nguyen Si, Trong Van Hung, Dat Vo Ngoc, and Quan Ngo Le 
Abstract Stochastic gradient descent (SGD) and Alternating least squares (ALS) are 
two popular algorithms applied on matrix factorization. Moreover recent researches 
pay attention to how to parallelize them on daily increasing data. About large-scale 
datasets issue, however, SGD still suffers with low convergence by depending on the 
parameters. While ALS is not scalable due to the cubic complexity with the target time 
rank. The remaining issue, how to operate system, almost parallel algorithms conduct 
matrix factorization on a batch of training data while the system data is real-time. 
In this work, the authors proposed FSGD algorithm overcomes drawbacks in large-
scale issue base on coordinate descent, a novel optimization approach. According 
to that, algorithm updates rank-one factors one by one to get faster and more stable 
convergence than SGD and ALS. In addition, FSGD is feasible to parallelize and 
operates on a stream of incoming data. The experimental results show that FSGD 
performs much better in solving the matrix factorization issue compared to existing 
state-of-the-art parallel models. 
Keywords Matrix factorization · Stochastic gradient descent · Alterating least 
squares
T. N. Si envelope symbol · T. Van Hung · D. V. Ngoc · Q. N. Le 
Vietnam-Korea University of Information and Communication Technology, Da Nang City, 
Vietnam 
e-mail: nsthin@vku.udn.vn 
T. Van Hung 
e-mail: vhtrong@vku.udn.vn 
D. V. Ngoc 
e-mail: vndat@vku.udn.vn 
Q. N. Le 
e-mail: nlquan@vku.udn.vn 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_5 
55

56
T. N. Si et al.
1 
Introduction 
With the explosion of information, human access to necessary information with 
more difﬁculties, which is called information overload problem. The way to solve 
this problem is through recommender system and search engine. With recommender 
systems, the key approach is how to support users ﬁnd their products precisely [1]. 
Collaborative ﬁltering using matrix factorization has been considered as one of the 
best approaches for recommender systems. And there has been a good deal of work is 
to the design a scalable and fast model for large-scale matrix factorization problems 
[2, 4]. 
Given up permR element of double struck upper R Superscript m times n, rating matrix with m is the number of users, n is the number of 
items, and up p er R Subscript i j is the rating which user i sign to item j [5]. The following formulation 
is matrix factorization problem: 
Subs
cript
 
left par
e
nt h e sis u
p er  
W c omma upper 
X r ight pa
ren
thesis Superscript m i n Baseline sigma summation Underscript left parenthesis i comma j element of upper D right parenthesis Endscripts left parenthesis upper R Subscript i j Baseline minus bold italic w Subscript i Superscript upper T Baseline bold italic x Subscript j Baseline right parenthesis squared plus lamda left parenthesis double vertical bar upper W double vertical bar Subscript upper F Superscript 2 Baseline plus double vertical bar upper X double vertical bar Subscript upper F Superscript 2 Baseline right parenthesis
In which, upper D is the set of all observed ratings; double vertical bar upper W double vertical bar Subscript upper F and double vertical bar upper X double vertical bar Subscript upper F are the Frobenius 
norm; bol
d italic x Subscript j Superscript upper T and bol
d italic w Subscript i Superscript upper T are the j t hand the i t hrow vectors of the upper X and upper W matrices, respec-
tively; lamdais the regularization parameter. The goal of problem (1) is to approximate 
the ulity matrix up per R almost equals upper W period upper X Superscript upper T (upper W and upper X are low-rank matrix with rank is k). 
In survey, the authors observe that stochastic gradient descent (SGD) and alter-
nating least squares (ALS) gain much attention and widely applied for matrix factor-
ization [2, 4, 6]. With ALS, each low-rank matrix is alternatively ﬁxed while the 
other is optimized using ridge regression or least-squares regression [7]. Although 
the time complexity is upper O  left  parenthesis StartAbsoluteValue upper D EndAbsoluteValue k squared plus left parenthesis m plus n right parenthesis k cubed right parenthesison each iteration, [2] assume that ALS 
is suitable to parallelization. Zhou et al. [8] show a parallelized application of ALS 
using parallelized MATLAB, while Sebastian Schelter has proposed a parallelized 
application of ALS with the Apache Mahout project using the Hadoop MapReduce 
framework [9]. 
Compare to ALS, SGD algorithm chooses rating value with random and update 
a vector bold italic w and a vector bold italic x by set a small step of the direction opposite to gradient in 
the loss function. The time complexity of SGD is upper O left parenthesis StartAbsoluteValue upper D EndAbsoluteValue k right parenthesison each iteration, which 
is simpler than ALS. However, SGD needs to get more iterations to achieve a good 
enough training, and the results is sensitive to the set of the learning rate. Moreover, 
SGD is serial and difﬁcult to be parallelized for handling large-scale problems [8– 
10]. Gemulla et al. [10] show a parallelized application of SGD using MapReduce. 
The study conducts on batch data. 
Stem from above issues, this work aims to design an easily parallelizable and efﬁ-
cient approach for matrix factorization on streaming data. With survey, [11] and [12] 
have assumed that coordinate descent approaches are effective for matrix factoriza-
tion. This motivates the authors to adapt coordinate descent method for (1). In detail, 
this work propose USSGD algorithm, which performs with faster running time and 
can be feasible to paralize. The key contributions of this work include:

A Study on Parallel Recommender System with Stream Data Using …
57
– Propose a efﬁcient and scalable USSGD method which has the time complexity 
lower than ALS, and gain a faster convergence than SGD. 
– Being feasible to paralize on distributed recommender systems. 
– Showing an approach for operating system on streaming data. In this part, the 
authors start with an empty data points. These data points are stream in an online 
process before execute full data points. 
The structure of this study is designed as follows. An brieﬂy introduction to SGD, 
ALS and the developing to parallelize in Sect. 2. The authors then present issues to 
upgrade in Sect. 3. In Sect. 4, the author show detail to FSGD method. Data and 
evaluation are describe in Sect. 5. Finally, Sect. 6 is conclusion. 
2 
Parallel Algorithm for Matrix Factorization 
2.1 
Alternating Least Squares 
If we ﬁx either upper X or upper W from (1), the problem becomes a quadratic formula with a 
globally optimal approach instead of becoming a non-convex problem. Sterm from 
that, ALS alternately changes between keeping upper W ﬁxed while optimizing upper X and 
execute conversely [13]. As a result, ALS monotonically decreases the target function 
value in (1) to convergence. 
From this alternating optimization process, (1) can be further divided into vari-
ously independent least squares issues. Speciﬁcally, if we minimize over upper W and ﬁx 
upper X, the optimal bol
d italic w Subscript bold italic i Superscript bold asterisk can be set independently from other rows of upper W by process the 
regularized least squares issue [13]. 
m i n
 
Subscrip
t
 b o l d ita
li c w
 Su b Subscript
 bold italic i Baseline sigma summation Underscript left parenthesis j element of upper D Subscript i Baseline right parenthesis Endscripts left parenthesis upper R Subscript i j Baseline minus bold italic w Subscript i Superscript upper T Baseline bold italic x Subscript j Baseline right parenthesis squared plus lamda double vertical bar bold italic w Subscript bold italic i Baseline double vertical bar squared
which leads to the different process 
w S
ub scrip t 
i S up ers cript 
aster isk 
Baseline equals left parenthesis upper X Subscript upper D Sub Subscript i Subscript Superscript upper T Baseline upper X Subscript upper D Sub Subscript i Subscript Baseline plus lamda upper I right parenthesis Superscript negative 1 Baseline upper X Superscript upper T Baseline r Subscript i
where up pe
r X Subscript upper D Sub Subscript i Superscript upper T as the submatrix including columns Sta rt Se t bo ld italic x Subscript bold italic j Baseline colon j element of upper D Subscript i Baseline EndSet, and bol
d italic r Subscript bold italic i Superscript bold italic upper T is the ith 
row of upper R matrix with unobservable entries set by zeros. To execute each bol
d italic w Subscript bold italic i Superscript bold asterisk , ALS  
take upper O left parenthesis StartAbsoluteValue upper D Subscript i Baseline EndAbsoluteValue k squared right parenthesistime to make the matrix up pe
r X  S ubscript upper D Sub Subscript i Superscript upper T Baseline upper X Subscript upper D Sub Subscript i size k times k and addition upper O left parenthesis k cubed right parenthesis
time to take the least squares issue. Thus, the total time complexity of an iteration is 
upper O left parenthesis StartAbsoluteValue upper D EndAbsoluteValue k squared plus left parenthesis m plus n right parenthesis k cubed right parenthesis
About parallelization problem, [2] assume that ALS algorithm can be feasible to 
parallelize by a row-by row process as every row of upper X or upper W updated independently. 
In a distributed recommentder system, however, the parallelization of ALS becomes 
more difﬁcult when upper X or upper W over the memory capacity of a excuting node.

58
T. N. Si et al.
2.2 
Stochastic Gradient Descent 
As stated earlier, many approaches have been proposed to over the matrix factoriza-
tion problem with large-scale data. Among them, SGD is a popular approach because 
it is suitable for the non-convex problem [14]. The main idea of SGD algorithm is 
that a rating left parenthesis i comma j right parenthesisis randomly taken from upper D, and the corresponding values bold italic w Subscript bold italic i and 
bo ld italic x Subscript bold italic j are updated by formula in Algorithm 1. 
wherein: etais the learing rate. With every rating left parenthesis i comma j right parenthesis, SGD  takes  upper O left parenthesis k right parenthesisconsecutive to 
set bold w Subscript bold italic i and bo ld italic x Subscript bold italic j. If we need StartAbsoluteValue upper D EndAbsoluteValueconsecutive updates in one iteration of SGD algorithm, 
the total time complexity per an iteration is thus only upper O left parenthesis StartAbsoluteValue upper D EndAbsoluteValue k right parenthesis. If compared to ALS, 
SGD is faster in terms of total time complexity for each iteration, however it takes 
more iterations than ALS to gain a good enough model. 
Recently, several update process to parallelize SGD have been showed. For 
instance, “delayed updates” are showed in [6] and [15]. A lock-free method called 
HogWild is showed in [10], in which the overwriting problem is ignored depend on 
the situation that the probability for updating same row of upper W or upper H is very small 
when upper R is sparse. For matrix factorization, [10] and [8] showed distributed SGD 
(DSGD) algorithm, which partitions upper R into different blocks and updates other inde-
pendent blocks at the same time. Thus, DSGD can be considered to an exact SGD 
implementation. 
2.3 
Distributed Stochastic Gradient Descent 
Suppose that we parallelize SGD algorithm to distributed system with d processors. 
This can be conducted by dividing the matrix upper R into blocks and set these blocks into 
different processors [16]. The issue is that two processors executing on two blocks 
may update same column of upper X or the same column of upper W. We need to distribute the 
blocks, which avoids conﬂicting updates. In algorithm 1, SGD updates only column 
bold italic w Subscript bold italic i and column bo ld italic x Subscript bold italic j in earch left pare n thesis i comma j comma upper R Subscript bold italic i bold italic j Baseline right parenthesis. If we divide matrix  upper R into d squared blocks with size 
left parenthesis m divided by d right parenthesis times left parenthesis n divided by d right parenthesis, matrix  upper W into d blocks with size k times  left parenthesis m equals d right parenthesis, and matrix upper X into d
blocks with size k times  left parenthesis n equals d right parenthesisas shown in Fig. 1. Note that, with for all left parenthesis p comma q right parenthesis element of left bracket d right bracket, the  
data in block upper B Subscript p q impact only the columns of u p per W pand the columns of upper V Subscript q.

A Study on Parallel Recommender System with Stream Data Using …
59
Fig. 1 Block partitioning of 
matrices 
R=       
B11 
B12 
... 
B11 
B11 
B11 
... 
B11 
B11 
B11 
... 
B11 
W= 
W1 
W1 
 
W1 
X= 
W1 
W1 
 
W1 
Consiquently, if the columns and rows of two blocks upper B Subscript p q and upper B Subscript p prime q prime are not overlap 
(i.e.,p  not equals p prime and q  not equals q prime), then the update process is executed by a processor computed 
on block upper B Subscript p q will not conﬂict to those excuted by a processor computed on block 
upper B Subscript p prime q prime. Gemulla et al. [10] name a set of d blocks whose columns and rows are not 
overlap a stratum. Thus, the processors can compute on a different block in the same 
stratum without forming conﬂicting updates. 
3 
Issues in Existing Parallel Matrix Factorization 
In this section, the authors point out that DSGD method mentioned in Sect. 2 may 
suffer some issues when they are used in a parallel environment. These issues are 
memory discontinuity and locking issues. The authors introduce what these issues 
are and show how they execute in performance degradation.

60
T. N. Si et al.
3.1 
Locking Issue 
With a parallel method, to enhance the performance, keeping all threads always busy 
is very important. The locking issue occurs when a thread idles due to wait the other 
threads. In DSGD algorithm, if s threads are executed, then according to process, s 
independent blocks will be updated in a batch. If the executing time for each block is 
different, then a thread which ﬁnish earlier should be to wait for executing threads. 
The locking issue become more serious if R matrix is unbalanced. The reason is, 
observed ratings are not uniformly set across all positions of R matrix. In situation, 
the thread executing a block for fewer ratings should be to wait for other threads. 
3.2 
Memory Discontinuity 
When a system accesses data in a memory discontinuously, it will suffer from a high 
performance degradation and cache-miss rate. Most solutions for matrix factorization 
including DSGD and HogWild randomly take instances from a block of upper R (or from 
R) to update. We assume that this process as the random method. Though the random 
method generally execute a good convergence, it may takes a memory discontinuity 
seriously. Because not only all rating randomly accessed, but also user/item become 
discontinuous. And the seriousness of the memory discontinuity differ from different 
methods. For example in HogWild, every thread randomly takes instances among upper R
matrix independently, so it take from memory discontinuity of upper R, upper P, and upper Q matrix. 
With DSGD, in contrast, although ratings in each block are randomly picked, we can 
easily set the update order to decrease the memory discontinuity. 
4 
Proposed Algorithm 
In this work, the authors propose two approaches, lock-free and partial random 
approach, to respectively tackle all issues mentioned in Sect. 3. Finally, the author 
design training examples as a stream data. 
4.1 
Lock Free 
The authors follow DSGD to divided upper R into blocks and set a maker to keep s threads 
busy when running a set of different blocks. With a block up p er B Subscript i j independent from other 
blocks executing we consider it as a free block. Of course, it will be a non-free block. 
If a thread ﬁnishes its job, the maker will assigns a new block which take following

A Study on Parallel Recommender System with Stream Data Using …
61
0
1
 
0
1
2
 
x 
0
x 
x 
0 
1 
x 
 
 
x
1 
2 
x 
 
(a)
 block 
(b) 
 block 
Fig. 2 An illustration for divided bold italic upper R matrix 
two criteria: (1) a free block; (2) having the smallest number of past updates among 
free blocks. 
Wherein, the number of updates show how many times the block has been 
processed. With the second criterion we can keep a similar number for updates 
on each block. In situation, two or more blocks take the aforementined two criteria, 
the model randomly choose one. Given s threads, FSGD divided upper R matrix into at 
least lef t parenthe sis s plus 1 right parenthesis times left parenthesis s plus 1 right parenthesisblocks. For example with two threads: uper T 1 is a thread which 
is updating ﬁx block and uper T 2 is a thread which just ﬁnished with a block, receiving a 
new job from the maker. If we divided upper R into 2 times 2 blocks presented in Fig. 2a, then 
uper T 2 just has only one choice: the left block. If uper T 1 gets a new job, a similar situation 
happens. Because uper T 1 and uper T 2 always execute the same block, the left two blocks are 
never executed. However, uper T 2 take three choices b Subscript 1 comma 1, b Subscript 1 comma 2 and b Subscript 2 comma 1 when receiving a 
new block if we devided upper R matrix into 3 times 3 blocks shown in Fig. 2b. 
4.2 
Parital Random 
In contrast to the random method, to gain memory continuity, it is helpful to consider 
an ordered method to sequentially choose rating instances by item identities or user 
identities. This approach is shown in Fig. 3. According to that, upper P matrix can be 
processed continuously. If we conduct the order of items, alternatively, then the 
continuous process of upper Q will be set. With upper R matrix, if the order of choosing rating 
instances is set, we could store upper R matrix into memory in the same order is to ensure 
the continuous process. Although the ordered approach can process data in a more 
continuous handler, empirically the work show that it is not stable.
A random process of variables/data may be valuable for the convergence base 
on aforementioned experiment. This feature has been proved in related optimization 
methods. For instance, in coordinate descent technique, to tackle some optimiza-
tion issues, Chang et al. proved that a random process rather than a sequential order

62
T. N. Si et al.
Fig. 3 Ordered method
process to update properties signiﬁcantly upgrades the convergence speed [17]. To 
agreement between convergence speed and data continuity, in FSGD, the author 
propose a partial random technique, which takes ratings in a block orderly, however 
randomizes the choosen blocks. Although proposed model’s scheduling is close to 
identify by selecting blocks having the smallest numbers of processes, the random-
ness feature can be improved by dividing upper R matrix into many blocks. At any time 
point, then, some blocks have been accessed by the same number of times, which 
the scheduler can randomly take one of them. The experiments show that FSGD 
enjoys both excellent RMSE and fast convergence. Some related approaches have 
been explored in Gemulla et al. [10]. Their results is opposite to experimental results 
in this work. A suitable reason is that they consider to training loss while this work 
consider to RMSE on the testing set. 
4.3 
Stream Algorithm 
In this scenario, training examples come as a continuous stream. The model is to begin 
training process even if system have not all the data. The reason is that modifying a 
master to allocate dynamically blocks to workers rather than allocating a work with 
sets of strata. More detail, the master takes randomly a free block uniformly, locks 
all block in the same column and row, and allocates a worker to execute the chosen 
block. When a worker ﬁnishes its job, it gives a reply message to the master, then 
unlocks to the blocks with the appropriate column and row. 
The workers is divided to store the training examples. In detail, each worker 
stores examples that is necessary to process while the master play a role to route the 
examples to the corresponding worker. Storage of the item matrix upper X and user matrix 
upper W is also splitted among various data stores called matrix stores. 
At any point in time, therefore, there are no process to store the all data set. 
Algorithm 3 describe all process with stream data of FSGD. Notice that algorithm did 
not identify a termination condition, because it is applied to continuously execute an 
unending stream of data. For the purposes to test, however, it should be advantageous 
to limit a number of times to each training example. With implementation, the model 
keep a counter to each training example then stop training when each example was 
processed with a speciﬁc number of times.

A Study on Parallel Recommender System with Stream Data Using …
63
4.4 
FSGD Algorithm 
Algorithm 3 gives the overall process of FSGG. According to the mentioned discus-
sion, FSGD ﬁrst randomly deranges upper R matrix to avoid data imbalance. It, then, 
divides upper R matrix into at least lef t parenthe sis s plus 1 right parenthesis times left parenthesis s plus 1 right parenthesisblocks and uses the partial random 
method showed in part B Sect. 4 by sorting every block by item (or user) identities. 
Finally, it creates a maker and launches s threads. All working threads are stopped 
by a message from maker after a required number of iterations was reached. The 
pseudocode of each working thread an the maker are shown in algorithms 4 and 
algorithm 5. Invoking get_job help continuously return a block from the maker for 
each working thread. Of course the return block has to meet two criteria mentioned 
in part A Sect. 4. After a working thread receives a new block, it access ratings in 
that block with an ordered manner (part B Sect. 4). Finally, the thread invokes put_ 
job from the maker to update continuously the number of times which that block has 
been processed.

64
T. N. Si et al.
Table 1 The statistics for 
data
Dataset
Movielens
Yahoo-Music
Netﬂix 
m
71,567
1,000,990
2,649,429 
n
65,133
624,961
17,770 
StartAbsoluteValue upper D EndAbsoluteValue
9,301,274
252,800,275
99,072,112
Star tAbsoluteValue upper D Superscript upper T e s t Baseline EndAbsoluteValue
698,780
4,003,960
1,408,395 
k
40
100
40 
StartAbsoluteValue lamda Subscript upper P Baseline EndAbsoluteValue
0.05
1
0.05 
StartAbsoluteValue lamda Subscript upper P Baseline EndAbsoluteValue
0.05
1
0.05 
gamma
0.003
0.0001
0.002 
5 
Data and Evaluation 
This section, the authors describe the details about experimental settings, and how 
to compare FSGD with other parallel model in matrix factorization. 
5.1 
Data 
This work considers three public datasets to the experiment: Movie-lens, Yahoo-
Music and Netﬂix. These datasets are public in the research to test the performance 
of matrix factorization model [3, 7]. The initial training/test split is applied for repro-
ducibility. With Yahoo-Music, because the test set is not available, the authors take 
the last four ratings from each user to test, while the remaining ratings is to train. 
The initial values of regularization parameter lamda Subscript upper P and lamda Subscript upper Q are chosen randomly 
from {1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001}. With platform, this work use 12 thread 
with 12 physical cores in all experiments. The detail information of data is shown in 
Table 1. 
5.2 
Evaluation 
The metric this work apply to evaluate is RMSE and speedup indicator. If RMSE 
metric is popular in recommender system, speedup is an indicator to compare effec-
tiveness in parallel model. The methods using to compare include: ALS, HogWild, 
DSGD and proposed mode FSGD. The authors set up the same framework for all 
methods. For experiment, this work conducts three implementations: 
– Comparison on RSME versus training time 
– Comparison on speedup 
– Comparison FSGD on Batch data and Stream Data.

A Study on Parallel Recommender System with Stream Data Using …
65
Fig. 4 Speedup among 
methods with Yahoo-Music 
data 
6 
Result and Discussion 
6.1 
Comparison on Speedup 
Speedup is another important measurement to evaluate parallel model—when the 
number of cores is mor, how faster the parallel model is. To test the speedup indicator, 
the authors run each parallel algorithm on yahoo-music data with different numbers 
of cores (value from 1 to 8), then measure the total running time for iteration one by 
one. From Fig. 4, FSGD outperforms HogWild and DSGD but is comparable with 
ALS. 
This can be understood by cache-miss rate of each algorithm. FSGD aimt to 
improve drawbacks of DSGD and HogWild while ALS access variables at contiguous 
memory spaces. All of them prefer better locality. Due to the randomness, in contrast, 
two consecutive updates of SGD usually process non-contiguous properties in upper X and 
upper W, which raises the cache-miss rate. Let give a ﬁxed size for cache, time spent when 
loading data from cache to memory becomes the bottleneck for HogWild and DSGD 
to gain better speedup while the number of cores raises. 
6.2 
Comparison on RMSE and Training Time 
Figure 5 show the test training time and RMSE among various parallel matrix factor-
ization algorithms: FSGD, ALS, HogWild and DSGD. Among the three parallel 
Stochastic Gradient algorithms, FSGD is faster than HogWild and DSGD. This is 
reasonable because FSGD is proposed to effectively tackle issues discussed in Sect. 3. 
With DSGD, however, the authors must note that it can be feasible to incorporate

66
T. N. Si et al.
Fig. 5 Compare on RSME and training time 
similar approaches (for example the partial random technique) to enhance its perfor-
mance. About training time, we can assume that when data is larger, the effective 
of parallel model is more obvious (for example in Nextﬂict data). Generally, ALS 
is the fastest one at the beginning but still slower than FPSG at the end. Because 
the optimization issue of matrix factorization problem is nonconvex. As mentioned 
explaining, the nonconvex problem becomes a quadratic formula with a globally 
optimal approach instead of a non-convex problem in ALS. 
6.3 
Comparison FSGD on Batch Data and Stream Data 
This implement experimented running both Streaming FSGD as well Batch FSGD 
on Movie-Lens data. For each algorithm, the authors applied a cluster of 10 nodes 
to parallelize computation. With Batch FSGD, the number of iterations shows the 
number of times from full set in. With Streaming FSGD the number of iterations 
shows the number of times which a data point was permitted to be processed. For 
instance, if we take the number of iterations with 5, then, data streams on and blocks 
are set to compute, once the data point has been executed 5 times, it will be never used 
to process again. In this manner, model can conduct the number of times which each

A Study on Parallel Recommender System with Stream Data Using …
67
Fig. 6 Plot RMSE and time 
elapsed 
data point is processed and terminate when all data points in system have streamed 
on and been executed a maximum number of times. The result is shown in Fig. 2. It  
plots a total elapsed time with milliseconds times 10 Superscript 5 versus the RMSE of the test data 
for each approach. 
The result displayed in Fig. 6 show that streaming FSGD is faster to decrease 
RMSE, but it levels out with a worse level than another one. It is reasonable because 
of having no bottleneck placed from the slowest worker. For instance, in accessing 
a stratum in Batch FSGD, an other stratum cannot be accessed until all blocks from 
previous stratum ﬁnished its job. If with each block, the distribution of ratings is 
not uniform, it could be likely that other blocks will end to process early though no 
any new executing can be computed until the slowest block ﬁnished. With stream 
FSGD, in contrast, when a block ﬁnishes its job, computation to a new block can 
start immediately without waiting for any full stratum. This is the reason, Streaming 
FSGD was expected to operate faster than Batch FSGD. 
7 
Conclusion 
In this work, the author point out some computational drawbacks in existing parallel 
Stochastic Gradient methods for parallel systems. In addition, the authors have prove 
that the coordinate descent approach is scalable and efﬁcient to solve large-scale 
matrix factorization issue in recommender systems. With experiments result, the 
proposed method FSGD not only achieve lower time complexity for each iteration 
than ALS, but also gain more stable and faster convergence than SGD. 
To develop a more complete SG methods for recommender systems, the authors 
intend to extend proposed algorithm to tackle variants of matrix-factorization 
issue. One more, to further decrease the cache-miss rate, the authors will explore 
nonuniform splits for permutation methods or other rating matrix methods.

68
T. N. Si et al.
References 
1. Li F, Wu B, Xu L, Shi C, Shi J (2014) A fast distributed stochastic gradient descent algorithm 
for matrix factorization. In: Proceedings of the 3rd International workshop on big data, streams 
and heterogeneous source mining: algorithms, systems, programming models and applications. 
PMLR, pp 77–87 
2. Zhou Y, Wilkinson D, Schreiber R, Pan R (2008) Large-scale parallel collaborative ﬁltering 
for the netﬂix prize. In: International conference on algorithmic applications in management. 
Springer, Berlin, Heidelberg, pp 337–348 
3. Koren Y, Bell R, Volinsky C (2009) Matrix factorization techniques for recommender systems. 
Computer 42(8):30–37 
4. Takács G, Pilászy I, Németh B, Tikk D (2009) Scalable collaborative ﬁltering approaches for 
large recommender systems. J Mach Learn Res 10:623–656 
5. Nguyen ST, Kwak HY, Lee SH, Gim GY (2019) Using stochastic gradient decent algorithm 
for incremental matrix factorization in recommendation system. In: 2019 20th IEEE/ACIS 
International Conference on Software Engineering, Artiﬁcial Intelligence, Networking and 
Parallel/Distributed Computing (SNPD). IEEE, pp 308–319 
6. Langford J, Smola A, Zinkevich M (2009) Slow learners are fast. arXiv preprint arXiv:0911. 
0491 
7. Testa A, Cibinel GA, Portale G, Forte P, Giannuzzi R, Pignataro G, Silveri NG (2010) The 
proposal of an integrated ultrasonographic approach into the ALS algorithm for cardiac arrest: 
the PEA protocol. Eur Rev Med Pharmacol Sci 14(2):77–88 
8. Recht B, Re C, Wright S, Niu F (2011) Hogwild!: A lock-free approach to parallelizing 
stochastic gradient descent. Adv Neural Inf Process Syst, 24. 
9. Zinkevich M, Weimer M, Li L, Smola A (2010) Parallelized stochastic gradient descent. Adv 
Neural Inf Process Syst, 23 
10. Gemulla R, Nijkamp E, Haas PJ, Sismanis Y (2011) Large-scale matrix factorization with 
distributed stochastic gradient descent. In: Proceedings of the 17th ACM SIGKDD international 
conference on Knowledge discovery and data mining, pp 69–77 
11. Cichocki A, Phan AH (2009) Fast local algorithms for large scale nonnegative matrix and 
tensor factorizations. IEICE Trans Fundam Electron Commun Comput Sci 92(3):708–721 
12. Hsieh CJ, Dhillon IS (2011) Fast coordinate descent methods with variable selection for 
non-negative matrix factorization. In: Proceedings of the 17th ACM SIGKDD international 
conference on Knowledge discovery and data mining. pp 1064–1072 
13. Ramlatchan A, Yang M, Liu Q, Li M, Wang J, Li Y (2018) A survey of matrix completion 
methods for recommendation systems. Big Data Min Anal 1(4):308–323 
14. Li Y, Liang Y (2018) Learning overparameterized neural networks via stochastic gradient 
descent on structured data. Adv Neural Inf Process Syst, 31 
15. Agarwal A, Duchi JC (2011) Distributed delayed stochastic optimization. In: NIPS 
16. Xie C, Koyejo S, Gupta I (2019) Zeno: Distributed stochastic gradient descent with suspicion-
based fault-tolerance. In: International conference on machine learning. PMLR, pp 6893–6901 
17. Chang KW, Hsieh CJ, Lin CJ (2008) Coordinate descent method for large-scale l2-loss linear 
support vector machines. J Mach Learn Res, 9(7)

Using Incremental Algorithm in Hybrid 
Recommender System Combined 
Sentiment Analysis 
Thin Nguyen Si and Trong Van Hung 
Abstract Beside the problem how to improve hybrid system combine sentiment 
analysis, developing incremental algorithms become an interesting research in real-
data en-vironment. While improving the extension of Vietnamese language sentiment 
analysis is still difﬁcult, stochastic gradient descent algorithm (SGD) exposes the 
limitations about optimal process in incremental learning. Sterm from two issues, the 
study proposed model combine Long Short Term Memory with KSGD algorithms 
in matrix factorization to improve the time and accuracy of predict model. With 
experimental results, this work proves that proposed system achieves better results 
with accuracy and learning time. 
Keywords Hybrid recommender system · Stochastic gradient descent ·
Incremental algorithm 
1 
Introduction 
In recommender system, the data can be divided into two main groups: the ﬁrst 
group is the rating data (user evaluate to the product; the second one is textual data 
through comments, product descriptions … [1] Starting from these input data types, 
the recommender system model is also divided into two main models: a collab-
orative ﬁltering model (CF) using rating matrix [2, 3] and content- based model 
(CB) using textual data [4]. While the CB model is suitable with text semantic anal-
ysis to generate product and ignores the relationship between users or items, CF is 
widely used with more accuracy based on the relationship between the users and 
items [1]. In the previous work, the authors have proven that integrating the senti-
ment analysis problem for text data and the matrix factorization of CF is feasible
T. N. Si envelope symbol · T. Van Hung 
Vietnam-Korea University of Information and Communication Technology, Da Nang City, 
Vietnam 
e-mail: nsthin@vku.udn.vn 
T. Van Hung 
e-mail: vhtrong@vku.udn.vn 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_6 
69

70
T. N. Si and T. Van Hung
and brings precisely predictive results [1]. Accordingly, the two-dimensional rating 
matrix left parenthesis u s e r s x i t e m s right parenthesisshould be added with other dimensions such as emotion, time, 
location … to increase accuracy [1]. 
Another problem, how to build a prediction model with Vietnamese textual data 
is very not simple because of the polymorphism and polymorphism, especially the 
dataset analyzed by authors based on comments from the social website for movies 
[5]. 
The third problem, how to improve the learning time when data is continuous 
larger daily. The systems continuously update new users and products every second. 
To overcome this problem, the incremental algorithm is a good solution including 
the Stochastic Gradient Descent (SGD) [6]. However, the inﬂexibility in determining 
the learning rate parameters of the SGD becomes issue that needs to be tackled. 
Stemming from these above problems and from previous studies, the authors aims 
to answer two research questions: 
Which algorithm is optimal for the sentiment analysis to solve the polymorphism 
and polymorphism of Vietnamese language? 
How to tackle SGD’s disadvantages to improve learning time to model? 
From the survey from related studies, the authors selected the Long Short Term 
Memory (LSTM) algorithm for the sentiment analysis problem and used KSGD 
algorithm improved from SGD for the proposed model. 
The rest of the study is designed as follows: the authors present an overview of 
sentiment analysis in Viet Nam and the world in Sect. 2; The incremental algorithm 
with SGD is show in Sect. 3; The proposed model is presented in Sect. 4; Data and 
evaluation issues are presented in Sect. 5; before making conclusions in Sect. 7, 
results experiment is discussed in Sect. 6. 
2 
Sentiment Analysis 
2.1 
Sentiment Analysis in Viet Nam 
In Viet Nam, Kieu et al. [7] show a rule based system to sentiment classiﬁcation with 
the Gate framework [8] and conduct experiments with computer product reviews. 
Duyen et al. apply Naïve Bayes, Support Vector Machine (SVM) and Max Entropy 
Model to evaluate comments on the Agoda website [9], which permits customer to 
book hotel rooms easily. The results proved that the SVM algorithm achieves the 
best results. Especially, Quan et al. [10] conduct a deep learning model, proposing 
a system that combines CNN and Long Short-Term Memory (LSTM), called multi-
channel LSTMCNN. This model achieves a result better than both LSTM an CNN 
alone. This method is similar to [11], a deep learning system is applied to manage 
negative comments from social networks.

Using Incremental Algorithm in Hybrid Recommender System …
71
2.2 
The Development of Sentiment Analysis Model 
This section, the author brieﬂy present popular algorithm discussed on previous 
section. 
Support vector machines model 
SVM algorithm is one of the algorithms of binary classiﬁcation by regression in 
machine learning [12]. The purpose of the algorithm is to map the original data 
points to new points in an n-dimensional vector space. 
Given data set of feature vectors is classiﬁed as follows: 
up per X equals StartSet ModifyingAbove x With quotation dash Subscript 1 Baseline comma ModifyingAbove x With quotation dash Subscript 2 Baseline comma ellipsis ModifyingAbove x With quotation dash Subscript n Baseline EndSet w h e r e ModifyingAbove x With quotation dash Subscript i Baseline element of upper R Superscript mupper X equals StartSet ModifyingAbove x With quotation dash Subscript 1 Baseline comma ModifyingAbove x With quotation dash Subscript 2 Baseline comma ellipsis ModifyingAbove x With quotation dash Subscript n Baseline EndSet w h e r e ModifyingAbove x With quotation dash Subscript i Baseline element of upper R Superscript mupp e r  X equals StartSet ModifyingAbove x With quotation dash Subscript 1 Baseline comma ModifyingAbove x With quotation dash Subscript 2 Baseline comma ellipsis ModifyingAbove x With quotation dash Subscript n Baseline EndSet w h e r e ModifyingAbove x With quotation dash Subscript i Baseline element of upper R Superscript mupper X equals StartSet ModifyingAbove x With quotation dash Subscript 1 Baseline comma ModifyingAbove x With quotation dash Subscript 2 Baseline comma ellipsis ModifyingAbove x With quotation dash Subscript n Baseline EndSet w h e r e ModifyingAbove x With quotation dash Subscript i Baseline element of upper R Superscript m up er X 
equals StartSet ModifyingAbove x With quotation dash Subscript 1 Baseline comma ModifyingAbove x With quotation dash Subscript 2 Baseline comma ellipsis ModifyingAbove x With quotation dash Subscript n Baseline EndSet w h e r e ModifyingAbove x With quotation dash Subscript i Baseline element of upper R Superscript m
In which, we can consider the best separating hyperplane to the following 
equation: 
Mo
difyingAbove upper W With quotation dash Superscript upper T Baseline ModifyingAbove x With quotation dash plus b equals 0 w h e r e ModifyingAbove w With quotation dash equals Start 3 By 1 Matrix 1st Row w 1 2nd Row ellipsis 3rd Row w Subscript n Baseline EndMatrix a n d ModifyingAbove x With quotation dash equals Start 3 By 1 Matrix 1st Row x 1 2nd Row ellipsis 3rd Row x Subscript m EndMatrix Mo dify ingA bove upper W With quotation dash Superscript upper T Baseline ModifyingAbove x With quotation dash plus b equals 0 w h e r e ModifyingAbove w With quotation dash equals Start 3 By 1 Matrix 1st Row w 1 2nd Row ellipsis 3rd Row w Subscript n Baseline EndMatrix a n d ModifyingAbove x With quotation dash equals Start 3 By 1 Matrix 1st Row x 1 2nd Row ellipsis 3rd Row x Subscript m EndMatrix Modi
fy
in
gAb
o v e  
upp
er
 W With quotation dash Superscript upper T Baseline ModifyingAbove x With quotation dash plus b equals 0 w h e r e ModifyingAbove w With quotation dash equals Start 3 By 1 Matrix 1st Row w 1 2nd Row ellipsis 3rd Row w Subscript n Baseline EndMatrix a n d ModifyingAbove x With quotation dash equals Start 3 By 1 Matrix 1st Row x 1 2nd Row ellipsis 3rd Row x Subscript m EndMatrix Mo di
fy
in
gAb
o v e  
up
er
 
W With quotation dash Superscript upper T Baseline ModifyingAbove x With quotation dash plus b equals 0 w h e r e ModifyingAbove w With quotation dash equals Start 3 By 1 Matrix 1st Row w 1 2nd Row ellipsis 3rd Row w Subscript n Baseline EndMatrix a n d ModifyingAbove x With quotation dash equals Start 3 By 1 Matrix 1st Row x 1 2nd Row ellipsis 3rd Row x Subscript m EndMatrix
2.3 
Naïve Bayes 
Naïve Bayes is also a classiﬁcation algorithm in machine learning but unlike SVM, 
it uses statistical probability to make classiﬁcation predictions [13]. 
For instance, given a data set to be predicted like (1), and bold italic upper Y is the predict data 
with the value speciﬁed in a given set StartSet 0  c omma 1 comma 2 comma ellipsis upper P EndSet: 
up p er Y equ a l s  StartSet y  1 comma y 2  c o mma
 ellipsis y Subscript n Baseline EndSet w h e r e y Subscript n Baseline epsilon left brace 0 comma 1 comma 2 comma ellipsis upper P right brace
Since each value of the output y is only selected in the upper P set with given values, 
the chosen y value will be based on the most probable y value based on Eq. (4): 
u
p
per P le f t  p are
n
thesis y
 
ve
rtical 
bar x Subscript 1 comma Baseline x 2 comma ellipsis comma x Subscript m Baseline right parenthesis equals alpha upper P Subscript left parenthesis y right parenthesis Baseline product Underscript i Endscripts upper P left parenthesis x Subscript i Baseline vertical bar y right parenthesis
According to (4), the predicted value y will be calculated by the product of the 
conditional probabilities of all x feature vectors including m components.

72
T. N. Si and T. Van Hung
2.4 
Long Short Term Memory (LSTM) 
LSTM includes a memory cell [14] with four basic elements: a neuron of a self-
recurrent connection (the connection of itself), an input gate, an output gate and a 
forget gate. All three nonlinear gates show in the block called the summation unit, 
it conducts the inside-outside movement from information by activations cell with 
multiplications. And this multiplication executes at output cell and each input by 
their respective gates, while allowing the cell to remember or forget its previous state 
apply sigmoid activation. Generally, (‘ f Subscript t’) gate activation function is pick as logistic 
sigmoid, then gate activation will be between 1 (gate open) and 0 (gate close), whereas 
logistic or tanh sigmoid is to ‘upper O Subscript t’ output activation function to tackle the vanishing 
gradient issue, whose second derivative could sustain with a long range before go to 
zero., Augmentation, further, is possible that depends on a different issue statement 
[14]. The weight values (‘peephole’ connection) link the cell with the gates, which is 
shown in Fig. 1, then the rest of this connection will be unweighted. Then the memory 
prevents output connects to the rest of network by output gate multiplication. 
We can consider the model input to x eq uals  l e f t p are n t h e sis x 1 comma ellipsis comma x Subscript j Baseline comma ellipsis comma x Subscript t Baseline right parenthesis, and the output 
sequence to y equals l e f t  parenth e s i s  x Subscript t plus 1 Baseline comma ellipsis comma x Subscript t plus i Baseline comma ellipsis comma x Subscript t plus t prime Baseline right parenthesiswhere t prime is the next time of step predic-
tion and t is prediction period. In situation of low-ﬂow prediction, x should be 
considered to historical input data, while y is the single lag of period series. The 
purpose of LSTM is to predict for low-ﬂow discharge with the next time step by 
previous data and is computed by the following equation: 
i S ubscript t Ba seline equa ls sigma l eft pa
renthesis upper W Subscript i x Baseline period x Subscript t Baseline plus upper W Subscript i h Baseline period h Subscript t minus 1 Baseline plus upper W Subscript i c Baseline c Subscript t minus 1 Baseline plus b Subscript i Baseline right parenthesis
i S ubs
c
ri p t  t Ba seli n e equal s si g ma lef t pa r
e
nthesis upper W Subscript i x Baseline period x Subscript t Baseline plus upper W Subscript i h Baseline period h Subscript t minus 1 Baseline plus upper W Subscript i c Baseline c Subscript t minus 1 Baseline plus b Subscript i Baseline right parenthesis
Fig. 1 LSTM memory block with one cell 

Using Incremental Algorithm in Hybrid Recommender System …
73
i S ub sc ript t B aseline equals s igma left paren
thesis upper W Subscript i x Baseline period x Subscript t Baseline plus upper W Subscript i h Baseline period h Subscript t minus 1 Baseline plus upper W Subscript i c Baseline c Subscript t minus 1 Baseline plus b Subscript i Baseline right parenthesis
i S ubscript t Ba seli ne equal s sigma l eft p
arenthesis upper W Subscript i x Baseline period x Subscript t Baseline plus upper W Subscript i h Baseline period h Subscript t minus 1 Baseline plus upper W Subscript i c Baseline c Subscript t minus 1 Baseline plus b Subscript i Baseline right parenthesis
i S ubscript t
 Baseline equals sigma left parenthesis upper W Subscript i x Baseline period x Subscript t Baseline plus upper W Subscript i h Baseline period h Subscript t minus 1 Baseline plus upper W Subscript i c Baseline c Subscript t minus 1 Baseline plus b Subscript i Baseline right parenthesis
i S ubscript t Ba
seline equals sigma left parenthesis upper W Subscript i x Baseline period x Subscript t Baseline plus upper W Subscript i h Baseline period h Subscript t minus 1 Baseline plus upper W Subscript i c Baseline c Subscript t minus 1 Baseline plus b Subscript i Baseline right parenthesis
where sigmais the sigmoid function. 
Notable that, the memory block include an output gate, an input gate, and a 
forget gate, which the outputs of all three gates represented, respectively, as follows: 
o Subscript t Baseline comma i Subscript t Baseline comma f Subscript t Baseline and is outlined with a box. The activation vectors for memory block and 
each cell are o Subscript t and h Subscript t, respectively. The bias vectors b and the weight matrices upper W are 
utilized to create connections between output layer, the input layer and the memory 
block. 
3 
Incremental Learning Algorithm 
3.1 
Incremental Learning 
Traditional training methods bring certain beneﬁts to models with less volatile data. 
Training is performed on ﬁxed data once, which can consider to train on static 
data [15]. In contrast, the incremental algorithms are seen as the training model for 
continuously updated data [16]. This process is performed sequentially whenever, 
for example, in autonomous driving applications, smart home, etc. Moreover, the 
need for a system capable of continuous learning is essential when training data is 
update continuously in commercial systems [17]. 
There are many previous studies on reinforcement learning in the problem of 
recommendations. Notably: research of two groups of authors Miranda and Vinagre 
on reinforcement learning based on nearest neighbor [18] 19; the work of Sarwar in 
the matrix factorization problem [20]; Bell et al. research for continuous user-factor 
vector updates using group processing methods [21]. The study on SGD algorithm of 
Takács et al. signiﬁcantly improved the prediction results, especially the processing 
speed.

74
T. N. Si and T. Van Hung
3.2 
Stochastic Gradient Descent 
As mentioned discussion, many method have been proposed to overcome the matrix 
factorization issue with large-scale data. SGD algorithm, among them, is a popular 
method because it is better to the non-convex problem [21]. The key idea of SGD is 
that each rating left parenthesis i comma j right parenthesisis randomly picked from upper D, then both corresponding values 
bold italic w Subscript bold italic i and bo ld italic x Subscript bold italic j are set by formula as Algorithm 1. 
Algorithm 1 Stochastic gradient descent 
wherein: eta—the learning rate. 
4 
Proposed Algorithm 
In this section, the authors propose two methods: improving SGD to increase learning 
speed and forming the way to combine sentiment analysis into matrix factorization. 
4.1 
KSGD 
Figure 2 explains the KSGD algorithm we propose. The algorithm is designed to 
execute as an incremental method, which updates upper W matrix and upper X matrix just by 
k current observation. Compare to SGD, there are two major differences. First, the 
SGD’s learning process requires only one single pass over all available data. Notable 
that in KSGD, at k observation less th an u comma i greater than, the alteration of upper W matrix and upper X matrix 
are taken in a single step. It mean that model executes several iterations over each 
new observation. KSGD is described as Algorithm 2.

Using Incremental Algorithm in Hybrid Recommender System …
75
Fig. 2 KSGD algorithm 
Algorithm 2 K-Stochastic gradient descent (KSGD) 
4.2 
Feature Hybrid System 
To combine sentiment analysis and factor matrix, based on the results of previous 
research [22], the authors ﬁrst designed the feature vector of the product. This vector 
is built based on sentiment analysis with LSTM algorithm. Feature vectors are built 
with three features corresponding to the following three components: negative (−1), 
neutur (0) and positive (1) (Fig. 3).
Ratings matrix upper R of matrix factorization with size (m × n) should be added three 
columns (d = 3) for features of items. Then, the new matrix is rechanged size to 
lef t par e nthesis m times left parenthesis n plus 3 right parenthesis right parenthesis, where m is the number of users and n is the number of items. Finally, 
the objective function is set as following equation with the parameter vector θ: 
up per J equa ls upper M a t r i x f a c t o r i z a t i o n left par
enthesis theta right parenthesis plus beta period upper S e n t i m e n t left parenthesis theta right parenthesis plus upper R e g u l a r i z a t i o n

76
T. N. Si and T. Van Hung
Data 
Rating Data 
Comment Data 
HYBRID SYSTEM 
Matrix Factorization 
(matrix 
 ) 
LSTM  
Algorithm 
(feature vector (3)) 
MATRIX (
)c 
Fig. 3 Proposed model—feature hybrid system
5 
Data and Evaluation 
5.1 
Data 
All data for the empirical evaluation, the authors gathered from the comments and 
reviews about movies from the websites of Vietnam. More detail, the authors uses 
the automatic comment collection tool from the website: https://ghienreview.com/ 
and https://khenphim.com/ with 1,689 comments, 688 ratings from 716 users to 800 
movies. The feature of this dataset is that most of the movies have been rated because 
all websites aim to purpose for review movies. However, for the evaluation data, the 
authors had to change the scale 5 instead of the 10 scale from website. The description 
of the data are described in Table 1. 
Table 1 The statistics for 
data
Features
Data 
# users
716 
# items
800 
# ratings
688 
# comments
1,689

Using Incremental Algorithm in Hybrid Recommender System …
77
5.2 
Evaluation 
The authors apply two indicators: Root Mean Squared Error (RMSE) indicator which 
shown by equation and learning time indicator to evaluate models 
upper R upper M upper S upper E equals StartRoot StartFraction 1 Over upper N EndFraction sigma summation Underscript i comma j Endscripts left parenthesis upper R Subscript i j Baseline minus ModifyingAbove upper R With caret Subscript i j Baseline right parenthesis squared EndRoot
 
 
 
 upper R upper M upper S upper E equals StartRoot StartFraction 1 Over upper N EndFraction sigma summation Underscript i comma j Endscripts left parenthesis upper R Subscript i j Baseline minus ModifyingAbove upper R With caret Subscript i j Baseline right parenthesis squared EndRoot
u
p
per R
 up p e r M
 
u p per
 S
 upper E equals StartRoot StartFraction 1 Over upper N EndFraction sigma summation Underscript i comma j Endscripts left parenthesis upper R Subscript i j Baseline minus ModifyingAbove upper R With caret Subscript i j Baseline right parenthesis squared EndRoot
All models compared divided to three group: Neighborhood base (NBCF), Model 
Base (MFCF) and proposed model—hybrid system. The experiments aim to two 
evaluation. The ﬁrst one is to evaluate KSGD with CF model about learning time. 
The second one is evaluate hybrid system with all model about accuracy. 
6 
Result and Discussion 
The experiment results is shown in Table 2. 
6.1 
Accuracy 
If only considering KSGD with CF group algorithms, its accuracy is approximately 
SGD (0.964 and 0.966 with RMSE). This can be understood because KSGD is 
designed in favor of improving machine learning speed. 
Let compare the accuracy of the hybrid model with the rest of the groups. With 
impressive results 0.954 RMSE, the proposed model achieved the best results. This 
shows that the combination of emotional analysis and factor matrix is valid.
Table 2 The experiment data 
Measures
Collaboration base—ﬁltering (CF)
Hybrid system 
Neighborhood-base 
(NBCF) 
Model-based (MFCF) 
User-user
Item-item
ALS
SGD
KSGD 
RMSE
0.989
0.988
0.969
0.966
0.964
0.954 
Learning time
1.068
1.078
1.090
1.088
1.066
1.082 

78
T. N. Si and T. Van Hung
6.2 
Comparison on Speedup 
Similarly, if we only compare KSGD with CF group about learning time. The results 
showed that with the value 1,066 ms, KSGD achieved the best results in reducing the 
learning time for the system. This result is much better than the MFCF group (1,090 
for ALS and 1,088 for SGD) but only slightly better for the NBCF group. This can 
also be explained that the MFCF group, which improves the accuracy better than the 
NBCF group, has yet to overcome the learning time of the system. 
However, with the hybrid system, learning time has not really been improved. 
With a result of 1,082 ms, the hybrid system still took a long time to training even 
though the system’s accuracy was signiﬁcantly improved. 
7 
Conclusion 
By combining feature extraction in Vietnamese sentiment analysis problem with 
LSTM algorithm and factor matrix of collaborating with the improvement of SGD 
algorithm, the authors proposed a hybrid system uses different input data types. By 
experimental results, the study has proven that the hybrid system combined sentiment 
analysis and matrix factorization is completely feasible for the recommended system 
with continuously updated data. 
However, the limitation of that study is that the data is still at a simple level and the 
evaluation method is still at a relatively low level that does not show clear continuous 
change. In the future, the authors continuously exploit to distributed recommender 
system with parallel algorithm. 
References 
1. Nguyen ST, Kwak HY, Lee SH, Gim GY (2019) Using stochastic gradient decent algorithm for 
incremental matrix factorization in recommendation system. In: 2019 20th IEEE/ACIS inter-
national conference on software engineering, artiﬁcial intelligence, networking and parallel/ 
distributed computing (SNPD), pp 308–319. IEEE 
2. Landia N, Anand SS (2009) Personalised tag recommender. In: Proceedings of the 2009 ACM 
conference on recommendation systems, pp 83–36 
3. Bobadilla J, Ortega F, Hernando A, Gutiérrez A (2013) Recommendation systems survey. 
Knowl Based Syst 46:109–132 
4. Adomavicius G, Tuzhilin A (2005) Toward a next generation of recommendation systems: a 
survey of the state-of-art and the possible extensions. IEEE Trans Knowl Data Eng 17(6):734– 
749 
5. Nguyen QT, Nguyen TL, Luong NH, Ngo QH (2020) Fine-tuning bert for sentiment analysis 
of Vietnamese reviews. In 2020 7th NAFOSTED conference on information and computer 
science (NICS), pp 302–307. IEEE 
6. Huang Z, Zeng D, Chen H (2007) The comparison on collaborative-ﬁltering recommender 
algorithms in ecommerce. IEEE Intell Syst 22(5)

Using Incremental Algorithm in Hybrid Recommender System …
79
7. Kieu BT, Pham SB (2010) Sentiment analysis for Vietnamese. In: Proceedings of second 
international conference on knowledge and systems engineering (KSE), pp 152–157 
8. Cunningham H, Maynard D, Bontcheva K, Tablan V (2002) GATE, a framework and graphical 
development environment for robust NLP tools and applications. In: Proceedings of the annual 
meeting of the association for computational linguistics (ACL), pp 168–175 
9. Duyen NT, Bach NX, Phuong TM (2014) An empirical study on sentiment analysis for Viet-
namese. In: 2014 international conference on advanced technologies for communications (ATC 
2014), pp 309–314. IEEE 
10. Vo QH, Nguyen HT, Le B, Nguyen ML (2017) Multi-channel LSTM-CNN model for Viet-
namese sentiment analysis. In: 2017 9th international conference on knowledge and systems 
engineering (KSE), pp 24–29. IEEE 
11. Vo K, Nguyen T, Pham D, Nguyen M, Truong M, Nguyen D, Quan T (2019) Handling negative 
mentions on social media channels using deep learning. J Inf Telecommun 3(3):271–293 
12. Mullen T, Collier N (2004) Sentiment analysis using SVM with diverse information sources. 
In: Proceedings of 2004 conference for empirical methods in natural language processing 
13. Bonaccorso G (2017) Machine learning algorithms. Packt Publishing Ltd (2017). Cambria E, 
White B (2014) Jumping NLP curves: a review of the natural language processing research. 
IEEE Comput Intell Mag 9(2):48–57 
14. Abidogun OA (2005) Data mining, fraud detection and mobile telecommunications: call pattern 
analysis with unsupervised neural networks (Doctoral dissertation, University of the Western 
Cape) 
15. Amirat Y, Daney D, Mohammed S, Spalanzani A, Chibani A, Simonin O (2016) Assistance 
and service robotics in a human environment. Robot Auton Syst 75(Part A):1–3 
16. Hammer B, Toussaint M (2015) Special issue on autonomous learning. KI 29(4):323–327 
17. Silver DL (2011) Machine lifelong learning: challenges and beneﬁts for artiﬁcial general 
intelligence. In: Artiﬁcial general intelligence—4th international conference, AGI 2011, pp 
370–375 
18. Miranda C, Jorge AM (2008) Incremental collaborative ﬁltering for binary ratings. In: Proceed-
ings of the 2008 IEEE/WIC/ACM international conference on web intelligence and intelligent 
agent technology, vol 01. IEEE Computer Society, pp 389–392 
19. Vinagre J, Jorge AM (2012) Forgetting mechanisms for scalable collaborative ﬁltering. J Braz 
Comput Soc 18(4):271–282 
20. Sarwar BM, Karypis G, Konstan J, Riedl J (2002) Incremental SVD-based algorithms for highly 
scalable recommender systems. In: Fifth international conference on computer and information 
technology, pp 27–28 
21. Berry M, Dumais S, O’Brien G (1995) Using linear algebra for intelligent information retrieval. 
SIAM Rev 573–595 
22. Aggarwal C, Parthasarathy S (2001) Mining massively incomplete data sets by conceptual 
reconstruction. In: ACM KDD conference, pp 227–232

A Bottom-Up Generic Probabilistic 
Building and Enriching Approach 
for Knowledge Graph Using 
the LDA-Based Clustering Method 
Amani Mechergui, Sami Zghal, and Wahiba Ben Abdessalem Karaa 
Abstract The world’s knowledge appears to be limitless, making it appear impos-
sible to ever fully comprehend it. Therefore, methods that autonomously infer new 
knowledge and enrich the Knowledge Graph (KG) are of particular interest. However, 
the majority of existing works related to distributional graph clustering of terms use 
“the top-down principle” for enriching KG. They adopt as input data a corpus that 
contains documents belonging to a single domain and a predeﬁned personalized 
KG model related to this corpus. On this basis, they enrich it semi-automatically 
or manually from different data sources using multiple techniques. We introduce a 
novel semi-supervised framework, called BUGPBE-LDA, which is an LDA adap-
tation. It builds and enriches a Generic Core KG (GCKG) with its Core Concepts 
(CC) using advanced machine-learning techniques. We will conduct this experi-
mental approach by following four principal workﬂows with on focus on extracting 
the NP is-a NP pattern. Our textual corpora are a collection of vocabulary including 
terms and classes that covers the ﬁsh hunting and ontology domains. As a result, our 
contribution can enhance KG with probabilistic weights and semantic enrichment. 
We evaluate our proposal on two textual corpora ONTO and HUNT and compare it 
to the LDA unsupervised clustering baseline. The F-measure results show that our 
model outperformed its competitor.
A. Mechergui envelope symbol
RIADI Laboratory, National School of Computer Science, Manouba University, Tunisia, High 
Institute of Management of Tunis, Tunis University, Tunis, Tunisia 
e-mail: amenimechergui47@gmail.com 
S. Zghal 
LIPAH-LR Laboratory, Tunis El Manar University, 11ES14 El Manar University Campus, 2092 
Tunis, Tunisia 
e-mail: zghal.sami@gmail.com 
W. B. A. Karaa 
RIADI Laboratory, National School of Computer Science, Manouba University, Tunisia, High 
Institute of Management of Tunis, Tunis University, Tunis, Tunisia 
e-mail: wahiba.bak@gmail.com 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_7 
81

82
A. Mechergui et al.
Keywords Distributional graph · Term clustering · Bottom-up · Generic core 
KG · Core concept · Latent Dirichlet allocation · KG construction · KG 
enrichment · NP is-a NP pattern 
1 
Introduction 
The KG makes facts about entities (people, real-world goods, locations, etc.) under-
standable by representing links between these things that encompass types and char-
acteristics with explicit meanings. By recognizing a group of semantically related 
terms as a concept in a KG, clustering approaches show promise in the task of term 
generation. 
Pattern-based techniques and distributional approaches are the two primary groups 
into which they are often divided. Hearst proposed the use of lexico-syntactic patterns 
in the form of regular expressions to harvest is-a relation from texts which provide 
higher precision at the price of lower coverage [1–3]. The primary notion under-
lying distributional approaches, whether supervised or unsupervised, according to 
Harris’ distributional theory [4] is to group terms by the co-occurrence distribution 
of contexts in which they are encountered to ﬁnd semantic relationships between 
word pairs. Terms that occur in similar contexts tend to have similar meanings. To 
be precise, the Latent Dirichlet Allocation (LDA) is among the best existing distri-
butional approaches that can handle a sizable number of documents and address the 
text sparsity issue. 
The notion of enriching KG via “a top-down paradigm” is present in the bulk of 
extant publications on distributional graph clustering of terms. However, the largest 
difﬁculty in the literature that has not been tackled till now is the construction of a 
GCKG that refers to several domains and/or enhances it by adopting LDA as a major 
instrument and following “the bottom-up philosophy”. 
In the context of machine learning, we propose a new semi-supervised generic 
framework called “a Bottom-Up Generic Probabilistic Building and Enriching 
approachforKGusingLDA—BUGPBE-LDA—basedclusteringmethod”. Our work 
makes several key contributions. It builds and enriches GCKG from semi-structured 
textual resources belonging to unrelated areas. We will conduct this experimental 
approach by following four principal workﬂows. Our BUGPBE-LDA (an implemen-
tation of LDA) is a Topic Models (TM) approach that contains more than millions 
of concepts and subsumption relations for the task of text classiﬁcation. Analogous 
to the works of [1, 5–7], we aim to extract the NP is-a NP pattern. They are special-
izations of the KG entities in our initial GCKG and characterize the subsumption 
relations between them. 
The paper is organized as follows. Section 2 covers distributional term clustering 
techniques for KG after ﬁrst providing background knowledge of LDA. Section 3 
presents our approach to adapting the LDA model to our goal. Section 4 illustrates 
the implementation and performance evaluation of BUGPBE-LDA conducted on 
two corpora. Finally, Sect. 5 provides our conclusion and future works.

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
83
2 
Literature Review 
2.1 
Background Knowledge of LDA 
LDA is a type of probabilistic model that employs Bayesian probability. Utilizing 
word co-occurrence data to the fullest extent possible, LDA seeks to maximize the 
probability of the corpus produced by the model while obtaining the latent topic struc-
ture. LDA assumes that a document may include many topics but it also assumes that 
each word appearing in the document was generated by only one topic. The semantic 
features of documents and words will be expressed using these latent probabilistic 
interpretable topics composed of correlated terms. 
They are derived from Dirichlet distributions (DD). The name of this distribution 
was inspired by Johann Peter Gustav Lejeune Dirichlet, and it represents a probability 
distribution over all possible multinomial distributions of the same dimensionality. 
Typically, it is utilized in situations where there exists a multinomial distribution, 
and we have some knowledge about what it may look like. However, we cannot 
directly observe the distribution, instead, we observe individual outcomes from the 
multinomial. 
The concept of a DD is analogous to a dice factory, while a multinomial distribu-
tion is similar to a dice. Just like when we roll a die and observe which side comes 
up, when we take a sample from a multinomial distribution, we can see the outcome. 
The DD serves as the conjugate prior to the multinomial likelihood when categorical 
or discrete data is present, which is common in many machine learning applications. 
The DD is preferred because it can help prevent overﬁtting, a phenomenon where 
models that perform well during training do not perform as well on additional test 
data due to being trained on both the true signal and noise. If a prior is not used, 
overﬁtting can occur, and the model’s performance on new data may suffer. 
LDA is considered among semi-supervised classiﬁcation in the sense that the 
number of topics is given in advance, but the topics of these classes are unknown. 
LDA adheres to the Bag of Words (BoW) presumption and disregards word order. 
Rather than being an ordered list regardless of the word sequence, the documents are 
approximated as a grouping of words. 
The originality of LDA is demonstrated by the simulation of document generation 
using DD. In the ﬁrst step of the generative process, the hyper-parameter vector of the 
DD alpha α represents the corpus, at least towards the logic of the model. Thus, the 
vector θ represents the documents in LDA, that is the distribution of the topics in the 
document whose notation is p(z|d = m). Accordingly, the documents are represented 
as a random ﬁnite mixture of latent topics z. Then, for each word w, LDA will 
establish a link with a topic z that will be modeled as a random mix of words on a set 
of thematic probability  . The theta  distributions (that is the distribution of words 
in the topic k whose notation is p(w|z = k)) follow on their part a prior parameter 
β of the DD of the proportion of words in a topic. The topic probabilities provide a 
reduced representation of the documents in the collection by leveraging correlations 
between corpus words and latent semantic topics. The generative process for LDA

84
A. Mechergui et al.
corresponds to the following joint distribution of the variables in Eq. (1). 
upper P le ft  p are
nt
hesi
s upper W
 
c
omma
 upper Z c
o
m
ma t
heta co
mma alpha com
ma b eta r
ight parenthesis equals product Underscript i equals 1 Overscript k Endscripts upper P left parenthesis phi Subscript i Baseline semicolon beta right parenthesis product Underscript j equals 1 Overscript upper M Endscripts upper P left parenthesis theta Subscript j Baseline semicolon alpha right parenthesis product Underscript t equals 1 Overscript upper N Endscripts upper P left parenthesis upper Z Subscript j comma t Baseline vertical bar theta Subscript j Baseline right parenthesis upper P left parenthesis upper W Subscript j comma t Baseline vertical bar phi Subscript z Sub Subscript j comma t Subscript Baseline right parenthesis
2.2 
Related Work 
By employing the Kullback-Leibler (KL) distance to measure term similarity, Pereira 
et al. [8] developed the distributional clustering of terms. With a special focus on 
this principle, Caraballo [9] used a bottom-up clustering approach to group noun 
terms and construct the hypernym linkages among clusters. Similarly, Cimiano 
[10] presented a guided agglomerative clustering algorithm for creating concept 
hierarchies from text corpora to increase cluster coherence. Different criteria were 
employed by Colace et al. [11] to choose aggregate terms that reﬂect the concepts 
derived from topics. They employed LDA to determine the conditional probability 
between term pairs for categorizing topics. Also, it has been used to identify the 
terms whose presence is not suggested by the occurrence of other terms in the corpus 
as the aggregate roots, and at last, each topic is labeled using its aggregate root. 
Added to that, Rani et al. [12] investigated the Mr. LDA-based approach for 
concept creation. Though this technique didn’t take into account the inﬂuence of 
irrelevant terms to their domains on topic formation, and it required more manual 
labor to designate each topic as a valid concept. To tackle this issue, an LDA model 
is trained on a corpus for the ﬁrst time to detect terms that are unrelated to their 
domains, according to Xu et al. [13]’s proposal for a twice-trained LDA. The lowest 
probability terms on each topic are eliminated from the corpus. The cleaned corpus 
is then used to train LDA once more, this time to build up topics. However, the 
loss of domain-relevant terms wasn’t investigated. Unfortunately, most LDA-based 
techniques ignored topic semantic coherence and as a result, the generated topics 
overlap and may be unwanted. Thus, few works have focused on applying LDA for 
term clustering to detect concepts. 
Besides, Besbes [14] created many CCs and built taxonomic and semantic rela-
tionships between concepts within a partition. Prior knowledge can drive the CC, and 
topic features from documents can also lead to CC representation. As well, Mustapha 
et al. [15] predeﬁned the topic and relation deﬁnitions. Then, for each topic, a deep 
analysis will be performed to relate entities inside that partition. Yet, this approach 
just uses topics to detect the CCs, not to identify all terms related to CCs. 
The notion of term representation by graphs was ﬁrst proposed by Widdows et al. 
[16]. G = (V, E, W), where V is a collection of vertices that represent words, E 
is a collection of edges connecting terms, and W stands for the weights of those 
links, deﬁnes a graph. In light of this graph, Matsuo et al. [17] developed the word 
graph clustering algorithm known as Newman clustering. Based on the frequency

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
85
of each word and that of its word pair, several metrics such as point-wise mutual 
information, Jaccard coefﬁcient, and chi-square are used to quantify the weight of 
an edge. If the weight of an edge between two words is less than a certain amount, 
it is deleted. Similarly, to calculate edge weights, Lee and Luo [18] used cosine 
similarity and word2vec for word representation. Following that and similarly to Qiu 
et al. [19], they suggested a term graph clustering approach based on the Louvain 
community identiﬁcation algorithm [20] to create concepts. Three methods—term 
co-occurrence-based, word2vec-based, and LDA-based—are used to compute the 
edge weights for Qiu et al. [19]. To locate related words using the distance range 
criteria on a graph where each edge is weighted by the inverse of word pair frequency, 
Thaiprayoon et al. [21] introduced a hierarchical agglomerative clustering. In the 
NOUS framework suggested by [22], as more online article entities are added to 
their dynamic KG, they only use neighborhoods in KG to calculate the contextual 
semantic similarity. At the level of their reasoning process, they used LDA to select 
the nodes with the least topic divergence compared to the target nodes to calculate the 
consistency score between the paths and then select the path with the least divergence. 
Tür et al. [23] present three methods for computing entity-type weights to enrich 
semantic KG with probabilistic weights. One of these strategies is Seeded LDA 
which assigns the highest weights to entity types that are used regularly in the natural 
language for a speciﬁc entity. This weighting method outperforms the unweighted 
versions on the relation detection task. Investigating the same context, Li et al. 
[24] propose OptimSearch which is an efﬁcient best-ﬁrst search algorithm to ﬁnd 
compact, representative, and relevant Entity Relation Graphs (ERGs) for effective 
document enrichment. It performs pruned searches starting from each input entity 
and merges paths having a common end vertex into an ERG. Then, they exploit onto-
logical knowledge to rank these ERGs by the relevance of entity-based intra-KG and 
document-KG with a focus on subsumption relationships to return the top-ranked 
results. In parallel, the experimentation outcome conﬁrms the robustness and the 
generalizability of the EDGE approach which outperforms existing graph embedding 
methods on link prediction and node classiﬁcation tasks. The authors of [25] propose a 
general framework to enrich original KG and node embeddings by exploiting auxil-
iary knowledge sources. Analogous to [25], Hoyt et al. [26] developed two semi-
automated generalizable workﬂows to generate an enriched and updated biological 
KG. Withal, this method needs extra human effort in the enrichment process that 
is done by manually revising automatically extracted relations from a large-scale 
corpus. 
We can conclude that the majority of existing works based on distributional graph 
clustering of terms share “the top-down principle” in the enrichment strategy. More 
speciﬁcally, they use a corpus related to a single domain as their input data, and 
together with a specialized KG model that is relevant to their corpus, they semi-
automatically or manually enrich the KG from additional data sources. As illus-
trated above, they used different tools to achieve successfully the enrichment process 
as well as different techniques. Although, the biggest subject in the literature that 
hasn’t been proposed yet is the establishment of a GCKG in an initial round that

86
A. Mechergui et al.
belongs to numerous domains at once and/or enriches it in a second part by imple-
menting the LDA methodology as a primary instrument and following “the bottom-up 
philosophy” in their overall experimental approach. 
3 
Proposed Approach: BUGPBE-LDA 
We present in Fig. 1 a novel generic framework namely BUGPBE-LDA for both 
producing and enriching enhanced GCKG. We will conduct this experimental 
approach by following four principal workﬂows, namely (1) acquisition of “candi-
date knowledge” (or candidate term—or NPs—sets), (2) training the LDA algorithm 
and annotating its results, (3) construction of GCKG with its CC by experts, and 
ﬁnally (4) enrichment of the original constructed GCKG. 
3.1 
Step 1: Acquisition of “Candidate Term Sets” 
In this step, we have three important sub-stages. We illustrate the sequence of the 
ﬁrst substage “Cleaning-up of document”. The corpus documents were ﬁrst cleaned 
by converting them into text extensions and eliminating all extraneous information
Fig. 1 An overview of BUGPBE-LDA steps. Here “A” denotes “Automatic”, “M” refers to 
“Manual”, and “SA” indicates “Semi-Automatic” tasks 

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
87
like equations, tables, and graphics. To extract only the NPs patterns, we next entered 
these textual data into the General Architecture for Text Engineering (GATE) natural 
language processing tool. The objective of employing patterns is to enable one to 
deduce a term’s semantic relationship even if they have never come across it before. 
As a result, GATE will use particular tags to annotate the textual components and 
give us XML documents as output data. These texts, however, may also contain 
unwelcome patterns like adjectives, verbs, etc. 
Thus, we present the second sub-stage “Technical adaptation for candidates”. 
Between the input of LDA and the output result of GATE, we made a technological 
adjustment that will guarantee that only NPs are extracted from the corpus. The four 
tasks assigned to this program are to (1) link NPs composed of multiple terms using 
the low dash symbol, (2) order the NPs in the result display by referencing their 
positions in the XML documents, (3) eliminate words included in the stop words list, 
and (4) lemmatize every word. 
We demonstrate the third sub-stage “Preparing the candidate NPs for LDA”. 
Because the papers on a website are written by various users, they are expressed in 
various writing styles. To replace these confusing terms with a single NP in common, 
we then built a synonym list of NPs that are expressed differently but yet convey the 
same idea. Then, we utilized a parser to (1) convert these CSV documents, containing 
the candidate NPs, to text extension and (2) rearrange them such that all the terms are 
separated by a single space and they can be modiﬁed and used by LDA afterward. 
The candidate NP sets, as one of the inputs of the BUGPBE-LDA model, are pillars 
of our strategy and have an immediate effect on the quality of LDA topics. 
3.2 
Step 2: Training the LDA Algorithm and Annotating Its 
Results 
We present in the ﬁrst subsection how to train the LDA model, and in the second 
subsection, we describe the annotation of the generated results. 
Step 2.1. Training the LDA algorithm. First, we illustrate “Step 2.1.1. Model estab-
lishment”. We propose a different setting for the BUGPBE-LDA algorithm (see 
Appendix) compared with the original LDA model. To train the joint model, we 
speciﬁcally used the collapsed Gibbs sampling algorithm [27] which is a suite of 
Java libraries for linguistic analysis of human languages. It is used for computing the 
topic-term probability. It is based on a class of algorithms known as the Monte Carlo 
Markov chain (MCMC) techniques. As well, we have implemented ﬁve R pack-
ages in the algorithm namely “NLP” [28], “tm” [29], “topicmodels” [30], “lda” [31], 
and “LDAVis” [32]. Second, we present “Step 2.1.2. Parameters’ model learning”. 
We have speciﬁed the input candidate NPs to be processed for each of its releases 
and parameterized its four other inputs with parameters: k (the number of topics), α 
and β, and G (the number of iterations). Third, we demonstrate “Step 2.1.3. Model

88
A. Mechergui et al.
training”. After ﬁxing the input parameters, it’s time to train our algorithm for cluster 
formation. 
Step 2.2. Annotation of the LDA output topics. First, we present “Step 2.2.1. 
Visualize the k-top most frequent NPs/Topic”. In our approach, the generated topic-
word distribution (ϕ) is the key for the expert annotation. We will concentrate on the 
K-top most frequent vocabulary deﬁning a topic. We can ﬁx any desired number of 
them. To limit the time complexity of our algorithm, we will get only the top “ten” 
terms. Second, we describe “Step 2.2.2. Annotate the results”. The visualization and 
interpretation of the LDA result is the experts’ task as a supplemental step. In Excel, 
the experts have produced “a manual reference evaluation” that includes vocabulary 
that best describes the desired domains. This vocabulary baseline technique identiﬁes 
the outcome that a perfect algorithm would provide. We tended to use structural 
approaches, which are based on the internal structure of terms, to categorize the 
NPs. The experts tend to calculate the similarity between the LDA result (based on 
its top most frequent NPs for each topic) and the desired result (the CC classes in 
our corpus) to ensure the semantics evaluation of the generated topics. Finding a 
single CC for each topic is the best-case scenario for us. However, the worst-case 
scenario is discovering a topic that includes several CCs at once. It’s worth paying 
great attention to the fact that we will use a threshold (8NPs/10NPs) to annotate the 
most frequent terms characterizing each generated topic. 
3.3 
Step 3: Construction of a GCKG with Its CCs Classes 
by Experts 
The Core KG is a KG that has the bare minimum of relations and concepts, called CCs 
classes. The CCs are the minimal concepts that allow deﬁning the other concepts of 
their domain [33]. We claim that there are connections among the CCs. In this study, 
we employ the CC synonyms and hyponyms as terms of a topic labeled by this CC 
to ensure the broadest semantic range possible for the CCs and all terms referring to 
them will get their impact. The GCKG is a standard or a global KG that may contain 
many CKGs at once accompanied by their relevant CCs that are connected to many 
well-deﬁned domains. This research looked at two CKGs which are ﬁsh hunting and 
ontology areas. 
In Fig. 2, we illustrate the 11 CCs connected to the ﬁsh hunting CKG, namely 
ship (SH, which is the basic concept that attaches all the rest of the concepts), 
mariner (MA), technical characteristic (TC), biological characteristic (BC), ﬁshing 
area (FA), ﬁsh species (FS), ﬁshing time (TI), ﬁshing gear (FG), shipowner (SO), 
regulation (RE), and environment (EN). Figure 3 presents the ontology CKG that 
contains 9 CCs entitled ontology (O), technique (Te), process (P), resource (R), step 
(S), tool (To), user (U), domain (D), and component (C).

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
89
Fig. 2 The CKG related to “the ﬁsh hunting area” with its CC 
Fig. 3 The CKG speciﬁc to “the ontology area” with its CC 
3.4 
Step 4: Enrichment of “The Constructed GCKG” 
Given the reference evaluation, we can conduct a quantitative analysis of the NPs 
describing the CCs found in the corpus. We create text ﬁles by extracting the NPs 
describing the CCs. Only the vocabulary necessary for one particular CC class is 
included in each ﬁle. Secondly, we visualized the NPs in each LDA input document 
and assigned determined percentages to each CC class in the corpus to conduct a 
qualitative study demonstrating the relationship between the CCs and our corpus 
and having an understanding of their distributions. Thirdly, based on the quantita-
tive and qualitative studies, these calculated percentages have been changed to NP 
occurrences. Fourthly, we seek to strike a balance between the CC occurrences to 
overcome the drawback of the stark disparity in them, reinforce their presence, and

90
A. Mechergui et al.
improve the caliber of the topics produced by the model. Any instance of a CC occur-
rence can be ﬁxed as a reference, and the resting CC occurrences can be rendered 
equitable to it. 
We are conﬁdent that this enhancement measure will assist our model in compre-
hending and capturing the clearly deﬁned topics in the corpus. It will boost the 
quality of the topics that are generated. Because BUGPBE-LDA adheres to the BoW 
assumption, only the occurrences of NPs in the corpus, not their order, matter. We 
have created new input data in the form of enriched textual ﬁles that better address 
the corpus and achieve our goal. Given an input GCKG accompanied by its relevant 
CCs, our approach ensures its semantic enrichment using our implemented algorithm 
to impart as output data the new subsumption relations between NPs extracted from 
the original dataset terms and classes based on the semantic similarities between 
entities within the same context. 
4 
Experiments 
We have performed two kinds of experiments by following our BUGPBE-LDA 
approach. In the ﬁrst one, we aim to use all the vocabularies of the articles and 
the CC classes over the whole two datasets. Since the corpus contains other NPs 
irrelevant to CCs, a new CC has been added to the interpretation of the results. It is 
called “others” given its general appointment and its ability to encompass all NPs that 
do not carry to the CCs. It contains 16,093 vocabularies and more exactly 90,292 NP 
occurrences. In the second experiment, we tend to clean our LDA textual input data 
from “the others CC class”. Thus, only the pertinent terms to the CC classes related 
to our CKGs have been kept. It includes 14,307 vocabularies and more precisely 
72,560 NP occurrences. However, in the third experiment, our goal is to use the orig-
inal LDA unsupervised clustering baseline over our two datasets and only with the 
pertinent terms to the CCs having 14,307 vocabularies to compare the results with 
the previous ones of our proposal. In this section, we ﬁrst set up the experiments in 
Sect. 4.1, and then report the results and analysis in Sect. 4.2. 
4.1 
Experiment Setup 
Corpora. As domain experts, we manually built two domain corpora and their gold 
standards. The ﬁrst corpus ONTO is about the ontology domain and contains 37 docu-
ments. The second one HUNT focuses on the ﬁsh hunting area, covering 74 separate 
documents which is double the initial corpus to reinforce the presence of terms 
related to various CC and therefore visualize whether there would be an improve-
ment in the outcome level produced by the model. All of the documents are online 
academic scientiﬁc papers consisting of several keywords beneﬁcial to our purpose 
and labeled by a CC of these two domains. We illustrate in Table 1 more details of

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
91
Table 1 Description of the carried-out experiments data following the BUGPBE-LDA approach 
Copus
The output of step 1.1.1
The output of step 1.2.4
The output of step 1.3.3 
Initial words per corpus 
(cleaned.txt docs) 
NP occ and errors (CSV 
docs) 
Prepared NPs (.txt docs) 
ONTO
138,746
30,063
21,763 
HUNT
204,963
84,878
68,529 
our dataset by following the ﬁrst step of BUGPBE-LDA for acquiring the candidate 
NPs. 
The LDA parameters settings. After executing many tests with our model by 
varying each time the values of its hyperparameters, we decided to follow the param-
eter setting of β from [34] where β = 0.01 because we try to discover a word in a 
topic with a 1% value. We kept the iteration number parameters equal to 100 as their 
default setting. Concerning α, we have set it to 1/2 = 0.5 since in our study we are 
looking for a small number of topics compared to the one sought daily. About value 
choices for the number of topics k in experiments 2 and 3, we set it equal to 20 since 
the CKGs related to ONTO and HUNT contain respectively 9 CCs and 11 CCs. 
However, in experiment 1, we set k equal to 22 in light of each CKG being given “an 
others class”. 
Evaluation Metrics. Based on the reference evaluation in step 2.2.2, the experts 
can annotate the 10 top frequent terms with percentages demonstrating the preci-
sion metric. By dividing the total number of terms identiﬁed by the number of 
correctly identiﬁed terms, precision is calculated. Next, they calculated the recall 
value. Regardless matter how many erroneous identiﬁcations were made, this recall 
statistic shows how many of the terms that were supposed to be identiﬁed were found. 
Finally, they accord to these topics the F-measure quantitative task-based evaluation 
[35] values that lie between 0 and 1. The ideal circumstance is when the F-measure 
values are very near to 1. F-measure is a combined precision and recall weighted 
average. Its formula is as follows in Eq. (2). 
upper F hy ph en m e a s u r e equals l eft parenthesi s 2 ast
erisk upper P r e c i s i o n asterisk upper R e c a l l right parenthesis divided by left parenthesis upper P r e c i s i o n plus upper R e c a l l right parenthesis
4.2 
Experiment Results and Analysis 
Comparison with Baseline. In experiment 3, the LDA unsupervised baseline model 
scored worse on both of the two datasets. It generated clusters with poor semantic 
coherence. We ﬁnd that even after the elimination of vocabulary related to “the others 
CC class”, the generated topics do not appear to be connected to any CC class a priori 
and overlap so many CCs at once. In Fig. 4, we illustrate some illustrations of the

92
A. Mechergui et al.
Fig. 4 The qualitative analysis for some generated topics by the LDA unsupervised clustering 
baseline in experiment 3. The X-axis represents the topics (T). The Y-axis represents the number 
of terms (terms of “the Others CC class” are not counted) in each topic 
topic composition including the 10 most frequent terms that vary between 8 and 10 
CCs. Many clusters, such as topics 4, 16, and 17 include 10 CCs at once. They have 
the worst cluster compositions. They are made up of a combination of terms relating 
to various CCs in roughly equal proportions. For instance, Topic 16 is composed 
of the following CCs; FA (10%), BC (10%), FG (10%), TI (10%), EN (10%), RE 
(10%), FS (10%), P (10%), C (10%), O (10%). These topics have terrible semantic 
separateness. 
We can apply the encapsulation principle that seeks to assemble or combine the 
CCs, whether by two or four, or six classes of CCs, into a single class. Indeed, in this 
case, the number of CCs is reduced and it varies between four and eight CC classes. 
This may present a smooth advancement. However, it does not allow us to attain our 
goal in its entirety. 
Also, a large number of clusters are dominated by the terms related to the “C, 
FA, FS, and BC” CCs. For instance, no clusters are generated in at least 20% of “S, 
U, and TC” classes. We can explain the fact that when applying these unsupervised 
approaches, this circumstance causes concepts to be missed and thus poor-quality 
partitions. 
The investigation of these corpora, and more speciﬁcally their quantitative study 
that is presented in Tables 2 and 3, enables the identiﬁcation of a signiﬁcant variation 
in the size of its CC classes that may result in an undesirable partition. Indeed, “C” 
is 13 times larger than “S”, and “FS” is 34 times larger than “TC” and nearly three 
times larger than “C” size. Thus, we agree that some CC classes are dominating while 
others are not. The LDA-generating procedure is the cause. By allocating the relevant 
terms to the topic with the highest probability, it generates topics. However, all terms 
in a topic share the same total probability mass, which is 1. Each term under a topic 
can only receive a minor chance if it corresponds to a dominant CC. As a result, 
some terms are more likely to appear under other topics related to non-dominant 
CCs. We concur that CC hunting classes are more dominant than ONTO classes in 
the results because the HUNT vocabulary is dominant and 2.5 times bigger than the 
ONTO vocabulary as shown in Tables 2 and 3.

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
93
Table 2 The quantitative analysis of “ONTO vocabulary” labeled by each CC class or by Others 
CC. Here “ALL” denotes “all vocabulary” and “Pertinent” refers to “pertinent vocabulary” 
Number of 
CC/corpus 
C
D
O
P
R
S
Te
To
U
Others 
ONTO 
(ALL) 
1,082
115
213
277
683
82
1,208
324
115
270 
ONTO 
(pertinent) 
1,082
115
213
277
683
82
1,208
324
115
0 
Table 3 The quantitative analysis of “HUNT vocabulary” labeled by each CC or by “Others CC”. 
Here “ALL” denotes “all vocabulary” and “Pertinent” refers to “pertinent vocabulary” 
Number 
CC/corpus 
FA
FS
BC
SH
SO
MA
FG
TC
EN
RE
TI
Others 
HUNT 
(ALL) 
3,100
3,096
1,486
340
150
100
980
90
688
475
303
1,516 
HUNT 
(pertinent) 
3,100
3,096
1,486
340
150
100
980
90
688
475
303
0 
Although, the difﬁculty in comprehending the LDA results can be attributed to 
the similarities between the topics that were generated. In reality, the fact that our 
corpus papers are highly connected and have a signiﬁcant intersection between them 
may be responsible for a substantial portion of this. Although, the expert accords to 
the generated topics an F-measure value of almost zero as an outcome of the weak 
semantics they provide. To be speciﬁc, for topics 1, 4, 9, 11, 14, 16, and 17, the 
corresponding F-measure values are in that order 0.1, 0, 0.1, 0.3, 0.2, 0, 0, and 0.1. 
Given that we have not discovered an excellent F-measure value, at least for the top 
most frequent NPs characterizing speciﬁc topics, the LDA-generated clustering is 
far from our overarching goal. 
BUGPBE-LDA. For both datasets in experiments 1 and 2, it’s approved that our 
BUGPBE-LDA semi-supervised approach outperforms good results. We analyze 
in Tables 4 and 5 the topic compositions that were learned from BUGPBE-LDA 
and the entire dataset to better understand how this approach behaves. Numerous 
terms are referring to each cluster’s CC. It picked up on good semantically coherent 
groupings and provide human-expected concepts. They achieved the highest value 
of expert annotation (100% correct classiﬁcations). We believe that the quality of 
cluster partitions is boosted with step 4 of our BUGPBE-LDA proposal since each 
of these clusters contains the semantics for the CCs.
Thanks to BUGPBE-LDA, there cannot be any overlap between any two topics 
related to CCs. In contrast, the LDA original models allow this overlap, for example, 
one term can be shared by two or more topics. Hence, the estimated F-measure equals 
1 for its higher performance and stability. 
It is remarkable that the use of the ﬁltered and pertinent vocabulary in experiment 
2 gave positive results and is more effective than the use of a mixture between all

94
A. Mechergui et al.
Table 4 The results of experiment 2 show the 10 top most frequent NPs/topics related to 11 CCs of “the ﬁsh hunting domain” and learned from two datasets, 
by our BUGPBE-LDA model with k = 20. The last row of each model is the annotation value computed by an expert 
Topic 1
Topic 4
Topic 6
Topic 8
Topic 10 
Topic 11
Topic 14
Topic 15
Topic 17
Topic 19
Topic 20 
yacht_ 
holder 
billﬁshes
seasonal_ 
migration 
marine_ 
environment 
ﬁshing_ 
timetable 
diesel_ 
consumption 
tropical_water
speciﬁc_ 
multilateral_ 
convention 
ﬁsherman
longline
sail_boart 
boat_ 
capitain 
scombrids
long_range_ 
swimming 
biological_ 
pollution 
stretch_ 
duration 
draught
mediterranean_ 
sea 
united_ 
nations_ 
convention_ 
on_sea_law 
boater
spinning_net
container_ 
ship 
kayak_ 
proprietor 
tuna_species
tuna_ 
thermoregulation 
accidental_ 
pollution 
time_ 
frame 
electricity_ 
production 
atlantic_ocean
freshwater_ 
ﬁshing_ 
regulation 
ship_captain
towed_gear
vessel 
ship_ 
leader 
Istiompax_ 
indica 
fecundity
sea_ 
surface_ 
temperature 
half_ 
hour_ 
after_ 
sunset 
hydro_ 
electric_ 
power 
indian_ocean
international_ 
maritime_ 
organization_ 
law 
sailor
trap
merchant_ 
ship 
ship_ 
landlord 
coastal_ 
species 
foraging_ 
behavior 
dissolved_ 
oxygen 
ﬁshing_ 
season 
propulsion
kerkennah_ 
archipelago 
regional_ 
convention 
seaman
dredge
recreational_ 
trawler 
cargo_ 
ship_chief 
billﬁshes_ 
rostrum 
sexual_ 
dimorphism 
atmosphere
open_ 
ﬁsh_ 
period 
overall_ 
length 
hadopelagic_ 
zone 
ﬁshing_ 
autorisation 
professional_ 
ﬁsherman 
ﬁshing_ 
tackle 
liner 
watercraft_ 
operator 
mid_ocean_ 
species 
natural_ 
mortality 
18 °C_ 
temperate 
ﬁshing_ 
night_ 
time 
crew
epipelagic_ 
water 
International_ 
compensation_ 
fund 
yachtie
ﬁshing_rig
watercraft 
saliling_ 
manager 
istiophoridae_ 
family 
juveniles_ 
morphology 
chemical_ 
pollution 
ﬁshing_ 
month 
hybrid_ 
diesel_ 
electric_ 
propulsion 
mesopelagic_ 
water 
ﬁshing_license 
mariner
spinner
bark
(continued)

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
95
Table 4 (continued)
Topic 1
Topic 4
Topic 6
Topic 8
Topic 10
Topic 11
Topic 14
Topic 15
Topic 17
Topic 19
Topic 20
sailboat_ 
owner 
xiphiidae_ 
family 
trophic_growth
terrestrial_ 
sources 
time_ 
frame 
vessel_ 
technical_ 
characteristic 
artctic_ocean
FAO_ﬁshers_ 
aquaculture_ 
regulation 
sea_farer
landing_net
barge 
yacht_ 
capitain 
tribe_sardini
diving_behavior
rivers
best_ 
ﬁshing_ 
year 
overall_ 
width 
souther_ocean
International_ 
maritime_ 
organization_ 
regulation 
navigator
paraphernalia racing_yacht 
100% CC 
SO 
100% CC 
FS 
100% CC 
BC 
100% CC 
EN 
100% 
CC TI 
100% CC 
TC 
100% CC 
FA 
100% CC 
RE 
100% CC 
MA 
100% CC 
FG 
100% CC 
SH

96
A. Mechergui et al.
Table 5 The results of experiment 2 showing the 10 top most frequent NPs/topics related to the 9 CCs of “the ontology domain” and learned from two datasets, 
by our BUGPBE-LDA model with k = 20. The last row of each model is the annotation value computed by an expert 
Topic 2
Topic 3
Topic 5
Topic 7
Topic 9
Topic 12
Topic 13
Topic 16
Topic 18 
common_ 
approach 
consensual_ 
conceptualization 
domain_ 
speciﬁc_ 
taxonomy 
agent
biology
biomedical_ 
repository 
basic_ 
building_ 
process 
common_ 
sense_ 
reasoning_ 
engine 
appropriate_ 
relation 
cohesion_ 
metric 
consequent_ 
specialization 
dynamic_ 
gene_ 
ontology 
client
philosophical_ 
domain 
cluster_output_ 
data 
available_ 
ontology_ 
engineering_ 
methodology 
common_ 
workbench 
alternative_ 
concept 
comparing_ 
technique 
crucial_step
entire_ 
ontology 
computer_ 
scientist 
pedagogical_ 
domain 
complex_ 
domain_ 
knowledge 
axiomatization
complex_ 
vocabulary_ 
system 
atomic_ 
concept 
complex_ 
algorithm 
current_ 
implementation 
formal_ 
extensional_ 
ontology 
concept_person 
politics_ 
domain 
co_occurrence_ 
information 
biological_ 
process 
conceptual_ 
system 
aircraft_ 
component 
common_ 
sense_ 
knowledge_ 
acquisition 
development_ 
phase 
example_ 
ontology 
consultant
complex_ 
multi_ 
disciplinary_ 
ﬁeld 
complementary_ 
source 
bottom_up_ 
development_ 
process 
complex_ 
application 
application_ 
domain_ 
concept 
ontology_ 
engineering_ 
method 
rigorous_ 
quantitative_ 
evaluation 
core_ 
domain_ 
otnology 
actor
autonomous_ 
domain 
protein_database
domain_ 
independent_ 
methodology 
inductive_ 
learning_ 
system 
binary_ 
relation_ 
diagram 
ontology_ 
design_ 
pattern 
ontology_ 
implementation_ 
activity 
substancial_ 
hierarchy 
specialist_ 
language_ 
enginner 
elearning_ 
domain 
reference_ 
document 
manual_ 
ontology_ 
development_ 
process 
representation_ 
system 
coordination_ 
component
(continued)

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
97
Table 5 (continued)
Topic 2
Topic 3
Topic 5
Topic 7
Topic 9
Topic 12
Topic 13
Topic 16
Topic 18
ontological_ 
mapping 
iterative_three_ 
step 
informal_ 
ontology 
ontology_user
innovative_ 
ﬁeld 
procedural_ 
knowledge 
self_learning_ 
cycle 
general_ 
purpose_ 
sentence_ 
splitter 
domain_ 
relevant_ 
syntactic_ 
relation 
ontological_ 
component_ 
architecture 
key_domain_ 
conceptualization 
well_ 
known_ 
upper_ 
ontology 
inexperienced_ 
person 
limited_ 
context 
referenced_paper 
successful_ 
collaborative_ 
building 
knowledge_ 
acquisition_tool 
article_entity 
ontology_ 
design_ 
classiﬁcation 
manual_ 
ontology_ 
evaluation 
bilingual_ 
ontology 
scientist
legal_domain
real_data_corpus 
classical_data_ 
mining_ 
process 
automatic_ 
keyword_ 
extractor 
taxonomic_ 
organization_ 
concept 
100% CC 
Te 
100% CC 
S 
100% CC 
O 
100% CC 
U 
100% CC 
D 
100% CC 
R 
100% CC 
P 
100% CC 
To 
100% CC 
C

98
A. Mechergui et al.
relevant and irrelevant vocabularies of CCs and corpora documents. From where we 
can afﬁrm that this hypothesis has strengthened the presence of our CCs. 
The ﬁndings of this study demonstrate how effective and promising the suggested 
approach is in automatically discovering and extracting pertinent terms from schol-
arly literature. However, the downside of this study is that since LDA is based on the 
principle of randomness and only approximates desired outcomes, there is a chance 
that the value of the F-measure will decline in the scenario where we visualize all 
the remaining terms for each generated topic rather than just its top most frequent 
NPs. 
5 
Conclusion and Future Works 
In this paper, we presented BUGPBE-LDA which is a new semi-supervised approach 
to building and enriching a constructed GCKG by adding semantically related knowl-
edge and using an implementation of the LDA technique. Since our dataset was 
trained on a controlled vocabulary, it provides more accurate results in ﬁsh hunting 
and ontology domains for category classiﬁcation, and semantic similarity of scien-
tiﬁc concepts. We evaluated our approach against the basic LDA baseline algorithm. 
The results show that our model is better than its competitor trained on the same two 
datasets. It is necessary to evaluate our method using larger datasets and a greater 
number of topics in LDA related to CC of more CKGs. Thus, it will enhance the 
validity of the suggested GCKG methodology, even though the tests on small corpora 
are signiﬁcant. Our comparison ﬁndings will be released soon. 
Appendix 
In the following Appendix, we present the implementation of our BUPBE-LDA 
algorithm on the R studio tool. 
Algorithm 1 Implementation of the BUGPBE-LDA algorithm 
library(NLP) 
library(tm) 
library(topicmodels) 
library(lda) 
library(LDAvis) 
path <- ﬁle.path("/Users/AM/Desktop") 
pos <- list.ﬁles(ﬁle.path(path,"corpus")) 
all.ﬁles <- ﬁle.path(path,"corpus", pos) 
txt <- lapply(all.ﬁles, readLines, encoding = "UTF-8") 
nsm <- gsub(path,"", all.ﬁles) 
aroms <- setNames(txt,nsm)

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
99
aroms <- sapply(aroms, function(x) paste(x, collapse = " ")) 
doc.list <- strsplit(aroms, "[[:space:]] + ") 
term.table <- table(unlist(doc.list)) 
length(term.table) 
term.table <- sort(term.table, decreasing = TRUE) 
vocab <- names(term.table) 
term.frequency <- as.integer(term.table) 
term.frequency[head(term.table)] 
term.frequency[tail(term.table) 
get.terms <- function(x) { 
index <- match(x, vocab) 
index <- index[!is.na(index)] 
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))} 
documents <- lapply(doc.list, get.terms) 
D <- length(documents) 
W <- length(vocab). 
doc.length <- sapply(documents, function(x) sum(x[2,])) 
N <- sum(doc.length). 
K <-21  
G <- 100 
alpha <-0.5 
eta <- 0.01 
t1 <- Sys.time() 
ﬁt <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = 
vocab, num.iterations = G, alpha = alpha, 
eta = eta, compute.log.likelihood = TRUE) 
t2 <- Sys.time() 
t2 - t1 
plot(ﬁt$log.likelihoods[2,], type = "l") 
top.words <- top.topic.words(ﬁt$topics, 10, by.score = TRUE) 
epsilon <- 1E-10 
theta <- t(apply(ﬁt$document_sums + epsilon, 2, function(x) x/sum(x))) 
phi <- t(apply(t(ﬁt$topics) + epsilon, 2, function(x) x/sum(x))) 
Test <- list(phi = phi, theta = theta, doc.length = doc.length, vocab = vocab, 
term.frequency = term.frequency) 
json <- createJSON(phi = Test$phi, theta = Test$theta, doc.length = 
Test$doc.length, 
vocab = Test$vocab, term.frequency = Test$term.frequency) 
serVis(json, out.dir = ’‘/Users/AM/Desktop’, open.browser = TRUE)

100
A. Mechergui et al.
References 
1. Hearst MA (1992) Automatic acquisition of hyponyms from large text corpora. In: COLING. 
The 14th international conference on computational linguistics, vol 2 
2. Hearst MA (1998) Automated discovery of wordnet relations. In: Fellbaum C (ed) WordNet: 
an electronic lexical database, pp 132–152 
3. Luis V-P, Gomez MM, Ortega-Mendoza RM (2007) Using lexical patterns for extracting 
hyponyms from the web. In: Mexican international conference on artiﬁcial intelligence, vol 
4827, pp 904–911. Springer 
4. Harris ZS (1968) Mathematical structures of language. Interscience tracts in pure and applied 
mathematics 
5. Sanderson CBM (1999) Deriving concept hierarchies from text. In: Proceedings of the 22nd 
annual international ACM SIGIR conference on research and development in information 
retrieval. ACMSIGIR, pp 206–213 
6. Koehn P (2005) Europarl: a parallel corpus for statistical machine translation. In: Proceedings 
of the 10th machine translation summit , pp 79–86 
7. Girardi C, Federico M, Cettolo M (2012) Wit3: web inventory of transcribed and translated 
talks. In: Proceedings of the conference of European association for machine translation, pp 
261–268 
8. Tishby N, Lee L, Pereira F (1994) Distributional clustering of English words. arXiv preprint 
cmp-lg/9408011 
9. Caraballo SA (1999) Automatic construction of a hypernym-labeled noun hierarchy from text. 
In: Proceedings of the 37th annual meeting of the association for computational linguistics, pp 
120–126 
10. Philipp Cimiano SS (2005) Learning concept hierarchies from text with a guided agglomerative 
clustering algorithm. In: Proceedings of the ICML 2005 workshop on learning and extending 
lexical ontologies with machine learning methods 
11. Colace F, De Santo M, Greco L, Amato F, Moscato V, Picariello A (2014) Terminological 
ontology learning and population using latent Dirichlet allocation. J Visual Lang Comput 
25:818–826 
12. Dhar AK, Vyas OP, Rani M (2017) Semi-automatic terminology ontology learning based on 
topic modeling. Eng Appl Artif Intell 63:108–125 
13. Xu Z, Harzallah M, Guillet F, Ichise R (2019) Modular ontology learning with topic modelling 
over core ontology. Proc Comput Sci 159:562–571 
14. Hajer B-Z, Besbes G (2015) Modular ontologies and CBR-based hybrid system for web 
information retrieval. Multimedia Tools Appl 74:8053–8077 
15. Mustapha NAMZHGH (2012) Modular ontological warehouse for adaptive information search. 
Int Conf Model Data Eng 7602:79–90 
16. Dorow B, Widdows D (2002) A graph model for unsupervised lexical acquisition. COLING 
2002 : the 19th international conference on computational linguistics 
17. Matsuo Y, Sakaki T, Uchiyama K, Ishizuka M (2006) Graph-based word clustering using a 
web search engine. In: Proceedings of the 2006 conference on empirical methods in natural 
language processing, pp 542–550 
18. Luo M, Lee J (2016) Word clustering for parallelism in classical Chinese poems. In: 2016 
international conference on Asian language processing (IALP). IEEE, pp 49–52 
19. Qiu J, Chai Y, Tian Z, Du X, Guizani M (2019) Automatic concept extraction based on semantic 
graphs from big data in smart city. IEEE Trans Comput Soc Syst 1:225–233 
20. Blondel VD, Guillaume J-L, Lambiotte R, Lefebvre E (2008) Fast unfolding of communities 
in large networks. J Stat Mech Theory Exp 2008:10008 
21. Thaiprayoon S, Unger H, Kubek M (2020) Graph and centroid-based word clustering. In: 
Proceedings of the 4th international conference on natural language processing and information 
retrieval, pp 163–168

A Bottom-Up Generic Probabilistic Building and Enriching Approach …
101
22. Choudhury S, Agarwal K, Purohit S, Zhang B, Pirrung M, Smith W, Thomas M (2017) NOUS: 
construction and querying of dynamic knowledge graphs. In: Conference: 8th international 
workshop on data engineering meets the semantic web (DESWeb) under ICDE 2017At, San 
Diego, California 
23. Dilek H-T, Asli C, Heck LP, Tur G (2014) Probabilistic enrichment of knowledge graph entities 
for relation detection in conversational understanding. Interspeech 
24. Li S, Huang Z, Cheng G, Kharlamov E, Gunaratna K (2020) Enriching documents with 
compact, representative, relevant knowledge graphs. In: Proceedings of the twenty-ninth 
international joint conference on artiﬁcial intelligence, pp 1748–1754 
25. Rezayi S, Zhao H, Kim S, Rossi R, Lipka N, Li S (2021) Edge: enriching knowledge graph 
embeddings with external text. In: Proceedings of the 2021 conference of the North American 
Chapter of the association for computational linguistics: human language technologies, pp 
2767–2776 
26. Hoyt CT, Domingo-Fernández D, Aldisi R, Xu L, Kolpeja K, Spalek S, Wollert E, Bachman 
J, Gyori BM, Greene P, Hofmann-Apitius M (2019) Re-curation and rational enrichment of 
knowledge graphs in biological expression language. Database 2019 
27. Grifﬁths TSM (2004) Finding scientiﬁc topics. Proc Natl Acad Sci 101:5228–5235 
28. Hornik K (2022) Package ‘NLP’ natural language processing infrastructure 
29. Feinerer I (2022) tm: Text Mining Package. A framework for text mining applications within 
R 
30. Grün B, Hornik K, Blei DM, Lafferty JD, Phan X-H, Matsumoto M, Nishimura T, Cokus S 
(2022) Package ‘topicmodels’ 
31. Chang J (2015) lda: Collapsed Gibbs Sampling Methods for Topic Models (2015) 
32. Carson Sievert KS (2022) Package ‘LDAvis’ Interactive Visualization of Topic Models 
33. Burita L, Pavel G, Tomas V (2012) K-gate ontology driven knowledge based system for decision 
support 
34. Hoffman M (2010) Online learning for latent Dirichlet allocation. In: Advances in neural 
information processing systems. Citeseer, pp 856–864 
35. Granada RL (2015) Evaluation of methods for taxonomic relation extraction from text. PhD 
thesis. Pontifícia Universidade Católica do Rio Grande do Sul

Measuring the Effects of Signal-To-Noise 
in EEG Emotion Recognition 
Zachary Estreito, Vinh Le, Jr. Frederick Harris, and Sergiu Dascalu 
Abstract Predicting valence and arousal values from EEG signals has been a stead-
fast research topic within the ﬁeld of Affective Computing or Emotional AI. Although 
many valid techniques to predict valence and arousal values from EEG signals have 
been established and veriﬁed, the EEG data collection process itself is relatively 
undocumented. This creates an artiﬁcial learning curve for new researchers seeking 
to incorporate EEGs within their research workﬂow. In this paper, we present a study 
that illustrates the importance of a strict EEG data collection process for EEG affec-
tive computing studies. We evaluate our work by ﬁrst validating the effectiveness of 
a machine learning prediction model on the DREAMER dataset, then showcasing 
the lack of effectiveness of the same machine learning prediction model on cursorily 
obtained EEG data. 
Keywords EEG · Affective computing · BCI · Emotion classiﬁcation · Data 
analysis · Machine learning · User study · Data integrity · Valence · Arousal 
1 
Introduction 
The recognition of human emotions from electroencephalography (EEG) signals 
has been a steadfast research topic within the ﬁeld of affective computing. Many 
valid techniques to interpret human emotional states from EEG signals have been 
documented, and a common workﬂow has been established. This process involves 
Z. Estreito (B) · V. Le · Jr. Frederick Harris · S. Dascalu 
University of Nevada, Reno, 1664 N Virginia St, Reno, NV 89557, USA 
e-mail: zestreito@unr.edu 
V. Le 
e-mail: vle@unr.edu 
Jr. Frederick Harris 
e-mail: fred.harris@cse.unr.edu 
S. Dascalu 
e-mail: dascalus@cse.unr.edu 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_8 
103

104
Z. Estreito et al.
EEG signal collection, band separation, feature extraction, and emotion classiﬁca-
tion. Information about proper EEG data collection processes, however, is scarce. 
As such, this scarcity leads to an artiﬁcial bottleneck for new researchers seeking to 
incorporate EEGs within their research workﬂow. 
EEG signals are extremely vulnerable to external artifacts, and a controlled envi-
ronment is essential for obtaining usable EEG signal data. While there are measures 
that can be taken to remove artifacts and noise from EEG data, these techniques are 
limited in their efﬁcacy. A participant’s involuntarily movement or a random sound 
emitted nearby would already be enough to compromise the data being collected. 
Furthermore, the collection of data could also be compromised by factors uncontrol-
lable by an experiment’s facilitator, such as the participant’s hair density or current 
medication. 
In this paper, we present an EEG user study in which external artifacts, such as 
noise or involuntary movement, accumulated within the EEG data collected due to 
an uncontrolled testing environment. From there, the results of a machine learning 
prediction model on the established DREAMER data-set is compared to the results of 
the same prediction model on the EEG data obtained in the before-mentioned poorly-
controlled environment. This study demonstrates the importance of a meticulous 
and methodical approach to EEG data collection by showing the ineffectuality of 
validated data analysis techniques when applied EEG data with a high signal-to-
noise ratio. 
The remainder of this paper is structured as follows: Section 2 presents a brief 
background of the valence and arousal within Affective EEG studies and the related 
techniques utilized during the study, Sect. 3 provides details of the experiment 
methodology, Sect. 4 showcases a comparison between the results of the experiment 
and the DREAMER data-set, Sect. 5 provides a brief discussion of the experiment 
and EEG data collection, and Sect. 6 wraps up the paper with the conclusion and 
planned future work. 
2 
Background and Related Works 
2.1 
Valence-Arousal Model 
RusselâŁ™s Valence-Arousal Model [ 14] is a human emotion classiﬁcation model 
consisting of two dimensions, valence and arousal. Valence represents the positivity 
of the emotion being felt, with positive emotions existing on one side and negative 
emotions on the other. Arousal represents the level of stimulation of the emotion 
being felt, with high stimulation emotions on one side and low stimulation emotions 
on the other. The combination of these two axes allows emotions to be categorized 
in four unique quadrants: high valence high arousal, high valence low arousal, low 
valence high arousal, and low valence low arousal. Discrete emotions can then be 
placed in their associated quadrants, as seen in Fig. 1.

Measuring the Effects of Signal-To-Noise …
105
Fig. 1 The representation of emotions on the valence-arousal model 
2.2 
Pre-processing 
The goal of pre-processing EEG data is to remove noise from the signals, yielding a 
higher signal-to-noise ratio. EEG electrodes are highly prone to artifacts and inter-
ferences. Notch ﬁltering is used to remove alternating current power line interference 
from EEG signals, most commonly at 50 Hz or 60 Hz. Bandpass ﬁltering is used 
to remove frequencies outside of the useful range. Signals can also be detrended 
to compensate for the dehydration of wet electrodes over the course of a recording. 
Artifacts from wearer movement, including facial muscle movement and speech, can 
only be removed with limited success depending on the severity of the movement. 
Regression analysis is an effective approach to remove artifacts, but requires a ref-
erence channel. Both EEG devices used in this study provide a reference channel, 
so this approach was utilized. Artifactual segment rejection involves removing arti-
facted sections of EEG data [ 9]. A visualization of EEG signal pre-processing can

106
Z. Estreito et al.
Fig. 2 EEG signal pre-processing. a Raw EEG signal. b EEG signal with power line notch ﬁlter 
applied. c EEG signal with power line notch ﬁlter and regression analysis artifact removal applied 
be seen in Fig. 2. Multiple approaches to pre-processing were taken, including the 
use of notch ﬁltering, bandpass ﬁltering, signal detrending, and regression analysis, 
but no combination was found to have any signiﬁcant effects on the usability of the 
data obtained in this study for emotion classiﬁcation. 
2.3 
EEG Frequency Bands 
EEG signals represent neural oscillations, which are grouped in ﬁve frequency bands: 
delta (1–4 Hz), theta (4–8 Hz), alpha (8–12 Hz), beta (12–30 Hz), and gamma (30+ 
Hz). Delta and theta waves are associated with unconscious or subconscious mental 
activities, so the delta and theta band brain waves were ﬁltered out of the data used in 
this study. Alpha, beta, and gamma waves are associated more with active perception, 
and are useful for the classiﬁcation of emotions. For this reason, EEG data used in 
this study was run through a bandpass ﬁlter to separate it into the alpha, beta, and 
gamma components. Additionally, asymmetry in brain wave activity is useful for the 
classiﬁcation of valence, especially on the alpha band [ 2], so alpha band features 
were converted to alpha asymmetry differential features by subtracting the right side 
alpha features from the left side alpha features [ 17]. 
2.4 
EEG Feature Extraction 
EEG feature extraction can be divided into four domains: frequency domain meth-
ods, time domain methods, time-frequency domain (wavelet transform) methods, 
and nonlinear methods [ 1]. For the purposes of emotion classiﬁcation, all of these 
have been found to be useful. Galvão et al. [ 6] found that the best features to use 
with the random forest classiﬁer are the time domain Hjorth Activity parameter, the 
time-frequency domain wavelet energy, and wavelet entropy parameters of the alpha 
differential asymmetry, beta, and gamma bands. This study utilizes the Hjorth Activ-
ity parameter on the alpha differential asymmetry, beta, and gamma bands, which 
was found to be nearly as successful as the combination of the three aforementioned 
features.

Measuring the Effects of Signal-To-Noise …
107
2.5 
Emotion Classiﬁcation 
Classiﬁcation of emotions from EEG features with machine learning techniques has 
been proven to be a valid approach to EEG signal analysis. The random forest (RF) 
and K-nearest neighbors (KNN) classiﬁers are two of the most successful machine 
learning methods for emotion classiﬁcation in recent literature, with RF showing 
slightly better performance when provided with less features [ 6, 18]. Because only 
the Hjorth Activity parameter is being used in this study, the random forest classiﬁer 
was chosen. A tree depth of 100 was used due to the conclusions from Giannakaki 
et al. [ 7] and our own testing revealing that increasing the tree depth past 100 did not 
yield better results. 
3 
Methodology 
3.1 
Participants 
Eight participants between the ages of 22 and 35 were recruited for this study. There 
were no speciﬁc characteristics being sought after. The only known characteristic 
of participants relevant to this study would be the type, thickness, and length of 
the participant’s hair. Participants were not selected based on their hair, and the 
participants had a wide variety of hair types. 
3.2 
Hardware 
The Emotiv Epoc  X [  4] and OpenBCI EEG Electrode Cap [ 15] were selected for 
this study. Both devices are among the most commonly used consumer-grade EEG 
devices in affective computing research, and they have both been validated in multiple 
studies [ 3]. 
The Emotiv Epoc X, shown in Fig. 3, is one of the two EEG-based BCI devices 
being utilized in this study. The Epoc X reads EEG signals through the conductive 
felt pads, which are hydrated with a saline solution before use. These felt pads must 
contact the scalp of the wearer. Adequate contact quality can be difﬁcult to achieve 
in wearers with curly, thick, and/or long hair, and the wearer’s hair may need to be 
parted where the hydrated felt pads make contact. The Emotiv Epoc X requires the 
use of Emotiv’s proprietary software, and there is innate pre-processing done by the 
device. Both of these factors make the Emotiv Epoc X less useful as a research device 
due to their enigmatic nature. 
The OpenBCI EEG Electrode Cap, shown in Fig. 4, uses a saline gel to create a 
contact surface that can vary in size as desired, as well as pass through hair more 
easily. The cap is put on the wearer before the saline gel is applied. While the cap

108
Z. Estreito et al.
Fig. 3 The experimental setup with the Emotiv Epoc X 
is being worn, the saline gel is drawn into a blunt tip syringe to be injected into the 
gaps in the contact points. The amount of gel used is variable and depends on the hair 
type, density, and length of the wearer. Hair that is curly, dense, and/or long requires 
a greater volume of saline gel due. 
3.3 
Software 
Throughout this user study, a couple of software packages were used in order to 
collect the EEG Data. As Emotiv and OpenBCI have their own proprietary designs, 
we opted to use the built-in interfaces to affect each headset. To consolidate the 
experience and make it a bit more autonomous, we utilized a software suite designed 
to automatically label events within EEG streams. 
EmotivPRO [ 5] is the software package required to stream and record EEG data 
from the Emotiv Epoc X. EmotivPRO is proprietary and requires a subscription-

Measuring the Effects of Signal-To-Noise …
109
Fig. 4 The experimental setup with the OpenBCI All-In-One EEG Electrode Cap 
based license to access the most important features, such as Lab Streaming Layer 
(LSL) streaming. EmotivPRO also includes features to assist with EEG device setup 
by showing a live display of the contact quality of each electrode. EmotivPRO was 
used in this study to verify contact quality and generate LSL streams to work in 
conjunction with the GEDAPS software suite. 
The OpenBCI GUI [ 13] is an open-source GUI tool that allows streaming and 
recording of EEG data from OpenBCI devices. There are also visualization tools, 
including a contact quality visualizer to assist with cap setup. It offers the ability 
to stream EEG data with the BrainFlow library, but not LSL. This was used in 
conjunction with the OpenBCI LSL [ 8] command-line plugin to utilize the LSL 
protocol. 
GEDAPS [ 12] is the software suite used to autonomously display the emotional 
stimuli images presented to participants in this study. GEDAPS removes the need for 
a facilitator to manually records timings and metadata within an EEG user study. It 
works together with a backend that utilizes PyLSL to generate event marker times-
tamps and inserts it directly into a EEG headset’s datastream. This is so that EEG 
data can later be divided up into time-based sections speciﬁc to the content being per-
ceived by the EEG headset wearer. This suite was designed to work with the Emotiv 
Epoc X but was modiﬁed as part of this research to accommodate the OpenBCI EEG 
Electrode Cap for the purpose of this study.

110
Z. Estreito et al.
3.4 
Datasets 
The International Affective Picture System [11] is a picture data-set with standardized 
emotion labels. Each picture is accompanied by self-reported ratings of valence, 
arousal, and dominance. This data-set contains many pictures with very graphic 
content. In lieu of this, the media used for this study were speciﬁcally curated to 
invoke positive and negative reactions in both valence and arousal. To this end, there 
were four combinations used for the experiment in this paper. These combinations 
were: high valence with low arousal, low valence with low arousal, low valence with 
high arousal, and high valence with high arousal. 
DREAMER [ 10] is a dataset of EEG signals and ECG signals accompanied by 
self-reported valence and arousal labels, conducted over 18 video samples. The EEG 
signal analysis methods used in this study were ﬁrst veriﬁed on the DREAMER data 
alone, by creating a machine learning model out of the EEG data for 22 of the 23 
entries within the DREAMER database, then using that model to predict the valence 
and arousal labels of the EEG data of the 23rd entry, and comparing the predicted 
values to the self-reported values. The feature extraction and classiﬁcation model 
used in this study was approximately 80% accurate at predicting both valence and 
arousal values when tested in this manner. 
3.5 
Procedure 
All participants were tested with both the Emotiv Epoc X and the OpenBCI EEG 
Electrode Cap. Four of the eight participants started with the Emotiv Epoc X and 
four of the participants started with the OpenBCI EEG Electrode Cap. Participants 
were asked not to speak, move their body, or move any facial muscles during the 
task, but were also informed that they would be free to leave at any point should they 
wish to not continue. After being ﬁtted with the ﬁrst EEG device and conﬁrming 
that the participant was in a comfortable position and was ready to proceed, the EEG 
task was initiated. After the task was completed, the EEG device was removed from 
the participant’s head, and they were given towels to dry their scalp on the contact 
points. The ﬁtting process was then repeated with the other EEG device, and the task 
was performed again once the participant was ready. Upon completion of the task 
with the second device, the participant was once again given towels to dry their scalp 
and then their involvement in the user study ended. 
3.6 
Tasks 
As part of the user study, the tasks that were performed involved the participant 
watching a series of images from the IAPS data-set chosen speciﬁcally to invoke

Measuring the Effects of Signal-To-Noise …
111
emotions ﬁrmly in one of the four valence-arousal quadrants indicated above in 
Sect. 3.4. Before viewing the images, participants were prompted to hold their eyes 
open for 60 s to the best of their ability, then closed for an additional 60 s to establish 
a baseline. The image series sequence began after establishing this baseline. Each 
series contained 12 images displayed for 5 s each, for a total of 60 s. A gray screen 
was displayed for 5 s between each series to allow the participant’s affect to return 
to baseline. While this was happening, the GEDAPS software was autonomously 
logging the EEG data and labeling these events for future processing. 
4 
Results and Comparative Analysis 
The ﬁrst part of the comparative analysis for this study was to conﬁrm that the feature 
extraction and classiﬁcation methods chosen were valid. This was accomplished 
using the EEG data from the DREAMER dataset. Two machine learning models 
were created. The ﬁrst model used the random forest regressor with a tree depth 
of 100 trained on the Hjorth activity of the alpha differential asymmetry, beta, and 
gamma bands. This model was created 23 times (once for each entry in DREAMER) 
to generate a continuous number prediction. The second model used the random 
forest classiﬁer with a tree depth of 100 trained on the Hjorth activity of the alpha 
differential asymmetry, beta, and gamma bands. This model was created 23 times 
(once for each DREAMER entry) to generate a discrete number classiﬁcation. In each 
case, the model was trained on the EEG data and self-reported labels from the other 
22 entries to predict the self-reported valence and arousal values of the selected entry. 
The self-reported valence and arousal labels in the DREAMER dataset are integers 
from 1 to 5. An example of the continuous prediction results generated for all of 
DREAMER’s 18 video samples for subjects 1–3 using machine learning model 1 
can be seen in Fig. 5. 
To determine signiﬁcance, the discrete accuracy of the predicted values generated 
by machine learning model 2 was compared to the expected 20% accuracy of a 
random number generator constrained to the integer values between 1 and 5, as 
shown in Fig. 6. The combined results of all 23 subjects in the DREAMER data-set 
easily reached statistical signiﬁcance (p less than<0.001) when compared to the expected 
results of a random number generator. 
With the validity of the model veriﬁed, the next step was to analyze the data 
collected from the participants in this study. Two approaches were taken for this 
analysis. Approach 1 involved using machine learning model 1 to predict the values of 
the participants in this study. The expected results would be as follows: High valence 
greater than or equals≥3.0, low valence less than<3.0, high arousal greater than or equals≥3.0, and low arousal less than<3.0. Unfortunately, 
the data obtained from the participants in this study was not able to reach statistical 
signiﬁcance for either of the two EEG devices using this approach, and the data did 
not seem to follow even a non-signiﬁcant trend towards expected results, as presented 
in Fig. 7.

112
Z. Estreito et al.
Fig. 5 Valence and arousal prediction results for DREAMER subjects 1–3 using 22-subject con-
tinuous DREAMER model 
The second comparative analysis approach involved training a model using data 
from 7 of the 8 participants to predict the valence-arousal quadrant of the 8th partici-
pant. The classiﬁer labels used for this were binary, with 1s representing high values 
for valence and arousal, and 0s representing low values for valence and arousal. Due 
to the smaller training sample size, machine learning model 1 was re-validated on the 
DREAMER dataset for a smaller sample size by training it on every possible combi-
nation involving 8 subjects (7 to train and 1 to predict). Instead of using the provided 
discrete ratings from 1 to 5, DREAMER data was re-labeled to match the binary clas-
siﬁer labels of high/low valence and high/low arousal to check quadrant accuracy. 
Mean quadrant accuracy for all participants was approximately 52%, which is much 
higher than the 25% that could be expected from a random number generator. This 
approach also reached statistical signiﬁcance with a z-score of 3.3 and pless than<0.001. A 
table of these predictions is presented in Fig. 8.

Measuring the Effects of Signal-To-Noise …
113
Fig. 6 Valence and arousal prediction results for DREAMER subjects 1–3 using 22-subject discrete 
DREAMER model 
Unfortunately, this approach was also unsuccessful in generating statistically sig-
niﬁcant results for both the Emotiv Epoc X and the OpenBCI EEG Electrode Cap. As 
was the case with approach 1, there was also no visible non-signiﬁcant trend towards 
expected results. The mean valence and arousal values for all four quadrants were 
all within a single standard deviation of each other, indicating that the model could 
not reliably identify valence or arousal in the participant EEG data. These results are 
shown in Fig. 9. 
5 
Discussion 
While validated, the analytical processes and statistical operations could not in the end 
overcome the sheer amount of artifact pollution within the collected data. Although 
statistical operation can indeed suppress or clean up some of the noise within a

114
Z. Estreito et al.
Fig. 7 Valence and arousal prediction results for study participants using 22-subject DREAMER 
model 
data-set, there is an upper limit on what is achievable with a data-set from a poorly 
controlled environment. Techniques such as notch ﬁltering, bandpass ﬁltering, signal 
detrending, and regression analysis [ 9], exist to remove artifacts from EEG data but 
unfortunately, none were found to be sufﬁcient to make this data usable. In order 
to mitigate these issues, it is important to take effort in creating a controllable and 
secluded environment that is conducive to EEG data collection. 
In order to establish a better solution of a more controllable environment, it is 
important to revisit some of issues that occurred as part of the data collection portion 
of this paper. Despite being asked to remain still and silent, several participants 
involuntarily spoke or ﬁdgeted during the task. The combination of these factors 
added a lot of noise to the EEG signals, resulting in a poor signal-to-noise ratio.

Measuring the Effects of Signal-To-Noise …
115
Fig. 8 Valence and arousal prediction results for DREAMER subjects 1–3 using 7-subject quadrant 
DREAMER model 
It is also likely that some less commonly discussed external factors played a role 
in making the data unusable, namely participant distraction due to nearby audible 
conversations. If participants were distracted by nearby conversations, they may 
not have been processing the visual stimuli of the images being shown to them. 
Participant distraction does not appear as noise or artifacts on EEG data, but rather it 
produces EEG data that simply does not match the expected results. For this reason, 
participant distraction cannot be mitigated with any data analysis approach. Lastly, 
the participants also had varying hair types, and it was observed during the data 
collection process that thicker and curlier hair types signiﬁcantly worsened EEG 
contact quality.

116
Z. Estreito et al.
Fig. 9 Valence and arousal prediction results for each quadrant, separated by device 
6 
Conclusion and Future Work 
The primary goal of this study was to compare the efﬁcacy of a machine learning 
model for predicting valence and arousal values from both clean and noisy EEG signal 
data. Data collection was performed in an uncontrolled environment, with numerous 
sources of external inﬂuence. The results of the captured data and the established data-
set, DREAMER, were placed in the same analytical model to perform a comparative 
analysis. It was discovered that the combination of involuntary artifacts and external 
factors caused a too severe reaction with the noise within the stream. Greater care 
and effort has to be made in order to record EEG data, as technology has not reached 
the point in which there is a reliable way to ﬁlter out excessive noise within the data 
stream. 
As a part of our future work, we have plans for further research to determine 
the effects of different hair types on EEG signal quality. We hope there could be an 
applicable solution that could be included in the analysis to compensate for hair type.

Measuring the Effects of Signal-To-Noise …
117
Additionally, we plan to expand this research by testing the effects of uncontrolled 
environments on events that would maybe beneﬁt or remain indifferent of artifacts, 
such as confusion detection. Lastly, with the recent release of OpenBCI’s gelfree 
EEG cap [ 16], a direct comparison between the efﬁcacy and usability of consumer-
grade saline hydrated felt electrode EEG devices and consumer-grade gel electrode 
EEG devices can be studied without the confounding variables of different hardware 
manufacturers and proprietary software. 
Acknowledgements This material is based in part upon work supported by the National Science 
Foundation under grants OAC-2209806, OIA-2019609, and OIA-2148788. Any opinions, ﬁndings, 
and conclusions or recommendations expressed in this material are those of the authors and do not 
necessarily reﬂect the views of the National Science Foundation. 
References 
1. Acharya UR, Sree SV, Swapna G, Martis RJ, Suri JS (2013) Automated EEG analysis of 
epilepsy: a review. Knowl Based Syst 45:147–165 
2. Alarcao SM, Fonseca MJ (2017) Emotions recognition using eeg signals: a survey. IEEE Trans 
Affect Comput 10(3):374–393 
3. Dadebayev D, Goh WW, Tan EX (2022) EEG-based emotion recognition: review of commercial 
eeg devices and machine learning techniques. J King Saud Univ Comput Inf Sci 34(7):4385– 
4401 
4. EMOTIV. Emotiv epoc x–14 channel mobile eeg headset. https://www.emotiv.com/epoc-x/. 
Accessed 16 Apr 2023 
5. EMOTIV. Emotivpro. https://www.emotiv.com/emotivpro/. Accessed 16 Apr 2023 
6. Galvão F, Alarcão SM, Fonseca MJ (2021) Predicting exact valence and arousal values from 
eeg. Sensors 21(10):3414 
7. Giannakaki K, Giannakakis G, Farmaki C, Sakkalis V (2017) Emotional state recognition 
using advanced machine learning techniques on eeg data. In 2017 IEEE 30th international 
symposium on computer-based medical systems (CBMS). IEEE, pp 337–342 
8. GitHub. Openbci lsl/openbci lsl.py at master–openbci-archive/openbci lsl. https://github.com/ 
openbci-archive/OpenBCI_LSLI. Accessed 16 Apr 2023 
9. Islam MK, Rastegarnia A, Yang Z (2016) Methods for artifact detection and removal from 
scalp eeg: a review. Neurophys Clin/Clin Neurophys 46(4–5):287–305 
10. Katsigiannis Stamos, Ramzan Naeem (2017) Dreamer: a database for emotion recognition 
through eeg and ecg signals from wireless low-cost off-the-shelf devices. IEEE J Biomed 
Health Inf 22(1):98–107 
11. Lang PJ, Bradley MM, Cuthbert BN et al (2005) International affective picture system (IAPS): 
affective ratings of pictures and instruction manual. NIMH, Center for the Study of Emotion 
& Attention Gainesville, FL 
12. Le VD, Carthen CD, Kamaruddin N, Tavakkoli A, Dascalu SM, Jr Harris FC (2023) Generalized 
eeg data acquisition and processing system. In: Proceedings of the 20th international conference 
on information technology: new generations (ITNG 2023) 
13. OpenBCI. The openbci gui. https://github.com/OpenBCI/OpenBCI_GUI. Accessed 16 Apr  
2023 
14. Russell JA (1979) Affective space is bipolar. J Personal Soc Psychol 37(3):345 
15. OpenBCI Online Store. All-in-one EEG electrode cap starter kit. https://shop.openbci.com/ 
products/openbci-eeg-electrocap-kit. Accessed 16 Apr 2023

118
Z. Estreito et al.
16. OpenBCI Online Store. All-in-one gelfree BCI electrode cap kit. https://shop.openbci.com/ 
products/all-in-one-gelfree-bci-electrode-cap-kit. Accessed 16 Apr 2023 
17. Thammasan Nattapong, Moriyama Koichi, Fukui Ken-ichi, Numao Masayuki (2017) Famil-
iarity effects in EEG-based emotion recognition. Brain Inf 4:39–50 
18. Xu H, Plataniotis KN (2012) Affect recognition using eeg signal. In 2012 IEEE 14th interna-
tional workshop on multimedia signal processing (MMSP). IEEE, pp 299–304

Phishy? Detecting Phishing Emails Using
Machine Learning and Natural
Language Processing
Md. Fazle Rabbi, Arifa I. Champa, and Minhaz F. Zibran
Abstract Phishing emails, a type of cyberattack using fake emails, are difﬁcult to
recognize due to sophisticated techniques employed by attackers. In this paper, we
use a natural language processing (NLP) and machine learning (ML) based approach
for detecting phishing emails. We compare the efﬁcacy of six different ML algorithms
for the purpose. An empirical evaluation on two public datasets demonstrates that our
approach detects phishing emails with high accuracy, precision, and recall. The ﬁnd-
ings from this work are useful in devising more efﬁcient techniques for recognizing
and preventing phishing attacks.
Keywords Phishing · Spam · Email · Classiﬁcation · Detection · Machine
learning · Natural language processing
1
Introduction
Phishing is a common approach that cybercriminals use to steal sensitive information
from individuals and organizations. Messaging apps, phone calls, social media, and
emails are just a few of the many platforms that phishing attacks target. The outbreak
of the coronavirus epidemic resulted in an increase in phishing attacks that was 220%
higher than the yearly average [35]. According to the Anti-Phishing Working Group,
there were a record-breaking total of 1,270,883 unique phishing incidents reported
in the third quarter of 2022, demonstrating the prevalence and persistence of this
threat [2].
Md. F. Rabbi (B) · A. I. Champa · M. F. Zibran
Idaho State University, Pocatello, ID, USA
e-mail: mdfazlerabbi@isu.edu
A. I. Champa
e-mail: arifaislamchampa@isu.edu
M. F. Zibran
e-mail: minhazzibran@isu.edu
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_9
119

120
Md. F. Rabbi et al.
Email has long been arguably the most used platform for electronic communica-
tion in both formal and informal settings. Therefore, email platforms have been the
most frequent targets for phishing attacks where cybercriminals create scam emails
that appear genuine but contain traps for the end-user for becoming a victim of
information leaks or other malicious attacks.
According to Forrester Research, 91% of all hacking attacks begin with a phishing
email [22]. In 2022, 96% of organizations were targeted by email-related phishing
attempts [23]. Phishing attacks are becoming increasingly sophisticated and can
deceive even experienced users, as illustrated by the case of the John Podesta phishing
attack [33]. This incident, involving a fake email that tricked the former chairman of
the Hillary Clinton presidential campaign, demonstrated that anyone is susceptible
to an email phishing attack.
Phishing often takes advantage of the naivety of end-users, and combating it
requires human intervention in the form of awareness and training campaigns [7,
9, 24]. Despite the ongoing efforts in employee training and social awareness of
email phishing attacks, these attacks continue to increase, resulting in an increasing
number of email phishing victims. This indicates that we need more sophisticated
approaches for the automatic detection of phishing emails before those emails are
presented to the end-users, who may or may not be technically well-educated.
To develop an effective approach for detecting phishing emails, it is crucial to
understand the characteristics of phishing emails that distinguish them from legiti-
mate ones. To accomplish this, we examine the characteristics and traits of phishing
emails to identify patterns that can be used to develop an effective detection model.
In particular, we address the following research questions:
RQ1: What is the most effective machine learning (ML) algorithm for detecting
phishing emails?
—Since different ML algorithms have their strengths and weaknesses in different
application contexts, it is critical to identify the most effective algorithm for detecting
phishing emails. We measure effectiveness in terms of accuracy, precision, recall,
and F-score.
RQ2: What impact does ML algorithm training time have on the performance of
phishing email detection?
—Lengthy training times may not be feasible in some operational settings, and if
longer training times do not produce signiﬁcant improvements in the performance of
the model, the additional time and resources invested would just be wasted. Hence,
a deep understanding of the relationship between training time and performance can
assist in the development of more efﬁcient phishing detection systems for adoption
in practical settings.
RQ3: To what extent does the ‘subject’ feature of an email contribute to the accurate
detection of phishing emails?
—One of an email’s most noticeable and prominent features is the ‘subject’, and
the content of that line has a signiﬁcant impact on how the recipient perceives the
email’s authenticity. Investigating the ‘subject’ feature’s contribution to accurately

Phishy? Detecting Phishing Emails Using …
121
detecting phishing emails can provide insights into the creation and identiﬁcation
of phishing emails, which will aid in the development of more effective phishing
detection systems.
To address the aforementioned research questions, we use two phishing email
datasets, which we process using natural language processing (NLP) and then we
compare six machine learning algorithms by separately operating them on the two
datasets.
The rest of the paper is structured as follows. Section 2 describes the data sets
used in this work. Section 3 describes the methodology of this study. The ﬁndings
from this study are presented and discussed in Sect. 4. In Sect. 5, we describe the
limitations of our work. Previous work in the literature that are related to ours are
discussed in Sect. 6. Finally, Sect. 7 concludes the paper.
2
Datasets
For our work, we use two publicly available datasets. Table 1 presents the number of
phishing and legitimate emails in each of the datasets, which are further described
below.
2.1
Ling Dataset
The Ling Spam dataset [30] is a collection of emails speciﬁcally focused on topics of
interest to linguists. The dataset is organized around two main attributes: ‘subject’ and
‘message,’ as well as a ‘label’ indicating whether each email is spam or legitimate.
The subject and body of an email are captured in ‘subject’ and ‘message’ respectively.
Table 1 Datasets used in our work
Email
Dataset
Ling [30]
TREC [6]
Legitimate
2,412
25,220
Phishing
481
50,199
Total
2,893
75,419

122
Md. F. Rabbi et al.
2.2
TREC Dataset
The TREC Public Corpus [6] is a collection of email messages collected between
April8andJuly6,2007.Forourstudy,apreprocessedTRECPublicCorpusDatasetis
collected from Kaggle [5]. This dataset includes four attributes: ‘subject,’ ‘email_to,’
‘email_from,’and‘message,’aswellasa‘label’indicatingwhethertheemailisphish-
ing or legitimate. The ‘email_to’ and ‘email_from’ attributes respectively capture the
email addresses of the recipients and sender of the email.
3
Methodology
We use a three-phase procedure for recognizing phishing emails. First, we sanitize
the dataset through a preprocessing phase. Second, subsets of the preprocessed data
are then separately fed to six ML algorithms [10] to create ML models. Finally,
the ML models are operated on separate subsets of the preprocessed datasets. The
models’ performances are measured using a set of metrics described in Sect. 3.3.
To avoid the overhead of installing and conﬁguring different tools and libraries
on a local machine for operating the ML algorithms, we leverage a cloud-based
platform, Google Colab [3], for the purpose. However, we use the processing power
of our local machine rather than Colab’s GPU or TPU. Our local machine, which runs
a Windows 10 operating system, is equipped with an Intel Core i7-8650U processor
with a base clock speed of 1.90 GHz and turbo boost up to 4.2 GHz, and 16GB of
DDR4 memory.
3.1
Data Preprocessing
In the preprocessing phase, we handle missing attribute values, eliminate dupli-
cates/redundancy, clean data, and convert them to a format for feeding to the ML
algorithms.
3.1.1
Handling Missing Attribute Values
In the email datasets, we notice that some emails do not include a ‘subject’ but have a
message body while some emails include a ‘subject’ with an empty message body. In
cases where the ‘subject’ attribute is missing for an email, we exclude the attribute
from consideration and only use the ‘message’ body for that particular email. If
the ‘message’ attribute is missing for a particular email, we exclude that entire email
from our study. Table 2 presents the number of instances we encountered the missing
attribute values in the two datasets used in our work.

Phishy? Detecting Phishing Emails Using …
123
Table 2 Missing attribute values
Attribute
Dataset
Ling [30]
TREC [6]
Subject
62
793
Message
0
1487
3.1.2
Removing Duplicates
In the TREC dataset, the ‘message’ attribute contains 14,885 instances of duplicate
data, and when we combine the ‘subject’ and ‘message’ attributes, we identify 11,107
instances of duplicate emails. In the Ling dataset, the ‘message’ feature contains 34
instances of duplicate data, and when we merge the ‘subject’ and ‘message’ features,
we discover 17 instances of duplicate emails. We eliminate the duplicates keeping
only one instance from each duplicate set.
3.1.3
Data Cleansing
To prepare data for ML algorithms, we perform further cleanup operations as listed
below:
. We convert all text to lowercase, to ensure consistency.
. We remove any leading and trailing white spaces from each email body.
. We replace actual email addresses, URLs, currency symbols, and contact numbers
with the placeholders ‘MAILID’, ‘LINKS’, ‘MONEY’, and ‘contact number’,
respectively.
. Any non-alphanumeric character found in the email’s subject or body/message is
replaced with a white space.
. We remove stop words (e.g., ‘the’, ‘is’) from both the subject and body of each
email to reduce noise.
3.1.4
Vectorization
To convert email information into numerical vectors, we use the term frequency-
inverse document frequency (TF-IDF). This well-known NLP technique multiplies
a word’s term frequency (TF) by its inverse document frequency (IDF). The TF refers
to how many times a word appears in a document, whereas the IDF refers to how
frequently a word appears across the entire corpus of documents. Table 3 presents
the total number of phishing and legitimate email instances we obtain in each of the
datasets after the completion of the preprocessing step.

124
Md. F. Rabbi et al.
Table 3 Datasets after preprocessing
Ling [30]
TREC [6]
Legitimate
2408
24673
Phishing
468
38152
Total
2876
62825
3.2
ML Algorithms for Classiﬁcation
For the detection of phishing emails, we separately attempt with six ML algo-
rithms [10] as brieﬂy introduced below.
Logistic Regression (LR):
LR uses a linear combination to combine the input fea-
tures before running them through a sigmoid function to create a probability.
A binary prediction can be made by thresholding this probability. To reduce the
binary cross-entropy loss, the model learns its parameters from training data using
gradient descent.
K-Nearest Neighbors (KNN):
KNN identiﬁes related samples in the training
dataset and applies a class label to the input sample. To determine the majority
class label, the algorithm calculates the distance to the K nearest neighbors from
the input data. KNN is simple to use and effective at managing high-dimensional
and noisy data.
AdaBoost (AB):
AdaBoostcombinesanumberofweakclassiﬁerstocreateastrong
one. Weak classiﬁers are learned using weighted examples after each example has
been given a weight during training. To create the ﬁnal strong classiﬁer, these weak
classiﬁers are then combined.
Multinomial Naive Bayes (MNB):
MNB uses the Bayes theorem to calculate the
likelihood that a new email might fall into each class depending on its attributes. It
determines key features, such as important words or phrases, and then calculates
the likelihood that an email falls into each class.
Gradient Boosting (GB):
GB integrates the predictions from various models to
enhance performance. Using a gradient descent technique to the residual errors,
the algorithm trains distinct models that correct the faults generated by the prior
model. The weighted average of all the predictions from each model is the ﬁnal
prediction.
Random Forest (RF):
RF combines the predictions of multiple decision trees to
produce a more accurate ﬁnal prediction. To avoid overﬁtting and enhance model
performance, each decision tree is trained using a random subset of the data. The
algorithm can ﬁnd the most crucial factors in the data by concentrating on the
most important characteristics of a phishing email and disregarding unimportant
ones.

Phishy? Detecting Phishing Emails Using …
125
Table 4 Confusion matrix used in our work
Actual
Prediction/classiﬁcation
Legitimate
Phishing
Legitimate
TN
FP
Phishing
FN
TP
Further details of all these well-known ML algorithms can be found else-
where [10]. The attributes in the datasets, as described in Sect. 2, are used as features
while operating the ML algorithms on the datasets.
3.3
Evaluation Metrics
For each ML algorithm operated on each dataset, we record the number of true
positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).
We then measure the performance of each algorithm in terms of accuracy, recall,
precision, and F-score, which are characterized according to the confusion matrix
shown in Table 4. We then measure the performance of each algorithm in terms of
accuracy, recall, precision, and F-score as deﬁned as follows.
• Accuracy is measured by the percentage of correctly categorized instances in the
dataset. Mathematically
accuracy equals StartFraction upper T upper P plus upper T upper N Over upper T upper P plus upper T upper N plus upper F upper P plus upper F upper N EndFractionaccuracy =
T P + T N
T P + T N + F P + F N
(1)
• Precision measures the number of actual positive cases among all those that are
predicted to be positive. Mathematically,
precision equals StartFraction upper T upper P Over upper T upper P plus upper F upper P EndFractionprecision =
T P
T P + F P
(2)
• Recall calculates the proportion of true positive cases among all positive examples.
Mathematically deﬁned as,
recall equals StartFraction upper T upper P Over upper T upper P plus upper F upper N EndFractionrecall =
T P
T P + F N
(3)
• F-score is the harmonic mean of precision and recall. Mathematically deﬁned as
follows:
upper F hyphen score equals 2 times StartFraction p times r Over p plus r EndFractionF-score = 2 × p × r
p + r
(4)

126
Md. F. Rabbi et al.
True positive rate (TPR), also known as sensitivity, indicates how accurately
phishing email detection systems identify true positive instances. Another essential
metric is the false positive rate (FPR), also known as speciﬁcity, which represents
the percentage of true negative cases that a detection system incorrectly interprets as
positive.
For a typical detection system, when TPR increases, FPR also goes high. But high
TPR and low FPR are desirable. Thus, by plotting TPR versus FPR at various clas-
siﬁcation levels, the receiver operating characteristic (ROC) curve is used to assess
the overall effectiveness of a phishing email detection system. These evaluation indi-
cators assist pinpoint problem areas and offer insights into how well a classiﬁcation
model is performing.
3.4
Applying the ML Algorithms
We operate ML algorithms on the two datasets, Ling and TREC, using only their com-
mon ‘subject’ and ‘message’ features, and excluding TREC’s additional ‘email_to’
and ‘email_from’ features.
From the Ling dataset, we ﬁrst randomly pick 80% of phishing emails and 80% of
legitimate emails and combine them to create our training subset of the Ling dataset.
The rest 20% phishing and legitimate emails are combined to create the testing subset
of the Ling dataset. Using the same procedure, we also create separate training and
testing subsets of the TREC dataset. The training subsets are used to train the ML
algorithms and the testing subsets are used to evaluate the algorithms’ performances.
4
Analysis and Findings
We now describe our analyses for addressing each of the research questions and
derive their answers.
4.1
Performance of ML Algorithms (RQ1)
Figures 1 and 2 present the number of TP, TN, FP, and FN in the ML algorithms’
detection of phishing emails in the Ling and TREC datasets respectively. In the
current context, it is important to minimize FN predictions, which happen when the
algorithm mistakenly classiﬁes a phishing email as a legitimate one. With respect to
the number of FNs, the RF algorithm appears to have outperformed others with zero
FN in the Ling dataset and only 45 in the TREC dataset.
The performance ML algorithms on the Ling dataset are presented in Table 5.
As seen in the table, the RF algorithm achieves the highest ROC score (0.9879)

Phishy? Detecting Phishing Emails Using …
127
481
18
1
76
469
3
13
91
479
5
3
89
482
71
0
23
480
10
2
84
482
12
0
82
0
100
200
300
400
500
600
T N
FP
FN
T P
LR
KNN
AB
MNB
GB
RF
Fig. 1 TN, FP, FN, and TP observed for the six ML algorithms operated on the Ling dataset
4863
139
72
7491
4882
342
53
7288
4770
237
165
7393
4820
154
115
7476
4731
176
204
7454
4890
154
45
7476
0
1000
2000
3000
4000
5000
6000
7000
8000
T N
F P
F N
T P
LR
KNN
AB
MNB
GB
RF
Fig. 2 TN, FP, FN, and TP observed for the six ML algorithms operated on the TREC dataset
Table 5 Performance of the six ML algorithms when operated on the Ling dataset
Algorithms
Accuracy (%)
Precision (%)
Recall (%)
ROC
F-score (%)
LR
96.70
97.26
96.70
0.9755
96.84
KNN
97.22
97.21
97.22
0.9343
97.17
AB
98.61
98.63
98.61
0.9785
98.62
MNB
87.67
96.98
87.67
0.9358
90.99
GB
97.92
98.06
97.92
0.9782
97.95
RF
97.92
98.18
97.92
0.9879
97.97

128
Md. F. Rabbi et al.
Table 6 Performance of the six ML algorithms when operated on the TREC dataset
Algorithms
Accuracy (%)
Precision (%)
Recall (%)
ROC
F-score (%)
LR
98.32
98.32
98.32
0.9813
98.32
KNN
96.86
96.93
96.86
0.9637
96.84
AB
96.80
96.80
96.80
0.9654
96.80
MNB
97.86
97.86
97.86
0.9769
97.86
GB
96.98
96.98
96.98
0.9687
96.98
RF
98.42
98.43
98.42
0.9817
98.41
Fig. 3 ROC Curve for the six ML algorithms when operated on the Ling Dataset
and second highest F-score (97.97%), which is slightly (0.65%) lower than that of
Adaboost. Adaboost is found to have achieved slightly higher accuracy, precision,
recall, and F-score compared to those of RF but the differences always remain less
than 0.7% only.
The classiﬁcation performance of the ML algorithms on the TREC dataset is
presented in Table 6. As seen in the table, the RF algorithm clearly outperforms the
other ﬁve algorithms achieving the highest scores in all the evaluation metrics. This
is possibly because RF is effective in identifying phishing emails in large datasets
such as TREC as it can improve the model’s generalization performance and reduce
overﬁtting by incorporating the predictions of multiple decision trees.
Figures3and4presenttheROCcurvesfortheMLalgorithmsoperatedontheLing
and TREC datasets respectively. The closer an ROC curve reaches to the upper-left
corner, the better the performance of the corresponding classiﬁer. On both datasets,
RF performs the best, with its ROC curve being the closest to the upper left corner.
Based on the observations discussed above, we now derive the answer to the
research question RQ1 as follows.

Phishy? Detecting Phishing Emails Using …
129
Fig. 4 ROC Curve for the six ML algorithms when operated on the TREC Dataset
Ans. to RQ1: In detecting phishing emails, the classiﬁcation performance of the
RF algorithm appears superior to the other ML algorithms in our study. This
superiority is more evident when the algorithms are operated on the larger dataset.
Especially the low number of FNs makes RF particularly suitable for phishing
email detection.
4.2
ML Algorithms’ Training Time (RQ2)
We calculate the training time of six ML algorithms while separately training the
datasets. Figures 5 and 6 respectively present the training time taken by the ML
algorithms when trained on the Ling and TREC datasets.
As seen in the ﬁgures, on both datasets, KNN and MNB algorithms have signiﬁ-
cantly shorter training times compared to the other ML algorithms. This is because
these two algorithms are computationally simpler than the others.
The MNB algorithm uses probability theory to determine the likelihood that a new
data point is a member of a certain class based on the feature values, whereas the
KNN algorithm compares the new data point to all existing data points and classiﬁes
it based on the nearest neighbors. Both of these algorithms require less computation
compared to others such as GB or RF. Both GB and RF utilize complex decision trees,
and ensemble methods, and thus take more time to execute, which is also reﬂected
in the ﬁgures. Based on the observations, the answer to the research question RQ2
is derived as follows.

130
Md. F. Rabbi et al.
1.64
0.74
4.55
0.75
18.23
3.45
0
2
4
6
8
10
12
14
16
18
20
LR
KNN
AB
MNB
GB
RF
Fig. 5 Time (in seconds) consumed in training each of the six ML algorithms on the Ling dataset
107.04
44.34
269.75
38.72
1087.6
1052.06
0
200
400
600
800
1000
1200
LR
KNN
AB
MNB
GB
RF
Fig. 6 Time (in seconds) consumed in training each of the six ML algorithms on the TREC dataset

Phishy? Detecting Phishing Emails Using …
131
480
17
0
75
474
2
6
90
475
5
5
87
480
81
0
11
478
10
2
82
480
15
0
77
0
100
200
300
400
500
600
T N
F P
F N
T P
LR
KNN
AB
MNB
GB
RF
Fig. 7 TN, FP, FN, and TP for the ML algorithms on Ling dataset excluding the ‘subject’ feature
Ans. to RQ2: KNN and MNB algorithms have much shorter training times com-
pared to the other ML algorithms studied.
4.3
Impact of the ‘Subject’ Feature (RQ3)
To investigate the importance of the ‘subject’ feature in accurately detecting phishing
emails, we operate the ML algorithms on the dataset according to the procedure
described in Sect. 3.4, but at this phase, we make the ML algorithms disregard the
‘subject’ feature in the datasets.
In Fig. 7, we present the number of TN, FP, FN, and TP found in the ML algo-
rithms’ detection of phishing emails in the Ling dataset excluding the ‘subject’ fea-
ture. If we compare the results in this ﬁgure with those of Fig. 1, we notice that,
when the ‘subject’ feature is excluded, there is a slight decrease in TP for all the
algorithms. Figure 8 presents the number of TN, FP, FN, and TP found in the ML
algorithms’ detection of phishing emails in the TREC dataset excluding the ‘subject’
feature. Now, if we compare the results in this ﬁgure with those in Fig. 2, we again
see that all the ML algorithms’ TP also decreases for the TREC dataset when the
‘subject’ feature is disregarded.
Table 7 presents the accuracy, precision, recall, and F-score of the ML algorithms’
detection of phishing emails in the Ling dataset while disregarding the ‘subject’
feature. A comparison of the results in this table with those in Table 5 reveals the
following.Inthisrelativelysmallerdataset,whenthe‘subject’featureisexcluded,LR
and KNN perform better, while AB, MNB, GB, and RF perform worse. Because the
Ling dataset is relatively smaller, exclusion of the ‘subject’ feature might not have a

132
Md. F. Rabbi et al.
4834
135
64
6777
3029
120
1869
6792
4723
263
175
6649
4811
183
87
6729
4701
183
197
6729
4858
151
40
6761
0
1000
2000
3000
4000
5000
6000
7000
8000
T N
F P
F N
T P
LR
KNN
AB
MNB
GB
RF
Fig. 8 TN, FP, FN, and TP for the ML algorithms on TREC dataset excluding the ‘subject’ feature
Table 7 Performance of the six ML algorithms on the Ling dataset, excluding the ‘subject’ feature
Algorithms
Accuracy (%)
Precision (%)
Recall (%)
ROC
F-score (%)
LR
97.03
97.58
97.03
0.9829
97.15
KNN
98.60
98.59
98.60
0.9666
98.59
AB
98.25
98.25
98.25
0.9676
98.25
MNB
85.84
98.31
85.84
0.9278
90.86
GB
97.90
98.05
97.90
0.9779
97.94
RF
97.38
97.81
97.38
0.9848
97.47
substantial impact on the algorithms’ performances. Alternatively, the ‘subject’ may
not really have a distinguishable impact in the determination of whether an email is
phishing or legitimate in general, irrespective of the algorithms in use. We further
examine these possibilities by looking at the performances of the ML algorithms on
the larger TREC dataset.
In Table 8, we present the performance of the ML algorithms’ phishing email
detection in the TREC dataset without taking into account the ‘subject’ feature. If
we compare the results of this table with those in Table 6, we notice that accuracy,
precision, recall, and F-score decreases for all the algorithms when the ‘subject’
feature is excluded. This larger TREC dataset includes a varied collection of emails,
making it more challenging for the ML algorithms to correctly distinguish phish-
ing emails based only on message bodies without taking the ‘subject’ feature into
account. Based on the observations and discussion above, we, therefore, derive the
answer to the research question RQ3 as follows.

Phishy? Detecting Phishing Emails Using …
133
Table 8 Performance of the six ML algorithms on TREC dataset, excluding the ‘subject’ feature
Algorithms
Accuracy (%)
Precision (%)
Recall (%)
ROC
F-score (%)
LR
98.31
98.32
98.31
0.9817
98.31
KNN
83.16
88.55
83.16
0.8730
84.04
AB
96.29
96.29
96.29
0.9608
96.29
MNB
97.71
97.72
97.71
0.9753
97.71
GB
96.78
96.78
96.78
0.9670
96.78
RF
98.38
98.40
98.38
0.9820
98.38
Ans. to RQ3: Inclusion of the ‘subject’ feature increases accuracy of the ML
algorithms in detection of phishing emails. However, this impact of the ‘subject’
feature is negligible when ML algorithms are applied on small datasets, but signif-
icant when the algorithms operate on larger datasets having a diverse collection
of phishing and legitimate emails.
5
Threats to Validity
Our phishing email detection model is developed using only two datasets (i.e., TREC
andLingdatasets).TheLingdatasetwasreleasedin2003whiletheTRECdatasetwas
made available in 2007. These datasets, being old, might not have captured the latest
tricks and strategies adopted in modern phishing attacks. Thus the generalizability
of our ﬁndings can be argued limited in scope.
We solely focus on the ‘subject’ and ‘message’ (i.e., email body) features of
the dataset in this work. However, the sender of an email, along with other header
details, email attachments, and URLs in the email body can offer useful information
and boost a classiﬁcation model’s performance. We plan to include all these in our
future work.
We examine the effectiveness of six ML algorithms in detecting phishing emails.
There are a number of other ML algorithms, which are popular but not included in
our work. We plan to include them in future extension to this work.
The methodology of this study including the procedures for data collection and
analysis are documented in this paper. The ML algorithms are well-known while the
datasets used in this study are freely available to the public. Therefore, it should be
possible to replicate this work of ours.

134
Md. F. Rabbi et al.
6
Related Work
ML and NLP have been used in many software engineering studies [12, 13, 16,
17, 36] while many other studies explored different security aspects of software
systems [4, 15, 18, 19, 25, 28, 29]. Sattar et al. [31] applied predictive model
analysis for detecting web-spams in web-graphs. Several recent attempts to detect
phishing emails have tried with machine learning [1, 14, 34] and deep learning [8,
11, 14, 21, 27] techniques. These studies have used various strategies to improve
the performance of detection algorithms. Some studies used a single algorithm [1,
8, 27], while others have compared several algorithms to identify the most effective
approaches [14, 34]. Researchers have also tried using single [1, 34] and multiple
datasets [8, 14, 27], as well as combination of multiple datasets [14].
Islam et al. [14] compared four ML algorithms on TREC and Ling datasets, using
seven features of TREC and two features of Ling. Similar to our work, they also
used the Ling and TREC datasets, and RF was reported to have the highest accuracy
compared to the four ML algorithms included in their work. However, in their work,
the accuracy of RF did not exceed 95% on the TREC dataset (and 96% on the Ling
dataset) and while in our study RF achieves more than 98% accuracy on the TREC
dataset (and over 97% on the Ling dataset).
Agarwal and Kumar [1] combined Naive Bayes (NB) with Particle Swarm Opti-
mization and evaluated its performance on the Ling dataset. They found that while
using NB alone resulted in an accuracy below 90%, the integrated approach achieved
an accuracy above 90%. We have also used the Ling dataset and achieved almost
98% accuracy on this dataset while the Multinomial Naive Bayes (MNB) algorithm
in our work has been able to achieve more than 97% accuracy on the TREC dataset.
Pan et al. [27] used a Semantic Graph Neural Network on the TREC dataset but
did not compare their method with any other algorithms. Dhavale [8] employed a
convolutional neural network-based approach on the TREC dataset without compar-
ing their approach with any other algorithms. Similar to ours, Dhavale’s method also
achieved approximately 98% accuracy. However, unlike the work of Dhavale [8] or
Pan et al. [27], we compared the performances of six ML algorithms.
Unlike any of the work discussed above, in our study, we examined the impact
of a particular email feature on the six ML algorithms’ detection of phishing emails.
This also makes our work unique from the earlier work in the literature.
7
Conclusion and Future Work
In this paper, we have presented a performance comparison of six different machine
learning (ML) algorithms in distinguishing phishing emails from legitimate ones.
The algorithms are trained and evaluated on two publicly available datasets (i.e.,
Ling and TREC datasets). Classiﬁcation performances are measured in terms of
accuracy, precision, recall, F-score, and ROC. The training time required for the ML

Phishy? Detecting Phishing Emails Using …
135
algorithms is also compared for measuring their applicability in a practical context.
We have also examined the importance of a particular feature in the identiﬁcation of
phishing emails.
Our study reveals that the Random Forest (RF) algorithm performs better than
other ML algorithms, particularly for larger datasets. Due to its low false negatives,
RF can be particularly suitable for pragmatic application in phishing email detection,
if training time is not a limiting factor. Classiﬁcation performance of the ML algo-
rithms generally decreases when the ‘subject’ feature is disregarded. The impact of
this particular feature is found more substantial when the ML algorithms are oper-
ated on larger datasets. KNN (K-nearest Neighbours) and MNB (Multinomial Naive
Bayes) algorithms tool much less time in training compared to other ML algorithms
in our study. In our work, Gradient Boosting (GB) has taken the longest time in
training, but this algorithm has not produced the best results in terms of classiﬁca-
tion performances. This suggests that simply choosing the algorithm with the longest
training time may not necessarily lead to better results. It is important to choose an
ML algorithm based on the size and nature of the datasets as well as classiﬁcation
performance requirements.
Ourfutureresearchwilladdressthelimitationsofthisworkandwillinvestigatethe
use of additional features to improve the performance of ML algorithms in phishing
email detection. There are other benchmark datasets for phishing emails, such as
the Enron [20], Nazario [26], and SpamAssassin [32] corpora. These datasets are
available in different formats, such as mdir and mbox. They can be combined into
a single dataset in a single format, such as CSV, to create a bigger, more diversiﬁed
dataset. This remains within our plans for future work. The planned extension of this
work will also include additional ML algorithms. We hypothesize that applying deep
learning approaches, such as convolutional neural networks (CNN), could improve
the results by automatically recognizing complex patterns and features that might
be difﬁcult to detect with conventional ML approaches. We also plan to extend this
work in this direction in the future.
Acknowledgements This work is supported in part by a grant from the Center for Advanced
Energy Studies (CAES) in Idaho, USA.
References
1. Agarwal K, Kumar T (2018) Email spam detection using integrated approach of naïve bayes and
particle swarm optimization. In: 2018 second international conference on intelligent computing
and control systems (ICICCS). IEEE, pp 685–690
2. APWG:
Phishing
Activity
Trends
Reports.
https://www.f5.com/labs/articles/threat-
intelligence/2020-phishing-and-fraud-report. (Veriﬁed: March 2023)
3. Bisong E (2019) Building machine learning and deep learning models on Google cloud plat-
form. Springer

136
Md. F. Rabbi et al.
4. Champa A, Rabbi M, Eishita F, Zibran M (2023) Are we aware? An empirical study on the
privacy and security awareness of smartphone sensors. In: 21st IEEE international conference
on software engineering, management and applications (SERA), p (to appear)
5. Chatterjee A (2023) Preprocessed TREC 2007 Public Corpus Dataset. https://www.kaggle.
com/datasets/imdeepmind/preprocessed-trec-2007-public-corpus-dataset. (Veriﬁed: March
2023)
6. Cormack GV (2007) Trec 2007 spam track overview. In: Proceedings of the 16th text retrieval
conference (TREC), vol 500, p 274
7. Das A, Baki S, El Aassal A, Verma R, Dunbar A (2019) Sok: a comprehensive reexamination
of phishing research from the security perspective. IEEE Commun Surv Tutor 22(1):671–708
8. Dhavale S (2020) C-asft: convolutional neural networks-based anti-spam ﬁltering technique.
In: Proceeding of international conference on computational science and applications: ICCSA
2019, pp 49–55
9. Dou Z, Khalil I, Khreishah A, Al-Fuqaha A, Guizani M (2017) Systematization of knowledge
(sok): a systematic review of software-based web phishing detection. IEEE Commun Surv
Tutor 19(4):2797–2819
10. Géron A (2022) Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow.
O’Reilly Media, Inc
11. Halgaš L, Agraﬁotis I, Nurse JR (2020) Catching the phish: detecting phishing attacks using
recurrent neural networks (rnns). In: 20th international conference on information security
applications, pp 219–233
12. Islam M, Zibran M (2018) Sentistrength-SE: exploiting domain speciﬁcity for improved sen-
timent analysis in software engineering text. J Syst Softw 145:125–146
13. Islam M, Ahmmed M, Zibran M (2019) Marvalous: machine learning based detection of
emotions in the valence-arousal space in software engineering text. In: 34th ACM/SIGAPP
symposium on applied computing (SAC), pp 1786–1793
14. Islam M, Al Amin M, Islam M, Mahbub M, Showrov M, Kaushal C (2021) Spam-detection
with comparative analysis and spamming words extractions. In: 9th international conference
on reliability, infocom technologies and optimization, pp 1–9
15. Islam M, Zibran M (2016) A comparative study on vulnerabilities in categories of clones and
non-cloned code. In: 10th IEEE international workshop on software clones, pp 8–14
16. Islam M, Zibran M (2017) Leveraging automated sentiment analysis in software engineering.
In: 14th IEEE international conference on mining software repository (MSR), pp 203–214
17. Islam M, Zibran M (2018) Deva: sensing emotions in the valence arousal space in software
engineering text. In: 33rd ACM/SIGAPP symposium on applied computing (SAC), pp 1536–
1543
18. Islam M, Zibran M, Nagpal A (2017) Security vulnerabilities in categories of clones and non-
cloned code: an empirical study. In: 11th ACM/IEEE international symposium on empirical
software engineering and measurement, pp 20–29
19. Joseph R, Zibran M, Eishita F (2021) Choosing the weapon: a comparative study of secu-
rity analyzers for android applications. In: International conference on software engineering,
management and applications, pp 51–57
20. Klimt B, Yang Y (2004) The enron corpus: a new dataset for email classiﬁcation research. In:
Machine learning: ECML 2004: 15th European conference on machine learning, Pisa, Italy,
20–24 Sept 2004. Proceedings, vol 15. Springer, pp 217–226
21. Magdy S, Abouelseoud Y, Mikhail M (2022) Efﬁcient spam and phishing emails ﬁltering based
on deep learning. Comput Netw 206:108,826
22. MimeCast: How to Stop Phishing Attacks (Whitepaper). https://www.mimecast.com/
resources/white-papers/how-to-stop-phishing-attacks/. (Veriﬁed: March 2023)
23. MimeCast: The State of Email Security 2023 (E-book). https://www.mimecast.com/state-of-
email-security/. (Veriﬁed: March 2023)
24. Mukherjee A, Agarwal N, Gupta S (2019) A survey on automatic phishing email detection
using natural language processing techniques. Int Res J Eng Technol 6(11):1881–1886

Phishy? Detecting Phishing Emails Using …
137
25. Murphy D, Zibran M, Eishita F (2021) Plugins to detect vulnerable plugins: an empirical assess-
ment of the security scanner plugins for wordpress. In: International conference on software
engineering, management and applications, pp 39–44
26. Nazario J (2023) The online phishing corpus. http://monkey.org/jose/wiki/doku.php. (Veriﬁed:
March 2023)
27. Pan W, Li J, Gao L, Yue L, Yang Y, Deng L, Deng C (2022) Semantic graph neural network:
a conversion from spam email classiﬁcation to graph classiﬁcation. Sci Program 2022:1–8
28. Rajbhandari A, Zibran M, Eishita F (2022) Security versus performance bugs: How bugs
are handled in the chromium project. In: International conference on software engineering,
management and applications, pp 70–76
29. Rodriguez J, Zibran M, Eishita F (2022) Finding the middle ground: measuring passwords for
security and memorability. In: 20th IEEE international conference on software engineering,
management and applications, pp 77–82
30. Sakkis G, Androutsopoulos I, Paliouras G, Karkaletsis V, Spyropoulos CD, Stamatopoulos P
(2003) A memory-based approach to anti-spam ﬁltering for mailing lists. Inf Retr 6:49–73
31. Sattar N, Arifuzzaman S, Zibran M, Sakib M (2019) Detecting web spam in webgraphs with
predictive model analysis. In: 3rd international workshop on big data analytic for cyber crime
investigation and prevention, pp 4299–4308
32. Schwartz A (2023) Apache SpamAssassin. https://spamassassin.apache.org/. (Veriﬁed: March
2023)
33. Uchill J (2016) Typo led to podesta email hack: Report. The Hill 13
34. Unnithan NA, Harikrishnan N, Vinayakumar R, Soman K, Sundarakrishna S (2018) Detecting
phishing e-mail using machine learning techniques. In: Proceedings of 1st anti-phishing shared
task pilot, 4th acm iwspa co-located, 8th acm conference on data and application security and
privacy (codaspy), pp 51–54
35. Warburton D (2023) 2020 Phishing and Fraud Report. https://www.f5.com/labs/articles/threat-
intelligence/2020-phishing-and-fraud-report. (Veriﬁed: March 2023)
36. Zibran M (2016) On the effectiveness of labeled latent dirichlet allocation in automatic bug-
report categorization. In: 38th ACM/IEEE international conference on software engineering
(ICSE), pp 713–715

Are We Aware? An Empirical Study 
on the Privacy and Security Awareness 
of Smartphone Sensors 
Arifa I. Champa, Md. Fazle Rabbi, Farjana Z. Eishita, and Minhaz F. Zibran 
Abstract Smartphones are equipped with a wide variety of sensors, which can 
pose signiﬁcant security and privacy risks if not properly protected. To assess the 
privacy and security risks of smartphone sensors, we ﬁrst systematically reviewed 
55 research papers. Driven by the ﬁndings of the systematic review, we carried out 
a follow-up questionnaire-based survey on 23 human end-users. The results reﬂect 
that the participants have a varying level of familiarity with smartphone sensors, and 
there is a noticeable dearth of awareness about the potential threats and preventive 
measures associated with these sensors. The ﬁndings from this study will inform 
the development of effective solutions for addressing security and privacy in mobile 
devices and beyond. 
Keywords Smartphone · Sensor · Attacks · Security · Privacy · Awareness ·
Perception · Survey · Quantitative · Analysis · Study · Systematic literature review 
1 
Introduction 
Smartphones are an indispensable part of our lives, with small and discreet sensors 
that play the crucial role of functioning and user experience. Many different types 
of sensors are found in smartphones, each with its unique function [ 49]. In addition 
to providing a better user experience, smartphone sensors also have the potential 
to improve safety and security [ 5]. For instance, setting a passcode, the inclusion 
A. I. Champa · Md. F. Rabbi (B) · F. Z. Eishita · M. F. Zibran 
Idaho State University, Pocatello, ID, USA 
e-mail: mdfazlerabbi@isu.edu 
A. I. Champa 
e-mail: arifaislamchampa@isu.edu 
F. Z. Eishita 
e-mail: farjanaeishita@isu.edu 
M. F. Zibran 
e-mail: minhazzibran@isu.edu 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_10 
139

140
A. I. Champa et al.
of ﬁngerprint scanners, and facial recognition technology on smartphones can help 
protect against unauthorized access to sensitive information. 
In recent years, a variety of studies have been carried out to look at the threats 
and defense mechanisms of various systems [ 14, 59]. These investigations, however, 
have frequently concentrated on well-known system vulnerabilities or network-based 
risks that result from bad architectural design. The thorough explanation of sensor-
based risks has thus far mostly been disregarded. This instigates a high level of risk 
in terms of security and privacy since these systems can be seriously endangered by 
sensor-based threats. 
The usage of smartphone sensors raises signiﬁcant concerns about security and pri-
vacy. For example, an attacker could use a smartphone’s GPS sensor to track a user’s 
location without their knowledge [ 33]. Similarly, a phone’s camera or microphone 
can be hacked to record an audio or video clip without the user’s knowledge [ 56]. 
Motion sensors on mobile devices could be exploited to secretly infer the PINs or 
passwords inputted by users on mobile web applications [ 53]. 
While some users may be aware of smartphone sensors’ potential security and 
privacy risks, others may possess less or no awareness of the risks associated with the 
sensors. Lack of awareness could lead to users unknowingly sharing their sensitive 
information, resulting in security breaches or loss of privacy. A clear understanding 
of the present scenario is required to understand this level of risks and awareness. 
With this level of understanding, individuals and organizations can initiate informed 
decisions for protection against cyber attacks and data leaks. 
Therefore, in this work, we ﬁrst conduct a systematic review of existing research 
in the literature on smartphone sensors’ security and privacy issues. Later, an end-
user survey is conducted to assess user awareness and perception of the privacy 
and security risks associated with smartphone sensors. In particular, we address the 
following research questions: 
RQ1:
To what extent are individuals familiar with sensors in smartphones? 
RQ2:
How well are people aware of the existing mobile phone sensor attacks? 
RQ3:
How do people perceive the use of these sensors? 
Several works in the past have explored smartphone sensor-based threats includ-
ing end-user awareness and perception of the threats [ 10, 16, 19, 26, 37– 39, 52, 
57]. These earlier studies were conducted either as literature reviews or end-user 
surveys only. Ours is the ﬁrst work along this direction taking on wholistic approach 
combining both a systematic literature review and follow-up end-user survey. 
We organize our paper as follows. First, we describe our systematic literature 
review in Sect. 2. Then, in Sect. 3, we describe our end-user survey, which is designed 
based on the ﬁndings from the systematic literature review. In Sect. 4, we further 
discuss the results from both the literature review and end-user survey including the 
threats to validity of the results as well as our plan for future work. Finally, Sect. 5 
concludes the paper.

Are We Aware? An Empirical Study on the Privacy …
141
Table 1 Keywords used for searching relevant papers 
Database
Searched keywords 
ACM Digital Library
[[‘Smartphone’ OR ‘Mobile’] AND [‘sensor’]] AND [‘security’ OR 
‘privacy’ OR ‘awareness’] 
ScienceDirect
[[‘Smartphone’ OR ‘Mobile’] AND [‘sensor’ OR ‘sensors’]] AND 
[‘security’ OR ‘privacy’ OR ‘awareness’] 
IEEE Xplore
[[‘Smartphone’ OR ‘Mobile’] AND [‘sensor’ OR ‘sensors’]] AND 
[‘security’ OR ‘privacy’ OR ‘awareness’] NOT [‘IoT’ OR ‘Wearable’] 
Springer
[‘Smartphone’ OR ‘Mobile’] AND [‘sensor’ AND ‘security’ AND 
‘privacy’ AND ‘awareness’] 
Taylor & Francis
[[‘Smartphone’ OR ‘Mobile’] AND [‘sensor’]] AND [‘security’ OR 
‘Sensor privacy’] NOT [‘IoT’ OR ‘Wearable’] 
PubMed
[[‘Smartphone’ OR ‘Mobile’] AND [‘sensors’]] AND [‘security’ OR 
‘privacy’] NOT [‘IoT’ OR ‘wearable’] 
MDPI
[‘Smartphone sensor’ OR ‘Mobile sensor’ AND [‘security’ OR‘privacy’ 
OR ‘awareness’]] NOT [‘IoT’ OR ‘wearable’] 
Table 2 Inclusion and exclusion criteria 
Inclusion criteria
Exclusion criteria 
bullet. Relevant to smartphone sensors and security 
or privacy or awareness 
bullet. Irrelevant to smartphone sensors security or 
privacy or awareness 
bullet. Research article
bullet. Book, chapter, reviewed article 
bullet. Published between 2010 and 2022
bullet. Not peer-reviewed papers 
bullet. Written in English language
bullet. Papers not accessible online 
bullet. Articles related to wearable smart devices 
and IoT 
2 
Systematic Literature Review 
2.1 
Methodology 
Our systematic review was carried out using the following four phases: 
Phase-1 (Search of Research Articles): The
seven
databases
mentioned
in 
Table 1 were chosen for their well-known sources of scholarly research in a 
wide range of ﬁelds to identify relevant research articles. The search for arti-
cles to be included in this systematic review began in November 2022 using a 
keyword-based substring search method. These searched keywords are listed in 
Table 1. 
Phase-2 (Preliminary Filtering): Table 2 outlines the speciﬁc criteria that have been 
applied to determine the articles that are included in the review and those that are 
excluded.

142
A. I. Champa et al.
Identification 
Eligibility 
Screening 
Included 
Records identified from 7 databases (n=1754) 
Records screened (n=1361)
393 duplicate articles removed 
Records excluded (n=1167) due to non-compliance 
with inclusion & exclusion criteria 
Full text articles accessed 
(n= 194) 
Articles included in this systematic review (n=55) 
Full text articles excluded (n=139) 
Fig. 1 States of our work at different stages of PRISMA-P 
Phase-3 (Final Filtering): The PRISMA-P (Preferred Reporting Items for System-
atic Reviews and Meta-Analyses Protocols) [ 50] guidelines were followed (as 
shown in Fig. 1) to identify relevant papers, ensuring a systematic and thorough 
review. 
Phase-4 (Analysis): The relevant information identiﬁed from the reviewed research 
articles is then further analyzed around the research questions outlined before. 
This analysis leading to ﬁndings is elaborated in the following section (Sect. 2.2). 
2.2 
Analysis and Findings 
The searched keywords in Table 1 were used to identify relevant studies published 
from 2010 to 2022 for the systematic review. The search result yielded 1754 articles 
among which 393 were duplicate articles. After removing the duplicates, 1361 arti-
cles were left for the screening phase. 139 papers were excluded from 194 eligible 
articles for full-text evaluation. Finally, 55 articles were included in the study. The 
states of the work at different stages of PRISMA-P [ 50] are summarized in Fig. 1 
and the numbers of articles selected from different sources are listed in Table 3. 
We thoroughly read all 55 papers and identiﬁed various kinds of attacks that can be 
carried out using smartphone sensors. 
2.2.1
Sensor-Related Security Threats 
We found eight such threats that are most commonly discussed in the literature. These 
eight common threats are brieﬂy described below.

Are We Aware? An Empirical Study on the Privacy …
143
Table 3 Number of papers selected from different sources 
Database
Identiﬁcation
Eligibility
Included 
ScienceDirect
327
25
9 
IEEE Xplore
477
79
18 
Springer
308
24
10 
Taylor & Francis
34
7
1 
PubMed
399
12
3 
MDPI
26
6
2 
ACM Digital Library
183
41
12 
Total
1754
194
55 
Keystroke Inference (KIn): An 
attacker 
can 
potentially 
determine 
the 
exact 
keystrokes entered, including sensitive information such as passwords or credit 
card numbers, by analyzing the subtle vibrations and movements of the device as 
the user types. 
Location Inference (LIn): A LIn attack using smartphone sensors is when data is 
collected and the physical position of a smartphone user is determined without 
the user’s knowledge or consent. 
Device Fingerprinting (DFP): A DFP attack based on smartphone sensor data 
encompasses creating a unique device proﬁle or ‘ﬁngerprint’ based on the sen-
sor data, which can then be used to track the device and its user across various 
applications and services. 
Task Inference (TIn): A TIn attack is to infer the user’s current activity or task, such 
as browsing the internet or sending a message without the user’s knowledge or 
consent. 
Eavesdropping (Evd): An Evd attack refers to the unauthorized interception and 
recording of audio using the smartphone’s microphone, without the user’s aware-
ness or consent. 
Transmitting Malicious Sensor Commands (TMC): TMC involves the unauthorized 
manipulation of sensor data by sending malicious commands to the device’s sen-
sors. 
Pin Inference (PinIn): A PinIn involves the unauthorized inference or extraction of 
the user’s PIN or password by analyzing the sensor data. 
Physical and Behavioral Activity Recognition (PhBAR): An attacker can potentially 
deduce the user’s current activity or behavior by analyzing the patterns and timing 
of the user’s interactions with the device’s sensors. 
Table 4 identiﬁes the smartphone sensors associated with these threats and the 
articles in the literature that at least mentioned them. We also identiﬁed that 15 
sensors are particularly reported susceptible to threats. Based on the type of smart-
phone’s operations these sensors are used for, they are categorized into four groups 
and presented in Table 5. The literature suggests that environmental sensors, which

144
A. I. Champa et al.
Table 4 Various threats and issues related to smartphone sensors 
Threats
#
Research articles
Smartphone sensors 
LIn
11
[ 4, 13, 15, 40, 60– 62, 65, 68, 69, 72]
Barometer, Gyroscope, Accelerometer, Speaker, 
Camera, Magnetometer, Microphone, GPS, 
Compass, WiFi, NFC 
TIn
10
[ 1, 18, 32, 34, 42, 53, 56, 58, 67, 70]
Gyroscope, Accelerometer, Speaker, Ambient 
Light Sensor, Magnetometer, Microphone, 
Biometric Sensors, Camera, GPS, WiFi, 
Bluetooth, NFC 
KIn
6
[ 2, 6, 20, 22, 54, 64]
Gyroscope, Accelerometer, Speaker, Ambient 
Light Sensor, Magnetometer, Microphone, 
Biometric Sensors, Camera, Proximity Sensor, 
WiFi, Bluetooth, NFC 
Evd
8
[ 3, 7, 11, 25, 29, 31, 41, 67]
Gyroscope, Accelerometer, Speaker, Ambient 
Light Sensor, Magnetometer, Microphone, 
Bluetooth 
TMC
2
[ 29, 67]
Ambient Light Sensor, Microphone, WiFi, 
Bluetooth 
DFP
7
[ 11, 24, 28, 30, 35, 64, 66]
Gyroscope, Accelerometer, Speaker, 
Microphone, Camera, Biometric Sensors, WiFi, 
Bluetooth, NFC 
PinIn
4
[ 36, 37, 41, 55]
Biometric Sensor, Gyroscope, Accelerometer, 
Magnetometer, Barometer, Proximity Sensor, 
Ambient Light Sensor 
PhBAR
13
[ 12, 13, 27, 40, 47, 48, 56, 60– 62, 68, 70, 72]
GPS, Camera, Microphone, Speaker, Biometric 
Sensor, Gyroscope, Accelerometer, 
Magnetometer, Barometer, Compass, WiFi 
Table 5 15 smartphone sensors categorized in groups 
Sensor type
Sensors 
Identity-related
GPS, Microphone, Speaker, Camera, Biometric 
Communicational
WiFi, Bluetooth, Near-ﬁeld communication 
(NFC) 
Motion
Gyroscope, Accelerometer, Proximity, 
Magnetometer 
Environmental
Ambient Light Sensor, Barometer, Compass 
measure factors such as temperature and humidity, are generally less known to users 
compared to other types of sensors [ 38, 39]. 
2.2.2
Protection Mechanisms 
We also identiﬁed various mechanisms that were discussed in the literature for pro-
tection against attacks on smartphone sensors. In Table 6, we brieﬂy present the 
synopsis, performance, and overhead of the security and privacy preserving mecha-
nisms identiﬁed.

Are We Aware? An Empirical Study on the Privacy …
145
Table 6 Security and privacy preserving mechanisms identiﬁed 
Mechanisms
Synopsis
Performance 
SemaDroid [ 63]
bullet. Android sensor management system
100% accurate against sensor-based malware 
bullet. Uses simulated data to evaluate the potential 
risks of apps 
bullet. Offers users the ability to customize sensor 
policies to their liking 
AWare [ 43]
bullet. An authorization framework
Successful compatibility and usability test with 
1000 most downloaded Android apps
bullet. Allows users to authorize sensitive sensor 
operations 
bullet. Binds application operation requests to the 
corresponding user input events 
EnTrust [ 45]
bullet. Android Sensor authorization framework
Low overhead in Android smartphones 
bullet. Generates authorization queries in response to 
input occurrences from complying programs 
and delegation graphs 
6thSense [ 51]
bullet. An intrusion detection system
Achieved 96% accuracy against many 
sensor-based threats with minimal overhead
bullet. Employs sensor data to comprehend the 
context of the user’s activity 
bullet. Identiﬁes malicious activity on the device 
LocPPM [ 44]
bullet. Employs Synthetic data to mimic real data
Decreases the likelihood of a white-box attack 
by 3%
bullet. Uses targeted movements to combine real and 
synthetic sensor data 
AuDroid [ 46]
bullet. A trust evaluation framework
High accuracy tested with 17 mobile 
applications on an Android smartphone
bullet. Scrutinizes app demands for sensor access and 
decides whether the access is legit 
bullet. Detects instances of over-privilege and defend 
sensors from unauthorized access 
SensorSafe [ 8, 9] bullet. Based on trusted remote data stores and a 
broker who arbitrates access to the data 
stores of the users 
Prevents unauthorized access to sensed data of 
workers’ identity and position 
Perceptual 
Assistant [ 71] 
bullet. A privacy protection system
Less than 7.6% overhead and high adaptability 
bullet. Allows modiﬁcation of personalized sensor 
policy for all third-party sensing apps 
Android
Exten-
sion [ 17, 21] 
bullet. Manages information and stops malicious 
applications 
Effectively enforces privacy over sensed and 
contextual data without scalability issues 
bullet. Uses semantically rich context models 
(Xposed framework) 
3 
End User Survey 
Now, we want to understand three aspects: (a) to what extent the smartphone end-
users are familiar with the 15 smartphone sensors (classiﬁed in four categories) 
that are identiﬁed (from literature survey) as susceptible to attacks or data leaks.

146
A. I. Champa et al.
(b) To what extent the end-users are aware of the smartphone sensor-related security 
threats and the identiﬁed mechanisms identiﬁed from the literature survey. (c) How 
the end-users perceive the use of the 15 smartphone sensors. Aspects (a), (b), and 
(c) are respectively addressed in research questions RQ1, RQ2, and RQ3 outlined 
in Sect. 1. We, therefore, carried out a questionnaire based survey on smartphone 
end-users as described below. 
3.1 
Survey Procedure 
3.1.1
Questionnaire 
The questionnaire we used for the survey is brieﬂy presented in Table 7. The  Likert-
scale questions (i.e., 12, 14, 15, and 17) about familiarity, the participants had the 
followings ﬁve options to choose from: extremely, moderately, somewhat, slightly, 
and not at all. Along with the questionnaire, a set of three appendices were also 
provided to the participants. The appendices included brief description of the 15 
sensors, the sensor-based attacks, and the security mechanisms against the sensor-
based attacks. 
Table 7 Survey questionnaire 
1.
Age? (15–19, 20–24, 25–29, 30–34, 35–39, 40 years or more) 
2.
Gender? (Male, Female, Other) 
3.
What is the highest educational level you have attained? (High School, College, 
Bachelors, Masters, Doctoral) 
4.
What is your profession? 
5.
Ethnicity? (Caucasian, African American, Asian, Hispanic, Other) 
6.
Time spent on the internet per day in hours? (greater than>2, 2–5, 6–10,less than<10) 
7.
What do you use the internet for? Check all that apply. (Social Media, Research, 
Education, Entertainment, Financial Purpose, Others) 
8.
How many hours per day do you spend browsing the internet? 
9.
Average number of hours spent using smartphone apps per day? 
10.
How long have you been using a smartphone (in years)? 
11.
Operating system of your smartphone (Android, iOS, or Windows)? 
12.
What is your level of concern (in Likert scale) about unauthorized access to data? 
13.
Have you personally experienced privacy or security issues while using a smartphone? 
Check all that apply (options in Table 10). 
14.
Familiarity with 15 smartphone sensors (Table 5) in Likert scale? 
15.
Awareness about the security threats (Table 4) in Likert scale? 
16.
Perception of 15 smartphone sensors (Table 5) with respect to the security threats 
(Table 4), (i.e., sensortimes× threats)? 
17.
Familiarity with the security mechanism (Table 6) in Likert scale?

Are We Aware? An Empirical Study on the Privacy …
147
3.1.2
Participant Recruitment 
First, we recruited 15 student participants from a computer science class at the Idaho 
State University. Then additional eight participants were recruited for the study from 
the entire institution. Out of the total 23 participants, 14 completed the survey in 
person, while the remaining nine participants completed it online via Google Forms. 
More than half of the participants are Asians, and none are African Americans. Out of 
the 23 participants recruited, seven are females and 16 are males with ages between 
20 and 40 years, with the majority (13) falling in the age range of 20–24. 
Among these participants, nine (39.13%) have a bachelor’s degree, eight (34.78%) 
have a college degree, ﬁve (21.74%) have a graduate degree, and one (4.35%) have a 
high school degree. Among the 23 participants, 13 use iOS and the rest use Android. 
Most of the participants have good technical knowledge, either as students or from 
a work environment. The amount of time spent per day on the internet, using apps, 
and the duration of smartphone ownership on average are 3.33 hours per day, 2.42 
hours per day, and 11.8 years respectively. The details of the participants’ internet 
and smartphone usage is shown in Table 8. 
3.1.3
Participants’ Response Analysis 
After collecting the questionnaire responses from all the participants, a thorough 
analysis was performed on these collected data. To gain insight into the participants’ 
perception of smartphone sensors, we identify True Positives (TP), True Negatives 
(TN), False Positives (FP), and False Negatives (FN) in participants responses. Then, 
we compute precision (rhoρ) and recall (script upper RR) for the responses. 
Table 8 Participants’ usage of internet and smartphones 
Age range
#
Internet usage 
(in hours/day) 
App usage 
(in hours/day) 
Owning 
smartphone 
(in years) 
20–24
13
3.23
2.69
8.38 
25–29
6
3.83
3.00
8.33 
30–34
2
3.50
2.00
11.50 
35–40
2
2.75
2.00
19.00

148
A. I. Champa et al.
3.2 
Survey Outcome 
3.2.1
Familiarity (RQ1) 
The participants are found the most familiar with the camera and least familiar with 
the magnetometer. The ranking of the smartphone sensors’ familiarity among the 
participants based on the responses to the survey questionnaire is listed in Table 9. 
From the systematic review part, it was found that environmental sensors are not 
well-known to users [ 38, 39]. However, this is not reﬂected in the results of our 
survey, as the participants are found to be least familiar with motion sensors. 
Table 10 presents the percentage of participants who reported to have ﬁrst-hand 
experience of facing privacy and security issues when using a smartphone in given 
different scenarios. When questioned about their experiences with privacy and secu-
rity threats, 13 (57%) participants reported having experienced at least one attack, 
while six (26%) reported experiencing two or more attacks. Their response to the 
Table 9 Participants’ familiarity with smartphone sensors 
Rank*
Sensors
Rank*
Sensors 
1
Camera
09
Ambient light sensor 
2
Microphone
10
NFC 
3
Speaker
11
Gyroscope 
4
WiFi
12
Accelerometer 
5
GPS
13
Proximity sensor 
6
Bluetooth
14
Barometer 
7
Compass
15
Magnetometer 
8
Biometrics
(*Rank 1 indicates the most familiar) 
Table 10 Security/privacy issues faced by the participants 
Privacy and security issues
Faced (%) 
The smartphone had a virus or other harmful 
software installed 
26 
Passwords or other account information for 
banking, email, social networking, or other 
personal accounts were stolen and exploited 
35 
Was misled to pay for or use a service that 
turned out to be a scam 
17 
Personal or private information was posted on 
the Internet on social networks or online 
forums without permission 
10 
Nothing suspicious was ever noticed
43 
Other
10

Are We Aware? An Empirical Study on the Privacy …
149
possibility of facing privacy and security issues while using a smartphone is shown 
in Table 10. Furthermore, among participants who used internet for more than 10 
hours per day, 18 (80%) reported being exposed to at least one attack. This implies 
that heavy internet users may be particularly vulnerable to these types of attacks. We 
derive the answer to RQ1 as follows: 
Ans. to RQ1: Identity-related sensors are the most familiar, while motion sensors 
are the least familiar to the end-users. 
3.2.2
Awareness (RQ2) 
On average, the participants are aware of the six sensor attacks mentioned in the 
survey. However, the participants are least aware of TMC and most aware of DFP 
and KIn. Figure 2 provides a visual representation of the level of awareness associated 
with all the attacks. The majority of the participants exhibit the most concern about 
the security of their passwords and ﬁnancial information in response to unauthorized 
access. In contrast, only a small number of participants expressed concern about the 
potential for PhBAR to be used to access their information without their permission. 
The level of concern for each aspect of unauthorized access is illustrated in Fig. 3. 
However, the majority of the participants in the study are not at all aware of the 
security mechanism’s capabilities for protecting against sensor attacks, with only 
a small number being somewhat familiar, and only one individual being extremely 
familiar with one of the mechanisms. Familiarity of the participants with the available 
security preserving mechanisms is presented in Fig. 4. We, therefore, formulate the 
answer to RQ2 as follows: 
Smartphone Sensor Attacks 
Participants (%) 
0 
5 
10 
15 
20 
25 
LIn
TIn
KIn
Evd
TMC
DFP 
Extremely
Moderately
Somewhat
Slightly
Not at all 
Fig. 2 Participants’ awareness of smartphone sensor-based attacks

150
A. I. Champa et al.
. 
Types of Information/Data 
Participants (%) 
0% 
25% 
50% 
75% 
100% 
Passwords 
Videos 
Financial Info 
Audio 
Medical info 
Photo 
Documents 
PhBAR 
Extremely
Moderately
Somewhat
Slightly
Not at all 
Fig. 3 Participants’ levels of concern about unauthorized access 
Security Mechanisms 
Participants (%) 
0% 
25% 
50% 
75% 
100% 
SemaDroid 
AWare
EnTrust 
6thSense 
AuDroid
LocPPM 
Extremely 
Moderately
Somewhat 
Slightly 
Not at all 
Fig. 4 Participants’ familiarity with existing security mechanisms

Are We Aware? An Empirical Study on the Privacy …
151
Ans. to RQ2: Participants are the least aware of TMC threat. The majority of the 
participants are not aware of the security mechanisms against sensor attacks. 
3.2.3
Perception (RQ3) 
To measure participants’ perception of smartphone sensors and sensor attacks, a 
survey question (i.e., question 16) asked them to identify which attacks are possible 
for which of the 15 sensors. The seven columns from the left in Table 11 shows 
the participants’ perceptions of sensor-related threats as well as the facts drawn 
from the literature review as well as the computed TP, TN, FP, FN, precision (rhoρ), 
and recall (script upper RR). Here, a reddish-colored cell indicates that the literature identiﬁed 
corresponding sensor susceptible to the corresponding security threat. For example, 
according to the literature, the camera can potentially cause LIn, TIn, KIn, and DFP 
threats. A white/colorless cell indicates that literature identiﬁed corresponding sensor 
not susceptible to the corresponding security threat. For example, according to the 
literature, the camera is not vulnerable to Evd or TMC attacks. 
A value in a cell reports the number of survey participants reported to 
believe/perceive the corresponding sensor susceptible to the corresponding security 
threat. For example, 11 participants correctly perceived that camera is susceptible to 
the LIn attack. But ﬁve participants incorrectly thought that the camera is vulnerable 
to Evd. Five participants incorrectly also thought that the camera is vulnerable to 
TMC. 
Table 11 Perception of the threats associated with smartphone sensors

152
A. I. Champa et al.
That is why the FP value for the camera is 10, while TP is 20 as a total of 20 
participants correctly identiﬁed the LIn (11 participants), TIn (four participants), KIn 
(two participants), and DFP (three participants) threats posed by the camera. We see 
that the proximity sensor has the lowest precision and recall values. This indicates 
that participants have the most incorrect perceptions about the proximity sensor. In 
contrast, their perceptions of the WiFi and microphone sensors are more accurate. 
Based on the precision (rhoρ) and recall (script upper RR) values, it can be concluded that participants 
have relatively accurate perceptions of the other surveyed sensors in smartphones. 
We derive the answer to RQ3 as follows: 
Ans. to RQ3: Participants’ perceptions of the proximity sensor are the most inac-
curate, whereas their views of the WiFi and microphone sensors are the most 
precise. 
4 
Discussion 
While a systematic literature review identiﬁes the gaps in the current state of the art, 
a follow-up end-user study complements with a comprehensive understanding of the 
topic with new insights, as accomplished in our work. In our study, the survey result 
demonstrates that the participants are less familiar with motion sensors, which differs 
from the ﬁndings of our systematic review where environmental sensors were the least 
familiar [ 38]. In terms of sensor attacks, there is a conﬂicting familiarity with device 
ﬁngerprinting, with some users being extremely knowledgeable and others having 
no knowledge at all. Additionally, the participants are not familiar with security 
and privacy-preserving mechanisms against smartphone sensor attacks. This lack of 
familiarity may make participants more vulnerable to sensor-based attacks, as they 
may not be aware of the potential risks or know how to protect themselves from these 
types of attacks. 
A concerning factor the survey demonstrated is that a majority of participants 
reported experiencing at least one attack during the usage of a smartphone. It is 
noticed that those who spend a signiﬁcant amount of time online are at a higher risk of 
experiencing privacy and security issues while using their smartphones. Individuals 
need to educate themselves about smartphone sensor attacks to protect themselves. 
Passwords or PINs are an essential barrier to preventing unauthorized access. 
Participants show the highest level of concern for password protection for their 
personal information. However, participants have the least concern about the risks 
associated with PhBAR. But they do not understand that this collects a large amount 
of data about a person’s activity, location, speed, duration of the activity, and even 
stress level. These data can be accessed by unauthorized parties and lead to sensitive 
personal information being exposed to third parties. Then this information can be 
used for a variety of nefarious purposes, such as targeted marketing, information 
selling, disclosing classiﬁed data, and inferring and manipulating user habits [ 23].

Are We Aware? An Empirical Study on the Privacy …
153
By understanding the different ways in which sensors can be exploited, they can take 
steps to prevent these attacks and protect their sensitive information. 
The survey illustrates that the participants have the highest number of incorrect 
perceptions about the proximity sensor, as indicated by its low precision and recall 
values. Moreover, the familiarity of sensors is somewhat in line with the participant 
perception level, except for the magnetometer. The participant’s perception of this 
sensor is moderately clear with 78% precision, even though it is the least familiar 
one. The survey ﬁndings suggest that familiarity with a sensor may not necessarily 
correlate with participant perception. 
4.1 
Threats to Validity 
We recognize that our participant group is not very diverse and consists primarily 
of individuals with a technical background. While analyzing the survey responses, 
we did not take into account whether the participants used iOS or Android smart-
phones. Analyzing this aspect could have revealed some interesting ﬁndings. From 
the systematic literature review, we identiﬁed eight sensor attacks (listed in Table 4), 
from which we chose six sensors for the survey. We identiﬁed nine security measures 
(Table 6) from the literature review. In our survey, we chose the recently reported six 
security measures. This may be argued as a limitation of our work. 
To conduct the survey, we provide our participants with a brief overview of the 
smartphone sensors, sensor-based attacks, and the security measures in three appen-
dices. The purpose of this is to provide uniform brieﬁng and the same set of instruc-
tions to the participants. However, this method of informing users still might have 
fallen short to ensure equity as it is possible that some of the participants might not 
have fully understood the descriptions provided while others did. 
A limitation of our end-user survey is that it relies only on self-reported responses 
to the questionnaire, which may be subject to exaggeration or other biases. It would 
have been more informative to conduct a live interview with the participants to get a 
more accurate understanding of their perceptions. This could have provided a more 
in-depth understanding of their views and experiences. 
4.2 
Future Work 
By addressing these limitations, we gain a better understanding of the human impact 
of this rapidly advancing technology and provide reliable recommendations. There-
fore, we plan to ﬁrst increase diversity by reaching out to a varied demographic 
groups, communities, and/or organizations. The inclusion of participants with lit-
tle to no technical knowledge would provide a more comprehensive picture of the 
actual situation. Secondly, we aim to analyze the survey responses based on the 
smartphone’s two major operating systems (i.e. Android and iOS). Thirdly, we want

154
A. I. Champa et al.
to further extend this work by including sensors of wearable devices such as smart 
watches and smart glasses. Finally, we plan to develop a deeper understanding by 
combining both quantitative and qualitative approaches for collecting participants’ 
responses and analyzing them in depth. 
5 
Conclusion 
Smartphone sensor awareness is a crucial kind of literacy required to secure individ-
ual’s conﬁdential information to avoid breach or cyber attack. The utmost goal of 
this research was to investigate individuals‘ familiarity, their level of awareness and 
perception on smartphone sensors. To achieve this goal, in this study, we analyzed 
the privacy, security, and awareness concerns of smartphone sensors involving an 
extensive systematic review and a subsequent questionnaire-based survey conducted 
both online and in person. 
The systematic literature review highlights the complex and multifaceted nature 
of smartphone technology, with both beneﬁts and risks to consider. We conducted 
the descriptive investigation to establish the foundation for the research, to identify 
interesting phenomena, and developed the research questions to analyze further. The 
results from the end-user survey revealed that identity-related sensors are the most 
familiar to the participants, while motion sensors are the least familiar. Furthermore, 
participants have a distorted perception of the proximity sensor and are unaware of 
the security mechanisms available to protect against various sensor-related attacks. 
These ﬁndings emphasize the importance of users being aware of the potential 
risks and taking steps to protect their data and privacy. In future, we plan to address 
the limitations identiﬁed in Sect. 4.1 and extend this work for further research on the 
vulnerabilities and attacks associated with different types of smartphone sensors. 
Acknowledgements This work is supported in part by a grant from the Center for Advanced 
Energy Studies (CAES) in Idaho, USA. 
References 
1. Aguinaga S, Poellabauer C (2013) Stealthy health sensing to objectively characterize motor 
movement disorders. Proc Comput Sci 19:1182–1189 
2. Al-Haiqi A, Ismail M, Nordin R (2013) On the best sensor for keystrokes inference attack on 
android. Proc Technol 11:989–995 
3. Anand SA, Wang C, Liu J, Saxena N, Chen Y (2021) Spearphone: a lightweight speech privacy 
exploit via accelerometer-sensed reverberations from smartphone loudspeakers. In: Proceed-
ings of the 14th ACM conference on security and privacy in wireless and mobile networks, pp 
288–299 
4. Azzakhnini S, Staudemeyer RC (2020) Extracting speech from motion-sensitive sensors. In: 
ESORICS 2020 international workshops on data privacy management, cryptocurrencies and 
blockchain technology, pp 145–160

Are We Aware? An Empirical Study on the Privacy …
155
5. Brady S (2018) The brainpower behind smart sensors and their use in security 
6. Cai L, Chen H (2012) On the practicality of motion based keystroke inference attack. In: 5th 
International conference on trust and trustworthy computing, pp 273–290 
7. Chakraborty S, Tripp O (2016) Eavesdropping and obfuscation techniques for smartphones. 
In: Proceedings of the international conference on mobile software engineering and systems, 
pp 291–292 
8. Choi H, Chakraborty S, Charbiwala ZM, Srivastava MB (2011) Sensorsafe: a framework for 
privacy-preserving management of personal sensory information. In: 8th Workshop on secure 
data management, pp 85–100 
9. Choi H, Chakraborty S, Srivastava MB (2012) Design and evaluation of sensorsafe: a framework 
for achieving behavioral privacy in sharing personal sensory information. In: 2012 IEEE 11th 
International conference on trust, security and privacy in computing and communications, pp 
1004–1011 
10. Crager K, Maiti A (2017) Information leakage through mobile motion sensors: user awareness 
and concerns. In: Proceedings of the European workshop on usable security (EuroUSEC) 
11. Diamantaris M, Marcantoni F, Ioannidis S, Polakis J (2020) The seven deadly sins of the html5 
webapi: a large-scale study on the risks of mobile sensor-based attacks. ACM Trans Priv Secur 
(TOPS) 23(4):1–31 
12. Ehatisham-ul Haq M, Azam M, Naeem U, Rehman S, Khalid A (2017) Identifying smartphone 
users based on their activity patterns via mobile sensing. Proc Comput Sci 113:202–209 
13. Ehatisham-ul Haq M, Azam MA, Asim Y, Amin Y, Naeem U, Khalid A (2020) Using smart-
phone accelerometer for human physical activity and context recognition in-the-wild. Proc 
Comput Sci 177:24–31 
14. Fernandes E, Jung J, Prakash A (2016) Security analysis of emerging smart home applications. 
In: 2016 IEEE symposium on security and privacy (SP), pp 636–654 
15. Fyke Z, Griswold-Steiner I, Serwadda A (2019) Prying into private spaces using mobile device 
motion sensors. In: 2019 17th international conference on privacy, security and trust (PST), pp 
1–10 
16. Gao C, Fawaz K, Sur S, Banerjee S (2019) Privacy protection for audio sensing against multi-
microphone adversaries. Proc Priv Enhanc Technol 2019(2):146–165 
17. Ghosh D, Joshi A, Finin T, Jagtap P (2012) Privacy control in smart phones using semantically 
rich reasoning and context modeling. In: 2012 IEEE symposium on Security and privacy 
workshops, pp 82–85 
18. Han W, Cao C, Chen H, Li D, Fang Z, Xu W, Wang XS (2017) sendroid: auditing sensor access 
in android system-wide. IEEE Trans Dependable Secur Comput 17(2):407–421 
19. Hernández-Álvarez L, de Fuentes JM, González-Manzano L, Hernández Encinas L (2020) 
Privacy-preserving sensor-based continuous authentication and user proﬁling: a review. Sensors 
21(1):92 
20. Huang S, Wu R, Wang Y, Sun Y, Zhang J, Li X (2022) Inferring user input through smart-
phone gyroscope. In: 2022 2nd international conference on consumer electronics and computer 
engineering (ICCECE) pp 623–628 
21. Jagtap P, Joshi A, Finin T, Zavala L (2011) Preserving privacy in context-aware systems. In: 
2011 IEEE Fifth international conference on semantic computing, pp 149–153 
22. Javed AR, Rehman SU, Khan MU, Alazab M, Khan HU (2021) Betalogger: smartphone sensor-
based side-channel attack detection and text inference using language modeling and dense 
multilayer neural network. Trans Asian Low-Resour Lang Inf Process 20(5):1–17 
23. Jayakumar P, Lawrence L, Chean RLW, Brohi SN (2019) A review and survey on smartphones: 
the closest enemy to privacy. In: 2nd International conference on emerging technologies in 
computing, pp 106–118 
24. Kosono R, Nishio T, Morikura M, Yamamoto K, Maki Y, Goda T, Matsukawa H, Indo T (2018) 
Mobile user identiﬁcation by camera-based motion capture and mobile device acceleration 
sensors. In: Proceedings of the 13th workshop on challenged networks, pp 25–31 
25. Kröger JL, Raschke P (2019) Is my phone listening in? on the feasibility and detectability 
of mobile eavesdropping. In: IFIP annual conference on data and applications security and 
privacy, pp 102–120

156
A. I. Champa et al.
26. Kröger JL, Gellrich L, Pape S, Brause SR, Ullrich S (2022) Personal information inference 
from voice recordings: user awareness and privacy concerns. Proc Priv Enhancing Technol 
2022(1):6–27 
27. KV GL, Sait U, Kumar T, Bhaumik R, Shivakumar S, Bhalla K (2020) Design and development 
of a smartphone-based application to save lives during accidents and emergencies. Proc Comput 
Sci 167:2267–2275 
28. Lee, Y., Li, J., Kim, Y.: Micprint: acoustic sensor ﬁngerprinting for spoof-resistant mobile 
device authentication. In: Proceedings of the 16th EAI international conference on mobile and 
ubiquitous systems: computing, networking and services, pp 248–257 
29. Lei L, Wang Y, Zhou J, Wang L, Zhang Z (2013) A threat to mobile cyber-physical systems: 
sensor-based privacy theft attacks on android smartphones. In: 12th IEEE international confer-
ence on trust, security and privacy in computing and communications, pp 126–133 
30. Li XY, Liu H, Zhang L, Wu Z, Xie Y, Chen G, Wan C, Liang Z (2019) Finding the stars 
in the ﬁreworks: deep understanding of motion sensor ﬁngerprint. IEEE/ACM Trans Netw 
27(5):1945–1958 
31. Liu Y, Huang K, Song X, Yang B, Gao W (2020) Maghacker: eavesdropping on stylus pen 
writing via magnetic sensing from commodity mobile devices. In: Proceedings of the 18th 
international conference on mobile systems, applications, and services, pp 148–160 
32. Liu X, Liu J, Wang W (2015) Exploring sensor usage behaviors of android applications based 
on data ﬂow analysis. In: 2015 IEEE 34th international performance computing and commu-
nications conference (IPCCC), pp 1–8 
33. Maharjan SM, Poudyal A, van Heerden A, Byanjankar P, Thapa A, Islam C, Kohrt BA, Hagaman 
A (2021) Passive sensing on mobile devices to improve mental health services with adolescent 
and young mothers in low-resource settings: the role of families in feasibility and acceptability. 
BMC Med Inf Decis Mak 21(1):1–19 
34. Massollar J, Garcia ACB (2021) Fencebot: an elderly tracking app for mitigating health risk 
contacts. In: 2021 IEEE 24th international conference on computer supported cooperative work 
in design (CSCWD), pp 1009–1014 
35. Matovu R, Serwadda A (2018) Gaming the gamer: adversarial ﬁngerprinting of gaming apps 
using smartphone accelerometers. In: 2018 9th IEEE annual ubiquitous computing, electronics 
& mobile communication conference (UEMCON), pp 489–496 
36. Mehrnezhad M, Toreini E, Shahandashti S, Hao F (2016) Touchsignatures: identiﬁcation of user 
touch actions and pins based on mobile sensor data via javascript. J Inf Secur Appl 26:23–38 
37. Mehrnezhad M, Toreini E, Shahandashti SF, Hao F (2018) Stealing pins via mobile sensors: 
actual risk versus user perception. Int J Inf Secur 17(3):291–313 
38. Mehrnezhad M, Toreini E (2019) What is this sensor and does this app need access to it? In: 
Informatics, vol 6, p 7 
39. Mehrnezhad M, Toreini E, Alajrami S (2018) Making sense of sensors: mobile sensor security 
awareness and education. In: 7th workshop on socio-technical aspects in security and trust, pp 
40–52 
40. Muralidharan K, Ramesh A, Rithvik G, Prem S, Reghunaath A, Gopinath M (2021) 1d convo-
lution approach to human activity recognition using sensor data and comparison with machine 
learning algorithms. Int J Cogn Comput Eng 2:130–143 
41. Naval S, Pandey A, Gupta S, Singal G, Vinoba V, Kumar N (2021) Pin inference attack: a threat 
to mobile security and smartphone-controlled robots. IEEE Sens J 22(18):17475–17482 
42. Ning R, Wang C, Xin C, Li J, Wu H (2018) Deepmag: snifﬁng mobile apps in magnetic 
ﬁeld through deep convolutional neural networks. In: 2018 IEEE international conference on 
pervasive computing and communications (PerCom), pp 1–10 
43. Petracca G, Atamli-Reineh A, Sun Y, Grossklags J, Jaeger T (2017) Aware: preventing abuse of 
privacy-sensitive sensors via operation bindings. In: USENIX security symposium, pp 379–396 
44. Petracca G, Marvel LM, Swami A, Jaeger T (2016) Agility maneuvers to mitigate inference 
attacks on sensed location data. In: MILCOM 2016-2016 IEEE military communications con-
ference, pp 259–264

Are We Aware? An Empirical Study on the Privacy …
157
45. Petracca G, Sun Y, Atamli-Reineh A, McDaniel PD, Grossklags J, Jaeger T (2019) Entrust: 
regulating sensor access by cooperating programs via delegation graphs. In: USENIX security 
symposium, pp 567–584 
46. Petracca G, Sun Y, Jaeger T, Atamli A (2015) Audroid: preventing attacks on audio channels in 
mobile devices. In: Proceedings of the 31st annual computer security applications conference, 
pp 181–190 
47. Rabbi M, Ali S, Choudhury T, Berke E (2011) Passive and in-situ assessment of mental and 
physical well-being using mobile sensors. In: Proceedings of the 13th international conference 
on Ubiquitous computing, pp 385–394 
48. Sabir AT, Maghdid HS, Asaad SM, Ahmed MH, Asaad AT (2019) Gait-based gender classiﬁca-
tion using smartphone accelerometer sensor. In: 2019 5th international conference on frontiers 
of signal processing (ICFSP), pp 12–20 
49. Sensors
overview.
https://developer.android.com/guide/topics/sensors/sensors_overview. 
Android Developers (Veriﬁed: Apr 2023) 
50. Shamseer L, Moher D, Clarke M, Ghersi D, Liberati A, Petticrew M, Shekelle P, Stewart LA 
(2015) Preferred reporting items for systematic review and meta-analysis protocols (prisma-p) 
2015: elaboration and explanation. Bmj, vol 349 
51. Sikder AK, Aksu H, Uluagac AS (2017) 6thsense: a context-aware sensor-based attack detector 
for smart devices. In: USENIX security symposium, pp 397–414 
52. Sikder AK, Petracca G, Aksu H, Jaeger T, Uluagac AS (2021) A survey on sensor-based threats 
and attacks to smart devices and applications. IEEE Commun Surv Tutor 23(2):1125–1159 
53. Song R, Song Y, Dong Q, Hu A, Gao S (2017) Weblogger: stealing your personal pins via 
mobile web application. In: 2017 9th international conference on wireless communications 
and signal processing (WCSP), pp 1–6 
54. Song R, Song Y, Gao S, Xiao B, Hu A (2018) I know what you type: Leaking user privacy via 
novel frequency-based side-channel attacks. In: 2018 IEEE global communications conference, 
pp 1–6 
55. Spreitzer R (2014) Pin skimming: exploiting the ambient-light sensor in mobile devices. In: 
Proceedings of the 4th ACM workshop on security and privacy in smartphones & mobile 
devices, pp 51–62 
56. Stachl C, Au Q, Schoedel R, Gosling SD, Harari GM, Buschek D, Völkel ST, Schuwerk T, 
Oldemeier M, Ullmann T et al (2020) Predicting personality from patterns of behavior collected 
with smartphones. Proc Natl Acad Sci 117(30):17680–17687 
57. Struminskaya B, Toepoel V, Lugtig P, Haan M, Luiten A, Schouten B (2020) Understanding 
willingness to share smartphone-sensor data. Public Opin Q 84(3):725–759 
58. Struse E, Seifert J, Üllenbeck S, Rukzio E, Wolf C (2012) Permissionwatcher: creating user 
awareness of application permissions in mobile systems. In: International joint conference on 
ambient intelligence, pp 65–80 
59. Suarez-Tangil G, Tapiador JE, Peris-Lopez P, Ribagorda A (2013) Evolution, detection and 
analysis of malware for smart devices. IEEE Commun Surv Tutor 16(2):961–987 
60. Subasi A, Dammas DH, Alghamdi RD, Makawi RA, Albiety EA, Brahimi T, Sarirete A (2018) 
Sensor based human activity recognition using adaboost ensemble classiﬁer. Proc Comput Sci 
140:104–111 
61. Wampﬂer R, Klingler S, Solenthaler B, Schinazi VR, Gross M, Holz C (2022) Affective state 
prediction from smartphone touch and sensor data in the wild. In: Proceedings of the 2022 CHI 
conference on human factors in computing systems, pp 1–14 
62. Watanabe Y, Sara S (2016) Toward an immunity-based gait recognition on smart phone: a study 
of feature selection and walking state classiﬁcation. Proc Comput Sci 96:1790–1800 
63. Xu Z, Zhu S (2015) Semadroid: a privacy-aware sensor management framework for smart-
phones. In: Proceedings of the 5th ACM conference on data and application security and 
privacy, pp 61–72 
64. Yang Z, Zhao R, Yue C (2018) Effective mobile web user ﬁngerprinting via motion sensors. 
In: 17th IEEE international conference on trust, security and privacy, pp 1398–1405

158
A. I. Champa et al.
65. Ye H, Sheng L, Gu T, Huang Z (2019) Seloc: collect your location data using only a barometer 
sensor. IEEE Access 7:88705–88717 
66. Yue C (2016) Sensor-based mobile web ﬁngerprinting and cross-site input inference attacks. 
In: 2016 IEEE security and privacy workshops (SPW), pp 241–244 
67. Zhang R, Chen X, Wen S, Zheng J (2019) Who activated my voice assistant? A stealthy attack 
on android phones without users’ awareness. In: 2nd international conference machine learning 
for cyber security, pp 378–396 
68. Zhang W, Wang X (2014) A lightweight user state monitoring system on android smartphones. 
In: ICSOC 2014 workshops on service-oriented computing, pp 259–269 
69. Zhao, F., Gao, L., Zhang, Y., Wang, Z., Wang, B., Guo, S.: You are where you app: An assess-
ment on location privacy of social applications. In: 2018 IEEE 29th International Symposium 
on Software Reliability Engineering (ISSRE), pp. 236–247 (2018) 
70. Zhao S, Zhao Z, Huang R, Luo Z, Li S, Tao J, Cheng S, Fan J, Pan G (2019) Discovering 
individual life style from anonymized wiﬁ scan lists on smartphones. IEEE Access 7:22698– 
22709 
71. Zhao K, Zou D, Jin H, Tian Z, Qiang W, Dai W (2015) Privacy protection for perceptual 
applications on smartphones. In: 2015 IEEE international conference on mobile services, pp 
174–181 
72. Zhuo S, Sherlock L, Dobbie G, Koh YS, Russello G, Lottridge D (2020) Real-time smartphone 
activity classiﬁcation using inertial sensors–recognition of scrolling, typing, and watching 
videos while sitting or walking. Sensors 20(3):655

Using Effective Dummy Locations 
and Routes to Conceal User Locations 
Sanjaikanth E Vadakkethil Somanathan Pillai and Wen-Chen Hu 
Abstract Smartphones are extremely popular in these days because millions of 
apps are available. People use the apps for almost everything like shopping, banking, 
watching movies, and playing games. One set of the favorite apps is location-based 
services (LBSs), which provide various services like navigation and recommenda-
tions based on users’ location data. However, not all users like to share their location 
data with the service providers. Various methods are proposed to solve this problem. 
One of the methods is using dummy locations, which are sent along with the true 
locations to the service providers, so the providers would not know which locations 
are true. This method is simple and seems effective. Nevertheless, there are some 
loopholes from this method. There are two kinds of locations, discrete and continuous 
locations. This research proposes effective methods of using discrete and continuous 
dummy locations to conceal user locations and routes, so the service providers are 
not able to ﬁnd the true user locations or routes. Preliminary experiment results show 
the proposed methods are simple, but effective. 
Keywords Privacy · Location-based service · LBS · Dummy location · Dummy 
route · Mobile computing 
1 
Introduction 
Location-based services (LBSs), such as location-based advertising, mobile recom-
mendations, and navigation, are very popular as the smartphones are used everywhere 
and anytime. LBSs require the users to report their data such as user IDs, locations, 
routes, time stamps, landmarks, etc. The LBS users are assumed to receive the recom-
mendations right after they send their data to the service providers, which supply the
S. E. V. S. Pillai · W.-C. Hu envelope symbol
School of Electrical Engineering & Computer Science, University of North Dakota, Grand Forks, 
ND 58202, USA 
e-mail: wenchen@cs.und.edu 
S. E. V. S. Pillai 
e-mail: s.evadakkethil@und.edu 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_11 
159

160
S. E. V. S. Pillai and W.-C. Hu
recommendations upon request. However, in order to use the services, users have 
to share their data with the service providers. Many users are reluctant to use the 
services because of this privacy concern. Various methods have been proposed to 
solve this problem. One of the methods, dummy locations, is used by many LBS 
users. It sends the true locations along with several fake locations to confuse the 
service providers, so the providers would not be able to tell the true one from the 
fake ones. The method is simple and effective, but the service providers may be able 
to ﬁgure out the true locations if the dummy locations are not generated carefully. 
For example, the 4 million miles of roads covered in the US are only a fraction of a 
percent of the total land area. If the dummy locations are not carefully planned, they 
may land on wild ﬁelds or water and could be easily perceived as fake. 
Various loopholes from using the traditional methods of dummy locations and 
routes are discussed ﬁrst in this paper. Robust methods are then proposed to solve 
the problems expectantly. The following features must be taken into consideration 
when creating dummy locations: (i) they should not be too far away from the true 
location, but cannot be too close to each other either, (ii) they should be located on 
the valid space like roads or parking lots, and (iii) their number should be kept as 
low as possible like 3 to 5, so the management would not become too complicated. 
On the other end, the service providers will be able to tell the locations are fake if the 
above features are not followed. This research investigates the ﬂaws of using dummy 
locations to uphold user privacy from both the users’ and service providers’ points 
of view, and proposes innovative methods to close the loopholes, so more users will 
be willing to use location-based services. Preliminary experiment results show the 
proposed method is simple, but effective. 
The rest of this article is organized as follows. Section 2 gives the background 
information and related research of this research. Section 3 introduces the problems of 
current methods of using dummy locations and routes, and Sect. 4 gives the proposed 
methods trying to solve the problems. Experiment results are shown in Sect. 5. The  
last section summarizes this research. 
2 
Background and Related Research 
This section presents the related research of privacy-preserving methods for location-
based services. Various methods of privacy preservation are discussed ﬁrst and related 
dummy-location methods will be introduced next. An overview of dummy-based 
location privacy protection techniques for location-based services can be found from 
the article [23].

Using Effective Dummy Locations and Routes to Conceal User Locations
161
2.1 
Privacy Preservation of Location-Based Services 
One of the privacy-preservation methods is spatial cloaking, which obscures a user’s 
true location into a cloaked area, so there is low possibility of associating users to 
locations. Montazeri et al. [11] use a method of anonymization (identity perturbation 
instead of location perturbation) to achieve location privacy. A random permutation
 (N) is applied to the set of N users, and then the pseudonym  (N)(i) is assigned 
to the user i. They claim perfect privacy can be achieved under certain conditions 
by using their method. Wang, Hu, Sun, & Huang [17] propose a query content 
preservation approach with the aim of providing accurate LBS answer with zero 
server knowledge on query content. Peng, Liu, Wang, Xiang, & Chen [12] propose a 
multidimensional privacy preservation scheme that provides full protection for user 
privacy without any need for a trusted third party. The proposed scheme employs a 
semi-trusted middle entity to perform user anonymization and result-blind ﬁltering 
while unaware of any sensitive information regarding the mobile users. They utilize 
the Hilbert curve to transform user locations, and preserve users’ query contents 
using encryption technology. Related research can be found in the articles [4, 5, 18] 
and surveys of privacy-preserving techniques for location-based services are given 
in the articles [2, 9] or a book [3]. 
2.2 
Privacy Preservation Using Discrete Dummy Locations 
For dummy/fake locations, users send their true location data along with several 
dummy/false location data to the service providers. Though this approach is effec-
tive, it is also simple, so not many articles put the focus on this approach. Zhang and 
Li [20] preserve the user privacy by using the following three steps. First, the dummy 
location candidate set is constructed based on WordNet by randomly selecting offset 
location, and conforming to probability similarity. The dummy location set is then 
ﬁltered out by discretizing dummy locations based on the Heron formula. Finally, 
the secure anonymity set is constructed according to the anonymity level. Sun et al. 
[14] propose a region-of-interest division-based algorithm to preserve the location 
privacy of mobile device users in location-based services. Unlike existing methods, 
the proposed approach generates dummy locations while considering the semantic 
information of those locations. It enables the generated locations to exclude or 
reduce the exposure of a user’s real location. Wang and Xie [16] propose a scal-
able location privacy preservation (LPP) method based on the paradigm of counter-
feiting locations by forging the fake locations through synthesizing artiﬁcial impos-
tors (AIs). Two dedicated techniques are devised: the sampling-based synthesis 
method and population-level semantic model. Bindschaedler and Shokri [1] design 
a privacy-preserving generative model to synthesize location traces. The location 
traces are plausible to be trajectories of some individuals with consistent lifestyles 
and meaningful mobility. Related research can also be found in the articles [15, 19].

162
S. E. V. S. Pillai and W.-C. Hu
2.3 
Privacy Preservation Using Continuous Dummy 
Locations 
For Other than creating discrete dummy locations to preserve privacy, location-based 
services may consider generating continuous dummy locations for hiding the trav-
eling trajectories. Sun et al. [14] use three real-life trajectory datasets, ﬁve existing 
anonymization mechanisms (identiﬁer anonymization, grid-based anonymization, 
dummy trajectories, k-anonymity and ε-differential privacy), and two practical appli-
cations (travel time estimation and window range queries) to facilitate privacy preser-
vation for continuous location-based services. They found there is a long way to go 
for the privacy preservation for trajectories in the general sense. An attack model 
representing a realistic privacy threat on user trajectories is proposed by Shaham 
et al. [13]. They also propose a metric called transition entropy that enables the 
evaluation of dummy-based algorithms, followed by developing a robust algorithm 
that can defend users against the attack while maintaining signiﬁcantly high perfor-
mance in terms of the traditional metrics. Zhang et al. [21] propose an algorithm 
based on the k-anonymity criterion, to generate dummy locations to protect users’ 
privacy. Their simulation results on the real-life dataset show the proposed algorithm 
performs better than other methods. Related articles can be found from the articles 
[8, 10, 22]. 
3 
Problems of Using Dummy Locations and Routes 
If dummy locations are not used carefully, hackers may be able to ﬁnd out the true 
locations or routes. This section is to discuss the various problems associated with 
using dummy locations and routes. 
3.1 
Discrete Dummy Locations 
For simple location-based services, like ﬁnding a nearby ethnic restaurant or gas 
station based on the user’s current location, the user’s privacy may be breached if 
the dummy locations are not generated carefully. Two kinds of dummy locations 
are generally used in this approach, random locations within a distance and random 
locations.
• Random locations within a distance: Random locations are randomly generated 
within a distance d from the true location. For example, Fig. 1 shows four dummy 
locations, D1–D4 and the true location, R. It seems the service provider would not 
know which location is true. Therefore, it will generate ﬁve sets of recommenda-
tions based on the ﬁve locations, respectively. However, if the service provider is 
smart enough, it may ﬁgure out an approximate distance d’ ≤ d after few rounds.

Using Effective Dummy Locations and Routes to Conceal User Locations
163
d 
R(t) 
D3 
D4 
D1 
D2 
Fig. 1 Four dummy locations D generated based on the true location R at the time t 
D 
D 
D 
D 
D
D 
D1
D 
Next  
generation 
R(t) 
R(t+1) 
Fig. 2 True location R and four dummy locations D generated randomly at the times t and t + 1 
The longest distance l, like |D1–D2|, is collected in each round. After a while, 
the d’ would be the longest one l’ of the collected distances divided by 2 like d’ 
= l’/2. Once the approximate distance d’ is found, some dummy locations could 
be eliminated. For example, D1 could not be a true location because each of the 
distances |D1–D2| and |D1–D3| is much greater than d’. For the same reason, D2 
could not be the true location, either. Therefore, the true location may be one of 
the three locations, R, D3, and D4.
• Random locations: Dummy locations are generated randomly. Figure 2 shows 
four dummy locations D randomly generated. After several rounds of generations, 
some locations may be ruled out as the true location R. For example, the D1 may 
be dropped if no previous locations can reach it according to the distance based 
on time stamps and calculated travel speeds. 
3.2 
Continuous Dummy Locations 
For Continuous LBSs require users to continuously send their location data to service 
providers. For example, better transportation planning is based on trafﬁc ﬂows instead 
of vehicle counts; recommendations (like interesting places) would be more effective 
if travel routes (sequences of locations), not individual locations, are used. Compared 
to discrete LBSs, continuous LBSs have more pitfalls. Two of the problems are given 
next:

164
S. E. V. S. Pillai and W.-C. Hu
D 
D 
R(t) 
R(t+1) 
R(t+2) 
R(t+3) 
Fig. 3 Dummy routes created by using random locations D within a distance from the true location 
R at the time t 
D1 
D2 
R(t) 
R(t+1) 
R(t+2) 
R(t+3) 
Fig. 4 Dummy routes created by using random locations D from the true locations R at the time t 
• Random locations within a distance: The two endpoints of a dummy route are 
random locations within a distance d from the true locations. Figure 3 shows 
an example of this approach. The major problem of this approach is the service 
provider may pick one route and treat it as the true route. The result may not be 
much different because the true route is close to dummy routes, so an approximate 
true route may be found. Another problem of this approach is the end point of a 
segment is the start point of the next segment. The service provider may be able to 
ﬁgure out the true route after receiving few segments because the dummy routes 
all are surrounding the true route. 
• Random locations: The dummy routes are generated randomly. Figure 4 shows 
an example of this approach. The problem of this approach is the dummy routes 
may be far away from the true route after a while and maintain and keep track of 
them may not be easy. If the dummy routes are not planned carefully, the service 
provider may ﬁgure out the true route. For example, if the dummy locations could 
not be reached in a speciﬁc time according to the previous locations, road and 
street conditions, and the travel speeds, then they may be ruled out as true routes. 
4 
The Proposed Methods 
This research proposes dummy location and route generation, so the service providers 
will not be able to tell which the true locations or routes are. Compared to discrete 
LBSs, continuous LBSs are more challenging because the users have to continuously

Using Effective Dummy Locations and Routes to Conceal User Locations
165
report their location data to the service providers and it makes their privacy more 
fragile. 
4.1 
Discrete Location-Based Services 
The problem of dummy locations discussed in the previous section is some dummy 
locations may be ruled out because the distance d may be ﬁgured out, where random 
locations are randomly generated within a distance d from the true location. Instead 
of building the system from the ground up, this research tries to solve the problem by 
generating genuine dummy locations based on Roads API of Google Maps Platform 
(https://developers.google.com/maps/documentation/roads/overview), which identi-
ﬁes the roads a vehicle was traveling along and provides additional metadata about 
those roads, such as speed limits. The proposed algorithm is given as follows: 
1. Find the device location including latitude and longitude. 
2. Generates three random locations (latitudes and longitudes) within a distance 
from the device location. 
3. The new latitude is a random double value that is between the integer-converted 
device latitude and the next integer such as 
lat = integer( device latitude ) 
new latitude = ﬂoat [ lat, lat+1] 
4. Find the longitude by using a similar formula as above. 
5. Call Google Maps API for Road (https://roads.googleapis.com/v1/nearestRoads) 
by passing the three pairs of latitudes and longitudes delimited by the symbol ‘|’, 
for example, 
https://roads.googleapis.com/v1/nearestRoads?points=34.5082,-
97.6989|34.51352,97.7397|34.52352,-97.7197 
6. The returned three locations (latitudes and longitudes) are on a road within 50-
60 m to the input latitude and longitude. If there is no such road found, it will 
return null. 
4.2 
Continuous Location-Based Services 
Other than sending the location data to the service providers whenever a service 
is requested (a discrete LBS), this research also has the users send their routes 
(sequences of location data) to the service providers continuously (a continuous 
LBS). For example, a discrete LBS recommends a nearby landmark based on the 
current location data, whereas a continuous LBS suggests landmarks along the 
predicted routes according to the previous routes. However, compared to dummy 
locations, dummy routes are more difﬁcult to build since roads and streets have to 
be considered. For example, if a portion of a route crosses a building, the route may 
be considered fake. Instead of building the dummy routes from the ground up, this

166
S. E. V. S. Pillai and W.-C. Hu
Dummy route 2 
R 
Dummy route 1 
R(t+1) 
R(t+2) 
R(t+3) 
True route 
Fig. 5 Dummy routes created by using random locations based on the true locations R and the time 
t 
research takes the advantage of Google Directions API (https://developers.google. 
com/maps/documentation/directions/overview), which is a service that calculates 
directions between locations using an HTTP request. Users can search for direc-
tions for several modes of transportation, include transit, driving, walking or cycling. 
Directions may specify origins, destinations and waypoints either as text strings (e.g. 
“Chicago, IL” or “Darwin, NT, Australia”) or as latitude/longitude coordinates. The 
proposed method generates the dummy routes based on the random locations created 
from the start and end locations of the current true route. Figure 5 shows an example 
of this approach. The problem of this approach is the dummy routes may be far away 
from the true route after a while and maintain and keep track of them may not be 
easy. A brief algorithm of the dummy-route generation is given as follows: 
1. Generate n dummy locations randomly based on the true locations, R(t) and R(t 
+ 1), respectively. 
2. Generate n + 1 routes by using Google Direction API based on the true and 
dummy locations. 
3. Send the true and dummy routes to the server. 
4. The server returns the recommendations based on the true and dummy routes 
back to the clients. 
The algorithm is assumed to be able to preserve the user privacy, but it needs 
a formal proof to show the method is solid because if the dummy routes are not 
planned carefully, the service provider may ﬁgure out the true route. For example, if 
the dummy route is very long or crosses a building, then it may be ruled out as a true 
route. The Android source code of the proposed algorithm is given at the GitHub [6]. 
5 
Experiment Results 
This section is to show the results of our method by building a prototype system, 
which is to recommend the mobile user an interesting place based on the user quests 
and location. In order to receive the recommendation, the user has to send his/her 
current location information to the service provider. Instead of actual road testing,

Using Effective Dummy Locations and Routes to Conceal User Locations
167
this experiment uses Android emulators to perform the testing. Actual road testing 
will be conducted in the future. 
5.1 
Experiment Setup 
This sub-section discusses a prototype of a location-based service for showing the 
effectiveness of the proposed method. The proposed service is made simple on 
purpose and its system structure is shown in Fig. 6, where a set of services like 
ﬁnding nearby gas stations or restaurants are saved by the service provider before 
the app is put to use. When the user requests a recommendation, his/her current 
location along with three dummy locations are sent to the service providers. The 
recommended destinations are sent back to the user and a route between the current 
location and the genuine destination is drawn. The same steps are applied to routes. 
Building a working system from the ground up for this research is not trivial. This 
research tries to use the services provided by the Google Maps Platform (https://dev 
elopers.google.com/maps) including. 
• Maps SDK for Android is used to build dynamic, interactive, customized maps, 
location, and geospatial experiences for your Android apps. 
• The Places API is a service that returns information about places using HTTP 
requests. Places are deﬁned within this API as establishments, geographic 
locations, or prominent points of interest. 
• The Directions API is a web service that uses an HTTP request to return JSON 
or XML-formatted directions between locations. 
The proposed system can be found from GitHub [6].
Suggestions 
Locations 
or routes 
Locations 
or routes 
Suggestions or data 
Data 
Mobile user 
LBS Database 
Server side 
Client side 
LBS func-
Google 
Maps 
functions including 
direction generation 
Dummy location 
or route generation 
Service provider 
Fig. 6 System structure of the proposed location-based service 

168
S. E. V. S. Pillai and W.-C. Hu
Events 
EID 
Type 
Name 
Location 
E01 
Restaurant 
Gourmet Café 
47° 55’ 31” N, 97° 01’ 57” W 
E02 
Motel 
Best Motel 
47° 55’ 37” N, 97° 01’ 63” W 
E03 
Theater 
AMC 
47° 55’ 28” N, 97° 01’ 59” W 
E04 
Restaurant 
Pasta House 
47° 55’ 38” N, 97° 01’ 84” W 
... 
... 
... 
... 
Transactions 
T# 
EID 
T05 
E12 
T02 
E03 
T05 
E26 
T24 
E18 
... 
... 
Sequences 
S# 
EID 
Next 
S01 
E03 
S04 
S02 
E12 
S10 
S03 
E38 
S54 
S01 
E27 
— 
... 
... 
... 
Fig. 7 Database schemas and sample Values of the Tables Events, Transactions, and  Sequences 
A location-based service has to store various kinds and huge amount of data such 
as IDs, locations, routes, places, etc. Instead of saving all data by ourselves, this 
research uses Google Places API (https://developers.google.com/maps/ documenta-
tion/places/web-service/overview), which is a service that returns information about 
places using HTTP requests. Places are deﬁned within this API as establishments, 
geographic locations, or prominent points of interest. The LBS Database used in this 
research is a tiny geographical database including three tables: Events, Transactions, 
and Sequences, whose schema and sample values are given in Fig. 7. The  Events table 
stores a small set of events such as restaurants, motels, and entertainment events like 
theaters and concerts. Each transaction or sequence consists of a list of events. The 
difference between a transaction and a sequence is the events of the former are 
unordered, whereas the ones of the latter are ordered. In addition, each item of a 
sequence is an event, instead of a set of events. 
5.2 
The Result Screenshots 
This sub-section shows some of the screenshots from this service. It is to help readers 
understand what the service can achieve. Instead of doing actual road tests, this 
research uses Android AVD (Android Virtual Device) to simulate the testing. Figure 8 
shows three screenshots from this app. Figure 8a shows the app in the Android 
Launcher, Fig. 8b is the control panel associated with the emulator, and Fig. 8c is

Using Effective Dummy Locations and Routes to Conceal User Locations
169
(a)
(b) 
(c) 
Fig. 8 (a) The proposed app shown in the Android Launcher, (b) the Control Panel, and (c) the  
Extended Controls 
the extended controls after clicking the item at the bottom of the control panel. The 
extended controls include a location function, which allows users to enter location 
data including latitude, longitude, and attitude and submit it to the app. Using this 
function is similar to the app receiving location data from the GPS (Global Positioning 
System). 
Figure 9 shows dummy locations generated by the proposed method. All locations 
are valid as they are not located at improper locations like water, buildings, or wild 
ﬁelds. 
Figure 10a shows the user is marked according to his/her current location sent from 
the extended controls of Android. Figure 10b shows three valid dummy locations are
(a)
(b)
(c) 
Fig. 9 Valid dummy locations generated by the proposed method 

170
S. E. V. S. Pillai and W.-C. Hu
)
b
(
)a(
(c)
Fig. 10 (a) User location marked, (b) three dummy locations generated, and (c) direction between 
the user and a recommended destination 
generated by our method in addition to the user true location, and a direction between 
the user and a recommended destination is drawn in Fig. 10c by using Google Maps 
Direction API. 
Examples of dummy routes are given in Fig. 11, where (a) shows two locations. 
Figure 11b displays a direction generated by using Google Maps Direction API. A 
dummy direction based on the true direction is shown in the Fig. 11c. 
)c(
)
b
(
)a( 
Fig. 11 (a) Two locations marked, (b) a direction drawn, and (c) a dummy direction created

Using Effective Dummy Locations and Routes to Conceal User Locations
171
6 
Conclusion 
More than 1,395 million smartphones were sold in 2022. They are the most popular 
devices compared to PCs and tablets. People carry them everywhere and use them 
anytime. Smartphones include many features such as high mobility, low bandwidth, 
and small screens not found in desk or lap-top computers. Because of their unique 
features, new usage data of smartphones is generated and new research and appli-
cations are created. One of the applications, location-based services (LBSs), is to 
provide services based on user location data. However, there is one major concern 
for these services: user privacy preservation. Most LBSs require the users to send 
their current locations or routes and many users are reluctant to share their location 
data and identities. Dummy (fake) locations and routes have been used frequently 
in location-based services to protect users’ privacy. This research ﬁrst shows the 
possible pitfalls of the methods of using dummy locations and routes. Robust methods 
are then proposed to solve the problems by sending users’ true location data along 
with dummy location data to the service providers. User privacy is assumed to be 
preserved because service providers cannot distinguish the true location data from 
the dummies’. 
Preliminary experiment results show this method is simple and seems effective for 
privacy preservation of location-based services. However, it is not without problems 
because user privacy may not be fully protected if it is not planned carefully. For 
example, the 4 million miles of roads covered in the US is only a fraction of a 
percent of the total land area in the lower 48 states. The dummy locations may land 
on wild ﬁelds or water and could be easily perceived as fake. Actual road testing and 
solid proofs need to be given to prove the soundness and robustness of the proposed 
method. The ideas will be further improved or revised based on users’ feedbacks 
and testing data. In addition, this research discusses some pitfalls of using dummy 
locations and routes, but it is possible that there are more pitfalls not included. A 
thorough study of the risk needs to be given in the future. 
References 
1. Bindschaedler V, Shokri R (2016) Synthesizing plausible privacy-preserving location traces. In: 
Proceedings of the IEEE symposium on security and privacy (S&P 2016), San Jose, California 
2. Chow C-F, Mokbel MF, Liu X (2011) Spatial cloaking for anonymous location-based services 
in mobile peer-to-peer environments. GeoInformatica 15:351–380 
3. Chow C-Y, Mokbel MF (2011) Privacy of spatial trajectories. In: Zheng Y, Zhou X (eds) 
Computing with spatial trajectories. Springer, New York (pp 109–141) 
4. Chow C-Y, Mokbel MF (2011) Trajectory privacy in location-based services and data 
publication. SIGKDD Explor 13(1):19–29 
5. Deutsch A, Hull R, Vyas A, Zhao KK (2010) Policy aware sender anonymity in location based 
services. In: Proceedings of the 26th international conference data engineering (ICDE 2010), 
Long Beach, California, USA 
6. Vadakkethil E, Somanathan Pillai S (2023) Google Maps with privacy preservation. GitHub. 
https://github.com/sanjaikanth/GooleMapWithPrivacy. Accessed 21 Mar 2023

172
S. E. V. S. Pillai and W.-C. Hu
7. Hu W-C, Kaabouch N, Yang H-J (2016) Secure spatial trajectory prediction based on 
trafﬁc ﬂows. In: Proceedings of the 2016 IEEE international electro/information technology 
conference (EIT 2016), Grand Forks, North Dakota, USA 
8. Huang Q, Xu X, Chen H, Xie L (2022) A vehicle trajectory privacy preservation method based 
on caching and dummy locations in the internet of vehicles. Sensors 22(4423). https://doi.org/ 
10.3390/s22124423 
9. Jiang H, Li J, Zhao P, Zeng F, Xiao Z, Iyengar A (2021) Location privacy-preserving mech-
anisms in location-based services: A comprehensive survey. ACM Comput Surv (CSUR) 
54(1):1–36 
10. Lu H, Jensen CS, Liu ML (2008) PAD: Privacy-area aware, dummy-based location privacy in 
mobile devices. In: Proceedings of the 7th ACM international workshop on data engineering 
for wireless and mobile access (Mobide 2008), Vancouver, British Columbia, Canada 
11. Montazeri Z, Houmansadr A, Pishro-Nik H (2016) Deﬁning perfect location privacy using 
anonymization. In: Proceedings of the 2016 annual conference on information science and 
systems (CISS 2016). Princeton, New Jersey, USA (pp 204–209) 
12. Peng T, Liu Q, Wang G, Xiang Y, Chen S (2019) Multidimensional privacy preservation in 
location-based services. Futur Gener Comput Syst 93:312–326 
13. Shaham S, Ding M, Liu B, Dang S, Lin Z, Li J (2020) Privacy preservation in location-based 
services: a novel metric and attack model. IEEE Trans Mob Comput 20(10):3006–3019 
14. Sun G, Cai S, Yu H, Maharjan S, Chang V, Du X, Guizani M (2019) Location privacy preser-
vation for mobile users in location-based services. IEEE Access 7:87425–87438. https://doi. 
org/10.1109/ACCESS.2019.2925571 
15. Tang J, Zhu H, Lu R, Lin X, Li H, Wang F (2022) DLP: Achieve customizable location privacy 
with deceptive dummy techniques in LBS applications. IEEE Internet Things J 9(9):6969–6984. 
https://doi.org/10.1109/JIOT.2021.3115849 
16. Wang C, Xie Z (2018) Artiﬁcial impostors for location privacy preservation. arXiv:1801.06827 
[cs.SI] 
17. Wang S, Hu Q, Sun Y, Huang J (2018) Privacy preservation in location-based services. IEEE 
Commun Mag 56(3):134–140 
18. Wu Z, Wang R, Li Q, Lian X, Xu G, Chen E, Liu X (2020) A location privacy-preserving 
system based on query range cover-up or location-based services. IEEE Trans Veh Technol 
69(5):5244–5254 
19. Yao L, Lin C, Liu G, Deng F, Wu F (2012) Location anonymity based on fake queries in 
continuous location-based services. In: Proceedings of the 2012 7th international conference 
on availability, reliability and security (ARES 2012), Washington, DC (pp 375–382) 
20. Zhang A, Li X (2022) Research on privacy protection of dummy location interference for 
location-based service location. Int J Distrib Sens Netw 18(9). https://doi.org/10.1177/155013 
29221125111 
21. Zhang L, Qian Y, Ding M, Ma C, Li J, Shaham S (2019) Location privacy preservation based 
on continuous queries for location-based services. In: Proceedings of IEEE INFOCOM 2019– 
IEEE conference on computer communications workshops (INFOCOM WKSHPS) (pp 1–6) 
22. Zhang S, Li M, Liang W, Sandor VKA, Li X (2022) A survey of dummy-based location privacy 
protection techniques for location-based services. Sensors 22(6141):1–19. https://doi.org/10. 
3390/s22166141 
23. Zhang S, Wang G, Bhuiyan M, Liu Q (2018) A dual privacy preserving scheme in continuous 
location-based services. IEEE Internet Things J 5:4191–4200

Anomalous Node Detection in a Graph 
Considering Text Features 
Yeonju Song and Ki Yong Lee 
Abstract Recently, anomaly detection methods for graph nodes have been actively 
researched in various ﬁelds. Most of the previous studies designed models using graph 
neural networks (GNNs) to detect outliers. Anomaly detection for graph nodes aims 
to identify rare nodes that differ from the majority of nodes by learning the char-
acteristics of the entire graph or each node. This paper speciﬁcally focuses on the 
characteristics of social network graphs. Most of the existing outlier detection stud-
ies compare the usage patterns of regular users with those of anomalous users, or 
outliers. Graph-based methods propose more effective detection methods by utiliz-
ing the usage patterns of anomalous users. However, there is a limitation in that 
the characteristic information of each node is extracted using only structured data. 
This paper proposes a technique for detecting outlier nodes in a graph considering 
their text attributes. In this paper, an extended feature matrix is created that can 
more accurately predict outlier detection when learning the pattern of each node. 
Speciﬁcally, Word2vec is used to generate embedding vectors, which are then added 
to the extended feature matrix to evaluate the performance improvement of GNN. 
Word2vec is a natural language processing model mainly used for generating word 
embeddings. The transformed embedding vectors are added to the feature matrix to 
detect values that the existing model cannot restore well as anomalies. In this paper, 
virtual data consisting of mixed normal and anomalous data were used. As a result 
of applying the proposed method, it resulted in a high F1-score of 98% and an AUC 
of 99.3%. 
Keywords Graph neural network (GNNs) · Graph anomaly detection · Word2vec 
Y. Song (B) · K. Y. Lee 
Department of Computer Science, Sookmyung Women’s University, Seoul, Korea 
e-mail: eiwisi99@sookmyung.ac.kr 
K. Y. Lee 
e-mail: kiyonglee@sookmyung.ac.kr 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_12 
173

174
Y. Song and K. Y. Lee
1 
Introduction 
A graph is a data structure composed of nodes and edges that connect them, and is 
used to represent relationships between users in social networks or the structure of 
proteins [ 11]. 
Data outlier detection is widely used in inﬂuential ﬁelds such as security, ﬁnance, 
and medicine. Over the past few years, many techniques have been developed 
for detecting outliers in unstructured multidimensional sets, but with the recent 
widespread use of graph data, techniques for structured graph data have gained atten-
tion. Since objects in graphs have long-range correlations, anomaly detection meth-
ods for graph nodes have been actively researched in various ﬁelds [ 12]. Anomaly 
detection for graph nodes aims to identify rare nodes that differ from the majority 
of nodes by learning the characteristics of the entire graph or each node [ 10]. This 
paper speciﬁcally focuses on the characteristics of social network graphs. Most of 
the existing outlier detection studies compare the usage patterns of regular users with 
those of anomalous users, or outliers. Graph-based methods that utilize anomalous 
user patterns can lead to more effective detection. 
Spam is mostly aimed at spreading advertising content, so most of the attacks are 
in the form of one-to-many, such as email spam. They are usually found in online 
social network services that provide messaging services, such as Twitter, Facebook, 
and Instagram. This paper proposes a more efﬁcient detection method using graph-
based outlier detection. In the case of social network graphs, even if an abnormal 
actor tries to look like an ordinary user, their behavior patterns will inevitably be 
different because the purpose of using SNS is different from that of an ordinary 
user. When normal users form relationships with abnormal users such as spammers 
and fake reviewers on social networks, they can obtain unnecessary information. To 
automatically learn the characteristics of such nodes, graph neural networks (GNNs) 
aggregate attribute information from neighbors. Even for large-scale graph data, 
many node attributes can be quickly learned in an unsupervised manner. 
By utilizing the characteristics of the graph, it is possible to effectively learn the 
pattern of abnormal users. Abnormal users are likely to be unconnected or have few 
common neighbors with normal users. Additionally, anomalous users have different 
characteristics compared to the majority of all users. The results of outlier detection 
models can vary signiﬁcantly depending on which features are used as inputs. Since 
abnormal users mimic the patterns of normal users, they may have similar attributes. 
Therefore, capturing precise attribute that can properly extract anomalous user is an 
important and challenging problem. Recently, creating a model that classiﬁes nodes 
by embedding various user behavior information has not been extensively studied. 
Therefore, this paper proposes a method of embedding user information as a way 
to overcome the problem of which feature to use. The proposed method creates a 
GNN that can effectively learn both structured data and unstructured data, such as 
text, as each node’s attribute information. In this process, text data is transformed 
into embedding vectors using Word2vec, and then added to the feature matrix to 
detect values that the existing model cannot restore well as outliers.

Anomalous Node Detection in a Graph Considering Text Features
175
Word2Vec is a tool for vectorizing words, calculating relationships and exploring 
connections between words [ 7]. Depending on the type of input and output, it is 
roughly divided into two models: Continuous Skip-gram model and Continuous Bag 
of Words (CBOW). Word2vec converts words into word vectors and can vectorize 
text while maintaining the core semantic relationships of the sentence. Word2vec 
can distinguish the semantic similarity of short texts. 
In this paper, we conﬁrmed through virtual data experiments that using a feature 
matrix with added text can effectively detect anomalous users. The structure of this 
paper is as follows. Section 2 reviews related research, and Sect. 3 proposes an outlier 
detection technique that can effectively learn both structured and unstructured data. 
Section 4 presents the performance evaluation results, and the ﬁnal Sect. 5 concludes 
the paper. 
2 
Related Work 
As mentioned in Sect. 1, most of the existing research on anomaly detection for 
graph nodes focuses on extracting attribute information from each node using only 
structured data. However, there are some studies that use unstructured data. This 
section reviews research related to anomaly detection for graph nodes. 
2.1 
Anomaliy Detection 
Anomaly detection is a fundamental task in machine learning and data mining, with 
applications in a wide range of domains, such as intrusion detection, fraud detec-
tion, and medical diagnosis [ 2]. In general, anomaly detection aims to identify data 
instances that differ signiﬁcantly from the norm or the expected behavior. One of the 
key challenges in anomaly detection is the lack of labeled data, as the majority of the 
data is typically normal, and only a small portion is anomalous. This makes super-
vised learning approaches, which require labeled data, impractical. Distance-based 
methods and one-class classiﬁcation methods are popular unsupervised anomaly 
detection techniques that do not require labeled data. Another approach is one-class 
classiﬁcation methods, such as one-class SVMs [ 9], which are trained only on normal 
data (Fig. 1). 
In distance-based methods, a distance metric is used to evaluate the similarity 
between data points, and those data points that are too far from their nearest neighbors 
or cluster centers are considered anomalous. In one-class classiﬁcation, a model is 
trained on normal data to learn the boundaries of the normal region in the feature 
space, and any data point outside this region is considered anomalous. 
Deep neural networks have shown promise in anomaly detection, especially meth-
ods based on autoencoders [ 14], which can learn a compressed representation of the 
input data and use it to reconstruct normal data. If the reconstruction error is high,

176
Y. Song and K. Y. Lee
Fig. 1 A simple example of anomalies in a two-dimensional data set [ 2] 
the data point is considered anomalous. Despite the progress made in anomaly detec-
tion [ 8, 15], developing effective methods for complex and high-dimensional data 
remains a challenging task due to the difﬁculty in deﬁning the notion of normality 
and the large variation in anomaly types and patterns. 
2.2 
Graph Based Anomaly Detection 
Detecting anomalies in data is an essential task in many ﬁelds, such as healthcare, 
ﬁnance, and transportation systems. It can prevent potential threats, avoid ﬁnancial 
losses, and ensure public safety. Anomalies are patterns that differ signiﬁcantly from 
normal data or expected behavior. These patterns can be found in various forms, such 
as unusual data points, sequences, or behaviors. 
In relational data, anomalies can be interdependent, and detecting anomalous signs 
requires considering the relationships between data objects. By utilizing connections 
between related objects in a graph, it is possible to represent interdependency and 
capture the mutual dependence between them. Graph-based anomaly detection has 
gained increasing attention in recent years due to the widespread use of graph data. 
Recent research has focused on node-subgraph and node-node comparisons in 
graph-based anomaly detection. The proposed method in [ 1, 3] focuses on classifying 
which nodes are anomalous when a real graph is given. The applications of such

Anomalous Node Detection in a Graph Considering Text Features
177
methods are diverse, including network intrusion detection, social network analysis, 
and fraud detection. 
Anomaly detection is very useful for ﬁnding rare patterns or for data cleaning. 
Most anomaly detection algorithms focus on clouds of multi-dimensional points. 
However, in this paper, the focus is on extracting features from each node to train a 
graph neural network for detecting anomalous nodes in situations where data labels 
are not available. 
2.3 
Application of GCN Model 
Graph Convolutional Networks (GCN) have shown excellent performance in recent 
graph data research due to their powerful graph information collection capabilities 
[ 6]. GCNs use multilayer layers of nonlinear transformations to handle high-degree 
node interactions, making them effective for anomaly detection [ 3] Fig.  2. 
The proposed method in [ 3] extends the capabilities of GCNs for anomaly detec-
tion in attributed networks. An attributed network is a graph where nodes have associ-
ated features or attributes. The method leverages a graph convolutional autoencoder 
framework, which allows for the reconstruction of both the graph’s topological struc-
ture and the node attributes. 
The proposed framework consists of two main stages, encoding and decoding. 
In the encoding stage, the GCN is used to embed the input attributed network into 
a lower-dimensional space. In the decoding stage, the decoder is applied to recon-
struct both the topological structure and node attributes. Anomalies in the attributed 
network are detected by using the reconstruction error of the nodes after passing 
through the encoder and decoder stages. 
However, in this paper, the focus is on embedding node attributes using GCN on 
attributed networks. 
Fig. 2 Information Flow of Graph Convolutional Networks Framework [ 6]

178
Y. Song and K. Y. Lee
2.4 
Application of Attribute 
Jiang et al. [ 5] proposes an anomaly detection model based on GCN to detect abnor-
mal behavior. The proposed model constructs a feature matrix F for the nodes as input 
to GCN. To build each user’s behavior proﬁle within the network, 31 features are 
collected from user activities. This module not only extracts user’s behavior features 
such as email, web, ﬁle, log- on, and device but also extracts user’s content-based 
features using natural language processing. However, this method uses labeled data 
for each node and does not add embedding vectors itself as a feature matrix Fig. 3. 
Zhao et al. [ 13] proposes to utilize the pattern mining algorithms in GNN encoder 
learning as a graph anomaly detection technique. Speciﬁcally, the anomaly detection 
method described in this paper uses pattern mining to discover overall anomalous 
structures in given graph data and utilizes a graph neural networks (GNNs) to aggre-
gate information from local neighbors. In other words, the pattern mining algorithm 
is used to ensure that the representation learned by the GNN encoder includes both 
local neighbor information and overall structural characteristics. 
Zhao et al. [ 13] can be divided into two parts when constructing feature vectors. 
First, the location where each user wrote a blog post is represented as a one-hot vec-
tor. Then, singular value decomposition (SVD) is used to reduce the dimensionality 
of location features to 100. The next case converts the user’s hashtags into a fea-
ture vector by quantifying the frequency of word appearances using a bag-of-words 
approach. However, this method does not consider the order of words, and it is not 
about anomaly detection techniques that consider the sentences each user entered. 
In [ 4], a new anomaly detection framework is proposed to solve the problem of 
anomaly detection on attributed graphs in a cross-domain setting. Most previous 
studies have been conducted using unsupervised methods since the cost of obtaining 
anomaly labels is very high, especially for newly formed domains. This paper utilizes 
Fig. 3 The overall framework of deep anomaly detection on attributed networks [ 5]

Anomalous Node Detection in a Graph Considering Text Features
179
auxiliary information in labeled attribute graphs to make anomaly detection easier 
in unlabeled attribute graphs. 
Similar to [ 4, 13] the Bag of Words approach is also used in each user’s written 
reviews. It compares the similarity of the cosine, after representing all the features 
of the bag of words in each user’s review as a feature vector. This method also does 
not consider the order of words and focuses only on the frequency of occurrence. 
3 
Proposed Approach 
This chapter proposes an effective method for detecting anomalies in nodes. An 
extended feature matrix is created to more accurately predict outlier detection when 
learning the pattern of each node. Speciﬁcally, Word2vec is used to generate embed-
ding vectors, which are then added to the extended feature matrix to evaluate the 
performance improvement of GNN. A GCN with three convolution layers is used in 
the experiment. 
Figure 4 represents the overall process of building GCN, which consists of (1) 
Preprocessing of Training Data (2) GNN Generation, and (3) Threshold setting and 
anomaly detection, using GCN in the GNN generation. 
3.1 
Preprocessing of Training Data 
The proposed method converts the given text information into embedding vectors 
for each node before classifying the features of each node in the given graph using 
GNN. To do this, only the text data is extracted from each node’s given characteristic 
information. The proposed embedding model uses Word2vec, which is a natural lan-
guage processing model mainly used to create word embeddings. Word embedding 
is a technique that can represent individual words as vectors, discerns the similarity 
between words. 
In the experiment, the average of individual words was calculated to represent a 
given sentence. Figure 5 shows the structure of average Word2vec, where the input 
Fig. 4 The overall process of building GCN

180
Y. Song and K. Y. Lee
Fig. 5 The architecture of average Word2vec 
and output are one-hot vectors of words. Average Word2vec is a technique that uses 
the average of word embeddings of all the words given in a sentence as the numerical 
vector for a given sentence. This technique can obtain an extended feature matrix 
that reﬂects the given text information. 
3.2 
GNN Generation 
In the experiment of this paper, a graph convolutional network (GCN) is used. Figure 4 
represents the overall process of building GCN. GCN is a representative GNN model 
that applies convolutional neural networks (CNNs) for images to graph data by 
generalizing them. 
CNN, a traditional deep learning algorithm, can obtain good results when pro-
cessing Euclidean data, regular spatial structure data, such as images and sounds. 
However, most real-world graph data such as social networks, knowledge graphs, and 
market graphs have irregular spatial structures and cannot be represented as Euclidean 
data. Therefore, to overcome the limitations of previous models, the GCN model was 
proposed. Using the GCN model, data can be represented in the graph domain from 
the spatial domain, and this overcomes constraints of processing non-Euclidean data 
that CNN can’t handles. The GCN model can include not only individual attributes 
of nodes but also group information among nodes in the graph. 
To create the GCN model, the input graph upper G equals left parenthesis upper V comma upper E right parenthesisG = (V, E) is given. Where V is 
a set of N nodes, i.e., upper V equals StartSet v 1 comma v 2 comma period period period comma v Subscript n Baseline EndSetV = {v1, v2, ..., vn}, and E is a set of edges, i.e., upper E equals left brace left parenthesis v Subscript i Baseline comma v Subscript j Baseline right parenthesis vertical bar v Subscript i Baseline comma v Subscript j Baseline element of upper V right braceE =
 
(vi, v j)|vi, v j ∈V
 
. Let upper X element of double struck upper R Superscript upper N times upper FX ∈RN×Fbe the node feature matrix where the iith row 
upper X Subscript iXi represents the features of nodev Subscript ivi. Here,upper FF is the number of features of each node. 
upper X element of double struck upper R Superscript upper N times upper NX ∈RN×Nis the adjacency matrix where each element upper A Subscript i j Baseline equals 1Ai j = 1 if left parenthesis v Subscript i Baseline comma v Subscript j Baseline right parenthesis element of upper E(vi, v j) ∈E and 
upper A Subscript i j Baseline equals 0Ai j = 0 otherwise. The process of classifying the nodes of a given graph into normal 
users and anomalous users using GCN is as follows. After the matrix X and A rep-
resenting the information of the given graph are given, multiple graph convolutional 
layers are applied to extract the inherent features of each node. Each graph convo-
lutional layer collects information on the nodes connected to each node and updates 
the values of the feature matrix of that node. In this paper, three graph convolutional 
layers were performed to allow the feature matrix to contain more high-dimensional

Anomalous Node Detection in a Graph Considering Text Features
181
information. The following equation represents the operation performed in each 
graph convolutional layer. 
upper H Superscript left parenthesis l plus 1 right parenthesis Baseline equals sigma left parenthesis upper A upper H Superscript left parenthesis l right parenthesis Baseline upper W plus upper B right parenthesisH (l+1) = σ(AH (l)W + B)
(1) 
In the equation, H represents the feature matrix obtained in the Ith layer, A rep-
resents the adjacency matrix, W and B represent the weights and biases obtained 
through learning, and sigma represents the ReLU activation function. GCN is trained 
with the goal of minimizing the error between the initial input feature matrix and 
the ﬁnal matrix obtained after passing through all convolutional layers. The mean 
square error (MSE) is used as the loss function for this purpose. 
3.3 
Threshold Setting and Anomaly Detection 
After the model is trained, the nodes in the given graph are classiﬁed as normal or 
anomalous users. To achieve this, the proposed method in this paper considers values 
that are signiﬁcantly different from the original feature matrix as anomalous users 
learned from the GCN. In this case, the reconstruction error between the feature 
matrix of the given graph and the restored matrix is calculated. By calculating the 
reconstruction error, it is possible to determine whether it represents normal users 
or anomalous users based on its size. The reconstruction error value used in this 
paper uses mean square error. In this case, a threshold for the reconstruction error is 
needed to distinguish normal users from abnormal users. Since the threshold affects 
the performance of the anomaly detection system, setting the threshold is crucial. 
The proposed method in this paper sets the threshold of the percentile value of the 
calculated reconstruction error to 95. 
4 
Experiments 
The proposed method for anomaly node detection in this paper was implemented 
using Python 3.9.12 and PyTorch 1.12.1. The model training and algorithm execution 
were performed on a computer equipped with an NVIDIA GeForce RTX 2080 Ti, 
an Intel i9-9900K 3.60 GHz CPU, 64GB RAM, a 2TB SSD, and a 2TB HDD. 
In this experiment, a GNN was created to effectively learn both structured and 
unstructured data for each node’s characteristics. Unlike previous studies that only 
considered single structured data, this research also considers unstructured data for 
each node. To achieve this, this paper conducted experiments by creating virtual data. 
The speciﬁc method of generating virtual data is as follows. First, 1000 nodes were 
randomly created. Each node was constructed to share the same attribute information 
for normal users, and for abnormal users, they shared the attribute information of

182
Y. Song and K. Y. Lee
normal users in different proportions. The proportion of anomalous situations in the 
entire graph data is 5%. 
Each column that constitutes the features was speciﬁed to have four structured 
data, and text data was generated for each node. Speciﬁcally, all nodes have four 
structured data that make up the feature, in binary format. In the case of normal nodes, 
all of them have a value of 1, while in the case of anomalous nodes, they randomly 
mix 1 s and 0 s, creating a total of 10 patterns. Anomalous nodes are represented by 
ﬁve nodes that repeat each pattern.For the experiment, 1000 text data were generated. 
Text data are consisted of sentences, and among them, anomalies were constructed 
to include the same words other anomalies. Examining the connectivity of each 
node, normal nodes are typically connected to each other, while anomalous nodes 
are unlikely to be mutually connected and may not share common neighbors with 
normal nodes. Therefore, anomalous nodes tend to be connected to normal nodes in 
a one-way manner. The generated virtual data realistically depicts the situation when 
analyzing the network between nodes. 
Performance is evaluated using precision, recall, F1-score and AUC. F1-score 
is a key evaluation metric as it effectively represents the system’s performance by 
considering both precision and recall. 
Figure 6 and Table 1 show the comparison of the performance between the pro-
posed method and the method that uses a simple circular feature matrix. As shown in 
Fig. 6, the proposed method demonstrates a very high precision, recall, and F1-score 
Fig. 6 Performance improvement results 
Table 1 Performance improvement results 
Model
Precision
Recall
F1-score 
Existing method
0.90
0.88
0.89 
Proposed method
0.98
0.98
0.98

Anomalous Node Detection in a Graph Considering Text Features
183
Table 2 Performance improvement of AUC 
Model
AUC 
Existing method
0.90 
Proposed method
0.993 
of 98% by using word vectors as training data. In addition, Table 2 shows that the 
proposed method achieves an AUC value of 99.3%. Therefore, it is expected that the 
proposed method will be effective in detecting abnormal users than the method that 
only uses structured data as feature values. 
5 
Conclusion 
This paper proposes a technique for detecting outlier nodes in a graph considering 
their text attributes. The proposed method generates a GNN in a form that can effec-
tively learn both structured and unstructured data for each node by using an expanded 
feature matrix that includes word embedding vectors. Then, the reconstruction error 
between the matrix obtained from the trained GNN and the original feature matrix 
is then calculated. If the reconstruction error exceeds a threshold, it is detected as an 
anomalous user. The experimental results using virtual data show that the proposed 
method achieves a high F1-score of 98% and an AUC of 99.3%. 
Therefore, it is expected that the proposed method can effectively detect anoma-
lous users compared to the method that uses only structured data as feature values. 
Acknowledgements This research was supported by the MSIT (Ministry of Science and ICT), 
Korea, under the ICAN program (IITP-2023-RS-2022-00156299) supervised by the IITP. This 
research was also supported by the National Research Foundation of Korea(NRF) grant funded by 
the Korea government(MSIT) (No. NRF-2021R1A2C1012543). 
References 
1. Akoglu L, McGlohon M, Faloutsos C (2010) Oddball: spotting anomalies in weighted graphs. 
In: Proceedings of the 2010 SIAM international conference on data mining, pp 731–742 
2. Chandola V, Banerjee A, Kumar V (2009) Anomaly detection: a survey. ACM Comput Surv 
(CSUR) 41(3):15. http://scholar.google.de/scholar.bib?q=info:jAfBmk-9uAcJ:scholar.google. 
com/&output=citation&hl=de&ct=citation&cd=0 
3. Ding K, Li J, Bhanushali R, Liu H (2019) Deep anomaly detection on attributed networks. In: 
Proceedings SIAM international conference data mining, pp 594–602. SIAM, Philadelphia, 
PA, USA (2019) 
4. Ding K, Shu K, Shan X, Li J, Liu H (2021) Cross-domain graph anomaly detection. IEEE 
5. Jiang J, Zhang R, Guan J, Ji Y, Zhu Q, Guizani M (2019) Anomaly detection with graph 
convolutional networks for insider threat and fraud detection. In: Proceedings of IEEE military 
communications conference (MILCOM), pp 109–114. IEEE

184
Y. Song and K. Y. Lee
6. Kipf TN, Welling M (2017) Semi-supervised classiﬁcation with graph convolutional networks 
7. Mikolov T, Chen K, Corrado G, Dean J (2013) Efﬁcient estimation of word representations in 
vector space 
8. Pimentel MA, Clifton DA, Clifton L, Tarassenko L (2014) A review of novelty detection. Signal 
Process 99:215–249 
9. Schölkopf B, Williamson R, Smola A, Shawe-Taylor J, Platt J (1999) Support vector method for 
novelty detection. In: Proceedings of the 12th international conference on neural information 
processing systems, pp 582–588 (1999) 
10. Sudrich S, Borges J, Beigl M (2017) Graph-based anomaly detection for smart cities: a survey. 
In: 2017 IEEE SmartWorld, ubiquitous intelligence & computing, advanced & trusted com-
puted, scalable computing & communications, cloud & big data computing, internet of people 
and smart city innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), pp. 1–7. 
https://doi.org/10.1109/UIC-ATC.2017.8397570 
11. Wu Z, Pan S, Chen F, Long G, Zhang C, Yu PS (2021) A comprehensive survey on graph neural 
networks. IEEE Trans Neural Netw Learn Syst 32(1):4–24. https://doi.org/10.1109/tnnls.2020. 
2978386 
12. Zhao H, Wang Y, Duan J, Huang C, Cao D, Tong Y, Xu B, Bai J, Tong J, Zhang Q (2020) 
Multivariate time-series anomaly detection via graph attention network 
13. Zhao T, Jiang T, Shah N, Jiang M (2021) A synergistic approach for graph anomaly detection 
with pattern mining and feature learning. IEEE Trans Neural Netw Learn Syst 33(6):1–13 
14. Zhou C, Paffenroth RC (2017) Anomaly detection with robust deep autoencoders. In: Proceed-
ings of the 23rd ACM SIGKDD international conference on knowledge discovery and data 
mining, pp 665–674 
15. Zimek A, Schubert E, Kriegel HP (2012) A survey on unsupervised outlier detection in high-
dimensional numerical data. Stat Anal Data Min: ASA Data Sci J 5(5):363–387. https://doi. 
org/10.1002/sam.11161

Robust Federated Learning: A 
Heterogeneity Index Based Clustering 
Approach 
Papa Pene, Pu Tian, Weixian Liao, Qianlong Wang, and Wei Yu 
Abstract Federated learning (FL) has made possible the collaborative training of 
machine learning models between aggregation server and clients without sharing 
their private data. With the massive volume of heterogeneous data from various 
clients, the server faces challenges such as data unbalance, data corruption, and/or 
data irrelevancy. As a result, the FL setting is exposed to numerous security risks 
that lead to performance deterioration of learning effectiveness. To tackle the issue, 
in this paper we propose the Heterogeneity Index Based Clustering (HIC) approach, 
which enables the dynamic categorization of clients into clusters. Particularly, the 
model weights are dynamically clustered based on their heterogeneity level using an 
afﬁnity propagation method. The HIC approach uses a simple, but effective way of 
scaling data heterogeneity and dynamic clustering to create a resilient learning sys-
tem against backdoor attacks that outperforms the existing works on FL robustness. 
Our experimental results demonstrate that the clustering client’s weight based on 
their heterogeneity level decreases data unbalance and reduces attack success rate, 
increasing model performance, and encouraging clients’ contribution in FL. 
Keywords Robust federated learning · Data heterogeneity · Afﬁnity propagation 
1 
Introduction 
Federated Leaning (FL), as a distributed machine learning paradigm, enables the 
collaboration of a group of agents to collaboratively conduct a learning model train-
ing, while preserving the data privacy [ 1– 3]. Federated learning is viable distributed 
machine learning framework that can be applied to different smart-world systems, 
supported by the advancement of rapidly growing networking and computing tech-
niques (Internet of Things, edge computing, etc.) [ 4– 8]. Privacy of local data is 
essential in FL, which explains a decentralized server setting compared to other tra-
ditional distributed machine learning (ML) that are centralized. ML has been applied 
P. Pene · P. Tian · W. Liao (B) · Q. Wang · W. Yu 
Department of Computer and Information Sciences, Towson University, Towson, USA 
e-mail: wliao@towson.edu 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_13 
185

186
P. Pene et al.
numerous industries, making them smarter and efﬁcient. With the exploding data, the 
ML models nowadays can learn complex patterns among the data, which is insight-
ful knowledge of the system, and perform complicated tasks (e.g., jointed design 
of complex systems, resource allocation and optimization) [ 9– 16]. Nonetheless, this 
learning process has raised numerous privacy concerns among clients in various 
systems. 
As a result, FL has brought a disruptive touch to the world of Artiﬁcial Intelligence 
(AI) by teaching machines while keeping their local data privacy preserved. However, 
the privacy consideration creates a certain opacity between the server and the clients 
data since this ﬁrst sees clients as black-boxes. By only receiving model updates 
from clients, the FL server to a certain extent is vulnerable to backdoor attacks [ 3, 
10, 17, 18]. In FL, a backdoor attack consists of implanting some features inside the 
model during the training so that misclassiﬁcation can be triggered. An illustration 
of misclassiﬁcation is making an FL model that has the mission of classifying digits 
from 1 to 9 to classify digit 1 s as 7 s. During the training samples, adversaries 
sneakily inject an interference in 1 s and make the sever mislabel them as 7 s [ 19]. 
Backdoor injections are carried out furtively avoiding any negative impacts on the 
performance of main tasks, which makes them harder to detect and counter. As a 
result, the FL setting is exposed to a variety of security risks (backdoor attacks, etc.) 
and its performance can further deteriorate posted by threats. 
Defending against attacks such as backdoor threats has compelled researchers to 
improve robustness in FL aggregation methods. There is a variety of robust aggre-
gation methods as defensive strategies in the literature, which are built on federated 
averaging (FedAvg). Generally speaking, FedAvg is one of the most essential algo-
rithms utilized in the FL process [ 1]. The early robust aggregation methods mainly 
focus on preventing convergence attacks. Such attacks are categorized as untargeted 
attacks [ 20]. Studies in FL have been done in either independent and identical distri-
bution (IID) or non-independent and identical distribution (non-IID) settings. Most 
of the works about robustness have been done in an IID setting. Nonetheless, non-IID 
setting reﬂects more of the reality as an agent local dataset is not a representative 
one for reﬂecting the whole population distribution. 
Despite the existence of substantial works on building a robust FL framework, 
to the best of our knowledge, none of them has fully exploited the impact of data 
heterogeneity. There are divided perceptions about data heterogeneity in the research 
community since some see it as a curse while others as a redemption. Among the 
curses of data heterogeneity in FL, the server faces numerous challenges in its col-
laboration with various clients such as data imbalance, data corruption, and/or data 
irrelevancy. In addition, some clients are not fully participating during the sample 
training and will beneﬁt from the aggregated model. Such a lack of participation 
leads to worsen the data heterogeneity issue as some important clients may reluctant 
to contribute during model training. 
Recently, data heterogeneity has been considered as a viable way of improving 
FL robustness. It is realized that the heterogeneous traits of data in FL can reduce 
the efﬁciency of adversarial attacks, prevents adversaries from foreseeing attack 
outcomes, and makes attack plans much harder to design and deploy. Based on

Robust Federated Learning: A Heterogeneity Index.. . .
187 
this, in this paper, we design a new approach, called Heterogeneity Index Based 
Clustering (HIC), to help enhance the attack resistance in FL. Initially, we focus on 
calibrating data heterogeneity and attributing a measure to it by carefully adjusting 
the data heterogeneity index (HI) [ 21]. Then, we associate the quantiﬁed level of 
heterogeneity for each client with the cosine similarity mechanism to create dynamic 
clusters. In addition, we consider a non-IID setting in the system. We introduce 
injection attacks on selected datasets and measure the Injection Success Rate (ISR). 
Furthermore, we evaluate the resilience of the various clusters against the attacks 
with respect to their heterogeneity level. Our experimental results conﬁrm that clients 
with similar heterogeneity levels provide more stability and robustness, and the HIC 
approach outperforms the representative techniques based on the standard FedAvg. 
The key contributions of this paper are summarized as follows: 
• We give the semantics and syntax of the HIC language that supports the relevancy 
of the process. It is an innovative feature of generating dynamic clusters based on 
client’s heterogeneity while conserving the essential of FL is elaborated. 
• We design a system of data heterogeneity quantiﬁcation and use it with afﬁnity 
propagation to inhibit model perturbation from data heterogeneity, and improve 
client’s distributions and participation in a non-IID setting. 
• Via extensive simulations, we demonstrate that our HIC framework is resilient 
towards backdoor attacks with higher performances than the existing methods. 
The remainder of the paper is organized as follows: In Sect. 2, a brief review 
of relevant research efforts is conducted. In Sect. 3, the preliminary materials are 
introduced. In Sect. 4, our proposed approach is presented in detail. In Sect. 5, the  
evaluation results are presented and discussed. Finally, a summary of the paper with 
some future research perspectives is given in Sect. 6. 
2 
Related Work 
Federated Learning: Important research and progress have been carried out in 
FL that prioritizes privacy and robustness. The particularity of FL compared to 
other traditional ML scheme is the decentralized setting of the data before its 
distribution to agents [ 22, 23]. Progresses made on the privacy aspect is critical 
because the conﬁdentiality of the agents’ data is the essence of FL. In the FL 
paradigm, the gradients that participating agents share are locally derived. To this end, 
Zhu et al. [ 24] recently proposed a scheme, which is capable of recovering the con-
tributed data fully given its gradient. In addition, valuable development has been 
carried out to improve the robustness of FL. Research efforts such as [ 1] made the 
progress to ameliorate communication efﬁciency. Furthermore, the algorithms such 
as Stochastic Gradient Descent (SGD) have been considerably optimized since the 
data in FL is usually unbalanced. Note that optimizing the SGD algorithm provides 
better results and makes the concept more understandable.

188
P. Pene et al.
Robustness in Federated Learning: In FL, robustness consist of preserving 
the performance of the model. This model integrity can be impacted by different 
factors (data instability, data alterations, adversarial attacks, etc.) to induce its out-
performance. Various schemes have been designed for a more robust FL scheme. 
This robustness relies on the capability of scheme to overcome adversaries and with-
stand corrupted data. Robust aggregation is the strategy that has been developed and 
used to keep the integrity of FL settings [ 20]. In fact, existing efforts [ 20, 25, 26] 
made extensive progresses in ﬁnding and developing robust aggregation methods 
(coordinate-wise median, alphaα-trimmed mean, etc.). 
Sun et al. [ 27] demonstrated that FedAvg could be robust against backdoor attacks 
in some settings when weight-clipping and noise addition are combined [ 28]. In 
their settings, server veriﬁes whether the update’s standard is above a threshold M 
and clips it if that is the case. The clipped updates are aggregated, and a Gaussian 
noise is added later. Afterwards, a robust method named per-client learning rate was 
designed [ 29]. This approach assigns to each agent its own learning rate alpha Subscript k Superscript tαt
k, where 
tt is the time and kk the client. Fung et al. [ 29] leveraged cosine similarity to update 
the analogous directions. Recently, research efforts [ 19] extended the study in [ 30] 
to craft a simple but effective FL defensive method against backdoor attacks called 
Robust Learning Rate (RLR). Bernstein et al. [ 30] designed a scheme that allows 
agents to internally update their models using the sign of aggregation sent by the 
sever while the RLR strategy tunes the learning rate of the aggregation server in each 
iteration and dimension as the sign information of agent updates. RLR approach 
greatly minimizes the degradation of validation accuracy and the backdoor accuracy. 
Data Heterogeneity: A predominant but often unheeded aspect when it comes 
to robustness in FL is data heterogeneity [ 31– 35]. The heterogeneous nature of data 
in FL affects the efﬁciency in different ways. The heterogeneous feature of the data 
draws a key challenge in FL in comparison with other conventional distributed ML. 
It is part of the disadvantage of data heterogeneity to result in overﬁtting of benign 
agents during the local training. In fact, adversaries can exploit this curse as a cam-
ouﬂage to mislead FL defense, which relies on distribution skewness. Consequently, 
it is difﬁcult to clearly distinguish adversarial updates from normal heterogeneous 
ones. To this end, Zhao et al. [ 36] established the dissimilarity between the weights 
of agent’s local model affected by data heterogeneity. Sattler et al. [ 37] carried out 
extensive experiments, showing how heterogeneity impacts the performance of mod-
els in FL. Likewise, Ozdayi et al. [ 19] claimed that the result obtained from their RLR 
method to defend against backdoor attacks could be affected by data heterogeneity 
because the highest performances are obtained in the non-IID setting. Nonetheless, 
the impact level of data heterogeneity on FL robustness has been overlooked in all 
these existing efforts. 
Zawad et al. [ 21] conducted research on how data heterogeneity affects the robust-
ness of FL, and the results showed that depending on the level of data heterogeneity, 
the effect of backdoor attacks could vary considerably. Nonetheless, this work is 
mostly limited to identifying the curse and redemption of data heterogeneity. Now, 
the need of pushing this investigation about data heterogeneity further is imminent 
to utilize its power to build a robust defensive strategy for FL. Consequently, we

Robust Federated Learning: A Heterogeneity Index.. . .
189 
Table 1 Table of notation
left parenthesis w right parenthesis(w)
Weight or global parameter vector 
f Subscript k Baseline left parenthesis dot right parenthesis fk(·)
Loss function of a clientkk
f left parenthesis dot right parenthesis f (·)
Global Loss function 
l left parenthesis dot right parenthesisl(·)
Loss function for an instance 
w Subscript t Superscript kwk
t
The weight parameter for nodekk
w Subscript t plus 1 Superscript kwk
t+1
FedAvg weight for the new round 
n Subscript knk
total number of samples of a clientkk
etaη
Learning rate 
D,upper D Subscript kDk
Global and Local datasets 
thetaθ
Cosine distance 
upper H upper IH I
Heterogeneity Index 
proceed, in this work, on clustering clients’ model weight based on their hetero-
geneity level to minimize the security risks and bad performance caused by data 
heterogeneity in FL. 
3 
Preliminaries 
In a more elaborated approach, FL establishes a cooperative training of models via 
a succession of protocols between an aggregation server and a set of agents. During 
the communication between the server and the clients, each entity has to play its part 
in an utmost manner. In Table 1, we can ﬁnd all key notations used in the paper. 
The server’s role is to train a classiﬁerf left parenthesis w right parenthesis f (w), withleft parenthesis w right parenthesis(w) being the global parameter 
vector, by ensuring communication efﬁciency and privacy of local data at agents. 
During FL process, the average of loss is minimized by active agents, also known as 
clients, determined by 
arg min Underscript w epsilon upper R Superscript d Baseline Endscripts f left parenthesis w right parenthesis equals StartFraction 1 Over k EndFraction sigma summation Underscript k equals 1 Overscript k Endscripts f Subscript k Baseline left parenthesis w right parenthesis comma arg min
wϵRd f (w) = 1
k
k
∑
k=1
fk(w),
(1) 
where f Subscript k Baseline left parenthesis w right parenthesis fk(w) is the loss function of the kth agent. 
In a non-IID setting, we setup a FL model, in which a learning model is jointly 
trained in a multi-round (t = 0, 1, 2, …) protocol executed by an aggregation server 
and a set of K clients. For each client also referred as node k = 1, 2, …,  K, we allow 
the access to a local dataset upper D Subscript kDk. The loss function f Subscript k fk in Eq. (1) is the empirical risk 
minimization under a loss function of a speciﬁc instance l denoted by the following, 
f Subscript k Baseline left parenthesis w right parenthesis approaches the limit StartFraction 1 Over n Subscript k Baseline EndFraction sigma summation Underscript j equals 1 Overscript n Subscript k Baseline Endscripts l left parenthesis bold x Subscript bold j Baseline bold comma bold y Subscript bold j Baseline comma w right parenthesis comma fk(w) .= 1
nk
nk
∑
j=1
l(xj, yj, w),
(2)

190
P. Pene et al.
where the total number of samples inkk’s dataset isn Subscript k Baseline equals StartAbsoluteValue upper D Subscript k Baseline EndAbsoluteValuenk = |Dk| with (k epsilon nkϵn) andbold left parenthesis bold italic x Subscript j Baseline comma bold italic y Subscript j Baseline bold right parenthesis(x j,y j)
being, respectively, jth sample’s feature vector and label. The global loss function of 
the entire dataset denoted as StartAbsoluteValue bold upper D EndAbsoluteValue equals sigma summation Underscript k equals 1 Overscript n Endscripts StartAbsoluteValue upper D Subscript k Baseline EndAbsoluteValue|D| = ∑n
k=1 |Dk| is shown as 
f left parenthesis w right parenthesis approaches the limit StartFraction sigma summation Underscript k equals 1 Overscript k Endscripts StartAbsoluteValue upper D Subscript k Baseline EndAbsoluteValue f Subscript k Baseline left parenthesis w right parenthesis Over StartAbsoluteValue bold upper D EndAbsoluteValue EndFraction period f (w) .=
∑k
k=1 |Dk| fk(w)
|D|
.
(3) 
A protocol in FL begins with a sampling of a subset of agents upper S Subscript tSt by a server,  
which also sends the model weights w Subscript twt to the same agents at a round t. Then, there 
is an initialization and a training of the transmitted model performed by the kth 
agent to have the resulting weight w Subscript t Superscript kwk
t using some iterative method such as SGD. A 
local update computation Delta Subscript t Superscript k Baseline equals w Subscript t Superscript k Baseline minus w Subscript tΔk
t = wk
t −wt is performed by the agent before sending 
it back to the server. In return, the server will proceed to the computation of the next 
round’s weights via the aggregation function bold g colon upper R Superscript StartAbsoluteValue upper S Super Subscript t Superscript EndAbsoluteValue asterisk d Baseline right arrow upper R Superscript dg : R|St|∗d →Rd to obtain the new 
weight w Subscript t plus 1 Baseline equals w Subscript t Baseline plus eta dot bold g colon bold left parenthesis StartSet Delta Subscript bold t Baseline EndSet bold right parenthesiswt+1 = wt + η · g : ({∆t}). Here StartSet Delta Subscript t Baseline EndSet{Δt} is equal to union Subscript k epsilon upper S Sub Subscript t Baseline Delta Subscript t Superscript k∪kϵStΔk
t . Typically, in FL, 
this mentioned aggregation rule is referred as the FedAvg expressed as follows: 
w Subscript t plus 1 Baseline equals w Subscript t Baseline plus eta StartFraction sigma summation Underscript k epsilon upper S Subscript t Baseline Endscripts n Subscript k Baseline dot Delta Subscript t Superscript k Baseline Over sigma summation Underscript k epsilon upper S Subscript t Baseline Endscripts n Subscript k Baseline EndFraction periodwt+1 = wt + η
∑
kϵSt nk · Δk
t
∑
kϵSt nk
.
(4) 
Concretely, the server manages a validation dataset on which aforementioned proto-
cols will loop to the point, in which the appropriate accuracy or statistics are obtained 
for the model. 
4 
Our Approach: A HIC Based Clustering Algorithm 
In this section, our approach is described in detail. In the following, we ﬁrst give the 
design rationale of our approach. We then explain the process of our threat model 
that is a backdoor attack. We close this section with a detailed description of the 
minimum loss occurring during this type of threat model. 
4.1 
Design Rationale 
The key idea behind our mechanism is to mitigate security risk and performance 
deterioration of models in FL. The HIC algorithm exploits the data heterogeneity 
factors to offer a more robust FL framework. It is undeniable that aggregated mod-
els are greatly impacted by the heterogeneity between clients, which explains the 
pertinence of our approach. Here, the rationality of our designed model follows the 
logic of inciting the server to cluster clients with the same kind of heterogeneity 
level. At ﬁrst, we quantify the level of heterogeneity of each client. Then, via some

Robust Federated Learning: A Heterogeneity Indexellipsis. . .
191 
computations, we explained in Sect. 4.4, clients with similar statistical heterogene-
ity are clustered together. For example, during the training of the CelebA 1 dataset, 
we establish four clusters with a respective interval level of heterogeneity being 
left bracket 0 comma 0.25 right bracket comma left parenthesis 0.25 comma 0.5 right bracket comma left parenthesis 0.5 comma 0.75 right bracket comma[0, 0.25], (0.25, 0.5], (0.5, 0.75], and left parenthesis 0.75 comma 1 right bracket(0.75, 1]. The cosine distance clients’ weight 
(w Subscript i Superscript t Baseline comma w Subscript j Superscript twt
i, wt
j) of each model is associated with their heterogeneity level. Furthermore, 
the afﬁnity propagation technique is used to distribute them in the right cluster for 
training. We also perform some injection attacks to see clusters with what range of 
heterogeneity produce more resilient model. 
The workﬂow of our approach is described in Fig. 1. As seen in the ﬁgure, we have 
the overview of Sever - Clients communications during HIC model training session. 
We can observe that the general FL protocols are followed with the exception of 
clustering and adjustment of model’s heterogeneity until aggregation is reached. On 
the client-side, the local datasetupper D Subscript kDk for each client {1 comma 2 comma 3 comma ellipsis comma k1, 2, 3, . . . , k } is split into two 
types of data, benign and malicious. Selected clients will receive the model global 
parameter w Subscript twt for a local update before sending the new weight w Subscript t Superscript kwk
t . On the server-
side, the ﬁrst step is the client selection, in which the sever creates a model that is 
sent to a number of clients. These last ones will send back the locally updated weight 
associated with theirupper H upper I left parenthesis c right parenthesisH I (c). Following that step, Fig. 1 put on view the HIC session, 
in which the clustering happens. Now depending on whether aggregation is reach 
or not, models are either broadcast for the upcoming round with an updated weight 
w Subscript t plus 1wt+1 or sent for the next round of selection. The HIC also incorporates the FedAvg 
methods, which is further used inside the clusters for model training. 
4.2 
Backdoor Attacks 
FL paradigm is mostly susceptible to poisoning attacks. The two primary poisoning 
attacks are performance degradation, where adversaries try to weaken the accuracy of 
the model, and backdoor attacks, which is as furtive as possible. Our work focuses on 
the second type of poisoning attacks. In FL, malicious agents convey backdoor attacks 
to lead model to a misclassiﬁcation of a set of targeted samples. In this direction, 
Chen [ 38] established how misclassiﬁcation is caused by attacks by inserting a pattern 
in training samples from one source class and change their label to another target 
class. The process leading to misclassiﬁcation follows stealthy steps avoiding any 
impact on the performance. The attack approach taken by [ 39] focuses on creating 
efﬁcient trigger patterns in decentralized setting and colluding adversaries. Now, with 
the decentralization of training data, the aggregation server is strictly vulnerable to 
model updates. Therefore, backdoor attacks are conducted through model poisoning, 
which consist of exhibiting the backdoor once aggregated with other updates [ 40]. 
For example, the weight update of the kth client is carried out using SGD, 
w Subscript t plus 1 Superscript k Baseline equals left parenthesis w Subscript t Superscript k Baseline minus eta nabla f Subscript k Baseline left parenthesis w Subscript t Superscript k Baseline right parenthesis right parenthesis commawk
t+1 = (wk
t −η∇fk(wk
t )),
(5)
1 The full description of the dataset and source can be found in Sect. 5. 

192
P. Pene et al.
Fig. 1 Overview of Federated HIC in a Non-IID setting 
wherenabla f Subscript k Baseline left parenthesis w Subscript t Superscript k Baseline right parenthesis∇fk(wk
t ) represents the gradient forw Subscript t Superscript kwk
t of the loss functionf left parenthesis dot right parenthesis f (·). Adversaries 
use this opportunity to replace the global model. They substitute the new global 
model w Subscript t plus 1 Superscript kwk
t+1 in Eq. (5) with a corrupted one. 
Note  that  the work in [  21], when investigated the curses and redemption of data 
heterogeneity, classiﬁed the dominant factor of data heterogeneity against the effec-
tiveness of backdoor attacks as a redemption. Even if a number of research efforts 
show that data heterogeneity might have an impact on the defense strategy, none of 
them conducted substantial studies of the subject. Thus, it is essential to reveal how 
the same attack model can have different effects on the same type of sub-populations 
depending on a unique feature. We focus on analyzing FL backdoor attack and loss 
minimization in the lens of data distribution clustered based on their heterogeneity 
level. 
4.3 
Attacks with Minimum Loss 
With backdoor as the attack strategy, the goal of adversaries is to poison the model 
without being detected. Corrupting the joint global model leads to the insufﬁciency 
of weight optimization. With loss of generality, we assume that all the upper KK learning 
nodes are organized in a non-IID setting, keeping the same hyper-parameters with 
the identical deep neural network (DNN) structure. As shown in Fig. 1, the selection 
of data during each iteration is conducted from the local dataset upper D Subscript kDk and the dataset

Robust Federated Learning: A Heterogeneity Indexellipsis. . .
193 
of each node k is divided into two equal batches: one benign and the other mali-
cious. Nodes send their updated model to the sever for aggregation. Upon receiving 
the initial update model with the heterogeneity index, the proposed HIC algorithm 
clusters the learning models with the derived values using cosine distance. Since the 
objective is to build a robust method that minimizes performance degradation, the 
server optimizes the global weight, which minimizes the loss function for each node 
represented by 
w Subscript t plus 1 Baseline equals arg min Underscript w epsilon upper R Superscript d Baseline Endscripts left brace f Subscript k Baseline left parenthesis w Subscript t Superscript k Baseline right parenthesis right brace periodwt+1 = arg min
wϵRd{ fk(wk
t )}.
(6) 
Here, denote f left parenthesis dot right parenthesis f (·) as the loss function and w Subscript t Superscript kwk
t as the weight parameter for node k. 
4.4 
A HIC-Based Algorithm 
We now introduce the detail of the HIC approach. First, to measure the impact level 
of data heterogeneity against backdoor attacks, we quantify the heterogeneity index 
by following [ 21], 
upper H upper I left parenthesis c right parenthesis equals 1 minus StartFraction 1 Over upper C Subscript m a x Baseline minus 1 EndFraction dot left parenthesis c minus 1 right parenthesis commaH I (c) = 1 −
1
Cmax −1 · (c −1),
(7) 
where c allows the adjustment of the number of classes per agent, and upper C Subscript m a xCmax is the 
overall number of classes within the dataset. A data normalization is performed with 
0 being the lowest level of data heterogeneity and 1 being the highest. 
Algorithm 1 Heterogeneity index based clustering 
1: Initialize the global weight wt at t = 0 
2: Set threshold δ = 2% for H I  (c) Eq. (7) 
3: Distribute w0 → k ϵ [K ] 
4: while Clients ¬ converge do 
5:
Receive local updates wt 1, wt 2, wt 3, …,  wt k 
6:
Select random benchmark j ϵ [1, n] 
7:
Empty distance list θ = [ ]  
8:
for k ϵ {1, 2, 3, …,  n} do 
9:
Calculate θk = CosineDistance (H I  k 
t , H I  j 
t ) 
10:
Update θk to θ 
11:
end for 
12:
Cluster List = AfﬁnityPropagation (θ) 
13:
for li ϵ List do 
14:
Aggregate partially for nodes with li for wli 
t+1 
15:
Send aggregated wli 
t+1 to node in li 
16:
end for 
17:
Calculate the average global weight wt+1 
18:
Update wt → wt+1 
19: end while

194
P. Pene et al.
In Algorithm 1, the HIC follows the similar process as a typical FL. Prior to the 
distribution of the global weight to the selected clients, this one is ﬁrst initialized. At 
roundt equals 0t = 0, the global weight parameter is initialized asw 0w0 and shared with selected 
clients kk (Line 1 and 2). On the client’s side, upon receiving the initial weight, the 
HI is computed using Eq. (7) and assigned to thek normal t normal hkth client’s model. Then, the client 
kk trains the received model via SGD and ends up with a new weight w 0 Superscript kwk
0. After  a  
computation of its update, the client sends the updated weight Delta Subscript 0 Superscript k Baseline equals w 0 Superscript k Baseline minus w 0∆k
0 = wk
0 −w0 and 
itsupper H upper IH I back to the server. Then, we have the iterative aggregation on the server side, 
in which the clustering process based on the weight and HI is executed. Finally, the 
average global weight is calculated and updated for new client selection. 
Clients with similar heterogeneity index are clustered together. Then, inside each 
cluster, nodes collaboratively train distinct models with the standard FedAvg algo-
rithm. As shown in Line 9, the model’s weight is associated with the computed cosine 
distance of the heterogeneity index for each sub-population of the dataset. A hetero-
geneity index value associated with other parameter of cosine similaritythetaθ of the node 
kk. theta Subscript kθk is appended to thetaθ after each iteration when kk epsilonϵ StartSet 1 comma 2 comma 3 comma ellipsis comma n EndSet{1, 2, 3, . . . , n}. Subsequently, 
to effectively estimate the number of clusters and properly spread them, we use a 
dynamic clustering mechanism. Line 12 indicates the use of afﬁnity propagation 
(AP) clustering [ 41]. The AP clustering algorithm is able to determine the number of 
target clusters without a predeﬁned value, which offers a great beneﬁt for federated 
learning with an unknown number of clients before the training process. 
5 
Performance Evaluation 
Extensive evaluation has been conducted to conﬁrm the efﬁcacy of our HIC approach. 
In the following, we ﬁrst present the methodology (including the experiment setup 
and procedure), and then evaluate the obtained results. 
5.1 
Methodology 
We use an open-source for FL bench-marking LEAF [ 42]. LEAF 2 provides six 
datasets that a number of existing works in the literature have used to replicate 
data heterogeneity. In Table 2, we have the statistics of three LEAF’s datasets [ 42]. 
We exploit CelebA, an image dataset built on a large scale of celebrity faces with 
two classiﬁcation tasks (smiling /not smiling). We also use the Federated Extended 
MNIST (FEMNIST) dataset, which is an image dataset with 62 distinct classes of 
image classiﬁcation task. Another dataset used in this experiment is Sentiment140 
(Sent140), which is a generated sentiment analysis dataset from twitter users with a 
positive or negative class. We list our dataset conﬁgurations in Table 2.
2 LEAF: https://github.com/TalwalkarLab/leaf. 

Robust Federated Learning: A Heterogeneity Indexellipsis. . .
195 
Table 2 Statistics of datasets in LEAF 
Dataset
Number of users
Total samples
Samples per user 
Mean
Stdev. 
CelebA
9,343
200,288
21.44
7.63 
FEMNIST
3,550
805,263
226.83
88.94 
Sent140
660,120
1,600,498
2.42
4.71 
Initially, we replicate the same data distribution as [ 42] which is a non-IID one. 
We partition the data as follows: 60% of the data for training, and 40% for validation. 
For the CelebA, 10% of the overall users is subsampled with the rate of 0.001 and 
a batch size of 5. For the FEMNIST experiment, we subsample 5% of the data. 
The model used for both CelebA and FEMNIST is same and has two convolutional 
neural network (CNN) and two dense layers. The used learning rate for the FEMNIST 
experiment is 0.004 for the FedAvg and 0.006 for the batch size. For the Sent140 
experiment, logistic regression is used with words model, and a learning rate 0.0003. 
For the hyperparameters, we conserve the ones used in the Leaf Framework. The 
objective of these experimental procedures is to conﬁrm the efﬁcacy of the proposed 
HIC approach on reducing the negative effect of data heterogeneity on FL robustness. 
Thus, we ﬁrst run of a standard FL process for a number of rounds R, in which each 
dataset is partitioned as benign and malicious among K clients. After sampling these 
distributed datasets among different clients, we derive the value of their heterogeneity 
index upper H upper I left parenthesis c right parenthesisH I (c) associated with their weight bold italic w Subscript t Superscript bold kw k
t . 
Next, we establish a system of measurement of the heterogeneity level to allow 
their regulation as needed during the experimentation. The regulation of the hetero-
geneity level of each cluster is carried out by adjusting upper C Subscript m a xCmax for each client, but we 
keep the thresholddelta equals 2 percent signδ = 2% as default. We then apply the trigger on half of the dataset 
and assess the global model on it using backdoor injection. In this experimentation, 
we consider an injection attack successful if the classiﬁcation result matches the 
targeted one and the injection success rate (ISR) is evaluated relatively to the Het-
erogeneity Index average of each cluster [ 21]. We deliberately lead the classiﬁer to 
misclassify malicious samples as target class for the backdoor attack. For that, we 
include a trigger string on 50% of the Sent140 dataset for the Natural Language Pro-
cessing (NLP) and append additional patterns on 50% of the samples in the CelabA 
and FEMNIST datasets. 
5.2 
Experimental Results 
We ﬁrst train the datasets by applying the standard FedAvg in one case and the 
HIC algorithm in another one. Without running any injection attacks, we allow the 
distribution of the training data to the participating clients. During each round, the 
selected weights are clustered based on their level of heterogeneity. In Fig. 2, we

196
P. Pene et al.
Fig. 2 Performance of 
FedAvg versus HIC 
(FEMNIST) 
can see in blue and red the respective evolution curves of the FedAvg and HIC 
accuracy during the training. Figure 2 reﬂects the convergence of the accuracy for 
both algorithms after 100 rounds. The graph curve in red shows a better performance 
than the one in blue right after the5 normal t normal h5th round. We also notice a smoother convergence 
model for the red curve after the 40 normal t normal h40th round in opposite to the blue curve. 
The resulting graph from the ﬁrst experiments encourage us to push the experi-
mentation further. Consequently, we run another experiment to see the effectiveness 
of the HIC algorithm in comparison with the one with FedAvg alone. The acquired 
results is listed on Table 3. It displays a net amelioration of HIC algorithm compared 
to FedAvg after a number of rounds. We limit the number of iterations to 30, but we 
can observer an improvement of the performance up to 6.55%. Similarly, in Fig. 3, 
the behavior of the HIC accuracy curves is starting to highly increase after 30 rounds 
and shows a higher performance than the standalone FedAvg. 
Then, we execute another experiments where injection attacks are applied on half 
of the clients during each round. All evaluations are executed 10 times for each cluster 
of all three datasets and the ISR on each cluster is measured. The results displayed 
on Fig. 3 show on box-and-whisker plots that those attacks are less successful on 
Table 3 Accuracy comparison (FEMNIST) 
Comm. rounds
FedAvg (%)
HIC (%)
Improvement (%) 
1
78.67
81.35
2.68 
5
83.59
86.95
3.36 
10
80.77
82.92
2.15 
20
83.05
87.27
4.22 
30
83.16
89.71
6.55

Robust Federated Learning: A Heterogeneity Indexellipsis. . .
197 
(a) CelebA
(b) FEMNIST
(c) Sent140 
Fig. 3 Injection Success Rate vs Heterogeneity Level 
(a) FedAvg
(b) FedAvg with HIC 
Fig. 4 FedAvg with HIC during Injection Attack 
clusters with higher heterogeneity level. The success attack rate gradually decreases 
as the level of heterogeneity approaches to 1. We also note that the thickness of boxes 
in almost each cluster for all datasets is bigger as heterogeneity increase in Fig. 3. The  
results conﬁrm our initial hypothesis about the efﬁcacy of attacks that decrease as 
the level heterogeneity increases. Nonetheless, it is important to note that the marge 
of increment in the interval of heterogeneity level between 0.6 and 0.9 is very low 
with an ISR close to 0.5. 
The box-and-whisker plots show that our attack effectiveness signiﬁcantly reduces 
as heterogeneity increase. Still, we need to show the performance of our general 
approach outside the clusters. In Fig. 4, we can see the training data plot via uniform 
sampling of the FEMNIST dataset. Here, we have the curve of the Backdoor task that 
classiﬁes digit 1 (base class) with plus pattern as 7 (target class). The training curves 
for FedAvg are displayed in Fig. 4a and for HIC in Fig. 4b, respectively. Figure 4a 
displays a high level of accuracy of the injection attacks. When HIC is associated 
with standard FedAvg, the accuracy of these injection drops consistently as Fig. 4b

198
P. Pene et al.
illustrates. With this improved performance and a good impact on backdoor attacks 
demonstrated by the success rate of backdoor attack related to the heterogeneity level, 
the needs for heterogeneity clustering becomes imminent to improve robustness in 
FL. 
We now provide a deeper analysis on why the empirical results obtained in the 
previous subsection support the efﬁcacy of the HIC approach. The ﬁrst challenge 
brought by data heterogeneity we wanted to tackle is data unbalance. In Fig. 2, we  
can see a HIC performance curve in red that is steadier than the FedAvg curve in 
blue after 40 rounds. This outcome demonstrates that isolated clients with similar 
heterogeneity level can help minimize disturbance in FL. It is also important to note 
that afﬁnity propagation has helped for a smarter convergence of weights into clusters 
to facilitate the stability of models. This stability of the model leads us to the second 
challenge which is the performance. 
As aforementioned, it is part of the HIC’s objective to avoid any performance 
degradation. Results obtained during the experimentation phase shows a higher per-
formance of the HIC in compared to the standalone FedAvg. This greater performance 
of HIC performance can be explained by the dynamic clusters formed. Indeed, the 
HIC clusters help reduce the noise from irrelevant client updates for a better opti-
mization of the global model. Nagalapatti et al. [ 43] demonstrated how irrelevant 
updates could negatively impact the stability of the model that further affects the 
performance of the aggregated model. Therefore, by reducing disturbance during 
model optimization, the HIC method implicitly ameliorate the performance of this 
one. 
Additionally, part of the HIC approach’s main goal is to build a robust FL frame-
work against backdoor attacks. We manually map the base class instance with another 
feature to the target class. Evoking the threat model explains in Sect. 4, we understand 
that adversaries replace the client’s updatew Subscript t plus 1 Superscript kwk
t+1 in Eq. (5) with a new malicious global 
model upper M equals w Subscript t plus 1 Superscript kM = wk
t+1. Recall that backdoor attacks are performed in a stealthy way to 
avoid any detection, so adversaries ignoring the upper H upper I Subscript t Superscript kH I k
t while updating the malicious 
weight become easier to be detected. Therefore, more benign updates are selected 
and the detected malicious ones are discarded. Figure 4 shows that our approach 
was able to mitigate the inﬂuence malicious tasks with a higher number of honest 
ones. Injection attacks considerably decrease as the number of iterations increase in 
HIC, whereas in Fig. 4, we notice a higher ratio of Injection attack accuracy. The 
box-and-whisker graphs indicates disturbance increases as heterogeneity level does 
and it affects the stability attacks. Also, graphs in Fig. 2 shows how attack efﬁcacy 
signiﬁcantly decrease to go under 0.5 as heterogeneity becomes higher than 0.6. This 
regression of the rate of attack success seen in the HIC outperforms the one seen in 
related works.

Robust Federated Learning: A Heterogeneity Indexellipsis. . .
199 
6 
Final Remarks 
In this article, we have investigated the issue of robustness in FL under impacts of 
both data heterogeneity and backdoor attacks. Existing research efforts dealing with 
FL robustness have been overlooking the impact of data heterogeneity. To address the 
issue, we have proposed a Heterogeneity-based Clustering algorithm. This scheme 
tends to quantify the level of data heterogeneity of each client and associate it with 
their local model weights during the FL protocol. The HIC clusters allow the server 
to optimize weight selection by hampering the noise from unbalance data induced 
by data heterogeneity. The obtained results from experiments have validated the 
resiliency of the HIC approach against backdoor attacks. Now, one can wonder how 
would the HIC react against some other sophisticated adversaries such as attacks 
that use label ﬂipping. To tackle this issue, our ongoing research considers the use 
of the Shapley Value in the HIC framework to enable a smarter client selection with 
a detection method for corrupted label. 
Acknowledgements This material is based upon work in part supported by the Air Force Ofﬁce of 
Scientiﬁc Research under award number FA9550-20-1-0418. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material are those of the author(s) and do not necessarily 
reﬂect the views of the United States Air Force. 
References 
1. McMahan B, Moore E, Ramage D, Hampson S, Arcas BA (2017) Communication-efﬁcient 
learning of deep networks from decentralized data, pp 1273–1282 
2. Li Q, Wen Z, Wu Z, Hu S, Wang N, Li Y, Liu X, He B (2021) A survey on federated learning 
systems: vision, hype and reality for data privacy and protection. IEEE Trans Knowl Data Eng 
1 
3. Chen Z, Tian P, Liao W, Yu W (2021) Towards multi-party targeted model poisoning attacks 
against federated learning systems. High-Conﬁd Comput 1(1):100002 
4. Tian P, Liao W, Yu W, Blasch E (2022) WSCC: a weight-similarity-based client clustering 
approach for non-iid federated learning. IEEE Internet Things J 9(20):20 243–20 256 
5. Chen Z, Liao W, Hua K, Lu C, Yu W (2021) Towards asynchronous federated learning for 
heterogeneous edge-powered internet of things. Digit Commun Netw 7(3):317–326 
6. Xu H, Yu W, Grifﬁth D, Golmie N (2018) A survey on industrial internet of things: a cyber-
physical systems perspective. IEEE Access 6:78 238–78 259 
7. Liang F, Yu W, Liu X, Grifﬁth D, Golmie N (2020) Toward edge-based deep learning in 
industrial internet of things. IEEE Internet Things J 7(5):4329–4341 
8. Liang Y, Cai Z, Yu J, Han Q, Li Y (2018) Deep learning based inference of private information 
using embedded sensors in smart devices. IEEE Netw 32(4):8–14 
9. Hatcher WG, Yu W (2018) A survey of deep learning: platforms, applications and emerging 
research trends. IEEE Access 6:24 411–24 432 
10. Liang F, Hatcher WG, Liao W, Gao W, Yu W (2019) Machine learning for security and the 
internet of things: the good, the bad, and the ugly. IEEE Access 7:158 126–158 147 
11. Xu H, Liu X, Yu W, Grifﬁth D, Golmie N (2020) Reinforcement learning-based control and 
networking co-design for industrial internet of things. IEEE J Sel Areas Commun 38(5):885– 
898

200
P. Pene et al.
12. Liang F, Yu W, Liu X, Grifﬁth D, Golmie N (2022) Toward deep q-network-based resource 
allocation in industrial internet of things. IEEE Internet Things J 9(12):9138–9150 
13. Qian C, Yu W, Lu C, Grifﬁth D, Golmie N (2022) Toward generative adversarial networks for 
the industrial internet of things. IEEE Internet Things J 9(19):19 147–19 159 
14. Balkus SV, Wang H, Cornet BD, Mahabal C, Ngo H, Fang H (2022) A survey of collaborative 
machine learning using 5g vehicular communications. IEEE Commun Surv Tutor 24(2):1280– 
1303 
15. Al-Garadi MA, Mohamed A, Al-Ali AK, Du X, Ali I, Guizani M (2020) A survey of machine 
and deep learning methods for internet of things (IoT) security. IEEE Commun Surv Tutor 
22(3):1646–1685 
16. Liang F, Qian C, Yu W, Grifﬁth D, Golmie N (2022) Survey of graph neural networks and 
applications. Wirel Commun Mob Comput 
17. Chen Z, Tian P, Liao W, Yu W (2021) Zero knowledge clustering based adversarial mitigation 
in heterogeneous federated learning. IEEE Trans Netw Sci Eng 8(2):1070–1083 
18. Xiong Z, Cai Z, Takabi D, Li W (2022) Privacy threat and defense for federated learning with 
non-i.i.d. data in aiot. IEEE Trans Ind Inform 18(2):1310–1321 
19. Ozdayi MS, Kantarcioglu M, Gel YR (2021) Defending against backdoors in federated learning 
with robust learning rate. Proc AAAI Conf Artif Intell 35(10):9268–9276. https://ojs.aaai.org/ 
index.php/AAAI/article/view/17118 
20. Blanchard P, El Mhamdi EM, Guerraoui R, Stainer J (2017) Machine learning with adversaries: 
byzantine tolerant gradient descent. Adv Neural Inf Process Syst 30 
21. Zawad S, Ali A, Chen P-Y, Anwar A, Zhou Y, Baracaldo N, Tian Y, Yan F (2021) Curse 
or redemption? How data heterogeneity affects the robustness of federated learning. In: Proc 
AAAI conference artiﬁcial intelligence 
22. Dean J, Corrado G, Monga R, Chen K, Devin M, Mao M, Ranzato M, Senior A, Tucker P, 
Yang K et al (2012) Large scale distributed deep networks. Adv Neural Inf Process Syst 25 
23. Chen Z, Liao W, Tian P, Wang Q, Yu W (2022) A fairness-aware peer-to-peer decentralized 
learning framework with heterogeneous devices. Future Internet 14(5). https://www.mdpi.com/ 
1999-5903/14/5/138 
24. Zhu L, Liu Z, Han S (2019) Deep leakage from gradients. Adv Neural Inf Process Syst 32 
25. Guerraoui R, Rouault S et al (2018) The hidden vulnerability of distributed learning in byzan-
tium. International conference on machine learning. PMLR 2018:3521–3530 
26. Li T, Sanjabi M, Beirami A, Smith V (2020) Fair resource allocation in federated learn-
ing. In: International conference on learning representations. https://openreview.net/forum? 
id=ByexElSYDr 
27. Sun Z, Kairouz P, Suresh AT, McMahan HB (2019) Can you really backdoor federated learn-
ing?. In: 2nd international workshop on federated learning for data privacy and conﬁdentiality, 
NeurIPS 
28. Geyer RC, Klein T, Nabi M (2017) Differentially private federated learning: a client level 
perspective. In: NIPS workshop 
29. Fung C, Yoon CJ, Beschastnikh I (2018) Mitigating sybils in federated learning poisoning. 
arXiv:1808.04866 
30. Bernstein J, Zhao J, Azizzadenesheli K, Anandkumar A (2019) signSGD with majority vote 
is communication efﬁcient and fault tolerant. https://openreview.net/forum?id=BJxhijAcY7 
31. Ghosh A, Chung J, Yin D, Ramchandran K (2020) An efﬁcient framework for clustered fed-
erated learning. Adv Neural Inf Process Syst 33:19 586–19 597 
32. Li T, Sahu AK, Zaheer M, Sanjabi M, Talwalkar A, Smith V (2020) Federated optimization in 
heterogeneous networks. Proc Mach Learn Syst 2:429–450 
33. Fallah A, Mokhtari A, Ozdaglar A (2020) Personalized federated learning with theoretical 
guarantees: a model-agnostic meta-learning approach. Adv Neural Inf Process Syst 33:3557– 
3568 
34. Wang J, Liu Q, Liang H, Joshi G, Poor HV (2020) Tackling the objective inconsistency problem 
in heterogeneous federated optimization. Adv Neural Inf Process Syst 33:7611–7623

Robust Federated Learning: A Heterogeneity Indexellipsis. . .
201 
35. Pang J, Huang Y, Xie Z, Han Q, Cai Z (2021) Realizing the heterogeneity: a self-organized 
federated learning framework for iot. IEEE Internet Things J 8(5):3088–3098 
36. Zhao Y, Li M, Lai L, Suda N, Civin D, Chandra V (2018) Federated learning with non-iid data. 
arXiv:1806.00582 
37. Sattler F, Wiedemann S, Müller K-R, Samek W (2019) Robust and communication-efﬁcient 
federated learning from non-iid data. IEEE Trans Neural Netw Learn Syst 31(9):3400–3413 
38. Chen B, Carvalho W, Baracaldo N, Ludwig H, Edwards B, Lee T, Molloy I, Srivastava B (2018) 
Detecting backdoor attacks on deep neural networks by activation clustering. arXiv:1811.03728 
39. Xie C, Huang K, Chen P-Y, Li B (2019) Dba: distributed backdoor attacks against federated 
learning. In: International conference on learning representations 
40. Bhagoji AN, Chakraborty S, Mittal P, Calo S (2019) Analyzing federated learning through an 
adversarial lens. In: International conference on machine learning. PMLR, pp 634–643 
41. Frey BJ, Dueck D (2007) Clustering by passing messages between data points. Science 
315(5814):972–976 
42. Caldas S, Duddu SMK, Wu P, Li T, Koneˇcn`y J, McMahan HB, Smith V, Talwalkar A (2018) 
Leaf: a benchmark for federated settings. arXiv:1812.01097 
43. Nagalapatti L, Narayanam R (2021) Game of gradients: mitigating irrelevant clients in federated 
learning. Proc AAAI Conf Artif Intell 35(10):9046–9054

Evaluation of Improvement Plans 
to Increase the Efﬁciency of Performance 
Data Collection/Transfer for Server 
Systems 
Chika Iiyama, Akira Hirai, Mari Yamaoka, Naoto Fukumoto, 
and Masato Oguchi 
Abstract In recent years, the demand for the shared use and the use of distributed 
processing of many servers has increased. To analyze the performance data in real-
time, it is necessary to reduce the overhead of data collection and transfer and handle 
them efﬁciently. Therefore, the objective of this study is to develop an efﬁcient method 
based on the results of measuring resource usage and overhead during data collection 
at the target server and data transfer to the server for data analysis. In this report, 
we examine the effect of parallelizing the transfer process as an efﬁcient method 
to reduce CPU load, which is one of the bottlenecks in data transfer processing. In 
addition, we will analyze the impact of the parallelized transfer process and CPU 
load benchmarks by running them concurrently. 
Keywords Time-series data · Performance data collection · Data transfer · Data 
compression · CPU load factor · Parallelization 
C. Iiyama (B) 
Humanities and Sciences Advanced Science, Ochanomizu University, Tokyo, Japan 
e-mail: chika@ogl.is.ocha.ac.jp 
A. Hirai · M. Yamaoka · N. Fukumoto 
Computing Laboratory, Fujitsu Limited, Tokyo, Japan 
e-mail: ahirai@fujitsu.com 
M. Yamaoka 
e-mail: yamaoka.mari@fujitsu.com 
N. Fukumoto 
e-mail: fukumoto.naoto@fujitsu.com 
M. Oguchi 
Department of Information Sciences, Ochanomizu University, Tokyo, Japan 
e-mail: oguchi@is.ocha.ac.jp 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_14 
203

204
C. Iiyama et al.
1 
Introduction 
There is an increasing demand for shared use and distributed processing use of 
many servers, including cloud computing environments. In such an environment, 
load balancing and tuning of systems and applications require a method to collect 
performance data including the lower layers of each server with low overhead, and 
to analyze and present the data in real time. However, the data size of time-series 
data such as Linux performance data used in such an environment is relatively large, 
and the overhead of handling such data may be high, and thus an efﬁcient method 
for handling such data is required. 
Therefore, we measured the resource usage and overhead during data collection 
and transfer, assuming an environment in which the server performing data collection 
and the server performing analysis are split. As a result, we found that the transfer 
processing delay increased with the number of cores in the method of transferring 
information from multiple cores using a single core. Therefore, in this report, we 
parallelized the transfer process from one CPU core to each CPU core, evaluated 
and compared the performance, and investigated ways to improve the delay. Fur-
thermore, to verify the impact of the performance information collection process on 
the operating environment, we analyzed the impact of the parallelized data transfer 
process on the benchmark performance running simultaneously on the same CPU 
core and the impact on the data transfer process. 
2 
Related Research 
Tools whose performance data to be collected are similar to those in this study include 
scalable performance measurement infrastructure for parallel codes (Score-P) and 
visualization and analysis of MPI resources (Vampir). The Score-P [ 1] tool can 
mainly analyze the performance of parallel HPC applications. The Vampir [ 2] tool 
allows integrated analysis and visualization of proﬁle data and intercommunication 
data from many compute nodes. These tools collect/analyze/visualize the perfor-
mance data collected at each node on a ﬁle basis after the application is executed, 
which is different from the method of real-time collection and analysis of runtime 
performance data as time-series data that this study aims for. In a previous study 
[ 3], a method was proposed to convert proprietary CPU performance data from the 
supercomputer K computer and others into a general-purpose data format so that it 
can be handled by Score-P and Vampir as described above. Although the content this 
prior study is trying to analyze is similar to that in this study, it is difﬁcult to collect, 
transfer, and analyze data at the same time. 
A highly relevant research area for this study, including overhead reduction meth-
ods for data collection, is a technique called distributed tracing. It can analyze the 
detailed behavior of software based on microservice architecture, which is widely 
used in cloud environments. The difference is that distributed tracing mainly focuses

Evaluation of Improvement Plans to Increase the Efﬁciency.. . .
205 
on the application as the target domain to collect and analyze performance data, 
whereas this research focuses on the lower layers. A Google’s 2010 paper [ 4] led  
to the development of Twitter’s Zipkin [ 5] and Uber’s Jeager [ 6]. Each one of these 
tools has been released as Open Source Software (OSS) and is used in many cloud 
infrastructures. 
A previous study [ 7], which is similar to this study in its method of capturing 
and analyzing proﬁle data in a database, describes the method of collecting and 
analyzing proﬁle data performed at Google’s data center. In another prior study [ 8], 
the workload of the data center is analyzed using the above methodology. Proﬁle data 
collection is performed randomly from thousands of servers and the information 
is used for application tuning as well as performance monitoring and application 
deployment (afﬁnity) optimization. This research aims to collect/transfer/analyze 
low-layer performance data such as CPU and OS using/improving OSS. 
3 
Experiment 
3.1 
Assessment Environment 
Two servers, one for data collection (hereinafter referred to as “collection server”) and 
the other for data analysis (hereinafter referred to as “analysis server”), were prepared 
for construction of the environment. An overview of the experimental environment 
is shown in Fig. 1. 
For the collection server, we used perf [ 9], a standard event data collection/tracing 
framework for Linux Kernel. Dhrystone and Whetstone from UnixBench [ 10] and 
7-Zip [ 11] were used as CPU-loading performance benchmarks. For the analysis 
server, we used InﬂuxDB [ 12] as a time series DB to manage the large amount of time 
series data collected. InﬂuxDB is explained in [ 13]. Data generated on the collection 
server were transferred using a data transfer program and stored in InﬂuxDB on the 
analysis server. For the data to be transferred, we used the perf record data at the 
stress [ 14] runtime that was collected in advance. The environments of the collection 
Fig. 1 Overview of the experimental environment

206
C. Iiyama et al.
Table 1 Collection server environment 
Model
Fujitsu PRIMERGY CX2550 M1 
CPU
Intel(R) Xeon(R) CPU E5-2697 
v3 @ 2.60 GHz (times×2 CPU)  
Number of cores
14 
Memory
128 GB 
OS
CentOS 7 
Table 2 Analysis server environment 
Model
Dell PowerEdge R620 
CPU
Intel(R) Xeon(R) CPU E5-2603 
v2 @ 1.80 GHz (times×1 CPU)  
Number of cores
4 
Memory
16 GB 
OS
CentOS 7 
Table 3 Versions of the used technologies 
perf
3.10.0-1160.45.1.el7.x86_64.debug 
UnixBench
5.1.3 
7-Zip
22.01 
InﬂuxDB
1.8.3.x86_64 
and analysis servers are listed in Tables 1 and 2, respectively, and the version of each 
used technology is presented in Table 3. 
3.2 
Experiment Summary 
The background of the experiment is shown in Fig. 2. A common method for collect-
ing performance data for each CPU core is Linux perf, which is widely used. In perf, 
in the case of a multi-CPU conﬁguration, data from all cores are stored in a table in 
shared memory, and then collected/output by one core at a time. In the experimen-
tal in our previous report [ 15], in accordance with the above format, we conducted 
analyses such as the overhead measurement when data from all cores are transferred 
by one core. As a result, it was found that the method of transferring information 
from multiple cores with a single core cannot keep up with the transfer process as 
the number of cores increases, and it is necessary to improve the efﬁciency of the 
transfer process. Therefore, in this experiment, as one of the efﬁciency improvement 
methods, we used parallelization in which each core’s information is transferred by

Evaluation of Improvement Plans to Increase the Efﬁciency.. . .
207 
Fig. 2 Background of the experiment 
Fig. 3 DataFrame format data sample 
its own core. Furthermore, to investigate the impact of the data transfer process on 
programs running on the same CPU core, we simultaneously ran performance bench-
mark programs that applied CPU load, and measured and analyzed the changes in 
benchmark performance and the delay in the data transfer process. 
The ﬂow of converting the collected CPU performance data into a format that 
can be transferred to InﬂuxDB and then transferring the data is shown below. First, 
only the necessary data (time, process ID, thread ID, and execution address) are 
extracted from the collected data. Because the collected data are the data for 1 s 
of one CPU core, 10,000 records of data are extracted if the data are collected in 
units of 100 muµs. At this time, there are 4 items of 64-bit data each, so the transfer 
data size for one CPU core is approximately 320 KB. Next, the extracted data are 
stored in a DataFrame format using Python Pandas [ 16]. A sample of the data stored 
in DataFrame format is shown in Fig. 3. Finally, these data are transferred to the 
analysis server using the Python Client module of InﬂuxDB. The above module has 
an API (Inﬂuxdb.DataFrameClient) [ 17] for writing Pandas DataFrame format data 
to the InﬂuxDB server, which is used this time.

208
C. Iiyama et al.
3.3 
Transfer Processing Parallelization 
3.3.1
Experimental Details 
To verify the effect of parallelization, in which each core’s information is transferred 
by its own core, the following experiments 1 through 4 were measured. In Experiment 
1, the data transfer time was measured while changing the number of CPU cores. In 
Experiment 2, the CPU load ratio of the data transfer processing part was measured. 
In Experiment 3, the CPU load ratio of InﬂuxDB was measured. In Experiment 
4, the I/O load of InﬂuxDB was measured. Hereafter, the case in which data from 
all CPU cores to be transferred is transferred by a single core is described as “the 
one-core transfer version” and the case in which information from each CPU core is 
transferred by its own core is described as “the parallel transfer version” 
3.3.2
Experimental Results 
The results of Experiment 1 are shown in Figs. 4 and 5. The average transfer time 
for each number of CPU cores in Fig. 5 is listed in Table 4. 
From Fig. 4, it can be read that the one-core transfer version requires more than 
1 s to transfer data for 8 or more cores. Meanwhile, the parallel transfer version can 
transfer data for 28 cores in 0.51 s, as can be observed in Table 4, indicating that 
the effect of parallelization has been achieved. However, according to Fig. 5, for  
each CPU core number n, the data transfer time of the parallel transfer version is 
longer than 1/n of the one-core transfer version, suggesting that parallelization has 
an overhead. For example, at nequals= 8, the average transfer time of the parallel transfer 
version (0.24 s) is longer than 1/8 of the one-core transfer version (0.15 s), which 
Fig. 4 Data transfer time for one-core transfer version

Evaluation of Improvement Plans to Increase the Efﬁciency.. . .
209 
Fig. 5 Data transfer time for parallel transfer version 
Table 4 Average transfer time for each number of CPU cores in Fig. 5 
nequals= 1
0.17 s 
nequals= 2
0.17 s 
nequals= 4
0.19 s 
nequals= 8
0.24 s 
nequals= 14
0.31 s 
nequals= 16
0.34 s 
nequals= 28
0.51 s 
is approximately 1/5. In the parallel transfer version, it can be read that there is a 
variation in transfer time among the cores executing the transfer, and as the number 
of CPU cores increases, the variation also increases at the same time. One reason for 
this is that the InﬂuxDB side may not be able to keep pace with processing when the 
number of cores increases and the CPU load rate and iowait for disk writes on the 
InﬂuxDB side increase. 
To analyze the parallelization overhead in more detail, experiments 2–4 were 
conducted with n equals= 14 CPU cores. This is because we assumed that the overhead 
due to parallelization would be easier to see if the number of cores was as large as 
possible, and the number of cores per CPU is 14, as shown in Table 1. 
The measurement results of the CPU load ratio for the data transfer processing 
portion of Experiment 2 are presented in Table 5. 
The CPU load ratio and data transfer time for each core at nequals= 14 for the parallel 
transfer version are extracted and the results are shown in Fig. 6. Each point in the 
ﬁgure represents a core. It can be read that there is variation in data transfer time 
for each core, and that the longer the data transfer time is, the lower the CPU load 
ratio (Idle is increasing). In addition to the transfer processing time, the time the 
relevant process was allocated to the CPU (task-clock) was obtained by using the

210
C. Iiyama et al.
Table 5 CPU load rate of the data transfer processing part 
One-core transfer version
50.3 (%) 
Parallel transfer version
Average
31.6 
Max
40.0 
Min
23.6 
Fig. 6 CPU load ratio of the 
data transfer processing part 
(parallel transfer version, n 
equals= 14 CPU cores) 
perf function to collect the CPU load ratio, which was calculated using the following 
formula. 
CPU load ratioequals= (time that the transfer processing portion of the relevant process 
was allocated to the CPU)/(transfer processing time) * 100 (%) 
We implemented the above CPU load ratio collection in the Python code by using 
a library (module) called libpfm4 [ 18], which is used to call the perf function from 
Python. 
The measured CPU load ratio of InﬂuxDB in Experiment 3 is shown in Fig. 7. In  
the parallel transfer version, it can be read that all four cores on the analysis server 
on which InﬂuxDB is running are at approximately 30% at the same timing. Given 
that the transfer time at n equals= 14 CPU cores is approximately 0.32 s, the CPU load 
factor during the process is approximately 100 %. It can be inferred that this is a 
CPU bottleneck and a factor in the variation. 
The measured I/O load of InﬂuxDB in Experiment 4 is shown in Fig. 8. In the  
parallel transfer version, it can be read that the analysis server running InﬂuxDB 
is processing approximately 2.74 MB of writes per second. It is assumed that the 
compressed transfer data are being written at once at this timing, but disk performance 
is measured by the ﬁo benchmark [ 19], which has a random write performance of 
1280 MB/s. Thus, it is unlikely that this is a disk bottleneck.

Evaluation of Improvement Plans to Increase the Efﬁciency.. . .
211 
Fig. 7 CPU load ratio of 
InﬂuxDB (parallel transfer 
version) 
Fig. 8 I/O load of InﬂuxDB 
(parallel transfer version) 
3.4 
Benchmark Simultaneous Operation 
3.4.1
Experimental Details 
To examine the impact of data transfer processing on programs running on the same 
CPU core and the impact of the programs on transfer processing, the following 
experiments 5–7 were conducted. In experiments 5–6, we used UnixBench, a CPU 
load benchmark. In Experiment 5, we used dhry2reg, one of UnixBench’s test cases, 
which performs integer arithmetic operations. In Experiment 6, we used whetstone-
double, one of the UnixBench test cases, which performs ﬂoating-point arithmetic 
operations. Experiment 7 used 7-Zip, which compresses and decompresses 7z format 
ﬁles, as a benchmark. 
An overview of simultaneous operations is shown in Fig. 9. On each CPU core 
of the collection server, the data transfer program transfers the data of its own core 
for 1 s, which was acquired in advance, to InﬂuxDB every second. In addition, a 
benchmark is running simultaneously on each CPU core.

212
C. Iiyama et al.
Fig. 9 Overview of simultaneous operations 
Fig. 10 Comparison of benchmark values(dhry2reg) 
3.4.2
Experimental Results 
The results of Experiment 5 with Dhrystone (dhry2reg) are shown in Figs. 10 and 
11. It can be read that running the dhry2reg benchmark and the data transfer pro-
gram simultaneously degraded the benchmark value by 7–8% and increased the data 
transfer time by a factor of 1.17–1.31 compared to when each was run alone. 
The results for the Whetstone (whetstone-double) in Experiment 6 are shown in 
Figs. 12 and 13. It can be read that by running the whetstone-double benchmark and 
the data transfer program simultaneously, the benchmark value is almost no worse

Evaluation of Improvement Plans to Increase the Efﬁciency.. . .
213 
Fig. 11 Comparison of data transfer time(dhry2reg) 
Fig. 12 Comparison of benchmark values(whetstone-double) 
than that when each was run independently, but the data transfer time is increased 
by a factor of 1.13 to 1.31. 
The results of Experiment 7 with 7-Zip are shown in Figs. 14 and 15. It can be 
read that running the 7-Zip benchmark and the data transfer program simultaneously 
degraded the benchmark value by approximately 7%–8% and increased the data 
transfer time by a factor of approximately 1.04–1.38 compared to when each was 
run alone. 
The degradation rate of the benchmark values and the growth rate of the data 
transfer time are compared for each benchmark in Figs. 16 and 17, respectively. 
From Fig. 16, it can be observed that the degradation rate differs for each benchmark 
for each number of CPU cores n. The degradation rate for each benchmark does not

214
C. Iiyama et al.
Fig. 13 Comparison of data transfer time(whetstone-double) 
Fig. 14 Comparison of benchmark values (7-Zip) 
change when the number of CPU cores n is increased. This indicates that the impact 
on the benchmark value is independent of the number of CPU cores n, although 
the presence or absence and degree of impact vary from benchmark to benchmark. 
Figure 17 shows that the data transfer time increased for all benchmarks used in this 
study. It can also be conﬁrmed that the growth rate decreased as the number of CPU 
cores n increased. In addition to this, Experiments 1–4 showed that in the case of 
single execution, the data transfer time increased (idle time during transfer increased) 
due to the CPU load on the InﬂuxDB side caused by the increase in the number of 
CPU cores n in this experimental environment. From these results, it can be inferred 
that when the number of CPU cores n is increased in concurrent execution, the timing

Evaluation of Improvement Plans to Increase the Efﬁciency.. . .
215 
Fig. 15 Comparison of data transfer time(7-Zip) 
Fig. 16 Comparison of the degradation rate of benchmark values 
of transfer is affected by the benchmark program and varies, and the CPU load on 
the InﬂuxDB side is equalized in this experimental environment, and thus the effect 
of the increase in the number of CPU cores n on the data transfer time appears 
to be lower. The above results show that transfer processing is affected regardless 
of the benchmark, but the impact on the transfer process tends to decrease as the 
number of CPU cores n increases. However, this trend is presumably dependent on 
the performance of the InﬂuxDB server.

216
C. Iiyama et al.
Fig. 17 Comparison of the growth rate of data transfer time 
4 
Conclusion and Future Work 
In this report, we evaluated parallelized transfer, in which data from each core are 
transferred by its own core, as a method to improve the efﬁciency of data transfer 
assuming an environment in which the server performing data collection and the 
server performing analysis are divided. In addition, parallelized transfer processes 
and CPU-intensive benchmarks were run concurrently to analyze the impact on each. 
Experiment 1, which measured the data transfer time while changing the number of 
CPU cores, showed that data transfer for 14 cores required 2.12 s with the one-
core transfer version, while it required only 0.31 s with the parallel transfer version, 
indicating that transfer time can be reduced. Experiment 1 and Experiment 2, which 
measured the CPU load ratio of the data transfer processing portion, showed that 
the data transfer time varied from core to core, with the variation increasing as the 
number of cores increased. From Experiment 3, which measured the CPU load ratio of 
InﬂuxDB, it was inferred that the CPU load ratio of InﬂuxDB during processing was 
100%, indicating a CPU bottleneck. From Experiment 4, which measured the I/O load 
of InﬂuxDB, it was found that the amount of write operations of InﬂuxDB had enough 
room to spare and was not a bottleneck on the disk side. Experiments 5–7, which 
measured the impact on benchmarks and the impact on transfer processing, showed 
that the degree of mutual inﬂuence between data transfer programs and benchmarks 
varied from benchmark to benchmark. It was also inferred that the inﬂuence of the 
benchmark program levels the CPU load on the InﬂuxDB side, and that an increase in 
the number of CPU cores n has a small impact on the data transfer time. In the future, 
we will conduct an evaluation in an environment where the number of CPU cores on 
the InﬂuxDB server is increased, a detailed analysis of the ease of receiving/giving 
mutual inﬂuence by benchmarks, and a detailed analysis of the change in the growth 
rate of data transfer time with an increase in the number of CPU cores n. In addition,

Evaluation of Improvement Plans to Increase the Efﬁciency.. . .
217 
as improvements to the transfer program, we plan to implement a process to shift the 
timing of transfer processing for each core and a process to detect cores with low 
CPU load on the transfer side and make those cores do the data transfer. 
References 
1. score-p. https://www.vi-hps.org/projects/score-p 
2. vampir. https://vampir.eu 
3. Abe F, Nakamura T, Shida N (2016) Generic cpu performance information and display functions 
in the proﬁler. Technical Report 18, Fujitsu Limited, next generation technical computing unit, 
Fujitsu limited, next generation technical computing unit, Fujitsu Limited, next generation 
technical computing unit 
4. Sigelman BH, Barroso LA, Burrows M, Stephenson P, Plakal M, Beaver D, Jaspan S, Shanbhag 
C (2010) Dapper, a large-scale distributed systems tracing infrastructure. Google technical 
report dapper-2010-1 
5. zipkin. https://zipkin.io 
6. jeager. https://www.jaegertracing.io/ 
7. Ren G, Tune E, Moseley T, Shi Y, Rus S, Hundt R (2010) Google-wide proﬁling: a continuous 
proﬁling infrastructure for data centers. IEEE Micro 65–79 
8. Kanev S, Darago JP, Hazelwood K, Ranganathan P, Moseley T, Wei G-Y, Brooks D (2015) Pro-
ﬁling a warehouse-scale computer. In: 2015 ACM/IEEE 42nd annual international symposium 
on computer architecture (ISCA), pp 158–169 
9. perf. https://perf.wiki.kernel.org/index.php/Main_Page 
10. Unixbench. https://code.google.com/archive/p/byte-unixbench/ 
11. 7-zip. https://7-zip.org 
12. inﬂuxdb. https://www.inﬂuxdata.com/products/inﬂuxdb/ 
13. Naqvi SNZ, Yfantidou S (2017) Time series databases and inﬂuxdb, université libre de brux-
elles. In: Advanced databases winter semester 2017–2018 
14. stress. https://linux.die.net/man/1/stress 
15. Iiyama C, Hirai A, Yamaoka M, Fukumoto N, Oguchi M (2022) Consideration of improvement 
plans to increase the efﬁciency of performance data collection/transfer for server systems. In: 
14th data engineering and information management forum(DEIM2022) 
16. pandas. https://pandas.pydata.org 
17. Dataframeclient.
https://inﬂuxdb-python.readthedocs.io/en/latest/api-documentation.html# 
dataframeclient 
18. libpfm4. https://sourceforge.net/p/perfmon2/libpfm4/ci/master/tree/ 
19. ﬁo. https://ﬁo.readthedocs.io/en/latest/ﬁo_doc.html

A Learning Enhancement System 
for Learner’s Community 
Yuta Ishii, Ayako Sugiyama, Kosuke Fukushima, Ryotaro Okada, 
and Takafumi Nakanishi 
Abstract This paper proposes a realization method for a learning activity enhance-
ment system for learner’s community. One of the key points in learning is to review 
the learner’s own learning activities. To achieve this, it is necessary to analyze and 
visualize learning records from multifaceted viewpoints. We also think that analyzing 
and visualizing learners’ learning activities from multifaceted viewpoints will help 
users recognize their own learning experiences, thereby contributing to their motiva-
tion to learn. This is because motivation is triggered differently for each person. We 
propose a learning activity enhancement model that enables users to analyze his/her 
daily learning records from multifaceted viewpoints. Using our model, we design 
a method for visualizing daily learning records from multifaceted viewpoints. This 
method aggregates and visualizes data from multiple perspectives on where, when, 
and what learning activities were performed by each learner. We also implement a 
system that records learning as data and visualizes the learning records by our method. 
Experiments with this system show that it is possible to increase the motivation of 
learners. 
Keywords Learning activity · Learning activity enhancement model · Learner’s 
community · Multifaceted viewpoints
Y. Ishii envelope symbol · A. Sugiyama · R. Okada · T. Nakanishi 
Department of Data Science Musashino University, Koto City, Japan 
e-mail: yt08.ds@gmail.com 
A. Sugiyama 
e-mail: s2122078@stu.musashino-u.ac.jp 
R. Okada 
e-mail: ryotaro.okada@ds.musashino-u.ac.jp 
T. Nakanishi 
e-mail: takafumi.nakanishi@ds.musashino-u.ac.jp 
K. Fukushima 
ITOKI Corporation, Chuo City, Japan 
e-mail: fukushima8jk9@itoki.jp 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1_15 
219

220
Y. Ishii et al.
1 Introduction 
In recent years, various online systems that support learning have appeared with the 
promotion of online education. The user not only learns but also records the learning 
and receives feedback using the learning record data so that the user can grasp the 
learning situation. One of the key points in learning is to review the learner’s own 
learning activities. To facilitate review by each learner, it is important to realize a 
system that aggregates data on their learning activities and analyzes and visualizes it 
from multifaceted viewpoints. We also think that analyzing and visualizing learners’ 
learning activities from multifaceted viewpoints will help users recognize their own 
learning experiences, thereby contributing to their motivation to learn. This is because 
motivation is triggered differently for each person. 
This paper proposes a realization method for a learning enhancement system for 
Learner’s Community. 
We propose a learning activity enhancement model to evaluate users’ learning 
activities. In this model, we deﬁne a learning time function. By aggregating this 
learning time function into an arbitrary variable and deriving the amount of change, 
it is possible to express the user’s learning situation from multifaceted viewpoints. 
This method stores the daily learning records of individuals in a learner’s commu-
nity. Our method makes it possible to visualize a learner’s daily learning situation 
from multifaceted viewpoints by using each user’s learning time, learning space and 
learning activity as input data. In other words, our method aggregates and visualizes 
data from multifaceted viewpoints on when, where, and what learning activities the 
learner performs This method enables not only the learner but also the teacher to 
observe the learner’s learning situation from multifaceted viewpoints. 
Learning activity is deﬁned inspired by The enhanced learning cycle. The 
enhanced learning cycle is a learning style proposed by Musashino University, to 
which we belong [1]. The enhanced learning cycle consists of four steps in the ﬁgure 
on the right, and each step requires a different way of thinking. We assumed that the 
order in which learners select steps is related to motivation to learn. We adopted six 
variables to evaluate the learning record: four from the enhanced learning cycle plus 
two from taking a break and the other as learning activity. 
We implement a system using the proposed methodology. Implemented system 
stores the daily learning activities of each user. By visualizing these activities, the 
system provides feedback to the user and allows the user to reﬂect on his/her learning 
status. The stored learning record data can be analyzed and visualized from multiple 
perspectives to provide more effective feedback. 
As an example of multifaceted viewpoints, the experiment in Sect. 5 of this paper 
realizes aggregation and visualization of the change in time per learning activity 
for each learner and the change in time per learning activity for all learners. By 
showing learners this multifaceted visualization, we showed the effectiveness of the 
experiment in increasing learner motivation. 
The main features of this paper are as follows.

A Learning Enhancement System for Learner’s Community
221
. We propose a realization method for enhancing learning activity in a learner’s 
community.
. The proposed method sets evaluation indicators in multifaceted viewpoints by 
constructing a learning activity enhancement model. 
The structure of this paper is as follows. Section 2 introduces related works of 
this method. Section 3 presents a learning activity enhancement model. Section 4 
presents a proposed realization method for a learning activity enhancement system 
in a learner’s community. Section 5 shows the implementation of the system to which 
this method is applied. Finally, Sect. 6 summarizes this paper. 
2 Related Works 
Reference [2] investigates the effect of technology integration on enhancing student 
motivation in secondary education. These indicate that incorporating technology 
elements into lessons can increase student engagement and motivation compared to 
traditional learning methods. 
Reference [3] investigates digital systems in the education industry that contain 
learning records, collectively known as LMS (Learning Management System). These 
indicate the support of learners online, which is not bound by physical space. Refer-
ence [4] explores differences in research on LMS between China and Australia. 
These indicate that there are regional differences in LMS research themes and 
research methods. Reference [5] describes developing and evaluating an LMS-
based e-learning system. These indicate that using an LMS improves satisfaction 
and quality of learning. In this research, we focus on learning records, especially 
among LMS. 
Reference [6] describes a method for acquiring and utilizing learning records using 
a smartphone. Reference [6] shows that in an intermediate-level lecture, the learning 
record system improves the quality of students’ self-study. Reference [7] investigates 
the relationship between motivation to learn and learning outcomes. Reference [7] 
shows that in lectures, students’ low motivation to learn does not improve their 
knowledge and learning ability. These indicate that improving motivation to learn is 
effective in improving learning outcomes. 
Reference [8] explores the evaluation of electronic learning records. The elec-
tronic learning records are cumulative records of an individual’s learning, skills, and 
knowledge using electronic devices. These indicate that electronic learning records 
improve the effectiveness and efﬁciency of learning. 
Reference [9] explores the degree of interest and academic sentiment experi-
enced during an engaging lecture course and how such variables are associated with 
ﬂow experience, self-study time, and learning success. They found that enthusiastic 
students spent the most time studying and performed the best, while non-enthusiastic 
students were the least active and performed the worst. Reference [10] also describes

222
Y. Ishii et al.
using learning records to support community management. Reference [10] realizes 
group generation for group work using learning log data. 
We propose a learning activity enhancement model that analyzes users’ daily 
learning records from multifaceted viewpoints. We propose a method to enhance 
learning through visualizations using that model. We implement a system that has 
the function to record learning and to visualize it by our method. 
3 A Learning Activity Enhancement Model 
This paper proposes a learning activity enhancement model for evaluating user 
learning. In this model, a learning time function f left parent hesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesisis given by the 
learning activity a Subscript j and the learning space s Subscript k at time t for the user u Subscript i. By deriving 
the amount of change in the various variables of this learning time function 
f left parent hesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesis, we deﬁne indicators to evaluate user learning. 
The cumulative amount of learning indicates the cumulative time of learning so 
far. The cumulative learning amount c Subscript i Baseline left parenthesis u Subscript i Baseline right parenthesisof a user u Subscript i up to time t can be expressed 
by multiple integrations of learning time function f left parent hesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesisas (1). 
c Subscript
 i
 B aseline lef t parenthesis u S
ubscript i Baseline right parenthesis equals triple integral f left parenthesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesis d t d a Subscript j Baseline d s Subscript k
The time-series change in the amount of learning indicates the change in daily 
learning time. The time-series change uper T S ub script i Baseline left parenthesis u Subscript i Baseline comma tau right parenthesisof the amount of learning can be 
expressed by the following multiple integral of f left parent hesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesiswith respect to t
focusing on u Subscript i Baseline a s (2). tau indicates the date. This makes it possible to calculate the 
cumulative learning time for each day. 
uper T S ubsc
r ip
t i 
Ba
se line left p arenthesis u Subs
cript i Baseline comma tau right parenthesis equals integral Subscript tau minus 1 Superscript tau Baseline double integral f left parenthesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesis d a Subscript j Baseline d s Subscript k Baseline d t
Learning situations refer to learning spaces and learning activities. Cumulative 
learning time uper A Sub script i Baseline left parenthesis u Subscript i Baseline comma a Subscript j Baseline right parenthesis, uper S Subscript i Baseline left parenthesis u Subscript i Baseline comma s Subscript k Baseline right parenthesisof learning space and learning activity can be 
expressed by the following multiple integral of f left parent hesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesiswith respect to a Subscript j Baseline comma s Subscript k Baseline
centered on u Subscript i as (3) and (4). 
uper A Sub script
 i
 
B
aseline le ft par
e
nthes
is u Subscript i Baseline comma a Subscript j Baseline right parenthesis equals double integral f left parenthesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesis d s Subscript k Baseline d t
uper A Subscri
pt
 
i
 Baseline left p
a
renthes
is u Subscript i Baseline comma a Subscript j Baseline right parenthesis equals double integral f left parenthesis u Subscript i Baseline comma t comma a Subscript j Baseline comma s Subscript k Baseline right parenthesis d s Subscript k Baseline d t
The model can be multi-dimensionally expressed as in the example above, using 
the amount of change in various variables of the learning time function f left parenthesis u normal bar i comma t comma a normal bar j comma s normal bar k right parenthesis

A Learning Enhancement System for Learner’s Community
223
Fig. 1 An overview of our system 
j, s_k). Our method uses this model to visualize learning records from multifaceted 
viewpoints. 
4 A Realization Method for a Learning Activity 
Enhancement System in a Learner’s Community 
4.1 
Overview 
Figure 1 provides an overview of a learning activity enhancement system in a learner’s 
community. The input of this system is the daily learning data. This system consists 
of four parts output, but as a method it is possible to output more diverse. This system 
consists of a learning activity management module and a learning record visualization 
module. The learning activity management module stores daily learning records in 
the Log database. The learning record visualization module visualizes learning record 
data in the Log database using the learning activity enhancement model. 
4.2 
Learning Activity Management Module 
The learning activity management module stores and manages the daily learning data 
entered by the user in the Log database. The learning record data consists of 5 items, 
as shown in Table 1. Student ID, Start time, End time are required to evaluate the 
user’s learning time. Learning Space and Learning Activity are required to evaluate 
the learning situation.
The learning activity management module consists of two sub-modules: the 
learning start sub-module, and the learning end sub-module (Table 2).

224
Y. Ishii et al.
Table 1 The list of learning data 
Data
Explanation 
Student ID 
It is an ID that identiﬁes the user. This system is used only by people who belong to 
the learner community. Therefore, only administrators can register users, and a 
Student ID can be set for each user at the time of registration 
Learning 
Space 
It’s a learning space. The user selects a learning space from among eight types of 
spaces shown in Table 2 
Learning 
Activity 
It’s a learning category. The user selects one of the six types of Activities shown in 
Table 2. In this system, we adopted 6 activities, which are 4 learning activities of 
this method plus “Taking a break” and “Other” 
Start Time 
It is the time at which the study is started. When a user starts learning, he/she 
performs the Start Learning operation. The time is recorded as the start time. The 
format is “YYYY-MM-DD hh:mm:ss” 
End Time
It is the time when the learning is ﬁnished. When the user ﬁnishes learning, he/she 
performs the End of Learning operation. That time is recorded as the end time. The 
format is “YYYY-MM-DD hh:mm:ss”
Table 2 Selectable leaning 
space and learning activity
Learning space
Learning activity 
Classroom
Asking questions 
Laboratory
Thinking/Acting 
Free space
Shaping 
Library
Reviewing 
During the move
Taking a break 
Restaurant
Other 
My house 
Friend’s house 
4.2.1 
Start Learning Sub-Module 
The learning start sub-module is a sub-module that is executed when the user starts 
learning. The user interface of the sign-in page is shown in Fig. 2. The user interface 
of the learning start page is shown in Fig. 3. The user inputs the Student id in the 
sign-in page. The student ID is entered as a string, and the learning activity and the 
learning space are entered as selections. The learning start time is saved as Start time.
4.2.2 
Learning End Sub-Module 
The learning end sub-module is a sub-module that is executed when the user ﬁnishes 
learning. The user interface of the learning page is shown in Fig. 4. The User interface 
of the learning page displays the Learning Space, Learning Activity, and Learning 
Time. The learning time is calculated as the difference between the current time and

A Learning Enhancement System for Learner’s Community
225
Fig. 2 User interface of the sign-in page 
Fig. 3 User Interface of the learning start page
the start time. When the user performs the learning end operation, the learning end 
sub-module is executed and the end time is saved as End time.
4.3 
Learning Record Visualization Module 
The learning record visualization module visualizes user activity records using four 
of the various evaluation indicators set by the learning activity enhancement model. 
Moreover, since it is in a learner community, statistical information of other users in 
the community can be utilized.

226
Y. Ishii et al.
Fig. 4 User interface of the learning page
4.3.1 
Learning Time Part 
The learning time part displays the learning time for today and this month. Figure 5 
shows an example of the time display. This visualization part is using the Cumulative 
amount of learning, the indicator derived by Eq. (1). Its input is Student id, Start time, 
and End time, and output is today’s learning time and this month’s learning time. In 
Log database, acquire data that meets the following conditions for the learning time 
of today and this month. 
1. Student id matches user 
2. Start time or End time contains the speciﬁed date 
If the Start time and End time dates are the same for one piece of data, the learning 
time is calculated from the difference between the Start time and End time. If the 
Start time and End time dates are different, the learning time is calculated from the 
difference between today’s 0 o’clock and End time as learning across days. After 
that, the total learning time is calculated.
Learning time today
Learning time of the month 
2h24m32s
16h12m51s 
Fig. 5 A display example of learning time part 

A Learning Enhancement System for Learner’s Community
227
Fig. 6 A display example of the learning time transition part 
4.3.2 
Learning Time Transition Part 
In the learning time transition part, all users’ average learning time in the community 
and the user’s learning time is displayed as a one-week bar graph. Figure 6 shows 
an example of the learning time transition part. This visualization part is using the 
indicator Time-series change in the amount of learning derived by Eq. (2). The 
learning time for each day is calculated similarly to the learning time part. For the 
average time of other users, extract the data whose Start time or End time includes 
the corresponding date, and calculate the learning time in the same way as for the 
learning time part. The learning time is then divided by the number of users who 
learned the data. 
4.3.3 
Activity/Space Ratio Part 
In the activity/space ratio part, the ratio of users in the learner community and the ratio 
of users are visualized as pie charts for activities and activity spaces, respectively. The 
data period is one week. Figure 7 shows an example of the activity/space ratio part. 
This visualization part is using the metric variety of learning activities and locations 
derived by Eqs. (3) and (4). The learning time is calculated similarly to the learning 
time transition part. After that, it aggregates by learning activity and learning space 
and visualizes it as a pie chart.
4.3.4 
Learning Activity Transition Part 
The learning activity transition part visualizes the activity transitions of users and 
users belonging to the learner community by means of Sankey diagrams. The learning 
activity transition part visualizes activity transitions for the user and users other than 
the user who belong to the learner community through Sankey diagrams. This part 
extracts the last week’s activity for each user and all users and visualizes it in the form 
of a Sankey diagram. The data period is one week. An example of a learning activity 
transition part is shown in Fig. 8. The upper graph in Fig. 8 shows the transition of 
the user’s learning activity. The graph below it shows the learning activity transitions

228
Y. Ishii et al.
Asking   Thinking/Acting   Shaping 
Reviewing   Taking a break   Other 
Asking   Thinking/Acting   Shaping 
Reviewing   Taking a break   Other 
Classroom   Laboratory Free space 
Library   During the move    Restaurant   My house 
Friend’s house 
the ratio of the learning activity 
for the learner community 
the ratio of the learning activity 
for the user 
the ratio of the learning space 
for the learner community 
the ratio of the learning space 
for the user 
Fig. 7 A display example of the activity/pace ratio part
of other users. If more people perform the same learning activity in a particular 
order, the height of the bars in the graph will also be greater. In other words, the 
large ﬂow in the graph is the trend of the community, and users compare the ﬂow of 
their learning activities with the trend. This visualization part visualizes the order of 
learning activities by adding an element of order to the indicator derived in Eq. (3). It 
is possible to compare the order in which users in the learner community are learning 
activities with your own order.
Our method provides feedback to users on his/her learning records by visualizing 
these four parts using metrics from the learning activity enhancement model.

A Learning Enhancement System for Learner’s Community
229
 
User learning flow 
all user’s learning flow in  the community 
Activity_number of learning 
the height of the box: Number of people 
Fig. 8 A display example of the learning activity transition part. The visualization above shows the 
ﬂow of the user’s learning activity. In the visualization above, colors indicate different study dates. 
Users can see the order of activities. The visualization below shows the ﬂow of learning activities in 
the user’s community of learners. Each vertical line represents the number of people. The horizontal 
is a new record from left to right. Since the number of times of learning within a certain period 
varies from person to person, the vertical width of the entire bar is smaller on the right side. The 
visualization below shows trends in activity within the learner community by following the learning 
activity of the high-volume bar. By comparing it with your own activity, user can understand the 
difference from the trend
5 Experiment 
In this section, experiments were conducted to verify the effectiveness of this system. 
As an example of multifaceted viewpoints, we realized the aggregation and visualiza-
tion of the time change of each learning activity of each learner and the time change 
of each learning activity of all learners. We conducted two experiments. Experiment 
1 examined the impact of different perspectives to the user. Experiment 2 veriﬁed 
the usability of this system. 
5.1 
Experimental Environment 
This experiment was conducted with the cooperation of 20 second- and third-year 
university students each. The experimental period was one semester from November 
22, 2022, to January 27, 2023. Also, there is no class from December 24 to January 
11. The period from January 28 to January 31 was not a class period but was set as an

230
Y. Ishii et al.
Table 3 Survey question items 
Questionnaire
Contents of question 
Pre-questionnaire
Do you usually use learning record apps? 
Are you motivated to learning? 
Do you review your learning? 
Mid-term questionnaire
Are you able to record each time? 
Are you motivated to learning? 
Do you review your learning? 
Final questionnaire
Did this app improve your motivation to learn? 
Was the Reviewing function helpful? 
Would you like to use this app in the future? 
Are you motivated to learning? 
Do you review your learning?
experimental period because it was a period with assignments. Half of the subjects 
were from Group A, and the other half were from Group B. The Reviewing of the 
system used by Group A shall compare current and past data records. Reviewing 
the method used by Group B shall compare its records with other users’ statistics. 
Table 3 shows the differences in visualization content between the two groups. During 
the experiment, the questionnaire was administered three separate times. For the 
ﬁrst time, a pre-questionnaire was conducted before the start of the experiment. 
The second time, we conducted an interim questionnaire during the year-end and 
new year period when there were no classes. The third time, a ﬁnal questionnaire 
was conducted after the end of the experiment. Table 4 shows the items of each 
questionnaire. In Table 4, the three questions in the pre-questionnaire are designed 
to ascertain the subject’s pre-experiment status. The “Are you able to record each 
time?” question in the interim questionnaire was designed to ascertain the recording 
status of the subjects during the experiment. The “Are you motivated to learning?” 
and “Do you review your learning?” questions in the interim and ﬁnal questionnaires 
are designed to determine the impact of this system on the subject’s motivation. 
The ﬁnal questionnaire “Did this app improve your motivation to learn?”, “Was the 
reviewing function helpful?”, and “Would you like to use this app in the future?” 
is intended to conﬁrm the subject’s evaluation of the system after completing the 
experiment. Experiment 1 was veriﬁed by the recording status of subjects in groups 
A and B. Experiment 2 was veriﬁed by comparing the results of three questionnaires. 
5.2 
Results of Experiment 1 
Figure 9 shows the total number of recordings in Groups A and B during the experi-
mental period. From Fig. 9, we ﬁnd from the recording conditions of the subjects that

A Learning Enhancement System for Learner’s Community
231
Table 4 Differences in the display part of A and B groups 
Display part
Group A
Group B 
Learning time part
The same 
Learning time transition part
Comparison with last week’s 
user data 
Comparison with the average 
value in the community 
Activity/space ratio part
Comparison with last week’s 
user data 
Comparison with the average 
value in the community 
Learning activity transition 
part 
User learning activity 
transition 
User’s learning activity 
transition and the learning 
activity transition within their 
community
341 
566 
0 
100 
200 
300 
400 
500 
600 
Group A
Group B 
Record Number 
Fig. 9 The record number of groups A, B
the number of records during the experiment period was more signiﬁcant in group B 
than in group A. Figure 10 shows the transition of the number of daily recordings in 
Groups A and B during the experiment. From Fig. 10, there were some days when 
Group A had a higher number of records in the ﬁrst half. However, it was almost 
non-existent in the latter half. 
5.3 
Results of Experiment 2 
Figure 11 shows the results of the preliminary questionnaire. In the initial question-
naire, We obtained a ﬁnal total of 39 responses from 40 subjects. In response to 
the question, “Do you usually use learning record apps?”, 92% of the respondents 
answered, “No.” Also, as the reason, many people gave the opinion that “there is no 
motivation to record.” In response to the question, “Are you motivated to learning?”,

232
Y. Ishii et al.
0 
5 
10 
15 
20 
25 
30 
Group A
Group B 
Fig. 10 Changes in the number of records in Groups A, B
56% of the respondents answered, “Yes.” In response to the question, “Do you review 
your learning?”, 18% of the respondents answered, “Yes.” 
Figure 12 shows the results of the mid-term questionnaire. We obtained a ﬁnal 
total of 30 responses from 40 subjects. 56% of the respondents answered “No” to 
“Are you able to record every time?” Reasons for answering “No” include the opinion 
that “learning itself is not done” and the opinion that “it has not become a habit.” 63% 
of the respondents answered “Yes” to the question, “Are you motivated to learning?” 
Compared to the results of similar questions in the pre-questionnaire, the number has 
increased. 23% of the respondents answered “Yes” to the question, “Do you review 
your learning?” Compared to the results of similar questions in the pre-questionnaire, 
the number has increased.
36 
17 
32 
3 
22 
7 
0 
5 
10 
15 
20 
25 
30 
35 
40 
Do you usually use 
learning record apps? 
Are you motivated to 
study? 
Do you review your 
learning? 
No
Yes 
Fig. 11 Result of the pre-questionnaire 

A Learning Enhancement System for Learner’s Community
233
17 
11 
23 
13 
19 
7 
0 
5 
10 
15 
20 
25 
Are you able to record 
each time? 
Are you motivated to 
learning? 
Are you reviewing your 
learning? 
No
Yes 
Fig. 12 Result of the mid-term questionnaire 
Figure 13 shows the results of the ﬁnal questionnaire. We obtained a ﬁnal total of 21 
responses from 40 subjects. 66% of the respondents answered “Yes” to the question, 
“Did this app improve your motivation to learning?” 57% of people answered “Yes” to 
the question, “Was the reviewing function useful?” 76% of the respondents answered 
“Yes” to the question, “Are you motivated to learning?” Comparing the results of 
similar questions in the pre-and interim questionnaires, we see that the number has 
increased. 52% of the respondents answered “Yes” to “Do you review your learning?” 
In response to the question, “Would you like to use this app in the future?”, 62% of 
the respondents answered “Yes.“ 
14 
12
13 
16 
11 
7 
9
8 
5 
10 
0 
2 
4 
6 
8 
10 
12 
14 
16 
18 
Did this app 
improve your 
motivation to 
learn? 
Was the 
reviewing 
function 
helpful? 
Would you 
like to use 
this app in the 
future? 
Are you 
motivated to 
learning? 
Do you 
review your 
learning? 
Yes
No 
Fig. 13 Result of the ﬁnal questionnaire

234
Y. Ishii et al.
5.4 
Considerations of Experiment 1 
From the recording situation of the subjects, we see that the number of recordings 
during the experiment period was more signiﬁcant in Group B than in Group A. Also, 
when comparing the number of records by day in the ﬁrst and second halves, the 
number of records in Group A exceeded Group B several times in the ﬁrst half but 
almost disappeared in the second half. This result suggests that the presentation of 
others’ statistical information contributes to improving and maintaining motivation 
for learning. 
5.5 
Considerations of Experiment 2 
From the results of the preliminary questionnaire, it is possible to grasp the subject’s 
characteristics before the experiment. From the results of each question, we see that 
although more than half of the people are motivated to learning, most of them cannot 
take concrete actions such as reﬂection and awareness of learning time. In addition, 
we see that most of the subjects do not usually record their learning. 
From the mid-term questionnaire, it is possible to consider the progress of the 
experiment. The results of the mid-term questionnaire found that forgetting to record 
occurred because the learning recording was not habituated. We consider this can 
be improved by implementing a function that encourages habituation. Speciﬁcally, 
implementing a notiﬁcation function that prompts the start of learning and a learning 
planning function that plans to learn in advance can be mentioned. Group B’s record 
number was maintained, even though some people forgot the record. For this reason, 
other users’ statistical representations may effectively motivate records. 
The questions “Are you actively working on your regular learning?” and “Are you 
reﬂecting on your learning?” were asked three times: a pre-questionnaire, an interim 
questionnaire, and a ﬁnal questionnaire. In addition, we ﬁnd from the results of the 
responses to the question, “Did this app improve your motivation to learn?” through 
the use of this system from the transition of results, it can be said that the improvement 
of learning motivation and the promotion of reﬂection have been realized. 
The effectiveness of the entire system will be veriﬁed from the results of the 
ﬁnal questionnaire. More than half of the respondents answered “Yes” to “Was the 
Reviewing useful?” and “Would you like to use this app in the future?” This indicates 
that the overall system is validated.

A Learning Enhancement System for Learner’s Community
235
6 Conclusion 
In this paper, we proposed a learning enhancement system for the learner’s 
community. 
Our method uses as input data the time, place, and learning activities of each user’s 
learning in his/her daily life to represent the learner’s learning situation, allowing for 
multifaceted analysis and visualization. 
This method deﬁnes a multifaceted learning activity evaluation index by building 
a learning activity enhancement model. In this model, by deﬁning a learning time 
function and obtaining the amount of change in each variable, it is possible to deﬁne 
multiple indicators that represent the user’s learning status. In addition, by using the 
fact that it is within the learner community and outputting the statistical information 
of other users, it is possible to visualize it from multifaceted viewpoints. These will 
make it possible to improve the user’s motivation to learn. 
We implemented a system according to the proposed method. As an example of 
multifaceted viewpoints, in the experiment, we realized the aggregation and visu-
alization of the time change of each learning activity of each learner and the time 
change of each learning activity of all learners. The results of experiments using this 
system suggest that the presentation of statistical information on others contributes 
to the improvement and maintenance of learning motivation. 
In the experiment, forgetting to record was identiﬁed as a future issue. In our 
system, it is necessary to operate the system at the start and end of the learning. To 
prevent forgetting to record at the start of the learning, periodic notiﬁcations can be 
provided. To prevent forgetting to record at the end of the learning, the system should 
sound an alarm after a certain period of time has elapsed. 
Future work includes variable expansion and its evaluation, prediction of user 
learning behavior, and validation through experiments in different communities. 
References 
1. “The enhanced learning cycle,” Muscatine University. https://risyuyouran.musashino-u.ac.jp/ 
spiral/ 
2. Banitt J, Theis S, Leeuwe LV (2013) The effects of technology integration on student 
engagement. Masters of Arts in Education Action Research Papers, ed, August 2013 
3. Turnbull D, Chugh R, Luck J (2020) Learning management systems, an overview. Encyclopedia 
of Education and Information Technologies (pp 1052–1058). August 2020 
4. Turnbull D, Chugh R, Luck J, (2020) Learning management systems: a review of the research 
methodology literature in Australia and China. Int J Res Method Educ. 164-178. (Mar 2020). 
5. Rabiman R, Nurtanto M, Kholifah N (2020) Design and development e-learning system by 
learning management system (LMS) in vocational education. Int J Sci Technol Res 9, January 
2020 
6. Yamamoto N (2014) An interactive learning system using smartphone: improving students’ 
learning motivation and self-learning. In: 2014 Ninth international conference on broadband 
and wireless computing, communication and applications, (pp 428–431), 2014.

236
Y. Ishii et al.
7. Hirayama Y, Hirayama S (2001) An analysis of the two-factor model of learning motivation in 
university students. Cult Soc Sci 41:101–105. (Bulletin of Tokyo Kasei University, 1) 
8. Barker K (1999) The electronic learning record: assessment and management of skills and 
knowledge. FetureEd, August 1999 
9. Lonka K, Ketonen E (2012) How to make a lecture course an engaging learning experience? 
Stud Learn Soc 
10. Liang C, Majumdar R (2021) Learning log-based automatic group formation: system design 
and classroom implementation study. Res Pract Technol Enhanced Learn

Author Index 
C 
Champa, Arifa I., 119, 139 
D 
Dascalu, Sergiu, 103 
E 
Eishita, Farjana Z., 139 
Estreito, Zachary, 103 
F 
Frederick Harris, Jr., 103 
Fukumoto, Naoto, 203 
Fukushima, Kosuke, 219 
H 
Han, Sumin, 41 
Hirai, Akira, 203 
Hung Van, Trong, 55, 69 
Hu, Wen-Chen, 159 
I 
Iiyama, Chika, 203 
Ishii, Yuta, 219 
J 
Jo, Dong Hyuk, 15, 29 
Jung, Hyeil, 1 
K 
Karaa, Wahiba Ben Abdessalem, 81 
Kim, Su Young, 29 
Kim, Youngju, 1 
L 
Le, Quan Ngo, 55 
Lee, Ki Yong, 173 
Lee, Sungtaek, 41 
Le, Vinh, 103 
Liao, Weixian, 185 
M 
Mechergui, Amani, 81 
N 
Nakanishi, Takafumi, 219 
Ngoc, Dat Vo, 55 
O 
Oguchi, Masato, 203 
Okada, Ryotaro, 219 
P 
Pan, Zhi, 15, 29 
Park, Myeongsook, 41 
Pene, Papa, 185 
Pillai, Sanjaikanth E Vadakkethil 
Somanathan, 159
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2024 
R. Lee (ed.), Software Engineering and Management: Theory and Application, Studies 
in Computational Intelligence 1137, https://doi.org/10.1007/978-3-031-55174-1 
237

238
Author Index
R 
Rabbi, Md. Fazle, 119, 139 
S 
Seo, Hang Ju, 15 
Si, Thin Nguyen, 55, 69 
Song, Yeonju, 173 
Sugiyama, Ayako, 219 
T 
Tian, Pu, 185 
W 
Wang, Qianlong, 185 
Y 
Yamaoka, Mari, 203 
Yook, Hyojin, 41 
Yu, Wei, 185 
Z 
Zghal, Sami, 81 
Zibran, Minhaz F., 119, 139


DRAFT
i
Computational Complexity: A Modern
Approach
Draft of a book: Dated January 2007
Comments welcome!
Sanjeev Arora and Boaz Barak
Princeton University
complexitybook@gmail.com
Not to be reproduced or distributed without the authors’ permission
This is an Internet draft. Some chapters are more ﬁnished than others. References and
attributions are very preliminary and we apologize in advance for any omissions (but hope you
will nevertheless point them out to us).
Please send us bugs, typos, missing references or general comments to
complexitybook@gmail.com — Thank You!!

DRAFT
ii

DRAFT
About this book
Computational complexity theory has developed rapidly in the past three decades. The list of
surprising and fundamental results proved since 1990 alone could ﬁll a book: these include new
probabilistic deﬁnitions of classical complexity classes (IP = PSPACE and the PCP Theorems)
and their implications for the ﬁeld of approximation algorithms; Shor’s algorithm to factor integers
using a quantum computer; an understanding of why current approaches to the famous P versus
NP will not be successful; a theory of derandomization and pseudorandomness based upon com-
putational hardness; and beautiful constructions of pseudorandom objects such as extractors and
expanders.
This book aims to describe such recent achievements of complexity theory in the context of
the classical results. It is intended to both serve as a textbook as a reference for self-study. This
means it must simultaneously cater to many audiences, and it is carefully designed with that goal.
Throughout the book we explain the context in which a certain notion is useful, and why things
are deﬁned in a certain way. Examples and solved exercises accompany key deﬁnitions. We assume
essentially no computational background and very minimal mathematical background, which we
review in Appendix A.
We have also provided a web site for this book at http://www.cs.princeton.edu/theory/complexity/
with related auxiliary material. This includes web chapters on automata and computability theory,
detailed teaching plans for courses based on this book, a draft of all the book’s chapters, and links
to other online resources covering related topics.
The book is divided into three parts:
Part I: Basic complexity classes. This volume provides a broad introduction to the ﬁeld. Start-
ing from the deﬁnition of Turing machines and the basic notions of computability theory, this
volumes covers the basic time and space complexity classes, and also includes a few more
modern topics such probabilistic algorithms, interactive proofs and cryptography.
Part II: Lower bounds on concrete computational models. This part describes lower bounds
on resources required to solve algorithmic tasks on concrete models such as circuits, decision
trees, etc. Such models may seem at ﬁrst sight very diﬀerent from Turing machines, but
looking deeper one ﬁnds interesting interconnections.
Part III: Advanced topics. This part is largely devoted to developments since the late 1980s. It
includes average case complexity, derandomization and pseudorandomness, the PCP theorem
and hardness of approximation, proof complexity and quantum computing.
Almost every chapter in the book can be read in isolation (though we recommend reading
Chapters 1, 2 and 7 before reading later chapters). This is important because the book is aimed
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
iii

DRAFT
iv
at many classes of readers:
• Physicists, mathematicians, and other scientists. This group has become increasingly inter-
ested in computational complexity theory, especially because of high-proﬁle results such as
Shor’s algorithm and the recent deterministic test for primality. This intellectually sophisti-
cated group will be able to quickly read through Part I. Progressing on to Parts II and III
they can read individual chapters and ﬁnd almost everything they need to understand current
research.
• Computer scientists (e.g., algorithms designers) who do not work in complexity theory per se.
They may use the book for self-study or even to teach a graduate course or seminar.
• All those —professors or students— who do research in complexity theory or plan to do so.
They may already know Part I and use the book for Parts II and III, possibly in a seminar
or reading course. The coverage of advanced topics there is detailed enough to allow this.
This book can be used as a textbook for several types of courses.
We will provide several
teaching plans and material for such courses on the book’s web site.
• Undergraduate Theory of Computation Course. Part I may be suitable for an undergraduate
course that is an alternative to the more traditional Theory of Computation course currently
taught in most computer science departments (and exempliﬁed by Sipser’s excellent book
with the same name [SIP96]). Such a course would have a greater emphasis on modern topics
such as probabilistic algorithms and cryptography. We note that in contrast to Sipser’s book,
the current book has a quite minimal coverage of computability and no coverage of automata
theory, but we provide web-only chapters with more coverage of these topics on the book’s web
site. The prerequisite mathematical background would be some comfort with mathematical
proofs and elementary probability on ﬁnite sample spaces, topics that are covered in typical
“discrete math”/“math for CS” courses currently oﬀered in most CS departments.
• Advanced undergraduate/beginning graduate introduction to complexity course. The book can
be used as a text for an introductory complexity course aimed at undergraduate or non-theory
graduate students (replacing Papadimitriou’s 1994 book [Pap94] that does not contain many
recent results). Such a course would probably include many topics from Part I and then
a sprinkling from Parts II and III, and assume some background in algorithms and/or the
theory of computation.
• Graduate Complexity course. The book can serve as a text for a graduate complexity course
that prepares graduate students interested in theory to do research in complexity and related
areas. Such a course can use parts of Part I to review basic material, and then move on to the
advanced topics of Parts II and III. The book contains far more material than can be taught
in one term, and we provide on our website several alternative outlines for such a course.
We hope that this book conveys our excitement about this new ﬁeld and the insights it provides
in a host of older disciplines.
Web draft 2007-01-08 21:59

DRAFT
Contents
About this book
iii
Introduction
p0.1 (1)
I
Basic Complexity Classes
p0.9 (9)
1
The computational model —and why it doesn’t matter
p1.1 (11)
1.1
Encodings and Languages: Some conventions . . . . . . . . . . . . . . . . . . . . . . p1.2 (12)
1.1.1
Representing objects as strings . . . . . . . . . . . . . . . . . . . . . . . . . . p1.2 (12)
1.1.2
Decision problems / languages
. . . . . . . . . . . . . . . . . . . . . . . . . . p1.3 (13)
1.1.3
Big-Oh notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.3 (13)
1.2
Modeling computation and eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.4 (14)
1.2.1
The Turing Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.5 (15)
1.2.2
Robustness of our deﬁnition.
. . . . . . . . . . . . . . . . . . . . . . . . . . . p1.9 (19)
1.2.3
The expressive power of Turing machines. . . . . . . . . . . . . . . . . . . . . p1.12 (22)
1.3
Machines as strings and the universal Turing machines.
. . . . . . . . . . . . . . . . p1.12 (22)
1.3.1
The Universal Turing Machine
. . . . . . . . . . . . . . . . . . . . . . . . . . p1.13 (23)
1.4
Uncomputable functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.15 (25)
1.4.1
The Halting Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.15 (25)
1.5
Deterministic time and the class P. . . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.17 (27)
1.5.1
On the philosophical importance of P . . . . . . . . . . . . . . . . . . . . . . p1.17 (27)
1.5.2
Criticisms of P and some eﬀorts to address them . . . . . . . . . . . . . . . . p1.18 (28)
1.5.3
Edmonds’ quote
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.19 (29)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.20 (30)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p1.21 (31)
1.A Proof of Theorem 1.13: Universal Simulation in O(T log T)-time
. . . . . . . . . . . p1.25 (35)
2
NP and NP completeness
p2.1 (39)
2.1
The class NP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.2 (40)
2.1.1
Relation between NP and P
. . . . . . . . . . . . . . . . . . . . . . . . . . . p2.3 (41)
2.1.2
Non-deterministic Turing machines.
. . . . . . . . . . . . . . . . . . . . . . . p2.4 (42)
2.2
Reducibility and NP-completeness
. . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.5 (43)
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
v

DRAFT
vi
CONTENTS
2.3
The Cook-Levin Theorem: Computation is Local . . . . . . . . . . . . . . . . . . . . p2.6 (44)
2.3.1
Boolean formulae and the CNF form.
. . . . . . . . . . . . . . . . . . . . . . p2.7 (45)
2.3.2
The Cook-Levin Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.7 (45)
2.3.3
Warmup: Expressiveness of boolean formulae . . . . . . . . . . . . . . . . . . p2.8 (46)
2.3.4
Proof of Lemma 2.12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.9 (47)
2.3.5
Reducing SAT to 3SAT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.11 (49)
2.3.6
More thoughts on the Cook-Levin theorem
. . . . . . . . . . . . . . . . . . . p2.11 (49)
2.4
The web of reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.12 (50)
In praise of reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.16 (54)
Coping with NP hardness.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.16 (54)
2.5
Decision versus search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.17 (55)
2.6
coNP, EXP and NEXP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.18 (56)
2.6.1
coNP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.18 (56)
2.6.2
EXP and NEXP
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.19 (57)
2.7
More thoughts about P, NP, and all that . . . . . . . . . . . . . . . . . . . . . . . . p2.20 (58)
2.7.1
The philosophical importance of NP . . . . . . . . . . . . . . . . . . . . . . . p2.20 (58)
2.7.2
NP and mathematical proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.20 (58)
2.7.3
What if P = NP?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.21 (59)
2.7.4
What if NP = coNP? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.21 (59)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.22 (60)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p2.23 (61)
3
Diagonalization
p3.1 (65)
3.1
Time Hierarchy Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p3.2 (66)
3.2
Space Hierarchy Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p3.2 (66)
3.3
Nondeterministic Time Hierarchy Theorem
. . . . . . . . . . . . . . . . . . . . . . . p3.3 (67)
3.4
Ladner’s Theorem: Existence of NP-intermediate problems. . . . . . . . . . . . . . . p3.4 (68)
3.5
Oracle machines and the limits of diagonalization? . . . . . . . . . . . . . . . . . . . p3.6 (70)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p3.8 (72)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p3.9 (73)
4
Space complexity
p4.1 (75)
4.1
Conﬁguration graphs.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p4.2 (76)
4.2
Some space complexity classes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p4.4 (78)
4.3
PSPACE completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p4.5 (79)
4.3.1
Savitch’s theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p4.8 (82)
4.3.2
The essence of PSPACE: optimum strategies for game-playing.
. . . . . . . p4.8 (82)
4.4
NL completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p4.10 (84)
4.4.1
Certiﬁcate deﬁnition of NL: read-once certiﬁcates
. . . . . . . . . . . . . . . p4.12 (86)
4.4.2
NL = coNL
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p4.13 (87)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p4.14 (88)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p4.14 (88)
Web draft 2007-01-08 21:59

DRAFT
CONTENTS
vii
5
The Polynomial Hierarchy and Alternations
p5.1 (91)
5.1
The classes Σp
2 and Πp
2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p5.1 (91)
5.2
The polynomial hierarchy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p5.3 (93)
5.2.1
Properties of the polynomial hierarchy. . . . . . . . . . . . . . . . . . . . . . . p5.3 (93)
5.2.2
Complete problems for levels of PH . . . . . . . . . . . . . . . . . . . . . . . p5.4 (94)
5.3
Alternating Turing machines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p5.5 (95)
5.3.1
Unlimited number of alternations? . . . . . . . . . . . . . . . . . . . . . . . . p5.6 (96)
5.4
Time versus alternations: time-space tradeoﬀs for SAT. . . . . . . . . . . . . . . . . . p5.6 (96)
5.5
Deﬁning the hierarchy via oracle machines.
. . . . . . . . . . . . . . . . . . . . . . . p5.8 (98)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p5.9 (99)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p5.10 (100)
6
Circuits
p6.1 (101)
6.1
Boolean circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.1 (101)
6.1.1
Turing machines that take advice . . . . . . . . . . . . . . . . . . . . . . . . . p6.5 (105)
6.2
Karp-Lipton Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.6 (106)
6.3
Circuit lowerbounds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.7 (107)
6.4
Non-uniform hierarchy theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.8 (108)
6.5
Finer gradations among circuit classes . . . . . . . . . . . . . . . . . . . . . . . . . . p6.8 (108)
6.5.1
Parallel computation and NC . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.9 (109)
6.5.2
P-completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.10 (110)
6.6
Circuits of exponential size
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.11 (111)
6.7
Circuit Satisﬁability and an alternative proof of the Cook-Levin Theorem . . . . . . p6.12 (112)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.13 (113)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p6.13 (113)
7
Randomized Computation
p7.1 (115)
7.1
Probabilistic Turing machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.2 (116)
7.2
Some examples of PTMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.3 (117)
7.2.1
Probabilistic Primality Testing . . . . . . . . . . . . . . . . . . . . . . . . . . p7.3 (117)
7.2.2
Polynomial identity testing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.4 (118)
7.2.3
Testing for perfect matching in a bipartite graph. . . . . . . . . . . . . . . . . p7.5 (119)
7.3
One-sided and zero-sided error: RP, coRP, ZPP
. . . . . . . . . . . . . . . . . . . p7.6 (120)
7.4
The robustness of our deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.7 (121)
7.4.1
Role of precise constants, error reduction. . . . . . . . . . . . . . . . . . . . . p7.7 (121)
7.4.2
Expected running time versus worst-case running time.
. . . . . . . . . . . . p7.10 (124)
7.4.3
Allowing more general random choices than a fair random coin. . . . . . . . . p7.10 (124)
7.5
Randomness eﬃcient error reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.11 (125)
7.6
BPP ⊆P/poly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.12 (126)
7.7
BPP is in PH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.13 (127)
7.8
State of our knowledge about BPP . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.14 (128)
Complete problems for BPP? . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.14 (128)
Does BPTIME have a hierarchy theorem? . . . . . . . . . . . . . . . . . . . p7.15 (129)
Web draft 2007-01-08 21:59

DRAFT
viii
CONTENTS
7.9
Randomized reductions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.15 (129)
7.10 Randomized space-bounded computation
. . . . . . . . . . . . . . . . . . . . . . . . p7.15 (129)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.17 (131)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.18 (132)
7.A Random walks and eigenvalues
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.21 (135)
7.A.1
Distributions as vectors and the parameter λ(G). . . . . . . . . . . . . . . . . p7.21 (135)
7.A.2
Analysis of the randomized algorithm for undirected connectivity.
. . . . . . p7.24 (138)
7.B
Expander graphs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.25 (139)
7.B.1
The Algebraic Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p7.25 (139)
7.B.2
Combinatorial expansion and existence of expanders. . . . . . . . . . . . . . . p7.27 (141)
7.B.3
Error reduction using expanders. . . . . . . . . . . . . . . . . . . . . . . . . . p7.29 (143)
8
Interactive proofs
p8.1 (147)
8.1
Warmup: Interactive proofs with a deterministic veriﬁer . . . . . . . . . . . . . . . . p8.1 (147)
8.2
The class IP
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.3 (149)
8.3
Proving that graphs are not isomorphic. . . . . . . . . . . . . . . . . . . . . . . . . . p8.4 (150)
8.4
Public coins and AM
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.5 (151)
8.4.1
Set Lower Bound Protocol.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.6 (152)
Tool: Pairwise independent hash functions. . . . . . . . . . . . . . . . . . . . p8.7 (153)
The lower-bound protocol. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.9 (155)
8.4.2
Some properties of IP and AM . . . . . . . . . . . . . . . . . . . . . . . . . . p8.10 (156)
8.4.3
Can GI be NP-complete? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.11 (157)
8.5
IP = PSPACE
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.11 (157)
8.5.1
Arithmetization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.12 (158)
8.5.2
Interactive protocol for #SATD . . . . . . . . . . . . . . . . . . . . . . . . . . p8.12 (158)
Sumcheck protocol. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.13 (159)
8.5.3
Protocol for TQBF: proof of Theorem 8.17
. . . . . . . . . . . . . . . . . . . p8.14 (160)
8.6
The power of the prover . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.15 (161)
8.7
Program Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.16 (162)
8.7.1
Languages that have checkers . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.17 (163)
8.8
Multiprover interactive proofs (MIP)
. . . . . . . . . . . . . . . . . . . . . . . . . . p8.18 (164)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.19 (165)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.20 (166)
8.A Interactive proof for the Permanent . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.21 (167)
8.A.1
The protocol
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p8.23 (169)
9
Complexity of counting
p9.1 (171)
9.1
The class #P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p9.2 (172)
9.1.1
The class PP: decision-problem analog for #P. . . . . . . . . . . . . . . . . . p9.3 (173)
9.2
#P completeness.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p9.4 (174)
9.2.1
Permanent and Valiant’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . p9.4 (174)
9.2.2
Approximate solutions to #P problems
. . . . . . . . . . . . . . . . . . . . . p9.8 (178)
9.3
Toda’s Theorem: PH ⊆P#SAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p9.9 (179)
Web draft 2007-01-08 21:59

DRAFT
CONTENTS
ix
9.3.1
The class ⊕P and hardness of satisﬁability with unique solutions.
. . . . . . p9.9 (179)
Proof of Theorem 9.15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p9.11 (181)
9.3.2
Step 1: Randomized reduction from PH to ⊕P . . . . . . . . . . . . . . . . . p9.11 (181)
9.3.3
Step 2: Making the reduction deterministic . . . . . . . . . . . . . . . . . . . p9.13 (183)
9.4
Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p9.14 (184)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p9.14 (184)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p9.15 (185)
10 Cryptography
p10.1 (187)
10.1 Hard-on-average problems and one-way functions . . . . . . . . . . . . . . . . . . . . p10.2 (188)
10.1.1 Discussion of the deﬁnition of one-way function . . . . . . . . . . . . . . . . . p10.4 (190)
10.1.2 Random self-reducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p10.5 (191)
10.2 What is a random-enough string? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p10.5 (191)
10.2.1 Blum-Micali and Yao deﬁnitions
. . . . . . . . . . . . . . . . . . . . . . . . . p10.6 (192)
10.2.2 Equivalence of the two deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . p10.8 (194)
10.3 One-way functions and pseudorandom number generators . . . . . . . . . . . . . . . p10.10 (196)
10.3.1 Goldreich-Levin hardcore bit
. . . . . . . . . . . . . . . . . . . . . . . . . . . p10.10 (196)
10.3.2 Pseudorandom number generation
. . . . . . . . . . . . . . . . . . . . . . . . p10.13 (199)
10.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p10.13 (199)
10.4.1 Pseudorandom functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p10.13 (199)
10.4.2 Private-key encryption: deﬁnition of security
. . . . . . . . . . . . . . . . . . p10.14 (200)
10.4.3 Derandomization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p10.15 (201)
10.4.4 Tossing coins over the phone and bit commitment
. . . . . . . . . . . . . . . p10.16 (202)
10.4.5 Secure multiparty computations
. . . . . . . . . . . . . . . . . . . . . . . . . p10.16 (202)
10.4.6 Lowerbounds for machine learning . . . . . . . . . . . . . . . . . . . . . . . . p10.17 (203)
10.5 Recent developments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p10.17 (203)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p10.17 (203)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p10.18 (204)
II
Lowerbounds for Concrete Computational Models
p10.21 (207)
11 Decision Trees
p11.2 (211)
11.1 Certiﬁcate Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p11.4 (213)
11.2 Randomized Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p11.6 (215)
11.3 Lowerbounds on Randomized Complexity . . . . . . . . . . . . . . . . . . . . . . . . p11.6 (215)
11.4 Some techniques for decision tree lowerbounds . . . . . . . . . . . . . . . . . . . . . . p11.8 (217)
11.5 Comparison trees and sorting lowerbounds . . . . . . . . . . . . . . . . . . . . . . . . p11.9 (218)
11.6 Yao’s MinMax Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p11.9 (218)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p11.9 (218)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p11.10 (219)
Web draft 2007-01-08 21:59

DRAFT
x
CONTENTS
12 Communication Complexity
p12.1 (221)
12.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p12.1 (221)
12.2 Lowerbound methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p12.2 (222)
12.2.1 Fooling set
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p12.2 (222)
12.2.2 The tiling lowerbound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p12.3 (223)
12.2.3 Rank lowerbound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p12.4 (224)
12.2.4 Discrepancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p12.5 (225)
A technique for upperbounding the discrepancy . . . . . . . . . . . . . . . . . p12.6 (226)
12.2.5 Comparison of the lowerbound methods . . . . . . . . . . . . . . . . . . . . . p12.7 (227)
12.3 Multiparty communication complexity . . . . . . . . . . . . . . . . . . . . . . . . . . p12.8 (228)
Discrepancy-based lowerbound
. . . . . . . . . . . . . . . . . . . . . . . . . . p12.9 (229)
12.4 Probabilistic Communication Complexity
. . . . . . . . . . . . . . . . . . . . . . . . p12.10 (230)
12.5 Overview of other communication models
. . . . . . . . . . . . . . . . . . . . . . . . p12.10 (230)
12.6 Applications of communication complexity . . . . . . . . . . . . . . . . . . . . . . . . p12.11 (231)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p12.11 (231)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p12.12 (232)
13 Circuit lowerbounds
p13.1 (235)
13.1 AC0 and H˚astad’s Switching Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.1 (235)
13.1.1 The switching lemma
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.2 (236)
13.1.2 Proof of the switching lemma (Lemma 13.2) . . . . . . . . . . . . . . . . . . . p13.3 (237)
13.2 Circuits With “Counters”:ACC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.5 (239)
13.3 Lowerbounds for monotone circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.8 (242)
13.3.1 Proving Theorem 13.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.8 (242)
Clique Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.8 (242)
Approximation by clique indicators.
. . . . . . . . . . . . . . . . . . . . . . . p13.9 (243)
13.4 Circuit complexity: The frontier
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.11 (245)
13.4.1 Circuit lowerbounds using diagonalization . . . . . . . . . . . . . . . . . . . . p13.11 (245)
13.4.2 Status of ACC versus P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.12 (246)
13.4.3 Linear Circuits With Logarithmic Depth . . . . . . . . . . . . . . . . . . . . . p13.13 (247)
13.4.4 Branching Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.13 (247)
13.5 Approaches using communication complexity
. . . . . . . . . . . . . . . . . . . . . . p13.14 (248)
13.5.1 Connection to ACC0 Circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.14 (248)
13.5.2 Connection to Linear Size Logarithmic Depth Circuits . . . . . . . . . . . . . p13.15 (249)
13.5.3 Connection to branching programs . . . . . . . . . . . . . . . . . . . . . . . . p13.15 (249)
13.5.4 Karchmer-Wigderson communication games and depth lowerbounds . . . . . p13.15 (249)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.17 (251)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p13.18 (252)
14 Algebraic computation models
p14.1 (255)
14.1 Algebraic circuits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p14.2 (256)
14.2 Algebraic Computation Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p14.4 (258)
14.3 The Blum-Shub-Smale Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p14.8 (262)
Web draft 2007-01-08 21:59

DRAFT
CONTENTS
xi
14.3.1 Complexity Classes over the Complex Numbers . . . . . . . . . . . . . . . . . p14.9 (263)
14.3.2 Hilbert’s Nullstellensatz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p14.10 (264)
14.3.3 Decidability Questions: Mandelbrot Set . . . . . . . . . . . . . . . . . . . . . p14.10 (264)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p14.11 (265)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p14.11 (265)
III
Advanced topics
p14.13 (267)
15 Average Case Complexity: Levin’s Theory
p15.1 (269)
15.1 Distributional Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p15.2 (270)
15.1.1 Formalizations of “real-life distributions.” . . . . . . . . . . . . . . . . . . . . p15.3 (271)
15.2 DistNP and its complete problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . p15.4 (272)
15.2.1 Polynomial-Time on Average . . . . . . . . . . . . . . . . . . . . . . . . . . . p15.4 (272)
15.2.2 Reductions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p15.5 (273)
15.2.3 Proofs using the simpler deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . p15.8 (276)
15.3 Existence of Complete Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . p15.10 (278)
15.4 Polynomial-Time Samplability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p15.10 (278)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p15.11 (279)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p15.11 (279)
16 Derandomization, Expanders and Extractors
p16.1 (281)
16.1 Pseudorandom Generators and Derandomization . . . . . . . . . . . . . . . . . . . . p16.3 (283)
16.1.1 Hardness and Derandomization . . . . . . . . . . . . . . . . . . . . . . . . . . p16.5 (285)
16.2 Proof of Theorem 16.10: Nisan-Wigderson Construction . . . . . . . . . . . . . . . . p16.7 (287)
16.2.1 Warmup: two toy examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.8 (288)
Extending the input by one bit using Yao’s Theorem.
. . . . . . . . . . . . . p16.8 (288)
Extending the input by two bits using the averaging principle.
. . . . . . . . p16.9 (289)
Beyond two bits: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.10 (290)
16.2.2 The NW Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.10 (290)
Conditions on the set systems and function. . . . . . . . . . . . . . . . . . . . p16.11 (291)
Putting it all together: Proof of Theorem 16.10 from Lemmas 16.18 and 16.19p16.12 (292)
Construction of combinatorial designs. . . . . . . . . . . . . . . . . . . . . . . p16.13 (293)
16.3 Derandomization requires circuit lowerbounds . . . . . . . . . . . . . . . . . . . . . . p16.13 (293)
16.4 Explicit construction of expander graphs . . . . . . . . . . . . . . . . . . . . . . . . . p16.16 (296)
16.4.1 Rotation maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.17 (297)
16.4.2 The matrix/path product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.17 (297)
16.4.3 The tensor product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.18 (298)
16.4.4 The replacement product
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.19 (299)
16.4.5 The actual construction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.21 (301)
16.5 Deterministic logspace algorithm for undirected connectivity.
. . . . . . . . . . . . . p16.22 (302)
16.6 Weak Random Sources and Extractors . . . . . . . . . . . . . . . . . . . . . . . . . . p16.25 (305)
16.6.1 Min Entropy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.25 (305)
16.6.2 Statistical distance and Extractors . . . . . . . . . . . . . . . . . . . . . . . . p16.26 (306)
Web draft 2007-01-08 21:59

DRAFT
xii
CONTENTS
16.6.3 Extractors based upon hash functions . . . . . . . . . . . . . . . . . . . . . . p16.27 (307)
16.6.4 Extractors based upon random walks on expanders . . . . . . . . . . . . . . . p16.28 (308)
16.6.5 An extractor based upon Nisan-Wigderson
. . . . . . . . . . . . . . . . . . . p16.28 (308)
16.7 Applications of Extractors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.31 (311)
16.7.1 Graph constructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.31 (311)
16.7.2 Running randomized algorithms using weak random sources . . . . . . . . . . p16.32 (312)
16.7.3 Recycling random bits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.33 (313)
16.7.4 Pseudorandom generators for spacebounded computation
. . . . . . . . . . . p16.33 (313)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.37 (317)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p16.38 (318)
17 Hardness Ampliﬁcation and Error Correcting Codes
p17.1 (321)
17.1 Hardness and Hardness Ampliﬁcation. . . . . . . . . . . . . . . . . . . . . . . . . . . p17.1 (321)
17.2 Mild to strong hardness: Yao’s XOR Lemma. . . . . . . . . . . . . . . . . . . . . . . p17.2 (322)
Proof of Yao’s XOR Lemma using Impagliazzo’s Hardcore Lemma. . . . . . . p17.3 (323)
17.3 Proof of Impagliazzo’s Lemma
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.4 (324)
17.4 Error correcting codes: the intuitive connection to hardness ampliﬁcation
. . . . . . p17.8 (328)
17.4.1 Local decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.10 (330)
17.5 Constructions of Error Correcting Codes . . . . . . . . . . . . . . . . . . . . . . . . . p17.12 (332)
17.5.1 Walsh-Hadamard Code. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.12 (332)
17.5.2 Reed-Solomon Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.13 (333)
17.5.3 Concatenated codes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.14 (334)
17.5.4 Reed-Muller Codes.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.15 (335)
17.5.5 Decoding Reed-Solomon.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.16 (336)
Randomized interpolation: the case of ρ < 1/(d + 1) . . . . . . . . . . . . . . p17.16 (336)
Berlekamp-Welch Procedure: the case of ρ < (m −d)/(2m) . . . . . . . . . . p17.16 (336)
17.5.6 Decoding concatenated codes. . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.17 (337)
17.6 Local Decoding of explicit codes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.17 (337)
17.6.1 Local decoder for Walsh-Hadamard. . . . . . . . . . . . . . . . . . . . . . . . p17.17 (337)
17.6.2 Local decoder for Reed-Muller
. . . . . . . . . . . . . . . . . . . . . . . . . . p17.18 (338)
17.6.3 Local decoding of concatenated codes. . . . . . . . . . . . . . . . . . . . . . . p17.19 (339)
17.6.4 Putting it all together. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.20 (340)
17.7 List decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.21 (341)
17.7.1 List decoding the Reed-Solomon code
. . . . . . . . . . . . . . . . . . . . . . p17.22 (342)
17.8 Local list decoding: getting to BPP = P. . . . . . . . . . . . . . . . . . . . . . . . . p17.23 (343)
17.8.1 Local list decoding of the Walsh-Hadamard code. . . . . . . . . . . . . . . . . p17.24 (344)
17.8.2 Local list decoding of the Reed-Muller code . . . . . . . . . . . . . . . . . . . p17.24 (344)
17.8.3 Local list decoding of concatenated codes. . . . . . . . . . . . . . . . . . . . . p17.26 (346)
17.8.4 Putting it all together. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.26 (346)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.27 (347)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p17.28 (348)
Web draft 2007-01-08 21:59

DRAFT
CONTENTS
xiii
18 PCP and Hardness of Approximation
p18.1 (351)
18.1 PCP and Locally Testable Proofs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . p18.2 (352)
18.2 PCP and Hardness of Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . p18.5 (355)
18.2.1 Gap-producing reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p18.6 (356)
18.2.2 Gap problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p18.6 (356)
18.2.3 Constraint Satisfaction Problems . . . . . . . . . . . . . . . . . . . . . . . . . p18.7 (357)
18.2.4 An Alternative Formulation of the PCP Theorem . . . . . . . . . . . . . . . p18.8 (358)
18.2.5 Hardness of Approximation for 3SAT and INDSET. . . . . . . . . . . . . . . . p18.9 (359)
18.3 n−δ-approximation of independent set is NP-hard. . . . . . . . . . . . . . . . . . . . p18.11 (361)
18.4 NP ⊆PCP(poly(n), 1): PCP based upon Walsh-Hadamard code
. . . . . . . . . . p18.13 (363)
18.4.1 Tool: Linearity Testing and the Walsh-Hadamard Code
. . . . . . . . . . . . p18.13 (363)
18.4.2 Proof of Theorem 18.21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p18.15 (365)
18.4.3 PCP’s of proximity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p18.17 (367)
18.5 Proof of the PCP Theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p18.19 (369)
18.5.1 Gap Ampliﬁcation: Proof of Lemma 18.29 . . . . . . . . . . . . . . . . . . . . p18.21 (371)
18.5.2 Alphabet Reduction: Proof of Lemma 18.30 . . . . . . . . . . . . . . . . . . . p18.27 (377)
18.6 The original proof of the PCP Theorem.
. . . . . . . . . . . . . . . . . . . . . . . . p18.29 (379)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p18.29 (379)
19 More PCP Theorems and the Fourier Transform Technique
p19.1 (385)
19.1 Parallel Repetition of PCP’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p19.1 (385)
19.2 H˚astad’s 3-bit PCP Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p19.3 (387)
19.3 Tool: the Fourier transform technique
. . . . . . . . . . . . . . . . . . . . . . . . . . p19.4 (388)
19.3.1 Fourier transform over GF(2)n
. . . . . . . . . . . . . . . . . . . . . . . . . . p19.4 (388)
The connection to PCPs: High level view . . . . . . . . . . . . . . . . . . . . p19.6 (390)
19.3.2 Analysis of the linearity test over GF(2) . . . . . . . . . . . . . . . . . . . . . p19.6 (390)
19.3.3 Coordinate functions, Long code and its testing . . . . . . . . . . . . . . . . . p19.7 (391)
19.4 Proof of Theorem 19.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p19.9 (393)
19.5 Learning Fourier Coeﬃcients
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p19.13 (397)
19.6 Other PCP Theorems: A Survey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p19.14 (398)
19.6.1 PCP’s with sub-constant soundness parameter. . . . . . . . . . . . . . . . . . p19.14 (398)
19.6.2 Amortized query complexity.
. . . . . . . . . . . . . . . . . . . . . . . . . . . p19.15 (399)
19.6.3 Unique games.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p19.15 (399)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p19.15 (399)
20 Quantum Computation
p20.1 (401)
20.1 Quantum weirdness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.2 (402)
20.1.1 The 2-slit experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.2 (402)
20.1.2 Quantum entanglement and the Bell inequalities. . . . . . . . . . . . . . . . . p20.3 (403)
20.2 A new view of probabilistic computation.
. . . . . . . . . . . . . . . . . . . . . . . . p20.5 (405)
20.3 Quantum superposition and the class BQP . . . . . . . . . . . . . . . . . . . . . . . p20.8 (408)
20.3.1 Universal quantum operations . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.13 (413)
20.3.2 Spooky coordination and Bell’s state . . . . . . . . . . . . . . . . . . . . . . . p20.13 (413)
Web draft 2007-01-08 21:59

DRAFT
xiv
CONTENTS
20.4 Quantum programmer’s toolkit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.15 (415)
20.5 Grover’s search algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.16 (416)
20.6 Simon’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.21 (421)
20.6.1 The algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.21 (421)
20.7 Shor’s algorithm: integer factorization using quantum computers. . . . . . . . . . . . p20.22 (422)
20.7.1 Quantum Fourier Transform over ZM. . . . . . . . . . . . . . . . . . . . . . . p20.23 (423)
Deﬁnition of the Fourier transform over ZM.
. . . . . . . . . . . . . . . . . . p20.23 (423)
Fast Fourier Transform
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.24 (424)
Quantum Fourier transform: proof of Lemma 20.20 . . . . . . . . . . . . . . . p20.24 (424)
20.7.2 The Order-Finding Algorithm.
. . . . . . . . . . . . . . . . . . . . . . . . . . p20.25 (425)
Analysis: the case that r|M . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.26 (426)
The case that r ̸ |M
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.26 (426)
20.7.3 Reducing factoring to order ﬁnding.
. . . . . . . . . . . . . . . . . . . . . . . p20.28 (428)
20.8 BQP and classical complexity classes
. . . . . . . . . . . . . . . . . . . . . . . . . . p20.29 (429)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.29 (429)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p20.31 (431)
20.A Rational approximation of real numbers . . . . . . . . . . . . . . . . . . . . . . . . . p20.32 (432)
21 Logic in complexity theory
p21.1 (433)
21.1 Logical deﬁnitions of complexity classes
. . . . . . . . . . . . . . . . . . . . . . . . . p21.2 (434)
21.1.1 Fagin’s deﬁnition of NP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p21.2 (434)
21.1.2 MAX-SNP
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p21.3 (435)
21.2 Proof complexity as an approach to NP versus coNP . . . . . . . . . . . . . . . . . p21.3 (435)
21.2.1 Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p21.3 (435)
21.2.2 Frege Systems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p21.4 (436)
21.2.3 Polynomial calculus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p21.4 (436)
21.3 Is P ̸= NP unproveable?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p21.4 (436)
22 Why are circuit lowerbounds so diﬃcult?
p22.1 (437)
22.1 Formal Complexity Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p22.1 (437)
22.2 Natural Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p22.3 (439)
22.3 Limitations of Natural Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p22.5 (441)
22.4 My personal view . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p22.6 (442)
Exercises
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p22.7 (443)
Chapter notes and history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . p22.7 (443)
Appendices
p22.9 (445)
A Mathematical Background.
pA.1 (447)
A.1 Mathematical Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.1 (447)
A.2 Sets, Functions, Pairs, Strings, Graphs, Logic. . . . . . . . . . . . . . . . . . . . . . . pA.3 (449)
A.3 Probability theory
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.4 (450)
A.3.1
Random variables and expectations. . . . . . . . . . . . . . . . . . . . . . . . pA.5 (451)
Web draft 2007-01-08 21:59

DRAFT
CONTENTS
xv
A.3.2
The averaging argument . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.6 (452)
A.3.3
Conditional probability and independence . . . . . . . . . . . . . . . . . . . . pA.7 (453)
A.3.4
Deviation upperbounds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.7 (453)
A.3.5
Some other inequalities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.9 (455)
Jensen’s inequality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.9 (455)
Approximating the binomial coeﬃcient . . . . . . . . . . . . . . . . . . . . . . pA.9 (455)
More useful estimates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.10 (456)
A.4 Finite ﬁelds and groups
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.10 (456)
A.4.1
Non-prime ﬁelds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.11 (457)
A.4.2
Groups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.11 (457)
A.5 Vector spaces and Hilbert spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.12 (458)
A.6 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pA.12 (458)
Web draft 2007-01-08 21:59

DRAFT
xvi
CONTENTS
Web draft 2007-01-08 21:59

DRAFT
Introduction
“As long as a branch of science oﬀers an abundance of problems, so long it is alive;
a lack of problems foreshadows extinction or the cessation of independent develop-
ment.”
David Hilbert, 1900
“The subject of my talk is perhaps most directly indicated by simply asking two ques-
tions: ﬁrst, is it harder to multiply than to add? and second, why?...I (would like
to) show that there is no algorithm for multiplication computationally as simple as
that for addition, and this proves something of a stumbling block.”
Alan Cobham, 1964 [Cob64]
The notion of computation has existed in some form for thousands of years. In its everyday
meaning, this term refers to the process of producing an output from a set of inputs in a ﬁnite
number of steps. Here are three examples for computational tasks:
• Given two integer numbers, compute their product.
• Given a set of n linear equations over n variables, ﬁnd a solution if it exists.
• Given a list of acquaintances and a list of containing all pairs of individuals who are not
on speaking terms with each other, ﬁnd the largest set of acquaintances you can invite to a
dinner party such that you do not invite any two who are not on speaking terms.
In the ﬁrst half of the 20th century, the notion of “computation” was made much more precise
than the hitherto informal notion of “a person writing numbers on a note pad following certain
rules.” Many diﬀerent models of computation were discovered —Turing machines, lambda calculus,
cellular automata, pointer machines, bouncing billiards balls, Conway’s Game of life, etc.— and
found to be equivalent. More importantly, they are all universal, which means that each is capable
of implementing all computations that we can conceive of on any other model (see Chapter 1). The
notion of universality motivated the invention of the standard electronic computer, which is capable
of executing all possible programs. The computer’s rapid adoption in society in the subsequent
half decade brought computation into every aspect of modern life, and made computational issues
important in design, planning, engineering, scientiﬁc discovery, and many other human endeavors.
However, computation is not just a practical tool, but also a major scientiﬁc concept.
General-
izing from models such as cellular automata, scientists have come to view many natural phenomena
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p0.1 (1)

DRAFT
p0.2 (2)
CONTENTS
as akin to computational processes. The understanding of reproduction in living things was trig-
gered by the discovery of self-reproduction in computational machines. (In fact, a famous article
by Pauli predicted the existence of a DNA-like substance in cells almost a decade before Watson
and Crick discovered it.) Today, computational models underlie many research areas in biology
and neuroscience. Several physics theories such as QED give a description of nature that is very
reminiscent of computation, motivating some scientists to even suggest that the entire universe
may be viewed as a giant computer (see Lloyd [?]). In an interesting twist, such physical theories
have been used in the past decade to design a model for quantum computation; see Chapter 20.
From 1930s to the 1950s, researchers focused on the theory of computability and showed that
several interesting computational tasks are inherently uncomputable: no computer can solve them
without going into inﬁnite loops (i.e., never halting) on certain inputs. Though a beautiful theory,
it will not be our focus here. (But, see the texts [SIP96, HMU01, Koz97, ?].) Instead, we focus on
issues of computational eﬃciency. Computational complexity theory is concerned with how much
computational resources are required to solve a given task. The questions it studies include the
following:
1. Many computational tasks involve searching for a solution across a vast space of possibilities
(for example, the aforementioned tasks of solving linear equations and ﬁnding a maximal set
of invitees to a dinner party). Is there an eﬃcient search algorithm for all such tasks, or do
some tasks inherently require an exhaustive search?
As we will see in Chapter 2, this is the famous “P vs. NP” question that is considered the
central problem of complexity theory. Computational search tasks of the form above arise
in a host of disciplines including the life sciences, social sciences and operations research,
and computational complexity has provided strong evidence that many of these tasks are
inherently intractable.
2. Can algorithms use randomness (i.e., coin tossing) to speed up computation?
Chapter 7 presents probabilistic algorithms and shows several algorithms and techniques that
use probability to solve tasks more eﬃciently. But Chapters 16 and 17 show a surprising
recent result giving strong evidence that randomness does not help speed up computation,
in the sense that any probabilistic algorithm can be replaced with a deterministic algorithm
(tossing no coins) that is almost as eﬃcient.
3. Can hard problems be solved quicker if we allow the algorithms to err on a small number of
inputs, or to only compute an approximate solution?
Average-case complexity and approximation algorithms are studied in Chapters 15, 17, 18
and 19. These chapters also show fascinating connections between these questions, the power
of randomness, diﬀerent notions of mathematical proofs, and the theory of error correcting
codes.
4. Is there any use for computationally hard problems?
For example, can we use them to
construct secret codes that are unbreakable? (at least in the universe’s lifetime).
Our society increasingly relies on digital cryptography for commerce and security. As de-
scribed in Chapter 10, these secret codes are built using certain hard computational tasks
Web draft 2007-01-08 21:59

DRAFT
CONTENTS
p0.3 (3)
such as factoring integers. The security of digital cryptography is intimately related to the P
vs. NP question (see Chapter 2) and average-case complexity (see Chapters 15).
5. Can we use the counterintuitive quantum mechanical properties of our universe to solve hard
problems faster?
Chapter 20 describes the fascinating notion of quantum computers that use such properties to
speed up certain computations. Although there are many theoretical and practical obstacles
to actually building such computers, they have generated tremendous interest in recent years.
This is not least due to Shor’s algorithm that showed that, if built, quantum computers will be
able to factor integers eﬃciently. (Thus breaking many of the currently used cryptosystems.)
6. Can we generate mathematical proofs automatically? Can we check a mathematical proof
by only reading 3 probabilistically chosen letters from it? Do interactive proofs, involving
a dialog between prover and veriﬁer, have more power than standard “static” mathematical
proofs?
The notion of proof, central to mathematics, turns out to be central to computational com-
plexity as well, and complexity has shed new light on the meaning of mathematical proofs.
Whether mathematical proofs can be generated automatically turns out to depend on the
P vs. NP question (see Chapter 2). Chapter 18 describes probabilistically checkable proofs.
These are surprisingly robust mathematical proofs that can checked by only reading them in
very few probabilistically chosen locations. Interactive proofs are studied in Chapter 8. Fi-
nally, proof complexity, a subﬁeld of complexity studying the minimal proof length of various
statements, is studied in Chapter 21.
At roughly 40 years of age, Complexity theory is still an infant science. Thus we still do not
have complete answers for any of these questions. (In a surprising twist, computational complexity
has also been used to provide evidence for the hardness to solve some of the questions of . . .
computational complexity; see Chapter 22.) Furthermore, many major insights on these questions
were only found in recent years.
Meaning of eﬃciency
Now we explain the notion of computational eﬃciency, using the three examples for computational
tasks we mentioned above.
We start with the task of multiplying two integers.
Consider two
diﬀerent methods (or algorithms) to perform this task. The ﬁrst is repeated addition: to compute
a · b, just add a to itself b −1 times. The other is the grade-school algorithm illustrated in Figure 1.
Though the repeated addition algorithm is perhaps simpler than the grade-school algorithm, we
somehow feel that the latter is better. Indeed, it is much more eﬃcient. For example, multiplying
577 by 423 using repeated addition requires 422 additions, whereas doing it with the grade-school
algorithm requires only 3 additions and 3 multiplications of a number by a single digit.
We will quantify the eﬃciency of an algorithm by studying the number of basic operations it
performs as the size of the input increases. Here, the basic operations are addition and multiplication
of single digits. (In other settings, we may wish to throw in division as a basic operation.) The
Web draft 2007-01-08 21:59

DRAFT
p0.4 (4)
CONTENTS
5
7
7
4
2
3
1
7
3
1
1
1
5
4
2
3
0
8
2
4
4
0
7
1
Figure 1: Grade-school algorithm for multiplication. Illustrated for computing 577 · 423.
size of the input is the number of digits in the numbers. The number of basic operations used to
multiply two n-digit numbers (i.e., numbers between 10n−1 and 10n) is at most 2n2 for the grade-
school algorithm and at least n10n−1 for repeated addition. Phrased this way, the huge diﬀerence
between the two algorithms is apparent: even for 11-digit numbers, a pocket calculator running the
grade-school algorithm would beat the best current supercomputer running the repeated addition
algorithm. For slightly larger numbers even a ﬁfth grader with pen and paper would outperform a
supercomputer. We see that the eﬃciency of an algorithm is to a considerable extent much more
important than the technology used to execute it.
Surprisingly enough, there is an even faster algorithm for multiplication that uses the Fast
Fourier Transform. It was only discovered some 40 years ago and multiplies two n-digit numbers
using cn log n operations where c is some absolute constant independent of n. (Using asymptotic
notation, we call this an O(n log n)-step algorithm; see Chapter 1.)
Similarly, for the task of solving linear equations, the classic Gaussian elimination algorithm
(named after Gauss but already known in some form to Chinese mathematicians of the ﬁrst century)
uses O(n3) basic arithmetic operations to solve n equations over n variables. In the late 1960’s,
Strassen found a more eﬃcient algorithm that uses roughly O(n2.81) operations, and the best current
algorithm takes O(n2.376) operations.
The dinner party task also has an interesting story. As in the case of multiplication, there is an
obvious and simple ineﬃcient algorithm: try all possible subsets of the n people from the largest
to the smallest, and stop when you ﬁnd a subset that does not include any pair of guests who
are not on speaking terms. This algorithm can take as much time as the number of subsets of a
group of n people, which is 2n. This is highly unpractical —an organizer of, say, a 70-person party,
would need to plan it at least a thousand years in advance, even if she has a supercomputer at her
disposal. Surprisingly, we still do not know of a signiﬁcantly better algorithm. In fact, as we will
see in Chapter 2, we have reasons to suspect that no eﬃcient algorithm exists for this task. We
will see that it is equivalent to the independent set computational problem, which, together with
thousands of other important problems, is NP-complete. The famous “P versus NP” question
asks whether or not any of these problems has an eﬃcient algorithm.
Proving nonexistence of eﬃcient algorithms
We have seen that sometimes computational tasks have nonintuitive algorithms that are more
eﬃcient than algorithms that were known for thousands of years.
It would therefore be really
Web draft 2007-01-08 21:59

DRAFT
CONTENTS
p0.5 (5)
interesting to prove for some computational tasks that the current algorithm is the best —in other
words, no better algorithms exist. For instance, we could try to prove that the O(n log n)-step
algorithm for multiplication can never be improved (thus implying that multiplication is inherently
more diﬃcult than addition, which does have an O(n)-step algorithm). Or, we could try to prove
that there is no algorithm for the dinner party task that takes fewer than 2n/10 steps.
Since we cannot very well check every one of the inﬁnitely many possible algorithms, the only
way to verify that the current algorithm is the best is to mathematically prove that there is no better
algorithm. This may indeed be possible to do, since computation can be given a mathematically
precise model. There are several precedents for proving impossibility results in mathematics, such
as the independence of Euclid’s parallel postulate from the other basic axioms of geometry, or the
impossibility of trisecting an arbitrary angle using a compass and straightedge. Such results count
among the most interesting, fruitful, and surprising results in mathematics.
Given the above discussion, it is no surprise that mathematical proofs are the main tool of
complexity theory, and that this book is ﬁlled with theorems, deﬁnitions and lemmas. However,
we hardly use any fancy mathematics and so the main prerequisite for this book is the ability to
read (and perhaps even enjoy!) mathematical proofs. The reader might want to take a quick look
at Appendix A, that reviews mathematical proofs and other notions used, and come back to it as
needed.
We conclude with another quote from Hilbert’s 1900 lecture:
Proofs of impossibility were eﬀected by the ancients ... [and] in later mathematics, the
question as to the impossibility of certain solutions plays a preminent part. ...
In other sciences also one meets old problems which have been settled in a manner most
satisfactory and most useful to science by the proof of their impossibility. ... After
seeking in vain for the construction of a perpetual motion machine, the relations were
investigated which must subsist between the forces of nature if such a machine is to be
impossible; and this inverted question led to the discovery of the law of the conservation
of energy. ...
It is probably this important fact along with other philosophical reasons that gives rise
to conviction ... that every deﬁnite mathematical problem must necessary be susceptible
of an exact settlement, either in the form of an actual answer to the question asked, or
by the proof of the impossibility of its solution and therewith the necessary failure of all
attempts. ... This conviction... is a powerful incentive to the worker. We hear within
us the perpetual call: There is the problem. Seek its solution. You can ﬁnd it by pure
reason, for in mathematics there is no ignorabimus.
Web draft 2007-01-08 21:59

DRAFT
p0.6 (6)
CONTENTS
Web draft 2007-01-08 21:59

DRAFT
CONTENTS
p0.7 (7)
Conventions:
A whole number is a number in the set Z = {0, ±1, ±2, . . .}. A number denoted
by one of the letters i, j, k, ℓ, m, n is always assumed to be whole. If n ≥1, then we denote by [n]
the set {1, . . . , n}. For a real number x, we denote by ⌈x ⌉the smallest n ∈Z such that n ≥x and
by ⌊x ⌋the largest n ∈Z such that n ≤x. Whenever we use a real number in a context requiring
a whole number, the operator ⌈⌉is implied. We denote by log x the logarithm of x to the base 2.
We say that a condition holds for suﬃciently large n if it holds for every n ≥N for some number N
(for example, 2n > 100n2 for suﬃciently large n). We use expressions such as P
i f(i) (as opposed
to, say, Pn
i=1 f(i)) when the range of values i takes is obvious from the context. If u is a string or
vector, then ui denotes the value of the ith symbol/coordinate of u.
Web draft 2007-01-08 21:59

DRAFT
p0.8 (8)
CONTENTS
Web draft 2007-01-08 21:59

DRAFT
Part I
Basic Complexity Classes
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p0.9 (9)

DRAFT

DRAFT
Chapter 1
The computational model —and why
it doesn’t matter
“The idea behind digital computers may be explained by saying that these machines
are intended to carry out any operations which could be done by a human computer.
The human computer is supposed to be following ﬁxed rules; he has no authority to
deviate from them in any detail. We may suppose that these rules are supplied in a
book, which is altered whenever he is put on to a new job. He has also an unlimited
supply of paper on which he does his calculations.”
Alan Turing, 1950
“[Turing] has for the ﬁrst time succeeded in giving an absolute deﬁnition of an in-
teresting epistemological notion, i.e., one not depending on the formalism chosen.”
Kurt G¨odel, 1946
The previous chapter gave an informal introduction to computation and eﬃcient computations
in context of arithmetic.
IN this chapter we show a more rigorous and general deﬁnition.
As
mentioned earlier, one of the surprising discoveries of the 1930s was that all known computational
models are able to simulate each other. Thus the set of computable problems does not depend upon
the computational model.
In this book we are interested in issues of computational eﬃciency, and therefore in classes of
“eﬃciently computable” problems. Here, at ﬁrst glance, it seems that we have to be very careful
about our choice of a computational model, since even a kid knows that whether or not a new video
game program is “eﬃciently computable” depends upon his computer’s hardware. Surprisingly
though, we can restrict attention to a single abstract computational model for studying many
questions about eﬃciency—the Turing machine. The reason is that the Turing Machine seems able
to simulate all physically realizable computational models with very little loss of eﬃciency. Thus
the set of “eﬃciently computable” problems is at least as large for the Turing Machine as for any
other model. (One possible exception is the quantum computer model, but we do not currently
know if it is physically realizable.)
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p1.1 (11)

DRAFT
p1.2 (12)
1.1. ENCODINGS AND LANGUAGES: SOME CONVENTIONS
The Turing machine is a simple embodiment of the age-old intuition that computation consists
of applying mechanical rules to manipulate numbers, where the person/machine doing the manip-
ulation is allowed a scratch pad on which to write the intermediate results. The Turing Machine
can be also viewed as the equivalent of any modern programming language — albeit one with no
built-in prohibition about memory size1. In fact, this intuitive understanding of computation will
suﬃce for most of the book and most readers can skip many details of the model on a ﬁrst reading,
returning to them later as needed.
The rest of the chapter formally deﬁnes the Turing Machine and the notion of running time,
which is one measure of computational eﬀort. It also presents the important notion of the universal
Turing machine. Section 1.5 introduces a class of “eﬃciently computable” problems called P (which
stands for Polynomial time) and discuss its philosophical signiﬁcance. The section also points out
how throughout the book the deﬁnition of the Turing Machine and the class P will be a starting
point for deﬁnitions of many other models, including nondeterministic, probabilistic and quantum
Turing machines, Boolean circuits, parallel computers, decision trees, and communication games.
Some of these models are introduced to study arguably realizable modes of physical computation,
while others are mainly used to gain insights on Turing machines.
1.1
Encodings and Languages: Some conventions
Below we specify some of the notations and conventions used throughout this chapter and this
book to represent computational problem. We make use of some notions from discrete math such
as strings, sets, functions, tuples, and graphs. All of these notions are reviewed in Appendix ??.
1.1.1
Representing objects as strings
In general we study the complexity of computing a function whose input and output are ﬁnite
strings of bits. (A string of bits is a ﬁnite sequence of zeroes and ones. The set of all strings of
length n is denoted by {0, 1}n, while the set of all strings is denoted by {0, 1}∗= ∪n≥0 {0, 1}n; see
Appendix A.) Note that simple encodings can be used to represent general objects—integers, pairs
of integers, graphs, vectors, matrices, etc.— as strings of bits. For example, we can represent an
integer as a string using the binary expansion (e.g., 34 is represented as 100010) and a graph as
its adjacency matrix (i.e., an n vertex graph G is represented by an n × n 0/1-valued matrix A
such that Ai,j = 1 iﬀthe edge (i, j) is present in G). We will typically avoid dealing explicitly with
such low level issues of representation, and will use ⌞x⌟to denote some canonical (and unspeciﬁed)
binary representation of the object x. Often we will drop the symbols
⌞⌟and simply use x to
denote both the object and its representation.
Representing pairs and tuples.
We use the notation ⟨x, y⟩to denote the ordered pair consisting
of x and y. A canonical representation for ⟨x, y⟩can be easily obtained from the representations
of x and y. For example, we can ﬁrst encode ⟨x, y⟩as the string
⌞x⌟◦# ◦⌞y⌟over the alphabet
1Though the assumption of an inﬁnite memory may seem unrealistic at ﬁrst, in the complexity setting it is of no
consequence since we will restrict the machine to use a ﬁnite amount of tape cells for any given input (the number
allowed will depend upon the input size).
Web draft 2007-01-08 21:59

DRAFT
1.1. ENCODINGS AND LANGUAGES: SOME CONVENTIONS
p1.3 (13)
{0, 1, #} (where ◦denotes concatenation) and then use the mapping 0 7→00, 1 7→11, # 7→01 to
convert this into a string of bits. To reduce notational clutter, instead of
⌞⟨x, y⟩⌟we use ⟨x, y⟩to
denote not only the pair consisting of x and y but also the representation of this pair as a binary
string. Similarly, we use ⟨x, y, z⟩to denote both the ordered triple consisting of x, y, z and its
representation, and use similar notation for 4-tuples, 5-tuples etc..
1.1.2
Decision problems / languages
An important special case of functions mapping strings to strings is the case of Boolean functions,
whose output is a single bit. We identify such a function f with the set Lf = {x : f(x) = 1} and
call such sets languages or decision problems (we use these terms interchangeably). We identify the
computational problem of computing f (i.e., given x compute f(x)) with the problem of deciding
the language Lf (i.e., given x, decide whether x ∈Lf).
Example 1.1
By representing the possible invitees to a dinner party with the vertices of a graph having an edge
between any two people that can’t stand one another, the dinner party computational problem
from the introduction becomes the problem of ﬁnding a maximum sized independent set (set of
vertices not containing any edges) in a given graph. The corresponding language is:
INDSET = {⟨G, k⟩: ∃S ⊆V (G) s.t. |S| ≥k and ∀u, v ∈S, u v ̸∈E(G)}
An algorithm to solve this language will tell us, on input a graph G and a number k, whether
there exists a conﬂict-free set of invitees, called an independent set, of size at least k. It is not
immediately clear that such an algorithm can be used to actually ﬁnd such a set, but we will see
this is the case in Chapter 2. For now, let’s take it on faith that this is a good formalization of this
problem.
1.1.3
Big-Oh notation
As mentioned above, we will typically measure the computational eﬃciency algorithm as the number
of a basic operations it performs as a function of its input length. That is, the eﬃciency of an
algorithm can be captured by a function T from the set of natural numbers N to itself such that
T(n) is equal to the maximum number of basic operations that the algorithm performs on inputs
of length n. However, this function is sometimes be overly dependant on the low-level details of
our deﬁnition of a basic operation. For example, the addition algorithm will take about three times
more operations if it uses addition of single digit binary (i.e., base 2) numbers as a basic operation,
as opposed to decimal (i.e., base 10) numbers. To help us ignore these low level details and focus
on the big picture, the following well known notation is very useful:
Web draft 2007-01-08 21:59

DRAFT
p1.4 (14)
1.2. MODELING COMPUTATION AND EFFICIENCY
Definition 1.2 (Big-Oh notation)
If f, g are two functions from N to N, then we (1) say that f = O(g) if there exists a constant c
such that f(n) ≤c · g(n) for every suﬃciently large n, (2) say that f = Ω(g) if g = O(f), (3) say
that f = Θ(g) is f = O(g) and g = O(f), (4) say that f = o(g) if for every ϵ > 0, f(n) ≤ϵ · g(n)
for every suﬃciently large n, and (5) say that f = ω(g) if g = o(f).
To emphasize the input parameter, we often write f(n) = O(g(n)) instead of f = O(g), and
use similar notation for o, Ω, ω, Θ.
Example 1.3
Here are some examples for use of big-Oh notation:
1. If f(n) = 100n log n and g(n) = n2 then we have the relations f = O(g), g = Ω(f), f = o(g),
g = ω(f).
2. If f(n) = 100n2 +24n+2logn and g(n) = n2 then f = O(g). We will often write this relation
as f(n) = O(n2). Note that we also have the relation g = O(f) and hence f = Θ(g) and
g = Θ(f).
3. If f(n) = min{n, 106} and g(n) = 1 for every n then f = O(g). We use the notation f = O(1)
to denote this. Similarly, if h is a function that tends to inﬁnity with n (i.e., for every c it
holds that h(n) > c for n suﬃciently large) then we write h = ω(1).
4. If f(n) = 2n then for every number c ∈N, if g(n) = nc then g = o(f). We sometimes write
this as 2n = nω(1). Similarly, we also write h(n) = nO(1) to denote the fact that h is bounded
from above by some polynomial. That is, there exist a number c > 0 such that for suﬃciently
large n, h(n) ≤nc.
For more examples and explanations, see any undergraduate algorithms text such as [KT06,
CLRS01] or Section 7.1 in Sipser’s book [SIP96].
1.2
Modeling computation and eﬃciency
We start with an informal description of computation. Let f be a function that takes a string
of bits (i.e., a member of the set {0, 1}∗) and outputs, say, either 0 or 1. Informally speaking, an
algorithm for computing f is a set of mechanical rules, such that by following them we can compute
f(x) given any input x ∈{0, 1}∗. The set of rules being followed is ﬁxed (i.e., the same rules must
work for all possible inputs) though each rule in this set may be applied arbitrarily many times.
Each rule involves one or more of the following “elementary” operations:
1. Read a bit of the input.
Web draft 2007-01-08 21:59

DRAFT
1.2. MODELING COMPUTATION AND EFFICIENCY
p1.5 (15)
2. Read a bit (or possibly a symbol from a slightly larger alphabet, say a digit in the set
{0, . . . , 9}) from the “scratch pad” or working space we allow the algorithm to use.
Based on the values read,
3. Write a bit/symbol to the scratch pad.
4. Either stop and output 0 or 1, or choose a new rule from the set that will be applied next.
Finally, the running time is the number of these basic operations performed.
Below, we formalize all of these notions.
1.2.1
The Turing Machine
The k-tape Turing machine is a concrete realization of the above informal notion, as follows (see
Figure 1.1).
Scratch Pad: The scratch pad consists of k tapes. A tape is an inﬁnite one-directional line of
cells, each of which can hold a symbol from a ﬁnite set Γ called the alphabet of the machine. Each
tape is equipped with a tape head that can potentially read or write symbols to the tape one cell
at a time. The machine’s computation is divided into discrete time steps, and the head can move
left or right one cell in each step.
The ﬁrst tape of the machine is designated as the input tape. The machine’s head can can only
read symbols from that tape, not write them —a so-called read-only head.
The k −1 read-write tapes are called work tapes and the last one of them is designated as the
output tape of the machine, on which it writes its ﬁnal answer before halting its computation.
Finite set of operations/rules: The machine has a ﬁnite set of states, denoted Q. The machine
contains a “register” that can hold a single element of Q; this is the ”state” of the machine at
that instant. This state determines its action at the next computational step, which consists of the
following: (1) read the symbols in the cells directly under the k heads (2) for the k −1 read/write
tapes replace each symbol with a new symbol (it has the option of not changing the tape by writing
down the old symbol again), (3) change its register to contain another state from the ﬁnite set Q
(it has the option not to change its state by choosing the old state again) and (4) move each head
one cell to the left or to the right.
One can think of the Turing machine as a simpliﬁed modern computer, with the machine’s tape
corresponding to a computer’s memory, and the transition function and register corresponding to
the computer’s central processing unit (CPU). However, it’s best to think of Turing machines as
simply a formal way to describe algorithms.
Even though algorithms are often best described
by plain English text, it is sometimes useful to express them by such a formalism in order to
argue about them mathematically. (Similarly, one needs to express an algorithm in a programming
language in order to execute it on a computer.)
Formal deﬁnition.
Formally, a TM M is described by a tuple (Γ, Q, δ) containing:
Web draft 2007-01-08 21:59

DRAFT
p1.6 (16)
1.2. MODELING COMPUTATION AND EFFICIENCY
Input
tape
Work
tape
Output
tape
>
0
0 0
1
1 0
1 0
0
0 1 0
0 0
0
> 1
1 0 1
0
1 0
0
0
1
>
0
1
q7
Register
read only head
read/write head
read/write head
Figure 1.1: A snapshot of the execution of a 3-tape Turing machine M with an input tape, a work tape, and an
output tape.
• A set Γ of the symbols that M’s tapes can contain. We assume that Γ contains a designated
“blank” symbol, denoted □, a designated “start” symbol, denoted ▷and the numbers 0 and
1. We call Γ the alphabet of M.
• A set Q of possible states M’s register can be in. We assume that Q contains a designated
start state, denoted qstart and a designated halting state, denoted qhalt.
• A function δ:Q × Γk →Q × Γk−1 × {L, S, R}k describing the rule M uses in performing each
step. This function is called the transition function of M (see Figure 1.2.)
IF
THEN
input
symbol
read
work/
output
tape
symbol
read
current
state
move
input
head
new
work/
output
tape
symbol
move
work/
output
tape
new
state
...
...
...
...
...
...
...
...
...
...
...
...
...
...
a
b
q
b’
q’
Figure 1.2: The transition function of a two tape TM (i.e., a TM with one input tape and one work/output tape).
If the machine is in state q ∈Q and (σ1, σ2, . . . , σk) are the symbols currently being read in the
k tapes, and δ(q, (σ1, . . . , σk+1)) = (q′, (σ′
2, . . . , σ′
k), z) where z ∈{L, SR}k then at the next step the
σ symbols in the last k −1 tapes will be replaced by the σ′ symbols, the machine will be in state
Web draft 2007-01-08 21:59

DRAFT
1.2. MODELING COMPUTATION AND EFFICIENCY
p1.7 (17)
q′, and the k + 1 heads will move Left, Right or Stay in place, as given by z. (If the machine tries
to move left from the leftmost position of a tape then it will stay in place.)
All tapes except for the input are initialized in their ﬁrst location to the start symbol ▷and in
all other locations to the blank symbol □. The input tape contains initially the start symbol ▷, a
ﬁnite non-blank string (“the input”), and the rest of its cells are initialized with the blank symbol
□. All heads start at the left ends of the tapes and the machine is in the special starting state qstart.
This is called the start conﬁguration of M on input x. Each step of the computation is performed
by applying the function δ as described above. The special halting state qhalt has the property that
once the machine is in qhalt, the transition function δ does not allow it to further modify the tape
or change states. Clearly, if the machine enters qhalt then it has halted. In complexity theory we
are typically only interested in machines that halt for every input in a ﬁnite number of steps.
Now we formalize the notion of running time. As every non-trivial algorithm needs to at least
read its entire input, by “quickly” we mean that the number of basic steps we use is small when
considered as a function of the input length.
Definition 1.4 (Computing a function and running time)
Let f : {0, 1}∗→{0, 1}∗and let T : N →N be some functions, and let M be a
Turing machine. We say that M computes f in T(n)-time2 if for every x ∈{0, 1}∗,
if M is initialized to the start conﬁguration on input x, then after at most T(|x|)
steps it halts with f(x) written on its output tape.
We say that M computes f if it computes f in T(n) time for some function T : N →
N.
Remark 1.5 (Time-constructible functions)
We say that a function T : N →N is time constructible if T(n) ≥n and there is a TM M
that computes the function x 7→
⌞T(|x|)⌟in time T(n). (As usual,
⌞T(|x|)⌟denotes the binary
representation of the number T(|x|).)
Examples for time-constructible functions are n, n log n, n2, 2n. Almost all functions encoun-
tered in this book will be time-constructible and, to avoid annoying anomalities, we will restrict
our attention to time bounds of this form. (The restriction T(n) ≥n is to allow the algorithm time
to read its input.)
Example 1.6
Let PAL be the Boolean function deﬁned as follows: for every x ∈{0, 1}∗, PAL(x) is equal to 1 if
x is a palindrome and equal to 0 otherwise. That is, PAL(x) = 1 if and only if x reads the same
from left to right as from right to left (i.e., x1x2 . . . xn = xnxn−1 . . . x1). We now show a TM M
that computes PAL within less than 3n steps.
2Formally we should write “T-time” instead of “T(n)-time”, but we follow the convention of writing T(n) to
emphasize that T is applied to the input length.
Web draft 2007-01-08 21:59

DRAFT
p1.8 (18)
1.2. MODELING COMPUTATION AND EFFICIENCY
Our TM M will use 3 tapes (input, work and output) and the alphabet {▷, □, 0, 1}. It operates
as follows:
1. Copy the input to the read/write work tape.
2. Move the input head to the beginning of the input.
3. Move the input-tape head to the right while moving the work-tape head to the left. If at any
moment the machine observes two diﬀerent values, it halts and output 0.
4. Halt and output 1.
We now describe the machine more formally: The TM M uses 5 states denoted by {qstart, qcopy, qright, qtest, qhalt}.
Its transition function is deﬁned as follows:
1. On state qstart, move the input-tape head to the right, and move the work-tape head to the
right while writing the start symbol ▷; change the state to qcopy. (Unless we mention this
explicitly, the function does not change the output tape’s contents or head position.)
2. On state qcopy:
• If the symbol read from the input tape is not the blank symbol □then move both the
input-tape and work-tape heads to the right, writing the symbol from the input-tape on
the work-tape; stay in the state qcopy.
• If the symbol read from the input tape is the blank symbol □, then move the input-tape
head to the left, while keeping the work-tape head in the same place (and not writing
anything); change the state to qright.
3. On state qright:
• If the symbol read from the input tape is not the start symbol ▷then move the input-
head to the left, keeping the work-tape head in the same place (and not writing anything);
stay in the state qright.
• If the symbol read from the input tape is the start symbol ▷then move the input-tape
to the right and the work-tape head to the left (not writing anything); change to the
state qtest.
4. On state qtest:
• If the symbol read from the input-tape is the blank symbol □and the symbol read from
the work-tape is the start symbol ▷then write 1 on the output tape and change state
to qhalt.
• Otherwise, if the symbols read from the input tape and the work tape are not the same
then write 0 on the output tape and change state to qhalt.
Web draft 2007-01-08 21:59

DRAFT
1.2. MODELING COMPUTATION AND EFFICIENCY
p1.9 (19)
• Otherwise, if the symbols read from the input tape and the work tape are the same,
then move the input-tape head to the right and the work-tape head to the left; stay in
the state qtest.
As you can see, fully specifying a Turing machine is somewhat tedious and not always very
informative. While it is useful to work out one or two examples for yourself (see Exercise 4), in the
rest of the book we avoid such overly detailed descriptions and specify TM’s in a more high level
fashion.
Remark 1.7
Some texts use as their computational model single tape Turing machines, that have one read/write
tape that serves as input, work and output tape. This choice does not make any diﬀerence for most
of this book’s results (see Exercise 10). However, Example 1.6 is one exception: it can be shown
that such machines require Ω(n2) steps to compute the function PAL.
1.2.2
Robustness of our deﬁnition.
Most of the speciﬁc details of our deﬁnition of Turing machines are quite arbitrary. For example,
the following simple claims show that restricting the alphabet Γ to be {0, 1, □, ▷}, restricting the
machine to have a single work tape, or allowing the tapes to be inﬁnite in both directions will not
have a signiﬁcant eﬀect on the time to compute functions: (Below we provide only proof sketches
for these claims; completing these sketches into full proofs is a very good way to gain intuition on
Turing machines, see Exercises 5, 6 and 7.)
Claim 1.8
For every f : {0, 1}∗→{0, 1} and time-constructible T : N →N, if f is computable in time T(n)
by a TM M using alphabet Γ then it is computable in time 4 log |Γ|T(n) by a TM ˜
M using the
alphabet {0, 1, □, ▷}.
M’s tape:
>
m a
c h
i
n
e
~
>
0 1
1
0
1 0
0
0
0
1 0
0
0 1
1
M’s tape:
Figure 1.3: We can simulate a machine M using the alphabet {▷, □, a, b, . . . , z} by a machine M ′ using {▷, □, 0, 1}
via encoding every tape cell of M using 5 cells of M ′.
Proof Sketch:
Let M be a TM with alphabet Γ, k tapes, and state set Q that computes the
function f in T(n) time. We will show a TM ˜
M computing f with alphabet {0, 1, □, ▷}, k tapes
and a set Q′ of states which will be described below. The idea behind the proof is simple: one can
encode any member of Γ using log |Γ| bits.3 Thus, each of ˜
M’s work tapes will simply encode one
3Recall our conventions that log is taken to base 2, and non-integer numbers are rounded up when necessary.
Web draft 2007-01-08 21:59

DRAFT
p1.10 (20)
1.2. MODELING COMPUTATION AND EFFICIENCY
of M’s tapes: for every cell in M’s tape we will have log |Γ| cells in the corresponding tape of ˜
M
(see Figure 1.3).
To simulate one step of M, the machine ˜
M will: (1) use log |Γ| steps to read from each tape the
log |Γ| bits encoding a symbol of Γ (2) use its state register to store the symbols read, (3) use M’s
transition function to compute the symbols M writes and M’s new state given this information,
(3) store this information in its state register, and (4) use log |Γ| steps to write the encodings of
these symbols on its tapes.
One can verify that this can be carried out if ˜
M has access to registers that can store M’s state,
k symbols in Γ and a counter from 1 to k. Thus, there is such a machine ˜
M utilizing no more than
10|Q||Γ|kk states. (In general, we can always simulate several registers using one register with a
larger state space. For example, we can simulate three registers taking values in the sets A,B and
C respectively with one register taking a value in the set A × B × C which is of size |A||B||C|.)
It is not hard to see that for every input x ∈{0, 1}n, if on input x the TM M outputs f(x)
within T(n) steps, then ˜
M will output the same value within less than 4 log |Γ|T(n) steps. ■
Claim 1.9
For every f : {0, 1}∗→{0, 1}, time-constructible T : N →N, if f is computable in time T(n)
by a TM M using k tapes (plus additional input and output tapes) then it is computable in time
5kT(n)2 by a TM ˜
M using only a single work tape (plus additional input and output tapes).
M’s 3 work tapes:
c
o m p
l e
t
e l
y
r
e
p
l a
c
e d
m a c
h
i n e
s
Encoding this in one tape of M:
c r
m o
e
a m
p c
p l
h l
a
i
e
c
n
1 2
3 1 2
3
1
2 3
1 2 3
1 2 3
1
2 3
Tape 1:
Tape 2:
Tape 3:
~
^
^
^
Figure 1.4: Simulating a machine M with 3 work tapes using a machine ˜
M with a single work tape (in addition to
the input and output tapes).
Proof Sketch: Again the idea is simple: the TM ˜
M encodes the k tapes of M on a single tape
by using locations 1, k +1, 2k +1, . . . to encode the ﬁrst tape, locations 2, k +2, 2k +2, . . . to encode
the second tape etc.. (see Figure 1.4). For every symbol a in M’s alphabet, ˜
M will contain both
the symbol a and the symbol ˆa. In the encoding of each tape, exactly one symbol will be of the “ˆ
type”, indicating that the corresponding head of M is positioned in that location (see ﬁgure).
˜
M
uses the input and output tape in the same way M does. To simulate one step of M, the machine
˜
M makes two sweeps of its work tape: ﬁrst it sweeps the tape in the left-to-right direction and
Web draft 2007-01-08 21:59

DRAFT
1.2. MODELING COMPUTATION AND EFFICIENCY
p1.11 (21)
records to its register the k symbols that are marked by ˆ. Then ˜
M uses M’s transition function
to determine the new state, symbols, and head movements and sweeps the tape back in the right-
to-left direction to update the encoding accordingly. Clearly, ˜
M will have the same output as M.
Also, since on n-length inputs M never reaches more than location T(n) of any of its tapes, ˜
M will
never need to reach more than location kT(n) of its work tape, meaning that for each the at most
T(n) steps of M, ˜
M performs at most 5kT(n) work (sweeping back and forth requires about 2T(n)
steps, and some additional steps may be needed for updating head movement and book keeping).
■
Remark 1.10
With a bit of care, one can ensure that the proof of Claim 1.9 yields a TM ˜
M with the following
property: the head movements of ˜
M are independent of the contents of its tapes but only on the
input length (i.e., ˜
M always performs a sequence of left to right and back sweeps of the same form
regardless of what is the input). A machine with this property is called oblivious and the fact that
every TM can be simulated by an oblivious TM will be useful for us later on (see Exercises 8 and 9
and the proof of Theorem 2.10).
Claim 1.11
Deﬁne a bidirectional TM to be a TM whose tapes are inﬁnite in both directions.
For every
f : {0, 1}∗→{0, 1}∗and time constructible T : N →N, if f is computable in time T(n) by a
bidirectional TM M then it is computable in time 4T(n) by a standard (unidirectional) TM ˜
M.
c
o m p
l e
t
e l
y
M’s tape is infinite in both directions:
e
t
e l
y
c
o m p
l
> e/l
t/d e/m l/o y/c
M uses a larger alphabet to represent it on a standard tape:
~
Figure 1.5: To simulate a machine M with alphabet Γ that has tapes inﬁnite in both directions, we use a machine
˜
M with alphabet Γ2 whose tapes encode the “folded” version of M’s tapes.
Proof Sketch: The idea behind the proof is illustrated in Figure 1.5. If M uses alphabet Γ then
˜
M will use the alphabet Γ2 (i.e., each symbol in ˜
M’s alphabet corresponds to a pair of symbols in
M’s alphabet). We encode a tape of M that is inﬁnite in both direction using a standard (inﬁnite in
one direction) tape by “folding” it in an arbitrary location, with each location of ˜
M’s tape encoding
two locations of M’s tape. At ﬁrst, ˜
M will ignore the second symbol in the cell it reads and act
according to M’s transition function. However, if this transition function instructs ˜
M to go “over
the edge” of its tape then instead it will start ignoring the ﬁrst symbol in each cell and use only the
second symbol. When it is in this mode, it will translate left movements into right movements and
vice versa. If it needs to go “over the edge” again then it will go back to reading the ﬁrst symbol
of each cell, and translating movements normally. ■
Web draft 2007-01-08 21:59

DRAFT
p1.12 (22)
1.3. MACHINES AS STRINGS AND THE UNIVERSAL TURING MACHINES.
Other changes that will not have a very signiﬁcant eﬀect include having two or three dimensional
tapes, allowing the machine random access to its tape, and making the output tape write only (see
Exercises 11 and 12; also the texts [SIP96, HMU01] contain more examples). In particular none
of these modiﬁcations will change the class P of polynomial-time computable decision problems
deﬁned below in Section 1.5.
1.2.3
The expressive power of Turing machines.
When you encounter Turing machines for the ﬁrst time, it may not be clear that they do indeed
fully encapsulate our intuitive notion of computation. It may be useful to work through some simple
examples, such as expressing the standard algorithms for addition and multiplication in terms of
Turing machines computing the corresponding functions (see Exercise 4). You can also verify that
you can simulate a program in your favorite programming language using a Turing machine. (The
reverse direction also holds: most programming languages can simulate a Turing machine.)
Example 1.12
(This example assumes some background in computing.) We give a hand-wavy proof that Turing
machines can simulate any program written in any of the familiar programming languages such
as C or Java. First, recall that programs in these programming languages can be translated (the
technical term is compiled) into an equivalent machine language program. This is a sequence of
simple instructions to read from memory into one of a ﬁnite number of registers, write a register’s
contents to memory, perform basic arithmetic operations, such as adding two registers, and control
instructions that perform actions conditioned on, say, whether a certain register is equal to zero.
All these operations can be easily simulated by a Turing machine. The memory and register can
be implemented using the machine’s tapes, while the instructions can be encoded by the machine’s
transition function. For example, it’s not hard to show TM’s that add or multiply two numbers,
or a two-tape TM that, if its ﬁrst tape contains a number i in binary representation, can move the
head of its second tape to the ith location.
Exercise 13 asks you to give a more rigorous proof of such a simulation for a simple tailor-made
programming language.
1.3
Machines as strings and the universal Turing machines.
It is almost obvious that a Turing machine can be represented as a string: since we can write the
description of any TM M on paper, we can deﬁnitely encode this description as a sequence of zeros
and ones. Yet this simple observation— that we can treat programs as data— has had far reaching
consequences on both the theory and practice of computing. Without it, we would not have had
general purpose electronic computers, that, rather than ﬁxed to performing one task, can execute
arbitrary programs.
Web draft 2007-01-08 21:59

DRAFT
1.3. MACHINES AS STRINGS AND THE UNIVERSAL TURING MACHINES.
p1.13 (23)
Because we will use this notion of representing TM’s as strings quite extensively, it may be
worth to spell out our representation out a bit more concretely. Since the behavior of a Turing
machine is determined by its transition function, we will use the list of all inputs and outputs of
this function (which can be easily encoded as a string in {0, 1}∗) as the encoding of the Turing
machine.4 We will also ﬁnd it convenient to assume that our representation scheme satisﬁes the
following properties:
1. Every string in {0, 1}∗represents some Turing machine.
This is easy to ensure by mapping strings that are not valid encodings into some canonical
trivial TM, such as the TM that immediately halts and outputs zero on any input.
2. Every TM is represented by inﬁnitely many strings.
This can be ensured by specifying that the representation can end with an arbitrary number of
1’s, that are ignored. This has somewhat similar eﬀect as the comments of many programming
languages (e.g., the /*...*/ construct in C,C++ and Java) that allows to add superﬂuous
symbols to any program.
If M is a Turing machine, then we use
⌞M⌟to denotes M’s representation as a binary string.
If α is a string then we denote the TM that α represents by Mα. As is our convention, we will also
often use M to denote both the TM and its representation as a string. Exercise 14 asks you to
fully specify a representation scheme for Turing machines with the above properties.
1.3.1
The Universal Turing Machine
It was Turing that ﬁrst observed that general purpose computers are possible, by showing a universal
Turing machine that can simulate the execution of every other TM M given M’s description as
input. Of course, since we are so used to having a universal computer on our desktops or even
in our pockets, today we take this notion for granted. But it is good to remember why it was
once counterintuitive.
The parameters of the universal TM are ﬁxed —alphabet size, number
of states, and number of tapes. The corresponding parameters for the machine being simulated
could be much larger. The reason this is not a hurdle is, of course, the ability to use encodings.
Even if the universal TM has a very simple alphabet, say {0, 1}, this is suﬃcient to allow it to
represent the other machine’s state and and transition table on its tapes, and then follow along in
the computation step by step.
Now we state a computationally eﬃcient version of Turing’s construction due to Hennie and
Stearns [HS66]. To give the essential idea we ﬁrst prove a slightly relaxed variant where the term
T log T below is replaced with T 2. But since the eﬃcient version is needed a few times in the book,
a full proof is also given at the end of the chapter (see Section 1.A).
4Note that the size of the alphabet, the number of tapes, and the size of the state space can be deduced from
the transition function’s table. We can also reorder the table to ensure that the special states qstart, qhalt are the ﬁrst
2 states of the TM. Similarly, we may assume that the symbols ▷, □, 0, 1 are the ﬁrst 4 symbols of the machine’s
alphabet.
Web draft 2007-01-08 21:59

DRAFT
p1.14 (24)
1.3. MACHINES AS STRINGS AND THE UNIVERSAL TURING MACHINES.
Theorem 1.13 (Efficient Universal Turing machine)
There exists a TM U such that for every x, α ∈{0, 1}∗, U(x, α) = Mα(x), where Mα
denotes the TM represented by α.
Furthermore, if Mα halts on input x within T steps then U(x, α) halts within
CT log T steps, where C is a number independent of |x| and depending only on
Mα’s alphabet size, number of tapes, and number of states.
Remark 1.14
A common exercise in programming courses is to write an interpreter for a particular programming
language using the same language. (An interpreter takes a program P as input and outputs the
result of executing the program P.) Theorem 1.13 can be considered a variant of this exercise.
Proof: Our universal TM U is given an input x, α, where α represents some TM M, and needs
to output M(x). A crucial observation is that we may assume that M (1) has a single work tape
(in addition to the input and output tape) and (2) uses the alphabet {▷, □, 0, 1}. The reason is
that U can transform a representation of every TM M into a representation of an equivalent TM
˜
M that satisﬁes these properties as shown in the proofs of Claims 1.8 and 1.9. Note that these
transformations may introduce a quadratic slowdown (i.e., transform M from running in T time to
running in C′T 2 time where C′ depends on M’s alphabet size and number of tapes).
Input
tape
Work
tapes
Output
tape
>
0
0 0
1
1 0
1 0
0
0 1 0
0 0
0
>
0
1
Description of M
Current state of M
Simulation of M’s work tape.
(used in the same way as M)
(used in the same way as M)
(used in the same way as M)
Figure 1.6: The universal TM U has in addition to the input and output tape, three work tapes. One work tape
will have the same contents as the simulated machine M, another tape includes the description M (converted to an
equivalent one-work-tape form), and another tape contains the current state of M.
The TM U uses the alphabet {▷, □, 0, 1} and three work tapes in addition to its input and
output tape (see Figure 1.6). U uses its input tape, output tape, and one of the work tapes in
the same way M uses its three tapes. In addition, U will use its ﬁrst extra work tape to store the
table of values of M’s transition function (after applying the transformations of Claims 1.8 and 1.9
as noted above), and its other extra work tape to store the current state of M. To simulate one
computational step of M, U scans the table of M’s transition function and the current state to ﬁnd
out the new state, symbols to be written and head movements, which it then executes. We see that
each computational step of M is simulated using C steps of U, where C is some number depending
on the size of the transition function’s table.
Web draft 2007-01-08 21:59

DRAFT
1.4. UNCOMPUTABLE FUNCTIONS.
p1.15 (25)
This high level description can turned into an exact speciﬁcation of the TM U, though we leave
this to the reader. If you are not sure how this can be done, think ﬁrst of how you would program
these steps in your favorite programming language and then try to transform this into a description
of a Turing machine. ■
Remark 1.15
It is sometimes useful to consider a variant of the universal TM U that gets a number t as an
extra input (in addition to x and α), and outputs Mα(x) if and only if Mα halts on x within t
steps (otherwise outputting some special failure symbol). By adding a counter to U, the proof of
Theorem 1.13 can be easily modiﬁed to give such a universal TM with the same eﬃciency.
1.4
Uncomputable functions.
It may seem “obvious” that every function can be computed, given suﬃcient time. However, this
turns out to be false: there exist functions that cannot be computed within any ﬁnite number of
steps!
Theorem 1.16
There exists a function UC : {0, 1}∗→{0, 1} that is not computable by any TM.
Proof: The function UC is deﬁned as follows: for every α ∈{0, 1}∗, let M be the TM represented
by α. If on input α, M halts within a ﬁnite number of steps and outputs 1 then UC(α) is equal to
0, otherwise UC(α) is equal to 1.
Suppose for the sake of contradiction that there exists a TM M such that M(α) = UC(α) for
every α ∈{0, 1}∗. Then, in particular, M( ⌞M⌟) = UC( ⌞M⌟). But this is impossible: by the
deﬁnition of UC, if UC( ⌞M⌟) = 1 then M( ⌞M⌟) cannot be equal to 1, and if UC( ⌞M⌟) = 0 then
M( ⌞M⌟) cannot be equal to 0. This proof technique is called “diagnoalization”, see Figure 1.7. ■
1.4.1
The Halting Problem
One might ask why should we care whether or not the function UC described above is computable—
why would anyone want to compute such a contrived function anyway? We now show a more natural
uncomputable function. The function HALT takes as input a pair α, x and outputs 1 if and only if
the TM Mα represented by α halts on input x within a ﬁnite number of steps. This is deﬁnitely a
function we want to compute: given a computer program and an input we’d certainly like to know
if the program is going to enter an inﬁnite loop on this input. Unfortunately, this is not possible,
even if we were willing to wait an arbitrary long time:
Theorem 1.17
HALT is not computable by any TM.
Proof: Suppose, for the sake of contradiction, that there was a TM MHALT computing HALT. We
will use MHALT to show a TM MUC computing UC, contradicting Theorem 1.16.
The TM MUC is simple: on input α, we run MHALT(α, α). If the result is 0 (meaning that the
machine represented by α does not halt on α) then we output 1. Otherwise, we use the universal
Web draft 2007-01-08 21:59

DRAFT
p1.16 (26)
1.4. UNCOMPUTABLE FUNCTIONS.
0
1 00
01 10 11
...
α
....
0
1
00
01
...
α
...
01
1
*
0
1
0
M0(α)
1
*1 0
1
*
1
...
*
0
10
0
1
*
1
*
0
01 *
0
Mα(0) ...
Mα(α) 1-Mα(α)
Figure 1.7: Suppose we order all strings in lexicographic order, and write in a table the value of Mα(x) for all
strings α, x, where Mα denotes the TM represented by the string α and we use ⋆to denote the case that Mα(x) is
not a value in {0, 1} or that Mα does not halt on input x. Then, UC is deﬁned by “negating” the diagonal of this
table, and by its deﬁnition it cannot be computed by any TM.
TM U to compute M(α), where M is the TM represented by α. If M(α) = 0 we output 1, and
otherwise we output 1. Note that indeed, under the assumption that MHALT(α, x) outputs within
a ﬁnite number of steps HALT(α, x), the TM MUC(α) will output UC(α) within a ﬁnite number of
steps. ■
Remark 1.18
The proof technique employed to show Theorem 1.17— namely showing that HALT is uncomputable
by showing an algorithm for UC using a hypothetical algorithm for HALT— is called a reduction.
We will see many reductions in this book, often used (as is the case here) to show that a problem
B is at least as hard as a problem A, by showing an algorithm that could solve A given a procedure
that solves B.
There are many other examples for interesting uncomputable (also known as undecidable) func-
tions, see Exercise 15. There are even uncomputable functions whose formulation has seemingly
nothing to do with Turing machines or algorithms. For example, the following problem cannot
be solved in ﬁnite time by any TM: given a set of polynomial equations with integer coeﬃcients,
ﬁnd out whether these equations have an integer solution (i.e., whether there is an assignment of
integers to the variables that satisﬁes the equations). This is known as the problem of solving
Diophantine equations, and in 1900 Hilbert mentioned ﬁnding such algorithm to solve it (which he
presumed to exist) as one of the top 23 open problems in mathematics.
For more on computability theory, see the chapter notes and the book’s website.
Web draft 2007-01-08 21:59

DRAFT
1.5. DETERMINISTIC TIME AND THE CLASS P.
p1.17 (27)
1.5
Deterministic time and the class P.
A complexity class is a set of functions that can be computed within a given resource. We will
now introduce our ﬁrst complexity classes. For reasons of technical convenience, throughout most
of this book we will pay special attention to Boolean functions (that have one bit output), also
known as decision problems or languages. (Recall that we identify a Boolean function f with the
language Lf = {x : f(x) = 1}.)
Definition 1.19 (The class DTIME.)
Let T : N →N be some function. We let DTIME(T(n)) be the set of all Boolean (one bit output)
functions that are computable in c · T(n)-time for some constant c > 0.
The following class will serve as our rough approximation for the class of decision problems that
are eﬃciently solvable.
Definition 1.20 (The class P)
P = ∪c≥1DTIME(nc)
Thus, we can phrase the question from the introduction as to whether the dinner party problem
has an eﬃcient algorithm as follows: “Is INDSET in P?”, where INDSET is the language deﬁned
in Example 1.6.
1.5.1
On the philosophical importance of P
The class P is felt to capture the notion of decision problems with “feasible” decision procedures.
Of course, one may argue whether DTIME(n100) really represents “feasible” computation in the
real world. However, in practice, whenever we show that a problem is in P, we usually ﬁnd an n3
or n5 time algorithm (with reasonable constants), and not an n100 algorithm. (It has also happened
a few times that the ﬁrst polynomial-time algorithm for a problem had high complexity, say n20,
but soon somebody simpliﬁed it to say an n5 algorithm.)
Note that the class P is useful only in a certain context. Turing machines are a poor model
if one is designing algorithms that must run in a fraction of a second on the latest PC (in which
case one must carefully account for ﬁne details about the hardware). However, if the question is
whether any subexponential algorithms exist for say INDSET then even an n20 algorithm would be
a fantastic breakthrough.
P is also a natural class from the viewpoint of a programmer. Suppose undergraduate program-
mers are asked to invent the deﬁnition of an “eﬃcient” computation. Presumably, they would agree
that a computation that runs in linear or quadratic time is “eﬃcient.” Next, since programmers
often write programs that call other programs (or subroutines), they might ﬁnd it natural to con-
sider a program “eﬃcient” if it performs only “eﬃcient” computations and calls subroutines that
are “eﬃcient”. The notion of “eﬃciency” obtained turns out to be exactly the class P [Cob64].
Web draft 2007-01-08 21:59

DRAFT
p1.18 (28)
1.5. DETERMINISTIC TIME AND THE CLASS P.
1.5.2
Criticisms of P and some eﬀorts to address them
Now we address some possible criticisms of the deﬁnition of P, and some related complexity classes
that address these.
Worst-case exact computation is too strict. The deﬁnition of P only considers algorithms
that compute the function exactly on every possible input. However, not all possible inputs
arise in practice (although it’s not always easy to characterize the inputs that do). Chapter 15
gives a theoretical treatment of average-case complexity and deﬁnes the analogue of P in that
context. Sometimes, users are willing to settle for approximate solutions. Chapter 18 contains
a rigorous treatment of the complexity of approximation.
Other physically realizable models. If we were to make contact with an advanced alien civi-
lization, would their class P be any diﬀerent from the class deﬁned here?
Most scientists believe the Church-Turing (CT) thesis, which states that every physically
realizable computation device— whether it’s silicon-based, DNA-based, neuron-based or using
some alien technology— can be simulated by a Turing machine. Thus they believe that the
set of computable problems would be the same for aliens as it is for us. (The CT thesis is not
a theorem, merely a belief about the nature of the world.)
However, when it comes to eﬃciently computable problems, the situation is less clear. The
strong form of the CT thesis says that every physically realizable computation model can
be simulated by a TM with polynomial overhead (in other words, t steps on the model can
be simulated in tc steps on the TM, where c is a constant that depends upon the model).
If true, it implies that the class P deﬁned by the aliens will be the same as ours. However,
several objections have been made to this strong form.
(a) Issue of precision: TM’s compute with discrete symbols, whereas physical quantities may
be real numbers in R. Thus TM computations may only be able to approximately simulate
the real world. Though this issue is not perfectly settled, it seems so far that TMs do not
suﬀer from an inherent handicap. After all, real-life devices suﬀer from noise, and physical
quantities can only be measured up to ﬁnite precision. Thus a TM could simulate the real-life
device using ﬁnite precision. (Note also that we often only care about the most signiﬁcant bit
of the result, namely, a 0/1 answer.)
Even so, in Chapter 14 we also consider a modiﬁcation of the TM model that allows computa-
tions in R as a basic operation. The resulting complexity classes have fascinating connections
with the usual complexity classes.
(b) Use of randomness: The TM as deﬁned is deterministic. If randomness exists in the
world, one can conceive of computational models that use a source of random bits (i.e.,
”coin tosses”). Chapter 7 considers Turing Machines that are allowed to also toss coins, and
studies the class BPP, that is the analogue of P for those machines. (However, we will see in
Chapters 16 and 17 the intriguing possibility that randomized computation may be no more
powerful than deterministic computation.)
(c) Use of quantum mechanics: A more clever computational model might use some of the
counterintuitive features of quantum mechanics. In Chapter 20 we deﬁne the class BQP,
Web draft 2007-01-08 21:59

DRAFT
1.5. DETERMINISTIC TIME AND THE CLASS P.
p1.19 (29)
that generalizes P in such a way. We will see problems in BQP that are currently not known
to be in P. However, currently it is unclear whether the quantum model is truly physically
realizable. Even if it is realizable it currently seems only able to eﬃciently solve only very
few ”well-structured” problems that are (presumed to be) not in P. Hence insights gained
from studying P could still be applied to BQP.
(d) Use of other exotic physics, such as string theory. Though an intriguing possibility, it
hasn’t yet had the same scrutiny as quantum mechanics.
Decision problems are too limited. Some computational problems are not easily expressed as
decision problems. Indeed, we will introduce several classes in the book to capture tasks such
as computing non-Boolean functions, solving search problems, approximating optimization
problems, interaction, and more. Yet the framework of decision problems turn out to be
surprisingly expressive, and we will often use it in this book.
1.5.3
Edmonds’ quote
We conclude this section with a quote from Edmonds [Edm65], that in the paper showing a
polynomial-time algorithm for the maximum matching problem, explained the meaning of such
a result as follows:
For practical purposes computational details are vital. However, my purpose is only
to show as attractively as I can that there is an eﬃcient algorithm. According to the
dictionary, “eﬃcient” means “adequate in operation or performance.” This is roughly
the meaning I want in the sense that it is conceivable for maximum matching to have
no eﬃcient algorithm.
...There is an obvious ﬁnite algorithm, but that algorithm increases in diﬃculty expo-
nentially with the size of the graph. It is by no means obvious whether or not there exists
an algorithm whose diﬃculty increases only algebraically with the size of the graph.
...When the measure of problem-size is reasonable and when the sizes assume values
arbitrarily large, an asymptotic estimate of ... the order of diﬃculty of an algorithm is
theoretically important. It cannot be rigged by making the algorithm artiﬁcially diﬃcult
for smaller sizes.
...One can ﬁnd many classes of problems, besides maximum matching and its general-
izations, which have algorithms of exponential order but seemingly none better ... For
practical purposes the diﬀerence between algebraic and exponential order is often more
crucial than the diﬀerence between ﬁnite and non-ﬁnite.
...It would be unfortunate for any rigid criterion to inhibit the practical development of
algorithms which are either not known or known not to conform nicely to the criterion.
Many of the best algorithmic idea known today would suﬀer by such theoretical pedantry.
... However, if only to motivate the search for good, practical algorithms, it is important
to realize that it is mathematically sensible even to question their existence. For one
thing the task can then be described in terms of concrete conjectures.
Web draft 2007-01-08 21:59

DRAFT
p1.20 (30)
1.5. DETERMINISTIC TIME AND THE CLASS P.
What have we learned?
• There are many equivalent ways to mathematically model computational pro-
cesses; we use the standard Turing machine formalization.
• Turing machines can be represented as strings. There is a universal TM that
can emulate (with small overhead) any TM given its representation.
• There exist functions, such as the Halting problem, that cannot be computed
by any TM regardless of its running time.
• The class P consists of all decision problems that are solvable by Turing ma-
chines in polynomial time. We say that problems in P are eﬃciently solvable.
• All low-level choices (number of tapes, alphabet size, etc..) in the deﬁnition of
Turing machines are immaterial, as they will not change the deﬁnition of P.
Chapter notes and history
Although certain algorithms have been studied for thousands of years, and some forms of computing
devices were designed before the 20th century (most most notably Charles Babbage’s diﬀerence and
analytical engines in the mid 1800’s), it seems fair to say that the foundations of modern computer
science were only laid in the 1930’s.
In 1931, Kurt G¨odel shocked the mathematical world by showing that certain true statements
about the natural numbers are inherently unprovable, thereby shattering an ambitious agenda set
in 1900 by David Hilbert to base all of mathematics on solid axiomatic foundations.
In 1936,
Alonzo Church deﬁned a model of computation called λ-calculus (which years later inspired the
programming language LISP) and showed the existence of functions inherently uncomputable in this
model [Chu36]. A few months later, Alan Turing independently introduced his Turing machines
and showed functions inherently uncomputable by such machines [Tur36]. Turing also introduced
the idea of the universal Turing machine that can be loaded with arbitrary programs. The two
models turned out to be equivalent, but in the words of Church himself, Turing machines have “the
advantage of making the identiﬁcation with eﬀectiveness in the ordinary (not explicitly deﬁned)
sense evident immediately”. The anthology [Dav65] contains many of the seminal papers in the
theory of computability. Part II of Sipser’s book [SIP96] is a good gentle introduction to this theory,
while the books [?, HMU01, Koz97] go into a bit more depth. This book’s web site also contains
some additional coverage of this theory.
During World War II Turing designed mechanical code-breaking devices and played a key role
in the eﬀort to crack the German “Enigma” cipher, an achievement that had a decisive eﬀect on
the war’s progress (see the biographies [Hod83, Lea05]).5
After World War II, eﬀorts to build
electronic universal computers were undertaken in both sides of the Atlantic. A key ﬁgure in these
5Unfortunately, Turing’s wartime achievements were kept conﬁdential during his lifetime, and so did not keep him
from being forced by British courts to take hormones to “cure” his homosexuality, resulting in his suicide in 1954.
Web draft 2007-01-08 21:59

DRAFT
1.5. DETERMINISTIC TIME AND THE CLASS P.
p1.21 (31)
eﬀorts was John von-Neumann, an extremely proliﬁc scientist that was involved in anything from
the Manhattan project to founding game theory in economics. To this day essentially all digital
computers follow the “von-Neumann architecture” he pioneered while working on the design of the
EDVAC, one of the earliest digital computers [vN45].
As computers became more prevalent, the issue of eﬃciency in computation began to take
center stage. Cobham [Cob64] deﬁned the class P and suggested it may be a good formalization
for eﬃcient computation. A similar suggestion was made by Edmonds ([Edm65], see quote above)
in the context of presenting a highly non-trivial polynomial-time algorithm for ﬁnding a maximum
matching in general graphs. Hartmanis and Stearns [HS65] deﬁned the class DTIME(T(n)) for
every function T, and proved the slightly relaxed version of Theorem 1.13 we showed above (the
version we stated and prove below was given by Hennie and Stearns [HS66]). They also coined
the name “computational complexity” and proved an interesting “speed-up theorem”: if a function
f is computable by a TM M in time T(n) then for every constant c ≥1, f is computable by a
TM ˜
M (possibly with larger state size and alphabet size than M) in time T(n)/c. This speed-up
theorem is another justiﬁcation for ignoring constant factors in the deﬁnition of DTIME(T(n)).
Blum [Blu67] suggested an axiomatic formalization of complexity theory, that does not explicitly
mention Turing machines.
We have omitted a discussion of some of the “bizarre conditions” that may occur when con-
sidering time bounds that are not time-constructible, especially “huge” time bounds (i.e., function
T(n) that are much larger than exponential in n). For example, there is a non-time constructible
function T : N →N such that every function computable in time T(n) can also be computed in the
much shorter time log T(n). However, we will not encounter non time-constructible time bounds
in this book.
Exercises
§1 For each of the following pairs of functions f, g determine whether f = o(g), g = o(f) or
f = Θ(g). If f = o(g) then ﬁnd the ﬁrst number n such that f(n) < g(n):
(a) f(n) = n2 , g(n) = 2n2 + 100√n.
(b) f(n) = n100, g(n) = 2n/100.
(c) f(n) = n100, g(n) = 2n1/100.
(d) f(n) = √n, g(n) = 2
√log n.
(e) f(n) = n100, g(n) = 2(log n)2.
(f) f(n) = 1000n, g(n) = n log n.
§2 For each of the following recursively deﬁned functions f, ﬁnd a closed (non-recursive) expres-
sion for a function g such that f(n) = Θ(g(n)).
(Note: below we only supply the recursive rule, you can assume that f(1) = f(2) = · · · =
f(10) = 1 and the recursive rule is applied for n > 10; in any case regardless how the base
case it won’t make any diﬀerence to the answer - can you see why?)
Web draft 2007-01-08 21:59

DRAFT
p1.22 (32)
1.5. DETERMINISTIC TIME AND THE CLASS P.
(a) f(n) = f(n −1) + 10.
(b) f(n) = f(n −1) + n.
(c) f(n) = 2f(n −1).
(d) f(n) = f(n/2) + 10.
(e) f(n) = f(n/2) + n.
(f) f(n) = 2f(n/2) + n.
(g) f(n) = 3f(n/2).
§3 The MIT museum contains a kinetic sculpture by Arthur Ganson called “Machine with con-
crete” (see Figure 1.8). It consists of 13 gears connected to one another in a series such that
each gear moves 50 times slower than the previous one. The fastest gear is constantly rotated
by an engine at a rate of 212 rotations per minute. The slowest gear is ﬁxed to a block of
concrete and so apparently cannot move at all. How come this machine does not break apart?
Figure 1.8: Machine with concrete by Arthur Ganson.
§4 Let f be the addition function that maps the representation of a pair of numbers x, y to the
representation of the number x + y. Let g be the multiplication function that maps ⟨x, y⟩to
⌞x · y⌟. Prove that both f and g are computable by writing down a full description (including
the states, alphabet and transition function) of the corresponding Turing machines.
Hint: Follow the gradeschool algorithms.
§5 Complete the proof of Claim 1.8 by writing down explicitly the description of the machine
˜
M.
§6 Complete the proof of Claim 1.9.
§7 Complete the proof of Claim 1.11.
Web draft 2007-01-08 21:59

DRAFT
1.5. DETERMINISTIC TIME AND THE CLASS P.
p1.23 (33)
§8 Deﬁne a TM M to be oblivious if its head movement does not depend on the input but only
on the input length. That is, M is oblivious if for every input x ∈{0, 1}∗and i ∈N, the
location of each of M’s heads at the ith step of execution on input x is only a function of |x|
and i. Show that for every time-constructible T : N →N, if L ∈DTIME(T(n)) then there
is an oblivious TM that decides L in time O(T(n)2). Furthermore, show that there is such a
TM that uses only two tapes: one input tape and one work/output tape.
Hint: Use the proof of Claim 1.9.
§9 Show that for every time-constructible T : N →N, if L ∈DTIME(T(n)) then there is an
oblivious TM that decides L in time O(T(n) log T(n)).
Hint: show that the universal TM U obtained by the proof of
Theorem 1.13 can be tweaked to be oblivious.
§10 Deﬁne a single-tape Turing machine to be a TM that has only one read/write tape, that is
used as input, work and output tape. Show that for every (time-constructible) T : N →N
and f ∈DTIME(T(n)), f can be computed in O(T(n)2) steps by a single-tape TM.
§11 Deﬁne a two dimensional Turing machine to be a TM where each of its tapes is an inﬁnite
grid (and the machine can move not only Left and Right but also Up and Down). Show that
for every (time-constructible) T : N →N and every Boolean function f, if g can be computed
in time T(n) using a two-dimensional TM then f ∈DTIME(T(n)2).
§12 Deﬁne a RAM Turing machine to be a Turing machine that has random access memory. We
formalize this as follows: the machine has additional two symbol on its alphabet we denote
by R and W and an additional state we denote by qaccess. We also assume that the machine
has an inﬁnite array A that is initialized to all blanks. Whenever the machine enters qaccess,
if its address tape contains
⌞i⌟R (where
⌞i⌟denotes the binary representation of i) then the
value A[i] is written in the cell next to the R symbol. If its tape contains
⌞i⌟Wσ (where σ is
some symbol in the machine’s alphabet) then A[i] is set to the value σ.
Show that if a Boolean function f is computable within time T(n) (for some time-constructible
T) by a RAM TM, then it is in DTIME(T(n)2).
§13 Consider the following simple programming language.
It has a single inﬁnite array A of
elements in {0, 1, □} (initialized to □) and a single integer variable i. A program in this
language contains a sequence of lines of the following form:
label : If A[i] equals σ then cmds
Where σ ∈{0, 1, □} and cmds is a list of one or more of the following commands: (1) Set
A[i] to τ where τ ∈{0, 1, □},
(2) Goto label,
(3) Increment i by one,
(4) Decrement
i by one,
and (5) Output b and halt. where b ∈{0, 1}. A program is executed on an
input x ∈{0, 1}n by placing the ith bit of x in A[i] and then running the program following
the obvious semantics.
Web draft 2007-01-08 21:59

DRAFT
p1.24 (34)
1.5. DETERMINISTIC TIME AND THE CLASS P.
Prove that for every functions f : {0, 1}∗→{0, 1} and (time constructible) T : N →N, if f
is computable in time T(n) by a program in this language, then f ∈DTIME(T(n)).
§14 Give a full speciﬁcation of a representation scheme of Turing machines as binary string strings.
That is, show a procedure that transforms any TM M (e.g., the TM computing the function
PAL described in Example 1.6) into a binary string
⌞M⌟. It should be possible to recover
M from
⌞M⌟, or at least recover a functionally equivalent TM (i.e., a TM ˜
M computing the
same function as M with the same running time).
§15 A partial function from {0, 1}∗to {0, 1}∗is a function that is not necessarily deﬁned on all
its inputs. We say that a TM M computes a partial function f if for every x on which f is
deﬁned, M(x) = f(x) and for every x on which f is not deﬁned M gets into an inﬁnite loop
when executed on input x. If S is a set of partial functions, we deﬁne fS to be the Boolean
function that on input α outputs 1 iﬀMα computes a partial function in S. Rice’s Theorem
says that for every non-trivial S (a set that is not the empty set nor the set of all partial
functions), the fS is not computable.
(a) Show that Rice’s Theorem yields an alternative proof for Theorem 1.17 by showing that
the function HALT is not computable.
(b) Prove Rice’s Theorem.
Hint: By possible changing from S to its complement, we may
assume that the empty function ∅(that is not deﬁned on any input)
is in S there is some function f that is deﬁned on some input x
that is not in S. Use this to show that an algorithm to compute
fS can compute the function HALTx which outputs 1 on input α iﬀ
Mα halts on input x. Then reduce computing HALT to computing
HALTx thereby deriving Rice’s Theorem from Theorem 1.17.
§16 Prove that the following languages/decision problems on graphs are in P: (You may pick
either the adjacency matrix or adjacency list representation for graphs; it will not make a
diﬀerence. Can you see why?)
(a) CONNECTED — the set of all connected graphs. That is, G ∈CONNECTED if every
pair of vertices u, v in G are connected by a path.
(b) TRIANGLEFREE — the set of all graphs that do not contain a triangle (i.e., a triplet
u, v, w of connected distinct vertices.
(c) BIPARTITE — the set of all bipartite graphs. That is, G ∈BIPARTITE if the vertices of
G can be partitioned to two sets A, B such that all edges in G are from a vertex in A to
a vertex in B (there is no edge between two members of A or two members of B).
(d) TREE — the set of all trees. A graph is a tree if it is connected and contains no cycles.
Equivalently, a graph G is a tree if every two distinct vertices u, v in G are connected
by exactly one simple path (a path is simple if it has no repeated vertices).
Web draft 2007-01-08 21:59

DRAFT
1.A. PROOF OF THEOREM ??: UNIVERSAL SIMULATION IN O(T LOG T)-TIME p1.25 (35)
§17 Recall that normally we assume that numbers are represented as string using the binary
basis. That is, a number n is represented by the sequence x0, x1, . . . , xlog n such that n =
Pn
i=0 xi2i. However, we could have used other encoding schemes. If n ∈N and b ≥2, then
the representation of n in base b, denoted by
⌞n⌟b is obtained as follows: ﬁrst represent n as
a sequence of digits in {0, . . . , b −1}, and then replace each digit by a sequence of zeroes and
ones. The unary representation of n, denoted by
⌞n⌟!unary is the string 1n (i.e., a sequence
of n ones).
(a) Show that choosing a diﬀerent base of representation will make no diﬀerence to the
class P. That is, show that for every subset S of the natural numbers, if we deﬁne
Lb
S = { ⌞n⌟b : n ∈S} then for every b ≥2, Lb
S ∈P iﬀL2
S ∈P.
(b) Show that choosing the unary representation make make a diﬀerence by showing that
the following language is in P:
UNARYFACTORING = {⟨⌞n⌟unary, ⌞ℓ⌟unary, ⌞k⌟unary⟩: there is j ∈(ℓ, k) dividing n}
It is not known to be in P if we choose the binary representation (see Chapters 10
and 20). In Chapter 3 we will see that there is a problem that is proven to be in P when
choosing the unary representation but not in P when using the binary representation.
1.A
Proof of Theorem 1.13: Universal Simulation in O(T log T)-
time
We now show how to prove Theorem 1.13 as stated. That is, we show a universal TM U such
that given an input x and a description of a TM M that halts on x within T steps, U outputs
M(x) within O(T log T) time (where the constants hidden in the O notation may depend on the
parameters of the TM M being simulated).
The general structure of U will be as in Section 1.3.1, using the input and output tape as M
does, and with extra work tapes to store M’s transition table and current state. We will also
have another “scratch” work tape to assist in certain computation. The main obstacle we need to
overcome is that we cannot use Claim 1.9 to reduce the number of M’s work tapes to one, as that
claim introduces too much of overhead in the simulation. Therefore, we need to show a diﬀerent
way to encode all of M’s work tapes using one tape of U.
Let k be the number of tapes that M uses and Γ its alphabet. Following the proof of Claim 1.8,
we may assume that U uses the alphabet Γk (as this can be simulated with a overhead depending
only on |Γ|). Thus we can encode in each cell of U’s work tape k symbols of Γ, each corresponding
to a symbol from one of M’s tapes. However, we still have to deal with the fact that M has k
read/write heads that can each move independently to the left or right, whereas U’s work tape only
has a single head. Paraphrasing the famous saying, our strategy to handle this can be summarized
as follows:
“If Muhammad will not come to the mountain then the mountain will go to Muhammad”.
Web draft 2007-01-08 21:59

DRAFT
p1.26 (36) 1.A. PROOF OF THEOREM ??: UNIVERSAL SIMULATION IN O(T LOG T)-TIME
That is, since we can not move U’s read/write head in diﬀerent directions at once, we simply
move the tape “under” the head. To be more speciﬁc, since we consider U’s alphabet to be Γk, we
can think of U’s main work tape not as a single tape but rather k parallel tapes; that is, we can
think of U as having k tapes with the property that in each step either all their read/write heads
go in unison one location to the left or they all go one location to the right (see Figure 1.9).
To simulate a single step of M we shift all the non-blank symbols in each of these parallel tapes
until the head’s position in these parallel tapes corresponds to the heads’ positions of M’s k tapes.
For example, if k = 3 and in some particular step M’s transition function speciﬁes the movements
L, R, R then U will shift all the non-blank entries of its ﬁrst parallel tape one cell to the right, and
shift the non-blank entries of its second and third tapes one cell to the left. (U can easily perform
these shifts using the additional “scratch” work tape.)
M’s 3 independent tapes:
c
o m p
l e
t
e
l
y
r
e
p
l a
c
e d
b
y
m a c
h
i n e
s
U’s 3 parallel tapes (i.e., one tape encoding 3 tapes)
c
o m p
l e
t
e
l
y
r
e
p
l a
c
e d b
y
m a c
h
i n c
e
s
Figure 1.9: Packing k tapes of M into one tape of U. We consider U’s single work tape to be composed of k parallel
tapes, whose heads move in unison, and hence we shift the contents of these tapes to simulate independent head
movement.
The approach above is still not good enough to get O(T log T)-time simulation. The reason is
that there may be as much as T non-blank symbols in each tape, and so each shift operation may
cost U at least T operations per each step of M. Our approach to deal with this is to create “buﬀer
zones”: rather than having each of U’s parallel tapes correspond exactly to a tape of M, we add a
special kind of blank symbol □⋄to the alphabet of U’s parallel tapes with the semantics that this
symbol is ignored in the simulation. For example, if the non-blank contents of M’s tape are 010
then this can be encoded in the corresponding parallel tape of U not just by 010 but also by 0□⋄01
or 0□⋄□⋄1□⋄0 and so on.
Web draft 2007-01-08 21:59

DRAFT
1.A. PROOF OF THEOREM ??: UNIVERSAL SIMULATION IN O(T LOG T)-TIME p1.27 (37)
- c
o m p l
e
t
e
-
-
-
-
-
-
-
l
y
r e
p
-
-
l
a c
e
-
-
-
-
-
-
m a c
h
-
-
-
-
i
n
e
s
R1
R2
R3
L2
L3
L1
..... -3 -2 -1 0 +1 +2 +3 .....
Before:
p
l
e
t
-
-
e -
l
y
-
-
-
-
-
-
r -
e
p
- l
a c
e
-
-
-
-
m a c
-
h
i
n -
-
-
-
-
e
s
R1
R2
R3
L2
L3
L1
After:
Figure 1.10:
Performing a shift of the parallel tapes.
The left shift of the ﬁrst tape involves zones
L1, R1, L2, R2, L3, R3, the right shift of the second tape involves only L1, R1, while the left shift of the third tape
involves zones L1, R1, L2, R2. We maintain the invariant that each zone is either empty, half-full or full. Note that -
denotes □⋄.
For convenience, we think of U’s parallel tapes as inﬁnite in both the left and right directions
(this can be easily simulated with minimal overhead, see Claim 1.11). Thus, we index their locations
by 0, ±1, ±2, . . .. Normally we keep U’s head on location 0 of these parallel tapes. We will only
move it temporarily to perform a shift when, following our general approach, we simulate a left
head movement by shifting the tape to the right and vice versa. At the end of the shift we return
the head to location 0.
We split the tapes into zones L0, R0, L1, R1, . . . (we’ll only need to go up to Llog T+1, Rlog T+1)
where zone Li contains the 2i cells in the interval [2i..2i+1 −1] and zone Ri contains the cells in the
interval [−2i+1 + 1.. −2i] (location 0 is not in any zone). We shall always maintain the following
invariants:
• Each of the zones is either empty, full, or half-full with non-□⋄symbols. That is, the number
of symbols in zone Li that are not □⋄is either 0,2i−1, or 2i and the same holds for Ri. (We
treat the ordinary □symbol the same as any other symbol in Γ and in particular a zone full
of □’s is considered full.)
We assume that initially all the zones are half-full. We can ensure this by ﬁlling half of each
zone with □⋄symbols in the ﬁrst time we encounter it.
• The total number of non-□⋄symbols in Li ∪Ri is 2i. That is, if Li is full then Ri is empty
and vice versa.
Web draft 2007-01-08 21:59

DRAFT
p1.28 (38) 1.A. PROOF OF THEOREM ??: UNIVERSAL SIMULATION IN O(T LOG T)-TIME
• Location 0 always contains a non-□⋄symbol.
The advantage in setting up these zones is that now when performing the shifts, we do not
always have to move the entire tape, but by using the “buﬀer zones” made up of □⋄symbols, we can
restrict ourselves to only using some of the zones. We illustrate this by showing how U performs a
left shift on the ﬁrst of its parallel tapes (see Figure 1.10):
1. U ﬁnds the smallest i such that Ri is not empty. Note that this is also the smallest i such
that Li is not full.
2. U puts the leftmost non-□⋄symbol of Ri in position 0 and shifts the remaining leftmost 2i−1−1
non-□⋄symbols from Ri into the zones R0, . . . , Ri−1 ﬁlling up exactly half the symbols of each
zone. Note that there is room to perform this since all the zones R0, . . . , Ri−1 were empty
and that indeed 2i−1 = Pi−2
j=0 2j + 1.
3. U performs the symmetric operation to the left of position 0: it shifts into Li the 2i−1 left-
most symbols in the zones Li−1, . . . , L1 and reorganizes Li−1, . . . , Li such that the remaining
Pi−1
j=1 2j −2i−1 = 2i−1−1 symbols, plus the symbol that was originally in position 0 (modiﬁed
appropriately according to M’s transition function) take up exactly half of each of the zones
Li−1, . . . , Li.
4. Note that at the end of the shift, all of the zones L0, R0, . . . , Li−1, Ri−1 are half-full and so
we haven’t violated our invariant.
Performing such a shift costs O(Pi
j=1 2j) = O(2i) operations. However, once we do this, we will
not touch Li again until we perform at least 2i−1 shifts (since now the zones L0, R0, . . . , Li−1, Ri−1
are half-full). Thus, when simulating T steps of M, we perform a shift involving Li and Ri during
the simulation of at most a
1
2i−1 fraction of these steps. Thus, the total number of operations used
by these shifts is when simulating T steps is
O(
log T+1
X
i=1
T
2i−1 2i) = O(T log T) .
■
Web draft 2007-01-08 21:59

DRAFT
Chapter 2
NP and NP completeness
“(if φ(n) ≈Kn2)a then this would have consequences of the greatest magnitude. That
is to say, it would clearly indicate that, despite the unsolvability of the (Hilbert)
Entscheidungsproblem, the mental eﬀort of the mathematician in the case of the
yes-or-no questions would be completely replaced by machines.... (this) seems to me,
however, within the realm of possibility.”
Kurt G¨odel in a letter to John von Neumann, 1956
aIn modern terminology, if SAT has a quadratic time algorithm
“I conjecture that there is no good algorithm for the traveling salesman problem.
My reasons are the same as for any mathematical conjecture: (1) It is a legitimate
mathematical possibility, and (2) I do not know.”
Jack Edmonds, 1966
“In this paper we give theorems that suggest, but do not imply, that these problems,
as well as many others, will remain intractable perpetually.”
Richard Karp, 1972
If you have ever attempted a crossword puzzle, you know that there is often a big diﬀerence
between solving a problem from scratch and verifying a given solution. In the previous chapter we
encountered P, the class of decision problems that can be eﬃciently solved. In this chapter, we
deﬁne the complexity class NP that aims to capture the set of problems whose solutions can be
eﬃciently veriﬁed. The famous P versus NP question asks whether or not the two are the same.
The resolution of this conjecture will be of great practical, scientiﬁc and philosophical interest; see
Section 2.7.
This chapter also introduces NP-completeness, an important class of computational problems
that are in P if and only if P = NP. Notions such as reductions and completeness encountered in
this study motivate many other deﬁnitions encountered later in the book.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p2.1 (39)

DRAFT
p2.2 (40)
2.1. THE CLASS NP
2.1
The class NP
As mentioned above, the complexity class NP will serve as our formal model for the class of
problems having eﬃciently veriﬁable solutions: a decision problem / language is in NP if given an
input x, we can easily verify that x is a YES instance of the problem (or equivalently, x is in the
language) if we are given the polynomial-size solution for x, that certiﬁes this fact. We will give
several equivalent deﬁnitions for NP. The ﬁrst one is as follows:
Definition 2.1 (The class NP)
A language L ⊆{0, 1}∗is in NP if there exists a polynomial p : N →N and a
polynomial-time TM M such that for every x ∈{0, 1}∗,
x ∈L ⇔∃u ∈{0, 1}p(|x|) s.t. M(x, u) = 1
If x ∈L and u ∈{0, 1}p(|x|) satisfy M(x, u) = 1 then we call u a certiﬁcate1for x
(with respect to the language L and machine M).
Example 2.2
Here are a few examples for decision problems in NP:
Independent set: (See Example 1.1 in the previous chapter.) Given a graph G and a number k,
decide if there is a k-size independent subset of G’s vertices. The certiﬁcate for membership
is the list of k vertices forming an independent set.
Traveling salesperson: Given a set of n nodes,
 n
2

numbers di,j denoting the distances between
all pairs of nodes, and a number k, decide if there is a closed circuit (i.e., a “salesperson
tour”) that visits every node exactly once and has total length at most k. The certiﬁcate is
the sequence of nodes in the tour.
Subset sum: Given a list of n numbers A1, . . . , An and a number T, decide if there is a subset of
the numbers that sums up to T. The certiﬁcate is the list of members in this subset.
Linear programming: Given a list of m linear inequalities with rational coeﬃcients over n vari-
ables u1, . . . , un (a linear inequality has the form a1u1 + a2u2 + . . . + anun ≤b for some
coeﬃcients a1, . . . , an, b), decide if there is an assignment of rational numbers to the variables
u1, . . . , un that satisﬁes all the inequalities. The certiﬁcate is the assignment.
Integer programming: Given a list of m linear inequalities with rational coeﬃcients over n
variables u1, . . . , um, ﬁnd out if there is an assignment of integer numbers to u1, . . . , un
satisfying the inequalities. The certiﬁcate is the assignment.
1Some texts use the term witness instead of certiﬁcate.
Web draft 2007-01-08 21:59

DRAFT
2.1. THE CLASS NP
p2.3 (41)
Graph isomorphism: Given two n × n adjacency matrices M1, M2, decide if M1 and M2 deﬁne
the same graph, up to renaming of vertices. The certiﬁcate is the permutation π : [n] →[n]
such that M2 is equal to M1 after reordering M1’s indices according to π.
Composite numbers: Given a number N decide if N is a composite (i.e., non-prime) number.
The certiﬁcate is the factorization of N.
Factoring: Given three numbers N, L, U decide if N has a factor M in the interval [L, U]. The
certiﬁcate is the factor M.
Connectivity: Given a graph G and two vertices s, t in G, decide if s is connected to t in G. The
certiﬁcate is the path from s to t.
2.1.1
Relation between NP and P
We have the following trivial relationships between NP and the classes P and DTIME(T(n))
deﬁned in the previous chapter:
Claim 2.3
P ⊆NP ⊆S
c>1 DTIME(2nc).
Proof: (P ⊆NP): Suppose L ∈P is decided in polynomial-time by a TM N. Then L ∈NP
since we can take N as the machine M in Deﬁnition 2.1 and make p(x) the zero polynomial (in
other words, u is an empty string).
(NP ⊆S
c>1 DTIME(2nc)): If L ∈NP and M, p() are as in Deﬁnition 2.1 then we can decide
L in time 2O(p(n)) by enumerating all possible u and using M to check whether u is a valid certiﬁcate
for the input x. The machine accepts iﬀsuch a u is ever found. Since p(n) = O(nc) for some c > 1
then this machine runs in 2O(nc) time. Thus the theorem is proven. ■
At the moment, we do not know of any stronger relation between NP and deterministic time
classes than the trivial ones stated in Claim 2.3. The question whether or not P = NP is considered
the central open question of complexity theory, and is also an important question in mathematics
and science at large (see Section 2.7). Most researchers believe that P ̸= NP since years of eﬀort
has failed to yield eﬃcient algorithms for certain NP languages.
Example 2.4
Here is the current knowledge regarding the NP decision problems mentioned in Example 2.2:
The Connectivity, Composite Numbers and Linear programming problems are known to be
in P. For connectivity this follows from the simple and well known breadth-ﬁrst search algorithm
(see [KT06, CLRS01]).
The composite numbers problem was only recently shown to be in P
by Agrawal, Kayal and Saxena [?], who gave a beautiful algorithm to solve it.
For the linear
programming problem this is again highly non-trivial, and follows from the Ellipsoid algorithm of
Khachiyan [?] (there are also faster algorithms, following Karmarkar’s interior point paradigm [?]).
Web draft 2007-01-08 21:59

DRAFT
p2.4 (42)
2.1. THE CLASS NP
All the rest of the problems are not known to have a polynomial-time algorithm, although we
have no proof that they are not in P. The Independent Set, Traveling Salesperson, Subset
Sum, and Integer Programming problems are known to be NP-complete, which, as we will see
in this chapter, implies that they are not in P unless P = NP. The Graph Isomorphism and
Factoring problems are not known to be either in P nor NP-complete.
2.1.2
Non-deterministic Turing machines.
The class NP can also be deﬁned using a variant of Turing machines called non-deterministic
Turing machines (abbreviated NDTM). In fact, this was the original deﬁnition and the reason for
the name NP, which stands for non-deterministic polynomial-time. The only diﬀerence between an
NDTM and a standard TM is that an NDTM has two transition functions δ0 and δ1. In addition
the NDTM has a special state we denote by qaccept. When an NDTM M computes a function,
we envision that at each computational step M makes an arbitrary choice as to which of its two
transition functions to apply. We say that M outputs 1 on a given input x if there is some sequence
of these choices (which we call the non-deterministic choices of M) that would make M reach qaccept
on input x. Otherwise— if every sequence of choices makes M halt without reaching qaccept— then
we say that M(x) = 0. We say that M runs in T(n) time if for every input x ∈{0, 1}∗and every
sequence of non-deterministic choices, M reaches either the halting state or qaccept within T(|x|)
steps.
Definition 2.5
For every function T : N →N and L ⊆{0, 1}∗, we say that L ∈NTIME(T(n)) if there is a
constant c > 0 and a cT(n)-time NDTM M such that for every x ∈{0, 1}∗, x ∈L ⇔M(x) = 1
The next theorem gives an alternative deﬁnition of NP, the one that appears in most texts.
Theorem 2.6
NP = ∪c∈NNTIME(nc)
Proof: The main idea is that the sequence of nondeterministic choices made by an accepting
computation of an NDTM can be viewed as a certiﬁcate that the input is in the language, and vice
versa.
Suppose p : N →N is a polynomial and L is decided by a NDTM N that runs in time p(n). For
every x ∈L, there is a sequence of nondeterministic choices that makes N reach qaccept on input x.
We can use this sequence as a certiﬁcate for x. Notice, this certiﬁcate has length p(|x|) and can be
veriﬁed in polynomial time by a deterministic machine, which checks that N would have entered
qaccept after using these nondeterministic choices. Thus L ∈NP according to Deﬁnition 2.1.
Conversely, if L ∈NP according to Deﬁnition 2.1, then we describe a polynomial-time NDTM
N that decides L. On input x, it uses the ability to make non-deterministic choices to write down
a string u of length p(|x|). (Concretely, this can be done by having transition δ0 correspond to
writing a 0 on the tape and transition δ1 correspond to writing a 1.) Then it runs the deterministic
Web draft 2007-01-08 21:59

DRAFT
2.2. REDUCIBILITY AND NP-COMPLETENESS
p2.5 (43)
veriﬁer M of Deﬁnition 2.1 to verify that u is a valid certiﬁcate for x, and if so, enters qaccept.
Clearly, N enters qaccept on x if and only if a valid certiﬁcate exists for x. Since p(n) = O(nc) for
some c > 1, we conclude that L ∈NTIME(nc). ■
As is the case with deterministic TM’s, NDTM’s can be easily represented as strings and there
exists a universal non-deterministic Turing machine, see Exercise 1. (In fact, using non-determinism
we can even make the simulation by a universal TM slightly more eﬃcient.)
2.2
Reducibility and NP-completeness
It turns out that the independent set problem is at least as hard as any other language in NP: if
it has a polynomial-time algorithm then so do all the problems in NP. This fascinating property
is called NP-hardness. Since most scientists conjecture that NP ̸= P, the fact that a language is
NP-hard can be viewed as evidence that it cannot be decided in polynomial time.
How can we prove that a language B is at least as hard as some other language A? The crucial
tool we use is the notion of a reduction (see Figure 2.1):
Definition 2.7 (Reductions, NP-hardness and NP-completeness)
We say that a language A ⊆{0, 1}∗is polynomial-time Karp reducible to a language
B ⊆{0, 1}∗(sometimes shortened to just “polynomial-time reducible”2) denoted by
A ≤p B if there is a polynomial-time computable function f : {0, 1}∗→{0, 1}∗such
that for every x ∈{0, 1}∗, x ∈A if and only if f(x) ∈B.
We say that B is NP-hard if A ≤p B for every A ∈NP.
We say that B is
NP-complete if B is NP-hard and B ∈NP.
L
L
L’
L’
f(L)
f(L)
Algorithm for L
Algorithm for L’
f
Input: x
f(x)
output:
1 iff f(x) in L’
Figure 2.1: A Karp reduction from L to L′ is a polynomial-time function f that maps strings in L to strings in L′
and strings in L = {0, 1}∗\ L to strings in L′. It can be used to transform a polynomial-time TM M ′ that decides
L′ into a polynomial-time TM M for L by setting M(x) = M ′(f(x)).
Now we observe some properties of polynomial-time reductions. Part 1 of the following Theorem
shows that this relation is transitive. (Later we will deﬁne other notions of reduction, and all will
2Some texts call this notion “many-to-one reducibility” or “polynomial-time mapping reducibility”.
Web draft 2007-01-08 21:59

DRAFT
p2.6 (44)
2.3. THE COOK-LEVIN THEOREM: COMPUTATION IS LOCAL
satisfy transitivity.)
Part 2 suggests the reason for the term NP-hard —namely, an NP-hard
languages is at least as hard as any other NP language.
Part 3 similarly suggests the reason
for the term NP-complete: to study the P versus NP question it suﬃces to study whether any
NP-complete problem can be decided in polynomial time.
Theorem 2.8
1. (Transitivity) If A ≤p B and B ≤p C, then A ≤p C.
2. If language A is NP-hard and A ∈P then P = NP.
3. If language A is NP-complete then A ∈P if and only if P = NP.
Proof: The main observation is that if p, q are two functions that have polynomial growth then
their composition p(q(n)) also has polynomial growth. We prove part 1 and leave the others as
simple exercises.
If f1 is a polynomial-time reduction from A to B and f2 is a reduction from B to C then
the mapping x 7→f2(f1(x)) is a polynomial-time reduction from A to C since f2(f1(x)) takes
polynomial time to compute given x and f2(f1(x)) ∈C iﬀx ∈A. ■
Do NP-complete languages exist? It may not be clear that NP should possess a language that
is as hard as any other language in the class. However, this does turn out to be the case:
Theorem 2.9
The following language is NP-complete:
TMSAT = {⟨α, x, 1n, 1t⟩: ∃u ∈{0, 1}n s.t. Mα outputs 1 on input ⟨x, u⟩within t steps}
where Mα denotes the TM represented by the string α.3
Once you internalize the deﬁnition of NP, the proof of Theorem 2.9 is straightforward and so
is left to the reader as Exercise 2. But TMSAT is not a very useful NP-complete problem since
its deﬁnition is intimately tied to the notion of the Turing machine, and hence the fact that it is
NP-complete does not provide much new insight.
2.3
The Cook-Levin Theorem: Computation is Local
Around 1971, Cook and Levin independently discovered the notion of NP-completeness and gave
examples of combinatorial NP-complete problems whose deﬁnition seems to have nothing to do
with Turing machines. Soon after, Karp showed that NP-completeness occurs widely and many
problems of practical interest are NP-complete. To date, thousands of computational problems in
a variety of disciplines have been found to be NP-complete.
3Recall that 1k denotes the string consisting of k 1’s. it is a common convention in complexity theory to provide
a polynomial TM with such an input to allow it to run in time polynomial in k.
Web draft 2007-01-08 21:59

DRAFT
2.3. THE COOK-LEVIN THEOREM: COMPUTATION IS LOCAL
p2.7 (45)
2.3.1
Boolean formulae and the CNF form.
Some of the simplest examples of NP-complete problems come from propositional logic. A Boolean
formula over the variables u1, . . . , un consists of the variables and the logical operators AND (∧),
NOT (¬) and OR (∨); see Appendix A for their deﬁnitions. For example, (a ∧b) ∨(a ∧c) ∨(b ∧c)
is a Boolean formula that is True if and only if the majority of the variables a, b, c are True. If
ϕ is a Boolean formula over variables u1, . . . , un, and z ∈{0, 1}n, then ϕ(z) denotes the value of
ϕ when the variables of ϕ are assigned the values z (where we identify 1 with True and 0 with
False). A formula ϕ is satisﬁable if there there exists some assignment z such that ϕ(z) is True.
Otherwise, we say that ϕ is unsatisﬁable.
A Boolean formula over variables u1, . . . , un is in CNF form (shorthand for Conjunctive Normal
Form) if it is an AND of OR’s of variables or their negations. For example, the following is a 3CNF
formula:
(u1 ∨¯u2 ∨u3) ∧(u2 ∨¯u3 ∨u4) ∧(¯u1 ∨u3 ∨¯u4) .
where ¯u denotes the negation of the variable u.
More generally, a CNF formula has the form
^
i

_
j
vij

,
where each vij is either a variable uk or to its negation ¬uk. The terms vij are called the literals
of the formula and the terms (∨jvij) are called its clauses. A kCNF is a CNF formula in which all
clauses contain at most k literals.
2.3.2
The Cook-Levin Theorem
The following theorem provides us with our ﬁrst natural NP-complete problems:
Theorem 2.10 (Cook-Levin Theorem [Coo71, Lev73])
Let SAT be the language of all satisﬁable CNF formulae and 3SAT be the language
of all satisﬁable 3CNF formulae. Then,
1. SAT is NP-complete.
2. 3SAT is NP-complete.
Remark 2.11
An alternative proof of the Cook-Levin theorem, using the notion of Boolean circuits, is described
in Section 6.7.
Both SAT and 3SAT are clearly in NP, since a satisfying assignment can serve as the certiﬁcate
that a formula is satisﬁable. Thus we only need to prove that they are NP-hard. We do so by ﬁrst
Web draft 2007-01-08 21:59

DRAFT
p2.8 (46)
2.3. THE COOK-LEVIN THEOREM: COMPUTATION IS LOCAL
proving that SAT is NP-hard and then showing that SAT is polynomial-time Karp reducible to
3SAT. This implies that 3SAT is NP-hard by the transitivity of polynomial-time reductions. Thus
the following lemma is the key to the proof.
Lemma 2.12
SAT is NP-hard.
Notice, to prove this we have to show how to reduce every NP language L to SAT, in other
words give a polynomial-time transformation that turns any x ∈{0, 1}∗into a CNF formula ϕx
such that x ∈L iﬀϕx is satisﬁable. Since we know nothing about the language L except that it is
in NP, this reduction has to rely just upon the deﬁnition of computation, and express it in some
way using a Boolean formula.
2.3.3
Warmup: Expressiveness of boolean formulae
As a warmup for the proof of Lemma 2.12 we show how to express various conditions using CNF
formulae.
Example 2.13
The formula (a ∨b) ∧(a ∨b) is in CNF form. It is satisﬁed by only those values of a, b that are
equal. Thus, the formula
(x1 ∨y1) ∧(x1 ∨y1) ∧· · · ∧(xn ∨yn) ∧(xn ∨yn)
is True if and only if the strings x, y ∈{0, 1}n are equal to one another.
Thus, though = is not a standard boolean operator like ∨or ∧, we will use it as a convenient
shorthand since the formula φ1 = φ2 is equivalent to (in other words, has the same satisfying
assignments as) (φ1 ∨φ2) ∧(φ1 ∨φ2).
In fact, CNF formulae of suﬃcient size can express every Boolean condition, as shown by the
following simple claim: (this fact is sometimes known as universality of the operations AND, OR
and NOT)
Claim 2.14
For every Boolean function f : {0, 1}ℓ→{0, 1} there is an ℓ-variable CNF formula ϕ of size ℓ2ℓ
such that ϕ(u) = f(u) for every u ∈{0, 1}ℓ, where the size of a CNF formula is deﬁned to be the
number of ∧/∨symbols it contains.
Proof Sketch: For every v ∈{0, 1}ℓ, it is not hard to see that there exists a clause Cv such that
Cv(v) = 0 and Cv(u) = 1 for every u ̸= v. For example, if v = ⟨1, 1, 0, 1⟩, the corresponding clause
is u1 ∨u2 ∨u3 ∨u4.
We let ϕ be the AND of all the clauses Cv for v such that f(v) = 0 (note that ϕ is indeed of
size at most ℓ2ℓ). Then for every u such that f(u) = 0 it holds that Cu(u) = 0 and hence ϕ(u) is
also equal to 0. On the other hand, if f(u) = 1 then Cv(u) = 1 for every v such that f(v) = 0 and
hence ϕ(u) = 1. We get that for every u, ϕ(u) = f(u). ■
In this chapter we will use Claim 2.14 only when the number of variables is some ﬁxed constant.
Web draft 2007-01-08 21:59

DRAFT
2.3. THE COOK-LEVIN THEOREM: COMPUTATION IS LOCAL
p2.9 (47)
2.3.4
Proof of Lemma 2.12
Let L be an NP language and let M be the polynomial time TM such that that for every x ∈{0, 1}∗,
x ∈L ⇔M(x, u) = 1 for some u ∈{0, 1}p(|x|), where p : N →N is some polynomial. We show L is
polynomial-time Karp reducible to SAT by describing a way to transform in polynomial-time every
string x ∈{0, 1}∗into a CNF formula ϕx such that x ∈L iﬀϕx is satisﬁable.
How can we construct such a formula ϕx? By Claim 2.14, the function that maps u ∈{0, 1}p(|x|)
to M(x, u) can be expressed as a CNF formula ψx (i.e., ψx(u) = M(x, u) for every u ∈{0, 1}p(|x|)).
Thus a string u such that M(x, u) = 1 exists if and only if ψx is satisﬁable. But this is not useful
for us, since the size of the formula ψx obtained from Claim 2.14 can be as large as p(|x|)2p(|x|). To
get a smaller formula we use the fact that M runs in polynomial time, and that each basic step of
a Turing machine is highly local (in the sense that it examines and changes only a few bits of the
machine’s tapes).
In the course of the proof we will make the following simplifying assumptions about the TM
M: (1) M only has two tapes: an input tape and a work/output tape and (2) M is an oblivious
TM in the sense that its head movement does not depend on the contents of its input tape. In
particular, this means that M’s computation takes the same time for all inputs of size n and for
each time step i the location of M’s heads at the ith step depends only on i and M’s input length.
We can make these assumptions without loss of generality because for every T(n)-time TM
M there exists a two-tape oblivious TM ˜
M computing the same function in O(T(n)2) time (see
Remark 1.10 and Exercise 8 of Chapter 1).4
Thus in particular, if L is in NP then there ex-
ists a two-tape oblivious polynomial-time TM M and a polynomial p such that x ∈L ⇔∃u ∈
{0, 1}p(|x|) s.t. M(x, u) = 1.
The advantage of assuming that M is oblivious is that for any given input length, we can deﬁne
functions inputpos(i), prev(i) where inputpos(i) denotes the location of the input tape head at
the ith step and prev(i) denotes the last step before i that M visited the same location on its work
tape, see Figure 2.3.5 These values can be computed in polynomial time by simulating M on, say,
the all-zeroes input.
Denote by Q the set of M’s possible states and by Γ its alphabet. The snapshot of M’s execution
on some input y at a particular step i is the triple ⟨a, b, q⟩∈Γ×Γ×Q such that a, b are the symbols
read by M’s heads from the two tapes and q is the state M is in at the ith step (see Figure 2.2).
For every m ∈N and y ∈{0, 1}m, the snapshot of M’s execution on input y at the ith step depends
on (1) its state in the i −1st step and (2) the contents of the current cells of its input and work
tapes. We write this relation as
zi = F(zi−1, zprev(i), yinputpos(i)) ,
where inputpos(i) and prev(i) are as deﬁned earlier, zi is the encoding of the ith snapshot as a
binary string of some length c, and F is some function (derived from M’s transition function) that
4In fact, with some more eﬀort we even simulate a non-oblivious T(n)-time TM by an oblivious TM running in
O(T(n) log T(n))-time, see Exercise 9 of Chapter 1. This oblivious machine may have more than two tapes, but the
proof below easily generalizes to this case.
5If i is the ﬁrst step that M visits a certain location, then we deﬁne prev(i) = 1.
Web draft 2007-01-08 21:59

DRAFT
p2.10 (48)
2.3. THE COOK-LEVIN THEOREM: COMPUTATION IS LOCAL
Input
tape
Work/
output
tape
>
0
0 0
1
1 0
1 0
0
0 1 0
0 0
0
> 1
1 0 1
1
1 0
0
0
1
q7
State register
read only head
read/write head
snapshot
0
1 q7
a
b
q
Figure 2.2: A snapshot of a TM contains the current state and symbols read by the TM at a particular step. If at
the ith step M reads the symbols 0, 1 from its tapes and is in the state q7 then the snapshot of M at the ith step is
⟨0, 1, q7⟩.
maps {0, 1}2c+1 to {0, 1}c. (Note that c is a constant depending only on M’s state and alphabet
size, and independent of the input size.)
1
m
inputpos(i)
....
....
....
....
....
1
prev(i)
i-1
i
T
input:
snapshots:
Figure 2.3: The snapshot of M at the ith step depends on its previous state (contained in the snapshot at the
i −1st step), and the symbols read from the input tape, which is in position inputpos(i), and from the work tape,
which was last written to in step prev(i).
Let n ∈N and x ∈{0, 1}n. We need to construct a CNF formula ϕx such that x ∈L ⇔
ϕx ∈SAT. Recall that x ∈L if and only if there exists some u ∈{0, 1}p(n) such that M(y) = 1
where y = x◦u (with ◦denoting concatenation). Since the sequence of snapshots in M’s execution
completely determines its outcome, this happens if and only if there exists a string y ∈{0, 1}n+p(n)
and a sequence of strings z1, . . . , zT ∈{0, 1}c (where T = T(n) is the number of steps M takes on
inputs of length n + p(n)) satisfying the following four conditions:
1. The ﬁrst n bits of y are equal to x.
2. The string z1 encodes the initial snapshot of M (i.e., the triple ⟨▷, □, qstart⟩where ▷is the
start symbol of the input tape, □is the blank symbol, and qstart is the initial state of the TM
Web draft 2007-01-08 21:59

DRAFT
2.3. THE COOK-LEVIN THEOREM: COMPUTATION IS LOCAL
p2.11 (49)
M).
3. For every i ∈{2, .., T}, zi = F(zi−1, zinputpos(i), zprev(i)).
4. The last string zT encodes a snapshot in which the machine halts and outputs 1.
The formula ϕx will take variables y ∈{0, 1}n+p(n) and z ∈{0, 1}cT and will verify that y, z
satisfy the AND of these four conditions. Clearly x ∈L ⇔ϕx ∈SAT and so all that remains is to
show that we can express ϕx as a polynomial-sized CNF formula.
Condition 1 can be expressed as a CNF formula of size 4n (see Example 2.13). Conditions 2
and 4 each depend on c variables and hence by Claim 2.14 can be expressed by CNF formulae of
size c2c. Condition 3, which is an AND of T conditions each depending on at most 3c+1 variables,
can be expressed as a CNF formula of size at most T(3c + 1)23c+1. Hence the AND of all these
conditions can be expressed as a CNF formula of size d(n+T) where d is some constant depending
only on M. Moreover, this CNF formula can be computed in time polynomial in the running time
of M. ■
2.3.5
Reducing SAT to 3SAT.
Since both SAT and 3SAT are clearly in NP, Lemma 2.12 completes the proof that SAT is NP-
complete. Thus all that is left to prove Theorem 2.10 is the following lemma:
Lemma 2.15
SAT ≤p 3SAT.
Proof: We will map a CNF formula ϕ into a 3CNF formula ψ such that ψ is satisﬁable if and only if
ϕ is. We demonstrate ﬁrst the case that ϕ is a 4CNF. Let C be a clause of ϕ, say C = u1∨u2∨u3∨u4.
We add a new variable z to the ϕ and replace C with the pair of clauses C1 = u1 ∨u2 ∨z and
C2 = u3 ∨u4 ∨z. Clearly, if u1 ∨u2 ∨u3 ∨u4 is true then there is an assignment to z that satisﬁes
both u1 ∨u2 ∨z and u3 ∨u4 ∨z and vice versa: if C is false then no matter what value we assign
to z either C1 or C2 will be false. The same idea can be applied to a general clause of size 4, and
in fact can be used to change every clause C of size k (for k > 3) into an equivalent pair of clauses
C1 of size k −1 and C2 of size 3 that depend on the k variables of C and an additional auxiliary
variable z. Applying this transformation repeatedly yields a polynomial-time transformation of a
CNF formula ϕ into an equivalent 3CNF formula ψ. ■
2.3.6
More thoughts on the Cook-Levin theorem
The Cook-Levin theorem is a good example of the power of abstraction. Even though the theorem
holds regardless of whether our computational model is the C programming language or the Turing
machine, it may have been considerably more diﬃcult to discover in the former context.
Also, it is worth pointing out that the proof actually yields a result that is a bit stronger than
the theorem’s statement:
Web draft 2007-01-08 21:59

DRAFT
p2.12 (50)
2.4. THE WEB OF REDUCTIONS
1. If we use the eﬃcient simulation of a standard TM by an oblivious TM (see Exercise 9,
Chapter 1) then for every x ∈{0, 1}∗, the size of the formula ϕx (and the time to compute
it) is O(T log T), where T is the number of steps the machine M takes on input x (see
Exercise 10).
2. The reduction f from an NP-language L to SAT presented in Lemma 2.12 not only satisﬁed
that x ∈L ⇔f(x) ∈SAT but actually the proof yields an eﬃcient way to transform a
certiﬁcate for x to a satisfying assignment for f(x) and vice versa. We call a reduction with
this property a Levin reduction. One can also verify that the proof supplied a one-to-one and
onto map between the set of certiﬁcates for x and the set of satisfying assignments for f(x),
implying that they are of the same size. A reduction with this property is called parsimonious.
Most of the known NP-complete problems (including all the ones mentioned in this chapter)
have parsimonious Levin reductions from all the NP languages (see Exercise 11). As we will
see in this book, this fact is sometimes useful for certain applications.
Why 3SAT?
The reader may wonder why is the fact that 3SAT is NP-complete so much more
interesting than the fact that, say, the language TMSAT of Theorem 2.9 is NP-complete. One
answer is that 3SAT is useful for proving the NP-completeness of other problems: it has very
minimal combinatorial structure and thus easy to use in reductions. Another answer has to do
with history: propositional logic has had a central role in mathematical logic —in fact it was
exclusively the language of classical logic (e.g. in ancient Greece). This historical resonance is one
reason why Cook and Levin were interested in 3SAT in the ﬁrst place. A third answer has to do
with practical importance: it is a simple example of constraint satisfaction problems, which are
ubiquitous in many ﬁelds including artiﬁcial intelligence.
2.4
The web of reductions
Cook and Levin had to show how every NP language can be reduced to SAT. To prove the NP-
completeness of any other language L, we do not need to work as hard: it suﬃces to reduce SAT or
3SAT to L. Once we know that L is NP-complete we can show that an NP-language L′ is in fact
NP-complete by reducing L to L′. This approach has been used to build a “web of reductions”
and show that thousands of interesting languages are in fact NP-complete. We now show the
NP-completeness of a few problems. More examples appear in the exercises (see Figure 2.4). See
the classic book by Garey and Johnson [GJ79] and the Internet site [?] for more.
Theorem 2.16 (Independent set is NP-complete)
Let INDSET = {⟨G, k⟩: G has independent set of size k}. Then INDSET is NP-complete.
Proof: Since INDSET is clearly in NP, we only need to show that it is NP-hard, which we do by
reducing 3SAT to INDSET. Let ϕ be a 3CNF formula on n variables with m clauses. We deﬁne a
graph G of 7m vertices as follows: we associate a cluster of 7 vertices in G with each clause of ϕ.
The vertices in cluster associated with a clause C correspond to the 7 possible partial assignments to
the three variables C depends on (we call these partial assignments, since they only give values for
some of the variables). For example, if C is u2 ∨u5 ∨u7 then the 7 vertices in the cluster associated
Web draft 2007-01-08 21:59

DRAFT
2.4. THE WEB OF REDUCTIONS
p2.13 (51)
∀L ∈NP
SAT
Exactone3SAT
?













9
dHAMPATH
HAMPATH
TSP
HAMCYCLE
?
  	
HHHHH
j
Theorem 2.18
?
Theorem 2.16
Ex 17
Ex 17
Theorem 2.10 (Lemma 2.12)












9
XXXXXXXXXXXXXXXX
X
z
3SAT
ZZ
~
?
SUBSETSUM
Ex 16
INDSET







CLIQUE
Ex 14
@
@
@
@
@
@@
R
?
VERTEXCOVER
Ex 15
MAXCUT
Ex 16
THEOREMS
Ex 6
Ex 18
QUADEQ





































Theorem 2.17
INTEGERPROG
Theorem 2.10 (Lemma 2.15)
Figure 2.4: Web of reductions between the NP-completeness problems described in this chapter
and the exercises. Thousands more are known.
Web draft 2007-01-08 21:59

DRAFT
p2.14 (52)
2.4. THE WEB OF REDUCTIONS
with C correspond to all partial assignments of the form u1 = a, u2 = b, u3 = c for a binary vector
⟨a, b, c⟩̸= ⟨1, 1, 1⟩. (If C depends on less than three variables then we repeat one of the partial
assignments and so some of the 7 vertices will correspond to the same assignment.) We put an
edge between two vertices of G if they correspond to inconsistent partial assignments. Two partial
assignments are consistent if they give the same value to all the variables they share. For example,
the assignment u1 = 1, u2 = 0, u3 = 0 is inconsistent with the assignment u3 = 1, u5 = 0, u7 = 1
because they share a variable (u3) to which they give a diﬀerent value. In addition, we put edges
between every two vertices that are in the same cluster.
Clearly transforming ϕ into G can be done in polynomial time. We claim that ϕ is satisﬁable if
and only if G has a clique of size m. Indeed, suppose that ϕ has a satisfying assignment u. Deﬁne
a set S of m vertices as follows: for every clause C of ϕ put in S the vertex in the cluster associated
with C that corresponds to the restriction of u to the variables C depends on. Because we only
choose vertices that correspond to restrictions of the assignment u, no two vertices of S correspond
to inconsistent assignments and hence S is an independent set of size m.
On the other hand, suppose that G has an independent set S of size m. We will use S to
construct a satisfying assignment u for ϕ. We deﬁne u as follows: for every i ∈[n], if there is a
vertex in S whose partial assignment gives a value a to ui, then set ui = a; otherwise set ui = 0.
This is well deﬁned because S is an independent set, and hence each variable ui can get at most
a single value by assignments corresponding to vertices in S. On the other hand, because we put
all the edges within each cluster, S can contain at most a single vertex in each cluster, and hence
there is an element of S in every one of the m clusters. Thus, by our deﬁnition of u, it satisﬁes all
of ϕ’s clauses. ■
We see that, surprisingly, the answer to the famous NP vs. P question depends on the seemingly
mundane question of whether one can eﬃciently plan an optimal dinner party. Here are some more
NP-completeness results:
Theorem 2.17 (Integer programming is NP-complete)
We say that a set of linear inequalities with rational coeﬃcients over variables u1, . . . , un is in
IPROG if there is an assignment of integer numbers in {0, 1, 2, . . .} to u1, . . . , un that satisﬁes it.
Then, IPROG is NP-complete.
Proof: IPROG is clearly in NP. To reduce SAT to IPROG note that every CNF formula can be
easily expressed as an integer program: ﬁrst add the constraints 0 ≤ui ≤1 for every i to ensure
that the only feasible assignments to the variables are 0 or 1, then express every clause as an
inequality. For example, the clause u1 ∨u2 ∨u3 can be expressed as u1 +(1−u2)+(1−u3) ≥1. ■
Theorem 2.18 (Hamiltonian path is NP-complete)
Let dHAMPATH denote the set of all directed graphs that contain a path visiting all of their vertices
exactly once. Then dHAMPATH is NP-complete.
Proof: Again, dHAMPATH is clearly in NP. To show it’s NP-complete we show a way to map
every CNF formula ϕ into a graph G such that ϕ is satisﬁable if and only if G has a Hamiltonian
path (i.e. path that visits all of G’s vertices exactly once).
Web draft 2007-01-08 21:59

DRAFT
2.4. THE WEB OF REDUCTIONS
p2.15 (53)
......
m vertices corresponding to clauses c1 .... cm
start vertex
end vertex
..........
For every variable ui we have a “chain” of 6m vertices.
chain 1:
chain n:
c10 = u1 OR u2 OR u3
link in chain 1:
link in chain 2:
link in chain 3:
left-to-right traversal = TRUE, right-to-left = FALSE
vertex c10 can be visited if chain 1 is traversed left-to-right
or if chains 2 or 3 are traversed right-to-left
u
v
chain 2:
Figure 2.5: Reducing SAT to dHAMPATH. A formula ϕ with n variables and m clauses is mapped to a graph G
that has m vertices corresponding to the clauses and n doubly linked chains, each of length 6m, corresponding to
the variables. Traversing a chain left to right corresponds to setting the variable to True, while traversing it right
to left corresponds to setting it to False. Note that in the ﬁgure every Hamiltonian path that takes the edge from
u to c10 must immediately take the edge from c10 to v, as otherwise it would get “stuck” the next time it visits v.
The reduction is described in Figure 2.5. The graph G has (1) m vertices for each of ϕ’s clauses
c1, . . . , cm, (2) a special starting vertex vstart and ending vertex vend and (3) n “chains” of 6m
vertices corresponding to the n variables of ϕ. A chain is a set of vertices v1, . . . , v6m such that for
every i ∈[6m −1], vi and vi+1 are connected by two edges in both directions.
We put edges from the starting vertex vstart to the two extreme points of the ﬁrst chain. We
also put edges from the extreme points of the jth chain to the extreme points to the j + 1th chain
for every j ∈[n −1]. We put an edge from the extreme points of the nth chain to the ending vertex
vend.
In addition to these edges, for every clause C of ϕ, we put edges between the chains correspond-
ing to the variables appearing in C and the vertex vC corresponding to C in the following way: if
C contains the literal uj then we take two neighboring vertices vi, vi+1 in the jth chain and put an
edge from vi to C and from C to vi+1. If C contains the literal uj then we connect these edges in
the opposite direction (i.e., vi+1 to C and C to vi). When adding these edges, we never “reuse”
a link vi, vi+1 in a particular chain and always keep an unused link between every two used links.
We can do this since every chain has 6m vertices, which is more than suﬃcient for this.
(ϕ ∈SAT ⇒G ∈dHAMPATH.) Suppose that ϕ has a satisfying assignment u1, . . . , un. We will
show a path that visits all the vertices of G. The path will start at vstart, travel through all the
chains in order, and end at vend. For starters, consider the path that travels the jth chain in left-
to-right order if uj = 1 and travels it in right-to-left order if uj = 0. This path visits all the vertices
except for those corresponding to clauses. Yet, if u is a satisfying assignment then the path can be
easily modiﬁed to visit all the vertices corresponding to clauses: for each clause C there is at least
one literal that is true, and we can use one link on the chain corresponding to that literal to “skip”
to the vertex vC and continue on as before.
Web draft 2007-01-08 21:59

DRAFT
p2.16 (54)
2.4. THE WEB OF REDUCTIONS
(G ∈dHAMPATH ⇒ϕ ∈SAT.) Suppose that G has an Hamiltonian path P. We ﬁrst note that
the path P must start in vstart (as it has no incoming edges) and end at vend (as it has no outgoing
edges). Furthermore, we claim that P needs to traverse all the chains in order, and within each
chain traverse it either in left-to-right order or right-to-left order. This would be immediate if the
path did not use the edges from a chain to the vertices corresponding to clauses. The claim holds
because if a Hamiltonian path takes the edge u →w, where u is on a chain and w corresponds to
a clause, then it must at the next step take the edge w →v where v is the vertex adjacent to u in
the link. Otherwise, the path will get stuck the next time it visits v (see Figure 2.1). Now, deﬁne
an assignment u1, . . . , un to ϕ as follows: uj = 1 if P traverses the jth chain in left-to-right order,
and uj = 0 otherwise. It is not hard to see that because P visits all the vertices corresponding to
clauses, u1, . . . , un is a satisfying assignment for ϕ. ■
In praise of reductions
Though originally invented as part of the theory of NP-completeness, the polynomial-time re-
duction (together with its ﬁrst cousin, the randomized polynomial-time reduction deﬁned in Sec-
tion 7.9) has led to a rich understanding of complexity above and beyond NP-completeness. Much
of complexity theory and cryptography today (thus, many chapters of this book) consists of using
reductions to make connections between disparate complexity theoretic conjectures. Why do com-
plexity theorists excel at reductions but not at actually proving lower bounds on Turing machines?
A possible explanation is that humans have evolved to excel at problem solving, and hence are
more adept at algorithms (after all, a reduction is merely an algorithm to transform one problem
into another) than at proving lower bounds on Turing machines.
Coping with NP hardness.
NP-complete problems turn up in great many applications, from ﬂight scheduling to genome se-
quencing. What do you do if the problem you need to solve turns out to be NP-complete? On the
outset, the situation looks bleak: if P ̸= NP then there simply does not exist an eﬃcient algorithm
to solve such a problem. However, there may still be some hope: NP completeness only means
that (assuming P ̸= NP) the problem does not have an algorithm that solves it exactly on every
input. But for many applications, an approximate solution on some of the inputs might be good
enough.
A case in point is the traveling salesperson problem (TSP), of computing, given a list of pairwise
distances between n cities, the shortest route that travels through all of them. Assume that you
are indeed in charge of coming up with travel plans for traveling salespersons that need to visit
various cities around your country. Does the fact that TSP is NP-complete means that you are
bound to do a hopelessly suboptimal job? This does not have to be the case.
First note that you do not need an algorithm that solves the problem on all possible lists of
pairwise distances. We might model the inputs that actually arise in this case as follows: the n
cities are points on a plane, and the distance between a pair of cities is the distance between the
corresponding points (we are neglecting here the diﬀerence between travel distance and direct/arial
distance). It is an easy exercise to verify that not all possible lists of pairwise distances can be
Web draft 2007-01-08 21:59

DRAFT
2.5. DECISION VERSUS SEARCH
p2.17 (55)
generated in such a way. We call those that do Euclidean distances. Another observation is that
computing the exactly optimal travel plan may not be so crucial. If you could always come up with
a travel plan that is at most 1% longer than the optimal, this should be good enough.
It turns out that neither of these observations on its own is suﬃcient to make the problem
tractable.
The TSP problem is still NP complete even for Euclidean distances.
Also if P ̸=
NP then TSP is hard to approximate within any constant factor. However, combining the two
observations together actually helps: for every ϵ there is a poly(n(log n)O(1/ϵ))-time algorithm that
given Euclidean distances between n cities comes up with a tour that is at most a factor of (1 + ϵ)
worse than the optimal tour [Aro98].
We see that discovering the problem you encounter is NP-complete should not be cause for
immediate despair. Rather you should view this as indication that a more careful modeling of
the problem is needed, letting the literature on complexity and algorithms guide you as to what
features might make the problem more tractable. Alternatives to worst-case exact computation
are explored in Chapters 15 and 18, that investigate average-case complexity and approximation
algorithms respectively.
2.5
Decision versus search
We have chosen to deﬁne the notion of NP using Yes/No problems (“Is the given formula sat-
isﬁable?”) as opposed to search problems (“Find a satisfying assignment to this formula if one
exists”). Clearly, the search problem is harder than the corresponding decision problem, and so
if P ̸= NP then neither one can be solved for an NP-complete problem. However, it turns out
that for NP-complete problems they are equivalent in the sense that if the decision problem can
be solved (and hence P = NP) then the search version of any NP problem can also be solved in
polynomial time.
Theorem 2.19
Suppose that P = NP. Then, for every NP language L there exists a polynomial-time TM B that
on input x ∈ L outputs a certiﬁcate for x.
That is, if, as per Deﬁnition 2.1, x ∈L iﬀ∃u ∈{0, 1}p(|x|) s.t. M(x, u) = 1 where p is some
polynomial and M is a polynomial-time TM, then on input x ∈L, B(x) will be a string u ∈
{0, 1}p(|x|) satisfying M(x, B(x)) = 1.
Proof: We start by showing the theorem for the case of SAT. In particular we show that given
an algorithm A that decides SAT, we can come up with an algorithm B that on input a satisﬁable
CNF formula ϕ with n variables, ﬁnds a satisfying assignment for ϕ using 2n + 1 calls to A and
some additional polynomial-time computation.
The algorithm B works as follows: we ﬁrst use A to check that the input formula ϕ is satisﬁable.
If so, we substitute x1 = 0 and x1 = 1 in ϕ (this transformation, that simpliﬁes and shortens the
formula a little, leaving a formula with n −1 variables, can certainly be done in polynomial time)
and then use A to decide which of the two is satisﬁable (it is possible that both are). Say the ﬁrst is
satisﬁable. Then we ﬁx x1 = 0 from now on and continue with the simpliﬁed formula. Continuing
this way we end up ﬁxing all n variables while ensuring that each intermediate formula is satisﬁable.
Thus the ﬁnal assignment to the variables satisﬁes ϕ.
Web draft 2007-01-08 21:59

DRAFT
p2.18 (56)
2.6. CONP, EXP AND NEXP
To solve the search problem for an arbitrary NP-language L, we use the fact that the reduction
of Theorem 2.10 from L to SAT is actually a Levin reduction. This means that we have a polynomial-
time computable function f such that not only x ∈L ⇔f(x) ∈SAT but actually we can map a
satisfying assignment of f(x) into a certiﬁcate for x. Therefore, we can use the algorithm above to
come up with an assignment for f(x) and then map it back into a certiﬁcate for x. ■
Remark 2.20
The proof above shows that SAT is downward self-reducible, which means that given an algorithm
that solves SAT on inputs of length smaller than n we can solve SAT on inputs of length n. Using
the Cook-Levin reduction, one can show that all NP-complete problems have a similar property,
though we do not make this formal.
2.6
coNP, EXP and NEXP
Now we deﬁne some related complexity classes that are very relevant to the study of the P versus
NP question.
2.6.1
coNP
If L ⊆{0, 1}∗is a language, then we denote by L the complement of L. That is, L = {0, 1}∗\ L.
We make the following deﬁnition:
Definition 2.21
coNP =

L : L ∈P
	
.
It is important to note that coNP is not the complement of the class NP. In fact, they have
a non-empty intersection, since every language in P is in NP ∩coNP (see Exercise 19). The
following is an example of a coNP language: SAT = {ϕ : ϕ is not satisﬁable} . Students sometimes
mistakenly convince themselves that SAT is in NP.
They have the following polynomial time
NDTM in mind: on input ϕ, the machine guesses an assignment. If this assignment does not
satisfy ϕ then it accepts (i.e., goes into qaccept and halts) and if it does satisfy ϕ then the machine
halts without accepting. This NDTM does not do the job: indeed it accepts every unsatisﬁable
ϕ but in addition it also accepts many satisﬁable formulae (i.e., every formula that has a single
unsatisfying assignment). That is why pedagogically we prefer the following deﬁnition of coNP
(which is easily shown to be equivalent to the ﬁrst, see Exercise 20):
Definition 2.22 (coNP, alternative definition)
For every L ⊆{0, 1}∗, we say that L ∈coNP if there exists a polynomial p : N →N and a
polynomial-time TM M such that for every x ∈{0, 1}∗,
x ∈L ⇔∀u ∈{0, 1}p(|x|) s.t. M(x, u) = 1
The key fact to note is the use of “∀” in this deﬁnition where Deﬁnition 2.1 used ∃.
We can deﬁne coNP-completeness in analogy to NP-completeness:
a language is coNP-
complete if it is in coNP and every coNP language is polynomial-time Karp reducible to it.
Web draft 2007-01-08 21:59

DRAFT
2.6. CONP, EXP AND NEXP
p2.19 (57)
Example 2.23
In classical logic, tautologies are true statements. The following language is coNP-complete:
TAUTOLOGY = {ϕ : ϕ is a Boolean formula that is satisﬁed by every assignment} .
It is clearly in coNP by Deﬁnition 2.22 and so all we have to show is that for every L ∈coNP,
L ≤p TAUTOLOGY. But this is easy: just modify the Cook-Levin reduction from L (which is in
NP) to SAT. For every input x ∈{0, 1}∗that reduction produces a formula ϕx that is satisﬁable
iﬀx ∈L. Now consider the formula ¬ϕx. It is in TAUTOLOGY iﬀx ∈L, and this completes the
description of the reduction.
It is a simple exercise to check that if P = NP then NP = coNP = P. Put in the contraposi-
tive, if we can show that NP ̸= coNP then we have shown P ̸= NP. Most researchers believe that
NP ̸= coNP. The intuition is almost as strong as for the P versus NP question: it seems hard to
believe that there is any short certiﬁcate for certifying that a given formula is a TAUTOLOGY, in
other words, to certify that every assignment satisﬁes the formula.
2.6.2
EXP and NEXP
The following two classes are exponential time analogues of P and NP.
Definition 2.24
EXP = ∪c≥0DTIME(2nc).
NEXP = ∪c≥0NTIME(2nc).
Because every problem in NP can be solved in exponential time by a brute force search for
the certiﬁcate, P ⊆NP ⊆EXP ⊆NEXP.
Is there any point to studying classes involving
exponential running times? The following simple result —providing merely a glimpse of the rich
web of relations we will be establishing between disparate complexity questions— may be a partial
answer.
Theorem 2.25
If EXP ̸= NEXP then P ̸= NP.
Proof: We prove the contrapositive: assuming P = NP we show EXP = NEXP.
Suppose
L ∈NTIME(2nc) and NDTM M decides it. We claim that then the language
Lpad =
n
⟨x, 12|x|c
⟩: x ∈L
o
(1)
is in NP. Here is an NDTM for Lpad: given y, ﬁrst check if there is a string z such that y = ⟨z, 12|z|c
⟩.
If not, output REJECT. If y is of this form, then run M on z for 2|z|c steps and output its answer.
Clearly, the running time is polynomial in |y|, and hence Lpad ∈NP. Hence if P = NP then Lpad
is in P. But if Lpad is in P then L is in EXP: to determine whether an input x is in L, we just
pad the input and decide whether it is in Lpad using the polynomial-time machine for Lpad. ■
Web draft 2007-01-08 21:59

DRAFT
p2.20 (58)
2.7. MORE THOUGHTS ABOUT P, NP, AND ALL THAT
Remark 2.26
The padding technique used in this proof, whereby we transform a language by “padding” every
string in a language with a string of (useless) symbols, is also used in several other results in
complexity theory. In many settings it can be used to show that equalities between complexity
classes “scale up”; that is, if two diﬀerent type of resources solve the same problems within bound
T(n) then this also holds for functions T ′ larger than T. Viewed contrapositively, padding can
be used to show that inequalities between complexity classes involving resurce bound T ′(n) “scale
down” to resource bound T(n).
Like P and NP, most of the complexity classes studied later are also contained in both EXP
and NEXP.
2.7
More thoughts about P, NP, and all that
2.7.1
The philosophical importance of NP
At a totally abstract level, the P versus NP question may be viewed as a question about the
power of nondeterminism in the Turing machine model. (Similar questions have been completely
answered for simpler models such as ﬁnite automata.)
However, the certiﬁcate deﬁnition of NP also suggests that the P versus NP question captures
a widespread phenomenon of some philosophical importance (and a source of great frustration to
students): recognizing the correctness of an answer is often easier than coming up with the answer.
To give other analogies from life: appreciating a Beethoven sonata is far easier than composing
the sonata; verifying the solidity of a design for a suspension bridge is easier (to a civil engineer
anyway!) than coming up with a good design; verifying the proof of a theorem is easier than coming
up with a proof itself (a fact referred to in G¨odel’s letter mentioned at the start of the chapter),
and so forth. In such cases, coming up with the right answer seems to involve exhaustive search
over an exponentially large set. The P versus NP question asks whether exhaustive search can be
avoided in general. It seems obvious to most people —and the basis of many false proofs proposed
by amateurs— that exhaustive search cannot be avoided: checking that a given salesperson tour
(provided by somebody else) has length at most k ought to be a lot easier than coming up with
such a tour by oneself. Unfortunately, turning this intuition into a proof has proved diﬃcult.
2.7.2
NP and mathematical proofs
By deﬁnition, NP is the set of languages where membership has a short certiﬁcate. This is remi-
niscent of another familiar notion, that of a mathematical proof. As noticed in the past century, in
principle all of mathematics can be axiomatized, so that proofs are merely formal manipulations of
axioms. Thus the correctness of a proof is rather easy to verify —just check that each line follows
from the previous lines by applying the axioms. In fact, for most known axiomatic systems (e.g.,
Peano arithmetic or Zermelo-Fraenkel Set Theory) this veriﬁcation runs in time polynomial in the
length of the proof. Thus the following problem is in NP for any of the usual axiomatic systems
A:
theorems = {(ϕ, 1n) : ϕ has a formal proof of length ≤n in system A} .
Web draft 2007-01-08 21:59

DRAFT
2.7. MORE THOUGHTS ABOUT P, NP, AND ALL THAT
p2.21 (59)
In fact, the exercises ask you to prove that this problem is NP-complete. Hence the P versus
NP question is a rephrasing of G¨odel’s question (see quote at the beginning of the chapter), which
asks whether or not there is a algorithm that ﬁnds mathematical proofs in time polynomial in the
length of the proof.
Of course, all our students know in their guts that ﬁnding correct proofs is far harder than
verifying their correctness. So presumably, they believe at an intuitive level that P ̸= NP.
2.7.3
What if P = NP?
If P = NP —speciﬁcally, if an NP-complete problem like 3SAT had a very eﬃcient algorithm
running in say O(n2) time— then the world would be mostly a Utopia. Mathematicians could
be replaced by eﬃcient theorem-discovering programs (a fact pointed out in Kurt G¨odel’s 1956
letter and discovered three decades later). In general for every search problem whose answer can be
eﬃciently veriﬁed (or has a short certiﬁcate of correctness), we will be able to ﬁnd the correct answer
or the short certiﬁcate in polynomial time. AI software would be perfect since we could easily do
exhaustive searches in a large tree of possibilities. Inventors and engineers would be greatly aided
by software packages that can design the perfect part or gizmo for the job at hand. VLSI designers
will be able to whip up optimum circuits, with minimum power requirements. Whenever a scientist
has some experimental data, she would be able to automatically obtain the simplest theory (under
any reasonable measure of simplicity we choose) that best explains these measurements; by the
principle of Occam’s Razor the simplest explanation is likely to be the right one. Of course, in
some cases it took scientists centuries to come up with the simplest theories explaining the known
data. This approach can be used to solve also non-scientiﬁc problems: one could ﬁnd the simplest
theory that explains, say, the list of books from the New York Times’ bestseller list. (NB: All these
applications will be a consequence of our study of the Polynomial Hierarchy in Chapter 5.)
Somewhat intriguingly, this Utopia would have no need for randomness. As we will later see, if
P = NP then randomized algorithms would buy essentially no eﬃciency gains over deterministic
algorithms; see Chapter 7. (Philosophers should ponder this one.)
This Utopia would also come at one price: there would be no privacy in the digital domain.
Any encryption scheme would have a trivial decoding algorithm. There would be no digital cash,
no SSL, RSA or PGP (see Chapter 10). We would just have to learn to get along better without
these, folks.
This utopian world may seem ridiculous, but the fact that we can’t rule it out shows how little
we know about computation. Taking the half-full cup point of view, it shows how many wonderful
things are still waiting to be discovered.
2.7.4
What if NP = coNP?
If NP = coNP, the consequences still seem dramatic. Mostly, they have to do with existence of
short certiﬁcates for statements that do not seem to have any. To give an example, remember the
NP-complete problem of ﬁnding whether or not a set of multivariate polynomials has a common
Web draft 2007-01-08 21:59

DRAFT
p2.22 (60)
2.7. MORE THOUGHTS ABOUT P, NP, AND ALL THAT
root, in other words, deciding whether a system of equations of the following type has a solution:
f1(x1, . . . , xn) = 0
f2(x1, . . . , xn) = 0
...
fm(x1, . . . , xn) = 0
where each fi is a quadratic polynomial.
If a solution exists, then that solution serves as a certiﬁcate to this eﬀect (of course, we have to
also show that the solution can be described using a polynomial number of bits, which we omit).
The problem of deciding that the system does not have a solution is of course in coNP. Can we
give a certiﬁcate to the eﬀect that the system does not have a solution? Hilbert’s Nullstellensatz
Theorem seems to do that: it says that the system is infeasible iﬀthere is a sequence of polynomials
g1, g2, . . . , gm such that P
i figi = 1, where 1 on the right hand side denotes the constant polynomial
1.
What is happening? Does the Nullstellensatz prove coNP = NP? No, because the degrees of
the gi’s —and hence the number of bits used to represent them— could be exponential in n, m.
(And it is simple to construct fi’s for which this is necessary.)
However, if NP = coNP then there would be some other notion of a short certiﬁcate to the
eﬀect that the system is infeasible. The eﬀect of such a result on mathematics would probably be
even greater than the eﬀect of Hilbert’s Nullstellensatz. Of course, one can replace Nullstellensatz
with any other coNP problem in the above discussion.
What have we learned?
• The class NP consists of all the languages for which membership can be cer-
tiﬁed to a polynomial-time algorithm. It contains many important problems
not known to be in P. NP can also be deﬁned using non-deterministic Turing
machines.
• NP-complete problems are the hardest problems in NP, in the sense that
they have a polynomial-time algorithm if and only if P =NP. Many natural
problems that seemingly have nothing to do with Turing machines turn out
to be NP-complete. One such example is the language 3SAT of satisﬁable
Boolean formulae in 3CNF form.
• If P = NP then for every search problem for which one can eﬃciently verify
a given solution, one can also eﬃciently ﬁnd such a solution from scratch.
Chapter notes and history
In the 1950’s, Soviet scientists were aware of the undesirability of using exhaustive or brute force
search, which they called perebor, for combinatorial problems, and asked the question of whether
Web draft 2007-01-08 21:59

DRAFT
2.7. MORE THOUGHTS ABOUT P, NP, AND ALL THAT
p2.23 (61)
certain problems inherently require such search (see [Tra84]).
In the west the ﬁrst published
description of this issue is by Edmonds [Edm65], in the paper quoted in the previous chapter.
However, on both sides of the iron curtain it took some time to realize the right way to formulate
the problem and to arrive at the modern deﬁnition of the classes NP and P. Amazingly, in his 1956
letter to von Neumann we quoted above, G¨odel essentially asks the question of P vs. NP, although
there is no hint that he realized that one of the particular problems he mentions is NP-complete.
Unfortunately, von Neumann was very sick at the time, and as far as we know, no further research
was done by either on them on this problem, and the letter was only discovered in the 1980’s.
In 1971 Cook published his seminal paper deﬁning the notion of NP-completeness and showing
that SAT is NP complete [Coo71].
Soon afterwards, Karp [Kar72] showed that 21 important
problems are in fact NP-complete, generating tremendous interest in this notion. Meanwhile in
the USSR Levin independently deﬁned NP-completeness (although he focused on search problems)
and showed that a variant of SAT is NP-complete. (Levin’s paper [Lev73] was published in 1973,
but he had been giving talks on his results since 1971, also in those days there was essentially zero
communication between eastern and western scientists.) See Sipser’s survey [Sip92] for more on
the history of P and NP and a full translation of G¨odel’s remarkable letter.
The “TSP book” by Lawler et al. [LLKS85] also has a similar chapter, and it traces interest
in the Traveling Salesman Problem back to the 19th century. Furthermore, a recently discovered
letter by Gauss to Schumacher shows that Gauss was thinking about methods to solve the famous
Euclidean Steiner Tree problem —today known to be NP-hard— in the early 19th century.
As mentioned above, the book by Garey and Johnson [GJ79] and the web site [CK00] contain
many more examples of NP complete problem. Also, Aaronson [Aar05] surveys various attempts
to solve NP complete problems via “non-traditional” computing devices.
Even if NP ̸= P, this does not necessarily mean that all of the utopian applications mentioned
in Section 2.7.3 are gone. It may be that, say, 3SAT is hard to solve in the worst case on every input
but actually very easy on the average, See Chapter 15 for a more detailed study of average-case
complexity. Also, Impagliazzo [Imp95] has an excellent survey on this topic.
Exercises
§1 Prove the existence of a non-deterministic Universal TM (analogously to the deterministic
universal TM of Theorem 1.13). That is, prove that there exists a representation scheme of
NDTMs, and an NDTM NU such that for every string α, and input x, NU(x, α) = Mα(x).
(a) Prove that there exists such a universal NDTM NU such that if Mα halts on x within
T steps, then NU halts on x, α within CT log T steps (where C is a constant depending
only on the machine represented by α).
(b) Prove that there is such a universal NDTM that runs on these inputs for at most Ct
steps.
Web draft 2007-01-08 21:59

DRAFT
p2.24 (62)
2.7. MORE THOUGHTS ABOUT P, NP, AND ALL THAT
Hint: A simulation in O(|α|t log t) time can be obtained by a
straightforward adaptation of the proof of Theorem 1.13. To do a
more eﬃcient simulation, the main idea is to ﬁrst run a simulation
of M without actually reading the contents of the work tapes, but
rather simply non-deterministically guessing these contents, and
writing those guesses down. Then, go over tape by tape and verify
that all guesses were consistent.
§2 Prove Theorem 2.
§3 Let HALT be the Halting language deﬁned in Theorem 1.17. Show that HALT is NP-hard.
Is it NP-complete?
§4 We have deﬁned a relation ≤p among languages. We noted that it is reﬂexive (that is, A ≤p A
for all languages A) and transitive (that is, if A ≤p B and B ≤p C then A ≤p C). Show that
it is not commutative, namely, A ≤p B need not imply B ≤p A.
§5 Suppose L1, L2 ∈NP. Then is L1 ∪L2 in NP? What about L1 ∩L2?
§6 Mathematics can be axiomatized using for example the Zermelo Frankel system, which has a
ﬁnite description. Argue at a high level that the following language is NP-complete.
{⟨ϕ, 1n⟩: math statement ϕ has a proof of size at most n in the ZF system} .
Hint: Why is this language in NP? Is boolean satisﬁability a
mathmatical statement?
The question of whether this language is in P is essentially the question asked by G¨odel in
the chapter’s initial quote.
§7 Show that NP = coNP iﬀ3SAT and TAUTOLOGY are polynomial-time reducible to one
another.
§8 Can you give a deﬁnition of NEXP without using NDTMs, analogous to the deﬁnition of
NP in Deﬁnition 2.1? Why or why not?
§9 We say that a language is NEXP-complete if it is in NEXP and every language in NEXP
is polynomial-time reducible to it. Describe a NEXP-complete language. Prove that if this
problem is in EXP then NEXP = EXP.
§10 Show that for every time constructible T : N →N, if L ∈NTIME(T(n)) then we can give a
polynomial-time Karp reduction from L to 3SAT that transforms instances of size n into 3CNF
formulae of size O(T(n) log T(n)). Can you make this reduction also run in O(T(n) log T(n))?
§11 Recall that a reduction f from an NP-language L to an NP-languages L′ is parsimonious if
the number of certiﬁcates of f is equal to the number of certiﬁcates of f(x).
(a) Prove that the reduction from every NP-language L to SAT presented in the proof of
Lemma 2.12 is parsimonious.
Web draft 2007-01-08 21:59

DRAFT
2.7. MORE THOUGHTS ABOUT P, NP, AND ALL THAT
p2.25 (63)
(b) Show a parsimonious reduction from SAT to 3SAT.
§12 The notion of polynomial-time reducibility used in Cook’s paper was somewhat diﬀerent: a
language A is polynomial-time Cook reducible to a language B if there is a polynomial time
TM M that, given an oracle for deciding B, can decide A. (An oracle for B is a magical extra
tape given to M, such that whenever M writes a string on this tape and goes into a special
“invocation” state, then the string —in a single step!—gets overwritten by 1 or 0 depending
upon whether the string is or is not in B, see Section ??)
Show that the notion of cook reducibility is transitive and that 3SAT is Cook-reducible to
TAUTOLOGY.
§13 (Berman’s Theorem 1978) A language is called unary if every string in it is of the form 1i
(the string of i ones) for some i > 0. Show that if a unary language is NP-complete then
P = NP. (See Exercise 6 of Chapter 6 for a strengthening of this result.)
Hint: If there is a nc time reduction from 3SAT to a unary lan-
guage L, then this reduction can only map size n instances of 3SAT
to some string of the form 1i where i ≤nc. Use this observation to
obtain a polynomial-time algorithm for SAT using the downward
self reducibility argument of Theorem 2.19.
§14 In the CLIQUE problem we are given an undirected graph G and an integer K and have to
decide whether there is a subset S of at least K vertices such that every two distinct vertices
u, v ∈S have an edge between them (such a subset is called a clique). In the VERTEX COVER
problem we are given an undirected graph G and an integer K and have to decide whether
there is a subset S of at most K vertices such that for every edge {i, j} of G, at least one of
i or j is in S. Prove that both these problems are NP-complete.
Hint: reduce from INDSET.
§15 In the MAX CUT problem we are given an undirected graph G and an integer K and have to
decide whether there is a subset of vertices S such that there are at least K edges that have
one endpoint in S and one endpoint in S. Prove that this problem is NP-complete.
§16 In the Exactly One 3SAT problem, we are given a 3CNF formula ϕ and need to decide if
there exists a satisfying assignment u for ϕ such that every clause of ϕ has exactly one True
literal. In the SUBSET SUM
problem we are given a list of n numbers A1, . . . , An and a
number T and need to decide whether there exists a subset S ⊆[n] such that P
i∈S Ai = T
(the problem size is the sum of all the bit representations of all numbers). Prove that both
Exactly One3SAT and SUBSET SUM are NP-complete.
Web draft 2007-01-08 21:59

DRAFT
p2.26 (64)
2.7. MORE THOUGHTS ABOUT P, NP, AND ALL THAT
Hint: For Exactly One 3SAT replace each occurrence of a literal
vi in a clause C by a new variable zi,C and clauses and auxiliary
variables ensuring that if vi is True then zi,C is allowed to be either
True or False but if vi is false then zi,C must be False. The
approach for the reduction of Exactly One 3SAT to SUBSET SUM
is that given a formula ϕ, we map it to a SUBSET SUM instance by
mapping each possible literal ui to the number P
j∈Si(2n)j where
Si is the set of clauses that the literal ui satisﬁes, and setting the
target T to be Pm
j=1(2n)j . An additional trick is required to ensure
that the solution to the subset sum instance will not include two
literals that correspond to a variable and its negation.
§17 Prove that the language HAMPATH of undirected graphs with Hamiltonian paths is NP-
complete. Prove that the language TSP described in Example 2.2 is NP-complete. Prove
that the language HAMCYCLE of undirected graphs that contain Hamiltonian cycle (a simple
cycle involving all the vertices) is NP-complete.
§18 Let quadeq be the language of all satisﬁable sets of quadratic equations over 0/1 variables
(a quadratic equations over u1, . . . , un has the form form P
ai,j uiuj = b), where addition is
modulo 2. Show that quadeq is NP-complete.
Hint: Reduce from SAT
§19 Prove that P ⊆NP ∩coNP.
§20 Prove that the Deﬁnitions 2.21 and 2.22 do indeed deﬁne the same class coNP.
§21 Suppose L1, L2 ∈NP ∩coNP. Then show that L1 ⊕L2 is in NP ∩coNP, where L1 ⊕L2 =
{x : x is in exactly one of L1, L2}.
§22 Deﬁne the language UNARY SUBSET SUM to be the variant of the SUBSET SUM problem of
Exercise 16 where all numbers are represented by the unary representation (i.e., the number
k is represented as 1k). Show that UNARY SUBSET SUM is in P.
Hint: Start with an exponential-time recursive algorithm for SUB-
SET SUM, and show that in this case you can make it into a
polynomial-time algorithm by storing previously computed values
in a table.
§23 Prove that if every unary NP-language is in P then EXP = NEXP. (A language L is unary
if it is a subset of {1}∗, see Exercise 13.)
Web draft 2007-01-08 21:59

DRAFT
Chapter 3
Diagonalization
“..the relativized P =?NP question has a positive answer for some oracles and a
negative answer for other oracles. We feel that this is further evidence of the diﬃculty
of the P =?NP question.”
Baker, Gill, Solovay. [BGS75]
One basic goal in complexity theory is to separate interesting complexity classes. To separate
two complexity classes we need to exhibit a machine in one class that gives a diﬀerent answer
on some input from every machine in the other class.
This chapter describes diagonalization,
essentially the only general technique known for constructing such a machine. We have already seen
diagonalization in Section 1.4, where it was used to show the existence of uncomputable functions.
In this chapter, we ﬁrst use diagonalization to prove hierarchy theorems, according to which giving
Turing machines more computational resources (such as time, space, and non-determinism) allows
them to solve a strictly larger number of problems. We will also use it to show that if P ̸= NP
then there exist problems that are neither in P nor NP-complete.
Though diagonalization led to some of these early successes of complexity theory, researchers
realized in the 1970s that diagonalization alone may not resolve P versus NP and other interesting
questions; see Section 3.5. Interestingly, the limits of diagonalization are proved using diagonaliza-
tion.
This last result caused diagonalization to go out of favor for many years. But some recent
results (see Section 16.3 for an example) use diagonalization as a key component. Thus future
complexity theorists should master this simple idea before going on to anything fancier!
Machines as strings and the universal TM.
The one common tool used in all diagonalization
proofs is the representation of TMs by strings, such that given a string x a universal TM can
simulate the machine Mx represented by x with a small (i.e. at most logarithmic) overhead, see
Theorems 1.13, ?? and ??. Recall that we assume that every string x represents some machine and
every machine is represented by inﬁnitely many strings. For i ∈N, we will also use the notation Mi
for the machine represented by the string that is the binary expansion of the number i (ignoring
the leading 1).
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p3.1 (65)

DRAFT
p3.2 (66)
3.1. TIME HIERARCHY THEOREM
3.1
Time Hierarchy Theorem
The Time Hierarchy Theorem shows that allowing Turing Machines more computation time strictly
increases the class of languages that they can decide. Recall that a function f : N →N is a time-
constructible function if there is a Turing machine that, given the input 1n, writes down 1f(n) on its
tape in O(f(n)) time. Usual functions like n log n or n2 satisfy this property, and we will restrict
attention to running times that are time-constructible.
Theorem 3.1
If f, g are time-constructible functions satisfying f(n) log f(n) = o(g(n)), then
DTIME(f(n)) ⊊DTIME(g(n))
(1)
Proof: To showcase the essential idea of the proof of Theorem 3.1, we prove the simpler statement
DTIME(n) ⊊DTIME(n1.5).
Consider the following Turing Machine D: “On input x, run for |x|1.4 steps the Universal TM
U of Theorem 1.13 to simulate the execution of Mx on x. If Mx outputs an answer in this time,
namely, Mx(x) ∈{0, 1} then output the opposite answer (i.e., output 1 −Mx(x)). Else output 0.”
Here Mx is the machine represented by the string x.
By deﬁnition, D halts within n1.4 steps and hence the language L decided by D is in DTIME(n1.5).
We claim that L ̸∈DTIME(n).
For contradiction’s sake assume that some TM M decides L but runs in time cn on inputs of
size n. Then every x ∈{0, 1}∗, M(x) = D(x).
The time to simulate M by the universal Turing machine U on every input x is at most
c′c|x| log |x| for some constant c′ (depending on the alphabet size and number of tapes and states of
M, but independent of |x|). There exists a number n0 such that for every n ≥n0, n1.4 > c′cn log n.
Let x be a string representing the machine M of length at least n0 (there exists such a string
since M is represented by inﬁnitely many strings). Then, D(x) will obtain the output M(x) within
|x|1.4 steps, but by deﬁnition of D, we have D(x) = 1 −M(x) ̸= M(x). Thus we have derived a
contradiction. ■
3.2
Space Hierarchy Theorem
The space hierarchy theorem is completely analogous to the time hierarchy theorem. One restricts
attention to space-constructible functions, which are functions f : N →N for which there is a
machine that, given any n-bit input, constructs f(n) in space O(f(n)). The proof of the next
theorem is completely analogous to that of Theorem 3.1. (The theorem does not have the log f(n)
factor because the universal machine for space-bounded computation incurs only a constant factor
overhead in space; see Theorem ??.)
Theorem 3.2
If f, g are space-constructible functions satisfying f(n) = o(g(n)), then
SPACE(f(n)) ⊊SPACE(g(n))
(2)
Web draft 2007-01-08 21:59

DRAFT
3.3. NONDETERMINISTIC TIME HIERARCHY THEOREM
p3.3 (67)
3.3
Nondeterministic Time Hierarchy Theorem
The following is the hierarchy theorem for non-deterministic Turing machines.
Theorem 3.3
If f, g are time constructible functions satisfying f(n + 1) = o(g(n)), then
NTIME(f(n)) ⊊NTIME(g(n))
(3)
Proof: Again, we just showcase the main idea by proving NTIME(n) ⊊NTIME(n1.5). The
technique from the previous section does not directly apply, since it has to determine the answer of
a TM in order to ﬂip it. To determine the answer of a nondeterminisitic that runs in O(n) time, we
may need to examine as many as 2Ω(n) possible strings of non-deterministic choices. So it is unclear
that how the “diagonalizer” machine can determine in O(n1.5) (or even O(n100)) time how to ﬂip
this answer. Instead we introduce a technique called lazy diagonalization, which is only guaranteed
to ﬂip the answer on some input in a fairly large range.
For every i ∈N we denote by Mi the non-deterministic TM represented by i’s binary expansion
according to the universal NDTM NU (see Theorem ??). We deﬁne the function f : N →N as
follows: f(1) = 2 and f(i + 1) = 2f(i)1.2. Note that given n, we can can easily ﬁnd in O(n1.5) time
the number i such that n is sandwiched between f(i) and f(i + 1). Our diagonalizing machine D
will try to ﬂip the answer of Mi on some input in the set {1n : f(i) < n ≤f(i + 1)}. It is deﬁned
as follows:
“On input x, if x ̸∈1∗, reject. If x = 1n, then compute i such that f(i) < n ≤f(i + 1) and
1. If f(i) < n < f(i + 1) then simulate Mi on input 1n+1 using nondeterminism in n1.1 time
and output its answer. (If the simulation takes more than that then halt and accept.)
2. If n = f(i + 1), accept 1n iﬀMi rejects 1f(i)+1 in (f(i) + 1)1.5 time.”
Note that Part 2 requires going through all possible exp((f(i) + 1)1.1) branches of Mi on input
1f(i)+1, but that is ﬁne since the input size f(i + 1) is 2f(i)1.2. We conclude that NDTM D runs in
O(n1.5) time. Let L be the language decided by D. We claim that L ̸∈NTIME(n).
Indeed, suppose for the sake of contradiction that L is decided by an NDTM M running in cn
steps (for some constant c). Since each NDTM is represented by inﬁnitely many strings, we can
ﬁnd i large enough such that M = Mi and on inputs of length n ≥f(i), Mi can be simulated in
less than n1.1 steps. Thus the two steps in the description of D ensure respectively that
If f(i) < n < f(i + 1),
then D(1n) = Mi(1n+1)
(4)
D(1f(i+1)) ̸= Mi(1f(i)+1)
(5)
see Figure 3.1.
By our assumption Mi and D agree on all inputs 1n for n ∈(f(i), f(i + 1)]. Together with (4),
this implies that D(1f(i+1)) = Mi(1f(i)+1), contradicting(5). ■
Web draft 2007-01-08 21:59

DRAFT
p3.4 (68)
3.4. LADNER’S THEOREM: EXISTENCE OF NP-INTERMEDIATE PROBLEMS.
D(1f(i)+1) D(1f(i)+2)
....
D(1f(i+1))
Mi(1f(i)+1) Mi(1f(i)+2)
....
Mi(1f(i+1))
=
=
=
=
=
=
=
= =
=
Figure 3.1: The values of D and Mi on inputs 1n for n ∈(f(i), f(i + 1)]. Full lines denote equality by the design
of D, dashed lines denote equality by the assumption that D(x) = Mi(x) for every x, and the dashed arrow denotes
inequality by the design of D. Note that together all these relations lead to contradiction.
3.4
Ladner’s Theorem: Existence of NP-intermediate problems.
One of the striking aspects of NP-completeness is the surprisingly large number of NP-problems
–including some that were studied for many decades— that turned out to be NP-complete. This
phenomenon suggests a bold conjecture: every problem in NP is either in P or NP complete. We
show that if P ̸= NP then this is false —there is a language L ∈NP \ P that is not NP-complete.
(If P = NP then the conjecture is trivially true but uninteresting.) The rest of this section proves
this.
Theorem 3.4 (Ladner’s Theorem [?])
Suppose that P ̸= NP. Then there exists a language L ∈NP \ P that is not NP-complete.
Proof: If P ̸= NP then we know at least one language in NP \ P: namely, the NP-complete lan-
guage SAT. Consider the language SATH of length n satisﬁable formulae that are padded with nH(n)
1’s for some polynomial-time computable function H : N →N (i.e., SATH =
n
ψ01nH(n) : ψ ∈SAT and n = |ψ|
o
).
Consider two possibilities:
(a) H(n) is at most some constant c for every n. In this case SATH is simply SAT with a polynomial
amount of “padding.” Thus, SATH is also NP-complete and is not in P if P ̸= NP.
(b) H(n) tends to inﬁnity with n, and thus the padding is of superpolynomial size. In this case,
we claim that SATH cannot be NP-complete.
Indeed, if there is a O(ni)-time reduction
f from SAT to SATH then such a reduction reduces the satisﬁability of SAT instances of
length n to instances of SATH of length O(ni), which must have the form ψ01|ψ|H(|ψ|), where
|ψ| + |ψ|H(|ψ|) = O(ni), and hence |ψ| = o(n). In other words, we have a polynomial-time
reduction from SAT instances of length n to SAT instances of length o(n), which implies SAT
can be solved in polynomial time. (The algorithm consists of applying the reduction again
and again, reducing the size of the instances each time until the instance is of size O(1) and
can be solved in O(1) time by brute force) This is a contradiction to the assumption P ̸= NP.
Web draft 2007-01-08 21:59

DRAFT
3.4. LADNER’S THEOREM: EXISTENCE OF NP-INTERMEDIATE PROBLEMS.
p3.5 (69)
The proof of the Theorem uses a language SATH for a function H that in some senses combines
the two cases above. This function tends to inﬁnity with n, so that SATH is not NP-complete as
in Case (b), but grows slowly enough to assure SATH ̸∈P as in Case (a). Function H is deﬁned as
follows:
H(n) is the smallest number i < log log n such that for every x ∈{0, 1}∗with |x| ≤log n,
Mi halts on x within i|x|i steps and Mi outputs 1 iﬀx ∈SATH
where Mi is the machine represented by the binary expansion of i according to the
representation scheme of the universal Turing machine U of Theorem 1.13. If there is
no such i then we let H(n) = log log n.
Notice, this is implicitly a recursive deﬁnition since the deﬁnition of H depends on SATH, but
a moment’s thought shows that H is well-deﬁned since H(n) determines membership in SATH of
strings whose length is greater than n, and the deﬁnition of H(n) only relies upon checking the
status of strings of length at most log n.
There is a trivial algorithm to compute H(n) in O(n3) time. After all, we only need to (1)
compute H(k) for every k ≤log n, (2) simulate at most log log n machines for every input of length
at most log n for log log n(log n)log log n = o(n) steps, and (3) compute SAT on all the inputs of
length at most log n.
Now we have the following two claims.
claim 1: SATH is not in P. Suppose, for the sake of contradiction, that there is a machine M
solving SATH in at most cnc steps. Since M is represented by inﬁnitely many strings, there is
a number i > c such that M = Mi. By the deﬁnition of H(n) this implies that for n > 22i,
H(n) ≤i. But this means that for all suﬃciently large input lengths, SATH is simply the language
SAT padded with a polynomial (i.e., ni) number of 1’s, and so cannot be in P unless P = NP.
claim 2: SATH is not NP-complete. As in Case (b), it suﬃces to show that H(n) tends to inﬁnity
with n. We prove the equivalent statement that for every integer i, there are only ﬁnitely many
n’s such that H(n) = i: since SATH ̸∈P, for each i we know that there is an input x such that
given i|x|i time, Mi gives the incorrect answer to whether or not x ∈SATH. Then the deﬁnition of
H ensures that for every n > 2|x|, H(x) ̸= i.
■
Remark 3.5
We do not know of a natural decision problem that, assuming NP ̸= P, is proven to be in NP \ P
but not NP-complete, and there are remarkably few candidates for such languages.
However,
there are a few fascinating examples for languages not known to be either in P nor NP-complete.
Two such examples are the Factoring and Graph isomorphism languages (see Example 2.2). No
polynomial-time algorithm is currently known for these languages, and there is some evidence that
they are not NP complete (see Chapter 8).
Web draft 2007-01-08 21:59

DRAFT
p3.6 (70)
3.5. ORACLE MACHINES AND THE LIMITS OF DIAGONALIZATION?
3.5
Oracle machines and the limits of diagonalization?
Quantifying the limits of “diagonalization” is not easy. Certainly, the diagonalization in Sections 3.3
and 3.4 seems more clever than the one in Section 3.1 or the one that proves the undecidability of
the halting problem.
For concreteness, let us say that “diagonalization” is any technique that relies upon the following
properties of Turing machines:
I The existence of an eﬀective representation of Turing machines by strings.
II The ability of one TM to simulate any another without much overhead in running time or space.
Any argument that only uses these facts is treating machines as blackboxes: the machine’s
internal workings do not matter. We now show a general way to deﬁne a variant of Turing Machines
called oracle Turing Machines that still satisfy the above two properties. However, one way of
deﬁning the variants results in TMs for which P = NP, whereas the other way results in TMs for
which P ̸= NP. We conclude that to resolve P versus NP we need to use some other property
besides the above two.
Oracle machines will be used elsewhere in this book in other contexts. These are machines that
are given access to an “oracle” that can magically solve the decision problem for some language
O ⊆{0, 1}∗. The machine has a special oracle tape on which it can write a string q ∈{0, 1}∗on a
and in one step gets an answer to a query of the form “Is q in O?”. This can be repeated arbitrarily
often with diﬀerent queries. If O is a diﬃcult language that cannot be decided in polynomial time
then this oracle gives an added power to the TM.
Definition 3.6 (Oracle Turing Machines)
An oracle Turing machine is a TM M that has a special read/write tape we call M’s oracle tape and
three special states qquery, qyes, qno. To execute M, we specify in addition to the input a language
O ⊆{0, 1}∗that is used as the oracle for M. Whenever during the execution M enters the state
qquery, the machine moves into the state qyes if q ∈O and qno if q ̸∈O, where q denotes the contents
of the special oracle tape. Note that, regardless of the choice of O, a membership query to O
counts only as a single computational step. If M is an oracle machine, O ⊆{0, 1}∗a language, and
x ∈{0, 1}∗, then we denote the output of M on input x and with oracle O by MO(x).
Nondeterministic oracle TMs are deﬁned similarly.
Definition 3.7
For every O ⊆{0, 1}∗, PO is the set of languages decided by a polynomial-time deterministic
TM with oracle access to O and NPO is the set of languages decided by a polynomial-time non-
deterministic TM with oracle access to O.
To illustrate these deﬁnitions we show a few simple claims.
Claim 3.8
1. Let SAT denote the language of unsatisﬁable formulae. Then SAT ∈PSAT.
2. Let O ∈P. Then PO = P.
Web draft 2007-01-08 21:59

DRAFT
3.5. ORACLE MACHINES AND THE LIMITS OF DIAGONALIZATION?
p3.7 (71)
3. Let EXPCOM be the following language
{⟨M, x, 1n⟩: M outputs 1 on x within 2n steps} .
Then PEXPCOM = NPEXPCOM = EXP. (Recall that EXP = ∪cDTIME(2nc).)
Proof:
1. Given oracle access to SAT, to decide whether a formula ϕ is in SAT, the machine asks the
oracle if ϕ ∈SAT, and then gives the opposite answer as its output.
2. Allowing an oracle can only help compute more languages and so P ⊆PO. If O ∈P then
it is redundant as an oracle, since we can transform any polynomial-time oracle TM using O
into a standard (no oracle) by simply replacing each oracle call with the computation of O.
Thus PO ⊆P.
3. Clearly, an oracle to EXPCOM allows one to perform an exponential-time computation at the
cost of one call, and so EXP ⊆PEXPCOM. On the other hand, if M is a non-deterministic
polynomial-time oracle TM, we can simulate its execution with a EXPCOM oracle in expo-
nential time: such time suﬃces both to enumerate all of M’s non-deterministic choices and
to answer the EXPCOM oracle queries. Thus, EXP ⊆PEXPCOM ⊆NPEXPCOM ⊆EXP.
■
The key fact to note about oracle TMs is the following: Regardless of what oracle O is, the set
of all oracle TM’s with access to oracle O satisfy Properties I and II above. The reason is that we
can represent TMs with oracle O as strings, and we have a universal TM OU that, using access
to O, can simulate every such machine with logarithmic overhead, just as Theorem 1.13 shows for
non-oracle machines. Indeed, we can prove this in exactly the same way of Theorem 1.13, except
that whenever in the simulation M makes an oracle query, OU forwards the query to its own oracle.
Thus any result about TMs or complexity classes that uses only Properties I and II above also
holds for the set of all TMs with oracle O. Such results are called relativizing results.
All of the results on universal Turing machines and the diagonalizations results in this chapter
are of this type.
The next theorem implies that whichever of P = NP or P ̸= NP is true, it cannot be a
relativizing result.
Theorem 3.9 (Baker, Gill, Solovay [BGS75])
There exist oracles A, B such that PA = NPA and PB ̸= NPB.
Proof: As seen in Claim 3.8, we can use A = EXPCOM. Now we construct B. For any language
B, let UB be the unary language
UB = {1n : some string of length n is in B} .
For every oracle B, the language UB is clearly in NPB, since a non-deterministic TM can make a
non-deterministic guess for the string x ∈{0, 1}n such that x ∈B. Below we construct an oracle
B such that UB ̸∈PB, implying that PB ̸= NPB.
Web draft 2007-01-08 21:59

DRAFT
p3.8 (72)
3.5. ORACLE MACHINES AND THE LIMITS OF DIAGONALIZATION?
Construction of B:
For every i, we let Mi be the oracle TM represented by the binary expansion
of i. We construct B in stages, where stage i ensures that MB
i
does not decide UB in 2n/10 time.
Initially we let B be empty, and gradually add strings to it. Each stage determines the status (i.e.,
whether or not they will ultimately be in B) of a ﬁnite number of strings.
Stage i: So far, we have declared for a ﬁnite number of strings whether or not they are in B.
Choose n large enough so that it exceeds the length of any such string, and run Mi on input 1n
for 2n/10 steps. Whenever it queries the oracle about strings whose status has been determined,
we answer consistently. When it queries strings whose status is undetermined, we declare that the
string is not in B. Note that until this point, we have not declared that B has any string of length
n. Now we make sure that if Mi halts within 2n/10 steps then its answer on 1n is incorrect. If Mi
accepts, we declare that all strings of length n are not in B, thus ensuring 1n ̸∈Bu. If Mi rejects,
we pick a string of length n that it has not queried (such a string exists because Mi made at most
2n/10 queries) and declare that it is in B, thus ensuring 1n ∈Bu. In either case, the answer of Mi
is incorrect. Our construction ensures that UB is not in PB (and in fact not in DTIMEB(f(n))
for every f(n) = o(2n)). ■
Let us now answer our original question: Can diagonalization or any simulation method resolve
P vs NP? Answer: Possibly, but it has to use some fact about TMs that does not hold in presence
of oracles.
Such facts are termed nonrelativizing and we will later see examples of such facts.
However, a simple one was already encountered in Chapter ??: the Cook-Levin theorem! It is not
true for a general oracle A that every language L ∈NPA is polynomial-time reducible to 3SAT
(see Exercise 6). Note however that nonrelativizing facts are necessary, not suﬃcient. It is an
open question how to use known nonrelativizing facts in resolving P vs NP (and many interesting
complexity theoretic conjectures).
Whenever we prove a complexity-theoretic fact, it is useful to check whether or not it can be
proved using relativizing techniques. The reader should check that Savitch’s theorem (Corollary ??)
and Theorem 4.18 do relativize.
Later in the book we see other attempts to separate complexity classes, and we will also try to
quantify —using complexity theory itself!—why they do not work for the P versus NP question.
What have we learned?
• Diagonalization uses the representation of Turing machines as strings to sep-
arate complexity classes.
• We can use it to show that giving a TM more of the same type of resource
(time, non-determinism, space) allows it to solve more problems, and to show
that, assuming NP ̸= P, NP has problems neither in P nor NP-complete.
• Results proven solely using diagonalization relativize in the sense that they
hold also for TM’s with oracle access to O, for every oracle O ⊆{0, 1}∗. We
can use this to show the limitations of such methods. In particular, relativizing
methods alone cannot resolve the P vs. NP question.
Web draft 2007-01-08 21:59

DRAFT
3.5. ORACLE MACHINES AND THE LIMITS OF DIAGONALIZATION?
p3.9 (73)
Chapter notes and history
Georg Cantor invented diagonalization in the 19th century to show that the set of real numbers
is uncountable.
Kurt G¨odel used a similar technique in his proof of the Incompleteness Theo-
rem. Computer science undergraduates often encounter diagonalization when they are taught the
undecidabilty of the Halting Problem.
The time hierarchy theorem is from Hartmanis and Stearns’ pioneering paper [HS65].
The
space hierarchy theorem is from Stearns, Hartmanis, and Lewis [SHL65]. The nondeterministic
time hierarchy theorem is from Cook [Coo73], though the simple proof given here is essentially
from [Zak83]. A similar proof works for other complexity classes such as the (levels of the) poly-
nomial hierarchy discussed in the next chapter. Ladner’s theorem is from [?] but the proof here is
due to an unpublished manuscript by Impagliazzo. The notion of relativizations of the P versus
NP question is from Baker, Gill, and Solovay [BGS75], though the authors of that paper note
that other researchers independently discovered some of their ideas. The notion of relativization is
related to similar ideas in logic (such as independence results) and recursive function theory.
The notion of oracle Turing machines can be used to study interrelationships of complexity
classes.
In fact, Cook [Coo71] deﬁned NP-completeness using oracle machines.
A subﬁeld of
complexity theory called structural complexity has carried out a detailed study of oracle machines
and classes deﬁned using them; see [].
Whether or not the Cook-Levin theorem is a nonrelativizing fact depends upon how you for-
malize the question. There is a way to allow the 3SAT instance to “query” the oracle, and then the
Cook-Levin theorem does relativize. However, it seems safe to say that any result that uses the
locality of computation is looking at the internal workings of the machine and hence is potentially
nonrelativizing.
The term superiority introduced in the exercises does not appear in the literature but the
concept does. In particular, ??? have shown the limitations of relativizing techniques in resolving
certain similar open questions.
Exercises
§1 Show that the following language is undecidable:

⌞M⌟: M is a machine that runs in 100n2 + 200 time
	
.
§2 Show that SPACE(n) ̸= NP. (Note that we do not know if either class is contained in the
other.)
§3 Show that there is a language B ∈EXP such that NPB ̸= PB.
§4 Say that a class C1 is superior to a class C2 if there is a machine M1 in class C1 such that for
every machine M2 in class C2 and every large enough n, there is an input of size between n
and n2 on which M1 and M2 answer diﬀerently.
(a) Is DTIME(n1.1) superior to DTIME(n)?
Web draft 2007-01-08 21:59

DRAFT
p3.10 (74)
3.5. ORACLE MACHINES AND THE LIMITS OF DIAGONALIZATION?
(b) Is NTIME(n1.1) superior to NTIME(n)?
§5 Show that there exists a function that is not time-constructible.
§6 Show that there is an oracle A and a language L ∈NPA such that L is not polynomial-time
reducible to 3SAT even when the machine computing the reduction is allowed access to A.
§7 Suppose we pick a random language B, by deciding for each string independently and with
probability 1/2 whether or not it is in B. Show that with high probability PB ̸= NPB. (To
give an answer that is formally correct you may need to know elementary measure theory.)
Web draft 2007-01-08 21:59

DRAFT
Chapter 4
Space complexity
“(our) construction... also suggests that what makes “games” harder than “puzzles”
(e.g. NP-complete problems) is the fact that the initiative (“the move”) can shift
back and forth between the players.”
Shimon Even and Robert Tarjan, 1976
In this chapter we will study the memory requirements of computational tasks. To do this we
deﬁne space-bounded computation, which has to be performed by the TM using a restricted number
of tape cells, the number being a function of the input size. We also study nondeterministic space-
bounded TMs. As in the chapter on NP, our goal in introducing a complexity class is to “capture”
interesting computational phenomena— in other words, identify an interesting set of computational
problems that lie in the complexity class and are complete for it. One phenomenon we will “capture”
this way (see Section 4.3.2) concerns computation of winning strategies in 2-person games, which
seems inherently diﬀerent from (and possibly more diﬃcult than) solving NP problems such as
SAT, as alluded to in the above quote. The formal deﬁnition of deterministic and non-deterministic
space bounded computation is as follows (see also Figure 4.1):
Input
tape
Work
tape
Output
tape
Register
read only head
read/write head
read/write head
Figure 4.1: Space bounded computation. Only cells used in the read/write tapes count toward the space bound.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p4.1 (75)

DRAFT
p4.2 (76)
4.1. CONFIGURATION GRAPHS.
Definition 4.1 (Space-bounded computation.)
Let S : N →N and L ⊆{0, 1}∗.
We say that L ∈SPACE(s(n)) (resp.
L ∈
NSPACE(s(n))) if there is a constant c and TM (resp. NDTM) M deciding L
such that on every input x ∈{0, 1}∗, the total number of locations that are at some
point non-blank during M’s execution on x is at most c·s(|x|). (Non-blank locations
in the read-only input tape do not count.)
As in our deﬁnitions of all nondeterministic complexity classes, we require all branches of
nondeterministic machines to always halt.
Remark 4.2
Analogously to time complexity, we will restrict our attention to space bounds S : N →N that are
space-constructible functions, by which we mean that there is a TM that computes S(n) in O(S(n))
space when given 1n as input. (Intuitively, if S is space-constructible, then the machine “knows”
the space bound it is operating under.) This is a very mild restriction since functions of interest,
including log n,n and 2n, are space-constructible.
Also, realize that since the work tape is separated from the input tape, it makes sense to consider
space-bounded machines that use space less than the input length, namely, S(n) < n. (This is in
contrast to time-bounded computation, where DTIME(T(n)) for T(n) < n does not make much
sense since the TM does not have enough time to read the entire input.) We will assume however
that S(n) > log n since the work tape has length n, and we would like the machine to at least be
able to “remember” the index of the cell of the input tape that it is currently reading. (One of the
exercises explores classes that result when S(n) ≪log n.)
Note that DTIME(S(n)) ⊆SPACE(S(n)) since a TM can access only one tape cell per step.
Also, notice that space can be reused: a cell on the work tape can be overwritten an arbitrary
number of times.
A space S(n) machine can easily run for as much as 2Ω(S(n)) steps —think
for example of the machine that uses its work tape of size S(n) to maintain a counter which it
increments from 1 to 2S(n)−1. The next easy theorem (whose proof appears a little later) shows
that this is tight in the sense that any languages in SPACE(S(n)) (and even NSPACE(S(n)))
is in DTIME(2O(S(n))). Surprisingly enough, up to logarithmic terms, this theorem contains the
only relationships we know between the power of space-bounded and time-bounded computation.
Improving this would be a major result.
Theorem 4.3
For every space constructible S : N →N,
DTIME(S(n)) ⊆SPACE(S(n)) ⊆NSPACE(S(n)) ⊆DTIME(2O(S(n)))
4.1
Conﬁguration graphs.
To prove Theorem 4.3 we use the notion of a conﬁguration graph of a Turing machine. This notion
will also be quite useful for us later in this chapter and the book. Let M be a (deterministic or
Web draft 2007-01-08 21:59

DRAFT
4.1. CONFIGURATION GRAPHS.
p4.3 (77)
Cstart
Caccept
αqβ
Figure 4.2: The conﬁguration graph GM,x is the graph of all conﬁgurations of M’s execution on x where there is
an edge from a conﬁguration C to a conﬁguration C′ if C′ can be obtained from C in one step. It has out-degree one
if M is deterministic and out-degree at most two if M is non-deterministic.
non-deterministic) TM. A conﬁguration of a TM M consists of the contents of all non-blank entries
of M’s tapes, along with its state and head position, at a particular point in its execution. For
every TM M and input x ∈{0, 1}∗, the conﬁguration graph of M on input x, denoted GM,x, is
a directed graph whose nodes correspond to possible conﬁgurations that M can reach from the
starting conﬁguration Cx
start (where the input tape is initialized to contain x). The graph has a
directed edge from a conﬁguration C to a conﬁguration C′ if C′ can be reached from C in one
step according to M’s transition function (see Figure 4.2). Note that if M is deterministic then
the graph has out-degree one, and if M is non-deterministic then it has an out-degree at most two.
Also note that we can assume that M’s computation on x does not repeat the same conﬁguration
twice (as otherwise it will enter into an inﬁnite loop) and hence that the graph is a directed acyclic
graph (DAG). By modifying M to erase all its work tapes before halting, we can assume that there
is only a single conﬁguration Caccept on which M halts and outputs 1. This means that M accepts
the input x iﬀthere exists a (directed) path in GM,x from Cstart to Caccept. We will use the following
simple claim about conﬁguration graphs:
Claim 4.4
Let GM,x be the conﬁguration graph of a space-S(n) machine M on some input x of length n.
Then,
1. Every vertex in GM,x can be described using cS(n) bits for some constant c (depending on
M’s alphabet size and number of tapes) and in particular, GM,x has at most 2cS(n) nodes.
2. There is an O(S(n))-size CNF formula ϕM,x such that for every two strings C, C′, ϕM,x(C, C′) =
1 if and only if C, C′ encode two neighboring conﬁguration in GM,x.
Proof sketch:
Part 1 follows from observing that a conﬁguration is completely described by
giving the contents of all work tapes, the position of the head, and the state that the TM is in
(see Section 1.2.1). We can encode a conﬁguration by ﬁrst encoding the snapshot (i.e., state and
current symbol read by all tapes) and then encoding in sequence the non-blank contents of all the
work-tape, inserting a special “marker” symbol, to denote the locations of the heads.
Web draft 2007-01-08 21:59

DRAFT
p4.4 (78)
4.2. SOME SPACE COMPLEXITY CLASSES.
Part 2 follows using similar ideas as in the proof of the Cook-Levin theorem (Theorem 2.10).
There we showed that deciding whether two conﬁgurations are neighboring can be expressed as the
AND of many checks, each depending on only a constant number of bits, where such checks can be
expressed by constant-sized CNF formulae by Claim 2.14. ■
Now we can prove Theorem 4.3.
Proof of Theorem 4.3:
Clearly SPACE(S(n)) ⊆NSPACE(S(n)) and so we just need to
show NSPACE(S(n)) ⊆DTIME(2O(S(n))). By enumerating over all possible conﬁgurations we
can construct the graph GM,x in 2O(S(n))-time and check whether Cstart is connected to Caccept
in GM,x using the standard (linear in the size of the graph) breadth-ﬁrst search algorithm for
connectivity (e.g., see [?]). ■
We also note that there exists a universal TM for space bounded computation analogously
to Theorems 1.13 and ?? for deterministic and non-deterministic time bounded computation, see
Section ?? below.
4.2
Some space complexity classes.
Now we deﬁne some complexity classes, where PSPACE, NPSPACE are analogs of P and NP
respectively.
Definition 4.5
PSPACE = ∪c>0SPACE(nc)
NPSPACE = ∪c>0NSPACE(nc)
L = SPACE(log n)
NL = NSPACE(log n)
Example 4.6
We show how 3SAT ∈PSPACE by describing a TM that decides 3SAT in linear space (that is,
O(n) space, where n is the size of the 3SAT instance). The machine just uses the linear space to
cycle through all 2k assignments in order, where k is the number of variables. Note that once an
assignment has been checked it can be erased from the worktape, and the worktape then reused
to check the next assignment. A similar idea of cycling through all potential certiﬁcates applies to
any NP language, so in fact NP ⊆PSPACE.
Example 4.7
The reader should check (using the gradeschool method for arithmetic) that the following languages
are in L:
EVEN = {x : x has an even number of 1s} .
MULT = {( ⌞n⌟, ⌞m⌟, ⌞nm⌟) : n ∈N} .
Web draft 2007-01-08 21:59

DRAFT
4.3. PSPACE COMPLETENESS
p4.5 (79)
Its seems diﬃcult to conceive of any complicated computations apart from arithmetic that use
only O(log n) space. Nevertheless, we cannot currently even rule out that 3SAT ∈L (in other
words —see the exercises— it is open whether NP ̸= L). Space-bounded computations with space
S(n) ≪n seem relevant to computational problems such as web crawling. The world-wide-web
may be viewed crudely as a directed graph, whose nodes are webpages and edges are hyperlinks.
Webcrawlers seek to explore this graph for all kinds of information. The following problem PATH
is natural in this context:
PATH = {⟨G, s, t⟩: G is a directed graph in which there is a path from s to t}
(1)
We claim that PATH ∈NL. The reason is that a nondeterministic machine can take a “non-
deterministic walk” starting at s, always maintaining the index of the vertex it is at, and using
nondeterminism to select a neighbor of this vertex to go to next. The machine accepts iﬀthe walk
ends at t in at most n steps, where n is the number of nodes. If the nondeterministic walk has
run for n steps already and t has not been encountered, the machine rejects. The work tape only
needs to hold O(log n) bits of information at any step, namely, the number of steps that the walk
has run for, and the identity of the current vertex.
Is PATH in L as well? This is an open problem, which, as we will shortly see, is equivalent to
whether or not L = NL. That is, PATH captures the “essence” of NL just as 3SAT captures the
“essence” of NP. (Formally, we will show that PATH is NL-complete. ) A recent surprising result
shows that the restriction of PATH to undirected graphs is in L; see Chapters 7 and 16.
4.3
PSPACE completeness
As already indicated, we do not know if P ̸= PSPACE, though we strongly believe that the answer
is YES. Notice, P = PSPACE implies P = NP. Since complete problems can help capture the
essence of a complexity class, we now present some complete problems for PSPACE.
Definition 4.8
A language A is PSPACE-hard if for every L ∈PSPACE, L ≤p A. If in addition A ∈PSPACE
then A is PSPACE-complete.
Using our observations about polynomial-time reductions from Chapter ?? we see that if any
PSPACE-complete language is in P then so is every other language in PSPACE. Viewed con-
trapostively, if PSPACE ̸= P then a PSPACE-complete language is not in P. Intuitively, a
PSPACE-complete language is the “most diﬃcult” problem of PSPACE. Just as NP trivially
contains NP-complete problems, so does PSPACE. The following is one (Exercise 3):
SPACETM = {⟨M, w, 1n⟩: DTM M accepts w in space n} .
(2)
Now we see some more interesting PSPACE-complete problems.
We use the notion of a
quantiﬁed boolean formula, which is a boolean formula in which variables are quantiﬁed using ∃
Web draft 2007-01-08 21:59

DRAFT
p4.6 (80)
4.3. PSPACE COMPLETENESS
and ∀which have the usual meaning “there exists” and “for all” respectively. It is customary
to also specify the universe over which these signs should be interpreted, but in our case the
universe will always be the truth values {0, 1}. Thus a quantiﬁed boolean formula has the form
Q1x1Q2x2 · · · Qnxnϕ(x1, x2, . . . , xn) where each Qi is one of the two quantiﬁers ∀or ∃and ϕ is an
(unquantiﬁed) boolean formula1.
If all variables in the formula are quantiﬁed (in other words, there are no free variables) then a
moment’s thought shows that such a formula is either true or false —there is no “middle ground”.
We illustrate the notion of truth by an example.
Example 4.9
Consider the formula ∀x∃y (x ∧y) ∨(x ∧y) where ∀and ∃quantify over the universe {0, 1}. Some
reﬂection shows that this is saying “for every x ∈{0, 1} there is a y ∈{0, 1} that is diﬀerent from
it”, which we can also informally represent as ∀x∃y(x ̸= y). This formula is true. (Note: the
symbols = and ̸= are not logical symbols per se, but are used as informal shorthand to make the
formula more readable.)
However, switching the second quantiﬁer to ∀gives ∀x∀y (x ∧y) ∨(x ∧y), which is false.
Example 4.10
Recall that the SAT problem is to decide, given a Boolean formula ϕ that has n free variables
x1, . . . , xn, whether or not ϕ has a satisfying assignment x1, . . . , xn ∈{0, 1}n such that ϕ(x1, . . . , xn)
is true. An equivalent way to phrase this problem is to ask whether the quantiﬁed Boolean formula
ψ = ∃x1, . . . , xnϕ(x1, . . . , xn) is true.
The reader should also verify that the negation of the formula
Q1x1Q2x2 · · · Qnxnϕ(x1, x2, . . . , xn) is the same as
Q′
1x1Q′
2x2 · · · Q′
nxn¬ϕ(x1, x2, . . . , xn),
where Q′
i is ∃if Qi was ∀and vice versa. The switch of ∃to ∀in case of SAT gives instances of
TAUTOLOGY, the coNP-complete language encountered in Chapter ??.
We deﬁne the language TQBF to be the set of quantiﬁed boolean formulae that are true.
1 We are restricting attention to quantiﬁed boolean formulae which are in prenex normal form, i.e., all quantiﬁers
appear to the left.
However, this is without loss of generality since we can transform a general formula into an
equivalent formula in prenex form in polynomial time using identities such as p∨∃xϕ(x) = ∃xp∨ϕ(x) and ¬∀xφ(x) =
∃x¬φ(x).
Also note that unlike in the case of the SAT and 3SAT problems, we do not require that the inner
unquantiﬁed formula ϕ is in CNF or 3CNF form. However this choice is also not important, since using auxiliary
variables in a similar way to the proof of the Cook-Levin theorem, we can in polynomial-time transform a general
quantiﬁed Boolean formula to an equivalent formula where the inner unquantiﬁed formula is in 3CNF form.
Web draft 2007-01-08 21:59

DRAFT
4.3. PSPACE COMPLETENESS
p4.7 (81)
Theorem 4.11
TQBF is PSPACE-complete.
Proof: First we show that TQBF ∈PSPACE. Let
ψ = Q1x1Q2x2 . . . Qnxnϕ(x1, x2, . . . , xn)
(3)
be a quantiﬁed Boolean formula with n variables, where we denote the size of ϕ by m. We show a
simple recursive algorithm A that can decide the truth of ψ in O(n + m) space. We will solve the
slightly more general case where, in addition to variables and their negations, ϕ may also include
the constants 0 (i.e., “false”) and 1 (i.e., “true”). If n = 0 (there are no variables) then the formula
contains only constants and can be evaluated in O(m) time and space. Let n > 0 and let ψ be
as in (3). For b ∈{0, 1}, denote by ψ↾x1=b the modiﬁcation of ψ where the ﬁrst quantiﬁer Q1
is dropped and all occurrences of x1 are replaced with the constant b. Algorithm A will work as
follows: if Q1 = ∃then output 1 iﬀat least one of A(ψ↾x1=0) and A(ψ↾x1=1) returns 1. If Q1 = ∀
then output 1 iﬀboth A(ψ↾x1=0) and A(ψ↾x1=1). By the deﬁnition of ∃and ∀, it is clear that A
does indeed return the correct answer on any formula ψ.
Let sn,m denote the space A uses on formulas with n variables and description size m. The crucial
point is —and here we use the fact that space can be reused—that both recursive computations
A(ψ↾x1=0) and A(ψ↾x1=1) can run in the same space. Speciﬁcally, after computing A(ψ↾x1=0), the
algorithm A needs to retain only the single bit of output from that computation, and can reuse
the rest of the space for the computation of A(ψ↾x1=1). Thus, assuming that A uses O(m) space to
write ψ↾x1 = b for its recursive calls, we’ll get that sn,m = sn−1,m +O(m) yielding sn,m = O(n·m).
2
We now show that L ≤p TQBF for every L ∈PSPACE. Let M be a machine that decides
L in S(n) space and let x ∈{0, 1}n. We show how to construct a quantiﬁed Boolean formula
ψ of size O(S(n)2) that is true iﬀM accepts x. Recall that by Claim 4.4, there is a Boolean
formula ϕM,x such that for every two strings C, C′ ∈{0, 1}m (where m = O(S(n)) is the number
of bits require to encode a conﬁguration of M), ϕM(C, C′) = 1 iﬀC and C′ are valid encodings of
two adjacent conﬁgurations in the conﬁguration graph GM,x. We will use ϕM,x to come up with
a polynomial-sized quantiﬁed Boolean formula ψ′ that has polynomially many Boolean variables
bound by quantiﬁers and additional 2m unquantiﬁed Boolean variables C1, . . . , Cm, C′
1, . . . , C′
m (or,
equivalently, two variables C, C′ over {0, 1}m) such that for every C, C′ ∈{0, 1}m, ψ(C, C′) is true
iﬀC has a directed path to C′ in GM,x. By plugging in the values Cstart and Caccept to ψ′ we get a
quantiﬁed Boolean formula ψ that is true iﬀM accepts x.
We deﬁne the formula ψ′ inductively. We let ψi(C, C′) be true if and only if there is a path of
length at most 2i from C to C′ in GM,x. Note that ψ′ = ψm and ψ0 = ϕM,x. The crucial observation
is that there is a path of length at most 2i from C to C′ if and only if there is a conﬁguration C′′
2The above analysis already suﬃces to show that TQBF is in PSPACE. However, we can actually show that the
algorithm runs in linear space, speciﬁcally, O(m+n) space. Note that algorithm always works with restrictions of the
same formula ψ. So it can keep a global partial assignment array that for each variable xi will contain either 0, 1 or
’q’ (if it’s quantiﬁed and not assigned any value). Algorithm A will use this global space for its operation, where in
each call it will ﬁnd the ﬁrst quantiﬁed variable, set it to 0 and make the recursive call, then set it to 1 and make the
recursive call, and then set it back to ’q’. We see that A’s space usage is given by the equation sn,m = sn−1,m +O(1)
and hence it uses O(n + m) space.
Web draft 2007-01-08 21:59

DRAFT
p4.8 (82)
4.3. PSPACE COMPLETENESS
with such that there are paths of length at most 2i−1 path from C to C′′ and from C′′ to C′. Thus
the following formula suggests itself: ψi(C, C′) = ∃C′′ ψi−1(C, C′) ∧ψi−1(C′′, C).
However, this formula is no good. It implies that ψi’s is twice the size of ψi−1, and a simple
induction shows that ψm has size about 2m, which is too large. Instead, we use additional quantiﬁed
variables to save on description size, using the following more succinct deﬁnition for ψi(C, C′):
∃C′′∀D1∀D2 (D1 = C ∧D2 = C′) ∨(D1 = C′ ∧D2 = C′′)

⇒ψi−1(D1, D2)
(Here, as in Example 4.9, = and ⇒are convenient shorthands, and can be replaced by appropriate
combinations of the standard Boolean operations ∧and ¬.) Note that size(ψi) ≤size(ψi−1)+O(m)
and hence size(ψm) ≤O(m2). We leave it to the reader to verify that the two deﬁnitions of ψi
are indeed logically equivalent. As noted above we can convert the ﬁnal formula to prenex form in
polynomial time. ■
4.3.1
Savitch’s theorem.
The astute reader may notice that because the above proof uses the notion of a conﬁguration graph
and does not require this graph to have out-degree one, it actually yields a stronger statement: that
TQBF is not just hard for PSPACE but in fact even for NPSPACE!. Since TQBF ∈PSPACE
this implies that PSPACE = NSPACE, which is quite surprising since our intuition is that the
corresponding classes for time (P and NP) are diﬀerent. In fact, using the ideas of the above proof,
one can obtain the following theorem:
Theorem 4.12 (Savitch [Sav70])
For any space-constructible S : N →N with S(n) ≥logn, NSPACE(S(n)) ⊆SPACE(S(n)2)
We remark that the running time of the algorithm obtained from this theorem can be as high
as 2Ω(s(n)2).
Proof: The proof closely follows the proof that TQBF is PSPACE-complete. Let L ∈NSPACE(S(n))
be a language decided by a TM M such that for every x ∈{0, 1}n, the conﬁguration graph
G = GM,x has at most M = 2O(S(n)) vertices. We describe a recursive procedure reach?(u, v, i)
that returns “YES” if there is a path from u to v of length at most 2i and “NO” otherwise. Note
that reach?(s, t, ⌈log M ⌉) is “YES” iﬀt is reachable from s.
Again, the main observation is
that there is a path from u to v of length at most 2i iﬀthere’s a vertex z with paths from u
to z and from z to v of lengths at most 2i−1.
Thus, on input u, v, i, reach?
will enumerate
over all vertices z (at a cost of O(log M) space) and output “YES” if it ﬁnds one z such that
reach?(u, z, i −1)=“YES” and reach?(z, v, i −1)=“YES”. Once again, the crucial observation is
that although the algorithm makes n recursive invocations, it can reuse the space in each of these
invocations. Thus, if we let sM,i be the space complexity of reach?(u, v, i) on an M-vertex graph
we get that sM,i = sM,i−1 + O(log M) and thus sM,log M = O(log2 M) = O(S(n)2). ■
4.3.2
The essence of PSPACE: optimum strategies for game-playing.
Recall that the central feature of NP-complete problems is that a yes answer has a short certiﬁcate.
The analogous unifying concept for PSPACE-complete problems seems to be that of a winning
Web draft 2007-01-08 21:59

DRAFT
4.3. PSPACE COMPLETENESS
p4.9 (83)
strategy for a 2-player game with perfect information. A good example of such a game is Chess: two
players alternately make moves, and the moves are made on a board visible to both. Thus moves
have no hidden side eﬀects; hence the term “perfect information.” What does it mean for a player
to have a “winning strategy?” The ﬁrst player has a winning strategy iﬀthere is a 1st move for
player 1 such that for every possible 1st move of player 2 there is a 2nd move of player 1 such that....
(and so on) such that at the end player 1 wins. Thus deciding whether or not the ﬁrst player has
a winning strategy seems to require searching the tree of all possible moves. This is reminiscent of
NP, for which we also seem to require exponential search. But the crucial diﬀerence is the lack of
a short “certiﬁcate” for the statement “Player 1 has a winning strategy,” since the only certiﬁcate
we can think of is the winning strategy itself, which as noticed, requires exponentially many bits
to even describe. Thus we seem to be dealing with a fundamentally diﬀerent phenomenon than the
one captured by NP.
The interplay of existential and universal quantiﬁers in the description of the the winning
strategy motivates us to invent the following game.
Example 4.13 (The QBF game)
The “board” for the QBF game is a Boolean formula ϕ whose free variables are x1, x2, . . . , x2n.
The two players alternately make moves, which involve picking values for x1, x2, . . . , in order. Thus
player 1 will pick values for the odd-numbered variables x1, x3, x5, . . . (in that order) and player 2
will pick values for the even-numbered variables x2, x4, x6, . . . ,. We say player 1 wins iﬀat the end
ϕ becomes true.
Clearly, player 1 has a winning strategy iﬀ
∃x1∀x2∃x3∀x4 · · · ∀x2nϕ(x1, x2, . . . , x2n),
namely, iﬀthis quantiﬁed boolean formula is true.
Thus deciding whether player 1 has a winning strategy for a given board in the QBF game is
PSPACE-complete.
At this point, the reader is probably thinking of familiar games such as Chess, Go, Checkers
etc. and wondering whether complexity theory may help diﬀerentiate between them—for example,
to justify the common intuition that Go is more diﬃcult than Chess. Unfortunately, formalizing
these issues in terms of asymptotic complexity is tricky because these are ﬁnite games, and as far as
the existence of a winning strategy is concerned, there are at most three choices: Player 1 has has a
winning strategy, Player 2 does, or neither does (they can play to a draw). However, one can study
generalizations of these games to an n × n board where n is arbitrarily large —this may involve
stretching the rules of the game since the deﬁnition of chess seems tailored to an 8 × 8 board—
and then complexity theory can indeed by applied.
For most common games, including chess,
determining which player has a winning strategy in the n × n version is PSPACE-complete (see
[?]or [?]). Note that if NP ̸= PSPACE then in general there is no short certiﬁcate for exhibiting
that either player in the TQBF game has a winning strategy, which is alluded to in Evens and
Tarjan’s quote at the start of the chapter.
Web draft 2007-01-08 21:59

DRAFT
p4.10 (84)
4.4. NL COMPLETENESS
Proving PSPACE-completeness of games may seem like a frivolous pursuit, but similar ideas
lead to PSPACE-completeness of some practical problems. Usually, these involve repeated moves
that are in turn counteracted by an adversary. For instance, many computational problems of
robotics are PSPACE-complete: the “player” is the robot and the “adversary” is the environment.
(Treating the environment as an adversary may appear unduly pessimistic; but unfortunately even
assuming a benign or “indiﬀerent” environment still leaves us with a PSPACE-complete problem;
see the Chapter notes.)
4.4
NL completeness
Now we consider problems that form the “essence” of non-deterministic logarithmic space com-
putation, in other words, problems that are complete for NL. What kind of reduction should we
use? We cannot use the polynomial-time reduction since NL ⊆P. Thus every language in NL is
polynomial-time reducible to the trivial language {1} (reduction: “decide using polynomial time
whether or not the input is in the NL language, and then map to 1 or 0 accordingly”). Intuitively,
such trivial languages should not be the “hardest” languages of NL.
When choosing the type of reduction to deﬁne completeness for a complexity class, we must keep
in mind the complexity phenomenon we seek to understand. In this case, the complexity question is
whether or not NL = L. The reduction should not be more powerful than the weaker class, which is
L. For this reason we use logspace reductions —for further, justiﬁcation, see part (b) of Lemma 4.15
below). To deﬁne such reductions we must tackle the tricky issue that a reduction typically maps
instances of size n to instances of size at least n, and so a logspace machine computing such a
reduction does not have even the memory to write down its output. The way out is to require that
the reduction should be able to compute any desired bit of the output in logarithmic space. In
other words, if the reduction were given a separate output tape, it could in principle write out the
entire new instance by ﬁrst computing the ﬁrst bit, then the second bit, and so on. (Many texts
deﬁne such reductions using a “write-once” output tape.) The formal deﬁnition is as follows.
Definition 4.14 (logspace reduction)
Let f : {0, 1}∗→{0, 1}∗be a polynomially-bounded function (i.e., there’s a constant c > 0 such
that f(x) ≤|x|c for every x ∈{0, 1}∗). We say that f is implicitly logspace computable, if the
languages Lf = {⟨x, i⟩| f(x)i = 1} and L′
f = {⟨x, i⟩| i ≤|f(x)|} are in L.
Informally, we can think of a single O(log |x|)-space machine that given input (x, i) outputs
f(x)|i provided i ≤|f(x)|.
Language A is logspace reducible to language B, denoted A ≤l B, if there is a function f :
{0, 1}∗→{0, 1}∗that is implicitly logspace computable and x ∈A iﬀf(x) ∈B for every x ∈{0, 1}∗.
Logspace reducibility satisﬁes usual properties one expects.
Lemma 4.15
(a) If A ≤l B and B ≤l C then A ≤l C. (b) If A ≤l B and B ∈L then A ∈L.
Proof: We prove that if f, g are two functions that are logspace implicitly computable, then so
is the function h where h(x) = g(f(x)). Then part (a) of the Lemma follows by letting f be the
Web draft 2007-01-08 21:59

DRAFT
4.4. NL COMPLETENESS
p4.11 (85)
reduction from A to B and g be the reduction from B to C. Part (b) follows by letting f be the
reduction from A to B and g be the characteristic function of B (i.e. g(y) = 1 iﬀy ∈B).
So let Mf, Mg be the logspace machines that compute the mappings x, i 7→f(x)i and y, j 7→
g(y)j respectively. We construct a machine Mh that computes the mapping x, j 7→g(f(x))j, in
other words, given input x, j outputs g(f(x))j provided j ≤|g(f(x))|. Machine Mh will pretend
that it has an additional (ﬁctitious) input tape on which f(x) is written, and it is merely simulating
Mg on this input (see Figure 4.3). Of course, the true input tape has x, j written on it. To maintain
its ﬁction, Mh always maintains on its worktape the index, say i, of the cell on the ﬁctitious tape
that Mg is currently reading; this requires only log |f(x)| space. To compute for one step, Mg needs
to know the contents of this cell, in other words, f(x)|i. At this point Mh temporarily suspends its
simulation of Mg (copying the contents of Mg’s worktape to a safe place on its own worktape) and
invokes Mf on inputs x, i to get f(x)|i. Then it resumes its simulation of Mg using this bit. The
total space Mh uses is O(log |g(f(x))| + s(|x|) + s′(|f(x)|)) = O(log |x|). ■
Input
tape
Work
tape
Output
tape
>
0
0 0
1
1 0
1 0
0
0 1 0
0 0
0
read only head
read/write head
Mf
Work
tape
Output
tape
Virtual
input
tape
Mg
Figure 4.3: Composition of two implicitly logspace computable functions f, g. The machine Mg uses calls to f to
implement a “virtual input tape”. The overall space used is the space of Mf + the space of Mg + O(log |f(x)|) =
O(log|x|).
We say that A is NL-complete if it is in NL and for every B ∈NL, A ≤l B. Note that an
NL-complete language is in L iﬀNL =L.
Theorem 4.16
PATH is NL-complete.
Proof: We have already seen that PATH is in NL. Let L be any language in NL and M be a
machine that decides it in space O(log n). We describe a logspace implicitly computable function
f that reduces L to PATH. For any input x of size n, f(x) will be the conﬁguration graph GM,x
whose nodes are all possible 2O(log n) conﬁgurations of the machine on input x, along with the start
conﬁguration Cstart and the accepting conﬁguration Cacc. In this graph there is a path from Cstart
to Cacc iﬀM accepts x. The graph is represented as usual by an adjacency matrix that contain
1 in the ⟨C, C′⟩th position (i.e., in the Cth row and C′th column if we identify the conﬁgurations
with numbers between 0 and 2O(log n)) iﬀthere’s an edge C from C′ in GM,x. To ﬁnish the proof
we need to show that this adjacency matrix can be computed by a logspace reduction. This is easy
Web draft 2007-01-08 21:59

DRAFT
p4.12 (86)
4.4. NL COMPLETENESS
since given a ⟨C, C′⟩a deterministic machine can in space O(|C| + |C′|) = O(log |x|) examine C, C′
and check whether C′ is one of the (at most two) conﬁgurations that can follow C according to the
transition function of M. ■
4.4.1
Certiﬁcate deﬁnition of NL: read-once certiﬁcates
In Chapter 2 we gave two equivalent deﬁnitions of NP— one using non-deterministic TM’s and
another using the notion of a certiﬁcate. The idea was that the nondeterministic choices of the
NDTM that lead it to accept can be viewed as a “certiﬁcate” that the input is in the language,
and vice versa. We can give a certiﬁcate-based deﬁnition also for NL, but only after addressing
one tricky issue: a certiﬁcate may be polynomially long, and a logspace machine does not have the
space to store it. Thus, the certiﬁcate-based deﬁnition of NL assumes that the logspace machine
on a separate read-only tape. Furthermore, on each step of the machine the machine’s head on
that tape can either stay in place or move to the right. In particular, it cannot reread any bit to
the left of where the head currently is. (For this reason the this kind of tape is called “read once”.)
It is easily seen that the following is an alternative deﬁnition of NL (see also Figure 4.4):
Input
tape
Work
tape
Output
tape
Register
read only head
read/write head
read/write head
Certificate
tape
read once head
Figure 4.4: Certiﬁcate view of NL. The certiﬁcate for input x is placed on a special “read-once” tape on which
the machine’s head can never move to the left.
Definition 4.17 (NL- alternative definition.)
A language L is in NL if there exists a deterministic TM M and a with an additional special
read-once input tape polynomial p : N →N such that for every x ∈{0, 1}∗,
x ∈L ⇔∃u ∈{0, 1}p(|x|) s.t. M(x, u) = 1
where by M(x, u) we denote the output of M where x is placed on its input tape and u is placed
on its special read-once tape, and M uses at most O(log |x|) space on its read/write tapes for every
input x.
Web draft 2007-01-08 21:59

DRAFT
4.4. NL COMPLETENESS
p4.13 (87)
4.4.2
NL = coNL
Consider the problem PATH, i.e., the complement of PATH. A decision procedure for this language
must accept when there is no path from s to t in the graph. Unlike in the case of PATH, there
is no natural certiﬁcate for the non-existence of a path from s to t and thus it seemed “obvious”
to researchers that PATH ̸∈NL, until the discovery of the following theorem in the 1980s proved
them wrong.
Theorem 4.18 (Immerman-Szlepcsenyi)
PATH ∈NL.
Proof: As we saw in Section 4.4.1, we need to show an O(log n)-space algorithm A such that
for every n-vertex graph G and vertices s and t, there exists a polynomial certiﬁcate u such that
A(⟨G, s, t⟩, u) = 1 if and only if t is not reachable from u in G, where A has only read-once access
to u.
What can we certify to an O(log n)-space algorithm? Let Ci be the set of vertices that are
reachable from s in G within at most i steps. For every i ∈[n] and vertex v, we can easily certify
that v is in Ci. The certiﬁcate simply contains the labels v0, v1, . . . , vk of the vertices along the
path from s to v (we can assume without loss of generality that vertices are labeled by the numbers
1 to n and hence the labels can be described by log n bit strings). The algorithm can check the
certiﬁcate using read-once access by verifying that (1) v0 = s, (2) for j > 0, there is an edge from
vj−1 to vj, (3) vk = v and (using a counter) that (4) the path ends within at most i steps. Note
that the certiﬁcate is indeed of size at most polynomial in n.
Our algorithm uses the following two procedures:
1. Procedure to certify that a vertex v is not in Ci given the size of Ci.
2. Procedure to certify that |Ci| = c for some number c, given the size of Ci−1.
Since C0 = {s} and Cn contains all the vertices reachable from s, we can apply the second
procedure iteratively to learn the sizes of the sets C1, . . . , Cn, and then use the ﬁrst procedure to
certify that t ̸∈Cn.
Certifying that v is not in Ci, given |Ci|.
The certiﬁcate is simply the list of certiﬁcates
that u is in Ci for every u ∈Ci sorted in ascending order of labels (recall that we identify labels
with numbers in [n]). The algorithm checks that (1) each certiﬁcate is valid, (2) the label of a
vertex u for which a certiﬁcate is given is indeed larger than the label of the previous vertex, (3)
no certiﬁcate is provided for v, and (4) the total number of certiﬁcates provided is exactly |Ci|. If
v ̸∈Ci then the algorithm will accept the above certiﬁcate, but if v ∈Ci there will not exist |Ci|
certiﬁcates that vertices u1 < u2 < . . . < u|Ci| are in Ci where uj ̸= v for every j.
Certifying that v is not in Ci, given |Ci−1|.
Before showing how we certify that |Ci| = c
given |Ci−1|, we show how to certify that v ̸∈Ci with this information. This is very similar to the
above procedure: the certiﬁcate is the list of |Ci−1| certiﬁcates that u ∈Ci−1 for every u ∈Ci−1 in
ascending order. The algorithm checks everything as before except that in step (3) it veriﬁes that
no certiﬁcate is given for v or for a neighbor of v. Since v ∈Ci if and only if there exists u ∈Ci−1
such that u = v or u is a neighbor of v in G, the procedure will not accept a false certiﬁcate by the
same reasons as above.
Web draft 2007-01-08 21:59

DRAFT
p4.14 (88)
4.4. NL COMPLETENESS
Certifying that |Ci| = c given |Ci−1|.
For every vertex v, if v ∈Ci then there is a certiﬁcate for
this fact, and by the above procedure, given |Ci−1|, if v ̸∈Ci then there is a certiﬁcate for this fact
as well. The certiﬁcate that |Ci| = c will consist of n certiﬁcates for each of the vertices 1 to n in
ascending order. For every vertex u, there will be an appropriate certiﬁcate depending on whether
u ∈Ci or not. The algorithm will verify all the certiﬁcate and count the number of certiﬁcate that
a vertex is in Ci. It accepts if this count is equal to c. ■
Using the notion of the conﬁguration graph we can modify the proof of Theorem 4.18 to prove
the following:
Corollary 4.19
For every space constructible S(n) > log n, NSPACE(S(n)) = coNSPACE(S(n)).
Our understanding of space-bounded complexity.
The following is our understanding of space-bounded complexity.
DTIME(s(n))⊆SPACE(s(n))⊆NSPACE(s(n))=coNSPACE(s(n))⊆DTIME(2O(s(n))).
None of the inclusions are known to be strict though we believe they all are.
Chapter notes and history
The concept of space complexity had already been explored in the 1960s; in particular, Savitch’s the-
orem predates the Cook-Levin theorem. Stockmeyer and Meyer proved the PSPACE-completeness
of TQBF soon after Cook’s paper appeared. A few years later Even and Tarjan pointed out the
connection to game-playing and proved the PSPACE-completeness of a game called General-
ized Hex. Papadimitriou’s book gives a detailed account of PSPACE-completeness. He also shows
PSPACE-completeness of several Games against nature ﬁrst deﬁned in [Pap85]. Unlike the TQBF
game, where one player is Existential and the other Universal, here the second player chooses moves
randomly. The intention is to model games played against nature—where “nature” could mean not
just weather for example, but also large systems such as the stock market that are presumably “in-
diﬀerent” to the fate of individuals. Papadimitriou gives an alternative characterization PSPACE
using such games. A stronger result, namely, a characterization of PSPACE using interactive
proofs, is described in Chapter 8.
Exercises
§1 Show that SPACE(S(n)) = SPACE(0) when S(n) = log log n.
§2 Prove the existence of a universal TM for space bounded computation (analogously to the
deterministic universal TM of Theorem 1.13). That is, prove that there exists a a TM SU
such that for every string α, and input x, if the TM Mα represented by α halts on x before
using t cells of its work tapes then SU(α, t, x) = Mα(x), and moreover, SU uses at most
Ct cells of its work tapes, where C is a constant depending only on Mα. (Despite the fact
Web draft 2007-01-08 21:59

DRAFT
4.4. NL COMPLETENESS
p4.15 (89)
that the bound here is better than the bound of Theorem 1.13, the proof of this statement is
actually easier than the proof of Theorem 1.13.)
§3 Prove that the language SPACETM of (2) is PSPACE-complete.
§4 Show that the following language is NL-complete:
{ ⌞G⌟: G is a strongly connected digraph} .
§5 Show that 2SAT is in NL.
§6 Suppose we deﬁne NP-completeness using logspace reductions instead of polynomial-time
reductions. Show (using the proof of the Cook-Levin Theorem) that SAT and 3SAT continue
to be NP-complete under this new deﬁnition. Conclude that SAT ∈L iﬀNP = L.
§7 Show that TQBF is complete for PSPACE also under logspace reductions.
§8 Show that in every ﬁnite 2-person game with perfect information (by ﬁnite we mean that
there is an a priori upperbound n on the number of moves after which the game is over and
one of the two players is declared the victor —there are no draws) one of the two players has
a winning strategy.
§9 Deﬁne polyL to be ∪c>0SPACE(logc n). Steve’s Class SC (named in honor of Steve Cook)
is deﬁned to be the set of languages that can be decided by deterministic machines that run
in polynomial time and logc n space for some c > 0.
It is an open problem whether PATH ∈SC. Why does Savitch’s Theorem not resolve this
question?
Is SC the same as polyL ∩P?
Web draft 2007-01-08 21:59

DRAFT
p4.16 (90)
4.4. NL COMPLETENESS
Web draft 2007-01-08 21:59

DRAFT
Chapter 5
The Polynomial Hierarchy and
Alternations
“..synthesizing circuits is exceedingly diﬃculty. It is even more diﬃcult to show that
a circuit found in this way is the most economical one to realize a function. The
diﬃculty springs from the large number of essentially diﬀerent networks available.”
Claude Shannon 1949
This chapter discusses the polynomial hierarchy, a generalization of P, NP and coNP that
tends to crop up in many complexity theoretic investigations (including several chapters of this
book). We will provide three equivalent deﬁnitions for the polynomial hierarchy, using quantiﬁed
predicates, alternating Turing machines, and oracle TMs (a fourth deﬁnition, using uniform families
of circuits, will be given in Chapter 6). We also use the hierarchy to show that solving the SAT
problem requires either linear space or super-linear time.
5.1
The classes Σp
2 and Πp
2
To understand the need for going beyond nondeterminism, let’s recall an NP problem, INDSET,
for which we do have a short certiﬁcate of membership:
INDSET = {⟨G, k⟩: graph G has an independent set of size ≥k} .
Consider a slight modiﬁcation to the above problem, namely, determining the largest indepen-
dent set in a graph (phrased as a decision problem):
EXACT INDSET = {⟨G, k⟩: the largest independent set in G has size exactly k} .
Now there seems to be no short certiﬁcate for membership: ⟨G, k⟩∈EXACT INDSET iﬀthere
exists an independent set of size k in G and every other independent set has size at most k.
Similarly, consider the language MIN-DNF, the decision version of a problem in circuit mini-
mization, a topic of interest in electrical engineering (and referred to in Shannon’s paper). We say
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p5.1 (91)

DRAFT
p5.2 (92)
5.1. THE CLASSES ΣP
2 AND ΠP
2
that two boolean formulae are equivalent if they have the same set of satisfying assignments.
MIN −DNF = { ⌞ϕ⌟: ϕ is a DNF formula not equivalent to any smaller DNF formula} .
= { ⌞ϕ⌟: ∀ψ, |ψ| < |ϕ| , ∃assignment s such that ϕ(s) ̸= ψ(s)} .
Again, there is no obvious notion of a certiﬁcate of membership. Note that both the above
problems are in PSPACE, but neither is believed to be PSPACE-complete.
It seems that the way to capture such languages is to allow not only an “exists“ quantiﬁer (as in
Deﬁnition 2.1 of NP) or only a “for all” quantiﬁer (as Deﬁnition 2.22 of coNP) but a combination
of both quantiﬁers. This motivates the following deﬁnition:
Definition 5.1
The class Σp
2 is deﬁned to be the set of all languages L for which there exists a polynomial-time
TM M and a polynomial q such that
x ∈L ⇔∃u ∈{0, 1}q(|x|) ∀v ∈{0, 1}q(|x|) M(x, u, v) = 1
for every x ∈{0, 1}∗.
Note that Σp
2 contains both the classes NP and coNP.
Example 5.2
The language EXACT INDSET above is in Σp
2, since as we noted above, a pair ⟨G, k⟩is in EXACT INDSET
iﬀ
∃S ∀S′ set S is an independent set of size k in G and
S′ is not a independent set of size ≥k + 1.
We deﬁne the class Πp
2 to be the set

L : L ∈sigp
2
	
. It is easy to see that an equivalent deﬁnition
is that L ∈Πp
2 if there is a polynomial-time TM M and a polynomial q such that
x ∈L ⇔∀u ∈{0, 1}q(|x|) ∃v ∈{0, 1}q(|x|) M(x, u, v) = 1
for every x ∈{0, 1}∗.
Example 5.3
The language EXACT INDSET is also in Πp
2 since a pair ⟨G, k⟩is in EXACT INDSET if for every S′,
if S′ has size at least k + 1 then it is not an independent set, but there exists an independent set
S of size k in G. (Exercise 8 shows a ﬁner placement of EXACT INDSET.)
The reader can similarly check that MIN −DNF is in Πp
2. It is conjectured to be complete for
Πp
2.
Web draft 2007-01-08 21:59

DRAFT
5.2. THE POLYNOMIAL HIERARCHY.
p5.3 (93)
5.2
The polynomial hierarchy.
The polynomial hierarchy generalizes the deﬁnitions of NP, coNP, Σp
2, Πp
2 to consists all the lan-
guages that can be deﬁned via a combination of a polynomial-time computable predicate and a
constant number of ∀/∃quantiﬁers:
Definition 5.4 (Polynomial Hierarchy)
For every i ≥1, a language L is in Σp
i if there exists a polynomial-time TM M and
a polynomial q such that
x ∈L ⇔∃u1 ∈{0, 1}q(|x|) ∀u2 ∈{0, 1}q(|x|) · · · Qiui ∈{0, 1}q(|x|) M(x, u1, . . . , ui) = 1,
where Qi denotes ∀or ∃depending on whether i is even or odd respectively.
We say that L is in Πp
i if there exists a polynomial-time TM M and a polynomial
q such that
x ∈L ⇔∀u1 ∈{0, 1}q(|x|) ∃u2 ∈{0, 1}q(|x|) · · · Qiui ∈{0, 1}q(|x|) M(x, u1, . . . , ui) = 1,
where Qi denotes ∃or ∀depending on whether i is even or odd respectively.
The polynomial hierarchy is the set PH = ∪iΣp
i .
Remark 5.5
Note that Σp
1 = NP and Πp
2 = coNP. More generally, for evert i ≥1, Πp
i = coΣp
i =

L : L ∈Σp
i
	
.
Note also that that Σp
i ⊆Πp
i+1, and so we can also deﬁne the polynomial hierarchy as ∪i>0Πp
i .
5.2.1
Properties of the polynomial hierarchy.
We believe that P ̸= NP and NP ̸= coNP. An appealing generalization of these conjectures is
that for every i, Σp
i is strictly contained in Σp
i+1. This is called the conjecture that the polynomial
hierarchy does not collapse, and is used often in complexity theory. If the polynomial hierarchy
does collapse this means that there is some i such that Σp
i = ∪jΣp
j = PH. In this case we say that
the polynomial hierarchy has collapsed to the ith level. The smaller i is, the weaker, and hence
more plausible, is the conjecture that PH does not collapse to the ith level.
Theorem 5.6
1. For every i ≥1, if Σp
i = Πp
i then PH = Σp
i (i.e., the hierarchy collapses to the ith level).
2. If P = NP then PH = P (i.e., the hierarchy collapses to P).
Proof: We do the second part; the ﬁrst part is similar and also easy.
Assuming P = NP we prove by induction on i that Σp
i , Πp
i ⊆P. Clearly this is true for i = 1
since under our assumption P = NP = coNP. We assume it is true for i−1 and prove it for i. Let
Web draft 2007-01-08 21:59

DRAFT
p5.4 (94)
5.2. THE POLYNOMIAL HIERARCHY.
L ∈Σp
i , we will show that L ∈P. By deﬁnition, there is a polynomial-time M and a polynomial q
such that
x ∈L ⇔∃u1 ∈{0, 1}q(|x|) ∀u2 ∈{0, 1}q(|x|) · · · Qiui ∈{0, 1}q(|x|) M(x, u1, . . . , ui) = 1,
where Qi is ∃/∀as in Deﬁnition 5.4. Deﬁne the language L′ as follows:
u ∈L′ ⇔∃∀u2 ∈{0, 1}q(|x|) · · · Qiui ∈{0, 1}q(|x|) M(u1, u2, . . . , ui) = 1.
Clearly, L′ ∈Πp
i−1 and so under our assumption is in P. This implies that there is a TM M′ such
that
x ∈L ⇔∃u1 ∈{0, 1}q(|x|) M′(x, u1) = 1 .
But this means L ∈NP and hence under our assumption L ∈P. The same idea shows that if
L ∈Πp
i then L ∈P. ■
5.2.2
Complete problems for levels of PH
For every i, we say that a language L is Σp
i -complete if L ∈Σp
i and for every L′ ∈Σp
i , L′ ≤p L.
We deﬁne Πp
i -completeness and PH-completeness in the same way. In this section we show that
for every i ∈N, both Σp
i and Πp
i have complete problems. In contrast the polynomial hierarchy
itself is believed not to have a complete problem, as is shown by the following simple claim:
Claim 5.7
Suppose that there exists a language L that is PH-complete, then there exists an i such that
PH = Σp
i (and hence the hierarchy collapses to its ith level.)
Proof sketch: Since L ∈PH = ∪iΣp
i , there exists i such that L ∈Σp
i . Since L is PH-complete,
we can reduce every language of PH to Σp
i to L, and thus PH ⊆Σp
i . ■
Remark 5.8
It is not hard to see that PH ⊆PSPACE. A simple corollary of Claim 5.7 is that unless the
polynomial hierarchy collapses, PH ̸= PSPACE. Indeed, otherwise the problem TQBF would be
PH-complete.
Example 5.9
The following are some examples for complete problems for individual levels of the hierarchy:
For every i ≥1, the class Σp
i has the following complete problem involving quantiﬁed boolean
expression with limited number of alternations:
ΣiSAT = ∃u1∀u2∃· · · Qiui ϕ(u1, u2, . . . , ui) = 1,
(1)
where ϕ is a Boolean formula (not necessarily in CNF form, although this does not make much
diﬀerence), each ui is a vector of boolean variables, and Qi is ∀or∃depending on whether i is odd
or even. Notice that this is a special case of the TQBF problem deﬁned in Chapter 4. Exercise 1
Web draft 2007-01-08 21:59

DRAFT
5.3. ALTERNATING TURING MACHINES
p5.5 (95)
asks you to prove that ΣiSAT is indeed Σp
i -complete. One can similarly deﬁne a problem ΠiSAT
that is Πp
i -complete.
In the SUCCINCT SET COVER problem we are given a collection S = {ϕ1, ϕ2, . . . , ϕm} of 3-
DNF formulae on n variables, and an integer k. We need to determine whether there is a subset
S′ ⊆{1, 2, . . . , m} of size at most K for which ∨i∈S′ϕi is a tautology (i.e., evaluates to 1 for every
assignment to the variables). Umans showed that SUCCINCT SET COVER is Σp
2-complete [Uma01].
5.3
Alternating Turing machines
Alternating Turing Machines (ATM), are generalizations of nondeterministic Turing machines.
Recall that even though NDTMs are not a realistic computational model, studying them helps us
to focus on a natural computational phenomenon, namely, the apparent diﬀerence between guessing
an answer and verifying it. ATMs plays a similar role for certain languages for which there is no
obvious short certiﬁcate for membership and hence cannot be characterized using nondeterminism
alone.
Alternating TMs are similar to NDTMs in the sense that they have two transition functions
between which they can choose in each step, but they also have the additional feature that every
internal state except qaccept and qhalt is labeled with either ∃or ∀. Similar to the NDTM, an ATM
can evolve at every step in two possible ways. Recall that a non-deterministic TM accepts its input
if there exists some sequence of choices that leads it to the state qaccept. In an ATM, the exists
quantiﬁer over each choice is replaced with the appropriate quantiﬁer according to the labels.
Definition 5.10
Let M be an alternating TM. For a function T : N →N, we say that M is an T(n)-time ATM if
for every input x ∈{0, 1}∗and for every possible sequence of transition function choices, M will
halt after at most T(|x|) steps.
For every x ∈{0, 1}∗, we let GM,x be the conﬁguration graph of x, whose vertices are the
conﬁgurations of M on input x and there is an edge from conﬁguration C to C′ if there C′ can
be obtained from C in one step using one of the two transition functions (see Section 4.1). Recall
that this is a directed acyclic graph. We label some of the nodes in the graph by “ACCEPT” by
repeatedly applying the following rules until they cannot be applied anymore:
• The conﬁguration Caccept where the machine is in qaccept is labeled “ACCEPT”.
• If a conﬁguration C is in a state labeled ∃and one of the conﬁgurations C′ reachable from it
in one step is labeled “ACCEPT” then we label C “ACCEPT”.
• If a conﬁguration C is in a state labeled ∀and both the conﬁgurations C′, C′′ reachable from
it one step is labeled “ACCEPT” then we label C “ACCEPT”.
We say that M accepts x if at the end of this process the starting conﬁguration Cstart is labeled
“ACCEPT”. The language accepted by M is the set of all x’s such that M accepts x. We denote
by ATIME(T(n)) the set of all languages accepted by some T(n)-time ATM.
Web draft 2007-01-08 21:59

DRAFT
p5.6 (96)
5.4. TIME VERSUS ALTERNATIONS: TIME-SPACE TRADEOFFS FOR SAT.
For every i ∈N, we deﬁne ΣiTIME(T(n)) (resp. ΠiTIME(T(n)) to be the set of languages
accepted by a T(n)-time ATM M whose initial state is labeled “∃” (resp. “∀”) and on which every
input and sequence of choices leads M to change at most i −1 times from states with one label to
states with the other label.
The following claim is left as an easy exercise (see Exercise 2):
Claim 5.11
For every i ∈N,
Σp
i = ∪cΣiTIME(nc)
Πp
i = ∪cΠiTIME(nc)
5.3.1
Unlimited number of alternations?
What if we consider polynomial-time alternating Turing machines with no a priori bound on the
number of quantiﬁers?
We deﬁne the class AP to be ∪cATIME(nc).
We have the following
theorem:
Theorem 5.12
AP = PSPACE.
Proof: PSPACE ⊆AP follows since TQBF is trivially in AP (just “guess” values for each
existentially quantiﬁed variable using an ∃state and for universally quantiﬁed variables using a ∀
state) and every PSPACE language reduces to TQBF.
AP ⊆PSPACE follows using a recursive procedure similar to the one used to show that
TQBF ∈PSPACE. ■
Similarly, one can consider alternating Turing machines that run in polynomial space. The class
of languages accepted by such machines is called APSPACE, and Exercise 6 asks you to prove
that APSPACE = EXP. One can similarly consider alternating logspace machines; the set of
languages accepted by them is exactly P.
5.4
Time versus alternations: time-space tradeoﬀs for SAT.
Despite the fact that SAT is widely believed to require exponential (or at least super-polynomial)
time to solve, and to require linear (or at least super-logarithmic) space, we currently have no way
to prove these conjectures. In fact, as far as we know, SAT may have both a linear time algorithm
and a logarithmic space one. Nevertheless, we can prove that SAT does not have an algorithm
that runs simultaneously in linear time and logarithmic space. In fact, we can prove the following
stronger theorem:
Theorem 5.13 (??)
For every two functions S, T : N →N, deﬁne TISP(T(n), S(n)) to be the set of languages decided
by a TM M that on every input x takes at most O(T(|x|)) steps and uses at most O(S(|x|)) cells
of its read/write tapes. Then, SAT ̸∈TISP(n1.1, n0.1).
Web draft 2007-01-08 21:59

DRAFT
5.4. TIME VERSUS ALTERNATIONS: TIME-SPACE TRADEOFFS FOR SAT.
p5.7 (97)
Remark 5.14
The class TISP(T(n), S(n)) is typically deﬁned with respect to TM’s with RAM memory (i.e.,
TM’s that have random access to their tapes; such machines can be deﬁned in a similar way to the
deﬁnition of oracle TM’s in Section 3.5). Theorem 5.13 and its proof carries over for that model
as well. We also note that a stronger result is known for both models: for every c < (
√
5 + 1)/2,
there exists d > 0 such that SAT ̸∈TISP(nc, nd) and furthermore, d approaches 1 from below as
c approaches 1 from above.
Proof: We will show that
NTIME(n) ⊈TISP(n1.2, n0.2) .
(2)
This implies the result for SAT by following the ideas of the proof of the Cook-Levin Theorem
(Theorem 2.10). A careful analysis of that proof yields a reduction from the task of deciding mem-
bership in an NTIME(T(n))-language to the task deciding whether an O(T(n) log T(n))-sized
formula is satisﬁable, such that every output bit of this reduction can be computed in polyloga-
rithmic time and space. (See also the proof of Theorem 6.7 for a similar analysis.)
Hence, if
SAT ∈TISP(n1.1, n0.1) then NTIME(n) ⊆TISP(n1.1polylog(n), n0.1polylog(n)). Our main step
in proving (2) is the following claim, showing how to replace time with alternations:
Claim 5.14.1
TISP(n12, n2) ⊆Σ2TIME(n8).
Proof: The proof is similar to the proofs of Savitch’s Theorem and the PSPACE-completeness
of TQBF (Theorems 4.12 and 4.11). Suppose that L is decided by a machine M using n12 time
and n2 space. For every x ∈{0, 1}∗, consider the conﬁguration graph GM,x of M on input x. Each
conﬁguration in this graph can be described by a string of length O(n2) and x is in L if and only
if there is a path of length n12 in this graph from the starting conﬁguration Cstart to an accepting
conﬁguration. There is such a path if and only if there exist n6 conﬁgurations C1, . . . , Cn6 (requiring
O(n8) to specify) such that if we let C0 = Cstart then Cn6 is accepting and for every i ∈[n6] the
conﬁguration Ci is computed from Ci−1 within n6 steps. Because this condition can be veriﬁed in
n6 time, we can we get an O(n8)-time σ2-TM for deciding membership in L. ■
Our next step will show that, under the assumption that (2) does not hold (and hence NTIME(n) ⊆
TISP(n1.2, n0.2)), we can replace alternations with time:
Claim 5.14.2
Suppose that NTIME(n) ⊆DTIME(n1.2). Then Σ2TIME(n8) ⊆NTIME(n9.6).
Proof: Using the characterization of the polynomial hierarchy by alternating machines, L is in
Σ2TIME(n8) if and only if there is an O(n8)-time TM M such that
x ∈L ⇔∃u ∈{0, 1}c|x|8 ∀v ∈{0, 1}d|x|8 M(x, u, v) = 1 .
for some constants c, d. Yet if NTIME(n) ⊆DTIME(n1.2) then by a simple padding argument (a
la the proof of Theorem 2.25) we have a deterministic algorithm D that on inputs x, u with |x| = n
Web draft 2007-01-08 21:59

DRAFT
p5.8 (98)
5.5. DEFINING THE HIERARCHY VIA ORACLE MACHINES.
and |u| = cn8 runs in time O((n8)1.2) = O(n9.6)-time and returns 1 if and only if there exists some
v ∈{0, 1}dn8 such that M(x, u, v) = 0. Thus,
x ∈L ⇔∃u ∈{0, 1}c|x|8 D(x, u) = 0 .
implying that L ∈NTIME(n9.6). ■
Claims 5.14.1 and 5.14.2 show that the assumption that NTIME(n) ⊆TISP(n1.2, n0.2) leads
to contradiction: by simple padding it implies that NTIME(n10) ⊆TISP(n12, n2) which by
Claim 5.14.1 implies that NTIME(n10) ⊆Σ2TIME(n8). But together with Claim 5.14.2 this
implies that NTIME(n10) ⊆NTIME(n9.6), contradicting the non-deterministic time hierarchy
theorem (Theorem 3.3). ■
5.5
Deﬁning the hierarchy via oracle machines.
Recall the deﬁnition of oracle machines from Section 3.5. These are machines that are executed
with access to a special tape they can use to make queries of the form “is q ∈O” for some language
O. For every O ⊆{0, 1}∗, oracle TM M and input x, we denote by MO(x) the output of M on x
with access to O as an oracle. We have the following characterization of the polynomial hierarchy:
Theorem 5.15
For every i ≥2, Σp
i = NPΣi−1SAT, where the latter class denotes the set of languages decided by
polynomial-time NDTMs with access to the oracle Σi−1SAT.
Proof: We showcase the idea by proving that Σp
2 = NPSAT. Suppose that L ∈Σp
2. Then, there
is a polynomial-time TM M and a polynomial q such that
x ∈L ⇔∃u1 ∈{0, 1}q(|x|) ∀u2 ∈{0, 1}q(|x|) M(x, u1, u2) = 1
yet for every ﬁxed u1 and x, the statement “for every u2, M(x, u1, u2) = 1” is the negation of an
NP-statement and hence its truth can be determined using an oracle for SAT. We get that there is
a simple NDTM N that given oracle access for SAT can decide L: on input x, non-deterministically
guess u1 and use the oracle to decide if ∀u2M(x, u1, u2) = 1. We see that x ∈L iﬀthere exists a
choice u1 that makes N accept.
On the other hand, suppose that L is decidable by a polynomial-time NDTM N with oracle
access to SAT. Then, x is in L if and only if there exists a sequence of non-deterministic choices and
correct oracle answers that makes N accept x. That is, there is a sequence of choices c1, . . . , cm ∈
{0, 1} and answers to oracle queries a1, . . . , ak ∈{0, 1} such that on input x, if the machine N
uses the choices c1, . . . , cm in its execution and receives ai as the answer to its ith query, then
(1) M reaches the accepting state qaccept and (2) all the answers are correct. Let ϕi denote the
ith query that M makes to its oracle when executing on x using choices c1, . . . , xm and receiving
answers a1, . . . , ak. Then, the condition (2) can be phrased as follows: if ai = 1 then there exists
an assignment ui such that ϕi(ui) = 1 and if ai = 0 then for every assignment vi, ϕi(vi) = 0. Thus,
Web draft 2007-01-08 21:59

DRAFT
5.5. DEFINING THE HIERARCHY VIA ORACLE MACHINES.
p5.9 (99)
we have that
x ∈L ⇔∃c1, . . . , cm, a1, . . . , ak, u1, . . . , uk∀v1, . . . , vk such that
N accepts x using choices c1, . . . , cm and answers a1, . . . , ak AND
∀i ∈[k] if ai = 1 then ϕi(ui) = 1 AND
∀i ∈[k] if ai = 0 then ϕi(vi) = 0
implying that L ∈Σp
2. ■
Remark 5.16
Because having oracle access to a complete language for a class allows to solve every language in
that class, some texts use the class name instead of the complete language in the notation for the
oracle. Thus, some texts denote the class Σp
2 = NPSAT by NPNP, the class Σp
3 by NPNPNP and
etc.
What have we learned?
• The polynomial hierarchy is the set of languages that can be deﬁned via a
constant number of alternating quantiﬁers. It also has equivalent deﬁnitions
via alternating TMs and oracle TMs. It contains several natural problems
that are not known (or believed) to be in NP.
• We conjecture that the hierarchy does not collapse in the sense that each of
its levels is distinct from the previous ones.
• We can use the concept of alternations to prove that SAT cannot be solved
simultaneously in linear time and sublinear space.
Chapter notes and history
The polynomial hierarchy was formally deﬁned by Stockmeyer [Sto77], though the concept appears
in the literature even earlier. For instance, Karp [Kar72] notes that “a polynomial-bounded version
of Kleene’s Arithmetic Hierarchy (Rogers 1967) becomes trivial if P = NP.”
The class DP was deﬁned by Papadimitriou and Yannakakis [PY82], who used it to characterize
the complexity of identifying the facets of a polytope.
The class of complete problems for various levels of PH is not as rich as it is for NP, but it
does contain several interesting ones. See Schaeﬀer and Umans [SU02a, SU02b] for a recent list.
The SUCCINCT SET-COVER problem is from Umans [Uma01], where it is also shown that the
following optimization version of MIN-DNF is Σp
2-complete:

⟨ϕ, k⟩: ∃DNFϕ′ of size at most k, that is equivalent to DNF ϕ
	
.
Web draft 2007-01-08 21:59

DRAFT
p5.10 (100)
5.5. DEFINING THE HIERARCHY VIA ORACLE MACHINES.
Exercises
§1 Show that the language ΣiSAT of (1) is complete for Σp
i under polynomial time reductions.
Hint:: Use the NP-completeness of SAT.
§2 Prove Claim 5.11.
§3 Show that if 3SAT is polynomial-time reducible to 3SAT then PH = NP.
§4 Show that PH has a complete language iﬀit collapses to some ﬁnite level Σp
i .
§5 Show that the deﬁnition of PH using ATMs coincides with our other deﬁnitions.
§6 Show that APSPACE = EXP.
Hint: The nontrivial direction EXP ⊆APSPACE uses ideas
similar to those in the proof of Theorem 5.13.
§7 Show that Σp
2 = NPSAT. Generalize your proof to give a characterization of PH in terms of
oracle Turing machines.
§8 The class DP is deﬁned as the set of languages L for which there are two languages L1 ∈
NP, L2 ∈coNP such that L = L1 ∩L2. (Do not confuse DP with NP ∩coNP, which may
seem superﬁcially similar.) Show that
(a) EXACT INDSET ∈DP.
(b) Every language in DP is polynomial-time reducible to EXACT INDSET.
§9 Suppose A is some language such that PA = NPA. Then show that PHA ⊆PA (in other
words, the proof of Theorem ?? relativizes).
§10 Show that SUCCINCT SET-COVER ∈Σp
2.
§11 (Suggested by C. Umans) This problem studies VC-dimension, a concept important in ma-
chine learning theory. If S = {S1, S2, . . . , Sm} is a collection of subsets of a ﬁnite set U, the
VC dimension of S, denoted V C(S), is the size of the largest set X ⊆U such that for every
X′ ⊆X, there is an i for which Si ∩X = X′. (We say that X is shattered by S.)
A boolean circuit C succinctly represents collection S if Si consists of exactly those elements
x ∈U for which C(i, x) = 1. Finally,
VC-DIMENSION = {⟨C, k⟩: C represents a collection S s.t. V C(S) ≥k} .
(a) Show that VC-DIMENSION ∈Σp
3.
(b) Show that VC-DIMENSION is Σp
3-complete.
Hint: Reduce from Σ3-3SAT. Also, the collection S produced by
your reduction can use the same set multiple times.
Web draft 2007-01-08 21:59

DRAFT
Chapter 6
Circuits
“One might imagine that P ̸= NP, but SAT is tractable in the following sense: for
every ℓthere is a very short program that runs in time ℓ2 and correctly treats all
instances of size ℓ. ”
Karp and Lipton, 1982
This chapter investigates a model of computation called a Boolean circuit, which is a general-
ization of Boolean formulae and a rough formalization of the familiar ”silicon chip.” Here are some
motivations for studying it.
First, it is a natural model for nonuniform computation, by which we mean that a diﬀerent
”algorithm” is allowed for each input size. By contrast, our standard model thus far was uniform
computation: the same Turing Machine (or algorithm) solves the problem for inputs of all (inﬁnitely
many) sizes. Nonuniform computation crops up often in complexity theory, and also in the rest of
this book.
Second, in principle one can separate complexity classes such as P and NP by proving lower-
bounds on circuit size. This chapter outlines why such lowerbounds ought to exist. In the 1980s,
researchers felt boolean circuits are mathematically simpler than the Turing Machine, and thus
proving circuit lowerbounds may be the right approach to separating complexity classes. Chap-
ter 13 describes the partial successes of this eﬀort and Chapter 22 describes where it is stuck.
This chapter deﬁnes the class P/poly of languages computable by polynomial-sized boolean
circuits and explores its relation to NP. We also encounter some interesting subclasses of P/poly,
including NC, which tries to capture computations that can be eﬃciently performed on highly
parallel computers. Finally, we show a (yet another) characterization of the polynomial hierarchy,
this time using exponential-sized circuits of constant depth.
6.1
Boolean circuits
A Boolean circuit is a just a diagram showing how to derive an output from an input by a combi-
nation of the basic Boolean operations of OR (∨), AND (∧) and NOT (¬). For example, Figure 6.1
shows a circuit computing the XOR function. Here is the formal deﬁnition.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p6.1 (101)

DRAFT
p6.2 (102)
6.1. BOOLEAN CIRCUITS




Figure 6.1: A circuit C computing the XOR function (i.e., C(x1, x2) = 1 iﬀx1 ̸= x2).
Definition 6.1 (Boolean circuits)
For every n, m ∈N a Boolean circuit C with n inputs and m outputs1is a directed
acyclic graph. It contains n nodes with no incoming edges; called the input nodes
and m nodes with no outgoing edges, called the output nodes.
All other nodes
are called gates and are labeled with one of ∨, ∧or ¬ (in other words, the logical
operations OR, AND, and NOT). The ∨and ∧nodes have fanin (i.e., number of
incoming edges) of 2 and the ¬ nodes have fanin 1. The size of C, denoted by |C|,
is the number of nodes in it.
The circuit is called a Boolean formula if each node has at most one outgoing edge.
The boolean circuit in the above deﬁnition implements a function from {0, 1}n to {0, 1}m. This
may be clear intuitively to most readers (especially those who have seen circuits in any setting)
but here is the proof. Assume that the n input nodes and m output nodes are numbered in some
canonical way. Thus each n-bit input can be used to assigned a value in {0, 1} to each input node.
Next, since the graph is acyclic, we can associate an integral depth to each node (using breadth-ﬁrst
search, or the so-called topological sorting of the graph) such that each node has incoming edges
only from nodes of higher depth. Now each node can be assigned a value from {0, 1} in a unique
way as follows. Process the nodes in decreasing order of depth. For each node, examine its incoming
edges and the values assigned to the nodes at the other end, and then apply the boolean operation
(∨, ∧, or ¬) that this node is labeled with on those values. This gives a value to each node; the
values assigned to the m output nodes by this process constitute an m-bit output of the circuit.
For every string u ∈{0, 1}n, we denote by C(u) the output of the circuit C on input u.
We recall that the Boolean operations OR, AND, and NOT form a universal basis, by which
we mean that every function from {0, 1}n to {0, 1}m can be implemented by a boolean circuit (in
fact, a boolean formula). See Claim 2.14. Furthermore, the “silicon chip” that we all know about
is nothing but2 an implementation of a boolean circuit using a technology called VLSI. Thus if we
have a small circuit for a computational task, we can implement it very eﬃciently as a silicon chip.
Of course, the circuit can only solve problems on inputs of a certain size. Nevertheless, this may
not be a big restriction in our ﬁnite world. For instance, what if a small circuit exists that solves
2Actually, the circuits in silicon chips are not acyclic; in fact the cycles in the circuit are crucial for implementing
”memory.” However any computation that runs on a silicon chip of size C and ﬁnishes in time T can be performed
by a boolean circuit of size O(C · T).
Web draft 2007-01-08 21:59

DRAFT
6.1. BOOLEAN CIRCUITS
p6.3 (103)
3SAT instances of up to say 100, 000 variables? If so, one could imagine a government-ﬁnanced
project akin to the Manhattan project that would try to discover such a small circuit, and then
implement it as a silicon chip. This could be used in all kinds of commercial products (recall our
earlier depiction of a world in which P = NP) and in particular would jeopardize every encryption
scheme that does not use a huge key. This scenario is hinted at in the quote from Karp and Lipton
at the start of the chapter.
As usual, we resort to asymptotic analysis to study the complexity of deciding a language by
circuits.
Definition 6.2 (Circuit families and language recognition)
Let T : N →N be a function. A T(n)-sized circuit family is a sequence {Cn}n∈N of Boolean circuits,
where Cn has n inputs and a single output, such that |Cn| ≤T(n) for every n.
We say that a language L is in SIZE(T(n)) if there exists a T(n)-size circuit family {Cn}n∈N
such that for every x ∈{0, 1}n, x ∈L ⇔C(x) = 1.
As noted in Claim 2.14, every language is decidable by a circuit family of size O(n2n), since
the circuit for input length n could contain 2n “hardwired” bits indicating which inputs are in the
language. Given an input, the circuit looks up the answer from this table. (The reader may wish
to work out an implementation of this circuit.) The following deﬁnition formalizes what we can
think of as “small” circuits.
Definition 6.3
P/poly is the class of languages that are decidable by polynomial-sized circuit families, in other
words, ∪cSIZE(nc).
Of course, one can make the same kind of objections to the practicality of P/poly as for P:
viz., in what sense is a circuit family of size n100 practical, even though it has polynomial size. This
was answered to some extent in Section 1.5.1. Another answer is that as complexity theorists we
hope (eventually) to show that languages such as SAT are not in P/poly. Thus the result will only
be stronger if we allow even such large circuits in the deﬁnition of P/poly.
The class P/poly contains P. This is a corollary of Theorem 6.7 that we show below. Can we
give a reasonable upperbound on the computational power of P/poly? Unfortunately not, since it
contains even undecidable languages.
Example 6.4
Recall that we say that a language L is unary if it is a subset of {1n : n ∈N}. Every unary language
has linear size circuits since the circuit for an input size n only needs to have a single “hardwired”
bit indicating whether or not 1n is in the language. Hence the following unary language has linear
size circuits, even though it is undecidable:
{1n : Mn outputs 1 on input 1n} .
(1)
where Mn is the machine represented by (the binary expansion of) the number n.
Web draft 2007-01-08 21:59

DRAFT
p6.4 (104)
6.1. BOOLEAN CIRCUITS
This example suggests that it may be fruitful to consider the restriction to circuits that can
actually be built, say using a fairly eﬃcient Turing machine. It will be most useful to formalize
this using logspace computations.
Recall that a function f : {0, 1}∗→{0, 1}∗is implicitly logspace computable if the mapping
x, i 7→f(x)i can be computed in logarithmic space (see Deﬁnition 4.14).
Definition 6.5 (logspace-uniform circuit families)
A circuit family {Cn} is logspace uniform if there is an implicitly logspace computable function
mapping 1n to the description of the circuit Cn.
Actually, to make this concrete we need to ﬁx some representation of the circuits as strings. We
will assume that the circuit of size N is represented by its N ×N adjacency matrix and in addition
an array of size N that gives the labels (gate type and input/output) of each node. This means
that {Cn} is logspace uniform if and only if the following functions are computable in O(log n)
space:
• SIZE(n) returns the size m (in binary representation) of the circuit Cn.
• TYPE(n, i), where i ∈[m], returns the label and type of the ith node of Cn. That is it returns
one of {∨, ∧, ¬, NONE} and in addition ⟨OUTPUT, j⟩or ⟨INPUT, j⟩if i is the jth input
or output node of Cn.
• EDGE(n, i, j) returns 1 if there is a directed edge in Cn between the ith node and the jth node.
Note that both the inputs and the outputs of these functions can be encoded using a logarithmic
(in |Cn|) number of bits. The requirement that they run in O(log n) space means that we require
that log |Cn| = O(log n) or in other words that Cn is of size at most polynomial in n.
Remark 6.6
Exercise 7 asks you to prove that the class of languages decided by such circuits does not change if we
use the adjacency list (as opposed to matrix) representation. We will use the matrix representation
from now on.
Polynomial circuits that are logspace-uniform correspond to a familiar complexity class:
Theorem 6.7
A language has logspace-uniform circuits of polynomial size iﬀit is in P.
Remark 6.8
Note that this implies that P ⊆P/poly.
Proof sketch:
The only if part is trivial.
The if part follows the proof of the Cook-Levin
Theorem (Theorem 2.10). Recall that we can simulate every time O(T(n)) TM M by an oblivious
TM
˜
M (whose head movement is independent of its input) running in time O(T(n)2) (or even
O(T(n) log T(n)) if we are more careful). In fact, we can ensure that the movement of the oblivious
TM ˜
M do not even depend on the contents of its work tape, and so, by simulating ˜
Mwhile ignoring
Web draft 2007-01-08 21:59

DRAFT
6.1. BOOLEAN CIRCUITS
p6.5 (105)
its read/write instructions, we can compute in O(log T(n)) space for every i the position its heads
will be at the ith step.3
Given this insight, it is fairly straightforward to translate the proof of Theorem 2.10 to prove
that every language in P has a logspace-uniform circuit family. The idea is that if L ∈P then it is
decided by an oblivious TM ˜
M of the form above. We will use that to construct a logspace uniform
circuit family {Cn}n∈N such that for every x ∈{0, 1}n, Cn(x) = ˜
M(x).
Recall that, as we saw in that proof, the transcript of ˜
M’s execution on input x is the sequence
z1, . . . , zT of snapshots (machine’s state and symbols read by all heads) of the execution at each
step in time. Assume that each such zi is encoded by a string (that needs only to be of constant
size). We can compute the string zi based the previous snapshots zi−1 and zi1, . . . , zik where zij
denote the last step that ˜
M’s jth head was in the same position as it is in the ith step. Because these
are only a constant number of strings of constant length, we can compute zi from these previous
snapshot using a constant-sized circuit. Also note that, under our assumption above, given the
indices i and i′ < i we can easily check whether zi depends on zi′.
The composition of all these constant-sized circuits gives rise to a circuit that computes from
the input x, the snapshot zT of the last step of ˜
M’s execution on x. There is a simple constant-sized
circuit that, given zT outputs 1 if and only if zT is an accepting snapshot (in which ˜
M outputs 1
and halts). Thus, we get a circuit C such that C(x) = ˜
M(x) for every x ∈{0, 1}n.
Because our circuit C is composed of many small (constant-sized) circuits, and determining
which small circuit is applied to which nodes can be done in logarithmic space, it is not hard to see
that we can ﬁnd out every individual bit of C’s representation in logarithmic space. (In fact, one
can show that the functions SIZE, TYPE and EDGE above can be computed using only logarithmic
space and polylogarithmic time.) ■
6.1.1
Turing machines that take advice
There is a way to deﬁne P/poly using Turing machines that ”take advice.”
Definition 6.9
Let T, a : N →N be functions. The class of languages decidable by time-T(n) TM’s with a(n)
advice, denoted DTIME(T(n))/a(n), contains every L such that there exists a sequence {αn}n∈N
of strings with αn ∈{0, 1}a(n) and a TM M satisfying
M(x, αn) = 1 ⇔x ∈L
for every x ∈{0, 1}n, where on input (x, αn) the machine M runs for at most O(T(n)) steps.
Example 6.10
Every unary language can be be decided by a polynomial time Turing machine with 1 bit of advice.
The advice string for inputs of length n is the single bit indicating whether or not 1n is in the
language. In particular this is true of the language of Example 6.4.
3In fact, typically the movement pattern is simple enough (for example a sequence of T(n) left to right and back
sweeps of the tape) that for every i we can compute this information using only O(log T(n)) space and polylogT(n)
time.
Web draft 2007-01-08 21:59

DRAFT
p6.6 (106)
6.2. KARP-LIPTON THEOREM
This is an example of a more general phenomenon described in the next theorem.
Theorem 6.11
P/poly = ∪c,dDTIME(nc)/nd
Proof: If L ∈P/poly, we can provide the polynomial-sized description of its circuit family as
advice to a Turing machine. When faced with an input of size n, the machine just simulates the
circuit for this circuit provided to it.
Conversely, if L is decidable by a polynomial-time Turing machine M with access to an advice
family {αn}n∈N of size a(n) for some polynomial a, then we can use the construction of Theorem 6.7
to construct for every n, a polynomial-sized circuit Dn such that on every x ∈{0, 1}n, α ∈{0, 1}a(n),
Dn(x, α) = M(x, α). We let the circuit Cn be the polynomial circuit that maps x to Dn(x, αn).
That is, Cn is equal to the circuit Dn with the string αn “hardwired” as its second input. ■
Remark 6.12
By “hardwiring” an input into a circuit we mean taking a circuit C with two inputs x ∈{0, 1}n , y ∈
{0, 1}m and transforming it into the circuit Cy that for every x returns C(x, y). It is easy to do so
while ensuring that the size of Cy is not greater than the size of C. This simple idea is often used
in complexity theory.
6.2
Karp-Lipton Theorem
Karp and Lipton formalized the question of whether or not SAT has small circuits as: Is SAT in
P/poly? They showed that the answer is “NO” if the polynomial hierarchy does not collapse.
Theorem 6.13 (Karp-Lipton, with improvements by Sipser)
If NP ⊆P/poly then PH = Σp
2.
Proof: To show that PH = Σp
2 it is enough to show that Πp
2 ⊆Σp
2 and in particular it suﬃces
to show that Σp
2 contains the Πp
2-complete language Π2SAT consisting of all true formulae of the
form
∀u ∈{0, 1}n ∃v ∈{0, 1}n ϕ(u, v) = 1 .
(2)
where ϕ is an unquantiﬁed Boolean formula.
If NP ⊆P/poly then there is a polynomial p and a p(n)-sized circuit family {Cn}n∈N such that
for every Boolean formula ϕ and u ∈{0, 1}n, Cn(ϕ, u) = 1 if and only if there exists v ∈{0, 1}n
such that ϕ(u, v) = 1. Yet, using the search to decision reduction of Theorem 2.19, we actually
know that there is a q(n)-sized circuit family {C′
n}n∈N such that for every such formula ϕ and
u ∈{0, 1}n, if there is a string v ∈{0, 1}n such that ϕ(u, v) = 1 then C′
n(ϕ, u) outputs such a string
v. Since C′
n can be described using 10q(n)2 bits, this implies that if (2) is true then the following
quantiﬁed formula is also true:
∃w∈{0, 1}10q(n)2 ∀u∈{0, 1}n w describes a circuit C′ s.t. ϕ(u, C′(ϕ, u)) = 1 .
(3)
Web draft 2007-01-08 21:59

DRAFT
6.3. CIRCUIT LOWERBOUNDS
p6.7 (107)
Yet if (2) is false then certainly (regardless of whether P = NP) the formula (3) is false as well,
and hence (3) is actually equivalent to (2)! However, since evaluating a circuit on an input can be
done in polynomial time, evaluating the truth of (3) can be done in Σp
2. ■
Similarly the following theorem can be proven, though we leave the proof as Exercise 3.
Theorem 6.14 (Karp-Lipton, attributed to A. Meyer)
If EXP ⊆P/poly then EXP = Σp
2.
Combining the time hierarchy theorem (Theorem 3.1) with the previous theorem implies that
if P = NP then EXP ̸⊆P/poly. Thus upperbounds (in this case, NP ⊆P) can potentially be
used to prove circuit lowerbounds.
6.3
Circuit lowerbounds
Since P ⊆P/poly, if NP ⊈P/poly then P ̸= NP. The Karp-Lipton theorem gives hope that
NP ̸⊆P/poly.
Can we resolve P versus NP by proving NP ⊈P/poly?
There is reason to
invest hope in this approach as opposed to proving direct lowerbounds on Turing machines. By
representing computation using circuits we seem to actually peer into the guts of it rather than
treating it as a blackbox. Thus we may be able to get around the limitations of relativizing methods
shown in Chapter 3.
Sadly, such hopes have not yet come to pass. After two decades, the best circuit size lowerbound
for an NP language is only 5n. (However, see Exercise 1 for a better lowerbound for a language in
PH.) On the positive side, we have had notable success in proving lowerbounds for more restricted
circuit models, as we will see in Chapter 13.
By the way, it is easy to show that for large enough n, almost every boolean function on n
variables requires large circuits.
Theorem 6.15
For n ≥100, almost all boolean functions on n variables require circuits of size at least 2n/(10n).
Proof: We use a simple counting argument. There are at most s3s circuits of size s (just count
the number of labeled directed graphs, where each node has indegree at most 2). Hence this is an
upperbound on the number of functions on n variables with circuits of size s. For s = 2n/(10n),
this number is at most 22n/10, which is miniscule compared 22n, the number of boolean functions
on n variables. Hence most Boolean functions do not have such small circuits. ■
Remark 6.16
Another way to present this result is as showing that with high probability, a random function from
{0, 1}n to {0, 1} does not have a circuit of size 2n/10n. This kind of proof method, showing the
existence of an object with certain properties by arguing that a random object has these properties
with high probability, is called the probabilistic method, and will be repeatedly used in this book.
The problem with the above counting argument is of course, that it does not yield an explicit
Boolean function (say an NP language) that requires large circuits.
Web draft 2007-01-08 21:59

DRAFT
p6.8 (108)
6.4. NON-UNIFORM HIERARCHY THEOREM
6.4
Non-uniform hierarchy theorem
As in the case of deterministic time, non-deterministic time and space bounded machines, Boolean
circuits also have a hierarchy theorem. That is, larger circuits can compute strictly more functions
than smaller ones:
Theorem 6.17
For every functions T, T ′ : N →N with 2n/(100n) > T ′(n) > T(n) > n and T(n) log T(n) =
o(T ′(n)),
SIZE(T(n)) ⊊SIZE(T ′(n))
Proof: The diagonalization methods of Chapter 3 do not seem to work for such a function, but
nevertheless, we can prove it using the counting argument from above. To show the idea, we prove
that SIZE(n) ⊊SIZE(n2).
For every ℓ, there is a function f : {0, 1}ℓ→{0, 1} that is not computable by 2ℓ/(10ℓ)-sized
circuits. On the other hand, every function from {0, 1}ℓto {0, 1} is computable by a 2ℓ10ℓ-sized
circuit.
Therefore, if we set ℓ= 1.1 log n and let g : {0, 1}n →{0, 1} be the function that applies f on
the ﬁrst ℓbits of its input, then
g ∈
SIZE(2ℓ10ℓ) =
SIZE(11n1.1 log n) ⊆
SIZE(n2)
g ̸∈
SIZE(2ℓ/(10ℓ)) =
SIZE(n1.1/(11 log n)) ⊇
SIZE(n)
■
6.5
Finer gradations among circuit classes
There are two reasons why subclasses of P/poly are interesting. First, proving lowerbounds for
these subclasses may give insight into how to separate NP from P/poly. Second, these subclasses
correspond to interesting computational models in their own right.
Perhaps the most interesting connection is to massively parallel computers. In such a computer
one uses simple oﬀ-the-shelf microprocessors and links them using an interconnection network that
allows them to send messages to each other. Usual interconnection networks such as the hypercube
allows linking n processors such that interprocessor communication is possible —assuming some
upperbounds on the total load on the network—in O(log n) steps.
The processors compute in
lock-step (for instance, to the ticks of a global clock) and are assumed to do a small amount of
computation in each step, say an operation on O(log n) bits. Thus each processor computers has
enough memory to remember its own address in the interconnection network and to write down
the address of any other processor, and thus send messages to it. We are purposely omitting many
details of the model (Leighton [Lei92] is the standard reference for this topic) since the validity
of Theorem 6.24 below does not depend upon them. (Of course, we are only aiming for a loose
characterization of parallel computation, not a very precise one.)
Web draft 2007-01-08 21:59

DRAFT
6.5. FINER GRADATIONS AMONG CIRCUIT CLASSES
p6.9 (109)
6.5.1
Parallel computation and NC
Definition 6.18
A computational task is said to have eﬃcient parallel algorithms if inputs of size n can be solved
using a parallel computer with nO(1) processors and in time logO(1) n.
Example 6.19
Given two n bit numbers x, y we wish to compute x + y fast in parallel. The gradeschool algorithm
proceeds from the least signiﬁcant bit and maintains a carry bit.
The most signiﬁcant bit is
computed only after n steps. This algorithm does not take advantage of parallelism. A better
algorithm called carry lookahead assigns each bit position to a separate processor and then uses
interprocessor communication to propagate carry bits. It takes O(n) processors and O(log n) time.
There are also eﬃcient parallel algorithms for integer multiplication and division (the latter is
quite nonintuitive and unlike the gradeschool algorithm!).
Example 6.20
Many matrix computations can be done eﬃciently in parallel: these include computing the product,
rank, determinant, inverse, etc. (See exercises.)
Some graph theoretic algorithms such as shortest paths and minimum spanning tree also have
fast parallel implementations.
But many well-known polynomial-time problems such as minimum matching, maximum ﬂows,
and linear programming are not known to have any good parallel implementations and are conjec-
tured not to have any; see our discussion of P-completeness below.
Now we relate parallel computation to circuits. The depth of a circuit is the length of the longest
directed path from an input node to the output node.
Definition 6.21 (Nick’s class or NC)
A language is in NCi if there are constants c, d > 0 such that it can be decided by a logspace-
uniform family of circuits {Cn} where Cn has size O(nc) and depth O(logd n). The class NC is
∪i≥1NCi.
A related class is the following.
Definition 6.22 (AC)
The class ACi is deﬁned similarly to NCi except gates are allowed to have unbounded fanin. The
class AC is ∪i≥0ACi.
Web draft 2007-01-08 21:59

DRAFT
p6.10 (110)
6.5. FINER GRADATIONS AMONG CIRCUIT CLASSES
Since unbounded (but poly(n)) fanin can be simulated using a tree of ORs/ANDs of depth
O(log n), we have NCi ⊆ACi ⊆NCi+1, and the inclusion is known to be strict for i = 0 as we
will see in Chapter 13. (Notice, NC0 is extremely limited since the circuit’s output depends upon
a constant number of input bits, but AC0 does not suﬀer from this limitation.)
Example 6.23
The language PARITY ={x : x has an odd number of 1s} is in NC1. The circuit computing it
has the form of a binary tree. The answer appears at the root; the left subtree computes the parity
of the ﬁrst |x| /2 bits and the right subtree computes the parity of the remaining bits. The gate
at the top computes the parity of these two bits. Clearly, unwrapping the recursion implicit in our
description gives a circuit of dept O(log n).
The classes AC, NC are important because of the following.
Theorem 6.24
A language has eﬃcient parallel algorithms iﬀit is in NC.
Proof: Suppose a language L ∈NC and is decidable by a circuit family {Cn} where Cn has size
N = O(nc) and depth D = O(logd n). Take a general purpose parallel computer with N nodes
and conﬁgure it to decide L as follows. Compute a description of Cn and allocate the role of each
circuit node to a distinct processor. (This is done once, and then the computer is ready to compute
on any input of length n.) Each processor, after computing the output at its assigned node, sends
the resulting bit to every other circuit node that needs it. Assuming the interconnection network
delivers all messages in O(log N) time, the total running time is O(logd+1 N).
The reverse direction is similar, with the circuit having N · D nodes arranged in D layers, and
the ith node in the tth layer performs the computation of processor i at time t. The role of the
interconnection network is played by the circuit wires. ■
6.5.2
P-completeness
A major open question in this area is whether P = NC.
We believe that the answer is NO
(though we are currently even unable to separate PH from NC1). This motivates the theory of
P-completeness, a study of which problems are likely to be in NC and which are not.
Definition 6.25
A language is P-complete if it is in P and every language in P is logspace-reducible to it (as per
Deﬁnition 4.14).
The following easy theorem is left for the reader as Exercise 12.
Theorem 6.26
If language L is P-complete then
1. L ∈NC iﬀP = NC.
Web draft 2007-01-08 21:59

DRAFT
6.6. CIRCUITS OF EXPONENTIAL SIZE
p6.11 (111)
2. L ∈L iﬀP = L. (Where L is the set languages decidable in logarithmic space, see Deﬁni-
tion 4.5.)
The following is a fairly natural P-complete language:
Theorem 6.27
Let CIRCUIT-EVAL denote the language consisting of all pairs ⟨C, x⟩such that C is an n-inputs
single-output circuit and x ∈{0, 1}n satisﬁes C(x) = 1. Then CIRCUIT-EVAL is P-complete.
Proof: The language is clearly in P. A logspace-reduction from any other language in P to this
language is implicit in the proof of Theorem 6.7. ■
6.6
Circuits of exponential size
As noted, every language has circuits of size O(n2n). However, actually ﬁnding these circuits may
be diﬃcult— sometimes even undecidable. If we place a uniformity condition on the circuits, that
is, require them to be eﬃciently computable then the circuit complexity of some languages could
exceed n2n. In fact it is possible to give alternative deﬁnitions of some familiar complexity classes,
analogous to the deﬁnition of P in Theorem 6.7.
Definition 6.28 (DC-Uniform)
Let {Cn}n≥1 be a circuit family. We say that it is a Direct Connect uniform (DC uniform) family if,
given ⟨n, i⟩, we can compute in polynomial time the ith but of (the representation of) the circuit Cn.
More concretely, we use the adjacency matrix representation and hence a family {Cn}n∈N is DC
uniform iﬀthe functions SIZE, TYPE and EDGE deﬁned in Remark ?? are computable in polynomial
time.
Note that the circuits may have exponential size, but they have a succinct representation in
terms of a TM which can systematically generate any required node of the circuit in polynomial
time.
Now we give a (yet another) characterization of the class PH, this time as languages computable
by uniform circuit families of bounded depth. We leave it as Exercise 13.
Theorem 6.29
L ∈PH iﬀL can be computed by a DC uniform circuit family {Cn} that
• uses AND, OR, NOT gates.
• has size 2nO(1) and constant depth (i.e., depth O(1)).
• gates can have unbounded (exponential) fanin.
• the NOT gates appear only at the input level.
If we drop the restriction that the circuits have constant depth, then we obtain exactly EXP
(see Exercise 14).
Web draft 2007-01-08 21:59

DRAFT
p6.12 (112)
6.7. CIRCUIT SATISFIABILITY AND AN ALTERNATIVE PROOF OF THE COOK-LEVIN
THEOREM
6.7
Circuit Satisﬁability and an alternative proof of the Cook-
Levin Theorem
Boolean circuits can be used to deﬁne the following NP-complete language:
Definition 6.30
The circuit satisﬁability language CKT-SAT consists of all (strings representing) circuits with a
single output that have a satisfying assignment. That is, a string representing an n-input circuit
C is in CKT-SAT iﬀthere exists u ∈{0, 1}n such that C(u) = 1.
CKT-SAT is clearly in NP because the satisfying assignment can serve as the certiﬁcate. It is
also clearly NP-hard as every CNF formula is in particular a Boolean circuit. However, CKT-SAT
can also be used to give an alternative proof (or, more accurately, a diﬀerent presentation of the
same proof) for the Cook-Levin Theorem by combining the following two lemmas:
Lemma 6.31
CKT-SAT is NP-hard.
Proof: Let L be an NP-language and let p be a polynomial and M a polynomial-time TM such
that x ∈L iﬀM(x, u) = 1 for some u ∈{0, 1}p(|x|). We reduce L to CKT-SAT by mapping (in
polynomial-time) x to a circuit Cx with p(|x|) inputs and a single output such that Cx(u) = M(x, u)
for every u ∈{0, 1}p(|x|).
Clearly, x ∈L ⇔Cx ∈CKT-SAT and so this suﬃces to show that
L ≤P CKT-SAT.
Yet, it is not hard to come up with such a circuit. Indeed, the proof of Theorem 6.7 yields a
way to map M, x into the circuit Cx in logarithmic space (which in particular implies polynomial
time). ■
Lemma 6.32
CKT-SAT ≤p 3SAT
Proof: As mentioned above this follows from the Cook-Levin theorem but we give here a direct
reduction. If C is a circuit, we map it into a 3CNF formula ϕ as follows:
For every node vi of C we will have a corresponding variable zi in ϕ. If the node vi is an AND of
the nodes vj and vk then we add to ϕ clauses that are equivalent to the condition “zi = (zj ∧zk)”.
That is, we add the clauses
(zi ∨zj ∨zk) ∧(zi ∨zj ∨zk) ∧(zi ∨zj ∨zk) ∧(zi ∨zj ∨zk) .
Similarly, if vi is an OR of vj and vk then we add clauses equivalent to “zi = (zj ∨zk)”, and if vi
is the NOT of vj then we add the clauses (zi ∨zj) ∧(zi ∨zj).
Finally, if vi is the output node of C then we add the clause zi to ϕ. It is not hard to see that
the formula ϕ will be satisﬁable if and only if the circuit C is. ■
Web draft 2007-01-08 21:59

DRAFT
6.7. CIRCUIT SATISFIABILITY AND AN ALTERNATIVE PROOF OF THE COOK-LEVIN
THEOREM
p6.13 (113)
What have we learned?
• Boolean circuits can be used as an alternative computational model to TMs.
The class P/poly of languages decidable by polynomial-sized circuits is a strict
superset of P but does not contain NP unless the hierarchy collapses.
• Almost every function from {0, 1}n to {0, 1} requires exponential-sized circuits.
Finding even one function in NP with this property would show that P ̸= NP.
• The class NC of languages decidable by (uniformly constructible) circuits with
polylogarithmic depth and polynomial size corresponds to computational tasks
that can be eﬃciently parallelized.
Chapter notes and history
Karp-Lipton theorem is from [KL82]. Karp and Lipton also gave a more general deﬁnition of advice
that can be used to deﬁne the class C/a(n) for every complexity class C and function a. However,
we do not use this deﬁnition here since it does not seem to capture the intuitive notion of advice
for classes such as NP ∩coNP, BPP and others.
The class of NC algorithms as well as many related issues in parallel computation are discussed
in Leighton [?].
Exercises
§1 [Kannan [Kan82]] Show for every k > 0 that PH contains languages whose circuit complexity
is Ω(nk).
Hint: Keep in mind the proof of the existence of functions with
high circuit complexity.
§2 Solve the previous question with PH replaced by Σp
2.
§3 ([KL82], attributed to A. Meyer) Show that if EXP ⊆P/poly then EXP = Σp
2.
§4 Show that if P = NP then there is a language in EXP that requires circuits of size 2n/n.
§5 A language L ⊆{0, 1}∗is sparse if there is a polynomial p such that |L ∩{0, 1}n | ≤p(n) for
every n ∈N. Show that every sparse language is in P/poly.
§6 (X’s Theorem 19??) Show that if a sparse language is NP-complete then P = NP. (This is
a strengthening of Exercise 13 of Chapter 2.)
Web draft 2007-01-08 21:59

DRAFT
p6.14 (114)
6.7. CIRCUIT SATISFIABILITY AND AN ALTERNATIVE PROOF OF THE COOK-LEVIN
THEOREM
Hint: Show a recursive exponential-time algorithm S that on in-
put a n-variable formula ϕ and a string v ∈{0, 1}n outputs 1 iﬀ
ϕ has a satisfying assignment v such that v > u when both are
interpreted as the binary representation of a number in [2n]. Use
the reduction from SAT to L to prune possibilities in the recursion
tree of S.
§7 Show a logspace implicitly computable function f that maps any n-vertex graph in adjacency
matrix representation into the same graph in adjacency list representation. You can think
of the adjacency list representation of an n-vertex graph as a sequence of n strings of size
O(n log n) each, where the ith string contains the list of neighbors of the ith vertex in the
graph (and is padded with zeros if necessary).
§8 (Open) Suppose we make a stronger assumption than NP ⊆P/poly: every language in NP
has linear size circuits. Can we show something stronger than PH = Σp
2?
§9
(a) Describe an NC circuit for the problem of computing the product of two given n × n
matrices A, B.
(b) Describe an NC circuit for computing, given an n × n matrix, the matrix An.
Hint: Use repeated squaring: A2k = (A2k−1)2.
(c) Conclude that the PATH problem (and hence every NL language) is in NC.
Hint: What is the meaning of the (i, j)th entry of An?
§10 A formula is a circuit in which every node (except the input nodes) has outdegree 1. Show
that a language is computable by polynomial-size formulae iﬀit is in (nonuniform) NC1.
Hint: a formula may be viewed —once we exclude the input
nodes—as a directed binary tree, and in a binary tree of size m
there is always a node whose removal leaves subtrees of size at
most 2m/3 each.
§11 Show that NC1 = L. Conclude that PSPACE ̸= NC1.
§12 Prove Theorem 6.26. That is, prove that if L is P-complete then L ∈NC (resp. L) iﬀ
P = NC (resp. L).
§13 Prove Theorem 6.29 (that PH is the set of languages with constant-depth DC uniform cir-
cuits).
§14 Show that EXP is exactly the set of languages with DC uniform circuits of size 2nc where c
is some constant (c may depend upon the language).
§15 Show that if linear programming has a fast parallel algorithm then P = NC.
Hint: in your reduction, express the CIRCUIT-EVAL problem as a
linear program and use the fact that x ∨y = 1 iﬀx + y ≥1. Be
careful; the variables in a linear program are real-valued and not
boolean!
Web draft 2007-01-08 21:59

DRAFT
Chapter 7
Randomized Computation
“We do not assume anything about the distribution of the instances of the problem
to be solved. Instead we incorporate randomization into the algorithm itself... It may
seem at ﬁrst surprising that employing randomization leads to eﬃcient algorithm.
This claim is substantiated by two examples. The ﬁrst has to do with ﬁnding the
nearest pair in a set of n points in Rk. The second example is an extremely eﬃcient
algorithm for determining whether a number is prime.”
Michael Rabin, 1976
Thus far our standard model of computation has been the deterministic Turing Machine. But
everybody who is even a little familiar with computation knows that that real-life computers need
not be deterministic since they have built-in ”random number generators.” In fact these generators
are very useful for computer simulation of ”random” processes such as nuclear ﬁssion or molecular
motion in gases or the stock market. This chapter formally studies probablistic computation, and
complexity classes associated with it.
We should mention right away that it is an open question whether or not the universe has any
randomness in it (though quantum mechanics seems to guarantee that it does). Indeed, the output
of current ”random number generators” is not guaranteed to be truly random, and we will revisit
this limitation in Section 7.4.3. For now, assume that true random number generators exist. Then
arguably, a realistic model for a real-life computer is a Turing machine with a random number
generator, which we call a Probabilistic Turing Machine (PTM). It is natural to wonder whether
diﬃcult problems like 3SAT are eﬃciently solvable using a PTM.
We will formally deﬁne the class BPP of languages decidable by polynomial-time PTMs and
discuss its relation to previously studied classes such as P/poly and PH. One consequence is that
if PH does not collapse, then 3SAT does not have eﬃcient probabilistic algorithms.
We also show that probabilistic algorithms can be very practical by presenting ways to greatly
reduce their error to absolutely minuscule quantities. Thus the class BPP (and its sister classes
RP, coRP and ZPP) introduced in this chapter are arguably as important as P in capturing
eﬃcient computation. We will also introduce some related notions such as probabilistic logspace
algorithms and probabilistic reductions.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p7.1 (115)

DRAFT
p7.2 (116)
7.1. PROBABILISTIC TURING MACHINES
Though at ﬁrst randomization seems merely a tool to allow simulations of randomized physical
processes, the surprising fact is that in the past three decades randomization has led to more eﬃcient
—and often simpler—algorithms for problems in a host of other ﬁelds—such as combinatorial
optimization, algebraic computation, machine learning, and network routing.
In complexity theory too, the role of randomness extends far beyond a study of randomized
algorithms and classes such as BPP. Entire areas such as cryptography and interactive and prob-
abilistically checkable proofs rely on randomness in an essential way, sometimes to prove results
whose statement did not call for randomness at all. The groundwork for studying those areas will
be laid in this chapter.
In a later chapter, we will learn something intriguing: to some extent, the power of randomness
may be a mirage. If a certain plausible complexity-theoretic conjecture is true (see Chapters 16
and 17), then every probabilistic algorithm can be simulated by a deterministic algorithm (one that
does not use any randomness whatsoever) with only polynomial overhead.
Throughout this chapter and the rest of the book, we will use some notions from elementary
probability on ﬁnite sample spaces; see Appendix A for a quick review.
7.1
Probabilistic Turing machines
We now deﬁne probabilistic Turing machines (PTMs). Syntactically, a PTM is no diﬀerent from a
nondeterministic TM: it is a TM with two transition functions δ0, δ1. The diﬀerence lies in how we
interpret the graph of all possible computations: instead of asking whether there exists a sequence
of choices that makes the TM accept, we ask how large is the fraction of choices for which this
happens. More precisely, if M is a PTM, then we envision that in every step in the computation,
M chooses randomly which one of its transition functions to apply (with probability half applying
δ0 and with probability half applying δ1). We say that M decides a language if it outputs the right
answer with probability at least 2/3.
Notice, the ability to pick (with equal probability) one of δ0, δ1 to apply at each step is equivalent
to the machine having a ”fair coin”, which, each time it is tossed, comes up ”Heads” or ”Tails”
with equal probability regardless of the past history of Heads/Tails. As mentioned, whether or not
such a coin exists is a deep philosophical (or scientiﬁc) question.
Definition 7.1 (The classes BPTIME and BPP)
For T : N →N and L ⊆{0, 1}∗, we say that a PTM M decides L in time T(n), if
for every x ∈{0, 1}∗, M halts in T(|x|) steps regardless of its random choices, and
Pr[M(x) = L(x)] ≥2/3, where we denote L(x) = 1 if x ∈L and L(x) = 0 if x ̸∈L.
We let BPTIME(T(n)) denote the class of languages decided by PTMs in O(T(n))
time and let BPP = ∪cBPTIME(nc).
Remark 7.2
We will see in Section 7.4 that this deﬁnition is quite robust. For instance, the ”coin” need not
be fair. The constant 2/3 is arbitrary in the sense that it can be replaced with any other constant
Web draft 2007-01-08 21:59

DRAFT
7.2. SOME EXAMPLES OF PTMS
p7.3 (117)
greater than half without changing the classes BPTIME(T(n)) and BPP. Instead of requiring
the machine to always halt in polynomial time, we could allow it to halt in expected polynomial
time.
Remark 7.3
While Deﬁnition 7.1 allows the PTM M, given input x, to output a value diﬀerent from L(x)
with positive probability, this probability is only over the random choices that M makes in the
computation. In particular for every input x, M(x) will output the right value L(x) with probability
at least 2/3. Thus BPP, like P, is still a class capturing worst-case computation.
Since a deterministic TM is a special case of a PTM (where both transition functions are equal),
the class BPP clearly contains P. As alluded above, under plausible complexity assumptions it
holds that BPP = P. Nonetheless, as far as we know it may even be that BPP = EXP. (Note
that BPP ⊆EXP, since given a polynomial-time PTM M and input x ∈{0, 1}n in time 2poly(n)
it is possible to enumerate all possible random choices and compute precisely the probability that
M(x) = 1.)
An alternative deﬁnition.
As we did with NP, we can deﬁne BPP using deterministic TMs
where the ”probabilistic choices” to apply at each step can be provided to the TM as an additional
input:
Definition 7.4 (BPP, alternative definition)
BPP contains a language L if there exists a polynomial-time TM M and a polynomial p : N →N
such that for every x ∈{0, 1}∗, Prr∈R{0,1}p(|x|)[M(x, r) = L(x)] ≥2/3.
7.2
Some examples of PTMs
The following examples demonstrate how randomness can be a useful tool in computation. We will
see many more examples in the rest of this book.
7.2.1
Probabilistic Primality Testing
In primality testing we are given an integer N and wish to determine whether or not it is prime.
Generations of mathematicians have learnt about prime numbers and —before the advent of
computers— needed to do primality testing to test various conjectures1.
Ideally, we want eﬃ-
cient algorithms, which run in time polynomial in the size of N’s representation, in other words,
poly(log n). We knew of no such eﬃcient algorithms2 until the 1970s, when an eﬀﬁcient proba-
bilistic algorithm was discovered. This was one of the ﬁrst to demonstrate the power of proba-
bilistic algorithms. In a recent breakthrough, Agrawal, Kayal and Saxena [?] gave a deterministic
polynomial-time algorithm for primality testing.
1Though a very fast human computer himself, Gauss used the help of a human supercomputer –an autistic person
who excelled at fast calculations—to do primality testing.
2In fact, in his letter to von Neumann quoted in Chapter 2, G¨odel explicitly mentioned this problem as an example
for an interesting problem in NP but not known to be eﬃciently solvable.
Web draft 2007-01-08 21:59

DRAFT
p7.4 (118)
7.2. SOME EXAMPLES OF PTMS
Formally, primality testing consists of checking membership in the language PRIMES = { ⌞N⌟: N is a prime num
Notice, the corresponding search problem of ﬁnding the factorization of a given composite number
N seems very diﬀerent and much more diﬃcult. It is the famous FACTORING problem, whose
conjectured hardness underlies many current cryptosystems. Chapter 20 describes Shor’s algorithm
to factors integers in polynomial time in the model of quantum computers.
We sketch an algorithm showing that PRIMES is in BPP (and in fact in coRP). For every
number N, and A ∈[N −1], deﬁne
QRN(A) =











0
gcd(A, N) ̸= 1
+1
A is a quadratic residue modulo N
That is, A = B2
(mod N) for some B with gcd(B, N) = 1
−1
otherwise
We use the following facts that can be proven using elementary number theory:
• For every odd prime N and A ∈[N −1], QRN(A) = A(N−1)/2 (mod N).
• For every odd N, A deﬁne the Jacobi symbol (N
A ) as Qk
i=1 QRPi(A) where P1, . . . , Pk are all
the (not necessarily distinct) prime factors of N (i.e., N = Qk
i=1 Pi). Then, the Jacobi symbol
is computable in time O(log A · log N).
• For every odd composite N,
{A ∈[N −1] : gcd(N, A) = 1 and ( N
A ) = A(N−1)/2}
 ≤1
2
{A ∈
[N −1] : gcd(N, A) = 1}

Together these facts imply a simple algorithm for testing primality of N (which we can assume
without loss of generality is odd): choose a random 1 ≤A < N, if gcd(N, A) > 1 or (N
A ) ̸= A(N−1)/2
(mod N) then output “composite”, otherwise output “prime”. This algorithm will always output
“prime” is N is prime, but if N is composite will output “composite” with probability at least 1/2.
(Of course this probability can be ampliﬁed by repeating the test a constant number of times.)
7.2.2
Polynomial identity testing
So far we described probabilistic algorithms solving problems that have known deterministic poly-
nomial time algorithms. We now describe a problem for which no such deterministic algorithm is
known:
We are given a polynomial with integer coeﬃcients in an implicit form, and we want to decide
whether this polynomial is in fact identically zero. We will assume we get the polynomial in the
form of an arithmetic circuit. This is analogous to the notion of a Boolean circuit, but instead of the
operators ∧, ∨and ¬, we have the operators +, −and ×. Formally, an n-variable arithmetic circuit
is a directed acyclic graph with the sources labeled by a variable name from the set x1, . . . , xn,
and each non-source node has in-degree two and is labeled by an operator from the set {+, −, ×}.
There is a single sink in the graph which we call the output node. The arithmetic circuit deﬁnes
a polynomial from Zn to Z by placing the inputs on the sources and computing the value of
each node using the appropriate operator. We deﬁne ZEROP to be the set of arithmetic circuits
that compute the identically zero polynomial. Determining membership in ZEROP is also called
Web draft 2007-01-08 21:59

DRAFT
7.2. SOME EXAMPLES OF PTMS
p7.5 (119)
polynomial identity testing, since we can reduce the problem of deciding whether two circuits C, C′
compute the same polynomial to ZEROP by constructing the circuit D such that D(x1, . . . , xn) =
C(x1, . . . , xn) −C′(x1, . . . , xn).
Since expanding all the terms of a given arithmetic circuit can result in a polynomial with
exponentially many monomials, it seems hard to decide membership in ZEROP. Surprisingly, there
is in fact a simple and eﬃcient probabilistic algorithm for testing membership in ZEROP. At the
heart of this algorithm is the following fact, typically known as the Schwartz-Zippel Lemma, whose
proof appears in Appendix A (see Lemma A.25):
Lemma 7.5
Let p(x1, x2, . . . , xm) be a polynomial of total degree at most d and S is any ﬁnite set of integers.
When a1, a2, . . . , am are randomly chosen with replacement from S, then
Pr[p(a1, a2, . . . , am) ̸= 0] ≥1 −d
|S|.
Now it is not hard to see that given a size m circuit C on n variables, it deﬁnes a polynomial of
degree at most 2m. This suggests the following simple probabilistic algorithm: choose n numbers
x1, . . . , xn from 1 to 10·2m (this requires O(n·m) random bits), evaluate the circuit C on x1, . . . , xn
to obtain an output y and then accept if y = 0, and reject otherwise. Clearly if C ∈ZEROP then
we always accept. By the lemma, if C ̸∈ZEROP then we will reject with probability at least 9/10.
However, there is a problem with this algorithm. Since the degree of the polynomial represented
by the circuit can be as high as 2m, the output y and other intermediate values arising in the
computation may be as large as (10 · 2m)2m — this is a value that requires exponentially many bits
just to describe!
We solve this problem using the technique of ﬁngerprinting. The idea is to perform the evalu-
ation of C on x1, . . . , xn modulo a number k that is chosen at random in [22m]. Thus, instead of
computing y = C(x1, . . . , xn), we compute the value y (mod k). Clearly, if y = 0 then y (mod k) is
also equal to 0. On the other hand, we claim that if y ̸= 0, then with probability at least δ =
1
10m,
k does not divide y. (This will suﬃce because we can repeat this procedure O(1/δ) times to ensure
that if y ̸= 0 then we ﬁnd this out with probability at lest 9/10.) Indeed, assume that y ̸= 0 and
let S = {p1, . . . , pℓ} denote set of the distinct prime factors of y. It is suﬃcient to show that with
probability at δ, the number k will be a prime number not in S. Yet, by the prime number theorem,
the probability that k is prime is at least
1
5m = 2δ. Also, since y can have at most log y ≤5m2m
distinct factors, the probability that k is in S is less than 5m2m
22m
≪
1
10m = δ. Hence by the union
bound, with probability at least δ, k will not divide y.
7.2.3
Testing for perfect matching in a bipartite graph.
If G = (V1, V2, E) is the bipartite graph where |V1| = |V2| and E ⊆V1 × V2 then a perfect matching
is some E′ ⊆E such that every node appears exactly once among the edges of E′. Alternatively,
we may think of it as a permutation σ on the set {1, 2, . . . , n} (where n = |V1|) such that for
each i ∈{1, 2, . . . , n}, the pair (i, σ(i)) is an edge. Several deterministic algorithms are known for
detecting if a perfect matching exists. Here we describe a very simple randomized algorithm (due
to Lov´asz) using the Schwartz-Zippel lemma.
Web draft 2007-01-08 21:59

DRAFT
p7.6 (120)
7.3. ONE-SIDED AND ZERO-SIDED ERROR: RP, CORP, ZPP
Consider the n × n matrix X (where n = |V1| = |V2|) whose (i, j) entry Xij is the variable xij
if (i, j) ∈E and 0 otherwise. Recall that the determinant of matrix det(X) is
det(X) =
X
σ∈Sn
(−1)sign(σ)
n
Y
i=1
Xi,σ(i),
(1)
where Sn is the set of all permutations of {1, 2, . . . , n}. Note that every permutation is a potential
perfect matching, and the corresponding monomial in det(X) is nonzero iﬀthis perfect matching
exists in G. Thus the graph has a perfect matching iﬀdet(X) ̸= 0.
Now observe two things. First, the polynomial in (1) has |E| variables and total degree at most
n. Second, even though this polynomial may be of exponential size, for every setting of values to
the Xij variables it can be eﬃciently evaluated, since computing the determinant of a matrix with
integer entries is a simple polynomial-time computation (actually, even in NC2).
This leads us to Lov´asz’s randomized algorithm: pick random values for Xij’s from [1, . . . , 2n],
substitute them in X and compute the determinant. If the determinant is nonzero, output “accept”
else output “reject.” The advantage of the above algorithm over classical algorithms is that it can
be implemented by a randomized NC circuit, which means (by the ideas of Section 6.5.1) that it
has a fast implementation on parallel computers.
7.3
One-sided and zero-sided error: RP, coRP, ZPP
The class BPP captured what we call probabilistic algorithms with two sided error. That is, it
allows the machine M to output (with some small probability) both 0 when x ∈L and 1 when
x ̸∈L. However, many probabilistic algorithms have the property of one sided error. For example
if x ̸∈L they will never output 1, although they may output 0 when x ∈L. This is captured by
the deﬁnition of RP.
Definition 7.6
RTIME(t(n)) contains every language L for which there is a is a probabilistic TM M running in
t(n) time such that
x ∈L ⇒Pr[M accepts x] ≥2
3
x ̸∈L ⇒Pr[M accepts x] = 0
We deﬁne RP = ∪c>0RTIME(nc).
Note that RP ⊆NP, since every accepting branch is a “certiﬁcate” that the input is in the
language. In contrast, we do not know if BPP ⊆NP. The class coRP = {L | L ∈RP} captures
one-sided error algorithms with the error in the “other direction” (i.e., may output 1 when x ̸∈L
but will never output 0 if x ∈L).
For a PTM M, and input x, we deﬁne the random variable TM,x to be the running time of M
on input x. That is, Pr[TM,x = T] = p if with probability p over the random choices of M on input
x, it will halt within T steps. We say that M has expected running time T(n) if the expectation
E[TM,x] is at most T(|x|) for every x ∈{0, 1}∗. We now deﬁne PTMs that never err (also called
“zero error” machines):
Web draft 2007-01-08 21:59

DRAFT
7.4. THE ROBUSTNESS OF OUR DEFINITIONS
p7.7 (121)
Definition 7.7
The class ZTIME(T(n)) contains all the languages L for which there is an expected-time O(T(n))
machine that never errs. That is,
x ∈L ⇒Pr[M accepts x] = 1
x ̸∈L ⇒Pr[M halts without accepting on x] = 1
We deﬁne ZPP = ∪c>0ZTIME(nc).
The next theorem ought to be slightly surprising, since the corresponding statement for nonde-
terminism is open; i.e., whether or not P = NP ∩coNP.
Theorem 7.8
ZPP = RP ∩coRP.
We leave the proof of this theorem to the reader (see Exercise 4). To summarize, we have the
following relations between the probabilistic complexity classes:
ZPP =RP ∩coRP
RP ⊆BPP
coRP ⊆BPP
7.4
The robustness of our deﬁnitions
When we deﬁned P and NP, we argued that our deﬁnitions are robust and were likely to be the
same for an alien studying the same concepts in a faraway galaxy. Now we address similar issues
for probabilistic computation.
7.4.1
Role of precise constants, error reduction.
The choice of the constant 2/3 seemed pretty arbitrary. We now show that we can replace 2/3 with
any constant larger than 1/2 and in fact even with 1/2 + n−c for a constant c > 0.
Lemma 7.9
For c > 0, let BPPn−c denote the class of languages L for which there is a polynomial-time PTM
M satisfying Pr[M(x) = L(x)] ≥1/2 + |x|−c for every x ∈{0, 1}∗. Then BPPn−c = BPP.
Since clearly BPP ⊆BPPn−c, to prove this lemma we need to show that we can transform a
machine with success probability 1/2+n−c into a machine with success probability 2/3. We do this
by proving a much stronger result: we can transform such a machine into a machine with success
probability exponentially close to one!
Web draft 2007-01-08 21:59

DRAFT
p7.8 (122)
7.4. THE ROBUSTNESS OF OUR DEFINITIONS
Theorem 7.10 (Error reduction)
Let L ⊆{0, 1}∗be a language and suppose that there exists a polynomial-time PTM
M such that for every x ∈{0, 1}∗, Pr[M(x) = L(x) ≥1
2 + |x|−c.
Then for every constant d > 0 there exists a polynomial-time PTM M′ such that
for every x ∈{0, 1}∗, Pr[M′(x) = L(x)] ≥1 −2−|x|d.
Proof: The machine M′ is quite simple: for every input x ∈{0, 1}∗, run M(x) for k times
obtaining k outputs y1, . . . , yk ∈{0, 1}, where k = 8|x|2d+c. If the majority of these values are 1
then accept, otherwise reject.
To analyze this machine, deﬁne for every i ∈[k] the random variable Xi to equal 1 if yi = L(x)
and to equal 0 otherwise. Note that X1, . . . , Xk are independent Boolean random variables with
E[Xi] = Pr[Xi = 1] ≥1/2 + n−c (where n = |x|). The Chernoﬀbound (see Theorem A.18 in
Appendix A) implies the following corollary:
Corollary 7.11
Let X1, . . . , Xk be independent identically distributed Boolean random variables, with Pr[Xi =
1] = p for every 1 ≤i ≤k. Let δ ∈(0, 1). Then,
Pr


1
k
k
X
i=1
Xi −p
 > δ

< e−δ2
4 pk
In our case p = 1/2+n−c, and plugging in δ = n−c/2, the probability we output a wrong answer
is bounded by
Pr[ 1
n
k
X
i=1
Xi ≤1/2 + n−c/2] ≤e−
1
4n−2c 1
2 8n2c+d ≤2−nd
■
A similar result holds for the class RP. In fact, there we can replace the constant 2/3 with
every positive constant, and even with values as low as n−c. That is, we have the following result:
Theorem 7.12
Let L ⊆{0, 1}∗such that there exists a polynomial-time PTM M satisfying for every x ∈{0, 1}∗:
(1) If x ∈L then Pr[M(x) = 1)] ≥n−c and (2) if x ̸∈L, then Pr[M(x) = 1] = 0.
Then for every d > 0 there exists a polynomial-time PTM M′ such that for every x ∈{0, 1}∗,
(1) if x ∈L then Pr[M′(x) = 1] ≥1 −2−nd and (2) if x ̸∈L then Pr[M′(x) = 1] = 0.
These results imply that we can take a probabilistic algorithm that succeeds with quite modest
probability and transform it into an algorithm that succeeds with overwhelming probability. In
fact, even for moderate values of n an error probability that is of the order of 2−n is so small that
for all practical purposes, probabilistic algorithms are just as good as deterministic algorithms.
If the original probabilistic algorithm used m coins, then the error reduction procedure we use
(run k independent trials and output the majority answer) takes O(m · k) random coins to reduce
the error to a value exponentially small in k. It is somewhat surprising that we can in fact do
better, and reduce the error to the same level using only O(m + k) random bits (see Section 7.5).
Web draft 2007-01-08 21:59

DRAFT
7.4. THE ROBUSTNESS OF OUR DEFINITIONS
p7.9 (123)
Note 7.13 (The Chernoff Bound)
The Chernoﬀbound is extensively used (sometimes under diﬀerent names)
in many areas of computer science and other sciences. A typical scenario is
the following: there is a universe U of objects, a fraction µ of them have a
certain property, and we wish to estimate µ. For example, in the proof of
Theorem 7.10 the universe was the set of 2m possible coin tosses of some
probabilistic algorithm and we wanted to know how many of them cause the
algorithm to accept its input. Another example is that U may be the set of
all the citizens of the United States, and we wish to ﬁnd out how many of
them approve of the current president.
A natural approach to compute the fraction µ is to sample n members of the
universe independently at random, ﬁnd out the number k of the sample’s
members that have the property and to estimate that µ is k/n. Of course,
it may be quite possible that 10% of the population supports the president,
but in a sample of 1000 we will ﬁnd 101 and not 100 such people, and so
we set our goal only to estimate µ up to an error of ±ϵ for some ϵ > 0.
Similarly, even if only 10% of the population have a certain property, we
may be extremely unlucky and select only people having it for our sample,
and so we allow a small probability of failure δ that our estimate will be
oﬀby more than ϵ. The natural question is how many samples do we need
to estimate µ up to an error of ±ϵ with probability at least 1 −δ?
The
Chernoﬀbound tells us that (considering µ as a constant) this number is
O(log(1/δ)/ϵ2).
This implies that if we sample n elements, then the probability that the
number k having the property is ρ√n far from µn decays exponentially with
ρ: that is, this probability has the famous “bell curve” shape:
k
Pr[
]
0
n
0
1
µn
µn+ρn1/2
µn-ρn1/2
k have
property
We will use this exponential decay phenomena several times in this book,
starting with the proof of Theorem 7.17, showing that BPP ⊆P/poly.
Web draft 2007-01-08 21:59

DRAFT
p7.10 (124)
7.4. THE ROBUSTNESS OF OUR DEFINITIONS
7.4.2
Expected running time versus worst-case running time.
When deﬁning RTIME(T(n)) and BPTIME(T(n)) we required the machine to halt in T(n) time
regardless of its random choices. We could have used expected running time instead, as in the
deﬁnition of ZPP (Deﬁnition 7.7). It turns out this yields an equivalent deﬁnition: we can add a
time counter to a PTM M whose expected running time is T(n) and ensure it always halts after
at most 100T(n) steps. By Markov’s inequality (see Lemma A.10), the probability that M runs
for more than this time is at most 1/100. Thus by halting after 100T(n) steps, the acceptance
probability is changed by at most 1/100.
7.4.3
Allowing more general random choices than a fair random coin.
One could conceive of real-life computers that have a “coin” that comes up heads with probability
ρ that is not 1/2. We call such a coin a ρ-coin. Indeed it is conceivable that for a random source
based upon quantum mechanics, ρ is an irrational number, such as 1/e. Could such a coin give
probabilistic algorithms new power? The following claim shows that it will not.
Lemma 7.14
A coin with Pr[Heads] = ρ can be simulated by a PTM in expected time O(1) provided the ith bit
of ρ is computable in poly(i) time.
Proof: Let the binary expansion of ρ be 0.p1p2p3 . . .. The PTM generates a sequence of random
bits b1, b2, . . . , one by one, where bi is generated at step i. If bi < pi then the machine outputs
“heads” and stops; if bi > pi the machine outputs “tails” and halts; otherwise the machine goes
to step i + 1. Clearly, the machine reaches step i + 1 iﬀbj = pj for all j ≤i, which happens with
probability 1/2i. Thus the probability of “heads” is P
i pi 1
2i , which is exactly ρ. Furthermore, the
expected running time is P
i ic · 1
2i . For every constant c this inﬁnite sum is upperbounded by
another constant (see Exercise 1). ■
Conversely, probabilistic algorithms that only have access to ρ-coins do not have less power
than standard probabilistic algorithms:
Lemma 7.15 (Von-Neumann)
A coin with Pr[Heads] = 1/2 can be simulated by a probabilistic TM with access to a stream of
ρ-biased coins in expected time O(
1
ρ(1−ρ)).
Proof: We construct a TM M that given the ability to toss ρ-coins, outputs a 1/2-coin. The
machine M tosses pairs of coins until the ﬁrst time it gets two diﬀerent results one after the other.
If these two results were ﬁrst “heads” and then “tails”, M outputs “heads”. If these two results
were ﬁrst “tails” and then “heads”, M outputs “tails”. For each pair, the probability we get two
“heads” is ρ2, the probability we get two “tails” is (1 −ρ)2, the probability we get “head” and
then“tails” is ρ(1 −ρ), and the probability we get “tails” and then “head” is (1 −ρ)ρ. We see that
the probability we halt and output in each step is 2ρ(1 −ρ), and that conditioned on this, we do
indeed output either “heads” or “tails” with the same probability. Note that we did not need to
know ρ to run this simulation. ■
Web draft 2007-01-08 21:59

DRAFT
7.5. RANDOMNESS EFFICIENT ERROR REDUCTION.
p7.11 (125)
Weak random sources.
Physicists (and philosophers) are still not completely certain that ran-
domness exists in the world, and even if it does, it is not clear that our computers have access to
an endless stream of independent coins. Conceivably, it may be the case that we only have access
to a source of imperfect randomness, that although unpredictable, does not consist of independent
coins. As we will see in Chapter 16, we do know how to simulate probabilistic algorithms designed
for perfect independent 1/2-coins even using such a weak random source.
7.5
Randomness eﬃcient error reduction.
In Section 7.4.1 we saw how we can reduce error of probabilistic algorithms by running them
several time using independent random bits each time. Ideally, one would like to be frugal with
using randomness, because good quality random number generators tend to be slower than the rest
of the computer. Surprisingly, the error reduction can be done just as eﬀectively without using
truly independent runs, and “recycling” the random bits. Now we outline this idea; a much more
general theory will be later presented in Chapter 16.
The main tool we use is expander graphs. Expander graphs have played a crucial role in nu-
merous computer science applications, including routing networks, error correcting codes, hardness
of approximation and the PCP theorem, derandomization, and more. Expanders can be deﬁned
in several roughly equivalent ways. One is that these are graphs where every set of vertices has a
very large boundary. That is, for every subset S of vertices, the number of S’s neighbors outside
S is (up to a constant factor) roughly equal to the number of vertices inside S. (Of course this
condition cannot hold if S is too big and already contains almost all of the vertices in the graph.)
For example, the n by n grid (where a vertex is a pair (i, j) and is connected to the four neighbors
(i ± 1, j ± 1)) is not an expander, as any k by k square (which is a set of size k2) in this graph only
has a boundary of size O(k) (see Figure 7.1).
Expander: no. of S’s neighbors = Omega(|S|)
Grid is not an expander:
no. of S’s neighbors = O(|S|1/2)
Figure 7.1: In a combinatorial expander, every subset S of the vertices that is not too big has at least Ω(|S|)
neighbors outside the set. The grid (and every other planar graph) is not a combinatorial expander as a k × k square
in the grid has only O(k) neighbors outside it.
We will not precisely deﬁne expanders now (but see Section 7.B at the end of the chapter).
However, an expander graph family is a sequence of graphs {GN}N∈N such for every N, GN is an
N-vertex D-degree graph for some constant D. Deep mathematics (and more recently, simpler
mathematics) has been used to construct expander graphs. These constructions yield algorithms
Web draft 2007-01-08 21:59

DRAFT
p7.12 (126)
7.6. BPP ⊆P/POLY
that, given the binary representation of N and an index of a node in GN, can produce the indices
of the D neighbors of this node in poly(log N) time.
We illustrate the error reduction procedure by showing how we transform an RP algorithm that
outputs the right answer with probability 1/2into an algorithm that outputs the right answer with
probability 1 −2−Ω(k). The idea is simple: let x be an input, and suppose we have an algorithm M
using m coins such that if x ∈L then Prr∈R{0,1}m[M(x, r) = 1] ≥1/2 and if x ̸∈L then M(x, r) = 0
for every r. Let N = 2m and let GN be an N-vertex expander family. We use m coins to select a
random vertex v from GN, and then use log Dk coins to take a k −1-step random walk from v on
GN. That is, at each step we choose a random number i in [D] and move from the current vertex
to its ith neighbor. Let v1, . . . , vk be the vertices we encounter along this walk (where v1 = v). We
can treat these vertices as elements of {0, 1}m and run the machine M on input x with all of these
coins. If even one of these runs outputs 1, then output 1. Otherwise, output 0. It can be shown
that if less than half of the r’s cause M to output 0, then the probability that the walk is fully
contained in these “bad” r’s is exponentially small in k.
We see that what we need to prove is the following theorem:
Theorem 7.16
Let G be an expander graph of N vertices and B a subset of G’s vertices of size at most βN, where
β < 1. Then, the probability that a k-vertex random walk is fully contained in B is at most ρk,
where ρ < 1 is a constant depending only on β (and independent of k).
Theorem 7.16 makes intuitive sense, as in an expander graph a constant fraction of the edges
adjacent to vertices of B will have the other vertex in B’s complement, and so it seems that at each
step we will have a constant probability to leave B. However, its precise formulation and analysis
takes some care, and is done at the end of the chapter in Section 7.B.
Intuitively, We postpone the full description of the error reduction procedure and its analysis
to Section 7.B.
7.6
BPP ⊆P/poly
Now we show that all BPP languages have polynomial sized circuits. Together with Theorem ??
this implies that if 3SAT ∈BPP then PH = Σp
2.
Theorem 7.17 (Adleman)
BPP ⊆P/poly.
Proof: Sup-
pose L ∈BPP, then by the alternative deﬁnition of BPP and the error reduction procedure of
Theorem 7.10, there exists a TM M that on inputs of size n uses m random bits and satisﬁes
x ∈L ⇒Prr [M(x, r) accepts ] ≥1 −2−(n+2)
x ̸∈L ⇒Prr [M(x, r) accepts ] ≤2−(n+2)
Web draft 2007-01-08 21:59

DRAFT
7.7. BPP IS IN PH
p7.13 (127)
(Such a machine exists by the error reduction arguments mentioned earlier.)
Say that an r ∈{0, 1}m is bad for an input x ∈{0, 1}n if M(x, r) is an incorrect answer,
otherwise we say its good for x. For every x, at most 2·2m/2(n+2) values of r are bad for x. Adding
over all x ∈{0, 1}n, we conclude that at most 2n × 2m/2(n+1) = 2m/2 strings r are bad for some
x. In other words, at least 2m −2m/2 choices of r are good for every x ∈{0, 1}n. Given a string
r0 that is good for every x ∈{0, 1}n, we can hardwire it to obtain a circuit C (of size at most
quadratic in the running time of M) that on input x outputs M(x, r0). The circuit C will satisfy
C(x) = L(x) for every x ∈{0, 1}n. ■
7.7
BPP is in PH
At ﬁrst glance, BPP seems to have nothing to do with the polynomial hierarchy, so the next
theorem is somewhat surprising.
Theorem 7.18 (Sipser-G´acs)
BPP ⊆Σp
2 ∩Πp
2
Proof: It is enough to prove that BPP ⊆Σp
2 because BPP is closed under complementation (i.e.,
BPP = coBPP).
Suppose L ∈BPP. Then by the alternative deﬁnition of BPP and the error reduction proce-
dure of Theorem 7.10 there exists a polynomial-time deterministic TM M for L that on inputs of
length n uses m = poly(n) random bits and satisﬁes
x ∈L ⇒Prr [M(x, r) accepts ] ≥1 −2−n
x ̸∈L ⇒Prr [M(x, r) accepts ] ≤2−n
For x ∈{0, 1}n, let Sx denote the set of r’s for which M accepts the input pair (x, r). Then
either |Sx| ≥(1 −2−n)2m or |Sx| ≤2−n2m, depending on whether or not x ∈L. We will show how
to check, using two alternations, which of the two cases is true.
Figure 7.2: There are only two possible sizes for the set of r’s such that M(x, r) =Accept: either this set is almost
all of {0, 1}m or a tiny fraction of {0, 1}m. In the former case, a few random “shifts” of this set are quite likely to
cover all of {0, 1}m. In the latter case the set’s size is so small that a few shifts cannot cover {0, 1}m
For k = m
n + 1, let U = {u1, . . . , uk} be a set of k strings in {0, 1}m. We deﬁne GU to be a
graph with vertex set {0, 1}m and edges (r, s) for every r, s such that r = s + ui for some i ∈[k]
Web draft 2007-01-08 21:59

DRAFT
p7.14 (128)
7.8. STATE OF OUR KNOWLEDGE ABOUT BPP
(where + denotes vector addition modulo 2, or equivalently, bitwise XOR). Note that the degree
of GU is k. For a set S ⊆{0, 1}m, deﬁne ΓU(S) to be all the neighbors of S in the graph GU. That
is, r ∈ΓU(S) if there is an s ∈S and i ∈[k] such that r = s + ui.
Claim 1: For every set S ⊆{0, 1}m with |S| ≤2m−n and every set U of size k, it holds that
ΓU(S) ̸= {0, 1}m. Indeed, since ΓU has degree k, it holds that |ΓU(S)| ≤k|S| < 2m.
Claim 2: For every set S ⊆{0, 1}m with |S| ≥(1 −2−n)2m there exists a set U of size k such that
ΓU(S) = {0, 1}m. We show this by the probabilistic method, by proving that for every S, if we
choose U at random by taking k random strings u1, . . . , uk, then Pr[ΓU(S) = {0, 1}m] > 0. Indeed,
for r ∈{0, 1}m, let Br denote the “bad event” that r is not in ΓU(S). Then, Br = ∩i∈[k]Bi
r where
Bi
r is the event that r ̸∈S + ui, or equivalently, that r + ui ̸∈S (using the fact that modulo 2,
a + b = c ⇔a = c + b). Yet, r + ui is a uniform element in {0, 1}m, and so it will be in S with
probability at least 1 −2−n. Since B1
r, . . . , Bk
r are independent, the probability that Br happens is
at most (1 −2−n)k < 2−m. By the union bound, the probability that ΓU(S) ̸= {0, 1}m is bounded
by P
r∈{0,1}m Pr[Br] < 1.
Together Claims 1 and 2 show x ∈L if and only if the following statement is true
∃u1, . . . , uk ∈{0, 1}m ∀r ∈{0, 1}m
k_
i=1
M(x, r ⊕ui)accepts
thus showing L ∈Σ2. ■
7.8
State of our knowledge about BPP
We know that P ⊆BPP ⊆P/poly, and furthermore, that BPP ⊆Σp
2 ∩Πp
2 and so if NP = p
then BPP = P. As mentioned above, there are complexity-theoretic reasons to strongly believe
that BPP ⊆DTIME(2ϵ) for every ϵ > 0, and in fact to reasonably suspect that BPP = P (see
Chapters 16 and 17). However, currently we are not even able to rule out that BPP = NEXP!
Complete problems for BPP?
Though a very natural class, BPP behaves diﬀerently in some ways from other classes we have
seen. For example, we know of no complete languages for it (under deterministic polynomial time
reductions). One reason for this diﬃculty is that the deﬁning property of BPTIME machines is
semantic, namely, that for every string they either accept with probability at least 2/3 or reject
with probability at least 1/3. Given the description of a Turing machine M, testing whether it has
this property is undecidable. By contrast, the deﬁning property of an NDTM is syntactic: given a
string it is easy to determine if it is a valid encoding of an NDTM. Completeness seems easier to
deﬁne for syntactically deﬁned classes than for semantically deﬁned ones. For example, consider
the following natural attempt at a BPP-complete language: L = {⟨M, x⟩: Pr[M(x) = 1] ≥2/3}.
This language is indeed BPP-hard but is not known to be in BPP. In fact, it is not in any level
of the polynomial hierarchy unless the hierarchy collapses. We note that if, as believed, BPP = P,
then BPP does have a complete problem. (One can sidestep some of the above issues by using
promise problems instead of languages, but we will not explore this.)
Web draft 2007-01-08 21:59

DRAFT
7.9. RANDOMIZED REDUCTIONS
p7.15 (129)
Does BPTIME have a hierarchy theorem?
Is BPTIME(nc) contained in BPTIME(n) for some c > 1? One would imagine not, and this
seems as the kind of result we should be able to prove using the tools of Chapter 3. However
currently we are even unable to show that BPTIME(nlog2 n) (say) is not in BPTIME(n). The
standard diagonalization techniques fail, for similar reasons as the ones above. However, recently
there has been some progress on obtaining hierarchy theorem for some closely related classes (see
notes).
7.9
Randomized reductions
Since we have deﬁned randomized algorithms, it also makes sense to deﬁne a notion of random-
ized reduction between two languages. This proves useful in some complexity settings (e.g., see
Chapters 9 and 8).
Definition 7.19
Language A reduces to language B under a randomized polynomial time reduction, denoted A ≤r B,
if there is a probabilistic TM M such that for every x ∈{0, 1}∗, Pr[B(M(x)) = A(x)] ≥2/3.
We note that if A ≤r B and B ∈BPP then A ∈BPP. This alerts us to the possibility that we
could have deﬁned NP-completeness using randomized reductions instead of deterministic reduc-
tions, since arguably BPP is as good as P as a formalization of the notion of eﬃcient computation.
Recall that the Cook-Levin theorem shows that NP may be deﬁned as the set {L : L ≤p 3SAT}.
The following deﬁnition is analogous.
Definition 7.20 (BP · NP)
BP · NP = {L : L ≤r 3SAT}.
We explore the properties of BP·NP in the exercises, including whether or not 3SAT ∈BP·NP.
One interesting application of randomized reductions will be shown in Chapter 9, where we
present a (variant of a) randomized reduction from 3SAT to the solving special case of 3SAT
where we are guaranteed that the formula is either unsatisﬁable or has a single unique satisfying
assignment.
7.10
Randomized space-bounded computation
A PTM is said to work in space S(n) if every branch requires space O(S(n)) on inputs of size n and
terminates in 2O(S(n)) time. Recall that the machine has a read-only input tape, and the work space
only cell refers only to its read/write work tapes. As a PTM it has two transition functions that
are applied with equal probability. The most interesting case is when the work tape has O(log n)
size. The classes BPL and RL are the two-sided error and one-sided error probabilistic analogs of
the class L deﬁned in Chapter 4.
Web draft 2007-01-08 21:59

DRAFT
p7.16 (130)
7.10. RANDOMIZED SPACE-BOUNDED COMPUTATION
Definition 7.21 ([)
The classes BPL and RL] A language L is in BPL if there is an O(log n)-space
probabilistic TM M such that Pr[M(x) = L(x)] ≥2/3.
A language L is in RL if there is an O(log n)-space probabilistic TM M such that
if x ∈L then Pr[M(x) = 1] ≥2/3 and if x ̸∈L then Pr[M(x) = 1] = 0.
The reader can verify that the error reduction procedure described in Chapter 7 can be imple-
mented with only logarithmic space overhead, and hence also in these deﬁnitions the choice of the
precise constant is not signiﬁcant. We note that RL ⊆NL, and thus RL ⊆P. The exercises ask
you to show that BPL ⊆P as well.
One famous RL-algorithm is the algorithm to solve UPATH: the restriction of the NL-complete
PATH problem (see Chapter 4) to undirected graphs. That is, given an n-vertex undirected graph
G and two vertices s and t, determine whether s is connected to t in G.
Theorem 7.22 ([AKL+79])
UPATH ∈RL.
The algorithm for UPATH is actually very simple: take a random walk of length n3 starting
from s. That is, initialize the variable v to the vertex s and in each step choose a random neighbor
u of v, and set v ←u. Accept iﬀthe walk reaches t within n3 steps. Clearly, if s is not connected to
t then the algorithm will never accept. It can be shown that if s is connected to t then the expected
number of steps it takes for a walk from s to hit t is at most 4
27n3 and hence our algorithm will accept
with probability at least 3
4. We defer the analysis of this algorithm to the end of the chapter at
Section 7.A, where we will prove that a somewhat larger walk suﬃces to hit t with good probability
(see also Exercise 9).
In Chapter 16 we show a recent deterministic logspace algorithm for the same problem. It is
known that BPL (and hence also RL) is contained in SPACE(log3/2 n). In Chapter 16 we will
see a somewhat weaker result: a simulation of BPL in log2 n space and polynomial time.
Web draft 2007-01-08 21:59

DRAFT
7.10. RANDOMIZED SPACE-BOUNDED COMPUTATION
p7.17 (131)
What have we learned?
• The class BPP consists of languages that can be solved by a probabilistic
polynomial-time algorithm. The probability is only over the algorithm’s coins
and not the choice of input. It is arguably a better formalization of eﬃcient
computation than P.
• RP, coRP and ZPP are subclasses of BPP corresponding to probabilistic
algorithms with one-sided and “zero-sided” error.
• Using repetition, we can considerably amplify the success probability of prob-
abilistic algorithms.
• We only know that P ⊆BPP ⊆EXP, but we suspect that BPP = P.
• BPP is a subset of both P/poly and PH. In particular, the latter implies
that if NP = P then BPP = P.
• Randomness is used in complexity theory in many contexts beyond BPP. Two
examples are randomized reductions and randomized logspace algorithms, but
we will see many more later.
Chapter notes and history
Early researchers realized the power of randomization since their computations —e.g., for design
of nuclear weapons— used probabilistic tools such as Monte Carlo simulations. Papers by von
Neumann [von61] and de Leeuw et al. [LMSS56] describe probabilistic Turing machines.
The
deﬁnitions of BPP, RP and ZPP are from Gill [Gil77]. (In an earlier conference paper [Gil74],
Gill studies similar issues but seems to miss the point that a practical algorithm for deciding a
language must feature a gap between the acceptance probability in the two cases.)
The algorithm used to show PRIMES is in coRP is due to Solovay and Strassen [SS77]. Another
primality test from the same era is due to Rabin [Rab80]. Over the years, better tests were proposed.
In a recent breakthrough, Agrawal, Kayal and Saxena ﬁnally proved that PRIMES ∈P. Both the
probabilistic and deterministic primality testing algorithms are described in Shoup’s book [?].
Lov´asz’s randomized NC algorithm [Lov79] for deciding the existence of perfect matchings is
unsatisfying in the sense that when it outputs “Accept,” it gives no clue how to ﬁnd a matching!
Subsequent probabilistic NC algorithms can ﬁnd a perfect matching as well; see [KUW86, MVV87].
BPP ⊆P/poly is from Adelman [Adl78]. BPP ⊆PH is due to Sipser [Sip83], and the stronger
form BPP ⊆Σp
2 ∩Πp
2 is due to P. G´acs. Recent work [] shows that BPP is contained in classes
that are seemingly weaker than Σp
2 ∩Πp
2.
Even though a hierarchy theorem for BPP seems beyond our reach, there has been some success
in showing hierarchy theorems for the seemingly related class BPP/1 (i.e., BPP with a single bit
of nonuniform advice) [Bar02, ?, ?].
Web draft 2007-01-08 21:59

DRAFT
p7.18 (132)
7.10. RANDOMIZED SPACE-BOUNDED COMPUTATION
Readers interested in randomized algorithms are referred to the books by Mitzenmacher and
Upfal [MU05] and Motwani and Raghavan [MR95].
still a lot missing
Expanders were well-studied for a variety of reasons in the 1970s but their application to
pseudorandomness was ﬁrst described by Ajtai, Komlos, and Szemeredi [AKS87]. Then Cohen-
Wigderson [CW89] and Impagliazzo-Zuckerman (1989) showed how to use them to “recycle” ran-
dom bits as described in Section 7.B.3. The upcoming book by Hoory, Linial and Wigderson (draft
available from their web pages) provides an excellent introduction to expander graphs and their
applications.
The explicit construction of expanders is due to Reingold, Vadhan and Wigderson [RVW00],
although we chose to present it using the replacement product as opposed to the closely related
zig-zag product used there. The deterministic logspace algorithm for undirected connectivity is due
to Reingold [?].
Exercises
§1 Show that for every c > 0, the following inﬁnite sum is ﬁnite:
X
i≥1
ic
2i .
§2 Show, given input the numbers a, n, p (in binary representation), how to compute an(modp)
in polynomial time.
Hint: use the binary representation of n and repeated squaring.
§3 Let us study to what extent Claim ?? truly needs the assumption that ρ is eﬃciently com-
putable. Describe a real number ρ such that given a random coin that comes up “Heads”
with probability ρ, a Turing machine can decide an undecidable language in polynomial time.
Hint: think of the real number ρ as an advice string. How can its
bits be recovered?
§4 Show that ZPP = RP ∩coRP.
§5 A nondeterministic circuit has two inputs x, y. We say that it accepts x iﬀthere exists y
such that C(x, y) = 1. The size of the circuit is measured as a function of |x|. Let NP/poly
be the languages that are decided by polynomial size nondeterministic circuits. Show that
BP · NP ⊆NP/poly.
§6 Show using ideas similar to the Karp-Lipton theorem that if 3SAT ∈BP · NP then PH
collapses to Σp
3. (Combined with above, this shows it is unlikely that 3SAT ≤r 3SAT.)
§7 Show that BPL ⊆P
Web draft 2007-01-08 21:59

DRAFT
7.10. RANDOMIZED SPACE-BOUNDED COMPUTATION
p7.19 (133)
Hint: try to compute the probability that the machine ends up
in the accept conﬁguration using either dynamic programming or
matrix multiplication.
§8 Show that the random walk idea for solving connectivity does not work for directed graphs.
In other words, describe a directed graph on n vertices and a starting point s such that the
expected time to reach t is Ω(2n) even though there is a directed path from s to t.
§9 Let G be an n vertex graph where all vertices have the same degree.
(a) We say that a distribution p over the vertices of G (where pi denotes the probability
that vertex i is picked by p) is stable if when we choose a vertex i according to p and
take a random step from i (i.e., move to a random neighbor j or i) then the resulting
distribution is p. Prove that the uniform distribution on G’s vertices is stable.
(b) For p be a distribution over the vertices of G, let ∆(p) = maxi{pi −1/n}. For every
k, denote by pk the distribution obtained by choosing a vertex i at random from p and
taking k random steps on G. Prove that if G is connected then there exists k such that
∆(pk) ≤(1 −n−10n)∆(p). Conclude that
i. The uniform distribution is the only stable distribution for G.
ii. For every vertices u, v of G, if we take a suﬃciently long random walk starting from
u, then with high probability the fraction of times we hit the vertex v is roughly
1/n. That is, for every ϵ > 0, there exists k such that the k-step random walk from
u hits v between (1 −ϵ)k/n and (1 + ϵ)k/n times with probability at least 1 −ϵ.
(c) For a vertex u in G, denote by Eu the expected number of steps it takes for a random
walk starting from u to reach back u. Show that Eu ≤10n2.
Hint: consider the inﬁnite random walk starting from u. If Eu >
K then by standard tail bounds, u appears in less than a 2/K
fraction of the places in this walk.
(d) For every two vertices u, v denote by Eu,v the expected number of steps it takes for
a random walk starting from u to reach v. Show that if u and v are connected by a
path of length at most k then Eu,v ≤100kn2. Conclude that for every s and t that are
connected in a graph G, the probability that an 1000n3 random walk from s does not
hit t is at most 1/10.
Hint: Start with the case k = 1 (i.e., u and v are connected by
an edge), the case of k > 1 can be reduced to this using linearity
of expectation. Note that the expectation of a random variable X
over N is equal to P
m∈N Pr[X ≥m] and so it suﬃces to show that
the probability that an ℓn2-step random walk from u does not hit
v decays exponentially with ℓ.
(e) Let G be an n-vertex graph that is not necessarily regular (i.e., each vertex may have
diﬀerent degree). Let G′ be the graph obtained by adding a suﬃcient number of parallel
self-loops to each vertex to make G regular. Prove that if a k-step random walk in G′
from a vertex s hits a vertex t with probability at least 0.9, then a 10n2k-step random
walk from s will hit t with probability at least 1/2.
Web draft 2007-01-08 21:59

DRAFT
p7.20 (134)
7.10. RANDOMIZED SPACE-BOUNDED COMPUTATION
The following exercises are based on Sections 7.A and 7.B.
§10 Let A be a symmetric stochastic matrix: A = A† and every row and column of A has non-
negative entries summing up to one. Prove that ∥A∥≤1.
Hint: ﬁrst show that ∥A∥is at most say n2. Then, prove that for
every k ≥1, Ak is also stochastic and ∥A2kv∥2 ≥∥Akv∥2
2 using the
equality ⟨w, Bz⟩= ⟨B†w, z⟩and the inequality ⟨w, z⟩≤∥w∥2∥z∥2.
§11 Let A, B be two symmetric stochastic matrices. Prove that λ(A + B) ≤λ(A) + λ(B).
§12 Let a n, d random graph be an n-vertex graph chosen as follows: choose d random permuta-
tions π1, ldots, πd from [n] to [n]. Let the the graph G contains an edge (u, v) for every pair
u, v such that v = πi(u) for some 1 ≤i ≤d. Prove that a random n, d graph is an (n, 2d, 2
3d)
combinatorial expander with probability 1 −o(1) (i.e., tending to one with n).
Hint: for every set S ⊆n with |S| ≤n/2 and set T ⊆[n] with
|T| ≤(1 + 2
3d)|S|, try to bound the probability that πi(S) ⊆T for
every i.
Web draft 2007-01-08 21:59

DRAFT
7.A. RANDOM WALKS AND EIGENVALUES
p7.21 (135)
The following two section assume some knowledge of elementary linear algebra (vector spaces and
Hilbert spaces); see Appendix A for a quick review.
7.A
Random walks and eigenvalues
In this section we study random walks on (undirected regular) graphs, introducing several important
notions such as the spectral gap of a graph’s adjacency matrix. As a corollary we obtain the proof
of correctness for the random-walk space-eﬃcient algorithm for UPATH of Theorem 7.22. We will
see that we can use elementary linear algebra to relate parameters of the graph’s adjacency matrix
to the behavior of the random walk on that graph.
Remark 7.23
In this section, we restrict ourselves to regular graphs, in which every vertex have the same degree,
although the deﬁnitions and results can be suitably generalized to general (non-regular) graphs.
7.A.1
Distributions as vectors and the parameter λ(G).
Let G be a d-regular n-vertex graph. Let p be some probability distribution over the vertices of G.
We can think of p as a (column) vector in Rn where pi is the probability that vertex i is obtained
by the distribution. Note that the L1-norm of p (see Note 7.24), deﬁned as |p|1 = Pn
i=1 |pi|, is
equal to 1. (In this case the absolute value is redundant since pi is always between 0 and 1.)
Now let q represent the distribution of the following random variable: choose a vertex i in G
according to p, then take a random neighbor of i in G. We can compute q as a function of p: the
probability qj that j is chosen is equal to the sum over all j’s neighbors i of the probability pi that
i is chosen times 1/d (where 1/d is the probability that, conditioned on i being chosen, the walk
moves to q). Thus q = Ap, where A = A(G) which is the normalized adjacency matrix of G. That
is, for every two vertices i, j, Ai,j is equal to the number of edges between i and j divided by d.
Note that A is a symmetric matrix,3 where each entry is between 0 and 1, and the sum of entries
in each row and column is exactly one (such a matrix is called a symmetric stochastic matrix).
Let {ei}n
i=1 be the standard basis of Rn (i.e. ei has 1 in the ith coordinate and zero everywhere
else). Then, AT es represents the distribution XT of taking a T-step random walk from the vertex
s. This already suggests that considering the adjacency matrix of a graph G could be very useful
in analyzing random walks on G.
Definition 7.25 (The parameter λ(G).)
Denote by 1 the vector (1/n, 1/n, . . . , 1/n) corresponding to the uniform distri-
bution.
Denote by 1⊥the set of vectors perpendicular to 1 (i.e., v ∈1⊥if
⟨v, 1⟩= (1/n) P
i vi = 0).
The parameter λ(A), denoted also as λ(G), is the maximum value of ∥Av∥2 over all
vectors v ∈1⊥with ∥v∥2 = 1.
3A matrix A is symmetric if A = A†, where A† denotes the transpose of A. That is, (A†)i,j = Aj,i for every i, j.
Web draft 2007-01-08 21:59

DRAFT
p7.22 (136)
7.A. RANDOM WALKS AND EIGENVALUES
Note 7.24 (Lp Norms)
A norm is a function mapping a vector v into a real number ∥v∥satisfying
(1) ∥v∥≥0 with ∥v∥= 0 if and only v is the all zero vector, (2) ∥αv∥=
|α| · ∥v∥for every α ∈R, and (3) ∥v + u∥≤∥v∥+ ∥u∥for every vector u.
The third inequality implies that for every norm, if we deﬁne the distance
between two vectors u,v as ∥u −v∥then this notion of distance satisﬁes the
triangle inequality.
For every v ∈Rn and number p ≥1, the Lp norm of v, denoted ∥v∥p, is equal
to (Pn
i=1 |vi|p)1/p. One particularly interesting case is p = 2, the so-called
Euclidean norm, in which ∥v∥2 =
qPn
i=1 v2
i =
p
⟨v, v⟩. Another interesting
case is p = 1, where we use the single bar notation and denote |v|1 =
Pn
i=1 |vi|. Another case is p = ∞, where we denote ∥v∥∞= limp→∞∥v∥p =
maxi∈[n] |vi|.
The H¨older inequality says that for every p, q with 1
p + 1
q = 1, ∥u∥p∥v∥q ≥
Pn
i=1 |uivi|.
To prove it, note that by simple scaling, it suﬃces to con-
sider norm one vectors, and so it enough to show that if ∥u∥p = ∥v∥q = 1
then Pn
i=1 |ui||vi| ≤1.
But Pn
i=1 |ui||vi| = Pn
i=1 |ui|p(1/p)|vi|q(1/q) ≤
Pn
i=1
1
p|ui|p + 1
q|vi|q = 1
p + 1
q = 1, where the last inequality uses the fact
that for every a, b > 0 and α ∈[0, 1], aαb1−α ≤αa + (1 −α)b. This fact is
due to the log function being concave— having negative second derivative,
implying that α log a + (1 −α) log b ≤log(αa + (1 −α)b).
Setting p = 1 and q = ∞, the H¨older inequality implies that
∥v∥2 ≤|v|1∥v∥∞
Setting p
=
q
=
2,
the H¨older inequality becomes the Cauchy-
Schwartz Inequality stating that Pn
i=1 |uivi| ≤∥u∥2∥v∥2.
Setting u =
(1/√n, 1/√n, . . . , 1/√n), we get that
|v|1/√n =
n
X
i=1
1
√n|vi| ≤∥v∥2
Web draft 2007-01-08 21:59

DRAFT
7.A. RANDOM WALKS AND EIGENVALUES
p7.23 (137)
Remark 7.26
The value λ(G) is often called the second largest eigenvalue of G. The reason is that since A is a
symmetric matrix, we can ﬁnd an orthogonal basis of eigenvectors v1, . . . , vn with corresponding
eigenvalues λ1, . . . , λn which we can sort to ensure |λ1| ≥|λ2| . . . ≥|λn|.
Note that A1 = 1.
Indeed, for every i, (A1)i is equal to the inner product of the ith row of A and the vector 1 which
(since the sum of entries in the row is one) is equal to 1/n. Thus, 1 is an eigenvector of A with
the corresponding eigenvalue equal to 1. One can show that a symmetric stochastic matrix has
all eigenvalues with absolute value at most 1 (see Exercise 10) and hence we can assume λ1 = 1
and v1 = 1. Also, because 1⊥= Span{v2, . . . , vn}, the value λ above will be maximized by (the
normalized version of) v2, and hence λ(G) = |λ2|. The quantity 1 −λ(G) is called the spectral gap
of the graph. We note that some texts use un-normalized adjacency matrices, in which case λ(G)
is a number between 0 and d and the spectral gap is deﬁned to be d −λ(G).
One reason that λ(G) is an important parameter is the following lemma:
Lemma 7.27
For every regular n vertex graph G = (V, E) let p be any probability distribution over V , then
∥AT p −1∥2 ≤λT
Proof: By the deﬁnition of λ(G), ∥Av∥2 ≤λ∥v∥2 for every v ⊥1. Note that if v ⊥1 then
Av ⊥1 since ⟨1, Av⟩= ⟨A†1, v⟩= ⟨1, v⟩= 0 (as A = A† and A1 = 1). Thus A maps the space
1⊥to itself and since it shrinks any member of this space by at least λ, λ(AT ) ≤λ(A)T . (In fact,
using the eigenvalue deﬁnition of λ, it can be shown that λ(AT ) = λ(A).)
Let p be some vector. We can break p into its components in the spaces parallel and orthogonal
to 1 and express it as p = α1 + p′ where p′ ⊥1 and α is some number. If p is a probability
distribution then α = 1 since the sum of coordinates in p′ is zero. Therefore,
AT p = AT (1 + p′) = 1 + AT p′
Since 1 and p′ are orthogonal, ∥p∥2
2 = ∥1∥2
2 + ∥p′∥2
2 and in particular ∥p′∥2 ≤∥p∥2. Since p is
a probability vector, ∥p∥2 ≤|p|1 · 1 ≤1 (see Note 7.24). Hence ∥p′∥2 ≤1 and
∥AT p −1∥2 = ∥AT p′∥2 ≤λT
■
It turns out that every connected graph has a noticeable spectral gap:
Lemma 7.28
For every d-regular connected G with self-loops at each vertex, λ(G) ≤1 −
1
8dn3 .
Proof: Let u ⊥1 be a unit vector and let v = Au. We’ll show that 1−∥v∥2
2 ≥
1
d4n3 which implies
∥v∥2
2 ≤1 −
1
d4n3 and hence ∥v∥2 ≤1 −
1
d8n3 .
Web draft 2007-01-08 21:59

DRAFT
p7.24 (138)
7.A. RANDOM WALKS AND EIGENVALUES
Since ∥u∥2 = 1, 1 −∥v∥2
2 = ∥u∥2
2 −∥v∥2
2. We claim that this is equal to P
i,j Ai,j(ui −vj)2
where i, j range from 1 to n. Indeed,
X
i,j
Ai,j(ui −vj)2 =
X
i,j
Ai,ju2
i −2
X
i,j
Ai,juivj +
X
i,j
Ai,jv2
j =
∥u∥2
2 −2⟨Au, v⟩+ ∥v∥2
2 = ∥u∥2
2 −2∥v∥2
2 + ∥v∥2
2 ,
where these equalities are due to the sum of each row and column in A equalling one, and because
∥v∥2
2 = ⟨v, v⟩= ⟨Au, v⟩= P
i,j Ai,juivj.
Thus it suﬃces to show P
i,j Ai,j(ui −vj)2 ≥
1
d4n3 . This is a sum of non-negative terms so it
suﬃces to show that for some i, j, Ai,j(ui −vj)2 ≥
1
d4n3 . First, because we have all the self-loops,
Ai,i ≥1/d for all i, and so we can assume |ui −vi| <
1
2n1.5 for every i ∈[n], as otherwise we’d be
done.
Now sort the coordinates of u from the largest to the smallest, ensuring that u1 ≥u2 ≥· · · un.
Since P
i ui = 0 it must hold that u1 ≥0 ≥un. In fact, since u is a unit vector, either u1 ≥1/√n
or un ≤1/√n and so u1−un ≥1/√n. One of the n−1 diﬀerences between consecutive coordinates
ui −ui+1 must be at least 1/n1.5 and so there must be an i0 such that if we let S = {1, . . . , i0}
and S = [n] \ Si, then for every i ∈S and j ∈S, ui −uj ≥1/n1.5. Since G is connected there
exists an edge (i, j) between S and S. Since |vj −uj| ≤
1
2n1.5 , for this choice of i, j, |ui −vj| ≥
|ui −uj| −
1
2n1.5 ≥
1
2n1.5 . Thus Ai,j(ui −vj)2 ≥1
d
1
4n3 . ■
Remark 7.29
The proof can be strengthened to show a similar result for every connected non-bipartite graph
(not just those with self-loops at every vertex). Note that this condition is essential: if A is the
adjacency matrix of a bipartite graph then one can ﬁnd a vector v such that Av = −v.
7.A.2
Analysis of the randomized algorithm for undirected connectivity.
Together, Lemmas 7.27 and 7.28 imply that, at least for regular graphs, if s is connected to t then
a suﬃciently long random walk from s will hit t in polynomial time with high probability.
Corollary 7.30
Let G be a d-regular n-vertex graph with all vertices having a self-loop. Let s be a vertex in G.
Let T > 10dn3 log n and let XT denote the distribution of the vertex of the T th step in a random
walk from s. Then, for every j connected to s, Pr[XT = j] >
1
2n.
Proof: By these Lemmas, if we consider the restriction of an n-vertex graph G to the connected
component of s, then for every probability vector p over this component and T ≥10dn3 log n,
∥AT p −1∥2 <
1
2n1.5 (where 1 here is the uniform distribution over this component). Using the
relations between the L1 and L2 norms (see Note 7.24), |AT p −1|1 <
1
2n and hence every element
in the connected component appears in AT p with at least 1/n −1/(2n) ≥1/(2n) probability. ■
Note that Corollary 7.30 implies that if we repeat the 10dn3 log n walk for 10n times (or equiv-
alently, if we take a walk of length 100dn4 log n) then we will hit t with probability at least 3/4.
Web draft 2007-01-08 21:59

DRAFT
7.B. EXPANDER GRAPHS.
p7.25 (139)
7.B
Expander graphs.
Expander graphs are extremely useful combinatorial objects, which we will encounter several times
in the book. They can be deﬁned in two equivalent ways. At a high level, these two equivalent
deﬁnitions can be described as follows:
• Combinatorial deﬁnition: A constant-degree regular graph G is an expander if for every subset
S of less than half of G’s vertices, a constant fraction of the edges touching S are from S to
its complement in G. This is the deﬁnition alluded to in Section 7.5 (see Figure 7.1).4
• Algebraic expansion: A constant-degree regular graph G is an expander if its parameter λ(G)
bounded away from 1 by some constant. That is, λ(G) ≤1 −ϵ for some constant ϵ > 0¿
What do we mean by a constant? By constant we refer to a number that is independent of the size
of the graph. We will typically talk about graphs that are part of an inﬁnite family of graphs, and
so by constant we mean a value that is the same for all graphs in the family, regardless of their
size.
Below we make the deﬁnitions more precise, and show their equivalence. We will then complete
the analysis of the randomness eﬃcient error reduction procedure described in Section 7.5.
7.B.1
The Algebraic Deﬁnition
.
The Algebraic deﬁnition of expanders is as follows:
Definition 7.31 ((n, d, λ)-graphs.)
If G is an n-vertex d-regular G with λ(G) ≤λ for some number λ < 1 then we say
that G is an (n, d, λ)-graph.
A family of graphs {Gn}n∈N is an expander graph family if there are some constants
d ∈N and λ < 1 such that for every n, Gn is an (n, d, λ)-graph.
Explicit constructions.
We say that an expander family {Gn}n∈N is explicit if there is a
polynomial-time algorithm that on input 1n with n ∈I outputs the adjacency matrix of Gn.
We say that the family is strongly explicit if there is a polynomial-time algorithm that for every
n ∈I on inputs ⟨n, v, i⟩where 1 ≤v ≤n′ and 1 ≤i ≤d outputs the ith neighbor of v. (Note that
the algorithm runs in time polynomial in the its input length which is polylogarithmic in n.)
As we will see below it is not hard to show that expander families exist using the probabilistic
method. But this does not yield explicit (or very explicit) constructions of such graphs (which, as
we saw in Section 7.4.1 are often needed for applications). In fact, there are also several explicit and
4The careful reader might note that there we said that a graph is an expander if a constant fraction of S’s
neighboring vertices are outside S. However, for constant-degree graphs these two notions are equivalent.
Web draft 2007-01-08 21:59

DRAFT
p7.26 (140)
7.B. EXPANDER GRAPHS.
Note 7.33 (Explicit construction of pseudorandom objects)
Expanders are one instance of a recurring theme in complexity theory (and
other areas of math and computer science): it is often the case that a ran-
dom object can be easily proven to satisfy some nice property, but the ap-
plications require an explicit object satisfying this property. In our case,
a random d-regular graph is an expander, but to use it for, say, reducing
the error of probabilistic algorithms, we need an explicit construction of an
expander family, with an eﬃcient deterministic algorithm to compute the
neighborhood relations. Such explicit constructions can be sometimes hard
to come by, but are often surprisingly useful. For example, in our case the
explicit construction of expander graphs turns out to yield a deterministic
logspace algorithm for undirected connectivity.
We will see another instance of this theme in Chapter 17, which discusses
error correcting codes.
strongly explicit constructions of expander graphs known. The smallest λ can be for a d-regular
n-vertex graph is Ω( 1
√
d) and there are constructions meeting this bound (speciﬁcally the bound
is (1 −o(1))2√d−1
d
where by o(1) we mean a function that tends to 0 as the number of vertices
grows; graphs meeting this bound are called Ramanujan graphs). However, for most applications in
Computer Science, any family with constant d and λ < 1 will suﬃce (see also Remark 7.32 below).
Some of these constructions are very simple and eﬃcient, but their analysis is highly non-trivial
and uses relatively deep mathematics.5 In Chapter 16 we will see a strongly explicit construction
of expanders with elementary analysis. This construction also introduces a tool that is useful to
derandomize the random-walk algorithm for UPATH.
Remark 7.32
One reason that the particular constants of an expander family are not extremely crucial is that we
can improve the constant λ (make it arbitrarily smaller) at the expense of increasing the degree:
this follows from the fact, observed above in the proof of Lemma 7.27, that λ(GT ) = λ(G)T , where
GT denotes the graph obtained by taking the adjacency matrix to the T th power, or equivalently,
having an edge for every length-T path in G. Thus, we can transform an (n, d, λ) graph into an
(n, dT , λT )-graph for every T ≥1. In Chapter 16 we will see a diﬀerent transformation called
the replacement product to decrease the degree at the expense of increasing λ somewhat (and also
increasing the number of vertices).
5An example for such an expander is the following 3-regular graph: the vertices are the numbers 1 to p −1 for
some prime p, and each number x is connected to x + 1,x −1 and x−1
(mod p).
Web draft 2007-01-08 21:59

DRAFT
7.B. EXPANDER GRAPHS.
p7.27 (141)
7.B.2
Combinatorial expansion and existence of expanders.
We describe now a combinatorial criteria that is roughly equivalent to Deﬁnition 7.31. One ad-
vantage of this criteria is that it makes it easy to prove that a non-explicit expander family exists
using the probabilistic method. It is also quite useful in several applications.
Definition 7.34 ([)
Combinatorial (edge) expansion] An n-vertex d-regular graph G = (V, E) is called
an (n, d, ρ)-combinatorial expander if for every subset S ⊆V with |S| ≤n/2,
|E(S, S)| ≥ρd|S|, where for subsets S, T of V , E(S, T) denotes the set of edges
(s, t) with s ∈S and t ∈T.
Note that in this case the bigger ρ is the better the expander. We’ll loosely use the term expander
for any (n, d, ρ)-combinatorial expander with c a positive constant. Using the probabilistic method,
one can prove the following theorem: (Exercise 12 asks you to prove a slightly weaker version)
Theorem 7.35 (Existence of expanders)
Let ϵ > 0 be some constant. Then there exists d = d(ϵ) and N ∈N such that for every n > N
there exists an (n, d, 1 −ϵ)-combinatorial expander.
The following theorem related combinatorial expansion with our previous Deﬁnition 7.31
Theorem 7.36 (Combinatorial and algebraic expansion)
1. If G is an (n, d, λ)-graph then it is an (n, d, (1−λ)/2)-combinatorial expander.
2. If G is an (n, d, ρ)-combinatorial expander then it is an (n, d, 1 −ρ2
2 )-graph.
The ﬁrst part of Theorem 7.36 follows by plugging T = S into the following lemma:
Lemma 7.37 (Expander Mixing Lemma)
Let G = (V, E) be an (n, d, λ)-graph. Let S, T ⊆V , then
|E(S, T)| −d
n|S||T|
 ≤λd
p
|S||T|
Proof: Let s denote the vector such that si is equal to 1 if i ∈S and equal to 0 otherwise, and let
t denote the corresponding vector for the set S. Thinking of s as a row vector and of t as a column
vector, the Lemma’s statement is equivalent to
sAt −|S||T|
n
 ≤λ
p
|S||T| ,
(2)
where A is G’s normalized adjacency matrix. Yet by Lemma 7.40, we can write A as (1−λ)J +λC,
where J is the matrix with all entries equal to 1/n and C has norm at most one. Hence,
sAt = (1 −λ)sJt + λsCt ≤|S||T|
n
+ λ
p
|S||T| ,
Web draft 2007-01-08 21:59

DRAFT
p7.28 (142)
7.B. EXPANDER GRAPHS.
where the last inequality follows from sJt = |S||T|/n and sCt = ⟨s, Ct⟩≤∥s∥2∥t∥2 =
p
|S||T|. ■
Proof of second part of Theorem 7.36.:
We prove a slightly relaxed version, replacing
the constant 2 with 8. Let G = (V, E) be an n-vertex d-regular graph such that for every subset
S ⊆V with |S| ≤n/2, there are ρ|S| edges between S and S = V \ S, and let A be G’s normalized
adjacency matrix.
Let λ = λ(G). We need to prove that λ ≤1 −ρ2/8. Using the fact that λ is the second
eigenvalue of A, there exists a vector u ⊥1 such that Au = λu. Write u = v + w where v is equal
to u on the coordinates on which u is positive and equal to 0 otherwise, and w is equal to u on the
coordinates on which u is negative, and equal to 0 otherwise. Note that, since u ⊥1, both v and
w are nonzero. We can assume that u is nonzero on at most n/2 of its coordinates (as otherwise
we can take −u instead of u).
Since Au = λu and ⟨v, w⟩= 0,
⟨Av, v⟩+ ⟨Aw, v⟩= ⟨A(v + w), v⟩= ⟨Au, v⟩= ⟨λ(v + w), v⟩= λ∥v∥2
2 .
Since ⟨Aw, v⟩is negative, we get that ⟨Av, v⟩/∥v∥2
2 ≥λ or
1 −λ ≥1 −⟨Av, v⟩
∥v∥2
2
= ∥v∥2
2 −⟨Av, v⟩
∥v∥2
2
=
P
i,j Ai,j(vi −vj)2
2∥v∥2
2
,
where the last equality is due to P
i,j Ai,j(vi −vj)2 = P
i,j Ai,jv2
i −2 P
i,j Ai,jvivj + P
i,j Ai,jv2
j =
2∥v∥2
2 −2⟨Av, v⟩. (We use here the fact that each row and column of A sums to one.) Multiply
both numerator and denominator by P
i,j Ai,j(v2
i + v2
j). By the Cauchy-Schwartz inequality,6 we
can bound the new numerator as follows:

X
i,j
Ai,j(vi −vj)2



X
i,j
Ai,j(vi + vj)2

≤

X
i,j
Ai,j(vi −vj)(vi + vj)


2
.
Hence, using (a −b)(a + b) = a2 −b2,
1 −λ ≥
P
i,j Ai,j(v2
i −v2
j)
2
2∥v∥2
2
P
i,j Ai,j(vi + vj)2 =
P
i,j Ai,j(v2
i −v2
j)
2
2∥v∥2
2
P
i,j Ai,jv2
i + 2 P
i,j Ai,jvivj + P
i,j Ai,jv2
j
 =
P
i,j Ai,j(v2
i −v2
j)
2
2∥v∥2
2
 2∥v∥2
2 + 2⟨Av, v⟩
 ≥
P
i,j Ai,j(v2
i −v2
j)
2
8∥v∥4
2
,
where the last inequality is due to A having matrix norm at most 1, implying ⟨Av, v⟩≤∥v∥2
2. We
conclude the proof by showing that
X
i,j
Ai,j(v2
i −v2
j) ≥ρ∥v∥2
2 ,
(3)
6The Cauchy-Schwartz inequality is typically stated as saying that for x, y ∈Rn, P
i xiyi ≤
p
(P
i x2
i )(P
i y2
i ).
However, it is easily generalized to show that for every non-negative µ1, . . . , µn, P
i µixiyi ≤
p
(P
i µix2
i )(P
i µiy2
i )
(this can be proven from the standard Cauchy-Schwartz by multiplying each coordinate of x and y by √µi. It is this
variant that we use here with the Ai,j’s playing the role of µ1, . . . , µn.
Web draft 2007-01-08 21:59

DRAFT
7.B. EXPANDER GRAPHS.
p7.29 (143)
which indeed implies that 1 −λ ≥
ρ2∥v∥4
2
8∥v∥4
2 = ρ2
8 .
To prove (3) sort the coordinates of v so that v1 ≥v2 ≥· · · ≥vn (with vi = 0 for i > n/2).
Then
X
i,j
Ai,j(v2
i −v2
j) ≥
n/2
X
i=1
n
X
j=i+1
Ai,j(v2
i −v2
i+1) =
n/2
X
i=1
ci(v2
i −v2
i+1) ,
where ci denotes P
j>i Ai,j. But ci is equal to the number of edges in G from the set {k : k ≤i} to
its complement, divided by d. Hence, by the expansion of G, ci ≥ρi, implying (using the fact that
vi = 0 for i ≥n/2):
X
i,j
Ai,j(v2
i −v2
j) ≥
n/2
X
i=1
ρi(v2
i −v2
i+1) =
n/2
X
i=1
(ρiv2
i −ρ · (i −1)v2
i ) = ρ∥v∥2
2 ,
establishing (3). ■
7.B.3
Error reduction using expanders.
We now complete the analysis of the randomness eﬃcient error reduction procedure described in
Section 7.5. Recall, that this procedure was the following: let N = 2m where m is the number of
coins the randomized algorithm uses. We use m + O(k) random coins to select a k-vertex random
walk in an expander graph GN, and then output 1 if and only if the algorithm outputs 1 when given
one of the vertices in the walk as random coins. To show this procedure works we need to show
that if the probabilistic algorithm outputs 1 for at least half of the coins, then the probability that
all the vertices of the walk correspond to coins on which the algorithm outputs 0 is exponentially
small in k. This will be a direct consequence of the following theorem: (think of the set B below
as the set of vertices corresponding to coins on which the algorithm outputs 0)
Theorem 7.38 (Expander walks)
Let G be an (N, d, λ) graph, and let B ⊆[N] be a set with |B| ≤βN. Let X1, . . . , Xk
be random variables denoting a k−1-step random walk from X1, where X1 is chosen
uniformly in [N]. Then,
Pr[∀1≤i≤kXi ∈B]
(∗)
≤((1 −λ)
p
β + λ)k−1
Note that if λ and β are both constants smaller than 1 then so is the expression (1 −λ)√β + λ.
Proof: For 1 ≤i ≤k, let Bi be the event that Xi ∈B. Note that the probability (∗) we’re trying
to bound is Pr[B1] Pr[B2|B1] · · · Pr[Bk|B1, . . . , Bk−1].
Let pi ∈RN be the vector representing
the distribution of Xi, conditioned on the events B1, . . . , Bi. Denote by ˆB the following linear
transformation from Rn to Rn: for every u ∈RN, and j ∈[N], ( ˆBu)j = uj if j ∈B and ( ˆBu)j = 0
otherwise.
It’s not hard to verify that p1 =
1
Pr[B1] ˆB1 (recall that 1 = (1/N, . . . , 1/N) is the
Web draft 2007-01-08 21:59

DRAFT
p7.30 (144)
7.B. EXPANDER GRAPHS.
vector representing the uniform distribution over [N]). Similarly, p2 =
1
Pr[B2|B1]
1
Pr[B1] ˆBA ˆB1 where
A = A(G) is the adjacency matrix of G. Since every probability vector p satisﬁes |p|1 = 1,
(∗) = |( ˆBA)k−1 ˆB1|1
We bound this norm by showing that
∥( ˆBA)k−1 ˆB1∥2 ≤((1−λ)√β+λ)k−1
√
N
(4)
which suﬃces since for every v ∈RN, |v|1 ≤
√
N∥v∥2 (see Note 7.24).
To prove (4), we use the following deﬁnition and lemma:
Definition 7.39 (Matrix Norm)
If A is an m by n matrix, then ∥A∥is the maximum number α such that ∥Av∥2 ≤α∥v∥2 for every
v ∈Rn.
Note that if A is a normalized adjacency matrix then ∥A∥= 1 (as A1 = 1 and ∥Av∥2 ≤∥v∥2
for every v). Also note that the matrix norm satisﬁes that for every two n by n matrices A, B,
∥A + B∥≤∥A∥+ ∥B∥and ∥AB∥≤∥A∥∥B∥.
Lemma 7.40
Let A be a normalized adjacency matrix of an (n, d, λ)-graph G. Let J be the adjacency matrix of
the n-clique with self loops (i.e., Ji,j = 1/n for every i, j). Then
A = (1 −λ)J + λC
(5)
where ∥C∥≤1.
Note that for every probability vector p, Jp is the uniform distribution, and so this lemma
tells us that in some sense, we can think of a step on a (n, d, λ)-graph as going to the uniform
distribution with probability 1 −λ, and to a diﬀerent distribution with probability λ. This is of
course not completely accurate, as a step on a d-regular graph will only go the one of the d neighbors
of the current vertex, but we’ll see that for the purposes of our analysis, the condition (5) will be
just as good.7
Proof of Lemma 7.40: Indeed, simply deﬁne C = 1
λ(A −(1 −λ)J). We need to prove ∥Cv∥2 ≤
∥v∥2 for very v. Decompose v as v = u + w where u is α1 for some α and w ⊥1, and ∥v∥2
2 =
∥u∥2
2 + ∥w∥2
2. Since A1 = 1 and J1 = 1 we get that Cu =
1
λ(u −(1 −λ)u) = u. Now, let
w′ = Aw. Then ∥w′∥2 ≤λ∥w∥2 and, as we saw in the proof of Lemma 7.27, w′ ⊥1. Furthermore,
since the sum of the coordinates of w is zero, Jw = 0. We get that Cw = 1
λw′. Since w′ ⊥u,
∥Cw∥2
2 = ∥u + 1
λw′∥2
2 = ∥u∥2
2 + ∥1
λw′∥2
2 ≤∥u∥2
2 + ∥w∥2
2 = ∥w∥2
2. ■
Returning to the proof of Theorem 7.38, we can write ˆBA = ˆB
 (1 −λ)J + λC

, and hence
∥ˆBA∥≤(1 −λ)∥ˆBJ∥+ λ∥ˆBC∥. Since J’s output is always a vector of the form α1, ∥ˆBJ∥≤√β.
Also, because ˆB is an operation that merely zeros out some parts of its input, ∥ˆB∥≤1 implying
7Algebraically, the reason (5) is not equivalent to going to the uniform distribution in each step with probability
1 −λ is that C is not necessarily a stochastic matrix, and may have negative entries.
Web draft 2007-01-08 21:59

DRAFT
7.B. EXPANDER GRAPHS.
p7.31 (145)
∥ˆBC∥≤1. Thus, ∥ˆBA∥≤(1−λ)√β +λ. Since B1 has the value 1/N in |B| places, ∥B1∥2 =
√β
√
N ,
and hence ∥( ˆBA)k−1 ˆB1∥2 ≤((1 −λ)√β + λ)k−1
√β
√
N , establishing (4). ■
One can obtain a similar error reduction procedure for two-sided error algorithms by running
the algorithm using the k sets of coins obtained from a k −1 step random walk and deciding on the
output according to the majority of the values obtained. The analysis of this procedure is based
on the following theorem, whose proof we omit:
Theorem 7.41 (Expander Chernoff Bound [?])
Let G be an (N, d, λ)-graph and B ⊆[N] with |B| = βN. Let X1, . . . , Xk be random
variables denoting a k −1-step random walk in G (where X1 is chosen uniformly).
For every i ∈[k], deﬁne Bi to be 1 if Xi ∈B and 0 otherwise. Then, for every δ > 0,
Pr
h
|
Pk
i=1 Bi
k
−β| > δ
i
< 2e(1−λ)δ2k/60
Web draft 2007-01-08 21:59

DRAFT
p7.32 (146)
7.B. EXPANDER GRAPHS.
Web draft 2007-01-08 21:59

DRAFT
Chapter 8
Interactive proofs
“What is intuitively required from a theorem-proving procedure?
First, that it is
possible to “prove” a true theorem. Second, that it is impossible to “prove” a false
theorem. Third, that communicating the proof should be eﬃcient, in the following
sense.
It does not matter how long must the prover compute during the proving
process, but it is essential that the computation required from the veriﬁer is easy.”
Goldwasser, Micali, Rackoﬀ1985
The standard notion of a mathematical proof follows the certiﬁcate deﬁnition of NP. That is,
to prove that a statement is true one provides a sequence of symbols that can be written down in a
book or on paper, and a valid sequence exists only for true statements. However, people often use
more general ways to convince one another of the validity of statements: they interact with one
another, with the person verifying the proof (henceforth the veriﬁer) asking the person providing
it (henceforth the prover) for a series of explanations before he is convinced.
It seems natural to try to understand the power of such interactive proofs from the complexity-
theoretic perspective. For example, can one prove that a given formula is not satisﬁable? (recall
that is this problem is coNP-complete, it’s not believed to have a polynomial-sized certiﬁcate).
The surprising answer is yes. Indeed, interactive proofs turned out to have unexpected powers
and applications. Beyond their philosophical appeal, interactive proofs led to fundamental insights
in cryptographic protocols, the power of approximation algorithms, program checking, and the
hardness of famous “elusive” problems (i.e., NP-problems not known to be in P nor to be NP-
complete) such as graph isomorphism and approximate shortest lattice vector.
8.1
Warmup: Interactive proofs with a deterministic veriﬁer
Let us consider what happens when we introduce interaction into the NP scenario. That is, we’d
have an interrogation-style proof system where rather than the prover send a written proof to the
veriﬁer, the prover and veriﬁer interact with the veriﬁer asking questions and the prover responding,
where at the end the veriﬁer decides whether or not to accept the input. Of course, both veriﬁer
and prover can keep state during the interaction, or equivalently, the message a party sends at any
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p8.1 (147)

DRAFT
p8.2 (148)
8.1. WARMUP: INTERACTIVE PROOFS WITH A DETERMINISTIC VERIFIER
point in the interaction can be a function of all messages sent and received so far. Formally, we
make the following deﬁnition:
Definition 8.1 (Interaction of deterministic functions)
Let f, g : {0, 1}∗→{0, 1}∗be functions. A k-round interaction of f and g on input x ∈{0, 1}∗,
denoted by ⟨f, g⟩(x) is the sequence of the following strings a1, . . . , ak ∈{0, 1}∗deﬁned as follows:
(1)
a1 = f(x)
a2 = g(x, a1)
. . .
a2i+1 = f(x, a1, . . . , a2i)
a2i+2 = g(x, a1, . . . , a2i+1)
(Where we consider a suitable encoding of i-tuples of strings to strings.)
The output of f (resp. g) at the end of the interaction denoted outf⟨f, g⟩(x) (resp. outg⟨f, g⟩(x)
) is deﬁned to be f(x, a1, . . . , ak) (resp. g(x, a1, . . . , ak)).
Definition 8.2 (Deterministic proof systems)
We say that a language L has a k-round deterministic interactive proof system if there’s a deter-
ministic TM V that on input x, a1, . . . , ai runs in time polynomial in |x|, satisfying:
(Completeness)x ∈L ⇒
∃P : {0, 1}∗→{0, 1}∗outV ⟨V, P⟩(x) = 1
(Soundness)x ̸∈L ⇒
∀P : {0, 1}∗→{0, 1}∗outV ⟨V, P⟩(x) = 1
The class dIP contains all languages with a k(n)-round deterministic interactive proof systems
with k(n) polynomial in n.
It turns out this actually does not change the class of languages we can prove:
Theorem 8.3
dIP = NP.
Proof: Clearly, every NP language has a 1-round proof system. Now we prove that if a L has
an interactive proof system of this type then L ∈NP. The certiﬁcate for membership is just the
transcript (a1, a2, . . . , ak) causing the veriﬁer to accept. To verify this transcript, check that indeed
V (x) = a1, V (x, a1, a2) = a3, . . ., and V (x, a1, . . . , ak) = 1. If x ∈L then there indeed exists such
a transcript. If there exists such a transcript (a1, . . . , ak) then we can deﬁne a prover function P to
satisfy P(x, a1) = a2, P(x, a1, a2, a3) = a4, etc. We see that outV ⟨V, P⟩(x) = 1 and hence x ∈L.
■
Web draft 2007-01-08 21:59

DRAFT
8.2. THE CLASS IP
p8.3 (149)
8.2
The class IP
In order to realize the full potential of interaction, we need to let the veriﬁer be probabilistic. The
idea is that, similar to probabilistic algorithms, the veriﬁer will be allowed to come to a wrong
conclusion (e.g., accept a proof for a wrong statement) with some small probability. However, as in
the case of probabilistic algorithms, this probability is over the veriﬁer’s coins and the veriﬁer will
reject proofs for a wrong statement with good probability regardless of the strategy the prover uses.
It turns out that the combination of interaction and randomization has a huge eﬀect: as we will
see, the set of languages which have interactive proof systems now jumps from NP to PSPACE.
Example 8.4
As an example for a probabilistic interactive proof system, consider the following scenario: Marla
claims to Arthur that she can distinguish between the taste of Coke (Coca-Cola) and Pepsi. To
verify this statement, Marla and Arthur repeat the following experiment 50 times: Marla turns her
back to Arthur, as he places Coke in one unmarked cup and Pepsi in another, choosing randomly
whether Coke will be in the cup on the left or on the right. Then Marla tastes both cups and states
which one contained which drinks. While, regardless of her tasting abilities, Marla can answer
correctly with probability 1
2 by a random guess, if she manages to answer correctly for all the 50
repetitions, Arthur can indeed be convinced that she can tell apart Pepsi and Coke.
To formally deﬁne this we extend the notion of interaction to probabilistic functions (actually,
we only need to do so for the veriﬁer).
To model an interaction between f and g where f is
probabilistic, we add an additional m-bit input r to the function f in (1), that is having a1 = f(x, r),
a3 = f(x, r, a1, a2), etc. The interaction ⟨f, g⟩(x) is now a random variable over r ∈R {0, 1}m.
Similarly the output outf⟨f, g⟩(x) is also a random variable.
Definition 8.5 (IP)
Let k : N →N be some function with k(n) computable in poly(n) time. A language L is in IP[k]
if there is a Turing machine V such that on inputs x, r, a1, . . . , ai, V runs in time polynomial in |x|
and such that
(Completeness)
x ∈L ⇒∃P Pr[outV ⟨V, P⟩(x) = 1] ≥2/3
(2)
(Soundness)
x ̸∈L ⇒∀P Pr[outV ⟨V, P⟩(x) = 1] ≤1/3.
(3)
We deﬁne IP = ∪c≥1IP[nc].
Remark 8.6
The following observations on the class IP are left as an exercise (Exercise 1).
1. Allowing the prover to be probabilistic (i.e., the answer function ai depends upon some
random string used by the prover) does not change the class IP. The reason is that for
any language L, if a probabilistic prover P results in making veriﬁer V accept with some
probability, then averaging implies there is a deterministic prover which makes V accept with
the same probability.
Web draft 2007-01-08 21:59

DRAFT
p8.4 (150)
8.3. PROVING THAT GRAPHS ARE NOT ISOMORPHIC.
Figure unavailable in pdf ﬁle.
Figure 8.1: Two isomorphic graphs.
2. Since the prover can use an arbitrary function, it can in principle use unbounded computa-
tional power (or even compute undecidable functions). However, one can show that given any
veriﬁer V , we can compute the optimum prover (which, given x, maximizes the veriﬁer’s ac-
ceptance probability) using poly(|x|) space (and hence 2poly(|x|) time). Thus IP ⊆PSPACE.
3. The probabilities of correctly classifying an input can be made arbitrarily close to 1 by using
the same boosting technique we used for BPP (see Section ??): to replace 2/3 by 1−exp(−m),
sequentially repeat the protocol m times and take the majority answer. In fact, using a more
complicated proof, it can be shown that we can decrease the probability without increasing the
number of rounds using parallel repetition (i.e., the prover and veriﬁer will run m executions
of the protocol in parallel). We note that the proof is easier for the case of public coin proofs,
which will be deﬁned below.
4. Replacing the constant 2/3 in the completeness requirement (2) by 1 does not change the
class IP. This is a nontrivial fact. It was originally proved in a complicated way but today
can be proved using our characterization of IP later in Section 8.5.
5. In contrast replacing the constant 2/3 by 1 in the soundness condition (3) is equivalent to
having a deterministic veriﬁer and hence reduces the class IP to NP.
6. We emphasize that the prover functions do not depend upon the veriﬁer’s random strings,
but only on the messages/questions the veriﬁer sends. In other words, the veriﬁer’s random
string is private. (Often these are called private coin interactive proofs.) Later we will also
consider the model where all the veriﬁer’s questions are simply obtained by tossing coins and
revealing them to the prover (this is known as public coins or Arthur-Merlin proofs).
8.3
Proving that graphs are not isomorphic.
We’ll now see an example of a language in IP that is not known to be in NP. Recall that the usual
ways of representing graphs —adjacency lists, adjacency matrices— involve a numbering of the
vertices. We say two graphs G1 and G2 are isomorphic if they are the same up to a renumbering
of vertices. In other words, if there is a permutation π of the labels of the nodes of G1 such that
π(G1) = G2. The graphs in ﬁgure ??, for example, are isomorphic with π = (12)(3654). (That is,
1 and 2 are mapped to each other, 3 to 6, 6 to 5, 5 to 4 and 4 to 1.) If G1 and G2 are isomorphic,
we write G1 ≡G2. The GI problem is the following: given two graphs G1, G2 (say in adjacency
matrix representation) decide if they are isomorphic. Note that clearly GI ∈NP, since a certiﬁcate
is simply the description of the permutation π.
The graph isomorphism problem is important in a variety of ﬁelds and has a rich history (see
[?]). Along with the factoring problem, it is the most famous NP-problem that is not known to be
Web draft 2007-01-08 21:59

DRAFT
8.4. PUBLIC COINS AND AM
p8.5 (151)
either in P or NP-complete. The results of this section show that GI is unlikely to be NP-complete,
unless the polynomial hierarchy collapses. This will follow from the existence of the following proof
system for the complement of GI: the problem GNI of deciding whether two given graphs are not
isomorphic.
Protocol: Private-coin Graph Non-isomorphism
V : pick i ∈{1, 2} uniformly randomly. Randomly permute the vertices of Gi to get a
new graph H. Send H to P.
P: identify which of G1, G2 was used to produce H. Let Gj be that graph. Send j to V .
V : accept if i = j; reject otherwise.
To see that Deﬁnition 8.5 is satisﬁed by the above protocol, note that if G1 ̸≡G2 then there exists
a prover such that Pr[V accepts] = 1, because if the graphs are non-isomorphic, an all-powerful
prover can certainly tell which one of the two is isomorphic to H. On the other hand, if G1 ≡G2
the best any prover can do is to randomly guess, because a random permutation of G1 looks exactly
like a random permutation of G2. Thus in this case for every prover, Pr[V accepts] ≤1/2. This
probability can be reduced to 1/3 by sequential or parallel repetition.
8.4
Public coins and AM
Allowing the prover full access to the veriﬁer’s random string leads to the model of interactive
proofs with public-coins.
Definition 8.7 (AM, MA)
For every k we denote by AM[k] the class of languages that can be decided by a k round interactive
proof in which each veriﬁer’s message consists of sending a random string of polynomial length,
and these messages comprise of all the coins tossed by the veriﬁer. A proof of this form is called a
public coin proof (it is sometimes also known an Arthur Merlin proof).1
We deﬁne by AM the class AM[2].2 That is, AM is the class of languages with an interactive
proof that consist of the veriﬁer sending a random string, the prover responding with a message,
and where the decision to accept is obtained by applying a deterministic polynomial-time function
to the transcript. The class MA denotes the class of languages with 2-round public coins interactive
proof with the prover sending the ﬁrst message. That is, L ∈MA if there’s a proof system for L
that consists of the prover ﬁrst sending a message, and then the veriﬁer tossing coins and applying
a polynomial-time predicate to the input, the prover’s message and the coins.
1Arthur was a famous king of medieval England and Merlin was his court magician. Babai named these classes
by drawing an analogy between the prover’s inﬁnite power and Merlin’s magic. One “justiﬁcation” for this model
is that while Merlin cannot predict the coins that Arthur will toss in the future, Arthur has no way of hiding from
Merlin’s magic the results of the coins he tossed in the past.
2Note that AM = AM[2] while IP = IP[poly]. While this is indeed somewhat inconsistent, this is the standard
notation used in the literature. We note that some sources denote the class AM[3] by AMA, the class AM[4] by
AMAM etc.
Web draft 2007-01-08 21:59

DRAFT
p8.6 (152)
8.4. PUBLIC COINS AND AM
Note that clearly for every k, AM[k] ⊆IP[k]. The interactive proof for GNI seemed to crucially
depend upon the fact that P cannot see the random bits of V . If P knew those bits, P would know
i and so could trivially always guess correctly. Thus it may seem that allowing the veriﬁer to keep
its coins private adds signiﬁcant power to interactive proofs, and so the following result should be
quite surprising:
Theorem 8.8 ([GS87])
For every k : N →N with k(n) computable in poly(n),
IP[k] ⊆AM[k + 2]
The central idea of the proof of Theorem 8.8 can be gleaned from the proof for the special case
of GNI.
Theorem 8.9
GNI ∈AM[k] for some constant k ≥2.
The key idea in the proof of Theorem 8.9 is to look at graph nonisomorphism in a diﬀerent,
more quantitative, way. (Aside: This is a good example of how nontrivial interactive proofs can be
designed by recasting the problem.) Consider the set S = {H : H ≡G1 or H ≡G2}. Note that it
is easy to prove that a graph H is a member of S, by providing the permutation mapping either
G1 or G2 to H. The size of this set depends on whether G1 is isomorphic to G2. An n vertex graph
G has at most n! equivalent graphs. If G1 and G2 have each exactly n! equivalent graphs (this will
happen if for i = 1, 2 there’s no non-identity permutation π such that π(Gi) = Gi) we’ll have that
if G1 ̸≡G2 then |S| = 2n!
(4)
if G1 ≡G2 then |S| = n!
(5)
(To handle the general case that G1 or G2 may have less than n! equivalent graphs, we actually
change the deﬁnition of S to
S = {(H, π) : H ≡G1 or H ≡G2 and π ∈aut(H)}
where π ∈aut(H) if π(H) = H. It is clearly easy to prove membership in the set S and it can be
veriﬁed that S satisﬁes (4) and (5).)
Thus to convince the veriﬁer that G1 ̸≡G2, the prover has to convince the veriﬁer that case (4)
holds rather than (5). This is done by using a set lower bound protocol.
8.4.1
Set Lower Bound Protocol.
In a set lower bound protocol, the prover proves to the veriﬁer that a given set S (where membership
in S is eﬃciently veriﬁable) has cardinality at least K up to accuracy of, say, factor of 2. That
is, if |S| ≥K then the prover can cause the veriﬁer to accept with high probability, while if
|S| ≤K/2 then the veriﬁer will reject with high probability, no matter what the prover does. By
the observations above, such a protocol suﬃces to complete the proof of Theorem 8.9.
Web draft 2007-01-08 21:59

DRAFT
8.4. PUBLIC COINS AND AM
p8.7 (153)
Tool: Pairwise independent hash functions.
The main tool we use for the set lower bound protocol is a pairwise independent hash function
collection. This is a simple but incredibly useful tool that has found numerous applications in
complexity theory and computer science at large (see Note 8.13).
Definition 8.10 (Pairwise independent hash functions)
Let Hn,k be a collection of functions from {0, 1}n to {0, 1}k. We say that Hn,k is
pairwise independent if for every x, x′ ∈{0, 1}n with x ̸= x′ and for every y, y′ ∈
{0, 1}k, Prh∈RHn,k[h(x) = y ∧h(x′) = y′] = 2−2n
Note that an equivalent formulation is that for every two distinct strings x, x′ ∈{0, 1}n the
random variable ⟨h(x), h(x′)⟩for h chosen at random from Hn,k is distributed according to the
uniform distribution on {0, 1}k × {0, 1}k.
Recall that we can identify the elements of {0, 1}n with the ﬁnite ﬁeld (see Section A.4 in
the appendix), denoted GF(2n), containing 2n elements, whose addition (+) and multiplication (·)
operations satisfy the usual commutative and distributive laws, where and every element x has an
additive inverse (denoted by −x) and, if nonzero, a multiplicative inverse (denoted by x−1). The
following theorem provides a construction of an eﬃciently computable pairwise independent hash
functions (see also Exercise 4 for a diﬀerent construction):
Theorem 8.11 (Efficient pairwise independent hash functions)
For every n deﬁne the collection Hn,n to be {ha,b}a,b∈GF(2n) where for every a, b ∈GF(2n), the func-
tion ha,b : GF(2n) →GF(2n) maps x to ax + b. Then, Hn,n is a collection of pairwise independent
hash functions.
Remark 8.12
Theorem 8.11 implies the existence of an eﬃciently computable pairwise independent hash functions
Hn,k for every n, k: if k > n we can use the collection Hk,k and reduce the size of the input to n
by padding it with zeros. If k < n then we can use the collection Hn,n and truncate the last n −k
bits of the output.
Proof: For every x ̸= x′ ∈GF(2n) and y, y′ ∈GF(2n), ha,b(x) = y and ha,b(x′) = y′ iﬀa, b satisfy
the equations:
a · x + b =y
a · x′ + b =y′
These imply a · (x −x′) = y −y′ or a = (y −y′)(x −x′)−1. Since b = y −a · x, the pair ⟨a, b⟩is
completely determined by these equations, and so the probability that this happens over the choice
of a, b is exactly one over the number of possible pairs, which indeed equals
1
22n . ■
Web draft 2007-01-08 21:59

DRAFT
p8.8 (154)
8.4. PUBLIC COINS AND AM
Note 8.13 (The Hashing paradigm)
A hash function collection is a collection of functions mapping a large uni-
verse, say {0, 1}n, to a smaller universe, say {0, 1}k for k ≪n. Typically, we
require of such a collection that it maps its input in a fairly uniform way to
the output range. For example, if S is a subset of {0, 1}n then we wish that,
if h is chosen at random from the collection, then most elements of {0, 1}k
have roughly |S|2−k preimages in S (which is the expected number if h was
a completely random function). In particular, if S has size roughly 2k then
we expect the mapping to be one-to-one or almost one-to-one, and so there
should be a relatively small number of collisions: pairs x ̸= x′ ∈S such that
h(x) = h(x′). Therefore, the image of S under h should look like this:
{0,1}n
{0,1}k
|S|~2k
2n-k
h
......
In databases, hash functions are used to maintain very eﬃcient databases
(that allow fast membership queries to a subset S ⊆{0, 1}n of size 2k re-
quiring only 2k as opposed to 2n bits of storage). In theoretical computer
science, hash functions have a variety of uses. An example is Lemma 9.16
of the next chapter that shows that if the collection is pairwise independent
and S ⊆{0, 1}n has size roughly 2k, then with good probability the value
0k will have exactly one preimage in S.
In all these cases it is important that the hash function is chosen at random
from some collection independently of the choice of set S. It is easy to see
that if k is small enough (e.g., k < n/2) then for every h : {0, 1}n →{0, 1}k
there is a set S ⊆{0, 1}n of size 2k that is “very bad” for h in the sense that
all the members of S map to the same element under h.
Pairwise independent hash functions are but one example of a hash func-
tion collection. Several types of such collections are known in the literature
featuring various tradeoﬀs between eﬃciency and uniformity of output.
Web draft 2007-01-08 21:59

DRAFT
8.4. PUBLIC COINS AND AM
p8.9 (155)
The lower-bound protocol.
The lower-bound protocol is as follows:
Protocol: Goldwasser-Sipser Set Lowerbound
Conditions: S ⊆{0, 1}m is a set such that membership in S can be certiﬁed. Both
parties know a number K. The prover’s goal is to convince the veriﬁer that |S| ≥K
and the veriﬁer should reject if |S| ≤K
2 . Let k be a number such that 2k−2 ≤K ≤
2k−1.
V: Randomly pick a function h : {0, 1}m →{0, 1}k from a pairwise independent hash
function collection Hm,k. Pick y ∈R {0, 1}k. Send h, y to prover.
P: Try to ﬁnd an x ∈S such that h(x) = y. Send such an x to V , together with a
certiﬁcate that x ∈S.
V’s output: If certiﬁcate validates that x ∈S and h(x) = y, accept; otherwise reject.
Let p = K
2k . If |S| ≤K
2 then clearly |h(S)| ≤p2k
2
and so the veriﬁer will accept with probability
at most p
2.
The main challenge is to show that if |S| ≥K then the veriﬁer will accept with
probability noticeably larger than p/2 (the gap between the probabilities can then be ampliﬁed
using repetition). That is, it suﬃces to prove
Claim 8.13.1
Let S ⊆{0, 1}m satisfy |S| ≤2k
2 . Then,
Pr
h∈RHm,k,y∈R{0,1}k[∃x∈Sh(x) = y] ≥3
4
|S|
2k .
Proof: For every y ∈{0, 1}m, we’ll prove the claim by showing that
Pr
h∈RHm,k
[∃x∈Sh(x) = y] ≥3
4p ,
(where p = |S|/2k). Indeed, for every x ∈S deﬁne the event Ex to hold if h(x) = y. Then,
Pr[∃x∈Sh(x) = y] = Pr[∪x∈SEx] but by the inclusion-exclusion principle this is at least
X
x∈S
Pr[Ex] −1
2
X
x̸=x′∈§
Pr[Ex ∩E′
x]
However, by pairwise independence, if x ̸= x′, then Pr[Ex] = 2−k and Pr[Ex ∩E′
x] = 2−2k and so
this probability is at least
|S|
2k −1
2
|S|2
2k = |S|
2k

1 −|S|
2k+1

≥3
4p
■
Web draft 2007-01-08 21:59

DRAFT
p8.10 (156)
8.4. PUBLIC COINS AND AM
Figure unavailable in pdf ﬁle.
Figure 8.2: AM[k] looks like Qp
k, with the ∀quantiﬁer replaced by probabilitic choice.
Proving Theorem 8.9.
The public-coin interactive proof system for GNI consists of the veriﬁer
and prover running several iterations of the set lower bound protocol for the set S as deﬁned
above, where the veriﬁer accepts iﬀthe fraction of accepting iterations was at least 0.6p (note that
both parties can compute p). Using the Chernoﬀbound (Theorem A.18) it can be easily seen
that a constant number of iteration will suﬃces to ensure completeness probability at least 2
3 and
soundness error at most 1
3. ■
Remark 8.14
How does this protocol relate to the private coin protocol of Section 8.3?
The set S roughly
corresponds to the set of possible messages sent by the veriﬁer in the protocol, where the veriﬁer’s
message is a random element in S. If the two graphs are isomorphic then the veriﬁer’s message
completely hides its choice of a random i ∈R {1, 2}, while if they’re not then it completely reveals it
(at least to a prover that has unbounded computation time). Thus roughly speaking in the former
case the mapping from the veriﬁer’s coins to the message is 2-to-1 while in the latter case it is
1-to-1, resulting in a set that is twice as large. Indeed we can view the prover in the public coin
protocol as convincing the veriﬁer that its probability of convincing the private coin veriﬁer is large.
While there are several additional intricacies to handle, this is the idea behind the generalization
of this proof to show that IP[k] ⊆AM[k + 2].
Remark 8.15
Note that, unlike the private coins protocol, the public coins protocol of Theorem 8.9 does not enjoy
perfect completeness, since the set lowerbound protocol does not satisfy this property. However,
we can construct a perfectly complete public-coins set lowerbound protocol (see Exercise 3), thus
implying a perfectly complete public coins proof for GNI. Again, this can be generalized to show that
any private-coins proof system (even one not satisfying perfect completeness) can be transformed
into a perfectly complete public coins system with a similar number of rounds.
8.4.2
Some properties of IP and AM
We state the following properties of IP and AM without proof:
1. (Exercise 5) AM[2] = BP · NP where BP · NP is the class in Deﬁnition ??. In particular it
follows thatAM[2] ⊆Σp
3.
2. (Exercise 4) For constants k ≥2 we have AM[k] = AM[2]. This “collapse” is somewhat
surprising because AM[k] at ﬁrst glance seems similar to PH with the ∀quantiﬁers changed
to “probabilistic ∀” quantiﬁers, where most of the branches lead to acceptance. See Figure 8.2.
3. It is open whether there is any nice characterization of AM[σ(n)], where σ(n) is a suitably
slow growing function of n, such as log log n.
Web draft 2007-01-08 21:59

DRAFT
8.5. IP = PSPACE
p8.11 (157)
8.4.3
Can GI be NP-complete?
We now prove that if GI is NP-complete then the polynomial hierarchy collapses.
Theorem 8.16 ([?])
If GI is NP-complete then Σ2 = Π2.
Proof: If GI is NP-complete then GNI is coNP-complete which implies that there exists a function
f such that for every n variable formula ϕ, ∀yϕ(y) holds iﬀf(ϕ) ∈GNI. Let
ψ = ∃x∈{0,1}n∀y∈{0,1}nϕ(x, y)
be a Σ2SAT formula. We have that ψ is equivalent to
∃x∈{0,1}ng(x) ∈GNI
where g(x) = f(ϕ↾x).
Using Remark 8.15 and the comments of Section 8.4.2, we have that GNI has a two round AM
proof with perfect completeness and (after appropriate ampliﬁcation) soundness error less than
2−n. Let V be the veriﬁer algorithm for this proof system, and denote by m the length of the
veriﬁer’s random tape and by m′ the length of the prover’s message and . We claim that ψ is
equivalent to
ψ∗= ∀r∈{0,1}m′∃x∈{0,1}n∃a∈{0,1}mV (g(x), r, a) = 1
Indeed, by perfect completeness if ψ is satisﬁable then ψ∗is satisﬁable.
If ψ is not satisﬁable
then by the fact that the soundness error is at most 2−n, we have that there exists a single string
r ∈{0, 1}m such that for every x with g(x) ̸∈GNI, there’s no a such that V (g(x), r, a) = 1, and so
ψ∗is not satisﬁable. Since ψ∗can easily be reduced to a Π2SAT formula, we get that Σ2 ⊆Π2,
implying (since Σ2 = coΠ2) that Σ2 = Π2. ■
8.5
IP = PSPACE
In this section we show a surprising characterization of the set of languages that have interactive
proofs.
Theorem 8.17 (LFKN, Shamir, 1990)
IP = PSPACE.
Note that this is indeed quite surprising: we already saw that interaction alone does not increase
the languages we can prove beyond NP, and we tend to think of randomization as not adding
signiﬁcant power to computation (e.g., we’ll see in Chapter 16 that under reasonable conjectures,
BPP = P). As noted in Section 8.4.2, we even know that languages with constant round interactive
proofs have a two round public coins proof, and are in particular contained in the polynomial
hierarchy, which is believed to be a proper subset of PSPACE. Nonetheless, it turns out that the
combination of suﬃcient interaction and randomness is quite powerful.
Web draft 2007-01-08 21:59

DRAFT
p8.12 (158)
8.5. IP = PSPACE
By our earlier Remark 8.6 we need only show the direction PSPACE ⊆IP. To do so, we’ll show
that TQBF ∈IP[poly(n)]. This is suﬃcient because every L ∈PSPACE is polytime reducible to
TQBF. We note that our protocol for TQBF will use public coins and also has the property that if
the input is in TQBF then there is a prover which makes the veriﬁer accept with probability 1.
Rather than tackle the job of designing a protocol for TQBF right away, let us ﬁrst think about
how to design one for 3SAT. How can the prover convince the veriﬁer than a given 3CNF formula
has no satisfying assignment? We show how to prove something even more general: the prover can
prove to the veriﬁer what the number of satisfying assignments is. (In other words, we will design
a prover for #SAT.) The idea of arithmetization introduced in this proof will also prove useful in
our protocol for TQBF.
8.5.1
Arithmetization
The key idea will be to take an algebraic view of boolean formulae by representing them as polyno-
mials. Note that 0, 1 can be thought of both as truth values and as elements of some ﬁnite ﬁeld F.
Thus we have the following correspondence between formulas and polynomials when the variables
take 0/1 values:
x ∧y
←→
X · Y
¬x
←→
1 −X
x ∨y
←→
1 −(1 −X)(1 −Y )
x ∨y ∨¬z
←→
1 −(1 −X)(1 −Y )Z
Given any 3CNF formula ϕ(x1, x2, . . . , xn) with m clauses, we can write such a degree 3 polyno-
mial for each clause. Multiplying these polynomials we obtain a degree 3m multivariate polynomial
Pϕ(X1, X2, . . . , Xn) that evaluates to 1 for satisfying assignments and evaluates to 0 for unsatis-
fying assignments. (Note: we represent such a polynomial as a multiplication of all the degree 3
polynomials without “opening up” the parenthesis, and so Pϕ(X1, X2, . . . , Xn) has a representation
of size O(m).) This conversion of ϕ to Pϕ is called arithmetization. Once we have written such
a polynomial, nothing stops us from going ahead and and evaluating the polynomial when the
variables take arbitrary values from the ﬁeld F instead of just 0, 1. As we will see, this gives the
veriﬁer unexpected power over the prover.
8.5.2
Interactive protocol for #SATD
To design a protocol for 3SAT we give a protocol for #SATD, which is a decision version of the
counting problem #SAT we saw in Chapter ??:
#SATD = {⟨φ, K⟩: K is the number of satisfying assignments of φ} .
and φ is a 3CNF formula of n variables and m clauses.
Theorem 8.18
#SATD ∈IP.
Web draft 2007-01-08 21:59

DRAFT
8.5. IP = PSPACE
p8.13 (159)
Proof: Given input ⟨φ, K⟩, we construct, by arithmetization, Pφ. The number of satisfying as-
signments #φ of φ is:
#φ =
X
b1∈{0,1}
X
b2∈{0,1}
· · ·
X
bn∈{0,1}
Pφ(b1, . . . , bn)
(6)
To start, the prover sends to the veriﬁer a prime p in the interval (2n, 22n]. The veriﬁer can check
that p is prime using a probabilistic or deterministic primality testing algorithm. All computations
described below are done in the ﬁeld F = Fp of numbers modulo p. Note that since the sum in (6)
is between 0 and 2n, this equation is true over the integers iﬀit is true modulo p. Thus, from now
on we consider (6) as an equation in the ﬁeld Fp. We’ll prove the theorem by showing a general
protocol, Sumcheck, for verifying equations such as (6).
Sumcheck protocol.
Given a degree d polynomial g(X1, . . . , Xn), an integer K, and a prime p, we present an interactive
proof for the claim
K =
X
b1∈{0,1}
X
b2∈{0,1}
· · ·
X
bn∈{0,1}
g(X1, . . . , Xn)
(7)
(where all computations are modulo p). To execute the protocol V will need to be able to evaluate
the polynomial g for any setting of values to the variables. Note that this clearly holds in the case
g = Pφ.
For each sequence of values b2, b3, . . . , bn to X2, X3, . . . , Xn, note that g(X1, b2, b3, . . . , bn) is a
univariate degree d polynomial in the variable X1. Thus the following is also a univariate degree d
polynomial:
h(X1) =
X
b2∈{0,1}
· · ·
X
bn∈{0,1}
g(X1, b2 . . . , bn)
If Claim (7) is true, then we have h(0) + h(1) = K.
Consider the following protocol:
Protocol: Sumcheck protocol to check claim (7)
V: If n = 1 check that g(1) + g(0) = K. If so accept, otherwise reject. If n ≥2, ask P
to send h(X1) as deﬁned above.
P: Sends some polynomial s(X1) (if the prover is not “cheating” then we’ll have s(X1) =
h(X1)).
V: Reject if s(0) + s(1) ̸= K; otherwise pick a random a. Recursively use the same
protocol to check that
s(a) =
X
b∈{0,1}
· · ·
X
bn∈{0,1}
g(a, b2, . . . , bn).
Web draft 2007-01-08 21:59

DRAFT
p8.14 (160)
8.5. IP = PSPACE
If Claim (7) is true, the prover that always returns the correct polynomial will always convince
V . If (7) is false then we prove that V rejects with high probability:
Pr[V rejects ⟨K, g⟩] ≥

1 −d
p
n
.
(8)
With our choice of p, the right hand side is about 1 −dn/p, which is very close to 1 since d ≤n3
and p ≫n4.
Assume that (7) is false. We prove (8) by induction on n. For n = 1, V simply evaluates
g(0), g(1) and rejects with probability 1 if their sum is not K. Assume the hypothesis is true for
degree d polynomials in n −1 variables.
In the ﬁrst round, the prover P is supposed to return the polynomial h. If it indeed returns
h then since h(0) + h(1) ̸= K by assumption, V will immediately reject (i.e., with probability 1).
So assume that the prover returns some s(X1) diﬀerent from h(X1). Since the degree d nonzero
polynomial s(X1) −h(X1) has at most d roots, there are at most d values a such that s(a) = h(a).
Thus when V picks a random a,
Pr
a [s(a) ̸= h(a)] ≥1 −d
p.
(9)
If s(a) ̸= h(a) then the prover is left with an incorrect claim to prove in the recursive step.
By the induction hypothesis, the prover fails to prove this false claim with probability at least
≥

1 −d
p
n−1
. Thus we have
Pr[V rejects] ≥

1 −d
p

·

1 −d
p
n−1
=

1 −d
p
n
(10)
This ﬁnishes the induction.
■
8.5.3
Protocol for TQBF: proof of Theorem 8.17
We use a very similar idea to obtain a protocol for TQBF. Given a quantiﬁed Boolean formula
Ψ = ∃x1∀x2∃x3 · · · ∀xnφ(x1, . . . , xn), we use arithmetization to construct the polynomial Pφ. We
have that Ψ ∈TQBF if and only if
0 <
X
b1∈{0,1}
Y
b2∈{0,1}
X
b3∈{0,1}
· · ·
Y
bn∈{0,1}
Pφ(b1, . . . , bn)
(11)
A ﬁrst thought is that we could use the same protocol as in the #SATD case, except check that
s(0)·s(1) = K when you have a Q. But, alas, multiplication, unlike addition, increases the degree of
the polynomial — after k steps, the degree could be 2k. Such polynomials may have 2k coeﬃcients
and cannot even be transmitted in polynomial time if k ≫log n.
The solution is to look more closely at the polynomials that are are transmitted and their relation
to the original formula. We’ll change Ψ into a logically equivalent formula whose arithmetization
Web draft 2007-01-08 21:59

DRAFT
8.6. THE POWER OF THE PROVER
p8.15 (161)
does not cause the degrees of the polynomials to be so large. The idea is similar to the way circuits
are reduced to formulas in the Cook-Levin theorem: we’ll add auxiliary variables. Speciﬁcally, we’ll
change ψ to an equivalent formula ψ′ that is not in prenex form in the following way: work from
right to left and whenever encountering a ∀quantiﬁer on a variable xi — that is, when considering
a postﬁx of the form ∀xiτ(x1, . . . , xi), where τ may contain quantiﬁers over additional variables
xi+1, . . . , xn — ensure that the variables x1, . . . , xi never appear to the right of another ∀quantiﬁer
in τ by changing the postﬁx to ∀xi∃x′
1, . . . , x′
i(x′
1 = x1)∧· · ·∧(x′
i = xi)∧τ(x1, . . . , xn). Continuing
this way we’ll obtain the formula ψ′ which will have O(n2) variables and will be at most O(n2)
larger than ψ. It can be seen that the natural arithmetization for ψ′ will lead to the polynomials
transmitted in the sumcheck protocol never having degree more than 2.
Note that the prover needs to prove that the arithmetization of Ψ′ leads to a number K diﬀerent
than 0, but because of the multiplications this number can be as large as 22n. Nevertheless the
prover can ﬁnd a prime p between 0 and 2n such that K mod p ̸= 0 (in fact as we saw in Chapter 7
a random prime will do). This ﬁnishes the proof of Theorem 8.17. ■
Remark 8.19
An alternative way to obtain the same result (or, more accurately, an alternative way to describe
the same protocol) is to notice that for x ∈{0, 1}, xk = x for all k ≥1. Thus, in principle we can
convert any polynomial p(x1, . . . , xn) into a multilinear polynomial q(x1, . . . , xn) (i.e., the degree of
q(·) in any variable xi is at most one) that agrees with p(·) on all x1, . . . , xn ∈{0, 1}. Speciﬁcally,
for any polynomial p(·) let Li(p) be the polynomial deﬁned as follows
Li(p)(x1, . . . , xn) = xiP(x1, . . . , xi−1, 1, xi+1, . . . , xn)+
(1 −xi)P(x1, . . . , xi−1, 0, xi+1, . . . , xn)
(12)
then L1(L2(· · · (Ln(p) · · · ) is such a multilinear polynomial agreeing with p(·) on all values in {0, 1}.
We can thus use O(n2) invocations operator to convert (11) into an equivalent form where all the
intermediate polynomials sent in the sumcheck protocol are multilinear. We’ll use this equivalent
form to run the sumcheck protocol, where in addition to having round for a P or Q operator,
we’ll also have a round for each application of the operator L (in such rounds the prover will send
a polynomial of degree at most 2).
8.6
The power of the prover
A curious feature of many known interactive proof systems is that in order to prove membership
in language L, the prover needs to do more powerful computation than just deciding membership
in L. We give some examples.
1. The public coin system for graph nonisomorphism in Theorem 8.9 requires the prover to
produce, for some randomly chosen hash function h and a random element y in the range of
h, a graph H such that h(H) is isomorphic to either G1 or G2 and h(x) = y. This seems
harder than just solving graph non-isomorphism.
Web draft 2007-01-08 21:59

DRAFT
p8.16 (162)
8.7. PROGRAM CHECKING
2. The interactive proof for 3SAT, a language in coNP, requires the prover to do #P compu-
tations, doing summations of exponentially many terms. (Recall that all of PH is in P#P.)
In both cases, it is an open problem whether the protocol can be redesigned to use a weaker
prover.
Note that the protocol for TQBF is diﬀerent in that the prover’s replies can be computed in
PSPACE as well.
This observation underlies the following result, which is in the same spirit
as the Karp-Lipton results described in Chapter ??, except the conclusion is stronger since MA
is contained in Σ2 (indeed, a perfectly complete MA-proof system for L trivially implies that
L ∈Σ2).
Theorem 8.20
If PSPACE ⊆P/poly then PSPACE = MA.
Proof: If PSPACE ⊆P/poly then the prover in our TQBF protocol can be replaced by a circuit
of polynomial size. Merlin (the prover) can just give this circuit to Arthur (the veriﬁer) in Round
1, who then runs the interactive proof using this “prover.” No more interaction is needed. Note
that there is no need for Arthur to put blind trust in Merlin’s circuit, since the correctness proof of
the TQBF protocol shows that if the formula is not true, then no prover can make Arthur accept
with high probability. ■
In fact, using the Karp-Lipton theorem one can prove a stronger statement, see Lemma ??
below.
8.7
Program Checking
The discovery of the interactive protocol for the permanent problem was triggered by a ﬁeld called
program checking.
Blum and Kannan’s motivation for introducing this ﬁeld was the fact that
program veriﬁcation (deciding whether or not a given program solves a certain computational task)
is undecidable. They observed that in many cases we can guarantee a weaker guarantee of the
program’s “correctness” on an instance by instance basis. This is encapsulated in the notion of
a program checker. A checker C for a program P is itself another program that may run P as
a subroutine. Whenever P is run on an input x, C’s job is to detect if P’s answer is incorrect
(“buggy”) on that particular instance x. To do this, the checker may also compute P’s answer on
some other inputs. Program checking is sometimes also called instance checking, perhaps a more
accurate name, since the fact that the checker did not detect a bug does not mean that P is a
correct program in general, but only that P’s answer on x is correct.
Definition 8.21
Let P be a claimed program for computational task T. A checker for T is a probabilistic polynomial
time TM, C, that, given any x, has the following behavior:
1. If P is a correct program for T (i.e., ∀y P(y) = T(y)), then P[CP accepts P(x)] ≥2
3
2. If P(x) ̸= T(x) then P[CP accepts P(x)] < 1
3
Web draft 2007-01-08 21:59

DRAFT
8.7. PROGRAM CHECKING
p8.17 (163)
Note that in the case that P is correct on x (i.e., P(x) = C(x)) but the program P is not correct
everywhere, there is no guarantee on the output of the checker.
Surprisingly, for many problems, checking seems easier than actually computing the problem.
(Blum and Kannan’s suggestion was to build checkers into the software whenever this is true; the
overhead introduced by the checker would be negligible.)
Example 8.22 (Checker for Graph Non-Isomorphism)
The input for the problem of Graph Non-Isomorphism is a pair of labelled graphs ⟨G1, G2⟩, and
the problem is to decide whether G1 ≡G2. As noted, we do not know of an eﬃcient algorithm for
this problem. But it has an eﬃcient checker.
There are two types of inputs, depending upon whether or not the program claims G1 ≡G2.
If it claims that G1 ≡G2 then one can change the graph little by little and use the program to
actually obtain the permutation π (). We now show how to check the claim that G1 ̸≡G2 using
our earlier interactive proof of Graph non-isomorphism.
Recall the IP for Graph Non-Isomorphism:
• In case prover admits G1 ̸≡G2 repeat k times:
• Choose i ∈R {1, 2}. Permute Gi randomly into H
• Ask the prover ⟨G1, H⟩; ⟨G2, H⟩and check to see if the prover’s ﬁrst answer is consistent.
Given a computer program that supposedly computes graph isomorphism, P, how would we check
its correctness? The program checking approach suggests to use an IP while regarding the program
as the prover. Let C be a program that performs the above protocol with P as the prover, then:
Theorem 8.23
If P is a correct program for Graph Non-Isomorphism then C outputs ”correct” always. Otherwise,
if P(G1, G2) is incorrect then P[C outputs ”correct” ] ≤2−k. Moreover, C runs in polynomial time.
8.7.1
Languages that have checkers
Whenever a language L has an interactive proof system where the prover can be implemented
using oracle access to L, this implies that L has a checker. Thus, the following theorem is a direct
consequence of the interactive proofs we have seen:
Theorem 8.24
The problems Graph Isomorphism (GI), Permanent (perm) and True Quantiﬁed Boolean Formulae
(TQBF) have checkers.
Using the fact that P-complete languages are reducible to each other via NC-reductions, it suﬃces
to show a checker in NC for one P-complete language (as was shown by Blum & Kannan) to obtain
the following interesting fact:
Web draft 2007-01-08 21:59

DRAFT
p8.18 (164)
8.8. MULTIPROVER INTERACTIVE PROOFS (MIP)
Theorem 8.25
For any P-complete language there exists a program checker in NC
Since we believe that P-complete languages cannot be computed in NC, this provides additional
evidence that checking is easier than actual computation.
8.8
Multiprover interactive proofs (MIP)
It is also possible to deﬁne interactive proofs that involve more than one prover. The important
assumption is that the provers do not communicate with each other during the protocol. They
may communicate before the protocol starts, and in particular, agree upon a shared strategy for
answering questions. (The analogy often given is that of the police interrogating two suspects in
separate rooms. The suspects may be accomplices who have decided upon a common story to tell
the police, but since they are interrogated separately they may inadvertently reveal an inconsistency
in the story.)
The set of languages with multiprover interactive provers is call MIP. The formal deﬁnition is
analogous to Deﬁnition 8.5. We assume there are two provers (though one can also study the case
of polynomially many provers; see the exercises), and in each round the veriﬁer sends a query to
each of them —the two queries need not be the same. Each prover sends a response in each round.
Clearly, IP ⊆MIP since we can always simply ignore one prover. However,it turns out that
MIP is probably strictly larger than IP (unless PSPACE = NEXP). That is, we have:
Theorem 8.26 ([BFL91])
NEXP = MIP
We will outline a proof of this theorem in Chapter ??. One thing that we can do using two
rounds is to force non-adaptivity. That is, consider the interactive proof as an “interrogation”
where the veriﬁer asks questions and gets back answers from the prover. If the veriﬁer wants to
ensure that the answer of a prover to the question q is a function only of q and does not depend
on the previous questions the prover heard, the prover can ask the second prover the question q
and accept only if both answers agree with one another. This technique was used to show that
multi-prover interactive proofs can be used to implement (and in fact are equivalent to) a model
of a “probabilistically checkable proof in the sky”. In this model we go back to an NP-like notion
of a proof as a static string, but this string may be huge and so is best thought of as a huge table,
consisting of the prover’s answers to all the possible veriﬁer’s questions. The veriﬁer checks the
proof by looking at only a few entries in this table, that are chosen randomly from some distribution.
If we let the class PCP[r, q] be the set of languages that can be proven using a table of size 2r and
q queries to this table then Theorem 8.26 can be restated as
Theorem 8.27 (Theorem 8.26, restated)
NEXP = PCP[poly, poly] = ∪cPCP[nc, nc]
It turns out Theorem 8.26 can be scaled down to to obtain NP = PCP[polylog, polylog]. In
fact (with a lot of work) the following is known:
Web draft 2007-01-08 21:59

DRAFT
8.8. MULTIPROVER INTERACTIVE PROOFS (MIP)
p8.19 (165)
Theorem 8.28 (The PCP theorem, [AS98, ALM+98])
NP = PCP[O(log n), O(1)]
This theorem, which will be proven in Chapter 18, has had many applications in complexity,
and in particular establishing that for many NP-complete optimization problems, obtaining an
approximately optimal solution is as hard as coming up with the optimal solution itself. Thus, it
seems that complexity theory has gone a full circle with interactive proofs: by adding interaction,
randomization, and multiple provers, and getting to classes as high as NEXP, we have gained new
and fundamental insights on the class NP the represents static deterministic proofs (or equivalently,
eﬃciently veriﬁable search problems).
What have we learned?
• An interactive proof is a generalization of mathematical proofs in which the
prover and polynomial-time probabilistic veriﬁer interact.
• Allowing randomization and interaction seems to add signiﬁcantly more power
to proof system: the class IP of languages provable by a polynomial-time
interactive proofs is equal to PSPACE.
• All languages provable by a constant round proof system are in the class AM:
that is, they have a proof system consisting of the the veriﬁer sending a single
random string to the prover, and the prover responding with a single message.
Chapter notes and history
Interactive proofs were deﬁned in 1985 by Goldwasser, Micali, Rackoﬀ[GMR89] for cryptographic
applications and (independently, and using the public coin deﬁnition) by Babai and Moran [BM88].
The private coins interactive proof for graph non-isomorphism was given by Goldreich, Micali and
Wigderson [GMW87]. Simulations of private coins by public coins we given by Goldwasser and
Sipser [GS87].
The general feeling at the time was that interactive proofs are only a “slight”
extension of NP and that not even 3SAT has interactive proofs. The result IP = PSPACE was
a big surprise, and the story of its discovery is very interesting.
In the late 1980s, Blum and Kannan [BK95] introduced the notion of program checking. Around
the same time, manuscripts of Beaver and Feigenbaum [BF90] and Lipton [Lip91] appeared. In-
spired by some of these developments, Nisan proved in December 1989 that #SAT has multiprover
interactive proofs. He announced his proof in an email to several colleagues and then left on va-
cation to South America. This email motivated a ﬂurry of activity in research groups around the
world. Lund, Fortnow, Karloﬀshowed that #SAT is in IP (they added Nisan as a coauthor and the
ﬁnal paper is [LFK92]). Then Shamir showed that IP =PSPACE [Sha92] and Babai, Fortnow and
Lund [BFL91] showed MIP = NEXP. The entire story —as well as related developments—are
described in Babai’s entertaining survey [Bab90].
Vadhan [Vad00] explores some questions related to the power of the prover.
Web draft 2007-01-08 21:59

DRAFT
p8.20 (166)
8.8. MULTIPROVER INTERACTIVE PROOFS (MIP)
The result that approximating the shortest vector is probably not NP-hard (as mentioned in
the introduction) is due to Goldreich and Goldwasser [GG00].
Exercises
§1 Prove the assertions in Remark 8.6. That is, prove:
(a) Let IP′ denote the class obtained by allowing the prover to be probabilistic in Deﬁni-
tion 8.5. That is, the prover’s strategy can be chosen at random from some distribution
on functions. Prove that IP′ = IP.
(b) Prove that IP ⊆PSPACE.
(c) Let IP′ denote the class obtained by changing the constant 2/3 in (2) and (3) to 1−2−|x|.
Prove that IP′ = IP.
(d) Let IP′ denote the class obtained by changing the constant 2/3 in (2) to 1. Prove that
IP′ = IP.
(e) Let IP′ denote the class obtained by changing the constant 2/3 in (3) to 1. Prove that
IP′ = NP.
§2 We say integer y is a quadratic residue modulo m if there is an integer x such that y ≡x2
(mod m). Show that the following language is in IP[2]:
QNR = {(y, m) : y is not a quadratic residue modulo m} .
§3 Prove that there exists a perfectly complete AM[O(1)] protocol for the proving a lowerbound
on set size.
Hint: First note that in the current set lowerbound protocol we
can have the prover choose the hash function. Consider the eas-
ier case of constructing a protocol to distinguish between the case
|S| ≥K and |S| ≤
1
cK where c > 2 can be even a function of
K. If c is large enough the we can allow the prover to use several
hash functions h1, . . . , hi, and it can be proven that if i is large
enough we’ll have ∪ihi(S) = {0, 1}k. The gap can be increased by
considering instead of S the set Sℓ, that is the ℓtimes Cartesian
product of S.
§4 Prove that for every constant k ≥2, AM[k + 1] ⊆AM[k].
§5 Show that AM[2] = BP · NP
§6 [BFNW93] Show that if EXP ⊆P/poly then EXP = MA.
Hint: The interactive proof for TQBF requires a prover that is a
PSPACE machine.
Web draft 2007-01-08 21:59

DRAFT
8.A. INTERACTIVE PROOF FOR THE PERMANENT
p8.21 (167)
§7 Show that the problem GI is downward self reducible. That is, prove that given two graphs
G1,G2 on n vertices and access to a subroutine P that solves the GI problem on graphs with
up to n −1 vertices, we can decide whether or not G1 and G2 are isomorphic in polynomial
time.
§8 Prove that in the case that G1 and G2 are isomorphic we can obtain the permutation π
mapping G1 to G2 using the procedure of the above exercise. Use this to complete the proof
in Example 8.22 and show that graph isomorphism has a checker. Speciﬁcally, you have to
show that if the program claims that G1 ≡G2 then we can do some further investigation
(including calling the programs on other inputs) and with high probability conclude that
either (a) conclude that the program was right on this input or (b) the program is wrong on
some input and hence is not a correct program for graph isomorphism.
§9 Deﬁne a language L to be downward self reducible there’s a polynomial-time algorithm R that
for any n and x ∈{0, 1}n, RLn−1(x) = L(x) where by Lk we denote an oracle that solves L
on inputs of size at most k. Prove that if L is downward self reducible than L ∈PSPACE.
§10 Show that MIP ⊆NEXP.
§11 Show that if we redeﬁne multiprover interactive proofs to allow, instead of two provers, as
many as m(n) = poly(n) provers on inputs of size n, then the class MIP is unchanged.
Hint: Show how to simulate poly(n) provers using two. In this
simulation, one of the provers plays the role of all m(n) provers,
and the other prover is asked to simulate one of the provers, chosen
randomly from among the m(n) provers. Then repeat this a few
times.
8.A
Interactive proof for the Permanent
The permanent is deﬁned as follows:
Definition 8.29
Let A ∈F n×n be a matrix over the ﬁeld F. The permanent of A is:
perm(A) =
X
σ∈Sn
n
Y
i=1
ai,σ(i)
The problem of calculating the permanent is clearly in PSPACE. In Chapter 9 we will see that if
the permanent can be computed in polynomial time then P = NP, and hence this problem likely
does not have a polynomial-time algorithm.
Although the existence of an interactive proof for the Permanent follows from that for #SAT
and TQBF, we describe a specialized protocol as well. This is both for historical context (this
protocol was discovered before the other two protocols) and also because this protocol may be
helpful for further research. (One example will appear in a later chapter.)
Web draft 2007-01-08 21:59

DRAFT
p8.22 (168)
8.A. INTERACTIVE PROOF FOR THE PERMANENT
We use the following observation:
f(x1, x2, ..., xn) := perm


x1,1
x1,2
. . .
x1,n
x2,1
...
...
x2,n
...
...
...
...
xn,1
xn,2
. . .
xn,n


is a degree n polynomial since
f(x1, x2, . . . , xn) =
X
σ∈Sn
n
Y
i=1
xi,σ(i).
We now show two properties of the permanent problem. The ﬁrst is random self reducibility, earlier
encountered in Section ??:
Theorem 8.30 (Lipton ’88)
There is a randomized algorithm that, given an oracle that can compute the permanent on 1 −1
3n
fraction of the inputs in F n×n (where the ﬁnite ﬁeld F has size > 3n), can compute the permanent
on all inputs correctly with high probability.
Proof: Let A be some input matrix. Pick a random matrix R ∈R F n×n and let B(x) := A + x · R
for a variable x. Notice that:
• f(x) := perm(B) is a degree n univariate polynomial.
• For any ﬁxed b ̸= 0, B(b) is a random matrix, hence the probability that oracle computes
perm(B(b)) correctly is at least 1 −1
3n.
Now the algorithm for computing the permanent of A is straightforward: query oracle on all
matrices {B(i)|1 ≤i ≤n + 1}.
According to the union bound, with probability of at least
1 −n+1
n
≈2
3 the oracle will compute the permanent correctly on all matrices.
Recall the fact (see Section ?? in Appendix A) that given n + 1 (point, value) pairs {(ai, bi)|i ∈
[n+1]}, there exists a unique a degree n polynomial p that satisﬁes ∀i p(ai) = bi. Therefore, given
that the values B(i) are correct, the algorithm can interpolate the polynomial B(x) and compute
B(0) = A. ■
Note: The above theorem can be strengthened to be based on the assumption that the oracle can
compute the permanent on a fraction of 1
2 +ε for any constant ε > 0 of the inputs. The observation
is that not all values of the polynomial must be correct for unique interpolation. See Chapter ??
Another property of the permanent problem is downward self reducibility, encountered earlier in
context of SAT:
perm(A) =
n
X
i=1
a1iperm(A1,i),
where A1,i is a (n−1)×(n−1) sub-matrix of A obtained by removing the 1’st row and i’th column
of A (recall the analogous formula for the determinant uses alternating signs).
Web draft 2007-01-08 21:59

DRAFT
8.A. INTERACTIVE PROOF FOR THE PERMANENT
p8.23 (169)
Definition 8.31
Deﬁne a (n−1)×(n−1) matrix DA(x), such that each entry contains a degree n polynomial. This
polynomial is uniquely deﬁned by the values of the matrices {A1,i|i ∈[n]}. That is:
∀i ∈[n] . DA(i) = A1,i
Where DA(i) is the matrix DA(x) with i substituted for x. (notice that these equalities force n
points and values on them for each polynomial at a certain entry of DA(x), and hence according
to the previously mentioned fact determine this polynomial uniquely)
Observation: perm(DA(x)) is a degree n(n −1) polynomial in x.
8.A.1
The protocol
We now show an interactive proof for the permanent (the decision problem is whether perm(A) = k
for some value k):
• Round 1: Prover sends to veriﬁer a polynomial g(x) of degree n(n −1), which is supposedly
perm(DA(x)).
• Round 2: Veriﬁer checks whether:
k =
m
X
i=1
a1,ig(i)
If not, rejects at once. Otherwise, veriﬁer picks a random element of the ﬁeld b1 ∈R F and
asks the prover to prove that g(b1) = perm(DA(b1)). This reduces the matrix dimension to
(n −2) × (n −2).
...
• Round 2(n −1) −1: Prover sends to veriﬁer a polynomial of degree 2, which is supposedly
the permanent of a 2 × 2 matrix.
• Round 2(n −1): Veriﬁer is left with a 2 × 2 matrix and calculates the permanent of this
matrix and decides appropriately.
Claim 8.32
The above protocol is indeed an interactive proof for perm.
Proof: If perm(A) = k, then there exists a prover that makes the veriﬁer accept with probability
1, this prover just returns the correct values of the polynomials according to deﬁnition.
On the other hand, suppose that perm(A) ̸= k. If on the ﬁrst round, the polynomial g(x) sent is
the correct polynomial DA(x), then:
k ̸=
m
X
i=1
a1,ig(i) = perm(A)
Web draft 2007-01-08 21:59

DRAFT
p8.24 (170)
8.A. INTERACTIVE PROOF FOR THE PERMANENT
And the veriﬁer would reject. Hence g(x) ̸= DA(x). According to the fact on polynomials stated
above, these polynomials can agree on at most n(n −1) points. Hence, the probability that they
would agree on the randomly chosen point b1 is at most n(n−1)
|F|
. The same considerations apply to
all subsequent rounds if exist, and the overall probability that the veriﬁer will not accepts is thus
(assuming |F| ≥10n3 and suﬃciently large n):
Pr
≥

1 −n(n −1)
|F|

·

1 −(n −1)(n −2)
|F|

· ...

1 −3 · 2
|F|

≥

1 −n(n −1)
|F|
n−1
≥1
2
■
Web draft 2007-01-08 21:59

DRAFT
Chapter 9
Complexity of counting
“It is an empirical fact that for many combinatorial problems the detection of the
existence of a solution is easy, yet no computationally eﬃcient method is known
for counting their number....
for a variety of problems this phenomenon can be
explained.”
L. Valiant 1979
The class NP captures the diﬃculty of ﬁnding certiﬁcates. However, in many contexts, one is
interested not just in a single certiﬁcate, but actually counting the number of certiﬁcates. This
chapter studies #P, (pronounced “sharp p”), a complexity class that captures this notion.
Counting problems arise in diverse ﬁelds, often in situations having to do with estimations of
probability. Examples include statistical estimation, statistical physics, network design, and more.
Counting problems are also studied in a ﬁeld of mathematics called enumerative combinatorics,
which tries to obtain closed-form mathematical expressions for counting problems.
To give an
example, in the 19th century Kirchoﬀshowed how to count the number of spanning trees in a graph
using a simple determinant computation. Results in this chapter will show that for many natural
counting problems, such eﬃciently computable expressions are unlikely to exist.
Here is an example that suggests how counting problems can arise in estimations of probability.
Example 9.1
In the GraphReliability problem we are given a directed graph on n nodes. Suppose we are told that
each node can fail with probability 1/2 and want to compute the probability that node 1 has a
path to n.
A moment’s thought shows that under this simple edge failure model, the remaining graph is
uniformly chosen at random from all subgraphs of the original graph. Thus the correct answer is
1
2n (number of subgraphs in which node 1 has a path to n.)
We can view this as a counting version of the PATH problem.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p9.1 (171)

DRAFT
p9.2 (172)
9.1. THE CLASS #P
In the rest of the chapter, we study the complexity class #P, a class containing the GraphReliability
problem and many other interesting counting problems. We will show that it has a natural and
important complete problem, namely the problem of computing the permanent of a given matrix.
We also show a surprising connection between PH and #P, called Toda’s Theorem. Along the way
we encounter related complexity classes such as PP and ⊕P.
9.1
The class #P
We now deﬁne the class #P. Note that it contains functions whose output is a natural number,
and not just 0/1.
Definition 9.2 (#P)
A function f : {0, 1}∗→N is in #P if there exists a polynomial p : N →N and a
polynomial-time TM M such that for every x ∈{0, 1}∗:
f(x) =

n
y ∈{0, 1}p(|x|) : M(x, y) = 1
o .
Remark 9.3
As in the case of NP, we can also deﬁne #P using non-deterministic TMs. That is, #P consists
of all functions f such that f(x) is equal to the number of paths from the initial conﬁguration to
an accepting conﬁguration in the conﬁguration graph GM,x of a polynomial-time NDTM M.
The big open question regarding #P, is whether all problems in this class are eﬃciently solvable.
In other words, whether #P = FP. (Recall that FP is the analog of the class P for functions
with more than one bit of output, that is, FP is the set of functions from {0, 1}∗to {0, 1}∗
computable by a deterministic polynomial-time Turing machine. Thinking of the output as the
binary representation of an integer we can identify such functions with functions from {0, 1}∗to N.
Since computing the number of certiﬁcates is at least as hard as ﬁnding out whether a certiﬁcate
exists, if #P = FP then NP = P.
We do not know whether the other direction also holds:
whether NP = P implies that #P = FP. We do know that if PSPACE = P then #P = FP,
since counting the number of certiﬁcates can be done in polynomial space.
Here are two more examples for problems in #P:
• #SAT is the problem of computing, given a Boolean formula φ, the number of satisfying
assignments for φ.
• #CYCLE is the problem of computing, given a directed graph G, the number of simple cycles
in G. (A simple cycle is one that does not visit any vertex twice.)
Clearly, if #SAT ∈FP then SAT ∈P and so P = NP. Thus presumably #SAT ̸∈FP. How
about #CYCLE? The corresponding decision problem —given a directed graph decide if it has a
Web draft 2007-01-08 21:59

DRAFT
9.1. THE CLASS #P
p9.3 (173)
cycle—can be solved in linear time by breadth-ﬁrst-search. The next theorem suggests that the
counting problem may be much harder.
u
v
1
2
m
Figure 9.1: Reducing Ham to #CYCLE: by replacing every edge in G with the above gadget to obtain G′, every
simple cycle of length ℓin G becomes (2m)ℓsimple cycles in G′.
Theorem 9.4
If #CYCLE ∈FP, then P = NP.
Proof: We show that if #CYCLE can be computed in polynomial time, then Ham ∈P, where Ham
is the NP-complete problem of deciding whether or not a given digraph has a Hamiltonian cycle
(i.e., a simple cycle that visits all the vertices in the graph). Given a graph G with n vertices, we
construct a graph G′ such that G has a Hamiltonian cycle iﬀG′ has at least nn2 cycles.
To obtain G′, replace each edge (u, v) in G by the gadget shown in Figure 9.1. The gadget
has m = n log n + 1 levels. It is an acyclic digraph, so cycles in G′ correspond to cycles in G.
Furthermore, there are 2m directed paths from u to v in the gadget, so a simple cycle of length ℓ
in G yields (2m)ℓsimple cycles in G′.
Notice, if G has a Hamiltonian cycle, then G′ has at least (2m)n > nn2 cycles. If G has no
Hamiltonian cycle, then the longest cycle in G has length at most n −1. The number of cycles is
bounded above by nn−1. So G′ can have at most (2m)n−1 × nn−1 < nn2 cycles. ■
9.1.1
The class PP: decision-problem analog for #P.
Similar to the case of search problems, even when studying counting complexity, we can often
restrict our attention to decision problems.
The reason is that there exists a class of decision
problems PP such that
PP = P ⇔#P = FP
(1)
Intuitively, PP corresponds to computing the most signiﬁcant bit of functions in #P. That is,
L is in PP if there exists a polynomial-time TM M and a polynomial p : N →N such that for
every x ∈{0, 1}∗,
x ∈L ⇔

n
y ∈{0, 1}p(|x|) : M(x, y) = 1
o ≥1
2 · 2p(|x|)
You are asked to prove the non-trivial direction of (1) in Exercise 1. It is instructive to compare
the class PP, which we believe contains problem requiring exponential time to solve, with the class
BPP, which although it has a seemingly similar deﬁnition, can in fact be solved eﬃciently using
probabilistic algorithms (and perhaps even also using deterministic algorithms, see Chapter 16).
Note that we do not know whether this holds also for the class of decision problems corresponding
to the least signiﬁcant bit of #P, namely ⊕P (see Deﬁnition 9.13 below).
Web draft 2007-01-08 21:59

DRAFT
p9.4 (174)
9.2. #P COMPLETENESS.
9.2
#P completeness.
Now we deﬁne #P-completeness. Loosely speaking, a function f is #P-complete if it is in #P and
a polynomial-time algorithm for f implies that #P = FP. To formally deﬁne #P-completeness,
we use the notion of oracle TMs, as deﬁned in Section 3.5. Recall that a TM M has oracle access
to a language O ⊆{0, 1}∗if it can make queries of the form “Is q ∈O?” in one computational
step. We generalize this to non-Boolean functions by saying that M has oracle access to a function
f : {0, 1}∗→{0, 1}∗, if it is given access to the language O = {⟨x, i⟩: f(x)i = 1}. We use the same
notation for functions mapping {0, 1}∗to N, identifying numbers with their binary representation
as strings. For a function f : {0, 1}∗→{0, 1}∗, we deﬁne FPf to be the set of functions that are
computable by polynomial-time TMs that have access to an oracle for f.
Definition 9.5
A function f is #P-complete if it is in #P and every g ∈#P is in FPf
If f ∈FP then FPf = FP. Thus the following is immediate.
Proposition 9.6
If f is #P-complete and f ∈FP then FP = #P.
Counting versions of many NP-complete languages such as 3SAT,Ham, and CLIQUE naturally
lead to #P-complete problems. We demonstrate this with #SAT:
Theorem 9.7
#SAT is #P-complete
Proof: Consider the Cook-Levin reduction from any L in NP to SAT we saw in Section 2.3. This
is a polynomial-time computable function f : {0, 1}∗→{0, 1}∗such that for every x ∈{0, 1}∗,
x ∈L ⇔f(x) ∈SAT.
However, the proof that the reduction works actually gave us more
information than that. It provided a Levin reduction, by which we mean the proof showed a way
to transform a certiﬁcate that x is in L into a certiﬁcate (i.e., satisfying assignment) showing that
f(x) ∈SAT, and also vice versa (transforming a satisfying assignment for f(x) into a witness that
x ∈L).
In particular, it means that the mapping from the certiﬁcates of x to the assignments of f(x)
was invertible and hence one-to-one. Thus the number of satisfying assignments for f(x) is equal
to the number of certiﬁcates for x. ■
As shown below, there are #P-complete problems for which the corresponding decision problems
are in fact in P.
9.2.1
Permanent and Valiant’s Theorem
Now we study another problem. The permanent of an n × n matrix A is deﬁned as
perm(A) =
X
σ∈Sn
n
Y
i=1
Ai,σ(i)
(2)
Web draft 2007-01-08 21:59

DRAFT
9.2. #P COMPLETENESS.
p9.5 (175)
where Sn denotes the set of all permutations of n elements. Recall that the expression for the
determinant is similar
det(A) =
X
σ∈Sn
(−1)sgn(σ)
n
Y
i=1
Aiσ(i)
except for an additional “sign” term.1 This similarity does not translate into computational equiv-
alence: the determinant can be computed in polynomial time, whereas computing the permanent
seems much harder, as we see below.
The permanent function can also be interpreted combinatorially. First, suppose the matrix A
has each entry in {0, 1}. It may be viewed as the adjacency matrix of a bipartite graph G(X, Y, E),
with X = {x1, . . . , xn}, Y = {y1, . . . , yn} and {xi, yj} ∈E iﬀAi,j = 1. Then the term Qn
i=1 Aiσ(i)
is 1 iﬀσ is a perfect matching (which is a set of n edges such that every node is in exactly one
edge). Thus if A is a 0.1 matrix then perm(A) is simply the number of perfect matchings in the
corresponding graph G and in particular computing perm(A) is in #P. If A is a {−1, 0, 1} matrix,
then perm(A) =

σ : Qn
i=1 Aiσ(i) = 1
	 −

σ : Qn
i=1 Aiσ(i) = −1
	, so one can make two calls to a
#SAT oracle to compute perm(A). In fact one can show for general integer matrices that computing
the permanent is in FP#SAT (see Exercise 2).
The next theorem came as a surprise to researchers in the 1970s, since it implies that if perm ∈
FP then P = NP. Thus, unless P = NP, computing the permanent is much more diﬃcult then
computing the determinant.
Theorem 9.8 (Valiant’s Theorem)
perm for 0, 1 matrices is #P-complete.
Before proving Theorem 9.8, we introduce yet another way to look at the permanent. Consider
matrix A as the the adjacency matrix of a weighted n-node digraph (with possible self loops). Then
the expression Qn
i=1 Ai,σ(i) is nonzero iﬀσ is a cycle-cover of A (a cycle cover is a subgraph in which
each node has in-degree and out-degree 1; such a subgraph must be composed of cycles). We deﬁne
the weight of the cycle cover to be the product of the weights of the edges in it. Thus perm(A) is
equal to the sum of weights of all possible cycle covers.
Example 9.9
Consider the graph in Figure 9.2. Even without knowing what the subgraph G′ is, we show that
the permanent of the whole graph is 0. For each cycle cover in G′ of weight w there are exactly
two cycle covers for the three nodes, one with weight +w and one with weight −w. Any non-zero
weight cycle cover of the whole graph is composed of a cycle cover for G′ and one of these two cycle
covers. Thus the sum of the weights of all cycle covers of G is 0.
1It is known that every permutation σ ∈Sn can be represented as a composition of transpositions, where a
transposition is a permutation that only switches between two elements in [n] and leaves the other elements intact
(one proof for this statement is the Bubblesort algorithm). If τ1, . . . , τm is a sequence of transpositions such that their
composition equals σ, then the sign of σ is equal to +1 if m is even and −1 if m is odd. It can be shown that the
sign is well-deﬁned in the sense that it does not depend on the representation of σ as a composition of transpositions.
Web draft 2007-01-08 21:59

DRAFT
p9.6 (176)
9.2. #P COMPLETENESS.
G’
+1
+1
+1
-1
-1
-1
+1
+1
-1
+1
-1
-1
weight= -1
weight= +1
Figure 9.2: The above graph G has cycle cover weight zero regardless of the choice of G′, since for every cycle cover
of weight w in G′, there exist two covers of weight +w and −w in the graph G. (Unmarked edges have +1 weight;
we follow this convention through out this chapter.)
Proof of Valiant’s Theorem (Theorem 9.8): We reduce the #P-complete problem #3SAT
to perm. Given a boolean formula φ with n variables and m clauses, ﬁrst we shall show how to
construct an integer matrix A′ with negative entries such that perm(A′) = 4m · (#φ). (#φ stands
for the number of satisfying assignments of φ). Later we shall show how to to get a 0-1 matrix A
from A′ such that knowing perm(A) allows us to compute perm(A′).
The main idea is that our construction will result in two kinds of cycle covers in the digraph G′
associated with A′: those that correspond to satisfying assignments (we will make this precise) and
those that don’t. We will use negative weights to ensure that the contribution of the cycle covers
that do not correspond to satisfying assignments cancels out. (This is similar reasoning to the one
used in Example 9.9.) On the other hand, we will show that each satisfying assignment contributes
4m to perm(A′), and so perm(A′) = 4m · (#φ).
To construct G′ from φ, we combine the following three kinds of gadgets shown in Figure 9.3:
Variable gadget The variable gadget has two possible cycle covers, corresponding to an assign-
ment of 0 or 1 to that variable.
Assigning 1 corresponds to a single cycle taking all the
external edges (“true-edges”), and assigning 0 correspond to taking all the self-loops and
taking the “false-edge”. Each external edge of a variable is associated with a clause in which
the variable appears.
Clause gadget The clause gadget is such that the only possible cycle covers exclude at least one
external edge. Also for a given (proper) subset of external edges used there is a unique cycle
cover (of weight 1). Each external edge is associated with a variable appearing in the clause.
XOR gadget We also use a graph called the XOR gadget whose purpose is to ensure that for
some pair of edges −−→
u u′ and −→
v v′, exactly one of these edges is present in any cycle cover that
counts towards the ﬁnal sum.
Suppose that we replace a pair of edges −−→
u u′ and −→
v v′ in some graph G with the XOR gadget as
described in Figure count:ﬁg:valiantgad to obtain some graph G′. Then, via similar reasoning
to Example 9.9, every cycle cover of G of weight w that uses exactly one of the edges −−→
u u′ and
Web draft 2007-01-08 21:59

DRAFT
9.2. #P COMPLETENESS.
p9.7 (177)
variable gadget:
....
False edge
external (true) edges - one per clause
Symbolic description:
...
Gadget:
clause gadget:
external edges - one per variable
external edges
variable gadget
clause gadget
XOR gadget:
u
u’
v’
v
-1
-1
-1
2
3
u
u’
v
v’
`  
The overall construction:
external edges
...
variable gadget
clause gadget
......
......
......
......
variable gadget
for every variable
clause gadget
for every clause
connect via XOR external
edges of gadgets for
variables that appear in clauses.
Figure 9.3: The gadgets used in the proof of Valiant’s Theorem.
Web draft 2007-01-08 21:59

DRAFT
p9.8 (178)
9.2. #P COMPLETENESS.
−→
v v′ is mapped to a set of cycle covers in G′ whose total weight is 4w (i.e., the set of covers
that enter the gadget at u and exit at u′ or enter it at v and exit it at v′), while all the other
cycle covers of G′ have total weight 0 (Exercise 3). For this reason, whenever we replace edges
−−→
u u′ and −→
v v′ with a XOR gadget, we can consider in the analysis only cycle covers that use
exactly one of these edges, as the other covers do not contribute anything to the total sum.
The XOR gadgets are used to connect the variable gadgets to the corresponding clause gadgets
so that only cycle covers corresponding to a satisfying assignment will be counted towards the total
number of cycle covers. Consider a clause, and a variable appearing in it. Each has an external
edge corresponding to the other, connected by an XOR gadget. If the external edge in the clause is
not taken then by the analysis of the XOR gadget the external edge in the variable must be taken
(and hence the variable is true). Since at least one external edge of each clause gadget has to be
omitted, each cycle cover that is counted towards the sum corresponds to a satisfying assignment.
Conversely, for each satisfying assignment, there is a a set of cycle covers with total weight 43m
(since they passes through the XOR gadget exactly 3m times). So perm(G′) = 43m#φ.
Reducing to the case 0, 1 matrices.
Finally we have to reduce ﬁnding perm(G′) to ﬁnding
perm(G), where G is an unweighted graph (or equivalently, its adjacency matrix has only 0, 1
entries). We start by reducing to the case that all edges have weights in {±1}. First, note that
replacing an edge of weight k by k parallel edges of weight 1 does not change the permanent.
Parallel edges are not allowed, but we can make edges non-parallel by cutting each edge −→
u v in two
and inserting a new node w with an edge from u to w, w to v and a self loop at w. To get rid
of the negative weights, note that the permanent of an n vertex graph with edge weights in {±1}
is a number x in [−n!, +n!] and hence this permanent can be computed from y = x (mod 2m+1)
where m is suﬃciently large (e.g., m = n2 will do). But to compute y it is enough to compute
the permanent of the graph where all weight −1 edges are replaced with edges of weight 2m. Such
edges can be converted to m edges of weight 2 in series, which again can be transformed to parallel
edges of weight +1 as above. ■
9.2.2
Approximate solutions to #P problems
Since computing exact solutions to #P-complete problems is presumably diﬃcult, a natural ques-
tion is whether we can approximate the number of certiﬁcates in the sense of the following deﬁnition.
Definition 9.10
Let f : {0, 1}∗→N and α < 1.
An algorithm A is an α-approximation for f if for every x,
αf(x) ≤A(x) ≤f(x)/α.
Not all #P problems behave identically with respect to this notion. Approximating certain
problems within any constant factor α > 0 is NP-hard (see Exercise 5). For other problems such
as 0/1 permanent, there is a Fully polynomial randomized approximation scheme (FPRAS), which
is an algorithm which, for any ϵ, δ, approximates the function within a factor 1 + ϵ (its answer may
be incorrect with probability δ) in time poly(n, log 1/δ, log 1/ϵ). Such approximation of counting
problems is suﬃcient for many applications, in particular those where counting is needed to obtain
Web draft 2007-01-08 21:59

DRAFT
9.3. TODA’S THEOREM: PH ⊆P#SAT
p9.9 (179)
estimates for the probabilities of certain events (e.g., see our discussion of the graph reliability
problem).
The approximation algorithm for the permanent —as well as other similar algorithms for a
host of #P-complete problems—use the Monte Carlo Markov Chain technique. The result that
spurred this development is due to Valiant and Vazirani and it shows that under fairly general
conditions, approximately counting the number of elements in a set ( membership in which is
testable in polynomial time) is equivalent —in the sense that the problems are interreducible via
polynomial-time randomized reductions— to the problem of generating a random sample from the
set. We will not discuss this interesting area any further.
Interestingly, if P = NP then every #P problem has an FPRAS (and in fact an FPTAS: i.e.,
a deterministic polynomial-time approximation scheme), see Exercise 6.
9.3
Toda’s Theorem: PH ⊆P#SAT
An important question in the 1980s was the relative power of the polynomial-hierarchy PH and
the class of counting problems #P. Both are natural generalizations of NP, but it seemed that
their features— alternation and the ability to count certiﬁcates, respectively — are not directly
comparable to each other. Thus it came as big surprise when in 1989 Toda showed:
Theorem 9.11 (Toda’s theorem [Tod91])
PH ⊆P#SAT.
That is, we can solve any problem in the polynomial hierarchy given an oracle to a #P-complete
problem.
Remark 9.12
Note that we already know, even without Toda’s theorem, that if #P = FP then NP = P
and so PH = P. However, this does not imply that any problem in PH can be computed in
polynomial-time using an oracle to #SAT. For example, one implication of Toda’s theorem is that
a subexponential (i.e., 2no(1)-time) algorithm for #SAT will imply such an algorithm for any problem
in PH. Such an implication is not known to hold from a 2no(1)-time algorithm for SAT.
9.3.1
The class ⊕P and hardness of satisﬁability with unique solutions.
The following complexity class will be used in the proof:
Definition 9.13
A language L in the class ⊕P (pronounced “parity P”) iﬀthere exists a polynomial time NTM M
such that x ∈L iﬀthe number of accepting paths of M on input x is odd.
Thus, ⊕P can be considered as the class of decision problems corresponding to the least sig-
niﬁcant bit of a #P-problem. As in the proof of Theorem 9.7, the fact that the standard NP-
completeness reduction is parsimonious implies the following problem ⊕SAT is ⊕P-complete (under
many-to-one Karp reductions):
Web draft 2007-01-08 21:59

DRAFT
p9.10 (180)
9.3. TODA’S THEOREM: PH ⊆P#SAT
Definition 9.14
Deﬁne the quantiﬁer L as follows: for every Boolean formula ϕ on n variables. L
x∈{0,1}n ϕ(x) is
true if the number of x’s such that ϕ(x) is true is odd.2 The language ⊕SAT consists of all the true
quantiﬁed Boolean formula of the form L
x∈{0,1}n ϕ(x) where ϕ is an unquantiﬁed Boolean formula
(not necessarily in CNF form).
Unlike the class #P, it is not known that a polynomial-time algorithm for ⊕P implies that
NP = P. However, such an algorithm does imply that NP = RP since NP can be probabilistically
reduced to ⊕SAT:
Theorem 9.15 (Valiant-Vazirani Theorem)
There exists a probabilistic polynomial-time algorithm A such that for every n-
variable Boolean formula ϕ
ϕ ∈SAT ⇒Pr[A(ϕ) ∈⊕SAT] ≥
1
8n
ϕ ̸∈SAT ⇒Pr[A(ϕ) ∈⊕SAT] = 0
To prove Theorem 9.15 we use the following lemma on pairwise independent hash functions:
Lemma 9.16 (Valiant-Vazirani Lemma [?])
Let Hn,k be a pairwise independent hash function collection from {0, 1}n to {0, 1}k and S ⊆{0, 1}n
such that 2k−2 ≤|S| ≤2k−1. Then,
Pr
h∈RHn,k
[

n
x ∈S : h(x) = 0ko = 1] ≥1
8
Proof: For every x ∈S, let p = 2−k be the probability that h(x) = 0k when h ∈R Hn,k. Note
that for every x ̸= x′, Pr[h(x)=0k ∧h(x′)=0k] = p2. Let N be the random variable denoting the
number of x ∈S satisfying h(x) = 0k. Note that E[N] = |S|p ∈[1
4, 1
2]. By the inclusion-exclusion
principle
Pr[N ≥1] ≥
X
x∈S
Pr[h(x)=0k] −
X
x<x′∈S
Pr[h(x)=0k ∧h(x′)=0k] = |S|p −
|S|
2

p2
and by the union bound we get that Pr[N ≥2] ≤
 |S|
2

p2. Thus
Pr[N = 1] = Pr[N ≥1] −Pr[N ≥2] ≥|S|p −2
|S|
2

p2 ≥|S|p −|S|2p2 ≥1
8
where the last inequality is obtained using the fact that 1
4 ≤|S|p ≤1
2. ■
2Note that if we identify true with 1 and 0 with false then L
x∈{0,1}n ϕ(x) = P
x∈{0,1}n ϕ(x) (mod 2). Also note
that L
x∈{0,1}n ϕ(x) = L
x1∈{0,1} · · · L
xn∈{0,1} ϕ(x1, . . . , xn).
Web draft 2007-01-08 21:59

DRAFT
9.3. TODA’S THEOREM: PH ⊆P#SAT
p9.11 (181)
Proof of Theorem 9.15
We now use Lemma 9.16 to prove Theorem 9.15. Given a formula ϕ on n variables, our probabilistic
algorithm A chooses k at random from {2, . . . , n + 1} and a random hash function h ∈R Hn,k. It
then uses the Cook-Levin reduction to compute a formula τ on variables x ∈{0, 1}n , y ∈{0, 1}m
(for m = poly(n)) such that h(x) = 0 if and only if there exists a unique y such that τ(x, y) = 1.3
The output of A if the formula
ψ =
M
x∈{0,1}n,y∈{0,1}m
ϕ(x) ∧τ(x, y) ,
It is equivalent to the statement
M
x∈{0,1}n
ϕ(x) ∧h(x) = 0k ,
If ϕ is unsatisﬁable then ψ is false, since we’ll have no x’s satisfying the inner formula and
zero is an even number. If ϕ is satisﬁable, we let S be the set of its satisfying assignments. With
probability 1/n, k satisﬁes 2k−2 ≤|S| ≤2k, conditioned on which, with probability 1/8, there is a
unique x such that ϕ(x) ∧h(x) = 0n. Since one happens to be an odd number, this implies that ψ
is true. ■
Remark 9.17 (Hardness of Unique Satisfiability)
The proof of Theorem 9.15 implies the following stronger statement: the existence of an algorithm
to distinguish between an unsatisﬁable Boolean formula and a formula with exactly one satisfying
assignment implies the existence of a probabilistic polynomial-time algorithm for all of NP. Thus,
the guarantee that a particular search problem has either no solutions or a unique solution does
not necessarily make the problem easier to solve.
9.3.2
Step 1: Randomized reduction from PH to ⊕P
We now go beyond NP (that is to say, the Valiant-Vazirani theorem) and show that we can actually
reduce any language in the polynomial hierarchy to ⊕SAT.
Lemma 9.18
Let c ∈N be some constant. There exists a probabilistic polynomial-time algorithm A such that
for every ψ a Quantiﬁed Boolean formula with c levels of alternations,
ψ is true ⇒Pr[A(ψ) ∈⊕SAT] ≥2
3
ψ is false ⇒Pr[A(ψ) ∈⊕SAT] = 0
Before proving the Lemma, let us make a few notations and observations: For a Boolean
formula ϕ on n variables, let #(ϕ) denote the number of satisfying assignments of ϕ. We consider
also formulae ϕ that are partially quantiﬁed. That is, in addition to the n variables ϕ takes as input
3For some implementations of hash functions, such as the one described in Exercise 4, one can construct directly
(without going through the Cook-Levin reduction) such a formula τ that does not use the y variables.
Web draft 2007-01-08 21:59

DRAFT
p9.12 (182)
9.3. TODA’S THEOREM: PH ⊆P#SAT
it may also have other variables that are bound by a ∀, ∃or L quantiﬁers (for example ϕ can be of
the form ϕ(x1, . . . , xn) = ∀y ∈{0, 1}n τ(x1, . . . , xn, y) where τ is, say, a 3CNF Boolean formula).
Given two (possibly partially quantiﬁed) formulae ϕ, ψ on variables x ∈{0, 1}n , y ∈{0, 1}m we
can construct in polynomial-time an n + m variable formula ϕ · ψ and a (max{n, m} + 1)-variable
formula ϕ + ψ such that #(ϕ · ψ) = #(ϕ)#(ϕ) and #(ϕ + ψ) = #(ϕ) + #(ψ).
Indeed, take
ϕ · ψ(x, y) = ϕ(x) ∧ϕ(y) and ϕ + ψ(z) =
 (z0 = 0) ∧ϕ(z1, . . . , zn)

∨
 (z0 = 1) ∧ψ(z1, . . . , zm)

.
For a formula ϕ, we use the notation ϕ + 1 to denote the formula ϕ + ψ where ψ is some canonical
formula with a single satisfying assignment. Since the product of numbers is even iﬀone of the
numbers is even, and since adding one to a number ﬂips the parity, for every two formulae ϕ, ψ as
above
 M
x
ϕ(x)

∧
 M
y
ψ(y)

⇔
M
x,y
(ϕ · ψ)(x, y)
(3)
¬
M
x
ϕ(x) ⇔
M
x,z
(ϕ + 1)(x, z)
(4)
 M
x
ϕ(x)

∨
 M
y
ψ(y)

⇔
M
x,y,z
((ϕ + 1) · (ψ + 1) + 1)(x, y, z)
(5)
Proof of Lemma 9.18:
Recall that membership in a PH-language can be reduced to deciding
the truth of a quantiﬁed Boolean formula with a constant number of alternating quantiﬁers. The
idea behind the proof is to replace one-by-one each ∃/∀quantiﬁers with a L quantiﬁer.
Let ψ be a formula with c levels of alternating ∃/∀quantiﬁers, possibly with an initial L
quantiﬁer. We transform ψ in probabilistic polynomial-time to a formula ψ′ such that ψ′ has only
c −1 levels of alternating ∃/∀quantiﬁers, an initial L quantiﬁer, satisfying (1) if ψ is false then
so is ψ′, and (2) if ψ is true then with probability at least 1 −
1
10c, ψ′ is true as well. The lemma
follows by repeating this step c times.
For ease of notation, we demonstrate the proof for the case that ψ has a single L quantiﬁer
and two additional ∃/∀quantiﬁers. We can assume without loss of generality that ψ is of the form
ψ =
M
z∈{0,1}ℓ
∃x∈{0,1}n∀w∈{0,1}kϕ(z, x, w) ,
as otherwise we can use the identities ∀xP(x) = ¬∃x¬P(x) and (4) to transform ψ into this form.
The proof of Theorem 9.15 provides for every n, a probabilistic algorithm that outputs a for-
mula τ on variables x ∈{0, 1}n and y ∈{0, 1}m such that for every nonempty set S ⊆{0, 1}n,
Pr[⊕x∈{0,1}n,y∈{0,1}mτ(x, y)] ≥1/(8n). Run this algorithm t = 100cℓlog n times to obtain the for-
mulae τ1, . . . , τt. Then, for every nonempty set S ⊆{0, 1}n the probability that there does not
exist i ∈[t] such that ⊕x∈{0,1}n,y∈{0,1}mτ(x, y) is True is less than 2−ℓ/(10c). We claim that this
implies that with probability at least 1 −1/(10c), the following formula is equivalent to ψ:
M
z∈{0,1}ℓ
θ(z) ,
(6)
where
θ(z) = ∨t
i=1


M
x∈{0,1}n,y∈{0,1}m
∀w∈{0,1}kτi(x, y) ∧ϕ(x, z, w)


Web draft 2007-01-08 21:59

DRAFT
9.3. TODA’S THEOREM: PH ⊆P#SAT
p9.13 (183)
Indeed, for every z ∈{0, 1}ℓdeﬁne Sz =
n
x ∈{0, 1}n : ∀w∈{0,1}kϕ(x, z, w)
o
. Then, ψ is equivalent
to ⊕z∈{0,1}ℓ|Sz| is nonempty. But by the union bound, with probability at least 1−1/(10c) it holds
that for every z such that Sz is nonempty, there exists τi satisfying ⊕x,yτi(x, y). This means that
for every such z, θ(z) is true. On the other hand, if Sz is empty then certainly θ(z) is false, implying
that indeed ψ is equivalent to (6).
By applying the identity (5), we can transform (6) into an equivalent formula of the desired
form
M
z,x,y,w
∀wϕ′(x, y, z, w)
for some unquantiﬁed polynomial-size formula ϕ′. ■
9.3.3
Step 2: Making the reduction deterministic
To complete the proof of Toda’s Theorem (Theorem 9.11), we prove the following lemma:
Lemma 9.19
There is a (deterministic) polynomial-time transformation T that, for every formula ψ that is an
input for ⊕SAT, T(ψ, 1m) is an unquantiﬁed Boolean formula and
ψ ∈⊕SAT ⇒#(ϕ) = −1
(mod 2m+1)
ψ ̸∈⊕SAT ⇒#(ϕ) = 0
(mod 2m+1)
Proof of Theorem 9.11 using Lemmas 9.18 and 9.19.: Let L ∈PH. We show that we can
decide whether an input x ∈L by asking a single question to a #SAT oracle. For every x ∈{0, 1}n,
Lemmas 9.18 and 9.19 together imply there exists a polynomial-time TM M such that
x ∈L ⇒
Pr
r∈R{0,1}m[#(M(x, r)) = −1
(mod 2m+1)] ≥2
3
x ̸∈L ⇒∀r∈R{0,1}m#(M(x, r)) = 0
(mod 2m+1)
where m is the (polynomial in n) number of random bits used by the procedure described in that
Lemma.
Furthermore, even in the case x ∈L, we are guaranteed that for every r ∈{0, 1}m,
#(M(x, r)) ∈{0, −1} (mod 2m+1).
Consider the function that maps two strings r, u into the evaluation of the formula M(x, r) on the
assignment u. Since this function is computable in polynomial-time, the Cook-Levin transformation
implies that we can obtain in polynomial-time a CNF formula θx on variables r, u, y such that for
every r, u, M(x, r) is satisﬁed by u if and only if there exist a unique y such that θx(r, u, y) is true.
Let fx(r) be the number of u, y such that θx(r, u, y) is true, then
#(θx) =
X
r∈{0,1}m
fx(r) ,
But if x ̸∈L then fx(r) = 0 (mod 2m+1) for every r, and hence #(θx) = 0 (mod 2m+1). On the
other hand, if x ∈L then fx(r) = −1 (mod 2m+1) for between 2
32m and 2m values of r, and is
Web draft 2007-01-08 21:59

DRAFT
p9.14 (184)
9.4. OPEN PROBLEMS
equal to 0 on the other values, and hence #(θx) ̸= 0 (mod 2m+1). We see that deciding whether
x ∈L can be done by computing #(θx).■
Proof of Lemma 9.19: For every pair of formulae ϕ,τ recall that we deﬁned formulas ϕ + τ and
ϕ · τ satisfying #(ϕ + τ) = #(ϕ) + #(τ) and #(ϕ · τ) = #(ϕ)#(τ), and note that these formulae
are of size at most a constant factor larger than ϕ, τ. Consider the formula 4τ 3 + 3τ 4 (where τ 3 for
example is shorthand for τ · (τ · τ)). One can easily check that
#(τ) = −1
(mod 22i) ⇒#(4τ 3 + 3τ 4) = −1
(mod 22i+1)
(7)
#(τ) = 0
(mod 22i) ⇒#(4τ 3 + 3τ 4) = 0
(mod 2)2i+1
(8)
Let ψ0 = ψ and ψi+1 = 4ψ3
i + 3ψ4
i . Let ψ∗= ψ⌈log(m+1)⌉. Repeated use of equations (7), (8)
shows that if #(ψ) is odd, then #(ψ∗) = −1 (mod 2m+1) and if #(ψ) is even, then #(ψ∗) = 0
(mod 2m+1). Also, the size of ψ∗is only polynomially larger than size of ψ. ■
What have we learned?
• The class #P consists of functions that count the number of certiﬁcates for a
given instance. If P ̸= NP then it is not solvable in polynomial time.
• Counting analogs of many natural NP-complete problems are #P-complete,
but there are also #P-complete counting problems for which the correspond-
ing decision problem is in P. One example for this is the problem perm of
computing the permanent.
• Surprisingly, counting is more powerful than alternating quantiﬁers: we can
solve every problem in the polynomial hierarchy using an oracle to a #P-
complete problem.
• The classes PP and ⊕P contain the decision problems that correspond to
the most signiﬁcant and least signiﬁcant bits (respectively) of a #P function.
The class PP is as powerful as #P itself, in the sense that if PP = P then
#P = FP. We do not know if this holds for ⊕P but do know that every
language in PH randomly reduces to ⊕P.
9.4
Open Problems
• What is the exact power of ⊕SAT and #SAT ?
• What is the average case complexity of n × n permanent modulo small prime, say 3 or 5 ?
Note that for a prime p > n, random self reducibility of permanent implies that if permanent
is hard to compute on at least one input then it is hard to compute on 1 −O(p/n) fraction
of inputs, i.e. hard to compute on average (see Theorem ??).
Web draft 2007-01-08 21:59

DRAFT
9.4. OPEN PROBLEMS
p9.15 (185)
Chapter notes and history
The deﬁnition of #P as well as several interesting examples of #P problems appeared in Valiant’s
seminal paper [Val79b]. The #P-completeness of the permanent is from his other paper [Val79a].
Toda’s Theorem is proved in [Tod91]. The proof given here follows the proof of [KVVY93] (although
we use formulas where they used circuits.)
For an introduction to FPRAS’s for computing approximations to many counting problems,
see the relevant chapter in Vazirani [Vaz01] ( an excellent resource on approximation algorithms in
general).
.
Exercises
§1 Let f ∈#P. Show a polynomial-time algorithm to compute f given access to an oracle for
some language L ∈PP (see Remark ??).
Hint:
without loss of generality you can think that f
=
#CKT −SAT, the problem of computing the number of satisfy-
ing assignments for a given Boolean circuit C, and that you are
given an oracle that tells you if a given n-variable circuit, has at
least 2n−1 satisfying assignments or not. The main observation you
can use is that if C has at least 2n−1 satisfying assignments then
it is possible to use the oracle to ﬁnd a string x such that C has
exactly 2n−1 satisfying assignments that are larger than x in the
natural lexicographic ordering of the strings in {0, 1}n.
§2 Show that computing the permanent for matrices with integer entries is in FP#SAT.
§3 Complete the analysis of the XOR gadget in the proof of Theorem 9.8. Let G be any weighted
graph containing a pair of edges −−→
u u′ and −→
v v′, and let G′ be the graph obtained by replacing
these edges with the XOR gadget. Prove that every cycle cover of G of weight w that uses
exactly one of the edges −−→
u u′ is mapped to a set of cycle covers in G′ whose total weight is
4w, and all the other cycle covers of G′ have total weight 0.
§4 Let k ≤n.
Prove that the following family Hn,k is a collection of pairwise independent
functions from {0, 1}n to {0, 1}k: Identify {0, 1} with the ﬁeld GF(2).
For every k × n
matrix A with entries in GF(2), and k-length vector b ∈GF(2)n, Hn,k contains the function
hA,b : GF(2)n →GF(2)k deﬁned as follows: hA,b(x) = Ax + b.
§5 Show that if there is a polynomial-time algorithm that approximates #CYCLE within a factor
1/2, then P = NP.
§6 Show that if NP = P then for every f ∈#P and there is a polynomial-time algorithm
that approximates f within a factor of 1/2. Can you show the same for a factor of 1 −ϵ for
arbitrarily small constant ϵ > 0? Can you make these algorithms deterministic?
Web draft 2007-01-08 21:59

DRAFT
p9.16 (186)
9.4. OPEN PROBLEMS
Note that we do not know whether P = NP implies that exact computation of functions in
#P can be done in polynomial time.
Hint: Use hashing and ideas similar to those in the proof of Toda’s
theorem, where we also needed to estimate the size of a set of
strings. If you ﬁnd this question diﬃcult you might want to come
back to it after seeing the Goldwasser-Sipser set lowerbound pro-
tocol of Chapter 8. To make the algorithm deterministic use the
ideas of the proof that BPP ⊆PH (Theorem 7.18).
§7 Show that every for every language in AC0 there is a depth 3 circuit of npoly(log n) size that
decides it on 1 −1/poly(n) fraction of inputs and looks as follows: it has a single ⊕gate at
the top and the other gates are ∨, ∧of fanin at most poly(log n).
Hint: use the proof of Lemma 9.18.
Web draft 2007-01-08 21:59

DRAFT
Chapter 10
Cryptography
“From times immemorial, humanity has gotten frequent, often cruel, reminders that
many things are easier to do than to reverse.”
L. Levin [Lev]
somewhat rough still
The importance of cryptography in today’s online world needs no introduction. Here we focus
on the complexity issues that underlie this ﬁeld. The traditional task of cryptography was to allow
two parties to encrypt their messages so that eavesdroppers gain no information about the message.
(See Figure 10.1.) Various encryption techniques have been invented throughout history with one
common characteristic: sooner or later they were broken.
Figure unavailable in pdf ﬁle.
Figure 10.1: People sending messages over a public channel (e.g., the internet) wish to use encryption so that
eavesdroppers learn “nothing.”
In the post NP-completeness era, a crucial new idea was presented: the code-breaker should be
thought of as a resource-bounded computational device. Hence the security of encryption schemes
ought to be proved by reducing the task of breaking the scheme into the task of solving some
computationally intractable problem (say requiring exponential time complexity or circuit size),
thus one could hope to design encryption schemes that are eﬃcient enough to be used in practice,
but whose breaking will require, say, millions of years of computation time.
Early researchers tried to base the security of encyption methods upon the (presumed) in-
tractability of NP-complete problems. This eﬀort has not succeeded to date, seemingly because
NP-completeness concerns the intractability of problems in the worst-case whereas cryptography
seems to need problems that are intractable on most instances. After all, when we encrypt email,
we require that decryption should be diﬃcult for an eavesdropper for all (or almost all) messages,
not just for a few messages. Thus the concept most useful in this chapter will be average-case
complexity1. We will see a class of functions called one-way functions that are easy to compute
1A problem’s average-case and worst-case complexities can diﬀer radically. For instance, 3COL is NP-complete
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p10.1 (187)

DRAFT
p10.2 (188)
10.1. HARD-ON-AVERAGE PROBLEMS AND ONE-WAY FUNCTIONS
but hard to invert for most inputs —they are alluded to in Levin’s quote above. Such functions
exist under a variety of assumptions, including the famous assumption that factoring integers re-
quires time super-polynomial time in the integer’s bit-length to solve in the average case (e.g., for
a product of two random primes).
Furthermore, in the past two decades, cryptographers have taken on tasks above and beyond the
basic task of encryption—from implementing digital cash to maintaining the privacy of individuals
in public databases. (We survey some applications in Section 10.4.) Surprisingly, many of these
tasks can be achieved using the same computational assumptions used for encryption. A crucial
ingredient in these developments turns out to be an answer to the question: “What is a random
string and how can we generate one?” The complexity-theoretic answer to this question leads to the
notion of a pseudorandom generator, which is a central object; see Section 10.2. This notion is very
useful in itself and is also a template for several other key deﬁnitions in cryptography, including
that of encryption (see Section 10.4).
Private key versus public key:
Solutions to the encryption problem today come in two distinct
ﬂavors. In private-key cryptography, one assumes that the two (or more) parties participating in
the protocol share a private “key” —namely, a statistically random string of modest size—that is
not known to the eavesdropper2. In a public-key encryption system (a concept introduced by Diﬃe
and Hellman in 1976 [DH76]) we drop this assumption. Instead, a party P picks a pair of keys:
an encryption key and decryption key, both chosen at random from some (correlated) distribution.
The encryption key will be used to encrypt messages to P and is considered public —i.e., published
and known to everybody including the eavesdropper.
The decryption key is kept secret by P
and is used to decrypt messages. A famous public-key encryption scheme is based upon the RSA
function of Example 10.4. At the moment we do not know how to base public key encryption on
the sole assumption that one-way functions exist and current constructions require the assumption
that there exist one-way functions with some special structure (such as RSA, factoring-based, and
Lattice-based one way functions). Most topics described in this chapter are traditionally labeled
private key cryptography.
10.1
Hard-on-average problems and one-way functions
A basic cryptographic primitive is a one-way function. Roughly speaking, this is a function f that is
easy to compute but hard to invert. Notice that if f is not one-to-one, then the inverse f−1(x) may
not be unique. In such cases “inverting” means that given f(x) the algorithm is able to produce
some preimage, namely, any element of f−1(f(x))). We say that the function is one-way function
if inversion is diﬃcult for the “average” (or “many”) x. Now we deﬁne this formally; a discussion
of this deﬁnition appears below in Section 10.1.1. A function family (gn) is a family of functions
where gn takes n-bit inputs. It is polynomial-time computable if there is a polynomial-time TM
that given an input x computes g|x|(x).
on general graphs, but on most n-node graphs is solvable in quadratic time or less. A deeper study of average case
complexity appears in Chapter 15.
2Practically, this could be ensured with a face-to-face meeting that might occur long before the transmission of
messages.
Web draft 2007-01-08 21:59

DRAFT
10.1. HARD-ON-AVERAGE PROBLEMS AND ONE-WAY FUNCTIONS
p10.3 (189)
Definition 10.1 (One-way function)
A family of functions {fn : {0, 1}n 7→{0, 1}m(n)} is ϵ(n) one-way with security s(n) if it is
polynomial-time computable and furthermore for every algorithm A that runs in time s(n),
Prx∈{0,1}n[A inverts fn(x)] ≤ϵ(n).
(1)
Now we give a few examples and discuss the evidence that they are hard to invert “on average
inputs.”
Example 10.2
The ﬁrst example is motivated by the fact that ﬁnding the prime factors of a given integer is
the famous FACTORING problem, for which the best current algorithm has running time about
2O(n1/3) (and even that bounds relies on the truth of some unproven conjectures in number theory).
The hardest inputs for current algorithms appear to be of the type x · y, where x, y are random
primes of roughly equal size.
Here is a ﬁrst attempt to deﬁne a one-way function using this observation. Let {fn} be a family
of functions where fn :{0, 1}n × {0, 1}n →{0, 1}2n is deﬁned as fn([x]2, [y]2) = [x · y]2. If x and y
are primes —which by the Prime Number Theorem happens with probability Θ(1/n2) when x, y
are random n-bit integers— then fn seems hard to invert. It is widely believed that there are
c > 1, f > 0 such that family fn is (1 −1/nc)-one-way with security parameter 2nf .
An even harder version of the above function is obtained by using the existence of a randomized
polynomial-time algorithm A (which we do not describe) that, given 1n, generates a random n-bit
prime number. Suppose A uses m random bits, where m = poly(n). Then A may be seen as
a (deterministic) mapping from m-bit strings to n-bit primes. Now let function ˜fm map (r1, r2)
to [A(r1) · A(r2)]2, where A(r1), A(r2) are the primes output by A using random strings r1, r2
respectively. This function seems hard to invert for almost all r1, r2. (Note that any inverse (r′
1, r′
2)
for ˜fm(r1, r2) allows us to factor the integer A(r1) · A(r2) since unique factorization implies that
the prime pair A(r′
1), A(r′
2) must be the same as A(r1), A(r2).) It is widely conjecture that there
are c > 1, f > 0 such that ˜fn is 1/nc-one-way with security parameter 2nf .
The FACTORING problem, a mainstay of modern cryptography, is of course the inverse of
multiplication. Who would have thought that the humble multiplication, taught to children in
second grade, could be the source of such power? The next two examples also rely on elementary
mathematical operations such as exponentiation, albeit with modular arithmetic.
Example 10.3
Let p1, p2, . . . be a sequence of primes where pi has i bits. Let gi be the generator of the group
Z∗
pi, the set of numbers that are nonzero mod pi. Then for every y ∈1, .., pi −1, there is a unique
x ∈{1, .., p −1} such that
gx
i ≡y
(mod pi).
Web draft 2007-01-08 21:59

DRAFT
p10.4 (190)
10.1. HARD-ON-AVERAGE PROBLEMS AND ONE-WAY FUNCTIONS
Then x →gx
i (mod pi) is a permutation on 1, .., pi −1 and is conjectured to be one-way. The
inversion problem is called the DISCRETE LOG problem.
We show below using random self-
reducibility that if it is hard on worst-case inputs, then it is hard on average.
We list some more conjectured one-way functions.
Example 10.4
RSA function. Let m = pq where p, q are large random primes and e be a random number coprime
to φ(m) = (p −1)(q −1). Let Z∗
m be the set of integers in [1, . . . , m] coprime to m. Then the
function is deﬁned to be fp,q,e(x) = xe (mod m). This function is used in the famous RSA public-
key cryptosystem.
Rabin function. For a composite number m, deﬁne fm(x) = x2 (mod m). If we can invert
this function on a 1/poly(log m) fraction of inputs then we can factor m in poly(log m) time (see
exercises).
Both the RSA and Rabin functions are useful in public-key cryptography. They are examples
of trapdoor one-way functions: if the factors of m (the “trapdoor” information) are given as well
then it is easy to invert the above functions. Trapdoor functions are fascinating objects but will
not be studied further here.
Random subset sum. Let m = 10n. Let the inputs to f be n positive m-bit integers a1, a2, . . . , an,
and a subset S of {1, 2, . . . , n}. Its output is (a1, a2, . . . , an, P
i∈S ai). Note that f maps n(m+1)-bit
inputs to nm + m bits.
When the inputs are randomly chosen, this function seems hard to invert. It is conjectured that
there is c > 1, d > 0 such that this function is 1/nc-one-way with security 2nd.
10.1.1
Discussion of the deﬁnition of one-way function
We will always assume that the the one-way function under consideration is such that the security
parameter s(n) is superpolynomial, i.e., larger than nk for every k > 0. The functions described
earlier are actually believed to be one-way with a larger security parameter 2nϵ for some ﬁxed ϵ > 0.
Of greater interest is the error parameter ϵ(n), since it determines the fraction of inputs for
which inversion is easy. Clearly, a continuum of values is possible, but two important cases to
consider are (i) ϵ(n) = (1 −1/nc) for some ﬁxed c > 0, in other words, the function is diﬃcult to
invert on at least 1/nc fraction of inputs. Such a function is often called a weak one-way function.
The simple one-way function fn of Example 10.2 is conjectured to be of this type. (ii) ϵ(n) < 1/nk
for every k > 1. Such a function is called a strong one-way function.
Yao showed that if weak one-way functions exist then so do strong one-way functions. We will
prove this surprising theorem (actually, something close to it) in Chapter 17. We will not use
it in this chapter, except as a justiﬁcation for our intuition that strong one-way functions exist.
Web draft 2007-01-08 21:59

DRAFT
10.2. WHAT IS A RANDOM-ENOUGH STRING?
p10.5 (191)
(Another justiﬁcation is of course the empirical observation that the candidate one-way functions
mentioned above do seem appear diﬃcult to invert on most inputs.)
10.1.2
Random self-reducibility
Roughly speaking, a problem is random-self-reducible if solving the problem on any input x reduces
to solving the problem on a sequence of random inputs y1, y2, . . . , where each yi is uniformly
distributed among all inputs. To put it more intuitively, the worst-case can be reduced to the
average case. Hence the problem is either easy on all inputs, or hard on most inputs. (In other
words, we can exclude the possibility that problem is easy on almost all the inputs but not all.) If
a function is one-way and also randomly self-reducible then it must be a strong one-way function.
This is best illustrated with an example.
Theorem 10.5
Suppose A is an algorithm with running time t(n) that, given a prime p, a generator g for Z∗
p,
and an input gx( mod p), manages to ﬁnd x for δ fraction of x ∈Z∗
p. Then there is a randomized
algorithm A′ with running time O(
1
δ log 1/ϵ(t(n) + poly(n))) that solves DISCRETE LOG on every
input with probability at least 1 −ϵ.
Proof: Suppose we are given y = gx( mod p) and we are trying to ﬁnd x. Repeat the following
trial O(1/(δ log 1/ϵ)) times: “Randomly pick r ∈{0, 1, . . . , p −2} and use A to try to compute
the logarithm of y · gr(modp). Suppose A outputs z. Check if gz−r(modp) is y, and if so, output
z −r(mod(p −1)) as the answer.”
The main observation is that if r is randomly chosen, then y ·gr( mod p) is randomly distributed
in Z∗
p and hence the hypothesis implies that A has a δ chance of ﬁnding its discrete log. After
O(1/(δ log 1/ϵ) trials, the probability that A failed every time is at most ϵ. ■
Corollary 10.6
If for any inﬁnite sequence of primes p1, p2, . . . , DISCRETE LOG mod pi is hard on worst-case
x ∈Z∗
pi, then it is hard for almost all x.
Later as part of the proof of Theorem 10.14 we give another example of random self-reducibility:
linear functions over GF(2).
10.2
What is a random-enough string?
Cryptography often becomes much easier if we have an abundant supply of random bits. Here is
an example.
Example 10.7 (One-time pad)
Suppose the message sender and receiver share a long string r of random bits that is not available
to eavesdroppers. Then secure communication is easy. To encode message m ∈{0, 1}n, take the
ﬁrst n bits of r, say the string s. Interpret both strings as vectors in GF(2)n and encrypt m by the
vector m+s. The receiver decrypts this message by adding s to it (note that s+s = 0 in GF(2)n).
Web draft 2007-01-08 21:59

DRAFT
p10.6 (192)
10.2. WHAT IS A RANDOM-ENOUGH STRING?
If s is statistically random, then so is m + s. Hence the eavesdropper provably cannot obtain even
a single bit of information about m regardless of how much computational power he expends.
Note that reusing s is a strict no-no (hence the name “one-time pad”).
If the sender ever
reuses s to encrypt another message m′ then the eavesdropper can add the two vectors to obtain
(m + s) + (m′ + s) = m + m′, which is some nontrivial information about the two messages.
Of course, the one-time pad is just a modern version of the old idea of using “codebooks” with
a new key prescribed for each day.
One-time pads are conceptually simple, but impractical to use, because the users need to agree
in advance on a secret pad that is large enough to be used for all their future communications. It
is also hard to generate because sources of quality random bits (e.g., those based upon quantum
phenomena) are often too slow. Cryptography’s suggested solution to such problems is to use a
pseudorandom generator. This is a deterministically computable function g:{0, 1}n →{0, 1}nc (for
some c > 1) such that if x ∈{0, 1}n is randomly chosen, then g(x) “looks” random. Thus so long as
users have been provided a common n-bit random string, they can use the generator to produce nc
“random looking” bits, which can be used to encrypt nc−1 messages of length n. (In cryptography
this is called a stream cipher.)
Clearly, at this point we need an answer to the question posed in the Section’s title! Philosophers
and statisticians have long struggled with this question.
Example 10.8
What is a random-enough string? Here is Kolmogorov’s deﬁnition: A string of length n is random
if no Turing machine whose description length is < 0.99n (say) outputs this string when started on
an empty tape. This deﬁnition is the “right” deﬁnition in some philosophical and technical sense
(which we will not get into here) but is not very useful in the complexity setting because checking
if a string is random according to this deﬁnition is undecidable.
Statisticians have also attempted deﬁnitions which boil down to checking if the string has the
“right number” of patterns that one would expect by the laws of statistics, e.g. the number of times
11100 appears as a substring. (See Knuth Volume 3 for a comprehensive discussion.) It turns out
that such deﬁnitions are too weak in the cryptographic setting: one can ﬁnd a distribution that
passes these statistical tests but still will be completely insecure if used to generate the pad for the
one-time pad encryption scheme.
10.2.1
Blum-Micali and Yao deﬁnitions
Now we introduce two complexity-theoretic deﬁnitions of pseudorandomness due to Blum-Micali
and Yao in the early 1980s. For a string y ∈{0, 1}n and S ⊆[n], we let y|S denote the projection
of Y to the coordinates of S. In particular, y|[1..i] denotes the ﬁrst i bits of y.
Web draft 2007-01-08 21:59

DRAFT
10.2. WHAT IS A RANDOM-ENOUGH STRING?
p10.7 (193)
The Blum-Micali deﬁnition is motivated by the observation that one property (in fact, the deﬁn-
ing property) of a statistically random sequence of bits y is that given y|[1..i], we cannot predict
yi+1 with odds better than 50/50 regardless of the computational power available to us. Thus one
could deﬁne a “pseudorandom” string by considering predictors that have limited computational
resources, and to show that they cannot achieve odds much better than 50/50 in predicting yi+1
from y|[1..i]. Of course, this deﬁnition has the shortcoming that any single ﬁnite string would be
predictable for a trivial reason: it could be hardwired into the program of the predictor Turing
machine. To get around this diﬃculty the Blum-Micali deﬁnition (and also Yao’s deﬁnition below)
deﬁnes pseudorandomness for distributions of strings rather than for individual strings. Further-
more, the deﬁnition concerns an inﬁnite sequence of distributions, one for each input size.
Definition 10.9 (Blum-Micali)
Let {gn} be a polynomial-time computable family of functions, where gn : {0, 1}n →{0, 1}m and
m = m(n) > n. We say the family is (ϵ(n), t(n))-unpredictable if for every probabilistic polynomial-
time algorithm A that runs in time t(n) and every large enough input size n,
Pr[A(g(x)[1..i]) = g(x)i+1] ≤1
2 + ϵ(n),
where the probability is over the choice of x ∈{0, 1}n , i ∈{1, . . . , n} , and the randomness used by
A.
If for every ﬁxed k, the family {gn} is (1/nc, nk)-unpredictable for every c > 1, then we say in
short that it is unpredictable by polynomial-time algorithms.
Remark 10.10
Allowing the tester to be an arbitrary polynomial-time machine makes perfect sense in a crypto-
graphic setting where we wish to assume nothing about the adversary except an upperbound on
her computational power.
Pseudorandom generators proposed in the pre-complexity era, such as the popular linear or
quadtratic congruential generators do not satisfy the Blum-Micali deﬁnition because bit-prediction
can in fact be done in polynomial time.
Yao gave an alternative deﬁnition in which the tester machine is given access to the entire string
at once. This deﬁnition implicitly sets up a test of randomness analogous to the more famous Turing
test for intelligence (see Figure 10.2). The tester machine A is given a string y ∈{0, 1}nc that is
produced in one of two ways: it is either drawn from the uniform distribution on {0, 1}nc or
generated by taking a random string x ∈{0, 1}n and stretching it using a deterministic function
g : {0, 1}n →{0, 1}nc. The tester is asked to output “1” if the string looks random to it and 0
otherwise. We say that g is a pseudorandom generator if no polynomial-time tester machine A has
a great chance of being able to determine which of the two distributions the string came from.
Definition 10.11 ([Yao82])
Let {gn} be a polynomial-time computable family of functions, where gn : {0, 1}n →{0, 1}m and
m = m(n) > n.
We say it is a (δ(n), s(n))-pseudorandom generator if for every probabilistic
algorithm A running in time s(n) and for all large enough n
|Pry∈{0,1}nc[A(y) = 1] −Prx∈{0,1}n[A(gn(x)) = 1]| ≤δ(n).
(2)
Web draft 2007-01-08 21:59

DRAFT
p10.8 (194)
10.2. WHAT IS A RANDOM-ENOUGH STRING?
We call δ(n) the distinguishing probability and s(n) the security parameter.
If for every c′, k > 1, the family is (1/nc′, nk)-pseudorandom then we say in short that it is a
pseudorandom generator.
Figure unavailable in pdf ﬁle.
Figure 10.2: Yao’s deﬁnition: If c > 1 then g : {0, 1}n →{0, 1}nc is a pseudorandom generator if no polynomial-
time tester has a good chance of distinguishing between truly random strings of length nc and strings generated by
applying g on random n-bit strings.
10.2.2
Equivalence of the two deﬁnitions
Yao showed that the above two deﬁnitions are equivalent —up to minor changes in the security
parameter, a family is a pseudorandom generator iﬀit is (bitwise) unpredictable.
The hybrid
argument used in this proof has become a central idea of cryptography and complexity theory.
The nontrivial direction of the equivalence is to show that pseudorandomness of the Blum-
Micali type implies pseudorandomness of the Yao type.
Not surprisingly, this direction is also
more important in a practical sense.
Designing pseudorandom generators seems easier for the
Blum-Micali deﬁnition —as illustrated by the Goldreich-Levin construction below— whereas Yao’s
deﬁnition seems more powerful for applications since it allows the adversary unrestricted access to
the pseudorandom string. Thus Yao’s theorem provides a bridge between what we can prove and
what we need.
Theorem 10.12 (Prediction vs. Indistinguishability [?])
Let Let gn :{0, 1}n →{0, 1}N(n) be a family of functions where N(n) = nk for some
k > 1.
If gn is ( ϵ(n)
N(n), 2t(n))-unpredictable where t(n) ≥N(n)2 then it is (ϵ(n), t(n))-
pseudorandom.
Conversely, if gn is (ϵ(n), t(n))-pseudorandom, then it is (ϵ(n), t(n))-unpredictable.
Proof: The
converse part is trivial since a bit-prediction algorithm can in particular be used to distinguish g(x)
from random strings of the same length. It is left to the reader.
Let N be shorthand for N(n). Suppose g is not (ϵ(n), t(n))-pseudorandom, and A is a distin-
guishing algorithm that runs in t(n) time and satisﬁes:
 Pr
x∈Bn[A(g(x)) = 1] −
Pr
y∈{0,1}N[A(y) = 1]
 > ϵ(n).
(3)
By considering either A or the algorithm that is A with the answer ﬂipped, we can assume that
the |·| can be removed and in fact
Pr
x∈Bn[A(g(x)) = 1] −
Pr
y∈{0,1}N[A(y) = 1] > ϵ(n).
(4)
Web draft 2007-01-08 21:59

DRAFT
10.2. WHAT IS A RANDOM-ENOUGH STRING?
p10.9 (195)
Consider B, the following bit-prediction algorithm. Let its input be g(x)|≤i where x ∈{0, 1}n
and i ∈{0, . . . , N −1} are chosen uniformly at random. B’s program is: “Pick bits ui+1, ui+2, . . . , uN
randomly and run A on the input g(x)|≤iui+1ui+2 . . . uN. If A outputs 1, output ui+1 else output
ui+1.” Clearly, B runs in time less than t(n) + O(N(n)) < 2t(n). To complete the proof we show
that B predicts g(x)i+1 correctly with probability at least 1
2 + ϵ(n)
N .
Consider a sequence of N + 1 distributions D0 through DN deﬁned as follows (in all cases,
x ∈{0, 1}n and u1, u2, . . . , uN ∈{0, 1} are assumed to be chosen randomly)
D0 = u1u2u3u4 · · · uN
D1 = g(x)1u2u3 · · · uN
...
...
Di = g(x)≤iui+1 · · · uN
...
...
DN = g(x)1g(x)2 · · · g(x)N
Furthermore, we denote by Di the distribution obtained from Di by ﬂipping the ith bit (i.e.,
replacing g(x)i by g(x)i). If D is any of these 2(N +1) distributions then we denote Pry∈D[A(y) = 1]
by q(D). With this notation we rewrite (4) as
q(DN) −q(D0) > ϵ(n).
(5)
Furthermore, in Di, the (i + 1)th bit is equally likely to be g(x)i+1 and g(x)i+1, so
q(Di) = 1
2(q(Di+1) + q(Di+1)),
(6)
Now we analyze the probability that B predicts g(x)i+1 correctly. Since i is picked randomly we
have
Pr
i,x[B is correct] = 1
N
n−1
X
i=0
1
2

Pr
x,u[B’s guess for g(x)i+1 is correct | ui+1 = g(x)i+1]
+
Pr
x,u[B’s guess for g(x)i+1 is correct | ui+1 = g(x)i+1]

.
Since B’s guess is ui+1 iﬀA outputs 1 this is
=
1
2N
N−1
X
i=0
(q(Di+1) + 1 −q(Di+1))
= 1
2 + 1
2N
N−1
X
i=0
(q(Di+1) −q(Di+1))
Web draft 2007-01-08 21:59

DRAFT
p10.10 (196)10.3. ONE-WAY FUNCTIONS AND PSEUDORANDOM NUMBER GENERATORS
From (6), q(Di+1) −q(Di+1) = 2(q(Di+1) −q(Di)), so this becomes
= 1
2 + 1
2N
N−1
X
i=0
2(q(Di+1) −q(Di))
= 1
2 + 1
N (q(DN) −q(D0))
> 1
2 + ϵ(n)
N .
This ﬁnishes our proof. ■
10.3
One-way functions and pseudorandom number generators
Do pseudorandom generators exist? Surprisingly the answer (though we will not prove it in full
generality) is that they do if and only if one-way functions exist.
Theorem 10.13
One-way functions exist iﬀpseudorandom generators do.
Since we had several plausible candidates for one-way functions in Section 10.1, this result helps
us design pseudorandom generators using those candidate one-way functions. If the pseudorandom
generators are ever proved to be insecure, then the candidate one-way functions were in fact not
one-way, and so we would obtain (among other things) eﬃcient algorithms for FACTORING and
DISCRETE LOG.
The “if” direction of Theorem 10.13 is trivial: if g is a pseudorandom generator then it must
also be a one-way function since otherwise the algorithm that inverts g would be able to distinguish
its outputs from random strings. The “only if” direction is more diﬃcult and involves using a
one-way function to explicitly construct a pseudorandom generator. We will do this only for the
special case of one-way functions that are permutations, namely, they map {0, 1}n to {0, 1}n in a
one-to-one and onto fashion. As a ﬁrst step, we describe the Goldreich-Levin theorem, which gives
an easy way to produce one pseudorandom bit, and then describe how to produce nc pseudorandom
bits.
10.3.1
Goldreich-Levin hardcore bit
Let {fn} be a one-way permutation where fn :{0, 1}n →{0, 1}n. Clearly, the function g :{0, 1}n ×
{0, 1}n →{0, 1}2n deﬁned as g(x, r) = (f(x), r) is also a one-way permutation. Goldreich and
Levin showed that given (f(x), r), it is diﬃcult for a polynomial-time algorithm to predict x ⊙r,
the scalar product of x and r (mod 2). Thus even though the string (f(x), r) in principle contains
all the information required to extract (x, r), it is computationally diﬃcult to extract even the
single bit x ⊙r. This bit is called a hardcore bit for the permutation. Prior to the Goldreich-Levin
result we knew of hardcore bits for some speciﬁc (conjectured) one-way permutations, not all.
Web draft 2007-01-08 21:59

DRAFT
10.3. ONE-WAY FUNCTIONS AND PSEUDORANDOM NUMBER GENERATORSp10.11 (197)
Theorem 10.14 (Goldreich-Levin Theorem)
Suppose that {fn} is a family of ϵ(n))-one-way permutation with security s(n). Let
S(n) = (min
n
s(n),
1
ϵ(n)
o
)1/8 Then for all algorithms A running in time S(n)
Prx,r∈{0,1}n[A(fn(x), r) = x ⊙r] ≤1
2 + O(
1
S(n)).
(7)
Proof: Sup-
pose that some algorithm A can predict x⊙r with probability 1/2+δ in time t(n). We show how to
invert fn(x) for O(δ) fraction of the inputs in O(n3t(n)/δ4) time, from which the theorem follows.
Claim 10.15
Suppose that
Prx,r∈{0,1}n[A(fn(x), r) = x ⊙r] ≥1
2 + δ.
(8)
Then for at least δ fraction of x’s
Prr∈{0,1}n[A(fn(x), r) = x ⊙r] ≥1
2 + δ
2.
(9)
Proof: We use an averaging argument. Suppose that p is the fraction of x’s satisfying (9). We
have p · 1 + (1 −p)(1/2 + δ/2) ≥1/2 + δ. Solving this with respect to p, we obtain
p ≥
δ
2(1/2 −δ/2) ≥δ.
■
We design an inversion algorithm that given fn(x), where x ∈R {0, 1}n, will try to recover x.
It succeeds with high probability if x is such that (9) holds, in other words, for at least δ fraction
of x. Note that the algorithm can always check the correctness of its answer, since it has fn(x)
available to it and it can apply fn to its answer and see if fn(x) is obtained.
Warmup: Reconstruction when the probability in (9) is ≥3/4 + δ.
Let P be any program that computes some unknown linear function over GF(2)n but errs on
some inputs. Speciﬁcally, there is an unknown vector x ∈GF(2)n such that
Pr
r [P(r) = x · r] = 3/4 + δ.
(10)
Then we show to add a simple “correction” procedure to turn P into a probabilistic program
P ′ such that
∀r
Pr[P ′(r) = x · r] ≥1 −1
n2 .
(11)
(Once we know how to compute x · r for every r with high probability, it is easy to recover x
bit-by-bit using the observation that if ei is the n-bit vector that is 1 in the ith position and zero
elsewhere then x · ei = ai, the ith bit of a.)
Web draft 2007-01-08 21:59

DRAFT
p10.12 (198)10.3. ONE-WAY FUNCTIONS AND PSEUDORANDOM NUMBER GENERATORS
“On input r, repeat the following trial O(log n/δ2) times. Pick y randomly from GF(2)n and
compute the bit P(r + y) + P(y). At the end, output the majority value.”
The main observation is that when y is randomly picked from GF(2)n then r + y and y are
both randomly distributed in GF(2)n, and hence the probability that P(r + y) ̸= a · (r + y) or
P(y) ̸= a · y is at most 2 · (1/4 −δ) = 1/2 −2δ. Thus with probability at least 1/2 + 2δ, each trial
produces the correct bit. Then Chernoﬀbounds imply that probability is at least 1 −1/n2 that
the ﬁnal majority is correct.
General Case:
The idea for the general case is very similar, the only diﬀerence being that this time we want to
pick r1, . . . , rm so that we already “know” x ⊙ri. The preceding statement may appear ridiculous,
since knowing the inner product of x with m ≥n random vectors is, with high probability, enough
to reconstruct x (see exercises). The explanation is that the ri’s will not be completely random.
Instead, they will be pairwise independent. Recall the following construction of a set of pairwise
independent vectors: Pick k random vectors t1, t2, . . . , tk ∈GF(2)n and for each nonempty S ⊆
{1, . . . , k} deﬁne YS = P
i∈S ti. This gives 2k −1 vectors and for S ̸= S′ the random variables
YS, YS′ are independent of each other.
Now let us describe the observation at the heart of the proof. Suppose m = 2k −1 and our
random strings r1, . . . , rm are {YS}’s from the previous paragraph. Then x ⊙YS = x ⊙(P
i∈S ti) =
P
i∈S x ⊙ti. Hence if we know x ⊙ti for i = 1, . . . , k, we also know x ⊙YS. Of course, we don’t
actually know x ⊙ti for i = 1, . . . , k since x is unknown and the ti’s are random vectors. But we
can just try all 2k possibilities for the vector (x ⊙ti)i=1,...,k and run the rest of the algorithm for
each of them. Whenever our “guess” for these innerproducts is correct, the algorithm succeeds in
producing x and this answer can be checked by applying fn on it (as already noted). Thus the
guessing multiplies the running time by a factor 2k, which is only m. This is why we can assume
that we know x ⊙YS for each subset S.
The details of the rest of the algorithm are similar to before. Pick m pairwise independent
vectors YS’s such that, as described above, we “know” x ⊙YS for all S. For each i = 1, 2, . . . , n,
and each S run A on the input (fn(x), YS ⊕ei) (where YS ⊕ei is YS with its ith entry ﬂipped).
Compute the majority value of A(fn(x), YS ⊕ei) −x ⊙YS among all S’s and use it as your guess
for xi.
Suppose x ∈GF(2)n satisﬁes (9). We will show that this algorithm produces all n bits of x
with probability at least 1/2. Fix i. For each i, the guess for xi is a majority of m bits. The
expected number of bits among these that agree with xi is m(1/2 + δ/2), so for the majority vote
to result in the incorrect answer it must be the case that the number of incorrect values deviates
from its expectation by more than mδ/2. Now, we can bound the variance of this random variable
and apply Chebyshev’s inequality (Lemma A.16 in Appendix A) to conclude that the probability
of such a deviation is ≤
4
mδ2 .
Here is the calculation using Chebyshev’s inequality. Let ξS denote the event that A produces
the correct answer on (fn(x), YS ⊕ei). Since x satisﬁes (9) and YS ⊕ei is randomly distributed over
GF(2)n, we have E(ξS) = 1/2+δ/2 and V ar(ξS) = E(ξS)(1−E(ξS)) < 1. Let ξ = P
S ξS denote the
number of correct answers on a sample of size m. By linearity of expectation, E[ξ] = m(1/2+δ/2).
Furthermore, the YS’s are pairwise independent, which implies that the same is true for the outputs
ξS’s produced by the algorithm A on them. Hence by pairwise independence V ar(ξ) < m. Now, by
Web draft 2007-01-08 21:59

DRAFT
10.4. APPLICATIONS
p10.13 (199)
Chebyshev’s inequality, the probability that the majority vote is incorrect is at most 4V ar(ξ)
m2δ2
≤
4
mδ2 .
Finally, setting m > 8/nδ2, the probability of guessing the ith bit incorrectly is at most 1/2n.
By the union bound, the probability of guessing the whole word incorrectly is at most 1/2. Hence,
for every x satisfying (9), we can ﬁnd the preimage of f(x) with probability at least 1/2, which
makes the overall probability of inversion at least δ/2. The running time is about m2n× (running
time of A), which is n3
δ4 × t(n), as we had claimed. ■
10.3.2
Pseudorandom number generation
We saw that if f is a one-way permutation, then g(x, r) = (f(x), r, x ⊙r) is a pseudorandom
generator that stretches 2n bits to 2n + 1 bits. Stretching to even more bits is easy too, as we
now show. Let fi(x) denote the i-th iterate of f on x (i.e., f(f(f(· · · (f(x))))) where f is applied
i times).
Theorem 10.16
If f is a one-way permutation then gN(x, r) = (r, x ⊙r, f(x) ⊙r, f2(x) ⊙r, . . . , fN(x) ⊙r) is a
pseudorandom generator for N = nc for any constant c > 0.
Proof: Since any distinguishing machine could just reverse the string as a ﬁrst step, it clearly
suﬃces to show that the string (r, fN(x) ⊙r, fN−1(x) ⊙r, . . . , f(x) ⊙r, x ⊙r) looks pseudorandom.
By Yao’s theorem (Theorem 10.12), it suﬃces to show the diﬃculty of bit-prediction. For contra-
diction’s sake, assume there is a PPT machine A such that when x, r ∈{0, 1}n and i ∈{1, . . . , N}
are randomly chosen,
Pr[A predicts fi(x) ⊙r given (r, fN(x) ⊙r, fN−1(x) ⊙r, . . . , fi+1(x) ⊙r)] ≥1
2 + ϵ.
We describe an algorithm B that given f(z), r where z, r ∈{0, 1}n are randomly chosen, predicts
the hardcore bit z ⊙r with reasonable probability, which contradicts Theorem 10.14.
Algorithm B picks i ∈{1, . . . , N} randomly. Let x ∈{0, 1}n be such that fi(x) = z. There is
of course no eﬃcient way for B to ﬁnd x, but for any l ≥1, B can eﬃciently compute fi+l(x) =
fl−1(f(z))! So it produces the string r, fN(x) ⊙r, fN−1(x) ⊙r, . . . , fi+1(x) ⊙r and uses it as input
to A. By assumption, A predicts fi(x) ⊙r = z ⊙r with good odds. Thus we have derived a
contradiction to Theorem 10.14. ■
10.4
Applications
Now we give some applications of the ideas introduced in the chapter.
10.4.1
Pseudorandom functions
Pseudorandom functions are a natural generalization of (and are easily constructed using) pseudo-
random generators. This is a function g : {0, 1}m × {0, 1}n →{0, 1}m. For each K ∈{0, 1}m we
denote by g|K the function from {0, 1}n to {0, 1}m deﬁned by g|K(x) = g(K, x). Thus the family
contains 2m functions from {0, 1}n to {0, 1}m, one for each K.
Web draft 2007-01-08 21:59

DRAFT
p10.14 (200)
10.4. APPLICATIONS
We say g is a pseudorandom function generator if it passes a “Turing test” of randomness
analogous to that in Yao’s deﬁnition of a pseudorandom generator (Deﬁnition 10.11).
Recall that the set of all functions from {0, 1}n to {0, 1}m, denoted Fn,m , has cardinality
(2m)2n. The PPT machine is presented with an “oracle” for a function from {0, 1}n to {0, 1}n. The
function is one of two types: either a function chosen randomly from Fn,m, or a function f|K where
K ∈{0, 1}m is randomly chosen. The PPT machine is allowed to query the oracle in any points
of its choosing. We say f|K is a pseudorandom function generator if for all c > 1 the PPT has
probability less than n−c of detecting which of the two cases holds. (A completely formal deﬁnition
would resemble Deﬁnition 10.1 and talk about a family of generators, one for each n. Then m is
some function of n.)
Figure unavailable in pdf ﬁle.
Figure 10.3: Constructing a pseudorandom function from {0, 1}n to {0, 1}m using a random key K ∈{0, 1}m and
a length-doubling pseudorandom generator g :{0, 1}m →{0, 1}2m.
Now we describe a construction of a pseudorandom function generator g from a length-doubling
pseudorandom generator f :{0, 1}m →{0, 1}2m. For any K ∈{0, 1}m let TK be a complete binary
tree of depth n whose each node is labelled with an m-bit string. The root is labelled K. If a node
in the tree has label y then its left child is labelled with the ﬁrst m bits of f(y) and the right child
is labelled with the last m bits of f(y). Now we deﬁne g(K, x). For any x ∈{0, 1}n interpret x as
a label for a path from root to leaf in TK in the obvious way and output the label at the leaf. (See
Figure 10.3.)
We leave it as an exercise to prove that this construction is correct.
A pseudorandom function generator is a way to turn a random string K into an implicit de-
scription of an exponentially larger “random looking” string, namely, the table of all values of the
function g|K. This has proved a powerful primitive in cryptography; see the next section. Further-
more, pseudorandom function generators have also ﬁgured in a very interesting explanation of why
current lowerbound techniques have been unable to separate P from NP; see Chapter ??.
10.4.2
Private-key encryption: deﬁnition of security
We hinted at a technique for private-key encryption in our discussion of a one-time pad (including
the pseudorandom version) at the start of Section 10.2. But that discussion completely omitted
what the design goals of the encryption scheme were. This is an important point: design of insecure
systems often traces to a misunderstanding about the type of security ensured (or not ensured) by
an underlying protocol.
The most basic type of security that a private-key encryption should ensure is semantic security.
Informally speaking, this means that whatever can be computed from the encrypted message is also
computable without access to the encrypted message and knowing only the length of the message.
The formal deﬁnition is omitted here but it has to emphasize the facts that we are talking about
an ensemble of encryption functions, one for each message size (as in Deﬁnition 10.1) and that the
encryption and decryption is done by probabilistic algorithms that use a shared private key, and
Web draft 2007-01-08 21:59

DRAFT
10.4. APPLICATIONS
p10.15 (201)
that for every message the guarantee of security holds with high probability with respect to the
choice of this private key.
Now we describe an encryption scheme that is semantically secure. Let f :{0, 1}n × {0, 1}n →
{0, 1}n be a pseudorandom function generator. The two parties share a secret random key K ∈
{0, 1}n. When one of them wishes to send a message x ∈{0, 1}n to the other, she picks a random
string r ∈{0, 1}n and transmits (r, x ⊕fK(r)). To decrypt the other party computes fK(r) and
then XORs this string with the last n bits in the received text.
We leave it as an exercise to show that this scheme is semantically secure.
10.4.3
Derandomization
The existence of pseudorandom generators implies subexponential deterministic algorithms for
BPP: this is usually referred to as derandomization of BPP. (In this case, the derandomization
is only partial since it results in a subexponential deterministic algorithm. Stronger complexity
assumptions imply a full derandomization of BPP, as we will see in Chapter 16.)
Theorem 10.17
If for every c > 1 there is a pseudorandom generator that is secure against circuits of size nc, then
BPP ⊆∩ε>0DTIME(2nε).
Proof: Let us ﬁx an ε > 0 and show that BPP ⊆DTIME(2nε).
Suppose that M is a BPP machine running in nk time. We can build another probabilistic
machine M′ that takes nε random bits, streches them to nk bits using the pseudorandom generator
and then simulates M using this nk bits as a random string. Obviously, M′ can be simulated by
going over all binary strings nε, running M′ on each of them, and taking the majority vote.
It remains to prove that M and M′ accept the same language. Suppose otherwise. Then there
exists an inﬁnite sequence of inputs x1, . . . , xn, . . . on which M distinguishes a truly random string
from a pseudorandom string with a high probability, because for M and M′ to produce diﬀerent
results, the probability of acceptance should drop from 2/3 to below 1/2. Hence we can build a
distinguisher similar to the one described in the previous theorem by hardwiring these inputs into
a circuit family. ■
The above theorem shows that the existence of hard problems implies that we can reduce the
randomness requirement of algorithms. This “hardness versus randomness” tradeoﬀis studied more
deeply in Chapter 16.
Remark 10.18
There is an interesting connection to discrepancy theory, a ﬁeld of mathematics. Let S be a set of
subsets of {0, 1}n. Subset A ⊂{0, 1}n has discrepancy ϵ with respect to S if for every s ∈S,

|s ∩A|
|S|
−|A|
2n
 ≤ϵ.
Our earlier result that BPP ⊆P/poly showed the existence of polynomial-size sets A that have
low discrepancy for all sets deﬁned by polynomial-time Turing machines (we only described dis-
crepancy for the universe {0, 1}n but one can deﬁne it for all input sizes using lim sup). The goal
of derandomization is to explicitly construct such sets; see Chapter 16.
Web draft 2007-01-08 21:59

DRAFT
p10.16 (202)
10.4. APPLICATIONS
10.4.4
Tossing coins over the phone and bit commitment
How can two parties A and B toss a fair random coin over the phone?
(Many cryptographic
protocols require this basic primitive.) If only one of them actually tosses a coin, there is nothing
to prevent him from lying about the result. The following ﬁx suggests itself: both players toss a
coin and they take the XOR as the shared coin. Even if B does not trust A to use a fair coin, he
knows that as long as his bit is random, the XOR is also random. Unfortunately, this idea also
does not work because the player who reveals his bit ﬁrst is at a disadvantage: the other player
could just “adjust” his answer to get the desired ﬁnal coin toss.
This problem is addressed by the following scheme, which assumes that A and B are polynomial
time turing machines that cannot invert one-way permutations. The protocol itself is called bit
commitment. First, A chooses two strings xA and rA of length n and sends a message (fn(xA), rA),
where fn is a one-way permutation. This way, A commits the string xA without revealing it. Now
B selects a random bit b and conveys it. Then A reveals xA and they agree to use the XOR of b
and (xA ⊙rA) as their coin toss. Note that B can verify that xA is the same as in the ﬁrst message
by applying fn, therefore A cannot change her mind after learning B’s bit. On the other hand, by
the Goldreich–Levin theorem, B cannot predict xA ⊙rA from A’s ﬁrst message, so this scheme is
secure.
10.4.5
Secure multiparty computations
This concerns a vast generalization of the setting in Section 10.4.4. There are k parties and the ith
party holds a string xi ∈{0, 1}n. They wish to compute f(x1, x2, . . . , xk) where f :{0, 1}nk →{0, 1}
is a polynomial-time computable function known to all of them. (The setting in Section 10.4.4 is
a subcase whereby each xi is a bit —randomly chosen as it happens—and f is XOR.) Clearly,
the parties can just exchange their inputs (suitably encrypted if need be so that unauthorized
eavesdroppers learn nothing) and then each of them can compute f on his/her own. However, this
leads to all of them knowing each other’s input, which may not be desirable in many situations.
For instance, we may wish to compute statistics (such as the average) on the combination of several
medical databases that are held by diﬀerent hospitals. Strict privacy and nondisclosure laws may
forbid hospitals from sharing information about individual patients. (The original example Yao
gave in introducing the problem was of k people who wish to compute the average of their salaries
without revealing their salaries to each other.)
We say that a multiparty protocol for computing f is secure if at the end no party learns
anything new apart from the value of f(x1, x2, . . . , xk). The formal deﬁnition is inspired by the
deﬁnition of a pseudorandom generator, and states that for each i, the bits received by party i
during the protocol should be computationally indistinguishable from completely random bits3.
It is completely nonobvious why such protocols must exist. Yao [Yao86] proved existence for
k = 2 and Goldreich, Micali, Wigderson [GMW87] proved existence for general k. We will not
3Returning to our medical database example, we see that the hospitals can indeed compute statistics on their
combined databases without revealing any information to each other —at least any information that can be extracted
feasibly. Nevetheless, it is unclear if current privacy laws allow hospitals to perform such secure multiparty protocols
using patient data— an example of the law lagging behind scientiﬁc progress.
Web draft 2007-01-08 21:59

DRAFT
10.5. RECENT DEVELOPMENTS
p10.17 (203)
describe this protocol in any detail here except to mention that it involves “scrambling” the circuit
that computes f.
10.4.6
Lowerbounds for machine learning
In machine learning the goal is to learn a succinct function f : {0, 1}n →{0, 1} from a sequence
of type (x1, f(x1)), (x2, f(x2)), . . . , where the xi’s are randomly-chosen inputs.
Clearly, this is
impossible in general since a random function has no succinct description. But suppose f has a
succinct description, e.g. as a small circuit. Can we learn f in that case?
The existence of pseudorandom functions implies that even though a function may be polynomial-
time computable, there is no way to learn it from examples in polynomial time. In fact it is possible
to extend this impossibility result (though we do not attempt it) to more restricted function families
such as NC1 (see Kearns and Valiant [KV94]).
10.5
Recent developments
The earliest cryptosystems were designed using the SUBSET SUM problem. They were all shown to
be insecure by the early 1980s. In the last few years, interest in such problems —and also the related
problems of computing approximate solutions to the shortest and nearest lattice vector problems—
has revived, thanks to a one-way function described in Ajtai [Ajt96], and a public-key cryptosystem
described in Ajtai and Dwork [AD97] (and improved on since then by other researchers). These
constructions are secure on most instances iﬀthey are secure on worst-case instances. (The idea
used is a variant of random self-reducibility.)
Also, there has been a lot of exploration of the exact notion of security that one needs for various
cryptographic tasks. For instance, the notion of semantic security in Section 10.4.2 may seem quite
strong, but researchers subsequently realized that it leaves open the possibility of some other kinds
of attacks, including chosen ciphertext attacks, or attacks based upon concurrent execution of
several copies of the protocol. Achieving security against such exotic attacks calls for many ideas,
most notably zero knowledge (a brief introduction to this concept appears in Section ??).
Chapter notes and history
In the 1940s, Shannon speculated about topics reminiscent of complexity-based cryptography. The
ﬁrst concrete proposal was made by Diﬃe and Hellman [DH76], though their cryptosystem was later
broken. The invention of the RSA cryptosystem (named after its inventors Ron Rivest, Adi Shamir,
and Len Adleman) [RSA78] brought enormous attention to this topic. In 1981 Shamir [Sha83]
suggested the idea of replacing a one-time pad by a pseudorandom string. He also exhibited a weak
pseudorandom generator assuming the average-case intractability of the RSA function. The more
famous papers of Blum and Micali [BM84] and then Yao [Yao82] laid the intellectual foundations
of private-key cryptography. (The hybrid argument used by Yao is a stronger version of one in
an earlier important manuscript of Goldwasser and Micali [GM84] that proposed probabilistic
encryption schemes.)
The construction of pseudorandom functions in Section 10.4.1 is due to
Goldreich, Goldwasser, and Micali [GGM86]. The question about tossing coins over a telephone
Web draft 2007-01-08 21:59

DRAFT
p10.18 (204)
10.5. RECENT DEVELOPMENTS
was raised in an inﬂuential paper of Blum [Blu82]. Today complexity-based cryptography is a vast
ﬁeld with several dedicated conferences. Goldreich [Gol04]’s two-volume book gives a deﬁnitive
account.
A scholarly exposition of number theoretic algorithms (including generating random primes
and factoring integers) appears in Victor Shoup’s recent book [?] and the book of Bach and Shal-
lit [BS96].
Theorem 10.13 and its very technical proof is in H˙astad et al. [HILL99] (the relevant conference
publications are a decade older).
Our proof of the Goldreich-Levin theorem is usually attributed to Rackoﬀ(unpublished).
Exercises
§1 Show that if P = NP then one-way functions and pseudorandom generators do not exist.
§2 (Requires just a little number theory). Prove that if some algorithm inverts the Rabin func-
tion fm(x) = x2 (mod m) on a 1/poly(log m) fraction of inputs then we can factor m in
poly(log m) time.
Hint: Suppose m = pq where p, q are prime numbers. Then x2
has 4 “square roots” modulo m.
§3 Show that if f is a one-way permutation then so is fk (namely, f(f(f(· · · (f(x))))) where f
is applied k times) where k = nc for some ﬁxed c > 0.
§4 Assuming one-way functions exist, show that the above fails for one-way functions.
Hint: You have to design a one-way function where f k is not
one-way.
§5 Suppose a ∈GF(2)m is an unknown vector.
Let r1, r2, . . . , rm ∈GF(2)m be randomly
chosen, and a ⊙ri revealed to us for all i = 1, 2, . . . , m. Describe a deterministic algorithm
to reconstruct a from this information, and show that the probability (over the choice of the
ri’s) is at least 1/4 that it works.
Hint: You need to show that a certain determinant is nonzero.
This shows that the “trick” in Goldreich-Levin’s proof is necessary.
§6 Suppose somebody holds an unknown n-bit vector a.
Whenever you present a randomly
chosen subset of indices S ⊆{1, . . . , n}, then with probability at least 1/2 + ϵ, she tells you
the parity of the all the bits in a indexed by S. Describe a guessing strategy that allows you
to guess a (an n bit string!) with probability at least ( ϵ
n)c for some constant c > 0.
§7 Suppose g : {0, 1}n →{0, 1}n+1 is any pseudorandom generator. Then use g to describe a
pseudorandom generator that stretches n bits to nk for any constant k > 1.
§8 Show the correctness of the pseudorandom function generator in Section 10.4.1.
Web draft 2007-01-08 21:59

DRAFT
10.5. RECENT DEVELOPMENTS
p10.19 (205)
Hint: Use a hybrid argument which replaces the labels on the
ﬁrst k levels of the tree by completely random strings. Note that
the random labels do not need to be assigned ahead of time —
this would take at least 2k time —but can be assigned on the ﬂy
whenever they are needed by the distinguishing algorithm.
§9 Formalize the deﬁnition of semantic security and show that the encryption scheme in Sec-
tion 10.4.2 is semantically secure.
Hint: First show that for all message pairs x, y their encryptions
are indistinguishable by polynomial-time algorithms.
Why does
this suﬃce?
Web draft 2007-01-08 21:59

DRAFT
p10.20 (206)
10.5. RECENT DEVELOPMENTS
Web draft 2007-01-08 21:59

DRAFT
Part II
Lowerbounds for Concrete
Computational Models
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p10.21 (207)

DRAFT

DRAFT
p10.23 (209)
In the next few chapters the topic will be concrete complexity, the study of lowerbounds on
models of computation such as decision trees, communication games, circuits, etc. Algorithms or
devices considered in this lecture take inputs of a ﬁxed size n, and we study the complexity of these
devices as a function of n.
Web draft 2007-01-08 21:59

DRAFT
p10.1 (210)
Web draft 2007-01-08 21:59

DRAFT
Chapter 11
Decision Trees
A decision tree is a model of computation used to study the number of bits of an input that
need to be examined in order to compute some function on this input. Consider a function f :
{0, 1}n →{0, 1}. A decision tree for f is a tree for which each node is labelled with some xi, and
has two outgoing edges, labelled 0 and 1. Each tree leaf is labelled with an output value 0 or 1.
The computation on input x = x1x2 . . . xn proceeds at each node by inspecting the input bit xi
indicated by the node’s label. If xi = 1 the computation continues in the subtree reached by taking
the 1-edge. The 0-edge is taken if the bit is 0. Thus input x follows a path through the tree. The
output value at the leaf is f(x). An example of a simple decision tree for the majority function is
given in Figure 11.1
Figure unavailable in pdf ﬁle.
Figure 11.1: A decision tree for computing the majority function Maj(x1, x2, x3) on three bits. Outputs 1 if at
least two input bits are 1, else outputs 0.
Recall the use of decision trees in the proof of the lower bound for comparison-based sorting
algorithms. That study can be recast in the above framework by thinking of the input —which
consisted of n numbers — as consisting of
 n
2

bits, each giving the outcome of a pairwise comparison
between two numbers.
We can now deﬁne two useful decision tree metrics.
Definition 11.1
The cost of tree t on input x, cost(t, x), is the number of bits of x examined by t.
Definition 11.2
The decision tree complexity of function f, D(f), is deﬁned as follows, where T below refers to
the set of decision trees that decide f.
D(f) = min
t∈T
max
x∈{0,1}n cost(t, x)
(1)
The decision tree complexity of a function is the number of bits examined by the most eﬃcient
decision tree on the worst case input to that tree. We are now ready to consider several examples.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p11.2 (211)

DRAFT
p11.3 (212)
Example 11.3
(Graph connectivity) Given a graph G as input, in adjacency matrix form, we would like to know
how many bits of the adjacency matrix a decision tree algorithm might have to inspect in order to
determine whether G is connected. We have the following result.
Theorem 11.4
Let f be a function that computes the connectivity of input graphs with m vertices. Then D(f) =
 m
2

.
The idea of the proof of this theorem is to imagine an adversary that constructs a graph, edge by
edge, in response to the queries of a decision tree. For every decision tree that decides connectivity,
the strategy implicitly produces an input graph which requires the decision tree to inspect each of
the
 m
2

possible edges in a graph of m vertices.
Adversary Strategy:
Whenever the decision tree algorithm asks about edge ei,
answer “no” unless this would force the graph to be disconnected.
After i queries, let Ni be the set of edges for which the adversary has replied “no”, Yi the set
of edges for which the adversary has replied “yes”. and Ei the set of edges not yet queried. The
adversary’s strategy maintains the invariant that Yi is a disconnected forest for i <
 m
2

and Yi ∪Ei
is connected. This ensures that the decision tree will not know whether the graph is connected
until it queries every edge.
Example 11.5
(OR Function) Let f(x1, x2, . . . xn) = Wn
i=1 xi. Here we can use an adversary argument to show
that D(f) = n. For any decision tree query of an input bit xi, the adversary responds that xi
equals 0 for the ﬁrst n −1 queries. Since f is the OR function, the decision tree will be in suspense
until the value of the nth bit is revealed. Thus D(f) is n.
Example 11.6
Consider the AND-OR function, with n = 2k. We deﬁne fk as follows.
fk(x1, . . . , xn) =





fk−1(x1, . . . x2k−1−1) ∧fk−1(x2k−1, . . . x2k)
if k is even
fk−1(x1, . . . x2k−1−1) ∨fk−1(x2k−1, . . . x2k)
if k > 1 and is odd
xi
if k = 1
(2)
A diagram of a circuit that computes the AND-OR function is shown in Figure 11.2. It is left as
an exercise to prove, using induction, that D(fk) = 2k.
Web draft 2007-01-08 21:59

DRAFT
11.1. CERTIFICATE COMPLEXITY
p11.4 (213)
Figure unavailable in pdf ﬁle.
Figure 11.2: A circuit showing the computation of the AND-OR function. The circuit has k layers of alternating
gates, where n = 2k.
11.1
Certiﬁcate Complexity
We now introduce the notion of certiﬁcate complexity, which, in a manner analogous to decision
tree complexity above, tells us the minimum amount of information needed to be convinced of the
value of a function f on input x.
Definition 11.7
Consider a function f : {0, 1}n →{0, 1}. If f(x) = 0, then a 0-certiﬁcate for x is a sequence of
bits in x that proves f(x) = 0. If f(x) = 1, then a 1-certiﬁcate is a sequence of bits in x that
proves f(x) = 1.
Definition 11.8
The certiﬁcate complexity C(f) of f is deﬁned as follows.
C(f) = max
x:input {number of bits in the smallest 0- or 1- certiﬁcate for x}
(3)
Example 11.9
If f is a function that decides connectivity of a graph, a 0-certiﬁcate for an input must prove that
some cut in the graph has no edges, hence it has to contain all the possible edges of a cut of the
graph. When these edges do not exist, the graph is disconnected. Similarly, a 1-certiﬁcate is the
edges of a spanning tree. Thus for those inputs that represent a connected graph, the minimum
size of a 1-certiﬁcate is the number of edges in a spanning tree, n −1. For those that represent a
disconnected graph, a 0 certiﬁcate is the set of edges in a cut. The size of a 0-certiﬁcate is at most
(n/2)2 = n2/4, and there are graphs (such as the graph consisting of two disjoint cliques of size
n/2) in which no smaller 0-certiﬁcate exists. Thus C(f) = n2/4.
Example 11.10
We show that the certiﬁcate complexity of the AND-OR function fk of Example 11.6 is 2⌈k/2⌉.
Recall that fk is deﬁned using a circuit of k layers. Each layer contains only OR-gates or only
AND-gates, and the layers have alternative gate types. The bottom layer receives the bits of input
x as input and the single top layer gate outputs the answer fk(x). If f(x) = 1, we can construct
a 1-certiﬁcate as follows. For every AND-gate in the tree of gates we have to prove that both its
children evaluate to 1, whereas for every OR-gate we only need to prove that some child evaluates
to 1. Thus the 1-certiﬁcate is a subtree in which the AND-gates have two children but the OR gates
only have one each. Thus the subtree only needs to involve 2⌈k/2⌉input bits. If f(x) = 0, a similar
Web draft 2007-01-08 21:59

DRAFT
p11.5 (214)
11.1. CERTIFICATE COMPLEXITY
argument applies, but the role of OR-gates and AND-gates, and values 1 and 0 are reversed. The
result is that the certiﬁcate complexity of fk is 2⌈k/2⌉, or about √n.
The following is a rough way to think about these concepts in analogy to Turing machine
complexity as we have studied it.
low decision tree complexity ↔
P
(4)
low 1-certiﬁcate complexity ↔
NP
(5)
low 0-certiﬁcate complexity ↔
coNP
(6)
The following result shows, however, that the analogy may not be exact since in the decision tree
world, P = NP ∩coNP. It should be noted that the result is tight, for example for the AND-OR
function.
Theorem 11.11
For function f, D(f) ≤C(f)2.
Proof: Let S0, S1 be the set of minimal 0-certiﬁcates and 1-certiﬁcates, respectively, for f. Let
k = C(f), so each certiﬁcate has at most k bits.
Remark 11.12
Note that every 0-certiﬁcate must share a bit position with every 1-certiﬁcate, and furthermore,
assign this bit diﬀerently. If this were not the case, then it would be possible for both a 0-certiﬁcate
and 1-certiﬁcate to be asserted at the same time, which is impossible.
The following decision tree algorithm then determines the value of f in at most k2 queries.
Algorithm: Repeat until the value of f is determined: Choose a remaining 0-certiﬁcate from S0
and query all the bits in it. If the bits are the values that prove the f to be 0, then stop. Otherwise,
we can prune the set of remaining certiﬁcates as follows. Since all 1-certiﬁcates must intersect the
chosen 0-certiﬁcate, for any c1 ∈S1, one bit in c1 must have been queried here. Eliminate c1 from
consideration if the certifying value of c1 at at location is diﬀerent from the actual value found.
Otherwise, we only need to consider the remaining k −1 bits of c1.
This algorithm can repeat at most k times.
For each iteration, the unﬁxed lengths of the
uneliminated 1-certiﬁcates decreases by one. This is because once some values of the input have
been ﬁxed due to queries, for any 0-certiﬁcate, it remains true that all 1-certiﬁcates must intersect
it in at least one location that has not been ﬁxed, otherwise it would be possible for both a 0-
certiﬁcate and a 1-certiﬁcate to be asserted. With at most k queries for at most k iterations, a
total of k2 queries is used. ■
Web draft 2007-01-08 21:59

DRAFT
11.2. RANDOMIZED DECISION TREES
p11.6 (215)
11.2
Randomized Decision Trees
There are two equivalent ways to look at randomized decision trees. We can consider decision trees
in which the branch taken at each node is determined by the query value and by a random coin
ﬂip. We can also consider probability distributions over deterministic decision trees. The analysis
that follows uses the latter model.
We will call P a probability distribution over a set of decision trees T that compute a particular
function. P(t) is then the probability that tree t is chosen from the distribution. For a particular
input x, then, we deﬁne c(P, x) = P
tinT P(t)cost(t, x). c(P, x) is thus the expected number of
queries a tree chosen from T will make on input x. We can then characterize how well randomized
decision trees can operate on a particular problem.
Definition 11.13
The randomized decision tree complexity, R(f), of f, is deﬁned as follows.
R(f) = min
P max
x
c(P, x)
(7)
The randomized decision tree complexity thus expresses how well the best possible probability
distribution of trees will do against the worst possible input for a particular probability distribution
of trees. We can observe immediately that R(f) ≥C(f). This is because C(f) is a minimum value
of cost(t, x). Since R(f) is just an expected value for a particular probability distribution of these
cost values, the minimum such value can be no greater than the expected value.
Example 11.14
Consider the majority function, f = Maj(x1, x2, x3). It is straightforward to see that D(f) = 3.
We show that R(f) ≤8/3. Let P be a uniform distribution over the (six) ways of ordering the
queries of the three input bits. Now if all three bits are the same, then regardless of the order
chosen, the decision tree will produce the correct answer after two queries. For such x, c(P, x) = 2.
If two of the bits are the same and the third is diﬀerent, then there is a 1/3 probability that the
chosen decision tree will choose the two similar bits to query ﬁrst, and thus a 1/3 probability that
the cost will be 2. There thus remains a 2/3 probability that all three bits will need to be inspected.
For such x, then, c(P, x) = 8/3. Therefore, R(f) is at most 8/3.
How can we prove lowerbounds on randomized complexity? For this we need another concept.
11.3
Lowerbounds on Randomized Complexity
needs cleanup now
To prove lowerbounds on randomized complexity, it suﬃces by Yao’s Lemma (see Section 11.6)
to prove lowerbounds on distributional complexity. Where randomized complexity explores distribu-
tions over the space of decision trees for a problem, distributional complexity considers probability
distributions on inputs. It is under such considerations that we can speak of “average case analysis.”
Web draft 2007-01-08 21:59

DRAFT
p11.7 (216)
11.3. LOWERBOUNDS ON RANDOMIZED COMPLEXITY
Let D be a probability distribution over the space of input strings of length n. Then, if A is
a deterministic algorithm, such as a decision tree, for a function, then we deﬁne the distributional
complexity of A on a function f with inputs distributed according to D as the expected cost for
algorithm A to compute f, where the expectation is over the distribution of inputs.
Definition 11.15
The distributional complexity d(A, D) of algorithm A given inputs distributed according to D
is deﬁned as:
d(A, D) =
X
x:input
D(x)cost(A, x) = Ex∈D[cost(A, x)]
(8)
From this we can characterize distributional complexity as a function of a single function f
itself.
Definition 11.16
The distributional decision tree complexity, ∆(f) of function f is deﬁned as:
∆(f) = max
D
min
A d(A, D)
(9)
Where A above runs over the set of decision trees that are deciders for f.
So the distributional decision tree complexity measures the expected eﬃciency of the most
eﬃcient decision tree algorithm works given the worst case distribution of inputs.
The following theorem follows from Yao’s lemma.
Theorem 11.17
R(f) = ∆(f).
So in order to ﬁnd a lower bound on some randomized algorithm, it suﬃces to ﬁnd a lower
bound on ∆(f). Such a lower bound can be found by postulating an input distribution D and
seeing whether every algorithm has expected cost at least equal to the desired lower bound.
Example 11.18
We return to considering the majority function, and we seek to ﬁnd a lower bound on ∆(f).
Consider a distribution over inputs such that inputs in which all three bits match, namely 000
and 111, occur with probability 0. All other inputs occur with probability 1/6. For any decision
tree, that is, for any order in which the three bits are examined, there is exactly a 1/3 probability
that the ﬁrst two bits examined will be the same value, and thus there is a 1/3 probability that
the cost is 2. There is then a 2/3 probability that the cost is 3. Thus the overall expected cost
for this distribution is 8/3.
This implies that ∆(f) ≥8/3 and in turn that R(f) ≥8/3.
So
∆(f) = R(f) = 8/3.
Web draft 2007-01-08 21:59

DRAFT
11.4. SOME TECHNIQUES FOR DECISION TREE LOWERBOUNDS
p11.8 (217)
11.4
Some techniques for decision tree lowerbounds
Definition 11.19 (Sensitivity)
If f : {0, 1}n →{0, 1} is a function and x ∈{0, 1}n then the sensitivity of f on x, denoted sx(f),
is the number of bit positions i such that f(x) ̸= f(xi), where xi is x with its ith bit ﬂipped. The
sensitivity of f, denoted s(f), is maxx {sx(f)}.
The block sensitivity of f on x, denoted bsx(f), is the maximum number b such that there are
disjoint blocks of bit positions B1,2 , . . . , Bb such that f(x) ̸= f(xBi) where xBi is x with all its bits
ﬂipped in block Bi. The block sensitivity of f denoted bs(f) is maxx {bsx(f)}.
It is conjectured that there is a constant c (as low as 2) such that bs(f) = O(s(f)c) for all f but
this is wide open. The following easy observation is left as an exercise.
Lemma 11.20
For any function, s(f) ≤bs(f) ≤D(f).
Theorem 11.21 (Nisan)
C(f) ≤s(f)bs(f).
Proof: For any input x ∈{0, 1}n we describe a certiﬁcate for x of size s(f)bs(f). This certiﬁcate
is obtained by considering the largest number of disjoint blocks of variables B1, B2, . . . , Bb that
achieve b = bsx(f) ≤bs(f). We claim that setting these variables according to x constitutes a
certiﬁcate for x.
Suppose not, and let x′ be an input that is consistent with the above certiﬁcate. Let Bb+1 be
a block of variables such that x′ = xBb+1. Then Bb+1 must be disjoint from B1, B2, . . . Bb, which
contradicts b = bsx(f).
Note that each of B1, B2, . . . , Bb has size at most s(f) by deﬁnition of s(f), and hence the size
of the certiﬁcate we have exhibited is at most s(f)bs(f). ■
Recent work on decision tree lowerbounds has used polynomial representations of boolean func-
tions. Recall that a multilinear polynomial is a polynomial whose degree in each variable is 1.
Definition 11.22
An n-variate polynomial p(x1, x2, . . . , xn) represents f : {0, 1}n →{0, 1} if p(x) = f(x) for all
x ∈{0, 1}n.
The degree of f, denoted deg(f), is the degree of the multilinear polynomial that represents f.
(The exercises ask you to show that the multilinear polynomial representation is unique, so deg(f)
is well-deﬁned.)
Example 11.23
The AND of n variables x1, x2, . . . , xn is represented by the multilinear polynomial Qn
i=1 xi and
OR is represented by 1 −Qn
i=1(1 −xi).
Web draft 2007-01-08 21:59

DRAFT
p11.9 (218)
11.5. COMPARISON TREES AND SORTING LOWERBOUNDS
The degree of AND and OR is n, and so is their decision tree complexity. There is a similar
connection for other problems too, but it is not as tight. The ﬁrst part of the next theorem is an
easy exercise; the second part is nontrivial.
Theorem 11.24
1. deg(f) ≤D(f).
2. (Nisan-Smolensky) D(f) ≤deg(f)2bs(f) ≤O(deg(f)4).
11.5
Comparison trees and sorting lowerbounds
to be written
11.6
Yao’s MinMax Lemma
This section presents Yao’s minmax lemma, which is used in a variety of settings to prove lower-
bounds on randomized algorithms. Therefore we present it in a very general setting.
Let X be a ﬁnite set of inputs and A be a ﬁnite set of algorithms that solve some computational
problem on these inputs. For x ∈X, a ∈A, we denote by cost(A, x) the cost incurred by algorithm
A on input x. A randomized algorithm is a probability distribution R on A. The cost of R on
input x, denoted cost(R, x), is EA∈R[cost(A, x)]. The randomized complexity of the problem is
min
R max
x∈X cost(R, x).
(10)
Let D be a distribution on inputs. For any deterministic algorithm A, the cost incurred by it
on D, denoted cost(A, D), is Ex∈D[cost(A, x)]. The distributional complexity of the problem is
max
D
min
A∈A cost(A, D).
(11)
Yao’s Lemma says that these two quantitities are the same. It is easily derived from von Neu-
mann’s minmax theorem for zero-sum games, or with a little more work, from linear programming
duality.
Yao’s lemma is typically used to lowerbound randomized complexity. To do so, one deﬁnes
(using some insight and some luck) a suitable distribution D on the inputs. Then one proves that
every deterministic algorithm incurs high cost, say C, on this distribution. By Yao’s Lemma, it
follows that the randomized complexity then is at least C.
Exercises
§1 Suppose f is any function that depends on all its bits; in other words, for each bit position i
there is an input x such that f(x) ̸= f(xi). Show that s(f) = Ω(log n).
§2 Consider an f deﬁned as follows. The n-bit input is partitioned into ⌊√n⌋blocks of size
about √n. The function is 1 iﬀthere is at least one block in which two consecutive bits are 1
and the remaining bits in the block are 0. Estimate s(f), bs(f), C(f), D(f) for this function.
Web draft 2007-01-08 21:59

DRAFT
11.6. YAO’S MINMAX LEMMA
p11.10 (219)
§3 Show that there is a unique multilinear polynomial that represents f :{0, 1}n →{0, 1}. Use
this fact to ﬁnd the multilinear representation of the PARITY of n variables.
§4 Show that deg(f) ≤D(f).
Chapter notes and history
The result that the decision tree complexity of connectivity and many other problems is
 n
2

has
motivated the following conjecture (atributed variously to Anderaa, Karp, Yao):
Every monotone graph property has D(·) =
 n
2

.
Here “monotone” means that adding edges to the graph cannot make it go from having the
property to not having the property (e.g., connectivity). “Graph property” means that the property
does not depend upon the vertex indices (e.g., the property that vertex 1 and vertex 2 have an
edge between them). This conjecture is known to be true up to a O(1) factor; the proof uses
topology and is excellently described in Du and Ko [DK00]. A more ambitious conjecture is that
even the randomized decision tree complexity of monotone graph properties is Ω(n2) but here the
best lowerbound is close to n4/3.
The polynomial method for decision tree lowerbounds is surveyed in Buhrman and de Wolf [BdW02].
The method can be used to lowerbound randomized decision tree complexity (and more recently,
quantum decision tree complexity) but then one needs to consider polynomials that approximately
represent the function.
Web draft 2007-01-08 21:59

DRAFT
p11.11 (220)
11.6. YAO’S MINMAX LEMMA
Web draft 2007-01-08 21:59

DRAFT
Chapter 12
Communication Complexity
Communication complexity concerns the following scenario. There are two players with unlimited
computational power, each of whom holds an n bit input, say x and y. Neither knows the other’s
input, and they wish to collaboratively compute f(x, y) where function f :{0, 1}n×{0, 1}n →{0, 1}
is known to both. Furthermore, they had foreseen this situation (e.g., one of the parties could be
a spacecraft and the other could be the base station on earth), so they had already —before they
knew their inputs x, y— agreed upon a protocol for communication1. The cost of this protocol is
the number of bits communicated by the players for the worst-case choice of x, y.
Researchers have studied many modiﬁcations of the above basic scenario, including randomized
protocols, nondeterministic protocols, average-case protocols (where x, y are assumed to come from
a distribution), multiparty protocols, etc. Truly, this is a self-contained mini-world within com-
plexity theory. Furthermore, lowerbounds on communication complexity have uses in a variety of
areas, including lowerbounds for parallel and VLSI computation, circuit lowerbounds, polyhedral
theory, data structure lowerbounds, etc. We give a very rudimentary introduction to this area; an
excellent and detailed treatment can be found in the book by Kushilevitz and Nisan [KN97].
12.1
Deﬁnition
Now we formalize the informal description of communication complexity given above.
A t-round communication protocol for f is a sequence of function pairs (S1, C1), (S2, C2), . . . , (St, Ct), (f1, f2).
The input of Si is the communication pattern of the ﬁrst i−1 rounds and the output is from {1, 2},
indicating which player will communicate in the ith round. The input of Ci is the input string of
this selected player as well as the communication pattern of the ﬁrst i −1 rounds. The output of
Ci is the bit that this player will communicate in the ith round. Finally, f1, f2 are 0/1-valued func-
tions that the players apply at the end of the protocol to their inputs as well as the communication
pattern in the t rounds in order to compute the output. These two outputs must be f(x, y). The
1Do not confuse this situation with information theory, where an algorithm is given messages that have to be
transmitted over a noisy channel, and the goal is to transmit them robustly while minimizing the amount of com-
munication. In communication complexity the channel is not noisy and the players determine what messages to
send.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p12.1 (221)

DRAFT
p12.2 (222)
12.2. LOWERBOUND METHODS
communication complexity of f is
C(f) =
min
protocols P max
x,y
{Number of bits exchanged by P on x, y.}
Notice, C(f) ≤n+1 since the trivial protocol is for one player to communicate his entire input,
whereupon the second player computes f(x, y) and communicates that single bit to the ﬁrst. Can
they manage with less communication?
Example 12.1 (Parity)
Suppose the function f(x, y) is the parity of all the bits in x, y. We claim that C(f) = 2. Clearly,
C(f) ≥2 since the function depends nontrivially on each input, so each player must transmit at
least one bit. Next, C(f) ≤2 since it suﬃces for each player to transmit the parity of all the bits
in his possession; then both know the parity of all the bits.
Remark 12.2
Sometimes students ask whether a player can communicate by not saying anything? (After all,
they have three options: send a 0, or 1, or not say anything in that round.) We can regard such
protocols as communicating with a ternary, not binary, alphabet, and analyze them analogously.
12.2
Lowerbound methods
Now we discuss methods for proving lowerbounds on communication complexity. As a running
example in this chapter, we will use the equality function:
EQ(x, y) =
(
1
if x = y
0
otherwise
We will see that C(EQ) ≥n.
12.2.1
Fooling set
We show C(EQ) ≥n. For contradiction’s sake, suppose a protocol exists whose complexity is
at most n −1. Then there are only 2n−1 communication patterns possible between the players.
Consider the set of all 2n pairs (x, x). Using the pigeonhole principle we conclude there exist two
pairs (x, x) and (x′, x′) on which the communication pattern is the same. Of course, thus far we
have nothing to object to, since the answers EQ(x, x) and EQ(x′, x′) on both pairs are 1. However,
now imagine giving one player x and the other player x′ as inputs. A moment’s thought shows
that the communication pattern will be the same as the one on (x, x) and (x′, x′). (Formally, this
can be shown by induction. If player 1 communicates a bit in the ﬁrst round, then clearly this bit
is the same whether his input is x or x′. If player 2 communicates in the 2nd round, then his bit
must also be the same on both inputs since he receives the same bit from player 1. And so on.)
Web draft 2007-01-08 21:59

DRAFT
12.2. LOWERBOUND METHODS
p12.3 (223)
Hence the player’s answer on (x, x) must agree with their answer on (x, x′). But then the protocol
must be incorrect, since EQ(x, x′) = 0 ̸= EQ(x, x).
The lowerbound argument above is called a fooling set argument. It is formalized as follows.
Definition 12.3
A fooling set for f :{0, 1}n × {0, 1}n →{0, 1} is a set S ⊆{0, 1}n × {0, 1}n and a value b ∈{0, 1}
such that:
1. For every (x, y) ∈S, f(x, y) = b.
2. For every two distinct pairs (x1, y1), (x2, y2) ∈S, either f(x1, y2) ̸= b or f(x2, y1) ̸= b.
Lemma 12.4
If f has a fooling set with m pairs then C(f) ≥log2 m.
Example 12.5 (Disjointness)
Let x, y be interpreted as characteristic vectors of subsets of {1, 2, . . . , n}. Let DISJ(x, y) = 1 if
these two subsets are disjoint, otherwise DISJ(x, y) = 0. Then C(DISJ) ≥n since the following 2n
pairs constitute a fooling set:
S =

(A, A) : A ⊆{1, 2, . . . , n}
	
.
12.2.2
The tiling lowerbound
The tiling lowerbound takes a more global view of f. Consider the matrix of f, denoted M(f),
which is a 2n × 2n matrix whose (x, y)’th entry is f(x, y).
See Figure 12.1.
We visualize the
Figure unavailable in pdf ﬁle.
Figure 12.1: Matrix M(f) for the equality function when the inputs to the players have 3 bits. The numbers in the
matrix are values of f.
communication protocol in terms of this matrix. A combinatorial rectangle (or just rectangle) in
the matrix is a submatrix corresponding to A × B where A ⊆{0, 1}n, B ⊆{0, 1}n. If the protocol
begins with the ﬁrst player sending a bit, then M(f) partitions into two rectangles of the type
A0 × {0, 1}n, A1 × Bn, where Ab is the subset of strings for which the ﬁrst player communicates
bit b. Notice, A0 ∪A1 = {0, 1}n. If the next bit is sent by the second player, then each of the two
rectangles above is further partitioned into two smaller rectangles depending upon what this bit
was. If the protocol continues for k steps, the matrix gets partitioned into 2k rectangles. Note that
each rectangle in the partition corresponds to a subset of input pairs for which the communication
sequence thus far has been identical. (See Figure 12.2 for an example.)
Web draft 2007-01-08 21:59

DRAFT
p12.4 (224)
12.2. LOWERBOUND METHODS
Figure unavailable in pdf ﬁle.
Figure 12.2: Two-way communication matrix after two steps. The large number labels are the concatenation of
the bit sent by the ﬁrst player with the bit sent by the second player.
If the protocol stops, then the value of f is determined within each rectangle, and thus must be
the same for all pairs x, y in that rectangle. Thus the set of all communication patterns must lead
to a partition of the matrix into monochromatic rectangles. (A rectangle A × B is monochromatic
if for all x in A and y in B, f(x, y) is the same.)
Definition 12.6
A monochromatic tiling of M(f) is a partition of M(f) into disjoint monochromatic rectangles.
We denote by χ(f) the minimum number of rectangles in any monochromatic tiling of M(f).
The following theorem is immediate from our discussion above.
Theorem 12.7
If f has communication complexity C then it has a monochromatic tiling with at most 2C rectangles.
Consequently, C ≥log2 χ(f).
The following observation shows that the tiling bound subsumes the fooling set bound.
Lemma 12.8
If f has a fooling set with m pairs, then χ(f) ≥m.
Proof: If (x1, y1) and (x2, y2) are two of the pairs in the fooling set, then they cannot be in a
monochromatic rectangle since not all of (x1, y1), (x2, y2), (x1, y2), (x2, y1) have the same f value.
■
12.2.3
Rank lowerbound
Now we introduce an algebraic method to lowerbound χ(f) (and hence communication complexity).
Recall the high school notion of rank of a square matrix: it is the size of the largest subset of
rows/colums that are independent. The following is another deﬁnition.
Definition 12.9
If a matrix has entries from a ﬁeld F then the rank of an n × n matrix M is the minimum value
of l such that M can be expressed as
M =
l
X
i=1
αiBi,
where αi ∈F \ {0} and each Bi is an n × n matrix of rank 1.
Note that 0, 1 are elements of every ﬁeld, so we can compute the rank over any ﬁeld we like. The
choice of ﬁeld can be crucial; see Problem 5 in the exercises.
The following theorem is trivial, since each monochromatic rectangle can be viewed (by ﬁlling
out entries outside the rectangle with 0’s) as a matrix of rank at most 1 .
Theorem 12.10
For every function f, χ(f) ≥rank(M(f)).
Web draft 2007-01-08 21:59

DRAFT
12.2. LOWERBOUND METHODS
p12.5 (225)
12.2.4
Discrepancy
The discrepancy of a rectangle A × B in M(f) is
1
22n |number of 1’s in A × B −number of 0’s in A × B| .
(1)
The discrepancy of the matrix M(f), denote Disc(f), is the largest discrepancy among all
rectangles. The following Lemma relates it to χ(f).
Lemma 12.11
χ(f) ≥
1
Disc(f).
Proof: For a monochromatic rectangle, the discrepancy is its size divided by 22n.
The total
number of entries in the matrix is 22n. The bound follows. ■
Example 12.12
Lemma 12.11 can be very loose. For the EQ() function, the discrepancy is at least 1−2−n (namely,
the discrepancy of the entire matrix), which would only give a lowerbound of 2 for χ(f). However,
χ(f) is at least 2n, as already noted.
Now we describe a method to upperbound the discrepancy using eigenvalues.
Lemma 12.13 (eigenvalue bound)
For any matrix M, the discrepancy of a rectangle A × B is at most λmax(M)
p
|A| |B|/22n, where
λmax(M) is the magnitude of the largest eigenvalue of M.
Proof: Let 1A, 1B ∈Rn denote the characteristic vectors of A, B. Then |1A|2 =
pP
i∈A 12 =
p
|A|.
The discrepancy of the rectangle A × B is
1
22n 1T
AM1B ≤
1
22n λmax(M)
1T
A1B
 ≤
1
22n λmax(M)
p
|A| |B|.
explain this.
■
Example 12.14
The mod 2 inner product function deﬁned as f(x, y) = (x · y)2 = P
i xiyi(mod2) has been encoun-
tered a few times in this book. To bound its discrepancy, we consider the matrix 2M(f) −1. This
transformation makes the range of the function {−1, 1} and will be useful again later. Let this new
Web draft 2007-01-08 21:59

DRAFT
p12.6 (226)
12.2. LOWERBOUND METHODS
matrix be denoted N. It is easily checked that every two distinct rows (columns) of N are orthog-
onal, every row has ℓ2 norm 2n/2, and that NT = N. Thus we conclude that N2 = 2nI where I is
the unit matrix. Hence every eigenvalue is either +2n/2 or −2n/2, and thus Lemma 12.13 implies
that the discrepancy of a rectangle A × B is at most 2n/2p
|A| |B| and the overall discrepancy is
at most 23n/2 (since |A| , |B| ≤2n).
A technique for upperbounding the discrepancy
Now we describe an upperbound technique for the discrepancy that will later be useful in the
multiparty setting (Section 12.3). For ease of notation, in this section we change the range of f to
{−1, 1} by replacing 1’s in M(f) with −1’s and replacing 0’s with 1’s. Note that now
Disc(f) = max
A,B
1
22n

X
a∈A,b∈B
f(a, b)

.
Definition 12.15
E(f) = Ea1,a2,b1,b2
hQ
i=1,2
Q
j=1,2 f(ai, bj)
i
.
Note that E(f) can be computed, like the rank, in polynomial time given the M(f) as input.
Lemma 12.16
Disc(f) ≤E(f)1/4.
Proof: The proof follows in two steps.
Claim 1: For every function h:{0, 1}n × {0, 1}n →{1, −1}, E(h) ≥(Ea,b[f(a, b)])4.
We will use the Cauchy-Schwartz inequality, speciﬁcally, the version according to which E[z2] ≥
(E[z])2 for every random variable z.
E(h) = Ea1,a2

Eb1,b2

Y
i=1,2
Y
j=1,2
h(ai, bj)




(2)
= Ea1,a2
h
(Eb[h(a1, b)h(a2, b)])2i
(3)
≥(Ea1,a2 [Eb[h(a1, b)h(a2, b)]])2
(Cauchy Schwartz)
(4)
≥(Ea,b[h(a, b)])4 .
(repeating prev. two steps)
(5)
Claim 2: For every function f there is a function h such that E(f) = E(h) and Ea,b[h(a, b)] ≥
Disc(f).
Web draft 2007-01-08 21:59

DRAFT
12.2. LOWERBOUND METHODS
p12.7 (227)
First, we note that for every two functions g1, g2 :{0, 1}n →{−1, 1}, if we deﬁne h = f ◦g1 ◦g2
as
h(a, b) = f(a, b)g1(a)g2(b)
then E(f) = E(h). The reason is that for all a1, a2, b1, b2,
Y
i=1,2
Y
j=1,1
h(ai, bj) = g1(a1)2g1(a2)2g2(b1)2g2(b2)2 Y
i=1,2
Y
j=1,2
f(ai, bj)
and the square of any value of g1, g2 is 1.
Now we prove Claim 2 using the probabilistic method. Deﬁne two random functions g1, g2 :
{0, 1}n →{−1, 1} as follows:
g1(a)
=
(
1
if a ∈A
ra
ra ∈{−1, 1} is randomly chosen
g2(b)
=
(
1
if b ∈B
sb
sb ∈{−1, 1} is randomly chosen
Let h = f ◦g1 ◦g2, and therefore E(h) = E(f). Furthermore
Eg1,g2 [Ea,b[h(a, b)]] = Ea,b [Eg1,g2[f(a, b)g1(a)g2(b)]]
(6)
=
1
22n
X
a∈A,b∈B
f(a, b)
(7)
= Disc(f)
(8)
where the second line follows from the fact that Eg1[g1(a)] = Eg2[g2(b)] = 0 for a ̸∈A and b ̸∈B.
Thus in particular there exist g1, g2 such that |Ea,b[h(a, b)]| ≥Disc(f). ■
12.2.5
Comparison of the lowerbound methods
As already noted, discrepancy upperbounds imply lowerbounds on χ(f). Of the other three meth-
ods, the tiling argument is the strongest, since it subsumes the other two. The rank method is the
weakest, since the rank lowerbound always implies a tiling lowerbound and a fooling set lowerbound
(the latter follows from Problem 3 in the exercises).
Also, we can separate the power of these lowerbound arguments. For instance, we know functions
for which there is a signiﬁcant gap between log χ(f) and log rank(M(f)). However, the following
conjecture (we only state one form of it) says that all three methods (except discrepancy, which as
already noted can be arbitrarily far from χ(f)) give the same bound up to a polynomial factor.
Conjecture 12.17 (log rank conjecture)
There is a constant c > 1 such that C(f) = O(log(rank(M(f)))c) for all f and all input sizes n.
Web draft 2007-01-08 21:59

DRAFT
p12.8 (228)
12.3. MULTIPARTY COMMUNICATION COMPLEXITY
12.3
Multiparty communication complexity
There is more than one way to generalize communication complexity to a multiplayer setting. The
most interesting model is the “number on the forehead” model often encountered in math puzzles
that involve people in a room, each person having a bit on their head which everybody else can
see but they cannot. More formally, there is some function f : ({0, 1}n)k →{0, 1}, and the input
is (x1, x2, . . . , xk) where each xi ∈{0, 1}n. The ith player can see all the xj such that j ̸= i. As
in the 2-player case, the k players have an agreed-upon protocol for communication, and all this
communication is posted on a “public blackboard”. At the end of the protocol all parties must
know f(x1, . . . , xk).
Example 12.18
Consider computing the function
f(x1, x2, x3) =
n
M
i=1
maj(x1i, x2i, x3i)
in the 3-party model where x1, x2, x3 are n bit strings. The communication complexity of this
function is 3: each player counts the number of i’s such that she can determine the majority of
x1i, x2i, x3i by examining the bits available to her. She writes the parity of this number on the
blackboard, and the ﬁnal answer is the parity of the players’ bits. This protocol is correct because
the majority for each row is known by either 1 or 3 players, and both are odd numbers.
Example 12.19 (Generalized Inner Product)
The generalized inner product function GIPk,n maps nk bits to 1 bit as follows
f(x1, . . . , xk) =
n
M
i=1
k^
j=1
xij.
(9)
Notice, for k = 2 this reduces to the mod 2 inner product of Example 12.14.
In the 2-party model we introduced the notion of a monochromatic rectangle in order to prove
lower bounds.
For the k-party case we will use cylinder intersections.
A cylinder in dimen-
sion i is a subset S of the inputs such that if (x1, . . . , xk) ∈S then for all x′
i we have that
(x1, . . . , xi−1, x′
i, xi+1, . . . , xk) ∈S also. A cylinder intersection is ∩k
i=1Ti where Ti is a cylinder in
dimension i.
As noted in the 2-party case, a communication protocol can be viewed as a way of partitioning
the matrix M(f). Here M(f) is a k-dimensional cube, and player i’s communication does not
depend upon xi. Thus we conclude that if f has a multiparty protocol that communicates c bits,
then its matrix has a tiling using at most 2c monochromatic cylinder intersections.
Web draft 2007-01-08 21:59

DRAFT
12.3. MULTIPARTY COMMUNICATION COMPLEXITY
p12.9 (229)
Lemma 12.20
If every partition of M(f) into monochromatic cylinder intersections requires at least R cylinder
intersections, then the k-party communication complexity isat least log2 R.
Discrepancy-based lowerbound
In this section, we will assume as in our earlier discussion of discrepancy that the range of the
function f is {−1, 1}. We deﬁne the k-party discrepancy of f by analogy to the 2-party case
Disc(f) =
1
2nk max
T

X
(a1,a2,...,ak)∈T
f(a1, a2, . . . , ak)

,
where T ranges over all cylinder intersections.
To upperbound the discrepancy we introduce the k-party analogue of E(f). Let a cube be a
set D in {0, 1}nk of 2k points of the form {a1,1, a2,1} × {a1,2, a2,2} × · · · × {a1,k, a2,k}, where each
ai,j ∈{0, 1}n.
E(f) = ED
" Y
a∈D
f(a)
#
.
Notice that the deﬁnition of E() for the 2-party case is recovered when k = 2. The next lemma
is also an easy generalization.
Lemma 12.21
Disc(f) ≤(E(f))1/2k.
Proof: The proof is analogous to Lemma 12.16 and left as an exercise. The only diﬀerence is that
instead of deﬁning 2 random functions we need to deﬁne k random functions g1, g2, gk :{0, 1}nk →
{−1, 1}, where gi depends on every one of the k coordinates except the ith. ■
Now we can prove a lowerbound for the Generalized Inner Product function. Note that since
we changed the range to {−1, 1} it is now deﬁned as
GIPk,n(x1, x2, . . . , xk) = (−1)
P
i≤n
Q
j≤k xij(mod2).
(10)
Theorem 12.22
The function GIPk,n has k-party communication complexity Ω(n/8k) as n grows larger.
Proof: We use induction on k. For k ≥1 let βk be deﬁned using β1 = 0 and βk+1 = 1+βk
2
. We
claim that
E(GIPk,n) ≤βn
k .
Web draft 2007-01-08 21:59

DRAFT
p12.10 (230)
12.4. PROBABILISTIC COMMUNICATION COMPLEXITY
Assuming truth for k −1 we prove for k.
A random cube D in {0, 1}nk is picked by picking
a11, a21 ∈{0, 1}n and then picking a random cube D′ in {0, 1}(k−1)n.
E(GIPk,n) = Ea11,a21

ED′


Y
a∈{a11,a21}×D′
GIPk,n(a)




(11)
The proof proceeds by considering the number of coordinates where strings a11 and a21 are identical.
Examining the expression for GIPk,n in (10) we see that these coordinates contribute nothing once
we multiply all the terms in the cube, since their contributions get squared and thus become 1.
The coordinates that contribute are
to be completed ■
12.4
Probabilistic Communication Complexity
Will deﬁne the model, give the protocol for EQ, and describe the discrepancy-based lowerbound.
12.5
Overview of other communication models
We outline some of the alternative settings in which communication complexity has been studied.
Nondeterministic protocols: These are deﬁned by analogy to NP. In a nondeterministic pro-
tocol, the players are both provided an additional third input z (“nondeterministic guess”).
Apart from this guess, the protocol is deterministic. The cost incurred on x, y is
min
z
{|z| + number of bits exchanged by protocol when guess is z} .
The nondeterministic communication complexity of f is the minimum k such that there is a
nondeterministic protocol whose cost for all input pairs is at most k.
In general, one can consider communication protocols analogous to NP, coNP, PH etc.
Randomized protocols: These are deﬁned by analogy to RP, BPP. The players are provided
with an additional input r that is chosen uniformly at random from m-bit strings for some
m. Randomization can signiﬁcantly reduce the need for communication. For instance we
can use ﬁngerprinting with random primes (explored in Chapter 7), to compute the equality
function by exchanging O(log n) bits: the players just pick a random prime p of O(log n) bits
and exchange x (mod p) and y (mod p).
Average case protocols: Just as we can study average-case complexity in the Turing machine
model, we can study communication complexity when the inputs are chosen from a distribu-
tion D. This is deﬁned as
CD(f) =
min
protocols P
X
x,y
Pr[(x, y) ∈D] × {Number of bits exchanged by P on x, y.}
Web draft 2007-01-08 21:59

DRAFT
12.6. APPLICATIONS OF COMMUNICATION COMPLEXITY
p12.11 (231)
Computing a non boolean function: Here the function’s output is not just {0, 1} but an m-bit
number for some m. We discuss one example in the exercises.
Asymmetric communication: The “cost” of communication is asymmetric: there is some B
such that it costs the ﬁrst player B times as much to transmit a bit than it does the second
player. The goal is to minimize the total cost.
Multiparty settings: The most obvious generalization to multiparty settings is whereby f has k
arguments x1, x2, . . . , xk and player i gets xi. At the end all players must know f(x1, x2, . . . , xk).
This is not as interesting as the so-called “number of the forehead” where player i can see all
of the input except for xi. We discuss it in Section ?? together with some applications.
Computing a relation: There is a relation R ⊆{0, 1}n × {0, 1}n × {1, 2, . . . , m} and given x, y ∈
Bn the players seek to agree on any b ∈{1, 2, . . . , m} such that (x, y, b) ∈R. See section ??.
These and many other settings are discussed in [KN97].
12.6
Applications of communication complexity
We brieﬂy discussed parallel computation in Chapter 6. Yao [Yao79] invented communication com-
plexity as a way to lowerbound the running time of parallel computers for certain tasks. The idea is
that the input is distributed among many processors, and if we partition these processors into two
halves, we may lowerbound the computation time by considering the amount of communication
that must necessarily happen between the two halves. A similar idea is used to prove time/space
lowerbounds for VLSI circuits. For instance, in a VLSI chip that is an m×m grid, if the communi-
cation complexity for a function is greater than c, then the time required to compute it is at least
c/m.
Communication complexity is also useful in time-space lowerbounds for Turing machines (see
Problem 1 in exercises), and circuit lowerbounds (see Chapter 13).
Data structures such as heaps, sorted arrays, lists etc. are basic objects in algorithm design.
Often, algorithm designers wish to determine if the data structure they have designed is the best
possible. Communication complexity lowerbounds can be used to establish such results. See [KN97].
Yannakakis [Yan91] has shown how to use communication complexity lowerbounds to prove
lowerbounds on the size of polytopes representing NP-complete problems. Solving the open prob-
lem mentioned in Problem 8 in the exercises would prove a lowerbound for the polytope representing
vertex cover.
Exercises
§1 If S(n) ≤n, show that a space S(n) TM takes at least Ω(n/S(n)) steps to decide the language
{x#x : x ∈{0, 1}∗}.
§2 Show that the high school deﬁnition of rank (the size of the largest set of independent rows
or columns) is equivalent to that in Deﬁnition 12.9.
Web draft 2007-01-08 21:59

DRAFT
p12.12 (232)
12.6. APPLICATIONS OF COMMUNICATION COMPLEXITY
§3 Give a fooling set argument that proves that C(f) ≥⌈log rank(M(f))⌉.
§4 Show that C(f)rank(M(f) + 1.
§5 Consider x, y as vectors over GF(2)n and let f(x, y) be their inner product mod 2. Prove
that the communication complexity is n.
Hint: Lowerbound the rank of the matrix 2M(f) −J where J is
the all-1 matrix.
What ﬁeld should you use to compute the rank? Does it matter?
§6 Let f : {0, 1}n × {0, 1}n →{0, 1} be such that all rows of M(f) are distinct. Show that
C(f) ≥log n.
Hint: Lowerbound the rank.
§7 (Aho, Ullman, Yannakakis) Show that C(f) = O(log2 χ(f)).
Hint: The players try to determine which of the |χ(f)| rectangles
their input-pair lies in. The protocol has O(log χ(f)) phases, and
in each phase O(log χ(f)) bits get communicated.
§8 For any graph G with n vertices, consider the following communication problem: Player 1
receives a clique C in G, and Player 2 receives an independent set I. They have to com-
municate in order to determine |C ∩I|. (Note that this number is either 0 or 1.) Prove an
O(log2 n) upperbound on the communication complexity.
Can you improve your upperbound or prove a lower bound better than Ω(log n)? (Open
question)
§9 Prove Lemma 12.21 using the hint given there.
§10 (Karchmer-Wigderson) Consider the following problem about computing a relation. Associate
the following communication problem with any function f : {0, 1}n →{0, 1}. Player 1 gets
any input x such that f(x) = 0 and player 2 gets any input y such that f(y) = 1. They have
to communicate in order to determine a bit position i such that xi ̸= yi.
Show that the communication complexity of this problem is exactly the minixmum depth of
any circuit that computes f. (The maximum fanin of each gate is 2.)
§11 Use the previous question to show that computing the parity of n bits requires depth at least
2 log n.
§12 Show that the following computational problem is in EXP: given the matrix M(f) of a
boolean function, and a number K, decide if C(f) ≤K.
(Open since Yao [Yao79]) Can you show this problem is complete for some complexity class?
Web draft 2007-01-08 21:59

DRAFT
12.6. APPLICATIONS OF COMMUNICATION COMPLEXITY
p12.13 (233)
Chapter notes and history
Communication complexity was ﬁrst deﬁned by Yao [Yao79]. Other early papers that founded the
ﬁeld were Papadimitriou and Sipser [PS84], Mehlhorn and Schmidt [MS82] (who introduced the
rank lowerbound) and Aho, Ullman and Yannakakis [AUY83].
The original log rank conjecture was that C(f) = O(rank(M(f))) but this was disproved by
Raz and Spieker [RS95].
The book by Nisan and Kushilevitz [KN97] is highly recommended.
Web draft 2007-01-08 21:59

DRAFT
p12.14 (234)
12.6. APPLICATIONS OF COMMUNICATION COMPLEXITY
Web draft 2007-01-08 21:59

DRAFT
Chapter 13
Circuit lowerbounds
Complexity theory’s Waterloo
We believe that NP does not have polynomial-sized circuits.
We’ve seen that if true, this
implies that NP ̸= P. In the 1970s and 1980s, many researchers came to believe that the route
to resolving P versus NP should go via circuit lowerbounds, since circuits seem easier to reason
about than Turing machines. The success in this endeavor was mixed.
Progress on general circuits has been almost nonexistent: a lowerbound of n is trivial for any
function that depends on all its input bits.
We are unable to prove even a superlinear circuit
lowerbound for any NP problem— the best we can do after years of eﬀort is 4.5n −o(n).
To make life (comparatively) easier, researchers focussed on restricted circuit classes, and were
successful in proving some decent lowerbounds. We prove some of the major results of this area and
indicate where researchers are currently stuck. In Chapter 22 we’ll explain some of the inherent
obstacles that need to be overcome to make further progress.
13.1
AC0 and H˚astad’s Switching Lemma
As we saw in Chapter 6, AC0 is the class of languages computable by circuit families of constant
depth, polynomial size, and whose gates have unbounded fanin. (Constant depth circuits with
fanin 2 can only compute functions depending on a constant number of input bits.) The burning
question in the late 1970s was whether problems like Clique and TSP have AC0 circuits. However,
in 1981, Furst, Saxe and Sipser and independently, Ajtai, proved a lowerbound for a much simpler
function:
Theorem 13.1 ([?, ?])
Let L be the parity function. That is, for every x ∈{0, 1}n, L(x1, . . . , xn) = Pn
i=1 xi (mod 2).
Then L ̸∈AC0.
Often courses in digital logic design teach students how to do “circuit minimization” using
Karnaugh maps. Note that circuits talked about in those courses are depth 2 circuits, i.e. CNF or
DNF. Indeed, it is easy to show (using for example the Karnaugh map technique studied in logic
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p13.1 (235)

DRAFT
p13.2 (236)
13.1. AC0 AND H˚ASTAD’S SWITCHING LEMMA
design) that the parity function requires exponentially many gates if the depth is two. However,
those simple ideas do not seem to generalize to even depth 3 circuits.
The main tool in the proof of Theorem 13.1 is the concept of random restrictions. Let f be a
function computable by a depth d circuit and suppose that we choose at random a vast majority
(i.e., n −nϵ for some constant ϵ > 0 depending on d) of the input variables and assign to each such
variable either 0 or 1 at random. We’ll prove that with positive probability, the function f subject
to this restriction is constant (i.e., either always zero or always one). Since the parity function
cannot be made a constant by ﬁxing values to a subset of the variables, it follows that it cannot be
computed by a constant depth circuit.
13.1.1
The switching lemma
Now we prove the main lemma about how a circuit simpliﬁes under a random restriction. A k-DNF
(resp. k-CNF) formula is an OR of AND’s (resp. AND or OR’s) where each AND (resp. OR)
involves at most k variables.
Lemma 13.2 (H˚astad’s switching lemma [Has86])
Suppose f is expressible as a k-DNF, and let ρ denote a random restriction that assigns random
values to t randomly selected input bits. Then for every s ≥2.
Prρ[f|ρ is not expressible as s-CNF ] ≤
(n −t)k10
n
s/2
(1)
where f|ρ denotes the function f restricted to the partial assignment ρ.
We’ll typically use this lemma with k, s constant and t ≈n −√n in which case the guaranteed
bound on the probability will be n−c for some constant c. Note that by applying the lemma to the
function ¬f, we can get the same result with the terms DNF and CNF interchanged.
Proving Theorem 13.1 from Lemma 13.2.
Now we show how H˚astad’s lemma implies that
parity is not in AC0. We start with any AC0 circuit and assume that the circuit has been simpliﬁed
as follows (the simpliﬁcations are straightforward to do and are left as Exercises 1 and 2): (a) All
fanouts are 1; the circuit is a tree (b) All not gates to the input level of the circuit; equivalently,
the circuit has 2n input wires, with the last n of them being the negations of the ﬁrst n (c) ∨and
∧gates alternate —at worst this assumption doubles the depth of the circuit (d) The bottom level
has ∧gates of fanin 1.
We randomly restrict more and more variables, where each step with high probability will reduce
the depth of the circuit by 1 and will keep the bottom level at a constant fanin. Speciﬁcally, letting
ni stand for the number of unrestricted variables after step i, we restrict ni −√ni variables at step
i + 1. Since n0 = n, we have ni = n1/2i. Let nb denote an upper bound on the number of gates in
the circuit and let ki = 10b2i. We’ll show that with high probability, after the ith restriction we’re
left with a depth-d −i circuit with at most ki fanin in the bottom level. Indeed, suppose that the
bottom level contains ∧gates and the level above it contains ∨gates. The function each such ∨
gate computes is a ki-DNF and hence by Lemma 13.2, with probability 1 −

k10
i
n1/2i+1
ki+1/2
, which
Web draft 2007-01-08 21:59

DRAFT
13.1. AC0 AND H˚ASTAD’S SWITCHING LEMMA
p13.3 (237)
is at least 1 −1/(10nb) for large enough n, the function such a gate computes will be expressible
as a ki+1-CNF. We can then merge this CNF with the ∧-gate above it, reducing the depth of the
circuit by one (see Figures 13.1 and 13.2). The symmetric reasoning applies in the case the bottom
level consists of ∨gates— in this case we use the lemma to transform the ki-CNF of the level
above it into a ki+1-DNF. Note that we apply the lemma at most once per each of the at most nb
gates of the original circuit. By the union bound, with probability 9/10, if we continue this process
for d −2 steps, we’ll get a depth two circuit with fanin k = kd−2 at bottom level (i.e., a k-CNF
or k-DNF formula). If we then choose to restrict each variable with probability half (i.e., restrict
about half of the variables to a random value), this circuit will be reduced to a constant function
with probability at least 2−k. Since the parity function is not constant under any restriction of less
than n variables, this proves Theorem 13.1. ■
Figure unavailable in pdf ﬁle.
Figure 13.1: Circuit before H˚astad switching transformation.
Figure unavailable in pdf ﬁle.
Figure 13.2: Circuit after H˚astad switching transformation. Notice that the new layer of ∧gates can be collapsed
with the single ∧parent gate, to reduce the number of levels by one.
13.1.2
Proof of the switching lemma (Lemma 13.2)
Now we prove the Switching Lemma. The original proof was more complicated; this one is due
to Razborov. Let f be expressible as a k-DNF on n variables. Let t be as in the lemma and let
Rt denote the set of all restrictions to t variables (note we can assume t > n/2). We have that
|Rt| =
 n
t

2t. Let Kt,s denote the set of restrictions ρ such that f|ρ is not a s-CNF. We need to
bound |Kt,s|/|Rt| by the right hand side of (1) to prove the lemma. We’ll do that by showing a
one-to-one function mapping Kt,s into the set Z ×S where Z is the set of restrictions of at least t+s
variables (i.e. Z = ∪t′≥t+sRt′) and S is some set of size 32ks. This will prove the lemma since at he
range t′ ≫n/2,
 n
t′

≈

n
n−t′
n−t′
and hence Z will be of size bounded by roughly n2s   n−t
n
s |Rt|.
We leave verifying the exact bound as Exercise 3.
Mapping Kt,s into Z × S.
Let ρ ∈Kt,s be a restriction ﬁxing t variables such that f|ρ is not an
s-CNF. We need to map ρ in a one-to-one way into some restriction ρ∗of at least t + s variables,
and some additional element in a set S of size at most 32ks.
Special case: each term has at most one “live” variable.
To get some intuition for the
proof, consider ﬁrst the case that for each term t in the k-DNF formula for f, ρ either ﬁxed t to
the value 0 or left a single unassigned variable in t, in which case we say that t′s value is ? (ρ can’t
ﬁx a term to the value 1 since we assume f|ρ is not constant). We denote by x1, . . . , xs denote the
Web draft 2007-01-08 21:59

DRAFT
p13.4 (238)
13.1. AC0 AND H˚ASTAD’S SWITCHING LEMMA
ﬁrst s such unassigned variables, according to some canonical ordering of the terms for the k-DNF
formula of f (there are more than s since otherwise f|ρ would be expressible as an s-CNF). For
each such variable xi, let termi be the ?-valued term in which xi appears. Let Ri be the operation
of setting xi to the value that ensures termi is true. We’ll map ρ to τ1 = R1R2 · · · Rsρ. That is,
apply Rs to ρ, then apply Rk−1 to ρ, · · · , then apply R1 to ρ. The crucial insight is that given τ1,
one can deduce term1: this is the ﬁrst term that is true in f|τ1. One might think that the second
term that is true in f|τ1 is term2 but that’s not necessarily the case, since the variable x1 may have
appeared several times, and so setting it to R1 may have set other terms to true (it could not have
set other terms to false, since this would imply that f|ρ includes an OR of xi and ¬xi, and hence
is the constant one function). We thus supply as part of the mapping a string w1 ∈{0, 1, ⋆}s that
tells us the assignment of the k variables of term1 in τ2 = R2 · · · Rsρ. Given that information we
can “undo” R1 and move from τ1 to τ2. Now in τ2, term2 is the ﬁrst satisﬁed term. Continuing
on this way we see that from τ1 (which is an assignment of at least t + s variables) and strings
w1, . . . , ws that are deﬁned as above, we can recover ρ, implying that we have a one-to-one mapping
that takes ρ into an assignment of at least t + s variables and a sequence in {0, 1, ⋆}ks.
The general case.
We now consider the general case, where some terms might have more than
one unassigned variable in them. We let term1 be the ﬁrst ?-valued term in f|ρ and let x1 be the
ﬁrst unassigned variable in term1. Once again, we have an operation R1 that will make term1 true,
although this time we think of R1 as assigning to all the k variables in term1 the unique value that
makes the term true. We also have an operation L1 assigning a value to x1 such that f|L1ρ cannot
be expressed by an s −1-CNF. Indeed, if for both possible assignments to x1 we get an s −1-CNF
then f|ρ is an s-CNF. We note that it’s not necessarily the case that x1’s value under L1ρ is diﬀerent
from its value under R1ρ, but it is the case that term1’s value is either ? or False under L1ρ (since
otherwise f|L1ρ would be constant). We let term2 be the ﬁrst ?-valued term in f|L1ρ (note that
term2 ≥term1) and let x2 be the ﬁrst unassigned variable in term2. Once again, we have an
operation R2 such that term2 is the ﬁrst true term in f|R2L1ρ and operation L2 such that f|L2L1ρ
is not a s −2-CNF. Continuing in this way we come up with operations L1, . . . , Ls, R1, . . . , Rs such
that if we let ρi be the assignment Li · · · L1ρ (with ρ0 = ρ) then for 1 ≤i ≤s:
• termi is the ﬁrst ?-valued term in f|ρi−1.
• termi is the ﬁrst true-valued term in f|Riρi−1.
• Li agrees with ρi−1 on all variables assigned a value by ρi−1.
• Ri agrees with ρi on all variables assigned a value by ρi.
For 1 ≤i ≤s, deﬁne τi to be RiRi+1 · · · Rsρs, and deﬁne τs+1 = ρs. We have that termi is
the ﬁrst true term in f|τi: indeed, all the operations in τi do not change variables assigned values
by ρi−1 and there termi is the ﬁrst ?-valued term. Thus τi cannot make any earlier term true.
However, since the last operation applied is Ri, termi is true in f|τi.
Let z1, . . . , zs and w1, . . . , ws be 2s strings in {0, 1, ⋆}s deﬁned as follows: zi describes the
values assigned to the k variables appearing in termi by ρi−1 and wi describes the value assigned to
termi’s variables by τi+1. Clearly, from termi, zi and the assignment ρi one can compute ρi−1 and
Web draft 2007-01-08 21:59

DRAFT
13.2. CIRCUITS WITH “COUNTERS”:ACC
p13.5 (239)
from termi, wi and the assignment τi one can compute τi+1. We’ll map ρ to τ1 and the sequence
z1, . . . , zs, w1, . . . , ws. Note that τ1 does assign values to at least s variables not assigned by ρ, and
that from τ1 we can ﬁnd term1 (as this is the ﬁrst true term in f|τ1) and then using w1 recover
τ2 and continue in this way until we recover the original assignment ρ. Thus this mapping is a
one-to-one map from Tt,s to Z × {0, 1, ⋆}2ks. ■
13.2
Circuits With “Counters”:ACC
One way to extend the AC0 lowerbounds of the previous section was to deﬁne a more general class
of circuits. What if we allow more general gates? The simplest example is a parity gate. Clearly,
an AC0 circuit provided with parity gates can can compute the parity function. But are there
still other functions that it cannot compute? Razborov proved the ﬁrst such lowerbound using his
Method of Approximations. Smolensky later extended this work and clariﬁed this method for the
circuit class considered here.
Normally we think of a modular computation as working with numbers rather than bit, but it
is suﬃcient to consider modular gates whose output is always 0/1.
Definition 13.3 (modular gates)
For any integer m, the MODm gate outputs 0 if the sum of its inputs is 0 modulo m, and 1
otherwise.
Definition 13.4 (ACC)
For integers m1, m2, . . . , mk > 1 we say a language L is in ACC0[m1, m2, . . . , mk] if there exists a
circuit family {Cn} with constant depth and polynomial size (and unbounded fan-in) consisting of
∧, ∨, ¬ and MODm1, . . . , MODmk gates accepting L.
The class ACC0 contains every language that is in ACC0(m1, m2, . . . , mk) for some k ≥0 and
m1, m2, . . . , mk > 1.
Good lowerbounds are known only when the circuit has one kind of modular gate.
Theorem 13.5 (Razborov,Smolensky)
For distinct primes p and q, the function MODp is not in ACC0(q).
We exhibit the main idea of this result by proving that the parity function cannot be computed
by an ACC0(3) circuit.
Proof: The proof proceeds in two steps.
Step 1. In the ﬁrst step, we show (using induction on h) that for any depth h MOD3 circuit on
n inputs and size S, there is a polynomial of degree (2l)h which agrees with the circuit on
1 −S/2l fraction of the inputs. If our circuit C has depth d then we set 2l = n1/2d to obtain
a degree √n polynomial that agrees with C on 1 −S/2n1/2d/2 fraction of inputs.
Step 2 We show that no polynomial of degree √n agrees with MOD2 on more than 49/50 fraction
of inputs.
Web draft 2007-01-08 21:59

DRAFT
p13.6 (240)
13.2. CIRCUITS WITH “COUNTERS”:ACC
Together, the two steps imply that S > 2n1/2d/2/50 for any depth d circuit computing MOD2,
thus proving the theorem. Now we give details.
Step 1. Consider a node g in the circuit at a depth h . (The input is assumed to have depth 0.)
If g(x1, · · · , xn) is the function computed at this node, we desire a polynomial ˜g(x1, · · · , xn) over
GF(3) with degree (2l)h, such that g(x1, . . . , xn) = ˜g(x1, . . . , xn) for “most” x1, . . . , xn ∈{0, 1}.
We will also ensure that on every input in {0, 1}n ⊆GF(3), polynomial ˜g takes a value in {0, 1}.
This is without loss of generality since we can just square the polynomial. (Recall that the elements
of GF(3) are 0, −1, 1 and 02 = 0, 12 = 1 and (−1)2 = 1.)
We construct the approximator polynomial by induction. When h = 0 the “gate” is an input
wire xi, which is exactly represented by the degree 1 polynomial xi. Suppose we have constructed
approximators for all nodes up to height h −1 and g is a gate at height h.
1. If g is a NOT gate, then g = ¬f1 for some other gate f1 that is at height h −1 or less.
The inductive hypothesis gives an approximator ˜f1 for f1. Then we use ˜g = 1 −˜f1 as the
approximator polynomial for g; this has the same degree as ˜f1. Whenever ˜f1 = f1 then ˜g = g,
so we introduced no new error.
2. If g is a MOD3 gate with inputs f1, f2, . . . , fk, we use the approximation ˜g = (Pk
i=0 ˜fi)2. The
degree increases to at most 2 × (2l)h−1 < (2l)h. Since 02 = 0 and (−1)2 = 1, we introduced
no new error.
3. If g is an AND or an OR gate, we need to be more careful. Suppose g = ∧k
i=0fi. The naive
approach would be to replace g with the polynomial Πi∈I ˜fi. For an OR gate g = ∨k
i=0fi De
Morgan’s law gives a similar naive approximator 1 −Q
i∈I(1 −˜fi). Unfortunately, both of
these multiply the degree by k, the fanin of the gate, which could greatly exceed 2l.
The correct solution involves introducing some error. We give the solution for OR; De Mor-
gan’s law allows AND gates to be handled similarly.
If g = ∨k
i=0fi, then g = 1 if and only if at least one of the fi = 1. Furthermore, by the random
subsum principle (see Section ?? in Appendix A) if any of the fi = 1, then the sum (over
GF(3)) of a random subset of {fi} is nonzero with probability at least 1/2.
Randomly pick l subsets S1, · · · , Sl of {1, . . . , k}. Compute the l polynomials (P
j∈Si ˜fj)2,
each of which has degree at most twice that of the largest input polynomial.
Compute
the OR of these l terms using the naive approach. We get a polynomial of degree at most
2l×(2l)h−1 = (2l)h. For any x, the probability over the choice of subsets that this polynomial
diﬀers from OR( ˜f1, . . . , ˜fk) is at most 1
2l . So, by the probabilistic method, there exists a choice
for the l subsets such that the probability over the choice of x that this polynomial diﬀers from
OR( ˜f1, · · · , ˜fk) is at most 1
2l . We use this choice of the subsets to construct the approximator.
Applying the above procedure for each gate gives an approximator for the output gate of degree
(2l)d where d is depth of the entire circuit. Each operation of replacing the gate by its approximator
polynomial introduces error on at most 1/2l fraction of all inputs, so the overall fraction of erroneous
inputs for the approximator is at most S/2l. (Note that errors at diﬀerent gates may aﬀect each
other. Error introduced at one gate may be cancelled out by errors at another gate higher up. We
Web draft 2007-01-08 21:59

DRAFT
13.2. CIRCUITS WITH “COUNTERS”:ACC
p13.7 (241)
are being pessimistic in applying the union bound to upperbound the probability that any of the
approximator polynomials anywhere in the circuit miscomputes.)
Step 2.
Suppose that a polynomial f agrees with the MOD2 function for all inputs in a set
G′ ⊆0, 1n. If the degree of f is bounded by √n, then we show |G′| <
 49
50

2n.
Consider the change of variables yi = 1 + xi (mod 3). (Thus 0 →1 and 1 →−1.) Then, G′
becomes some subset G of {−1, 1}n, and f becomes some other polynomial, say g(y1, y2, . . . , yn),
which still has degree √n. Moreover,
MOD2(x1, x2, . . . , xn) =
(
1
⇒Πn
i=1yi = −1
0
⇒Πn
i=1yi = 1
(2)
Thus g(y1, y2, . . . , yn), a degree √n polynomial, agrees with Πn
i=1yi on G. This is decidedly odd,
and we show that any such G must be small.
Speciﬁcally, let FG be the set of all functions
S :G →{0, 1, −1}. Clearly, |FG| = 3|G|, and we will show |FG| ≤3(49
50)2n, whence Step 2 follows.
Lemma 13.6
For every S ∈FG, there exists a polynomial gS which is a sum of monomials aI
Q
i∈I yi where
|I| ≤n
2 + √n such that gS(x) = S(x) for all x ∈G.
Proof: Let ˆS : GF(3)n →GF(3) be any function which agrees with S on G. Then ˆS can be
written as a polynomial in the variables yi.
However, we are only interested in its values on
(y1, y2, . . . , yn) ∈{−1, 1}n, when y2
i = 1 and so every monomial Πi∈Iyri
i
has, without loss of
generality, ri ≤1. Thus ˆS is a polynomial of degree at most n. Now consider any of its monomial
terms Πi∈Iyi of degree |I| > n/2. We can rewrite it as
Πi∈Iyi = Πn
i=1yiΠi∈¯Iyi,
(3)
which takes the same values as g(y1, y2, . . . , yn)Πi∈¯Iyi over {−1, 1}n. Thus every monomial in ˆS
has degree at most n
2 + √n. ■
To conclude, we bound the number of polynomials whose every monomial with a degree at most
n
2 + √n. Clearly this number is #polynomials ≤3#monomials, and
#monomials ≤
{N ⊆{1 · · · n}| |N| ≤n
2 + √n

(4)
≤
X
i≤n
2
+√n
n
i

(5)
Using knowledge of the tails of a binomial distribution (or alternatively, direct calculation),
≤49
502n
(6)
■
Web draft 2007-01-08 21:59

DRAFT
p13.8 (242)
13.3. LOWERBOUNDS FOR MONOTONE CIRCUITS
13.3
Lowerbounds for monotone circuits
A Boolean circuit is monotone if it contains only AND and OR gates, and no NOT gates. Such a
circuit can only compute monotone functions, deﬁned as follows.
Definition 13.7
For x, y ∈{0, 1}n, we denote x ≼y if every bit that is 1 in x is also 1 in y. A function f :{0, 1}n →
{0, 1} is monotone if f(x) ≤f(y) for every x ≼y.
Remark 13.8
An alternative characterization is that f is monotone if for every input x, changing a bit in x from
0 to 1 cannot change the value of the function from 1 to 0.
It is easy to check that every monotone circuit computes a monotone function, and every mono-
tone function can be computed by a (suﬃciently large) monotone circuit. CLIQUE is a monotone
function since adding an edge to the graph cannot destroy any clique that existed in it. In this
section we show that the CLIQUE function can not be computed by polynomial (and in fact even
subexponential) sized monotone circuits:
Theorem 13.9 ([Raz85b, AB87])
Denote by CLIQUEk,n : {0, 1}(n
2) →{0, 1} be the function that on input an adjacency matrix of an
n-vertex graph G outputs 1 iﬀG contains a k-vertex clique.
There exists some constant ϵ > 0 such that for every k ≤n1/4, there’s no monotone circuit of
size less than 2ϵ
√
k that computes CLIQUEk,n.
We believe CLIQUE does not have polynomial-size circuits even allowing NOT gates (i.e., that
NP ⊈P/poly). In fact, a seemingly plausible approach to proving this might be to show that
for every monotone function f, the monotone circuit complexity of f is polynomially related to
the general (non-monotone) circuit complexity.
Alas, this conjecture was refuted by Razborov
([Raz85a], see also [Tar88]).
13.3.1
Proving Theorem 13.9
Clique Indicators
To get some intuition why this theorem might be true, lets show that CLIQUEk,n can’t be computed
(or even approximated) by subexponential monotone circuits of a very special form. For every
S ⊆[n], let CS denote the function on {0, 1}(n
2) that outputs 1 on a graph G iﬀthe set S is a clique
in G. We call CS the clique indicator of S. Note that CLIQUEk,n = W
S⊆[n],|S|=k CS. We’ll now
prove that CLIQUEk,n can’t be computed by an OR of less than n
√
k/20 clique indicators.
Let Y be the following distribution on n-vertex graphs: choose a set K ⊆[n] with |K| = k at
random, and output the graph that has a clique on K and no other edges. Let N be the following
distribution on n-vertex graphs: choose a function c : [n] →[k −1] at random, and place an edge
between u and v iﬀc(u) ̸= c(v). With probability one, CLIQUEn,k(Y) = 1 and CLIQUEn,k(N) = 0.
The fact that CLIQUEn,k requires an OR of at least n
√
k/20 clique indicators follows immediately
from the following lemma:
Web draft 2007-01-08 21:59

DRAFT
13.3. LOWERBOUNDS FOR MONOTONE CIRCUITS
p13.9 (243)
Lemma 13.10
Let n be suﬃciently large, k ≤n1/4 and S ⊆[n]. Then either Pr[CS(N) = 1] ≥0.99 or Pr[CS(Y) =
1] ≤n−
√
k/20
Proof: Let ℓ=
√
k −1/10. If |S| ≤ℓthen by the birthday bound, we expect a random f : S →
[k −1] to have less than 0.01 collisions and hence by Markov the probability f is one to one is at
least 0.99. This implies that Pr[CS(N) = 1] ≥0.99.
If |S| > ℓthen Pr[CS(Y) = 1] is equal to the probability that S ⊆K for a random K ⊆[n]
of size k. This probability is equal to
 n−ℓ
k−ℓ

/
 n
k

which is at most
 n
k−√k−1/10

/
 n
k

which, by the
formula for the binomial coeﬃcients, is less than
  2k
n
ℓ≤n−0.7ℓ< n−
√
k/20 (for suﬃciently large
n). ■
Approximation by clique indicators.
Together with Lemma 13.10, the following lemma implies Theorem 13.9:
Lemma 13.11
Let C be a monotone circuit of size s. Let ℓ=
√
k/10. Then, there exist sets S1, . . . , Sm with
m ≤n
√
k/20 such that
PrG∈RY[
_
i
CSi(G) ≥C(G)] >0.9
(7)
PrG∈RN [
_
i
CSi(G) ≤C(G)] >0.9
(8)
(9)
Proof: Set ℓ=
√
k/10, p = 10
√
k log n and m = (p −1)ℓℓ!. Note that m ≪n
√
k/20. We can think
of the circuit C as the sequence of s monotone functions f1, . . . , fs from {0, 1}(n
2) to {0, 1} where
each function fk is either the AND or OR of two functions fk′, fk′′ for k′, k′′ < k or is the value
of an input variable xu,v for u, v ∈[n] (i.e., fk = C{u,v}). The function that C computes is fs.
We’ll show a sequence of functions ˜f1, . . . , ˜fs such that each function ˜fk is (1) an OR of at most m
clique indicators CS1, . . . , CSm with |Si| ≤ℓand (2) ˜fk approximates fk in the sense of (7) and (8).
We call a function ˜fk satisfying (1) an (ℓ, m)-function. The result will follow by considering the
function ˜fs.
We construct the functions ˜f1, . . . , ˜fs by induction. For 1 ≤k ≤s, if fk is an input variable then
we let ˜fk = fk. If fk = fk′ ∨fk′′ then we let ˜fk′ ⊔˜fk′′ and if fk = fk′ ∧fk′′ then we let ˜fk′ ⊓˜fk′′, where
the operations ⊔, ⊓will be deﬁned below. We’ll prove that for every f, g : {0, 1}(n
2) →{0, 1} (a) if
f and g are (m, ℓ)-functions then so is f ⊔g (resp. f ⊓g) and (b) PrG∈RY[f⊔g (G) < f∪g (G)] <
1/(10S) (resp. PrG∈RY[f⊓g (G) < f∩g (G)] < 1/(10S)) and PrG∈RN [f⊔g (G) > f∪g (G)] < 1/(10S)
(resp. PrG∈RY[f ⊓g (G) < f ∩g (G)] < 1/(10S)). The lemma will then follow by showing using
the union bound that with probability ≥0.9 the equations of Condition (b) hold for all ˜f1, . . . , ˜fs.
We’ll now describe the two operations ⊔, ⊓. Condition (a) will follow from the deﬁnition of the
operations, while Condition (b) will require a proof.
Web draft 2007-01-08 21:59

DRAFT
p13.10 (244)
13.3. LOWERBOUNDS FOR MONOTONE CIRCUITS
The operation f ⊔g.
Let f, g be two (m, ℓ)-functions: that is f = Wm
i=1 CSi and g = Wm
j=1 CTj
(if f or g is the OR of less than m clique indicators we can add duplicate sets to make the number
m). Consider the function h = CZ1 ∪· · · ∪CZ2m where Zi = Si and Zm+j = Tj for 1 ≤i, j ≤m.
The function h is not an (m, ℓ)-function since it is the OR of 2m clique indicators. We make it
into an (m, ℓ)-function in the following way: as long as there are more than m distinct sets, ﬁnd
p subsets Zi1, . . . , Zip that are in a sunﬂower formation. That is, there exists a set Z ⊆[n] such
that for every 1 ≤j, j′ ≤p, Zij ∩Zi,j′ = Z. Replace the functions CZi1, . . . , CZip in the function h
with the function CZ. Once we obtain an (m, ℓ)-function h′ we deﬁne f ⊔g to be h′. We won’t get
stuck because of the following lemma (whose proof we defer):
Lemma 13.12 (Sunflower lemma [ER60])
Let Z be a collection of distinct sets each of cardinality at most ℓ. If |Z| > (p −1)ℓℓ! then there
exist p sets Z1, . . . , Zp ∈Z and set Z such that Zi ∩Zj = Z for every 1 ≤i, j ≤p.
The operation f ⊓g.
Let f, g be two (m, ℓ)-functions: that is f = Wm
i=1 CSi and g = Wm
j=1 CTj.
Let h be the function W
1≤i,j≤m CSi∪Tj. We perform the following steps on h: (1) Discard any
function CZ for |Z| > ℓ. (2) Reduce the number of functions to m by applying the sunﬂower
lemma as above.
Proving Condition (b).
To complete the proof of the lemma, we prove the following four
equations:
• PrG∈RY[f ⊔g (G) < f ∪g (G)] < 1/(10S).
If Z ⊆Z1, . . . , Zp then for every i, CZi(G) implies that CZ(G) and hence the operation f ⊔g
can’t introduce any “false negatives”.
• PrG∈RN [f ⊔g (G) > f ∪g (G)] < 1/(10S).
We can introduce a “false positive” on a graph G only if when we replace the clique indicators
for a sunﬂower Z1, . . . , Zp with the clique indicator for the common intersection Z, it is the
case that CZ(G) holds even though CZi(G) is false for every i. Recall that we choose G ∈R N
by choosing a random function c : [n] →[k −1] and adding an edge for every two vertices u, v
with c(u) ̸= c(v). Thus, we get a false positive if c is one-to-one on Z (we denote this event
by B) but not one-to-one on Zi for every 1 ≤i ≤p (we denote these events by A1, . . . , Ap).
We’ll show that the intersection of B and A1, . . . , Ap happens with probability at most 2−p
which (by the choice of p) is less than 1/(10m2s). Since we apply the reduction step at most
m times the equation will follow.
Since ℓ<
√
k −1/10, for every i, Pr[Ai|B] < 1/2 (the probability that there’ll be a collision
on the at most ℓelements of Zi\Z is less than half). Conditioned on B, the events A1, . . . , Ap
are independent, since they depend on the values of c on disjoint sets, and hence we have
that Pr[A1 ∧· · · ∧Ap ∧B] ≤Pr[A1 ∧· · · ∧Ap|B] = Qp
i=1 Pr[Ap|B] ≤2−p.
• PrG∈RY[f ⊓g (G) < f ∩g (G)] < 1/(10S).
By the distributive law f ∩g = W
i,j(CSi ∩CTj). A graph G in the support of Y consists of a
clique over some set K. For such a graph CSi∩CTj holds iﬀSi, Tj ⊆K and thus CSi∩CTj holds
Web draft 2007-01-08 21:59

DRAFT
13.4. CIRCUIT COMPLEXITY: THE FRONTIER
p13.11 (245)
iﬀCSi∪Tj holds. We can introduce a false negative when we discard functions of the form CZ
for |Z| > ℓ, but by Lemma 13.10, for such sets Z, Pr[CZ(Y) = 1] < n−
√
k/20 < 1/(10sm2).
The equation follows since we discard at most m2 such sets.
• PrG∈RN [f ⊓g (G) > f ∩g (G)] < 1/(10S).
Since CS∪T implies both CS and CT , we can’t introduce false positives by moving from f ∩g to
W
i,j CSi∪Tj. We can’t introduce false positives by discarding functions from the OR. Thus, the
only place where we can introduce false positives is where we replace the clique indicators of
a sunﬂower with the clique indicator of the common intersection. We bound this probability
in the same way as this was done for the ⊔operator.
■
Proof of the sunﬂower lemma (Lemma 13.12).
The proof is by induction on ℓ. The case
ℓ= 1 is trivial since distinct sets of size 1 must be disjoint.
For ℓ> 1 let M be a maximal
subcollection of Z containing only disjoint sets. Because of M’s maximality for every Z ∈Z there
exists x ∈∪M = ∪M∈MM such that x ∈Z. If |M| ≥p we’re done, since such a collection is
already a sunﬂower. Otherwise, since | ∪M| ≤(p −1)ℓby averaging there’s an x ∈∪M that
appears in at least a
1
ℓ(p−1) fraction of the sets in Z. Let Z1, . . . , Zk be the sets containing x, and
note that k > (p −1)ℓ−1(ℓ−1)!. Thus, by induction there are p sets among the ℓ−1-sized sets
Z1 \{x}, · · · , Zk \{x} that form a sunﬂower, adding back x we get the desired sunﬂower among the
original sets. Note that the statement (and proof) assume nothing about the size of the universe
the sets in Z live in. ■
13.4
Circuit complexity: The frontier
Now we sketch the “frontier” of circuit lowerbounds, namely, the dividing line between what we
can prove and what we cannot. Along the way we also deﬁne multi-party communication, since it
may prove useful for proving some new circuit lowerbounds.
13.4.1
Circuit lowerbounds using diagonalization
We already mentioned that the best lowerbound on circuit size for an NP problem is 4.5n −o(n).
For PH better lowerbounds are known: one of the exercises in Chapter 6 asked you to show that
some for every k > 0, some language in PH (in fact in Σp
2) requires circuits of size Ω(nk). The
latter lowerbound uses diagonalization, and one imagines that classes “higher up” than PH should
have even harder languages.
Frontier 1: Does NEXP have languages that require super-polynomial size circuits?
If we go a little above NEXP, we can actually prove a super-polynomial lowerbound: we know
that MAEXP ⊈P/poly where MAEXP is the set of languages accepted by a one round proof with
an all powerful prover and an exponential time probabilistic veriﬁer. This follows from the fact
Web draft 2007-01-08 21:59

DRAFT
p13.12 (246)
13.4. CIRCUIT COMPLEXITY: THE FRONTIER
Figure unavailable in pdf ﬁle.
Figure 13.3: The depth 2 circuit with a symmetric output gate from Theorem 13.13.
that if MAEXP ⊆P/poly then in particular PSPACE ⊆P/poly. However, by IP = PSPACE
(Theorem 8.17) we have that in this case PSPACE = MA (the prover can send in one round the
circuit for computing the prover strategy in the interactive proof). However, by simple padding this
implies that MAEXP equals the class of languages in exponential space, which can be directly shown
to not contain P/poly using diagonalization. Interestingly, this lower bound does not relativize (i.e.,
there’s an oracle under which MANEXP ⊆P/poly [BFT98]).
13.4.2
Status of ACC versus P
The result that PARITY is not in AC0 separates NC1 from AC0. The next logical step would be
to separate ACC0 from NC1. Less ambitiously, we would like to show even a function in P or NP
that is not in ACC0.
The Razborov-Smolenksy method seems to fail when we allow the circuit even two types of
modular gates, say MOD2 and MOD3. In fact if we allow the bounded depth circuit modular
gates that do arithmetic mod q, when q is not a prime —a prime power, to be exact— we reach
the limits of our knowledge. (The exercises ask you to ﬁgure out why the proof of Theorem 13.5
does not seem to apply when the modulus is a composite number.) To give one example, it it is
consistent with current knowledge that the majority of n bits can be computed by linear size circuits
of constant depth consisting entirely of MOD6 gates. The problem seems to be that low-degree
polynomials modulo m where m is composite are surprisingly expressive [BBR94].
Frontier 2: Show Clique is not in ACC0(6).
Or even less ambitiously:
Frontier 2.1: Exhibit a language in NEXP that is not in ACC0(6).
It is worth noting that thus far we are talking about nonuniform circuits (to which Theorem 13.5
also applies). Stronger lower bounds are known for uniform circuits: Allender and Gore [AG94]
have shown that a decision version of the Permanent (and hence the Permanent itself) requires
exponential size “Dlogtime-uniform” ACC0 circuits. (A circuit family {Cn} is Dlogtime uniform
if there exists a deterministic Turing machine M that given a triple (n, g, h) determines in linear
time —i.e., O(log n) time when g, h ≤poly(n)— what types of gates g and h are and whether g is
h’s parent in Cn.)
But going back to nonuniform ACC0, we wish to mention an alternative representation of
ACC0 circuits that may be useful in further lowerbounds. Let a symmetric gate be a gate whose
output depends only on the number of inputs that are 1. For example, majority and mod gates
are symmetric. Yao has shown that ACC0 circuits can be simpliﬁed to give an equivalent depth 2
circuits with a symmetric gate at the output (ﬁgure ??). Beigel and Tarui subsequently improved
Yao’s result:
Web draft 2007-01-08 21:59

DRAFT
13.4. CIRCUIT COMPLEXITY: THE FRONTIER
p13.13 (247)
Theorem 13.13 (Yao [Yao90], Beigel and Tarui [BT94])
If f ∈ACC0, then f can be computed by a depth 2 circuit C with a symmetric gate with
quasipolynomial (i.e., 2logk n) fan-in at the output level and ∨gates with polylogarithmic fan-in at
the input level.
We will revisit this theorem below in Section 13.5.1.
13.4.3
Linear Circuits With Logarithmic Depth
When we restrict circuits to have bounded fanin we necessarily need to allow them to have non-
constant (in fact, Ω(log n)) depth to have any reasonable power. With this in mind, the simplest
interesting circuit class seems to be one of circuits wth linear size and logarithmic depth.
Frontier 3: Find an explicit function that cannot be computed by circuits of linear size and
logarithmic depth.
(Note that by counting one can easily show that some function on n bits requires superpoly-
nomial size circuits and hence bounded fan-in circuits with more than logarithmic depth; see the
exercises on the chapter on circuits. Hence we want to show this for an explicit function, e.g.
CLIQUE.)
Valiant thought about this problem in the ’70s. His initial candidates for lowerbounds boiled
down to showing that a certain graph called a superconcentrator needed to have superlinear size.
He failed to prove thisand instead ended up proving that such superconcentrators do exist!
Another sideproduct of Valiant’s investigations was the following important lemma concerning
depth-reduction for such circuits.
Lemma 13.14 (Valiant)
In any circuit with m edges and depth d, there are km/ log d edges whose removal leaves a circuit
with depth at most d/2k−1.
This lemma can be applied as follows. Suppose we have a circuit C of depth c log n with n
inputs {x1, . . . , xn} and n outputs {y1, . . . , yn}, and suppose 2k ∼c/ϵ where ϵ > 0 is arbitrarily
small. Removing O(n/ log log n) edges from C then results in a circuit with depth at most ϵ log n.
But then, since C has bounded fan-in, we must have that each output yi is connected to at most
2ϵ log n = nϵ inputs. So each output yi in C is completely determined by nϵ inputs and the values
of the omitted edges. So we have a “dense” encoding for the function fi(x1, . . . , xn) = yi. We do
not expect this to be the case for any reasonably diﬃcult function.
13.4.4
Branching Programs
Just as circuits are used to investigate time requirements of Turing Machines, branching programs
are used to investigate space complexity.
A branching program on n input variables x1, x2, . . . , xn is a directed acyclic graph all of whose
nodes of nonzero outdegree are labeled with a variable xi. It has two nodes of outdegree zero that
are labeled with an output value, ACCEPT or REJECT. The edges are labeled by 0 or 1. One of
the nodes is designated the start node. A setting of the input variables determines a way to walk
Web draft 2007-01-08 21:59

DRAFT
p13.14 (248)
13.5. APPROACHES USING COMMUNICATION COMPLEXITY
on the directed graph from the start node to an output node. At any step, if the current node has
label xi, then we take an edge going out of the node whose label agrees with the value of xi. The
branching program is deterministic if every nonoutput node has exactly one 0 edge and one 1 edge
leaving it. Otherwise it is nondeterministic. The size of the branching program is the number of
nodes in it. The branching program complexity of a language is deﬁned analogously with circuit
complexity. Sometimes one may also require the branching program to be leveled, whereby nodes
are arranged into a sequence of levels with edges going only from one level to the next. Then the
width is the size of the largest level.
Theorem 13.15
If S(n) ≥log n and L ∈SPACE(S(n)) then L has branching program complexity at most cS(n)
for some constant c > 1.
Proof: Essentially mimics our proof of Theorem‘?? that SPACE(S(n)) ⊆DTIME(2O(S(n))).
The nodes of the branching program correspond to the conﬁgurations of the space-bounded TM,
and it is labeled with variable xi if the conﬁguration shows the TM reading the ith bit in the input.
■
Of course, a similar theorem is true about NDTMs and nondeterministic branching program
complexity.
Frontier 4: Describe a problem in P (or even NP) that requires branching programs of size greater
than n1+ϵ for some constant ϵ > 0.
There is some evidence that branching programs are more powerful than one may imagine. For
instance, branching programs of constant width (reminiscent of a TM with O(1) bits of memory)
seem inherently weak. Thus the next result is unexpected.
Theorem 13.16 (Barrington [?])
A language has polynomial size, width 5 branching programs iﬀit is in NC1.
13.5
Approaches using communication complexity
Here we outline a concrete approach (rather, a setting) in which better lowerbounds may lead to a
resolution of some of the questions above. It relates to generalizations of communication complexity
introduced earlier. Mostly we will use multiparty communication complexity, though Section 13.5.4
will use communication complexity of a relation.
13.5.1
Connection to ACC0 Circuits
Suppose f(x1, . . . , xk) has a depth-2 circuit with a symmetric gate with fan-in N at the output and
∧gates with fan-in k −1 at the input level (ﬁgure 2). The claim is that f’s k-party communication
complexity is at most k log N. (This observation is due to Razborov and Wigderson [RW93]). To
see the claim, ﬁrst partition the ∧gates amongst the players. Each bit is not known to exactly one
player, so the input bits of each ∧gate are known to at least one player; assign the gate to such a
player with the lowest index. Players then broadcast how many of their gates output 1. Since this
number has at most log N bits, the claim follows.
Web draft 2007-01-08 21:59

DRAFT
13.5. APPROACHES USING COMMUNICATION COMPLEXITY
p13.15 (249)
Figure unavailable in pdf ﬁle.
Figure 13.4: If f is computed by the above circuit, then f has a k-party protocol of complexity k log N.
Our hope is to employ this connection with communication complexity in conjunction with
Theorem 13.13 to obtain lower bounds on ACC0 circuits. For example, note that the function in
Example ?? above cannot have k < log n/4. However, this is not enough to obtain a lower bound
on ACC0 circuits since we need to show that k is not polylogarithmic to employ Theorem 13.13.
Thus a strengthening of the Babai Nisan Szegedy lowerbound to Ω(n/poly(k)) for say the CLIQUE
function would close Frontier 2.
13.5.2
Connection to Linear Size Logarithmic Depth Circuits
Suppose that f : {0, 1}n × {0, 1}log n →{0, 1}n has bounded fan-in circuits of linear size and
logarithmic depth. If f(x, j, i) denotes the ith bit of f(x, j), then Valiant’s Lemma implies that
f(x, j, i) has a simultaneous 3-party protocol—that is, a protocol where all parties speak only once
and write simultaneously on the blackboard (i.e., non-adaptively)—where,
• (x, j) player sends n/ log log n bits;
• (x, i) player sends nϵ bits; and
• (i, j) player sends O(log n) bits.
So, if we can show that a function does not have such a protocol, then we would have a lower bound
for the function on linear size logarithmic depth circuits with bounded fan-in.
Conjecture: The function f(x, j, i) = xj⊕i, where j ⊕i is the bitwise xor, is conjectured to be
hard, i.e., f should not have a compact representation.
13.5.3
Connection to branching programs
The notion of multiparty communication complexity (at least the “number on the forehead” model
discussed here) was invented by Chandra Furst and Lipton [?] for proving lowerbounds on branching
programs, especially constant-width branching programs discussed in Section ??
13.5.4
Karchmer-Wigderson communication games and depth lowerbounds
The result that PARITY is not in AC0 separates NC1 from AC0. The next step would be to
separate NC2 from NC1. (Of course, ignoring for the moment the issue of separating ACC0 from
NC1.) Karchmer and Wigderson [KW90] described how communication complexity can be used
to prove lowerbounds on the minimum depth required to compute a function. They showed the
following result about monotone circuits, which we will not prove this result.
Theorem 13.17
Detecting whether a graph has a perfect matching is impossible with monotone circuits of depth
O(log n)
Web draft 2007-01-08 21:59

DRAFT
p13.16 (250)
13.5. APPROACHES USING COMMUNICATION COMPLEXITY
However, we do describe the basic Karchmer-Wigderson game used to prove the above result,
since it is relevant for nonmonotone circuits as well. For a function f : {0, 1}n →{0, 1} this game
is deﬁned as follows.
There are two players, ZERO and ONE. Player ZERO receives an input x such that f(x) = 0
and Player ONE receives an input y such that f(y) = 1. They communicate bits to each other,
until they can agree on an i ∈{1, 2, . . . , n} such that xi ̸= yi.
The mechanism of communication is deﬁned similarly as in Chapter 12; there is a protocol that
the players agree on in advance before receiving the input. Note that the key diﬀerence from the
scenario in Chapter 12 is that the ﬁnal answer is not a single bit, and furthermore, the ﬁnal answer
is not unique (the number of acceptable answers is equal to the number of bits that x, y diﬀer on).
Sometimes this is described as computing a relation. The relation in this case consists of all triples
(x, y, i) such that f(x) = 0, f(y) = 1 and xi ̸= yi.
We deﬁne CKW (f) as the communication complexity of the above game; namely, the maximum
over all x ∈f−1(0), y ∈f−1(1) of the number of bits exchanged in computing an answer for x, y. The
next theorem shows that this parameter has a suprising alternative characterization. It assumes
that circuits don’t have NOT gates and instead the NOT gates are pushed down to the inputs
using De Morgan’s law. (In other words, the inputs may be viewed as x1, x2, . . . , xn, x1, x2, . . . , xn.)
Furthermore, AND and OR gates have fanin 2. (None of these assumptions is crucial and aﬀects
the theorem only marginally.)
Theorem 13.18 ([KW90])
CKW (f) is exactly the minimum depth among all circuits that compute f.
Proof: First, we show that if there is a circuit C of depth K that computes f then CKW (f) ≤K.
Each player has a copy of C, and evaluates this circuit on the input given to him. Of course, it
ealuates to 0 for Player ZERO and to 1 for Player ONE. Suppose the top gate is an OR. Then
at least one of the two incoming wires to this gate must be 1, and in the ﬁrst round, Player ONE
sends one bit communicating which of these wires it was. Note that this wire is 0 for Player ZERO.
In the next round the players focus on the gate that produced the value on this wire. (If the top
gate is an AND on the other hand, then in the ﬁrst round Player ZERO speaks, conveying which
of the two incoming wires was 0. This wire will be 1 for Player ONE.) This goes on and the players
go deeper down the circuit, always maintaining the invariant that the current gate has value 1 for
Player ONE and 0 for Player ZERO. Finally, after at most K steps they arrive at an input bit.
According to the invariant being maintained, this bit must be 1 for Player ONE and 0 for Player
ZERO. Thus they both know an index i that is a valid answer.
For the reverse direction, we have to show that if CKW (f) = K then there is a circuit of depth
at most K that computes f. We prove a more general result. For any two disjoint nonempty
subsets A ⊆f−1(0) and B ⊆f−1(1), let CKW (A, B) be the communication complexity of the
Karchmer-Wigderson game when x always lies in A and y in B. We show that there is a circuit
of depth CKW (A, B) that outputs 0 on every input from A and 1 on every input from B. Such a
circuit is called a distinguisher for sets A, B. The proof is by induction on K = CKW (A, B). The
base case K = 0 is trivial since this means the players do not have to communicate at all to agree
on an answer, say i. Hence xi ̸= yi for all x ∈A, y ∈B, which implies that either (a) xi = 0 for
Web draft 2007-01-08 21:59

DRAFT
13.5. APPROACHES USING COMMUNICATION COMPLEXITY
p13.17 (251)
every x ∈A and yi = 0 for every y ∈B or (b) xi = 1 for every x ∈A and yi = 1 for every y ∈B.
In case (a) we can use the depth 0 circuit xi and in case (b) we can use the circuit xi to distinguish
A, B.
For the inductive step, suppose CKW (A, B) = K, and at the ﬁrst round Player ZERO speaks.
Then A is the disjoint union of two sets A0, A1 where Ab is the set of inputs in A for which Player
ZERO sends bit b. Then CKW (Ab, B) ≤K −1 for each b, and the inductive hypothesis gives a
circuit Cb of depth at most K −1 that distinguishes Ab, B. We claim that C0 ∧C1 distinguishes
A, B (note that it has depth at most K). The reason is that C0(y) = C1(y) = 1 for every y ∈B
whereas for every x ∈A, C0(x) ∧C1(x) = 0 since if x ∈Ab then Cb(x) = 0. ■
Thus we have the following frontier.
Frontier 5: Show that some function f in P (or even NEXP!) has CKW (f) = Ω(log n log log n).
Karchmer, Raz, and Wigderson [KRW95] describe a candidate function that may work. It uses
the fact a function on k bits has a truth table of size 2k, and that most functions on k bits are hard
(e.g., require circuit size Ω(2k/k), circuit depth Ω(k), etc.). They deﬁne the function by assuming
that part of the n-bit input encodes a very hard function, and this hard function is applied to the
remaining input in a “tree” fashion.
For any function g :{0, 1}k →{0, 1} and s ≥1 deﬁne g◦s :{0, 1}ks →{0, 1} as follows. If s = 1
then g◦s = g. Otherwise express the input x ∈{0, 1}ks as x1x2x3 · · · xk where each xi ∈{0, 1}ks−1
and deﬁne
g◦s(x1x2 · · · xk) = g(g◦(s−1)(x1)g◦(s−1)(x2) · · · g◦(s−1)(xk)).
Clearly, if g can be computed in depth d then g◦s can be computed in depth sd. Furthermore, if
one fails to see how one could reduce the depth for an arbitrary function.
Now we describe the KRW candidate function f : {0, 1}n →{0, 1}. Let k = ⌈log n
2 ⌉and s be
the largest integer such that ks ≤n/2 (thus s = Θ(
log n
log log n).) For any n-bit input x, let gx be the
function whose truth table is the ﬁrst 2k bits of x. Let x|2 be the string of the last ks bits of x.
Then
f(x) = g◦s
x (x|2).
According to our earlier intuition, when the ﬁrst 2k bits of x represent a really hard function —as
they must for many choices of the input— then g◦s
x (x|2) should require depth Ω(sk) = Ω( log2 n
log log n).
Of course, proving this seems diﬃcult.
This type of complexity questions, whereby we are asking whether s instances of a problem are
s times as hard as a single instance, are called direct sum questions. Similar questions have been
studied in a variety of computational models, and sometimes counterintuitive results have been
proven for them. One example is that by a counting argument there exists an n × n matrix A over
{0, 1}, such that the smallest circuit computing the linear function v 7→Av for v ∈{0, 1}n is of
size Ω(n2). However, computing this function on n instances v1, . . . , vn can be done signiﬁcantly
faster than n3 steps using fast matrix multiplication [Str69] (the current record is roughly O(n2.38)
[CW90]).
Web draft 2007-01-08 21:59

DRAFT
p13.18 (252)
13.5. APPROACHES USING COMMUNICATION COMPLEXITY
Chapter notes and history
Shannon deﬁned circuit complexity, including monotone circuit complexity, in 1949. The topic was
studied in Russia since the 1950s. (See Trakhtenbrot [Tra84] for some references.) Savage [Sav72]
was the ﬁrst to observe the close relationship between time required to decide a language on a
TM and its circuit complexity, and to suggest circuit lowerbounds as a way to separate complexity
classes. A burst of results in the 1980s, such as the separation of P from AC0 [FSS84, Ajt83]
and Razborov’s separation of monotone NP from monotone P/poly [Raz85b] raised hopes that a
resolution of P versus NP might be near. These hopes were dashed by Razborov himself [Raz89]
when he showed that his method of approximations was unlikely to apply to nonmonotone circuits.
Later Razborov and Rudich [RR97] formalized what they called natural proofs to show that all
lines of attack considered up to that point were unlikely to work. (See Chapter 22.)
Our presentation in Sections 13.2 and 13.3 closely follows that in Boppana and Sipser’s excellent
survey of circuit complexity [BS90], which is still useful and current 15 years later.
(It omits
discussion of lowerbounds on algebraic circuits; see [Raz04] for a recent result.)
H˚astad’s switching lemma [Has86] is a stronger form of results from[FSS84, Ajt83, Yao85].
The Razborov-Smolensky method of using approximator polynomials is from [Raz87], strength-
ened in[Smo87].
Valiant’s observations about superlinear circuit lowerbounds are from a 1975
paper [Val75] and an unpublished manuscript—lack of progress on this basic problem gets more
embarrassing by the day!.
The 4.5n −o(n) lowerbound on general circuits is from Lachish-Raz [LR01].
Exercises
§1 Suppose that f is computable by an AC 0 circuit C of depth d and size S. Prove that f is
computable by an AC 0 circuit C′ of size 10S and depth d that does not contain NOT gates
but instead has n additional inputs that are negations of the original n inputs.
Hint: each gate in the old circuit gets a twin that computes its
negation.
§2 Suppose that f is computable by an AC 0 circuit C of depth d and size S. Prove that f is
computable by an AC0 C′ circuit of size (10S)d and depth d where each gate has fanout 1.
§3 Prove that for t > n/2,
  n
t+k

≤
 n
t
 
n
n−t
k
. Use this to complete the proof of Lemma 13.2
(Section 13.1.2).
§4 Show that ACC0 ⊆NC1.
§5 Identify reasons why the Razborov-Smolensky method does not work when the circuit has
modm gates, where m is a composite number.
§6 Show that representing the OR of n variables x1, x2, . . . , xn exactly with a polynomial over
GF(q) where q is prime requires degree exactly n.
Web draft 2007-01-08 21:59

DRAFT
13.5. APPROACHES USING COMMUNICATION COMPLEXITY
p13.19 (253)
§7 The Karchmer-Wigderson game can be used to prove upperbounds, and not just lowerbounds.
Show using this game that PARITY and MAJORITY are in NC1.
§8 Show that if a language is computed by a polynomial-size branching program of width 5 then
it is in NC1.
§9 Prove Valiant’s Lemma (Lemma 13.14).
Hint: A directed acyclic graph can be be turned into a leveled
graph, such that if u →v is an edge then u occurs at a lower
level than v. Label this edge by looking at the numbers given to
the levels of u, v and remove the edges corresponding to the least
popular label.
Web draft 2007-01-08 21:59

DRAFT
p13.20 (254)
13.5. APPROACHES USING COMMUNICATION COMPLEXITY
Web draft 2007-01-08 21:59

DRAFT
Chapter 14
Algebraic computation models
The Turing machine model captures computations on bits (equivalently, integers), but it does not
always capture the spirit of algorithms which operate on, say the real numbers R or complex num-
bers C. Such algorithms arise in a variety of applications such as numerical analysis, computational
geometry, robotics, and symbolic algebra. A simple example is Newton’s method for ﬁnding roots
of a given real-valued function function f. It iteratively produces a sequence of candidate solutions
x0, x1, x2, . . . , ∈R where xi+1 = xi −f(xi)/f′(xi). Under appropriate conditions this sequence can
be shown to converge to a root of f.
Of course, a perfectly defensible position to take is that even the behavior of such algorithms
should be studied using TMs, since they will be run on real-life computers, which represent real
numbers using ﬁnite precision. In this chapter though, we take a diﬀerent approach and study
models which do allow arithmetic operations on real numbers (or numbers from ﬁelds other than
R). Such an idealized model may not be implementable, strictly speaking, but it provides a useful
approximation to the asymptotic behavior as computers are allowed to use more and more precision
in their computations. Furthermore, one may be able to prove nontrivial lowerbounds for these
models using techniques from well-developed areas of mathematics such as algebraic geometry and
topology. (By contrast, boolean circuit lowerbounds have proven very diﬃcult.)
However, coming up with a meaningful, well-behaved model of algebraic computation is not an
easy task, as the following example suggests.
Example 14.1 (Pitfalls awaiting designers of such models)
A real number can encode inﬁnite amount of information. For example, a single real number is
enough to encode the answer to every instance of SAT (or any other language, in general). Thus,
a model that can store any real number with inﬁnite precision may not be realistic. Shamir has
shown how to factor any integer n in poly(log n) time on a computer that can do real arithmetic
with arbitrary precision.
The usual way to avoid this pitfall is to restrict the algorithms’ ability to access individual
bits (e.g., the machine may require more than polynomial time to extract a particular digit from
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p14.1 (255)

DRAFT
p14.2 (256)
14.1. ALGEBRAIC CIRCUITS
a real number). Or, sometimes (as in case of Algebraic Computation Trees) it is OK to consider
unrealistically powerful models since the goal is to prove nontrivial lowerbounds —say, superlinear
or quadratic— rather than arbitrary polynomial lowerbounds. After all, lowerbounds for unrealis-
tically powerful models will apply to more realistic (and weaker) models as well.
This chapter is a sketchy introduction to algebraic complexity. It introduces three algebraic
computation models: algebraic circuits, algebraic computation trees, and algebraic Turing Ma-
chines. The algebraic TM is closely related to the standard Turing Machine model and allows
us to study similar questions for arbitrary ﬁelds — including decidability and complexity–that
we earlier studied for strings over {0, 1}. We introduce an undecidable problem (namely, deciding
membership in the Mandelbrot set) and and an NP-complete problem (decision version of Hilbert’s
Nullstellensatz) in this model.
14.1
Algebraic circuits
An algebraic circuit over a ﬁeld F is deﬁned by analogy with a boolean circuit. It consists of a
directed acyclic graph. The leaves are called input nodes and labeled x1, x2, . . . , xn, except these
take values in a ﬁeld F rather than boolean variables. There are also special input nodes, labeled
with the constants 1 and −1 (which are ﬁeld elements). Each internal node, called a gate, is labeled
with one of the arithmetic operations {+, ⋆} rather than with the boolean operations ∨, ∧, ¬ used
in boolean circuits. There is only output node. We restrict indegree of each gate to 2. The size of
the circuit is the number of gates in it. One can also consider algebraic circuits that allow division
(÷) at the gates. One can also study circuits that have access to “constants” other than 1; though
typically one assumes that this set is ﬁxed and independent of the input size n. Finally, as in the
boolean case, if each gate has outdegree 1, we call it an arithmetic formula.
A gate’s operation consists of performing the operation it is labeled with on the numbers present
on the incoming wires, and then passing this output to all its outgoing wires. After each gate has
performed its operation, an output appears on the circuit’s lone output node. Thus the circuit may
be viewed as a computing a function f(x1, x2, . . . , xn) of the input variables, and simple induction
shows that this output function is a (multivariate) polynomial in x1, x2, . . . , xn. If we allow gates to
also be labeled with the division operation (denoted “÷”) then the function is a rational function
of x1, . . . , xn, in other words, functions of the type f1(x1, x2, . . . , xn)/f2(x1, . . . , xn) where f1, f2
are polynomials. Of course, if the inputs come from a ﬁeld such as R, then rational functions can
be used to approximate —via Taylor series expansion —all “smooth” real-valued functions.
As usual, we are interested in the asymptotic size (as a function of n) of the smallest family
of algebraic circuits that computes a family of polynomials {fn} where fn is a polynomial in n
variables. The exercises ask you to show that circuits over GF(2) (with no ÷) are equivalent to
boolean circuits, and the same is true for circuits over any ﬁnite ﬁeld. So the case when F is inﬁnite
is usually of greatest interest.
Example 14.2
The discrete fourier transform of a vector a = (a0, a1, . . . , an−1) where ai ∈C is vector M ·a, where
M is a ﬁxed n × n matrix whose (i, j) entry is ωij where ω is an nth root of 1 (in other words, a
complex number satisfying ωn = 1).
Web draft 2007-01-08 21:59

DRAFT
14.1. ALGEBRAIC CIRCUITS
p14.3 (257)
Interpreting the trivial algorithm for matrix-vector product as an arithmetic circuit, one obtains
an algebraic formula of size O(n2). Using the famous fast fourier transform algorithm, one can
obtain a smaller circuit (or formula??; CHECK) of size O(n log n).
status of lowerbounds??
Example 14.3
The determinant of an n × n matrix X = (Xij) is
det(X) =
X
σ∈Sn
n
Y
i=1
xiσ(i),
(1)
where Sn is the set of all n! permutations on {1, 2, . . . , n}. This can be computed using the familiar
Gaussian elimination algorithm. Interpreting the algorithm as a circuit one obtains an arithmetic
circuit of size O(n3). Using the NC2 algorithm for Gaussian elimination, one obtains an arithmetic
formula of size 2O(log2 n). No matching lowerbounds are known for either upperbound.
The previous example is a good illustration of how the polynomial deﬁning a function may have
exponentially many terms —in this case n!—but nevertheless be computable with a polynomial-size
circuit (as well as a subexponential-size formula).
By contrast, no polynomial-size algebraic circuit is conjectured to exist for the permanent
function, which at ﬁrst sight seems is very similar to the determinant but as we saw in Section ??,
is #P-complete.
permanent(X) =
X
σ∈Sn
(−1)sgn(σ)
n
Y
i=1
xiσ(i),
(2)
The determinant and permanent functions also play a vital role in the world of algebraic circuits,
since they are complete problems for two important classes. To give the deﬁnition, we need the
notion of degree of a multivariate polynomial, namely, the minimum d such that each monomial
term Q
i xdi
i
satisﬁes P
i di ≤d. A family of polynomials in x1, x2, . . . , xn is poly-bounded if the
degree is at most O(nc) for some constant c > 0.
Definition 14.4 (AlgP)
The class AlgP is the class of polynomials of polynomial degree that are computable by arithmetic
formulae (using no ÷) of polynomial size.
Definition 14.5 (AlgNP)
AlgNP is the class of polynomials of polynomial degree that are deﬁnable as
f(x1, x2, . . . , xn) =
X
e∈{0,1}m−n
gn(x1, x2, . . . , xn, en+1, . . . , em),
where gn ∈AlgP and m is polynomial in n.
Web draft 2007-01-08 21:59

DRAFT
p14.4 (258)
14.2. ALGEBRAIC COMPUTATION TREES
Definition 14.6 (Projection reduction)
A function f(x1, . . . , xn) is a projection of a function g(y1, y2, . . . , ym) if there is a mapping σ from
{y1, y2, . . . , ym} to {0, 1, x1, x2, . . . , xn} such that f(x1, x2, . . . , xn) = g(σ(y1), σ(y2), . . . , σ(ym)).
We say that f is projection-reducible to g if f is a projection of g.
Theorem 14.7 (Valiant)
Every polynomial on n variables that is computable by a circuit of size u is projection reducible to
the Determinant function (over the same ﬁeld) on u + 2 variables.
Every function in AlgNP is projection reducible to the Permanent function (over the same
ﬁeld).
14.2
Algebraic Computation Trees
An algebraic computation tree is reminiscent of a boolean decision tree (Chapter ??) but it computes
a boolean-valued function f :Rn →{0, 1}. Consider for example the ELEMENT DISTINCTNESS
problem of deciding, given n numbers x1, x2, . . . , xn, whether any two of them are the same. To
study it in the decision tree model, we might study it by thinking of the input as a matrix of size n2
where the (i, j) entry indicates whether or or not xi > xj or xi = xj or xi < xj. But one can also
study it as a problem whose input is a vector of n real numbers. Consider the trivial algorithm in
either viewpoint: sort the numbers in O(n log n) time and then check if any two adjacent numbers
in the sorted order are the same.
Is this trivial algorithm actually optimal?
This question is
still open, but one can prove optimality with respect to a more restricted class of algorithms that
includes the above trivial algorithm.
Recall that comparison-based sorting algorithms only ask questions of the type “Is xi > xj?”,
which is the same as asking whether xi −xj > 0. The left hand side term of this last inequality is a
linear function. Other algorithms may use more complicated functions. In this section we consider
a model called Algebraic Computation Trees, where we examine the eﬀect of allowing a) the use of
any polynomial function and b) the introduction of new variables together with the ability to ask
questions about them.
Definition 14.8 (Algebraic Computation Tree)
An Algebraic Computation Tree is a way to represent a function f : ℜn →{0, 1} by showing how
to compute f(x1, x2, . . . , xn) for any input vector (x1, x2, . . . , xn). It is a complete binary tree that
describes where each of the nodes has one of the following types:
• Leaf labeled “Accept” or “Reject”.
• Computation node v labeled with yv, where yv = yu ◦yw and yu, yw are either one of
{x1, x2, . . . , xn} or the labels of ancestor nodes and the operator ◦is in {+, −, ×, ÷, √}.
• Branch node with out-degree 2. The branch that is taken depends on the evaluation of some
condition of the type yu = 0 or yu ≥0 or yu ≤0 where yu is either one of {x1, x2, . . . , xn} or
the labels of an ancestor node in the tree.
Web draft 2007-01-08 21:59

DRAFT
14.2. ALGEBRAIC COMPUTATION TREES
p14.5 (259)
Figure unavailable in pdf ﬁle.
Figure 14.1: An Algebraic Computation Tree
Figure unavailable in pdf ﬁle.
Figure 14.2: A computation path p of length d deﬁnes a set of constraints over the n input variables xi and d
additional variables yj, which correspond to the nodes on p.
The computation on any input (x1, x2, . . . , xn) follows a single path the root to a leaf, evaluating
functions at internal nodes (including branch nodes) in the obvious way. The complexity of the
computation on the path is measured using the following costs (which reﬂect real-life costs to some
degree):
• +, −are free.
• ×, ÷, √are charged unit cost.
The depth of the tree is the maximum cost of any path in it.
A fragment of an algebraic decision tree is shown in ﬁgure 14.1. The following examples illustrate
some of the languages (over real numbers) whose complexity we want to study.
Example 14.9
[Element Distinctness Problem] Given n numbers x1, x2, . . . , xn we need to determine whether they
are all distinct. This is equivalent to the question whether Q
i̸=j(xi −xj) ̸= 0. As indicated earlier,
this can be computed by a tree of depth O(n log n) whose internal nodes only compute functions
of the type xi −xj.
Example 14.10
[Real number version of subset sum] Given a set of n real numbers X = {x1, x2, . . . , xn} we ask
whether there is a subset S ⊆X such that P
i∈S xi = 1.
Of course, a tree of depth d could have 2d nodes, so a small depth decision tree does not always
guarantee an eﬃcient algorithm. This is why the following theorem (which we do not prove) does
not have any implication for P versus NP.
Theorem 14.11
The real number version of subset sum can be solved using an algebraic computation tree of depth
O(n5).
Web draft 2007-01-08 21:59

DRAFT
p14.6 (260)
14.2. ALGEBRAIC COMPUTATION TREES
This theorem suggests that Algebraic Computation Trees are best used to investigate lower-
bounds such as n log n or n2. To prove lowerbounds for a function f, we will use the topology of the
sets f−1(1) and f−1(0), speciﬁcally, the number of connected components. In fact, we will think of
any function f :Rn →R as being deﬁned by a subset W ⊆Rn, where W = f−1(1).
Definition 14.12
Let W ⊆Rn. The algebraic computation tree complexity of W is
C(W) =
min
computation
trees C for W
{depth of C}
Definition 14.13 (connected components)
A set S ⊆Rn is connected if for all x, y ∈S there is path p that connects x and y and lies entirely
in S. For S ⊆Rn we deﬁne #(S) to be the number of connected components of S.
Theorem 14.14
Let W = {(x1, . . . , xn)| Q
i̸=j (xi −xj) ̸= 0}. Then,
#(W) ≥n!
Proof: For each permutation σ let
Wσ = {(x1, . . . , xn) | xσ(1) < xσ(2) < . . . < xσ(n)}.
That is, let Wσ be the set of n-tuples (x1, . . . , xn) to which σ gives order. It suﬃces to prove for
all σ′ ̸= σ that the sets Wσ and Wσ′ are not connected.
For any two distinct permutations σ and σ′, there exist two distinct i, j with 1 ≤i, j ≤n, such
that σ−1(i) < σ−1(j) but σ−1(i) > σ−1(j). Thus, in Wσ we have Xj −Xi > 0 while in Wσ′ we
have Xi −Xj > 0. Consider any path from Wσ to Wσ′. Since Xj −Xi has diﬀerent signs at the
endpoints, the intermediate value principle says that somewhere along the path this term must
become 0. Deﬁnition 14.13 then implies that Wσ and Wσ′ cannot be connected. ■
The connection between the two parameters we have deﬁned thus far is the following theo-
rem, whose proof will use a fundamental theorem of topology. It also implies, using our obser-
vation above, that the algebraic computation tree complexity of ELEMENT DISTINCTNESS is
Ω(log(n!)) = Ω(n log n).
Theorem 14.15 (Ben-Or)
C(W) = Ω

log (max {#(W), #(Rn −W)}) −n

This theorem is proved in two steps. First, we try to identify the property of functions with
low decision tree complexity: they can be deﬁned using a “few” systems of equations.
Web draft 2007-01-08 21:59

DRAFT
14.2. ALGEBRAIC COMPUTATION TREES
p14.7 (261)
Lemma 14.16
If f :ℜn →{0, 1} has a decision tree of depth d then f−1(1) (and also f−1(0)) is a union of at most
2d sets C1, C2, . . . , where Ci is the set of solutions to some algebraic system of up to d equations
of the type
pi(y1, . . . , yd, x1, . . . , xn) ▷◁0,
where pi for i ≤d is a degree 2 polynomial, ▷◁is in {≤, ≥, =, ̸=}, and y1, . . . , yd are new variables.
(Rabinovitch’s Trick) Additionally, we may assume without loss of generality (at the cost of
doubling the number of yi’s) that there are no ̸= constraints in this system of equations.
Proof: The tree has 2d leaves, so it suﬃces to associate a set with each leaf. This is simply the set
of (x1, x2, . . . , xn) that end up at that leaf. Associate variables y1, y2, . . . , yd with the d tree nodes
appearing along the path from root to that leaf. For each tree nodes associate an equation with it
in the obvious way (see ﬁgure 14.2). For example, if the node computes yv = yu ÷yw then it implies
the constraint yvyw −yu = 0. Thus any (x1, x2, . . . , xn) that end up at the leaf is a vector with an
associated value of y1, y2, . . . , yd such that the combined vector is a solution to these d equations.
To replace the “̸=” constraints with “=” constraints we take a constraint like
pi(y1, . . . , ym) ̸= 0,
introduce a new variable zi and impose the constraint
qi(y1, . . . , ym, zi) ≡1 −zipi(y1, . . . , ym) = 0.
(This transformation holds for all ﬁelds.) Notice, the maximum degree of the constraint remains
2, because the trick is used only for the branch yu ̸= 0 which is converted to 1 −zvyu = 0.
■
Remark 14.17
We ﬁnd Rabinovitch’s trick useful also in Section 14.3.2 where we prove a completeness result for
Hilbert’s Nullstellensatz.
Another version of the trick is to add the constraint
p2
i (y1, . . . , ym) > 0,
which doubles the degree and does not hold for all ﬁelds (e.g., the complex numbers).
Thus we need some result about the number of connected components of the set of solutions to
an algebraic system. The following is a central result in mathematics.
Theorem 14.18 (Simple consequence of Milnor-Thom)
If S ⊆Rn is deﬁned by degree d constraints with m equalities and h inequalities then
#(S) ≤d(2d −1)n+h−1
Remark 14.19
Note that the above upperbound is independent of m.
Web draft 2007-01-08 21:59

DRAFT
p14.8 (262)
14.3. THE BLUM-SHUB-SMALE MODEL
Figure unavailable in pdf ﬁle.
Figure 14.3: Projection can merge but not add connected components
Now we can prove Ben-Or’s Theorem.
Proof: (Theorem 14.15) Suppose that the depth of a computation tree for W is d, so that there
are at most 2d leaves. We will use the fact that if S ⊆Rn and S|k is the set of points in S with
their n −k coordinates removed (projection on the ﬁrst k coordinates) then #(S|k) ≤#(S) (ﬁgure
14.3).
For every leaf there is a set of degree 2 constraints. So, consider a leaf ℓand the corresponding
constraints Cℓ, which are in variables x1, . . . , xn, y1, . . . , yd. Let Wℓ⊆Rn be the subset of inputs
that reach ℓand Sℓ⊆Rn+d the set of points that satisfy the constraints Cℓ. Note that Wℓ= Cℓ|n
i.e., Wℓis the projection of Cℓonto the ﬁrst n coordinates. So, the number of connected components
in Wℓis upperbounded by #(Cℓ). By Theorem 14.18 #(Cℓ) ≤2 · 3n+d−1 ≤3n+d. Therefore the
total number of connected components is at most 2d3n+d, so d ≥log(#(W)) −O(n). By repeating
the same argument for Rn −W we have that d ≥log(#(Rn −W)) −O(n). ■
14.3
The Blum-Shub-Smale Model
Blum, Shub and Smale introduced Turing Machines that compute over some arbitrary ﬁeld K (e.g.,
K = R, C, Z2). This is a generalization of the standard Turing Machine model which operates over
the ring Z2. Each cell can hold an element of K, Initially, all but a ﬁnite number of cells are “blank.”
In our standard model of the TM, the computation and branch operations can be executed in the
same step. Here we perform these operations separately. So we divide the set of states into the
following three categories:
• Shift state: move the head to the left or to the right of the current position.
• Branch state: if the content of the current cell is a then goto state q1 else goto state q2.
• Computation state: replace the contents of the current cell with a new value. The machine
has a hardwired function f and the new contents of the cell become a ←f(a). In the standard
model for rings, f is a polynomial over K, while for ﬁelds f is a rational function p/q where
p, q are polynomials in K[x] and q ̸= 0. In either case, f can be represented using a constant
number of elements of K.
• The machine has a single “register” onto which it can copy the contents of the cell currently
under the head. This register’s contents can be used in the computation.
In the next section we deﬁne some complexity classes related to the BSS model. As usual, the
time and space complexity of these Turing Machines is deﬁned with respect to the input size, which
is the number of cells occupied by the input.
Web draft 2007-01-08 21:59

DRAFT
14.3. THE BLUM-SHUB-SMALE MODEL
p14.9 (263)
Remark 14.20
The following examples show that some modiﬁcations of the BSS model can increase signiﬁcantly
the power of an algebraic Turing Machine.
• If we allow the branch states to check, for arbitrary real number a, whether a > 0 (in
other words, with arbitrary precision) the model becomes unrealistic because it can decide
problems that are undecidable on the normal Turing machine. In particular, such a machine
can compute P/poly in polynomial time; see Exercises. (Recall that we showed that P/poly
contains undecidable languages.) If a language is in P/poly we can represent its circuit family
by a single real number hardwired into the Turing machine (speciﬁcally, as the coeﬃcient of
of some polynomial p(x) belonging to a state). The individual bits of this coeﬃcient can be
accessed by dividing by 2, so the machine can extract the polynomial length encoding of each
circuit. Without this ability we can prove that the individual bits cannot be accessed.
• If we allow rounding (computation of ⌊x⌋) then it is possible to factor integers in polynomial
time, using some ideas of Shamir. (See exercises.)
Even without these modiﬁcations, the BSS model seems more powerful than real-world com-
puters: Consider the execution of the operation x ←x2 for n times. Since we allow each cell to
store a real number, the Turing machine can compute and store in one cell (without overﬂow) the
number x2n in n steps.
14.3.1
Complexity Classes over the Complex Numbers
Now we deﬁne the corresponding to P and NP complexity classes over C:
Definition 14.21 (PC,NPC)
PC is the set of languages that can be decided by a Turing Machine over C in polynomial time.
NPC is the set of languages L for which there exists a language L0 in PC, such that an input x is
in L iﬀthere exists a string (y1, . . . , ync) in Cnc such that (x, y) is in L0.
The following deﬁnition is a restriction on the inputs of a TM over C. These classes are useful
because they help us understand the relation between algebraic and binary complexity classes.
Definition 14.22 (0-1-NPC)
0-1-NPC = {L ∩{0, 1}∗| L ∈NPC}
Note that the input for an NPC machine is binary but the nondeterministic “witness” may
consist of complex numbers. Trivially, 3SAT is in 0-1-NPC: even though the “witness” consists of
a string of complex numbers, the machine ﬁrst checks if they are all 0 or 1 using equality checks.
Having veriﬁed that the guess represents a boolean assignment to the variables, the machine con-
tinues as a normal Turing Machine to verify that the assignment satisﬁes the formula.
It is known that 0-1-NPC ⊆PSPACE. In 1997 Koiran proved that if one assumes the Riemann
hypothesis, then 0-1-NPC ⊆AM[2]. Recall that AM[2] is BP · NP so Koiran’s result suggests
that 0-1-NPC may not be much bigger than NP.
Web draft 2007-01-08 21:59

DRAFT
p14.10 (264)
14.3. THE BLUM-SHUB-SMALE MODEL
Figure unavailable in pdf ﬁle.
Figure 14.4: Tableau of Turing Machine conﬁgurations
14.3.2
Hilbert’s Nullstellensatz
The language HNC is deﬁned as the decision version of Hilbert’s Nullstellensatz over C. The input
consists of m polynomials pi of degree d over x1, . . . , xn. The output is “yes” iﬀthe polynomials
have a common root a1, . . . , an. Note that this problem is general enough to include SAT. We
illustrate that by the following example:
x ∨y ∨z ↔(1 −x)(1 −y)(1 −z) = 0.
Next we use this fact to prove that the language 0-1-HNC (where the polynomials have 0-1 coeﬃ-
cients) is complete for 0-1-NPC.
Theorem 14.23 (BSS)
0-1-HNC is complete for 0-1-NPC.
Proof: (Sketch) It is straightforward to verify that 0-1-HNC is in 0-1-NPC. To prove the hard-
ness part we imitate the proof of the Cook-Levin theorem; we create a computation tableau and
show that the veriﬁcation is in 0-1-HNC.
To that end, consider the usual computation tableau of a Turing Machine over C and as in the
case of the standard Turing Machines express the fact that the tableau is valid by verifying all the
2×3 windows, i.e., it is suﬃcient to perform local checks (Figure 14.4). Reasoning as in the case of
algebraic computation trees (see Lemma 14.16) we can express these local checks with polynomial
constraints of bounded degree. The computation states c ←q(a, b)/r(a, b) are easily handled by
setting p(c) ≡q(a, b) −cr(a, b). For the branch states p(a, b) ̸= 0 we can use Rabinovitch’s trick
to convert them to equality checks q(a, b, z) = 0. Thus the degree of our constraints depends upon
the degree of the polynomials hardwired into the machine. Also, the polynomial constraints use
real coeﬃcients (involving real numbers hardwired into the machine). Converting these polynomial
constraints to use only 0 and 1 as coeﬃcients requires work. The idea is to show that the real
numbers hardwired into the machine have no eﬀect since the input is a binary string. We omit this
mathematical argument here. ■
14.3.3
Decidability Questions: Mandelbrot Set
Since the Blum-Shub-Smale model is more powerful than the ordinary Turing Machine, it makes
sense to revisit decidability questions. In this section we show that some problems do indeed remain
undecidable. We study the decidability of the Mandelbrot set with respect to Turing Machines over
C. Roger Penrose had raised this question in his meditation regarding artiﬁcial intelligence.
Web draft 2007-01-08 21:59

DRAFT
14.3. THE BLUM-SHUB-SMALE MODEL
p14.11 (265)
Definition 14.24 (Mandelbrot set decision problem)
Let PC(Z) = Z2 + C. Then, the Mandelbrot set is deﬁned as
M = {C | the sequence PC(0), PC(PC(0)), PC(PC(PC(0))) . . . is bounded }.
Note that the complement of M is recognizable if we allow inequality constraints.
This is
because the sequence is unbounded iﬀsome number P k
C(0) has complex magnitude greater than 2
for some k (exercise!) and this can be detected in ﬁnite time. However, detecting that P k
C(0) is
bounded for every k seems harder. Indeed, we have:
Theorem 14.25
M is undecidable by a machine over C.
Proof: (Sketch) The proof uses the topology of the Mandelbrot set. Let M be any TM over the
complex numbers that supposedly decides this set. Consider T steps of the computation of this
TM. Reasoning as in Theorem 14.23 and in our theorems about algebraic computation trees, we
conclude that the sets of inputs accepted in T steps is a ﬁnite union of semialgebraic sets (i.e., sets
deﬁned using solutions to a system of polynomial equations). Hence the language accepted by M
is a countable union of semi-algebraic sets, which implies that its Hausdorft dimension is 1. But it
is known Mandelbrot set has Hausdorﬀdimension 2, hence M cannot decide it. ■
Exercises
§1 Show that if ﬁeld F is ﬁnite then arithmetic circuits have exactly the same power —up to
constant factors—as boolean circuits.
§2 Equivalence of circuits of depth d to straight line programs of size exp(d). (Lecture 19 in
Madhu’s notes.)
§3 Bauer-Strassen lemma?
§4 If function computed in time T on algebraic TM then it has algebraic computation tree of
depth O(d).
§5 Prove that if we give the BSS model (over R) the power to test “a > 0?” with arbitrary preci-
sion, then all of P/poly can be decided in polynomial time. (Hint: the machine’s “program”
can contain a constant number of arbitrary real numbers.)
§6 Shamir’s trick?
Chapter notes and history
needs a lot
General reference on algebraic complexity
Web draft 2007-01-08 21:59

DRAFT
p14.12 (266)
14.3. THE BLUM-SHUB-SMALE MODEL
P. Brgisser, M. Clausen, and M. A. Shokrollahi, Algebraic complexity theory, Springer-Verlag,
1997.
Best reference on BSS model
Blum Cucker Shub Smale.
Algebraic P and NP from Valiant 81 and Skyum-Valiant’86.
Roger Penrose: emperor’s new mind.
Mandelbrot : fractals.
Web draft 2007-01-08 21:59

DRAFT
Part III
Advanced topics
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p14.13 (267)

DRAFT

DRAFT
Chapter 15
Average Case Complexity: Levin’s
Theory
1
needs more work
Our study of complexity —- NP-completeness, #P-completeness etc.— thus far only concerned
worst-case complexity. However, algorithms designers have tried to design eﬃcient algorithms for
NP-hard problems that work for “many” or “most” instances.
This motivates a study of the
diﬃculty of the “average” instance. Let us ﬁrst examine the issues at an intuitive level, so we may
be better prepared for the elements of the theory we will develop.
Many average case algorithms are targeted at graph problems in random graphs. One can deﬁne
random graphs in many ways: the simplest one generates a graph on n vertices randomly by picking
each potential edge with probability 1/2. (This method ends up assigning equal probability to every
n-vertex graph.) On such rand om graphs, many NP-complete problems are easy. 3-COLOR can
be solved in linear time with high probability (exercise). CLIQUE and INDEPENDENT SET can
be solved in n2 log n time (exercise) which is only a little more than polynomial and much less than
2ϵn, the running time of the best algorithms on worst-case instances.
However, other NP-complete problems appear to require exponential time even on average. One
example is SUBSET SUM: we pick n integers a1, a2, . . . , an randomly from [1, 2n], pick a random
subset S of {1, . . . , n}, and produce b = P
i∈S ai. We do not know of any eﬃcient average-case
algorithm that, given the ai’s and b, ﬁnds S. Surprisingly, eﬃcient algorithms do exist if the ai’s are
picked randomly from the slightly larger interval [1, 2n log2 n]. This illustrates an important point,
namely, that average-case complexity is sensitive to the choice of the input distribution.
The above discussion suggests that even though NP-complete problems are essentially equiva-
lent with respect to worst case complexity, they may diﬀer vastly in their average case complexity.
Can we nevertheless identify some problems that remain “complete” even for the average case; in
other words, are at least as hard as every other average-case NP problem?
This chapter covers Levin’s theory of average-case complexity. We will formalize the notion
of “distributional problems,” introduce a working deﬁnition of “algorithms that are eﬃcient on
1This chapter written with Luca Trevisan
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p15.1 (269)

DRAFT
p15.2 (270)
15.1. DISTRIBUTIONAL PROBLEMS
Note 15.1 (Impagliazzo’s Possible Worlds)
At the moment we don’t know if the best algorithm for 3SAT runs in time
O(n) or 2Ω(n) but there are also many other qualitative open questions about
the hardness of problems in NP. Russell Impagliazzo characterized a central
goal of complexity theory as the question of ﬁnding out which of the following
possible worlds is the world we live in:
Algorithmica.
Heuristica.
Pessiland.
Minicrypt.
average,” and deﬁne a reduction that preserves eﬃcient average-case solvability.
We will also
exhibit an NP-complete problem that is complete with respect to such reductions. However, we
cannot yet prove the completeness of natural distributional problems such as SUBSET SUM or one
of the number theoretic problems described in the chapter on cryptography.
15.1
Distributional Problems
In our intuitive discussion of average case problems, we ﬁrst ﬁxed an input size n and then considered
the average running time of the algorithm when inputs of size n are chosen from a distribution. At
the back of our mind, we knew that complexity has to be measured asymptotically as a function of
n. To formalize this intuitive discussion, we will deﬁne distributions on all (inﬁnitely many) inputs.
Definition 15.2 (Distributional Problem)
A distributional problem is a pair ⟨L, D⟩, where L is a decision problem and D is a distribution
over the set {0, 1}∗of possible inputs.
Example 15.3
We can deﬁne the “uniform distribution” to be one that assigns an input x ∈{0, 1}∗the probability
Pr [x] =
1
|x| (1 + |x|)2−|x|.
(1)
We call this “uniform” because it assigns equal probabilities to all strings with the same length.
It is a valid distribution because the probabilities sum to 1:
rect??
Web draft 2007-01-08 21:59

DRAFT
15.1. DISTRIBUTIONAL PROBLEMS
p15.3 (271)
X
x∈{0,1}∗
1
|x| (1 + |x|)2−|x| =
X
n≥0
2n
2−n
n(n + 1) = 1.
(2)
Here is another distribution; the probabilities sum to 1 since P
n≥1
1
n2 = π2/6.
Pr[x] = 6
π2
2−|x|
|x|2
if |x| ≥1
(3)
To pick a string from these distributions, we can ﬁrst an input length n with the appropriate
probability (for the distribution in (2), we pick n with probability 6/π2n2) and then pick x uniformly
from inputs of length n. This uniform distribution corresponds to the intuitive approach to average
case complexity discussed in the introduction. However, the full generality of Deﬁnition 15.2 will
be useful later when we study nonuniform input distributions.
15.1.1
Formalizations of “real-life distributions.”
Real-life problem instances arise out of the world around us (images that have to be understood,
a building that has to be navigated by a robot, etc.), and the world does not spend a lot of
time tailoring instances to be hard for our algorithm —arguably, the world is indiﬀerent to our
algorithm. One may formalize this indiﬀerence in terms of computational eﬀort, by hypothesizing
that the instances are produced by an eﬃcient algorithm. We can formalize this in two ways.
Polynomial time computable distributions. Such distributions have an associated determin-
istic polynomial time machine that, given input x, can compute the cumulative probability
µD(x), where
µD(x) =
X
y≤x
Pr
D [y]
(4)
Here PrD[y] denotes the probability assigned to string y and y ≤x means y either precedes x
in lexicographic order or is equal to x. Denoting the lexicographic predecessor of x by x −1,
we have
Pr
D [x] = µD(x) −µD(x −1),
(5)
which shows that if µD is computable in polynomial time, then so is PrD[x]. The uniform
distributions in (1) and (1) are polynomial time computable, as are many other distributions
that are deﬁned using explicit formulae.
Polynomial time samplable distributions. These distributions have an associated probabilis-
tic polynomial time machine that can produce samples from the distribution. In other words,
it outputs x with probability PrD[x]. The expected running time is polynomial in the length
of the output |x|.
Many such samplable distributions are now known, and the sampling algorithm often uses
Monte Carlo Markov Chain (MCMC) techniques.
Web draft 2007-01-08 21:59

DRAFT
p15.4 (272)
15.2. DISTNP AND ITS COMPLETE PROBLEMS
If a distribution is polynomial time computable then we can eﬃciently produce samples from
it. (Exercise.) However, if P ̸= P#P there are polynomial time samplable distributions (including
some very interesting ones) that are not polynomial time computable. (See exercises.)
In this lecture, we will restrict attention to distributional problems involving a polynomial time
computable distribution. This may appear to a serious limitation, but with some work the results
of this chapter can be generalized to samplable distributions.
15.2
DistNP and its complete problems
The following complexity class is at the heart of our study of average case complexity.
dist NP = {⟨L, D⟩: L ∈NP, D polynomial-time computable} .
(6)
Since the same NP language may have diﬀerent complexity behavior with respect to two diﬀerent
input distributions (SUBSET SUM was cited earlier as an example), the deﬁnition wisely treats the
two as distinct computational problems. Note that every problem mentioned in the introduction
to the chapter is in dist NP.
Now we need to deﬁne the average-case analogue of P.
15.2.1
Polynomial-Time on Average
Now we deﬁne what it means for a deterministic algorithm A to solve a distributional problem
⟨L, D⟩in polynomial time on average. The deﬁnition should be robust to simple changes in model
of computation or representation. If we migrate the algorithm to a slower machine that has a
quadratic slowdown (so t steps now take t2 time), then polynomial-time algorithms should not
suddenly turn into exponential-time algorithms. (This migration to a slower machine is not merely
hypothetical, but also one way to look at a reduction.) As we will see, some intuitively appealing
deﬁnitions do not have this robustness property.
Denote by t(x) the running time of A on input x. First, note that D is a distribution on all
possible inputs. The most intuitive choice of saying that A is eﬃcient if
E[t(x)] is small
is problematic because the expectation could be inﬁnite even if A runs in worst-case polynomial
time.
Next, we could try to deﬁne A to be polynomial provided that for some constant c and for every
suﬃciently large n,
E[t(x)| |x| = n] ≤nc
This has two problems. First, it ignores the possibility that there could be input lengths on
which A takes a long time, but that are generated with very low probability under D. In such
cases A may still be regarded as eﬃcient, but the deﬁnition ignores this possibility. Second, and
Web draft 2007-01-08 21:59

DRAFT
15.2. DISTNP AND ITS COMPLETE PROBLEMS
p15.5 (273)
more seriously, the deﬁnition is not robust to changes in computational model. To give an example,
suppose D is the uniform distribution and t(x0) = 2n for just one input x0 of size n For every other
input of size n, t(x) = n. Then E[t(x) | |x| = n] ≤n + 1. However, changing to a model with a
quadratic slowdown will square all running times, and E[(t(x))2 | |x| = n] > 2n.
We could try to deﬁne A to be polynomial if there is a c > 0 such that
E
t(x)
|x|c

= O(1),
but this is also not robust. (Verify this!)
We now come to a satisfying deﬁnition.
Definition 15.4 (Polynomial on average and dist P)
A problem ⟨L, D⟩∈dist NP is said to be in dist P if there is an algorithm A for L that satisﬁes
for some constants c, c1
E[ t(x)1/c
|x|
] = c1,
(7)
where t(x) is the running time of A on input x.
Notice that P ⊆dist P: if a language can be decided deterministically in time t(x) = O(|x|c),
then t(x)1/c = O(|x|) and the expectation in (7) converges regardless of the distribution. Second,
the deﬁnition is robust to changes in computational models: if the running times get squared, we
just multiply c by 2 and the expectation in (7) again converges.
We also point out an additional interesting property of the deﬁnition: there is a high probability
that the algorithm runs in polynomial time. For, if
E[t (x)1/c
|x|
] = c1,
(8)
then we have
Pr[t(x) ≥k · |x|c]
= Pr[t(x)1/c
|x|
≥k1/c]
≤
c1
k1/c
(9)
where the last claim follows by Markov’s inequality. Thus by increasing k we may reduce this
probability as much as required.
15.2.2
Reductions
Now we deﬁne reductions. Realize that we think of instances as being generated according to a
distribution. Deﬁning a mapping on strings (e.g., a reduction) gives rise to a new distribution on
strings. The next deﬁnition formalizes this observation.
Definition 15.5
If f is a function mapping strings to strings and D is a distribution then the distribution f ◦D is
one that assigns to string y the probability P
x:f(x)=y PrD[x]
Web draft 2007-01-08 21:59

DRAFT
p15.6 (274)
15.2. DISTNP AND ITS COMPLETE PROBLEMS
Definition 15.6 (Reduction)
A distributional problem ⟨L1, D1⟩reduces to a distributional problem ⟨L2, D2⟩(denoted ⟨L1, D1⟩≤
⟨L2, D2⟩) if there is a polynomial-time computable function f and an ϵ > 0 such that:
1. x ∈L1 iﬀf (x) ∈L2.
2. For every x, |f(x)| = Ω(|x|ϵ).
3. There are constants c, c1 such that for every string y,
Pr
f◦D1(y) ≤c1 |y|c Pr
D2(y).
(Domination)
The ﬁrst condition is standard for many-to-one reductions, ensuring that a decision algorithm
for L2 easily converts into a decision algorithm for L1. The second condition is a technical one,
needed later. All interesting reductions we know of satisfy this condition. Next, we motivate the
third condition, which says that D2 “dominates” (up to a polynomial factor) the distribution f ◦D1
obtained by applying f on D1.
Realize that the goal of the deﬁnition is to ensure that “if (L1, D1) is hard, then so is (L2, D2)”
(or equivalently, the contrapositive “if (L2, D2) is easy, then so is (L1, D1).”) Thus if an algorithm
A2 is eﬃcient for problem (L2, D2), then the following algorithm ought to be eﬃcient for problem
(L1, D1): on input x obtained from distribution D1, compute f(x) and then run algorithm A2 on
f(x). A priori, one cannot rule out the possibility that that A2 is very slow on some inputs, which
are unlikely to be sampled according to distribution D2 but which show up with high probability
when we sample x according to D1 and then consider f(x). The domination condition helps rule
out this possibility.
In fact we have the following result, whose non-trivial proof we omit.
his proof.
Theorem 15.7
If ⟨L1, D1⟩≤⟨L2, D2⟩and ⟨L2, D2⟩has an algorithm that is polynomial on average, then ⟨L1, D1⟩
also has an algorithm that is polynomial on average.
Of course, Theorem 15.7 is useful only if we can ﬁnd reductions between interesting problems.
Now we show that this is the case: we exhibit a problem (albeit an artiﬁcial one) that is complete
for dist NP.
Let the inputs have the form

M, x, 1t, 1l
, where M is an encoding of a Turing
machine and 1t is a sequence of t ones. Then we deﬁne the following “universal” problem U.
• Decide whether there exists a string y such that |y| ≤l and M (x, y) accepts in at most t
steps.
Since part of the input is in unary, we need to modify our deﬁnition of a “uniform” distribution
to the following.
Pr
D
D
M, x, 1t, 1lE
=
1
|M| (|M| + 1) 2|M| ·
1
|x| (|x| + 1) 2|x| ·
1
(t + l) (t + l + 1).
(10)
Web draft 2007-01-08 21:59

DRAFT
15.2. DISTNP AND ITS COMPLETE PROBLEMS
p15.7 (275)
This distribution is polynomial-time computable (exercise).
Theorem 15.8 (Levin)
⟨U, D⟩is complete for dist NP, where D is the uniform ditribution.
The proof requires the following lemma, which shows that for polynomial-time computable dis-
tributions, we can apply a simple transformation on the inputs such that the resulting distribution
has no “peaks” (i.e., no input has too high a probability).
Lemma 15.9 (Peak Elimination)
Suppose D is a polynomial-time computable distribution over x. Then there is a polynomial-time
computable function g such that
1. g is injective: g (x) = g (z) iﬀx = z.
2. |g(x)| ≤|x| + 1.
3. For every string y, Prg◦D(y) ≤2−|y|+1.
Proof: For any string x such that PrD(x) > 2−|x|, deﬁne h(x) to be the largest common preﬁx
of binary representations of µD(x), µD(x −1).
Then h is polynomial-time computatable since
µD(x) −µD(x −1) = PrD(x) > 2−|x|, which implies that µD(x) and µD(x −1) must diﬀer in the
somewhere in the ﬁrst |x| bits. Thus |h(x)| ≤log 1/ PrD (x) ≤|x|. Furthermore, h is injective
because only two binary strings s1 and s2 can have the longest common preﬁx z; a third string s3
sharing z as a preﬁx must have a longer preﬁx with either s1 or s2.
Now deﬁne
g(x) =
(
0x
if PrD (x) ≤2−|x|
1h(x)
otherwise
(11)
Clearly, g is injective and satisﬁes |g(x)| ≤|x| + 1. We now show that g ◦D does not give
probability more than 2−|y|+1 to any string y. If y is not g(x) for any x, this is trivially true since
Prg◦D(y) = 0.
If y = 0x, where PrD (x) ≤2−|x|, then Prg◦D(y) ≤2−|y|+1 and we also have nothing to prove.
Finally, if y = g(x) = 1h(x) where PrD (x) > 2−|x|, then as already noted, |h(x)| ≤log 1/ PrD(x)
and so Prg◦D(y) = PrD(x) ≤2−|y|+1.
Thus the Lemma has been proved. ■
Now we are ready to prove Theorem 15.8.
Proof: (Theorem 15.8) At ﬁrst sight the proof may seem trivial since U is just the “universal”
decision problem for nondeterministic machines, and every NP language trivially reduces to it.
However, we also need to worry about the input distributions and enforce the domination condition
as required by Deﬁnition 15.6.
Let ⟨L, D1⟩∈dist NP. Let M be a proof-checker for language L that runs in time nc; in
other words, x ∈L iﬀthere is a witness y of length |y| = |x|c such that M(x, y) = Accept. (For
notational ease we drop the big-O notation in this proof.) In order to deﬁne a reduction from L to
Web draft 2007-01-08 21:59

DRAFT
p15.8 (276)
15.2. DISTNP AND ITS COMPLETE PROBLEMS
U, the ﬁrst idea would be to map input x for L to
D
M, x, 1|x|c, 1|x|cE
. However, this may violate
the domination condition because the uniform distribution assigns a probability 2−|x|/poly(|x|) to
⟨M, x, 1|x|c⟩whereas x may have much higher probability under D1. Clearly, this diﬃculty arises
only if the distribution D1 has a “peak” at x, so we see an opportunity to use Lemma 15.9, which
gives us an injective mapping g such that g ◦D1 has no “peaks” and g is computable say in nd time
for some ﬁxed constant d.
The reduction is as follows: map x to ⟨M′, g(x), 1|x|c+|x|, 1|x|c+|x|d⟩. Here M′ is a modiﬁcation
of M that expects as input a string z and a witness (x, y) of length |x| + |x|c. Given (z, x, y) where
y = |x|c, M′ checks in |x|d time if g(x) = z. If so, it simulates M on (x, y) and outputs its answer.
If g(x) ̸= z then M′ rejects.
To check the domination condition, note that y = ⟨M′, g(x), 1|x|c+|x|, 1|x|c+|x|d⟩has probability
Pr
D (y) =
2−|M′|
|M′| (|M′| + 1) ·
2−|g(x)|
|g(x)| (|g(x)| + 1) ·
1
(|x| + 2 |x|c + |x|d)(|x| + 2 |x|c + |x|d + 1)
≤
2−|M′|
|M′| (|M′| + 1)
1
|x|2(c+d+1) · 2−g(x)
(12)
under the uniform distribution whereas
Pr
D1(x) ≤2−g(x)+1 ≤G |x|2(c+d+1) Pr
D (y) ,
if we allow the constant G to absorb the term 2|M′| |M′| (|M′| + 1). Thus the domination condition
is satisﬁed.
Notice, we rely crucially on the fact that 2|M′| |M′| (|M′| + 1) is a constant once we ﬁx the
language L; of course, this constant will usually be quite large for typical NP languages, and this
would be a consideration in practice. ■
15.2.3
Proofs using the simpler deﬁnitions
In the setting of one-way functions and in the study of the average-case complexity of the permanent
and of problems in EXP (with applications to pseudorandomness), we normally interpret “average
case hardness” in the following way: that an algorithm of limited running time will fail to solve
the problem on a noticeable fraction of the input. Conversely, we would interpret average-case
tractability as the existence of an algorithm that solves the problem in polynomial time, except on
a negligible fraction of inputs. This leads to the following formal deﬁnition.
Definition 15.10 (Heuristic polynomial time)
We say that an algorithm A is a heuristic polynomial time algorithm for a distributional problem
⟨L, µ⟩if A always runs in polynomial time and for every polynomial p
X
x:A(x)̸=χL(x)
µ′(x)p(|x|) = O(1)
Web draft 2007-01-08 21:59

DRAFT
15.2. DISTNP AND ITS COMPLETE PROBLEMS
p15.9 (277)
In other words, a polynomial time algorithm for a distributional problem is a heuristic if the
algorithm fails on a negligible fraction of inputs, that is, a subset of inputs whose probability
mass is bounded even if multiplied by a polynomial in the input length. It might also make sense
to consider a deﬁnition in which A is always correct, although it does not necessarily work in
polynomial time, and that A is heuristic polynomial time if there is a polynomial q such that for
every polynomial p, P
x∈Sq µ′(x)p(|x|) = O(1), where Sq is the set of inputs x such that A(x)
takes more than q(|x|) time. Our deﬁnition is only more general, because from an algorithm A as
before one can obtain an algorithm A satisfying Deﬁnition 15.10 by adding a clock that stops the
computation after q(|x|) steps.
The deﬁnition of heuristic polynomial time is incomparable with the deﬁnition of average poly-
nomial time. For example, an algorithm could take time 2n on a fraction 1/nlog n of the inputs of
length n, and time n2 on the remaining inputs, and thus be a heuristic polynomial time algorithm
with respect to the uniform distribution, while not beign average polynomial time with respect
to the uniform distribution. On the other hand, consider an algorithm such that for every input
length n, and for 1 ≤k ≤2n/2, there is a fraction about 1/k2 of the inputs of length n on which
the algorithm takes time Θ(kn). Then this algorithm satisﬁes the deﬁnition of average polynomial
time under the uniform distribution, but if we impose a polynomial clock there will be an inverse
polynomial fraction of inputs of each length on which the algorithm fails, and so the deﬁnition of
heuristic polynomial time cannot be met.
It is easy to see that heuristic polynomial time is preserved under reductions.
Theorem 15.11
If ⟨L1, µ1⟩≤⟨L2, µ2⟩and ⟨L2, µ2⟩admits a heuristic polynomial time algorithm, then ⟨L1, µ1⟩also
admits a heuristic polynomial time algorithm.
Proof: Let A2 be the algorithm for ⟨L2, µ2⟩, let f be the function realizing the reduction, and let
p be the polynomial witnessing the domination property of the reduction. Let c and ϵ be such that
for every x we have |x| ≤c|f(x)|1/ϵ.
Then we deﬁne the algorithm A1 than on input x outputs A2(f(x)). Clearly this is a polynomial
time algorithm, and whenever A2 is correct on f(x), then A1 is correct on x. We need to show that
for every polynomial q
X
x:A2(f(x))̸=χL2(f(x))
µ′
1(x)q(|x|) = O(1)
and the left-hand side can be rewritten as
X
y:A2(y)̸=χL2(y)
X
x:f(x)=y
µ′
1(x)q(|x|)
≤
X
y:A2(y)̸=χL2(y)
X
x:f(x)=y
µ′
1(x)q(c · |y|1/ϵ))
≤
X
y:A2(y)̸=χL2(y)
µ′
2(y)p(|y|)q′(|y|)
=
O(1)
Web draft 2007-01-08 21:59

DRAFT
p15.10 (278)
15.3. EXISTENCE OF COMPLETE PROBLEMS
where the last step uses the fact that A2 is a polynomial heuristic for ⟨L2, µ2⟩and in the second-
to-last step we introduce the polynomial q′(n) deﬁned as q(c · n1/ϵ)
■
15.3
Existence of Complete Problems
We now show that there exists a problem (albeit an artiﬁcial one) complete for dist NP. Let the
inputs have the form

M, x, 1t, 1l
, where M is an encoding of a Turing machine and 1t is a sequence
of t ones. Then we deﬁne the following “universal” problem U.
• Decide whether there exists a string y such that |y| ≤l and M (x, y) accepts in at most t
steps.
That U is NP-complete follows directly from the deﬁnition. Recall the deﬁnition of NP: we
say that L ∈NP if there exists a machine M running in t = poly (|x|) steps such that x ∈L iﬀ
there exists a y with y = poly (|x|) such that M (x, y) accepts. Thus, to reduce L to U we need
only map x onto R (x) =

M, x, 1t, 1l
where t and l are suﬃciently large bounds.
15.4
Polynomial-Time Samplability
Definition 15.12 (Samplable distributions)
We say that a distribution µ is polynomial-time samplable if there exists a probabilistic algorithm
A, taking no input, that outputs x with probability µ′ (x) and runs in poly (|x|) time.
Any polynomial-time computable distribution is also polynomial-time samplable, provided that
for all x,
µ′ (x) ≥2−poly(|x|) or µ′ (x) = 0.
(13)
For a polynomial-time computable µ satisfying the above property, we can indeed construct a
sampler A that ﬁrst chooses a real number r uniformly at random from [0, 1], to poly (|x|) bits of
precision, and then uses binary search to ﬁnd the ﬁrst x such that µ (x) ≥r.
On the other hand, under reasonable assumptions, there are eﬃciently samplable distributios
µ that are not eﬃciently computable.
In addition to dist NP, we can look at the class
⟨NP, P-samplable⟩= {⟨L, µ⟩: L ∈NP, µ polynomial-time samplable} .
(14)
A result due to Impagliazzo and Levin states that if ⟨L, µ⟩is dist NP-complete, then ⟨L, µ⟩is
also complete for the class ⟨NP, P-samplable⟩.
Web draft 2007-01-08 21:59

DRAFT
15.4. POLYNOMIAL-TIME SAMPLABILITY
p15.11 (279)
This means that the completeness result established in the previous section extends to the class
of NP problems with samplable distributions.
Exercises
§1 Describe an algorithm that decides 3-colorability on almost all graphs in linear expected time.
Hint: A 3-colorable graph better not contain a complete graph
on 4 vertices.
§2 Describe an algorithm that decides CLIQUE on almost all graphs in n2 log n time.
Hint: The chance that a random graph has a clique of size more
than k is at most
 n
k

2−k2/2.
§3 Show that if a distribution is polynomial-time computable, then it is polynomial-time sam-
pleable.
Hint: Binary search.
§4 Show that if P#P ̸= P then there is a polynomial time samplable distribution that is not
polynomial time computable.
§5 Show that the function g deﬁned in Lemma 15.9 (Peak Elimination) is eﬃciently invertible
in the following sense: if y = g(x), then given y we can reconstruct x in |x|O(1) time.
§6 Show that if one-way functions exist, then dist NP ̸⊆dist P.
Chapter notes and history
Suppose P ̸= NP and yet dist NP ⊆dist P. This would mean that generating hard instances of NP
problems requires superpolynomial computations. Cryptography is thus impractical. Also, it seems to imply
that everyday instances of NP-complete problems would also be easily solvable. Such instances arise from
the world around us —we want to understand an image, or removing the obstacles in the path of a robot—
and it is hard to imagine how the inanimate world would do the huge amounts of computation necessary to
generate a hard instance.
Web draft 2007-01-08 21:59

DRAFT
p15.12 (280)
15.4. POLYNOMIAL-TIME SAMPLABILITY
Web draft 2007-01-08 21:59

DRAFT
Chapter 16
Derandomization, Expanders and
Extractors
“God does not play dice with the universe”
Albert Einstein
“Anyone who considers arithmetical methods of producing random digits is, of
course, in a state of sin.”
John von Neumann, quoted by Knuth 1981
“How hard could it be to ﬁnd hay in a haystack?”
Howard Karloﬀ
The concept of a randomized algorithm, though widespread, has both a philosophical and a
practical diﬃculty associated with it.
The philosophical diﬃculty is best represented by Einstein’s famous quote above. Do random
events (such as the unbiased coin ﬂip assumed in our deﬁnition of a randomized turing machine)
truly exist in the world, or is the world deterministic?
The practical diﬃculty has to do with
actually generating random bits, assuming they exist.
A randomized algorithm running on a
modern computer could need billions of random bits each second. Even if the world contains some
randomness —say, the ups and downs of the stock market — it may not have enough randomness to
provide billions of uncorrelated random bits every second in the tiny space inside a microprocessor.
Current computing environments rely on shortcuts such as taking a small “fairly random looking”
bit sequence—e.g., interval between the programmer’s keystrokes measured in microseconds—and
applying a deterministic generator to turn them into a longer sequence of “sort of random looking”
bits. Some recent devices try to use quantum phenomena. But for all of them it is unclear how
random and uncorrelated those bits really are.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p16.1 (281)

DRAFT
p16.2 (282)
Such philosophical and practical diﬃculties look deterring; the philosophical aspect alone has
been on the philosophers’ table for centuries. The results in the current chapter may be viewed as
complexity theory’s contribution to these questions.
The ﬁrst contribution concerns the place of randomness in our world. We indicated in Chap-
ter 7 that randomization seems to help us design more eﬃcient algorithms. A surprising conclusion
in this chapter is this could be a mirage to some extent. If certain plausible complexity-theoretic
conjectures are true (e.g., that certain problems can not be solved by subexponential-sized circuits)
then every probabilistic algorithm can be simulated deterministically with only a polynomial slow-
down. In other words, randomized algorithms can be derandomized and BPP = P. Nisan and
Wigderson [NW94] named this research area Hardness versus Randomness since the existence of
hard problems is shown to imply derandomization. Section 16.3 shows that the converse is also
true to a certain extent: ability to derandomize implies circuit lowerbounds (thus, hardness) for
concrete problems. Thus the Hardness ↔Randomness connection is very real.
Is such a connection of any use at present, given that we have no idea how to prove circuit
lowerbounds? Actually, yes. Just as in cryptography, we can use conjectured hard problems in
the derandomization instead of provable hard problems, and end up with a win-win situation: if
the conjectured hard problem is truly hard then the derandomization will be successful; and if the
derandomization fails then it will lead us to an algorithm for the conjectured hard problem.
The second contribution of complexity theory concerns another practical question: how can we
run randomized algorithms given only an imperfect source of randomness? We show the existence
of randomness extractors: eﬃcient algorithms to extract (uncorrelated, unbiased) random bits
from any weakly random device.Their analysis is unconditional and uses no unproven assumptions.
Below, we will give a precise deﬁnition of the properties that such a weakly random device needs
to have.
We do not resolve the question of whether such weakly random devices exist; this is
presumably a subject for physics (or philosophy).
A central result in both areas is Nisan and Wigderson’s beautiful construction of a certain
pseudorandom generator. This generator is tailor-made for derandomization and has somewhat
diﬀerent properties than the secure pseudorandom generators we encountered in Chapter 10.
Another result in the chapter is a (unconditional) derandomization of randomized logspace
computations, albeit at the cost of some increase in the space requirement.
Example 16.1 (Polynomial identity testing)
One example for an algorithm that we would like to derandomize is the algorithm described in
Section 7.2.2 for testing if a given polynomial (represented in the form of an arithmetic zero) is
the identically zero polynomial. If p is an n-variable nonzero polynomial of total degree d over a
large enough ﬁnite ﬁeld F (|F| > 10d will do) then most of the vectors u ∈Fn will satisfy p(u) ̸= 0
(see Lemma A.25. Therefore, checking whether p ≡0 can be done by simply choosing a random
u ∈R Fn and applying p on u. In fact, it is easy to show that there exists a set of m2-vectors
u1, . . . , um2 such that for every such nonzero polynomial p that can be computed by a size m
arithmetic circuit, there exists an i ∈[m2] for which p(ui) ̸= 0.
This suggests a natural approach for a deterministic algorithm: show a deterministic algorithm
that for every m ∈N, runs in poly(m) time and outputs a set u1, . . . , um2 of vectors satisfying the
above property. This shouldn’t be too diﬃcult— after all the vast majority of the sets of vectors
Web draft 2007-01-08 21:59

DRAFT
16.1. PSEUDORANDOM GENERATORS AND DERANDOMIZATION
p16.3 (283)
have this property, so hard can it be to ﬁnd a single one? (Howard Karloﬀcalls this task “ﬁnding
a hay in a haystack”). Surprisingly this turns out to be quite hard: without using complexity
assumptions, we do not know how to obtain such a set, and in Section 16.3 we will see that in fact
such an algorithm will imply some nontrivial circuit lowerbounds.1
16.1
Pseudorandom Generators and Derandomization
The main tool in derandomization is a pseudorandom generator. This is a twist on the deﬁnition of
a secure pseudorandom generator we gave in Chapter 10, with the diﬀerence that here we consider
nonuniform distinguishers –in other words, circuits— and allow the generator to run in exponential
time.
Definition 16.2 (Pseudorandom generators)
Let R be a distribution over {0, 1}m, S ∈N and ϵ > 0.
We say that R is an
(S, ϵ)-pseudorandom distribution if for every circuit C of size at most S,
|Pr[C(R) = 1] −Pr[C(Um) = 1]| < ϵ
where Um denotes the uniform distribution over {0, 1}m.
If S : N →N is a polynomial-time computable monotone function (i.e., S(m) ≥S(n)
for m ≥n)2 then a function G : {0, 1}∗→{0, 1}∗is called an (S(ℓ)-pseudorandom
generator (see Figure 16.1) if:
• For every z ∈{0, 1}ℓ, |G(z)| = S(ℓ) and G(z) can be computed in time 2cℓfor
some constant c. We call the input z the seed of the pseudorandom generator.
• For every ℓ∈N, G(Uℓ) is an (S(ℓ)3, 1/10)-pseudorandom distribution.
Remark 16.3
The choices of the constant 3 and 1/10 in the deﬁnition of an S(ℓ)-pseudorandom generator are
arbitrary and made for convenience.
The relation between pseudorandom generators and simulating probabilistic algorithm is straight-
forward:
1Perhaps it should not be so surprising that “ﬁnding a hay in a haystack” is so hard. After all, the hardest open
problems of complexity— ﬁnding explicit functions with high circuit complexity— are of this form, since the vast
majority of the functions from {0, 1}n to {0, 1} have exponential circuit complexity.
2We place these easily satisﬁable requirements on the function S to avoid weird cases such as generators whose
output length is not computable or generators whose output shrinks as the input grows.
Web draft 2007-01-08 21:59

DRAFT
p16.4 (284)
16.1. PSEUDORANDOM GENERATORS AND DERANDOMIZATION
?
C
$$$
$$$$$$$$$$$$$$$$
G
m
m
Figure 16.1: A pseudorandom generator G maps a short uniformly chosen seed z ∈R {0, 1}ℓinto a longer output
G(z) ∈{0, 1}m that is indistinguishable from the uniform distribution Um by any small circuit C.
Lemma 16.4
Suppose that there exists an S(ℓ)-pseudorandom generator for some polynomial-time computable
monotone S : N →N. Then for every polynomial-time computable function ℓ: N →N, BPTIME(S(ℓ(n))) ⊆
DTIME(2cℓ(n)) for some constant c.
Proof: A language L is in BPTIME(S(ℓ(n))) if there is an algorithm A that on input x ∈{0, 1}n
runs in time cS(ℓ(n)) for some constant c, and satisﬁes
Pr
r∈R{0,1}m[A(x, r) = L(x)] ≥2
3
where m ≤S(ℓ(n)) and we deﬁne L(x) = 1 if x ∈L and L(x) = 0 otherwise.
The main idea is that if we replace the truly random string r with the string G(z) produced
by picking a random z ∈{0, 1}ℓ(n), then an algorithm like A that runs in only S(ℓ) time cannot
detect this switch most of the time, and so the probability 2/3 in the previous expression does not
drop below 2/3−0.1. Thus to derandomize A, we do not need to enumerate over all r; it suﬃces to
enumerates over all z ∈{0, 1}ℓ(n) and check how many of them make A accept. This derandomized
algorithm runs in exp(ℓ(n)) time instead of the trivial 2m time.
Now we make this formal. Our deterministic algorithm B will on input x ∈{0, 1}n, go over all
z ∈{0, 1}ℓ(n), compute A(x, G(z)) and output the majority answer. Note this takes 2O(ℓ(n)) time.
We claim that for n suﬃciently large, the fraction of z’s such that A(x, G(z)) = L(x) is at least
2
3 −0.1. (This suﬃces to prove that L ∈DTIME(2cℓ(n)) as we can “hardwire” into the algorithm
the correct answer for ﬁnitely many inputs.)
Suppose this is false and there exists an inﬁnite sequence of x’s for which Pr[A(x, G(z)) =
L(x) < 2/3 −0.1. Then we would get a distinguisher for the pseudorandom generator —just use
the Cook-Levin transformation to construct a circuit that computes the function z 7→A(x, G(z)),
where x is hardwired into the circuit.
This circuit has size O(S(ℓ(n)))2 which is smaller than
S(ℓ(n))3 for suﬃciently large n. ■
Remark 16.5
The proof shows why it is OK to allow the pseudorandom generator in Deﬁnition 16.2 to run in
time exponential in its seed length. The derandomized algorithm enumerates over all possible seeds
Web draft 2007-01-08 21:59

DRAFT
16.1. PSEUDORANDOM GENERATORS AND DERANDOMIZATION
p16.5 (285)
of length ℓ, and thus would take exponential time (in ℓ) even if the generator itself were to run in
less than exponential time.
Notice, these generators have to fool distinguishers that run for less time than they do. By
contrast, the deﬁnition of secure pseudorandom generators (Deﬁnition 10.11 in Chapter 10) re-
quired the generator to run in polynomial time, and yet have the ability to fool distinguishers that
have super-polynomial running time. This diﬀerence in these deﬁnitions stems from the intended
usage. In the cryptographic setting the generator is used by honest users and the distinguisher is
the adversary attacking the system — and it is reasonable to assume the attacker can invest more
computational resources than those needed for normal/honest use of the system. In derandom-
ization, generator is used by the derandomized algorithm, the ”distinguisher” is the probabilistic
algorithm that is being derandomized, and it is reasonable to allow the derandomized algorithm
higher running time than the original probabilistic algorithm.
Of course, allowing the generator to run in exponential time as in this chapter potentially makes
it easier to prove their existence compared with secure pseudorandom generators, and this indeed
appears to be the case. (Note that if we place no upperbounds on the generator’s eﬃciency, we
could prove the existence of generators unconditionally as shown in Exercise 2, but these do not
suﬃce for derandomization.)
We will construct pseudorandom generators based on complexity assumptions, using quan-
titatively stronger assumptions to obtain quantitatively stronger pseudorandom generators (i.e.,
S(ℓ)-pseudorandom generators for larger functions S). The strongest (though still reasonable) as-
sumption will yield a 2Ω(ℓ)-pseudorandom generator, thus implying that BPP = P. These are
described in the following easy corollaries of the Lemma that are left as Exercise 1.
Corollary 16.6
1. If there exists a 2ϵℓ-pseudorandom generator for some constant ϵ > 0 then BPP = P.
2. If there exists a 2ℓϵ-pseudorandom generator for some constant ϵ > 0 then BPP ⊆QuasiP =
DTIME(2polylog(n)).
3. If there exists an S(ℓ)-pseudorandom generator for some super-polynomial function S (i.e.,
S(ℓ) = ℓω(1)) then BPP ⊆SUBEXP = ∩ϵ>0DTIME(2nϵ).
16.1.1
Hardness and Derandomization
We construct pseudorandom generators under the assumptions that certain explicit functions are
hard. In this chapter we use assumptions about average-case hardness, while in the next chapter
we will be able to construct pseudorandom generators assuming only worst-case hardness. Both
worst-case and average-case hardness refers to the size of the minimum Boolean circuit computing
the function:
Web draft 2007-01-08 21:59

DRAFT
p16.6 (286)
16.1. PSEUDORANDOM GENERATORS AND DERANDOMIZATION
Definition 16.7 (Hardness)
Let f : {0, 1}∗→{0, 1} be a Boolean function.
The worst-case hardness of f,
denoted Hwrs(f), is a function from N to N that maps every n ∈N to the largest
number S such that every Boolean circuit of size at most S fails to compute f on
some input in {0, 1}n.
The average-case hardness of f, denoted Havg(f), is a function from N to N that maps
every n ∈N, to the largest number S such that Prx∈R{0,1}n[C(x) = f(x)] < 1
2 + 1
S
for every Boolean circuit C on n inputs with size at most S.
Note that for every function f : {0, 1}∗→{0, 1} and n ∈N, Havg(f)(n) ≤Hwrs(f)(n) ≤n2n.
Remark 16.8
This deﬁnition of average-case hardness is tailored to the application of derandomization, and in
particular only deals with the uniform distribution over the inputs. See Chapter 15 for a more
general treatment of average-case complexity. We will also sometimes apply the notions of worst-
case and average-case to ﬁnite functions from {0, 1}n to {0, 1}, where Hwrs(f) and Havg(f) are deﬁned
in the natural way. (E.g., if f : {0, 1}n →{0, 1} then Hwrs(f) is the largest number S for which every
Boolean circuit of size at most S fails to compute f on some input in {0, 1}n.)
Example 16.9
Here are some examples of functions and their conjectured or proven hardness:
1. If f is a random function (i.e., for every x ∈{0, 1}∗we choose f(x) using an independent
unbiased coin) then with high probability, both the worst-case and average-case hardness of
f are exponential (see Exercise 3). In particular, with probability tending to 1 with n, both
Hwrs(f)(n) and Havg(f)(n) exceed 20.99n. We will often use the shorthand Hwrs(f), Havg(f) ≥20.99n
for such expressions.
2. If f ∈BPP then, since BPP ⊆P/poly, both Hwrs(f) and Havg(f) are bounded by some
polynomial.
3. It seems reasonable to believe that 3SAT has exponential worst-case hardness; that is, Hwrs(3SAT) ≥
2Ω(n). It is even more believable that NP ⊈P/poly, which implies that Hwrs(3SAT) is super-
polynomial. The average case complexity of 3SAT is unclear, and in any case dependent upon
the way we choose to represent formulas as strings.
4. If we trust the security of current cryptosystems, then we do believe that NP contains func-
tions that are hard on the average. If g is a one-way permutation that cannot be inverted with
polynomial probability by polynomial-sized circuits, then by Theorem 10.14, the function f
that maps the pair x, r ∈{0, 1}n to g−1(x) ⊙r has super-polynomial average-case hardness:
Havg(f) ≥nω(1). (Where x ⊙r = Pn
i=1 xiri (mod 2).) More generally there is a polynomial
relationship between the size of the minimal circuit that inverts g (on the average) and the
average-case hardness of f.
Web draft 2007-01-08 21:59

DRAFT
16.2. PROOF OF THEOREM ??: NISAN-WIGDERSON CONSTRUCTION
p16.7 (287)
The main theorem of this section uses hard-on-the average functions to construct pseudorandom
generators:
Theorem 16.10 (Consequences of NW Generator)
For every polynomial-time computable monotone S : N →N, if there exists a
constant c and function f ∈DTIME(2cn) such that Havg(f) ≥S(n) then there
exists a constant ϵ > 0 such that an S(ϵℓ)ϵ-pseudorandom generator exists.
In
particular, the following corollaries hold:
1. If there exists f ∈E = DTIME(2O(n)) and ϵ > 0 such that Havg(f) ≥2ϵn then
BPP = P.
2. If there exists f ∈E = DTIME(2O(n)) and ϵ > 0 such that Havg(f) ≥2nϵ then
BPP ⊆QuasiP.
3. If there exists f ∈E = DTIME(2O(n)) such that Havg(f) ≥nω(1) then BPP ⊆
SUBEXP.
Remark 16.11
We can replace E with EXP = DTIME(2poly(n)) in Corollaries 2 and 3 above. Indeed, for every
f ∈DTIME(2nc), the function g that on input x ∈{0, 1}∗outputs the f applies to the ﬁrst
|x|1/c bits of x is in DTIME(2n) and satisﬁes Havg(g)(n) ≥Havg(f)(n1/c). Therefore, if there exists
f ∈EXP with Havg(f) ≥2nϵ then there there exists a constant ϵ′ > 0 and a function g ∈E
with Havg(g) ≥2nϵ′
, and so we can replace E with EXP in Corollary 2. A similar observation
holds for Corollary 3. Note that EXP contains many classes we believe to have hard problems,
such as NP,PSPACE,⊕P and more, which is why we believe it does contain hard-on-the-average
functions. In the next chapter we will give even stronger evidence to this conjecture, by showing it
is implied by the assumption that EXP contains hard-in-the-worst-case functions.
Remark 16.12
The original paper of Nisan and Wigderson [NW94] did not prove Theorem 16.10 as stated above.
It was proven in a sequence of works [?]. Nisan and Wigderson only proved that under the same
assumptions there exists an S′(ℓ)-pseudorandom generator, where S′(ℓ) = S

ϵ
√
ℓlog(S(ϵ
√
ℓ)
ϵ
for
some ϵ > 0. Note that this is still suﬃcient to derive all three corollaries above. It is this weaker
version we prove in this book.
16.2
Proof of Theorem 16.10: Nisan-Wigderson Construction
How can we use a hard function to construct a pseudorandom generator?
Web draft 2007-01-08 21:59

DRAFT
p16.8 (288)
16.2. PROOF OF THEOREM ??: NISAN-WIGDERSON CONSTRUCTION
16.2.1
Warmup: two toy examples
For starters, we demonstrate this by considering the “toy example” of a pseudorandom generator
whose output is only one bit longer than its input. Then we show how to extend by two bits. Of
course, neither suﬃces to prove Theorem 16.10 but they do give insight to the connection between
hardness and randomness.
Extending the input by one bit using Yao’s Theorem.
The following Lemma uses a hard function to construct such a “toy” generator:
Lemma 16.13 (One-bit generator)
Suppose that there exist f ∈E with Havg(f) ≥n4.
Then, there exists an S(ℓ)-pseudorandom
generator G for S(ℓ) = ℓ+ 1.
Proof: The generator G will be very simple: for every z ∈{0, 1}ℓ, we set
G(z) = z ◦f(z)
(where ◦denotes concatenation). G clearly satisﬁes the output length and eﬃciency requirements
of an (ℓ+1)-pseudorandom generator. To prove that its output is 1/10-pseudorandom we use Yao’s
Theorem from Chapter 10 showing that pseudorandomness is implied by unpredictiability:3
Theorem 16.14 (Theorem 10.12, restated)
Let Y be a distribution over {0, 1}m. Suppose that there exist S > 10n,ϵ > 0 such that for every
circuit C of size at most 2S and i ∈[m],
Pr
r∈RY [C(r1, . . . , ri−1) = ri] ≤1
2 + ϵ
m
Then Y is (S, ϵ)-pseudorandom.
Using Theorem 16.14 it is enough to show that there does not exist a circuit C of size 2(ℓ+1)3 <
ℓ4 and a number i ∈[ℓ+ 1] such that
Pr
r=G(Uℓ)[C(r1, . . . , ri−1) = ri] > 1
2 +
1
20(ℓ+1).
(1)
However, for every i ≤ℓ, the ith bit of G(z) is completely uniform and independent from the ﬁrst
i −1 bits, and hence cannot be predicted with probability larger than 1/2 by a circuit of any size.
For i = ℓ+ 1, Equation (1) becomes,
Pr
z∈R{0,1}ℓ[C(z) = f(z)] > 1
2 +
1
20(ℓ+ 1) > 1
2 + 1
ℓ4 ,
which cannot hold under the assumption that Havg(f) ≥n4. ■
3Although this theorem was stated and proved in Chapter 10 for the case of uniform Turing machines, the proof
easily extends to the case of circuits.
Web draft 2007-01-08 21:59

DRAFT
16.2. PROOF OF THEOREM ??: NISAN-WIGDERSON CONSTRUCTION
p16.9 (289)
Extending the input by two bits using the averaging principle.
We now continue to progress in “baby steps” and consider the next natural toy problem: construct-
ing a pseudorandom generator that extends its input by two bits. This is obtained in the following
Lemma:
Lemma 16.15 (Two-bit generator)
Suppose that there exists f ∈E with Havg(f) ≥n4. Then, there exists an (ℓ+2)-pseudorandom
generator G.
Proof: The construction is again very natural: for every z ∈{0, 1}ℓ, we set
G(z) = z1 · · · zℓ/2 ◦f(z1, . . . , zℓ/2) ◦zℓ/2+1 · · · zℓ◦f(zℓ/2+1, . . . , zℓ).
Again, the eﬃciency and output length requirements are clearly satisﬁed.
To show G(Uℓ) is 1/10-pseudorandom, we again use Theorem 16.14, and so need to prove that
there does not exists a circuit C of size 2(ℓ+ 1)3 and i ∈[ℓ+ 2] such that
Pr
r=G(Uℓ)[C(r1, . . . , ri−1) = ri] > 1
2 +
1
20(ℓ+ 2).
(2)
Once again, (2) cannot occur for those indices i in which the ith output of G(z) is truly random,
and so the only two cases we need to consider are i = ℓ/2 + 1 and i = ℓ+ 2. Equation (2) cannot
hold for i = ℓ/2 + 1 for the same reason as in Lemma 16.13. For i = ℓ+ 2, Equation (2) becomes:
Pr
r,r′∈R{0,1}ℓ/2[C(r ◦f(r) ◦r′) = f(r′)] > 1
2 +
1
20(ℓ+ 2)
(3)
This may seem somewhat problematic to analyze since the input to C contains the bit f(r),
which C could not compute on its own (as f is a hard function). Couldn’t it be that the input
f(r) helps C in predicting the bit f(r′)? The answer is NO, and the reason is that r′ and r are
independent. Formally, we use the following principle (see Section A.3.2 in the appendix):
The Averaging Principle: If A is some event depending on two independent random
variables X, Y , then there exists some x in the range of X such that
Pr
Y [A(x, Y ) ≥Pr
X,Y [A(X, Y )]
Applying this principle here, if (3) holds then there exists a string r ∈{0, 1}ℓ/2 such that
Pr
r′∈R{0,1}ℓ/2[C(r, f(r), r′) = f(r′)] > 1
2 +
1
20(ℓ+ 2) .
(Note that this probability is now only over the choice of r′.) If this is the case, we can “hardwire”
the ℓ/2+1 bits r ◦f(r) to the circuit C and obtain a circuit D of size at most (ℓ+2)3 +2ℓ< (ℓ/2)4
such that
Pr
r′∈R{0,1}ℓ/2[D(r′) = f(r′)] > 1
2 +
1
20(ℓ+ 2) ,
contradicting the hardness of f. ■
Web draft 2007-01-08 21:59

DRAFT
p16.10 (290)
16.2. PROOF OF THEOREM ??: NISAN-WIGDERSON CONSTRUCTION
Beyond two bits:
A generator that extends the output by two bits is still useless for our goals. We can generalize the
proof Lemma 16.15 to obtain a generator G that extends the output by k bits setting
G(z1, . . . , zℓ) = z1 ◦f(z1) ◦z2 ◦f(z2) · · · zk ◦f(zk) ,
(4)
where zi is the ith block of ℓ/k bits in z. However, no matter how big we set k and no matter how
hard the function f is, we cannot get a generator that expands its input by a multiplicative factor
larger than two. Note that to prove Theorem 16.10 we need a generator that, depending on the
hardness we assume, has output that can be exponentially larger than the input! Clearly, we need
a new idea.
16.2.2
The NW Construction
The new idea is still inspired by the construction of (4), but instead of taking z1, . . . , zk to be
independently chosen strings (or equivalently, disjoint pieces of the input z), we take them to be
partly dependent by using combinatorial designs. Doing this will allow us to take k so large that
we can drop the actual inputs from the generator’s output and use only f(z1) ◦f(z2) · · · ◦f(zk).
The proof of correctness is similar to the above toy examples and uses Yao’s technique, except the
ﬁxing of the input bits has to be done more carefully because of dependence among the strings.
First, some notation. For a string z ∈{0, 1}ℓand subset I ⊆[ℓ], we deﬁne z↾I to be |I|-length
string that is the projection of z to the coordinates in I. For example, z↾[1..i] is the ﬁrst i bits of z.
Definition 16.16 (NW Generator)
If I = {I1, . . . , Im} is a family of subsets of [ℓ] with each |Ij| = l and f : {0, 1}n →
{0, 1} is any function then the (I, f)-NW generator (see Figure 16.2) is the function
NWf
I : {0, 1}ℓ→{0, 1}m that maps any z ∈{0, 1}ℓto
NWf
I(z) = f(z↾I1) ◦f(z↾I2) · · · ◦f(z↾Im)
(5)
Ij
Ij+1
f
f
Figure 16.2:
The NW generator, given a set system I = {I1, . . . , Im} of size n subsets of [ℓ] and a function
f : {0, 1}n →{0, 1} maps a string z ∈{0, 1}ℓto the output f(z↾I1), . . . , f(z↾Im).
Note that these sets are not
necessarily disjoint (although we will see their intersections need to be small).
Web draft 2007-01-08 21:59

DRAFT
16.2. PROOF OF THEOREM ??: NISAN-WIGDERSON CONSTRUCTION
p16.11 (291)
Conditions on the set systems and function.
We will see that in order for the generator to produce pseudorandom outputs, function f must
display some hardness, and the family of subsets must come from an eﬃciently constructible com-
binatorial design.
Definition 16.17 (Combinatorial designs)
If d, n, ℓ∈N are numbers with ℓ> n > d then a family I = {I1, . . . , Im} of subsets of [ℓ] is an
(ℓ, n, d)-design if |Ij| = n for every j and |Ij ∩Ik| ≤d for every j ̸= k.
The next lemma yields eﬃcient constructions of these designs and is proved later.
Lemma 16.18 (Construction of designs)
There is an algorithm A such that on input ℓ, d, n ∈N where n > d and ℓ> 10n2/d, runs for 2O(ℓ)
steps and outputs an (ℓ, n, d)-design I containing 2d/10 subsets of [ℓ].
The next lemma shows that if f is a hard function and I is a design with suﬃciently good
parameters, than NWf
I(Uℓ) is indeed a pseudorandom distribution:
Lemma 16.19 (Pseudorandomness using the NW generator)
If I is an (ℓ, n, d)-design with |I| = 2d/10 and f : {0, 1}n →{0, 1} a function satisfying 2d <
p
Havg(f)(n), then the distribution NWf
I(Uℓ) is a (Havg(f)(n)/10, 1/10)-pseudorandom distribution.
Proof: Let S denote Havg(f)(n). By Yao’s Theorem, we need to prove that for every i ∈[2d/10]
there does not exist an S/2-sized circuit C such that
Pr
Z∼Uℓ
R=NWf
I(Z)
[C(R1, . . . , Ri−1) = Ri] ≥1
2 +
1
10 · 2d/10 .
(6)
For contradiction’s sake, assume that (6) holds for some circuit C and some i. Plugging in the
deﬁnition of NWf
I, Equation (6) becomes:
Pr
Z∼Uℓ[C(f(Z↾I1), · · · , f(Z↾Ii−1)) = f(Z↾Ii)] ≥1
2 +
1
10 · 2d/10 .
(7)
Letting Z1 and Z2 denote the two independent variables corresponding to the coordinates of Z
in Ii and [ℓ] \ Ii respectively, Equation (7) becomes:
Pr
Z1∼Un
Z2∼Uℓ−n
[C(f1(Z1, Z2), . . . , fi−1(Z1, Z2)) = f(Z1)] ≥1
2 +
1
10 · 2d/10 ,
(8)
where for every j ∈[2d/10], fj applies f to the coordinates of Z1 corresponding to Ij ∩Ii and the
coordinates of Z2 corresponding to Ij \ Ii. By the averaging principle, if (8) holds then there exists
a string z2 ∈{0, 1}ℓ−n such that
Pr
Z1∼Un[C(f1(Z1, z2), . . . , fi−1(Z1, z2)) = f(Z1)] ≥1
2 +
1
10 · 2d/10 .
(9)
Web draft 2007-01-08 21:59

DRAFT
p16.12 (292)
16.2. PROOF OF THEOREM ??: NISAN-WIGDERSON CONSTRUCTION
We may now appear to be in some trouble, since all of fj(Z1, z2) for j ≤i −1 do depend upon
Z1, and the fear is that if they together contain enough information about Z1 then a circuit could
potentially predict fi(Z1) after looking at all of them. To prove that this fear is baseless we use the
fact that the circuit C is small and f is a very hard function.
Since |Ij ∩Ii| ≤d for j ̸= i, the function Z1 7→fj(Z1, z2) depends at most d coordinates of z1
and hence can be computed by a d2d-sized circuit. (Recall that z2 is ﬁxed.) Thus if if (8) holds
then there exists a circuit B of size 2d/10 · d2d + S/2 < S such that
Pr
Z1∼Un[B(Z1) = f(Z1)] ≥1
2 +
1
10 · 2d/10 > 1
2 + 1
S .
(10)
But this contradicts the fact that Havg(f)(n) = S. ■
Remark 16.20 (Black-box proof)
Lemma 16.19 shows that if NWf
I(Uℓ) is distinguishable from the uniform distribution U2d/10 by
some circuit D, then there exists a circuit B (of size polynomial in the size of D and in 2d) that
computes the function f with probability noticeably larger than 1/2.
The construction of this
circuit B actually uses the circuit D as a black-box, invoking it on some chosen inputs.
This
property of the NW generator (and other constructions of pseudorandom generators) turned out
to be useful in several settings.
In particular, Exercise 5 uses it to show that under plausible
complexity assumptions, the complexity class AM (containing all languages with a constant round
interactive proof, see Chapter 8) is equal to NP. We will also use this property in the construction
of randomness extractors based on pseudorandom generators.
Putting it all together: Proof of Theorem 16.10 from Lemmas 16.18 and 16.19
As noted in Remark 16.12, we do not prove here Theorem 16.10 as stated but only the weaker state-
ment, that given f ∈E and S : N →N with Havg(f) ≥S, we can construct an S′(ℓ)-pseudorandom
generator, where S′(ℓ) = S

ϵ
√
ℓlog(S(ϵ
√
ℓ)
ϵ
for some ϵ > 0.
For such a function f, we denote our pseudorandom generator by NW f. Given input z ∈{0, 1}ℓ,
the generator NWf operates as follows:
• Set n to be the largest number such that ℓ> 100n2/ log S(n). Set d = log S(n)/10. Since
S(n) < 2n, we can assume that ℓ≤300n2/ log S(n).
• Run the algorithm of Lemma 16.18 to obtain an (ℓ, n, d)-design I = {I1, . . . , I2d/5}.
• Output the ﬁrst S(n)1/40 bits of NWf
I(z).
Clearly, NWf(z) runs in 2O(ℓ) time.
Moreover, since 2d ≤S(n)1/10, Lemma 16.19 implies
that the distribution NWf(Uℓ) is (S(n)/10, 1/10)-pseudorandom.
Since n ≥
√
ℓlog S(n)/300 ≥
√
ℓlog S(
√
ℓ
300)/300 (with the last inequality following from the fact that S is monotone), this con-
cludes the proof of Theorem 16.10. ■
Web draft 2007-01-08 21:59

DRAFT
16.3. DERANDOMIZATION REQUIRES CIRCUIT LOWERBOUNDS
p16.13 (293)
Construction of combinatorial designs.
All that is left to complete the proof is to show the construction of combinatorial designs with the
required parameters:
Proof of Lemma 16.18 (construction of combinatorial designs): On inputs ℓ, d, n with
ℓ> 10n2/d, our Algorithm A will construct an (ℓ, n, d)-design I with 2d/10 sets using the simple
greedy strategy:
Start with I = ∅and after constructing I = {I1, . . . , Im} for m < 2d/10, search all
subsets of [ℓ] and add to I the ﬁrst n-sized set I satisfying |I ∩Ij| ≤d for every j ∈[m].
We denote this latter condition by (*).
Clearly, A runs in poly(m)2ℓ= 2O(ℓ) time and so we only need to prove it never gets stuck.
In other words, it suﬃces to show that if ℓ= 10n2/d and {I1, . . . , Im} is a collection of n-sized
subsets of [ℓ] for m < 2d/10, then there exists an n-sized subset I ⊆[ℓ] satisfying (*). We do so by
showing that if we pick I at random by choosing independently every element x ∈[ℓ] to be in I
with probability 2n/ℓthen:
Pr[|I| ≥n] ≥0.9
(11)
Pr[|I ∩Ij| ≥d] ≤0.5 · 2−d/10
(∀j ∈[m])
(12)
Because the expected size of I is 2n, while the expected size of the intersection I ∩Ij is
2n2/ℓ< d/5, both (12) and (11) follow from the Chernoﬀbound. Yet together these two conditions
imply that with probability at least 0.4, the set I will simultaneously satisfy (*) and have size at
least n. Since we can always remove elements from I without damaging (*), this completes the
proof. ■
16.3
Derandomization requires circuit lowerbounds
We saw in Section 16.2 that if we can prove certain strong circuit lowerbounds, then we can partially
(or fully) derandomize BPP. Now we prove a result in the reverse direction: derandomizing BPP
requires proving circuit lowerbounds. Depending upon whether you are an optimist or a pessimist,
you can view this either as evidence that derandomizing BPP is diﬃcult, or, as a reason to double
our eﬀorts to derandomize BPP.
We say that a function is in AlgP/poly if it can be computed by a polynomial size arithmetic
circuit whose gates are labeled by +, −, × and ÷, which are operations over some underlying ﬁeld
or ring. We let perm denote the problem of computing the permanent of matrices over the integers.
(The proof can be extended to permanent computations over ﬁnite ﬁelds of characteristic > 2.) We
prove the following result.
Theorem 16.21 ([?])
P = BPP ⇒NEXP ⊈P/poly or perm /∈AlgP/poly.
Web draft 2007-01-08 21:59

DRAFT
p16.14 (294)
16.3. DERANDOMIZATION REQUIRES CIRCUIT LOWERBOUNDS
Remark 16.22
It is possible to replace the “poly” in the conclusion perm /∈AlgP/poly with a subexponential
function by appropriately modifying Lemma 16.25. It is open whether the conclusion NEXP ⊈
P/poly can be similarly strengthened.
In fact, we will prove the following stronger theorem. Recall the Polynomial Identity Testing
(ZEROP) problem in which the input consists of a polynomial represented by an arithmetic circuit
computing it (see Section 7.2.2 and Example 16.1), and we have to decide if it is the identically
zero polynomial. This problem is in coRP ⊆BPP and we will show that if it is in P then the
conclusions of Theorem 16.21 hold:
Theorem 16.23 (Derandomization implies lower bounds)
If ZEROP ∈P then either NEXP ⊈P/poly or perm /∈AlgP/poly.
The proof relies upon many results described earlier in the book.4 Recall that MA is the class
of languages that can be proven by a one round interactive proof between two players Arthur and
Merlin (see Deﬁnition 8.7). Merlin is an all-powerful prover and Arthur is a polynomial-time veriﬁer
that can ﬂip random coins. That is, given an input x, Merlin ﬁrst sends Arthur a “proof” y. Then
Arthur with y in hand ﬂips some coins and decides whether or not to accept x. For this to be an
MA protocol, Merlin must convince Arthur to accept strings in L with probability one while at the
same time Arthur must not be fooled into accepting strings not in L except with probability smaller
than 1/2. We will use the following result regarding MA:
Lemma 16.24 ([BFL91],[BFNW93])
EXP ⊆P/poly ⇒EXP = MA.
Proof: Suppose EXP ⊆P/poly. By the Karp-Lipton theorem (Theorem 6.14), in this case EXP
collapses to the second level Σp
2 of the polynomial hierarchy. Hence Σp
2 = PH = PSPACE =
IP = EXP ⊆P/poly. Thus every L ∈EXP has an interactive proof, and furtheremore, since
EXP = PSPACE, we can just the use the interactive proof for TQBF, for which the prover is a
PSPACE machine. Hence the prover can be replaced by a polynomial size circuit family Cn. Now
we see that the interactive proof can actually be carried out in 2 rounds, with Merlin going ﬁrst.
Given an input x of length n, Merlin gives Arthur a polynomial size circuit C, which is supposed to
be the Cn for L. Then Arthur runs the interactive proof for L, using C as the prover. Note that if
the input is not in the language, then no prover has a decent chance of convincing the veriﬁer, so
this is true also for prover described by C. Thus we have described an MA protocol for L implying
that EXP ⊆MA and hence that EXP = MA. ■
Our next ingredient for the proof of Theorem 16.23 is the following lemma:
Lemma 16.25
If ZEROP ∈P, and perm ∈AlgP/poly. Then Pperm ⊆NP.
4This is a good example of “third generation” complexity results that use a clever combination of both “classical”
results from the 60’s and 70’s and newer results from the 1990’s.
Web draft 2007-01-08 21:59

DRAFT
16.3. DERANDOMIZATION REQUIRES CIRCUIT LOWERBOUNDS
p16.15 (295)
Proof: Suppose perm has algebraic circuits of size nc, and that ZEROP has a polynomial-time
algorithm. Let L be a language that is decided by an nd-time TM M using queries to a perm-
oracle. We construct an NP machine N for L.
Suppose x is an input of size n. Clearly, M’s computation on x makes queries to perm of size
at most m = nd. So N will use nondeterminism as follows: it guesses a sequence of m algebraic
circuits C1, C2, . . . , Cm where Ci has size ic. The hope is that Ci solves perm on i × i matrices,
and N will verify this in poly(m) time. The veriﬁcation starts by verifying C1, which is trivial.
Inductively, having veriﬁed the correctness of C1, . . . , Ct−1, one can verify that Ct is correct using
downward self-reducibility, namely, that for a t × t matrix A,
perm(A) =
t
X
i=1
a1iperm(A1,i),
where A1,i is the (t−1)×(t−1) sub-matrix of A obtained by removing the 1st row and ith column
of A. Thus if circuit Ct−1 is known to be correct, then the correctness of Ct can be checked by
substituting Ct(A) for perm(A) and Ct−1(A1,i) for perm(A1,i): this yields an identity involving
algebraic circuits with t2 inputs which can be veriﬁed deterministically in poly(t) time using the
algorithm for ZEROP.
Proceeding this way N veriﬁes the correctness of C1, . . . , Cm and then
simulates Mperm on input x using these circuits. ■
The heart of the proof is the following lemma, which is interesting in its own right:
Lemma 16.26 ([?])
NEXP ⊆P/poly ⇒NEXP = EXP.
Proof: We prove the contrapositive. Suppose that NEXP ̸= EXP and let L ∈NEXP \ EXP.
Since L ∈NEXP there exists a constant c > 0 and a relation R such that
x ∈L ⇔∃y ∈{0, 1}2|x|c
s.t. R(x, y) holds ,
where we can test whether R(x, y) holds in time 2|x|c′
for some constant c′.
For every constant d > 0, let Md be the following machine: on input x ∈{0, 1}n enumerate
over all possible Boolean circuits C of size n100d that take nc inputs and have a single output. For
every such circuit let tt(C) be the 2nc-long string that corresponds to the truth table of the function
computed by C. If R(x, tt(C)) holds then halt and output 1. If this does not hold for any of the
circuits then output 0.
Since Md runs in time 2n101d+nc, under our assumption that L ̸∈EXP, for every d there exists
an inﬁnite sequence of inputs Xd = {xi}i∈N on which Md(xi) outputs 0 even though xi ∈L (note
that if Md(x) = 1 then x ∈L). This means that for every string x in the sequence Xd and every y
such that R(x, y) holds, the string y represents the truth table of a function on nc bits that cannot
be computed by circuits of size n100d, where n = |x|. Using the pseudorandom generator based on
worst-case assumptions (Theorem ??), we can use such a string y to obtain an ℓd-pseudorandom
generator.
Now, if NEXP ⊆P/poly then as noted above NEXP ⊆MA and hence every language in
NEXP has a proof system where Merlin proves that an n-bit string is in the language by sending
Web draft 2007-01-08 21:59

DRAFT
p16.16 (296)
16.4. EXPLICIT CONSTRUCTION OF EXPANDER GRAPHS
a proof which Arthur then veriﬁes using a probabilistic algorithm of at most nd steps. Yet, if n is
the input length of some string in the sequence Xd and we are given x ∈Xd with |x| = n, then we
can replace Arthur by non-deterministic poly(nd)2nc time algorithm that does not toss any coins:
Arthur will guess a string y such that R(x, y) holds and then use y as a function for a pseudorandom
generator to verify Merlin’s proof.
This means that there is a constant c > 0 such that every language in NEXP can be decided on
inﬁnitely many inputs by a non-deterministic algorithm that runs in poly(2nc)-time and uses n bits
of advice (consisting of the string x ∈Xd). Under the assumption that NEXP ⊆P/poly we can
replace the poly(2nc) running time with a circuit of size nc′ where c′ is a constant depending only
on c, and so get that there is a constant c′ such that every language in NEXP can be decided on
inﬁnitely many inputs by a circuit family of size n+nc′. Yet this can be ruled out using elementary
diagonalization. ■
Remark 16.27
It might seem that Lemma 16.26 should have an easier proof that goes along the proof that EXP ⊆
P/poly ⇒EXP = MA, but instead of using the interactive proof for TQBF uses the multi-prover
interactive proof system for NEXP. However, we do not know how to implement the provers’
strategies for this latter system in NEXP. (Intuitively, the problem arises from the fact that a
NEXP statement may have several certiﬁcates, and it is not clear how we can ensure all provers
use the same one.)
We now have all the ingredients for the proof of Theorem 16.23.
Proof of Theorem 16.23: For contradiction’s sake, assume that the following are all true:
ZEROP ∈P
(13)
NEXP ⊆P/poly,
(14)
perm ∈AlgP/poly.
(15)
Statement (14) together with Lemmas 16.24 and 16.26 imply that NEXP = EXP = MA. Now
recall that MA ⊆PH, and that by Toda’s Theorem (Theorem 9.11) PH ⊆P#P. Recall also that
by Valiant’s Theorem (Theorem 9.8) perm is #P-complete. Thus, under our assumptions
NEXP ⊆Pperm.
(16)
Since we assume that ZEROP ∈P, Lemma 16.25 together with statements (15) and (16) implies
that NEXP ⊆NP, contradicting the Nondeterministic Time Hierarchy Theorem (Theorem 3.3).
Thus the three statements at the beginning of the proof cannot be simultaneously true. ■
16.4
Explicit construction of expander graphs
Recall that an expander graph family is a family of graphs {Gn}n∈I such that for some constants
λ and d, for every n ∈I, the graph Gn has n-vertices, degree d and its second eigenvalue is at
most λ (see Section 7.B). A strongly explicit expander graph family is such a family where there
Web draft 2007-01-08 21:59

DRAFT
16.4. EXPLICIT CONSTRUCTION OF EXPANDER GRAPHS
p16.17 (297)
is an algorithm that given n and the index of a vertex v in Gn, outputs the list of v’s neighbors
in poly(log(n)) time. In this section we show a construction for such a family. Such construction
have found several applications in complexity theory and other areas of computer science (one such
application is the randomness eﬃcient error reduction procedure we saw in Chapter 7).
The main tools in our construction will be several types of graph products. A graph product is
an operation that takes two graphs G, G′ and outputs a graph H. Typically we’re interested in the
relation between properties of the graphs G, G′ to the properties of the resulting graph H. In this
section we will mainly be interested in three parameters: the number of vertices (denoted n), the
degree (denoted d), and the 2nd largest eigenvalue of the normalized adjacency matrix (denoted λ),
and study how diﬀerent products aﬀect these parameters. We then use these products to obtain a
construction of a strongly explicit expander graph family. In the next section we will use the same
products to show a deterministic logspace algorithm for undirected connectivity.
16.4.1
Rotation maps.
In addition to the adjacency matrix representation, we can also represent an n-vertex degree-d
graph G as a function ˆG from [n] × [d] to [n] that given a pair ⟨v, i⟩outputs u where the ith
neighbor of v in G. In fact, it will be convenient for us to have ˆG output an additional value j ∈[d]
where j is the index of v as a neighbor of u. Given this deﬁnition of ˆG it is clear that we can invert
it by applying it again, and so it is a permutation on [n] × [d]. We call ˆG the rotation map of G.
For starters, one may think of the case that ˆG(u, i) = (v, i) (i.e., v is the ith neighbor of u iﬀu is
the ith neighbor of v). In this case we can think of ˆG as operating only on the vertex. However,
we will need the more general notion of a rotation map later on.
We can describe a graph product in the language of graphs, adjacency matrices, or rotation
maps. Whenever you see the description of a product in one of this forms (e.g., as a way to map
two graphs into one), it is a useful exercise to work out the equivalent descriptions in the other
forms (e.g., in terms of adjacency matrices and rotation maps).
16.4.2
The matrix/path product
G: (n,d,λ)-graph
G’: (n,d’,λ’)-graph
G’G: (n,dd’,λλ’)-graph
For every two n vertex graphs G, G′ with degrees d, d′ and adjacency matrices A, A′, the graph
G′G is the graph described by the adjacency matrix A′A. That is, G′G has an edge (u, v) for every
length 2-path from u to v where the ﬁrst step in the path is taken on en edge of G and the second
is on an edge of G′. Note that G has n vertices and degree dd′. Typically, we are interested in
the case G = G′, where it is called graph squaring. More generally, we denote by Gk the graph
G · G · · · G (k times). We already encountered this case before in Lemma 7.27, and similar analysis
yields the following lemma (whose proof we leave as exercise):
Web draft 2007-01-08 21:59

DRAFT
p16.18 (298)
16.4. EXPLICIT CONSTRUCTION OF EXPANDER GRAPHS
Lemma 16.28 (Matrix product improves expansion)
λ(G′G) ≤λ(G′)λ(G′)
It is also not hard to compute the rotation map of G′G from the rotation maps of G and G′.
Again, we leave verifying this to the reader.
16.4.3
The tensor product
G: (n,d,λ)-graph
G’: (n’,d’,λ’)-graph
GOG’: (nn’,dd’,max{λ,λ’})-graph
x
Let G and G′ be two graphs with n (resp n′) vertices and d (resp. d′) degree, and let ˆG :
[n] × [d] →[n] × [d] and ˆG′ : [n′] × [d′] →[n′] × [d′] denote their respective rotation maps. The
tensor product of G and G′, denoted G ⊗G′, is the graph over nn′ vertices and degree dd′ whose
rotation map
ˆ
G ⊗G′ is the permutation over ([n] × [n′]) × ([d] × [d′]) deﬁned as follows
ˆ
G ⊗G′(⟨u, v⟩, ⟨i, j⟩) = ⟨u′, v′⟩, ⟨i′, j′⟩,
where (u′, i′) = ˆG(u, i) and (v′, j′) = ˆG′(v, j). That is, the vertex set of G ⊗G′ is pairs of vertices,
one from G and the other from G′, and taking a the step ⟨i, j⟩on G ⊗G′ from the vertex ⟨u, v⟩is
akin to taking two independent steps: move to the pair ⟨u′, v′⟩where u′ is the ith neighbor of u in
G and v′ is the ith neighbor of v in G′.
In terms of adjacency matrices, the tensor product is also quite easy to describe. If A = (ai,j)
is the n × n adjacency matrix of G and A′ = (a′
i′,j′) is the n′ × n′ adjacency matrix of G′, then the
adjacency matrix of G ⊗G′, denoted as A ⊗A′, will be an nn′ × nn′ matrix that in the ⟨i, i′⟩th row
and the ⟨j, j′⟩column has the value ai,j · a′
i′,j′. That is, A ⊗A′ consists of n2 copies of A′, with the
(i, j)th copy scaled by ai,j:
A ⊗A′ =





a1,1A′
a1,2A′
. . .
a1,nA′
a2,1A′
a2,2A′
. . .
a2,nA′
...
...
an,1A′
an,2A′
. . .
an,nA′





The tensor product can also be described in the language of graphs as having a cluster of n′
vertices in G ⊗G′ for every vertex of G. Now if, u and v are two neighboring vertices in G, we will
put a bipartite version of G′ between the cluster corresponding to u and the cluster corresponding
to v in G. That is, if (i, j) is an edge in G′ then there is an edge between the ith vertex in the
cluster corresponding to u and the jth vertex in the cluster corresponding to v.
Web draft 2007-01-08 21:59

DRAFT
16.4. EXPLICIT CONSTRUCTION OF EXPANDER GRAPHS
p16.19 (299)
Lemma 16.29 (Tensor product preserves expansion)
Let λ = λ(G) and λ′ = λ(G′) then λ(G ⊗G′) ≤max{λ, λ′}.
One intuition for this bound is the following: taking a T step random walk on the graph G⊗G′
is akin to taking two independent random walks on the graphs G and G′. Hence, if both walks
converge to the uniform distribution within T steps, then so will the walk on G ⊗G′.
Proof: Given some basic facts about tensor products and eigenvalues this is immediate since if
λ1, . . . , λn are the eigenvalues of A (where A is the adjacency matrix of G) and λ′
1, . . . , λ′
n′ are the
eigenvalues of A (where A′ is the adjacency matrix of G′), then the eigenvalues of A ⊗A′ are all
numbers of the form λi · λ′
j, and hence the largest ones apart from 1 are of the form 1 · λ(G′) or
λ(G) · 1 (see also Exercise 14). ■
We note that one can show that λ(G ⊗G′) ≤λ(G) + λ(G′) without relying on any knowledge
of eigenvalues (see Exercise 15). This weaker bound suﬃces for our applications.
16.4.4
The replacement product
G: (n,D,1-ε)-graph
G’: (D,d,1-ε’)-graph
GOG’: (nD,2d,1-εε’/4)-graph
R
In both the matric and tensor products, the degree of the resulting graph is larger than the
degree of the input graphs. The following product will enable us to reduce the degree of one of the
graphs. Let G, G′ be two graphs such that G has n vertices and degree D, and G′ has D vertices
and degree d. The balanced replacement product (below we use simply replacement product for
short) of G and G′ is denoted by G ⃝
R G′ is the nn′-vertex 2d-degree graph obtained as follows:
1. For every vertex u of G, the graph G⃝
R G′ has a copy of G′ (including both edges and vertices).
2. If u, v are two neighboring vertices in G then we place d parallel edges between the ith vertex
in the copy of G′ corresponding to u and the jth vertex in the copy of G′ corresponding to v,
where i is the index of v as a neighbor of u and j is the index of u as a neighbor of v in G.
(That is, taking the ith edge out of u leads to v and taking the jth edge out of v leads to u.)
Note that we essentially already encountered this product in the proof of Claim ?? (see also
Figure ??), where we reduced the degree of an arbitrary graph by taking its replacement product
with a cycle (although there we did not use parallel edges).5 The replacement product also has
5The addition of parallel edges ensures that a random step from a vertex v in G ⃝
R G′ will move to a neighbor
within the same cluster and a neighbor outside the cluster with the same probability. For this reason, we call this
product the balanced replacement product.
Web draft 2007-01-08 21:59

DRAFT
p16.20 (300)
16.4. EXPLICIT CONSTRUCTION OF EXPANDER GRAPHS
a simple description in terms of rotation maps: since G ⃝
R G′ has nD vertices and 2d degree, its
rotation map
ˆ
G ⃝
R G′ is a permutation over ([n] × [D]) × ([d] × {0, 1}) and so can be thought of as
taking four inputs u, v, i, b where u ∈[n], v ∈[D], i ∈[d] and b ∈{0, 1}. If b = 0 then it outputs
u, ˆG′(v, i), b and if b = 1 then it outputs ˆG(u, v), i, b. That is, depending on whether b is equal to 0
or 1, the rotation map either treats v as a vertex of G′ or as an edge label of G.
In the language of adjacency matrices the replacement product can be easily seen to be described
as follows: A⃝
R A′ = 1/2(A⊗ID)+ 1/2(In ⊗A′), where A, A′ are the adjacency matrices of the graphs
G and G′ respectively, and Ik is the k × k identity matrix.
If D ≫d then the replacement product’s degree will be signiﬁcantly smaller than G’s degree.
The following Lemma shows that this dramatic degree reduction does not cause too much of a
deterioration in the graph’s expansion:
Lemma 16.30 (Expansion of replacement product)
If λ(G) ≤1 −ϵ and λ(G′) ≤1 −ϵ′ then λ(G ⃝
R G′) ≤1 −ϵϵ′/4.
The intuition behind Lemma 16.30 is the following: Think of the input graph G as a good
expander whose only drawback is that it has a too high degree D. This means that a k step random
walk on G′ requires O(k log D) random bits. However, as we saw in Section 7.B.3, sometimes we
can use fewer random bits if we use an expander. So a natural idea is to generate the edge labels for
the walk by taking a walk using a smaller expander G′ that has D vertices and degree d ≪D. The
deﬁnition of G⃝
R G′ is motivated by this intuition: a random walk on G⃝
R G′ is roughly equivalent
to using an expander walk on G′ to generate labels for a walk on G. In particular, each step a
walk over G⃝
R G′ can be thought of as tossing a coin and then, based on its outcome, either taking
a a random step on G′, or using the current vertex of G′ as an edge label to take a step on G.
Another way to gain intuition on the replacement product is to solve Exercise 16, that analyzes
the combinatorial (edge) expansion of the resulting graph as a function of the edge expansion of
the input graphs.
Proof of Lemma 16.30: Let A (resp. A′) denote the n × n (resp. D × D) adjacency matrix of
G (resp. G′) and let λ(A) = 1 −ϵ and λ(A′) = 1 −ϵ′. Then by Lemma 7.40, A = (1 −ϵ)C + Jn
and A′ = (1 −ϵ′)C′ + JD, where Jk is the k × k matrix with all entries equal to 1/k.
The adjacency matrix of G ⃝
R G′ is equal to
1
2(A ⊗ID) + 1
2(In ⊗A′) = 1−ϵ
2 C ⊗ID + ϵ
2Jn ⊗ID + 1−ϵ′
2 In ⊗C′ + ϵ′
2 In ⊗JD ,
where Ik is the k × k identity matrix.
Thus, the adjacency matrix of (G ⃝
R G′)2 is equal to

1−ϵ
2 C ⊗ID + ϵ
2Jn ⊗ID + 1−ϵ′
2 In ⊗C′ + ϵ′
2 In ⊗JD
2
=
ϵϵ′
4 (Jn ⊗ID)(In ⊗JD) + ϵ′ϵ
4 (In ⊗JD)(Jn ⊗ID) + (1 −ϵϵ′
2 )F ,
where F is some nD × nD matrix of norm at most 1 (obtained by collecting together all the other
terms in the expression). But
(Jn ⊗ID)(In ⊗JD) = (In ⊗JD)(Jn ⊗ID) = Jn ⊗JD = JnD .
Web draft 2007-01-08 21:59

DRAFT
16.4. EXPLICIT CONSTRUCTION OF EXPANDER GRAPHS
p16.21 (301)
(This can be veriﬁed by either direct calculation or by going through the graphical representation
or the rotation map representation of the tensor and matrix products.)
Since every vector v ∈RnD that is orthogonal to 1 satisﬁes JnDv = 0, we get that
 λ(G ⃝
R G′)
2 = λ
 (G ⃝
R G′)2
= λ

(1 −ϵϵ′
2 )F + ϵϵ′
2 JnD

≤1 −ϵϵ′
2 ,
and hence
λ(G ⃝
R G′) ≤1 −ϵϵ′
4 .
■
16.4.5
The actual construction.
We now use the three graph products of described above to show a strongly explicit construction
of an expander graph family. Recall This is an inﬁnite family {Gk} of graphs (with eﬃcient way to
compute neighbors) that has a constant degree and an expansion parameter λ. The construction
is recursive: we start by a ﬁnite size graph G1 (which we can ﬁnd using brute force search), and
construct the graph Gk from the graph Gk−1. On a high level the construction is as follows: each
of the three product will serve a diﬀerent purpose in the construction. The Tensor product allows
us to take Gk−1 and increase its number of vertices, at the expense of increasing the degree and
possibly some deterioration in the expansion. The replacement product allows us to dramatically
reduce the degree at the expense of additional deterioration in the expansion. Finally, we use the
Matrix/Path product to regain the loss in the expansion at the expense of a mild increase in the
degree.
Theorem 16.31 (Explicit construction of expanders)
There exists a strongly-explicit λ, d-expander family for some constants d and λ < 1.
Proof: Our expander family will be the following family {Gk}k∈N of graphs:
• Let H be a (D = d40, d, 0.01)-graph, which we can ﬁnd using brute force search. (We choose
d to be a large enough constant that such a graph exists)
• Let G1 be a (D, d20, 1/2)-graph, which we can ﬁnd using brute force search.
• For k > 1, let Gk = ((Gk−1 ⊗Gk−1) ⃝z H)20.
The proof follows by noting the following points:
1. For every k, Gk has at least 22k vertices.
Indeed, if nk denotes the number of vertices of Gk, then nk = (nk−1)2D. If nk−1 ≥22k−1 then
nk ≥

22k−12
= 22k.
Web draft 2007-01-08 21:59

DRAFT
p16.22 (302)
16.5. DETERMINISTIC LOGSPACE ALGORITHM FOR UNDIRECTED CONNECTIVITY.
2. For every k, the degree of Gk is d20.
Indeed, taking a replacement produce with H reduces the degree to d, which is then increased
to d20 by taking the 20th power of the graph (using the matrix/path product).
3. There is a 2O(k)-time algorithm that given a label of a vertex u in Gk and an index i ∈[d20],
outputs the ith neighbor of u in Gk. (Note that this is polylogarithmic in the number of
vertices.)
Indeed, such a recursive algorithm can be directly obtained from the deﬁnition of Gk. To
compute Gk’s neighborhood function, the algorithm will make 40 recursive calls to Gk−1’s
neighborhood function, resulting in 2O(k) running time.
4. For every k, λ(Gk) ≤1/3.
Indeed, by Lemmas 16.28, 16.29, and 16.30 If λ(Gk−1) ≤1/3 then λ(Gk−1 ⊗Gk−1) ≤2/3 and
hence λ((Gk−1⊗Gk−1)⃝
R H) ≤1−0.99
12 ≤1−1/13. Thus, λ(Gk) ≤(1−1/13)20 ∼e−20/13 ≤1/3.
■
Using graph powering we can obtain such a construction for every constant λ ∈(0, 1), at the
expense of a larger degree. There is a variant of the above construction supplying a denser family
of graphs that contains an n-vertex graph for every n that is a power of c, for some constant c.
Since one can transform an (n, d, λ)-graph to an (n′, cd′, λ)-graph for any n/c ≤n′ ≤n by making
a single “mega-vertex” out of a set of at most c vertices, the following theorem is also known:
Theorem 16.32
There exist constants d ∈N , λ < 1 and a strongly-explicit graph family {Gn}n∈N such that Gn is
an (n, d, λ)-graph for every n ∈N.
Remark 16.33
As mentioned above, there are known constructions of expanders (typically based on number theory)
that are more eﬃcient in terms of computation time and relation between degree and the parameter
λ than the product-based construction above. However, the proofs for these constructions are more
complicated and require deeper mathematical tools. Also, the replacement product (and its close
cousin, the zig-zag product) have found applications beyond the constructions of expander graphs.
One such application is the deterministic logspace algorithm for undirected connectivity described
in the next section. Another application is a construction of combinatorial expanders with greater
expansion that what is implied by the parameter λ. (Note that even for for the impossible to
achieve value of λ = 0, Theorem ?? implies combinatorial expansion only 1/2 while it can be shown
that a random graph has combinatorial expansion close to 1.)
16.5
Deterministic logspace algorithm for undirected connectiv-
ity.
This section describes a recent result of Reingold, showing that at least the most famous random-
ized logspace algorithm, the random walk algorithm for s-t-connectivity in undirected graphs (
Web draft 2007-01-08 21:59

DRAFT
16.5. DETERMINISTIC LOGSPACE ALGORITHM FOR UNDIRECTED CONNECTIVITY.
p16.23 (303)
Chapter 7) can be completely “derandomized.” Thus the s-t-connectivity problem in undirected
graphs is in L.
Theorem 16.34 (Reingold’s theorem [?])
UPATH ∈L.
Reingold describes a set of poly(n) walks starting from s such that if s is connected to t then
one of the walks is guaranteed to hit t. Of course, the existence of such a small set of walks is
trivial; this arose in our discussion of universal traversal sequences of Deﬁnition ??. The point is
that Reingold’s enumeration of walks can be carried out deterministically in logspace.
In this section, all graphs will be multigraphs, of the form G = (V, E) where E is a multiset
(i.e., some edges may appear multiple times, and each appearance is counted separately). We say
the graph is d-regular if for each vertex i, the number of edges incident to it is exactly d. We
will assume that the input graph for the s-t connectivity problem is d-regular for say d = 4. This
is without loss of generality: if a vertex has degree d′′ < 3 we add a self-loop of multiplicity to
bring the degree up to d, and if the vertex has degree d′ ≥3 we can replace it by a cycle of d′
vertices, and each of the d′ edges that were incident to the old vertex then attach to one of the cycle
nodes. Of course, the logspace machine does not have space to store the modiﬁed graph, but it can
pretend that these modiﬁcations have taken place, since it can perform them on the ﬂy whenever
it accesses the graph. (Formally speaking, the transformation is implicitly computable in logspace;
see Claim ??.) In fact, the proof below will perform a series of other local modiﬁcations on the
graph, each with the property that the logspace algorithm can perform them on the ﬂy.
Recall that checking connectivity in expander graphs is easy. Speciﬁcally, if every connected
component in G is an expander, then there is a number ℓ= O(log n) such that if s and t are
connected then they are connected with a path of length at most ℓ.
Theorem 16.35
If an n-vertex graph G is d-regular graph and λ(G) < 1/4 then the maximum distance between
every pair of nodes is at most O(d log n).
Proof: The exercises ask you to prove that for each subset S of size at most |V | /2, the number
of edges between S and S is at least (1 −λ) |S| /2 ≥3 |S| /8. Thus at least 3 |S| /8d vertices in
S must be neighbors of vertices in S. Iterating this argument l times we conclude the following
about the number of vertices whose distance to S is at most l: it is either more than |V | /2 (when
the abovementioned fact stops applying) or at least (1 + 3
8d)l. Let s, t be any two vertices. Using
S = {s}, we see that at least |V | /2+1 vertices must be within distance l = 10d log n of s. The same
is true for vertex t. Every two subsets of vertices of size at least |V | /2 + 1 necessarily intersect, so
there must be some vertex within distance l of both s and t. Hence the distance from s to t is at
most 2l. ■
We can enumerate over all ℓ-step random walks of a d-degree graph in O(dℓ) space by enu-
merating over all sequences of indices i1, . . . , iℓ∈[d]. Thus, in a constant-degree graph where all
connected components are expanders we can check connectivity in logarithmic space.
Web draft 2007-01-08 21:59

DRAFT
p16.24 (304)
16.5. DETERMINISTIC LOGSPACE ALGORITHM FOR UNDIRECTED CONNECTIVITY.
The idea behind Reingold’s algorithm is to transform the graph G (in an implicitly computable
in logspace way) to a graph G′ such that every connected component in G becomes an expander
in G′, but two vertices that were not connected will stay unconnected.
By adding more self-loops we may assume that the graph is of degree d20 for some constant d
that is suﬃciently large so that there exists a (d20, d, 0.01)-graph H. (See Fact ?? in the Appendix.)
Since the size of H is some constant, we assume the algorithm has access to it (either H could be
”hardwired” into the algorithm or the algorithm could perform brute force search to discover it).
Consider the following sequence of transformations.
• Let G0 = G.
• For k ≥1, we deﬁne Gk = (Gk−1 ⃝
R H)20.
Here ⃝
R is the replacement product of the graph, deﬁned in Chapter ??. If Gk−1 is a graph with
degree d20, then Gk−1 ⃝
R H is a graph with degree d and thus Gk = (Gk−1 ⃝
R H)20 is again a graph
with degree d20 (and size (2d20 |Gk−1|)20).
Note also that if two vertices were connected (resp.,
disconnected) in Gk−1, then they are still connected (resp., disconnected) in Gk. Thus to solve the
UPATH in G it suﬃces to solve a UPATH problem in any of the Gk’s.
Now we show that for k = O(log n), the graph Gk is an expander, and therefore an easy instance
of UPATH. By Lemmas 16.28 and 16.30, for every ϵ < 1/20 and D-degree graph F, if λ(F) ≤1 −ϵ
then λ(F ⃝
R H) ≤1 −ϵ/5 and hence λ
 (F ⃝
R H)20
≤1 −2ϵ. By Lemma 7.28, every connected
component of G has expansion parameter at most 1 −1/(8Dn3), where n denotes the number of
G’s vertices which is at least as large as the number of vertices in the connect component. It follows
that for k = 10 log D log N, in the graph Gk every connected component has expansion parameter
at most max{1 −1/20, 2k/(8Dn3)} = 1 −1/20.
To ﬁnish, we show how to solve the UPATH problem for Gk in logarithmic space for this value of
k. The catch is of course that the graph we are given is G, not Gk. Given G, we wish to enumerate
length ℓstarting from a given vertex in Gk since the graph is an expander. A walk describes, for
each step, which of the d20 outgoing edges to take from the current vertex. Thus it suﬃces to show
how we can compute in O(k + log n) space, the ith outgoing edge of a given vertex u in Gk. This
map’s input length is O(k + log n) and hence we can assume it is placed on a read/write tape, and
will compute the rotation map “in-place” changing the input to the output. Let sk be the additional
space (beyond the input) required to compute the rotation map of Gk. Note that s0 = O(log n).
We show a recursive algorithm to compute Gk satisfying the equation sk = sk−1 + O(1). In fact,
the algorithm will be a pretty straightforward implementation of the deﬁnitions of the replacement
and matrix products.
The input to
ˆ
Gk is a vertex in (Gk−1 ⃝
R H) and 20 labels of edges in this graph. If we can
compute the rotation map of Gk−1⃝
R H in sk−1 +O(1) space then we can do so for ˆ
Gk, since we can
simply make 20 consecutive calls to this procedure, each time reusing the space.6 Now, to compute
the rotation map of (Gk−1 ⃝
R H) we simply follow the deﬁnition of the replacement product. Given
6One has to be slightly careful while making recursive calls, since we don’t want to lose even the O(log log n) bits
of writing down k and keeping an index to the location in the input we’re working on. However, this can be done
by keeping k in global read/write storage and since storing the identity of the current step among the 50 calls we’re
making only requires O(1) space.
Web draft 2007-01-08 21:59

DRAFT
16.6. WEAK RANDOM SOURCES AND EXTRACTORS
p16.25 (305)
an input of the form u, v, i, b (which we think of as read/write variables), if b = 0 then we apply
the rotation map of H to (v, i) (can be done in constant space), while if b = 1 then we apply the
rotation map of Gk−1 to (u, v) using a recursive call at the cost of sk−1 space (note that u, v are
conveniently located consecutively at the beginning of the input tape).
16.6
Weak Random Sources and Extractors
Suppose, that despite the philosophical diﬃculties, we are happy with probabilistic algorithms, and
see no need to “derandomize” them, especially at the expense of some unproven assumptions. We
still need to tackle the fact that real world sources of randomness and unpredictability rarely, if
ever, behave as a sequence of perfectly uncorrelated and unbiased coin tosses. Can we still execute
probabilistic algorithms using real-world “weakly random” sources?
16.6.1
Min Entropy
For starters, we need to deﬁne what we mean by a weakly random source.
Definition 16.36
Let X be a random variable. The min entropy of X, denoted by H∞(X), is the largest real number
k such that Pr[X = x] ≤2−k for every x in the range of X.
If X is a distribution over {0, 1}n with H∞(X) ≥k then it is called an (n, k)-source.
It is not hard to see that if X is a random variable over {0, 1}n then H∞(X) ≤n with H∞(X) =
n if and only if X is distributed according to the uniform distribution Un. Our goal in this section is
to be able to execute probabilistic algorithms given access to a distribution X with H∞(X) as small
as possible. It can be shown that min entropy is a minimal requirement in the sense that in general,
to execute a probabilistic algorithm that uses k random bits we need access to a distribution X
with H∞(X) ≥k (see Exercise ??).
Example 16.37
Here are some examples for distributions X over {0, 1}n and their min-entropy:
• (Bit ﬁxing and generalized bit ﬁxing sources) If there is subset S ⊆[n] with |S| = k such that
X’s projection to the coordinates in S is uniform over {0, 1}k, and X’s projection to [n]\S is
a ﬁxed string (say the all-zeros string) then H∞(X) = k. The same holds if X’s projection to
[n] \ S is a ﬁxed deterministic function of its projection to S. For example, if the bits in the
odd positions of X are independent and uniform and for every even position 2i, X2i = X2i−1
then H∞(X) = ⌈n
2 ⌉. This may model a scenario where we measure some real world data at
too high a rate (think of measuring every second a physical event that changes only every
minute).
• (Linear subspaces) If X is the uniform distribution over a linear subspace of GF(2)n of
dimension k, then H∞(X) = k. (In this case X is actually a generalized bit-ﬁxing source—
can you see why?)
Web draft 2007-01-08 21:59

DRAFT
p16.26 (306)
16.6. WEAK RANDOM SOURCES AND EXTRACTORS
• (Biased coins) If X is composed of n independent coins, each outputting 1 with probability
δ < 1/2 and 0 with probability 1 −δ, then as n grows, H∞(X) tends to H(δ)n where H is the
Shannon entropy function. That is, H(δ) = δ log 1
δ + (1 −δ) log
1
1−δ.
• (Santha-Vazirani sources) If X has the property that for every i ∈[n], and every string
x ∈{0, 1}i−1, conditioned on X1 = x1, . . . , Xi−1 = xi−1 it holds that both Pr[Xi = 0] and
Pr[Xi = 1] are between δ and 1 −δ then H∞(X) ≥H(δ)n.
This can model sources such as
stock market ﬂuctuations, where current measurements do have some limited dependence on
the previous history.
• (Uniform over subset) If X is the uniform distribution over a set S ⊆{0, 1}n with |S| = 2k
then H∞(X) = k. As we will see, this is a very general case that “essentially captures” all
distributions X with H∞(X) = k.
We see that min entropy is a pretty general notion, and distributions with signiﬁcant min
entropy can model many real-world sources of randomness.
16.6.2
Statistical distance and Extractors
Now we try to formalize what it means to extract random —more precisely, almost random— bits
from an (n, k) source. To do so we will need the following way of quantifying when two distributions
are close.
Definition 16.38 (statistical distance)
For two random variables X and Y with range {0, 1}m, their statistical distance (also known as
variation distance) is deﬁned as δ(X, Y ) = maxS⊆{0,1}m{|Pr[X ∈S] −Pr[Y ∈S]|}. We say that
X, Y are ϵ-close, denoted X ≈ϵ Y , if δ(X, Y ) ≤ϵ.
Statistical distance lies in [0, 1] and satisﬁes triangle inequality, as suggested by its name. The
next lemma gives some other useful properties; the proof is left as an exercise.
Lemma 16.39
Let X, Y be any two distributions taking values in {0, 1}n.
1. δ(X, Y ) = 1
2
P
x∈{0,1}n |Pr[X = x] −Pr[Y = x]| .
2. (Restatement of Deﬁnition 16.38) δ(X, Y ) ≥ϵ iﬀthere is a boolean function D : {0, 1}m →
{0, 1} such that |Prx∈X[D(x) = 1] −Pry∈Y [D(y) = 1]| ≥ϵ.
3. If f : {0, 1}n →{0, 1}s is any function, then δ(f(X), f(Y )) ≤δ(X, Y ).
(Here f(X) is a
distribution on {0, 1}s obtained by taking a sample of X and applying f.)
Now we deﬁne an extractor. This is a (deterministic) function that transforms an (n, k) source
into an almost uniform distribution. It uses a small number of additional truly random bits, denoted
by t in the deﬁnition below.
Web draft 2007-01-08 21:59

DRAFT
16.6. WEAK RANDOM SOURCES AND EXTRACTORS
p16.27 (307)
Definition 16.40
A function Ext : {0, 1}n × {0, 1}t →{0, 1}m is a (k, ϵ) extractor if for any (n, k)-source X, the
distribution Ext(X, Ut) is ϵ-close to Um. (For every ℓ, Uℓdenotes the uniform distribution over
{0, 1}ℓ.)
Equivalently, if Ext : {0, 1}n × {0, 1}t →{0, 1}m is a (k, ϵ) extractor, then for every distribution
X ranging over {0, 1}n of min-entropy k, and for every S ⊆{0, 1}m, we have
|Pra∈X,z∈{0,1}t[Ext(a, z) ∈S] −Prr∈{0,1}m[r ∈S]| ≤ϵ
We use this fact to show in Section 16.7.2 how to use extractors and (n, k)-sources to to simulate
any probabilistic computation.
Why an additional input?
Our stated motivation for extractors is to execute probabilistic
algorithms without access to perfect unbiased coins. Yet, it seems that an extractor is not suﬃcient
for this task, as we only guarantee that its output is close to uniform if it is given an additional
input that is uniformly distributed. First, we note that the requirement of an additional input is
necessary: for every function Ext : {0, 1}n →{0, 1}m and every k ≤n −1 there exists an (n, k)-
source X such that the ﬁrst bit of Ext(X) is constant (i.e, is equal to some value b ∈{0, 1} with
probability 1), and so is at least of statistical distance 1/2 from the uniform distribution (Exercise 7).
Second, if the length t of the second input is suﬃciently short (e.g., t = O(log n)) then, for the
purposes of simulating probabilistic algorithms, we can do without any access to true random coins,
by enumerating over all the 2t possible inputs (see Section 16.7.2). Clearly, t has to be somewhat
short for the extractor to be non-trivial: for t ≥m, we can have a trivial extractor that ignores its
ﬁrst input and outputs the second input. This second input is called the seed of the extractor.
16.6.3
Extractors based upon hash functions
One can use pairwise independent (and even weaker notions of) hash functions to obtain extractors.
In this section, H denotes a family of hash functions h : {0, 1}n →{0, 1}k. We say it has collision
error δ if for any x1 ̸= x2 ∈{0, 1}n, Prh∈H[h(x1) = h(x2)] ≤(1 + δ)/2k. We assume that one
can choose a random function h ∈H by picking a string at random from {0, 1}t. We deﬁne the
extractor Ext:× {0, 1}t →{0, 1}k+t as follows:
Ext(x, h) = h(x) ◦h,
(17)
where ◦denotes concatenation of strings.
To prove that this is an extractor, we relate the min-entropy to the collision probability of a
distribution, which is deﬁned as P
a p2
a, where pa is the probability assigned to string a.
Lemma 16.41
If a distribution X has min-entropy at least k then its collision probability is at most 1/2k.
Proof: For every a in X’s range, let pa be the probability that X = a.
Then, P
a p2
a ≤
maxa {pa} (P
a pa) ≤
1
2k · 1 =
1
2k . ■
Web draft 2007-01-08 21:59

DRAFT
p16.28 (308)
16.6. WEAK RANDOM SOURCES AND EXTRACTORS
Lemma 16.42 (Leftover hash lemma)
If x is chosen from a distribution on {0, 1}n with min-entropy at least k/δ and H has collision error
δ, then h(X) ◦h has distance at most
√
2δ to the uniform distribution.
Proof: Left as exercise. (Hint: use the relation between the L2 and L1 norms ■
16.6.4
Extractors based upon random walks on expanders
This section assumes knowledge of random walks on expanders, as described in Chapter ??.
Lemma 16.43
Let ϵ > 0. For every n and k ≤n there exists a (k, ϵ)-extractor Ext : {0, 1}n × {0, 1}t →{0, 1}n
where t = O(n −k + log 1/ϵ).
Proof: Suppose X is an (n, k)-source and we are given a sample a from it. Let G be a (2n, d, 1/2)-
graph for some constant d (see Deﬁnition 7.31 and Theorem 16.32).
Let z be a truly random seed of length t = 10 log d(n −k + log 1/ϵ) = O(n −k + log 1/ϵ). We
interpret z as a random walk in G of length 10(n −k + log 1/ϵ) starting from the node whose label
is a. (That is, we think of z as 10(n −k + log 1/ϵ) labels in [d] specifying the steps taken in the
walk.) The output Ext(a, z) of the extractor is the label of the ﬁnal node on the walk.
We have ∥X −1∥2
2 ≤∥X∥2
2 = P
a Pr[X = a]2, which is at most 2−k by Lemma 16.41 since X is
an (n, k)-source. Therefore, after a random walk of length t the distance to the uniform distribution
is (by the upperbound in (??)):
∥MtX −1
2N 1∥1 ≤λt
2∥X −1
2N 1∥2
√
2N ≤λt
22(N−k)/2.
When t is a suﬃciently large multiple of N −k + log 1/ε, this distance is smaller than ε. ■
16.6.5
An extractor based upon Nisan-Wigderson
this section is still quite rough
Now we describe an elegant construction of extractors due to Trevisan.
Suppose we are given a string x obtained from an (N, k)-source. How can we extract k random
bits from it, given O(log N) truly random bits? Let us check that the trivial idea fails. Using
2 log N random bits we can compute a set of k (where k < N −1) indices that are uniformly
distributed and pairwise independent. Maybe we should just output the corresponding bits of x?
Unfortunately, this does not work: the source is allowed to set N −k bits (deterministically) to 0 so
long as the remaining k bits are completely random. In that case the expected number of random
bits in our sample is at most k2/N, which is less than even 1 if k <
√
N.
This suggests an important idea: we should ﬁrst apply some transformation on x to “smear out”
the randomness, so it is not localized in a few bit positions. For this, we will use error-correcting
codes. Recall that such codes are used to introduce error-tolerance when transmitting messages
over noisy channels. Thus intuitively, the code must have the property that it “smears” every bit
of the message all over the transmitted message.
Web draft 2007-01-08 21:59

DRAFT
16.6. WEAK RANDOM SOURCES AND EXTRACTORS
p16.29 (309)
Having applied such an encoding to the weakly random string, the construction selects bits
from it using a better sampling method than pairwise independent sampling, namely, the Nisan-
Wigderson combinatorial design.
Nisan-Wigderson as a sampling method:
In (??) we deﬁned a function NWf,S(z) using any function f : {0, 1}l →{0, 1} and a com-
binatorial design S. Note that the deﬁnition works for every function, not just hard-to-compute
functions. Now we observe that NWf,S(z) is actually a way to sample entries from the truth table
of f.
Think of f as a bitstring of length 2l, namely, its truth table. (Likewise, we can think of any
circuit with l-bit inputs and with 0/1 outputs as computing a string of length 2l.) Given any z
(“the seed”), NWf,S(z) is just a method to use z to sample a sequence of m bits from f. This is
completely analogous to pairwise independent sampling considered above; see Figure ??.
Figure unavailable in pdf ﬁle.
Figure 16.3: Nisan-Wigderson as a sampling method: An (l, α)-design (S1, S2, . . . , Sm) where each Si ⊆[t], |Si| = l
can be viewed as a way to use z ∈{0, 1}t to sample m bits from any string of length 2l, which is viewed as the truth
table of a function f :{0, 1}l →{0, 1}.
List-decodable codes
The construction will use the following kind of codes.
Definition 16.44
If δ > 0, a mapping σ:{0, 1}N →{0, 1}
¯
N is called an error-correcting code that is list-decodable up
to error 1/2 −δ if for every w ∈{0, 1}
¯
N, the number of y ∈BN such that w, σ(y) disagree in at
most 1/2 −δ fraction of bits is at most 1/δ2.
The set
n
σ(x) : x ∈{0, 1}No
is called the set of codewords.
The name “list-decodable” owes to the fact that if we transmit x over a noisy channel after ﬁrst
encoding with σ then even if the channel ﬂips 1/2 −δ fraction of bits, there is a small “list” of y
that the received message could be decoded to. (Unique decoding may not be possible, but this will
be of no consequence in the construction below.) The exercises ask you to prove that list-decodable
codes exist with ¯N = poly(N, 1/δ), where σ is computable in polynomial time.
Trevisan’s extractor:
Suppose we are given an (N, k)-source.
We ﬁx σ : {0, 1}N →{0, 1}
¯
N, a polynomial-time
computable code that is list-decodable upto to error 1/2 −ϵ/m. We assume that ¯N is a power
of 2 and let l = log2 ¯N.
Now every string x ∈{0, 1}
¯
N may be viewed as a boolean function
< x >: {0, 1}log ¯
N →{0, 1} whose truth table is x. Let S = (S1, . . . , Sm) be a (l, log m) design over
[t].
The extractor ExtNW : {0, 1}N × {0, 1}t →{0, 1}m is deﬁned as
Web draft 2007-01-08 21:59

DRAFT
p16.30 (310)
16.6. WEAK RANDOM SOURCES AND EXTRACTORS
ExtNWσ,S(x, z) = NW<σ(x)>,S(z) .
That is, ExtNW encodes its ﬁrst (“weakly random”) input x using an error-correcting code, then
uses Nisan-Wigderson sampling on the resulting string using the second (“truly random”) input z
as a seed.
Lemma 16.45
For suﬃciently large m and for ϵ > 2−m2, ExtNWσ,S is a (m3, 2ϵ)-extractor.
Proof: Let X be an (N, k) source where the min-entropy k is m3. To prove that the distribution
ExtNW(a, z) where a ∈X, z ∈{0, 1}t is close to uniform, it suﬃces (see our remarks after
Deﬁnition 16.38) to show for each function D : {0, 1}m →{0, 1} that
Prr[D(r) = 1] −Pra∈X,z∈{0,1}t[D(ExtNW(a, z)) = 1]
 ≤2ϵ.
(18)
For the rest of this proof, we ﬁx an arbitrary D and prove that (18) holds for it.
The role played by this test D is somewhat reminiscent of that played by the distinguisher
algorithm in the deﬁnition of a pseudorandom generator, except, of course, D is allowed to be
arbitrarily ineﬃcient. This is why we will use the black-box version of the Nisan-Wigderson analysis
(Corollary ??), which does not care about the complexity of the distinguisher.
Let B be the set of bad a’s for this D, where string a ∈X is bad for D if
Pr[D(r) = 1] −Prz∈{0,1}t[D(ExtNW(a, z)) = 1]
 > ϵ.
We show that B is small using a counting argument: we exhibit a 1-1 mapping from the set of
bad a’s to another set G, and prove G is small. Actually, here is G:
G =

circuits of size O(m2)
	
× {0, 1}2 log(m/ϵ) × {0, 1}2 .
The number of circuits of size O(m2) is 2O(m2 log m), so |G| ≤2O(m2 log m) × 2(m/ϵ)2 = 2O(m2 log m).
Let us exhibit a 1-1 mapping from B to G. When a is bad, Corollary ?? implies that there is
a circuit C of size O(m2) such that either the circuit D(C()) or its negation –XORed with some
ﬁxed bit b—agrees with σ(a) on a fraction 1/2 + ϵ/m of its entries. (The reason we have to allow
either D(C()) or its complement is the |·| sign in the statement of Corollary ??.) Let w ∈{0, 1}
¯
N
be the string computed by this circuit. Then σ(a) disagrees with w in at most 1/2−ϵ/m fraction of
bits. By the assumed property of the code σ, at most (m/ϵ)2 other codewords have this property.
Hence a is completely speciﬁed by the following information: (a) circuit C; this is speciﬁed by
O(m2 log m) bits (b) whether to use D(C()) or its complement to compute w, and also the value
of the unknown bit b; this is speciﬁed by 2 bits (c) which of the (m/ϵ)2 codewords around w to
pick as σ(a); this is speciﬁed by ⌈2 log(m/ϵ)⌉bits assuming the codewords around w are ordered
in some canonical way. Thus we have described the mapping from B to G.
We conclude that for any ﬁxed D, there are at most 2O(m2 log m) bad strings. The probability
that an element a taken from X is bad for D is (by Lemma ??) at most 2−m3 · 2O(m2 log m) < ϵ for
Web draft 2007-01-08 21:59

DRAFT
16.7. APPLICATIONS OF EXTRACTORS
p16.31 (311)
suﬃciently large m. We then have
Prr[D(r) = 1] −Pra∈X,z∈{0,1}t[D(ExtNW(a, z)) = 1]

≤
X
a
Pr[X = a]
Pr[D(r) = 1] −Prz∈{0,1}t[D(ExtNW(a, z)) = 1]

≤
Pr[X ∈B] + ϵ ≤2ϵ,
where the last line used the fact that if a ̸∈B, then by deﬁnition of B,
Pr[D(r) = 1] −Prz∈{0,1}t[D(ExtNW(a, z)) = 1]
 ≤ϵ. ■
The following theorem is an immediate consequence of the above lemma.
Theorem 16.46
Fix a constant ϵ; for every N and k = NΩ(1) there is a polynomial-time computable (k, ϵ)-extractor
Ext : {0, 1}N × {0, 1}t →{0, 1}m where m = k1/3 and t = O(log N).
16.7
Applications of Extractors
Extractors are deterministic objects with strong pseudorandom properties.
We describe a few
important uses for them; many more will undoubtedly be found in future.
16.7.1
Graph constructions
An extractor is essentially a graph-theoretic object; see Figure ??. (In fact, extractors have been
used to construct expander graphs.) Think of a (k, ϵ) extractor Ext : {0, 1}N × {0, 1}t →{0, 1}m
as a bipartite graph whose left side contains one node for each string in {0, 1}N and the right side
contains a node for each string in {0, 1}m. Each node a on the left is incident to 2t edges, labelled
with strings in {0, 1}t, with the right endpoint of the edge labeled with z being Ext(a, z).
An (N, k)-source corresponds to any distribution on the left side with min-entropy at least k.
The extractor’s deﬁnition implies that picking a node according to this distribution and a random
outgoing edge gives a node on the right that is essentially uniformly distributed.
Figure unavailable in pdf ﬁle.
Figure 16.4: An extractor Ext : {0, 1}N × {0, 1}T →{0, 1}m deﬁnes a bipartite graph where every node on the left
has degree 2T .
This implies in particular that for every set X on the left side of size exactly 2k —notice, this is
a special case of an (N, k)-source— its neighbor set Γ(X) on the right satisﬁes |Γ(X)| ≥(1 −ϵ)2m.
One can in fact show a converse, that high expansion implies that the graph is an extractor;
see Chapter notes.
Web draft 2007-01-08 21:59

DRAFT
p16.32 (312)
16.7. APPLICATIONS OF EXTRACTORS
16.7.2
Running randomized algorithms using weak random sources
We now describe how to use extractors to simulate probabilistic algorithms using weak random
sources. Suppose that A(·, ·) is a probabilistic algorithm that on an input of length n uses m = m(n)
random bits, and suppose that for every x we have Prr[A(x, r) = right answer ] ≥3/4. If A’s
answers are 0/1, then such algorithms can be viewed as deﬁning a BPP language, but here we
allow a more general scenario. Suppose Ext : {0, 1}N × {0, 1}t →{0, 1}m is a (k, 1/4)-extractor.
Consider the following algorithm A′: on input x ∈{0, 1}n and given a string a ∈{0, 1}N
from the weakly random source, the algorithm enumerates all choices for the seed z and computes
A(x, Ext(a, z). Let
A′(x, a) = majority value of

A(x, Ext(a, z)) : z ∈{0, 1}t	
(19)
The running time of A′ is approximately 2t times that of A. We show that if a comes from an
(n, k + 2) source, then A′ outputs the correct answer with probability at least 3/4.
Fix the input x. Let R = {r ∈{0, 1}m : A(x, r) = right answer }, and thus |R| ≥3
42m. Let
B be the set of strings a ∈{0, 1}N for which the majority answer computed by algorithm A′ is
incorrect, namely,
B =
n
a : Prz∈{0,1}t[A(x, Ext(a, z)) = right answer] < 1/2
o
=
n
a : Prz∈{0,1}t[Ext(a, z) ∈R] < 1/2
o
claim: |B| ≤2k.
Let random variable Y correspond to picking an element uniformly at random from B. Thus Y
has min-entropy log B, and may be viewed as a (N, log B)-source. By deﬁnition of B,
Pra∈Y,z∈{0,1}t[Ext(a, z) ∈R] < 1/2.
But |R| = 3
42m, so we have
Pra∈Y,z∈{0,1}t[Ext(a, z) ∈R] −Prr∈{0,1}m[r ∈R]
 > 1/4,
which implies that the statistical distance between the uniform distribution and Ext(Y, z) is at least
1/4. Since Ext is a (k, 1/4)-extractor, Y must have min-entropy less than k. Hence |B| ≤2k and
the Claim is proved.
The correctness of the simulation now follows since
Pra∈X[A′(x, a) = right answer ] = 1 −Pra∈X[a ∈B]
≥1 −2−(k+2) · |B| ≥3/4,
(by Lemma ??).
Thus we have shown the following.
Theorem 16.47
Suppose A is a probabilistic algorithm running in time TA(n) and using m(n) random bits on
inputs of length n.
Suppose we have for every m(n) a construction of a (k(n), 1/4)-extractor
Extn : {0, 1}N × {0, 1}t(n) →{0, 1}m(n) running in TE(n) time. Then A can be simulated in time
2t(TA + TE) using one sample from a (N, k + 2) source.
Web draft 2007-01-08 21:59

DRAFT
16.7. APPLICATIONS OF EXTRACTORS
p16.33 (313)
16.7.3
Recycling random bits
We addressed the issue of recycling random bits in Section ??. An extractor can also be used to
recycle random bits. (Thus it should not be surprising that random walks on expanders, which
were used to recycle random bits in Section ??, were also used to construct extractors above.)
Suppose A be a randomized algorithm that uses m random bits. Let Ext : {0, 1}N × {0, 1}t) →
{0, 1}m be any (k, ϵ)-extractor. Consider the following algorithm. Randomly pick a string a ∈
{0, 1}N, and obtain 2t strings in {0, 1}m obtained by computing Ext(a, z) for all z ∈{0, 1}t. Run
A for all these random strings. Note that this manages to run A as many as 2t times while using
only N random bits. (For known extractor constructions, N ≪2tm, so this is a big saving.)
Now we analyse how well the error goes down. Suppose D ⊆{0, 1}m be the subset of strings
for which A gives the correct answer. Let p = |D| /2m; for a BPP algorithm p ≥2/3. Call an
a ∈{0, 1}N bad if the above algorithm sees the correct answer for less than p −ϵ fraction of z’s. If
the set of all bad a’s were to have size more than 2k, the (N, k)-source X corresponding to drawing
uniformly at random from the bad a’s would satisfy
Pr[Ext(X, Ut) ∈D] −Pr[Um ∈D] > ϵ,
which would contradict the assumption that Ext is a (k, ϵ)-extractor. We conclude that the prob-
ability that the above algorithm gets an incorrect answer from A in p −ϵ fraction of the repeated
runs is at most 2k/2N.
16.7.4
Pseudorandom generators for spacebounded computation
Now we describe Nisan’s pseudo-random generators for space-bounded randomized computation,
which allows randomized logspace computations to be run with O(log2 n) random bits.
Throughout this section we represent logspace machines by their conﬁguration graph, which has
size poly(n).
Theorem 16.48 (Nisan)
For every d there is a c > 0 and a polynomial-time computable function g :{0, 1}c log2 n →{0, 1}nd
such that for every space-bounded machine M that has a conﬁguration graph of size ≤nd on inputs
of size n:

Pr
r∈{0,1}nd[M(x, r) = 1] −
Pr
z∈{0,1}c log2 n[M(x, g(z)) = 1]
 < 1
10.
(20)
We give a proof due to Impagliazzo, Nisan, and Wigderson [INW94] (with further improvements
by Raz and Reingold [RR99]) that uses extractors. Nisan’s original paper did not explicitly use
extractors —the deﬁnition of extractors came later and was inﬂuenced by results such as Nisan’s.
In fact, Nisan’s construction proves a result stronger than Theorem 16.48: there is a polynomial-
time simulation of every algorithm in BPL using O(log2 n) space.
(See Exercises.)
Note that
Savitch’s theorem (Theorem ??) also implies that BPL ⊆SPACE(log2 n), but the algorithm
in Savitch’s proof takes nlog n time. Saks and Zhou [SZ99a] improved Nisan’s ideas to show that
BPL ⊆SPACE(log1.5 n), which leads many experts to conjecture that BPL = L (i.e., randomness
does not help logspace computations at all). (For partial progress, see Section ?? later.)
Web draft 2007-01-08 21:59

DRAFT
p16.34 (314)
16.7. APPLICATIONS OF EXTRACTORS
The main intuition behind Nisan’s construction —and also the conjecture BPL = L— is that
the logspace machine has one-way access to the random string and only O(log n) bits of memory.
So it can only “remember” O(log n) of the random bits it has seen. To exploit this we will use
the following simple lemma, which shows how to recycle a random string about which only a little
information is known. (Throughout this section, ◦denotes concatenation of strings.)
Lemma 16.49 (Recycling lemma)
Let f :{0, 1}n →{0, 1}s be any function and Ext:{0, 1}n ×{0, 1}t →{0, 1}m be a (k, ϵ/2)-extractor,
where k = n −(s + 1) −log 1
ϵ. When X ∈R {0, 1}n, W ∈R {0, 1}m, z ∈R {0, 1}t, then
f(X) ◦W ≈ϵ f(X) ◦Ext(X, z).
Remark 16.50
When the lemma is used, s ≪n and n = m. Thus f(X), which has length s, contains only a small
amount of information about X. The Lemma says that using an appropriate extractor (whose
random seed can have length as small as t = O(s + log(1/ϵ)) if we use Lemma 16.43) we can get a
new string Ext(X, z) that looks essentially random, even to somebody who knows f(X).
Proof: For v ∈{0, 1}s we denote by Xv the random variable that is uniformly distributed over
the set f−1(v). Then we can express ∥(f(X) ◦W −f(X) ◦Ext(X, z) ∥as
= 1
2
X
v,w
Pr[f(X) = v ∧W = w] −Pr
z [f(X) = v ∧Ext(X, z) = w]

=
X
v
Pr[f(X) = v]· ∥W −Ext(Xv, z) ∥
(21)
Let V =

v : Pr[f(X) = v] ≥ϵ/2s+1	
. If v ∈V , then we can view Xv as a (n, k)-source, where
k ≥n −(s + 1) −log 1
ϵ. Thus by deﬁnition of an extractor, Ext(Xv, r) ≈ϵ/2 W and hence the
contributions from v ∈V sum to at most ϵ/2. The contributions from v ̸∈V are upperbounded by
P
v̸∈V Pr[f(X) = v] ≤2s ×
ϵ
2s+1 = ϵ/2. The lemma follows. ■
Now we describe how the Recycling Lemma is useful in Nisan’s construction.
Let M be a
logspace machine. Fix an input of size n and view the graph of all conﬁgurations of M on this
input as a leveled branching program. For some d ≥1, M has ≤nd conﬁgurations and runs in time
L ≤nd. Assume without loss of generality —since unneeded random bits can always be ignored—
that it uses 1 random bit at each step. Without loss of generality (by giving M a separate worktape
that maintains a time counter), we can assume that the conﬁguration graph is leveled: it has L
levels, with level i containing conﬁgurations obtainable at time i. The ﬁrst level contains only
the start node and the last level contains two nodes, “accept” and “reject;” every other level has
W = nd nodes. Each level i node has two outgoing edges to level i + 1 nodes and the machine’s
computation at this node involves using the next bit in the random string to pick one of these two
outgoing edges. We sometimes call L the length of the conﬁguration graph and W the width.
For simplicity we ﬁrst describe how to reduce the number of random bits by a factor 2. Think
of the L steps of the computation as divided in two halves, each consuming L/2 random bits.
Suppose we use some random string X of length L/2 to run the ﬁrst half, and the machine is now
Web draft 2007-01-08 21:59

DRAFT
16.7. APPLICATIONS OF EXTRACTORS
p16.35 (315)
Figure unavailable in pdf ﬁle.
Figure 16.5: Conﬁguration graph for machine M
at node v in the middle level. The only information known about X at this point is the index of
v, which is a string of length d log n. We may thus view the ﬁrst half of the branching program
as a (deterministic) function that maps {0, 1}L/2 bits to {0, 1}d log n bits. The Recycling Lemma
allows us to use a random seed of length O(log n) to recycle X to get an almost-random string
Ext(X, z) of length L/2, which can be used in the second half of the computation. Thus we can run
L steps of computation using L/2 + O(log n) bits, a saving of almost a factor 2. Using a similar
idea recursively, Nisan’s generator runs L steps using O(log n log L) random bits.
Now we formally deﬁne Nisan’s generator.
Definition 16.51 (Nisan’s generator)
For some r > 0 let Extk :{0, 1}kr × {0, 1}r →{0, 1}kr be an extractor function for each k ≥0. For
every integer k ≥0 the associated Nisan generator Gk : {0, 1}kr →{0, 1}2k is deﬁned recursively
as (where |a| = (k −1)r, |z| = r)
Gk(a ◦z) =



z1
(i.e., ﬁrst bit of z)
k = 1
Gk−1(a) ◦Gk−1(Extk−1(a, z))
k > 1
Now we use this generator to prove Theorem 16.48. We only need to show that the probability
that the machine goes from the start node to the “accept” node is similar for truly random strings
and pseudorandom strings. However, we will prove a stronger statement involving intermediate
steps as well.
If nodes u is a node in the conﬁguration graph, and s is a string of length 2k, then we denote by
fu,2k(s) the node that the machine reaches when started in u and its random string is s. Thus if s
comes from some distribution D, we can deﬁne a distribution fu,2k(D) on nodes that are 2k levels
further from u.
Theorem 16.52
Let r = O(log n) be such that for each k ≤d log n, Extk : {0, 1}kr × {0, 1}r →{0, 1}kr is a
(kr −2d log n, ϵ)-extractor. For every machine of the type described in the previous paragraphs,
and every node u in its conﬁguration graph:
∥fu,2k(U2k) −fu,2k(Gk(Ukr)) ∥≤3kϵ,
(22)
where Ul denotes the uniform distribution on {0, 1}l.
Remark 16.53
To prove Theorem 16.48 let u = u0, the start conﬁguration, and 2k = L, the length of the entire
computation. Choose 3kϵ < 1/10 (say), which means log 1/ϵ = O(log L) = O(log n). Using the
extractor of Section 16.6.4 as Extk, we can let r = O(log n) and so the seed length kr = O(r log L) =
O(log2 n).
Web draft 2007-01-08 21:59

DRAFT
p16.36 (316)
16.7. APPLICATIONS OF EXTRACTORS
Proof: (Theorem 16.52) Let ϵk denote the maximum value of the left hand side of (22) over all
machines. The lemma is proved if we can show inductively that ϵk ≤2ϵk−1 + 2ϵ. The case k = 1
is trivial. At the inductive step, we need to upperbound the distance between two distributions
fu,2k(D1), fu,2k(D4), for which we introduce two distributions D2, D3 and use triangle inequality:
∥fu,2k(D1) −fu,2k(D4) ∥≤
3
X
i=1
∥fu,2k(Di) −fu,2k(Di+1) ∥.
(23)
The distributions will be:
D1 = U2k
D4 = Gk(Ukr)
D2 = U2k−1 ◦Gk−1(U(k−1)r)
D3 = Gk−1(U(k−1)r) ◦Gk−1(U′
(k−1)r)
(U, U′ are identical but independent).
We bound the summands in (23) one by one.
Claim 1: ∥fu,2k(D1) −fu,2k(D2) ∥≤ϵk−1.
Denote Pr[fu,2k−1(U2k−1) = w] by pu,w and Pr[fu,2k−1(Gk−1(U(k−1)r)) = w] by qu,w. According to
the inductive assumption,
1
2
X
w
|pu,w −qu,w| =∥fu,2k−1(U2k−1) −fu,2k−1(Gk−1(U(k−1)r)) ∥≤ϵk−1.
Since D1 = U2k may be viewed as two independent copies of U2k−1 we have
∥fu,2k(D1) −fu,2k(D2) ∥=
X
v
1
2

X
w
puwpwv −
X
w
puwqwv

where w, v denote nodes 2k−1 and 2k levels respectively from u
=
X
w
puw
1
2
X
v
|pwv −qwv|
≤ϵk−1
(using inductive hypothesis and
X
w
puw = 1)
Claim 2: ∥fu,2k(D2) −fu,2k(D3) ∥≤ϵk−1.
The proof is similar to the previous case.
Claim 3: ∥fu,2k(D3) −fu,2k(D4) ∥≤2ϵ.
We use the Recycling Lemma. Let gu :{0, 1}(k−1)r →[1, W] be deﬁned as gu(a) = fu,2k−1(Gk−1(a)).
(To put it in words, apply the Nisan generator to the seed a and use the result as a random string
for the machine, using u as the start node. Output the node you reach after 2k−1 steps.) Let
X, Y ∈U(k−1)r and z ∈Ur. According to the Recycling Lemma,
gu(X) ◦Y ≈ϵ gu(X) ◦Extk−1(X, z),
Web draft 2007-01-08 21:59

DRAFT
16.7. APPLICATIONS OF EXTRACTORS
p16.37 (317)
and then part 3 of Lemma 16.39 implies that the equivalence continues to hold if we apply a
(deterministic) function to the second string on both sides. Thus
gu(X) ◦gw(Y ) ≈ϵ gu(X) ◦gw(Extk−1(X, z))
for all nodes w that are 2k−1 levels after u. The left distribution corresponds to fu,2k(D3) (by which
we mean that Pr[fu,2k(D3) = v] = P
w Pr[gu(X) = w ∧gw(Y ) = v]) and the right one to fu,2k(D4)
and the proof is completed. ■
Chapter notes and history
The results of this section have not been presented in chronological order and some important
intermediate results have been omitted. Yao [Yao82] ﬁrst pointed out that cryptographic pseudo-
random generators can be used to derandomize BPP. A short paper of Sipser [Sip88] initiated
the study of “hardness versus randomness,” and pointed out the usefulness of a certain family of
highly expanding graphs that are now called dispersers (they are reminiscent of extractors). This
research area received its name as well as a thorough and brilliant development in a paper of Nisan
and Wigderson [NW94]. missing discussion of followup works to NW94
Weak random sources were ﬁrst considered in the 1950s by von Neumann [von61]. The second
volume of Knuth’s seminal work studies real-life pseudorandom generators and their limitations.
The study of weak random sources as deﬁned here started with Blum [Blu84]. Progressively weaker
models were then deﬁned, culminating in the “correct” deﬁnition of an (N, k) source in Zucker-
man [Zuc90]. Zuckerman also observed that this deﬁnition generalizes all models that had been
studied to date. (See [SZ99b] for an account of various models considered by previous researchers.)
He also gave the ﬁrst simulation of probabilistic algorithms with such sources assuming k = Ω(N).
A succession of papers has improved this result; for some references, see the paper of Lu, Rein-
gold, Vadhan, and Wigderson [LRVW03], the current champion in this area (though very likely
dethroned by the time this book appears).
The earliest work on extractors —in the guise of leftover hash lemma of Impagliazzo, Levin,
and Luby [ILL89] mentioned in Section 16.6.3— took place in context of cryptography, speciﬁcally,
cryptographically secure pseudorandom generators. Nisan [Nis92] then showed that hashing could
be used to deﬁne provably good pseudorandom generators for logspace.
The notion of an extractor was ﬁrst formalized by Nisan and Zuckerman [NZ96]. Trevisan [Tre01]
pointed out that any “black-box” construction of a pseudorandom generator gives an extractor, and
in particular used the Nisan-Wigderson generator to construct extractors as described in the chap-
ter. His methodology has been sharpened in many other papers (e.g.,see [LRVW03]).
Our discussion of derandomization has omitted many important papers that successively im-
proved Nisan-Wigderson and culminated in the result of Impagliazzo and Wigderson [IW01]that
either NEXP = BPP (randomness is truly powerful!) or BPP has an a subexponential “simula-
tion.” 7 Such results raised hopes that we were getting close to at least a partial derandomization
of BPP, but these hopes were dashed by the Impagliazzo-Kabanets [KI03] result of Section 16.3.
7The “simulation” is in quotes because it could fail on some instances, but ﬁnding such instances itself requires
exponential computational power, which nature presumably does not have.
Web draft 2007-01-08 21:59

DRAFT
p16.38 (318)
16.7. APPLICATIONS OF EXTRACTORS
Trevisan’s insight about using pseudorandom generators to construct extractors has been greatly
extended. It is now understood that three combinatorial objects studied in three diﬀerent ﬁelds
are very similar: pseudorandom generators (cryptography and derandomization), extractors (weak
random sources) and list-decodable error-correcting codes (coding theory and information theory).
Constructions of any one of these objects often gives constructions of the other two. For a survey,
see Vadhan’s lecture notes [?].
still a lot missing
Expanders were well-studied for a variety of reasons in the 1970s but their application to
pseudorandomness was ﬁrst described by Ajtai, Komlos, and Szemeredi [AKS87]. Then Cohen-
Wigderson [CW89] and Impagliazzo-Zuckerman (1989) showed how to use them to “recycle” ran-
dom bits as described in Section 7.B.3. The upcoming book by Hoory, Linial and Wigderson (draft
available from their web pages) provides an excellent introduction to expander graphs and their
applications.
The explicit construction of expanders is due to Reingold, Vadhan and Wigderson [RVW00],
although we chose to present it using the replacement product as opposed to the closely related
zig-zag product used there. The deterministic logspace algorithm for undirected connectivity is due
to Reingold [?].
Exercises
§1 Verify Corollary 16.6.
§2 Show that there exists a number ϵ > 0 and a function G : {0, 1}∗→{0, 1}∗that satisﬁes all of
the conditions of a 2ϵn-pseudorandom generator per Deﬁnition ??, save for the computational
eﬃciency condition.
Hint: show that if for every n, a random function mapping n bits
to 2n/10 bits will have desired properties with high probabilities.
§3 Show by a counting argument (i.e., probabilistic method) that for every large enough n there
is a function f :{0, 1}n →{0, 1}, such that Havg(f) ≥2n/10.
§4 Prove that if there exists f ∈E and ϵ > 0 such that Havg(f)(n) ≥2ϵn for every n ∈N, then
MA = NP.
§5 We deﬁne an oracle Boolean circuit to be a Boolean circuit that have special gates with
unbounded fanin that are marked ORACLE. For a Boolean circuit C and language O ⊆{0, 1}∗,
we deﬁne by CO(x) the output of C on x, where the operation of the oracle gates when fed
input q is to output 1 iﬀq ∈O.
(a) Prove that if every f ∈E can be computed by a polynomial-size circuits with oracle to
SAT, then the polynomial hierarchy collapses.
(b) For a function f : {0, 1}∗→{0, 1} and O ⊆{0, 1}∗, deﬁne HavgO(f) to be the function
that maps every n ∈N to the largest S such that Prx∈R{0,1}n[CO(x) = f(x)] ≤1/2+1/S.
Web draft 2007-01-08 21:59

DRAFT
16.7. APPLICATIONS OF EXTRACTORS
p16.39 (319)
§6 Prove Lemma 16.39.
§7 Prove that for every function Ext : {0, 1}n →{0, 1}m and there exists an (n, n −1)-source X
and a bit b ∈{0, 1} such that Pr[Ext(X)1 = b] = 1 (where Ext(X)1 denotes the ﬁrst bit of
Ext(X)). Prove that this implies that δ(Ext(X), Um) ≥1/2.
§8 Show that there is a constant c > 0 such that if an algorithm runs in time T and requires
m random bits, and m > k + c log T, then it is not possible in general to simulate it in a
blackbox fashion using an (N, k) source and O(log n) truly random bits.
Hint: For each source show that there is a randomized algorithm
—it need not be eﬃcient, since it is being used as a “black box”—
for which the simulation fails.
§9 A ﬂat (N, k) source is a (N, k) source where for every x ∈{0, 1}N px is either 0 or exactly
2−k.
Show that a source X is an (N, k)-source iﬀit is a distribution on ﬂat sources. In other words,
there is a set of ﬂat (N, k)-sources X1, X2, . . . and a distribution D on them such that drawing
a sample of X corresponds to picking one of the Xi’s according to D, and then drawing a
sample from Xi.
Hint:
You need to view a distribution as a point in a 2N-
dimensional space, and show that X is in the convex hull of the
points that represent all possible ﬂat sources.
§10 Use Nisan’s generator to give an algorithm that produces universal traversal sequences for
n-node graphs (see Deﬁnition ??) in nO(log n)-time and O(log2 n) space.
§11 Suppose boolean function f is (S, ϵ)-hard and let D be the distribution on m-bit strings deﬁned
by picking inputs x1, x2, . . . , xm uniformly at random and outputting f(x1)f(x2) · · · f(xm).
Show that the statistical distance between D and the uniform distribution is at most ϵm.
§12 Prove Lemma 16.42.
§13 (Klivans and van Melkebeek 1999) Suppose the conclusion of Lemma ?? is true. Then show
that MA ⊆i.o.−[NTIME(2n)/n].
(Slightly harder) Show that if NEXP ̸= EXP then AM ⊆i.o.−[NTIME(2n)/n].
§14 Let A be an n × n matrix with eigenvectors u1, . . . , un and corresponding values λ1, . . . , λn.
Let B be an m×m matrix with eigenvectors v1, . . . , vm and corresponding values α1, . . . , αm.
Prove that the matrix A ⊗B has eigenvectors ui ⊗vj and corresponding values λi · αj.
§15 Prove that for every two graphs G, G′, λ(G ⊗G′) ≤λ(G) + λ(G′) without using the fact that
every symmetric matrix is diagonalizable.
Hint: Use Lemma 7.40.
Web draft 2007-01-08 21:59

DRAFT
p16.40 (320)
16.7. APPLICATIONS OF EXTRACTORS
§16 Let G be an n-vertex D-degree graph with ρ combinatorial edge expansion for some ρ > 0.
(That is, for every a subset S of G’s vertices of size at most n/2, the number of edges
between S and its complement is at least ρd|S|.) Let G′ be a D-vertex d-degree graph with
ρ′ combinatorial edge expansion for some ρ′ > 0. Prove that G ⃝
R G′ has at least ρ2ρ′/1000
edge expansion.
Hint: Every subset of G ⃝
R G′ can be thought of as n subsets of
the individual clusters. Treat diﬀerently the subsets that take up
more than 1 −ρ/10 portion of their clusters and those that take
up less than that. For the former use the expansion of G, while for
the latter use the expansion of G′.
Acknowledgements
We thank Luca Trevisan for cowriting an early draft of this chapter. Thanks also to Valentine
Kabanets, Omer Reingold, and Iannis Tourlakis for their help and comments.
Web draft 2007-01-08 21:59

DRAFT
Chapter 17
Hardness Ampliﬁcation and Error
Correcting Codes
We pointed out in earlier chapters (e.g., Chapter ?? the distinction between worst-case hardness
and average-case hardness. For example, the problem of ﬁnding the smallest factor of every given
integer seems diﬃcult on worst-case instances, and yet is trivial for at least half the integers –
namely, the even ones. We also saw that functions that are average-case hard have many uses,
notably in cryptography and derandomization.
In this chapter we study techniques for amplifying hardness. First, we see Yao’s XOR Lemma,
which transforms a “mildly hard” function (i.e., one that is hard to compute on a small fraction
of the instances) to a function that is extremely hard, for which the best algorithm is as bad as
the algorithm that just randomly guesses the answer. We mentioned Yao’s result in the chapter
on cryptography as a means to transform weak one-way functions into strong one-way functions.
The second result in this chapter is a technique to use error-correcting codes to transform worst-
case hard functions into average-case hard functions. This transformation unfortunately makes the
running time exponential, and is thus useful only in derandomization, and not in cryptography.
In addition to their applications in complexity theory, the ideas covered here have had other
uses, including new constructions of error-correcting codes and new algorithms in machine learning.
17.1
Hardness and Hardness Ampliﬁcation.
We now deﬁne a slightly more reﬁned notion of hardness, that generalizes both the notions of
worst-case and average-case hardness given in Deﬁnition 16.7:
Definition 17.1 (Hardness)
Let f : {0, 1}∗→{0, 1} and ρ : N →[0, 1].
We deﬁne Hρ
avg(f) to be the func-
tion from N to N that maps every number n to the largest number S such that
Prx∈R{0,1}n[C(x) = f(x)] < ρ(n) for every Boolean circuit C on n inputs with size
at most S.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p17.1 (321)

DRAFT
p17.2 (322)
17.2. MILD TO STRONG HARDNESS: YAO’S XOR LEMMA.
Note that, in the notation of Deﬁnition 16.7, Hwrs(f) = H1
avg(f) and Havg(f)(n) = max

S : H1/2+1/S
avg
(f)(n) ≥S
	
.
In this chapter we show the following results for every two functions S, S′ : N →N:
Worst-case to mild hardness. If there is a function f ∈E = DTIME(2O(n)) such that Hwrs(f)(n) =
H1
avg(f)(n) ≥S(n) then there is a function f′ ∈E such that H0.99
avg (f)(n) ≥S(ϵn)ϵ for some
constant ϵ > 0 and every suﬃciently large n.
Mild to strong hardness. If f′ ∈E satisﬁes H0.99
avg (f′)(n) ≥S′(n) then there is f′′ ∈E and ϵ > 0
such that Havg(f′′)(n) ≥S′(nϵ)ϵ.
Combining these two results with Theorem 16.10, this implies that if there exists a function
f ∈E with Hwrs(f)(n) ≥S(n) then there exists an S(ℓϵ)ϵ-pseudorandom generator for some ϵ > 0,
and hence:
Corollary 1 If there exists f ∈E and ϵ > 0 such that Hwrs(f) ≥2nϵ then BPP ⊆QuasiP =
∪cDTIME(2log nc).
Corollary 2 If there exists f ∈E such that Hwrs(f) ≥nω(1) then BPP ⊆SUBEXP = ∩ϵDTIME(2nϵ).
To get to BPP = P, we need a stronger transformation.
We do this by showing how to
transform in one fell swoop, a function f ∈E with Hwrs(f) ≥S(n) into a function f′ ∈E with
Havg(f) ≥S(ϵn)ϵ for some ϵ > 0. Combined with Theorem 16.10, this implies that BPP = P if
there exists f ∈E with Hwrs(f) ≥2Ω(n).
17.2
Mild to strong hardness: Yao’s XOR Lemma.
We start with the second result described above: transforming a function that has “mild” average-
case hardness to a function that has strong average-case hardness. The transformation is actually
quite simple and natural, but its analysis is somewhat involved (yet, in our opinion, beautiful).
Theorem 17.2 (Yao’s XOR Lemma)
For every f : {0, 1}n →{0, 1} and k ∈N, deﬁne f⊕k : {0, 1}nk →{0, 1} as follows:
f⊕k(x1, . . . , xk) = Pk
i=1 f(xi) (mod 2).
For every δ > 0, S and ϵ > 2(1 −δ/2)k, if H1−δ
avg (f) ≥S then
H1/2+ϵ
avg
(f⊕k) ≥
ϵ2
100 log(1/δϵ)S
The intuition behind Theorem 17.2 derives from the following fact. Suppose we have a biased
coin that, whenever it is tossed, comes up heads with probability 1 −δ and tails with probability
δ. If δ is small, each coin toss is fairly predictable. But suppose we now toss it k times and deﬁne
a composite coin toss that is “heads” iﬀthe coin came up heads an odd number of times. Then
the probability of “heads” in this composite coin toss is at most 1/2 + (1 −2δ)k (see Exercise 1),
which tends to 1/2 as k increases. Thus the parity of coin tosses becomes quite unpredictable. The
Web draft 2007-01-08 21:59

DRAFT
17.2. MILD TO STRONG HARDNESS: YAO’S XOR LEMMA.
p17.3 (323)
analogy to our case is that intuitively, for each i, a circuit of size S has chance at most 1 −δ of
“knowing” f(xi) if xi is random. Thus from its perspective, whether or not it will be able to know
f(xi) is like a biased coin toss. Hence its chance of guessing the parity of the k bits should be
roughly like 1/2 + (1 −2δ)k.
We transform this intuition into a proof via an elegant result of Impagliazzo, that provides some
fascinating insight on mildly hard functions.
Definition 17.3 (δ-density distribution)
For δ < 1 a δ-density distribution H over {0, 1}n is one such that for every x ∈{0, 1}n, Pr[H =
x] ≤2−n
δ .
Remark 17.4
Note that in Chapter 16 we would have called it a distribution with min entropy n −log 1/δ.
The motivating example for this deﬁnition is the distribution that is uniform over some subset
of size δ2n and has 0 probability outside this set.
A priori, one can think that a function f that is hard to compute by small circuits with
probability 1 −δ could have two possible forms: (a) the hardness is sort of “spread” all over the
inputs, and it is roughly 1 −δ-hard on every signiﬁcant set of inputs or (b) there is a subset H
of roughly a δ fraction of the inputs such that on H the function is extremely hard (cannot be
computed better than 1
2 +ϵ for some tiny ϵ) and on the rest of the inputs the function may be even
very easy. Such a set may be thought of as lying at the core of the hardness of f and is sometimes
called the hardcore set. Impagliazzo’s Lemma shows that actually every hard function has the form
(b). (While the Lemma talks about distributions and not sets, one can easily transform it into a
result on sets.)
Lemma 17.5 (Impagliazzo’s Hardcore Lemma)
For every δ > 0, f : {0, 1}n →{0, 1}n, and ϵ > 0, if H1−δ
avg (f) ≥S then there exists a distribution H
over {0, 1}n of density at least δ/2 such that for every circuit C of size at most
ϵ2S
100 log(1/δϵ),
Pr
x∈RH[C(x) = f(x)] ≤1/2 + ϵ ,
Proof of Yao’s XOR Lemma using Impagliazzo’s Hardcore Lemma.
We now use Lemma 17.5 to transform the biased-coins intuition discussed above into a proof of the
XOR Lemma. Let f : {0, 1}n →{0, 1} be a function such that H1−δ
avg (f) ≥S, let k ∈N and suppose,
for the sake of contradiction, that there is a circuit C of size
ϵ2
100 log(1/δϵ)S such that
Pr
(x1,...,xk)∈RUk
n
"
C(x1, . . . , xk) =
k
X
i=1
f(xi)
(mod 2)
#
≥1/2 + ϵ ,
(1)
where ϵ > 2(1 −δ/2)k.
Let H be the hardcore distribution of dens ity at least δ′ = δ/2 that is obtained from Lemma 17.5,
on which every circuit C′ fails to compute f with probability better than 1/2 + ϵ/2. Deﬁne a dis-
tribution G over {0, 1}n as follows: for every x ∈{0, 1}n, Pr[G = x] = (1 −δ′ Pr[H = x])/(1 −δ′).
Web draft 2007-01-08 21:59

DRAFT
p17.4 (324)
17.3. PROOF OF IMPAGLIAZZO’S LEMMA
Note that G is indeed a well-deﬁned distribution, as H has density at least δ′. Also note that if H
was the uniform distribution over some subset of {0, 1}n of size δ′2n, then G will be the uniform
distribution over the complement of this subset.
We can think of the process of picking a uniform element in {0, 1}n as follows: ﬁrst toss a
δ′-biased coin that comes up “heads” with probability δ. Then, if it came up “heads” choose a
random element out of H, and with probability 1 −δ′, and otherwise choose a random element out
of G. We shorthand this and write
Un = (1 −δ′)G + δ′H .
(2)
If we consider the distribution (Un)2 of picking two random strings, then by (2) it can be written
as (1 −δ′)2G2 + (1 −δ′)δ′GH + δ′(1 −δ′)HG + δ′2H2. Similarly, for every k
(Un)k = (1 −δ′)kGk + (1 −δ′)k−1δ′Gk−1H + · · · + δ′kHk .
(3)
For every distribution D over {0, 1}nk let PD be the probability of the event of the left-hand side of
(1) that C(x1, . . . , xk) = Pk
i=1 f(xi) (mod 2) where x1, . . . , xk are chosen from D. Then, combining
(1) and (3),
1/2 + ϵ ≤P(Un)k = (1 −δ′)kPGk + (1 −δ′)k−1δ′PGk−1H + · · · + δ′kPHk .
But since δ′ = δ/2 and ϵ > 2(1 −δ/2)k and PGk ≤1 we get
1/2 + ϵ/2 ≤1/2 + ϵ −(1 −δ′)k ≤(1 −δ′)k−1δ′PGk−1H + · · · + δ′kPHk .
Notice, the coeﬃcients of all distributions on the right hand side sum up to less than one, so there
must exist a distribution D that has at least one H component such that PD ≥1/2 + ϵ/2. Suppose
that D = Gk−1H (all other cases are handled in a similar way). Then, we get that
Pr
X1,...,Xk−1∈RG,Xk∈RH[C(X1, . . . , Xk−1, Xk) =
k
X
i=1
f(Xi)
(mod 2)] ≥1/2 + ϵ/2 .
(4)
By the averaging principle, (4) implies that there exist k −1 strings x1, . . . , xk−1 such that if
b = Pk−1
i=1 f(xi) (mod 2) then,
Pr
Xk∈RH[C(x1, . . . , xk−1, Xk) = b + f(Xk)
(mod 2)] ≥1/2 + ϵ/2 .
(5)
But by “hardwiring” the values x1, . . . , xk and b into the circuit C, (5) shows a direct contradiction
to the fact that H is a hardcore distribution for the function f. ■
17.3
Proof of Impagliazzo’s Lemma
Let f be a function with H1−δ
avg (f) ≥S. To Prove Lemma 17.5 we need to show a distribution H over
{0, 1}n (with no element of weight more than 2 · 2−n/δ) on which every circuit C of size S′ cannot
compute f with probability better than 1/2 + ϵ (where S′,ϵ are as in the Lemma’s statement).
Web draft 2007-01-08 21:59

DRAFT
17.3. PROOF OF IMPAGLIAZZO’S LEMMA
p17.5 (325)
Let’s think of this task as a game between two players named Russell and Noam. Russell ﬁrst
sends to Noam some distribution H over {0, 1}n with density at least δ. Then Noam sends to
Russell some circuit C of size at most S′. Russell then pays to Noam Ex∈RH[RightC(x)] dollars,
where RightC(x) is equal to 1 if C(x) = f(x) and equal to 0 otherwise. What we need to prove is
that there is distribution that Russell can choose, such that no matter what circuit Noam sends,
Russell will not have to pay him more than 1/2 + ϵ dollars.
An initial observation is that Russell could have easily ensured this if he was allowed to play
second instead of ﬁrst. Indeed, under our assumptions, for every circuit C of size S (and so, in
particular also for circuits of size S′ which is smaller than S), there exists a set SC of at least
δ2n ≥(δ/2)2n inputs such that C(x) ̸= f(x) for every x ∈SC. Thus, if Noam had to send his
circuit C, then Russell could have chosen H to be the uniform distribution over SC. Thus H would
have density at least δ/2 and Ex∈RH[RightC(x)] = 0, meaning that Russell wouldn’t have to pay
Noam a single cent.
Now this game is a zero sum game, since whatever Noam gains Russell loses and vice versa,
tempting us to invoke von-Neumann’s famous Min-Max Theorem (see Note 17.7) that says that
in a zero-sum game it does not matter who plays ﬁrst as long as we allow randomized strategies.1
What does it mean to allow randomized strategies in our context?
It means that Noam can
send a distribution C over circuits instead of a single circuit, and the amount Russell will pay is
EC∈RCEx∈RH[RightC(x)]. (It also means that Russell is allowed to send a distribution over δ/2-
density distributions, but this is equivalent to sending a single δ/2-density distribution.)
Thus, we only need to show that, when playing second, Russell can still ensure a payment
of at most 1/2 + ϵ dollars even when Noam sends a distribution C of S′-sized circuits. For every
distribution C, we say that an input x ∈{0, 1}n is good for Noam (good for short) with respect to
C if EC∈RC[RightC(x)] ≥1/2 + ϵ. It suﬃces to show that for every distribution C over circuits of size
at most S′, the number of good x’s with respect to C is at most 1 −δ/2. (Indeed, this means that
for every C, Russell could choose as its distribution H the uniform distribution over the bad inputs
with respect to C.)
Suppose otherwise, that there is at least a 1 −δ/2 fraction of inputs that are good for C. We
will use this to come up with an S-sized circuit C that computes f on at least a 1 −δ fraction
of the inputs in {0, 1}n, contradicting the assumption that H1−δ
avg (f) ≥S. Let t = 10 log(1/δϵ)/ϵ2,
choose C1, . . . , Ct at random from C and let C = maj{C1, . . . , Ct} be the circuit of size tS′ < S
circuit that on input x outputs the majority value of {C1(x), . . . , Ct(x)}. If x is good for C, then
by the Chernoﬀbound we have that C(x) = f(x) with probability at least 1 −δ/2 over the choice
of C1, . . . , Ct. Since we assume at least 1 −δ/2 of the inputs are good for C, we get that
Ex∈R{0,1}nEC1∈RC,...,Ct∈RC[Rightmaj{C1,...,Ct}(x)] ≥(1−δ
2)(1−δ
2) ≥1 −δ .
(6)
But by linearity of expectation, we can switch the order of expectations in (6) obtaining that
EC1∈RC,...,Ct∈RCEx∈R{0,1}n[Rightmaj{C1,...,Ct}(x)] ≥1 −δ ,
1The careful reader might note that another requirement is that the set of possible moves by each player is ﬁnite,
which does not seem to hold in our case as Russell can send any one of the inﬁnitely many δ/2-density distributions.
However, by either requiring that the probabilities of the distribution are multiples of
ϵ
100·2n (which won’t make any
signiﬁcant diﬀerence in the game’s outcome), or using the fact that each such distribution is a convex sum of uniform
distributions over sets of size at least (δ/2)2n (see Exercise 9 of Chapter 16), we can make this game ﬁnite.
Web draft 2007-01-08 21:59

DRAFT
p17.6 (326)
17.3. PROOF OF IMPAGLIAZZO’S LEMMA
which in particular implies that there exists a circuit C of size at most S such that Ex∈RUn[RightC(x)] ≥
1 −δ, or in other words, C computes f on at least a 1 −δ fraction of the inputs. ■
Remark 17.6
Taken in the contrapositive, Lemma 17.5 implies that if for every signiﬁcant chunk of the inputs
there is some circuit that computes f with on this chunk with some advantage over 1/2, then there
is a single circuit that computes f with good probability over all inputs. In machine learning such
a result (transforming a way to weakly predict some function into a way to strongly predict it) is
called Boosting of learning methods. Although the proof we presented here is non-constructive,
Impagliazzo’s original proof was constructive, and was used to obtain a boosting algorithm yielding
some new results in machine learning, see [?].
Web draft 2007-01-08 21:59

DRAFT
17.3. PROOF OF IMPAGLIAZZO’S LEMMA
p17.7 (327)
Note 17.7 (The Min-Max Theorem)
A zero sum game is, as the name implies, a game between two parties in
which whatever one party loses is won by the other party. It is modeled
by an m × n matrix A = (ai,j) of real numbers. The game consists of only
a single move. One party, called the minimizer or column player, chooses
an index j ∈[n] while the other party, called the maximizer or row player,
chooses an index i ∈[m]. The outcome is that the column player has to pay
ai,j units of money to the row player (if ai,j is negative then actually the row
player has to pay). Clearly, the order in which players make their moves is
important. Surprisingly, if we allow the players randomized strategies, then
the order of play becomes unimportant.
The game with randomized (also known as mixed) strategies is as follows.
The column player chooses a distribution over the columns; that is, a vector
p ∈[0, 1]n with Pn
i=1 pi = 1. Similarly, the row player chooses a distribution
q over the rows. The amount paid is the expectation of ai,j for j chosen from
p and i chosen from q. If we think of p as a column vector and q as a row
vector then this is equal to qAp. The min-max theorem says:
min
p∈[0,1]n
Σipi=1
max
q∈[0,1]m
Σiqi=1
qAp =
max
q∈[0,1]m
Σiqi=1
min
p∈[0,1]n
Σipi=1
qAp
(7)
The min-max theorem can be proven using the following result, known as
Farkas’ Lemma:2 if C and D are disjoint convex subsets of Rm, then there
is an m −1 dimensional hyperplane that separates them. That is, there is
a vector z and a number a such that for every x ∈C, ⟨x, z⟩= P
i xizi ≤a
and for every y ∈D, ⟨y, z⟩≥a.
(A subset C ⊆Rm is convex if
whenever it contains a pair of points x, y, it contains the line segment
{αx + (1 −α)y : 0 ≤α ≤1} that lies between them.) We ask you to prove
Farkas’ Lemma in Exercise 2 but here is a “proof by picture” for the two
dimensional case:
C
D
hyperplane
Farkas’
Lemma
implies
the
min-max
theorem
by
noting
that
maxq minp qAp
≥
c
if
and
only
if
the
convex
set
D
=
{Ap : p ∈[0, 1]n P
i pi = 1}
does
not
intersect
with
the
convex
set
C =

x ∈Rm : ∀i∈[m]xi < c
	
and using the Lemma to show that this
implies the existence of a probability vector q such that ⟨q, y⟩≥c for every
y ∈D (see Exercise 3). The Min-Max Theorem is equivalent to another
well-known result called linear programming duality, that can also be proved
using Farkas’ Lemma (see Exercise 4).
Web draft 2007-01-08 21:59

DRAFT
p17.8 (328)
17.4. ERROR CORRECTING CODES: THE INTUITIVE CONNECTION TO HARDNESS
AMPLIFICATION
17.4
Error correcting codes: the intuitive connection to hardness
ampliﬁcation
Now we construct average-case hard functions using functions that are only worst-case hard. To do
so, we desire a way to transform any function f to another function g such that if there is a small
circuit that computes g approximately (i.e., correctly outputs g(x) for many x) then there is a small
circuit that computes f at all points. Taking the contrapositive, we can conclude that if there is
no small circuit that computes f then there is no small circuit that computes g approximately.
Let us reason abstractly about how to go about the above task.
View a function f : {0, 1}n →{0, 1} as its truth table, namely, as a string of length 2n, and
view any circuit C for computing this function as a device that, given any index x ∈[2n], gives the
x’th bit in this string. If the circuit only computes g on ”average” then this device may be thought
of as only partially correct; it gives the right bit only for many indices x’s, but not all. Thus we
need to show how to turn a partially correct string for g into a completely correct string for f.
This is of course reminiscent of error correcting codes (ECC), but with a distinct twist involving
computational eﬃciency of decoding, which we will call local decoding.
The classical theory of ECC’s (invented by Shannon in 1949) concerns the following problem.
We want to record some data x ∈{0, 1}n on a compact disk to retrieve at a later date, but that
compact disk might scratched and say 10% of its contents might be corrupted. The idea behind
error correcting codes is to encode x using some redundancy so that such corruptions do not prevent
us from recovering x.
The naive idea of redundancy is to introduce repetitions but that does not work. For example
suppose we repeat each bit three times, in other words encode x as the string y = x1x1x1x2x2x2 . . . xnxnxn.
But now if the ﬁrst three coordinates of y are corrupted then we cannot recover x1, even if all other
coordinates of y are intact. (Note that the ﬁrst three coordinates take only a 1/n ≪10% fraction
of the entire string y.) Clearly, we need a smarter way.
Definition 17.8 (Error Correcting Codes)
For x, y ∈{0, 1}m, the fractional Hamming distance of x and y, denoted ∆(x, y), is
equal to
1
m |{i : xi ̸= yi}|.
For every δ ∈[0, 1], a function E : {0, 1}n →{0, 1}m is an error correcting code
(ECC) with distance δ, if for every x ̸= y ∈{0, 1}n, ∆(E(x), E(y)) ≥δ. We call the
set Im(E) = {E(x) : x ∈{0, 1}n} the set of codewords of E.
Suppose E : {0, 1}n →{0, 1}m is an ECC of distance δ > 0.2. Then the encoding x →E(x)
suﬃces for the CD storage problem (momentarily ignoring issues of computational eﬃciency).
Indeed, if y is obtained by corrupting 0.1m coordinates of E(x), then ∆(y, E(x)) < δ/2 and by the
triangle inequality ∆(y, E(x′)) > δ/2 for every x′ ̸= x. Thus, x is the unique string that satisﬁes
2Many texts use the name Farkas’ Lemma only to denote a special case of the result stated in Note 17.7. Namely
the result that there is a separating hyperplane between any disjoint sets C, D such that C is a single point and D
is a set of the form {Ax : ∀ixi > 0} for some matrix A.
Web draft 2007-01-08 21:59

DRAFT
17.4. ERROR CORRECTING CODES: THE INTUITIVE CONNECTION TO HARDNESS
AMPLIFICATION
p17.9 (329)
δ/2
δ/2
E(x)
E(x’)
E(x’’)
y
Figure 17.1: In a δ-distance error correcting code, ∆(E(x), E(x′)) ≥δ for every x ̸= x′. We can recover x from
every string y satisfying ∆(y, E(x)) < δ/2 since the δ/2-radius ball around every codeword z = E(x) does not contain
any other codeword.
∆(y, E(x)) < δ/2. (See Figure 17.1.)
Of course, we still need to show that error correcting codes with minimum distance 0.2 actually
exist. The following lemma shows this. It introduces H(δ), the so-called entropy function, which
lies strictly between 0 and 1 when δ ∈(0, 1).
Lemma 17.9
For every δ < 1/2 and suﬃciently large n, there exists a function E : {0, 1}n →{0, 1}2n/(1−H(δ))
that is an error correcting code with distance δ, where H(δ) = δ log(1/δ) + (1 −δ) log(1/(1 −δ)).
Proof: We simply choose the function E : {0, 1}n →{0, 1}m at random for m = 2n/(1 −H(δ)n.
That is, we choose 2n random strings y1, y2, . . . , y2n and E will map the input x ∈{0, 1}n (which
we can identify with a number in [2n]) to the string yx.
It suﬃces to show that the probability that for some i < j with i, j ∈[2n], ∆(yi, yj) < δ is less
than 1. But for every string yi, the number of strings that are of distance at most δ to it is
 m
⌈δm ⌉

which at most 0.99 · 2H(δ)m for m suﬃciently large (see Appendix A) and so for every j > i, the
probability that yj falls in this ball is bounded by 0.99 · 2H(δ)m/2m. Since there are at most 22n
such pairs i, j, we only need to show that
0.99 · 22n 2H(δ)m
2m
< 1 .
which is indeed the case for our choice of m. ■
Remark 17.10
By a slightly more clever argument, we can get rid of the constant 2 above, and show that there
exists such a code E : {0, 1}n →{0, 1}n/(1−H(δ)) (see Exercise 6). We do not know whether this is
the smallest value of m possible.
Why half?
Lemma 17.9 only provides codes of distance δ for δ < 1/2 and you might wonder
whether this is inherent or can we have codes of even greater distance. It turns out we can have
codes of distance 1/2 but only if we allow m to be exponentially larger than n (i.e., m ≥2n/2). For
Web draft 2007-01-08 21:59

DRAFT
p17.10 (330)
17.4. ERROR CORRECTING CODES: THE INTUITIVE CONNECTION TO HARDNESS
AMPLIFICATION
every δ > 1/2, if n is suﬃciently large then there is no ECC E : {0, 1}n →{0, 1}m that has distance
δ, no matter how large m is. Both these bounds are explored in Exercise 7.
The mere existence of an error correcting code is not suﬃcient for most applications: we need
to actually be able to compute them. For this we need to show an explicit function E : {0, 1}n →
{0, 1}m that is an ECC satisfying the following properties:
Eﬃcient encoding There is a polynomial time algorithm to compute E(x) from x.
Eﬃcient decoding There is a polynomial time algorithm to compute x from every y such that
∆(y, E(x)) < ρ for some ρ. (For this to be possible, the number ρ must be less than δ/2,
where δ is the distance of E.)
There is a very rich and still ongoing body of work dedicated to this task, of which Section 17.5
describes a few examples.
17.4.1
Local decoding
For use in hardness ampliﬁcation, we need ECCs with more than just eﬃcient encoding and decoding
algorithms: we need local decoders, in other words, decoding algorithms whose running time is
polylogarithmic. Let us see why.
Recall that we are viewing a function from {0, 1}n to {0, 1} as a string of length 2n. To amplify
its hardness, we take an ECC and map function f to its encoding E(f). To prove that this works,
it suﬃces to show how to turn any circuit that correctly computes many bits of E(f) into a circuit
that correctly computes all bits of f. This is formalized using a local decoder, which is a decoding
algorithm that can compute any desired bit in the string for f using a small number of random
queries in any string y that has high agreement with (in other words, low hamming distance to)
E(f). Since we are interested in the circuits of size poly(n)— in other words, polylogarithmic in 2n
—this must also be the running time of the local decoder.
Definition 17.12 (Local decoder)
Let E : {0, 1}n →{0, 1}m be an ECC and let ρ and q be some numbers. A local decoder for E
handling ρ errors is an algorithm L that, given random access to a string y such that ∆(y, E(x)) < ρ
for some (unknown) x ∈{0, 1}n, and an index j ∈N, runs for polylog(m) time and outputs xj with
probability at least 2/3.
Remark 17.13
The constant 2/3 is arbitrary and can be replaced with any constant larger than 1/2, since the
probability of getting a correct answer can be ampliﬁed by repetition.
Notice, local decoding may be useful in applications of ECC’s that have nothing to do with
hardness ampliﬁcation. Even in context of CD storage, it seems nice if we do not to have to read
the entire CD just to recover one bit of x.
Using a local decoder, we can turn our intuition above of hardness ampliﬁcation into a proof.
Web draft 2007-01-08 21:59

DRAFT
17.4. ERROR CORRECTING CODES: THE INTUITIVE CONNECTION TO HARDNESS
AMPLIFICATION
p17.11 (331)
Note 17.11 (High dimensional geometry)
While we are normally used to geometry in two or three dimensions, we can
get some intuition on error correcting codes by considering the geometry of
high dimensional spaces. Perhaps the strongest eﬀect of high dimension is the
following: compare the cube with all sides 1 and the ball of radius 1/4. In one
dimension, the ratio between their areas is 1/(1/2) = 2, in two dimensions
it is 1/(π1/42) = 16/π, while in three dimensions it is 1/(4/3π1/43) = 48/π.
Note that as the number of dimension grows, this ratio grows exponentially
in the number of dimensions. (Similarly for any two radii r1 > r2 the volume
of the m-dimension ball of radius r1 is exponentially larger than the volume
of the r2-radius ball.)
0   1/4        3/4   1
Ball volume=1/2
0   1/4        3/4   1
0   1/4        3/4   1
       1   3/4  1/4
B.V. = π(1/4)2~3.14/16
B.V. =4/3π(1/4)3 ~ 3.14/48
This intuition lies behind the existence of an error correcting code with
distance 1/4 mapping n bit strings into m = 5n bit strings. We can have 2m/5
codewords that are all of distance at least 1/4 from one another because, also
in the Hamming distance, the volume of the radius 1/4 ball is exponentially
smaller than the volume of the cube {0, 1}n. Therefore, we can “pack” 2m/5
such balls within the cube.
Web draft 2007-01-08 21:59

DRAFT
p17.12 (332)
17.5. CONSTRUCTIONS OF ERROR CORRECTING CODES
x
E(x)
corrupted E(x)
x
f
E(f)
algorithm computing f
w/ prob 1-ρ
length n string
function on {0,1}n =
string of length 2n
algorithm computing f perfectly
Figure 17.2: An ECC allows to map a string x to E(x) such as x can be reconstructed from a corrupted version of
E(x). The idea is to treat a function f : {0, 1}n →{0, 1} as a string in {0, 1}2n, encode it using an ECC to a function
ˆf. Intuitively, ˆf should be hard on the average case if f was hard on the worst case, since an algorithm to solve ˆf
with probability 1 −ρ could be transformed (using the ECC’s decoding algorithm) to an algorithm computing f on
every input.
Theorem 17.14
Suppose that there is an ECC with polynomial-time encoding algorithm and a local decoding
algorithm handling ρ errors (where ρ is a constant independent of the input length). Suppose also
that there is f ∈E with Hwrs(f)(n) ≥S(n) for some function S : N →N satisfying S(n) ≥n. Then,
there exists ϵ > 0 and g ∈E with Hwrs(g)(n) ≥S(ϵn)ϵ
The proof of Theorem 17.14 follows essentially from the deﬁnition, and we will prove it for the
case of a particular code later on in Theorem 17.24.
17.5
Constructions of Error Correcting Codes
We now describe some explicit functions that are error correcting codes, building up to the con-
struction of an explicit ECC of constant distance with polynomial-time encoding and decoding.
Section 17.6 describes local decoding algorithms for some of these codes.
17.5.1
Walsh-Hadamard Code.
For two strings x, y ∈{0, 1}n, deﬁne x ⊙y to be the number Pn
i=1 xiyi (mod 2). The Walsh-
Hadamard code is the function WH : {0, 1}n →{0, 1}2n that maps a string x ∈{0, 1}n into the
string z ∈{0, 1}2n where for every y ∈{0, 1}n, the yth coordinate of z is equal to x ⊙y (we identify
{0, 1}n with [2n] in the obvious way).
Claim 17.15
The function WH is an error correcting code of distance 1/2.
Web draft 2007-01-08 21:59

DRAFT
17.5. CONSTRUCTIONS OF ERROR CORRECTING CODES
p17.13 (333)
x
E(x)
corrupted E(x)
local
decoder
compute xj
Figure 17.3: A local decoder gets access to a corrupted version of E(x) and an index i and computes from it xi
(with high probability).
Proof: First, note that WH is a linear function. By this we mean that if we take x + y to be the
componentwise addition of x and y modulo 2, then WH(x + y) = WH(x) + WH(y). Now, for every
x ̸= y ∈{0, 1}n we have that the number of 1’s in the string WH(x) + WH(y) = WH(x + y) is equal
to the number of coordinates on which WH(x) and WH(y) diﬀer. Thus, it suﬃces to show that for
every z ̸= 0n, at least half of the coordinates in WH(z) are 1. Yet this follows from the random
subsum principle (Claim A.5) that says that the probability for y ∈R {0, 1}n that z ⊙y = 1 is
exactly 1/2. ■
17.5.2
Reed-Solomon Code
The Walsh-Hadamard code has a serious drawback: its output size is exponential in the input size.
By Lemma 17.9 we know that we can do much better (at least if we’re willing to tolerate a distance
slightly smaller than 1/2). To get towards explicit codes with better output, we need to make a
detour to codes with non-binary alphabet.
Definition 17.16
For every set Σ and x, y ∈Σm, we deﬁne ∆(x, y) = 1
m |{i : xi ̸= yi}|. We say that E : Σn →Σm is
an error correcting code with distance δ over alphabet Σ if for every x ̸= y ∈Σn, ∆(E(x), E(y)) ≥δ.
Allowing a larger alphabet makes the problem of constructing codes easier. For example, every
ECC with distance δ over the binary ({0, 1}) alphabet automatically implies an ECC with the same
distance over the alphabet {0, 1, 2, 3}: just encode strings over {0, 1, 2, 3} as strings over {0, 1} in
the obvious way. However, the other direction does not work: if we take an ECC over {0, 1, 2, 3}
and transform it into a code over {0, 1} in the natural way, the distance might grow from δ to 2δ
(Exercise 8).
The Reed-Solomon code is a construction of an error correcting code that can use as its alphabet
any ﬁeld F:
Web draft 2007-01-08 21:59

DRAFT
p17.14 (334)
17.5. CONSTRUCTIONS OF ERROR CORRECTING CODES
x
E1:{0,1}n-->Σm
E2
E2
E2:Σ-->{0,1}k
E2
oE1:{0,1}n-->{0,1}km
E1(x)1
E1(x)m
.....
E2(E1(x)1)
E2(E1(x)m)
Figure 17.4: If E1,E2 are ECC’s such that E1 : {0, 1}n →Σm and E2 : σ →{0, 1}k, then the concatenated code
E : {0, 1}n →{0, 1}nk maps x into the sequence of blocks E2(E1(x)1), . . . , E2(E1(x)m).
Definition 17.17
Let F be a ﬁeld and n, m numbers satisfying n ≤m ≤|F|. The Reed-Solomon code from Fn to
Fm is the function RS : Fn →Fm that on input a0, . . . , an−1 ∈Fn outputs the string z0, . . . , zm−1
where
zj =
n−1
X
i=0
aifi
j
and fj denotes the jth element of F under some ordering.
Lemma 17.18
The Reed-Solomon code RS : Fn →Fm has distance 1 −n
m.
Proof: As in the case of Walsh-Hadamard code, the function RS is also linear in the sense that
RS(a + b) = RS(a) + RS(b) (where addition is taken to be componentwise addition in F). Thus, as
before we only need to show that for every a ̸= 0n, RS(a) has at most n coordinates that are zero.
But this immediate from the fact that a nonzero n −1 degree polynomial has at most n roots (see
Appendix A). ■
17.5.3
Concatenated codes
The Walsh-Hadamard code has the drawback of exponential-sized output and the Reed-Solomon
code has the drawback of a non-binary alphabet. We now show we can combine them both to
obtain a code without neither of these drawbacks:
Definition 17.19
If RS is the Reed-Solomon code mapping Fn to Fm (for some n, m, F) and WH is the Walsh-
Hadamard code mapping {0, 1}log |F| to {0, 1}2log |F| = {0, 1}|F|, then the code WH ◦RS maps
{0, 1}n log |F| to {0, 1}m|F| in the following way:
1. View RS as a code from {0, 1}n log |F| to Fm and WH as a code from F to {0, 1}|F| using the
canonical representation of elements in F as strings in {0, 1}log |F|.
2. For every input x ∈{0, 1}n log |F|, WH◦RS(x) is equal to WH(RS(x)1), . . . , WH(RS(x)m) where
RS(x)i denotes the ith symbol of RS(x).
Web draft 2007-01-08 21:59

DRAFT
17.5. CONSTRUCTIONS OF ERROR CORRECTING CODES
p17.15 (335)
Note that the code WH ◦RS can be computed in time polynomial in n, m and |F|. We now
analyze its distance:
Claim 17.20
Let δ1 = 1 −n/m be the distance of RS and δ2 = 1/2 be the distance of WH. Then WH ◦RS is an
ECC of distance δ1δ2.
Proof: Let x, y be two distinct strings in {0, 1}log |F|n. If we set x′ = RS(x′) and y′ = RS(y′) then
∆(x′, y′) ≥δ1. If we let x′′ (resp. y′′) to be the binary string obtained by applying WH to each of
these blocks, then whenever two blocks are distinct, the corresponding encoding will have distance
δ2, and so δ(x′′, y′′) ≥δ1δ2. ■
Remark 17.21
Because for every k ∈N, there exists a ﬁnite ﬁeld |F| of size in [k, 2k] (e.g., take a prime in [k, 2k] or
a power of two) we can use this construction to obtain, for every n, a polynomial-time computable
ECC E : {0, 1}n →{0, 1}20n2 of distance 0.4.
Both Deﬁnition 17.19 and Lemma 17.20 easily generalize for codes other than Reed-Solomon
and Hadamard.
Thus, for every two ECC’s E1 : {0, 1}n →Σm and E2 : Σ →{0, 1}k their
concatenation E2 ◦E1 is a code from {0, 1}n to {0, 1}mk that has distance at least δ1δ2 where δ1
(resp. δ2) is the distance of E1 (resp. E2), see Figure 17.6. In particular, using a diﬀerent binary
code than WH, it is known how to use concatenation to obtain a polynomial-time computable ECC
E : {0, 1}n →{0, 1}m of constant distance δ > 0 such that m = O(n).
17.5.4
Reed-Muller Codes.
Both the Walsh-Hadamard and and the Reed-Solomon code are special cases of the following family
of codes known as Reed-Muller codes:
Definition 17.22 (Reed-Muller codes)
Let F be a ﬁnite ﬁeld, and let ℓ, d be numbers with d < |F|. The Reed Muller code with parameters
F, ℓ, d is the function RM : F(ℓ+d
d ) →F|F|ℓthat maps every ℓ-variable polynomial P over F of total
degree d to the values of P on all the inputs in Fℓ.
That is, the input is a polynomial of the form
g(x1, . . . , xℓ) =
X
i1+i2+...+iℓ≤ℓ
ci1,...,iℓxi1
1 xi2
2 · · · xiℓ
ℓ
speciﬁed by the vector of
 ℓ+d
d

coeﬃcients {ci1,...,iℓ} and the output is the sequence {g(x1, . . . , xℓ)}
for every x1, . . . , xℓ∈F.
Setting ℓ= 1 one obtains the Reed-Solomon code (for m = |F|), while setting d = 1 and
F = GF(2) one obtains a slight variant of the Walsh-Hadamard code. (I.e., the code that maps
every x ∈{0, 1}n into the 2·2n long string z such that for every y ∈{0, 1}n,a ∈{0, 1}, zy,a = x⊙y+a
(mod 2).)
The Schwartz-Zippel Lemma (Lemma A.25 in Appendix A) shows that the Reed-Muller code
is an ECC with distance 1 −d/|F|. Note that this implies the previously stated bounds for the
Walsh-Hadamard and Reed-Solomon codes.
Web draft 2007-01-08 21:59

DRAFT
p17.16 (336)
17.5. CONSTRUCTIONS OF ERROR CORRECTING CODES
17.5.5
Decoding Reed-Solomon.
To actually use an error correcting code to store and retrieve information, we need a way to
eﬃciently decode a data x from its encoding E(x) even if E(x) has been corrupted in a fraction ρ
of its coordinates. We now show this for the Reed-Solomon code, that treats x as a polynomial g,
and outputs the values of this polynomial on m inputs.
We know (see Theorem A.24 in Appendix A) that a univariate degree d polynomial can be
interpolated from any d + 1 values. Here we consider a robust version of this procedure, whereby
we wish to recover the polynomial from m values of which ρm are “faulty” or “noisy”.
Let (a1, b1), (a2, b2), . . . , (am, bm) be a sequence of (point, value) pairs. We say that a degree d
polynomial g(x) describes this (ai, bi) if g(ai) = bi.
We are interested in determining if there is a degree d polynomial g that describes (1 −ρ)m of
the pairs. If 2ρm > d then this polynomial is unique (exercise). We desire to recover it, in other
words, ﬁnd a degree d polynomial g such that
g(ai) = bi
for at (1 −ρ)m least values of i.
(8)
The apparent diﬃculty is in identifying the noisy points; once those points are identiﬁed, we
can recover the polynomial.
Randomized interpolation: the case of ρ < 1/(d + 1)
If ρ is very small, say, ρ < 1/(2d) then we can actually use the standard interpolation technique:
just select d + 1 points at random from the set {(ai, bi)} and use them to interpolate. By the union
bound, with probability at least 1 −ρ(d + 1) > 0.4 all these points will be non-corrupted and so we
will recover the correct polynomial. (Because the correct polynomial is unique, we can verify that
we have obtained it, and if unsuccessful, try again.)
Berlekamp-Welch Procedure: the case of ρ < (m −d)/(2m)
The Berlekamp-Welch procedure works when the error rate ρ is bounded away from 1/2; speciﬁcally,
ρ < (m −d)/(2m). For concreteness, assume m = 4d and ρ = 1/4.
1. We claim that if the polynomial g exists then there is a degree 2d polynomial c(x) and a
degree d nonzero polynomial e(x) such that
c(ai) = bie(ai)
for all i.
(9)
The reason is that the desired e(x) can be any nonzero degree d polynomial whose roots are
precisely the ai’s for which g(ai) ̸= bi, and then just let c(x) = g(x)e(x). (Note that this is
just an existence argument; we do not know g yet.))
2. Let c(x) = P
i≤2d cixi and e(x) = P
i≤d eixi. The ei’s and ci’s are our unknowns, and these
satisfy 4d linear equations given in (??), one for each ai. The number of unknowns is 3d + 2,
and our existence argument in part 1 shows that the system is feasible. Solve it using Gaussian
elimination to obtain a candidate c, e.
Web draft 2007-01-08 21:59

DRAFT
17.6. LOCAL DECODING OF EXPLICIT CODES.
p17.17 (337)
3. Let c, e are any polynomials obtained in part 2. Since they satisfy (9) and bi = g(ai) for at
least 3d values of i, we conclude that
c(ai) = g(ai)e(ai)
for at least 3d values of i.
Hence c(x) −g(x)e(x) is a degree 2d polynomial that has at least 3d roots, and hence is
identically zero. Hence e divides c and that in fact c(x) = g(x)e(x).
4. Divide c by e to recover g.
17.5.6
Decoding concatenated codes.
Decoding concatenated codes can be achieved through the natural algorithm. Recall that if E1 :
{0, 1}n →Σm and E2 : Σ →{0, 1}k are two ECC’s then E2 ◦E1 maps every string x ∈{0, 1}n to
the string E2(E1(x)1) · · · E2(E1(x)n). Suppose that we have a decoder for E1 (resp. E2) that can
handle ρ1 (resp. ρ2) errors. Then, we have a decoder for E2 ◦E1 that can handle ρ2ρ1 errors. The
decoder, given a string y ∈{0, 1}mk composed of m blocks y1, . . . , ym ∈{0, 1}k, ﬁrst decodes each
block yi to a symbol zi in Σ, and then uses the decoder of E1 to decode z1, . . . , zm. The decoder
can indeed handle ρ1ρ2 errors since if ∆(y, E2 ◦E1(x)) ≤ρ1ρ2 then at most ρ1 of the blocks of y
are of distance at least ρ2 from the corresponding block of E2 ◦E1(x).
17.6
Local Decoding of explicit codes.
We now show local decoder algorithm (c.f. Deﬁnition 17.12) for several explicit codes.
17.6.1
Local decoder for Walsh-Hadamard.
The following is a two-query local decoder for the Walsh-Hadamard code that handles ρ errors for
every ρ < 1/4. This fraction of errors we handle is best possible, as it can be easily shown that there
cannot exist a local (or non-local) decoder for a binary code handling ρ errors for every ρ ≥1/4.
Walsh-Hadamard Local Decoder for ρ < 1/4:
Input: j ∈[n], random access to a function f : {0, 1}n →{0, 1} such that Pry[g(y) ̸= x ⊙y] ≤ρ
for some ρ < 1/4 and x ∈{0, 1}n.
Output: A bit b ∈{0, 1}. (Our goal: xj = b.)
Operation: Let ej be the vector in {0, 1}n that is equal to 0 in all the coordinates except for
the jth and equal to 1 on the jth coordinate.
The algorithm chooses y ∈R {0, 1}n and
outputs f(y) + f(y + ej) (mod 2) (where y + ej denotes componentwise addition modulo 2,
or equivalently, ﬂipping the jth coordinate of y).
Analysis: Since both y and y+ej are uniformly distributed (even though they are dependent), the
union bound implies that with probability 1−2ρ, f(y) = x⊙y and f(y+ej) = x⊙(y+ej). But
by the bilinearity of the operation ⊙, this implies that f(y)+f(y+ej) = x⊙y+x⊙(y+ej) =
Web draft 2007-01-08 21:59

DRAFT
p17.18 (338)
17.6. LOCAL DECODING OF EXPLICIT CODES.
Lx
x
Figure 17.5: Given access to a corrupted version of a polynomial P : Fℓ→F, to compute P(x) we pass a random
line Lx through x, and use Reed-Solomon decoding to recover the restriction of P to the line Lx.
2(x ⊙y) + x ⊙ej = x ⊙ej (mod 2). Yet, x ⊙ej = xj and so with probability 1 −2ρ, the
algorithm outputs the right value.
Remark 17.23
This algorithm can be modiﬁed to locally compute not just xi = x ⊙ej but in fact the value x ⊙z
for every z ∈{0, 1}n. Thus, we can use it to compute not just every bit of the original message x
but also every bit of the uncorrupted codeword WH(x). This property is sometimes called the self
correction property of the Walsh-Hadamard code.
17.6.2
Local decoder for Reed-Muller
We now show a local decoder for the Reed-Muller code. (Note that Deﬁnition 17.12 can be easily
extended to the case of codes, such as Reed-Muller, that use non-binary alphabet.) It runs in
time polynomial in ℓand d, which, for an appropriate setting of the parameters, is polylogarithmic
in the output length of the code. Convention: Recall that the input to a Reed-Muller code is
an ℓ-variable d-degree polynomial P over some ﬁeld F. When we discussed the code before, we
assumed that this polynomial is represented as the list of its coeﬃcients. However, below it will be
more convenient for us to assume that the polynomial is represented by a list of its values on its
ﬁrst
 d+ℓ
ℓ

inputs according to some canonical ordering. Using standard interpolation, we still have
a polynomial-time encoding algorithm even given this representation. Thus, it suﬃces to show an
algorithm that, given access to a corrupted version of P, computes P(x) for every x ∈Fℓ
Reed-Muller Local Decoder for ρ < (1 −d/|F|)/4 −1/|F|.
Input: A string x ∈Fℓ, random access to a function f such that Prx∈Fℓ[P(x) ̸= f(x)] < ρ, where
P : Fℓ→F is an ℓ-variable degree-d polynomial.
Output: y ∈F (Goal: y = P(x).)
Operation:
1. Let Lx, be a random line passing through x. That is Lx = {x + ty : t ∈F} for a
random y ∈Fℓ.
Web draft 2007-01-08 21:59

DRAFT
17.6. LOCAL DECODING OF EXPLICIT CODES.
p17.19 (339)
x
E1:{0,1}n-->Σm
E2
E2
E2:Σ-->{0,1}k
E2
oE1:{0,1}n-->{0,1}km
E1(x)1
E1(x)m
.....
E2(E1(x)1)
E2(E1(x)m)
E1 decoder
E2 decoder
E2 decoder
q1 queries
O(q2 log q1) queries
Figure 17.6: To locally decode a concatenated code E2 ◦E1 we run the decoder for E1 using the decoder for E2.
The crucial observation is that if y is within ρ1ρ2 distance to E2 ◦E1(x) then at most a ρ1 fraction of the blocks in
y are of distance more than ρ2 the corresponding block in E2 ◦E1(x).
2. Query f on all the |F| points of Lx to obtain a set of points {(t, f(x + ty))} for every
t ∈F.
3. Run the Reed-Solomon decoding algorithm to obtain the univariate polynomial Q : F →
F such that Q(t) = f(x + ty) for the largest number of t’s (see Figure 17.5).3
4. Output Q(0).
Analysis: For every d-degree ℓ-variable polynomial P, the univariate polynomial Q(t) = P(x+ty)
has degree at most d. Thus, to show that the Reed-Solomon decoding works, it suﬃces to
show that with probability at least 1/2, the number of points on z ∈Lx for which f(z) ̸= P(z)
is less than (1 −d/|F|)/2. Yet, for every t ̸= 0, the point x + ty is uniformly distributed
(independently of x), and so the expected number of points on Lx for which f and P diﬀer
is at most ρ|F| + 1.
By Markov inequality, the probability that there will be more than
2ρ|F| + 2 < (1 −d/|F|)|F|/2 such points is at most 1/2 and hence Reed-Solomon decoding will
be successful with probability 1/2. In this case, we obtain the correct polynomial q that is the
restriction of Q to the line Lx and hence q(0) = P(x).
17.6.3
Local decoding of concatenated codes.
Given two locally decodable ECC’s E1 and E2, we can locally decode their concatenation E1 ◦E2
by the natural algorithm. Namely, we run the decoder for E1, but answer its queries using the
decoder for E2 (see Figure 17.6).
Local decoder for concatenated code: ρ < ρ1ρ2
The code: If E1 : {0, 1}n →Σm and E2 : Σ →{0, 1}k are codes with decoders of q1 (resp. q2)
queries with respect to ρ1 (resp. ρ2) errors, let E = E2◦E1 be the concatenated code mapping
{0, 1}n to {0, 1}mk.
3If ρ is suﬃciently small, (e.g., ρ < 1/(10d)), then we can use the simpler randomized Reed-Solomon decoding
procedure described in Section 17.5.5.
Web draft 2007-01-08 21:59

DRAFT
p17.20 (340)
17.6. LOCAL DECODING OF EXPLICIT CODES.
Input: An index i ∈[n], random access to a string y ∈{0, 1}km such that ∆(y, E1 ◦E2(x)) < ρ1ρ2
for some x ∈{0, 1}n.
Output: b ∈{0, 1}n (Goal: b = xi)
Operation: Simulate the actions of the decoder for E1, whenever the decoder needs access to the
jth symbol of E1(x), use the decoder of E2 with O(q2 log q1 log |Σ|) queries applied to the jth
block of y to recover all the bits of this symbol with probability at least 1 −1/(2q1).
Analysis: The crucial observation is that at most a ρ1 fraction of the length k blocks in y can
be of distance more than ρ2 from the corresponding blocks in E2 ◦E1(x). Therefore, with
probability at least 0.9, all our q1 answers to the decoder of E1 are consistent with the answer
it would receive when accessing a string that is of distance at most ρ1 from a codeword of E1.
17.6.4
Putting it all together.
We now have the ingredients to prove our second main theorem of this chapter: transformation of
a hard-on-the-worst-case function into a function that is “mildly” hard on the average case.
Theorem 17.24 (Worst-case hardness to mild hardness)
Let S : N →N and f ∈E such that Hwrs(f)(n) ≥S(n) for every n. Then there exists
a function g ∈E and a constant c > 0 such that H0.99
avg (g)(n) ≥S(n/c)/nc for every
suﬃciently large n.
Proof: For every n, we treat the restriction of f to {0, 1}n as a string f′ ∈{0, 1}N where N = 2n.
We then encode this string f′ using a suitable error correcting code E : {0, 1}N →{0, 1}NC for
some constant C > 1. We will deﬁne the function g on every input x ∈{0, 1}Cn to output the xth
coordinate of E(f′).4 For the function g to satisfy the conclusion of the theorem, all we need is for
the code E to satisfy the following properties:
1. For every x ∈{0, 1}N, E(x) can be computed in poly(N) time.
2. There is a local decoding algorithm for E that uses polylog(N) running time and queries and
can handle a 0.01 fraction of errors.
But this can be achieved using a concatenation of a Walsh-Hadamard code with a Reed-Muller
code of appropriate parameters:
1. Let RM denote the Reed-Muller code with the following parameters:
• The ﬁeld F is of size log5 N.
• The number of variables ℓis equal to log N/ log log N.
4By padding with zeros as necessary, we can assume that all the inputs to g are of length that is a multiple of C.
Web draft 2007-01-08 21:59

DRAFT
17.7. LIST DECODING
p17.21 (341)
• The degree is equal to log2 N.
RM takes an input of length at least (d
ℓ)ℓ> N (and so using padding we can assume its input
is {0, 1}n). Its output is of size |F|ℓ≤poly(n). Its distance is at least 1 −1/ log N.
2. Let WH denote the Walsh-Hadamard code from {0, 1}log F = {0, 1}5 log log N to {0, 1}|F| =
{0, 1}log5 N.
Our code will be WH◦RM. Combining the local decoders for Walsh-Hadamard and Reed-Muller
we get the desired result. ■
Combining Theorem 17.24 with Yao’s XOR Lemma (Theorem 17.2), we get the following corol-
lary:
Corollary 17.25
Let S : N →N and f ∈E with Hwrs(f)(n) ≥S(n) for every n. Then, there exists an S(
√
ℓ)ϵ-
pseudorandom generator for some constant ϵ > 0.
Proof: By Theorem 17.24, under this assumption there exists a function g ∈E with H0.99
avg (g)(n) ≥
S′(n) = S(n)/poly(n), where we can assume S′(n) ≥
p
S(n) for suﬃciently large n (otherwise S
is polynomial and the theorem is trivial). Consider the function g⊕k where k = c log S′(n) for a
suﬃciently small constant c. By Yao’s XOR Lemma, on inputs of length kn, it cannot be computed
with probability better than 1/2 + 2−cS′(n)/1000 by circuits of size S′(n). Since S(n) ≤2n, kn < √n,
and hence we get that Havg(g⊕k) ≥Sc/2000. ■
As already mentioned, this implies the following corollaries:
1. If there exists f ∈E such that Hwrs(f) ≥2nΩ(1) then BPP ⊆QuasiP.
2. If there exists f ∈E such that Hwrs(f) ≥nω(1) then BPP ⊆SUBEXP.
However, Corollary 17.25 is still not suﬃcient to show that BPP = P under any assumption on
the worst-case hardness of some function in E. It only yields an S(
√
ℓ)Ω(1)-pseudorandom generator,
while what we need is an S(Ω(ℓ))Ω(1)-pseudorandom generator.
17.7
List decoding
Our approach to obtain stronger worst-case to average-case reduction will be to bypass the XOR
Lemma, and use error correcting codes to get directly from worst-case hardness to a function that
is hard to compute with probability slightly better than 1/2. However, this idea seems to run into
a fundamental diﬃculty: if f is worst-case hard, then it seems hard to argue that the encoding
of f, under any error correcting code is hard to compute with probability 0.6. The reason is that
any error-correcting code has to have distant at most 1/2, which implies that there is no decoding
algorithm that can recover x from E(x) if the latter was corrupted in more than a 1/4 of its locations.
Indeed, in this case there is not necessarily a unique codeword closest to the corrupted word. For
example, if E(x) and E(x′) are two codewords of distance 1/2, let y be the string that is equal to
Web draft 2007-01-08 21:59

DRAFT
p17.22 (342)
17.7. LIST DECODING
E(x) on the ﬁrst half of the coordinates and equal to E(x′) on the second half. Given y, how can
a decoding algorithm know whether to return x or x′?
This seems like a real obstacle, and indeed was considered as such in many contexts where
ECC’s were used, until the realization of the importance of the following insight: “If y is obtained
by corrupting E(x) in, say, a 0.4 fraction of the coordinates (where E is some ECC with good
enough distance) then, while there may be more than one codeword within distance 0.4 to y, there
can not be too many such codewords.”
Theorem 17.26 (Johnson Bound)
If E : {0, 1}n →{0, 1}m is an ECC with distance at least 1/2 −ϵ, then for every x ∈{0, 1}m, and
δ ≥√ϵ, there exist at most 1/(2δ2) vectors y1, . . . , yℓsuch that ∆(x, yi) ≤1/2 −δ for every i ∈[ℓ].
Proof: Suppose that x, y1, . . . , yℓsatisfy this condition, and deﬁne ℓvectors z1, . . . , zℓin Rm as
follows: for every i ∈[ℓ] and k ∈[m], set zi,k to equal +1 if yk = xk and set it to equal −1 otherwise.
Under our assumptions, for every i ∈[ℓ],
m
X
k=1
zi,k ≥2δm ,
(10)
since zi agrees with x on an 1/2 + δ fraction of its coordinates. Also, for every i ̸= j ∈[ℓ],
⟨zi, zj⟩=
m
X
k=1
zi,kzj,k ≤2ϵm ≤2δ2m
(11)
since E is a code of distance at least 1/2 −ϵ. We will show that (10) and (11) together imply that
ℓ≤1/(2δ2).
Indeed, set w = Pℓ
i=1 zi. On one hand, by (11)
⟨w, w⟩=
ℓ
X
i=1
⟨zi, zi⟩+
X
i̸=j
⟨zi, zj⟩≤ℓm + ℓ22δ2m .
On the other hand, by (10), P
k wk = P
i,j zi,j ≥2δmℓand hence
⟨w, w⟩≥|
X
k
wk|2/m ≥4δ2mℓ2 ,
since for every c, the vector w ∈Rm with minimal two-norm satisfying P
k wk = c is the uniform
vector (c/m, c/m, . . . , c/m). Thus 4δ2mℓ2 ≤ℓm + 2ℓ2δ2m, implying that ℓ≤1/(2δ2). ■
17.7.1
List decoding the Reed-Solomon code
In many contexts, obtaining a list of candidate messages from a corrupted codeword can be just as
good as unique decoding. For example, we may have some outside information on which messages
are likely to appear, allowing us to know which of the messages in the list is the correct one.
Web draft 2007-01-08 21:59

DRAFT
17.8. LOCAL LIST DECODING: GETTING TO BPP = P.
p17.23 (343)
However, to take advantage of this we need an eﬃcient algorithm that computes this list. Such an
algorithm was discovered in 1996 by Sudan for the popular and important Reed-Solomon code. It
can recover a polynomial size list of candidate codewords given a Reed-Solomon codeword that was
corrupted in up to a 1 −2
p
d/|F| fraction of the coordinates. Note that this tends to 1 as |F|/d
grows, whereas the Berlekamp-Welch unique decoding algorithm of Section 17.5.5 gets “stuck”
when the fraction of errors surpasses 1/2.
On input a set of data points {(ai, bi)}m
i=1 in F2, Sudan’s algorithm returns all degree d poly-
nomials g such that the number of i’s for which g(ai) = bi is at least 2
p
d/|F|m. It relies on the
following observation:
Lemma 17.27
For every set of m data pairs (a1, b1), . . . , (am, bm), there is a bivariate polynomial Q(z, x) of degree
at most ⌈√m⌉+ 1 in z and x such that Q(bi, ai) = 0 for each i = 1, . . . , m. Furthermore, there is
a polynomial-time algorithm to construct such a Q.
Proof: Let k = ⌈√m⌉+ 1. Then the unknown bivariate polynomial Q = Pk
i=0
Pk
j=0 Qijzixj has
(k + 1)2 coeﬃcients and these coeﬃcients are required to satisfy m linear equations of the form:
k
X
i=0
k
X
j=0
Qij(bt)i(at)j
for t = 1, 2, . . . , m.
Note that the at’s, bt’s are known and so we can write down these equations.
Since the system is homogeneous and the number of unknowns exceeds the number of con-
straints, it has a nonzero solution. Furthermore this solution can be found in polynomial time. ■
Lemma 17.28
Let d be any integer and k > (d + 1)(⌈√m⌉+ 1). If p(x) is a degree d polynomial that describes k
of the data pairs, then z −p(x) divides the bivariate polynomial Q(z, x) described in Lemma 17.27.
Proof: By construction, Q(bt, at) = 0 for every data pair (at, bt).
If p(x) describes this data
pair, then Q(p(at), at) = 0. We conclude that the univariate polynomial Q(p(x), x) has at least k
roots, whereas its degree is d(⌈√n⌉+ 1) < k. Hence Q(p(x), x) = 0. By the division algorithm
for polynomials, Q(p(x), x) is exactly the remainder when Q(z, x) is divided by (z −p(x)). We
conclude that z −p(x) divides Q(z, x). ■
Now it is straightforward to describe Sudan’s list decoding algorithm. First, ﬁnd Q(z, x) by
the algorithm of Lemma 17.27. Then, factor it using a standard algorithm for bivariate factoring
(see [VG99]). For every factor of the form (z −p(x)), check by direct substitution whether or not
p(x) describes 2
p
d/|F|m data pairs. Output all such polynomials.
17.8
Local list decoding: getting to BPP = P.
Analogously to Section 17.4.1, to actually use list decoding for hardness ampliﬁcation, we need to
provide local list decoding algorithms for the codes we use. Fortunately, such algorithms are known
for the Walsh-Hadamard code, the Reed-Muller code, and their concatenation.
Web draft 2007-01-08 21:59

DRAFT
p17.24 (344)
17.8. LOCAL LIST DECODING: GETTING TO BPP = P.
Definition 17.29 (Local list decoder)
Let E : {0, 1}n →{0, 1}m be an ECC and let ρ > 0 and q be some numbers. An algorithm L is
called a local list decoder for E handling ρ errors, if for every x ∈{0, 1}n and y ∈{0, 1}m satisfying
∆(E(x), y) ≤ρ, there exists a number i0 ∈[poly(n/ϵ)] such that for every j ∈[m], on inputs i0, j
and with random access to y, L runs for poly(log(m)/ϵ) time and outputs xj with probability at
least 2/3.
Remark 17.30
One can think of the number i0 as the index of x in the list of poly(n/ϵ) candidate messages output
by L. Deﬁnition 17.29 can be easily generalized to codes with non-binary alphabet.
17.8.1
Local list decoding of the Walsh-Hadamard code.
It turns out we already encountered a local list decoder for the Walsh-Hadamard code: the proof
of the Goldreich-Levin Theorem (Theorem 10.14) provided an an algorithm that given access to
a “black box” that computes the function y 7→x ⊙y (for x, y ∈{0, 1}n) with probability 1/2 + ϵ,
computes a list of values x1, . . . , xpoly(n/ϵ) such that xi0 = x for some i0. In the context of that
theorem, we could ﬁnd the right value of x from that list by checking it against the value f(x)
(where f is a one-way permutation). This is a good example for how once we have a list decoding
algorithm, we can use outside information to narrow the list down.
17.8.2
Local list decoding of the Reed-Muller code
We now present an algorithm for local list decoding of the Reed-Muller code.
Recall that the
codeword of this code is the list of evaluations of a d-degree ℓ-variable polynomial P : Fℓ→F. The
local decoder for Reed-Muller gets random access to a corrupted version of P and two inputs: an
index i and x ∈Fℓ. Below we describe such a decoder that runs in poly(d, ℓ, |F|) and outputs P(x)
with probability at least 0.9 assuming that i is equal to the “right” index i0. Note: To be a valid
local list decoder, given the index i0, the algorithm should output P(x) with high probability for
every x ∈Fℓ. The algorithm described below is only guaranteed to output the right value for most
(i.e., a 0.9 fraction) of the x’s in Fℓ. We transform this algorithm to a valid local list decoder by
combining it with the Reed-Muller local decoder described in Section 17.6.2.
Reed-Muller Local List Decoder for ρ < 1 −10
p
d/|F|
Inputs:
• Random access to a function f such that Prx∈Fℓ[P(x) = f(x)] > 10
p
d/|F| where
P : Fℓ→F is an ℓ-variable d-degree polynomial. We assume |F| > d4 and that both
d > 1000. (This can always be ensured in our applications.)
• An index i0 ∈[|F|ℓ+1] which we interpret as a pair (x0, y0) with x0 ∈Fℓ, y0 ∈F,
• A string x ∈Fℓ.
Output: y ∈F (For some pair (x0, y0), it should hold that P(x) = y with probability at least 0.9
over the algorithm’s coins and x chosen at random from Fℓ.)
Web draft 2007-01-08 21:59

DRAFT
17.8. LOCAL LIST DECODING: GETTING TO BPP = P.
p17.25 (345)
Operation:
1. Let Lx,x0 be a random degree 3 curve passing through x, x0. That is, we ﬁnd a
random degree 3 univariate polynomial q : F →Fℓsuch that q(0) = x and q(r) = x0 for
some random r ∈F. (See Figure 17.7.)
2. Query f on all the |F| points of Lx,x0 to obtain the set S of the |F| pairs {(t, f(q(t)) :
t ∈F)}.
3. Run Sudan’s Reed-Solomon list decoding algorithm to obtain a list g1, . . . , gk of all degree
3d polynomials that have at least 8
p
d|F| agreement with the pairs in S.
4. If there is a unique i such that gi(r) = y0 then output gi(0). Otherwise, halt without
outputting anything.
Lx,x0
x
x0
Figure 17.7: Given access to a corrupted version of a polynomial P : Fℓ→F and some index (x0, y0), to compute
P(x) we pass a random degree-3 curve Lx,x0 through x and x0, and use Reed-Solomon list decoding to recover a list
of candidates for the restriction of P to the curve Lx,x0. If only one candidate satisﬁes that its value on x0 is y0,
then we use this candidate to compute P(x).
We will show that for every f : Fℓ→F that agrees with an ℓ-variable degree d polynomial
on a 10
p
d/|F| fraction of its input, and every x ∈Fℓ, if x0 is chosen at random from Fℓand
y0 = P(x0), then with probability at least 0.9 (over the choice of x0 and the algorithm’s coins) the
above decoder will output P(x). By a standard averaging argument, this implies that there exist
a pair (x0, y0) such that given this pair, the algorithm outputs P(x) for a 0.9 fraction of the x’s in
Fℓ.
Let x ∈Fℓ, if x0 is chosen randomly in Fℓand y0 = P(x0) then the following
For every x ∈Fℓ, the following ﬁctitious algorithm can be easily seen to have an identical output
to the output of our decoder on the inputs x, a random x0 ∈R Fℓand y0 = P(x0):
1. Choose a random degree 3 curve L that passes through x. That is, L = {q(t) : t ∈F} where
q : F →Fℓis a random degree 3 polynomial satisfying q(0) = x.
2. Obtain the list g1, . . . , gm of all univariate polynomials over F such that for every i, there are
at least 6
p
d|F| values of t such that gi(t) = f(q(t)).
3. Choose a random r ∈F. Assume that you are given the value y0 = P(q(r)).
Web draft 2007-01-08 21:59

DRAFT
p17.26 (346)
17.8. LOCAL LIST DECODING: GETTING TO BPP = P.
4. If there exists a unique i such that gi(r) = y0 then output gi(0). Otherwise, halt without an
input.
Yet, this ﬁctitious algorithm will output P(x) with probability at least 0.9. Indeed, since all
the points other than x on a random degree 3 curve passing through x are pairwise independent,
Chebyshev’s inequality implies that with probability at least 0.99, the function f will agree with
the polynomial P on at least 8
p
d|F| points on this curve (this uses the fact that
p
d/|F| is smaller
than 10−6). Thus the list g1, . . . , gm we obtain in Step 2 contains the polynomial g : F →F deﬁned
as g(t) = P(q(t)). We leave it as Exercise 9 to show that there can not be more than
p
|F|/4d
polynomials in this list. Since two 3d-degree polynomials can agree on at most 3d + 1 points, with
probability at least
(3d+1)√
|F|/4d
|F|
< 0.01, if we choose a random r ∈F, then g(r) ̸= gi(r) for every
gi ̸= g in this list. Thus, with this probability, we will identify the polynomial g and output the
value g(0) = P(x). ■
17.8.3
Local list decoding of concatenated codes.
If E1 : {0, 1}n →Σm and E2 : Σ →{0, 1}k are two codes that are locally list decodable then so
is the concatenated code E2 ◦E1 : {0, 1}n →{0, 1}mk. As in Section 17.6.3, the idea is to simply
run the local decoder for E1 while answering its queries using the decoder of E2. More concretely,
assume that the decoder for E1 takes an index in the set I1, uses q1 queries, and can handle 1 −ϵ1
errors, and that I2, q2 and ϵ2 are deﬁned analogously. Our decoder for E2 ◦E1 will take a pair
of indices i1 ∈I1 and i2 ∈I2 and run the decoder for E1 with the index i1, and whenever this
decoder makes a query answer it using the decoder E2 with the index i2. (See Section 17.6.3.) We
claim that this decoder can handle 1/2 −ϵ1ϵ2|I2| number of errors. Indeed, if y agrees with some
codeword E2 ◦E1(x) on an ϵ1ϵ2|I2| fraction of the coordinates then there are ϵ1|I2| blocks on which
it has at least 1/2 + ϵ2 agreement with the blocks this codeword. Thus, by an averaging argument,
there exists an index i2 such that given i2, the output of the E2 decoder agrees with E1(x) on ϵ1
symbols, implying that there exists an index i1 such that given (i1, i2) and every coordinate j, the
combined decoder will output xj with high probability.
17.8.4
Putting it all together.
As promised, we can use local list decoding to transform a function that is merely worst-case hard
into a function that cannot be computed with probability signiﬁcantly better than 1/2:
Theorem 17.31 (Worst-case hardness to strong hardness)
Let S : N →N and f ∈E such that Hwrs(f)(n) ≥S(n) for every n. Then there exists
a function g ∈E and a constant c > 0 such that Havg(g)(n) ≥S(n/c)1/c for every
suﬃciently large n.
Proof sketch:
As in Section 17.6.4, for every n, we treat the restriction of f to {0, 1}n as a
Web draft 2007-01-08 21:59

DRAFT
17.8. LOCAL LIST DECODING: GETTING TO BPP = P.
p17.27 (347)
string f′ ∈{0, 1}N where N = 2n and encode it using the concatenation of a Reed-Muller code
with the Walsh-Hadamard code. For the Reed-Muller code we use the following parameters:
• The ﬁeld F is of size S(n)1/100. 5
• The degree d is of size log2 N.
• The number of variables ℓis 2 log N/ log S(n).
The function g is obtained by applying this encoding to f. Given a circuit of size S(n)1/100
that computes g with probability better than 1/2 + 1/S(n)1/50, we will be able to transform it, in
S(n)O(1) time, to a circuit computing f perfectly. We hardwire the index i0 to this circuit as part
of its description. ■
What have we learned?
• Yao’s XOR Lemma allows to amplify hardness by transforming a Boolean
function with only mild hardness (cannot be computed with say 0.99 success)
into a Boolean function with strong hardness (cannot be computed with 0.51
success).
• An error correcting code is a function that maps every two strings into a pair
of strings that diﬀer on many of their coordinates. An error correcting code
with a local decoding algorithm can be used to transform a function hard in
the worst-case into a function that is mildly hard on the average case.
• A code over the binary alphabet can have distance at most 1/2. A code with
distance δ can be uniquely decoded up to δ/2 errors. List decoding allows to
a decoder to handle almost a δ fraction of errors, at the expense of returning
not a single message but a short list of candidate messages.
• We can transform a function that is merely hard in the worst case to a function
that is strongly hard in the average case using the notion of local list decoding
of error correcting codes.
Chapter notes and history
many attributions still missing.
Impagliazzo and Wigderson [IW01] were the ﬁrst to prove that BPP = P if there exists
f ∈E such that Hwrs(f) ≥2Ω(n) using a derandomized version of Yao’s XOR Lemma. However,
5We assume here that S(n) > log N 1000 and that it can be computed in 2O(n) time. These assumptions can
be removed by slightly complicating the construction (namely, executing it while guessing that S(n) = 2k, and
concatenating all the results.)
Web draft 2007-01-08 21:59

DRAFT
p17.28 (348)
17.8. LOCAL LIST DECODING: GETTING TO BPP = P.
the presentation here follows Sudan, Trevisan, and Vadhan [STV], who were the ﬁrst to point the
connection between local list decoding and hardness ampliﬁcation, and gave (a variant of) the Reed-
Muller local list decoding algorithm described in Section 17.8. They also showed a diﬀerent approach
to achieve the same result, by ﬁrst showing that the NW generator and a mildly hard function can
be used to obtain from a short random seed a distribution that has high pseudoentropy, which is
then converted to a pseudorandom distribution via a randomness extractor (see Chapter 16).
The question raised in Problem 5 is treated in O’Donnell [O’D04], where a hardness ampliﬁcation
lemma is given for NP. For a sharper result, see Healy, Vadhan, and Viola [HVV04].
Exercises
§1 Let X1, . . . , Xn be independent random variables such that Xi is equal to 1 with probability
1 −δ and equal to 0 with probability δ. Let X = Pk
i=1 Xi (mod 2). Prove that Pr[X = 1] =
1/2 + (1 −2δ)k.
Hint:Deﬁne Yi = (−1)Xi and Y = Qk
i=1 Yi. Then, use the fact
that the expectation of a product of independent random variables
is the product of their expectations.
§2 Prove Farkas’ Lemma: if C, D ⊆Rm are two convex sets then there exists a vector z ∈Rm
and a number a ∈R such that
x ∈C ⇒⟨x, z⟩≥a
y ∈D ⇒⟨y, z⟩≤a
Hint: Start by proving this in the case that C and D are ϵ-
separated, which means that for some ϵ > 0, ∥x−y∥2 ≥ϵ for every
x ∈C and y ∈D. In this case you can take z to be the shortest
vector of the form x −y for x ∈C and y ∈D.
§3 Prove the Min-Max Theorem (see Note 17.7) using Farkas’ Lemma.
§4 Prove the duality theorem for linear programming using Farkas’ Lemma. That is, prove that
for every m × n matrix A, and vectors c ∈Rn, b ∈Rn,
max
x∈Rns.t.
Ax≤b
x≥0
⟨x, c⟩=
min
y∈Rms.t.
A†y≥c
y≥0
⟨y, b⟩
where A† denotes the transpose of A and for two vectors u, v we say that u ≥v if ui ≥vi
for every i.
§5 Suppose we know that NP contains a function that is weakly hard for all polynomial-size
circuits. Can we use the XOR Lemma to infer the existence of a strongly hard function in
NP? Why or why not?
Web draft 2007-01-08 21:59

DRAFT
17.8. LOCAL LIST DECODING: GETTING TO BPP = P.
p17.29 (349)
§6 For every δ < 1/2 and suﬃciently large n, prove that there exists a function E : {0, 1}n →
{0, 1}n/(1−H(δ)) that is an error correcting code with distance δ, where H(δ) = δ log(1/δ) +
(1 −δ) log(1/(1 −δ)).
Hint:Use a greedy strategy, to select the codewords of E one by
one, never adding a codeword that is within distance δ to previous
ones. When will you get stuck?
§7 Show that for every E : {0, 1}n →{0, 1}m that is an error correcting code of distance 1/2,
2n < 10√n. Show if E is an error correcting code of distance δ > 1/2, then 2n < 10/(δ −1/2).
§8 Let E : {0, 1}n →{0, 1}m be a δ-distance ECC. Transform E to a code E′ : {0, 1, 2, 3}n/2 →
{0, 1, 2, 3}m/2 in the obvious way.
Show that E′ has distance δ.
Show that the opposite
direction is not true: show an example of a δ-distance ECC E′ : {0, 1, 2, 3}n/2 →{0, 1, 2, 3}m/2
such that the corresponding binary code has distance 2δ.
§9 Let f :F →F be any function. Suppose integer d ≥0 and number ϵ satisfy ϵ > 2
q
d
|F|. Prove
that there are at most 2/ϵ degree d polynomials that agree with f on at least an ϵ fraction of
its coordinates.
Hint: The ﬁrst polynomial describes f in an ϵ fraction of points
say S1, the second polynomial describes f in ϵ −d/|F| fraction of
points S2 where S1 ∩S2 = ∅, etc.
§10 (Linear codes) We say that an ECC E : {0, 1}n →{0, 1}m is linear if for every x, x′ ∈{0, 1}n,
E(x + x′) = E(x) + E(x′) where + denotes componentwise addition modulo 2. A linear ECC
E can be described by an m × n matrix A such that (thinking of x as a column vector)
E(x) = Ax for every x ∈{0, 1}n.
(a) Prove that the distance of a linear ECC E is equal to the minimum over all nonzero
x ∈{0, 1}n of the fraction of 1’s in E(x).
(b) Prove that for every δ > 0, there exists a linear ECC E : {0, 1}n →{0, 1}1.1n/(1−H(δ))
with distance δ, where H(δ) = δ log(1/δ) + (1 −δ) log(1/(1 −δ))¿
Hint:Use the probabilistic method - show this holds for a random
matrix.
(c) Prove that for some δ > 0 there is an ECC E : {0, 1}n →{0, 1}poly(n) of distance δ with
polynomial-time encoding and decoding mechanisms. (You need to know about the ﬁeld
GF(2k) to solve this, see Appendix A.)
Hint: Use the concatenation of Reed-Solomon over GF(2k) with
the Walsh-Hadamard code.
(d) We say that a linear code E : {0, 1}n →{0, 1}m is ϵ-biased if for every non-zero x ∈
{0, 1}n, the fraction of 1’s in E(x) is between 1/2−ϵ and 1/2+ϵ. Prove that for every ϵ > 0,
there exists an ϵ-biased linear code E : {0, 1}n →{0, 1}poly(n/ϵ) with a polynomial-time
encoding algorithm.
Web draft 2007-01-08 21:59

DRAFT
p17.30 (350)
17.8. LOCAL LIST DECODING: GETTING TO BPP = P.
Web draft 2007-01-08 21:59

DRAFT
Chapter 18
PCP and Hardness of Approximation
“...most problem reductions do not create or preserve such gaps...To create such a
gap in the generic reduction (cf. Cook)...also seems doubtful. The intuitive reason
is that computation is an inherently unstable, non-robust mathematical object, in the
the sense that it can be turned from non-accepting to accepting by changes that would
be insigniﬁcant in any reasonable metric.”
Papadimitriou and Yannakakis, 1991 [PY91]
The PCP Theorem provides an interesting new characterization for NP, as the set of languages
that have a “locally testable” membership proof. It is reminiscent of —and was motivated by—
results such as IP =PSPACE. Its essence is the following:
Suppose somebody wants to convince you that a Boolean formula is satisﬁable. He could present
the usual certiﬁcate, namely, a satisfying assignment, which you could then check by substituting
back into the formula.
However, doing this requires reading the entire certiﬁcate.
The PCP
Theorem shows an interesting alternative: this person can easily rewrite his certiﬁcate so you
can verify it by probabilistically selecting a constant number of locations—as low as 3 bits— to
examine in it.
Furthermore, this probabilistic veriﬁcation has the following properties: (1) A
correct certiﬁcate will never fail to convince you (that is, no choice of your random coins will make
you reject it) and (2) If the formula is unsatisﬁable, then you are guaranteed to reject every claimed
certiﬁcate with high probability.
Of course, since Boolean satisﬁability is NP-complete, every other NP language can be deter-
ministically and eﬃciently reduced to it. Thus the PCP Theorem applies to every NP language.
We mention one counterintuitive consequence. Let A be any one of the usual axiomatic systems of
mathematics for which proofs can be veriﬁed by a deterministic TM in time that is polynomial in
the length of the proof. Recall the following language is in NP:
L = {⟨ϕ, 1n⟩: ϕ has a proof in A of length ≤n} .
The PCP Theorem asserts that L has probabilistically checkable certiﬁcates. Such certiﬁcate
can be viewed as an alternative notion of “proof” for mathematical statements that is just as valid
as the usual notion. However, unlike standard mathematical proofs, where every line of the proof
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p18.1 (351)

DRAFT
p18.2 (352)
18.1. PCP AND LOCALLY TESTABLE PROOFS
has to be checked to verify its validity, this new notion guarantees that proofs are probabilistically
checkable by examining only a constant number of bits in them1.
This new, “robust” notion of certiﬁcate/proof has an important consequence: it implies that
many optimization problems are NP-hard not only to solve exactly but even to approximate. As
mentioned in Chapter 2, the P versus NP question is practically important —as opposed to “just”
philosophically important— because thousands of real-life combinatorial optimization problems are
NP-hard. By showing that even computing approximate solutions to many of these problems is
NP-hard, the PCP Theorem extends the practical importance of the theory of NP-completeness,
as well as its philosophical signiﬁcance.
This seemingly mysterious connection between the PCP Theorem —which concerns probabilis-
tic checking of certiﬁcates— and the NP-hardness of computing approximate solutions is actually
quite straightforward. All NP-hardness results ultimately derive from the Cook-Levin theorem
(Section 2.3), which expresses accepting computations of a nondeterministic Turing Machine with
satisfying assignments to a Boolean formula. Unfortunately, the standard representations of com-
putation are quite nonrobust, meaning that they can be incorrect if even one bit is incorrect (see
the quote at the start of this chapter). The PCP Theorem, by giving a robust representation of
the certiﬁcate for NP languages, allow new types of reductions; see Section 18.2.3.
Below, we use the term “PCP Theorems” for the body of other results of a similar nature to
the PCP Theorem that found numerous applications in complexity theory. Some important ones
appear in the next Chapter, including one that improves the PCP Theorem so that veriﬁcation is
possible by reading only 3 bits in the proof!
18.1
PCP and Locally Testable Proofs
According to our usual deﬁnition, language L is in NP if there is a poly-time Turing machine V
(“veriﬁer”) that, given input x, checks certiﬁcates (or membership proofs) to the eﬀect that x ∈L.
This means,
x ∈L ⇒∃π s.t. V π(x) = 1
x /∈L ⇒∀π
V π(x) = 0,
where V π denotes “a veriﬁer with access to certiﬁcate π”.
The class PCP (short for “Probabilistically Checkable Proofs”) is a generalization of this notion,
with the following changes. First, the veriﬁer is probabilistic. Second, the veriﬁer has random access
to the proof string Π. This means that each bit of the proof string can be independently queried
by the veriﬁer via a special address tape: if the veriﬁer desires say the ith bit in the proof string,
it writes i on the address tape and then receives the bit π[i].2 (This is reminiscent of oracle TMs
introduced in Chapter 3.) The deﬁnition of PCP treats queries to the proof as a precious resource,
to be used sparingly. Note also that since the address size is logarithmic in the proof size, this model
in principle allows a polynomial-time veriﬁer to check membership proofs of exponential size.
1One newspaper article about the discovery of the PCP Theorem carried the headline “New shortcut found for
long math proofs!”
2Though widely used, the term “random access” is misleading since it doesn’t involve any notion of randomness
per se. “Indexed access” would be more accurate.
Web draft 2007-01-08 21:59

DRAFT
18.1. PCP AND LOCALLY TESTABLE PROOFS
p18.3 (353)
Veriﬁers can be adaptive or nonadaptive. A nonadaptive veriﬁer selects its queries based only
on its input and random tape, whereas an adaptive veriﬁer can in addition rely upon bits it has
already queried in π to select its next queries. We restrict veriﬁers to be nonadaptive, since most
PCP Theorems can be proved using nonadaptive veriﬁers. (But Exercise 3 explores the power of
adaptive queries.)
Verifier
Input: x in {0,1}n
r(n) coins
q(n) queries
proof: π
Figure 18.1: A PCP veriﬁer for a language L gets an input x and random access to a string π. If x ∈L then there
exists a string π that makes the veriﬁer accepts, while if x ̸∈L then the veriﬁer rejects every proof π with probability
at least 1/2.
Definition 18.1 ((r, q)-verifier)
Let L be a language and q, r : N →N. We say that L has an (r(n), q(n))-veriﬁer if there’s a
polynomial-time probabilistic algorithm V satisfying:
Eﬃciency: On input a string x ∈{0, 1}n and given random access to a string π ∈{0, 1}∗(which
we call the proof ), V uses at most r(n) random coins and makes at most q(n) non-adaptive
queries to locations of π (see Figure 18.1). Then it outputs “1”(for “accept”) or “0” (for
“reject”). We use the notation V π(x) to denote the random variable representing V ’s output
on input x and with random access to π.
Completeness: If x ∈L then there exists a proof π ∈{0, 1}∗such that Pr[V π(x) = 1] = 1. We
call π the correct proof for x.
Soundness: If x ̸∈L then for every proof π ∈{0, 1}∗, Pr[V π(x) = 1] ≤1/2.
We say that a language L is in PCP(r(n), q(n)) if L has a (c · r(n), d · q(n))-veriﬁer for some
constants c, d.
Sometimes we consider veriﬁers for which the probability “1/2” is replaced by some other number,
called the soundness parameter.
Theorem 18.2 (PCP Theorem [AS98, ALM+98])
NP = PCP(log n, 1).
Notes:
1. Without loss of generality, proofs checkable by an (r, q)-veriﬁer contain at most q2r bits. The
veriﬁer looks at only q places of the proof for any particular choice of its random coins, and
there are only 2r such choices. Any bit in the proof that is read with 0 probability (i.e., for
no choice of the random coins) can just be deleted.
Web draft 2007-01-08 21:59

DRAFT
p18.4 (354)
18.1. PCP AND LOCALLY TESTABLE PROOFS
2. The previous remark implies PCP(r(n), q(n)) ⊆NTIME(2O(r(n))q(n)). The proofs check-
able by an (r(n), q(n)-veriﬁer have size at most 2O(r(n))q(n). A nondeterministic machine
could guess the proof in 2O(r(n))q(n) time, and verify it deterministically by running the ver-
iﬁer for all 2O(r(n)) possible choices of its random coin tosses. If the veriﬁer accepts for all
these possible coin tosses then the nondeterministic machine accepts.
As a special case, PCP(log n, 1) ⊆NTIME(2O(log n)) = NP: this is the trivial direction of
the PCP Theorem.
3. The constant 1/2 in the soundness requirement of Deﬁnition 18.1 is arbitrary, in the sense
that changing it to any other positive constant smaller than 1 will not change the class of
languages deﬁned. Indeed, a PCP veriﬁer with soundness 1/2 that uses r coins and makes q
queries can be converted into a PCP veriﬁer using cr coins and cq queries with soundness
2−c by just repeating its execution c times (see Exercise 1).
Example 18.3
To get a better sense for what a PCP proof system looks like, we sketch two nontrivial PCP
systems:
1. The language GNI of pairs of non-isomorphic graphs is in PCP(poly(n), 1). Say the input
for GNI is ⟨G0, G1⟩, where G0, G1 have both n nodes. The veriﬁer expects π to contain, for
each labeled graph H with n nodes, a bit π[H] ∈{0, 1} corresponding to whether H ≡G0 or
H ≡G1 (π[H] can be arbitrary if neither case holds). In other words, π is an (exponentially
long) array of bits indexed by the (adjacency matrix representations of) all possible n-vertex
graphs.
The veriﬁer picks b ∈{0, 1} at random and a random permutation. She applies the permuta-
tion to the vertices of Gb to obtain an isomorphic graph, H. She queries the corresponding
bit of π and accepts iﬀthe bit is b.
If G0 ̸≡G1, then clearly a proof π can be constructed which makes the veriﬁer accept with
probability 1. If G1 ≡G2, then the probability that any π makes the veriﬁer accept is at
most 1/2.
2. The protocols in Chapter 8 can be used (see Exercise 5) to show that the permanent has
PCP proof system with polynomial randomness and queries. Once again, the length of the
proof will be exponential.
In fact, both of these results are a special case of the following theorem a “scaled-up” version
of the PCP Theorem which we will not prove.
Theorem 18.4 (Scaled-up PCP, [?, ALM+98, AS98])
PCP(poly, 1) = NEXP
Web draft 2007-01-08 21:59

DRAFT
18.2. PCP AND HARDNESS OF APPROXIMATION
p18.5 (355)
18.2
PCP and Hardness of Approximation
The PCP Theorem implies that for many NP optimization problems, computing near-optimal
solutions is no easier than computing exact solutions.
We illustrate the notion of approximation algorithms with an example. MAX 3SAT is the prob-
lem of ﬁnding, given a 3CNF Boolean formula ϕ as input, an assignment that maximizes the number
of satisﬁed clauses. This problem is of course NP-hard, because the corresponding decision prob-
lem, 3SAT, is NP-complete.
Definition 18.5
For every 3CNF formula ϕ, deﬁne val(ϕ) to be the maximum fraction of clauses that can be satisﬁed
by any assignment to ϕ’s variables. In particular, if ϕ is satisﬁable then val(ϕ) = 1.
Let ρ ≤1. An algorithm A is a ρ-approximation algorithm for MAX 3SAT if for every 3CNF
formula ϕ with m clauses, A(ϕ) outputs an assignment satisfying at least ρ·val(ϕ)m of ϕ’s clauses.
In many practical settings, obtaining an approximate solution to a problem may be almost as
good as solving it exactly. Moreover, for some computational problems, approximation is much
easier than an exact solution.
Example 18.6 (1/2-approximation for MAX 3SAT)
We describe a polynomial-time algorithm that computes a 1/2-approximation for MAX 3SAT. The
algorithm assigns values to the variables one by one in a greedy fashion, whereby the ith variable is
assigned the value that results in satisfying at least 1/2 the clauses in which it appears. Any clause
that gets satisﬁed is removed and not considered in assigning values to the remaining variables.
Clearly, the ﬁnal assignment will satisfy at least 1/2 of all clauses, which is certainly at least half of
the maximum that the optimum assignment could satisfy.
Using semideﬁnite programming one can also design a polynomial-time (7/8−ϵ)-approximation
algorithm for every ϵ > 0 (see references). (Obtaining such a ratio is trivial if we restrict ourselves
to 3CNF formulae with three distinct variables in each clause. Then a random assignment has
probability 7/8 to satisfy it, and by linearity of expectation, is expected to satisfy a 7/8 fraction
of the clauses. This observation can be turned into a simple probabilistic or even deterministic
7/8-approximation algorithm.)
For a few problems, one can even design (1 −ϵ)-approximation algorithms for every ϵ > 0.
Exercise 10 asks you to show this for the NP-complete knapsack problem.
Researchers are extremely interested in ﬁnding the best possible approximation algorithms for
NP-hard optimization problems. Yet, until the early 1990’s most such questions were wide open. In
particular, we did not know whether MAX 3SAT has a polynomial-time ρ-approximation algorithm
for every ρ < 1. The PCP Theorem has the following Corollary.
Corollary 18.7
There exists some constant ρ < 1 such that if there is a polynomial-time ρ-approximation algorithm
for MAX 3SAT then P = NP.
Web draft 2007-01-08 21:59

DRAFT
p18.6 (356)
18.2. PCP AND HARDNESS OF APPROXIMATION
Later, in Chapter 19, we show a stronger PCP Theorem by H˚astad which implies that for
every ϵ > 0, if there is a polynomial-time (7/8+ϵ)-approximation algorithm for MAX 3SAT then
P = NP. Hence the approximation algorithm for this problem mentioned in Example 18.6 is very
likely optimal. The PCP Theorem (and the other PCP theorems that followed it) imply a host
of such hardness of approximation results for many important problems, often showing that known
approximation algorithms are optimal.
18.2.1
Gap-producing reductions
To prove Corollary 18.7 for some ﬁxed ρ < 1, it suﬃces to give a polynomial-time reduction f that
maps 3CNF formulae to 3CNF formulae such that:
ϕ ∈3SAT ⇒val(f(ϕ)) = 1
(1)
ϕ ̸∈L ⇒val(f(ϕ)) < ρ
(2)
After all, if a ρ-approximation algorithm were to exist for MAX 3SAT, then we could use it to
decide membership of any given formula ϕ in 3SAT by applying reduction f on ϕ and then running
the approximation algorithm on the resultant 3CNF formula f(ϕ).
If val(f(ϕ) = 1, then the
approximation algorithm would return an assignment that satisﬁes at least ρ fraction of the clauses,
which by property (2) tells us that ϕ ∈3SAT.
Later (in Section 18.2) we show that the PCP Theorem is equivalent to the following Theorem:
Theorem 18.8
There exists some ρ < 1 and a polynomial-time reduction f satisfying (1) and (2).
By the discussion above, Theorem 18.8 implies Corollary 18.7 and so rules out a polynomial-time
ρ-approximation algorithm for MAX 3SAT (unless P = NP).
Why doesn’t the Cook-Levin reduction suﬃce to prove Theorem 18.8?
The ﬁrst thing
one would try is the reduction from any NP language to 3SAT in the Cook-Levin Theorem (Theo-
rem 2.10). Unfortunately, it doesn’t give such an f because it does not satisfy property (2): we can
always satisfy almost all of the clauses in the formulae produced by the reduction (see Exercise 9
and also the “non-robustness” quote at the start of this chapter).
18.2.2
Gap problems
The above discussion motivates the deﬁnition of gap problems, a notion implicit in (1) and (2). It
is also an important concept in the proof of the PCP Theorem itself.
Definition 18.9 (GAP 3SAT)
Let ρ ∈(0, 1). The ρ-GAP 3SAT problem is to determine, given a 3CNF formula ϕ whether:
• ϕ is satisﬁable, in which case we say ϕ is a YES instance of ρ-GAP 3SAT.
• val(ϕ) ≤ρ, in which case we say ϕ is a NO instance of ρ-GAP 3SAT.
Web draft 2007-01-08 21:59

DRAFT
18.2. PCP AND HARDNESS OF APPROXIMATION
p18.7 (357)
An algorithm A is said to solve ρ-GAP 3SAT if A(ϕ) = 1 if ϕ is a YES instance of ρ-GAP 3SAT
and A(ϕ) = 0 if ϕ is a NO instance. Note that we do not make any requirement on A(ϕ) if ϕ is
neither a YES nor a NO instance of ρ-GAP qCSP.
Our earlier discussion of the desired reduction f can be formalized as follows.
Definition 18.10
Let ρ ∈(0, 1). We say that ρ-GAP 3SAT is NP-hard if for every language L there is a polynomial-
time computable function f such that
x ∈L ⇒f(x) is a YES instance of ρ-GAP 3SAT
x ̸∈L ⇒f(x) is a NO instance of ρ-GAP 3SAT
18.2.3
Constraint Satisfaction Problems
Now we generalize the deﬁnition of 3SAT to constraint satisfaction problems (CSP), which allow
clauses of arbitrary form (instead of just OR of literals) including those depending upon more than
3 variables. Sometimes the variables are allowed to be non-Boolean. CSPs arise in a variety of
application domains and play an important role in the proof of the PCP Theorem.
Definition 18.11
Let q, W be natural numbers. A qCSPW instance ϕ is a collection of functions ϕ1, . . . , ϕm (called
constraints) from {0..W−1}n to {0, 1} such that each function ϕi depends on at most q of its input
locations. That is, for every i ∈[m] there exist j1, . . . , jq ∈[n] and f : {0..W−1}q →{0, 1} such
that ϕi(u) = f(uj1, . . . , ujq) for every u ∈{0..W−1}n.
We say that an assignment u ∈{0..W−1}n satisﬁes constraint ϕi if ϕi(u) = 1. The fraction of
constraints satisﬁed by u is
Pm
i=1 ϕi(u)
m
, and we let val(ϕ) denote the maximum of this value over all
u ∈{0..W−1}n. We say that ϕ is satisﬁable if val(ϕ) = 1.
We call q the arity of ϕ and W the alphabet size. If W = 2 we say that ϕ uses a binary alphabet
and call ϕ a qCSP-instance (dropping the subscript 2).
Example 18.12
3SAT is the subcase of qCSPW where q = 3, W = 2, and the constraints are OR’s of the involved
literals.
Similarly, the NP-complete problem 3COL can be viewed as a subcase of 2CSP3 instances where
for each edge (i, j), there is a constraint on the variables ui, uj that is satisﬁed iﬀui ̸= uj. The
graph is 3-colorable iﬀthere is a way to assign a number in {0, 1, 2} to each variable such that all
constraints are satisﬁed.
Notes:
Web draft 2007-01-08 21:59

DRAFT
p18.8 (358)
18.2. PCP AND HARDNESS OF APPROXIMATION
1. We deﬁne the size of a qCSPW -instance ϕ to be the number of constraints m it has. Because
variables not used by any constraints are redundant, we always assume n ≤qm. Note that a
qCSPW instance over n variables with m constraints can be described using O(mnqW q) bits.
Usually q, W will be constants (independent of n, m).
2. As in the case of 3SAT, we can deﬁne maximization and gap problems for CSP instances.
In particular, for any ρ ∈(0, 1), we deﬁne ρ-GAP qCSPW as the problem of distinguishing
between a qCSPW -instance ϕ that is satisﬁable (called a YES instance) and an instance ϕ
with val(ϕ) ≤ρ (called a NO instance). As before, we will drop the subscript W in the case
of a binary alphabet.
3. The simple greedy approximation algorithm for 3SAT can be generalized for the MAX qCSP
problem of maximizing the number of satisﬁed constraints in a given qCSP instance. That
is, for any qCSPW instance ϕ with m constraints, the algorithm will output an assignment
satisfying val(ϕ)
W q m constraints. Thus, unless NP ⊆P, the problem 2−q-GAP qCSP is not NP
hard.
18.2.4
An Alternative Formulation of the PCP Theorem
We now show how the PCP Theorem is equivalent to the NP-hardness of a certain gap version of
qCSP. Later, we will refer to this equivalence as the “hardness of approximation viewpoint” of the
PCP Theorem.
Theorem 18.13 (PCP Theorem, alternative formulation)
There exist constants q ∈N, ρ ∈(0, 1) such that ρ-GAP qCSP is NP-hard.
We now show Theorem 18.13 is indeed equivalent to the PCP Theorem:
Theorem 18.2 implies Theorem 18.13.
Assume that NP ⊆PCP(log n, 1). We will show
that 1/2-GAP qCSP is NP-hard for some constant q. It is enough to reduce a single NP-complete
language such as 3SAT to 1/2-GAP qCSP for some constant q. Under our assumption, 3SAT has a
PCP system in which the veriﬁer V makes a constant number of queries, which we denote by q,
and uses c log n random coins for some constant c. Given every input x and r ∈{0, 1}c log n, deﬁne
Vx,r to be the function that on input a proof π outputs 1 if the veriﬁer will accept the proof π on
input x and coins r. Note that Vx,r depends on at most q locations. Thus for every x ∈{0, 1}n, the
collection ϕ = {Vx,r}r∈{0,1}c log n is a polynomial-sized qCSP instance. Furthermore, since V runs in
polynomial-time, the transformation of x to ϕ can also be carried out in polynomial-time. By the
completeness and soundness of the PCP system, if x ∈3SAT then ϕ will satisfy val(ϕ) = 1, while
if x ̸∈3SAT then ϕ will satisfy val(ϕ) ≤1/2. ■
Theorem 18.13 implies Theorem 18.2.
Suppose that ρ-GAP qCSP is NP-hard for some con-
stants q,ρ < 1. Then this easily translates into a PCP system with q queries, ρ soundness and
logarithmic randomness for any language L: given an input x, the veriﬁer will run the reduction
f(x) to obtain a qCSP instance ϕ = {ϕi}m
i=1. It will expect the proof π to be an assignment to the
Web draft 2007-01-08 21:59

DRAFT
18.2. PCP AND HARDNESS OF APPROXIMATION
p18.9 (359)
variables of ϕ, which it will verify by choosing a random i ∈[m] and checking that ϕi is satisﬁed
(by making q queries). Clearly, if x ∈L then the veriﬁer will accept with probability 1, while if
x ̸∈L it will accept with probability at most ρ. The soundness can be boosted to 1/2 at the expense
of a constant factor in the randomness and number of queries (see Exercise 1). ■
Remark 18.14
Since 3CNF formulas are a special case of 3CSP instances, Theorem 18.8 (ρ-GAP 3SAT is NP-hard)
implies Theorem 18.13 (ρ-GAP qCSP is NP-hard). Below we show Theorem 18.8 is also implied by
Theorem 18.13, concluding that it is also equivalent to the PCP Theorem.
It is worth while to review this very useful equivalence between the “proof view” and the
“hardness of approximation view” of the PCP Theorem:
PCP veriﬁer (V )
←→
CSP instance (ϕ)
PCP proof (π)
←→
Assignment to variables (u)
Length of proof
←→
Number of variables (n)
Number of queries (q)
←→
Arity of constraints (q)
Number of random bits (r)
←→
Logarithm of number of constraints (log m)
Soundness parameter
←→
Maximum of val(ϕ) for a NO instance
Theorem 18.2 (NP ⊆PCP(log n, 1))
←→
Theorem 18.13 (ρ-GAP qCSP is NP-hard)
18.2.5
Hardness of Approximation for 3SAT and INDSET.
The CSP problem allows arbitrary functions to serve as constraints, which may seem somewhat
artiﬁcial. We now show how Theorem 18.13 implies hardness of approximation results for the more
natural problems of MAX 3SAT (determining the maximum number of clauses satisﬁable in a 3SAT
formula) and MAX INDSET (determining the size of the largest independent set in a given graph).
The following two lemmas use the PCP Theorem to show that unless P = NP, both MAX 3SAT
and MAX INDSET are hard to approximate within a factor that is a constantless than 1. ( Sec-
tion 18.3 proves an even stronger hardness of approximation result for INDSET.)
Lemma 18.15 (Theorem 18.8, restated)
There exists a constant 0 < ρ < 1 such that ρ-GAP 3SAT is NP-hard.
Lemma 18.16
There exist a polynomial-time computable transformation f from 3CNF formulae to graphs such
that for every 3CNF formula ϕ, f(ϕ) is an n-vertex graph whose largest independent set has size
val(ϕ)n
7 .
Proof of Lemma 18.15: Let ϵ > 0 and q ∈N be such that by Theorem 18.13, (1−ϵ)-GAP qCSP
is NP-hard.
We show a reduction from (1−ϵ)-GAP qCSP to (1−ϵ′)-GAP 3SAT where ϵ′ > 0 is
some constant depending on ϵ and q. That is, we will show a polynomial-time function mapping
YES instances of (1−ϵ)-GAP qCSP to YES instances of (1−ϵ′)-GAP 3SAT and NO instances of
(1−ϵ)-GAP qCSP to NO instances of (1−ϵ′)-GAP 3SAT.
Let ϕ be a qCSP instance over n variables with m constraints. Each constraint ϕi of ϕ can be
expressed as an AND of at most 2q clauses, where each clause is the OR of at most q variables
Web draft 2007-01-08 21:59

DRAFT
p18.10 (360)
18.2. PCP AND HARDNESS OF APPROXIMATION
or their negations. Let ϕ′ denote the collection of at most m2q clauses corresponding to all the
constraints of ϕ. If ϕ is a YES instance of (1−ϵ)-GAP qCSP (i.e., it is satisﬁable) then there exists
an assignment satisfying all the clauses of ϕ′. if ϕ is a NO instance of (1−ϵ)-GAP qCSP then every
assignment violates at least an ϵ fraction of the constraints of ϕ and hence violates at least an
ϵ
2q
fraction of the constraints of ϕ. We can use the Cook-Levin technique of Chapter 2 (Theorem 2.10),
to transform any clause C on q variables on u1, . . . , uq to a set C1, . . . , Cq of clauses over the variables
u1, . . . , uq and additional auxiliary variables y1, . . . , yq such that (1) each clause Ci is the OR of at
most three variables or their negations, (2) if u1, . . . , uq satisfy C then there is an assignment to
y1, . . . , yq such that u1, . . . , uq, y1, . . . , yq simultaneously satisfy C1, . . . , Cq and (3) if u1, . . . , uq does
not satisfy C then for every assignment to y1, . . . , yq, there is some clause Ci that is not satisﬁes
by u1, . . . , uq, y1, . . . , yq.
Let ϕ′′ denote the collection of at most qm2q clauses over the n + qm variables obtained in this
way from ϕ′. Note that ϕ′′ is a 3SAT formula. Our reduction will map ϕ to ϕ′′. Completeness holds
since if ϕ was satisﬁable then so will be ϕ′ and hence ϕ′′. Soundness holds since if every assignment
violates at least an ϵ fraction of the constraints of ϕ, then every assignment violates at least an
ϵ
2q
fraction of the constraints of ϕ′, and so every assignment violates at least an
ϵ
q2q fraction of the
constraints of ϕ′′. ■
Proof of Lemma 18.16:
Let ϕ be a 3CNF formula on n variables with m clauses. We deﬁne
a graph G of 7m vertices as follows: we associate a cluster of 7 vertices in G with each clause of
ϕ. The vertices in cluster associated with a clause C correspond to the 7 possible assignments to
the three variables C depends on (we call these partial assignments, since they only give values for
some of the variables). For example, if C is u2 ∨u5 ∨u7 then the 7 vertices in the cluster associated
with C correspond to all partial assignments of the form u1 = a, u2 = b, u3 = c for a binary vector
⟨a, b, c⟩̸= ⟨1, 1, 1⟩. (If C depends on less than three variables we treat one of them as repeated and
then some of the 7 vertices will correspond to the same assignment.) We put an edge between two
vertices of G if they correspond to inconsistent partial assignments. Two partial assignments are
consistent if they give the same value to all the variables they share. For example, the assignment
u1 = 1, u2 = 0, u3 = 0 is inconsistent with the assignment u3 = 1, u5 = 0, u7 = 1 because they share
a variable (u3) to which they give a diﬀerent value. In addition, we put edges between every two
vertices that are in the same cluster.
Clearly transforming ϕ into G can be done in polynomial time. Denote by α(G) to be the
size of the largest independent set in G. We claim that α(G) = val(ϕ)m. For starters, note that
α(G) ≥val(ϕ)m. Indeed, let u be the assignment that satisﬁes val(ϕ)m clauses. Deﬁne a set S as
follows: for each clause C satisﬁed by u, put in S the vertex in the cluster associated with C that
corresponds to the restriction of u to the variables C depends on. Because we only choose vertices
that correspond to restrictions of the assignment u, no two vertices of S correspond to inconsistent
assignments and hence S is an independent set of size val(ϕ)m.
Suppose that G has an independent set S of size k. We will use S to construct an assignment
u satisfying k clauses of ϕ, thus showing that val(ϕ)m ≥α(G). We deﬁne u as follows: for every
i ∈[n], if there is a vertex in S whose partial assignment gives a value a to ui, then set ui = a;
otherwise set ui = 0. This is well deﬁned because S is an independent set, and each variable ui
can get at most a single value by assignments corresponding to vertices in S. On the other hand,
because we put all the edges within each cluster, S can contain at most a single vertex in each
Web draft 2007-01-08 21:59

DRAFT
18.3. N−δ-APPROXIMATION OF INDEPENDENT SET IS NP-HARD.
p18.11 (361)
cluster, and hence there are k distinct cluster with members in S. By our deﬁnition of u it satisﬁes
all the clauses associated with these clusters. ■
Remark 18.17
In Chapter 2, we deﬁned L′ to be NP-hard if every L ∈NP reduces to L′.
The reduction
was a polynomial-time function f such that x ∈L ⇔f(x) ∈L′. In all cases, we proved that
x ∈L ⇒f(x) ∈L′ by showing a way to map a certiﬁcate to the fact that x ∈L to a certiﬁcate
to the fact that x′ ∈L′. Although the deﬁnition of a Karp reduction does not require that this
mapping is eﬃcient, it often turned out that the proof did provide a way to compute this mapping
in polynomial time. The way we proved that f(x) ∈L′ ⇒x ∈L was by showing a way to map a
certiﬁcate to the fact that x′ ∈L′ to a certiﬁcate to the fact that x ∈L. Once again, the proofs
typically yield an eﬃcient way to compute this mapping.
A similar thing happens in the gap preserving reductions used in the proofs of Lemmas 18.15
and 18.16 and elsewhere in this chapter. When reducing from, say, ρ-GAP qCSP to ρ′-GAP 3SAT
we show a function f that maps a CSP instance ϕ to a 3SAT instance ψ satisfying the following
two properties:
Completeness We can map a satisfying assignment of ϕ to a satisfying assignment to ψ
Soundness Given any assignment that satisﬁes more than a ρ′ fraction of ψ’s clauses, we can map
it back into an assignment satisfying more than a ρ fraction of ϕ’s constraints.
18.3
n−δ-approximation of independent set is NP-hard.
We now show a much stronger hardness of approximation result for the independent set (INDSET)
problem than Lemma 18.16. Namely, we show that there exists a constant δ ∈(0, 1) such that
unless P = NP, there is no polynomial-time nδ-approximation algorithm for INDSET. That is, we
show that if there is a polynomial-time algorithm A that given an n-vertex graph G outputs an
independent set of size at least opt
nδ (where opt is the size of the largest independent set in G) then
P = NP. We note that an even stronger result is known: the constant δ can be made arbitrarily
close to 1 [?, ?]. This factor is almost optimal since the independent set problem has a trivial
n-approximation algorithm: output a single vertex.
Our main tool will be the notion of expander graphs (see Note 18.18 and Chapter ??). Expander
graphs will also be used in the proof of PCP Theorem itself. We use here the following property
of expanders:
Lemma 18.19
Let G = (V, E) be a λ-expander graph for some λ ∈(0, 1). Let S be a subset of V with |S| = β|V |
for some β ∈(0, 1). Let (X1, . . . , Xℓ) be a tuple of random variables denoting the vertices of a
uniformly chosen (ℓ−1)-step path in G. Then,
(β −2λ)k ≤Pr[∀i∈[ℓ]Xi ∈S] ≤(β + 2λ)k
The upper bound of Lemma 18.19 is implied by Theorem ??; we omit the proof of the lower
bound.
The hardness result for independent set follows by combining the following lemma with Lemma 18.16:
Web draft 2007-01-08 21:59

DRAFT
p18.12 (362)
18.3. N−δ-APPROXIMATION OF INDEPENDENT SET IS NP-HARD.
Note 18.18 (Expander graphs)
Expander graphs are described in Chapter ??. We deﬁne there a parameter
λ(G) ∈[0, 1], for every regular graph G (see Deﬁnition 7.25). The main
property we need in this chapter is that for every regular graph G = (V, E)
and every S ⊆V with |S| ≤|V |/2,
Pr
(u,v)∈E[u ∈S, v ∈S] ≤|S|
|V |
1
2 + λ(G)
2

(3)
Another property we use is that λ(Gℓ) = λ(G)ℓfor every ℓ∈N, where Gℓis
obtained by taking the adjacency matrix of G to the ℓth power (i.e., an edge
in Gℓcorresponds to an (ℓ−1)-step path in G).
For every c ∈(0, 1), we call a regular graph G satisfying λ(G) ≤c a c-
expander graph.
If c < 0.9, we drop the preﬁx c and simply call G an
expander graph. (The choice of the constant 0.9 is arbitrary.) As shown
in Chapter ??, for every constant c ∈(0, 1) there is a constant d and an
algorithm that given input n ∈N, runs in poly(n) time and outputs the
adjacency matrix of an n-vertex d-regular c-expander (see Theorem 16.32).
Lemma 18.20
For every λ > 0 there is a polynomial-time computable reduction f that maps every n-vertex graph
G into an m-vertex graph H such that
(˜α(G) −2λ)log n ≤˜α(H) ≤(˜α(G) + 2λ)log n
where ˜α(G) is equal to the fractional size of the largest independent set in G.
Recall that Lemma 18.16 shows that there are some constants β, ϵ ∈(0, 1) such that it is NP-
hard to tell whether a given graph G satisﬁes (1) ˜α(G) ≥β or (2) ˜α(G) ≤(1 −ϵ)β. By applying
to G the reduction of Lemma 18.20 with parameter λ = βϵ/8 we get that in case (1), ˜α(H) ≥
(β−βϵ/4)log n = (β(1−ϵ/4))log n, and in case (2), ˜α(H) ≤((1−ϵ)β+βϵ/4)log n = (β(1−0.75ϵ))log n.
We get that the gap between the two cases is equal to clog n for some c > 1 which is equal to mδ
for some δ > 0 (where m = poly(n) is the number of vertices in H).
Proof of Lemma 18.20:
Let G, λ be as in the lemma’s statement. We let K be an n-vertex
λ-expander of degree d (we can obtain such a graph in polynomial-time, see Note 18.18). We will
map G into a graph H of ndlog n−1 vertices in the following way:
• The vertices of H correspond to all the (log n−1)-step paths in the λ-expander K.
• We put an edge between two vertices u, v of H corresponding to the paths ⟨u1, . . . , ulog n⟩and
⟨v1, . . . , vlog n⟩if there exists an edge in G between two vertices in the set {u1, . . . , ulog n, v1, . . . , vlog n}.
Web draft 2007-01-08 21:59

DRAFT
18.4. NP ⊆PCP(POLY(N), 1): PCP BASED UPON WALSH-HADAMARD CODEp18.13 (363)
A subset T of H’s vertices corresponds to a subset of log n-tuples of numbers in [n], which we
can identify as tuples of vertices in G. We let V (T) denote the set of all the vertices appearing in
one of the tuples of T. Note that in this notation, T is an independent set in H if and only if V (T)
is an independent set of G. Thus for every independent set T in H, we have that |V (T)| ≤˜α(G)n
and hence by the upper bound of Lemma 18.19, T takes up less than an (˜α(H) + 2λ)log n fraction
of H’s vertices. On the other hand, if we let S be the independent set of G of size ˜α(G)n then by
the lower bound of Lemma 18.19, an (˜α −2λ)log n fraction of H’s vertices correspond to paths fully
contained in S, implying that ˜α(H) ≥(˜α(G) −2λ)log n. ■
18.4
NP ⊆PCP(poly(n), 1): PCP based upon Walsh-Hadamard
code
We now prove a weaker version of the PCP theorem, showing that every NP statement has an
exponentially-long proof that can be locally tested by only looking at a constant number of bits. In
addition to giving a taste of how one proves PCP Theorems, this section builds up to a stronger
Corollary 18.26, which will be used in the proof of the PCP theorem.
Theorem 18.21
NP ⊆PCP(poly(n), 1)
We prove this theorem by designing an appropriate veriﬁer for an NP-complete language. The
veriﬁer expects the proof to contain an encoded version of the usual certiﬁcate. The veriﬁer checks
such an encoded certiﬁcate by simple probabilistic tests.
18.4.1
Tool: Linearity Testing and the Walsh-Hadamard Code
We use the Walsh-Hadamard code (see Section 17.5, though the treatment here is self-contained).
It is a way to encode bit strings of length n by linear functions in n variables over GF(2); namely,
the function WH : {0, 1}∗→{0, 1}∗mapping a string u ∈{0, 1}n to the truth table of the function
x 7→u ⊙x, where for x, y ∈{0, 1}n we deﬁne x ⊙y = Pn
i=1 xiyi (mod 2). Note that this is a
very ineﬃcient encoding method: an n-bit string u ∈{0, 1}n is encoded using |WH(u)| = 2n bits.
If f ∈{0, 1}2n is equal to WH(u) for some u then we say that f is a Walsh-Hadamard codeword.
Such a string f ∈{0, 1}2n can also be viewed as a function from {0, 1}n to {0, 1}.
The Walsh-Hadamard code is an error correcting code with minimum distance 1/2, by which we
mean that for every u ̸= u′ ∈{0, 1}n, the encodings WH(u) and WH(u) diﬀer in half the bits. This
follows from the familiar random subsum principle (Claim A.5) since exactly half of the strings
x ∈{0, 1}n satisfy u ⊙x ̸= u′ ⊙x. Now we talk about local tests for the Walsh-Hadamard code
(i.e., tests making only O(1) queries).
Local testing of Walsh-Hadamard code.
Suppose we are given access to a function f :
{0, 1}n →{0, 1} and want to test whether or not f is actually a codeword of Walsh-Hadamard.
Since the Walsh-Hadamard codewords are precisely the set of all linear functions from {0, 1}n to
Web draft 2007-01-08 21:59

DRAFT
p18.14 (364)18.4. NP ⊆PCP(POLY(N), 1): PCP BASED UPON WALSH-HADAMARD CODE
{0, 1}, we can test f by checking that
f(x + y) = f(x) + f(y)
(4)
for all the 22n pairs x, y ∈{0, 1}n (where “+” on the left side of (pcp:eq:lintest) denotes vector
addition over GF(2)n and on the right side denotes addition over GF(2)).
But can we test f by querying it in only a constant number of places? Clearly, if f is not linear
but very close to being a linear function (e.g., if f is obtained by modifying a linear function on
a very small fraction of its inputs) then such a local test will not be able to distinguish f from a
linear function. Thus we set our goal on a test that on one hand accepts every linear function, and
on the other hand rejects with high probability every function that is far from linear. It turns out
that the natural test of choosing x, y at random and verifying (4) achieves this goal:
Definition 18.22
Let ρ ∈[0, 1]. We say that f, g : {0, 1}n →{0, 1} are ρ-close if Prx∈R{0,1}n[f(x) = g(x)] ≥ρ. We
say that f is ρ-close to a linear function if there exists a linear function g such that f and g are
ρ-close.
Theorem 18.23 (Linearity Testing [?])
Let f : {0, 1}n →{0, 1} be such that
Pr
x,y∈R{0,1}n[f(x + y) = f(x) + f(y)] ≥ρ
for some ρ > 1/2. Then f is ρ-close to a linear function.
We defer the proof of Theorem 18.23 to Section 19.3 of the next chapter. For every δ ∈(0, 1/2),
we can obtain a linearity test that rejects with probability at least 1/2 every function that is not
(1−δ)-close to a linear function, by testing Condition (4) repeatedly O(1/δ) times with independent
randomness. We call such a test a (1−δ)-linearity test.
Local decoding of Walsh-Hadamard code.
Suppose that for δ < 1
4 the function f : {0, 1}n →
{0, 1} is (1−δ)-close to some linear function ˜f. Because every two linear functions diﬀer on half of
their inputs, the function ˜f is uniquely determined by f. Suppose we are given x ∈{0, 1}n and
random access to f. Can we obtain the value ˜f(x) using only a constant number of queries? The
naive answer is that since most x’s satisfy f(x) = ˜f(x), we should be able to learn ˜f(x) with good
probability by making only the single query x to f. The problem is that x could very well be one of
the places where f and ˜f diﬀer. Fortunately, there is still a simple way to learn ˜f(x) while making
only two queries to f:
1. Choose x′ ∈R {0, 1}n.
2. Set x′′ = x + x′.
3. Let y′ = f(x′) and y′′ = f(x′′).
4. Output y′ + y′′.
Web draft 2007-01-08 21:59

DRAFT
18.4. NP ⊆PCP(POLY(N), 1): PCP BASED UPON WALSH-HADAMARD CODEp18.15 (365)
Since both x′ and x′′ are individually uniformly distributed (even though they are dependent),
by the union bound with probability at least 1 −2δ we have y′ = ˜f(x′) and y′′ = ˜f(x′′). Yet by
the linearity of ˜f, ˜f(x) = ˜f(x′ + x′′) = ˜f(x′) + ˜f(x′′), and hence with at least 1 −2δ probability
˜f(x) = y′ +y′′.3 This technique is called local decoding of the Walsh-Hadamard code since it allows
to recover any bit of the correct codeword (the linear function ˜f) from a corrupted version (the
function f) while making only a constant number of queries. It is also known as self correction of
the Walsh-Hadamard code.
18.4.2
Proof of Theorem 18.21
We will show a (poly(n), 1)-veriﬁer proof system for a particular NP-complete language L. The
result that NP ⊆PCP(poly(n), 1) follows since every NP language is reducible to L. The NP-
complete language L we use is QUADEQ, the language of systems of quadratic equations over
GF(2) = {0, 1} that are satisﬁable.
Example 18.24
The following is an instance of QUADEQ over the variables u1, . . . , u5:
u1u2 + u3u4 + u1u5 = 1
u2u3 + u1u4 = 0
u1u4 + u3u5 + u3u4 = 1
This instance is satisﬁable since the all-1 assignment satisﬁes all the equations.
More generally, an instance of QUADEQ over the variables u1, . . . , un is of the form AU = b,
where U is the n2-dimensional vector whose ⟨i, j⟩th entry is uiuj, A is an m × n2 matrix and
b ∈{0, 1}m. In other words, U is the tensor product u ⊗u, where x ⊗y for a pair of vectors
x, y ∈{0, 1}n denotes the n2-dimensional vector (or n × n matrix) whose (i, j) entry is xiyj. For
every i, j ∈[n] with i ≤j, the entry Ak,⟨i,j⟩is the coeﬃcient of uiuj in the kth equation (we identify
[n2] with [n] × [n] in some canonical way). The vector b consists of the right hand side of the m
equations. Since ui = (ui)2 in GF(2), we can assume the equations do not contain terms of the
form u2
i .
Thus a satisfying assignment consists of u1, u2, . . . , un ∈GF(2) such that its tensor product
U = u ⊗u satisﬁes AU = b.
We leave it as Exercise 12 to show that QUADEQ, the language of all
satisﬁable instances, is indeed NP-complete.
We now describe the PCP system for QUADEQ. Let A, b be an instance of QUADEQ and
suppose that A, b is satisﬁable by an assignment u ∈{0, 1}n. The correct PCP proof π for A, b
will consist of the Walsh-Hadamard encoding for u and the Walsh-Hadamard encoding for u ⊗u,
by which we mean that we will design the PCP veriﬁer in a way ensuring that it accepts proofs
3We use here the fact that over GF(2), a + b = a −b.
Web draft 2007-01-08 21:59

DRAFT
p18.16 (366)18.4. NP ⊆PCP(POLY(N), 1): PCP BASED UPON WALSH-HADAMARD CODE
WH(u)
WH(uOu)
x
Figure 18.2: The PCP proof that a set of quadratic equations is satisﬁable consists of WH(u) and WH(u ⊗u) for
some vector u. The veriﬁer ﬁrst checks that the proof is close to having this form, and then uses the local decoder of
the Walsh-Hadamard code to ensure that u is a solution for the quadratic equation instance.
of this form with probability one, satisfying the completeness condition. (Note that π is of length
2n + 2n2.)
Below, we repeatedly use the following fact:
random subsum principle: If u ̸= v then for at least 1/2 the choices of x, u⊙x ̸= v ⊙x. Realize
that x can be viewed as a random subset of indices in [1, . . . , n] and the principle says that with
probability 1/2 the sum of the ui’s over this index set is diﬀerent from the corresponding sum of vi’s.
The veriﬁer.
The veriﬁer V gets access to a proof π ∈{0, 1}2n+2n2
, which we interpret as a pair
of functions f : {0, 1}n →{0, 1} and g : {0, 1}n2 →{0, 1}.
Step 1: Check that f, g are linear functions.
As already noted, this isn’t something that the veriﬁer can check per se using local tests. Instead,
the veriﬁer performs a 0.99-linearity test on both f, g, and rejects the proof at once if either test
fails.
Thus, if either of f, g is not 0.99-close to a linear function, then V rejects with high probability.
Therefore for the rest of the procedure we can assume that there exist two linear functions ˜f :
{0, 1}n →{0, 1} and ˜g : {0, 1}n2 →{0, 1} such that ˜f is 0.99-close to f, and ˜g is 0.99-close to g.
(Note: in a correct proof, the tests succeed with probability 1 and ˜f = f and ˜g = g.)
In fact, we will assume that for Steps 2 and 3, the veriﬁer can query ˜f, ˜g at any desired point.
The reason is that local decoding allows the veriﬁer to recover any desired value of ˜f, ˜g with good
probability, and Steps 2 and 3 will only use a small (less than 15) number of queries to ˜f, ˜g. Thus
with high probability (say > 0.9) local decoding will succeed on all these queries.
notation: To simplify notation in the rest of the procedure we use f, g for ˜f, ˜g respectively.
Furthermore, we assume both f and g are linear, and thus they must encode some strings u ∈{0, 1}n
and w ∈{0, 1}n2. In other words, f, g are the functions given by f(r) = u ⊙r and g(z) = w ⊙z.
Step 2: Verify that g encodes u ⊗u, where u ∈{0, 1}n is the string encoded by f.
Veriﬁer V does the following test 3 times: “Choose r, r′ independently at random from {0, 1}n,
and if f(r)f(r′) ̸= g(r ⊗r′) then halt and reject.”
In a correct proof, w = u ⊗u, so
f(r)f(r′) =

X
i∈[n]
uiri



X
j∈[n]
ujr′
j

=
X
i,j∈[n]
uiujrir′
j = (u ⊗u) ⊙(r ⊗r′),
Web draft 2007-01-08 21:59

DRAFT
18.4. NP ⊆PCP(POLY(N), 1): PCP BASED UPON WALSH-HADAMARD CODEp18.17 (367)
which in the correct proof is equal to g(r ⊗r′). Thus Step 2 never rejects a correct proof.
Suppose now that, unlike the case of the correct proof, w ̸= u⊗u. We claim that in each of the
three trials V will halt and reject with probability at least 1
4. (Thus the probability of rejecting in
at least one trial is at least 1 −(3/4)3 = 37/64.) Indeed, let W be an n × n matrix with the same
entries as w, let U be the n × n matrix such that Ui,j = uiuj and think of r as a row vector and r′
as a column vector. In this notation,
g(r ⊗r′) = w ⊙(r ⊗r′) =
X
i,j∈[n]
wi,jrir′
j = rWr′
f(r)f(r′) = (u ⊙r)(u ⊙r′) = (
n
X
i=1
uiri)(
n
X
j=1
ujr′
j) =
X
i,j∈[n]
uiujrirj = rUr′
And V rejects if rWr′ ̸= rUr′. The random subsum principle implies that if W ̸= U then at
least 1/2 of all r satisfy rW ̸= rU. Applying the random subsum principle for each such r, we
conclude that at least 1/2 the r′ satisfy rWr′ ̸= rUr′. We conclude that the test rejects for at least
1/4 of all pairs r, r′.
Step 3: Verify that f encodes a satisfying assignment.
Using all that has been veriﬁed about f, g in the previous two steps, it is easy to check that any
particular equation, say the kth equation of the input, is satisﬁed by u, namely,
X
i,j
Ak,(i,j)uiuj = bk.
(5)
Denoting by z the n2 dimensional vector (Ak,(i,j)) (where i, j vary over [1..n]), we see that the
left hand side is nothing but g(z). Since the veriﬁer knows Ak,(i,j) and bk, it simply queries g at z
and checks that g(z) = bk.
The drawback of the above idea is that in order to check that u satisﬁes the entire system,
the veriﬁer needs to make a query to g for each k = 1, 2, . . . , m, whereas the number of queries is
required to be independent of m. Luckily, we can use the random subsum principle again! The
veriﬁer takes a random subset of the equations and computes their sum mod 2. (In other words,
for k = 1, 2, . . . , m multiply the equation in (5) by a random bit and take the sum.) This sum is a
new quadratic equation, and the random subsum principle implies that if u does not satisfy even
one equation in the original system, then with probability at least 1/2 it will not satisfy this new
equation. The veriﬁer checks that u satisﬁes this new equation.
(Actually, the above test has to be repeated twice to ensure that if u does not satisfy the system,
then Step 3 rejects with probability at least 3/4.)
18.4.3
PCP’s of proximity
Theorem 18.21 says that (exponential-sized) certiﬁcates for NP languages can be checked by ex-
amining only O(1) bits in them. The proof actually yields a somewhat stronger result, which will
be used in the proof of the PCP Theorem. This concerns the following scenario: we hold a circuit
C in our hands that has n input wires. Somebody holds a satisfying assignment u. He writes down
WH(u) as well as another string π for us. We do a probabilistic test on this by examining O(1) bits
in these strings, and at the end we are convinced of this fact.
Web draft 2007-01-08 21:59

DRAFT
p18.18 (368)18.4. NP ⊆PCP(POLY(N), 1): PCP BASED UPON WALSH-HADAMARD CODE
Concatenation test.
First we need to point out a property of Walsh-Hadamard codes and a
related concatenation test. In this setting, we are given two linear functions f, g that encode strings
of lengths n and n + m respectively. We have to check by examining only O(1) bits in f, g that if
u and v are the strings encoded by f, g (that is, f = WH(u) and h = WH(v)) then u is the same
as the ﬁrst n bits of v. By the random subsum principle, the following simple test rejects with
probability 1/2 if this is not the case. Pick a random x ∈{0, 1}n, and denote by X ∈GF(2)m+n
the string whose ﬁrst n bits are x and the remaining bits are all-0. Verify that f(X) = g(x).
With this test in hand, we can prove the following corollary.
Corollary 18.25 (Exponential-sized PCP of proximity.)
There exists a veriﬁer V that given any circuit C of size m and with n inputs has the following
property:
1. If u ∈{0, 1}n is a satisfying assignment for circuit C, then there is a string π2 of size 2poly(m)
such that V accepts WH(u) ◦π2 with probability 1. (Here ◦denotes concatenation.)
2. For every strings π1, π2 ∈{0, 1}∗, where π1 has 2n bits, if V accepts π1 ◦π2 with probability
at least 1/2, then π1 is 0.99-close to WH(u) for some u that satisﬁes C.
3. V uses poly(m) random bits and examines only O(1) bits in the provided strings.
Proof: One looks at the proof of NP-completeness of QUADEQ to realize that given circuit C
with n input wires and size m, it yields an instance of QUADEQ of size O(m) such that u ∈{0, 1}n
satisﬁes the circuit iﬀthere is a string v of size M = O(m) such that u ◦v satisﬁes the instance of
QUADEQ. (Note that we are thinking of u both as a string of bits that is an input to C and as a
string over GF(2)n that is a partial assignment to the variables in the instance of QUADEQ.)
The veriﬁer expects π2 to contain whatever our veriﬁer of Theorem 18.21 expects in the proof for
this instance of QUADEQ, namely, a linear function f that is WH(w), and another linear function
g that is WH(w ⊗w) where w satisﬁes the QUADEQ instance. The veriﬁer checks these functions
as described in the proof of Theorem 18.21.
However, in the current setting our verifer is also given a string π1 ∈{0, 1}2n. Think of this as
a function h:GF(2)n →GF(2). The veriﬁer checks that h is 0.99-close to a linear function, say ˜h.
Then to check that ˜f encodes a string whose ﬁrst n bits are the same as the string encoded by ˜h,
the veriﬁer does a concatenation test.
Clearly, the veriﬁer only reads O(1) bits overall. ■
The following Corollary is also similarly proven and is the one that will actually be used later.
It concerns a similar situation as above, except the inputs to the circuit C are thought of as the
concatenation of two strings of lengths n1, n2 respectively where n = n1 + n2.
Corollary 18.26 (PCP of proximity when assignment is in two pieces)
There exists a veriﬁer V that given any circuit C with n input wires and size m and also two
numbers n1, n2 such that n1 + n2 = n has the following property:
1. If u1 ∈{0, 1}n1 , u2 ∈{0, 1}n2 is such that u1 ◦u2 is a satisfying assignment for circuit C,
then there is a string π3 of size 2poly(m) such that V accepts WH(u1) ◦WH(u2) ◦π3 with
probability 1.
Web draft 2007-01-08 21:59

DRAFT
18.5. PROOF OF THE PCP THEOREM.
p18.19 (369)
2. For every strings π1, π2, π3 ∈{0, 1}∗, where π1 and π2 have 2n1 and 2n2 bits respectively, if
V accepts π1 ◦π2 ◦π3 with probability at least 1/2, then π1, π2 are 0.99-close to WH(u1),
WH(u2) respectively for some u1, u2 such that u1 ◦u2 is a satisfying assignment for circuit
C.
3. V uses poly(m) random bits and examines only O(1) bits in the provided strings.
18.5
Proof of the PCP Theorem.
As we have seen, the PCP Theorem is equivalent to Theorem 18.13, stating that ρ-GAP qCSP is
NP-hard for some constants q and ρ < 1. Consider the case that ρ = 1−ϵ where ϵ is not necessarily
a constant but can be a function of m (the number of constraints). Since the number of satisﬁed
constraints is always a whole number, if ϕ is unsatisﬁable then val(ϕ) ≤1 −1/m. Hence, the gap
problem (1−1/m)-GAP 3CSP is a generalization of 3SAT and is NP hard. The idea behind the
proof is to start with this observation, and iteratively show that (1−ϵ)-GAP qCSP is NP-hard for
larger and larger values of ϵ, until ϵ is as large as some absolute constant independent of m. This
is formalized in the following lemma.
Definition 18.27
Let f be a function mapping CSP instances to CSP instances. We say that f is a CL-reduction
(short for complete linear-blowup reduction) if it is polynomial-time computable and for every CSP
instance ϕ with m constraints, satisﬁes:
Completeness: If ϕ is satisﬁable then so is f(ϕ).
Linear blowup: The new qCSP instance f(ϕ) has at most Cm constraints and alphabet W,
where C and W can depend on the arity and the alphabet size of ϕ (but not on the number
of constraints or variables).
Lemma 18.28 (PCP Main Lemma)
There exist constants q0 ≥3, ϵ0 > 0, and a CL-reduction f such that for every q0CSP-instance ϕ
with binary alphabet, and every ϵ < ϵ0, the instance ψ = f(ϕ) is a q0CSP (over binary alphabet)
satisfying
val(ϕ) ≤1 −ϵ ⇒val(ψ) ≤1 −2ϵ
Lemma 18.28 can be succinctly described as follows:
Arity
Alphabet
Constraints
Value
Original
q0
binary
m
1 −ϵ
⇓
⇓
⇓
⇓
Lemma 18.28
q0
binary
Cm
1 −2ϵ
This Lemma allows us to easily prove the PCP Theorem.
Web draft 2007-01-08 21:59

DRAFT
p18.20 (370)
18.5. PROOF OF THE PCP THEOREM.
Proving Theorem 18.2 from Lemma 18.28.
Let q0 ≥3 be as stated in Lemma 18.28. As
already observed, the decision problem q0CSP is NP-hard. To prove the PCP Theorem we give
a reduction from this problem to GAP q0CSP. Let ϕ be a q0CSP instance. Let m be the number
of constraints in ϕ. If ϕ is satisﬁable then val(ϕ) = 1 and otherwise val(ϕ) ≤1 −1/m. We use
Lemma 18.28 to amplify this gap. Speciﬁcally, apply the function f obtained by Lemma 18.28 to
ϕ a total of log m times. We get an instance ψ such that if ϕ is satisﬁable then so is ψ, but if ϕ
is not satisﬁable (and so val(ϕ) ≤1 −1/m) then val(ψ) ≤1 −min{2ϵ0, 1 −2log m/m} = 1 −2ϵ0.
Note that the size of ψ is at most Clog mm, which is polynomial in m. Thus we have obtained
a gap-preserving reduction from L to the (1−2ϵ0)-GAP q0CSP problem, and the PCP theorem is
proved. ■
The rest of this section proves Lemma 18.28 by combining two transformations: the ﬁrst trans-
formation ampliﬁes the gap (i.e., fraction of violated constraints) of a given CSP instance, at the
expense of increasing the alphabet size. The second transformation reduces back the alphabet to
binary, at the expense of a modest reduction in the gap. The transformations are described in the
next two lemmas.
Lemma 18.29 (Gap Amplification [?])
For every ℓ∈N, there exists a CL-reduction gℓsuch that for every CSP instance ϕ with binary
alphabet, the instance ψ = gℓ(ϕ) has has arity only 2 (but over a non-binary alphabet) and satisﬁes:
val(ϕ) ≤1 −ϵ ⇒val(ψ) ≤1 −ℓϵ
for every ϵ < ϵ0 where ϵ0 > 0 is a number depending only on ℓand the arity q of the original
instance ϕ.
Lemma 18.30 (Alphabet Reduction)
There exists a constant q0 and a CL- reduction h such that for every CSP instance ϕ, if ϕ had
arity two over a (possibly non-binary) alphabet {0..W−1} then ψ = h(ϕ) has arity q0 over a binary
alphabet and satisﬁes:
val(ϕ) ≤1 −ϵ ⇒val(h(ϕ)) ≤1 −ϵ/3
Lemmas 18.29 and 18.30 together imply Lemma 18.28 by setting f(ϕ) = h(g6(ϕ)). Indeed, if
ϕ was satisﬁable then so will f(ϕ). If val(ϕ) ≤1 −ϵ, for ϵ < ϵ0 (where ϵ0 the value obtained in
Lemma 18.29 for ℓ= 6, q = q0) then val(g6(ϕ)) ≤1 −6ϵ and hence val(h(g6(ϕ))) ≤1 −2ϵ. This
composition is described in the following table:
Arity
Alphabet
Constraints
Value
Original
q0
binary
m
1 −ϵ
⇓
⇓
⇓
⇓
Lemma 18.29
2
W
Cm
1 −6ϵ
⇓
⇓
⇓
⇓
Lemma 18.30
q0
binary
C′Cm
1 −2ϵ
Web draft 2007-01-08 21:59

DRAFT
18.5. PROOF OF THE PCP THEOREM.
p18.21 (371)
18.5.1
Gap Ampliﬁcation: Proof of Lemma 18.29
To prove Lemma 18.29, we need to exhibit a function g that maps a qCSP instance to a 2CSPW
instance over a larger alphabet {0..W−1} in a way that increases the fraction of violated constraints.
We will show that we may assume without loss of generality that the instance of qCSP has a
speciﬁc form. To describe this we need a deﬁnition.
We will assume that the instance satisﬁes the following properties, since we can give a simple
CL-reduction from qCSP to this special type of qCSP. (See the “Technical Notes” section at the
end of the chapter.) We will call such instances “nice.”
Property 1: The arity q is 2 (though the alphabet may be nonbinary).
Property 2: Let the constraint graph of ψ be the graph G with vertex set [n] where for every
constraint of ϕ depending on the variables ui, uj, the graph G has the edge (i, j). We allow G
to have parallel edges and self-loops. Then G is d-regular for some constant d (independent
of the alphabet size) and at every node, half the edges incident to it are self-loops.
Property 3: The constraint graph is an expander.
The rest of the proof consists of a “powering” operation for nice 2CSP instances.
This is
described in the following Lemma.
Lemma 18.31 (Powering)
Let ψ be a 2CSPW instance satisfying Properties 1 through 3. For every number t, there is an
instance of 2CSP ψt such that:
1. ψt is a 2CSPW ′-instance with alphabet size W ′ < W d5t, where d denote the degree of ψ’s
constraint graph. The instance ψt has dt+
√
tn constraints, where n is the number of variables
in ψ.
2. If ψ is satisﬁable then so is ψt.
3. For every ϵ <
1
d
√
t, if val(ψ) ≤1 −ϵ then val(ψt) ≤1 −ϵ′ for ϵ′ =
√
t
105dW 4 ϵ.
4. The formula ψt is computable from ψ in time polynomial in m and W dt.
Proof: Let ψ be a 2CSPW -instance with n variables and m = nd constraints, and as before let G
denote the constraint graph of ψ.
The formula ψt will have the same number of variables as ψ. The new variables y = y1, . . . , yn
take values over an alphabet of size W ′ = W d5t, and thus a value of a new variable yi is a d5t-tuple
of values in {0..W−1}. We will think of this tuple as giving a value in {0..W−1} to every old variable
uj where j can be reached from ui using a path of at most t +
√
t steps in G (see Figure 18.3).
In other words, the tuple contains an assignment for every uj such that j is in the ball of radius
t+
√
t and center i in G. For this reason, we will often think of an assignment to yi as “claiming” a
certain value for uj. (Of course, another variable yk could claim a diﬀerent value for uj.) Note that
since G has degree d, the size of each such ball is no more than dt+
√
t+1 and hence this information
can indeed be encoded using an alphabet of size W ′.
Web draft 2007-01-08 21:59

DRAFT
p18.22 (372)
18.5. PROOF OF THE PCP THEOREM.
k
k’
i
t+t1/2
t+t1/2
t+t1/2
Figure 18.3: An assignment to the formula ψt consists of n variables over an alphabet of size less than W d5t, where
each variable encodes the restriction of an assignment of ψ to the variables that are in some ball of radius t +
√
t in
ψ’s constraint graph. Note that an assignment y1, . . . , yn to ψt may be inconsistent in the sense that if i falls in the
intersection of two such balls centered at k and k′, then yk may claim a diﬀerent value for ui than the value claimed
by yk′.
For every (2t+ 1)-step path p = ⟨i1, . . . , i2t+2⟩in G, we have one corresponding constraint Cp in
ψt (see Figure 18.4). The constraint Cp depends on the variables yi1 and yi2t+1 and outputs False
if (and only if) there is some j ∈[2t + 1] such that:
1. ij is in the t +
√
t-radius ball around i1.
2. ij+1 is in the t +
√
t-radius ball around i2t+2
3. If w denotes the value yi1 claims for uij and w′ denotes the value yi2t+2 claims for uij+1, then
the pair (w, w′) violates the constraint in ϕ that depends on uij and uij+1.
t+t1/2
t
t
2t+1
t+t1/2
i
k
i’
k’
Figure 18.4: ψt has one constraint for every path of length 2t + 1 in ψ’s constraint graph, checking that the views
of the balls centered on the path’s two endpoints are consistent with one another and the constraints of ψ.
A few observations are in order. First, the time to produce such an assignment is polynomial
in m and W dt, so part 4 of Lemma 18.29 is trivial.
Web draft 2007-01-08 21:59

DRAFT
18.5. PROOF OF THE PCP THEOREM.
p18.23 (373)
Second, for every assignment to u1, u2, . . . , un we can “lift” it to a canonical assignment to
y1, . . . , yn by simply assigning to each yi the vector of values assumed by uj’s that lie in a ball of
radius t +
√
t and center i in G. If the assignment to the uj’s was a satisfying assignment for ψ,
then this canonical assignment satisﬁes ψt, since it will satisfy all constraints encountered in walks
of length 2t + 1 in G. Thus part 2 of Lemma 18.29 is also trivial.
This leaves part 3 of the Lemma, the most diﬃcult part. We have to show that if val(ψ) ≤1−ϵ
then every assignment to the yi’s satisﬁes at most 1−ϵ′ fraction of constraints in ψt, where ϵ <
1
d
√
t
and ϵ′ =
√
t
105dW 4 ϵ. This is tricky since an assignment to the yi’s does not correspond to any obvious
assignment for the ui’s: for each uj, diﬀerent values could be claimed for it by diﬀerent yi’s. The
intuition will be to show that these inconsistencies among the yi’s can’t happen too often (at least
if the assignment to the yi’s satisﬁes 1 −ϵ′ constraints in ψt).
From now on, let us ﬁx some arbitrary assignment y = y1, . . . , yn to ψt’s variables. The following
notion is key.
The plurality assignment: For every variable ui of ψ, we deﬁne the random variable Zi over
{0, . . . , W −1} to be the result of the following process: starting from the vertex i, take a t step
random walk in G to reach a vertex k, and output the value that yk claims for ui. We let zi denote
the plurality (i.e., most likely) value of Zi. If more than one value is most likely, we break ties
arbitrarily. This assignment is called a plurality assignment (see Figure 18.5). Note that Zi = zi
with probability at least 1/W.
t+t1/2
t
i
k
Figure 18.5: An assignment y for ψt induces a plurality assignment u for ψ in the following way: ui gets the most
likely value that is claimed for it by yk, where k is obtained by taking a t-step random walk from i in the constraint
graph of ψ.
Since val(ψ) ≤1 −ϵ, every assignment for ψ fails to satisfy 1 −ϵ fraction of the constraints,
and this is therefore also true for the plurality assignment. Hence there exists a set F of ϵm = ϵnd
constraints in ψ that are violated by the assignment z = z1, . . . , zn. We will use this set F to show
that at least an ϵ′ fraction of ψt’s constraints are violated by the assignment y.
Why did we deﬁne the plurality assignment z in this way? The reason is illustrated by the
following claim, showing that for every edge f = (i, i′) of G, among all paths that contain the edge
f somewhere in their “midsection”, most paths are such that the endpoints of the path claim the
plurality values for ui and ui′.
Web draft 2007-01-08 21:59

DRAFT
p18.24 (374)
18.5. PROOF OF THE PCP THEOREM.
Claim 18.32
For every edge f = (i, i′) in G deﬁne the event Bj,f over the set of (2t+1)-step paths in G to contain
all paths ⟨i1, . . . , i2t+2⟩satisfying:
• f is the jth edge in the path. That is, f = (ij, ij+1).
• yi1 claims the plurality value for ui.
• yi2t+2 claims the plurality value for ui′.
Let δ =
1
100W 2 . Then for every j ∈

t, . . . , t + δ
√
t
	
, Pr[Bj,f] ≥
1
nd2W 2 .
Proof: First, note that because G is regular, the jth edge of a random path is a random edge, and
hence the probability that f is the jth edge on the path is equal to
1
nd. Thus, we need to prove
that,
Pr[endpoints claim plurality values for ui, ui′ (resp.)|f is jth edge] ≥
1
2W 2
(6)
We start with the case j = t + 1. In this case (6) holds essentially by deﬁnition: the left-hand
side of (6) is equal to the probability that the event that the endpoints claim the plurality for these
variables happens for a path obtained by joining a random t-step path from i to a random t-step
path from i′. Let k be the endpoint of the ﬁrst path and k′ be the endpoint of the second path. Let
Wi be the distribution of the value that yk claims for ui, where k is chosen as above, and similarly
deﬁne Wi′ to be the distribution of the value that yk′ claims for ui′. Note that since k and k′ are
chosen independently, the random variables Wi and Wi′ are independent. Yet by deﬁnition the
distribution of Wi identical to the distribution Zi, while the distribution of Wi′ is identical to Zi′.
Thus,
Pr[endpoints claim plurality values for ui, ui′ (resp.)|f is jth edge] =
Pr
k,k′[Wi = zi ∧Wi′ = zi′] = Pr
k [Wi = zi] Pr
k′ [Wi′ = zi′] ≥
1
W 2
In the case that j ̸= 2t+1 we need to consider the probability of the event that endpoints claim
the plurality values happening for a path obtained by joining a random t −1 + j-step path from
i to a random t + 1 −j-step path from i′ (see Figure 18.6). Again we denote by k the endpoint
of the ﬁrst path, and by k′ the endpoint of the second path, by Wi the value yk claims for ui
and by Wi′ the value yk′ claims for ui′. As before, Wi and Wi′ are independent. However, this
time Wi and Zi may not be identically distributed. Fortunately, we can show that they are almost
identically distributed, in other words, the distributions are statistically close. Speciﬁcally, because
half of the constraints involving each variable are self loops, we can think of a t-step random walk
from a vertex i as follows: (1) throw t coins and let St denote the number of the coins that came
up “heads” (2) take St “real” (non self-loop) steps on the graph. Note that the endpoint of a
t-step random walk and a t′-step random walk will be identically distributed if in Step (1) the
variables St and St′ turn out to be the same number. Thus, the statistical distance of the endpoint
of a t-step random walk and a t′-step random walk is bounded by the statistical distance of St
and St′ where Sℓdenotes the binomial distribution of the sum of ℓbalanced independent coins.
Web draft 2007-01-08 21:59

DRAFT
18.5. PROOF OF THE PCP THEOREM.
p18.25 (375)
However, the distributions St and St+δ
√
t are within statistical distance at most 10δ for every δ, t
(see Exercise 15) and hence in our case Wi and Wi′ are
1
10W -close to Zi and Zi′ respectively. Thus
| Prk[Wi = zi] −Pr[Zi = zi]| <
1
10W , | Prk[Wi′ = zi′] −Pr[Zi′ = zi′]| <
1
10W which proves (6) also
for the case j ̸= 2t + 1. ■
t+t1/2
t+εt1/2
t-εt1/2
2t+1
t+t1/2
i
k
i’
k’
Figure 18.6: By deﬁnition, if we take two t-step random walks from two neighbors i and i′, then the respective
endpoints will claim the plurality assignments for ui and uj with probability more than 1/(2W 2).
Because half
the edges of every vertex in G have self loops, this happens even if the walks are not of length t but of length in
[t −ϵ
√
t, t +
√
t] for suﬃciently small ϵ.
Recall that F is the set of constraints of ψ (=edges in G) violated by the plurality assignment
z. Therefore, if f ∈F and j ∈

t, . . . , t + δ
√
t
	
then all the paths in Bj,f correspond to constraints
of ψt that are violated by the assignment y. Therefore, we might hope that the fraction of violated
constraints in ψt is at least the sum of Pr[Bj,f] for every f ∈F and j ∈

t, . . . , t + δ
√
t
	
. If this
were the case we’d be done since Claim 18.32 implies that this sum is at least δ
√
tϵnd
2nW 2 = δ
√
tϵ
2W 2 > ϵ′.
However, this is inaaccurate since we are overcounting paths that contain more than one such
violation (i.e., paths which are in the intersection of Bj,f and Bj′,f′ for (j, f) ̸= (j′, f′)). To bound
the eﬀect of this overcounting we prove the following claim:
Claim 18.33
For every k ∈N and set F of edges with |F| = ϵnd for ϵ <
1
kd,
X
j,j′∈{t..t+k}
f,f′∈F
(j,f)̸=(j′,f′)
Pr[Bj,f ∩Bj,f′] ≤30kdϵ
(7)
Proof: Only one edge can be the jth edge of a path, and so for every f ̸= f′, Pr[Bj,f ∩Bj,f′] = 0.
Thus the left-hand side of (7) simpliﬁes to
X
j̸=j′∈{t..t+k}
X
f̸=f′
Pr[Bj,f ∩Bj′,f′]
(8)
Web draft 2007-01-08 21:59

DRAFT
p18.26 (376)
18.5. PROOF OF THE PCP THEOREM.
Let Aj be the event that the jth edge is in the set F. We get that (8) is equal to
X
j̸=j′∈{t..t+k}
Pr[Aj ∩Aj′] = 2
X
j<j′∈{t..t+k}
Pr[Aj ∩Aj′]
(9)
Let S be the set of at most dϵn vertices that are adjacent to an edge in F. For j′ < j, Pr[Aj∩Aj′]
is bounded by the probability that a random (j′−j)-step path in G has both endpoints in S, or in
other words that a random edge in the graph Gj′−j has both endpoints in S. Using the fact that
λ(Gj′−j) = λ(G)j′−j ≤0.9j′−j, this probability is bounded by dϵ(dϵ + 0.9|j−j′|) (see Note 18.18).
Plugging this into (9) and using the formula for summation of arithmetic series, we get that:
2
X
j<j′∈{t,...,t+k}
Pr[Aj ∩Aj′] ≤
2
X
j∈{t,...,t+k}
t+k−j
X
i=1
dϵ(dϵ + 0.9i) ≤
2k2d2ϵ2 + 2kdϵ
∞
X
i=1
0.9i ≤2k2d2ϵ2 + 20kdϵ ≤30kdϵ
where the last inequality follows from ϵ <
1
kd. ■
Wrapping up.
Claims 18.32 and 18.33 together imply that
X
j∈{t..t+δ
√
t}
f∈F
Pr[Bj,f] ≥δ
√
tϵ
1
2W 2
(10)
X
j,j′∈{t..t+δ
√
t}
f,f′∈F
(j,f)̸=(j′,f′)
Pr[Bj,f ∩Bj′,f′] ≤30δ
√
tdϵ
(11)
But (10) and (11) together imply that if p is a random constraint of ψt then
Pr[p violated by y] ≥Pr[
[
j∈{t..t+δ
√
t}
f∈F
Bj,f] ≥
δ
√
tϵ
240dW 2
where the last inequality is implied by the following technical claim:
Claim 18.34
Let A1, . . . , An be n subsets of some set U satisfying P
i<j |Ai ∩Aj| ≤C Pn
i=1 |Ai| for some number
C ∈N. Then,

n[
i=1
Ai
 ≥
Pn
i=1 |Ai|
4C
Web draft 2007-01-08 21:59

DRAFT
18.5. PROOF OF THE PCP THEOREM.
p18.27 (377)
Proof: We make 2C copies of every element u ∈U to obtain a set ˜U with | ˜U| = 2C|U|. Now for
every subset Ai ⊆U, we obtain ˜Ai ⊆˜U as follows: for every u ∈Ai, we choose at random one of
the 2C copies to put in ˜Ai. Note that | ˜Ai| = |Ai|. For every i, j ∈[n], u ∈Ai ∩Aj, we denote by
Ii,j,u the indicator random variable that is equal to 1 if we made the same choice for the copy of u
in ˜Ai and ˜Aj, and equal to 0 otherwise. Since E[Ii,j,u] =
1
2C ,
E
h
| ˜Ai ∩˜Aj|
i
=
X
u∈Ai∩Aj
E[Ii,j,u] = |Ai ∩Aj|
2C
and
E

X
i<j
| ˜Ai ∩˜Aj|

=
P
i<j |Ai ∩Aj|
2C
This means that there exists some choice of ˜A1, . . . , ˜Aj such that
n
X
i=1
| ˜Ai| =
n
X
i=1
|Ai| ≥2
X
i<j
| ˜Ai ∩˜Aj|
which by the inclusion-exclusion principle (see Section ??) means that | ∪i ˜Ai| ≥1/2 P
i | ˜Ai|. But
because there is a natural 2C-to-one mapping from ∪i ˜Ai to ∪iAi we get that
| ∪n
i=1 Ai| ≥| ∪n
i=1 ˜Ai|
2C
≥
Pn
i=1 | ˜Ai|
4C
=
Pn
i=1 |Ai|
4C
■
Since ϵ′ <
δ
√
tϵ
240dW 2 , this proves the lemma. ■
18.5.2
Alphabet Reduction: Proof of Lemma 18.30
Lemma 18.30 is actually a simple consequence of Corollary 18.26, once we restate it using our
“qCSP view” of PCP systems.
Corollary 18.35 (qCSP view of PCP of proximity.)
There exists positive integer q0 and an exponential-time transformation that given any circuit C of
size m and and n inputs and two numbers n1, n2 such that n1 + n2 = n produces an instance ψC
of q0CSP of size 2poly(m) over a binary alphabet such that:
1. The variables can be thought of as being partitioned into three sets π1, π2, π3 where π1 has
2n1 variables and π2 has 2n2 variables.
2. If u1 ∈{0, 1}n1 , u2 ∈{0, 1}n2 is such that u1 ◦u2 is a satisfying assignment for circuit C,
then there is a string π3 of size 2poly(m) such that WH(u1) ◦WH(u2) ◦π3 satisﬁes ψC.
Web draft 2007-01-08 21:59

DRAFT
p18.28 (378)
18.5. PROOF OF THE PCP THEOREM.
3. For every strings π1, π2, π3 ∈{0, 1}∗, where π1 and π2 have 2n1 and 2n2 bits respectively, if
π1 ◦π2 ◦π3 satisfy at least 1/2 the constraints of ψC, then π1, π2 are 0.99-close to WH(u1),
WH(u2) respectively for some u1, u2 such that u1 ◦u2 is a satisfying assignment for circuit
C.
Now we are ready to prove Lemma 18.30.
Proof of Lemma 18.30:
Suppose the given arity 2 formula ϕ has n variables u1, u2, . . . , un,
alphabet {0..W−1} and N constraints C1, C2, . . . , CN. Think of each variable as taking values that
are bit strings in {0, 1}k, where k = ⌈log W ⌉. Then if constraint Cℓinvolves variables say ui, uj
we may think of it as a circuit applied to the bit strings representing ui, uj where the constraint
is said to be satisﬁed iﬀthis circuit outputs 1. Say m is an upperbound on the size of this circuit
over all constraints. Note that m is at most 22k < W 4. We will assume without loss of generality
that all circuits have the same size.
If we apply the transformation of Corollary 18.35 to this circuit we obtain an instance of q0CSP,
say ψCl. The strings ui, uj get replaced by strings of variables Ui, Uj of size 22k < 2W 2 that take
values over a binary alphabet. We also get a new set of variables that play the role analogous to
π3 in the statement of Corollary 18.35. We call these new variables Πl.
Our reduction consists of applying the above transformation to each constraint, and taking
the union of the q0CSP instances thus obtained. However, it is important that these new q0CSP
instances share variables, in the following way: for each old variable ui, there is a string of new
variables Ui of size 22k and for each constraint Cl that contains ui, the new q0CSP instance ψCl
uses this string Ui. (Note though that the Πl variables are used only in ψCl and never reused.)
This completes the description of the new q0CSP instance ψ (see Figure 18.7). Let us see that it
works.
Original instance:
constraints:
variables:
(over alphabet [W])
u1
u2
u3
......
un
C1
C2
Cm
Transformed instance:
constraints:
variables:
(over alphabet {0.1})
U1=WH(u1)
......
......
U2=WH(u2)
Un=WH(un)
Π1
Πm
...
...
...
cluster 1
cluster 2
cluster m
.......
Figure 18.7:
The alphabet reduction transformation maps a 2CSP instance ϕ over alphabet {0..W−1} into a
qCSP instance ψ over the binary alphabet. Each variable of ϕ is mapped to a block of binary variables that in the
correct assignment will contain the Walsh-Hadamard encoding of this variable. Each constraint Cℓof ϕ depending
on variables ui, uj is mapped to a cluster of constraints corresponding to all the PCP of proximity constraints for
Cℓ. These constraint depend on the encoding of ui and uj, and on additional auxiliary variables that in the correct
assignment contain the PCP of proximity proof that these are indeed encoding of values that make the constraint
Cℓtrue.
Web draft 2007-01-08 21:59

DRAFT
18.6. THE ORIGINAL PROOF OF THE PCP THEOREM.
p18.29 (379)
Suppose the original instance ϕ was satisﬁable by an assignment u1, . . . , un.
Then we can
produce a satisfying assignment for ψ by using part 2 of Corollary 18.35, so that for each constraint
Cl involving ui, uj, the encodings WH(ui), WH(ui) act as π1, π2 and then we extend these via a
suitable string π3 into a satisfying assignment for ψCl.
On the other hand if val(ϕ) < 1 −ϵ then we show that val(ψ) < 1 −ϵ/2.
Consider any
assignment U1, U2, . . . , Un, Π1, . . . , ΠN to the variables of ψ. We “decode” it to an assignment
for ϕ as follows. For each i = 1, 2, . . . , n, if the assignment to Ui is 0.99-close to a linear function,
let ui be the string encoded by this linear function, and otherwise let ui be some arbitrary string.
Since val(ϕ) < 1 −ϵ, this new assignment fails to satisfy at least ϵ fraction of constraints in ϕ. For
each constraint Cl of ϕ that is not satisﬁed by this assignment, we show that at least 1/2 of the
constraints in ψCl are not satisﬁed by the original assignment, which leads to the conclusion that
val(ψ) < 1 −ϵ/2. Indeed, suppose Cl involves ui, uj. Then ui ◦uj is not a satisfying assignment to
circuit Cl, so part 3 of Corollary 18.35 implies that regardless of the value of variables in Πl, the
assignment Ui ◦uj ◦Πl must have failed to satisfy at least 1/2 the constraints of ψCl. ■
18.6
The original proof of the PCP Theorem.
The original proof of the PCP Theorem, which resisted simpliﬁcation for over a decade, used
algebraic encodings and ideas that are complicated versions of our proof of Theorem 18.21. (Indeed,
Theorem 18.21 is the only part of the original proof that still survives in our writeup.) Instead of the
linear functions used in Welsh-Hadamard code, they use low degree multivariate polynomials. These
allow procedures analogous to the linearity test and local decoding, though the proofs of correctness
are a fair bit harder. The alphabet reduction is also somewhat more complicated. The crucial part
of Dinur’s simpler proof, the one given here, is the gap ampliﬁcation lemma (Lemma 18.29) that
allows to iteratively improve the soundness parameter of the PCP from very close to 1 to being
bounded away from 1 by some positive constant. This general strategy is somewhat reminiscent
of the zig-zag construction of expander graphs and Reingold’s deterministic logspace algorithm for
undirect connectivity described in Chapter ??.
Chapter notes
Problems
§1 Prove that for every two functions r, q : N →N and constant s < 1, changing the constant in
the soundness condition in Deﬁnition 18.1 from 1/2 to s will not change the class PCP(r, q).
§2 Prove that for every two functions r, q : N →N and constant c > 1/2, changing the constant in
the completeness condition in Deﬁnition 18.1 from 1 to c will not change the class PCP(r, q).
§3 Prove that any language L that has a PCP-veriﬁer using r coins and q adaptive queries also
has a standard (i.e., non-adaptive) veriﬁer using r coins and 2q queries.
§4 Prove that PCP(0, log n) = P. Prove that PCP(0, poly(n)) = NP.
Web draft 2007-01-08 21:59

DRAFT
p18.30 (380)
18.6. THE ORIGINAL PROOF OF THE PCP THEOREM.
§5 Let L be the language of matrices A over GF(2) satisfying perm(A) = 1 (see Chapters ??
and 8). Prove that L is in PCP(poly(n), poly(n)).
§6 Show that if SAT ∈PCP(r(n), 1) for r(n) = o(log n) then P = NP. (Thus the PCP Theorem
is probably optimal up to constant factors.)
§7 (A simple PCP Theorem using logspace veriﬁers) Using the fact that a correct tableau can
be veriﬁed in logspace, we saw the following exact characterization of NP:
NP = {L : there is a logspace machine M s.t x ∈L iﬀ∃y : M accepts (x, y).} .
Note that M has two-way access to y.
Let L-PCP(r(n)) be the class of languages whose membership proofs can be probabilistically
checked by a logspace machine that uses O(r(n)) random bits but makes only one pass over
the proof. (To use the terminology from above, it has 2-way access to x but 1-way access
to y.) As in the PCP setting, “probabilistic checking of membership proofs” means that for
x ∈L there is a proof y that the machine accepts with probability 1 and if not, the machine
rejects with probability at least 1/2. Show that NP = L-PCP(log n). Don’t assume the PCP
Theorem!
Hint: Design a veriﬁer for 3SAT. The trivial idea would be that
the proof contains a satisfying assignment and the veriﬁer randomly
picks a clause and reads the corresponding three bits in the proof
to check if the clause is satisﬁed. This doesn’t work. Why? The
better idea is to require the “proof” to contain many copies of the
satisfying assignment. The veriﬁers uses pairwise independence to
run the previous test on these copies —which may or may not be
the same string.
(This simple PCP Theorem is implicit in Lipton [Lip90]. The suggested proof is due to van
Melkebeek.)
§8 Suppose we deﬁne J −PCP(r(n)) similarly to L −PCP(r(n)), except the veriﬁer is only
allowed to read O(r(n)) successive bits in the membership proof. (It can decide which bits
to read.) Then show that J −PCP(log n) ⊆L.
§9 Prove that there is an NP-language L and x ̸∈L such that f(x) is a 3SAT formula with
m constraints having an assignment satisfying more than m −m0.9 of them, where f is the
reduction from f to 3SAT obtained by the proof of the Cook-Levin theorem (Section 2.3).
Hint: show that for an appropriate language L, a slight change in
the input for the Cook-Levin reduction will also cause only a slight
change in the output, even though this change might cause a YES
instance of the language to become a NO instance.
Web draft 2007-01-08 21:59

DRAFT
18.6. THE ORIGINAL PROOF OF THE PCP THEOREM.
p18.31 (381)
§10 Show a poly(n, 1/ϵ)-time 1 + ϵ-approximation algorithm for the knapsack problem. That is,
show an algorithm that given n + 1 numbers a1, . . . , an ∈N (each represented by at most n
bits) and k ∈[n], ﬁnds a set S ⊆[n] with |S| ≤k such that P
i∈S ai ≥opt
1+ϵ where
opt =
max
S⊆[n],|S|≤k
X
i∈S
ai
Hint: ﬁrst show that the problem can be solved exactly using dy-
namic programming in time poly(n, m) if all the numbers involved
are in the set [m]. Then, show one can obtain an approximation
algorithm by keeping only the O(log(1/e) + log n) most signiﬁcant
bits of every number.
§11 Show a polynomial-time algorithm that given a satisﬁable 2CSP-instance ϕ (over binary
alphabet) ﬁnds a satisfying assignment for ϕ.
§12 Prove that QUADEQ is NP-complete.
Hint: show you can express satisﬁability for SAT formulas using
quadratic equations.
§13 Prove that if Z, U are two n × n matrices over GF(2) such that Z ̸= U then
Pr
r,r′∈R{0,1}n[rZr′ ̸= rUr′] ≥1
4
Hint: using linearity reduce this to the case that U is the all zero
matrix, and then prove this using two applications of the random
subsum principle.
§14 Show a deterministic poly(n, 2q)-time algorithm that given a qCSP-instance ϕ (over binary
alphabet) with m clauses outputs an assignment satisfying m/2q of these assignment.
Hint: one way to solve this is to use q-wise independent functions
??.
§15 Let St be the binomial distribution over t balanced coins. That is, Pr[St = k] =
 t
k

2−t. Prove
that for every δ < 1, the statistical distance of St and St+δ
√
t is at most 10ϵ.
Hint: approximate the binomial coeﬃcient using Stirling’s for-
mula for approximating factorials.
§16 The long-code for a set {0, . . . , W −1} is the function LC : {0, . . . , W −1} →{0, 1}2W such
that for every i ∈{0..W−1} and a function f : {0..W−1} →{0, 1}, (where we identify f with
an index in [2w]) the fth position of LC(i), denoted by LC(i)f, is f(i). We say that a function
L : {0, 1}2W →{0, 1} is a long-code codeword if L = LC(i) for some i ∈{0..W−1}.
Web draft 2007-01-08 21:59

DRAFT
p18.32 (382)
18.6. THE ORIGINAL PROOF OF THE PCP THEOREM.
(a) Prove that LC is an error-correcting code with distance half. That is, for every i ̸= j ∈
{0..W−1}, the fractional Hamming distance of LC(i) and LC(j) is half.
(b) Prove that LC is locally-decodable. That is, show an algorithm that given random access
to a function L : 2{0,1}W →{0, 1} that is (1−ϵ)-close to LC(i) and f : {0..W−1} →{0, 1}
outputs LC(i)f with probability at least 0.9 while making at most 2 queries to L.
(c) Let L = LC(i) for some i ∈{0..W−1}. Prove the for every f : {0..W−1} →{0, 1},
L(f) = 1−L(f), where f is the negation of f (i.e. , f(i) = 1−f(i) for every i ∈{0..W−1}).
(d) Let T be an algorithm that given random access to a function L : 2{0,1}W →{0, 1}, does
the following:
i. Choose f to be a random function from {0..W−1} →{0, 1}.
ii. If L(f) = 1 then output True.
iii. Otherwise, choose g : {0..W−1} →{0, 1} as follows: for every i ∈{0..W−1}, if
f(i) = 0 then set g(i) = 0 and otherwise set g(i) to be a random value in {0, 1}.
iv. If L(g) = 0 then output True; otherwise output False.
Prove that if L is a long-code codeword (i.e., L = LC(i) for some i) then T outputs
True with probability one.
Prove that if L is a linear function that is non-zero and not a longcode codeword then
T outputs True with probability at most 0.9.
(e) Prove that LC is locally testable. That is, show an algorithm that given random access
to a function L : {0, 1}W →{0, 1} outputs True with probability one if L is a long-
code codeword and outputs False with probability at least 1/2 if L is not 0.9-close to a
long-code codeword, while making at most a constant number of queries to L.
Hint: use the test T above combined with linearity testing, self
correction, and a simple test to rule out the constant zero function.
(f) Using the test above, give an alternative proof for the Alphabet Reduction Lemma
(Lemma 18.30).
Hint: To transform a 2CSPW formula ϕ over n variables into
a qCSP ψ over binary alphabet, use 2W variables u1
j, . . . , u2W
j
for
each variable uj of ϕ.
In the correct proof these variables will
contain the longcode encoding of uj. Then, add a set of 2W 2 vari-
ables y1
i , . . . , y2W 2
i
for each constraint ϕi of ϕ. In the correct proof
these variables will contain the longcode encoding of the assign-
ment for the constraint ϕi. For every constraint of ϕ, ψ will contain
constraints for testing the longcode of both the x and y variables
involved in the constraint, testing consistency between the x vari-
ables and the y variables, and testing that the y variables actually
encode a satisfying assignment.
Web draft 2007-01-08 21:59

DRAFT
18.6. THE ORIGINAL PROOF OF THE PCP THEOREM.
p18.33 (383)
Omitted proofs
The preprocessing step transforms a qCSP-instance ϕ into a “nice” 2CSP-instance ψ through the
following three claims:
Claim 18.36
There is a CL- reduction mapping any qCSP instance ϕ into a 2CSP2q instance ψ such that
val(ϕ) ≤1 −ϵ ⇒val(ψ) ≤1 −ϵ/q
Proof: Given a qCSP-instance ϕ over n variables u1, . . . , un with m constraints, we construct the
following 2CSP2q formula ψ over the variables u1, . . . , un, y1, . . . , ym. Intuitively, the yi variables
will hold the restriction of the assignment to the q variables used by the ith constraint, and we will
add constraints to check consistency: that is to make sure that if the ith constraint depends on the
variable uj then uj is indeed given a value consistent with yi. Speciﬁcally, for every ϕi of ϕ that
depends on the variables u1, . . . , uq, we add q constraints {ψi,j}j∈[q] where ψi,j(yi, uj) is true iﬀyi
encodes an assignment to u1, . . . , uq satisfying ϕi and uj is in {0, 1} and agrees with the assignment
yi. Note that the number of constraints in ψ is qm.
Clearly, if ϕ is satisﬁable then so is ψ. Suppose that val(ϕ) ≤1−ϵ and let u1, . . . , uk, y1, . . . , ym
be any assignment to the variables of ψ. There exists a set S ⊆[m] of size at least ϵm such that
the constraint ϕi is violated by the assignment u1, . . . , uk. For any i ∈S there must be at least one
j ∈[q] such that the constraint ψi,j is violated. ■
Claim 18.37
There is an absolute constant d and a CL- reduction mapping any 2CSPW instance ϕ into a 2CSPW
instance ψ such that
val(ϕ) ≤1 −ϵ ⇒val(ψ) ≤1 −ϵ/(100Wd).
and the constraint graph of ψ is d-regular.
That is, every variable in ψ appears in exactly d
constraints.
Proof: Let ϕ be a 2CSPW instance, and let {Gn}n∈N be an explicit family of d-regular expanders.
Our goal is to ensure that each variable appears in ϕ at most d + 1 times (if a variable appears
less than that, we can always add artiﬁcial constraints that touch only this variable). Suppose
that ui is a variable that appears in k constraints for some n > 1.
We will change ui into k
variables y1
i , . . . , yk
i , and use a diﬀerent variable of the form yj
i in the place of ui in each constraint
ui originally appeared in. We will also add a constraint requiring that yj
i is equal to yj′
i for every
edge (j, j′) in the graph Gk. We do this process for every variable in the original instance, until
each variable appears in at most d equality constraints and one original constraint. We call the
resulting 2CSP-instance ψ. Note that if ϕ has m constraints then ψ will have at most m + dm
constraints.
Clearly, if ϕ is satisﬁable then so is ψ. Suppose that val(ϕ) ≤1 −ϵ and let y be any assignment
to the variables of ψ. We need to show that y violates at least
ϵm
100W of the constraints of ψ. Recall
that for each variable ui that appears k times in ϕ, the assignment y has k variables y1
i , . . . , yk
i .
We compute an assignment u to ϕ’s variables as follows: ui is assigned the plurality value of
Web draft 2007-01-08 21:59

DRAFT
p18.34 (384)
18.6. THE ORIGINAL PROOF OF THE PCP THEOREM.
y1
i , . . . , yk
i . We deﬁne ti to be the number of yj
i ’s that disagree with this plurality value. Note that
0 ≤ti ≤k(1 −1/W) (where W is the alphabet size). If Pn
i=1 ti ≥ϵ
4m then we are done. Indeed,
by (3) (see Note 18.18), in this case we will have at least Pn
i=1
ti
10W ≥
ϵ
40W m equality constraints
that are violated.
Suppose now that Pn
i=1 ti < ϵ
4m. Since val(ϕ) ≤1−ϵ, there is a set S of at least ϵm constraints
violated in ϕ by the plurality assignment u. All of these constraints are also present in ψ and since
we assume Pn
i=1 ti < ϵ
4m, at most half of them are given a diﬀerent value by the assignment y than
the value given by u. Thus the assignment y violates at least ϵ
2m constraints in ψ. ■
Claim 18.38
There is an absolute constant d and a CL-reduction mapping any 2CSPW instance ϕ with d′-regular
constraint graph for d ≥d′ into a 2CSPW instance ψ such that
val(ϕ) ≤1 −ϵ ⇒val(ψ) ≤1 −ϵ/(10d)
and the constraint graph of ψ is a 4d-regular expander, with half the edges coming out of each
vertex being self-loops.
Proof: There is a constant d and an explicit family {Gn}n∈N of graphs such that for every n, Gn
is a d-regular n-vertex 0.1-expander graph (See Note 18.18).
Let ϕ be a 2CSP-instance as in the claim’s statement. By adding self loops, we can assume that
the constraint graph has degree d (this can at worst decrease the gap by factor of d). We now add
“null” constraints (constraints that always accept) for every edge in the graph Gn. In addition, we
add 2d null constraints forming self-loops for each vertex. We denote by ψ the resulting instance.
Adding these null constraints reduces the fraction of violated constraints by a factor at most four.
Moreover, because any regular graph H satisﬁes λ(H) ≤1 and because of λ’s subadditivity (see
Exercise 11, Chapter ??), λ(ψ) ≤3
4 + 1
4λ(Gn) ≤0.9 where by λ(ψ) we denote the parameter λ of
ψ’s constraint graph. ■
Web draft 2007-01-08 21:59

DRAFT
Chapter 19
More PCP Theorems and the Fourier
Transform Technique
The PCP Theorem has several direct applications in complexity theory, in particular showing
that unless P = NP, many NP optimization problems can not be approximated in polynomial-
time to within arbitrary precision. However, for some applications, the standard PCP Theorem
does not suﬃce, and we need stronger (or simply diﬀerent) “PCP Theorems”. In this chapter we
survey some of these results and their proofs. The Fourier transform technique turned out to be
especially useful in advanced PCP constructions, and in other areas in theoretical computer science.
We describe the technique and show two of its applications. First, we use Fourier transforms to
prove the correctness of the linearity testing algorithm of Section 18.4, completing the proof of the
PCP Theorem. We then use it to prove a stronger PCP Theorem due to H˚astad, showing tight
inapproximability results for many important problems, including MAX 3SAT.
19.1
Parallel Repetition of PCP’s
Recall that the soundness parameter of a PCP system is the probability that the veriﬁer may
accept a false statement. Deﬁnition 18.1 speciﬁed the soundness parameter to be 1/2, but as we
noted, it can be reduced to an arbitrary small constant by increasing the number of queries. Yet
for some applications we need a system with, say, three queries, but an arbitrarily small constant
soundness parameter. Raz has shown that this can be achieved if we consider systems with non
binary alphabet. (For a ﬁnite set S, we say that a PCP veriﬁer uses alphabet S if it takes as input
a proof string π in S∗.) The idea is simple and natural: use parallel repetition. That is, we take a
PCP veriﬁer V and run ℓindependent copies of it, to obtain a new veriﬁer V ℓsuch that a query
of V ℓis the concatenation of the ℓqueries of V , and an answer is a concatenation of the ℓanswers.
(So, if the original veriﬁer V used proofs over, say, the binary alphabet, then the veriﬁer V ℓwill
use the alphabet {0, 1}ℓ.) The veriﬁer V ℓaccepts the proof only of all the ℓexecutions of V accept.
Formally, we deﬁne parallel repetition as follows:
Definition 19.1 (Parallel repetition)
Let S be a ﬁnite set. Let V be a PCP veriﬁer using alphabet S and let ℓ∈N. The ℓ-times parallel
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p19.1 (385)

DRAFT
p19.2 (386)
19.1. PARALLEL REPETITION OF PCP’S
Original V
Parallel repeated V ℓ
Sequential repeated V seqℓ
Alphabet size
W
W ℓ
W
Proof size
m
mℓ
m
Random coins used
r
ℓr
ℓr
Number of queries
q
q
ℓq
Completeness probability
1
1
1
soundness parameter
1 −δ
(1 −δa)bℓ
(1 −δ)ℓ
Table 19.1: Parameters of ℓ-times parallel repeated veriﬁer V ℓvs. parameters for sequential repetition.
repeated V is the veriﬁer V ℓthat operates as follows:
1. V ℓuses the alphabet ˆS = Sℓ. We denote the input proof string to V ℓby ˆπ.
2. Let q denote the number of queries V makes. On any input x, V ℓchooses ℓindependent
random tapes r1, . . . , rℓfor V , and runs V on the input and these tapes to obtain ℓsets of q
queries
i1
1,
i1
2,
. . .
, i1
q
i2
1,
i2
2,
. . .
, i2
q
. . .
iℓ
1,
iℓ
2,
. . .
, iℓ
q
3. V ℓmakes q queries i1, . . . , iq to ˆπ where ij is ⟨i1
j, . . . , iℓ
j⟩(under a suitable encoding of Nℓinto
N).
4. For j ∈[q], denote ⟨a1
j, . . . , aℓ
j⟩= ˆπ(ij). The veriﬁer V ℓaccepts if and only for every k ∈[ℓ],
the veriﬁer V on random tape rk accepts when given the responses ak
1, . . . , ak
q.
Remark 19.2
For every input x, if there is a proof π such that on input x, the veriﬁer V accepts π with probability
one, then there is a proof ˆπ such that on input x, the veriﬁer V ℓaccepts ˆπ with probability one.
Namely, for every ℓ-tuple of positions i1, . . . , iℓ, the proof ˆπ contains the tuple ⟨π[i1], . . . , π[iℓ]⟩.
Note that |ˆπ| = |π|ℓ.
Why is it called “parallel repetition”?
We call the veriﬁer V ℓthe parallel repeated version
of V to contrast with sequential repetition. If V is a PCP veriﬁer and ℓ∈N, we say that ℓ-times
sequentially repeated V , denoted V seqℓ, is the veriﬁer that chooses ℓrandom tapes for V , then
makes the qℓqueries corresponding to these tapes one after the other, and accepts only if all the
instances accept. Note that V seqℓuses the same alphabet as V , and uses proofs of the same size.
The relation between the parameters of V , V ℓand V seqℓis described in Table 19.1.
Web draft 2007-01-08 21:59

DRAFT
19.2. H˚ASTAD’S 3-BIT PCP THEOREM
p19.3 (387)
It is a simple exercise to show that if V ’s soundness parameter was 1 −δ then V seqℓsoundness
parameter will be equal to (1−δ)ℓ. One may expect the soundness parameter of the parallel repeated
veriﬁer V ℓto also be (1−δ)ℓ. It turns out this is not the case (there is a known counterexample [?]),
however the soundness parameter does decay exponentially with the number of repetitions:
Theorem 19.3 (Parallel Repetition Lemma, [Raz98])
There exist constants a, b (independent of ℓbut depending on the alphabet size used and number
of queries) such that the soundness parameter of V ℓis at most (1 −δa)bℓ
We omit the proof of Theorem 19.3 for lack of space. Roughly speaking, the reason analyzing
soundness of V ℓis so hard is the following: for every tuple ⟨i1, . . . , iℓ⟩, the corresponding position
in the proof for V ℓis “supposed” to consist of the values π[i1] ◦· · · π[iℓ] where π is some proof for
V . However, a priori, we do not know if the proof satisﬁes this property. It may be that the proof
is inconsistent and that two tuples containing the ith position “claim” a diﬀerent assignment for
π[i].
Remark 19.4
The Gap Ampliﬁcation Lemma (Lemma 18.29) of the previous chapter has a similar ﬂavor, in the
sense that it also reduced the soundness parameter at the expense of an increase in the alphabet
size. However, that lemma assumed that the soundness parameter is very close to 1, and its proof
does not seem to generalize for soundness parameters smaller than 1/2. We note that a weaker
version of Theorem 19.3, with a somewhat simpler proof, was obtained by Feige and Kilian [?].
This weaker version is suﬃcient for many applications, including for H˚astad’s 3-query PCP theorem
(see Section 19.2 below).
19.2
H˚astad’s 3-bit PCP Theorem
In most cases, the PCP Theorem does not immediately answer the question of exactly how well can
we approximate a given optimization problem (even assuming P ̸= NP). For example, the PCP
Theorem implies that if P ̸= NP then MAX 3SAT cannot be c-approximated in polynomial-time
for some constant ρ < 1. But if one follows closely the proof of Theorem 18.13, this constant ρ
turns out to be very close to one, and in particular it is larger than 0.999. On the other hand,
as we saw in Example 18.6, there is a known 7/8-approximation algorithm for MAX 3SAT. What
is the true “approximation complexity” of this problem? In particular, is there a polynomial-time
0.9-approximation algorithm for it? Similar questions are the motivation behind many stronger
PCP theorems. In particular, the following theorem by H˚astad implies that for every ϵ > 0 there
is no polynomial-time (7/8+ϵ)-approximation for MAX 3SAT unless P = NP:
Theorem 19.5 (H˚astad’s 3-bit PCP [?])
For every ϵ > 0 and every language L ∈NP there is a PCP-veriﬁer V for L making three (binary)
queries having completeness probability at least 1 −ϵ and soundness parameter at most 1/2 + ϵ.
Moreover, the test used by V are linear. That is, given a proof π ∈{0, 1}m, V chooses a triple
(i1, i2, i3) ∈[m]3 and b ∈{0, 1} according to some distribution and accepts iﬀπi1 + πi2 + πi3 = b
(mod 2).
Web draft 2007-01-08 21:59

DRAFT
p19.4 (388)
19.3. TOOL: THE FOURIER TRANSFORM TECHNIQUE
Theorem 19.5 immediately implies that the problem MAX E3LIN is NP-hard to 1/2+ϵ-approximate
for every ϵ > 0, where MAX E3LIN is the problem of ﬁnding a solution maximizing the number of
satisﬁed equations among a given system of linear equations over GF(2), with each equation con-
taining at most 3 variables. Note that this hardness of approximation result is tight since a random
assignment is expected to satisfy half of the equations. Also note that ﬁnding out whether there
exists a solution satisfying all of the equations can be done in polynomial-time using Gaussian
elimination (and hence the imperfect completeness in Theorem 19.5 is inherent).
The result for MAX 3SAT is obtained by the following corollary:
Corollary 19.6
For every ϵ > 0, computing (7/8+ϵ)-approximation to MAX 3SAT is NP-hard.
Proof: We reduce MAX E3LIN to MAX 3SAT.
Take any instance of MAX E3LIN where we are
interested in determining whether (1−ϵ) fraction of the equations can be satisﬁed or at most 1/2+ϵ
are. Represent each linear constraint by four 3CNF clauses in the obvious way. For example, the
linear constraint a+b+c = 0 (mod 2) is equivalent to the clauses (a∨b∨c), (a∨b∨c), (a∨b∨c), (a∨
b ∨c). If a, b, c satisfy the linear constraint, they satisfy all 4 clauses and otherwise they satisfy at
most 3 clauses. We conclude that in one case at least (1 −ϵ) fraction of clauses are simultaneously
satisﬁable, and in the other case at most 1 −(1
2 −ϵ) × 1
4 = 7
8 −ϵ
4 fraction are. The ratio between
the two cases tends to 7/8 as ϵ decreases. Since Theorem 19.5 implies that distinguishing between
the two cases is NP-hard for every constant ϵ, the result follows. ■
19.3
Tool: the Fourier transform technique
The continuous Fourier transform is extremely useful in mathematics and engineering. Likewise,
the discrete Fourier transform has found many uses in algorithms and complexity, in particular for
constructing and analyzing PCP’s. The Fourier transform technique for PCP’s involves calculating
the maximum acceptance probability of the veriﬁer using Fourier analysis of the functions presented
in the proof string. It is delicate enough to give “tight” inapproximability results for MAX INDSET,
MAX 3SAT, and many other problems.
To introduce the technique we start with a simple example: analysis of the linearity test over
GF(2) (i.e., proof of Theorem 18.23). We then introduce the Long Code and show how to test for
membership in it. These ideas are then used to prove H˚astad’s 3-bit PCP Theorem.
19.3.1
Fourier transform over GF(2)n
The Fourier transform over GF(2)n is a tool to study functions on the Boolean hypercube. In this
chapter, it will be useful to use the set {+1, −1} = {±1} instead of {0, 1}. To transform {0, 1}
to {±1}, we use the mapping b 7→(−1)b (i.e., 0 7→+1 , 1 7→−1). Thus we write the hypercube
as {±1}n instead of the more usual {0, 1}n. Note this maps the XOR operation (i.e., addition in
GF(2)) into the multiplication operation.
The set of functions from {±1}n to R deﬁnes a 2n-dimensional Hilbert space (see Section ??)
as follows. Addition and multiplication by a scalar are deﬁned in the natural way: (f + g)(x) =
Web draft 2007-01-08 21:59

DRAFT
19.3. TOOL: THE FOURIER TRANSFORM TECHNIQUE
p19.5 (389)
f(x) + g(x) and (αf)(x) = αf(x) for every f, g : {±1}n →R, α ∈R. We deﬁne the inner product
of two functions f, g, denoted ⟨f, g⟩, to be Ex∈{±1}n[f(x)g(x)].
The standard basis for this space is the set {ex}x∈{±1}n, where ex(y) is equal to 1 if y = x,
and equal to 0 otherwise. This is an orthonormal basis, and every function f : {±1}n →R can be
represented in this basis as f = P
x axex. For every x ∈{±1}n, the coeﬃcient ax is equal to f(x).
The Fourier basis for this space is the set {χα}α⊆[n] where χα(x) = Q
i∈α xi (χ∅is the constant
1 function). These correspond to the linear functions over GF(2). To see this, note that every
linear function of the form b 7→a ⊙b (with a, b ∈{0, 1}n) is mapped by our transformation to the
function taking x ∈{±1}n to Q
i s.t. ai=1 xi.
The Fourier basis is indeed an orthonormal basis for the Hilbert space. Indeed, the random
subsum principle implies that for every α, β ⊆[n], ⟨χα, χβ⟩= δα,β where δα,β is equal to 1 iﬀα = β
and equal to 0 otherwise. This means that every function f : {±1}n →R can be represented as
f = P
α⊆[n] ˆfαχα. We call ˆfα the αth Fourier coeﬃcient of f.
We will often use the following simple lemma:
Lemma 19.7
Every two functions f, g:{±1}n →R satisfy
1. ⟨f, g⟩= P
α ˆfαˆgα.
2. (Parseval’s Identity) ⟨f, f⟩= P
α ˆf2
α
Proof: The second property follows from the ﬁrst. To prove the ﬁrst we expand
⟨f, g⟩= ⟨
X
α
ˆfαχα,
X
β
ˆgβχβ⟩=
X
α,β
ˆfαˆgβ⟨χα, χβ⟩=
X
α,β
ˆfαˆgβδα,β =
X
α
ˆfαˆgα
■
Example 19.8
Some examples for the Fourier transform of particular functions:
1. If f(u1, u2, . . . , un) = ui (i.e., f is a coordinate function, a concept we will see again soon)
then f = χ{i} and so ˆf{i} = 1 and ˆfα = 0 for α ̸= {i}.
2. If f is a random boolean function on n bits, then each ˆfα is a random variable that is a sum of
2n binomial variables (equally likely to be 1, −1) and hence looks like a normally distributed
variable with standard deviation 2n/2 and mean 0. Thus with high probability, all 2n Fourier
coeﬃcients have values in [−poly(n)
2n/2
, poly(n)
2n/2
].
Web draft 2007-01-08 21:59

DRAFT
p19.6 (390)
19.3. TOOL: THE FOURIER TRANSFORM TECHNIQUE
The connection to PCPs: High level view
In the PCP context we are interested in Boolean-valued functions, i.e., those from GF(2)n to GF(2).
Under our transformation these are mapped to functions from {±1}n to {±1}. Thus, we say that
: f {±1}n →R is Boolean if f(x) ∈{±1} for every x ∈{±1}n. Note that if f is Boolean then
⟨f, f⟩= Ex[f(x)2] = 1.
On a high level, we use the Fourier transform in the soundness proofs for PCP’s to show that
if the veriﬁer accepts a proof π with high probability then π is “close to” being “well-formed”
(where the precise meaning of “close-to” and “well-formed” is context dependent). Technically,
we will often be able to relate the acceptance probability of the veriﬁer to an expectation of the
form ⟨f, g⟩= Ex[f(x)g(x)], where f and g are Boolean functions arising from the proof. We then
use techniques similar to those used to prove Lemma 19.7 to relate this acceptance probability
to the Fourier coeﬃcients of f, g, allowing us to argue that if the veriﬁer’s test accepts with high
probability, then f and g have few relatively large Fourier coeﬃcients. This will provide us with
some nontrivial useful information about f and g, since in a “generic” or random function, all the
Fourier coeﬃcient are small and roughly equal.
19.3.2
Analysis of the linearity test over GF(2)
We will now prove Theorem 18.23, thus completing the proof of the PCP Theorem. Recall that
the linearity test is provided a function f : GF(2)n →GF(2) and has to determine whether f
has signiﬁcant agreement with a linear function. To do this it picks x, y ∈GF(2)n randomly and
accepts iﬀf(x + y) = f(x) + f(y).
Now we rephrase this test using {±1} instead of GF(2), so linear functions turn into Fourier basis
functions. For every two vectors x, y ∈{±1}n, we denote by xy their componentwise multiplication.
That is, xy = (x1y1, . . . , xnyn). Note that for every basis function χα(xy) = χα(x)χα(y).
For two Boolean functions f, g, ⟨f, g⟩is equal to the fraction of inputs on which they agree
minus the fraction of inputs on which they disagree. It follows that for every ϵ ∈[0, 1] and functions
f, g : {±1}n →{±1}, f has agreement 1
2 + ϵ
2 with g iﬀ⟨f, g⟩= ϵ. Thus, if f has a large Fourier
coeﬃcient then it has signiﬁcant agreement with some Fourier basis function, or in the GF(2)
worldview, f is close to some linear function. This means that Theorem 18.23 can be rephrased as
follows:
Theorem 19.9
Suppose that f : {±1}n →{±1} satisﬁes Prx,y[f(xy) = f(x)f(y)] ≥1
2 + ϵ. Then, there is some
α ⊆[n] such ˆfα ≥2ϵ.
Proof: We can rephrase the hypothesis as Ex,y[f(xy)f(x)f(y)] ≥(1
2 + ϵ) −(1
2 −ϵ) = 2ϵ. We note
that from now on we do not need f to be Boolean, but merely to satisfy ⟨f, f⟩= 1.
Expressing f by its Fourier expansion,
2ϵ ≤Ex,y[f(xy)f(x)f(y)] = Ex,y[(
X
α
ˆfαχα(xy))(
X
β
ˆfβχβ(x))(
X
γ
ˆfγχγ(y))].
Web draft 2007-01-08 21:59

DRAFT
19.3. TOOL: THE FOURIER TRANSFORM TECHNIQUE
p19.7 (391)
Since χα(xy) = χα(x)χα(y) this becomes
= Ex,y[
X
α,β,γ
ˆfα ˆfβ ˆfγχα(x)χα(y)χβ(x)χγ(y)].
Using linearity of expectation:
=
X
α,β,γ
ˆfα ˆfβ ˆfγEx,y[χα(x)χα(y)χβ(x)χγ(y)]
=
X
α,β,γ
ˆfα ˆfβ ˆfγEx [χα(x)χβ(x)] Ey [χα(y)χγ(y)]
(because x, y are independent).
By orthonormality Ex[χα(x)χβ(x)] = δα,β, so we simplify to
=
X
α
ˆf3
α
≤(max
α
ˆfα) × (
X
α
ˆf2
α)
Since P
α ˆf2
α = ⟨f, f⟩= 1, this expression is at most maxα
n
ˆfα
o
. Hence maxα ˆfα ≥2ϵ and the
theorem is proved. ■
19.3.3
Coordinate functions, Long code and its testing
Let W ∈N. We say that f : {±1}W →{±1} is a coordinate function if there is some w ∈[W],
such that f(x1, x2, . . . , xW ) = xw; in other words, f = χ{w}.
Definition 19.10 (Long Code)
The long code for [W] encodes each w ∈[W] by the table of all values of the function χ{w} :
{±1}[W] →{±1}.
Remark 19.11
Note that w, normally written using log W bits, is being represented using a table of 2W bits, a
doubly exponential blowup! This ineﬃciency is the reason for calling the code “long.”
Similar to the test for the Walsh-Hadamard code, when testing the long code, we are given a
function f :{±1}W →{±1}, and want to ﬁnd out if f has good agreement with χ{w} for some w,
namely, ˆf{w} is signiﬁcant. Such a test is described in Exercise 16 of the previous chapter, but it
is not suﬃcient for the proof of H˚astad’s Theorem, which requires a test using only three queries.
Below we show such a three query test albeit at the expense of achieving the following weaker
guarantee: if the test passes with high probability then f has a good agreement with a function
Web draft 2007-01-08 21:59

DRAFT
p19.8 (392)
19.3. TOOL: THE FOURIER TRANSFORM TECHNIQUE
χα with |α| small (but not necessarily equal to 1). This weaker conclusion will be suﬃcient in the
proof of Theorem 19.5.
Let ρ > 0 be some arbitrarily small constant. The test picks two uniformly random vectors
x, y ∈{±1}W and then a vector z ∈{±1}[ W] according to the following distribution: for every
coordinate i ∈[W], with probability 1 −ρ we choose zi = +1 and with probability ρ we choose
zi = −1. Thus with high probability, about ρ fraction of coordinates in z are −1 and the other
1 −ρ fraction are +1. We think of z as a “noise” vector. The test accepts iﬀf(x)f(y) = f(xyz).
Note that the test is similar to the linearity test except for the use of the noise vector z.
Suppose f = χ{w}. Then
f(x)f(y)f(xyz) = xwyw(xwywzw) = 1 · zw
Hence the test accepts iﬀzw = 1 which happens with probability 1 −ρ. We now prove a certain
converse:
Lemma 19.12
If the test accepts with probability 1/2 + ϵ then P
α ˆf3
α(1 −2ρ)|α| ≥2ϵ.
Proof: If the test accepts with probability 1/2 + ϵ then E[f(x)f(y)f(xyz)] = 2ϵ. Replacing f by
its Fourier expansion, we have
2ϵ ≤Ex,y,z

(
X
α
ˆfαχα(x)) · (
X
β
ˆfβχβ(y)) · (
X
γ
ˆfγχγ(xyz))


= Ex,y,z

X
α,β,γ
ˆfα ˆfβ ˆfγχα(x)χβ(y)χγ(x)χγ(y)χγ(z)


=
X
α,β,γ
ˆfα ˆfβ ˆfγEx,y,z [χα(x)χβ(y)χγ(x)χγ(y)χγ(z)] .
Orthonormality implies the expectation is 0 unless α = β = γ, so this is
=
X
α
ˆf3
αEz[χα(z)]
Now Ez[χα(z)] = Ez
Q
w∈α zw

which is equal to Q
w∈α E[zw] = (1 −2ρ)|α| because each coor-
dinate of z is chosen independently. Hence we get that
2ϵ ≤
X
α
ˆf3
α(1 −2ρ)|α|
■
The conclusion of Lemma 19.12 is reminiscent of the calculation in the proof of Theorem 19.9,
except for the extra factor (1 −2ρ)|α|. This factor depresses the contribution of ˆfα for large α,
allowing us to conclude that the small α’s must contribute a lot. This formalized in the following
corollary (left as Exercise 2).
Web draft 2007-01-08 21:59

DRAFT
19.4. PROOF OF THEOREM ??
p19.9 (393)
Corollary 19.13
If f passes the long code test with probability 1/2 + δ then
X
α:|α|≤k
ˆf3
α ≥2δ −ϵ,
where k =
1
2ρ log 1
ϵ.
19.4
Proof of Theorem 19.5
Recall that our proof of the PCP Theorem implies that there are constants γ > 0, s ∈N such that
(1−γ)-GAP 2CSPs is NP-hard (see Claim 18.36). This means that for every NP-language L we have
a PCP-veriﬁer for L making two queries over alphabet {0, . . . , s−1} with perfect completeness and
soundness parameter 1−γ. Furthermore this PCP system has the property that the veriﬁer accepts
the answer pair z1, z2 iﬀz2 = hr(z1) where hr is a function (depending on the veriﬁer’s randomness
r) mapping {0, . . . , s −1} to itself (see Exercise 3). We call this the projection property. Using
the Raz’s parallel repetition lemma (Theorem 19.3), we can reduce the soundness parameter to an
arbitrary small constant at the expense of increasing the alphabet. Note that parallel repetition
preserves the projection property.
Let L be an NP-language and ϵ > 0 an arbitrarily small constant. By the above there exists a
constant W and PCP-veriﬁer VRaz (having the projection property) that makes two queries to a
polynomial-sized PCP proof π with alphabet {1, . . . , W} such that for every x, if x ∈L then there
exists π such that Pr[V π
Raz(x) = 1] = 1 and if x ̸∈L then Pr[V π
Raz(x) = 1] < ϵ for every π.
Now we describe H˚astad’s veriﬁer VH. It essentially follows VRaz, but it expects each entry in
the PCP proof π to be encoded using the long code. It expects these encodings to be bifolded,
a technical property we now deﬁne and is motivated by the observation that coordinate functions
satisfy χ{w}(−u) = −χ{w}(u), where −u is the vector (−u1, . . . , −uW ).
Definition 19.14
A function f : {±1}W →{±1} is bifolded if for all u ∈{±1}W , f(−u) = −f(u).
Whenever the PCP proof is supposed to contain a longcode codeword then we may assume
without loss of generality that the function is bifolded. The reason is that the veriﬁer can identify,
for each pair of inputs u, −u, one designated representative —say the one whose ﬁrst coordinate is
+1— and just deﬁne f(−u) to be −f(u). One beneﬁt —though of no consequence in the proof—
of this convention is that bifolded functions require only half as many bits to represent. We will
use the following fact:
Lemma 19.15
If f : {±1}W →{±1} is bifolded and ˆfα ̸= 0 then |α| must be an odd number (and in particular,
nonzero).
Proof: By deﬁnition,
ˆfα = ⟨f, χα⟩=
1
2n
X
u
f(u)
Y
i∈α
ui.
Web draft 2007-01-08 21:59

DRAFT
p19.10 (394)
19.4. PROOF OF THEOREM ??
If |α| is even then Q
i∈α ui = Q
i∈α(−ui). So if f is bifolded, the terms corresponding to u and −u
have opposite signs and the entire sum is 0. ■
H˚astad’s veriﬁer.
Recall that VRaz uses its randomness to select a function two entries i, j in
the table π and a function h : [W] →[W], and accepts iﬀπ(j) = h(π(i)).
H˚astad’s veriﬁer,
denoted VH, expects the proof ˜π to consist of (bifolded) longcode encodings of each entry of π. The
veriﬁer VH emulates VRaz to pick two locations i,j in the table and a function h : [W] →[W] such
that VRaz’s test is to accept iﬀπ[j] = h(π[i]). The proof ˜π contains in the locations i and j two
functions f and g respectively (which may or may not be the longcode encoding of π(i) and π(j)).
Instead of reading the long codes f, g in their entirety, the veriﬁer VH performs a simple test that
is reminiscent of the long code test. For a string y ∈{±1}W we denote by h−1(y) the string such
that for every w ∈[W], h−1(y)w = yh(w). In other words, for each u ∈[W], the bit yu appears in
all coordinates of h−1(y) that are indexed by integers in the subset h−1(u). This is well deﬁned
because

h−1(u) : u ∈[W]
	
is a partition of [W]. VH chooses uniformly at random u, y ∈{±1}W
and chooses z ∈{±1}W by letting zi = +1 with probability 1 −ρ and zi = −1 with probability ρ.
It then accepts Iﬀ
f(u)g(y) = f(h−1(y)uz)
(1)
Translating back from {±1} to {0, 1}, note that VH’s test is indeed linear, as it accepts iﬀ
˜π[i1] + ˜π[i2] + ˜π[i3] = b for some i1, i2, i3 ∈[m2W ] and b ∈{0, 1}. (The bit b can indeed equal 1
because of the way VH ensures the bifolding property.)
Completeness of VH.
Suppose f, g are long codes of two integers w, u satisfying h(w) = u (in
other words, Vraz would have accepted the assignments represented by these integers). Then
f(u)g(y)f(h−1(y)uz) = uwyu(h−1(y)uzw
= uwyu(yh(w)uwzw)
= zw.
Hence VH accepts iﬀzw = 1, which happens with probability 1 −ρ.
Soundness of VH.
We now show that if VH accepts f, g with probability signiﬁcantly more than
1/2, then the Fourier transforms of f, g must be correlated. To formalize this we deﬁne for α ⊆[W],
h2(α) =

u ∈[W] :
h−1(u) ∩α
 is odd
	
Notice in particular that for every u ∈h2(α) there is at least one w ∈α such that h(w) = u.
In the next Lemma δ is allowed to be negative.
Lemma 19.16
Let f, g : {±1}W →{±1}, h : [W] →[W] be bifolded functions passing VH’s test (1) with
probability at least 1/2 + δ. Then
X
α⊆[W],α̸=∅
ˆf2
αˆgh2(α)(1 −2ρ)|α| ≥2δ
Web draft 2007-01-08 21:59

DRAFT
19.4. PROOF OF THEOREM ??
p19.11 (395)
Proof: By hypothesis, f, g are such that E[f(u)f(uh−1(y)z)g(y)] ≥2δ. Replace f, g by their
Fourier expansions. We get that
2δ ≤= Eu,y,z

(
X
α
ˆfαχα(x))(
X
β
ˆgβχβ(y))(
X
γ
ˆfγχγ(uh−1(y)z))


=
X
α,β,γ
ˆfαˆgβ ˆfγEu,y,z

χα(u)χβ(y)χγ(u)χγ(h−1(y))χγ(z)

By orthonormality this simpliﬁes to
=
X
α,β
ˆf2
αˆgβEy,z

χβ(y)χα(h−1(y))χα(z)

=
X
α,β
ˆf2
αˆgβ(1 −2ρ)|α|Ey

χα(h−1(y)χβ(y)

(2)
since χα(z) = (1 −2ρ)|α|, as noted in our analysis of the long code test. Now we have
Ey[χα(h−1(y))χβ(y)] = Ey[
Y
w∈α
h−1(y)w
Y
u∈β
yu]
= Ey[
Y
w∈α
yh(w)
Y
u∈β
yu],
which is 1 if h2(α) = β and 0 otherwise. Hence (2) simpliﬁes to
X
α
ˆf2
αˆgh2(α)(1 −2ρ)|α|.
Finally we note that since the functions are assumed to be bifolded, the Fourier coeﬃcients ˆf∅and
ˆg∅are zero. Thus those terms can be dropped from the summation and the Lemma is proved. ■
The following corollary of Lemma 19.16 completes the proof of H˚astad’s 3-bit PCP Theorem.
Corollary 19.17
Let ϵ be the soundness parameter of VRaz. If ρ, δ satisfy ρδ2 > ϵ then the soundness parameter of
VH is at most 1/2 + δ.
Proof: Suppose VH accepts a proof ˜π with probability at least 1/2 + δ. We give a probabilistic
construction of a proof π causing VRaz to accept the same statement with probability at least ρδ2.
Suppose that VRaz uses proofs π with m entries in [W]. We can think of ˜π as providing, for
every i ∈[m], a function fi : {±1}W {±1}. We will use ˜π to construct a proof π for VRaz as follows:
we ﬁrst use fi to come up with a distribution Di over [W]. We then let π[i] be a random element
from Di.
Web draft 2007-01-08 21:59

DRAFT
p19.12 (396)
19.4. PROOF OF THEOREM ??
The distribution Di.
Let f = fi. The distribution Di is deﬁned by ﬁrst selecting α ⊆[W] with
probability ˆf2
α and then selecting w at random from α. This is well deﬁned because P
α ˆf2
α = 1 and
(due to bifolding) f∅= 0.
Recall that VRaz picks using its random tape a pair i, j of locations and a function h : [W] →[W]
and then veriﬁes that π[j] = h(π[i]). Let r be some possible random tape of VRaz and let i, j, h be
the pair of entries in π and function that are determined by r. We deﬁne the indicator random
variable Ir to be 1 if for w ∈R Di and u ∈R Dj it holds that w = h(u) and to be 0 otherwise. Thus,
our goal is to show that
Eπ=D1,...,Dm[Er[Ir]] ≥ρδ2
(3)
since that would imply that there exists a table π causing VRaz to accept with probability at least
ρδ2, proving the corollary.
To prove (3) we ﬁrst notice that linearity of expectation allows us to exchange the order of the
two expectations and so it is enough to bound Er[EDi,Dj[Ir]] where i, j are the entries determined
by the random tape r. For every r denote by δr the probability that VH accepts ˜π when it uses r
as the random tape for VRaz. The acceptance probability of VH is Er[1
2 + δr] and hence Er[δr] = δ.
Let i, j, h be the pair and function determined by r and denote by f = fi and g = fj where
fi (resp. fj) is the function at the ith (resp. jth) entry of the table ˜π. What is the chance that a
pair of assignments w ∈R Di and v ∈R Dj will satisfy the constraint? (i.e., will satisfy v = h(w)?).
Recall that we pick w and u by choosing α with probability ˆf2
α, β with probability ˆg2
β and choosing
w ∈R α, v ∈R β. Now if β = h2(α) then for every v ∈β there exists w ∈α with h(w) = v and
hence the probability the constraint is satisﬁed is at least 1/|α|. Thus, we have that
X
α
1
|α|
ˆf2
αˆg2
h2(α) ≤EDi,Dj[Ir]
(4)
This is similar to (but not quite the same as) the expression in Lemma 19.16, according to
which
2δr ≤
X
α
ˆf2
αˆgh2(α)(1 −2ρ)|α|.
However, since one can easily see that (1 −2ρ)|α| ≤
2
p
ρ |α|
we have
2δr ≤
X
α
ˆf2
α
ˆgh2(α)

2
p
ρ |α|
Or
δr
√ρ ≤
X
α
ˆf2
α
ˆgh2(α)

1
√
|α|
Applying the Cauchy-Schwartz inequality, P
i aibi ≤(P
i a2
i )1/2(P
i b2
i )1/2, with ˆfα
ˆgπ2(α)

1
√
|α|
playing the role of the ai’s and ˆfα playing that of the bi’s, we obtain
δr
√ρ ≤
X
α
ˆf2
α
ˆgh2(α)

1
√
|α| ≤
 X
α
ˆf2
α
!1/2  X
α
ˆfα
2ˆg2
h2(α)
1
|α|
!1/2
(5)
Web draft 2007-01-08 21:59

DRAFT
19.5. LEARNING FOURIER COEFFICIENTS
p19.13 (397)
Since P
α ˆf2
α = 1, by squaring (5) and combining it with (4) we get that for every r,
δ2
rρ ≤EDi,Dj[Ir]
taking expectation over r and using E[X]2 ≤E[X2] we get that
δ2ρ = Er[δr]2ρ ≤Er[δ2
r]ρ ≤Er[EDi,Dj[Ir]]
proving (3). ■
19.5
Learning Fourier Coeﬃcients
Suppose that you are given random access to a Boolean function f : {±1}n →{±1} and want to
ﬁnd the high Fourier coeﬃcients of f. Of course, we can compute all of the coeﬃcients in time
polynomial in 2n, but is there a faster algorithm? By the Parseval equality (Lemma 19.7) we know
that there can be at most 1/ϵ2 coeﬃcients with absolute value larger than ϵ, and so we can hope
to learn these coeﬃcients in time polynomial in n, and 1/ϵ. It turns out we can (almost) achieve
this goal:
Theorem 19.18 ([?])
There is an algorithm A that given input n ∈N,ϵ ∈(0, 1) and random access to a function
f : {±1}n →{±1}, runs in poly(n, 1/ϵ) time and with probability at least 0.9 outputs a set L of
size at most O(1/ϵ2) such that for every α ⊆[n], if | ˆfα| > ϵ then α ∈L.
Proof: We identify subsets of [n] with strings in {0, 1}m in the obvious way. For k ≤n and
α ∈{0, 1}k denote
˜fα⋆=
X
β∈{0,1}n−k
ˆf2
α◦β,
where ◦denotes concatenation. By Parseval (Lemma 19.7) ˜f⋆= 1. Note also that for every k < n
and α ∈{0, 1}k, ˜fα⋆= ˜fα0⋆+ ˜fα1⋆. Therefore, if we think of the full depth-n binary labeled by
binary strings of length ≤n (with the root being the empty word and the two children of α are α0
and α1), then at any level of this tree there can be at most 1/ϵ2 strings α such that ˜fα⋆> ϵ2 (the
kth level of the tree corresponds to all strings of length k). Note that if a string α satisﬁes ˜fα⋆< ϵ2
then the same holds for every string of the form α ◦β. Our goal will be to ﬁnd all these strings at
all levels, and then output all the strings that label leaves in the tree (i.e., all n-bit strings).
The heart of the algorithm is a procedure Estimate that given α and oracle access to f(·),
outputs an estimate of fα within ϵ/4 accuracy with probability 1 −
ϵ2
100n. Using this procedure we
work our way from the root down, and whenever Estimate(α) gives a value smaller than ϵ/2 we
“kill” this node and will not deal with it and its subnodes. Note that unless the output of Estimate
is more than ϵ/4-far from the real value (which we will ensure by the union bound happens with
probability less than 0.1 over all the levels) at most 4/ϵ nodes will survive at any level.
The
algorithm will output the 4/ϵ leaves that survive.
The procedure Estimate uses the following claim:
Web draft 2007-01-08 21:59

DRAFT
p19.14 (398)
19.6. OTHER PCP THEOREMS: A SURVEY
Claim 19.19
For every α,
˜fα⋆= Ex,x′∈R{0,1}k,y∈R{0,1}n−k[f(x ◦y)f(x′ ◦y)χα(x)χα(x′)]
Proof: We start with the case that α = 0k. To get some intuition, suppose that ˜f0k⋆= 1. This
means that f can be expressed as a sum of functions of the form χ0k◦β and hence it does not depend
on its ﬁrst k variables. Thus f(x◦y) = f(x′◦y) and we’ll get that E[f(x◦y)f(x′◦y)] = E[f(z)2] = 1.
More generally, if ˜f0k⋆is large then that means that in the Fourier representation, the weight of
functions not depending on the ﬁrst k variables is large and hence we expect large correlation
between f(x′ ◦y) and f(x ◦y). This is veriﬁed by the following calculations:
2−n−k X
x,x′,y
f(x ◦y)f(x′ ◦y)
=
basis change
2−n−k X
x,x′,y

X
γ◦β
ˆf(γ ◦β)χγ◦β(x ◦y)



X
γ′◦β′
ˆf(γ′ ◦β′)χγ′◦β′(x′ ◦y)


=
χγ◦β(x ◦y) = χγ(x)χβ(y)
2−n−k X
x,x′,y

X
γ◦β
ˆf(γ ◦β)χγ(x)χβ(y)



X
γ′◦β′
ˆf(γ′ ◦β′)χγ′(x′)χβ′(y)


=
reordering terms
X
γ,β,γ′,β′
ˆf(γβ) ˆf(γ′β′)2−k
 X
x
χγ′(x)
!
2−k
 X
x′
χγ(x′)
!
2−(n−k)
 X
y
χβ(y)χβ′(y)
!
=
Σχγ(x) = 0 for γ ̸= 0k
X
β,β′
ˆf(0k ◦β) ˆf(0k ◦β′)δβ,β′ =
X
β
ˆf(0k ◦β)2 = ˜f0k⋆
For the case α ̸= 0k, we essentially add these factors to translate it to the case α = 0k. Indeed
one can verify that if we deﬁne g(x◦y) = f(x◦y)χα(x) then for every β ∈{0, 1}n−k. g0k◦β = fα◦β.
■
By the Chernoﬀbound, we can estimate the expectation of Claim 19.19 (and hence ˜fα⋆) using
repeated sampling, thus obtaining the procedure Estimate and completing the proof. ■
19.6
Other PCP Theorems: A Survey
The following variants of the PCP Theorem have been obtained and used for various applications.
19.6.1
PCP’s with sub-constant soundness parameter.
Because ℓ-times parallel repetition transforms a proof of size m to a proof of size mℓ, we cannot
use it with ℓlarger than a constant and still have a polynomial-sized proof. Fortunately, there have
been direct constructions of PCP’s achieving low soundness using larger alphabet size, but without
increasing the proof’s size. Raz and Safra [?] show that there is an absolute constant q such that
Web draft 2007-01-08 21:59

DRAFT
19.6. OTHER PCP THEOREMS: A SURVEY
p19.15 (399)
for every W ≤√log n, every NP language has a q-query veriﬁer over alphabet {0, . . . , W −1} that
uses O(log n) random bits, and has soundness 2−Ω(log W).
19.6.2
Amortized query complexity.
Some applications require binary-alphabet PCP systems enjoying a tight relation between the
number of queries (that can be an arbitrarily large constant) and the soundness parameter. The
relevant parameter here turns out to be the free bit complexity [?, ?]. This parameter is deﬁned as
follows. Suppose the number of queries is q. After the veriﬁer has picked its random string, and
picked a sequence of q addresses, there are 2q possible sequences of bits that could be contained
in those addresses.
If the veriﬁer accepts for only t of those sequences, then we say that the
free bit parameter is log t (note that this number need not be an integer).
In fact, for most
applications it suﬃces to consider the amortized free bit complexity [?]. This parameter is deﬁned
as lims→0 fs/ log(1/s), where fs is the number of free bits needed by the veriﬁer to ensure the
soundness parameter is at most s. H˚astad constructed systems with amortized free bit complexity
tending to zero [?]. That is, for every ϵ > 0, he gave a PCP-veriﬁer for NP that uses O(log n)
random bits and ϵ amortized free bits.
He then used this PCP system to show (using tools
from [?, ?, ?]) that MAX INDSET (and so, equivalently, MAX CLIQUE) is NP-hard to approximate
within a factor of n1−ϵ for arbitrarily small ϵ > 0.
19.6.3
Unique games.
Exercises
§1 Prove that there is a polynomial-time algorithm that given a satisﬁable 2CSPW instance ϕ
over {0..W−1} where all the constraints are permutations (i.e, ϕi checks that uj′ = h(uj) for
some j, j′ ∈[n] and permutation h : {0..W−1} →{0..W−1}) ﬁnds a satisfying assignment u
for ϕ.
§2 Prove Corollary 19.13.
§3 Prove that the PCP system resulting from the proof of Claim 18.36 (Chapter 18) satisﬁes
the projection property.
§4 Let f : {±1}n →{±1} and let I ⊆[n]. Let MI be the following distribution: we choose
z ∈R MI by for i ∈I, choose zi to be +1 with probability 1/2 and −1 with probability 1/2
(independently of other choices), for i ̸∈I choose zi = +1. We deﬁne the variation of f on I
to be Prx∈R{±1}n,z∈RMI[f(x) ̸= f(xz)].
Suppose that the variation of f on I is less than ϵ.
Prove that there exists a function
g : {±1}n →R such that (1) g does not depend on the coordinates in I and (2) g is 10ϵ-close
to f (i.e., Prx∈R{±1}n[f(x) ̸= g(x)] < 10ϵ). Can you come up with such a g that outputs
values in {±1} only?
§5 For f : {±1}n →{±1} and x ∈{±1}n we deﬁne Nf(x) to be the number of coordinates i
such that if we let y to be x ﬂipped at the ith coordinate (i.e., y = xei where ei has −1 in the
Web draft 2007-01-08 21:59

DRAFT
p19.16 (400)
19.6. OTHER PCP THEOREMS: A SURVEY
ith coordinate and +1 everywhere else) then f(x) ̸= f(y). We deﬁne the average sensitivity
of f, denoted by as(f) to be the expectation of Nf(x) for x ∈R {±1}n.
(a) Prove that for every balanced function f : {±1}n →{±1} (i.e., Pr[f(x) = +1] = 1/2),
as(f) ≥1.
(b) Let f be balanced function from {±1}n to {±1} with as(f) = 1. Prove that f is a
coordinate function or its negation (i.e., f(x) = xi or f(x) = −xi for some i ∈[n] and
for every x ∈{±1}n).
Web draft 2007-01-08 21:59

DRAFT
Chapter 20
Quantum Computation
“Turning to quantum mechanics.... secret, secret, close the doors! we always have
had a great deal of diﬃculty in understanding the world view that quantum mechanics
represents ... It has not yet become obvious to me that there’s no real problem. I
cannot deﬁne the real problem, therefore I suspect there’s no real problem, but I’m
not sure there’s no real problem. So that’s why I like to investigate things.”
Richard Feynman 1964
“The only diﬀerence between a probabilistic classical world and the equations of the
quantum world is that somehow or other it appears as if the probabilities would have
to go negative..”
Richard Feynman, in “Simulating physics with computers”, 1982
Quantum computers are a new computational model that may be physically realizable and
may have an exponential advantage over ‘classical” computational models such as probabilistic
and deterministic Turing machines.
In this chapter we survey the basic principles of quantum
computation and some of the important algorithms in this model.
As complexity theorists, the main reason to study quantum computers is that they pose a
serious challenge to the strong Church-Turing thesis that stipulates that any physically reasonable
computation device can be simulated by a Turing machine with polynomial overhead. Quantum
computers seem to violate no fundamental laws of physics and yet currently we do not know any
such simulation. In fact, there is some evidence to the contrary: as we will see in Section 20.7,
there is a polynomial-time algorithm for quantum computers to factor integers, where despite
much eﬀort no such algorithm is known for deterministic or probabilistic Turing machines. In fact,
the conjectured hardness of this problem underlies of several cryptographic schemes (such as the
RSA cryptosystem) that are currently widely used for electronic commerce and other applications.
Physicists are also interested in quantum computers as studying them may shed light on quantum
mechanics, a theory which, despite its great success in predicting experiments, is still not fully
understood.
This chapter utilizes some basic facts of linear algebra, and the space Cn. These are reviewed in
Appendix A. See also Note 20.8 for a quick reminder of our notations.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p20.1 (401)

DRAFT
p20.2 (402)
20.1. QUANTUM WEIRDNESS
20.1
Quantum weirdness
It is beyond this book (and its authors) to fully survey quantum mechanics. Fortunately, only
very little physics is needed to understand the main results of quantum computing.
However,
these results do use some of the more counterintuitive notions of quantum mechanics such as the
following:
Any object in the universe, whether it is a particle or a cat, does not have deﬁnite
properties (such as location, state, etc..) but rather has a kind of probability wave over
its potential properties. The object only achieves a deﬁnite property when it is observed,
at which point we say that the probability wave collapses to a single value.
At ﬁrst this may seem like philosophical pontiﬁcation analogous to questions such as “if a tree
falls and no one hears, does it make a sound?” but these probability waves are in fact very real, in
the sense that they can interact and interfere with one another, creating experimentally measurable
eﬀects. Below we describe two of the experiments that led physicists to accept this counterintuitive
theory.
20.1.1
The 2-slit experiment
In the 2-slit experiment a source that ﬁres electrons one by one (say, at the rate of one electron per
second) is placed in front of a wall containing two tiny slits (see Figure ??). Beyond the wall we
place an array of detectors that light up whenever an electron hits them. We measure the number
of times each detector lights up during an hour.
When we cover one of the slits, we would expect that the detectors that are directly behind the
open slit will receive the largest number of hits, and as Figure ?? shows, this is indeed the case.
When both slits are uncovered we expect that the number of times each detector is hit is the sum
of the number of times it is hit when the ﬁrst slit is open and the number of times it is hit when the
second slit is open. In particular, uncovering both slits should only increase the number of times
each location is hit.
Surprisingly, this is not what happens. The pattern of hits exhibits the “interference” phenom-
ena depicted in Figure ??. In particular, at several detectors the total electron ﬂux is lower when
both slits are open as compared to when a single slit is open. This deﬁes explanation if electrons
behave as particles or “little balls”.
According to Quantum mechanics, the explanation is that it is wrong to think of an electron has
a “little ball” that can either go through the ﬁrst slit or the second (i.e., has a deﬁnite property).
Rather, somehow the electron instantaneously explores all possible paths to the detectors (and so
“ﬁnds out” how many slits are open) and then decides on a distribution among the possible paths
that it will take.
You might be curious to see this “path exploration” in action, and so place a detector at each
slit that light up whenever an electron passes through that slit. When this is done, one can see
that every electron passes through only one of the slits like a nice little ball. But furthermore, the
interference phenomenon disappears and the graph of electron hits becomes, as originally expected,
the sum of the hits when each slit is open. The explanation is that, as stated above, observing an
Web draft 2007-01-08 21:59

DRAFT
20.1. QUANTUM WEIRDNESS
p20.3 (403)
Note 20.1 (Physically implementing quantum computers.)
object “collapses” their distribution of possibilities, and so changes the result of the experiment.
(One moral to draw from this is that quantum computers, if they are ever built, will have to be
carefully isolated from external inﬂuences and noise, since noise may be viewed as a “measurement”
performed by the environment on the system. Of course, we can never completely isolate the system,
which means we have to make quantum computation tolerant of a little noise. This seems to be
possible under some noise models, see the chapter notes.)
20.1.2
Quantum entanglement and the Bell inequalities.
Even after seeing the results of the 2-slit experiment, you might still be quite skeptical of the
explanation that quantum mechanics oﬀers.
If you do, you are in excellent company.
Albert
Einstein didn’t buy it either. While he agreed that the 2-slit experiment means that electrons
are not exactly “little balls”, he didn’t think that it is suﬃcient reason to give up such basic
notions of physics such as the existence of an independent reality, with objects having deﬁnite
properties that do not depend on whether one is observing them. To show the dangerous outcomes
of giving up such notions, in a 1951 paper with Podosky and Rosen (EPR for short) he described a
thought experiment showing that accepting Quantum mechanics leads to the seemingly completely
ridiculous conclusion that systems in two far corners of the universe can instantaneously coordinate
their actions.
In 1964 John Bell showed how the principles behind EPR thought experiment can be turned into
an actual experiment. In the years since, Bell’s experiment has been repeated again and again with
the same results: quantum mechanics’ predictions were veriﬁed and, contrary to Einstein’s expec-
tations, the experiments refuted his intuitions about how the universe operates. In an interesting
twist, in recent years the ideas behind EPR’s and Bell’s experiments were used for a practical goal:
encryption schemes whose security depends only on the principles of quantum mechanics, rather
than any unproven conjectures such as P ̸= NP.
For complexity theorists, probably the best way to understand Bell’s experiment is as a two
prover game. Recall that in the two prover setting, two provers are allowed to decide on a strategy
and then interact separately with a polynomial-time veriﬁer which then decides whether to accept
or reject the interaction (see Chapters 8 and 18).
The provers’ strategy can involve arbitrary
computation and even be randomized, with the only constraint being that the provers are not
allowed to communicate during their interaction with the veriﬁer.
Bell’s game.
In Bell’s setting, we have an extremely simple interaction between the veriﬁer and
two provers (that we’ll name Alice and Bob): there is no statement that is being proven, and all the
communication involves the veriﬁer sending and receiving one bit from each prover. The protocol
is as follows:
Web draft 2007-01-08 21:59

DRAFT
p20.4 (404)
20.1. QUANTUM WEIRDNESS
1. The veriﬁer chooses two random bits x, y ∈R {0, 1}.
2. It sends x to Alice and y to Bob.
3. Let a denote Alice’s answer and b Bob’s answer.
4. The veriﬁer accepts if and only if a ⊕b = x ∧y.
It is easy for Alice and Bob to ensure the veriﬁer accepts with probability 3/4 (e.g., by always
sending a = b = 0). It turns out this is the best they can do:
Theorem 20.2 ([Bel64])
Regardless of the strategy the provers use, the veriﬁer will accept with probability at most 3/4.
Proof: Assume for the sake of contradiction that there is a (possibly probabilistic) strategy that
causes the veriﬁer to accept with probability more than 3/4. By a standard averaging argument
there is a ﬁxed set of provers’ coins (and hence a deterministic strategy) that causes the veriﬁer to
accept with at least the same probability, and hence we may assume without loss of generality that
the provers’ strategy is deterministic.
A deterministic strategy for the two provers is a pair of functions f, g : {0, 1} →{0, 1} such as
the provers’ answers a, b are computed as a = f(x) and b = g(y). The function f can be one of only
four possible functions: it can be either the constant function zero or one, the function f(x) = x
or the function f(y) = 1 −y. We analyze the case that f(x) = x; the other case are similar.
If f(x) = x then the veriﬁer accepts iﬀb = (x ∧y) ⊕x. On input y, Bob needs to ﬁnd b that
makes the veriﬁer accept. If y = 1 then x ∧y = x and hence b = 0 will ensure the veriﬁer accepts
with probability 1. However, if y = 0 then (x ∧y) ⊕x = x and since Bob does not know x, the
probability that his output b is equal to x is at most 1/2. Thus the total acceptance probability is
at most 3/4. ■
What does this game have to do with quantum mechanics? The main point is that according
to “classical” pre-quantum physics, it is possible to ensure that Alice and Bob are isolated from
one another. Suppose that you are given a pair of boxes that implement some arbitrary strategy
for Bell’s game.
How can you ensure that these boxes don’t have some secret communication
mechanism that allows them to coordinate their answers? We might try to enclose the devices
in lead boxes, but even this does not ensure complete isolation. However, Einstein’s theory of
relativity allows us a foolproof way to ensure complete isolation: place the two devices very far
apart (say at a distance of a 1000 miles from one another), and perform the interaction with each
prover at a breakneck speed: toss each of the coins x, y and demand the answer within less than
one millisecond. Since according to the theory of relativity, nothing travels faster than light (that
only covers about 200 miles in a millisecond), there is no way for the provers to communicate and
coordinate their answers, no matter what is inside the box.
The upshot is that if someone provides you with such devices that consistently succeed in this
experiment with more than 3/4 = 0.75 probability, then she has refuted Einstein’s theory. As we
will see later in Section 20.3.2, quantum mechanics, with its instantaneous eﬀects of measurements,
can be used to actually build devices that succeed in this game with probability at least 0.8 (there
are other games with more dramatic diﬀerences of probabilities) and this has been experimentally
demonstrated.
Web draft 2007-01-08 21:59

DRAFT
20.2. A NEW VIEW OF PROBABILISTIC COMPUTATION.
p20.5 (405)
20.2
A new view of probabilistic computation.
To understand quantum computation, it is helpful to ﬁrst take a diﬀerent viewpoint of a process
we are already familiar with: probabilistic computation.
Suppose that we have an m-bit register.
Normally, we think of such a register as having
some deﬁnite value x ∈{0, 1}m. However, in the context of probabilistic computation, we can
also think of the register’s state as actually being a probability distribution over its possible val-
ues.
That is, we think of the register’s state as being described by a 2m-dimensional vector
v = ⟨v0m, v0m−11, . . . , v1m⟩, where for every x ∈{0, 1}m, vx ∈[0, 1] and P
x vx = 1.
When
we read, or measure, the register, we will obtain the value x with probability vx.
For every x ∈{0, 1}n, we denote by |x⟩the vector that corresponds to the degenerate distribu-
tion that takes the value x with probability 1. That is, |x⟩is the 2m-dimensional vector that has
zeroes in all the coordinates except a single 1 in the xth coordinate. Note that v = P
x∈{0,1}m vx |x⟩.
(We think of all these vectors as column vectors in the space Rm.)
Example 20.3
If a 1-bit register’s state is the distribution that takes the value 0 with probability p and 1 with
probability 1 −p, then we describe the state as the vector p |0⟩+ (1 −p) |1⟩.
The uniform distribution over the possible values of a 2-bit register is represented by 1/4 (|00⟩+ |01⟩+ |10⟩+ |11
The distribution that is uniform on every individual bit, but always satisﬁes that the two bits are
equal is represented by 1/2 (|00⟩+ |11⟩).
An probabilistic operation on the register involves reading its value, and, based on the value
read, modifying it in some deterministic or probabilistic way. If F is some probabilistic operation,
then we can think of F as a function from R2m to R2m that maps the previous state of the register
to its new state after the operation is performed. There are certain properties that every such
operation must satisfy:
• Since F depends only on the contents of the register, and not on the overall distribution, for
every v, F(v) = P
x∈{0,1}m vxF(|v⟩). That is, F is a linear function. (Note that this means
that F can be described by a 2n × 2n matrix.)
• If v is a distribution vector (i.e., a vector of non-negative entries that sum up to one), then so
is F(v). That is, F is a stochastic function. (Note that this means that viewed as a matrix,
F has non-negative entries with each column summing up to 1.)
Example 20.4
The operation that, regardless of the register’s value, writes into it a uniformly chosen random
string, is described by the function F such that F(|x⟩) = 2−m P
x∈{0,1}m |x⟩for every x ∈{0, 1}m.
(Because the set {|x⟩}x∈{0,1}n is a basis for R2m, a linear function is completely determined by its
output on these vectors.)
Web draft 2007-01-08 21:59

DRAFT
p20.6 (406)
20.2. A NEW VIEW OF PROBABILISTIC COMPUTATION.
The operation that ﬂips the ﬁrst bit of the register is described by the function F such that
F(|x1 . . . xm ⟩) = |(1 −x1)x2 . . . xm ⟩for every x1, . . . , xm ∈{0, 1}.
Of course there are many probabilistic operations that cannot be eﬃciently computed, but there
are very simple operations that can certainly be computed. An operation F is elementary if it only
reads and modiﬁes at most three bits of the register, leaving the rest of the register untouched.
That is, there is some operation G : R23 →R23 and three indices j, k, ℓ∈[m] such that for every
x1, . . . , xm ∈{0, 1}, F(|x1 . . . xm ⟩) = |y1 . . . ym ⟩where |yjykyℓ⟩= G(|xjxkxℓ⟩) and yi = xi for
every i ̸∈{j, k, ℓ}. Note that such an operation can be represented by a 23 × 23 matrix and three
indices in [m].
Example 20.5
Here are some examples for operations depending on at most three bits:
000 001 010
011 100 101 110 111
000
1
1
0
0
0
0
0
0
001
0
0
0
0
0
0
0
0
010
0
0
1
1
0
0
0
0
011
0
0
0
0
0
0
0
0
100
0
0
0
0
1
1
0
0
101
0
0
0
0
0
0
0
0
110
0
0
0
0
0
0
0
0
111
0
0
0
0
0
0
1
1
AND function
F|xyz> = |xy(x
y)>
Coin Tossing
F|x> = 1/2|0>+1/2|1>
0
1
0
1/2
1/2
1
1/2
1/2
Constant zero function
F|x> = |0>
0
1
0
1
1
1
0
0
For example, if we apply the coin tossing operation to the second bit of the register, then
this means that for every z = z1 . . . zm ∈{0, 1}m, the vector |z ⟩is mapped to 1/2 |z10z3 . . . zm ⟩+
1/2 |z11z3 . . . zm ⟩.
We deﬁne a probabilistic computation to be a sequence of such elementary operations applied
one after the other (see Deﬁnition 20.6 below). We will later see this corresponds exactly to our
previous deﬁnition of probabilistic computation as in the class BPP deﬁned in Chapter 7.
Definition 20.6 (Probabilistic Computation)
Let f : {0, 1}∗→{0, 1}∗and T : N →N be some functions. We say that f is computable in
probabilistic T(n)-time if for every n ∈N and x ∈{0, 1}n, f(x) can be computed by the following
process:
1. Initialize an m bit register to the state |x0n−m ⟩(i.e., x padded with zeroes), where m ≤T(n).
2. Apply one after the other T(n) elementary operations F1, . . . , FT to the register (where we
require that there is a polynomial-time TM that on input 1n, 1T(n) outputs the descriptions
of F1, . . . , FT ).
3. Measure the register and let Y denote the obtained value. (That is, if v is the ﬁnal state of
the register, then Y is a random variable that takes the value y with probability vy for every
y ∈{0, 1}n.)
Web draft 2007-01-08 21:59

DRAFT
20.2. A NEW VIEW OF PROBABILISTIC COMPUTATION.
p20.7 (407)
Denoting ℓ= |f(x)|, we require that the ﬁrst ℓbits of Y are equal to f(x) with probability at
least 2/3.
Probabilistic computation: summary of notations.
The state of an m-bit register is represented by a vector v ∈R2m such that
the register takes the value x with probability vx.
An operation on the register is a function F : R2m →R2m that is linear and
stochastic.
An elementary operation only reads and modiﬁes at most three bits of the
register.
A computation of a function f on input x ∈{0, 1}n involves initializing the
register to the state |x0m−n ⟩, applying a sequence of elementary operations
to it, and then measuring its value.
Now, as promised, we show that our new notion of probabilistic computation is equivalent to
the one encountered in Chapter 7.
Theorem 20.7
A Boolean function f : {0, 1}∗→{0, 1} is in BPP iﬀit is computable in probabilistic p(n)-time
for some polynomial p : N →N.
Proof: (⇒) Suppose that f ∈BPP. As we saw before (e.g., in the proof of Theorem 6.7) this
means that f can be computed by a polynomial-sized Boolean circuit C (that can be found by
a deterministic poly-time TM) if we allow the circuit C access to random coins. Thus we can
compute f as follows: we will use a register of n+r +s bits, where r is the number of random coins
C uses, and s is the number of gates C uses. That is, we have a location in the register for every
coin and every gate of C. The elementary coin tossing operation (see Example 20.5) can transform
a location initialized to 0 into a random coin. In addition, we have an elementary operation that
transforms three bits x, y and z into x, y, x ∧y and can similarly deﬁne elementary operations for
the OR and NOT functions. Thus, we can use these operations to ensure that for every gate of C,
the corresponding location in the register contains the result of applying this gate when the circuit
is evaluated on input x.
(⇐) We will show a probabilistic polynomial-time algorithm to execute an elementary operation
on a register. To simulate a p(n)-time probabilistic computation we can execute this algorithm
p(n) times one after the other. For concreteness, suppose that we need to execute an operation on
the ﬁrst three bits of the register, that is speciﬁed by an 8 × 8 matrix A. The algorithm will read
the three bits to obtain the value z ∈{0, 1}3, and then write to them a value chosen according to
the distribution speciﬁed in the zth column of A.
The only issue remaining is how to pick a value from an arbitrary distribution (p1, . . . , p8) over
{0, 1}3 (which we identify with the set [8]). One case is simple: suppose that for every i ∈[8],
pi = Ki/2ℓwhere ℓis polynomial in ℓand K1, . . . , K8 ∈[2ℓ]. In this case, the algorithm will
choose using ℓrandom coins a number X between 1 and 2ℓand output the largest i such that
X ≤Pi
j=1 Ki.
However, this essentially captures general case as well: every number p ∈[0, 1] can be ap-
proximated by a number of the form K/2ℓwithin 2−ℓaccuracy. This means that every general
Web draft 2007-01-08 21:59

DRAFT
p20.8 (408)
20.3. QUANTUM SUPERPOSITION AND THE CLASS BQP
Note 20.8 (A few notions from linear algebra)
We use in this chapter several elementary facts and notations involving the
space CM. These are reviewed in Appendix A, but here is a quick reminder:
• If z = a + ib is a complex number (where i = √−1), then z = a −ib
denotes the complex conjugate of z. Note that zz = a2 + b2 = |z|2.
• The inner product of two vectors u, v ∈Cm, denoted by ⟨u, v⟩, is equal
to P
x∈[M] uxvx.
• The norm of a vector u, denoted by ∥u∥2, is equal to
p
⟨u, u⟩=
qP
x∈[M] |ux|2.
• If ⟨u, v⟩= 0 we say that u and v are orthogonal.
More generally,
⊙u, v = cos θ∥u∥2∥v∥2, where θ is the angle between the two vectors
u and v.
• If A is an M × M matrix, then A† denotes the conjugate transpose of
A. That is, A†
x,y = Ay,x for every x, y ∈[M].
• An M × M matrix A is unitary if AA† = I, where I is the M × M
identity matrix.
Note that if z is a real number (i.e., z has no imaginary component) then
z = z. Hence, if all vectors and matrices involved are real then the inner
product is equal to the standard inner product of Rn and the conjugate
transpose operation is equal to the standard transpose operation. Also a
real matrix is unitary if and only if it is symmetric.
distribution can be well approximated by a distribution over the form above, and so by choosing a
good enough approximation, we can simulate the probabilistic computation by a BPP algorithm.
■
20.3
Quantum superposition and the class BQP
A quantum register is also composed of m bits, but in quantum parlance they are called “qubits”.
In principle such a register can be implemented by any collection of m physical systems that can
have an ON and OFF states, although in practice there are signiﬁcant challenges for such an
implementation (see Note 20.1). According to quantum mechanics, the state of such a register can
be described by a 2m-dimensional vector that, unlike the probabilistic case, can actually contain
negative and even complex numbers. That is, the state of the register is described by a vector
v ∈C2m. Once again, we denote v = P
x∈{0,1}n vx |x⟩(where again |x⟩is the column vector that
has all zeroes and a single one in the xth coordinate). However, according to quantum mechanics,
Web draft 2007-01-08 21:59

DRAFT
20.3. QUANTUM SUPERPOSITION AND THE CLASS BQP
p20.9 (409)
when the register is measured, the probability that we see the value x is given not by vx but by
|vx|2. This means that v has to be a unit vector, satisfying P
x |vx|2 = 1 (see Note 20.8).
Example 20.9
The following are two legitimate state vectors for a two-qubit quantum register:
1
√
2 |0⟩+
1
√
2 |1⟩
and
1
√
2 |0⟩−
1
√
2 |1⟩. Even though in both cases, if the register is measured it will contain either 0
or 1 with probability 1/2, these are considered distinct states and we will see that it is possible to
diﬀerentiate between them using quantum operations.
Because states are always unit vectors, we often drop the normalization factor and so, say, use
|0⟩−|1⟩to denote the state
1
√
2 |0⟩−
1
√
2 |1⟩.
We call the state where all coeﬃcients are equal the uniform state. For example, the uniform
state for a 4-qubit register is
|00⟩+ |01⟩+ |10⟩+ |11⟩,
(where we dropped the normalization factor of 1
2.) We will also use the notation |x⟩|y⟩to denote
the standard basis vector |xy⟩. It is easily veriﬁed that this operation respects the distributive law,
and so we can also write the uniform state of a 4-qubit register as
(|0⟩+ |1⟩) (|0⟩+ |1⟩)
Once again, we can view an operation applied to the register as a function F that maps its
previous state to the new state. That is, F is a function from C2m to C2m. According to quantum
mechanics, such an operation must satisfy the following conditions:
1. F is a linear function. That is, for every v ∈C2n, F(v) = P
x vxF(|x⟩).
2. F maps unit vectors to unit vectors. That is, for every v with ∥v∥2 = 1, ∥F(v)∥2 = 1.
Together, these two conditions imply that F can be described by a 2m × 2m unitary matrix.
That is, a matrix A satisfying AA† = I (see Note 20.8). We recall the following simple facts about
unitary matrices (left as Exercise 1):
Claim 20.11
For every M × M complex matrix A, the following conditions are equivalent:
1. A is unitary (i.e., AA† = I).
2. For every vector v ∈CM, ∥Av∥2 = ∥v∥2.
3. For every orthonormal basis

vi	
i∈[M] of CM (see below), the set

Avi	
i∈[M] is an orthonor-
mal basis of CM.
4. The columns of A form an orthonormal basis of CM.
Web draft 2007-01-08 21:59

DRAFT
p20.10 (410)
20.3. QUANTUM SUPERPOSITION AND THE CLASS BQP
Note 20.10 (The geometry of quantum states)
It is often helpful to think of quantum states geometrically as vectors in
space. For example, consider a single qubit register, in which case the state is
a unit vector in the two-dimensional plane spanned by the orthogonal vectors
|0⟩and |1⟩. For example, the state v = cos θ |0⟩+ sin θ |1⟩corresponds to a
vector making an angle θ with the |0⟩vector and an angle π/2 −θ with the
|1⟩vector. When v is measured it will yield 0 with probability cos2 θ and 1
with probability sin2 θ.
|1>
|0>
θ
cos θ
sin θ
v = cos θ |0> + sin θ |1>
Although it’s harder to visualize states with complex coeﬃcients or more
than one qubit, geometric intuition can still be useful when reasoning about
such states.
Web draft 2007-01-08 21:59

DRAFT
20.3. QUANTUM SUPERPOSITION AND THE CLASS BQP
p20.11 (411)
5. The rows of A form an orthonormal basis of CM.
(Recall that a set

vi	
i∈[M] of vectors in CM is an orthonormal basis of CM if for every i, j ∈[M],
⟨vi, vj⟩is equal to 1 if i = j and equal to 0 if i ̸= j, where ⟨v, u⟩is the standard inner product
over CM. That is, ⟨v, u⟩= PM
j=1 viui.)
As before, we deﬁne an elementary quantum operation to be an operation that only depends
and modiﬁes at most three qubits of the register.
Example 20.12
Here are some examples for quantum operations depending on at most three qubits. (Because all
the quantum operations are linear, it suﬃces to describe their behavior on any linear basis for the
space C2m and so we often specify quantum operations by the way they map the standard basis.)
• The standard NOT operation on a single bit can be thought of as the unitary operation that
maps |0⟩to |1⟩and vice versa.
• The Hadamard operation is the single qubit operation that (up to normalization) maps |0⟩
to |0⟩+ |1⟩and |1⟩to |0⟩−|1⟩. (More succinctly, the state |b⟩is mapped to |0⟩+ (−1)b |1⟩.)
It turns out to be a very useful operation in many algorithms for quantum computers. Note
that if we apply an Hadamard operation to every qubit of an n-qubit register, then for every
x ∈{0, 1}n, the state |x⟩is mapped to
(|0⟩+ (−1)x1 |1⟩)(|0⟩+ (−1)k2 |1⟩) · · · (|0⟩+ (−1)xn |1⟩) =
X
y∈{0,1}n
(πi : yi=1(−1)x
i ) |y⟩=
X
y∈{0,1}n
−1x⊙y |y⟩,
where x ⊙y denotes the inner product modulo 2 of x and y. That is, x ⊙y = Pn
i=1 xiyi
(mod 2).1
• Since we can think of the state of a single qubit register as a vector in two dimensional space,
a natural operation is for any angle θ, to rotate the single qubit by θ. That is, map |0⟩to
cos θ |0⟩+ sin θ |1⟩, and map |1⟩to −sin θ |0⟩+ cos θ |1⟩. Note that rotation by an angle of π
(i.e., 180°) is equal to ﬂipping the sign of the vector (i.e., the map v 7→−v).
• One simple two qubit operation is exchanging the two bits with one another. That is, mapping
|01⟩7→|10⟩and |10⟩7→|01⟩, with |00⟩and |11⟩being mapped to them selves. Note that by
combining these operations we can reorder the qubits of an n-bit register in any way we see
ﬁt.
• Another two qubit operation is the controlled-NOT operation: it performs a NOT on the ﬁrst
bit iﬀthe second bit is equal to 1. That is, it maps |01⟩7→|11⟩and |11⟩7→|01⟩, with |10⟩
and |11⟩being mapped to themselves.
1Note the similarity to the deﬁnition of the Walsh-Hadamard code described in Chapter 17, Section 17.5.
Web draft 2007-01-08 21:59

DRAFT
p20.12 (412)
20.3. QUANTUM SUPERPOSITION AND THE CLASS BQP
• The Tofolli operation is the three qubit operation that can be called a “controlled-controlled-
NOT”: it performs a NOT on the ﬁrst bit iﬀboth the second and third bits are equal to 1.
That is, it maps |011⟩to |111⟩and vice versa, and maps all other basis states to themselves.
Exercise 2 asks you to write down explicitly the matrices for these operations.
We deﬁne quantum computation as consisting of a sequence of elementary operations in an
analogous way to our previous deﬁnition of probabilistic computation (Deﬁnition 20.6):
Definition 20.13 (Quantum Computation)
Let f : {0, 1}∗→{0, 1}∗and T : N →N be some functions. We say that f is
computable in quantum T(n)-time if for every n ∈N and x ∈{0, 1}n, f(x) can be
computed by the following process:
1. Initialize an m qubit quantum register to the state |x0n−m ⟩(i.e., x padded
with zeroes), where m ≤T(n).
2. Apply one after the other T(n) elementary quantum operations F1, . . . , FT to
the register (where we require that there is a polynomial-time TM that on
input 1n, 1T(n) outputs the descriptions of F1, . . . , FT ).
3. Measure the register and let Y denote the obtained value. (That is, if v is the
ﬁnal state of the register, then Y is a random variable that takes the value y
with probability |vy|2 for every y ∈{0, 1}n.)
Denoting ℓ= |f(x)|, we require that the ﬁrst ℓbits of Y are equal to f(x) with
probability at least 2/3.
The following class aims to capture the decision problems with eﬃcient algorithms on quantum
computers:
Definition 20.14 (The class BQP)
A Boolean function f : {0, 1}∗→{0, 1} is in BQP if there is some polynomial p : N →N such that
f is computable in quantum p(n)-time.
Web draft 2007-01-08 21:59

DRAFT
20.3. QUANTUM SUPERPOSITION AND THE CLASS BQP
p20.13 (413)
Quantum computation: summary of notations.
The state of an m-qubit register is represented by a unit vector v ∈C2m
such that the register takes the value x with probability |vx|2.
An operation on the register is a function F : C2m →C2m that is unitary.
(i.e., linear and norm preserving).
An elementary operation only depends and modiﬁes at most three bits of
the register.
A computation of a function f on input x ∈{0, 1}n involves initializing the
register to the state |x0m−n ⟩, applying a sequence of elementary operations
to it, and then measuring its value.
Remark 20.15
Readers familiar with quantum mechanics or quantum computing may notice that we did not allow
in our deﬁnition of quantum computation several features that are allowed by quantum mechanics.
These include mixed states, that involve both quantum superposition and probability, measuring
in diﬀerent basis than the standard basis, and performing partial measurements during the com-
putation. However, none of these features adds to the computing power of quantum computers.
20.3.1
Universal quantum operations
Can we actually implement quantum computation? This is an excellent question, and no one really
knows.
However, one hurdle can be overcome: even though there is an inﬁnite set of possible
elementary operations, all of them can be generated (or at least suﬃciently well approximated) by
the Hadamard and Tofolli operations described in Example 20.12. In fact, every operation that
depends on k qubits can be approximated by composing 2O(k) of these four operations (times an
additional small factor depending on the approximation’s quality). Using a counting/dimension
argument, it can be shown that some unitary transformations do indeed require an exponential
number of elementary operations to compute (or even approximate).
One useful consequence of universality is the following: when designing quantum algorithms we
can assume that we have at our disposal the all operations that depend on k qubits as elementary
operations, for every constant k (even if k > 3). This is since these can be implemented by 3 qubit
elementary operations incurring only a constant (depending on k) overhead.
20.3.2
Spooky coordination and Bell’s state
To get our ﬁrst glimpse of how things behave diﬀerently in the quantum world, we will now show
how quantum registers and operations help us win the game described in Section 20.1.2 with higher
probability than can be achieved according to pre-quantum “classical” physics.
Recall that the game was the following:
1. The veriﬁer chooses random x, y ∈R {0, 1} and sends x to Alice and y to Bob, collecting their
respective answers a and b.
2. It accepts iﬀx ∧y = a ⊕b. In other words, it accepts if either ⟨x, y⟩̸= ⟨1, 1⟩and a = b or
x = y = 1 and a ̸= b.
Web draft 2007-01-08 21:59

DRAFT
p20.14 (414)
20.3. QUANTUM SUPERPOSITION AND THE CLASS BQP
It was shown in Section 20.1.2 that if Alice and Bob can not coordinate their actions then the
veriﬁer will accept with probability at most 3/4, but quantum eﬀects can allow them to bypass this
bound as follows:
1. Alice and Bob prepare a 2-qubit quantum register containing the EPR state |00⟩+ |11⟩.
(They can start with a register initialized to |00⟩and then apply an elementary operation
that maps the initial state to the EPR state.)
2. Alice and Bob split the register - Alice takes the ﬁrst qubit and Bob takes the second qubit.
(The components containing each qubit of a quantum register do not necessarily need to be
adjacent to one another.)
3. Alice receives the qubit x from the veriﬁer, and if x = 1 then she applies a rotation by π/8
(22.5°) operation to her qubit. (Since the operation involves only her qubit, she can perform
it even after the register was split.)
4. Bob receives the qubit y from the veriﬁer, and if y = 1 he applies a rotation by by −π/8
(−22.5°) operation to his qubit.
5. Both of them measure their respective qubits and output the values obtained as their answers
a and b.
Note that the order in which Alice and Bob perform their rotations and measurements does not
matter - it can be shown that all orders yield exactly the same distribution (e.g., see Exercise 3).
While splitting a quantum register and applying unitary transformations to the diﬀerent parts
may sound far fetched, this experiment had been performed several times in practice, verifying the
following predictions of quantum mechanics:
Theorem 20.16
Given the above strategy for Alice and Bob, the veriﬁer will accept with probability at least 0.8.
Proof: Recall that Alice and Bob win the game if they output the same answer when ⟨x, y⟩̸=
⟨1, 1⟩and a diﬀerent answer otherwise. The intuition behind the proof is that in the case that
⟨x, y⟩̸= ⟨1, 1⟩then the states of the two qubits will be “close” to one another (the angle between
them is less than π/8 or 22.5°) and in the other case the states will be “far” (having angle π/4 or
45°). Speciﬁcally we will show that:
(1) If x = y = 0 then a = b with probability 1.
(2) If x ̸= y then a = b with probability cos2(π/8) ≥0.85
(3) If x = y = 1 then a = b with probability 1/2.
Implying that the overall acceptance probability is at least 1
4 + 1
20.85 + 1
8 = 0.8.
In the case (1) both Alice and Bob perform no operation on their register, and so when measured
it will be either in the state |00⟩or |11⟩, both resulting in Alice and Bob’s outputs being equal. To
analyze case (2), it suﬃces to consider the case that x = 0, y = 1 (the other case is symmetrical).
Web draft 2007-01-08 21:59

DRAFT
20.4. QUANTUM PROGRAMMER’S TOOLKIT
p20.15 (415)
In this case Alice applies no transformation to her qubit, and Bob rotates his qubit in a −π/8 angle.
Imagine that Bob ﬁrst makes the rotation, then Alice measures her qubit and then Bob measures
his (this is OK as the order of measurements does not change the outcome). With probability 1/2,
Alice will get the value 0 and Bob’s qubit will collapse to the state |0⟩rotated by a −π/8 angle,
meaning that when measuring Bob will obtain the value 0 with probability cos2(π/8). Similarly, if
Alice gets the value 1 then Bob will also output 1 with cos2(π/8) probability.
To analyze case (3), we just use direct computation. In this case, after both rotations are
performed, the register’s state is
(cos(π/8) |0⟩+ sin(π/8) |1⟩) (cos(π/8) |0⟩−sin(π/8) |1⟩) +
(−sin(π/8) |0⟩+ cos(π/8) |1⟩) (sin(π/8) |0⟩+ cos(π/8) |1⟩) =
 cos2(π/8) −sin2(π/8)

|00⟩−2 sin(π/8) cos(π/8) |01⟩+
2 sin(π/8) cos(π/8) |10⟩+
 cos2(π/8) −sin2(π/8)

|11⟩.
But since
cos2(π/8) −sin2(π/8) = cos(π/4) =
1
√
2 = sin(π/4) = 2 sin(π/8) cos(π/8) ,
all coeﬃcients in this state have the same absolute value and hence when measured the register
will yield either one of the four values 00, 01, 10 and 11 with equal probability 1/4. ■
20.4
Quantum programmer’s toolkit
Quantum algorithms have some peculiar features that classical algorithm designers are not used to.
The following observations can serve as a helpful “bag of tricks” for designing quantum algorithms:
• If we can compute an n-qubit unitary transformation U in T steps then we can compute the
transformation Controlled-U in O(T) steps, where Controlled-U maps a vector |x1 . . . xnxn+1 ⟩
to |U(x1 . . . xn)xn+1 ⟩if xn+1 = 1 and to itself otherwise.
The reason is that we can transform every elementary operation F in the computation of U
to the analogous “Controlled-F” operation. Since the “Controlled-F” operation depends on
at most 4 qubits, it can be considered also as elementary.
For every two n-qubit transformations U, U′, we can use this observation twice to compute
the transformation that invokes U on x1 . . . xn if xn+1 = 1 and invokes U′ otherwise.
• Every permutation of the standard basis is unitary. That is, any operation that maps a vector
|x⟩into |π(x)⟩where π is a permutation of {0, 1}n is unitary. Of course, this does not mean
that all such permutations are eﬃciently computable in quantum polynomial time.
• For every function f : {0, 1}n →{0, 1}ℓ, the function x, y 7→x, (y ⊕f(x)) is a permutation
on {0, 1}n+ℓ(in fact, this function is its own inverse).
In particular, this means that we
can use as elementary operations the following “permutation variants” of AND, OR and
Web draft 2007-01-08 21:59

DRAFT
p20.16 (416)
20.5. GROVER’S SEARCH ALGORITHM.
copying: (1) |x1x2x3 ⟩7→|x1x2(x3 ⊕(x1∧x2))⟩, (2) |x1x2x3 ⟩7→|x1x2(x3 ⊕(x1∨x2))⟩, and
(3) |x1x2 ⟩7→|x1(x1 ⊕x2)⟩. Note that in all these cases we compute the “right” function if
the last qubit is initially equal to 0.
• If f : {0, 1}n →{0, 1}ℓis a function computable by a size-T Boolean circuit, then the
following transformation can be computed by a sequence of O(T) elementary operations: for
every x ∈{0, 1}m , y ∈{0, 1}ℓ, z ∈{0, 1}T
|x, y, z ⟩7→
x, (y ⊕f(x)), 0T ⟩if z = 0T .
(We don’t care on what the mapping does for z ̸= 0T .)
The reason is that by transforming every AND, OR or NOT gate into the corresponding
elementary permutation we can ensure that the ith qubit of z contains the result of the ith
gate of the circuit when executed on input x. We can then XOR the result of the circuit into
y using ℓelementary operations and run the entire computation backward to return the state
of z to 0T . .
• We can assume that we are allowed to make a partial measurement in the course of the
algorithm, and then proceed diﬀerently according to its outcome. That is, we can measure a
some of the qubits of the register. Note that if the register is at state v and we measure its ith
qubit then with probability P
z:zi=1 |vz|2 we will get the answer “1” and the register’s state
will change to (the normalized version of) the vector P
z:zi=1 vz |z ⟩. Symmetrically, with
probability P
z:zi=0 |vz|2 we will get the answer “0” and the new state will be P
z:zi=0 vz |z ⟩.
This is allowed since an algorithm using partial measurement can be replaced with an algo-
rithm not using it with at most a constant overhead (see Exercise 4).
• Since the 1-qubit Hadamard operation maps |0⟩to the uniform state |0⟩+|1⟩, it can be used
to simulate tossing a coin: we simply take a qubit in our workspace that is initialized to 0,
apply Hadamard to it, and measure the result.
Together, the last three observations imply that quantum computation is at least as powerful
as “classical” non-quantum computation:
Theorem 20.17
BPP ⊆BQP.
20.5
Grover’s search algorithm.
Consider the NP-complete problem SAT of ﬁnding, given an n-variable Boolean formula ϕ, whether
there exists an assignment a ∈{0, 1}n such that ϕ(a) = 1.
Using “classical” deterministic or
probabilistic TM’s, we do not know how to solve this problem better than the trivial poly(n)2n-time
algorithm.2 We now show a beautiful algorithm due to Grover that solves SAT in poly(n)2n/2-time
on a quantum computer. This is a signiﬁcant improvement over the classical case, even if it falls
2There are slightly better algorithms for special cases such as 3SAT.
Web draft 2007-01-08 21:59

DRAFT
20.5. GROVER’S SEARCH ALGORITHM.
p20.17 (417)
way short of showing that NP ⊆BQP. In fact, Grover’s algorithm solves an even more general
problem: for every polynomial-time computable function f : {0, 1}n →{0, 1} (even if f is not
expressed as a small Boolean formula3), it ﬁnds in poly(n)2n/2 time a string a such that f(a) = 1
(if such a string exists).
Grover’s algorithm is best described geometrically. We assume that the function f has a single
satisfying assignment a. (The techniques described in Chapter 9, Section 9.3.1 allow us to reduce
the general problem to this case.) Consider an n-qubit register, and let u denote the uniform state
vector of this register. That is, u =
1
2n/2
P
x∈{0,1}n |x⟩. The angle between u and |a⟩is equal to the
inverse cosine of their inner product ⟨u, |a⟩⟩=
1
2n/2 . Since this is a positive number, this angle is
smaller than π/2 (90°), and hence we denote it by π/2 −θ, where sin θ =
1
2n/2 and hence, assuming
n is suﬃciently large, θ ≥
1
2·2n/2 (since for small θ, sin θ ∼θ).
|a>
u
θ~2-n/2
2θ
w
Figure 20.1: Grover’s algorithm ﬁnds the string a such that f(a) = 1 as follows. It starts with the uniform vector
u whose angle with |a⟩is π/2 −θ for θ ∼2−n/2 and at each step transforms the state of the register into a vector
that is 2θ radians closer to |a⟩. After O(1/θ) steps, the state is close enough so that measuring the register yields
|a⟩with good probability.
The algorithm starts with the state u, and at each step it gets nearer the state |a⟩by trans-
forming its current state to a state whose angle with |a⟩is smaller by 2θ (see Figure 20.1). Thus,
in O(1/θ) = O(2n/2) steps it will get to a state v whose inner product with |a⟩is larger than, say,
1/2, implying that a measurement of the register will yield a with probability at least 1/4.
The main idea is that to rotate a vector w towards the unknown vector |a⟩by an angle of θ,
it suﬃces to take two reﬂections around the vector u and the vector e = P
x̸=a |a⟩(the latter is
the vector orthogonal to |a⟩on the plane spanned by u and |a⟩). See Figure 20.2 for a “proof by
picture”.
To complete the algorithm’s description, we need to show how we can perform the reﬂections
around the vectors u and e. That is, we need to show how we can in polynomial time transform
a state w of the register into the state that is w’s reﬂection around u (respectively, e). In fact,
we will not work with an n-qubit register but with an m-qubit register for m that is polynomial in
n. However, the extra qubits will only serve as “scratch workspace” and will always contain zero
except during intermediate computations, and hence can be safely ignored.
3We may assume that f is given to the algorithm in the form of a polynomial-sized circuit.
Web draft 2007-01-08 21:59

DRAFT
p20.18 (418)
20.5. GROVER’S SEARCH ALGORITHM.
|a>
u
θ~2-n/2
w
α
e
α+θ
w’
|a>
u
θ~2-n/2
e
θ+α
w’
α+2θ
Step 1: Reflect around e
Step 2: Reflect around u
w’’
Figure 20.2: We transform a vector w in the plane spanned by |a⟩and u into a vector w′′ that is 2θ radians close
to |a⟩by performing two reﬂections. First, we reﬂect around e = P
x̸=a |x⟩(the vector orthogonal to |a⟩on this
plane), and then we reﬂect around u. If the original angle between w and |a⟩was π/2 −θ −α then the new angle
will be π/2 −θ −α −2θ.
Reﬂecting around e.
Recall that to reﬂect a vector w around a vector v, we express w as
αv + v⊥(where v⊥is orthogonal to v) and output αv −v⊥. Thus the reﬂection of w around e is
equal to P
x̸=a wx |x⟩−wa |a⟩. Yet, it is easy to perform this transformation:
1. Since f is computable in polynomial time, we can compute the transformation |xσ⟩7→
|x(σ ⊕f(x))⟩in polynomial (this notation ignores the extra workspace that may be needed,
but this won’t make any diﬀerence). This transformation maps |x0⟩to |x0⟩for x ̸= a and
|a0⟩to |a1⟩.
2. Then, we apply the elementary transformation that multiplies the vector by −1 if σ = 1, and
does nothing otherwise. This maps |x0⟩to |x0⟩for x ̸= a and maps |a1⟩to −|a1⟩.
3. Then, we apply the transformation |xσ⟩7→|x(σ ⊕f(x))⟩again, mapping |x0⟩to |x0⟩for
x ̸= a and maps |a1⟩to |a0⟩.
The ﬁnal result is that the vector |x0⟩is mapped to itself for x ̸= a, but |a0⟩is mapped to
−|a0⟩. Ignoring the last qubit, this is exactly a reﬂection around |a⟩.
Reﬂecting around u.
To reﬂect around u, we ﬁrst apply the Hadamard operation to each qubit,
mapping u to |0⟩. Then, we reﬂect around |0⟩(this can be done in the same way as reﬂecting
around |a⟩, just using the function g : {0, 1}n →{0, 1} that outputs 1 iﬀits input is all zeroes
instead of f). Then, we apply the Hadamard operation again, mapping |0⟩back to u.
Together these operations allow us to take a vector in the plane spanned by |a⟩and u and rotate
it 2θ radians closer to |a⟩. Thus if we start with the vector u, we will only need to repeat them
Web draft 2007-01-08 21:59

DRAFT
20.5. GROVER’S SEARCH ALGORITHM.
p20.19 (419)
O(1/θ) = O(2n/2) to obtain a vector that, when measured, yields |a⟩with constant probability.
For the sake of completeness, Figure 20.3 contains the full description of Grover’s algorithm. ■
Web draft 2007-01-08 21:59

DRAFT
p20.20 (420)
20.5. GROVER’S SEARCH ALGORITHM.
Grover’s Search Algorithm.
Goal: Given a polynomial-time computable f : {0, 1}n →{0, 1} with a unique a ∈{0, 1}n
such that f(a) = 1, ﬁnd a.
Quantum register: We use an n + 1 + m-qubit register, where m is large enough so we
can compute the transformation |xσ0m ⟩7→|x(σ ⊕f(x))0m ⟩.
Operation
State (neglecting normalizing factors)
Initial state:
0n+m+1 ⟩
Apply Hadamard operation to ﬁrst n qubits.
u
0m+1 ⟩(where u denotes P
x∈{0,1}n |x⟩)
For i = 1, . . . , 2n/2 do:
vi 0m+1 ⟩
We let v1 = u and maintain the invariant
that ⟨vi, |a⟩⟩= sin(iθ), where θ ∼2−n/2 is
such that ⟨u, |a⟩⟩= sin(θ)
Step 1: Reﬂect around e = P
x̸=a |x⟩:
1.1 Compute
xσ0m+1 ⟩7→
x(σ ⊕f(x))0m+1 ⟩
P
x̸=a vi
x |x⟩
0m+1 ⟩+ vi
a |a⟩
10m+1 ⟩
1.2 If σ = 1 then multiply vector by −1, otherwise
do not do anything.
P
x̸=a vi
x |x⟩
0m+1 ⟩−vi
a |a⟩
10m+1 ⟩
1.3 Compute
xσ0m+1 ⟩7→
x(σ ⊕f(x))0m+1 ⟩.
wi 0m+1 ⟩
=
P
x̸=a vi
x |x⟩
0m+1 ⟩−
vi
a |a⟩|00m ⟩.
(wi is vi reﬂected around
P
x̸=a |x⟩.)
Step 2: Reﬂect around u:
2.1 Apply Hadamard operation to ﬁrst n qubits.
⟨wi, u⟩|0n ⟩
0m+1 ⟩+ P
x̸=0n αx |x⟩
0m+1 ⟩,
for some coeﬃcients αx’s (given by αx =
P
z(−1)x⊙zwi
z |z ⟩).
2.2 Reﬂect around |0⟩:
2.2.1 If ﬁrst n-qubits are all zero then ﬂip n + 1st
qubit.
⟨wi, u⟩|0n ⟩|10m ⟩+ P
x̸=0n αx |x⟩
0m+1 ⟩
2.2.2 If n + 1st qubit is 1 then multiply by −1
−⟨wi, u⟩|0n ⟩|10m ⟩+ P
x̸=0n αx |x⟩
0m+1 ⟩
2.2.3 If ﬁrst n-qubits are all zero then ﬂip n + 1st
qubit.
−⟨wi, u⟩|0n ⟩
0m+1 ⟩+P
x̸=0n αx |x⟩
0m+1 ⟩
2.3 Apply Hadamard operation to ﬁrst n qubits.
vi+1 0m+1 ⟩(where vi+1 is wi reﬂected
around u)
Measure register and let a′ be the obtained value in
the ﬁrst n qubits. If f(a′) = 1 then output a′. Oth-
erwise, repeat.
Figure 20.3: Grover’s Search Algorithm
Web draft 2007-01-08 21:59

DRAFT
20.6. SIMON’S ALGORITHM
p20.21 (421)
20.6
Simon’s Algorithm
Although beautiful, Grover’s algorithm still has a signiﬁcant drawback: it is merely quadratically
faster than the best known classical algorithm for the same problem. In contrast, in this section we
show Simon’s algorithm that is a polynomial-time quantum algorithm solving a problem for which
the best known classical algorithm takes exponential time.
Simon’s algorithm solves the following problem: given a polynomial-time computable function
f : {0, 1}n →{0, 1}n such that there exists a ∈{0, 1}∗satisfying f(x) = f(y) iﬀx = y ⊕a for every
x, y ∈{0, 1}n, ﬁnd this string a.
Two natural questions are (1) why is this problem interesting? and (2) why do we believe it is
hard to solve for classical computers? The best answer to (1) is that, as we will see in Section 20.7, a
generalization of Simon’s problem turns out to be crucial in the quantum polynomial-time algorithm
for famous integer factorization problem. Regarding (2), of course we do not know for certain that
this problem does not have a classical polynomial-time algorithm (in particular, if P = NP then
there obviously exists such an algorithm). However, some intuition why it may be hard can be
gleaned from the following black box model: suppose that you are given access to a black box (or
oracle) that on input x ∈{0, 1}n, returns the value f(x). Would you be able to learn a by making
at most a subexponential number of queries to the black box? It is not hard to see that if a is chosen
at random from {0, 1}n and f is chosen at random subject to the condition that f(x) = f(y) iﬀ
x = y⊕a then no algorithm can successfully recover a with reasonable probability using signiﬁcantly
less than 2n/2 queries to the black box. Indeed, an algorithm using fewer queries is very likely to
never get the same answer to two distinct queries, in which case it gets no information about the
value of a.
20.6.1
The algorithm
Simon’s algorithm is actually quite simple. It uses a register of 2n + m qubits, where m is the
number of workspace bits needed to compute f. (Below we will ignore the last m qubits of the
register, since they will be always set to all zeroes except in intermediate steps of f’s computation.)
The algorithm ﬁrst uses n Hadamard operations to set the ﬁrst n qubits to the uniform state and
then apply the operation |xz ⟩7→|x(z ⊕f(x)⟩to the register, resulting (up to normalization) in
the state
X
x∈{0,1}n
|x⟩|f(x)⟩=
X
x∈{0,1}n
(|x⟩+ |x ⊕a⟩) |f(x)⟩.
(1)
We then measure the second n bits of the register, collapsing its state to
|xf(x)⟩+ |(x ⊕a)f(x)⟩
(2)
for some string x (that is chosen uniformly from {0, 1}n). You might think that we’re done as the
state (2) clearly encodes a, however we cannot directly learn a from this state: if we measure the
ﬁrst n bits we will get with probability 1/2 the value x and with probability 1/2 the value x⊕a. Even
though a can be deduced from these two values combined, each one of them on its own yields no
information about a. (This point is well worth some contemplation, as it underlies the subtleties
Web draft 2007-01-08 21:59

DRAFT
p20.22 (422)
20.7. SHOR’S ALGORITHM: INTEGER FACTORIZATION USING QUANTUM
COMPUTERS.
involved in quantum computation and demonstrates why a quantum algorithm is not generally
equivalent to performing exponentially many classical computation in parallel.)
However, consider now what happens if we perform another n Hadamard operations on the ﬁrst
n bits. Since this maps x to the vector P
y(−1)x⊙y |y⟩, the new state of the ﬁrst n bits will be
X
y

(−1)x⊙y + (−1)(x⊕a)⊙y
|y⟩=
X
y
 (−1)x⊙y + (−1)x⊙y(−1)a⊙y
|y⟩.
(3)
For every y ∈{0, 1}m, the yth coeﬃcient in the state (3) is nonzero if and only if if and only if
a ⊙y = 0, and in fact if measured, the state (3) yields a uniform y ∈{0, 1}n satisfying a ⊙y = 0.
Repeating the entire process k times, we get k uniform strings y1, . . . , yk satisfying y ⊙a = 0
or in other words, k linear equations (over the ﬁeld GF(2)) on the variables a1, . . . , an. It can be
easily shown that if, say, k ≥2n then with high probability there will be n−1 linearly independent
equations among these (see Exercise 5), and hence we will be able to retrieve a from these equations
using Gaussian elimination. For completeness, a full description of Simon’s algorithm can be found
in Figure 20.4.
Simon’s Algorithm.
Goal: Given a polynomial-time computable f : {0, 1}n →{0, 1}n such that there is some
a ∈{0, 1}n satisfying f(x) = f(y) iﬀy = x ⊕a for every x, y ∈{0, 1}n, ﬁnd a.
Quantum register: We use an 2n + m-qubit register, where m is large enough so we
can compute the transformation |xz0m ⟩7→|x(z ⊕f(x))0m ⟩. (Below we ignore the last m
qubits of the register as they will always contain 0m except in intermediate computations
of f.)
Operation
State (neglecting normalizing factors)
Initial state:
02n ⟩
Apply Hadamard operation to ﬁrst n qubits.
P
x |x0n ⟩
Compute |xz ⟩7→|x(y ⊕f(x))⟩
P
x |xf(x)⟩= P
x (|x⟩+ |x ⊕a⟩) |f(x)⟩
Measure second n bits of register.
(|x⟩+ |x ⊕a⟩) |f(x)⟩
Apply Hadamard to ﬁrst n bits.
P
y(−1)x⊙y(1 + (−1)a⊙y) |y⟩

|f(x)⟩
=
2 P
y:a⊙y=0(−1)x⊙y |y⟩|f(x)⟩
Measure ﬁrst n qubits of register to obtain a value y
such that y ⊙a = 0. Repeat until we get a suﬃcient
number of linearly independent equations on a.
Figure 20.4: Simon’s Algorithm
20.7
Shor’s algorithm: integer factorization using quantum com-
puters.
The integer factorization problem is to ﬁnd, given an integer N, the set of all prime factors of N
(i.e., prime numbers that divide N). By a polynomial-time algorithm for this problem we mean an
Web draft 2007-01-08 21:59

DRAFT
20.7. SHOR’S ALGORITHM: INTEGER FACTORIZATION USING QUANTUM
COMPUTERS.
p20.23 (423)
algorithm that runs in time polynomial in the description of N, i.e., poly(log(N)) time. Although
people have been thinking about the factorization problem in one form or another for at least 2000
years, we still do not know of a polynomial-time algorithm for it: the best classical algorithm takes
roughly 2(log N)1/3 steps to factor N [?]. In fact, the presumed diﬃculty of this problem underlies
many popular encryption schemes (such as RSA). Therefore, it was quite a surprise when in 1994
Peter Shor showed a quantum polynomial-time algorithm for this problem. To this day it remains
the most famous algorithm for quantum computers, and the strongest evidence that BQP may
contain problems outside of BPP.
The order-ﬁnding problem.
Rather than showing an algorithm to factor a given number N,
we will show an algorithm for a related problem: given a number A with gcd(A, N) = 1, ﬁnd the
order of A modulo N, deﬁned to be the smallest positive integer r such that Ar = 1 (mod N).
Using elementary number theory, it is fairly straightforward to reduce the task of factoring N to
solving this problem, and we defer the description of this reduction to Section 20.7.3.
Remark 20.18
It is easy to see that for every positive integer k, if Ak = 1 (mod N) then r divides k. (Indeed,
otherwise if k = cr + d for c ∈Z and d ∈{1, .., r −1} then Ad = 1 (mod N), contradicting the
minimality of r.) Similarly, for every x, y it holds that Ax = Ay (mod N) iﬀx −y is a multiple of
r. Therefore, the order ﬁnding problem can be deﬁned as follows: given the function f : N →N
that maps x to Ax (mod N) and satisﬁes that f(x) = f(y) iﬀr|x −y, ﬁnd r. In this notation, the
similarity to Simon’s problem becomes more apparent.
20.7.1
Quantum Fourier Transform over ZM.
The main tool used to solve the order-ﬁnding problem is the quantum Fourier transform. We have
already encountered the Fourier transform in Chapter 19, but will now use a diﬀerent variant, which
we call the Fourier transform over ZM where M = 2m for some integer M. Recall that ZM is the
group of all number in {0, . . . , M −1} with the group operation being addition modulo M. The
Fourier transform over this group, deﬁned below, is a linear and in fact unitary operation from C2m
to C2m. The quantum Fourier transform is a way to perform this operation by composing O(m2)
elementary quantum operations (operations that depend on at most three qubits). This means that
we can transform a quantum system whose register is in state f to a system whose register is in the
state corresponding to the Fourier transform ˆf of f. This does not mean that we can compute in
O(m2) the Fourier transform over ZM - indeed this is not suﬃcient time to even write the output!
Nonetheless, this transformation still turns out to be very useful, and is crucial to Shor’s factoring
algorithm in the same way that the Hadamard transformation (which is a Fourier transform over
the group {0, 1}n with the operation ⊕) was crucial to Simon’s algorithm.
Deﬁnition of the Fourier transform over ZM.
Let M = 2m and let ω = e2πi/M. Note that ωM = 1 and ωK ̸= 1 for every positive integer K < N
(we call such a number ω a primitive Mth root of unity). A function χ : ZM →C is called a
character of ZM if χ(y +z) = χ(y)χ(z) for every y, z ∈ZM. ZM has M characters {χx}x∈ZM where
Web draft 2007-01-08 21:59

DRAFT
p20.24 (424)
20.7. SHOR’S ALGORITHM: INTEGER FACTORIZATION USING QUANTUM
COMPUTERS.
χx(y) = ωxy. Let ˜χx = χx/
√
M (this factor is added for normalization), then the set {˜χx}x∈ZM is
an orthonormal basis of the space CM since
⟨˜χx, ˜χy⟩=
1
M
M−1
X
z=0
ωxzωyz =
1
M
M−1
X
z=0
ω(x−y)z
which is equal to 1 if x = y and to
1
M
1−ω(x−y)M
1−ωx−y
= 0 if x ̸= y (the latter equality follows by the
formula for the sum of a geometric series and the fact that ωℓM = 1 for every ℓ).
Definition 20.19
For f a vector in CM, the Fourier transform of f is the representation of f in the basis {˜χx}.
We let ˆf(x) denote the coeﬃcient of ˜χ−x in this representation. Thus f = PM−1
x=0
ˆf(x)˜χ−x and so
ˆf(x) = ⟨f, χ−x⟩=
1
√
M
PM−1
y=0 ωxyf(x). We let FTM(f) denote the vector ( ˆf(0), . . . , ˆf(M −1)).
The function FTM is a unitary operation from CM to CM and is called the Fourier transform over
ZM.
Fast Fourier Transform
Note that
ˆf(x) =
1
√
M
X
y∈ZM
f(y)ωxy =
1
√
M
X
y∈ZM,y even
f(y)ω−2x(y/2) + ωx
1
√
M
X
y∈ZM,y odd
f(y)ω2x(y−1)/2 .
Now since ω2 is an M/2th root of unity and ωM/2 = −1, letting W be the M/2 diagonal matrix
with diagonal ω0, . . . , ωM/2−1, we get that
FTM(f)low = FTM/2(feven) + WFTM/2(fodd)
(4)
FTM(f)high = FTM/2(feven) −WFTM/2(fodd)
(5)
where for an M-dimensional vector v, we denote by veven (resp. vodd) the M/2-dimensional vector
obtained by restricting v to the coordinates whose indices have least signiﬁcant bit equal to 0 (resp.
1) and by vlow (resp. vhigh) the restriction of v to coordinates with most signiﬁcant bit 0 (resp.
1).
Equations (4) and (5) are the crux of the well known Fast Fourier Transform (FFT) algorithm
that computes the Fourier transform in O(M log M) (as opposed to the naive O(M2)) time. We
will use them for the quantum Fourier transform algorithm, obtaining the following lemma:
Lemma 20.20
There is an O(m2)-step quantum algorithm that transforms a state f = P
x∈Zm f(x) |x⟩into the
state ˆf = P
x∈Zm ˆf(x) |x⟩, where ˆf(x) =
1
√
M
P
y∈Zm ωxyf(x).
Quantum Fourier transform: proof of Lemma 20.20
To prove Lemma 20.20, we use the following algorithm:
Web draft 2007-01-08 21:59

DRAFT
20.7. SHOR’S ALGORITHM: INTEGER FACTORIZATION USING QUANTUM
COMPUTERS.
p20.25 (425)
Quantum Fourier Transform FTM
Initial state: f = P
x∈ZM f(x) |x⟩
Final state: ˆf = P
x∈ZM ˆf(x) |x⟩.
Operation
State (neglecting normalizing factors)
f = P
x∈ZM f(x) |x⟩
Recursively run FTM/2 on m−1 most
signiﬁcant qubits
(FTM/2feven) |0⟩+ (FTM/2fodd) |1⟩
If LSB is 1 then compute W on m −1
most signiﬁcant qubits (see below).
(FTM/2feven) |0⟩+ (WFTM/2fodd) |1⟩
Apply Hadmard gate H to least sig-
niﬁcant qubit.
(FTM/2feven)(|0⟩
+
|1⟩)
+
(WWFTM/2fodd)(|0⟩−|1⟩) =
(FTM/2feven
+
FTM/2fodd) |0⟩
+
(FTM/2feven −WFTM/2fodd) |1⟩
Move LSB to the most signiﬁcant po-
sition
|0⟩(FTM/2feven
+
FTM/2fodd)
+
|1⟩(FTM/2feven −WFTM/2fodd) = ˆf
The transformation W on m −1 qubits can be deﬁned by |x⟩7→ωx = ω
Pm−2
i=0 2ixi (where xi is
the ith qubit of x). It can be easily seen to be the result of applying for every i ∈{0, . . . , m −2}
the following elementary operation on the ith qubit of the register: |0⟩7→|0⟩and |1⟩7→ω2i |1⟩.
The ﬁnal state is equal to ˆf by (4) and (5). (We leave verifying this and the running time to
Exercise 9.) ■
20.7.2
The Order-Finding Algorithm.
We now present a quantum algorithm that on input a number A < N, ﬁnds the order of A
modulo N (i.e., the smallest r such that Ar = 1 (mod N)). We let m = 3 log N and M = 2m.
Our register will consist of m + log(N) qubits.
Note that the function x 7→Ax (mod N) can
be computed in polylog(N) time (see Exercise 6) and so we will assume that we can compute
the map |x⟩|y⟩7→|x⟩|y ⊕⌞Ax (mod N)⌟⟩(where
⌞X⌟denotes the representation of the number
X ∈{0, . . . , N −1} as a binary string of length log N).4 The order-ﬁnding algorithm is as follows:
4To compute this map we may need to extend the register by some additional qubits, but we can ignore them as
they will always be equal to zero except in intermediate computations.
Web draft 2007-01-08 21:59

DRAFT
p20.26 (426)
20.7. SHOR’S ALGORITHM: INTEGER FACTORIZATION USING QUANTUM
COMPUTERS.
Order ﬁnding algorithm.
Goal: Given numbers N and A < N such that gcd(A, N) = 1, ﬁnd the smallest r such
that Ar = 1 (mod N).
Quantum register: We use an m + n-qubit register, where m = 3 log N (and hence in
particular M ≥N3). Below we treat the ﬁrst m bits of the register as encoding a number
in ZM.
Operation
State (including normalizing factors)
Apply Fourier transform to the ﬁrst m bits.
1
√
M
P
x∈ZM |x⟩) |0n ⟩
Compute
the
transformation
|x⟩|y⟩
7→
|x⟩|y ⊕(Ax (mod N))⟩.
P
x∈ZM |x⟩|Ax (mod N)⟩
Measure the second register to get a value y0.
1
√
⌈M/r ⌉
P⌈M/r ⌉−1
ℓ=0
|x0 + ℓr⟩|y0 ⟩where x0
is the smallest number such that Ax0 = y0
(mod N).
Apply the Fourier transform to the ﬁrst register.
1
√
M√
⌈M/r ⌉
P
x∈Zn
P⌈M/r ⌉−1
ℓ=0
ω(x0+ℓr)x |x⟩

|y0 ⟩
Measure the ﬁrst register to obtain a number x ∈ZM. Find the best rational approxi-
mation a/b (with a, b coprime) for the fraction
x
M with denominator b at most 40M (see
Section 20.A). If Ab = A (mod M) then output b.
In the analysis, it will suﬃce to show that this algorithm outputs the order r with probability
at least 1/poly(log(N)) (we can always amplify the algorithm’s success by running it several times
and taking the smallest output).
Analysis: the case that r|M
We start by analyzing the algorithm in the (rather unrealistic) case that M = rc for some integer
c. In this case we claim that the value x measured will be equal to c′c for random c′ ∈0, . . . , r. In
this case, x/M = c′/r. However, with probability at least Ω(1/ log(r)), the number c′ will be prime
(and in particular coprime to r). In this case, the denominator of the rational approximation for
x/M is indeed equal to r.
Indeed, for every x ∈ZM, the absolute value of |x⟩’s coeﬃcient before the measurement is equal
(up to some normalization factor) to

c−1
X
ℓ=0
ω(x0+ℓr)x
 =
ωx0c′c

c−1
X
ℓ=0
ωrℓx
 = 1 ·

c−1
X
ℓ=0
ωrℓx
 .
(6)
But if x = cc′ then ωrℓcc′ = ωMc′ = 1, and hence the coeﬃcients of all such x’s are equal to the
same positive number. On the other hand, if c does not divide x then then since ωr is a cth root
of unity, Pc−1
ℓ=0 wrℓx = 0 by the formula for sums of geometric progressions. Thus, such a number
x would be measured with zero probability.
The case that r ̸ |M
In the general case, we will not be able to show that the value x measured satisﬁes M|xr. However,
we will show that with Ω(1/ log r) probability, (1) xr will be “almost divisible” by M in the sense
Web draft 2007-01-08 21:59

DRAFT
20.7. SHOR’S ALGORITHM: INTEGER FACTORIZATION USING QUANTUM
COMPUTERS.
p20.27 (427)
1
i
β=eiθ
1-β
βk=eiθk
θ
(k-1)θ
1-βk
α
α/2
α/2
1
1
sin(α/2)
sin(α/2)
Figure 20.5:
A complex number z = a + ib can be thought of as the two-dimensional vector (a, b) of length
|z| =
√
a2 + b2. The number β = eiθ corresponds to a unit vector of angle θ from the x axis.For any such β, if k is
not too large (say k < 1/θ) then by elementary geometric considerations |1−βk|
|1−β| =
2 sin(θ/2)
2 sin(kθ/2). We use here the fact
(proved in the boxed ﬁgure) that in a unit cycle, the chord corresponding to an angle α is of length 2 sin(α/2).
that 0 ≤xr (mod M) < r/10 and (2) ⌈xr/M ⌉is coprime to r.
Condition (1) implies that
|xr −cM| < r/10 for c = ⌈xr/M ⌉. Dividing by rM gives
 x
M −c
r
 <
1
10M . Therefore,
c
r is a
rational number with denominator at most N that approximates x
M to within 1/(10M) < 1/(2N2).
It is not hard to see that such an approximation is unique (Exercise 7) and hence in this case the
algorithm will come up with c/r and output the denominator r.
Thus all that is left is to prove the following two lemmas:
Lemma 20.21
There exist Ω(r/ log r) values x ∈ZM such that:
1. 0 ≤xr (mod M) < r/10
2. ⌈xr/M ⌉and r are coprime
Lemma 20.22
If x satisﬁes 0 ≤xr (mod M) < r/10 then, before the measurement in the ﬁnal step of the order-
ﬁnding algorithm, the coeﬃcient of |x⟩is at least Ω( 1
√r).
Proof of Lemma 20.21:
We prove the lemma for the case that r is coprime to M, leaving the
general case as Exercise 10. In this case, the map x 7→rx (mod M) is a permutation of Z∗
M and
we have a set of at least r/(20 log r) x’s such that xr (mod M) is a prime number p between 0 and
r/10. For every such x, xr + ⌈r/M ⌉M = p which means that ⌈r/M ⌉can not have a nontrivial
shared factor with r, as otherwise this factor would be shared with p as well. ■
Proof of Lemma 20.22:
Let x be such that 0 ≤xr (mod M) < r/10. The absolute value of
|x⟩’s coeﬃcient in the state before the measurement is
1
√
⌈M/r ⌉
√
M

⌈M/r ⌉−1
X
ℓ=0
ωℓrx

.
(7)
Setting β = ωrx (note that since M ̸ |rx, β ̸= 1) and using the formula for the sum of a geometric
series, this is at least
√r
2M
 1−β⌈M/r ⌉
1−β
 =
√r
2M
sin(θ⌈M/r ⌉/2)
sin(θ/2)
,
(8)
Web draft 2007-01-08 21:59

DRAFT
p20.28 (428)
20.7. SHOR’S ALGORITHM: INTEGER FACTORIZATION USING QUANTUM
COMPUTERS.
where θ = rx (mod M)
M
is the angle such that β = eiθ (see Figure 20.5 for a proof by picture of the
last equality). Under our assumptions ⌈M/r ⌉θ < 1/10 and hence (using the fact that sin α ∼α
for small angles α), the coeﬃcient of x is at least
√r
4M ⌈M/r ⌉≥
1
8√r ■
20.7.3
Reducing factoring to order ﬁnding.
The reduction of the factoring problem to the order-ﬁnding problem follows immediately from the
following two Lemmas:
Lemma 20.23
For every nonprime N, the probability that a random X in the set Z∗
N = {X ∈[N −1] : gcd(X, N) = 1}
has an even order r and furthermore, Xr/2 ̸= +1 (mod N) and X ̸= −1 (mod N) is at least 1/4.
Lemma 20.24
For every N and Y , if Y 2 = 1 (mod N) but Y ̸= +1 (mod N) and Y ̸= −1 (mod N), then
gcd(Y −1, N) > 1.
Together, Lemmas 20.23 and 20.24 show that the following algorithm will output a prime factor
P of N with high probability: (once we have a single prime factor P, we can run the algorithm
again on N/P)
1. Choose X at random from [N −1].
2. If gcd(X, N) > 1 then let K = gcd(X, N), otherwise compute the order r of X, and if r is
even let K = gcd(Xr/2 −1, N).
3. If K ∈{1, N} then go back to Step 1. If K is a prime then output K and halt. Otherwise,
use recursion to output a factor of K.
Note that if T(N) is the running time of the algorithm then it satisﬁes the equation T(N) ≤
T(N/2) + polylog(N) leading to polylog(N) running time.
Proof of Lemma 20.24: Under our assumptions, N divides Y 2 −1 = (Y −1)(X + 1) but does
not divide neither Y −1 or Y + 1. But this means that gcd(Y −1, N) > 1Z since if Y −1 and N
were coprime, then since N divides (Y −1)(Y + 1), it would have to divide X + 1 (Exercise 8). ■
Proof of Lemma 20.23: We prove this for the case that N = PQ for two primes P, Q (the proof
for the general case is similar and is left as Exercise ??). In this case, by the Chinese Reminder
Theorem, if we map every number X ∈Z∗
N to the pair ⟨X (mod P), X (mod Q)⟩then this map
is one-to-one. Also, the groups Z∗
P and Z∗
Q are known to by cyclic which means that there is a
number g ∈[P −1] such that the map j 7→gj (mod P) is a permutation of [P −1] and similarly
there is a number h ∈[Q −1] such that the map k 7→hk (mod P) is a permutation of [Q −1].
This means that instead of choosing X at random, we can think of choosing two numbers j, k at
random from [P −1] and [Q−1] respectively and consider the pair ⟨gj (mod P), hk (mod Q)⟩which
is in one-to-one correspondence with the set of X’s in Z∗
N. The order of this pair (or equivalently,
of X) is the smallest positive integer r such that gjr = 1 (mod P) and hkr = 1 (mod Q), which
Web draft 2007-01-08 21:59

DRAFT
20.8. BQP AND CLASSICAL COMPLEXITY CLASSES
p20.29 (429)
means that P −1|jr and Q −1|kr. Now suppose that j is odd and k is even (this happens with
probability 1/4). In this case r is of the form 2r′ where r′ is the smallest number such that P −1|2jr′
and Q−1|kr′ (the latter holds since we can divide the two even numbers k and Q−1 by two). But
this means that gj(r/2) ̸= 1 (mod Q) and hk(r/2) = 1 (mod Q). In other words, if we let X be the
number corresponding to ⟨gj (mod P), hk (mod Q)⟩then Xr/2 corresponds to a pair of the form
⟨a, 1⟩where a ̸= 1. However, since +1 (mod N) corresponds to the pair ⟨+1, +1⟩and −1 (mod N)
corresponds to the pair ⟨−1 (mod P), −1 (mod Q)⟩it follows that Xr/2 ̸= ±1 (mod N). ■
20.8
BQP and classical complexity classes
What is the relation between BQP and the classes we already encountered such as P, BPP and
NP? This is very much an open questions. It not hard to show that quantum computers are at
least not inﬁnitely powerful compared to classical algorithms:
Theorem 20.25
BQP ⊆PSPACE
Proof Sketch:
To simulate a T-step quantum computation on an m bit register, we need to
come up with a procedure Coeff that for every i ∈[T] and x ∈{0, 1}m, the xth coeﬃcient (up to
some accuracy) of the register’s state in the ith execution. We can compute Coeff on inputs x, i
using at most 8 recursive calls to Coeff on inputs x′, i −1 (for the at most 8 strings that agree
with x on the three bits that the Fi’s operation reads and modiﬁes). Since we can reuse the space
used by the recursive operations, if we let S(i) denote the space needed to compute Coeff(x, i)
then S(i) ≤S(i −1) + O(ℓ) (where ℓis the number of bits used to store each coeﬃcient).
To compute, say, the probability that if measured after the ﬁnal step the ﬁrst bit of the register
is equal to 1, just compute the sum of Coeff(x, T) for every x ∈{0, 1}n. Again, by reusing the
space of each computation this can be done using polynomial space. ■
Theorem 20.25 can be improved to show that BQP ⊆P#P (where #P is the counting version
of NP described in Chapter 9), but this is currently the best upper bound we know on BQP.
Does BQP = BPP? The main reason to believe this is false is the polynomial-time algorithm
for integer factorization.
Although this is not as strong as the evidence for, say NP ⊈BPP
(after all NP contains thousands of well-studied problems that have resisted eﬃcient algorithms),
the factorization problem is one of the oldest and most well-studied computational problems, and
the fact that we still know no eﬃcient algorithm for it makes the conjecture that none exists
appealing. Also note that unlike other famous problems that eventually found an algorithm (e.g.,
linear programming [?] and primality testing [?]), we do not even have a heuristic algorithm that
is conjectured to work (even without proof) or experimentally works on, say, numbers that are
product of two random large primes.
What is the relation between BQP and NP? It seems that quantum computers only oﬀer
a quadratic speedup (using Grover’s search) on NP-complete problems, and so most researchers
believe that NP ⊈BPP. On the other hand, there is a problem in BQP (the Recursive Fourier
Sampling or RFS problem [BV97]) that is not known to be in the polynomial-hierarchy , and so at
the moment we do not know that BQP = BPP even if we were given a polynomial-time algorithm
for SAT.
Web draft 2007-01-08 21:59

DRAFT
p20.30 (430)
20.8. BQP AND CLASSICAL COMPLEXITY CLASSES
Chapter notes and history
We did not include treatment of many fascinating aspects of quantum information and computation.
Many of these are covered in the book by Nielsen and Chuang [NC00]. See also Umesh Vazirani’s
excellent lecture notes on the topic (available from his home page).
One such area is quantum error correction, that tackles the following important issue: how can
we run a quantum algorithm when at every possible step there is a probability of noise interfering
with the computation? It turns out that under reasonable noise models, one can prove the following
threshold theorem: as long as the probability of noise at a single step is lower than some constant
threshold, one can perform arbitrarily long computations and get the correct answer with high
probability [?].
Quantum computing has a complicated but interesting relation to cryptography.
Although
Shor’s algorithm and its variants break many of the well known public key cryptosystems (those
based on the hardness of integer factorization and discrete log), the features of quantum mechanics
can actually be used for cryptographic purposes, a research area called quantum cryptography (see
[?]).
Shor’s algorithm also spurred research on basing public key encryption scheme on other
computational problems (as far as we know, quantum computers do not make the task of breaking
most known private key cryptosystems signiﬁcantly easier). Perhaps the most promising direction
is basing such schemes on certain problems on integer lattices (see the book [?] and [?]).
While quantum mechanics has had fantastic success in predicting experiments, some would
require more from a physical theory. Namely, to tell us what is the “actual reality” of our world.
Many physicists are understandably uncomfortable with the description of nature as maintaining
a huge array of possible states, and changing its behavior when it is observed. The popular science
book [Bru04] contains a good (even if a bit biased) review of physicists’ and philosophers’ attempts
at providing more palatable descriptions that still manage to predict experiments.
On a more technical level, while no one doubts that quantum eﬀects exist at microscopic scales,
scientists questioned why they do not manifest themselves at the macrosopic level (or at least
not to human consciousness). A Scientiﬁc American article by Yam [Yam97] describes various
explanations that have been advanced over the years. The leading theory is decoherence, which
tries to use quantum theory to explain the absence of macroscopic quantum eﬀects. Researchers are
not completely comfortable with this explanation. The issue is undoubtedly important to quantum
computing, which requires hundreds of thousands of particles to stay in quantum superposition for
large-ish periods of time. Thus far it is an open question whether this is practically achievable.
One theoretical idea is to treat decoherence as a form of noise, and to build noise-tolerance into
the computation —a nontrivial process. For details of this and many other topics, see the books
by Kitaev, Shen, and Vyalyi [AA02].
The original motivation for quantum computing was to construct computers that are able to
simulate quantum mechanical systems, and this still might be their most important application
if they are ever built.
Feynman [Fey82] was the ﬁrst to suggest the possibility that quantum
mechanics might allow Turing Machines more computational power than classical TMs. In 1985
Deutsch [Deu85] deﬁned a quantum Turing machine, though in retrospect his deﬁnition is unsatis-
factory. Better deﬁnitions then appeared in Deutsch-Josza [DJ92], Bernstein-Vazirani [BV97] and
Yao [Yao93], at which point quantum computation was ﬁrmly established as a ﬁeld.
Web draft 2007-01-08 21:59

DRAFT
20.8. BQP AND CLASSICAL COMPLEXITY CLASSES
p20.31 (431)
Exercises
§1 Prove Claim 20.11.
Hint: First prove that Condition 3 holds iﬀCondition 1 holds iﬀ
Condition 4 holds. This follows almost directly from the deﬁnition
of the inner product and the fact that for every matrices A, B
it holds that (AB)∗= B∗A∗and (A∗)∗= A. Then prove that
Condition 3 implies Condition 2, which follows from the fact that
the norm is invariant under a change of basis. Finally, prove that
Condition 2 implies Condition 3 by showing that if two orthogonal
unit vectors v, u are mapped to non-orthogonal unit vectors v′, u′,
then the norm of the vector u + v is not preserved.
§2 For each one of the following operations: Hadamard, NOT, controlled-NOT, rotation by π/4,
and Tofolli, write down the 8×8 matrix that describes the mapping induced by applying this
operation on the ﬁrst qubits of a 3-qubit register.
§3 Suppose that a two-bit quantum register is in an arbitrary state v. Show that the following
three experiments will yield the same probability of output:
(a) Measure the register and output the result.
(b) First measure the ﬁrst bit and output it, then measure the second bit and output it.
(c) First measure the second bit and output it, then measure the ﬁrst bit and output it.
§4 Suppose that f is computed in T time by a quantum algorithm that uses a partial measure-
ments in the middle of the computation, and then proceeds diﬀerently according to the result
of that measurement. Show that f is computable by O(T) elementary operations.
§5 Prove that if for some a ∈{0, 1}n, the strings y1, . . . , yn−1 are chosen uniformly at random
from {0, 1}n subject to yi ⊙a = 0 for every i ∈[n −1], then with probability at least 1/10,
there exists no nonzero string a′ ̸= a such that yi ⊙a′ = 0 for every i ∈[n −1]. (In other
words, the vectors y1, . . . , yn−1 are linearly independent.)
§6 Prove that given A, x ∈{0, . . . , M −1}, we can compute Ax (mod M) in time polynomial in
log M.
Hint: Start by solving the case that x = 2k for some k. Then,
show an algorithm for general x by using x’s binary expansion.
§7 Prove that for every α < 1, there is at most a single rational number a/b such that b < N
and |α −a/b| < 1/(2N2).
§8 Prove that if A, B are numbers such that N and A are coprime but N divides AB, then N
divides B.
Web draft 2007-01-08 21:59

DRAFT
p20.32 (432)
20.A. RATIONAL APPROXIMATION OF REAL NUMBERS
Hint: Use the fact that if N and A are co-prime then there are
whole numbers α, β such that αN + βA = 1 and multiply this
equation by B.
§9 Prove Lemma 20.20.
§10 Complete the proof of Lemma ?? for the case that r and M are not coprime.
That is,
prove that also in this case there exist at least Ω(r/ log r) values x’s such that 0 ≤rx
(mod M) ≤r/2 and ⌈M/x ⌉and r are coprime.
Hint: let d = gcd(r, M), r′ = r/d and M ′ = M/d. Now use the
same argument as in the case that M and r are coprime to argue
that there exist Ω(
r
d log r) values x ∈ZM ′ satisfying this condition,
and that if x satisﬁes it then so does x + cM for every c.
§11 (Uses knowledge of continued fractions) Suppose j, r ≤N are mutually coprime and unknown
to us. Show that if we know the ﬁrst 2 log N bits of j/r then we can recover j, r in polynomial
time.
20.A
Rational approximation of real numbers
A continued fraction is a number of the following form:
α1 +
1
α2 +
1
α2+
1
α3+...
Given a real number α > 0, we can ﬁnd its representation as an inﬁnite fraction as follows: split
α into the integer part ⌊α ⌋and fractional part α −⌊α ⌋, ﬁnd recursively the representation R of
1/(α −⌊α ⌋), and then write
α = ⌊α ⌋+ 1
R .
If we stop after ℓsteps, we get a rational number that can be represented as a/b with a, b coprime.
It can be veriﬁed that b ∈[2ℓ/2, 22ℓ]. The following theorem is also known:
Theorem 20.26
[?] If a/b is a rational number obtained by running the continued fraction algorithm on α for a
ﬁnite number of steps then |α −a/b| > |α −c/d for every rational number c/d with denominator
d ≤b.
This means that given any number α and bound N, we can use the continued fraction algorithm
to ﬁnd in polylog(N) steps a rational number a/b such that b ≤16N and a/b approximates α better
than any other rational number with denominator at most b.
Web draft 2007-01-08 21:59

DRAFT
Chapter 21
Logic in complexity theory
Very sketchy
As mentioned in the book’s introduction, complexity theory (indeed, all of computer science)
arose from developments in mathematical logic in the ﬁrst half of the century. Mathematical logic
continues to exert an inﬂuence today, suggesting terminology and choice of problems (e.g., “boolean
satisﬁability”) as well as approaches for attacking complexity’s central open questions. This chapter
is an introduction to the basic concepts.
Mathematical logic has also inﬂuenced many other areas of computer science, such as program-
ming languages, program veriﬁcation, and model checking. We will not touch upon them, except
to note that they supply interesting examples of hard computational problems —ranging from
NP-complete to EXPSPACE-complete to undecidable.
The rest of the chapter assumes only a nodding familiarity with logic terminology, which we
now recount informally; for details see a logic text.
A logic usually refers to a set of rules about constructing valid sentences. Here are a few logics
we will encounter. Propositional logic concerns sentences such as (p ∨¬q) ∧(¬p ∨r) where p, q, r
are boolean variables. Recall that the SAT problem consists of determining the satisﬁability of
such sentences. In ﬁrst order logic, we allow relation and function symbols as well as quantiﬁcation
symbols ∃and ∀. For instance, the statement ∀xS(x) ̸= x is a ﬁrst order sentence in which x is
quantiﬁed universally, S() is a unary relation symbol and ̸= is a binary relation. Such logics are
used in well-known axiomatizations of mathematics, such as Euclidean geometry, Peano Arithmetic
or Zermelo Frankel set theory. Finally, second order logic allows sentences in which one is allowed
quantiﬁcation over structures, i.e., functions and relations. An example of a second order sentence
is ∃S∀xS(x) ̸= x, where S is a unary relation symbol.
A sentence (or collection of sentences) in a logic has no intrinsic “meaning.” The meaning
—including truth or falsehood—can be discussed only with reference to a structure, which gives
a way of interpreting all symbols in the sentence. To give an example, Peano arithmetic consists
of ﬁve sentences (“axioms”) in a logic that consists of symbols like S(x), =, + etc. The standard
structure of these sentences is the set of positive integers, with S() given the intepretation of
“successor function,” + given the interpretation of addition, and so on. A structure is said to be a
model for a sentence or a group of sentences if those sentences are true in that structure.
Finally, a proof system consists of a set of sentences Σ called axioms and one or more derivation
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p21.1 (433)

DRAFT
p21.2 (434)
21.1. LOGICAL DEFINITIONS OF COMPLEXITY CLASSES
rules for deriving new sentences from the axioms. We say that sentence σ can be proved from Σ,
denoted Σ ⊢σ, if it can be derived from Σ using a ﬁnite number of applications of the derivation
rules. A proveable sentence is called a theorem.
Note that a theorem is a result of a mechanical (essentially, algorithmic) process of applying
derivation rules to the axioms. There is a related notion of whether or not σ is logically implied by
Σ, denoted Σ |= σ, which means that every model of Σ is also a model of σ. In other words, there
is no “counterexample model” in which the axioms Σ are true but σ is not. The two notions are in
general diﬀerent but G¨odel in his completeness theorem for ﬁrst order theories exhibited a natural
set of derivation rules such that logically implied sentences are exactly the set of theorems. (This
result was a stepping stone to his even more famous incompleteness theorem.)
Later in this chapter we give a complexity-theoretic deﬁnition of a proof system, and introduce
the area of proof complexity that studies the size of the smallest proof of a mathematical statement
in a given proof system.
21.1
Logical deﬁnitions of complexity classes
Just as Church and others deﬁned computation using logic without referring to any kind of com-
puting machine, it is possible to give “machineless” characterizations of many complexity classes
using logic. We describe a few examples below.
21.1.1
Fagin’s deﬁnition of NP
In 1974, just as the theory of NP-completeness was coming into its own, Fagin showed how to
deﬁne NP using second-order logic. We describe his idea using an example.
Example 21.1
(Representing 3-COLOR) We show how to represent the set of 3-colorable graphs using second
order logic.
Let E be a symbol for a binary relation, and C0, C1, C2 be symbols for unary relations, and
φ(E, C0, C1, C2) be a ﬁrst order formula that is a conjunction of the following formulae where
i + 1, i + 2 are meant to be understood modulo 3:
∀u, v
E(u, v) = E(v, u)
(1)
∀u
∧i=1,2,3 (Ci(u) ⇒¬(Ci+1(u) ∨Ci+2(u))
(2)
∀uCi(u) ∨Ci+1(u) ∨Ci+2(u)
(3)
∀u, v
E(u, v) ⇒∧i=1,2,3(Ci(u) ⇒¬Ci(v))
(4)
What set of E’s deﬁned on a ﬁnite set satisfy ∃C0∃C1∃C2φ(E, C0, C1, C2)? If E is deﬁned on a
universe of size n (i.e., u, v take values in this universe) then (1) says that E is symmetric, i.e., it
may be viewed as the edge set of an undirected graph on n vertices. Conditions (2) and (3) say that
C0, C1, C2 partition the vertices into three classes. Finally, condition (4) says that the partition is
a valid coloring.
Web draft 2007-01-08 21:59

DRAFT
21.2. PROOF COMPLEXITY AS AN APPROACH TO NP VERSUS CONP
p21.3 (435)
Now we can sketch the general result. To represent a general NP problem, there is a unary
relation symbol that represents the input (in the above case, E). The witness is a tableau (see
Chapter 2) of an accepting computation. If the tableau has size nk, the witness can be represented
by a k-ary relation (in the above case the witness is a 3-coloring, which has representation size 3n
and hence was represented using 3 unary relations). The ﬁrst order formula uses the Cook-Levin
observation that the tableau is correct iﬀit is correct in all 2 × 3 “windows”.
The formal statement of Fagin’s theorem is as follows; the proof is left as an exercise.
Theorem 21.2 (Fagin)
To be written.
21.1.2
MAX-SNP
21.2
Proof complexity as an approach to NP versus coNP
Proof complexity tries to study the size of the smallest proof of a statement in a given proof
system. First, we need a formal deﬁnition of what a proof system is. The following deﬁnition due
to Cook and Reckow focuses attention on the intuitive property that a mathematical proof is “easy
to check.”
Definition 21.3
A proof system consists of a polynomial-time Turing machine M. A statement T is said to be a
theorem of this proof system iﬀthere is a string π ∈{0, 1}∗such that M accepts (T, π).
If T is a theorem of proof system M, then the proof complexity of T with respect to M is the
minimun k such that there is some π ∈{0, 1}k for which M accepts (T, π).
Note that the deﬁnition of theoremhood ignores the issue of the length of the proof, and insists
only that the M’s running time is polynomial in the input length |T| + |π|. The following is an
easy consequence of the deﬁnition and the motivation for much of the ﬁeld of proof complexity.
Theorem 21.4
A proof system M in which SAT has polynomial proof complexity exists iﬀNP = coNP.
Many branches of mathematics, including logic, algebra, geometry, etc. give rise to proof sys-
tems. Algorithms for SAT and automated theorem provers (popular in some areas of computer
science) also may be viewed as proof systems.
21.2.1
Resolution
This concerns
Web draft 2007-01-08 21:59

DRAFT
p21.4 (436)
21.3. IS P ̸= NP UNPROVEABLE?
21.2.2
Frege Systems
21.2.3
Polynomial calculus
21.3
Is P ̸= NP unproveable?
Web draft 2007-01-08 21:59

DRAFT
Chapter 22
Why are circuit lowerbounds so
diﬃcult?
Why have we not been able to prove strong lower bounds for circuits? In 1994 Razborov and Rudich
formalized the notion of a “natural mathematical proof,” for a circuit lowerbound. They pointed out
that current lowerbound arguments involve “natural” mathematical proofs, and show that obtaining
strong lowerbound with such techniques would violate a widely believed cryptographic assumption
(namely, that factoring integers requires time 2nϵ for some ﬁxed ϵ > 0).
Thus presumably we
need to develop mathematical arguments that are not natural. This result may be viewed as a
modern analogue of the Baker, Gill, Solovay result from the 1970s (see Chapter ??) that showed
that diagonalization alone cannot resolve P versus NP and other questions.
Basically, a natural technique is one that proves a lowerbound for a random function and is
“constructive.” We formalize “constructive” later but ﬁrst consider why lowerbound proofs may
need to work for random functions.
22.1
Formal Complexity Measures
Let us imagine at a high level how one might approach the project of proving circuit lower bounds.
For concreteness, focus on formulas, which are boolean circuits where gates have indegree 2 and
outdegree 1. It is tempting to use some kind of induction. Suppose we have a function like the one
in Figure 22.1 that we believe to be “complicated.” Since the function computed at the output is
“complicated”, intuition says that at least one of the functions on the incoming edges to the output
gate should also be “pretty complicated” (after all those two functions can be combined with a
single gate to produce a “complicated” function). Now we try to formalize this intuition, and point
out why one ends up proving a lowerbound on the formula complexity of random functions.
The most obvious way to formalize a “complicatedness” is as a function µ that maps every
boolean function on {0, 1}n to a nonnegative integer. (The input to µ is the truth table of the
function.) We say that µ is a formal complexity measure if it satisﬁes the following properties:
First, the measure is low for trivial functions: µ(xi) ≤1 and µ(¯xi) ≤1 for all i. Second, we require
that
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p22.1 (437)

DRAFT
p22.2 (438)
22.1. FORMAL COMPLEXITY MEASURES
Figure unavailable in pdf ﬁle.
Figure 22.1: A formula for a hard function.
• µ(f ∧g) ≤µ(f) + µ(g) for all f, g; and
• µ(f ∨g) ≤µ(f) + µ(g) for all f, g.
For instance, the following function ρ is trivially a formal complexity measure
ρ(f) = 1 + the smallest formula size for f.
(1)
In fact, it is easy to prove the following by induction.
Theorem 22.1
If µ is any formal complexity measure, then µ(f) is a lowerbound on the formula complexity of f.
Thus to formalize the inductive approach outlined earlier, it suﬃces to deﬁne a measure µ such
that µ(CLIQUE) is high (say superpolynomial). For example, one could try “fraction of inputs for
which the function agrees with the CLIQUE function” or some suitably modiﬁed version of this. In
general, one imagines that deﬁning a measure that lets us prove a good lowerbound for CLIQUE
would involve some deep observation about the CLIQUE function. The next lemma seems to show,
however, that even though all we care about is the CLIQUE function, our lowerbound necessarily
must reason about random functions.
Lemma 22.2
Suppose µ is a formal complexity measure and there exists a function f : {0, 1}n →{0, 1} such
that µ(f) ≥c for some large number c. Then for at least 1/4 of all functions g : {0, 1}n →{0, 1}
we must have µ(g) ≥c/4.
Proof: Let g : {0, 1}n →{0, 1} be any function. Write f as f = h ⊕g where h = f ⊕g. So
f = (¯h ∧g) ∨(h ∧¯g) and µ(f) ≤µ(g) + µ(g) + µ(h) + µ(h).
Now suppose for contradiction’s sake that {g : µ(g) < c/4} contains more than 3/4 of all boolean
functions on n-bit inputs. If we pick the above function g randomly, then g, h, h are also random
(though not independent).
Using the trivial union bound we have Pr[All of h, ¯h, g, ¯g have µ <
c/4] > 0. Hence µ(f) < c, which contradicts the assumption. Thus the lemma is proved. ■
In fact, the following stronger theorem holds:
Theorem 22.3
If µ(f) > c then for all ϵ > 0 and for at least 1 −ϵ of all functions g we have that,
µ(g) ≥Ω

c
(n + log(1/ϵ))2

.
The idea behind the proof of the theorem is to write f as the boolean combination of a small
number of functions and then proceed similarly as in the proof of the lemma.
Web draft 2007-01-08 21:59

DRAFT
22.2. NATURAL PROPERTIES
p22.3 (439)
22.2
Natural Properties
Moving the above discussion forward, we think of a lowerbound proof as identifying some property
of “hard” functions that is not shared by “easy” functions.
Definition 22.4
A property Φ is a map from boolean functions to {0, 1}. A P-natural property useful against P/poly
is a property Φ such that:
1. Φ(f) = 1 for at least a 1/2n fraction of all boolean functions on n bits (recall that there are
22n functions on n bits);
2. Φ(f) = 1 implies that f ̸∈P/poly (or more concretely, that f has circuit complexity at least
nlog n, say); and
3. Φ is computable on n-bit functions in 2O(n) time (i.e., polynomial in the length of the function’s
truth table).
The term P-natural refers to requirement (3). The property is useful against P/poly because of
requirement (2). (Note that this requirement also ensures that Φ is not trivial, since it must be 0 for
functions in P/poly.) Requirement (1) corresponds to our above intuition that circuit lowerbounds
should prove the hardness of a random function.
By suitably modifying (2) and (3) we can analogously deﬁne, for any complexity class C1 and
circuit class C2, a C1-natural property that is useful against circuit class C2. We emphasize that
when the property is computed, the input is the truth table of a function, whose size is 2n. Thus
a P-natural property is computed in time 2cn for some constant c > 1 and a PSPACE-natural
property is computed in space 2cn.
Example 22.5
The result that PARITY is not computable in AC0 (Section ??) involved the following steps. (a)
Show that every AC0 circuit can be simpliﬁed by restricting at most n−nϵ input bits so that it then
becomes a constant function. (b) Show that the PARITY function does not have this property.
Thus the natural property lurking in this proof is the following: Φ(f) = 1 iﬀfor every way of
assigning values to at most n −nϵ input bits the function does not become a constant function.
Clearly, if Φ(f) = 1 then f ̸∈AC0, so f is useful against AC0. Furthermore, Φ can be computed in
2O(n) time — just enumerate all possible choices for the subsets of variables and all ways of setting
them to 0/1. This running time is polynomial in the length of the truth-table, so Φ is P-natural.
Finally, requirement (1) is also met since almost all boolean functions satisfy Φ(f) = 1 (easy to
check using a simple probability calculation; left as exercise).
Thinking further, we see that Φ is a AC0-natural property that is useful against AC0.
Web draft 2007-01-08 21:59

DRAFT
p22.4 (440)
22.2. NATURAL PROPERTIES
Example 22.6
The lowerbound for ACC0 circuits described in Section ?? is not natural per se. Razborov and
Rudich show how to naturalize the proof, in other words change it —while retaining its essence—so
that it does use a natural property. Recall that every boolean function on n bits can be represented
by a multilinear polynomial over GF(3). The space of all n-variate multilinear polynomials forms
a vector space, whose dimension is N = 2n. Then all multilinear polynomials in n variables of total
degree less than n/2 form a subspace of dimension N/2 (this assumes n is even), and we denote this
space by L. For a boolean function f let ˆf be a multilinear polynomial over GF(3) that represents
f. Then deﬁne Φ(F) = 1 iﬀthe dimension of the space
n
ˆfl1 + l2 : l1, l2 ∈L
o
is at least 3N/4. It can be checked that Φ is 1 for the parity function, as well as for most ran-
dom functions. Furthermore, rank computations can be done in NC2 so it is NC2-natural. The
technique of Section ?? can be used to show that if Φ(f) = 1 then f ̸∈ACC0[3]; thus Φ is useful
against ACC0[3].
Example 22.7
The lowerbound for monotone circuits in Section ?? does use constructive methods, but it is
challenging to show that it applies to a random function since a random function is not monotone.
Nobody has formulated a good deﬁnition of a random monotone function.
In the deﬁnition of natural proofs, requirement (3) is the most controversial in that there is no
inherent reason why mathematical proofs should go hand in hand with eﬃcient algorithms.
Remark 22.8
“Constructive mathematics” was a movement within mathematics that rejected any proofs of exis-
tence that did not yield an algorithm for constructing the object. Today this viewpoint is considered
quaint; nonconstructive proofs are integral to mathematics.
In our context, “constructive” has a stricter meaning, namely the proof has to yield a polynomial-
time algorithm. Many proofs that would be “constructive” for a mathematician would be noncon-
structive under our deﬁnition. Surprisingly, even with this stricter deﬁnition, proofs in combinato-
rial mathematics are usually constructive, and —as Razborov and Rudich are pointing out —the
same is true of current circuit lowerbounds as well.
In a few cases, combinatorial results initially proved “nonconstructively” later turned out to
have constructive proofs: a famous example is the Lov`asz Local Lemma (discovered in 1974; al-
gorithmic version is in Beck [Bec91]). The same is true for several circuit lowerbounds—cf. the
“naturalized” version of the Razborov-Smolensky lowerbound for ACC0[q] mentioned earlier, and
Raz’s proof [Raz00] of the Babai-Nisan-Szegedy [BNS] lowerbound on multiparty communication
complexity.
Web draft 2007-01-08 21:59

DRAFT
22.3. LIMITATIONS OF NATURAL PROOFS
p22.5 (441)
22.3
Limitations of Natural Proofs
The following theorem by Razborov and Rudich explains why we have not been able to use the
same techniques to obtain an upper bound on P/poly: constructing a P-natural property useful
against P/poly violates widely believed cryptographic assumptions.
Theorem 22.9 (Razborov, Rudich [RR97])
Suppose a P-natural property Φ exists that is useful against P/poly. Then there are no strong
pseudorandom function generators.
In particular, FACTORING and DISCRETE LOG can be
solved in less than 2nϵ time for all ϵ > 0.
Pseudorandom function generators were deﬁned in Section ??. The deﬁnition used a distin-
guisher polynomial-time machine that is given oracle access to either a truly random function or a
function from the pseudorandom family. The family is termed pseudorandom if the distinguisher
cannot distinguish between the two oracles. Now we tailor that more general deﬁnition for our nar-
row purposes in this section. We allow the distinguisher 2O(n) time and even allow it to examine the
truth table of the function! This is without loss of generality since in 2O(n) time the distinguisher
could construct the truth table using 2n queries to the oracle.
Definition 22.10
A pseudorandom function generator is a function f(k, x) computable in polynomial time where the
input x has n bits and the “key” k has nc bits, where c > 2 is a ﬁxed constant. Denoting by Fn
the function obtained by uniformly selecting k ∈{0, 1}nc and setting Fn to f(k, ·), we have the
property that the function ensemble F = {Fn}∞
n=1 is “pseudorandom,” namely, for each Turing
machine M running in time 2O(n), and for all suﬃciently large n,
| Pr[M(Fn) = 1] −Pr[M(Hn) = 1]| <
1
2n2 ,
where Hn is a random function on {0, 1}n.
We will denote f(k, ·) by fk.
Intuitively, the above deﬁnition says that if f is a pseudorandom function generator, then for
a random k, the probability is high that fk “looks like a random function” to all Turing machines
running in time 2O(n). Note that fk cannot look random to machines that run in 2O(nc) time since
they can just guess the key k. Thus restricting the running time to 2O(n) (or to some other ﬁxed
exponential function such as 2O(n2)) is crucial.
Recall that Section ?? described the Goldreich-Goldwasser-Micali construction of pseudorandom
function generators f(k, x) using a pseudorandom generator g that stretches nc random bits to 2nc
pseudorandom (also see Figure 22.2): Let g0(k) and g1(k) denote, respectively, the ﬁrst and last
nc bits of g(k). Then the following function is a pseudorandom function generator, where MSB(x)
refers to the ﬁrst bit of a string x:
f(k, x) = MSB(gxn ◦gxn−1 ◦· · · ◦gx2 ◦gx1(k)).
The exercises in Chapter 10 explored the security of this construction as a function of the
security parameter of g; basically, the two are essentially the same. By the Goldreich-Levin theorem
Web draft 2007-01-08 21:59

DRAFT
p22.6 (442)
22.4. MY PERSONAL VIEW
Figure unavailable in pdf ﬁle.
Figure 22.2: Constructing a pseudorandom function generator from a pseudorandom generator.
of Section ??, a pseudorandom generator with such a high security parameter exists if a oneway
permutation exists and some ϵ > 0, such that every 2nϵ time algorithm has inversion probability
less than 2−nϵ. The DISCRETE LOG function —a permutation— is conjectured to satisfy this
property. As mentioned in Chapter 10, researchers believe that there is a small ϵ > 0 such that the
worst-case complexity of DISCRETE LOG is 2nϵ, which by random self-reducibility also implies the
hardness of the average case. (One can also obtain pseudorandom generators using FACTORING,
versions of which are also believed to be just as hard as DISCRETE LOG.) If this belief is correct,
then pseudorandom function generators exist as outlined above. (Exercise.)
Now we can prove the above theorem.
Theorem 22.9: Suppose the property Φ exists, and f is a pseudorandom function generator. We
show that a Turing machine can use Φ to distinguish fk from a random function. First note that
fk ∈P/poly for every k (just hardwire k into the circuit for fk) so the contrapositive of property
(2) implies that Φ(fk) = 0. In addition, property (1) implies that PrHn[Φ(Hn) = 1] ≥1/2n. Hence,
Pr
Hn[Φ(Hn)] −
Pr
k∈{0,1}nc[Φ(fk)] ≥1/2n,
and thus Φ is a distinguisher against f. ■
22.4
My personal view
Discouraged by the Razborov-Rudich result, researchers (myself included) hardly ever work on
circuit lowerbounds. Lately, I have begun to think this reaction was extreme. I still agree that
a circuit lowerbound for say CLIQUE, if and when we prove it, will very likely apply to random
functions as well.
Thus the way to get around the Razborov-Rudich observation is to deﬁne
properties that are not P-natural; in other words, are nonconstructive. I feel that this need not be
such an insurmountable barrier since a host of mathematical results are nonconstructive.
Concretely, consider the question of separating NEXP from ACC0, one of the (admittedly
not very ambitious) frontiers of circuit complexity outlined in Chapter 13.
As observed there,
NEXP ̸= ACC0 will follow if we can improve the Babai-Nisan-Szegedy lowerbound of Ω(n/2k)
for k-party communication complexity to Ω(n/poly(k)) for some function in NEXP. One line of
attack is to lowerbound the discrepancy of all large cylinder intersections in the truth table, as we
saw in Raz’s proof of the BNS lowerbound1. (In other words, the “unnatural” property we are
deﬁning is Φ where Φ(f) = 1 iﬀf has high discrepancy and thus high multiparty communication
complexity.) For a long time, I found this question intimidating because the problem of computing
the discrepancy given the truth table of the function is coNP-hard (even for k = 2). This seemed
1Interestingly, Raz discovered this naturalization of the BNS proof after being brieﬂy hopeful that the original
BNS proof—which is not natural— may allow a way around the Razborov-Rudich result.
Web draft 2007-01-08 21:59

DRAFT
22.4. MY PERSONAL VIEW
p22.7 (443)
to suggest that a proof that the discrepancy is high for an explicit function (which presumably will
also show that it is high for random functions) must have a nonconstructive nature, and hence will
be very diﬃcult. Lately, I have begun to suspect this intuition.
A relevant example is Lov`asz’s lowerbound of the chromatic number of the Kneser graph [Lov78].
Lowerbounding the chromatic number is coNP-complete in general. Lov`asz gives a topological
proof (using the famous Borsuk-Ulam ﬁxed point theorem) that determines the chromatic number of
the Kneser graph exactly. From his proof one can indeed obtain an algorithm for solving chromatic
number on all graphs([MZ02]) —but it runs in PSPACE for general graphs! So if this were a circuit
lowerbound we could call it PSPACE-natural, and thus “nonconstructive.” Nevertheless, Lov`asz’s
reasoning for the particular case of the Kneser graph is not overly complicated because the graph is
highly symmetrical. This suggests we should not blindly trust the intuition that “nonconstructive
≡diﬃcult.”
I fervently hope that the next generation of researchers will view the Razborov-Rudich theorem
as a guide rather than as a big obstacle!
Exercises
§1 Prove Theorem 22.3.
§2 Prove that a random function satisﬁes Φ(f) = 1 with high probability, where Φ is the property
deﬁned in Example 22.5.
§3 Show that if the hardness assumption for discrete log is true, then pseudorandom function
generators as deﬁned in this chapter exist.
§4 Prove Wigderson’s observation: P-natural properties cannot prove that DISCRETE LOG
requires circuits of 2nϵ size.
Hint: If DISCRETE LOG is hard on worst-case inputs then it is
hard on most inputs, and then it can be used to construct pseudo-
random functions.
§5 (Razborov [Raz92]) A submodular complexity measure is a complexity measure that satisﬁes
µ(f ∨g) + µ(f ∧g) ≤µ(f) + µ(g) for all functions f, g. Show that for every n-bit function
fn, such a measure satisﬁes µ(fn) = O(n).
Hint: It suﬃces to prove this when fn is a random function. Use
induction on the number of variables, and the fact that both fn
and fn are random functions.
Chapter notes and history
The observation that circuit lowerbounds may unwittingly end up reasoning about random functions
ﬁrst appears in Razborov [Raz89]’s result about the limitations of the method of approximation.
Web draft 2007-01-08 21:59

DRAFT
p22.8 (444)
22.4. MY PERSONAL VIEW
We did not cover the full spectrum of ideas in the Razborov-Rudich paper [RR97], where it is
observed that candidate pseudorandom function generators exist even in the class TC0, which lies
between ACC0 and NC1. Thus natural proofs will probably not allow us to separate even TC0
from P.
Razborov’s observation about submodular measures in Problem 5 is important because many
existing approaches for formula complexity use submodular measures; thus they will fail to even
prove superlinear lowerbounds.
In contrast with my limited optimism, Razborov himself expresses (in the introduction to [Raz03])
a view that the obstacle posed by the natural proofs observation is very serious. He observes that
existing lowerbound approaches use weak theories of arithmetic such as Bounded Arithmetic. He
conjectures that any circuit lowerbound attempt in such a logical system must be natural (and
hence unlikely to work). But as I mentioned, several theorems even in discrete mathematics use
reasoning (e.g., ﬁxed point theorems like Borsuk-Ulam) that does not seem to be formalizable in
Bounded Arithmetic. Thus is my reason for optimism.
However, somen other researchers are far more pessimistic: they fear that P versus NP may
be independent of mathematics (say, of Zermelo-Fraenkel set theory). Razborov says that he has
no intuition about this.
Web draft 2007-01-08 21:59

DRAFT
Appendices
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
p22.9 (445)

DRAFT

DRAFT
Appendix A
Mathematical Background.
This appendix reviews the mathematical notions used in this book. However, most of these are
only used in few places, and so the reader might want to only quickly review Sections A.1, A.2 and
A.3, and come back to the other sections as needed. In particular, apart from probability, the ﬁrst
part of the book essentially requires only comfort with mathematical proofs and some very basic
notions of discrete math.
The topics described in this appendix are covered in greater depth in many texts and online
sources. Almost all of the mathematical background needed is covered in a good undergraduate
“discrete math for computer science” course as currently taught at many computer science depart-
ments. Some good sources for this material are the lecture notes by Papadimitriou and Vazirani
[PV06], Lehman and Leighton [LL06] and the book of Rosen [Ros06].
Although knowledge of algorithms is not strictly necessary for this book, it would be quite
useful. It would be helpful to review either one of the two excellent recent books by Dasgupta
et al [DPV06] and Kleinberg and Tardos [KT06] or the earlier text by Cormen et al [CLRS01].
This book does not require prior knowledge of computability and automata theory, but some basic
familiarity with that theory could be useful: see Sipser’s book [SIP96] for an excellent introduction.
Mitzenmacher and Upfal [MU05] and Prabhakar and Raghavan [?] cover both algorithmic reasoning
and probability. For more insight on discrete probability, see the book by Alon and Spencer [AS00].
A.1
Mathematical Proofs
Perhaps the mathematical prerequisite needed for this book is a certain level of comfort with
mathematical proofs. While in everyday life we might use “proof” to describe a fairly convincing
argument, in mathematics a proof is an argument that is convincing beyond any shadow of a doubt.1
For example, consider the following mathematical statement:
Every even number greater than 2 is equal to the sum of two primes.
1In a famous joke, as a mathematician and an engineer drive in Scotland they see a white sheep on their left side.
The engineer says “you see: all the sheep in Scotland are white”. The mathematician replies “All I see is that there
exists a sheep in Scotland whose right side is white”.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
pA.1 (447)

DRAFT
pA.2 (448)
A.1. MATHEMATICAL PROOFS
This statement, known as “Goldbach’s Conjecture”, was conjectured to be true by Christian
Goldbach in 1742. In the more than 250 years that have passed since, no one has ever found a
counterexample to this statement. In fact, it has been veriﬁed to be true for all even numbers from
4 till 100, 000, 000, 000, 000, 000. Yet still it is not considered proven, since we have not ruled out
the possibility that there is some (very large) even number that cannot be expressed as the sum of
two primes.
The fact that a mathematical proof has to be absolutely convincing does not mean that it has to
be overly formal and tedious. It just has to be clearly written, and contain no logical gaps. When
you write proofs try to be clear and concise, rather than using too much formal notation. When
you read proofs, try to ask yourself at every statement “am I really convinced that this statement
is true?”.
Of course, to be absolutely convinced that some statement is true, we need to be certain
of what that statement means.
This why there is a special emphasis in mathematics on very
precise deﬁnitions. Whenever you read a deﬁnition, try to make sure you completely understand
it, perhaps by working through some simple examples. Oftentimes, understanding the meaning of
a mathematical statement is more than half the work to prove that it is true.
Example A.1
Here is an example for a classical mathematical proof, written by Euclid around 300 B.C. Recall
that a prime number is an integer p > 1 whose only divisors are p and 1, and that every number n
is a product of prime numbers. Euclid’s Theorem is the following:
Theorem A.2
There exist inﬁnitely many primes.
Before proving it, let’s see that we understand what this statement means. It simply means
that for every natural number k, there are more than k primes, and hence the number of primes is
not ﬁnite.
At ﬁrst, one might think it’s obvious that there are inﬁnitely many primes because there are
inﬁnitely many natural numbers, and each natural number is a product of primes. However, this is
faulty reasoning: for example, the set of numbers of the form 3n is inﬁnite, even though their only
factor is the single prime 3.
To prove Theorem A.2, we use the technique of proof by contradiction. That is, we assume it is
false and try to derive a contradiction from that assumption. Indeed, assume that all the primes
can be enumerated as p1, p2, . . . , pk for some number k. Deﬁne the number n = p1p2 · · · pk + 1.
Since we assume that the numbers p1, . . . , pk are all the primes, all of n’s prime factors must come
from this set, and in particular there is some i between 1 and k such that pi divides n. That is,
n = pim for some number m. Thus,
pim = p1p2 · · · pk + 1
or equivalently,
pim −p1p2 · · · pk = 1 .
Web draft 2007-01-08 21:59

DRAFT
A.2. SETS, FUNCTIONS, PAIRS, STRINGS, GRAPHS, LOGIC.
pA.3 (449)
But dividing both sides of this equation by pi, we will get a whole number on the left hand side (as
pi is a factor of p1p2 · · · pk) and the fraction 1/pi on the right hand side, deriving a contradiction.
This allows us to rightfully place the QED symbol ■and consider Theorem A.2 as proven.
A.2
Sets, Functions, Pairs, Strings, Graphs, Logic.
A set contains a ﬁnite or inﬁnite number of elements, without repetition or respect to order, for ex-
ample {2, 17, 5}, N = {1, 2, 3, . . .} (the set of natural numbers), [n] = {1, 2, . . . , n} (the set of natural
numbers from 1 ro n), R (the set of real numbers). For a ﬁnite set A, we denote by |A| the number
of elements in A. Some operations on sets are: (1) union: A ∪B = {x : x ∈A or x ∈B}, (2) in-
tersection : A∩B = {x : x ∈A and x ∈B}, and (3) substraction: A\B = {x : x ∈A and x ̸∈B}.
We say that f is a function from a set A to B, denoted by f : A →B, if it maps any element
of A into an element of B. If B and A are ﬁnite, then the number of possible functions from A to
B is |B||A|. We say that f is one to one if for every x, w ∈A with x ̸= w, f(x) ̸= f(w). If A,B are
ﬁnite, the existence of such a function implies that |A| ≤|B|. We say that f is onto if for every
y ∈B there exists x ∈A such that f(x) = y. If A, B are ﬁnite, the existence of such a function
implies that |A| ≥|B|. We say that f is a permutation if it is both one-to-one and onto. For ﬁnite
A, B, the existence of a permutation from A to B implies that |A| = |B|.
If A,B are sets, then the A × B denotes the set of all ordered pairs ⟨a, b⟩with a ∈A, b ∈B.
Note that if A, B are ﬁnite then |A × B| = |A| · |B|. We can deﬁne similarly A × B × C to be
the set of ordered triples ⟨a, b, c⟩with a ∈A, b ∈B, c ∈C. For n ∈N, we denote by An the set
A × A × · · · × A (n times). We will often use the set {0, 1}n, consisting of all length-n sequences
of bits (i.e., length n strings), and the set {0, 1}∗= ∪n≥0 {0, 1}n ({0, 1}0 has a single element: a
binary string of length zero, which we call the empty word and denote by ε).
A graph G consists of a set V of vertices (which we often assume is equal to the set [n] =
{1, . . . , n} for some n ∈N) and a set E of edges, which consists of unordered pairs (i.e., size two
subsets) of elements in V . We denote the edge {u, v} of the graph by u v. For v ∈V , the neighbors
of v are all the vertices u ∈V such that u v ∈E. In a directed graph, the edges consist of ordered
pairs of vertices, to stress this we sometimes denote the edge ⟨u, v⟩in a directed graph by −→
u v. One
can represent an n-vertex graph G by its adjacency matrix which is an n × n matrix A such that
Ai,j is equal to 1 if the edge −→
i j is present in G ith and is equal to 0 otherwise. One can think of an
undirected graph as a directed graph G that satisﬁes that for every u, v, G contains the edge −→
u v if
and only if it contains the edge −→
v u. Hence, one can represent an undirected graph by an adjecancy
matrix that is symmetric (Ai,j = Aj,i for every i, j ∈[n]).
A Boolean variable is a variable that can be either True or False (we sometimes identify True
with 1 and False with 0). We can combine variables via the logical operations AND (∧), OR (∨)
and NOT (¬, sometimes also denoted by an overline), to obtain Boolean formulae. For example,
the following is a Boolean formulae on the variables u1, u2, u3: (u1∧u2)∨¬(u3∧u1). The deﬁnitions
of the operations are the usual: a ∧b = True if a = True and b = True and is equal to False
Web draft 2007-01-08 21:59

DRAFT
pA.4 (450)
A.3. PROBABILITY THEORY
otherwise; a = ¬a = True if a = False and is equal to False otherwise; a ∨b = ¬(a ∨b). If ϕ
is a formulae in n variables u1, . . . , un, then for any assignment of values u ∈{False, True}n (or
equivalently, {0, 1}n), we denote by ϕ(u) the value of ϕ when its variables are assigned the values
in u. We say that ϕ is satisﬁable if there exists a u such that ϕ(u) = True.
We will often use the quantiﬁers ∀(for all) and ∃(exists). That is, if ϕ is a condition that can
be True or False depending on the value of a variable x, then we write ∀xϕ(x) to denote the
statement that ϕ is True for every possible value that can be assigned to x. If A is a set then we
write ∀x∈Aϕ(x) to denote the statement that ϕ is True for every assignment for x from the set A.
The quantiﬁer ∃is deﬁned similarly. Formally, we say that ∃xϕ(x) holds if and only if ¬(∀x¬ϕ(x))
holds.
A.3
Probability theory
A ﬁnite probability space is a ﬁnite set Ω= {ω1, . . . , ωN} along with a set of numbers p1, . . . , pN ∈
[0, 1] such that PN
i=1 pi = 1. A random element is selected from this space by choosing ωi with
probability pi. If x is chosen from the sample space Ωthen we denote this by x ∈R Ω. If no
distribution is speciﬁed then we use the uniform distribution over the elements of Ω(i.e., pi =
1
N
for every i).
An event over the space Ωis a subset A ⊆Ωand the probability that A occurs, denoted by
Pr[A], is equal to P
i:ωi∈A pi. To give an example, the probability space could be that of all 2n
possible outcomes of n tosses of a fair coin (i.e., Ω= {0, 1}n and pi = 2−n for every i ∈[2n]) and
the event A can be that the number of coins that come up “heads” (or, equivalently, 1) is even. In
this case, Pr[A] = 1/2 (exercise). The following simple bound —called the union bound—is often
used in the book. For every set of events A1, A2, . . . , An,
Pr[∪n
i=1Ai] ≤
n
X
i=1
Pr[Ai].
(1)
Inclusion exclusion principle.
The union bound is a special case of a more general principle.
Indeed, note that if the sets A1, . . . , An are not disjoint then the probability of ∪iAi could be
smaller than P
i Pr[Ai] since we are overcounting elements that appear in more than one set. We
can correct this by substracting P
i<j Pr[Ai ∩Aj] but then we might be undercounting, since we
subtracted elements that appear in at least 3 sets too many times. Continuing this process we get
Claim A.3 (Inclusion-Exclusion principle)
For every A1, . . . , An,
Pr[∪n
i=1Ai] =
n
X
i=1
Pr[Ai] −
X
1≤i<j≤n
Pr[Ai ∩Aj] + · · · + (−1)n−1 Pr[A1 ∩· · · ∩An] .
Moreover, this is an alternating sum which means that if we take only the ﬁrst k summands of the
right hand side, then this upperbounds the left-hand side if k is odd, and lowerbounds it if k is
even.
Web draft 2007-01-08 21:59

DRAFT
A.3. PROBABILITY THEORY
pA.5 (451)
We sometimes use the following corollary of this claim:
Claim A.4
For every events A1, . . . , An,
Pr[∪n
i=1Ai] ≥
n
X
i=1
Pr[Ai] −
X
1≤i<j≤n
Pr[Ai ∩Aj]
Random subsum principle.
The following fact is used often in the book:
Claim A.5 (The random subsum principle)
For x, y ∈{0, 1}n, denote x ⊙y = Pn
i=1 xiyi (mod 2) (that is, x ⊙y is equal to 1 if the number of
i’s such that xi = yi = 1 is odd and equal to 0 otherwise). Then for every y ̸= 0n,
Pr
x∈R{0,1}n[x ⊙y = 1] = 1
2
Proof: Suppose that yj is nonzero. We can think of choosing x as follows: ﬁrst choose all the
coordinates of x other than the jth and only choose the jth coordinate last. After we choose all the
coordinates of x other than the jth, the value P
i:i̸=j xiyi (mod 2) is ﬁxed to be some c ∈{0, 1}.
Regardless of what c is, with probability 1/2 we choose xj = 0, in which case x ⊙y = c and with
probability 1/2 we choose xj = 1, in which case x ⊙y = 1 −c. We see that in any case x Q y will
be equal to 1 with probability 1/2. ■
A.3.1
Random variables and expectations.
A random variable is a mapping from a probability space to R. For example, if Ωis as above, the
set of all possible outcomes of n tosses of a fair coin, then we can denote by X the number of coins
that came up heads.
The expectation of a random variable X, denoted by E[X], is its weighted average. That is,
E[X] = PN
i=1 piX(ωi). The following simple claim follows from the deﬁnition:
Claim A.6 (Linearity of expectation)
For X, Y random variables over a space Ω, denote by X + Y the random variable that maps ω to
X(ω) + Y (ω). Then,
E[X + Y ] = E[X] + E[Y ]
This claims implies that the random variable X from the example above has expectation n/2.
Indeed X = Pn
i=1 Xi where Xi is equal to 1 if the ith coins came up heads and is equal to 0
otherwise. But clearly, E[Xi] = 1/2 for every i.
For a real number α and a random variable X, we deﬁne αX to be the random variable mapping
ω to α · X(ω). Note that E[αX] = αE[X].
Web draft 2007-01-08 21:59

DRAFT
pA.6 (452)
A.3. PROBABILITY THEORY
A.3.2
The averaging argument
We list various versions of the “averaging argument.” Sometimes we give two versions of the same
result, one as a fact about numbers and one as a fact about probability spaces.
Lemma A.7
If a1, a2, . . . , an are some numbers whose average is c then some ai ≥c.
Lemma A.8 (“The Probabilistic Method”)
If X is a random variable which takes values from a ﬁnite set and E[X] = µ then the event “X ≥µ”
has nonzero probability.
Lemma A.9
If a1, a2, . . . , an ≥0 are numbers whose average is c then the fraction of ai’s that are greater than
(resp., at least) kc is less than (resp, at most) 1/k.
Lemma A.10 (“Markov’s inequality”)
Any non-negative random variable X satisﬁes
Pr (X ≥kE[X]) ≤1
k.
Corollary A.11
If a1, a2, . . . , an ∈[0, 1] are numbers whose average is 1 −γ then at least 1 −√γ fraction of them
are at least 1 −√γ.
Can we give any meaningful upperbound on Pr[X < c·E[X]] where c < 1? Yes, if X is bounded.
Lemma A.12
If a1, a2, . . . , an are numbers in the interval [0, 1] whose average is ρ then at least ρ/2 of the ai’s
are at least as large as ρ/2.
Proof: Let γ be the fraction of i’s such that ai ≥ρ/2. Then γ + (1 −γ)ρ/2 must be at least ρ/2,
so γ ≥ρ/2. ■More generally, we have
Lemma A.13
If X ∈[0, 1] and E[X] = µ then for any c < 1 we have
Pr[X ≤cµ] ≤1 −µ
1 −cµ.
Example A.14
Suppose you took a lot of exams, each scored from 1 to 100. If your average score was 90 then in
at least half the exams you scored at least 80.
Web draft 2007-01-08 21:59

DRAFT
A.3. PROBABILITY THEORY
pA.7 (453)
A.3.3
Conditional probability and independence
If we already know that an event B happened, this reduces the space from Ωto Ω∩B, where we
need to scale the probabilities by 1/ Pr[B] so they will sum up to one. Thus, the probability of
an event A conditioned on an event B, denoted Pr[A|B], is equal to Pr[A ∩B]/ Pr[B] (where we
always assume that B has positive probability).
We say that two events A, B are independent if Pr[A∩B] = Pr[A] Pr[B]. Note that this implies
that Pr[A|B] = Pr[A] and Pr[B|A] = Pr[B]. We say that a set of events A1, . . . , An are mutually
independent if for every subset S ⊂[n],
Pr[∩i∈SAi] =
Y
i∈S
Pr[Ai] .
(2)
We say that A1, . . . , An are k-wise independent if (2) holds for every S ⊆[n] with |S| ≤k.
We say that two random variables X, Y are independent if for every x, y ∈R, the events {X = x}
and {Y = y} are independent. We generalize similarly the deﬁnition of mutual independence and
k-wise independence to sets of random variables X1, . . . , Xn. We have the following claim:
Claim A.15
If X1, . . . , Xn are mutually independent then
E[X1 · · · Xn] =
n
Y
i=1
E[Xi]
Proof:
E[X1 · · · Xn] =
X
x
x Pr[X1 · · · Xn = x] =
X
x1,...,xn
x1 · · · xn Pr[X1 = x1 and X2 = x2 · · · and Xn = xn] = (by independence)
X
x1,...,xn
x1 · · · xn Pr[X1 = x1] · · · Pr[Xn = xn] =
(
X
x1
x1 Pr[X1 = x1])(
X
x2
x2 Pr[X2 = x2]) · · · (
X
xn
xn Pr[Xn = xn]) =
n
Y
i=1
E[Xi]
where the sums above are over all the possible real numbers that can be obtained by applying the
random variables or their products to the ﬁnite set Ω. ■
A.3.4
Deviation upperbounds
Under various conditions, one can give upperbounds on the probability of a random variable “stray-
ing too far” from its expectation. These upperbounds are usually derived by clever use of Markov’s
inequality.
The variance of a random variable X is deﬁned to be Var[X] = E[(X −E(X))2]. Note that since
it is the expectation of a non-negative random variable, Var[X] is always non-negative. Also, using
Web draft 2007-01-08 21:59

DRAFT
pA.8 (454)
A.3. PROBABILITY THEORY
linearity of expectation, we can derive that Var[X] = E[X2] −(E[X])2. The standard deviation of
a variable X is deﬁned to be
p
Var[X].
The ﬁrst bound is Chebyshev’s inequality, useful when only the variance is known.
Lemma A.16 (Chebyshev inequality)
If X is a random variable with standard deviation σ, then for every k > 0,
Pr[|X −E[X]| > kσ] ≤1/k2
Proof: Apply Markov’s inequality to the random variable (X −E[X])2, noting that by deﬁnition
of variance, E[(X −E[X])2] = σ2. ■
Chebyshev’s inequality is often useful in the case that X is equal to Pn
i=1 Xi for pairwise
independent random variables X1, . . . , Xn. This is because of the following claim, that is left as an
exercise:
Claim A.17
If X1, . . . , Xn are pairwise independent then
Var(
n
X
i=1
Xi) =
n
X
i=1
Var(Xi)
The next inequality has many names, and is widely known in theoretical computer science as
the Chernoﬀbound. It considers scenarios of the following type. Suppose we toss a fair coin n
times. The expected number of heads is n/2. How tightly is this number concentrated? Should we
be very surprised if after 1000 tosses we have 625 heads? The bound we present is slightly more
general, since it concerns n diﬀerent coin tosses of possibly diﬀerent expectations (the expectation
of a coin is the probability of obtaining “heads”; for a fair coin this is 1/2). These are sometimes
known as Poisson trials.
Theorem A.18 (“Chernoff” bounds)
Let X1, X2, . . . , Xn be mutually independent random variables over {0, 1} (i.e., Xi can be either 0
or 1) and let µ = Pn
i=1 E[Xi]. Then for every δ > 0,
Pr[
n
X
i=1
Xi ≥(1 + δ)µ] ≤

eδ
(1 + δ)(1+δ)
µ
.
(3)
Pr[
n
X
i=1
≤(1 −δ)µ] ≤

e−δ
(1 −δ)(1−δ)
µ
.
(4)
Often, what we use need is only the corollary that under the above conditions, for every c > 0
Pr
"
n
X
i=1
Xi −µ
 ≥cµ
#
≤2−c2n/2
Web draft 2007-01-08 21:59

DRAFT
A.3. PROBABILITY THEORY
pA.9 (455)
Proof: Surprisingly, the Chernoﬀbound is also proved using the Markov inequality. We only
prove the ﬁrst inequality; a similar proof exists for the second. We introduce a positive dummy
variable t, and observe that
E[exp(tX)] = E[exp(t
X
i
Xi)] = E[
Y
i
exp(tXi)] =
Y
i
E[exp(tXi)],
(5)
where exp(z) denotes ez and the last equality holds because the Xi r.v.s are independent. Now,
E[exp(tXi)] = (1 −pi) + piet,
therefore,
Y
i
E[exp(tXi)] =
Y
i
[1 + pi(et −1)] ≤
Y
i
exp(pi(et −1))
= exp(
X
i
pi(et −1)) = exp(µ(et −1)),
(6)
as 1 + x ≤ex. Finally, apply Markov’s inequality to the random variable exp(tX), viz.
Pr[X ≥(1 + δ)µ] = Pr[exp(tX) ≥exp(t(1 + δ)µ)] ≤
E[exp(tX)]
exp(t(1 + δ)µ) = exp((et −1)µ)
exp(t(1 + δ)µ),
using lines (5) and (6) and the fact that t is positive. Since t is a dummy variable, we can choose
any positive value we like for it. Simple calculus shows that the right hand side is minimized for
t = ln(1 + δ) and this leads to the theorem statement. ■
By the way, if all n coin tosses are fair (Heads has probability 1/2) then the the probability of
seeing N heads where |N −n/2| > a√n is at most e−a2/2. The chance of seeing at least 625 heads
in 1000 tosses of an unbiased coin is less than 5.3 × 10−7.
A.3.5
Some other inequalities.
Jensen’s inequality.
The following inequality, generalizing the inequality E[X2] ≥E[X]2, is also often useful:
Claim A.19
We say that f : R →R is convex if for every p ∈[0, 1] and x, y ∈R, f(px + (1 −p)y) ≤
p·f(x)+(1−p)·f(y). Then, for every random variable X and convex function f, f(E[X]) ≤E[f(X)].
Approximating the binomial coeﬃcient
Of special interest is the Binomial random variable Bn denoting the number of coins that come up
“heads” when tossing n fair coins. For every k, Pr[Bn = k] = 2−n n
k

where
 n
k

=
n!
k!(n−k)! denotes
the number of size-k subsets of [n]. Clearly,
 n
k

≤nk, but sometimes we will need a better estimate
for
 n
k

and use the following approximation:
Web draft 2007-01-08 21:59

DRAFT
pA.10 (456)
A.4. FINITE FIELDS AND GROUPS
Claim A.20
For every n, k < n,
n
k
k
≤
n
k

≤
ne
k
k
The best approximation can be obtained via Stirling’s formula:
Lemma A.21 (Stirling’s formula)
For every n,
√
2πn
n
e
n
e
1
12n+1 < n! <
√
2πn
n
e
n
e
1
12n
It can be proven by taking natural logarithms and approximating ln n! = ln(1 · 2 · · · n) =
Pn
i=1 ln i by the integral
R n
1 ln x dx = n ln n −n + 1. It implies the following corollary:
Corollary A.22
For every n ∈N and α ∈[0, 1],
 n
αn

= (1 ± O(n−1))
1
√
2πnα(1−α)2H(α)n
where H(α) = α log(1/α) + (1 −α) log(1/(1 −α)) and the constants hidden in the O notation are
independent of both n and α.
More useful estimates.
The following inequalities can be obtained via elementary calculus:
• For every x ≥1,
 1 −1
x
x ≤1
e ≤

1 −
1
x+1
x
• For every k, Pn
i=1 ik = Θ

nk+1
k+1

• For every k > 1, P∞
i=1 n−k < O(1).
• For every c, ϵ > 0, P∞
i=1
nc
(1+ϵ)n < O(1).
• For every n, Pn
i=1 = ln n ± O(1)
A.4
Finite ﬁelds and groups
A ﬁeld is a set F that has an addition (+) and multiplication (·) operations that behave in the
expected way: satisfy associative, commutative and distributive laws, have both additive and
multiplicative inverses, and neutral elements 0 and 1 for addition and multiplication respectively.
Familiar ﬁelds are the real numbers (R), the rational numbers (Q) and the complex numbers (C),
but there are also ﬁnite ﬁelds.
If q is a prime, then we denote by GF(q) the ﬁeld consisting of the elements {0, . . . , q −1} with
addition and multiplication performed modulo q. For example, the numbers {0, . . . , 6} yield a ﬁeld
Web draft 2007-01-08 21:59

DRAFT
A.4. FINITE FIELDS AND GROUPS
pA.11 (457)
if addition and multiplication are performed modulo 7. We leave it to the reader to verify GF(q) is
indeed a ﬁeld for every prime q. The simplest example for such a ﬁeld is the ﬁeld GF(2) consisting
of {0, 1} where multiplication is the AND (∧) operation and addition is the XOR operation.
Every ﬁnite ﬁeld F has a number ℓsuch that for every x ∈F, x + x + · · · + x (ℓtimes) is equal
to the zero element of F (exercise). This number ℓis called the characteristic of F. For every prime
q, the characteristic of GF(q) is equal to q.
A.4.1
Non-prime ﬁelds.
One can see that if n is not prime, then the set {0, . . . , n −1} with addition and multiplication
modulo n is not a ﬁeld, as there exist two non-zero elements x, y in this set such that x · y = n = 0
(mod n). Nevertheless, there are ﬁnite ﬁelds of size n for non-prime n. Speciﬁcally, for every prime
q, and k ≥1, there exists a ﬁeld of qk elements, which we denote by GF(qk). We will very rarely
need to use such ﬁelds in this book, but still provide an outline of their construction below.
For every prime q and k there exists an irreducible degree k polynomial P over the ﬁeld GF(q)
(P is irreducible if it cannot be expressed as the product of two polynomials P ′, P ′′ of lower degree).
We then let GF(qk) be the set of all k−1-degree polynomials over GF(q). Each such polynomial can
be represented as a vector of its k coeﬃcients. We perform both addition and multiplication modulo
the polynomial P. Note that addition corresponds to standard vector addition of k-dimensional
vectors over GF(q), and both addition and multiplication can be easily done in poly(n, log q) time
(we can reduce a polynomial S modulo a polynomial P using a similar algorithm to long division
of numbers). It turns out that no matter how we choose the irreducible polynomial P, we will get
the same ﬁeld, up to renaming of the elements. There is a deterministic poly(q, k)-time algorithm
to obtain an irreducible polynomial of degree k over GF(q). There are also probabilistic algorithms
(and deterministic algorithms whose analysis relies on unproven assumptions) that obtain such a
polynomial in poly(log q, k) time.
For us, the most important example of a ﬁnite ﬁeld is GF(2k), which consists of the set {0, 1}k,
with addition being component-wise XOR, and multiplication being polynomial multiplication via
some irreducible polynomial which we can ﬁne in poly(k) time. In fact, we will mostly not even be
interested in the multiplicative structure of GF(2k) and only use the addition operation (i.e., use
it as the vector space GF(2)k, see below).
A.4.2
Groups.
A group is a set that only has a single operation, say ⋆, that is associative and has an inverse. That
is, (G, ⋆) is a group if
1. For every a, b, c ∈G , (a ⋆b) ⋆c = a ⋆(b ⋆c)
2. There exists a special element id ∈G such that a ⋆id = a for every a ∈G, and for every
a ∈G there exists b ∈G such that a ⋆b = b ⋆a = id.
If G is a ﬁnite group, it is known that for every a ∈G, a ⋆a ⋆· · · ⋆a (|G| times) is equal to
the element id. A group is called commutative or Abelian if its operation satisﬁes a ⋆b = b ⋆a for
every a, n ∈G. For every number n ≥2, the set {0, . . . , n −1} with the operation being addition
Web draft 2007-01-08 21:59

DRAFT
pA.12 (458)
A.5. VECTOR SPACES AND HILBERT SPACES
modulo n is an Abelian group. Also, the set {k : k ∈[n −1], gcd(k, n) = 1} with the operation
being multiplication modulo n is an Abelian group.
If F is a ﬁeld and k ≥1, then the set of k-dimensional vectors of F (i.e., Fk) together with
the operation of componentwise addition, yields an Abelian group. As mentioned above, the most
interesting special case for us is the group GF(2)k for some k. Note that in this group the identity
element is the vector 0k and for every x ∈GF(2)k, x + x = 0k. This group is often referred to as
the Boolean cube.
A.5
Vector spaces and Hilbert spaces
A.6
Polynomials
We list some basic facts about univariate polynomials.
Theorem A.23
A nonzero polynomial of degree d has at most d distinct roots.
Proof: Suppose p(x) = Pd
i=0 cixi has d + 1 distinct roots α1, . . . , αd+1 in some ﬁeld F. Then
d
X
i=0
αi
j · ci = p(αj) = 0,
for j = 1, . . . , d + 1. This means that the system Ay = 0 with
A =




1
α1
α2
1
. . .
αd
1
1
α2
α2
2
. . .
αd
2
. . . . . . . . . . . . . . . . . . .
1
αd+1
α2
d+1
. . .
αd
d+1




has a solution y = c. The matrix A is a Vandermonde matrix, and it can be shown that
det A =
Y
i>j
(αi −αj),
which is nonzero for distinct αi. Hence rankA = d + 1. The system Ay = 0 has therefore only a
trivial solution — a contradiction to c ̸= 0. ■
Theorem A.24
For any set of pairs (a1, b1), . . . , (ad+1, bd+1) there exists a unique polynomial g(x) of degree at most
d such that g(ai) = bi for all i = 1, 2, . . . , d + 1.
Proof: The requirements are satisﬁed by Lagrange Interpolating Polynomial:
d+1
X
i=1
bi ·
Q
j̸=i(x −aj)
Q
j̸=i(ai −aj).
Web draft 2007-01-08 21:59

DRAFT
A.6. POLYNOMIALS
pA.13 (459)
If two polynomials g1(x), g2(x) satisfy the requirements then their diﬀerence p(x) = g1(x)−g2(x) is
of degree at most d, and is zero for x = a1, . . . , ad+1. Thus, from the previous theorem, polynomial
p(x) must be zero and polynomials g1(x), g2(x) identical. ■
The following elementary result is usually attributed to Schwartz and Zippel in the computer
science community, though it was certainly known earlier (see e.g. DeMillo and Lipton [?]).
Lemma A.25
If a polynomial p(x1, x2, . . . , xm) over F = GF(q) is nonzero and has total degree at most d, then
Pr[p(a1..am) ̸= 0] ≥1 −d
q ,
where the probability is over all choices of a1..am ∈F.
Proof: We use induction on m. If m = 1 the statement follows from Theorem A.23. Suppose the
statement is true when the number of variables is at most m −1. Then p can be written as
p(x1, x2, . . . , xm) =
d
X
i=0
xi
1pi(x2, . . . , xm),
where pi has total degree at most d −i. Since p is nonzero, at least one of pi is nonzero. Let k be
the largest i such that pi is nonzero. Then by the inductive hypothesis,
Pr
a2,a3,...,am[pi(a2, a3, . . . , am) ̸= 0] ≥1 −d −k
q
.
Whenever pi(a2, a3, . . . , am) ̸= 0, p(x1, a2, a3, . . . , am) is a nonzero univariate polynomial of
degree k, and hence becomes 0 only for at most k values of x1. Hence
Pr[p(a1..am) ̸= 0] ≥(1 −k
q )(1 −d −k
q
) ≥1 −d
q ,
and the induction is completed. ■
Web draft 2007-01-08 21:59

DRAFT
pA.14 (460)
A.6. POLYNOMIALS
Web draft 2007-01-08 21:59

DRAFT
Bibliography
[AA02]
M.Vyalyi A.Kitaev and A.Shen. Classical and Quantum computation. AMS Press, 2002.
[Aar05]
Aaronson.
NP-complete problems and physical reality.
SIGACTN: SIGACT News
(ACM Special Interest Group on Automata and Computability Theory), 36, 2005.
[AB87]
Noga Alon and Ravi B. Boppana. The monotone circuit complexity of boolean functions.
Combinatorica, 7(1):1–22, 1987.
[AD97]
M. Ajtai and C. Dwork. A public-key cryptosystem with worst-case/average-case equiv-
alence. In Proceedings of the 29th Annual ACM Symposium on the Theory of Computing
(STOC ’97), pages 284–293, New York, 1997. Association for Computing Machinery.
[Adl78]
Leonard Adleman. Two theorems on random polynomial time. In 19th Annual Sympo-
sium on Foundations of Computer Science, pages 75–83, Ann Arbor, Michigan, 1978.
IEEE.
[AG94]
Eric Allender and Vivek Gore. A uniform circuit lower bound for the permanent. SIAM
Journal on Computing, 23(5):1026–1049, October 1994.
[Ajt83]
M. Ajtai. Σ1
1-formulae on ﬁnite structures. Annals of Pure and Applied Logic, 24:1–48,
1983.
[Ajt96]
M. Ajtai. Generating hard instances of lattice problems (extended abstract). In Pro-
ceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing,
pages 99–108, Philadelphia, Pennsylvania, 1996.
[AKL+79] Romas Aleliunas, Richard M. Karp, Lipton Lipton, Laszlo Lov´asz, and Charles Rackoﬀ.
Random walks, universal traversal sequences, and the complexity of maze problems. In
20th Annual Symposium on Foundations of Computer Science, pages 218–223, San
Juan, Puerto Rico, 29–31 October 1979. IEEE.
[AKS87]
M. Ajtai, J. Komlos, and E. Szemeredi. Deterministic simulation in LOGSPACE. In
ACM, editor, Proceedings of the nineteenth annual ACM Symposium on Theory of
Computing, New York City, May 25–27, 1987, pages 132–140, New York, NY, USA,
1987. ACM Press.
Web draft 2007-01-08 21:59
Complexity Theory: A Modern Approach. © 2006 Sanjeev Arora and Boaz Barak. References and attributions are
still incomplete.
pA.1 (461)

DRAFT
pA.2 (462)
BIBLIOGRAPHY
[ALM+98] Sanjeev Arora, Carsten Lund, Rajeev Motwani, Madhu Sudan, and Mario Szegedy.
Proof veriﬁcation and the hardness of approximation problems. Journal of the ACM,
45(3):501–555, May 1998. Prelim version IEEE FOCS 1992.
[Aro98]
Arora. Polynomial time approximation schemes for euclidean traveling salesman and
other geometric problems. JACM: Journal of the ACM, 45, 1998.
[AS98]
Sanjeev Arora and Shmuel Safra. Probabilistic checking of proofs: A new characteri-
zation of NP. Journal of the ACM, 45(1):70–122, January 1998. Prelim version IEEE
FOCS 1982.
[AS00]
Noga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 2000.
[AUY83]
A.V. Aho, J.D. Ullman, and M. Yannakakis. On notions of information transfer in
VLSI circuits. In Proceedings of the Fifteenth Annual ACM Symposium on Theory of
Computing, pages 133–139, 25–27 April 1983.
[Bab90]
L´aszl´o Babai. E-mail and the unexpected power of interaction. In Proceedings, Fifth
Annual Structure in Complexity Theory Conference, pages 30–44, Barcelona, Spain,
8–11 July 1990. IEEE Computer Society Press.
[Bar02]
Boaz Barak. A probabilistic-time hierarchy theorem for “Slightly Non-uniform” algo-
rithms. Lecture Notes in Computer Science, 2483:194–??, 2002.
[BBR94]
David A. Mix Barrington, Richard Beigel, and Rudich Rudich. Representing Boolean
functions as polynomials modulo composite numbers.
Computational Complexity,
4(4):367–382, 1994. Prelim. version in ACM STOC 1992.
[BdW02]
H. Buhrman and R. de Wolf. Complexity measures and decision tree complexity: A
survey. Theoretical Computer Science, 288:21–43, 2002.
[Bec91]
J. Beck. An algorithmic approach to the lov`asz local lemma. Random Structures and
Algorithms, 2(4):367–378, 1991.
[Bel64]
John S. Bell. On the Einstein-Podolsky-Rosen paradox. Physics, 1(3):195–290, 1964.
[BF90]
Donald Beaver and Joan Feigenbaum. Hiding instances in multioracle queries. In 7th
Annual Symposium on Theoretical Aspects of Computer Science, volume 415 of lncs,
pages 37–48, Rouen, France, 22–24 February 1990. Springer.
[BFL91]
L´aszl´o Babai, Lance Fortnow, and Lund Lund. Non-deterministic exponential time has
two-prover interactive protocols. Computational Complexity, 1:3–40, 1991.
[BFNW93] L´aszl´o Babai, Lance Fortnow, Noam Nisan, and Avi Wigderson.
BPP has subex-
ponential time simulations unless EXPTIME has publishable proofs. Computational
Complexity, 3(4):307–318, 1993.
Web draft 2007-01-08 21:59

DRAFT
BIBLIOGRAPHY
pA.3 (463)
[BFT98]
H. Buhrman, L. Fortnow, and T. Thierauf. Nonrelativizing separations. In Proceedings
of the 13th Annual IEEE Conference on Computational Complexity (CCC-98), pages
8–12, Los Alamitos, June 15–18 1998. IEEE Computer Society.
[BGS75]
Theodore Baker, John Gill, and Robert Solovay.
Relativizations of the P =? NP
question. SIAM Journal on Computing, 4(4):431–442, December 1975.
[BK95]
M. Blum and S. Kannan. Designing programs that check their work. Journal of the
ACM, 42(1):269–291, January 1995.
[Blu67]
M. Blum.
A machine-independent theory of the complexity of recursive functions.
JACM: Journal of the ACM, 14, 1967.
[Blu82]
M. Blum. Coin ﬂipping by telephone. In Proc. 1982 IEEE COMPCON, High Technology
in the Information Age, pages 133–137, 1982. See also SIGACT News, Vol 15, No. 1,
1983.
[Blu84]
M. Blum. Independent unbiased coin ﬂips from a correlated biased source: a ﬁnite state
Markov chain. In 25th Annual Symposium on Foundations of Computer Science, pages
425–433. IEEE, 24–26 October 1984.
[BM84]
M. Blum and S. Micali. How to generate cryptographically strong sequences of pseudo-
random bits. SIAM Journal on Computing, 13(4):850–864, Nov 1984.
[BM88]
L´aszl´o Babai and Shlomo Moran. Arthur-Merlin games: A randomized proof system,
and a hierarchy of complexity classes.
Journal of Computer and System Sciences,
36(2):254–276, April 1988.
[BNS]
L. Babai, N. Nisan, and M. Szegedy. Multiparty protocols, pseudorandom generators
for logspace, and time-space trade-oﬀs.
[Bru04]
Colin Bruce. Schrodinger’s Rabbits: Entering The Many Worlds Of Quantum. Joseph
Henry Press, 2004.
[BS90]
R.B. Boppana and M. Sipser.
The complexity of ﬁnite functions.
In Handbook of
Theoretical Computer Science, Ed. Jan van Leeuwen, Elsevier and MIT Press (Volume
A (= “1”): Algorithms and Complexity), volume 1. 1990.
[BS96]
Eric Bach and Jeﬀrey Shallit. Algorithmic Number Theory — Eﬃcient Algorithms,
volume I. MIT Press, Cambridge, USA, 1996.
[BT94]
Richard Beigel and Jun Tarui. On ACC. Computational Complexity, 4(4):350–366,
1994. Prelim version in IEEE FOCS 1991.
[BV97]
E. Bernstein and U. Vazirani. Quantum complexity theory. SIAM Journal on Comput-
ing, 26(5):1411–1473, October 1997. Prelim version in STOC 1993.
[Chu36]
Alonzo Church.
An Unsolvable Problem of Elementary Number Theory.
Amer. J.
Math., 58(2):345–363, 1936.
Web draft 2007-01-08 21:59

DRAFT
pA.4 (464)
BIBLIOGRAPHY
[CK00]
Pierluigi Crescenzi and Viggo Kann. A compendium of np optimization problems. http:
//www.nada.kth.se/~viggo/problemlist/, 2000. Website tracking the tractability of
many NP problems.
[CLRS01]
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms.
MIT Press, Cambridge, MA, 2001.
[Cob64]
A. Cobham.
The intrinsic computational diﬃculty of functions.
In Yehoshua Bar-
Hillel, editor, Proceedings of the 1964 International Congress for Logic, Methodology,
and Philosophy of Science, pages 24–30. Elsevier/North-Holland, 1964.
[Coo71]
S. A. Cook. The complexity of theorem proving procedures. In Proc. 3rd Ann. ACM
Symp. Theory of Computing, pages 151–158, NY, 1971. ACM.
[Coo73]
Stephen A. Cook. A hierarchy for nondeterministic time complexity. Journal of Com-
puter and System Sciences, 7(4):343–353, August 1973.
[CW89]
Aviad Cohen and Avi Wigderson. Dispersers, deterministic ampliﬁcation, and weak
random sources (extended abstract). In 30th Annual Symposium on Foundations of
Computer Science, pages 14–19, 30 October–1 November 1989.
[CW90]
Don Coppersmith and Shmuel Winograd. Matrix multiplication via arithmetic progres-
sions. Journal of Symbolic Computation, 9(3):251–280, March 1990.
[Dav65]
Martin Davis. The Undecidable. Dover Publications, 1965.
[Deu85]
D. Deutsch. Quantum theory, the Church-Turing principle and the universal quantum
computer. Proceedings of the Royal Society of London Ser. A, A400:97–117, 1985.
[DH76]
W. Diﬃe and M. E. Hellman. New directions in cryptography. IEEE Transactions on
Information Theory, 22(5):644–654, November 1976.
[DJ92]
D. Deutsch and R. Jozsa. Rapid solution of problems by quantum computation. Proc
Roy Soc Lond A, 439:553–558, October 1992.
[DK00]
D.Z. Du and K. I. Ko. Theory of Computational Complexity. Wiley, 2000.
[DPV06]
S. Dasgupta, C.H. Papadimitriou, and U.V. Vazirani. Algorithms. McGraw Hill, 2006.
Draft available from the authors’ webpage.
[Edm65]
J. Edmonds. Paths, trees, and ﬂowers. Canad. J. Math, 17:449–467, 1965.
[ER60]
P. Erd˝os and R. Rado. Intersection theorems for systems of sets. J. London Math. Soc.,
35:85–90, 1960.
[Fey82]
R. Feynman. Simulating physics with computers. International Journal of Theoretical
Physics, 21(6&7):467–488, 1982.
Web draft 2007-01-08 21:59

DRAFT
BIBLIOGRAPHY
pA.5 (465)
[FSS84]
M. Furst, J. Saxe, and M. Sipser. Parity, circuits, and the polynomial time hierarchy.
Mathematical Systems Theory, 17:13–27, 1984. Prelim version in IEEE FOCS 1981.
[GG00]
O. Goldreich and S. Goldwasser. On the limits of nonapproximability of lattice prob-
lems. JCSS: Journal of Computer and System Sciences, 60, 2000.
[GGM86]
O. Goldreich, S. Goldwasser, and S. Micali. How to construct random functions. Journal
of the ACM, 33(4):792–807, October 1986. Prelim. version in IEEE FOCS 1984.
[Gil74]
John T. Gill. Computational complexity of probabilistic Turing machines. In ACM,
editor, Conference record of sixth annual ACM Symposium on Theory of Computing:
papers presented at the symposium, Seattle, Washington, April 30–May 2, 1974, pages
91–95, New York, NY, USA, 1974. ACM Press.
[Gil77]
John Gill. Computational complexity of probabilistic Turing machines. SIAM Journal
on Computing, 6(4):675–695, December 1977.
[GJ79]
M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman and Company, New York, New York, 1979.
[GM84]
ShaﬁGoldwasser and Silvio Micali. Probabilistic encryption. Journal of Computer and
System Sciences, 28(2):270–299, April 1984.
[GMR89]
S. Goldwasser, S. Micali, and C. Rackoﬀ. The knowledge complexity of interactive proof
systems. SIAM Journal on Computing, 18(1):186–208, February 1989. Prelim version
in ACM STOC 1985.
[GMW87] O. Goldreich, S. Micali, and A. Wigderson. How to play any mental game — A com-
pleteness theorem for protocols with honest majority. In ACM, editor, Proceedings of
the nineteenth annual ACM Symposium on Theory of Computing, New York City, May
25–27, 1987, pages 218–229, New York, 1987. ACM Press.
[Gol04]
Oded Goldreich. Foundations of Cryptography, Volumes 1 and 2. Cambridge University
Press, 2001,2004.
[GS87]
S. Goldwasser and M. Sipser.
Private coins versus public coins in interactive proof
systems. In S. Micali, editor, Randomness and Computation. JAI Press, Greenwich,
CT, 1987.
Extended Abstract in Proc. 18th ACM Symp. on Theory of Computing,
1986.
[Has86]
Johan Hastad. Almost optimal lower bounds for small depth circuits. In Proceedings of
the Eighteenth Annual ACM Symposium on Theory of Computing, pages 6–20, Berkeley,
California, 28–30 May 1986.
[HILL99]
Johan H˚astad, Russell Impagliazzo, Levin Levin, and Michael Luby. A pseudorandom
generator from any one-way function. SIAM Journal on Computing, 28(4):1364–1396,
August 1999.
Web draft 2007-01-08 21:59

DRAFT
pA.6 (466)
BIBLIOGRAPHY
[HMU01]
J. E. Hopcroft, R. Motwani, and J. D. Ullman. “Introduction to Automata Theory,
Language, and Computation”. Addison–Wesley, 2nd edition edition, 2001.
[Hod83]
Andrew Hodges. Alan Turing: the enigma. Burnett Books, London, 1983.
[HS65]
J. Hartmanis and R. E. Stearns.
On the computational complexity of algorithms.
Transactions of the American Mathematical Society, 117:285–306, 1965.
[HS66]
F. C. Hennie and R. E. Stearns. Two-tape simulation of multitape Turing machines.
Journal of the ACM, 13(4):533–546, October 1966.
[HVV04]
Alexander Healy, Salil Vadhan, and Emanuele Viola. Using nondeterminism to amplify
hardness.
In Proceedings of the thirty-sixth annual ACM Symposium on Theory of
Computing (STOC-04), pages 192–201, New York, June 13–15 2004. ACM Press.
[ILL89]
Russell Impagliazzo, Leonid A. Levin, and Luby Luby. Pseudo-random generation from
one-way functions. In Proceedings of the 21st Annual Symposium on Theory of Comput-
ing (STOC ’89), pages 12–24, New York, May 1989. ACM Association for Computing
Machinery.
[Imp95]
Russell Impagliazzo. A personal view of average-case complexity. In Structure in Com-
plexity Theory Conference, pages 134–147, 1995.
[INW94]
Russell Impagliazzo, Noam Nisan, and Avi Wigderson. Pseudorandomness for network
algorithms. In Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory
of Computing, pages 356–364, 23–25 May 1994.
[IW01]
Russell Impagliazzo and Avi Wigderson. Randomness vs time: Derandomization under
a uniform assumption. JCSS: Journal of Computer and System Sciences, 63, 2001.
Prelim. version in IEEE FOCS 1998.
[Kan82]
R. Kannan. Circuit-size lower bounds and non-reducibility to sparse sets. Information
and Control, 55(1–3):40–56, 1982. Prelim version in IEEE FOCS 1981.
[Kar72]
R. M. Karp. Reducibility among combinatorial problems. In R. E. Miller and J. W.
Thatcher, editors, Complexity of Computer Computations, pages 85–103. Plenum, New
York, 1972.
[KI03]
Valentine Kabanets and Russell Impagliazzo. Derandomizing polynomial identity tests
means proving circuit lower bounds. In ACM, editor, Proceedings of the Thirty-Fifth
ACM Symposium on Theory of Computing, San Diego, CA, USA, June 9–11, 2003,
pages 355–364, New York, NY, USA, 2003. ACM Press.
[KL82]
R. Karp and R. Lipton.
Turing machines that take advice.
L’ Ensignement
Math´ematique, 28:191–210, 1982.
[KN97]
E. Kushilevitz and N. Nisan. Communication Complexity. Cambridge University Press,
1997.
Web draft 2007-01-08 21:59

DRAFT
BIBLIOGRAPHY
pA.7 (467)
[Koz97]
Dexter C. Kozen. Automata and Computability. Springer-Verlag, Berlin, 1997.
[KRW95]
Mauricio Karchmer, Ran Raz, and Avi Wigderson.
Super-logarithmic depth lower
bounds via the direct sum in communication complexity. Computational Complexity,
5(3/4):191–204, 1995.
[KT06]
Jon Kleinberg and ´Eva Tardos.
Algorithm Design.
Pearson Studium, Boston-San
Francisco-New
York-London-Toronto-Sydney-Tokyo-Singapore-Madrid-Mexico
City-
Munich-Paris-Cape Town-Hong Kong-Montreal, 2006.
[KUW86]
R.M. Karp, E. Upfal, and A. Wigderson. Constructing a perfect matching is in random
NC. COMBINAT: Combinatorica, 6:35–48, 1986.
[KV94]
M. Kearns and L. Valiant. Cryptographic limitations on learning Boolean formulae and
ﬁnite automata. J. ACM, 41(1):67–95, 1994. Prelim. version in 21th STOC, 1989.
[KVVY93] R. Kannan, H. Venkateswaran, V. Vinay, and A. C. Yao. A circuit-based proof of Toda’s
theorem. Information and Computation, 104(2):271–276, 1993.
[KW90]
Mauricio Karchmer and Avi Wigderson.
Monotone circuits for connectivity require
super-logarithmic depth. SIAM Journal on Discrete Mathematics, 3(2):255–265, May
1990. Prelim version in ACM STOC 1988.
[Lea05]
David Leavitt. The man who knew too much: Alan Turing and the invention of the
computer. Great discoveries. W. W. Norton & Co., New York, NY, USA, 2005.
[Lei92]
F. Thomson Leighton. Introduction to Parallel Algorithms and Architectures: Arrays •
Trees • Hypercubes. Morgan Kaufmann, 1992.
[Lev]
L. Levin.
The tale of one-way functions (english translation).
Problemy Peredachi
Informatsii, 39(1).
[Lev73]
L. Levin. Universal sequential search problems. PINFTRANS: Problems of Information
Transmission (translated from Problemy Peredachi Informatsii (Russian)), 9, 1973.
[LFK92]
C. Lund, L. Fortnow, and H. Karloﬀ. Algebraic methods for interactive proof systems.
Journal of the ACM, 39(4):859–868, October 1992.
[Lip90]
Richard J. Lipton. Eﬃcient checking of computations. In 7th Annual Symposium on
Theoretical Aspects of Computer Science, volume 415 of lncs, pages 207–215, Rouen,
France, 22–24 February 1990. Springer.
[Lip91]
R. J. Lipton.
New directions in testing.
In Distributed Computing and Cryptogra-
phy, volume 2 of DIMACS Series in Discrete Mathematics and Theoretical Computer
Science, pages 191–202. American Mathematics Society, 1991.
[LL06]
Eric Lehman and Tom Leighton. Mathematics for computer science. Technical report,
2006. Lecture notes.
Web draft 2007-01-08 21:59

DRAFT
pA.8 (468)
BIBLIOGRAPHY
[LLKS85]
E. L. Lawler, J. K. Lenstra, A. H. G. Rinnooy Kan, and D. B. Shmoys, editors. The
Traveling Salesman Problem. John Wiley, New York, 1985.
[LMSS56]
K. de Leeuw, E. F. Moore, C. E. Shannon, and N. Shapiro. Computability by proba-
bilistic machines. In C. E. Shannon and J. McCarthy, editors, Automata Studies, pages
183–212. 1956.
[Lov78]
L. Lov`asz. Kneser’s conjecture, chromatic number, and homotopy. J. Combin. Theory
Ser. A, 1978.
[Lov79]
L. Lov`asz. On determinants, matchings, and random algorithms. In L. Budach, editor,
Fundamentals of Computation Theory FCT ’79, pages 565–574, Berlin, 1979. Akademie-
Verlag.
[LR01]
Oded Lachish and Ran Raz. Explicit lower bound of 4.5n−o(n) for Boolean circuits. In
ACM, editor, Proceedings of the 33rd Annual ACM Symposium on Theory of Comput-
ing: Hersonissos, Crete, Greece, July 6–8, 2001, pages 399–408, New York, NY, USA,
2001. ACM Press.
[LRVW03] Chi-Jen Lu, Omer Reingold, Salil Vadhan, and Wigderson Wigderson.
Extractors:
optimal up to constant factors. In ACM, editor, Proceedings of the Thirty-Fifth ACM
Symposium on Theory of Computing, San Diego, CA, USA, June 9–11, 2003, pages
602–611, New York, NY, USA, 2003. ACM Press.
[MR95]
R. Motwani and P. Raghavan. Randomized algorithms. Cambridge University Press,
1995.
[MS82]
K. Mehlhorn and E.M. Schmidt. Las Vegas is better than determinism in VLSI and
distributed computing (extended abstract). In Proceedings of the Fourteenth Annual
ACM Symposium on Theory of Computing, pages 330–337, San Francisco, California,
5–7 May 1982.
[MU05]
Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algo-
rithms and Probabilistic Analysis. Cambridge University Press, 2005.
[MVV87]
K. Mulmuley, U. V. Vazirani, and V. V. Vazirani.
Matching is as easy as matrix
inversion. Combinatorica, 7:105–113, 1987.
[MZ02]
Jiri Matousek and G¨unter M. Ziegler.
Topological lower bounds for the chromatic
number: A hierarchy, November 24 2002. Comment: 16 pages, 1 ﬁgure. Jahresbericht
der DMV, to appear.
[NC00]
M. Nielsen and I. Chuang. Quantum Computation and Quantum Information. Cam-
bridge University Press, 2000.
[Nis92]
N. Nisan. Pseudorandom generators for space-bounded computation. Combinatorica,
12, 1992. Prelim version in ACM STOC 1990.
Web draft 2007-01-08 21:59

DRAFT
BIBLIOGRAPHY
pA.9 (469)
[NW94]
Noam Nisan and Avi Wigderson. Hardness vs randomness. Journal of Computer and
System Sciences, 49(2):149–167, October 1994. Prelim version in IEEE FOCS 1988.
[NZ96]
Noam Nisan and David Zuckerman. Randomness is linear in space. Journal of Computer
and System Sciences, 52(1):43–52, 1996. Prelim. version in ACM STOC 1993.
[O’D04]
O’Donnell. Hardness ampliﬁcation within NP. JCSS: Journal of Computer and System
Sciences, 69, 2004.
[Pap85]
Christos H. Papadimitriou. Games against nature. J. Comput. System Sci., 31(2):288–
301, 1985.
[Pap94]
Christos H. Papadimitriou. Computatational Complexity. Addison-Wesley, Reading,
Mass., 1994.
[PS84]
Christos H. Papadimitriou and Michael Sipser. Communication complexity. Journal
of Computer and System Sciences, 28(2):260–269, April 1984. Prelim version in ACM
STOC 1982.
[PV06]
Christos Papadimitriou and Umesh Vazirani. Lecture notes on discrete mathematics
for computer science, 2006. Available from the authors’ home pages.
[PY82]
C. H. Papadimitriou and M. Yannakakis. The complexity of facets (and some facets of
complexity). Journal of Computer and System Sciences, 28(2):244–259, 1982.
[PY91]
C. Papadimitriou and M. Yannakakis. Optimization, approximation, and complexity
classes.
Journal of Computer and System Sciences, 43(3):425–440, December 1991.
Prelim version STOC 1988.
[Rab80]
M. O. Rabin. A probabilistic algorithm for testing primality. Journal of Number Theory,
12, 1980.
[Raz85a]
A lower bound on the monotone network complexity of the logical permanent. Matem-
aticheskie Zametki, 37(6):887–900, 1985. In Russian, English translation in Mathemat-
ical Notes of the Academy of Sciences of the USSR 37:6 485-493.
[Raz85b]
Alexander A. Razborov. Lowerbounds on the monotone complexity of some boolean
functions. Doklady Akademii Nauk. SSSR, 281(4):798–801, 1985. Translation in Soviet
Math. Doklady 31, 354–357.
[Raz87]
Alexander A. Razborov.
Lower bounds on the size of bounded depth circuits over
a complete basis with logical addition. MATHNASUSSR: Mathematical Notes of the
Academy of Sciences of the USSR, 41, 1987.
[Raz89]
A. A. Razborov. On the method of approximations. In Proceedings of the Twenty First
Annual ACM Symposium on Theory of Computing, pages 167–176, 15–17 1989.
Web draft 2007-01-08 21:59

DRAFT
pA.10 (470)
BIBLIOGRAPHY
[Raz92]
A.A. Razborov. On submodular complexity measures. In Boolean Function Complexity,
(M. Paterson, Ed.), pages 76–83. London Mathematical Society Lecture Note Series
169, Cambridge University Press, 1992.
[Raz98]
Ran Raz. A parallel repetition theorem. SIAM Journal on Computing, 27(3):763–803,
June 1998. Prelim version in ACM STOC’95.
[Raz00]
R. Raz. The bns-chung criterion for multi-party communication complexity. Computa-
tional Complexity, 9(2):113–122, 2000.
[Raz03]
Alexander A. Razborov.
Pseudorandom generators hard for k-DNF resolution and
polynomial calculus resolution, 2003.
[Raz04]
Ran Raz. Multi-linear formulas for permanent and determinant are of super-polynomial
size. In Proceedings of ACM Symposium on Theory of Computing. ACM Press, 2004.
[Ros06]
Kenneth H. Rosen. Discrete Mathematics and Its Applications. McGraw-Hill, 2006.
[RR97]
Alexander A. Razborov and Steven Rudich. Natural proofs. Journal of Computer and
System Sciences, 55(1):24–35, August 1997. Prelim version ACM STOC 1994.
[RR99]
Ran Raz and Omer Reingold. On recycling the randomness of states in space bounded
computation. In Proceedings of the Thirty-First Annual ACM Symposium on Theory
of Computing (STOC’99), pages 159–168, New York, 1999. Association for Computing
Machinery.
[RS95]
R. Raz and B. Spieker. On the “log rank”-conjecture in communication complexity.
Combinatorica, 15, 1995. Prelim version in IEEE FOCS 1993.
[RSA78]
R. L. Rivest, A. Shamir, and L. Adelman. A method for obtaining digital signatures
and public-key cryptosystems. Communications of the ACM, 21(2):120–126, 1978.
[RVW00]
O. Reingold, S. Vadhan, and A. Wigderson. Entropy waves, the zig-zag graph prod-
uct, and new constant-degree expanders and extractors. In IEEE, editor, 41st Annual
Symposium on Foundations of Computer Science: proceedings: 12–14 November, 2000,
Redondo Beach, California, pages 3–13. IEEE Computer Society Press, 2000.
[RW93]
Alexander Razborov and Avi Wigderson. nΩ(log n) Lower bounds on the size of depth-
3 threshold circuits with AND gates at the bottom. Information Processing Letters,
45(6):303–307, 16 April 1993.
[Sav70]
W. J. Savitch. Relationships between nondeterministic and deterministic tape complex-
ities. JCSS: Journal of Computer and System Sciences, 4, 1970.
[Sav72]
J. E. Savage. Computational work and time on ﬁnite machines. Journal of the ACM,
19(4):660–674, October 1972.
Web draft 2007-01-08 21:59

DRAFT
BIBLIOGRAPHY
pA.11 (471)
[Sha83]
A. Shamir. On the generation of cryptographically strong pseudorandom sequences.
ACM Trans. on Computer Sys., 1(1):38–44, 1983. Prelim version presented at Crypto’81
and ICALP’81.
[Sha92]
Adi Shamir. IP = PSPACE. Journal of the ACM, 39(4):869–877, October 1992.
[SHL65]
R. E. Stearns, J. Hartmanis, and P. M. Lewis II. Hierarchies of memory limited com-
putations. In Proceedings of the Sixth Annual Symposium on Switching Circuit Theory
and Logical Design, pages 179–190. IEEE, 1965.
[Sip83]
Michael Sipser. A complexity theoretic approach to randomness. In Proceedings of the
Fifteenth Annual ACM Symposium on Theory of Computing, pages 330–335, Boston,
Massachusetts, 25–27 April 1983.
[Sip88]
Michael Sipser. Expanders, randomness, or time versus space. Journal of Computer and
System Sciences, 36(3):379–383, June 1988. Prelim. version in Proc. IEEE Structure in
Complexity ’86.
[Sip92]
M. Sipser.
The history and status of the P versus NP question.
In ACM, editor,
Proceedings of the twenty-fourth annual ACM Symposium on Theory of Computing,
Victoria, British Columbia, Canada, May 4–6, 1992, pages 603–618, New York, NY,
USA, 1992. ACM Press.
[SIP96]
M. SIPSER. Introduction to the Theory of Computation. PWS, Boston, MA, 1996.
[Smo87]
R. Smolensky. Algebraic methods in the theory of lower bounds for Boolean circuit
complexity. In Proceedings of the nineteenth annual ACM Symposium on Theory of
Computing, New York City, May 25–27, 1987, pages 77–82, New York, NY, USA,
1987. ACM Press.
[SS77]
R. Solovay and V. Strassen. A fast Monte Carlo test for primality. SIAM Journal on
Computing, 6(1):84–85, 1977.
[Sto77]
L. J. Stockmeyer. The polynomial-time hierarchy. Theoretical Computer Science, 3:1–
22, 1977.
[Str69]
Volker Strassen. Gaussian elimination is not optimal. Numer. Math., 13:354–356, 1969.
[STV]
M. Sudan, L. Trevisan, and S. Vadhan. Pseudorandom generators without the XOR
lemma. JCSS: Journal of Computer and System Sciences, 62(2).
[SU02a]
M. Schaefer and C. Umans. Completeness in the polynomial-time hierarchy: Part I.
SIGACTN: SIGACT News (ACM Special Interest Group on Automata and Computabil-
ity Theory), 33, September 2002.
[SU02b]
M. Schaefer and C. Umans. Completeness in the polynomial-time hierarchy: Part II.
SIGACTN: SIGACT News (ACM Special Interest Group on Automata and Computabil-
ity Theory), 33, December 2002.
Web draft 2007-01-08 21:59

DRAFT
pA.12 (472)
BIBLIOGRAPHY
[SZ99a]
M. Saks and S. Zhou. BP HSPACE(S) ⊆DSPACE(S3/2). JCSS: Journal of Com-
puter and System Sciences, 58(2):376–403, 1999.
[SZ99b]
Aravind Srinivasan and David Zuckerman. Computing with very weak random sources.
SIAM Journal on Computing, 28(4):1433–1459, August 1999. Prelim. version FOCS
1994.
[Tar88]
´Eva Tardos. The gap between monotone and non-monotone circuit complexity is expo-
nential. Combinatorica, 8(1):141–142, 1988.
[Tod91]
S. Toda. PP is as hard as the polynomial-time hierarchy. SIAM Journal on Computing,
20(5):865–877, 1991.
[Tra84]
B. A. Trakhtenbrot. A survey of Russian approaches to perebor (brute-force search)
algorithms.
Annals of the History of Computing, 6(4):384–400, October/December
1984. Also contains a good translation of [Lev73].
[Tre01]
Luca Trevisan.
Extractors and pseudorandom generators.
Journal of the ACM,
48(4):860–879, July 2001. Prelim version in ACM STOC 1999.
[Tur36]
A. M. Turing. On computable numbers, with an application to the entscheidungsprob-
lem. In Proceedings, London Mathematical Society,, pages 230–265, 1936. Published as
Proceedings, London Mathematical Society,, volume 2, number 42.
[Uma01]
C. Umans. The minimum equivalent DNF problem and shortest implicants. JCSS:
Journal of Computer and System Sciences, 63, 2001.
[Vad00]
Salil Vadhan. On transformation of interactive proofs that preserve the prover’s com-
plexity. In ACM, editor, Proceedings of the thirty second annual ACM Symposium on
Theory of Computing: Portland, Oregon, May 21–23, [2000], pages 200–207, New York,
NY, USA, 2000. ACM Press.
[Val75]
Leslie G. Valiant. On non-linear lower bounds in computational complexity. In STOC
’75: Proceedings of seventh annual ACM symposium on Theory of computing, pages
45–53. ACM Press, 1975.
[Val79a]
L. G. Valiant. The complexity of computing the permanent. Theoretical Computer
Science, 8(2):189–201, 1979.
[Val79b]
Leslie G. Valiant.
The complexity of enumeration and reliability problems.
SIAM
Journal on Computing, 8(3):410–421, 1979.
[Vaz01]
Vijay V. Vazirani. Approximation Algorithms. Springer-Verlag, Berlin-Heidelberg-New
York-Barcelona-Hong Kong-London-Milan-Paris-Singapur-Tokyo, 2001.
[VG99]
Joachim Von zur Gathen and J¨urgen Gerhard. Modern Computer Algebra. Cambridge
University Press, New York, NY, USA, 1999.
Web draft 2007-01-08 21:59

DRAFT
BIBLIOGRAPHY
pA.13 (473)
[vN45]
John von Neumann. First draft of a report on the EDVAC. University of pennsylvania
report for the u.s. army ordinance department, 1945. Reprinted in part in Brian Randell,
ed. The Origins of Digital Computers: Selected Papers, Springer Verlag, 1982.
[von61]
J. von Neumann. Probabilistic logics and synthesis of reliable organisms from unreliable
components, volume 5. Pergamon Press, 1961.
[Yam97]
P. Yam. Bringing schroedinger’s cat back to life. Scientiﬁc American, pages 124–129,
June 1997.
[Yan91]
M. Yannakakis. Expressing combinatorial optimization problems by linear programs.
Journal of Computer and System Sciences, 43(3):441–466, 1991. Prelim version in ACM
STOC 1988.
[Yao79]
A.C.C. Yao. Some complexity questions related to distributive computing(preliminary
report). In STOC ’79: Proceedings of the 11th ACM STOC, pages 209–213. ACM Press,
1979.
[Yao82]
Andrew C. Yao. Theory and applications of trapdoor functions (extended abstract). In
23rd Annual Symposium on Foundations of Computer Science, pages 80–91. IEEE, 3–5
November 1982.
[Yao85]
A. C.-C. Yao. Separating the polynomial-time hierarchy by oracles. In 26th Annual
Symposium on Foundations of Computer Science (FOCS ’85), pages 1–10, Los Angeles,
Ca., USA, 1985. IEEE Computer Society Press.
[Yao86]
Andrew Chi-Chih Yao. How to generate and exchange secrets (extended abstract). In
27th Annual Symposium on Foundations of Computer Science, pages 162–167. IEEE,
27–29 October 1986.
[Yao90]
Andrew Chi-Chih Yao. On ACC and threshold circuits. In 31st Annual Symposium
on Foundations of Computer Science, volume II, pages 619–627. IEEE, 22–24 October
1990.
[Yao93]
A. Yao. Quantum circuit complexity. In 34th Annual Symposium on Foundations of
Computer Science, pages 352–361, Palo Alto, California, 1993. IEEE.
[Zak83]
S. Zak. A Turing machine time hierarchy. Theoretical Computer Science, 26(3):327–333,
October 1983.
[Zuc90]
David Zuckerman. General weak random sources. In 31st Annual IEEE Symposium on
Foundations of Computer Science, volume II, pages 534–543, 22–24 October 1990.
Web draft 2007-01-08 21:59


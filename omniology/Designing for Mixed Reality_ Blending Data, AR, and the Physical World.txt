



Kharis O’Connell
Designing for Mixed Reality
Blending Data, AR, and
the Physical World
Boston
Farnham
Sebastopol
Tokyo
Beijing
Boston
Farnham
Sebastopol
Tokyo
Beijing

978-1-491-96238-1
[LSI]
Designing for Mixed Reality
by Kharis O’Connell
Copyright © 2016 O’Reilly Media Inc. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA
95472.
O’Reilly books may be purchased for educational, business, or sales promotional use.
Online editions are also available for most titles (http://safaribooksonline.com). For
more 
information, 
contact 
our 
corporate/institutional 
sales 
department:
800-998-9938 or corporate@oreilly.com .
Editor: Angela Rufino
Production Editor: Shiny Kalapurakkel
Copyeditor: Octal Publishing, Inc.
Interior Designer: David Futato
Cover Designer: Karen Montgomery
Illustrator: Rebecca Demarest
September 2016:
 First Edition
Revision History for the First Edition
2016-09-02: First Release
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Designing for
Mixed Reality, the cover image, and related trade dress are trademarks of O’Reilly
Media, Inc.
While the publisher and the author have used good faith efforts to ensure that the
information and instructions contained in this work are accurate, the publisher and
the author disclaim all responsibility for errors or omissions, including without limi‐
tation responsibility for damages resulting from the use of or reliance on this work.
Use of the information and instructions contained in this work is at your own risk. If
any code samples or other technology this work contains or describes is subject to
open source licenses or the intellectual property rights of others, it is your responsi‐
bility to ensure that your use thereof complies with such licenses and/or rights.

Table of Contents
1. What Exactly Is “Mixed Reality”?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1
The History of the Future of Computing                                         1
Pop Culture Attempts at Future Interfaces                                      6
What Kinds of End-Use-Cases Are Best Suited for MR?               7
2. What Are the End-User Benefits of Mixing the Virtual with the Real?. 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  11
The Age of Truly Contextual Information and Interpreting
Space as a Medium                                                                         11
The Physical Disappearance of Computers as We Know
Them                                                                                                13
The Rise of Body-Worn Computing                                               14
The Impact on the Web                                                                    14
3. How Is Designing for Mixed Reality Different from Other Platforms?. . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17
The Inputs: Touch, Voice, Tangible Interactions                          17
The Outputs: Screens, Targets, Context                                         20
Implications of Using Optical See-Through Displays                  21
4. Examples of Approaches to Date. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23
Not All Gestures Are Created Equal                                               23
Eye Tracking: A Tricky Approach to the Inference of Gaze-
Detection                                                                                         25
Of Light Fields and Prismatics                                                         27
Computer Vision: Using the Technologies That Can “Rank
and File” an Environment                                                             28
v

5. Future Fictions Around the Principles of Interaction. . . . . . . . . . . . .  31
Frameworks for Guidance: Space, Motion, Flow                          31
How to Mockup the Future: Effective Prototyping                      32
Less Boxes and Arrows, More Infoblobs and Contextual
Lassos                                                                                               33
PowerPoint and Keynote Are Your Friends!                                  35
Using Processing for UI Mockups                                                   36
Building Actual MR Experiences                                                    36
The Usability Standards and Metrics for Tomorrow                    39
6. Where Are the High-Value Areas of Investigation?. . . . . . . . . . . . . .  41
The Speculative Landscape for MR Adoption                               41
Emergent Futures: What Kinds of Business Could Grow
Alongside Mixed Reality?                                                             44
7. The Near-Future Impact on Society. . . . . . . . . . . . . . . . . . . . . . . . . . . .  47
The Near-Future Impact of Mixed Reality                                     47
vi 
| 
Table of Contents

CHAPTER 1
What Exactly Is “Mixed Reality”?
I don’t like dreams or reality. I like when dreams become reality because
that is my life
—Jean Paul Gaultier
The History of the Future of Computing
It’s 2016. Soon, humans will be able to live in a world in which
dreams can become part of everyday reality, all thanks to the ree‐
mergence and slow popularization of a class of technology that pur‐
ports to challenge the way that we understand what is real and what
is not. There are three distinct variants of this type of technological
marvel: virtual reality, augmented reality, and mixed reality. So it
would be helpful to try to lay out the key differences.
Virtual Reality
The way to think of virtual reality (VR) (Figure 1-1) is as a medium
that is 100% simulated and immersive. It’s a technology that
emerged back in the 1950s with the “Sword of Damocles,” and is
now back in the popular pschye after some false starts in the early
1990s. This reemergence is predominantly down to a single com‐
pany—Oculus—and its Rift Developer Kit 1 (DK1) headset that suc‐
cessfully kick started (literally) the entire modern VR movement
(Figure 1-2). Now, in 2016, there are many companies investing in
the space, such as HTC, Samsung, LG, Sony, and many more, and
with this, a raft of dedicated startups and investment that has only
served to fuel interest. VR will likely become the optimal way that
1

one experiences games and entertainment over the next decade or
so.
Figure 1-1. Virtual reality—everything you see is simulated, and the
real-world environment in which you experience VR is not taken into
account
Figure 1-2. The Oculus Rift DK1 headset—arguably responsible for the
rebirth of VR
2 
| 
Chapter 1: What Exactly Is “Mixed Reality”?

Augmented Reality
Augmented reality (AR) (Figure 1-3) became popularized as a term
a few years back when a few of the first wave of smartphone apps
began to appear that allowed users to hold their smartphones in
front of them, and then, using the rear-facing camera, “look
through” the screen and see information overlaid across whatever
the camera was pointing at. But after many apps implemented
poorly-concieved ways to integrate AR into their app experience, the
technology quickly declined in use, as the novelty wore off. It ree‐
merged into the public consciousness as a pair of $1,500 glasses—
Google Glass, to be precise (Figure 1-4). This new heads-up-display
approach was heralded by Google as the very way we could, and
should, access information about the world around us. The attempt
to free us from the tyranny of our phones and put that information
on your face, although incredibly forward-thinking, unfortunately
backfired for Google. Society was simply not ready for the rise of the
Glasshole, and so, after many months of the mocking and joking
reaching critical mass, Google pulled the product from the market.
There are still many manufacturers making AR headsets (Vuzix,
Recon, and Epson, among others) that are still a popular choice of
technology for many industrial use cases, such as logistics.
Figure 1-3. Augmented reality—everything you see is real, with an
extra data layer superimposed into your field of view, and the environ‐
ment in which you experience AR is often not taken into account
The History of the Future of Computing 
| 
3

Figure 1-4. The (now infamous) Google Glass augmented reality head‐
set
Mixed Reality
Mixed reality (MR) (Figure 1-5)—what this report really focuses on
—is arguably the newest kid on the block. In fact, it’s so new that
there is very little real-world experience with this technology due to
there being such a limited amount of these headsets in the wild. Yes,
there are small numbers of headsets available for developers, but
nothing is really out there for the common consumers to experi‐
ence. In a nutshell, MR allows the viewer to see virtual objects that
appear real, accurately mapped into the real world. This particular
subset of the “reality” technologies has the potential to truly blur the
boundaries between what we are, what everything else is, and what
we need to know about it all. Much like the way Oculus brought VR
back into the limelight a few years ago, the poster child to date for
MR is a company that seemed to appear from nowhere back in 2014
—Magic Leap. Until now, Magic Leap has never shown its hardware
or software to anyone outside of a very select few. It has not officially
announced yet—to anyone, including developers—when the technol‐
ogy will be available. But occasional videos of the Magic Leap expe‐
rience enthrall all those who have seen them. Magic Leap also
happens to be the company that has raised the largest amount of
venture funding (without actually having a product in the market)
in history. $1.4 billion dollars.
4 
| 
Chapter 1: What Exactly Is “Mixed Reality”?

Since that initial Magic Leap announcement back in 2014, other
companies have slowly begun to show what they are working on in
MR. Microsoft has announced and launched for select developers its
HoloLens headset (although, confusingly, on its website, the com‐
pany refers to it as an “AR headset”) (Figure 1-6). Meta, a company
that has been working publicly on MR for quite some time and has
one of the godfathers of AR/MR research as its chief scientist (Steve
Mann), announced its Meta 2 headset (Figure 1-7) at TED in Febru‐
ary 2016. DAQRI is another fast-rising player with its construction
industry focused “Smart Helmet”—an MR safety helmet with an
integrated computer, sensors, and optics. Unlike VR and AR, which
do not take into account the user’s environment, MR purposefully
blurs the lines between what is real in your field of view (FoV) and
what is not in order to create a new kind of relationship and under‐
standing of your environment. This makes MR the most disruptive,
exciting, and lucrative of all the reality technologies.
Figure 1-5. Mixed reality—everything you see might or might not be
real; with extra data overlaid into your FoV and physically attached to
real/not real objects and things, the environment you experience MR
in is mapped and directly taken into account
The History of the Future of Computing 
| 
5

Figure 1-6. Microsoft’s mixed reality headset—the Hololens
Figure 1-7. The Meta 2 mixed reality headset
Pop Culture Attempts at Future Interfaces
MR feels like science fiction. Everyone enjoys a bit of science fiction.
And why not? It gives the viewer or reader a guilt-free glimpse into a
myriad of possible futures, showing how the world could be. Show‐
ing how we could interact with technologies. It’s fun, generally
always looks cool and exciting, and also has the useful side effect of
6 
| 
Chapter 1: What Exactly Is “Mixed Reality”?

subliminally preconditioning the viewer for the eventual introduc‐
tion of some of these technological marvels. Hollywood always loves
a good futuristic user interface. The future interface is also appa‐
rently heavily translucent, as seen in everything from Minority
Report to Iron Man, Pacific Rim to Star Wars, and many, many more.
Clearly, the future will need to be dimly lit to be able to see these dis‐
plays that float effortlessly in thin air. They are generally made up of
lots of boxes that contain teeny, tiny fonts that scroll aimlessly in all
directions and contain graphs, grids, and random blinking things
that the future human will apparently be able to decipher at a speed
that makes me feel old, like I don’t understand anything anymore.
Of course, these interfaces are primarily created for the purpose of
entertainment. They rarely take up a large amount of screen time in
a film. They are decorative and serve to reinforce a plot line or
theme: to make it feel contemporary. They are not meant to be taken
seriously, right?
Some films do attempt to make a concerted effort in making believ‐
able, usable interfaces. One such recent film, Creative Control (http://
www.magpictures.com/creativecontrol/), has its entire story focus
around a particular product called “Augmenta,” which is a pair of
MR smart glasses that allow the wearer to not only perform the
usual types of computing tasks, but also to develop a relationship
with an entirely virtual avatar. The interface for the glasses is well
thought out, and doesn’t attempt to hide the interactions behind
superfluous visual touches. It’s arguably the closest a film has man‐
aged to achieve in designing a compelling product that could stand
up to the kind of scrutiny a real product must go through to reach
the market.
What Kinds of End-Use-Cases Are Best Suited
for MR?
So, now that we have all of this technology, what is it actually good
for? Although VR is currently enjoying its place in the sun, immers‐
ing people in joyful gaming and fun media experiences (and
recently even AR has come back into the public consciousness from
the immense success of the Pokemon Go smartphone app), MR
chooses to walk a slightly different path. Where VR and MR differ in
emphasis is that one posits that it is the future of entertainment,
whereas the other sees itself as the future of general-purpose com‐
What Kinds of End-Use-Cases Are Best Suited for MR? 
| 
7

puting—but now with a new spatial dimension. MR wants to embel‐
lish and outfit your world with not just virtual trinkets, but data,
context, and meaning. So, it is only natural to think of MR more as a
useful tool in your arsenal; a tool that can help you to get things
done better, more efficiently, with more spatial context. It’s a tool
that will help you at work and at play (if you have to). Following are
some examples of typical use cases that are potentially good fits for
MR.
Architecture
Architects follow their own design process that begins with ideation,
sketching, and early 3-D mockups. It then moves into 3-D printing
or hand-manufacturing models of buildings, and then into high-
fidelity formats that can be handed over to developers and engineers
to be built. MR is most useful in the earlier stages of 3-D mockups;
the ability to quickly view models as if they are already built and
share context with other MR-enabled colleagues is something that
makes this technology one of the most highly anticipated in the
architectural industry.
Training
How much time is spent training new employees for doing jobs out
in the field? What if those employees could learn by doing? Wearing
an MR headset would put the relevant information for their job
right there in front of them. No need to shift context, stop what you
are doing, and reference some web page or manual. Keeping new
workers focused on the task at hand helps them to absorb the learn‐
ings in a more natural way. It’s the equivalent of always having a
mentor with you to help when you need it.
Healthcare
We’ve already seen early trials of VR being used in surgical proce‐
dures, and although that is pretty interesting to watch, what if sur‐
geons could see the interior of the human body from the outside?
One use case that has been brought up many times is the ability for
doctors to have more context around the position of particular med‐
ical anomalies—being able to view where a cancerous tumor is pre‐
cisely located helps doctors target the tumor with chemotherapy,
reducing the negative impact this treatment can have on the patient.
8 
| 
Chapter 1: What Exactly Is “Mixed Reality”?

The ability to share that context in real time with other doctors and
garner second opinions reduces the risk associated with current
treatments.
Education
Magic Leap’s website has an image that shows a classroom full of
kids watching sea horses float by while the children sit at their desks
in the classroom. The website also has another video that shows a
gymnasium full of students sharing the experience of watching a
humpback whale breach the gym floor as if it were an ocean. Just
imagine how different learning could be if it were fully interactive;
for instance, allowing kids to really get a sense of just how big dino‐
saurs really were, or biology students to visualize DNA sequences, or
historians to reenact famous battles in the classroom, all while being
there with one another, sharing the experience. This could trans‐
form the relationship children have today with the art of learning
from being a “push” to learn, into a naturally inquisitive “pull” from
the children’s innate desire to experience things.
These kinds of use cases are only the very tip of the iceberg, as we
have yet to experience what effect this technology will have across
much broader aspects of work. VR has often been referred to as the
empathy machine. MR might allow us to collaborate—and thus
empathize—together in a much more natural fashion than with
other forms of technology.
What Kinds of End-Use-Cases Are Best Suited for MR? 
| 
9


CHAPTER 2
What Are the End-User Benefits of
Mixing the Virtual with the Real?
One of the definitions of sanity, itself, is the ability to tell real from
unreal. Shall we need a new definition?
—Alvin Toffler, Future Shock
The Age of Truly Contextual Information and
Interpreting Space as a Medium
In this age of truly contextual information and interpretation of
physical space as a medium, a unique “window on the world” is pro‐
vided that will potentially yield new insights in which designers need
to learn to absorb and design in order to make visual information
seamlessly integrate into our real-world surroundings. Magic Leap,
Microsoft, and Meta intend to make experiences that are relatively
indistinguishable from reality, which is in many ways, the ultimate
goal of mixed reality (MR). Magic Leap, in particular, recently sug‐
gested that it will need to purposely make its holograms “hyperreal”
so that humans will still be able to distinguish what is reality and
what is not. And although the amount of technical prowess needed
to do this is not insignificant, it does pose a new challenge: are we
ready to handle a society that is seeing things that are not real?
The 1960s was a time of wild experimentation. A time when humans
first began, en masse, to experiment with mind-altering hallucino‐
genic drugs. The mere thought of people running around and seeing
things that were not there seemed wrong to the general populace.
11

Thus, people who indulged in hallucinogenic trips began to be clas‐
sified as mentally ill (in some cases, officially so in the US) because
humans who react to imaginary objects and things are not of a
sound mind and need help. Horror stories of people having “bad
trips” and jumping off buildings, thinking they could fly, or chasing
things across busy roads only served to fuel the idea that these kinds
of drugs were bad. I wonder what those same critics of the halluci‐
nogenic movement would think of MR.
Picture the scene: it’s 2018, and John is going home from a day
working as a freelance, deskless worker. He’s wearing an MR head‐
set. So are many others these days, since they came down dramati‐
cally in price. John hops on the bus just in time to see another
passenger frantically jump off and scream that she is chasing the
Blue Goblin down the street, knocking people over in the process.
Anyway, John sits at the back of the bus—it’s full, and pretty much
everyone is wearing some brand of MR headset. One guy is trying to
touch the ear of the passenger seated next to him. He seems fascina‐
ted with it. John sees a man sitting down opposite him who is just
staring back at him. John feels uncomfortable. After some awkward
minutes, John shouts at the guy to stop staring at him. But the man
continues to stare. Other passengers are telling John to calm down—
“You’re crazy!” shouts one passenger at John. John decides to use his
MR headset to glean info on the man, using the computer vision
(CV) to recognize his face. Turns out, the man is wanted by authori‐
ties. John decides to be a hero and attempt a citizen’s arrest, so he
leaps at the guy, only to smash his face on the back of the seat. There
was no one sitting there. Other passengers get up and move away—
“If you can’t handle it, don’t use it!” one passenger says as he disem‐
barks to also follow his own imaginary things. John sighs—he real‐
ized that he had signed up for some kind of immersive RPG game a
while back. “Hey! Welcome to 2018!” shouts John as he gets off the
bus.
Even though this little anecdote is a fictitious stretch of the imagina‐
tion, we might be closer to this kind of world than we sometimes
think. MR technology is rapidly improving, and with it, the visual
“believability” is also increasing. This brings a new challenge: what
is real, and what is not? Will acceptable mass hallucination be deliv‐
ered via these types of headsets? Should designers purposefully cre‐
ate experiences that look less real in order to avoid situations such as
John’s story? How we design the future will increasingly become an
12 
| 
Chapter 2: What Are the End-User Benefits of Mixing the Virtual with the Real?

area closer in alliance to psychology than interaction design. So as a
designer, the shift begins now. We need to think about the implica‐
tions an experience can have on the user from an emotional-state
perspective. The designer of the future is an alchemist, responsible
for the impact these visual accruements can have on the user. One
thing is very clear right now—no one knows what might happen
after this technology is widely adopted. There is a lot of research
being conducted, but we won’t know the societal impact until the
assimilation is well under way.
The Physical Disappearance of Computers as
We Know Them
If we think about the move toward a screenless future, we need to
keep in mind what current technology, platforms, and practices are
affected by this direction. After all, we have lived in a world of com‐
puter screens, or “glowing rectangles,” for quite some time now, and
many, many millions of businesses run their livelihood through the
availability and access to these screens.
What seems to be the eventual physical disappearance of computers
as we know them began a while back with the smartphone, a class of
device that was originally intended to provide a set of functionalities
that helped business people work on the go. Over time, more and
more functionality became embedded in this diminutive workhorse,
and, as we know, it only served to broaden their popularity and util‐
ity over time. One early side effect of this popularity was the effect
on the Web—smartphone browsers initially served up web pages
that were clearly never designed to take into account this new plat‐
form, and so the Web quickly transformed its rendering approaches
and formatting style to work well on small screens. By and large,
from a designer’s standpoint, this is now a solved problem; that is,
there are today many, many books and websites that lay out in great
detail a blueprint for every variant of screen and experience, and
there is a myriad of tools and techniques available to help a designer
and developer create well-performing and compelling websites and
web apps.
The Physical Disappearance of Computers as We Know Them 
| 
13

The Rise of Body-Worn Computing
In the past couple of years, we have also seen the rise of the smart‐
watch. These devices are a further contextualization and abstraction
of the smartphone, but they have a much smaller screen, so design‐
ers needed to accommodate this in their design approach by turning
the core functions of web apps into native watch apps in order to
access functionality through the watch. But still, there are familiar
aspects of designing for a watch; the ever present rectangular or
round screen still forces constraint. It cajoles the designer into strip‐
ping the unnecessary aspects of an experience away. It purifies the
message. With these constraints, having access to the Web through a
web browser on your wrist makes little sense. That’s most likely why
there is no browser for a smartwatch.
The same “stripping back” of visual adornments and superfluous
design elements in interface design is also observed when designing
for the Internet of Things (IoT)—another category of hardware
devices that take the core aspects of the Web and combine it with
sensor technology to facilitate specific use cases. Taking all of this
into account and then adding virtual reality (VR)/augmented reality
(AR) and now MR into the mix shows that the journey on the road
to a rectangle-less future is well underway. So what about the Web
going forward?
The Impact on the Web
The Web has been a marvelous thing. It has fueled so much societal
change and has so deeply affected every aspect of every business that
it’s almost a basic human need. What made the Web really become
the juggernaut of change is accessibility—as long as you had a com‐
puter that had a screen, ran an operating system that was connected
to the Internet, and had a web browser, you had access to immeasur‐
able knowledge at your disposal. For the most part, the Web has
standardized its look and feel across differing screen sizes, and for
designers and developers alike, the trio of HTML, CSS, and Java‐
Script are a very powerful set of languages to learn. The concepts
and mental models around the Web are easy to understand: after all,
in essence, it’s a 2-D document parsing platform. So what about VR?
I mean, it’s simple—just make a VR app, pop in a virtual web
browser, and voilá! The Web is safely nestled in the future, still
working, and pretty much looking and feeling and partying like it’s
14 
| 
Chapter 2: What Are the End-User Benefits of Mixing the Virtual with the Real?

1999. Except it’s not. It’s 2016, and to keep using the Web in a way
that matches the operating system it is connected to, it will need to
adapt in a way that throws most of what people perceive as the Web
out the window. Say hello to a potential future Web of headless data
APIs serving native endpoints. Welcome to the Information Age 3.0!
The future of the Web will strip the noise or “window dressing,”
which is predominantly the styling of the website; aka, what you can
see and move, toward the signal; aka, all the incredible information
these pages contain, as the web slowly morphs toward providing the
data pipes and contextual information exchanges needed to unlock
the power of MR. MR is not a very compelling standalone experi‐
ence, and so the value and power that a myriad of data APIs will
provide to end users will free the Web from the confining shackles
of frontend development—all of the frontend work would likely be
done in native code, as a core part of the system UI. There won’t be
any “web pages”—the entire notion of viewing web pages in MR
would feel incredibly arcane. This should be seen as a great step for‐
ward for the Web, but, of course, there are technological impacts
and design sacrifices to be made. A lot of the principles and ideolo‐
gies that helped popularize the open Web will be put to the test, as
endpoints are potentially owned and controlled by the companies
developing the platforms. It remains to be seen how this pans out in
actuality.
The Impact on the Web 
| 
15


CHAPTER 3
How Is Designing for Mixed Reality
Different from Other Platforms?
Any sufficiently advanced technology is indistinguishable from magic.
—Arthur C. Clarke
The Inputs: Touch, Voice, Tangible
Interactions
So how does mixed reality (MR) actually work? Well, there are
inputs, which are primarily the system’s means to see the environ‐
ment by using sensors, and also the user interacting with the system.
And then there are outputs, which are primarily made up of holo‐
graphic objects and data that has been downloaded to the headset
and placed in the user’s field of view (FoV). Let’s first break down
how things get into the system.
To have virtual objects appear “anchored” to the real world, an MR
headset needs to be able to see the world around the wearer. This is
generally done through the use of one or more camera sensors.
What kind of cameras these are can vary, but they generally fall into
two camps: infra-red (IR), or standard red-green-blue (RGB). IR
cameras allow for depth-sensing the environment, whereas the RGB
camera works best for photogrammetric computer vision (CV).
Both approaches have their pluses and minuses, which we will dis‐
cuss in detail in the next chapter. Aside from cameras, other sensors
that are used to provide input are accelerometers, magnetometers,
and compasses, which are inside every smartphone. In the end, an
17

MR headset must utilize all of these inputs in real-time in order to
compute the headsets position in relation to the visual output. This
is often referred to as sensor fusion.
Now that we have an idea of how the headset can perceive and
understand the environment, what about the wearer? How can the
wearer input commands into the system?
Gestures are the most common approach to interacting with an MR
headset. As a species, we are naturally adept at using our own bodies
for signaling intent. Gestures allow us to make use of proprioception
—the knowing of the position of any given limb at any time without
visual identification. The only current downside with gestures is that
not all are created equal. The fidelity and meaning of those gestures
vary greatly across the different operating systems being used for
MR. Earlier gesture-based technologies, like Microsoft’s Kinect cam‐
era (now discontinued), could recognize a broad set of gestures, and
Leap Motion’s Leap peripheral used a similar approach. Both tech‐
nologies allowed granular control, but each recognized the same
gestures differently. This has had an unfortunate effect on compa‐
nies that are making hardware: many gestures end up proprietary.
For example, you cannot successfully use one MR platform (Holo‐
lens) and then immediately use another (Meta 2) with the exact
same gestures. This means the MR designer needs to understand all
the variances on inputs between the platforms.
Voice input is another communication channel that we can use for
interacting with MR, and is growing steadily in popularity—since
the birth of Apple’s Siri, Microsoft’s Cortana, Amazon’s Alexa, and
Google’s Assistant, we have become increasingly comfortable with
just talking to machines. The natural-language parsing software that
powers these services is becoming increasingly robust over time and
is a natural fit for a technology like MR. What could be better than
just telling the system what to do? Some of the biggest challenges in
using voice are environmental. What about ambient noise? What if
it’s noisy? What if it’s quiet? What if I don’t want anyone to hear
what I am saying?
Gaze-based interfaces have grown in popularity over the past few
years. Gaze uses a centered reticle (which looks like a small dot) in
the headset FoV as a kind of virtual mouse that is locked to the cen‐
ter of your view, and the wearer simply gazes, or stares, at a specific
object or item in order to involve a time-delayed event trigger. This
18 
| 
Chapter 3: How Is Designing for Mixed Reality Different from Other Platforms?

is a very simple interaction paradigm for the wearer to understand,
and because of its single function, it is used the same across all MR
platforms (and VR uses this input approach heavily). The challenge
here is that gaze can have unintended actions: what if I just wanted
to just look at something? How do I stop triggering an action? With
gaze-based interfaces there is no way around this; whatever you are
looking at will be selected and ready to trigger. A newer and more
powerful variant of the gaze-based approach is enabled through new
eye-tracking technology that provides more potential granularity to
how your gaze can trigger actions. This allows the wearer to move
her gaze toward a target, rather than her whole head, to move a reti‐
cle onto a target. The biggest hurdle to adoption of eye tracking is
that it requires even more technology—the wearers eyes must be
tracked by using cameras mounted toward the eyes in the headset.
So far, no headset on the market comes with eye tracking. However,
one company, FOVE (a VR headset), is intending to launch its prod‐
uct toward the end of 2016.
There are other ways to interact with MR, such as proprietary hard‐
ware controllers, also known as gamepads. These are generally opti‐
mized for gaming, but there are some simpler “clicker” style triggers
(Figure 3-1) that can serve in place of gesture-based triggers (Micro‐
soft’s Hololens comes with a clicker).
Figure 3-1. Microsoft’s HoloLens clicker style hardware controller
The Inputs: Touch, Voice, Tangible Interactions 
| 
19

The Outputs: Screens, Targets, Context
When it comes to output, we are referring to how the headset
wearer receives information. For the most part, this is commonly
known as the display. This area of the technology has many differing
approaches, so many, in fact, that this entire report could be just on
display technologies alone. To keep it a bit simpler, though, we will
cover only the most commonly used displays.
The Differing Types of Display Technologies
Following are the different types of display technologies and each of
their strengths and weaknesses.
Reflective/diffractive waveguide
Pros: A relatively cheap, proven technology (this is one of the oldest
display technologies).
Cons: Worst FoV (size of display) of all of the types of display tech‐
nologies, as well as worst color gamut. Not good for prescription-
glasses wearers.
Spectral refraction
Pros: A relatively cheap, proven technology (the optical technique is
taken from fighter-pilot helmets). Good for dealing with the
vergence-accomodation conflict problem (which is explained in
more detail in Chapter 4) and allows for a true cost-effective holo‐
graphic display without the need for a powerful graphics processing
unit (GPU—the viewable display is unpowered/passive).
Cons: It tends to have poor display quality in direct sunlight (which
is somewhat solved with a darkened/photochromically coated
visor). Holograms are partially opaque, so they’re not very good for
jobs that require an accurate color display (no AR solution to date
has this nailed, but Magic Leap is aiming to solve this).
Retinal display/lightfield
Pros: This is the most powerful imaging solution known to date.
Displays accurate, fully realistic images directly to the retina. Per‐
fectly in focus, always. Unaffected by sunlight (Retinal projection
can occlude actual sunlight!). No Vergence-Accommodation Con‐
flict. Awesome.
20 
| 
Chapter 3: How Is Designing for Mixed Reality Different from Other Platforms?

Cons: The Rolls-Royce of display tech comes at a cost—it’s the most
expensive, most technologically cumbersome, most in need of pow‐
erful hardware. The holy grail might become the lost ark of the cov‐
enant.
Optical waveguide
Pros: Good resolution. Reasonable color gamut.
Cons: Poor FoV and only a few manufacturers to choose from
(ODG invented the tech), so most solutions feel the same. Relatively
expensive tech for minor gains of color over spectral refraction.
Understanding screen technologies is something that every MR
designer should try to do, as each type of technology will affect your
design direction and constraints. What looks great on the Meta
headset, might look terrible on the Hololens due to its much smaller
FoV. The same goes for the effective resolution of each screen tech‐
nology—how legible and usable fonts are will vary between different
headsets.
Implications of Using Optical See-Through
Displays
Traditionally, applications that are built to utilize computer vision
libraries (these are the software libraries that process and make sense
of what is being received by the camera sensor) use a camera video
feed on which data and augmentations are then overlaid. This is
how AR apps, like the recent Pokemon Go, work on smartphones.
But instead of rendering both what the background camera sees and
the virtual objects layered on top, an optical see-through display
only renders the virtual objects, and the background is the real
world you see around you.
Stereo displays (one dedicated display for each eye, like all VR head‐
sets), render augmentations stereoscopically, which provides a simu‐
lated depth of field. This allows the virtual objects to feel like they sit
at the right distance from the headset wearer. Regardless of whether
you are using a monocular or stereoscopic display, the benefit with
see-through displays is that there is no separation from the real
world—you’re not looking at the world around you on a screen.
As a designer, be aware, however, that this can also cause user expe‐
rience issues: there will always be some perceptive lag between the
Implications of Using Optical See-Through Displays 
| 
21

virtual objects displayed on the optics and the real world passing by
behind them. This is due to the time needed for the headset to
detect the wearers physical movement, send these positional
changes to the CPU, recalculate the new position, and then re-
render the virtual object in the correct position. Nowadays, with
high-end devices like Microsoft’s Hololens, it is much less of a prob‐
lem, but older devices will still struggle with this lag, or “swimming”
effect.
22 
| 
Chapter 3: How Is Designing for Mixed Reality Different from Other Platforms?

CHAPTER 4
Examples of Approaches to Date
Gestures, in love, are incomparably more attractive, effective, and val‐
uable than words.
—Francois Rabelais
Not All Gestures Are Created Equal
The gestures we are using here are a bit more primitive, less cultur‐
ally loaded, and easy to master. But first, a brief history of using ges‐
tures in human-computer-interface design.
In the 1980s, NASA was working on virtual reality (VR), and came
up with the dataglove—a pair of physically-wired up gloves that
allowed for direct translation of gestures in the real world, to virtual
hands shown in the virtual world. This was a core theme that con‐
tinues in VR to this day.
In 2007 with the launch of Apple’s iPhone, gesture-based interaction
had a renaissance moment with the introduction of the now ubiqui‐
tous “pinch-to-zoom” gesture. This has continued to be extended
using more fingers to mean more types of actions.
In 2012, Leap Motion introduced a small USB-connected device that
allows a user’s hands to be tracked and mapped to desktop interac‐
tions. This device later became popular with the launch of Oculus’
Rift DK1, with developers duct-taping the Leap to the front of the
device in order to get their hands into VR. This became officially
supported with the DK2.
23

In 2014 Google launched Project Tango, its own device that com‐
bines a smartphone with a 3-D depth camera to explore new ways of
understanding the environment, and gesture-based interaction.
In 2015, Microsoft announced the Hololens, the company’s first
mixed reality (MR) device, and showed how you could interact with
the device (which uses Kinect technology for tracking the environ‐
ment) by using gaze, voice, and gestures. Leap Motion announced a
new software release that further enhanced the granularity and
detection of gestures with its Leap Motion USB device. This allowed
developers to really explore and fine-tune their gestures, and
increased the robustness of the recognition software.
In 2016, Meta announced the Meta 2 headset at TED, which show‐
cases its own approach to gesture recognition. The Meta headset uti‐
lizes a depth camera to recognize a simple “grab” gesture that allows
the user to move objects in the environment, and a “tap” gesture that
triggers an action (which is visually mapped as a virtual button
push).
From these high-profile technological announcements, one thing is
clear: gesture recognition will play an increasingly important part in
the future of MR, and the research and development of technologies
that enable ever more accurate interpretations of human motion will
continue to be heavily explored. For the future MR designer, one of
the more interesting areas of research might be the effect of gesture
interactions on physical fatigue—everything from RSI that can be
generated from small, repetitive micro interactions, all the way to
the classic “gorilla arm” (waving our limbs around continuously),
even though having no tangible physical resistance when we press
virtual buttons—will generate muscular pain over time. As human
beings, our limbs and muscular structure is not really optimized for
long periods of holding our arms out in front of our bodies. After a
short period of time, they begin to ache and fatigue sets in. Thus,
other methods of implementing gesture interactions should be
explored if we are to adopt this as a potential primary input. We
have excellent proprioception; that is, we know where our limbs are
in relation to our body without visual identification, and we know
how to make contact with that part of our body, without the need
for visual guidance. Our sense of touch is acute, and might offer a
way to provide a more natural physical resistance to interactions
that map to our own bodies. Treating our own bodies as a canvas to
which to map gestures is a way to combat the aforementioned fati‐
24 
| 
Chapter 4: Examples of Approaches to Date

gue effects because it provides physical resistance, and through
touch, gives us tactile feedback of when a gesture is used.
Eye Tracking: A Tricky Approach to the
Inference of Gaze-Detection
An eye for an eye.
One of the most important sensory inputs for human beings is our
eyes. They allow us to determine things like color, size, and distance
so that we can understand the world around us. There is a lot of
physical variance between different people’s eyes, and this creates a
challenge for any kind of MR designer—how to interface their spe‐
cific optical display with our eyeballs successfully.
One of the biggest challenges for MR is matching our natural ability
to visually traverse a scene, where our eyes automatically calculate
the depth of field, and correctly focus on any objects at a wide range
of distances in our FoV (Figure 4-1). Trying to match this mechani‐
cal feat of human engineering is incredibly difficult when we talk
about display technologies. Most of the displays we have had around
us for the past 50 years or so have been flat. Cinema, television,
computers, laptops, smartphones, tablets; we view them all at a
given distance from our eyes, with 2-D user interfaces. Aside from
the much older CRT display tech, LCD screens have dominated the
computing experience for the past 10 to 15 years. And this has been
working pretty well with our eyes—until the arrival of MR.
Eye Tracking: A Tricky Approach to the Inference of Gaze-Detection 
| 
25

Figure 4-1. This diagram proves unequivocally that we’re just not
designed for this
When the Oculus Rift VR headset launched on Kickstarter, it was
heralded as a technological breakthrough. At $350, it was orders-of-
magnitude cheaper than the insanely expensive VR headsets of yore.
One of the reasons for this was the smartphone war dividends: access
to cheap LCD panels that were originally created for use in smart‐
phones. This allowed the Rift to have (at the time) a really good dis‐
play. The screen was mounted inside the headset, close to the eyes,
which viewed the screen through a pair of lenses in order to change
the focal distance of the physical display so that your eyes could
focus on it correctly.
One of the side effects of this approach is that even though a simula‐
ted 3-D scene can be shown on the screen, our eyes actually don’t
change focus and, instead, are locked to a single near-focus. Over
time this creates eye strain, which is commonly referred to as
vergence-accommodation conflict (see Figure 4-2).
26 
| 
Chapter 4: Examples of Approaches to Date

Figure 4-2. Vergence-accomodation conflict
In the real world, we constantly shift focus. Things that are not in
focus appear to us as out of focus. These temporal cues help us
understand and perceive depth. In the virtual world, everything is in
focus all the time. There are no out-of-focus parts of a 3-D scene. In
VR headsets, you are looking at a flat LCD display, so everything is
perfectly in focus all the time. But in MR, a different challenge is
found—how do you view a virtual object in context and placement
in the physical world? Where does the virtual object “sit” in the
FoV? This is a challenge more for the technologies surrounding
optical displays, and in many ways, the only way to overcome this is
by using a more advanced approach to optics
Enter the light field!
Of Light Fields and Prismatics
Most conventional displays utilize a single field of light; that is, all
light arrives at the same time, spread across the same plane. But light
field technology changes that, and it could potentially eliminate the
issue of vergence-accommodation conflict and depth-of-field issues.
One particular company is attempting to fix this problem, and it has
the deep pockets needed to do so. Developing new kinds of optical
technologies is neither cheap nor easy, so Magic Leap has decided to
build its own optical system from scratch in an effort to make the
Of Light Fields and Prismatics 
| 
27

most advanced display technology the world has ever seen. What lit‐
tle we do know about Magic Leap’s particular approach is that it uti‐
lizes a light field that is refracted at differing wavelengths through
the use of a prismatic lens array. Rony Abovitz, the CEO of Magic
Leap, often enthuses about a new “cinematic reality” coming with
their technology.
Computer Vision: Using the Technologies That
Can “Rank and File” an Environment
Seeing Spaces
Computer vision (CV) is an area of scientific research that, again,
could take up an entire set of reports alone. CV is a technological
method of understanding images and performing analysis to help
software understand the real world and ultimately help make deci‐
sions. It is arguably the single most important and dependent tech‐
nological aspect of MR to date. Without CV, MR is rendered
effectively useless. With that in mind, there is no singular approach
to solving the problem of “seeing spaces,” and there are many var‐
iants of what is known as simultaneous localization and mapping
(SLAM) such as dense tracking and mapping (DTAM), parallel track‐
ing and mapping (PTAM), and, the newest variant, semi-direct mon‐
ocular visual odometry (SVO). As a designer, understanding the
capabilities that each one of these approaches affords us, allows for
better-designed experiences. For example, if I wanted to show to the
wearer an augmentation or object at a given distance, I need to
know what kind of CV library is used, because they are not all the
same. Depth tracking CV libraries will only detect as far as 3 to 4
meters away from the wearer, whereas SVO will detect up to 300
meters. So knowing the technology you are working with is more
important than ever. SVO is especially interesting because it was
designed from the ground up as an incredibly CPU-light library that
can run without issue on a mobile device (at up to 120 FPS!) to pro‐
vide unmanned aerial vehicles (UAVs), or drones, with a photogram‐
metric way to navigate urban environments. This technology might
enable long-throw CV in MR headsets; that is, the ability for the CV
to recognize things at a distance, rather than the limiting few meters
a typical depth camera can provide right now.
28 
| 
Chapter 4: Examples of Approaches to Date

One user-experience side effect of short-throw or depth camera
technology is that if the MR user is traversing the environment, the
camera does not have a lot of time to recognize, query, and ulti‐
mately push contextual information back to the user. It all happens
in a few seconds, which can have an uncalming effect on the user,
being hit by rapid succession of information. Technologies like SVO
might help to calm the inflow because the system can present infor‐
mation of a recognized target to the user in good time, well before
the actual physical encounter takes place.
The All-Seeing Eye
When most people notice a camera lens pointing at them, some‐
thing strange happens—it’s either interpreted as an opportunity to
be seen, to perform, bringing out the inner narcissism that many
enjoy flaunting and watching, or the reaction is adverse and some‐
thing more akin to panic—an invasion of privacy, of being watched,
observed, and monitored. Images of CCTV, Orwellian dystopias,
and other terrifying futures spring to mind. Most of these reactions
—both good and bad—are rooted in the idea of the self; of me, as
being somewhat important. But what if those camera lenses didn’t
care about you? What if cameras were just a way for computers to
see? This is the deep-seated societal challenge that besets any adop‐
tion of CV as a technological enabler. How do we remove the social
stigma around technology that can watch you? The computer is not
interested in what you are doing for its own or anyone else’s amuse‐
ment or exploitation, but to best work out how to help you do the
things you want to do. If we allowed more CV into our lives, and
allow the software to observe our behavior, and see where routine
tasks occur, we might finally have technology that helps us—when it
makes sense—to interject into a situation at the right time, and to
augment our own abilities when it sees us struggling. A recent
example of this is Tesla’s range of electric cars. The company uses
CV and a plethora of sensors both inside and outside the vehicle to
“watch” what is happening around the vehicle. Only recently, a Tesla
vehicle drove its owner to a hospital after the driver suffered a medi‐
cal emergency and engaged Autonomous Mode on the vehicle. This
would not have been possible without the technology, and the
human occupant trusting the technology.
Right now there is a lot of interest in Artificial Intelligence (AI) to
automate tasks through the parsing and processing of natural lan‐
Computer Vision: Using the Technologies That Can “Rank and File” an Environment 
| 
29

guage in an attempt to free us from the burden of continually inter‐
acting with these applications—namely, pressing buttons on a
screen. All these recent developments are a great step forward, but
right now it still requires the user to push requests to the AI or Bot.
The Bot does not know much about where you are, what you are
doing, who you are with, or how you are interacting with the envi‐
ronment. The Bot is essentially blind and requires the user to
describe the things to it in order to provide any value.
MR allows Bots to see. With advanced CV and embedded camera
sensors in a headset, AI would finally be able to watch and learn
through natural human behaviors, as well as language, allowing
computers to pull contextual information as necessary. The poten‐
tial augmentation of our skills could revolutionize our levels of effi‐
ciency—freeing up our minds from pushing requests to systems and
awaiting responses, to getting observational and contextual data as a
way to help us make better informed decisions. Of course, none of
this would be possible without the Internet, and, as was mentioned
earlier in this report, the Internet will take center stage in helping
couple the CV libraries that run on the headset with data APIs that
can be queried in real time for information. The incoming data that
flows back to the headset will need to be dealt with, and this is where
good interface design matters—to handle the flow of information
such that it stays relevant to the user’s context, and to purge infor‐
mation in a timely manner so as to not overwhelm the user. This is
the real challenge that awaits the future MR designer: how to attune
for temporality.
30 
| 
Chapter 4: Examples of Approaches to Date

CHAPTER 5
Future Fictions Around the
Principles of Interaction
Remain calm, serene, always in command of yourself. You will then find
out how easy it is to get along.
—Paramahansa Yogananda
Frameworks for Guidance: Space, Motion,
Flow
The real world—use it!
The physical environment will serve to help reinforce context
around virtual objects, fixing their placement and positioning. Uti‐
lizing real-world objects and using them as anchors for virtual
objects could allow a person wearing a mixed reality (MR) headset
to have a more contextual understanding of anything she might
encounter in the space. One technological challenge is object drift,
which is when a virtual object seems unattached from the environ‐
ment. This can have the side effect of breaking the immersiveness
and believability of an experience. The other side effect is limiting
the virtual visual pollution that poses a great barrier to social accept‐
ance. These are virtual objects and data drifting around real world
spaces, potentially having pileups of virtual objects with little con‐
text as to what they are and why they are there. This kind of visual
overload is perfectly laid out in director Keiichi Matsuda’s short film
Hyper-Reality. The film provides a really compelling reason to make
sure the real world is not stuffed-to-the-gills with random virtual
31

objects and data. It is up to the designer to ensure that the interfaces
remain calm and coherent within the context of use and to respect
the physical environment within which they appear.
With great power comes great responsibility, and so the budding
future MR designer is entrusted to ensure that the manifesting of
information is done so as to not physically endanger the user. For
example, although it would make contextual sense to show the user
map data if that user were wearing the headset while driving a vehi‐
cle, what if the computer vision (CV) detects an object or something
up ahead, like a roadside truck stop, and is able to provide the user
with contextually useful information through object recognition and
the web connection? Should this information be shown at all?
Should it then be physically attached to the truck stop? How much
information is too much information in your field of view (FoV)
while driving? Should it alert the user or employ a change in visual
intensity as you approach the target? When should the information
be purged? All of these questions have many ways to be answered,
but maybe the safest mantra to adopt is truly a “less is more”
approach to information surfacing. Keeping the incoming flow of
information slower when physically moving fast, and faster when
physically moving slower is a good rule of thumb in order to keep
eyes on the road ahead, hands on the steering wheel, and the mind
concentrated and focused on the actual task at hand. That is, until
there are self-driving cars everywhere.
How to Mockup the Future: Effective
Prototyping
Prototyping is a cornerstone of every designer’s approach at making
things more tangible. Interface design has come leaps and bounds in
the past few years with a plethora of prototyping tools and services
to get your idea up and tested faster. But alas, the future MR
designer is, right now, a little bit underserved. Most designers of the
Web or mobile come from a background of 2-D design tools, and
when designing for virtual reality (VR), augmented reality (AR), or
MR, are faced with a new challenge: spatiality. The challenge is com‐
pounded when presented with the reality of having to learn game
development tools in order to build these experiences. This can make
the entire process of designing for MR feel laborious, emotionally
overwhelming, and unnecessarily complex. But it doesn’t need to be
32 
| 
Chapter 5: Future Fictions Around the Principles of Interaction

this way. Yes, if you want to build the software, you will most defi‐
nitely need to learn one of the 3-D game engines: Unity and Unreal
are the most well supported and well documented ones out there.
But to begin, there is still sketching with pen and paper.
Less Boxes and Arrows, More Infoblobs and
Contextual Lassos
Figure 5-1 shows a 2-D–friendly way to explain where things might
be inside of a typical MR experience. There are two primary view‐
ports: top-down and side-on. Top-down helps understand where
things are in relation to the user, who is always in the center. Objects
can surround the user, but it’s important to understand the virtual
distance from the objects. This is your FoV, which should closely
align with what the camera sensors can see—optimally the camera
should see wider than your actual FoV so as to preload any objects
before you can see them.
Figure 5-1. This is a wireframing spatial template, that is handy for
quickly mapping the positions of interface elements and objects, and
making it understandable to others
When it comes to physical distance from the user, this also translates
to legible degradation: the further a virtual object is from you, the
more difficult it is to make out details about the object. In particular,
text is a challenge as it gets further away. So here, I’ve classified the
Less Boxes and Arrows, More Infoblobs and Contextual Lassos 
| 
33

radius around the user going from the closest to the furthest dis‐
tance away as such.
The Interaction Plane
This is the immediate area surrounding the user, which is typically
no further away than a comfortable arm’s length (which in this case
means approximately 50–70 cm from the user). You want the inter‐
actions to be close to the user so as to feel connected to whatever
task or behavior you are trying to do (Figure 5-2). Manipulating
objects on the other side of a room would feel disconnected and
would bring up a different challenge: what if I only wanted to
manipulate a specific object or piece of data? This is why a close
physical proximity is beneficial. It allows the user to feel in control
of exactly what objects or data he wants to manipulate. The other
reason for such close quarters is to reduce the “gorilla arm” effect on
the user. Physical fatigue is going to be a real problem with a lot of
these experiences, so keep arm movements calm and focused on the
task at hand.
Figure 5-2. This diagram shows the interaction challenges of virtual
buttons; because there is no true depth of field, locating buttons within
the interaction plane is incredibly difficult and generally ends up a
frustrating experience for the user; try to avoid these types of floating
controls, and use gesture recognition, instead
34 
| 
Chapter 5: Future Fictions Around the Principles of Interaction

The Mid Zone
This is where the majority of meaningful objects and data can be
manifested at full resolution. Text will still pose a challenge, espe‐
cially if composited onto objects at an angle. This is the most com‐
mon “view” shown in any promotional MR video—it’s always a user
within a small room that allows for everything to appear clearly,
composited against the walls, in full resolution. The mid zone is also
where CV and tracking is most effective, because as more distance
comes between the user and any given surface, the accuracy of the
tracking drops, which increases the incidents of objects swimming;
that is, becoming de-anchored from original position. Beyond a few
meters, depth cameras cannot see anything. And you’d better hope
this is not in a room with black walls because that’s where tracking
becomes really funky and begins losing it altogether.
The Legibility Horizon
This is the effective distance that objects can be discovered and seen
clearly. Anything beyond the horizon is reduced to symbolic mean‐
ing. Imagine a set of virtual sticky notes on a wall. When I am close
enough to be able to actually read them, they appear as fully ren‐
dered objects. When I back away, as I reach the legible horizon, they
reduce to a symbolic image that tells me there are notes there, noth‐
ing more. This can have a potential side effect of reducing the GPU
load on the headset, as objects are dynamically loaded and unloaded
depending on distance from the object.
All of this is to help the designer coming from a more traditional 2-
D design background to begin thinking more spatially. It’s also to
help get ideas across to developers who are well versed in 3-D con‐
structs.
The diagram in Figure 5-1 is here to help designers think more
about placement of objects, menus, actions, and so on. Print it out
and play with it. Make a better one.
PowerPoint and Keynote Are Your Friends!
When it comes to starting to flesh out design ideas, one of the things
that quickly becomes apparent in designing for MR is the need to
see how it might actually look, composited over the real world. All
those challenges around legibility and usability begin to pop up:
PowerPoint and Keynote Are Your Friends! 
| 
35

what kind of colors work on a holographic display? What about the
ambient lighting of the room? Should the information sit in the
middle of the users view? HOW BIG SHOULD THE FONTS BE?
This is when you need to begin getting some realism into the mock‐
ups. Luckily, it’s not as difficult as it seems, and to do this, software
like Keynote or PowerPoint can help. They are actually pretty good
at dynamically loading objects, compositing elements into a slide,
adding animation, and so on. Begin with a photograph of the
intended real-world scenario—it doesn’t need to be amazingly high-
resolution—and drop it into a slide. You can add elements from
your designs on top and play with the opacity of the elements. Note
at which point the elements begin to become unusable. White is the
strongest (noncolor) to work in a holographic display. Most of your
interface should be white. Subtle shades of color struggle to show up
because the background of the real world and the natural lux levels
effect the display contrast. Black is the secret—it doesn’t show up at
all as black. Black shows up as clear. In fact, black is used heavily to
mask areas you don’t want to see. As you can probably tell, if your
experience relies on a deep color reproduction accuracy and gamut,
well...don’t bother. No one will be color-proofing print jobs in MR
anytime soon. So play to the strengths of MR, don’t force it to do
what it cannot do well.
Using Processing for UI Mockups
I want to give a mention to the use of Processing (https://process‐
ing.org) for making incredibly high-fidelity 3-D prototypes. This is
a much easier application language to learn for most designers than
C-based languages because it is based on Java, with variants in Java‐
Script and Python. Heavily used in modern graphic arts, this flexible
framework has been used to make many kinds of interactive experi‐
ences, and recently it has been used to mockup VR and MR inter‐
faces. Of course, this is still a major leap from building out
interactive keynote slides, and for many designers, might be too
close to.
Building Actual MR Experiences
Yes, eventually we all end up here. I’m talking about building real
applications using Unity3d and Unreal, which, although on the sur‐
face might seem like slightly more involved versions of Adobe Pho‐
36 
| 
Chapter 5: Future Fictions Around the Principles of Interaction

toshop, are labyrinthine in complexity, contain a lot of things that a
future MR designer should never need to know about, and use arbi‐
trary naming conventions for everything. Oh, and it really helps if
you understand C, C#, or C++, because when you embark on creat‐
ing an MR experience, you will eventually launch Mono and face a
wall of native code. Depending on which MR platform you are tar‐
geting, you will need to download its own specific SDK that puts its
own functions into Unity or Unreal so that you can develop for that
specific headset directly. If you want to port to another MR headset,
you’ll need to download its own SDK and port the system calls
across. The future is difficult. The future sure seems a lot more
involved than the previous future, which was the Web in a browser
window. Of course, at this point, you might be asking, “Where is the
Web in all this?”
A Glimmer of Hope
Over on the Web, enterprising future-focused developers have been
working on a version called WebVR. Intended to allow web-savvy
designers and developers to build compelling VR experiences within
the web browser, this started out as a Mozilla/Google shared attempt
to bring the power of web technologies—and their gargantuan
development communities—to the future, targeting VR first. Early
WebVR demos worked pretty well on a desktop but pretty poorly, or
not at all, on mobile devices. Now, things are much better—WebVR
works incredibly well on both desktop and mobile browsers. Mozilla
launched A-Frame (https://aframe.io/) as a way to make develop‐
ment and prototyping easier in WebVR. Overall, the future is hope‐
ful for web-based VR. WebVR can allow for rapid prototyping and
simulation of MR experiences with the biggest issue being latency
and motion-to-photon round-trip times, and the need for a web
browser on mobile that supports WebRTC for accessing the camera.
At the very minimum, the use of A-Frame and WebVR is a valuable
tool for designers who feel more comfortable in web-based lan‐
guages to begin prototyping or mocking up MR experiences. But
one thing is clear: there will be a real need for a prototyping tool
that is the MR equivalent of Sketch in order to speed up the design‐
er’s efficiency to the level needed to really move fast and break
things.
Building Actual MR Experiences 
| 
37

Transition Paths for the Design Flows of Today
From paper to prototype to production. Designers will need to make
some new friends: working with 3-D artists, modelers, and anima‐
tors is very different to what interaction designers are used to.
The typical range of human encounters for a designer in a product
team range from the managers who decide who does what (some‐
times), to the developers who build it (always). The handoff between
these team members is well documented and usually falls into a typ‐
ical product process like Agile, Lean, Continuous Delivery, or some
other way to speed up and increase value. A modern designer,
depending on what she is working on, is often expected to handle
everything from the interaction design, research, best practices
around visual taxonomy, through to sometimes building a fully
functional app (the unicorns!). For a designer to handoff specifica‐
tion documents to a developer, this is not really a big deal.
But for the future MR designer, again, things are a little bit more
complex and involved. There might now be new members of your
team—people with titles like “animator,” or “3-D modeler.” But we
will need to speak the same language because these new members of
a design team are essential with their knowledge around 3-D as you
might be in the 2-D information space. Thus, the biggest challenge
is getting all these valuable project contributors lined up and in
sync. But until interaction design or user experience design begin
exploring and teaching spatial design, we are dependent on those
who already deeply understand 3-D spatial design. So, get to know
your local 3-D modeler and animator, and understand that making
stuff in 3-D is incredibly time consuming (argh, all these extra
dimensions!). Utilizing frameworks, as shown in the previous sec‐
tion, helps designers cross language/interpretation barriers, and
pretty soon it will feel natural.
In the end, the design process will remain as it always was—in a
state of continual flux and learning—but now with new actors and
agents to deal with. It’s simply the nature of increasingly complex
and involved technologies, and so it needs more broad knowledge
(like understanding differing optical displays, computer vision tech‐
nologies, etc.) to deliver quality experiences. Designers should know
that MR is not a simple proposition or transition, and might be the
most challenging platform to design for to date. But remember:
there is no wrong way to go about this. Embrace the freedom this
38 
| 
Chapter 5: Future Fictions Around the Principles of Interaction

emergent platform gives, and respect the incredibly visceral effect
your experiences will have on the user.
The Usability Standards and Metrics for
Tomorrow
So, how do we know what design approach works in MR if there has
been nothing to really reference and no body of evidence to date on
what works well? Where are all the best practice books? Where’s the
Dribbble of MR? None of these foundations and guidance tomes
exist yet, which makes the question of “Did I design it right?” a
much more complex question. There isn’t yet a really wrong answer.
But we do know that we should not try to just force old world inter‐
face approaches into the world of MR. Here’s a question I was once
asked by a room of design students: “So in this virtual world, if I
wanted to read a book, the book will behave like a real book, virtu‐
ally situated on a virtual shelf, in a virtual library, right?” Not nec‐
cessarily. We are not building these new behaviors to simply emulate
all the constraint and physical boundaries that we are forced to put
up with in the real world. The purpose of MR is to allow new ways
to understand and parse information. Be bold, and break rules.
We need to let go of the past ways of measuring an experience’s suc‐
cess; for example, the way a user effectively completes a set task and
moves toward something more cerebral, as the more classically
mechanical nature of the interface will slip into the background, and
the emotive qualities of an experience take center stage. We may end
up measuring the effectiveness of an MR experience not by observ‐
ing the hands, but more by the heart racing and the pupils’ dilation.
What kind of tools can the future MR designer use to better under‐
stand what kinds of augmentations attract attention or are ignored?
Well, there are already a few services out there that begin to measure
where the user is looking and what kinds of objects are being viewed
by building heat maps and journey maps of movement. Most of
these have focused on VR because there is much more of this kind
of content than MR at the moment. But expect more of these tools
to port over to the more popular MR platforms in the near future.
For now, here’s a couple of companies looking into the space:
The Usability Standards and Metrics for Tomorrow 
| 
39

• Cognitive VR: http://cognitivevr.co
• Fishbowl VR: www.fishbowlvr.com
40 
| 
Chapter 5: Future Fictions Around the Principles of Interaction

CHAPTER 6
Where Are the High-Value Areas of
Investigation?
Understanding your employee’s perspective can go a long way toward
increasing productivity and happiness.
—Kathryn Minshew
The Speculative Landscape for MR Adoption
We’ve looked at a lot of the current uses of mixed reality (MR) for
applications, and the way that we work right now, but what about
new types of uses? What can MR do that might entirely change a
given industry?
Health Care
MR allows people in the medical profession, from students just
starting out, all the way to trained neurosurgeons, to “see” the inside
of a real patient without opening them up. This technology also
allows effective remote collaboration, with doctors able to monitor
and see what other doctors might be working with. Companies like
AccuVein make a handheld scanner that projects an image on the
skin of the veins, valves, and bifurcations that lie underneath to help
make it easier for doctors and nurses to locate a vein for an injec‐
tion.
The biggest challenge in the healthcare industry is the certifications
and requirements needed to allow this class of device into hospitals.
41

Design/Architecture
One of the most obvious use cases for this kind of technology is in
design and architecture—it’s no surprise that the first Hololens dem‐
onstration video showcased a couple of architects (from Trimble)
using the Hololens to view a proposed building. As of today, most 3-
D work is still done on 2-D screens, but this will change and exam‐
ples of creating inside of virtual environments have already been
shown, such as Skillman and Hackett’s excellent Tiltbrush applica‐
tion that allows the user to sculpt entirely within a virtual space.
Logistics
This industry is vast and is the cornerstone for how things move
around the planet. To make this run smoother is in everybody’s
interest, and so it was no surprise when Google’s Glass found deep
support in the logistics industry as it allowed workers in vast ware‐
houses to quickly locate and pick up items, and then notify the sys‐
tem to remove the items from inventory and have the package sent
off to the right place.
Manufacturing
Improving manufacturing efficiencies is another strong existing use
case for MR-type technologies. Toshiba outfitted their automotive
factory workers with the Epson Moverio smart glasses a few years
ago to see how productivity gains could be found using this hands-
free technology. Expect MR to only grow inside of the manufactur‐
ing industry, as it empowers workers with the information they
need, in the right context, and at the right time—heads up, and
hands free.
Military
It’s not exactly surprising that MR has already played a large role in
the military.For many years now, fighter pilots have been wearing
helmets that overlay a wealth of information. The challenge is get‐
ting wider adoption on the ground, from training soldiers in com‐
munications, to medical support, and, of course, to deeply enhance
the situational awareness in the field. The biggest challenge here is
on the physical device itself; the headset must be rugged enough to
withstand some seriously rough environmental conditions like rain,
42 
| 
Chapter 6: Where Are the High-Value Areas of Investigation?

sand, dirt, and so on, while also being something that does not pose
a direct danger to the wearer if in a hostile situation.
Services
The most likely touchpoint for consumers to understand the value
that MR can bring is in the service industry. What if you could put
on an MR headset and have it guide you to fix a broken water pipe?
Or maybe help you to understand the engine of your car so that you
can fix it? What if there were a human able to connect and walk you
through a sequence of tasks? This is when people will feel less alone
to cope with issues, and more empowered to get on with things
themselves.
Aerospace
Nasa has already begun using the Hololens for simulating Mars by
utilizing the holographic images sent back from the Mars Rover.
This is not surprising given that NASA was one of the first organiza‐
tions to begin exploring VR back in the 1980s. The Hololens has
already turned up in the International Space Station for use in
Project Sidekick, which is a project to enable station crews with
assistance when they need it.
Automotive
In October of 2015, the automobile industry held its first conference
in automotive production that covered how MR can be used across
the board from helping with production to driving sales. Mini also
launched a new vehicle that shipped with a pair of MR glasses last
year to help Mini drivers have access to extra information while
driving.
Education
MR lends itself to educational use very well—it allows for a more
tactile and kinesic approach to learning, like having to turn an
object around to inspect it by using your hands versus clicking or
dragging with a mouse. As mentioned earlier in this report, Magic
Leap puts particular emphasis on the use of its technology to inspire
wonder, and so MR could transform the classroom as we know it
today into something far more wondrous for future generations.
The Speculative Landscape for MR Adoption 
| 
43

The Elephant in the Room: Gaming
Yes, you didn’t think I would leave out all the fun right? Gaming is
one area for MR that could also create the tipping point for con‐
sumer adoption. Magic Leap has shown some very compelling vid‐
eos that allow the wearer to live out fantastic situations, with
monsters, robots, ray guns, and the like. Hololens has also show‐
cased its “Project X” game, which has aliens climbing out of holes
that appear in your living room wall. The future is strange.
Emergent Futures: What Kinds of Business
Could Grow Alongside Mixed Reality?
Humans-as-a-Service
With the adoption of MR and the ability for headsets to “see” the
environment, expect an entire industry to emerge around (real, not
Bots) humans that can be hired to (virtually) accompany you on
your travels, as tour guides, friendly counsellors, human tama‐
gotchis, and even adult entertainment. All for a low monthly fee, of
course.
Data Services
The web coupled with computer vision will potentially launch an
entire new wave of innovation around data services. Imagine start‐
ups of the future that really concentrate on inventing or discovering
entirely new ways to parse particular sets of data and can serve up its
findings in real-time to MR users who pay a monthly fee to have
access to this information. According to many VCs I have spoken
with, and depending on what kind of service, these might become
the largest and most lucrative aspects of MR in the future. Big data,
indeed.
Artificial Intelligence
Automating routine behaviors is another emergent technological
direction. Although Artificial Intelligence (AI) and Bots are incredi‐
bly rudimentary at the moment, imagine how AI that can physically
identify the environment through your MR headset could take over
tasks that it observes the user doing repeatedly. After you have the
coupling of AI with computer vision, and then combine that with
44 
| 
Chapter 6: Where Are the High-Value Areas of Investigation?

the ability to automate many processes, you might never need to
physically perform certain routine tasks again. Merely gazing at the
device you want to operate triggers an action, or pulls up data, insti‐
gated by the AI, and based on previous routine behaviors.
Fantastic Voyages
As mentioned earlier in the report, with the increasing realism of
MR over time, the fidelity and believability will also increase, and
with it, expect fantasies to be played out, authentically merged with
your real life as a game, with the genre of role-playing games the
most logical fit. Don’t you want to see the Blue Goblins lurking
behind the kitchen table? Who’s that at the front door? MR could
provide the ultimate gaming voyage for users, probing deep into
latent fears, or providing light entertainment to brighten up your
day. It won’t be surprising to have Fantasy-as-a-Service in a few
years. Who doesn’t enjoy a bit of escapism now and then?
Emergent Futures: What Kinds of Business Could Grow Alongside Mixed Reality? 
| 
45


CHAPTER 7
The Near-Future Impact on Society
The first resistance to social change is to say it’s not necessary.
—Gloria Steinem
The Near-Future Impact of Mixed Reality
It is an incredibly exciting time to be a designer. Quite a few of the
shackles of our professional history are about to be thrown out the
window. This is at once both a blessing and a curse because design‐
ers have come to enjoy and respect constraint imposed by those ever
present rectangles embedded in our lives. But a new chapter of
human-computer interaction is beginning, and so the early design
approaches that emerge around mixed reality (MR) will continue to
evolve and change for some time ahead. This report only intends to
help frame what’s ahead—there are no best practices at this point.
What we can say today, though, is that MR, if adopted into common
use, will eventually have a profound impact on our relationship with
things—our world, our work, our lives. It could potentially turn us
into the augmented superhumans we have always liked to envision
ourselves evolving into. At the very minimum, we will all be more
closely bonded and reliant on technology. We will really all be
cyborgs then. Of course, the potential impact on society should not
be underestimated; we may not look at the world the same way, and
our understanding of what is reality and what is not might come
into question. Designers will be coerced to evolve from being the
mechanics of the interface, routed deeply in logic, to the spell-
casters and alchemists of tomorrow, using techniques that lean
47

increasingly on understanding psychology and sociology. This
developmental path is already forming with the rise of Artificial
Intelligence and conversational interfaces. Eventually, after the first
wave of mixed reality devices have been fully accepted and
entrenched into our everyday lives, it will be only a relatively short
hop, skip, and jump toward fully embedded wetware, but that’s a
whole different type of immersion entirely...
48 
| 
Chapter 7: The Near-Future Impact on Society

About the Author
Kharis O’Connell is the Head of Product for Archiact—Canada’s
fastest growing VR/MR studio. He has over 18 years of international
experience in crafting thoughtful products and services, and before
joining Archiact, co-founded the emerging-tech design studio:
HUMAN, and worked at Nokia Design in Berlin, Germany as lead
designer on a multitude of products. Previous works also include
flagship projects for Samsung—helping design their first smart‐
phones back in 2008, and an interactive hardware/software installa‐
tion for Nike.


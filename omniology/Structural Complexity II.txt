EATCS 
Monographs on Theoretical Computer Science 
Volume 22 
Editors: W. Brauer G. Rozenberg A. Salomaa 
Advisory Board: G.Ausiello M.Broy S.Even 
J.Hartmanis N.Jones T.Leighton M.Nivat 
C. Papadimitriou D. Scott 

Jose Luis Balcazar 
Josep Diaz 
Joaquim Gabarr6 
Structural Complexity II 
With 72 Figures 
Springer-Verlag Berlin Heidelberg New York 
London Paris Tokyo Hong Kong 

Authors 
Prof. Dr. Jose Luis Balcizar 
Prof. Dr. Josep Diaz 
Prof. Dr. Joaquim Gabarr6 
Facultat d'Infonnatica 
Universitat Politecnica de Catalunya 
Pau Gargallo, 5, E-08028 Barcelona, Spain 
Editors 
Prof. Dr. Wilfried Brauer 
Institut fUr Infonnatik, Technische Universitiit Miinchen 
Arcisstrasse 21, D-8000 Miinchen 2, FRG 
Prof. Dr. Grzegorz Rozenberg 
Institute of Applied Mathematics and Computer Science 
University of Leiden, Niels-Bohr-Weg 1, P. O. Box 9512 
NL-2300 RA Leiden, The Netherlands 
Prof. Dr.Arto Salomaa 
Department of Mathematics, University ofTurku 
SF-20 500 Turku 50, Finland 
ISBN-13 :978-3-642-75359-6 
e-ISBN-13:978-3-642-75357-2 
DOl: 10.1007/978-3-642-75357-2 
Library of Congress Cataloging-in-Publication Data 
(Revised for volume 2) 
Balc3zar, Jose Luis. 
Structural complexity. 
(EATCS monographs on theoretical computer science; v.ll, 
Includes bibliographies and indexes. 
1. Computational complexity. I. Diaz, J. (Josep), 1950-
. II. Gabarr6, Joaquim. III. Title. 
IV. Series: EATCS monographs on theoretical computer science; v.ll, etc. 
QA267.B34 1988 511.3 87-36933 
ISBN-13:978-3-642-75359-6 
This work is subject to copyright. Ail rights are reserved, whether the whole or part of the 
material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, 
recitation, broadcasting, reproduction on microfilms or in other ways, and storage in data 
banks. Duplication of this publication or parts thereofis only permitted under the provisions 
of the German Copyright Law of September 9, 1%5, in its current version, and a copyright fee 
must always be paid. Violations fall under the prosecution act of the German Copyright Law. 
© Springer-Verlag Berlin Heidelberg 1990 
Softcover reprint of the hardcover 1st edition 1990 
The use of registered names, trademarks, etc. in this publication does not imply, even in the 
absence of a specific statement, that such names are exempt from the relevant protective laws 
and regulations and therefore free for general use. 
2145/3020-543210 - Printed on acid-free paper 

Preface 
This is the second volume of a two volume collection on Structural Complexity. 
This volume assumes as a prerequisite knowledge about the topics treated in 
Volume I, but the present volume itself is nearly self-contained. 
As in Volume I, each chapter of this book ends with a section entitled 
"Bibliographical Remarks", in which the relevant references for the chapter are 
briefly commented upon. These sections might also be of interest to those 
wanting an overview of the evolution of the field, as well as relevant related 
results which are not included in the text. Each chapter includes a section 
of exercises. The reader is encouraged to spend some time on them. Some 
results presented as exercises are occasionally used later in the text. A reference 
is provided for the most interesting and for the most useful exercises. Some 
exercises are marked with a • to indicate that, to the best knowledge of the 
authors, the solution has a certain degree of difficulty. 
Many topics from the field of Structural Complexity are not treated in depth, 
or not treated at all. The authors bear all responsibility for the choice of topics, 
which has been made based on the interest of the authors on each topic. 
Many friends and colleagues have made suggestions or corrections. In partic-
ular we would like to express our gratitude to Richard Beigel, Ron Book, Rafael 
Casas, Jozef Gruska, Uwe Schoning, Pekka Orponen, and Osamu Watanabe. Ja-
cobo Toran not only checked most of the manuscript but suggested and worked 
out some of the proofs in the book. We also would like to thank Springer-Verlag, 
especially Dr. H. Wossner and Mrs. I. Mayer, for the assistance and patience 
through the elaboration of these two volumes, and Rosa Martin for her assis-
tance with the typesetting software. Finally we will like to thank C. Alvarez, 
J. Castro, R. Gavalda, A. Lozano, M. J. Serna, A. Torrecillas and B. Valles, 
graduate students in our Department, which had to listen to the genesis of most 
of the chapters in the book. To them we dedicate this work. 
Barcelona, September 11, 1989 
J. L. Balcazar 
J. Dfaz 
J. Gabarro 

Contents 
Introduction 
1 Vector Machines 
1.1 
Introduction 
1 
4 
4 
1.2 
Vector Machines: Definition and Basic Properties 
. 
5 
1.3 
Elementary Matrix Algebra on Vector Machines . . 
11 
1.4 
Relation Between Vector Machines and Turing Machines. 
21 
1.5 Exercises......... 
29 
1.6 
Bibliographical Remarks . . . . . . . . . . . . . . . . . . 
32 
2 The Parallel Computation Thesis 
33 
2.1 
Introduction . . . . . . . . . . . . . . . . 
33 
2.2 
An Array Machine: the APM . . . . . . . 
34 
2.3 
A Multiprocessor Machine: the SIMDAG . 
39 
2.4 
A Tree Machine: the k-PRAM 
46 
2.5 
Further Parallel Models . 
52 
2.6 Exercises......... 
57 
2.7 
Bibliographical Remarks. 
61 
3 Alternation 
63 
3.1 
Introduction . . . . . . . . . . . . 
63 
3.2 
Alternating Turing Machines ... 
63 
3.3 
Complexity Classes for Alternation 
70 
3.4 
Computation Graphs of a Deterministic Turing Machine 
78 
3.5 
Determinism Versus Nondeterminism for Linear Time 
88 
3.6 Exercises......... 
91 
3.7 
Bibliographical Remarks. . . . . . . . . . . . . . . . 
94 
4 Uniform Circuit Complexity 
4.1 
Introduction . . . . . . . 
4.2 
Uniform Circuits: Basic Definitions . . . . . . . . . . 
4.3 
Relationship with General-Purpose Parallel Computers 
4.4 
Other Uniformity Conditions ............ . 
97 
97 
97 
103 
108 

V I I I 
Contents 
4.5 
Alternating Machines and Uniformity 
110 
4.6 
Robustness of NC and Conclusions 
115 
4.7 Exercises......... 
115 
4.8 
Bibliographical Remarks. . . . 
117 
5 Isomorphism and NP-completeness 
119 
5.1 
Introduction . . . . . . . . . . 
119 
5.2 Polynomial Time Isomorphisms 
119 
5.3 
Polynomial Cylinders 
122 
5.4 Sparse Complete Sets . . 
125 
5.5 Exercises......... 
131 
5.6 
Bibliographical Remarks. 
132 
6 Bi-Immunity and Complexity Cores 
134 
6.1 
Introduction . . . . . . . . . . . . . . . . . . . . . . 
134 
6.2 
Bi-Immunity, Complexity Cores, and Splitting .. . . 
134 
6.3 
Bi-Immune Sets and Polynomial Time m-Reductions . 
136 
6.4 
Complexity Cores and Polynomial Time m-Reductions . 
139 
6.5 
Levelability, Proper Cores, and Other Properties 
143 
6.6 Exercises......... 
146 
6.7 
Bibliographical Remarks. 
147 
7 Relativization 
149 
7.1 
Introduction . . . . . . . . . . . 
149 
7.2 
Basic Results. . . . . . . . . . . 
150 
7.3 
Encoding Sets in NP Relativized 
154 
7.4 
Relativizing Probabilistic Complexity Classes. 
156 
7.5 
Isolating the Crucial Parameters 
162 
7.6 
Refining Nondeterminism . . . . 
165 
7.7 
Strong Separations. . . . . . . . 
168 
7.8 
Further Results in Relativizations 
172 
7.9 Exercises......... 
174 
7.10 Bibliographical Remarks. 
176 
8 Positive Relativizations 
178 
8.1 
Introduction 
. . . . . . . . . . . . . . . . . . . 
178 
8.2 
A Positive Relativization of the P J PSPACE Problem. 
180 
8.3 
A Positive Relativization of the NP J PSPACE Problem 
184 
8.4 
A Positive Relativization of the P J NP Problem. 
186 
8.5 
A Relativizing Principle . 
192 
8.6 Exercises......... 
196 
8.7 
Bibliographical Remarks. 
197 

Contents 
IX 
9 The Low and the High Hierarchies 
199 
9.1 
Introduction . . . . . . . . . . . . . . . . . . . . 
199 
9.2 
Definitions and Characterizations . . . . . . . . . 
200 
9.3 
Relationship with the Polynomial Time Hierarchy 
202 
904 
Some Classes of Low Sets. . . . . . . . 
204 
9.5 
Oracle-Restricted Positive Relativizations 
209 
9.6 Lowness Outside NP. . . 
213 
9.7 Exercises......... 
215 
9.8 
Bibliographical Remarks. 
217 
10 Resource-Bounded Kolmogorov Complexity 
219 
10.1 Introduction ... . . . . . . . . . . . . 
219 
10.2 Unbounded Kolmogorov Complexity . . 
220 
10.3 Resource-Bounded Kolmogorov Complexity 
222 
lOA Tally Sets, Printability, and Ranking. . . . . 
225 
10.5 Kolmogorov Complexity of Characteristic Functions 
230 
10.6 Exercises. . . . . . . . . 
232 
10.7 Bibliographical Remarks. . . . . . . . . . . . . . . 
233 
11 Probability Classes and Proof-Systems 
235 
11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 
235 
11.2 Interactive Proof-Systems: Basic Definitions and Examples 
235 
11.3 Arthur Against Merlin Games . . . . . . . . . . . . 
240 
11.4 Probabilistic Complexity Classes and Proof-Systems 
241 
11.5 Equivalence of AM and IP . 
251 
11.6 Exercises. . . . . . . . . 
253 
11.7 Bibliographical Remarks. . 
254 
Appendix: Complementation via Inductive Counting 
257 
1 
Nondeterministic Space is Closed Under Complement 
257 
2 
Bibliographical Remarks. . . . . . . . . . . . . . . . 
262 
References 
263 
Author Index 
274 
Symbol Index 
277 
Subject Index 
280 

Introduction 
The notion of algorithm is very rich and can be studied under many different 
approaches. One of them is given by structural complexity. In the sixties the 
notion of feasible algorithm was developed and gradually was identified with P 
problems. In the early seventies the class NP was raised and the polynomial time 
reductions were defined. From this moment on these ideas were enlarged and 
studied more deeply. In structural complexity we study and classify the inherent 
properties of the problems by means of resource-bounded reducibilities; special 
attention is paid to complete problems. It was necessary to bring together some 
of the most important or fundamental material in a "uniform" way. 
We present the material of our choice in two volumes; the first one contains 
the basic topics, and the second one treats more advanced ones. The topics, 
coinciding with chapters, covered in the first volume are: 
1. 
Basic Notions About Models of Computation. We present some concepts 
of formal languages, set theoretic operations, boolean formulas and models 
of computations like finite automata and deterministic, nondeterministic 
and oracle Turing machines. 
2. 
Time and Space Bounded Computations. We present the basic theorems 
about complexity classes. These classes are defined by imposing bounds 
on the time and space used by the different models of Turing machines. 
3. 
Central Complexity Classes. We present the basic complexity classes and 
their known relationships. We define m-reducibility and related concepts 
like closure, completeness, and hardness. Some NP and PSPACE complete 
problems are presented. We finish with the technique of "padding" and 
the basic facts about logarithmic space nt-reducibility. 
4. 
Time Bounded Turing Reducibilities. We introduce T-reducibility and the 
relativized complexity classes. We give a glimpse of the "sparseness" 
question and its influence on the relationship between several complexity 
classes. Finally we consider a more general form of reducibility known 
as SN-reducibility. 
5. 
Nonuniform Complexity. We introduce the nonuniform approach, which 
is a tool for dealing with finite sets. Instead of measuring resources used 
by algorithms, we measure sizes of algorithms accepting finite sets. To 
establish a connection between the uniform and the nonuniform approach 
we introduce the "advice functions". 

2 
Introduction 
6. 
Probabilistic Algorithms. A probabilistic algorithm is a procedure that be-
haves in a deterministic way, except that it eventually takes decisions with 
a fixed probability distribution. The probabilistic Turing machine gives 
us a formal definition for probabilistic algorithms. We study complexity 
classes defined by imposing bounds on the time of computation. Also the 
concept of the probability of error is considered. 
7. 
Uniform Diagonalization. We present a technique for proving the ex-
istence of certain "diagonal" recursive sets. It allows one to prove the 
existence of non-complete problems in NP - P and the existence of infi-
nite hierarchies of incomparable sets in NP - P, provided that NP i P. 
8. 
The Polynomial Time Hierarchy. This hierarchy is a polynomial version 
of the Kleene hierarchy studied in Recursive Function Theory. The Poly-
nomial Time Hierarchy lies in between P and PSPACE. We consider also 
the relation between some probabilistic classes and this hierarchy. 
This second volume contains more advanced material and provides a more 
detailed picture of Structural Complexity. In this volume we consider the fol-
lowing major topics: parallel complexity classes, recursion-theoretic aspects of 
complexity, relativizations, Kolmogorov complexity, interactive proof-systems, 
and inductive counting. We give now a brief description of the contents. 
Chapters 1 and 2 give a detailed approach to elementary parallel models of 
computations. The first chapter deals with vector machines, parallel algorithms 
and their relation with Turing machines. The second deals with other parallel 
machines like array machines, SIMDAGS, and tree machines. All the models 
defiped verify the parallel computation thesis, which states that classes defined 
by space bounds on sequential models correspond to classes defined by time 
bounds on parallel models. 
Chapter 3 and 4 analyse more deeply parallel models of computation. Chap-
ter 3 combines the power of nondeterminism with parallelism to define the 
alternating models of computations and the associated classes. A particularly 
important result in this chapter, is the strict inclusion DUN C NUN. Chapter 4 
is devoted to the study of the parallelism with a feasible number of processors; 
special attention is given to the class NC. 
Chapters 5 and 6 deal with complexity-theoretic concepts arising from Re-
cursive Function Theory. Chapter 5 discusses the so called "isomorphism con-
jecture", which is an analogue to Myhill's theorem for the recursively enumer-
able sets. A related, very interesting result is also discussed: unless P = NP, 
no sparse NP-complete sets exist. Chapter 6 studies various notions related to 
bi-immune sets. Useful tools for studying reducibilities are obtained. 
Chapter 7 develops several results regarding the equalities or inequalities 
of complexity classes in the presence of oracle sets. It is shown that many 
unknown relationships can be solved for appropriately constructed oracles, but 
that frequently this can be done in contradictory ways for different oracles. 
Chapter 8 tries to give explanations of the phenomena studied in Chapter 7 

Introduction 
3 
by comparing the power of the different computational models with respect to 
oracle access. 
Chapter 9 discusses the "low" and the "high" sets in NP, by comparing the 
power of the sets taken as oracles for the polynomial time hierarchy. These 
concepts are inspired also in notions from Recursive Function Theory, although 
their properties are substantially different. Several properties that imply lowness 
or highness are identified. Any lowness property, such as a set being sparse, 
and any highness property, such as a set being T -complete, are seen to be 
incompatible provided that the polynomial time hierarchy is proper up to some 
level (most frequently up to the second or third level). 
Chapter 10 discusses Kolmogorov complexity of strings and their relationship 
to the complexity of the sets they belong to. Resource-bounded variants of 
Kolmogorov complexity are used to characterize the sets isomorphic to tally 
sets and certain nonuniform complexity classes. 
Chapter 11 presents a brief introduction to the probabilistic classes from the 
structural complexity point of view, placing emphasis on the Interactive Proof-
Systems and Arthur-Merlin games. These concepts provide complexity classes 
corresponding to alternations between probabilistic computation and nondeter-
ministic computation, and can be characterized in several ways. As a conse-
quence, the relationship with the polynomial time hierarchy as well as other 
properties are shown. 
The Appendix contains an important result: the closure under complemen-
tation of nondeterministic space-bounded complexity classes. It also serves as 
an introduction to the technique of inductive counting, which has applications in 
the study of other complexity classes. This technique should have been included 
in Volume I, but at the time Volume I was written, the technique had not yet 
been published. 
Let us give some pedagogical and practical hints. The chapters of this 
volume are less interrelated than in Volume 1. In fact the "inherent complexity" 
of the book is rather "constant". If you are interested in a particular topic try 
to study it directly. You have a good chance of understanding it. In fact we 
think this is almost a "RAM-book". The first two chapters are quite elementary 
and self-contained. Chapter 3 is independent from Chapters 1 and 2. Almost 
all sections are very readable. Section 3.4 contains a proof of the difficult 
result DLIN =I NLIN. This section is more technical and difficult to read. The 
material contained in Chapter 4 is easily readable if preceding chapters are well 
understood. The remaining chapters are quite independent of the first four ones. 
Chapters 5 and 6 can be read almost on their own; only the proof of Mahaney's 
theorem in Chapter 5 is somewhat involved. Chapters 7 to 9 are assumed to 
be read in sequence; some of the constructions are a bit complicated. Chapters 
10 and 11 are again nearly independent of the preceding ones. As a whole, 
this volume is "almost" self contained. This means that it can be read with an 
elementary but solid background on complexity, as provided by Volume 1. 

1 
Vector Machines 
1.1 Introduction 
In recent times, parallel algorithms have become increasingly employed to solve 
certain problems. A strategy for increasing the speed of computing is to perform 
in parallel as much as possible of the desired computation. Many models of 
parallel machines have been studied. The next four chapters are devoted to the 
study of some of them. 
In this chapter we present a first model of parallel machines called vector 
machines. In the next we consider other parallel models. All of them verify a 
property called the parallel computation thesis, which states that classes defined 
by space bounds on sequential models correspond to classes defined by time 
bounds on parallel models. Chapter 3 combines the power of nondeterrninism 
with parallelism through alternating Turing machines. Finally, Chapter 4 is 
devoted to the study of parallelism with feasible number of processors; special 
attention is given to class NC. 
Let us consider this chapter in more detail. The notion of sequential compu-
tation is clear and well covered by the model of the multi tape Turing machine. 
However the most powerful computers are not sequential but parallel. It is an 
important challenge to find a correct model of parallel computer. The vector 
machine is one such model. Vector machines have the ability to process in con-
stant time arbitrarily long bit-vectors. Taking the different components of these 
vectors as independent data we have a single instruction, multiple data, SIMD 
for short, machine. 
First we define vector machines and develop some fundamental techniques 
on vector programming. Then we use the preceding techniques to give efficient 
programs to deal with the most usual matrix algorithms. Finally we study the 
relation between vector machines and Turing machines: we shall show that 
vector machines have an astonishing power. The main result is that, up to a 
polynomial, time on vector machines is equivalent to space on Turing machines, 
and therefore vector machines fulfill the parallel computation thesis. 

Vector Machines: Definition and Basic Properties 
5 
1.2 Vector Machines: Definition and Basic Properties 
Vector machines have the ability to process bit vectors in unit time. Like numbers 
in the random access memory machines these vectors can be arbitrarily long. 
In these machines vectors are ultimately constant sequences of bits written from 
right to left and infinite to the left. 
Positive and negative numbers can be represented by sequences of bits. An 
ultimately zero sequence represents a positive number. An ultimately one se-
quence denotes a negative number. The number -x is obtained by complement-
ing the whole vector representing x. Thus··· 00010 represents number 2, and 
the number -2 is represented by ... 11101. 
The length of a vector 0', denoted lal, is the length of the non-constant part. 
Vector machines hold vectors as contents of vector variables, denoted V!.V2, ... 
There are seven types of vector instructions. 
Vi := x; 
Vi := Vi 1\ Vk ; 
Vi := Vi i Vk ; 
Vi := Vi 1 Vk ; 
go to 111 if Vi = 0; 
go to 111 if V; =I 0; 
Load the number x into register Vi. 
Bitwise parallel negation. The whole vector is 
complemented. If Vi is ultimately 0 then Vi is 
ultimately 1 and vice versa. 
Bitwise parallel "and". 
If Vk contains a positive number, shift Vi to the 
left the distance given by Vk• If Vk is negative, 
shift Vi right. If Vk = 0, Vi is copied into Vi. 
When shifting left the new positions are filled with 
O. 
When shifting right the bits shifted out are 
discarded. 
Like the preceding one but changing left and right 
Test instruction. The control jumps to the instruc-
tion labeled by 111 if Vi is O. 
Test instruction. The control jumps to the instruc-
tion labeled by 111 if Vi is different from zero. 
Vector machines are like the usual deterministic sequential machines, but 
using only vector instructions. Instructions can be labeled to indicate the targets 
of branching instructions. It is assumed that each vector instruction is performed 
in unit time. We present vector machines using the same style of pseudo-code 
as for Turing machines. For example, "do" loops or bitwise operations like V or 
EB will be used. We can easily translate such constructs into vector instructions 
at a cost of a constant factor of time. Let us give an elementary example. 

6 
Vector Machines 
Example 1.1 The following scheme is translated into the program given in 
Figure 1.1 
V := ... Down-I; 
m vector instructions; 
V:= V 11; 
go to m if V i 0; 
do n times (vector instructions ) 
Figure 1.1 The translation of a "do" scheme 
Using vector instructions, very large numbers can be constructed in a short time. 
For example consider the program: 
V := ···001; do n times V := V i V 
Let us denote by x the contents of V. As the left shift operation i V is equivalent 
to a multiplication by 2x , the program can be rewritten as: 
x := 1; do n times x := x * 2X 
The final value of x is a very large number. In order to prevent reaching these 
numbers we consider only vector machines satisfying some conditions. These 
programs have two types of variables: the index variables denoted by I, J, ... 
and vector variables denoted by V, A, B ,... Each boolean operation involves 
either only index variables or only vector variables. Index variables serve to 
control the shifts of vector variables. We assume the only possible shifts are: 
Vi := V2 i I, Vi:= V2 1 I, 1:= J i 1, 1:= J 11 
For language recognition, we require the input variable to be a vector variable. In 
the sequel we consider only vector machines satisfying these constraints. Under 
these restrictions, the value of the variables cannot grow too fast. By induction 
it is easy to prove the following lemma. 
Lemma 1.1 Given a vector machine, there exist two constants p and q de-
pending on the program and on the initial values of the variables such that: at 
time t, the length of the contents of any index variable is bounded by p + t, and 
the length of the contents of any vector variable is bounded by 2p+t + q. 
Using vector operations we can quickly replicate any given data. This is a 
fundamental fact of vector programming. We explain this fact in the following 
lemma, which illustrates a process known as recursive doubling. 

Vector Machines: Definition and Basic Properties 
7 
Lemma 1.2 Consider three vector variables V, S, L such that: 
1. 
The contents of the vector variable V is an infinite binary word (3 called 
the word to replicate. 
2. 
The index variable S contains a number s giving the shift. 
3. 
The contents of the index variable L is the number k = 2r (that is the binary 
word lOr) indicating the number of times the vector must be replicated. 
We can construct in vector V in time O(r) the following word: 
k-l V ((3 i i * s) 
i~O 
Proof. 
Consider the program Dup(V, S, L) defined as: 
while L > 1 do V := V V (V is); S := S T 1; L := L L 1 od 
Since L contains initially the number r the instruction L := L L 1 is executed 
r times. It is easy to check that at the end of iteration t the contents of vector 
V is: 
The statement follows for t = r. 
o 
Let us give some comments on the previous proof. The vectors Sand L can 
be seen as numbers. The algorithm is then rewritten as: 
do r times V := V V (V T S); S := 2 * S od 
An interesting case happens when s = 1(31: then the result is (3k. 
The 
algorithm proceeds by repeatedly doubling the result each time, a total of r 
times, giving: 
( ... ((f32i) .. l) = (3k 
, 
...,. 
, 
r times 
This idea (with some modifications) is widely employed to construct some 
vectors quickly. Let us give some examples. 
Example 1.2 Fix n = 2t. The word an consisting of the concatenation of all 
words on-i l<V , 0 ~ j ~ n in increasing order can be constructed in time OCt). 
When n = 4 the list is: 
a4 = ... 00011000101001001010001 
Bars are included to improve readability, but of course they are not present in 
a4. The vector a4 can be also factorized as: 
a4 = ... 000100001100001100001100001 = ... 000(041)4 

8 
Vector Machines 
In general we have: 
an = ... 000(0" 1)" 
The vector an can be easily constructed by recursive doubling in OCt) steps. 
Example 1.3 Let us prove that the elements of {O, l}s can be listed in increas-
ing order in time O(s). We denote this list by a •. When s = 3 the vector to be 
constructed is: 
0'3 = ... 011111110110111001011101010011000 
We give an iterative algorithm working in O(s) steps. Intuitively, at step t - 1 it 
constructs {O, I}t from {O, 1 r- 1• Figure 1.2 gives the algorithm to construct as. 
Z := ... 0001Os; 
V := ... 00010s; 
I:= s; 
do s - 1 times 
Z := (Z V (Z i I» i (I + 1); 
I:= 2 * I; 
V := V V (V i I) V Z 
Figure 1.2 The procedure to construct <x, 
In order to argue the correctness of this program, we introduce two lists. We 
denote by O's,to 0 :::; t :::; s, the list in increasing (from the right to the left) order 
of the words of the form O·-t{O, 1}t. Remark that a.,. = as. When s = 4 and 
t = 2 we have: 
0'4,2 = ···00010011100101000110000 
We denote by f3.,t the list of the "most significant bits" or the list of the "bits 
of order t" in O's,t. In the preceding example: 
f34,2 = ···00010010100101000010000 
Let us comment the role of Z and V at time t - 1. The vector Z is in charge 
of constructing the most significant bit of os-t{O, l}t from the most significant 
bit of os-t+l{O, 1}t-l. To do this Z constructs f3.,t from f3.,t-l by adequate 
shifts. Vector V duplicates O·-t+l{O, 1}t-l and appends the most significant bit 
ofos-t{O, 1}t, constructing O'.,t from O's,t-l and f3.,t. The register I is used to 
control the shifts. At the end of the (t - I)th iteration, the contents of the vectors 
are as follows: 
Z contains f3s to V contains as to I contains 2t- 1 * s 
, 
, 
Therefore, at the end of the (s - l)th iteration, V contains a.,. = 0' •• 

Vector Machines: Definition and Basic Properties 
9 
We develop next the method of recursive folding. This method is presented as a 
counterpart of recursive doubling. We will apply the ideas of recursive folding 
to solve linear recurrences. The method is presented in the following lemma. 
Lemma 1.3 Fix p = 2r. Assume in vector A the following word a where every 
ai belongs to {O, I} 
a = ... OOOapap_1 ... al 
Denote bi = V}.l aj . It is possible to construct in vector A in O(r) steps, the 
word 
Proof. 
A program constructing the word f3 is given in the following program: 
I := 1; M := ... 0001 P do r times A := A V «A i I) 1\ M); 1:= 2 * I od 
Example 1.4 Consider the recurrence 
Xi = ai V Xi-l 
, 
1 < i ~ P 
The solution of this recurrence is: 
i 
X· - Va· 
1-
) 
j=l 
o 
Considering Lemma 1.3 this recurrence can be easily solved by vector machines 
by recursive folding. 
To deal with linear recurrences consider the following lemma. 
Lemma 1.4 Fix P = 2r. Assume that vectors A and B contain the following 
words 
a = ... OOOapap_1 ... a2al 
f3 = ... OOObpbp_1 ••• b2bl 
then it is possible to construct in O(r) steps the word 
where: 
CI = bl 
c; = (ai 1\ Ci-l) V bi , 
1 < i ~ p 

10 
Vector Machines 
Proof. 
Remark that the value of al is redundant in this proof. Note that the 
computation of Ci can be rewritten in matrix form as follows, where the addition 
is identified with "or" and the product is identified with "and". 
Note that the process can be iterated 
Using the associativity of the matrix product we have: 
(~i) = (aiI\Oai-1 (ail\\_I)Vbi ) = (Cit) 
To solve this problem by a recursive doubling we need to calculate the expres-
sions: 
ai 1\ ai-l 
and 
(ai 1\ bi-l) V bi 
This is done in the program given in Figure 1.3. 
1:= 1; 
M := .. ·OOO}!'; 
do r times 
B := (A 1\ «B i I) 1\ M» V B; 
A := A 1\ «A i I) 1\ M); 
1:= 2 * I 
Figure 1.3 The algorithm of Lemma 1.4 
o 
Example 1.S 
Consider for 1 < i ~ 2r a more sophisticated recurrence than 
that in Example 1.4: 
Xl = bl 
Xi = (ai 1\ Xi-I) V bi 
, 
1 < i ~ 2r 
This recurrence can be rewritten in a matrix form. We denote: 
Using matrices this recurrence can be rewritten as: 

Elementary Matrix Algebra on Vector Machines 
11 
X;=3;*X;_1 
Since the matrix product is associative, we can use the technique of recursive 
folding to solve it. 
Example 1.6 Let us apply these techniques to sum efficiently two numbers. 
Let m and n be numbers of p = 2r bits. 
The carry in the sum operation is defined by: 
C; = (m; 1\ n;) V (m; 1\ Cj-d V (n; 1\ Cj-l) 
Defining 
aj = mj V 11j , 1 ~ i ~ 2r 
bj = mj 1\ nj , 1 ~ i ~ 2r 
the carry can be computed from the recurrence 
Cj=(ajl\cj_l)Vbj , 
1<i~2r 
By the previous lemma this recurrence can be solved in OCr) steps. Assuming 
m and n contained in vectors Af and N and that the carry can be computed in 
vector C, the addition of m and n is 
We have shown that in a vector machine two numbers of 2r bits can be added 
in O(r) steps. It is also possible to add 2r numbers of 2r bits in O(r) steps; see 
the exercises. 
1.3 Elementary Matrix Algebra on Vector Machines 
In this section we shall deal with representation of boolean matrices. Let A be 
a matrix with dimensions dn - 1 * dn- 2 * ... * d1 * do. We denote this matrix as: 
In order to process matrices on vector machines we need to transform these 
matrices into vectors. We adopt the usual criterion taken in compiling to store 

12 
Vector Machines 
matrices into central memory. For example in the two dimensional case the ele-
ments are stored by rows. In order to define formally the mapping we introduce 
for 0 ::;: i < n the numbers: 
i 
Di = II dk 
k-O 
To a matrix A = (ai"_li,,_l ... ilio) we associate the vector A = (ai). The vector 
element ai is the matrix element ai,,_lin_l ... ilio' where: 
i = i n-IDn -2 + i n-2Dn -3 + ... + ilDo + io 
The decomposition of i as a tuple in-I. in - 2, ••• ,i!, io is unique (see exercises). 
Let us give an example of this mapping. 
We consider the case where all dimensions are powers of 2. Suppose dj = 
2ki, 0 ::;: j < n. In this case there is an easy relation between the binary 
expansion of numbers in-I. i n - 2, ••• , il. io and the binary expansion of i. The 
expansion of ij has at most kj bits. We denote by [i j ] this expansion with 
exactly kj bits, if necessary filling with zeros from the left. We have then: 
[i] = [i n-d[in-2]· .. [iIl[io] 
In this case the calculation of numbers in-t. in-2, ... ,it. io consists in the iden-
tification of n consecutive fields (set of consecutive numbers) on the binary 
expansion of i. 
Example 1.7 Consider a three dimensional matrix A = (aili1io)' 0 ::; i2 < 2, 
o ::;: i l < 4, 0 ::;: io < 2. The relation between i and i2, i!, io is given by 
i = 8i2 + 2il + io. The vector associated with this matrix is: 
A = ... OOOal3l ... alliallOalOIalOOa031 ... aOllaOlOaOOlaOOO 
To identify io we need one bit, to identify i l we need two bits and for i2 we 
need one bit As example als = al,ll,l = am. 
A characteristic of parallel processes consists in the necessity to rearrange 
data in order to take advantage of the parallelism. Perhaps one of the most ele-
mentary rearranging processes is matrix transposition. This operation appears in 
the parallel matrix product and in the parallel transitive closure. We modify the 
notion of matrix transposition in order to deal only with transposition of binary 
coordinates. The classical notion of transposition appears then as a transposition 
of two blocks of binary coordinates. 
Definition 1.1 
Consider a matrix A = 
(ai), 0 ::;: i < 2j . To define trans-
position of two binary coordinates, express i in binary as a binary word of j 
bits 
[i] = Uj_I··· up··· uq··· Uo = o:up{3uq'Y 
The transpose of A about binary coordinates p and q is denoted by: 

Elementary Mattix Algebra on Vector Machines 
13 
Example 1.8 Let us consider some examples of binary transpositions on a 4*4 
matrix A = (ailio)' We represent this matrix thus: 
~ : ~) 
j 
k pi 
n 
0 
= 
(
aoo,oo 
aOO,Ol 
a01,00 
a01,01 
a 10,00 
a 10,01 
all,OO 
all,Ol 
aOO,10 
aOO'll) 
a01,10 
a01,ll 
alO,10 
alO,ll 
all,10 all,ll 
The relation between indexes and bits is [iJ = [id[ioJ = U3U2U1UO. The vector 
associated with this matrix is: 
A = (ai) = . .. 0 0 0 p 0 n m I k j i h 9 fed c b a 
Let us give some examples of transpositions: 
Tt,o(A) 
The corresponding vectors are: 
1t,o(A) = ... 0 0 0 p nom I j k i h f 9 e d b c a 
Tj,2(A) = .. ·0 0 0 p 0 n m h 9 f elk j ide b a 
Ti,1 (A) = ... 0 0 0 pol k n m j i h 9 d c feb a 
Tt,o(A) = .. ·0 0 0 pin j 0 k m i h d f b gee a 
Remark. Formally the transposition of two bits up and u q does not depend on 
value j. We could note Tl,q(A) only as Tp,q(A). We will see later that some 
programs performing the transposition depend heavily on j. For this reason we 
have preferred to indicate explicitly th"e value of j. Let us study the transposition 
of two bits p and q. 
Theorem 1.1 Let A = (ai) be a 2j dimensional vector. The transposition about 
binary coordinates p and q, denoted as Tl,q(A), can be constructed by a vector 
machine" in time O(j). 
Proof In order to obtain T1.q(A) from A we need to move the elements ai in 
three different ways. To distinguish the elements that move in a given way, we 
will use vector masks. We call a vector mask any vector used to mask some 

14 
Vector Machines 
components of a given vector. We will construct three vector masks denoted as 
Rj(p, q), V(p, q) and sj(p, q). 
Case 1: up = 1 and uq = O. Then aUp{3uq"'{ > auq{3up"'{. More precisely: 
As a consequence, every ai such that bits Up and u q in the binary expansion of 
i are 1 and 0 respectively must to be shifted 21' - 2q positions to the right in 
order to occupy the correct position in the matrix Tt,q(A). In order to shift all 
these elements at once we introduce the vector Rj (p, q) = (rf (p, q» defined by: 
r~(p ) = { 1 if 0 ~ i < 2j , up = 1 , U q = 0 
• , q 
0 otherwise 
With the vector Rj(p, q) we compute: 
Case 2: up = 0 and u q = 1. Then we have: 
In this case we need to shift 21' - 2q positions to the left. As in the preceding 
case we define the corresponding vector mask Lj(p, q). To shift all elements at 
once we realize the instruction: 
Case 3: up = u q• Then: 
O:Up{3u q"'{ = auq{3up"'{ 
In this case we do not need to move the components. We denote by Sj(p, q) 
the corresponding vector mask. To isolate the corresponding components we 
realize: 
St,q(A) := (A 1\ sj(p, q» 
The whole matrix Tt,q(A) can be obtained as: 
Tt,q(A) = R~,q(A) V L~,q(A) V St,q(A) 
In order to obtain an efficient program we must find a way of constructing 
quickly the vector masks. To do this, we construct the mask Mi(p) = (mf(p». 
Denoting as up the bit number p in the binary expansion of i, the vector mf(p) 
is defined by: 
j (P) _ { 1 if 0 ~ i < 2j , Up = 1 
m· 
-
• 
0 otherwise 

Elementary Matrix Algebra on Vector Machines 
15 
When p = j - 1 we have Mi(j - 1) = ... 0012i-I()2H. This vector can be 
constructed by the following program: 
M := 10; I:= 1; do j - 1 times M:= (M V (M i I» i I;I:= 2 * I od 
The masks Mi(p) can be constructed easily by the recurrence 
Mi(p - 1) := Mi(p) ED (Mi(p) 1 2P- I ) 
The construction of the masks [} (p, q), Ri (p, q), Si (p, q) is straightforward once 
Mi is constructed: 
Ri(p, q) := (Mi(p) /\ -,Mi(q» 
Li(p, q) := (-,Mi(p) /\ Mi(q» 
Si(p,q) := (Mi(p) == Mi(q» 
The construction of all masks Mi can be done in O(j) steps, because Mi(j -1) 
can be constructed in time O(j) and every Mj(p - 1) can be constructed from 
Mj(p) in constant time. Assuming all Ali constructed, the masks Li(p, q), 
Ri(p, q), Si(p, q) are constructed in constant time. From these masks the vectors 
R~.q(A), L~.q(A), st./A) and the whole transposed vector can also be constructed 
in constant time. 
0 
Example 1.9 Let A be a 2 * 2 * 2 matrix defined by: 
A = .. ·1 a1ll I al10 I aIDI laIDO laOll I aOID I aOOI I aooo I 
Recall that bars are added to improve readability. The transposition of B = 
Ti,o(A) is schematized by: 
B = ... I a1ll I aOI1 I alOl I aODI I al10 I aOlO I alOO I aooo I 
The masks Mi(p) are given by: 
M 3(2) =···1 1 
1 
1 
1 
0 
0 
0 
0 
M 3(1) =···1 1 I 1 
0 
0 
1 
1 
0 
0 
M\O) = ···1 1 I 0 
1 
0 
1 
0 
1 
0 
The vectors R, L, S are defined by: 
R3(2,0) =···1 0 
1 
0 
1 
0 
0 
0 
0 
L\2,0) =···1 0 
0 
0 
0 
1 
0 
1 
0 
S3(2,0) =···1 1 
0 
1 
0 
0 
1 
0 
1 
And finally we have: 
Rto(A) =···1 0 I 0 I 0 I 0 I al10l 0 I alool 0 
L~o(A) = · .. 1 0 I aOlll 0 I aooll 0 
0 I 0 I 0 
. 
S?o(A) = .. ·1 a1ll1 0 I alOti 0 I 0 laOlOl 0 I aoool 

16 
Vector Machines 
Definition 1.2 Consider a matrix A = (a ... _I ••• i,. ... i, ... io) of dn - l * dn -2 * ... * do = 
2i dimensions. The transposition of two coordinates if" iq is defined by: 
Consider the case where all dimensions are the same, then we have: 
Theorem 1.2 The transposition of two indices of the same dimension in a 
matrix of 2i elements can be done in O(j) steps. 
Proof 
Consider the case when dn - l = dn - 2 = ... = do = 2k. In this case we 
have: 
[i) = [in-d· .. [i p ] ••• [i q] ••• [io] = 
Ui-l··· (Upk-l··· U(P-l)k)··· (Uqk-l··· U(q-l)k)··· UO 
" 
" 
, 
J 
Y 
v 
[il'l 
[i,1 
where we have indicated with parentheses the bits corresponding to the coordi-
nates ip and i q• In this case the transposition can be computed as a composition 
of k binary transpositions. Schematically: 
T~,i,(A) = T:k-1,qk-l(··· (T~-I)k+l,(q-l)k+l(T~-I)k,(q-l)k(A))) ..• ) 
This method can be easily implemented in vector machines as shown in Fig-
ure 1.4. 
The construction of the masks ."A.1i (pk - 1) and Ali(qk -
1) can be done 
in O(j) steps. Every iteration step is done in constant time. As the number of 
iterations is bounded by j we have obtained the result. 
0 
When the indexes have different dimensions we need to modify the algorithm 
(see Exercise 8). 
Our next subject is the study of the product of two numbers. To obtain an 
efficient program we will employ all the techniques developed here on vector 
programming. Recall that these techniques are: recursive doubling, recursive 
folding, masking and matrix transposition. 
Example 1.10 Let us prove that the product of two numbers of p 
can be calculated in 0(r2) steps. Consider the numbers: 
m = mp· ··ml 
The program consists of a vectorization of the usual multiplication algorithm. 
In order to give an efficient vector machine we construct by recursive doubling 
the words a = «()l'm)P and!3 = n 2p = (np •·• nl)2P• Consider the word !3 as 
p * 2p matrix !3i,i and define the word: 

Elementary Matrix Algebra on Vector Machines 
17 
comment: construction of pointers to the bits U1'k-l and Uqk-l 
P:=pk-l; 
Q:=qk-l; 
T(A):= A; 
comment: construction of initial masks 
Ml := Mj(P); M2 := Mj(Q); 
do k times 
comment: construction of masks R, L, S 
R := Ml /I. ,M2; L := ,Ml /I. M2; S := Ml == M2; 
comment: construction of R(A), L(A), S(A) 
R(A) := «T(A) /I. R) ! P) i Q; 
L(A) := «T(A) /I. L) i P) ! Q; 
S(A) := (T(A) /I. S); 
comment: construction of one bit transposition matrix 
T(A) := R(A) V L(A) V S(A); 
comment: move the pointers to the next bit 
P := P - 1; Q := Q - 1; 
comment: construction of the new masks 
Ml := Ml EB (Ml ! P); M2 := M2 EB (M2 ! Q) 
Figure 1.4 A vector machine for matrix transposition 
Remember that transposition can be done in O(r) steps. Figure 1.5 gives us an 
algorithm for the product. 
In order to obtain the product we need to compute all the rows appearing in 
the usual product algorithm. This is done in vector U by the word {) defined as: 
U := a: /I.,; 
111 := 11'2021'2; 
for i = 1 to r do 
M := M ! (P2/2i-1); 
V := (U /I. M) ! (1/2i - 1) * (p2 - I); 
U := (U + V) /I. ,111 
Figure 1.5 A product of two numbers of p bits 

18 
Vector Machines 
where Ui is a word of length p defined by: 
To add these rows we need prior to the addition to shift them in an appropriate 
way. To do this in parallel we need to superimpose the "first part" of 6 onto the 
"second part". We denote the "shifted" part of 0 as a vector J.t = (0 1\ 1~()pl) ! 
(p2 _ ~). 
In the second step we need to shift the second part of the content of vector V 
4*(p2_~) positions to the right. At step i the shift to be done is (l/2i-l)*(p2_~). 
In order to obtain the correct sums we need to mask the unused parts of 
vectors. The initial vector mask is Al0 = l',l02pl. To isolate the second part of 
vector 0 we need a mask All = Mo ! p2. Denoting by Mi- 1 the vector mask 
employed at iteration i-I, we denote as AIi the mask needed in step i. The 
relation between these masks is: Mi = Ali-I! (p2/2i- 1). 
The vectors a and f3 can be constructed in O(r) steps by recursive doubling. 
In every iteration we realize a sum of p bits. This sum is computed in O(r) 
steps. As the iterative algorithm has r steps we come to the result. 
Employing the techniques of "carry save adder" we can obtain an algorithm 
in O(r) steps (see Exercise 13). 
We deal now with matrix product. In order to give a fast parallel algorithm 
we need to introduce redundancy in data representation. In this sense we define 
the operations expand and reduce applied to square matrices. 
Definition 1.3 Given a matrix A = (aij), 0:::; i,j < 2r we define the operation 
expand about the coordinate number two (recall that coordinates are labeled 
···2,1,0) as: 
An operation in some sense complementary to expansion is the reduction about 
coordinate number two. Given a matrix B = (bjjk), 0 :::; i, j, k < 2r the reduction 
is defined as: 
2·-1 
R 2(B) = (r2(B)ij) , 0:::; i,j < 2r where r2(B)ij = V ak,i,j 
k-O 
Example 1.11 Intuitively the matrix E2(A) represents the 2r unfold of matrix 
A. As example consider a 4 * 4 matrix A, We have: 
A =···000 po n m 1 k j i h 9 fed c b a 
E2(A) = ···000 1 p ... c b a 1···1 p ... c b a 1 
... 
¥ 
' 
four times 

Elementary Matrix Algebra on Vector Machines 
19 
Every three dimensional matrix B can be seen as a set of 2r two dimensional 
matrices Bo, B),"', B2,_). Consider for example the 4 * 4 * 4 matrix B 
The matrix R 2(B) is a 4 * 4 matrix defined as: 
Expansions about other coordinates can be defined (see Exercise 14). The 
operations expand and reduce can be computed very fast by vector machines. 
Lemma 1.5 Let A be a 2r * 2r matrix. Let B be a 2r * 2r * 2r matrix. The 
matrices E2(A) and R2(B) can be computed in OCr) steps. 
Proof 
The expand operation can be calculated by recursive doubling as: 
where the procedure Dup was defined in the preceding section. To compute the 
reduce operation, consider a vector machine operating on three vectors S, B and 
R. The vector S initially contains· . ·00103r. This vector serves to count the 
shifts. Vector B is used for intermediate results. Formally the program working 
in OCr) is: 
do r times S := ! * S; B := B V (B 1 S) od; B := B 1\ ... 00012' 
o 
Let us see how to quickly compute the matrix product. Let A' = (a;j), 
B = (bij ) be two dimensional matrices, 0 ::; i, j < 2r. Recall that matrix product 
is defined by: 
Theorem 1.3 Let A and B be two matrices of dimensions 2r * 2r. The product 
of these matrices can be calculated as: 
A * B = R2(TJ'2(E2(A» 1\ T?'2(E2(B») 
, 
, 
This operation can be done in OCr) steps. 
Proof Let A = (aik)' The matrix E2(A) = (e2(A)jik) satisfies e2(A)jik = aik> 
and the matrix A' = T6,2(E2(A» = (akij) satisfies: 

20 
Vector Machines 
Consider B = (bkj ), the matrix E2(B) = (e2(B)ikj) satisfies e2(Bhj = bkj , the 
matrix B' = Tl~2(E2(B» = (bki) satisfies: 
b~ij = e2(B)ikj = bkj 
Consider C' = A' 1\ B' = (cki) where Ckij = akij 1\ bkij = aik 1\ bkj then: 
2?-1 
2?-1 
R2(C') = (V C~ij) = ( V (aik 1\ bkj ) = A * B 
k-O 
k-O 
As expansions, reductions and transpositions can be done in O(r) steps we obtain 
~~~ 
0 
Example 1.12 Let us give an example of this matrix product Let A and B be 
2 * 2 matrices: 
A = ... 0 0 0 d C b a 
and B = ... 0 0 0 h g f e 
The matrices given in the algorithm are: 
···OOOdcbadcba 
···OOOhgfehgfe 
Ti,o(~(A» = ... 0 0 0 d d b b c C a a 
Ti,l(~(B» = ···000 h g h g f e f e 
Adding bars to improve readability, the matrix C' is 
C' = ···000 Id 1\ hid 1\ glb 1\ hlb 1\ gl·· ·Ia 1\ fla 1\ el 
finally the matrix product Cis: 
C = ···000 I(d 1\ h) V (c 1\ f)1·· ·I(a 1\ e) V (b 1\ g)1 
Finally we deal with transitive closure. Recall that a transitive closure of a 
matrix A = (aij), 0 ~ i,j < r is defined as 
A * = 1 V A V A 2 ... = (A V 1)2? 
Theorem 1.4 Given a matrix A = (aij), 0 ~ i,j < 2r the matrix A* can be 
computed in O(r2). 
Proof The identity matrix is the vector 1 = ... OO(02? 1)2? 
This matrix can 
be calculated by recursive doubling in O(r) steps. The vector (A V 1)2? can be 
calculated as: 
s ... «A V 1)2)2 ... )2, 
v 
r times 
Every matrix product needs OCr) steps. There are OCr) matrix products. This 
gives a bound OCr2). 
0 

Relation Between Vector Machines and Turing Machines 
21 
1.4 Relation Between Vector Machines and Turing Ma€hines 
In this section we will study the relation between space bounds on Turing ma-
chines and time bounds on vector machines. First we prove that vector machines 
can efficiently simulate nondeterministic Turing machines. Let us consider the 
input conventions. Given a word w E {O, 1}*, we say that a vector variable 
contains w if this variable contains the bit vector ... OOOw. In the sequel we 
consider only languages LeI {O, I} * because the machine cannot distinguish 
among inputs w, Ow, OOw, etc. 
Let us define the time bounds on vector machines. 
Definition 1.4 Let LeI {O, I} *. A vector machine V accepts L in time T(n) 
iff V has one input register, and for all wEI {O, I}· we have: 
1. 
w E L iff there is an accepting computation C wlwse initial configuration 
has the bit vector· .. OOOw in the input register. 
2. 
The computation C does not exceed in time T(lwi). 
VECTOR-TIME(T(n» is the class of sets accepted by vector machines whose 
time is bounded above by T(n). 
In order to obtain efficient vector machines simulating computations on 
Turing Machines, let us explain the conventions adopted to describe the config-
urations of a Turing machine. 
Let L E NSPACE(S(n» and let M = (Q, E, 8, qo ,qa) be a nondetermin-
istic Turing machine accepting L in space Sen). Assume M has only one work 
tape infinite to the left. This assumption is consistent with our assumption about 
vectors. Recall that vectors are strings of bits infinite to the left. Assume the 
inputs are delimited with endmarkers $. If Iwl = n we note $w$ = W;l+l ••• woo 
For fixed w we denote a configuration M on input w (see Definition 1.32 of 
Volume I) as a pair: 
(0· . ·010· . ·0, aq(3) 
The first part has length n+2, recalll$w$1 = n+2, and indicates the head position 
on the input tape. The second part has length Sen) + 1 and encodes the state 
and the content of the work tape. When the machine is in this configuration, 
the work tape head is scanning the rightmost symbol of a. We assume the 
machine accepts w only by entering a unique designated accepting state qa with 
the work tape entirely blank and both heads scanning the rightmost squares of 
their respective tapes. In order to apply the techniques of recursive doubling 
and folding, we approximate Sen) by a power of two. Define: 8' = 2 Pog(S(n)+l)l 
With these considerations we denote a configuration of AI on w as a word: 

22 
Vector Machines 
where the input head scans the letter Wj of input tape and \rtqT21 = s'. Denoting 
by # the blank symbol, the initial configuration and accepting configuration are: 
For fixed W we denote by Step the one step transition matrix. Steps,s' = 1 if 
and only if 8 can reach 8' in one step of computation of M on input w. The 
acceptance problem can be reformulated as: 
W E L 
iff 
Stepinit,Acc = 1 
In order to construct the vectors required for computing the transitive closure, we 
need to know the number of possible configurations. To codify a tape symbol 
or a state in binary we need b = r1og(card{ 17 U Q})l bits. The number s = b * s' 
gives a bound to the number of necessary bits to code the work tape and the work 
head. The number of work tape configurations is bounded by m' = 2s• We also 
approximate the length of the input by a power of two defining 11,' = 2Pog(n+2l1. 
The input part has at most 11,' possibilities. The total number of configurations 
is bounded by m = 11,' * m'. 
Lemma 1.6 Assume that input word wand number s' are contained in the 
variables TV and S' respectively. The integers 11,', s, m', and m can be computed 
by a vector machine in time O(S(n) + log 11,). 
Proof. 
The integer 11,' can be calculated in register N' in time O(1og 11,) as: 
N' := 2; while (W i 2) .L N' i 0 do N' := 2 * N' 
As b is a constant independent of Iw I, the number s = b * s' can be calculated in 
O(S(n» steps in a variable S. The number m' can be calculated in a variable 
M' as M' := 1 i S. Finally m is calculated in M as M := N' i S. 
0 
In the sequellet us define binary words /'k. with h'kl = m, some of which 
encode instantaneous descriptions, as: 
/'k = om-n'-s < j > [p] 
where [p] E {O, l}s, 0 ~ p < m'. In some cases [p] codifies a description of 
work tape plus state. The word < j >= on'-j-1lOi with 0 ~ j < 11,', memorizes 
the head position on input tape. The padding om-n'-s will be useful in vector 
calculations. The relation between k, j, p is given by k = n'p + j. Let us give 
an example. 
Example 1.13 Consider the case 11,' = 4, m' = 23, m = m' * 11,' = 32. Then 
/'0 = ... 1 025 
1 0 0 0 1 1 0 0 0 1 = ... 025 < 0 > [0] 

Relation Between Vector Machines and Turing Machines 
23 
'Yl = .. ·1 
025 1 0 0 1 0 1 0 0 0 1 = ... 025 < 1 > [0] 
'Y2 = ···1 025 
1 0 1 0 0 1 0 0 0 1 = ... 025 < 2 > [0] 
'Y3 = ···1 
025 I 1 0 0 0 I 0 0 0 I = ... 025 < 3 > [0] 
'Y4 = ···1 025 I 000 1 I 00 1 1= ... 025 < 0 > [1] 
'Y5 = ···1 025 
1 00 1 0 I 00 1 I = ... 025 < 1 > [1] 
'Y6 = ···1 025 
1 0 1 0 0 1 0 0 1 1 = ... 025 < 2 > [1] 
'Y7 = ···1 025 
1 1 0 0 0 I 0 0 1 I = ... 025 < 3 > [1] 
The last configuration is: 
'Y31 = ... I 025 I 1 0 0 0 1 1 1 1 I = ... 025 < 3 > [7] 
We wish to construct the m * m matrix Step = (Step,,!;,,!;), 0 :::; 'Y;, 'Yj < m. If 
'Y; codes an instantaneous description 8, 'Yj codes a description 8' and (/ follows 
in one step from 8, then Step"!;"!; is 1, otherwise is O. Before this, it is easier to 
construct the m * m * m matrix Move = (!l1oVeijk), 0 :::; i, j, k < m defined as: 
111 
{ StePT"!" 
if k = 0 
ove""k -
• J 
IJ 
-
0 
otherwise 
Like a vector the matrix Move can be written as: 
Move = ···0001 om-l Step,,! ... ,,! ... 1···1 om-l Step"IO"!1 I om-l Step"IO"!o I 
To do this construction we define in the following three vectors 'Y, 'Yrt 'Yc dealing 
with the list of all configuations and two vectors inputw, lengthw dealing with 
the input word. 
We begin by constructing a vector 'Y giving the ordered list of all configu-
rations. Using this vector we construct other two vectors 'Yr, 'Yc. These vectors 
will be needed to match all possible pairs of configurations. Formally we have: 
'Y = .. ·000 'Ym'Ym-l ... 'Yno 
'Yr = .. ·0 0 0 ('Ym)m('Ym_l)m ... ('Yl)mho)m 
'Yc = ···000 hm'Ym-l··· 'Yno)m 
Notice that as every 'Y; needs m bits to be described and there is a total of m 
words 'Y;, the vector 'Y can be seen as an m * m dimensional matrix. For the 
same reason the words 'Yr and 'Y c can be seen as m * m * m dimensional matrices. 
Lemma 1.7 Assuming that the integers m, sand n' are known, the vectors 'Y, 
'Yr, 'Yc can be constructed in O(S(n) + log n) steps. 

24 
Vector Machines 
Proof Let us begin by constructing the vector /. We do this construction 
in several steps. Employing the ideas developed in Example 1.3, by recursive 
doubling we can construct: 
a = ... ° ° ° 
om-S[m - 1] .. ·om-. [1] om-s[o] 
Taking the vector a as a matrix and applying operations of replication and 
transposition we construct the following vector: 
Employing the ideas of Example 1.2 and using replication and transposition of 
matrices we can construct the following vector: 
J-l = '" (iom-n'-' < n'-l > OSI·· ·Iom-n'-' < 1 > OSlom-n'-' < ° > OSJ)m' 
The vector / can be constructed as / = j3 V J-l. Assuming / constructed by 
replication we can obtain /e and by transposition we can obtain /T from /e' The 
dimension of the matrices is o (2(S(n)+log n». As recursive doubling, transposition 
and replication can be done in O(S(n) + log n) steps, we obtain the result 
0 
To construct the matrix Move we need to "present" at every pair of possi-
ble configurations a "copy" of the input word in the part of the configurations 
corresponding to the input head. To do this we define the vector inputw as: 
inputw = ... ° ° ° 
(I om-n-2-s 10Iwn'" wIIOI 0· DmZ 
Recall that in order to construct the vectors /T and /e efficiently we have ap-
proximated the length of w by n'. To calculate the matrix Move we need to 
know the exact value n. For this reason we define the vector lengthw as: 
lengthw = ... ° ° ° 
(I om-n-2-s Ilion 111 Os DmZ 
Lemma 1.8 Assuming that the integers m and 8 are known, it is possible to 
construct the vectors inputw and lengthw in O(S(n) + log n) steps. 
Proof Recall that the input word is contained in the variable W. The vector 
inputw can be easily constructed on a register Vo by recursive doubling as: 
Vo := Wi (8 + 1); Du.p(Vo, m, m 2) 
Recall that we have assumed that the leftmost bit of w is 1. We construct in 
vector 113 the word lengthw as: 
113 := W i 1; Dup(V3, -1, n'); 113 := 113 EB (V3 i 1); Dup(V3 i s, m, m 2) 
where the -1 in the procedure Dup means a right shift. 
o 

Relation Between Vector Machines and Thring Machines 
2S 
Vo= ... 1 
om-n-a-2 
1 0 1 wn ' "Wk" 'WI 1 0 
0 .. ·000 1 ... 
Vi = ... 1 om-n'-a 
0 ........ · 001··· ...... 0 
Ca··· CJC2CI 1 ... 
V2= ... 1 om-n'-s 
1 0 ........ · 010 ...... · .. 0 
C~'" c~c~ci 1 . .. 
V3 = ... 1 
om-n-a-2 
1 1 100 ......... 00 1 1 
0 .. ·000 1 . .. 
Figure 1.6 The content of variables Yo. Vi. Vi. 1'3 
Assuming Vi contains Ir and V2 contains IC' the contents of variables Va, 
Vi, V2, V3 is given in Figure 1.6. 
Note that variables Va, Vi, V2 and V3 give us information about: the input 
symbol scanned by the head, the move of the head, and the contents of work 
tapes on configurations Ii and Ij. The following theorem explains how this 
information can be extracted and processed. 
Theorem 1.5 NSPACE(S(n» ~ VECTOR-T1ME(O(S(n) + log n)2) 
Proof Let us assume the number s' is known. By the preceding lemmas the 
vectors In IC' inputw and lengthw can be computed in time O(S(n) + log n) 
steps. In order to construct the matrix Move from the previous vectors we need 
to obtain more information about moves. 
Collecting information about one move. We will describe only what happens 
in a block of m bits. The program runs in parallel on all m2 blocks of m bits. 
The information is collected in two steps. In the first we obtain the symbol 
scanned by the input head. In the second we collect information about the head 
move. 
First step. There are two cases. The first case is when the head is scanning 
at some input symbol Wk different from $. Then, this symbol is obtained and 
spread to the part of of Vo corresponding to the work tape configuration of the 
vector Vi. Denoting d = 2f/og(s+n'>1, this is done by: 
Va := Vo /\ Vi; Dup(Vo, -1, d) 
A second case is when the head is scanning at the symbol # in #w#. We spread 
this information by: 
V3 := V3/\ Vi; DUp(V3, -1, d) 
Second step. We obtain information about the head move. We begin to construct 
in a register Z a vector mask· . ·000(1 m-sos)m2 isolating the part corresponding 
to the input tape. We indicate a move to the right in a vector 114 spreading 1 to 
the right as: 
114 := «VI! 1) /\ V2) /\ Z; DUp(V4, -1, d) 

26 
Vector Machines 
When the head stays in place, we obtain this infonnation in a vector Vs as: 
Vs := (Vi A V2) A Z; Dup(Vs, -1, d) 
We obtain infonnation about a left move in V6 as: 
Vt; := (Vi A (Vi L 1» A Z; DUp(V6, -1, d) 
At the end of this part all the infonnation has been spread to the positions looking 
at work tape contents. Infonnally this situation is explained in Figure 1.7. 
Va = .. ·1··· m - s bits .. ·1 Wk 
VI = .. ·1 ... m - s bits . ··1 CS 
Wk Wk Wk I··· 
C3 
C2 
CI I··· 
V2 = .. ·1 ... m - s bits .. ·1 
c~ 
C3 
c~ c~ 
V3 = .. ·1··· m - s bits···1 
end marker 
V4 = .. ·1··· m - s bits···1 
right move 
Vs = .. ·1 .. · m - s bits .. ·1 
don't move 
V6 = .. ·1··· m - s bits···1 
left move 
I .. · 
I .. · 
I .. · 
I .. · 
I· .. 
Figure 1.7 The information has been spread to the positions looking at work tape contents 
Analyzing a possible move. Assume that the corresponding block of m 
bits of VI are /i = om-s < i l > [i2] and the corresponding m bits of Vi are 
Ii = om-s < jl > [h] Given the input symbol and head shift infonnation, every 
neighborhood of three consecutive symbols 0"10"20"3 E {17 U Q P in an instanta-
neous description [i2] determines a set N(0"10"20"3) such that O"~ 0"~0"3 E N (0"1 0"20"3) 
must occupy the same neighborhood in any [h] which follows in one step from 
[i2]' It is easy to design a vector machine examining each neighborhood in 
Va ... Vt; and detennining if this neigborhood is consistent with a move. This 
infonnation is collected to the rightmost bit of the block of corresponding s bits 
and constitutes the bit SteP"Y,"Yj of matrix Move. This process can be done in 
O{s) steps, that is O(S(n» steps. The whole construction of matrix Move can 
be done in O(S(n) + log n) steps. 
Application of the transitive closure. Let us transfonn the matrix Move = 
(Moveiid to obtain the matrix Step = (Stepij. We have the equality MOVeijO = 
Stepij' Denoting 1= 3 log m we have: 
Step = .. ·0001 m 2 A T! .(T!k(Move» 
ItJ 
It 
When we have obtained the matrix Step applying Theorem 1.4 we obtain Step· 
in O«S(n) + log n)2) steps and 
W E L 
iff 
1 = StePinit,Acc 
The number s' is not known. If s' is not available the whole procedure runs for 
all successive powers of two, until one is found that accepts the input. 
0 

Relation Between Vector Machines and Turing Machines 
27 
Theorem 1.6 VECTOR-T1ME(T(n» ~ SPACE(O(T(n)(T(n) + log n») 
Proof Let P be a vector machine accepting L in time T(n). Let Vo, ... , Vm 
be the registers appearing in P. Assume that the input of length n is contained 
in the variable 1'0. By Lemma 1.1 there exists a constant c such that the length 
of any vector variable at time t is bounded by ct + n. There also exists a constant 
q such that the length of the contents of any scalar variable is bounded by q + t. 
Let us describe a program R accepting L in space O(T(n) * (T(n) + log n». 
This program list the successive instructions appearing in program P. This list 
has the form: 
(it, 1)(i2, 2)(i3, 3)··· (it, t)··· 
The program R accepts when the vector machine does. If at time t a test 
instruction is found then this instruction needs to be evaluated to determine the 
following instruction to be listed. Recall that test instructions have the form "go 
m if Vi 
= 0" or "go m if Vi 
-=I 0". To calculate the value of vector Vi the 
program calls the recursive procedure find(b, i, t). such that: 
find(b, i, t) = returns the bit number b of the contents of Vi at step t 
The test instruction "go m if Vi = 0" is evaluated as: 
ct+n 
V find(b, i, t) 
b-O 
The procedure find(b, i, t) is defined as: 
1. 
When t := 0 the procedure scans the number of the variable. When i = 0, 
we have the input variable, and the procedure returns the both bit of the 
input. Otherwise it returns O. 
2. 
If the instruction recorded at step t does not change the contents of Vi 
then the procedure returns find(b, i, t -
1). 
3. 
If the instruction recorded at time t is Vi := x then return the both bit of 
x. 
4. 
If the instruction recorded at time t is Vi := Vi 1\ Vk then return: 
find(b, j, t - 1) 1\ f ind(b, k, t - 1) 
5. 
Consider the case when at time t the recorded instruction is Vi := Vi i Vk • 
Recall that in this case Vk is an index variable, and the length of the 
contents of Vk is bounded by p + t. In the first step, procedure find 
computes the exact value of the length of Vk in a variable 1. To do this 
execute the procedure given in Figure 1.8. 
The integer 1 marks the beginning of the stationary part of Vk. That means: 

28 
Vector Machines 
1 := 0; l' := 0; 
while l' < q + t do 
if find(I', k, t - 1) =I find{l' + 1, k, t - 1) then 1 := l' + 1; 
l' := l' + 1 
end while; 
Figure 1.8 A procedure giving the exact value of V. 
As a consequence s = find(l, k, t -
1) gives the sign of the number z 
contained in Vk • The contents of Vk is: 
Vk = ... s s s find(l- 1, k, t - 1) find(l- 2, k, t - 1) .. · find(O, k, t - 1) 
When z is positive its value is computed as: 
Izl = E~:Jfind(b,k,t _1)2b 
When z is negative we complement the values found by find. Recall that 
the length of the content of V; is bounded by ct + q. When z is known 
and positive the bit Vi(b) can be found thus: 
(a) 
When 0 ~ b - z ~ ct + n the value b - z gives a coordinate "inside" 
the vector Vj. Then find returns find(b - z,j, t - 1) 
(b) 
When b -
z > ct + q the shift "encounters" the sign of Vj. The 
procedure returns find(d + q + l,j, t - 1). 
(c) When b - z < 0 the bit number b of Vi has been filled with a zero. 
The procedure returns O. 
It is not difficult to develop similar arguments when z is negative, or to 
find procedures to deal with right shift or shift between scalar vectors. 
Let us explain the space constraints of the program R. A maximum of 
O(T(n) * logT(n» bits are required to record the list 
Let us study the space bounds of procedure find. As the length of every 
vector register is bounded by O(cT(n) + n) all the variables appearing in find 
are bounded by O(T(n) + log n) bits. A stack implementation of find has depth 
O(T(n»). As every stack frame needs O(T(n) + log n) bits we obtain the result. 
o 

Exercises 
29 
1.5 Exercises 
1. Let V be a vector variable containing ... 001 and let I be an index reg-
ister containing 1. Give a vector machine constructing in V the vector 
... 00012• in OCr) steps. 
2. 
Fix p = 2". Give the contents of the vector A at the end of the following 
program: 
A := ." apap_l ... al; 
1:= 1; 
do r times 
A := A V (A 1 1); 
1:= 2 * I 
3. 
Give a vector machine solving in OCr) steps the recurrence: 
Assume the input is given as: 
4. 
Prove the identity: 
D n - 1 -
1 = (dn - 1 -
1)Dn_2 + (dn-2 -
1)Dn-3 + ... + do - 1 
Prove the uniqueness of the following decomposition : 
5. 
Consider a boolean matrix A = (ai .. _I".io) where all dimensions are powers 
of two: dj = 2kj. Recalling the definition: 
[i] = [in-d· .. [ijJ ... [ioJ 
prove that: 
[ij ] = Ukj+.,,+ko-l ••• Ukj_I+,,·+ko 
6. 
Defining x = 21' and y = 2j - p - 1 show that Mj(p) = (lxOZ)". Considering 
the following identity: 
deduce that: 

30 
Vector Machines 
7. 
Given a 2k * 2k matrix A = (ai,i)' this matrix can be seen as a 2 * 2 matrix 
like: 
A = (All A12) 
A21 
A22 
where each element is a 2k- 1 * 2k- 1 matrix. The transposition of the matrix 
A denoted as AT can be defined as: 
AT = (Ail 
Ail ) 
AI; 
Ai2 
Show that this is in fact the algorithm employed in Theorem 1.2. 
Hint. Considering A like a vector we have: 
A = (ai) where i = U2k-1 ••• UkUk_1 ••• Uo 
Show that the matrix TiL I,k-I (A) corresponds to the matrix: 
A = (!:~ !~~) 
8. 
Assuming that the vector masks Ali(p) are constructed, prove that the 
transposition of two blocks of bits can be realized in time proportional to 
the number of affected bits. 
Hint. An algorithm to interchange the blocks of bits f and 9 in the word 
oJ f3g, is given by: 
o:gf3f, = o(rf3TgT, 
where r means reverse. Note that to reverse a block of n bits we need a 
total of ljJ bit transpositions. 
9. 
Given a 4 * 4 matrix A = (aij) defined as: 
(i~~1) 
apply the preceding exercise to find A' = (a~i) where a~i = aji' Take 
ij = U3U2UIUO and 9 = U3 U2, f = UIUO· 
10. 
Given 0 ~ i < 28 denote as [i] the binary expansion of i with exactly s 
bits and consider: 
as = ... 000[28 -
1] ... [2][1][0] 
Recall that O:s is the list of the elements of {O, l}s. Give a vector machine 
which takes as input as and constructs the word 
f38 = ... (12'02')1 ... (12j02ji,-j-J ... (1100)2,-2(10)2'-1 
(parentheses are added to improve readability). 

Exercises 
31 
11. 
Given a vector A = (ai) with 0:::; i < 2r = p we define the perfect-shuffle 
of vector A = (ai) denoted as P SeA) = (aD as: 
, 
au,,_z."UOU,._l = aU.,._tU,._zo··uo 
Prove that: 
a~ = a2i 
for 
0 :::; i < I 
a~ = a2i+t-p 
for 
I:::; i < p 
Prove that it is possible to simulate the perfect-shuffle in time O(r) using 
only bit transpositions. 
12. 
Take as primitive operation the perfect-shuffle. Let A = (aij) be a matrix 
with 0 :::; i, j < 2r. Prove that transposition of matrix A can be done as a 
composition of r operations of perfect-shuffle. 
13. Given three binary numbers contained in registers X, Y and Z respec-
tively, let us define: 
S=XEBYEBZ 
C = «X J\ Y) V (Y J\ Z) V (Z J\ X» i 1 
Prove that S + C = X + Y + Z. 
Given four binary numbers W, X, Y and Z applying this method twice 
find S and C such that S + C = W + X + Y + Z. Use this method to 
demonstrate that the multiplication of nonnegative binary numbers can be 
performed within time o (log n), where n is the length of the result. 
Hint. Consider the program given in the Figure 1.5. Replace U and V 
by Us, Uc , Vs and Vc. The addition U + V at each iteration is replaced 
by the carry-save operations to compute the "new" Us and Uc from the 
"old" Us, Uc, Vs and Vc. 
14. 
Let us define expansions about other coordinates: 
Prove that: 
Et(A) = (et(A)ijk) where 
et(A)ijk = aik 
Eo(A) = (eo(A)ijk) 
where 
eo(A)ijk = aij 
E t (A) = T2,t (~(A» 
Eo(A) = Tt,o(Et (A» 
15. 
Give a vector machine that on input··· OOOw where w belongs to 1{O, 1}* 
outputs···OOOlength(w) where length(w) is the length of w written in 
binary. Design this program so that it works in O(llength(w)j2) steps. 
16. 
Use the preceding exercise to give a program accepting: 
L = { ww I w E I{O, 1}*} 
in O(log2 n) steps, where n is the length of the input. 

32 
Vector Machines 
1.6 Bibliographical Remarks 
The vector machine was the first theoretical model of "parallel" computer to 
appear in the literature. This model is due to Pratt, Rabin and Stockmeyer 
(1974). We have used the version of Pratt and Stockmeyer (1976). All the 
theorems appearing in this chapter are taken from this, although in some cases 
the presentation is slightly different 
The presented algorithms of matrix product and linear recurrences are well 
known in the literature. The same algorithms, presented in a context of com-
puter architecture, can be found in Hockney and Jesshope (1988). The reader 
interested in "practical" issues of vector programming can read Petersen (1983). 
Chapter 6 of Hong (1986) is also devoted to vector machines. We have 
chosen to study the connection between vector machines and Turing machines, 
whereas Hong studies the relation between vector machines and log-space trans-
form machines. 
Let us comment on some of the exercises. Exercise 7 is taken from Section 
27 of Hong (1986). Exercises 8 and 13 are taken from Pratt and Stockmeyer 
(1976). The notion of perfect-shuffle defined in Exercise 11 and Exercise 12 is 
taken from Stone (1971). 

2 
The Parallel Computation Thesis 
2.1 Introduction 
We shall study in this chapter the relation between sequential and parallel com-
putations. To this we consider in detail other models of parallel computers like 
Array machines, SIMDAG's machines or Tree machines. 
An Array machine is a random access machine with the ability to process 
arrays, i.e blocks of memory, in unit time. These machines have two different 
registers: the accumulator and the vector accumulator. The accumulator contains 
ordinary numbers and the vector accumulator contains arrays of numbers. The 
operations between differents blocks of the memory take place through the vector 
accumulator. 
The next is the SIMDAG machine. SIMDAG means single instruction 
stream, multiple data stream, global memory. This machine is a multiprocessor 
in which the whole process is controlled by a central processor unit. This pro-
cessor executes serial instructions and broadcasts instructions to be executed by 
the active parallel processor units. 
Another model is Tree machines. There is a root processor which starts with 
the computation. The root can activate a number of offspring. The root and 
the offspring can compute in parallel. This process can continue recursively. 
When a parent creates an offspring it sends the program to the new processor. 
All processors execute the same program, but different processors can execute 
different instructions. These machines are MIMD, multiple instructions, multiple 
data. 
Given a class of parallel machines :F we denote by :F-TIME(T(n» the class 
of problems computable in time T(n) with this model. It is interesting to study 
the computational power of :F. A way to do this consists in comparing with the 
class SPACE(TO(I)(n». When: 
:F-TIME(T°(1)(n» = SPACE(ro(1)(n» 
we say that :F verifies the parallel computation thesis. This thesis can be 
rephrased by saying that "sequential space" corresponds, within a polynomial, 
to "parallel time". We will prove that the previous models verify the parallel 
computation thesis. 

34 
The Parallel Computation Thesis 
Not all parallel models satisfy the parallel computation thesis. As an example 
the nondetenninistic parallel random access machines, denoted as NP - RAM 
verify: 
NP-RAM-TIME(O(n)) = NSPACE(20{T{n») 
The proof of this fact is left to the exercises. 
At the end of this chapter we consider other parallel models. In the next 
chapters we will study two other models of parallel machines: the alternating 
Turing machines and the uniform circuits. 
2.2 An Array Machine: the APM 
Schematically an array processing machine, Figure 2.1, APM for short, can be 
seen as a usual random access memory machine enlarged with array processing 
capabilities. As in random access models the APM has an unbounded memory 
consisting of an infinity of registers, Xo, Xi> ••• , X n , ••• Each register can hold 
an integer of arbitrary size. The APM machine has two types of instructions, 
the serial instructions and the array instructions. The serial instructions are one 
address instructions that take place through the accumulator denoted as ACe. 
The array instructions take place through the vector accumulator denoted as 
VAe. This vector is a potentially unbounded linear array of registers. Like 
Turing machines there is an input tape divided into cells. Each cell can contain 
an integer. If the input word is w = UIU2 ••• Un the integer Ul is contained in 
the first cell, U2 is contained in the second one and Un is contained in the cell 
number n. 
Let us consider the instructions of an APM machine. Let us begin with the 
serial instructions. 
READ a 
Loads ACC with the contents of the input cell number Xa' 
WAD a 
Loads ACC with the contents of the register Xa' 
STORE a 
Loads the register Xa with the content of ACe. 
ADD a 
Loads ACC with ACC + Xa' 
SUB a 
Loads ACC with ACC -
Xa' 
DIY a 
Loads ACC with l ~ J . 
ACCEPT 
The computation stops and it is accepting. 
JGTZ m 
Go to label m if Xo > O. 
We assume that every instruction has unit time cost. Notice that the in-
struction Xi := Xj * Xk does not belong to the list of serial instructions. If 

An Array Machine: the APM 
3S 
I 
I 
I J 
I 
I 
I J 
I 
I 1 .... 
I 
Control I 
I 
I 
XQ 
ACC 
xl 
VAC 
x2 
x3 
~ 
PROCESSOR 
MEMORY 
Figure 2.1 The APM machine 
multiplication was admitted, under unit cost multiplication, large numbers could 
be computed with few operations. Consider as an example the program 
x := 2; do t times x := x * x od 
This program constructs in time t + 1 the number 22'. When the instruction for 
unrestricted mUltiplication does not belong to the list of instructions the contents 
of the variables cannot grow too fast. The following lemma can be easily proved 
by induction on t. 
Lemma 2.1 In any computation on input UI ••• Un the contents of the registers 
at time t is bounded by: 
max{k * 2t, UI * 2t, ... , Un * 2t} 
where k is a constant depending on the program and on the initial contents of 
the registers. 

36 
The Parallel Computation Thesis 
Considering the last lemma we have that every data item at time t needs 
O(t) bits to be represented. 
Let us consider the array instructions. These instructions take place through 
the V AC. The vector accumulator is a potentially unbounded linear array of 
variables. Arrays are loaded into the V AC starting at its first position and 
overwriting the entire previous contents. This loading operation is assumed to 
be done in unit time. The VAC has a length equal to the length of the array 
stored in it. There can be no addressing of words within the VAC. Operations 
on the VAC take unit time and can use masks. 
Vectors and masks are indexed by scalar registers giving the address of the 
first and the last component. Vector instructions may have either two or four 
arguments. The first two arguments indicate the first and the last component of 
the vector instruction being applied. In the case that the second two (optional) 
arguments appear, they point to the first and last component of a mask vector. In 
this case, the vector instruction is applied only to the components corresponding 
to nonzero components of the mask. The list of vector instructions is: 
VREAD i,j 
VLOAD i,j{,p,q} 
VADD i,j{,p, q} 
VSUB i,j, {,p, q} 
VTGTZ i,j{,p,q} 
VTZERO i,j{,p,q} 
Read a vector of next available input into ad-
dress i to j. 
Copy the vector of operands in addresses i to j 
into the V AC. 
Add the vector of operands in addresses i to j 
to the VAC. 
Subtract the vector of operands in addresses i 
to j from the V AC. 
Compare the components of the V AC and store 
the results as a 0-1 valued vector in addresses i 
to j. A 0 means ":$ 0" and 1 means "> 0". 
Test the components of the V AC for zero and 
store the results as a 0-1 vector in addresses i 
to j. A 0 means "I" and 1 means "= 0". 
We remark that as the dimension of the vectors is controlled by scalar in-
structions, by the preceding lemma, this dimension cannot grow too fast. We 
give now some elementary examples of vector programming. These examples 
illustrate the methods of folding, doubling, and masking as described Chapter 1. 
Example 2.1 
This program computes the sum of N numbers in O(1og N) 
steps. We assume that the numbers are the components of a vector A[O .. N - 1]. 
This program is given in Figure 2.2 and uses the technique of recursive folding. 

Sum(A[O .. N - 1]): 
n=N; 
while n > 1 do 
if n is odd then 
A[O] := A[O] + A[n - 1]; 
n:= n - 1; 
m:= n/2; 
An Array Machine: the APM 
37 
A[O .. m - 1] := A[O .. m - 1] + A[m .. n - 1]; 
n :=m; 
return:=A[O] 
Figure 2.2 The sum of N numbers 
Example 2.2 We give an array implementation of the technique of recursive 
doubling. The procedure Doubling(X, N, r) takes as entry a vector X [O .. N -1] 
and finds the 2r -fold concatenation of vector X with itself in O(r) steps. The 
program is given in Figure 2.3 
Doubling(X, N, r): 
n:=N; 
A[O .. n - 1] := X[O .. n - 1]; 
do r times 
A[n .. 2n - 1] := A[O .. n - 1]; 
n:= 2 * n; 
return:=A[O .. n] 
Figure 2.3 Doubling of a vector 
Finally we illustrate the technique of masking. Given two vectors A, B 
the instruction "a:= mask of (A[O .. N -
1] < B[O .. n - 1])" such that "ai = 
if A[i] < B[i] then 1 else 0" can be implemented in constant time by array 
instructions. The same happens with "A := B masked by a" that changes the 
content of A as "ai:= if ai = 1 then bi else at Let us illustrate the technique 
of masking in the following example. 
Example 2.3 The following program Al ax(A[O .. N -1]) returns the maximum 
among the elements in A[O .. N - 1]. The program is given in Figure 2.4. 
APM programs and vector programs are quite similar. This similarity is 
expressed as follows: 

38 
The Parallel Computation Thesis 
Max(A[O .. N - 1]): 
n:= N; 
while n > 1 do 
if n is odd then 
A[O]:= max{A[O],A[n - In; 
n := n - 1; 
m:= n/2; 
a:=mask of (A[O .. m - 1] < A[m .. n - 1]); 
A[O .. m - 1] := A[m .. n - 1] masked by a; 
n:=m 
return:=A[O] 
Figure 2.4 Finding the maximum 
Theorem 2.1 Let T(n) be time constructible. Then: 
VECTOR-TIME(T(n» ~ APM-TIME(O(T2(n))) 
Proof Let A1 be a vector program running in time T(n). Assume ~his program 
contains as vector variables Vi, .. . , Vk and scalar variables It. .. . ,f[. We know by 
Lemma 1.1 that the length of the contents of any vector variable is bounded by 
the number length = 2P+T (n) + q where p and q are two constants depending 
on the program M and on initial contents of the variables. If T(n) is O(T(n» 
constructible by an array machine then the number length is also O(T(n» 
constructible. 
Let us construct an array program P simulating M in time O(T2(n». In 
the program P every index variable will be considered as a number and will 
be stored in a memory position; vector variables will be considered as an array 
of length numbers and will be stored in length consecutive positions of the 
memory. Assume that Vi is stored between the positions 0 and length - 1 of 
the central memory, V2 is stored between length and 2length - 1 and finally 
Vk is stored between (k - 1) * length and k * length - 1. To be consistent with 
notations on vector machines, the vector V; denoted as (Vi,length,'" , Vi,j, ••• Vi,!) 
is stored from the position (i - 1) * length to the position i * length - 1. The 
component Vi,j corresponds to the position (i - 1) * length + j - 1. The scalar 
variable I j for 1 ::; j ::; I is stored in the position k * length + j - 1. We assume 
that the memory is free from position k * length + I onwards and is filled with 
zeros. 
Let us simulate the instruction Vj := V; i Ip. In a first step the program M 
fills Vj with zeros. This can be easily done with the VAC and some free area 

A Multiprocessor Machine: the SIMDAG 
39 
memory of size length. In a second step it copies the vector (Vi,k-I" ... , Vi,l) 
into the V AC and copies the contents of the V AC into the positions of the 
memory corresponding to (Vi,k, ... , Vi,!,.I). The contents of the vector V; is 
then: 
(Vi,k-Ip , ••• , Vi,l, 0, ... ,00) 
The right shift and bitwise operations are simulated in a similar way. We con-
clude that every vector operation, different from the test, can be simulated in 
constant time by an array machine. 
Let us finally prove that the instruction "go to m if Vi = 0" can be simulated 
by an array machine in O(T(n» steps. Note that Vi = 0 iff 
length-I 
L 
Vi,i = 0 
JRO 
The addition can be computed by recursive doubling in O(T(n» steps. After 
this addition is computed the vector test can easily be simulated by a conditional 
instruction. 
0 
In the following section we will show how array machines can be simulated 
by space bounded Turing machines. 
2.3 A Multiprocessor Machine: the SIMDAG 
The SIMDAG is an example of a multiprocessor machine (Figure 2.5). It consists 
of a control processor called the CPU, a global memory, and a infinity of parallel 
processors units called the PPU's, numbered ppuo, ppUJ, ... ,PPUi,'" The control 
processor and the global memory behave like a random access machine. The 
registers of the global memory are denoted as Xo, XI, ••• ,Xi, •• ' The CPU executes 
the usual sequential instructions and broadcasts the instructions to be executed 
in parallel by the active PPU's. Each PpUi has an infinity of private registers 
denoted as YO,YIt . . ',Yi" •. The instructions permit every PPU to access and store 
into global and local memory. 
It is unreasonable to have a model where at each step of the computation an 
infinity of parallel processors remain active. To prevent this phenomenon it is 
necessary to introduce some mechanism to keep inactive all but a finite number 
of PPU's. This mechanism is implemented by a signature register. Assume 
that PPUi has signature i. The processor PpUi contains the signature in a special 
register sig. The register sig can be read by the PPUi but not overwritten. Each 
instruction broadcast by the CPU contains as a mandatory parameter a global 
register, for example Xl. When the PPUi receives an instruction it compares sig 
with Xl. If sig ~ Xl the PPU executes the instruction, otherwise it remains inac-
tive. A SIMDAG can execute two kind of instructions. The serial instructions, 
executed by the CPU and the global memory, are as usual in a random access 
machine: 

40 
The Parallel Computation Thesis 
Global Memory 
Sigl ... __ ---' 
Yo I--__ 
~ 
Yll--__ 
~ 
Sig\...I __ 
..-J 
Y0l-__ 
~ 
Yll-__ 
~ 
Figure 2.5 The SIMDAG machine 
Xi:= constant; 
Xi:= Xj +Xk; 
Xi := Xj -
Xk; 
Xi:= L¥J; 
Xi := XXi; 
XXi := Xj; 
accept; 
go to m if X j > 0; 

A Multiprocessor Machine: the SIMDAG 
41 
The parallel instructions are executed in parallel by ppUo through ppUXI • Each 
active PPU can execute some instructions between its private registers but these 
instructions do not contain an accept or a conditional instruction. These instruc-
tions are: 
Yi := constant 
Yi := Yj + Yk 
Yi := Yj -
Yk 
Y· .- l1!i..J 
,.-
2 
Yi := Yy; 
YYi := Yj 
Yi := sig 
Yi := Xy; 
[sig ~ xIl; 
[sig ~ xIl; 
[sig ~ Xl]; 
[sig ~ xIl; 
[sig ~ Xl]; 
[sig ~ Xl]; 
[sig ~ Xl]; 
[sig ~ Xl]; 
To isolate some PPU's from all others a conditional instruction is needed. For 
this reason the instruction which writes into the global memory is conditional. 
This instruction is: 
X Yi := Yj if Yk > 0 
[sig ~ xIl; 
The signature register is also used to solve the problem of simultaneous writes 
into the global memory. If various PPU's try to write simultaneously into the 
same global memory register only the PPU with lowest signature succeeds. 
In a SIMDAG only one instruction stream is executed or broadcast by the 
CPU; as a consequence, all active PPU's execute the same instruction. However 
different PPU's can process different data. Our model of a multiprocessor is 
a SIMD (single instruction stream, mUltiple date stream) machine. As com-
munication between different parallel processors takes place through the global 
memory our machine is a SIMDAG (single instruction stream, multiple data 
stream, global memory). 
In order to obtain sublinear execution times parts of the input could be read 
in parallel. To overcome this problem we assume the input is stored into the 
global memory. Initially the memory location Xi contains the i input symbol. 
As the global memory could be read and written in parallel by the differents 
PPU's the input-output process can be done in parallel. 
Example 2.4 Given 2% = p numbers ao, .. . ,ap_t. we want to calculate the 
partial sums E~_o aj. To do this assume that the processors PPUi for 0 ~ i < 2% 
contains Yo = ai. The program given in Figure 2.6 runs in O(z) steps and 
satisfies: 

42 
The Parallel Computation Thesis 
for t := 0 to z - 1 do 
Xsig := 0 
YI := sig + 2t 
XliI := yo 
yO := yo + Xsig 
[sig ~ 2t - 1]; 
[sig ~ 2% - 2t - 1]; 
[sig ~ 2% - 2t - 1]; 
[sig ~ 2% - 1]; 
Figure 2.6 The partial sums of p numbers 
i 
yo = L aj for 0 ~ i < 2% 
j-O 
Let us see how to simulate a SIMDAG on a Turing machine. The simulation 
will use the same ideas as the ones employed in the sequential simulation of a 
vector program. 
Theorem 2.2 SIMDAG-TIME(T(n» ~ SPACE(O(T2(n» 
Proof. 
Let 111 be a SIMDAG program bounded in time by T(n). Let us con-
struct a program P simulating 11'1. This program lists the successive instructions 
executed by the SIMDAG program and P accepts if the instruction accept is 
found. Denoting as (it, t) the instruction performed at time t this list has the 
form: 
(ii, 1), (i2 , 2), ... (it, t), ... 
Notice that P does not execute the instructions of 111, only lists these instructions. 
However when a branch instruction "go to m if Xi > 0" is found the value of 
Xi needs to be evaluated to choose the adequate branch of AI and continue the 
list. In the evaluation of the branch instructions the program P employs the 
procedures global(i, t) and local(i, n, t). The procedure global(i, t) returns 
the value of Xi at time t and the procedure local(i, n, t) returns the value 
of Yi at time t in the ppUn • Note that the contents of the SIMDAG registers 
at time t are completely determined by the SIMDAG registers at time t -
1. 
Considering the possible instructions of the SIMDAG one can define global(i, t) 
and local(i, n, t) in terms of global(i', t -
1) and local(i', n', t -
1) for 
various i' and n'. The procedures global and local are defined in Figures 2.7, 
2.8 respectively: 
Let us bound the size of the content of registers on a SIMDAG program. The 
PPUi has number i on its signature register, but PPUi remains inactive until it is 
woken up by the CPU. As unrestricted multiplication is forbidden the numbers 
generated by the SIMDAG program are bounded by O(2T(n»). As a consequence, 

A Multiprocessor Machine: the SIMDAG 
43 
global(i ,t) : 
if t < 0 return 0 
else if t = 0 then return Xi 
else if instruction at time tis: 
(Xi := constant) then return constant; 
(Xi := Xj + Xk) then return(global(j, t - 1) + global(k, t - 1» 
(Xi := Xj -
Xk) then return(global(j, t - 1) - global(k, t - I» 
(Xi := lXj/2J) then return <lglobal(j, t -l}/2J); 
(Xi := Xx;> then return global(global(j, t - 1), t - 1»; 
(xXi := Xk) and global(j, t - 1) = i then return global(k, t - 1); 
(XXi := Yk if Ym > 0 [sig =:; Xl]) then 
n :=0; 
while {(n =:; global(l, t - 1» 
and(local(m, n, t - 1) =:; 0 or local(j, n, t - 1) ::/ i}} 
do n := n + 1 od; 
if n =:; global(l, t - 1) then return local(k, n, t - 1) 
else return global(i, t - 1); 
(any other instruction) then return global(i, t - 1) 
Figure 2.7 The procedure global 
the parameters, local variables and values returned by the procedure are bounded 
in space by O(T(n». Recall that the program M is bounded in time by T(n). 
As consequence the depth of the stack implementing the procedures global(i, t) 
local(i, n, t): 
if t < 0 then return 0 
else if n =:; global(l, t - 1) and instruction at time tis: 
(Yi := constant) then return constant; 
(Yi := Yj + Yk) then return local(j, n, t - 1) + local(k, n, t - 1); 
(Yi := Yi - Yk) then return local(j, n, t - 1) - local(k, n, t - 1); 
(Yi := lYj/2J) then return 110cal(j, n, t - 1)/2J; 
(Yi := YlIi) then return local(local(j, 71, t - 1), 71, t - 1); 
(Ylli := Yk) and local(j, n, n - 1) = i then return local(k, n, t - 1); 
(Yi := sig) then return n; 
(Yi := Xli) then return global(local(j, n, t - 1), t - 1); 
(any other instruction) then return local(i, 71, t - 1) 
Figure 2.8 The procedure local 

44 
The Parallel Computation Thesis 
and local(i, n, t) is bounded by T(n). As stack frames are bounded in space 
by O(T(n», the whole procedure can be implemented in space O(T2(n». 
0 
We prove now that the ARRAY machines can be easily simulated by the 
SIMDAG machines. 
Theorem 2.3 Let T(n) be constructible. Then: 
APM-T1ME(T(n» ~ SIMDAG-T1ME(O(T(n» 
Proof We suppose that the global memory of the SIMDAG simulates the mem-
ory of the ARRAY machine, the register i of the array machine being simulated 
by the register Xi of the CPU. Suppose that each PPU has some distinguished 
registers: the accumulator AGG, the memory address register AfAR and the 
memory buffer register At BR. Denoting as AGGi the accumulator of the PPUi, 
the VAC is simulated then as : 
(AGGI , AGG2, ... , AGGi , ••• ) 
Let us simulate the unmasked instruction "VLOAD i,j" . The CPU calculates 
the dimension of the array to be load into the VAC as Xl := Xi -
Xi (recall 
the first PPU has number 0) and broadcasts into the active PPU's the following 
instructions: 
MAR := i + sig 
AGG:= XMAR 
[sig ~ xd; 
[sig ~ Xl]; 
This simulation is done in constant time. Other unmasked instructions have a 
similar constant time simulation. 
Let us consider masked instructions. Take as example the instruction: 
VLOAD i,j ,p,q 
Suppose the array to be loaded into the VAC is contained in XQ, •• "Xi-i' The 
vector mask is found in some other area of the memory, denoted as kQ, ... ,kq_ p• 
Assume j - i = q - p. As T(n) is constructible it is possible to find a free area 
of the global memory: suppose that Xt corresponds to the first position of the 
free memory. Assume that initially the registers of the free memory are filled 
with zeros. Denoting Xl = j - i the following program 
Yl := Xi 
Y2 := ki 
mar := t + sig 
X mar := Yl if Y2 > 0 
[sig ~ Xl]; 
[sig ~ xd; 
[sig ~ Xl]; 
[sig ~ xd; 

A Multiprocessor Machine: the SIMDAG 
45 
gives the result 
Xt+i = if ki > 0 then Xi else 0 
We can easily construct in the PPU's a vector mask complementary to the 
preceding one as: 
Y3 := 1 - Y2 
[sig $ xd; 
Let Xt', ... ,Xt'-j-J be some other area of free memory. Reasoning as before we 
can obtain: 
Xt'+i = if k; > 0 then ACi else 0 
To simulate the VLOAD it is sufficient to add, using the PPU's, the variables 
Xt'+i + Xt+i because 
Xt'+; + Xt+i = if ki > 0 then Xi else ACi 
The whole simulation is computed in constant time. Other simulations can also 
be computed in constant time. 
0 
Considering together Theorems 1.5, 2.1 and 2.3 we conclude the following 
corollary. 
Corollary 2.1 For 5(n) > log n and assuming that 5(n) is constructible we 
have: NSPACE(5(n» ~ SIMDAG-TIME(50(J)(n» 
Considering together Theorems 2.2, 2.3 we conclude the following: 
Corollary 2.2 Assuming that T(n) is constructible we have: 
APM-TIME(T(n» ~ SPACE(T2(n» 
Putting together Theorems 1.5, 2.1, 2.2, 2.3 we have that the APM and the 
SIMDAG machines verify the parallel computation thesis. 
Theorem 2.4 Let T(n) be a time constructible function such that T(n) > log n. 
The following classes coincide: 
SPACE(TO(J)(n» 
VECTOR-TIME(T°(J) (n» 
APM-TIME(T°(1)(n» 
SIMDAG-TIME(TO(J)(n» 

46 
The Parallel Computation Thesis 
2.4 A Tree Machine: the k·PRAM 
In the sequel we shall study the concept of a tree of processors and the idea of 
computation with such a tree. In a tree of processors each processor is a general 
purpose machine with unbounded memory capabilities. All processors execute 
the same program. But different processors can execute different instructions. 
It is a MIMD (multiple instruction stream, multiple data stream machine). The 
model has no global memory and there does not exist any processor with global 
control. The communication between processors is hierarchically organized and 
synchronized. 
The notion of k-offspring parallel random access machine, k-PRAM for 
short, gives a precise meaning to the concept of a tree of processors. More 
formally, in a k-PRAM every processor has available an infinity of registers 
numbered XO,XIo ••• ,Xn' ••• Every processor can create a total of k offspring. 
When a parent calls or accesses an offspring, this child starts its computation. 
The parent and the child can compute in parallel. The communication between 
the parent and the child is done in a precise way. When a parent calls an 
offspring I, it passes some parameters a" ... ,au • To do this the parent loads 
the offspring's registers xo, .. . ,Xu-l with at. ... ,au and the offspring starts the 
computation. During all the time the offspring is computing the parent cannot 
send more information to the offspring 1. When the offspring 1 wishes to send 
information to the parent, it does so via a special bank of parent's registers 
called channels, denoted as C{, C~, ... , C!. When 1 has sent information it stops 
the computation, reinitializes the program counter, and awaits another call. In 
the following we give the list of instructions available by a processor. 
A k-PRAM processor has the usual random access instructions like: 
Xi := constant; 
Xi := Xj + Xk; 
Xi:=Xj -
Xk; 
Xi:= l¥J; 
Xi := XXi; 
XXi := Xj; 
go to 1 if X j > 0; 
The connection between a parent and its offspring 1 is given by the following 
instructions: 
X '-C' 
i·-
h 
call1(al, a2, . .. ,au) 
return (a 1, a2, ... ,au) 
if I returned then ml else m2 
channel access 
call to offspring 1 
return to the parent 
I-return test branch 

A Tree Machine: the k-PRAM 
47 
If the parent attemps to access channel register e~ or call the offspring 1 while 
1 is still active the parent will be blocked at that point in the computation until 
offspring 1 completes its computation and returns. The l-returns-test branch is 
similar to sequential conditional instruction. If offspring 1 is still active the 
instruction labeled by m2 will be executed, otherwise the instruction labeled by 
ml will be executed. 
In the k-PRAM model the registers contain unbounded numbers. The input 
can be considered as a number, and loaded into the register Xo of the root 
processor. This point of view has the advantage that the whole input can be 
transmitted in unit time between the parent and the children. In this case, given 
an input y the "size" of y is the number n = log y. In order to compare k-
PRAM's with Turing machines it is necessary to consider the input as an string 
of characters. In this case it is necessary to unpack the number as a sequence 
of bits. When the computation time is greater than n the two approaches are 
equivalent. 
As usual we consider deterministic and nondeterministic programs. An input 
is accepted if there is a computation ending with the root processors executing 
a return instruction. A k-PRAM program is a list of optionally labelled instruc-
tions, and a positive integer u. The integer u is the number of parameters that 
are passed when a recursive call is made or returned. As for an ordinary pro-
gram, each k-PRAM processor has a pointer into the program giving the next 
instruction to execute. 
Example 2.S 
Let us give a 2-PRAM program solving the well known problem 
of computing the Fibonacci sequence defined as: 
fo = 0 
h = 1 
fn = fn-I + fn-2 
for n ~ 2 
A program solving this problem with u 
1 is given in Figure 2.9. 
if Xo = 0 then return(xo); 
if Xo = 1 then return(xo); 
call1(xo-l); 
call 2(xo - 2); 
X .- el • 
1·-
I' 
X2·- e2 • 
. -
I' 
X3 := XI + X2; 
return (x 3) 
Figure 2.9 A k-PRAM program for the Fibonacci sequence 

48 
The Parallel Computation Thesis 
Let us consider the time complexity. When several instructions are executed 
in parallel by different processors only one step of parallel time is considered. 
A set is accepted in nondetenninistic time T(n) if and only if for all inputs 
of size n there exists a computation that takes at most T(n) parallel steps of 
time. A program has depth complexity D(n) if and only if the associated tree 
of processors has depth D(n). 
We consider space complexity. When a register Xi contains a number z we 
say that the length of Xi denoted as length(xi) is log z. The length of a number 
i, denoted as length(i) is the number log i. Consider some computation on 
input x, where log X = n. We say that a processor is Sen) register bounded 
if at every point of the computation the registers of this processor satisfy the 
following condition: 
k 
u 
'L)length(xi) + length(i)] + E E length(Cj) ~ Sen) 
i 
I~I j-I 
where the sum is over all i such that the contents of Xi is nonzero. 
Lemma 2.2 Every nondeterministic T(n) ~ n time bounded k-PRAM M can 
be simulated by a nondeterministic O(T2(n» time bounded, O(T(n» depth 
bounded, O(T2(n» register bounded k-PRAM M' such that: 
1. 
The program for J.,f' has no return-test branches. 
2. If lv[' accepts an input x, then there is an accepting computation such 
that all calls return. 
Proof 
As Af can contain return-test branches we can assume that no processor 
is ever blocked waiting for a return. The machine Af will be simulated by a 
machine M'. 
In order to organize the communication between the processors, every active 
processor of the machine M' keeps a local clock set equal to the elapsed simu-
lated time of the corresponding processor of Af. Let us explain how this clock is 
used. Every time that a processor of M' executes a simulated instruction of Af, 
it increments by one the value of the clock. When a processor calls an offspring 
I it sends to it the simulated parameters of the call as well as the current value of 
the clock. The offspring initializes its clock to the current value of the parent's 
clock before starting the simulation. When the offspring 1 returns, it sends to the 
parent the current clock value as well as the corresponding returned parameters. 
Each processor uses the local clock to eliminate the return test branches as 
follows. During some steps the parent processor simulates that the offspring 
1 has not returned yet At some simulated time t/ the parent guesses that the 
offspring has returned. Then the parent accesses the I-channel trying to verify 
the guesses. There are three cases. First, the child I has returned in a time 
different from il. In this case the parent aborts the computation. Second, the 
child 1 has returned at time il. In this case the parent confirms the guess and 

A Tree Machine: the k-PRAM 
49 
continues the computation. From this time on, every 1 return test branch is 
simulated knowing that 1 has returned. Third, the child has not returned yet at 
time t/, and when the parent accesses channell it is blocked. To overcome this, 
the processor 1 will guess that the parent will access the channel at time t/. At 
time t/ the offspring 1 aborts the computation and returns to the parent signaling 
that it has not yet finished. In this case the parent aborts the computation. 
The local clock is also employed to simulate a return. First, when a processor 
wishes to return at simulated time t, it needs to confirm that all descendants have 
returned before it returns. If a child I has returned normally, there is no problem 
because the parent can access channell and confirm that I has returned. If a child 
1 has not yet returned and the parent accesses the channel I will be blocked. To 
overcome this problem the parent enters a phase of active waiting. At any step 
of computation the processor can nondeterministically choose between accessing 
the I channel or waiting until the next step. The processor I will guess that it 
should stop simulation at some simulated time t/. When I has confirmed that all 
descendants have returned it sends to the parent the value t/ and the fact that it 
has not yet finished the computation. 
Second, a processor can guess that it should stop computation and return to 
the parent. When it stops the computation, it stops also the clock giving the 
simulated time. Assume the clock has value t. The processor must wait until 
it has confirmed that all descendants have returned, then it can return. In the 
return it sends to the parent the simulated time t and the fact that it has not yet 
finished the computation. 
We consider the complexity of AI'. As the depth of Ai is bounded by the 
number of calls performed in T(n} steps, we have a bound of T(n}. As the 
calls and returns of Ai' "mimic" the calls and returns of Ai, the two programs 
give the "same" tree of proccessors. As consequence the machine Ai' has depth 
T(n}. 
Let us prove that Ai' is bounded in time by O(T2(n». Every processor can 
simulate any instruction, other than a return, of AI in constant time. The only 
situation in which time can be slowed down by more than a constant time is 
when a processor p decides to return. Assume p decides to return at simulated 
time t. All active descendants of p guess that they should stop computation at 
time t and return. These processors form a tree of depth bounded by T(n}. It 
is easy to design a bottom-up procedure to confirm that all these processors are 
returned in parallel time, bounded by T(n}. Since each time unit of Ai can be 
simulated by O(T(n» time units of Ai' we obtain a bound of O(T2(n». 
As in any processor the length of any register is bounded by max { O(T(n», n } 
and any processor has at most O(T(n» registers, we obtain a space bound of 
O(T2(n». 
0 
The following paragraphs are devoted to studying the relation between time 
bounded k-PRAM's and space bounded Turing machines. We assume that inputs 
are loaded as strings of characters. 

50 
The Parallel Computation Thesis 
Lemma 2.3 A T(n) 2: n time bounded nondeterministic k-PRAM can be 
simulated by an O(T3(n» space bounded Turing machine. 
Proof. 
Let M be a time T(n) bounded nondeterministic k-PRAM. Transform 
111 into Ml by the techniques of the preceding lemma so that the machine Ml 
has no return test branches and there exists a computation where all calls are 
returned. From 1111 construct M2 in such a way that every time a processor of Ml 
makes a call and continues to work in parallel with the child the corresponding 
processor of M2 also makes the call but "waits" until the child returns before 
continuing the simulation. As in Ml every call returns and the machine 1112 
cannot block. The tree of processors executing M2 is similar to the tree of 
a usual sequential recursive procedure. The machine 1112 can be implemented 
sequentially by a stack. The depth of the stack is bounded by O(T(n» because it 
is bounded by the depth of the tree. Each stack frame is bounded by O(T2(n» 
because this is the available space of every processor in the tree. We have 
obtained a nondeterministic sequential program simulating M with an O(T\n» 
space bound. 
0 
The simulation of a nondeterministic Turing machine by a k-PRAM employs 
a tree implementation of the usual divide and conquer procedure as developed 
in Theorem 2.9 of Volume I. 
Lemma 2.4 An Sen) 2: n space bounded nondeterministic Turing machine 
can be simulated by a nondeterministic 2-PRAM in time O(S2(n». 
Proof. 
Let M be a nondeterministic machine bounded by Sen). Let us describe 
a 2-PRAM M' simulating a computation of M. 
Any processor of M' receives a pair (Cl , C2) of configurations and tries to 
prove that C2 follows from Cl in one step. If this is true it sends an OK to 
the parent. In the other case the processor guesses a midpoint configuration C3, 
sends to the left child the pair (Cl, C3), and sends to the right child the pair 
(C3, C2). When the processor receives information from the left child about the 
validity of computation between Cl and C3 and from the right child about the 
validity of computation between C3 and C2, it transmits to the parent the validity 
of computation between C l and C2• 
This process gives us a tree of depth O(S(n». In every processor the guess 
of a midpoint configuration is bounded in time by O(S(n». These bounds give 
us a global bound on time of O(S2(n». 
0 
So far we have studied the relationship between k-PRAM's and Turing ma-
chines. In the following paragraphs we will study how to determinize k-PRAM 
machines. We will prove that the k-PRAM's can be taken to be deterministic 
with only a polynomial increase in the computation time. In order to simplify 
the proof we start by defining the notion of subcomputation. 

A Tree Machine: the AI-PRAM 
51 
Definition 2.1 
A subcomputation of k-PRAM machine M on parameter list 
(aI, az, ... , au) is a computation performed by a processor and its descendants, 
executing program M, from the instant it is initialized with Xi := ai+1 for 0 ~ 
i ~ u - 1 and Xi := 0 for i 2:: u, until it executes a return instruction. If it never 
executes a return, we also refer to this infinite computation as a subcomputation. 
We are dealing with nondeterministic computations. We assume some con-
ditions of consistency. 
Definition 2.2 A nondeterministic k-PRAM }.1 is said to be consistent when 
the following two conditions hold. 
1. 
For any parameter list and any two subcomputations of 111 on this param-
eter list, if both sub computations return, they return the same parameter 
list. 
2. 
The program 111 contains no return test branches. 
Consistency can be assumed with only a small increase in computation time. 
Lemma 2.5 Every nondeterministic T(n) time bounded k-PRAM }.1, where 
T(n) 2:: n, can be simulated by a nondeterministic consistent k-PRAM }.1' in 
time O(Tz(n». 
Proof Every processor of }.1' keeps a local clock giving the elapsed simulated 
time. With this clock return test branches can be eliminated as described in 
Lemma 2.2. 
In order to assure consistency, the machine M' deals with calls, returns, and 
access channels in a special way. When a processor calls an offspring it passes 
both the calling parameters and a guess of what will return as a result of the 
call. Both sets of values, the parameters and the guessed results, are saved by the 
parent and the called offspring. When a processor wishes to return it compares 
the guessed values received from the parent with the results that it would return. 
If the two lists agree it returns. 
0 
Consistent machines can be determinized. In the proposed algorithm the 
processors need to know the computation time allowed for the whole process. 
Thus T(n) needs to be time constructible. 
Lemma 2.6 Let T(n) be time constructible. Every nondeterministic T(n) time 
bounded consistent k-PRAM, with k 2:: 2, can be simulated by a deterministic 
k-PRAM in time O(Tz(n». 
Proof Let}.1 be a nondeterministic consistent k-PRAM. Let us construct a 
deterministic machine M'. Let us explain how }.1' behaves in a call. Suppose 
that a processor p executing 111 makes a call to the processor 1 with parameters 
al,. .. , an. Assume that, with the call, the processor p sends the value T(n) to the 

52 
The Parallel Computation Thesis 
processor 1. The processor 1 generates 2T(n) descendants by a sequence of T(n) 
recursive calls and 2T(n) vectors, one for every descendant These vectors have 
dimension T(n). Suppose a descendant contains a vector V giving the number 
of the instruction to be chosen at each step of the computation. If V[i] is k, at 
time i the instruction with label k will be chosen in the execution of simulated 
program 111. The processors involved in the generation of 2T (n) processors wait 
until one of their offspring returns. This is done by a cycle of test-branches 
running over all offspring. As soon as one of their offspring has returned, it 
returns. As M is consistent, the returned result is independent of the returning 
offspring. 
The computation time of M' is bounded by O(T2(n» because the depth of 
M is at most T(n) and at each level every processor of M' needs to compute 
2T(n) descendants in time O(T(n». 
0 
The preceding two lemmas give us a proof of the following theorem. 
Theorem 2.5 Every nondeterministic T(n) time bounded k-PRAM can be sim-
ulated by a deterministic k-PRAM in time O(T4(~», provided k ~ 2, T(n) ~ n 
and T(n) is time constructible. 
From the two preceding lemmas and theorems, the k-PRAM verifies the 
parallel computation thesis. 
Theorem 2.6 For T(n) ~ n we have: 
NSPACE(T°(1)(n» = k-PRAM-TIME(TO(I)(n» 
2.5 Further Parallel Models 
We review in this section some other models of parallel machines. These models 
satisfy the parallel computation thesis. We give just the definitions of the models 
and some of the theorems that can be proved; we do not present the formal 
proofs. 
First consider the Parallel RAM model, P-RAM for short. This model ap-
pears as a variant of a random access model combining some elements of a 
SIMDAG machine and some elements of the k-PRAM machines. This machine 
is able to wake up a large number of processors, all operating both on private 
memory and on global memory. In this aspect it is similar to a SIMDAG. How-
ever this machine is not restricted to execute the same instruction in all active 
processors at the same time; in this aspect it is similar to the k-PRAM. 
Formally a P-RAM consists of an unbounded set of processors called ppUo, 
PPUh .•. , ppun , ••• an unbounded global memory and a finite program. The 
positions of the global memory are denoted as xo, Xl. ••• , X n , ••• Each processor 

Further Parallel Models 
53 
ppU, has an unbounded local memory Yo, Yt. .. "Yn"" The register YO is called 
the accumulator of the processor. Each processor has a program counter and a 
flag indicating whether the processor is running or not A program consists of 
a sequence of possibly labeled instructions chosen from the following list: 
y, := constant; 
y, := Yj + Yk; 
Y· '- L1liJ' 
•• -
2 
' 
y, := YYj; 
accept; 
go to m if Yj > 0; 
y, := X yj ; 
X Yi := Yj; 
fork m; 
The parallelism is achieved by the "fork m" instruction. When a PPUi executes 
a fork instruction it selects the first inactive processor ppu j, clears the local 
memory of ppUj, and fetches the accumulator of PPUi into the accumulator of 
the ppUj. Then the processor ppUj starts its computation in the label m of 
the program. Remark that if PPUi initializes ppUj to perform a subtask, it can 
pass via its accumulator the address in global memory of a block containing the 
parameters of such a subtask. Among these parameters can be the address of 
the position where processor j must return its result. Processor i can repeatedly 
test this position to determine whether processor j has finished its computation. 
Note that a ppu, cannot access directly a position of the local memory of ppUj. 
The communication between processors takes place through the global memory. 
In the P-RAM model we allow simultaneous reads in local memory by 
different processors. If two processors try to write into the same position of the 
global memory the machine immediately halts and rejects. Several processors 
may read a position of the global memory while one processor writes into it; 
in this case all reads are performed before the value of the position is changed. 
The relation between Turing machines and P-RAMs is given by: 
Theorem 2.1 For T(n) ~ log n we have: 
P-RAM-TIME(yoO(I)(n» = SPACE(TO(I)(n» 
To deal with different degrees of concurrency when reading from or writ-
ing to the global memory, there are some variants of parallel random access 
machines: 
• 
EREW P-RAM model. A model with Exclusive Read and Exclusive Write. 
Neither concurrent reading nor concurrent writing is allowed. 

S4 
The Parallel Computation Thesis 
• 
CREW P-RAM model. Concurrent Read is permitted but there is Exclusive 
Write. 
• 
CRCW P-RAM model. Concurrent Read and Write is allowed. However, 
processors trying to write in the same memory location must be writing 
the same value, otherwise the program is illegal. 
• 
Priority CRCW P-RAM model. When several processors try to write into 
the same memory position, the lowest numbered processor succeeds. 
Second consider the Multiplication-RAM model, M-RAM for short. Consid-
ering the vector machines we have seen that a way to simulate parallelism is to 
manipulate huge data in unit time. In vector machines long strings of bits can 
be operated in unit time. 
It is possible to extend the random access machines to create and manipulate 
huge data. This is done in the multiplication random access machine. This 
model contains the usual sequential instructions, but contains also an operation 
of multiplication and a set of boolean operations. The contents of a register are 
considered both as an integer and as a vector of bits, and arithmetic and boolean 
operations may be used in the same register. The set of available instructions 
is: 
Xi := constant; 
Xi:=Xj+Xk; 
Xi := Xj -
Xk; 
xi:=L¥J; 
Xi := XXi; 
go to 1 if X j > 0; 
accept; 
This set of instructions is enlarged with: 
Xi := Xj bool Xk; 
Xi := Xj * Xk; 
Considering this model we have: 
Theorem 2.8 M-RAM-NTIME(n°(l» = PSPACE 
Third consider the conglomerates. A conglomerate is a machine composed 
of finite controls. Each control has r input channels and one output channel. 
The topology of the network of components is given by the connection function. 
Formally a conglomerate C = (I, F, f) is formed by: 
1. 
A finite input alphabet I. 

Further Parallel Models 
55 
2. 
An infinite set F of finite controls M; = (E, Q, 0, R), i ;:::: O. E is the 
finite communication alphabet. Q is a finite set of states. The starting 
state is qo. The accepting state is qa' For any x belonging to the input 
alphabet qx belongs to Q. The transition function 0: Q x Er ~ Q x E 
has r input channels and one output channel. 
3. 
The connection function f : {I, ... , r }. ~ IN U .1 gives the topology of 
the network. This function is defined inductively as: 
(a) 
f().) = O. Note that the root processor is Mo. 
(b) 
f(si) = j if and only if there exists k such that f(s) = k, and M/s 
output channel is connected to the input channel number i of !11k , 
(c) 
f(si) = .1 if and only if such k does not exist. 
The connection function is defined in such a way that when f(s) =I .1 this 
function gives the index of the finite control reached starting at Mo and following 
backward the chain of inputs represented by s. 
Let us define how a conglomerate processes a word w = XI ••• Xn belonging 
to r. When w is input to a conglomerate, the controls M; for 1 :::; i :::; n start 
in the state qXi' All other machines start in qo. Initially all the machines output 
the blank symbol. The conglomerate accepts w if Mo reaches qa at some time. 
An important aspect of the conglomerates is the interconnection pattern given 
by the connection function. One cannot expect the connection function of build-
able machines to be arbitrarily complicated, because this would demand com-
plicated wiring methods, rendering the construction of such machines infeasible. 
A natural question is how powerful are the conglomerates whose connection 
functions are computable in space Pen). 
Theorem 2.9 Let C be a conglomerate whose connection function can be 
computed in nondecreasing space Pen) by a Turing machine. If C accepts a 
language L in time T(n), then 
L E SPACE(O(T(n)+P(fIogrlT(n))) 
where r is the number of inputs to each finite control. 
In fact parallel machines can be simulated by conglomerates with easy con-
nection functions. The following theorem says that a SIMDAG can be imple-
mented by a conglomerate. 
Theorem 2.10 Given any SIMDAG which accepts a language L in time T(n), 
there exists a conglomerate C wich accepts L in time O(T2(n». Furthermore 
the connection function of C is fixed, independent of the SIMDAG, and can be 
computed in logarithmic space. 
Finally let us consider the Aggregates. The parallel time is clearly a funda-
mental resource in complexity. However the study of parallel time does not give 

S6 
The Parallel Computation Thesis 
complete insight into the cost of parallel computation. Clearly the number of 
processors and the amount of "hardware" are also important resources. Models 
like SIMDAG emphasize the number of processors but since every processor has 
unbounded memory devices, these models cannot be used to study the hardware 
limitations. The depth of boolean circuits gives us a nice characterization of 
parallel time. However to estimate the hardware by the number of gates seems 
to be unrealistic because in circuits the gates cannot be reused. Any reason-
able parallel machine would presumably be able to reuse its hardware. These 
considerations lead to the definition of the aggregate. 
The aggregate model combines features of the sequential circuits or logi-
cal networks and of the conglomerate model. Like networks, every aggregate 
works for exactly one input length and each gate computes only a boolean func-
tion. Like a conglomerate, the gates work in a synchronic way, and there is no 
ambiguity in the computation. 
Formally, an aggregate f3n on inputs x = XI ... Xn is a directed graph, not 
necessarily acyclic. Every node v has a label gv. When gv is a boolean function 
{O, l}i ~ {O, I} this node has i input nodes and one output node. When gv is 
X this node is an input node and its object is to read the input x. Associated 
with this node there is a "register" Rv consisting of log n nodes, which specifies 
which input Xi is presented to v. There are two distinguished nodes Vo and VI 
called output nodes. A configuration of f3n is an assignment of 0 and 1 to each 
node of f3n. 
A computation is a sequence Co, CI, ... such that: 
1. In the initial configuration Co all nodes have output 0 except any node 
labeled with the constant function 1. 
2. If the label of a node v is a boolean function gv then in Ct+1 the node v 
has output equal to gv applied to the inputs of v in Ct. 
3. We deal in a special way with input nodes. If v is an input node then v 
outputs 0 in Ct for t < log n. In general, in Ct+logn the node v outputs 
Xi+lt where i is the value in binary of Rv in Ct • 
4. The output of f3n is defined to be the output of Vo at the first time such 
that VI has value 1. 
The hardware size, denoted h(f3n), is the number of nodes in f3n. The running 
time of f3n is denoted as t(f3n). Let us justify the particular input conventions. 
Remark that if inputs were read directly into the aggregate, this would imply 
h(f3n) ~ n, whereas we are interested in sublinear bounds. In fact the value 
of v could be computed from the input register Rv using a decoding circuit of 
size O(n) and depth O(1og n). For this we have assumed a delay of log n. The 
assumption of not counting the decoding circuit is similar to the convention of 
not counting the input tape in measuring the space used by a Turing machine. 
A family of aggregates {f3nln ~ I} is uniform if and only if the function 
from Into a standard coded version of f3n can be computed in deterministic 

Exercises 
57 
space O(log(h(Bn ) * t(J3n))). UAG-HARDWARE(S(n» is the class of problems 
solved by uniform aggregates f3n with h(J3n) = O(S(n». We have the following 
relation 
Theorem 2.11 
UAG-HARDWARE(S(n» = SPACE(S(n» 
In the following two chapters we will consider two other models of parallel 
machines: alternating Turing machines and uniform circuits. 
2.6 Exercises 
1. 
Formally a RAM machine is an Array machine without vector instructions. 
The RAM machines are "equivalent" to the Turing machines; prove that: 
RAM-TIME(T°(1)(n» = TIME(TO(I)(n» 
2. 
We name vectorization the method tranforming some parts of sequential 
programs on vector programs. Some FORTRAN compilers vectorize the 
innermost loops. Sometimes, problems arise. Take the following piece of 
program: 
do 10 I = 2, N 
A(I -
1) = N EW(I) 
10 0 LD(I) = A (I) 
Taking N = 5, NEW = (2, 4, 3, 1, 5), and A = (1, 2, 5, 7, 8), 
construct OLD. 
Suppose that we vectorize it into following program: 
do 15 I = 2,N 
15 A(I - 1) = N EW(I) 
do 20 I 
= 2,N 
20 OLD(I) = A(I) 
where the loops mean in fact array instructions. Construct the content of 
OLD in this case. Show that the result is different from formerly. 
Show that to solve the problem there are two solutions. The first one 
consists of permuting the two instructions and the second one consists of 
introducing an auxiliary vector. 
3. 
Explain and vectorize the following program: 
do 10 I = 2,N 
B(l) 
A(I -
1) 
10 A(I) = C(l) 

58 
The Parallel Computation Thesis 
4. Test instructions can also be vectorized using vector masks. Consider the 
following program where if(e) = nJ, n2, n3 branches to nl if e < 0, to 
n2 if e = 0 and to n3 if e > O. 
do 3 I = I,N 
if(C(l» 2, 1, 1 
1 A(I) = Expressionl 
go to 3 
2 A(I) = Expression2 
3 continue 
Assume that the computation of Expressionl and Expression2 can be vec-
torized. Vectorize the whole program employing the following two vector 
masks: 
1.1 ASK(V)(i) = if V(i) ~ 0 then 1 else 0 
AfERGE(l't, Vi, ~)(i) = if ~(i) = 1 then Vi(i) else V2(i) 
5. 
Translate into a detailed APM program the algorithm SurnA[O .. N - 1] 
described previously in an Algol-like language enlarged with vector pro-
cessing capabilities. 
6. 
Given a vector A[O .. N - 1] the rank of an element x, denoted as rk(x), is 
the number of components of value smaller than x in the vector. Find 
an APM procedure Rank(A[O .. N -
1], x) returning rk(x) running in 
O(1og N) steps. 
7. 
Let N = 2k. Give an APM program taking as input a vector (aN-I, ... , ao) 
loaded into a vector A[O . .. N - 1] and outputs in O(k) steps in an scalar 
variable S the value 
8. 
An n dimensional matrix a = (aij) where 1 :5 i,j :5 n is lower triangular 
if: V i we have aji = 0 and V i, j with i < j we have aij = O. We note: 
a2i 
(
ali ) 
a.i = 
: . 
ana 
Let b be an n dimensional vector. Let us show how to solve a triangular 
system a * x = b. To do this define iteratively the n dimensional vectors 
yO, yl, ... , yn as: 
1+1 
1 
Y 
= Y -
ZI+l * a.,I+l 

Exercises 
59 
where the numbers ZI+1 are defined as: 
prove that: 
X= (f) 
Describe infonnally an APM program solving a triangular system in O(n) 
steps. Using the techniques of recursive doubling and LU decomposition 
this problem can be solved by an array machine in O(logZ n) steps. 
9. 
Consider a set of numbers a),az, .. . ,at where t = 2%. Assume these num-
bers are contained in the registers of the parallel processors of a SIMDAG. 
The register YO of the PPUi is set to ai and suppose that all other registers 
are set to O. Give a SIMDAG program constructing in time O(z) in the 
register yo of the PPUi the number E~-i aj. 
10. 
Give a SlMDAG program such that at the end of O(z) steps the register 
Yo of the PPUi satisfies: 
YO = { ~IOgiJ 
if i = 0 
if 0 < i ~ 2% 
11. 
Assume that in a SlMDAG the variables Xp,Xp+Io ••• correspond to free 
positions in the global memory. Assume the same for every PPUi on 
positions Xq,Xq+), ••.• Simulate in constant time the instruction: 
Yj := if Yk > 0 then Yl else Ym 
[sig ~ x,]; 
Use this instruction to simulate in constant time the following three in-
structions: 
Yi := XIIi if Yk > 0 
[sig ~ Xl]; 
XII. := Yj if Yk = 0 
[sig ~ x']; 
Yi := max.{Yh Yk} 
[sig ~ x']; 
12. 
Corollary 2.1 can be improved. Prove the following: if T(n) ~ logn is 
SIMDAG constructible then: 
NSPACE(T(n» ~ SIMDAG-TlME(O(T(n» 
Hint. The idea is to implement the transitive closure of the one step 
transition matrix. The algorithm is done in two stages. During the first 

60 
The Parallel Computation Thesis 
stage the PPU(i,j) is instructed to compute the component i,j of the one 
step transition matrix.. Next for O(T(n» steps the processor PPU(i,j,k) 
inspects the processors PPU(i,j) and PPU(j,k) and actualizes the component 
i, k of the matrix. Note that this step requires the instruction 
x Yi := Yj if Yk 2: 0 
[sig ~ Xl]; 
otherwise it seems to be impossible that the positive information of the 
1 found by some PPU(i,i,k) is not overruled by the negative 0 from some 
PPU(i,i',k) with j' < j. 
13. Let T(n) 2: n be any time constructible function. Prove that there exists a 
set L accepted by an O(T(n» time bounded deterministic 2-PRAM such 
that L cannot be accepted by any T(n) time bounded nondeterministic 
RAM with a program in which each label occurs at most 2 times. 
14. Let Tl (n) 2: n be any time constructible function. Prove that there exists 
a set L accepted by an O(T1(n» time bounded deterministic 2-PRAM but 
L is not accepted by any O(T2(n» time bounded nondeterministic RAM, 
provided that: T2(n) = o(Tl(n» 
15. Let us examine the effect of limiting parallelism by limiting the number 
of offspring a processor may call. Prove that a nondeterministic T(n) 2: n 
time bounded k-PRAM can be simulated by a nondeterministic consistent 
2-PRAM in time O(T3(n» 
16. Let us examine the effect of limiting parallelism by limiting the total 
number of processors computing in parallel. Let U(n) be a bound on the 
number of processors computing in parallel during the computation. Let 
T(n) be constructible in time O(U(n)T(n». Prove that a k-PRAM which 
is simultaneously T(n) time bounded and U(n) processor bounded can be 
simulated by an ordinary RAM in time O(U(n)T(n». Furthermore, if the 
k-PRAM program is deterministic, the simulating program will also be 
deterministic. 
17. Prove Theorem 2.7 
18. 
A P-RAM program is said to be nondeterministic if some label occurs 
more than once. We denote as N-P-RAM the class of nondeterministic 
programs. For T(n) 2: log n prove the following theorem: 
NP-RAM-TIME(O(T(n))) = NSPACE(20(T(n» 
Conclude that: 
NP-RAM-TIME(O(log n» = NP 
NP-RAM-TIME(nO(I» = NEXPTIME 
19. Prove Theorem 2.9 
Hint. Give a recursive procedure Reach(s, q, P, t) to be true iff finite 
controlll.fj(s) reaches state q and outputs P at time t. 

Bibliographical Remarlcs 
61 
2.7 Bibliographical Remarks 
The parallel models of synchronic computation and the parallel computation 
thesis seem to have been developed roughly between 1974 and 1978. In 1974 
the concept of random access machine was extended to include the processing of 
huge data (Pratt, Rabin, and Stockmeyer (1974), Hartmanis and Simon (1974». 
In these models the time is proved equivalent, within a polynomial, to the space 
of the Turing machines. In 1976 the alternating Turing machines are defined 
by Kozen (1976) and Chandra and Stockmeyer (1976) as a model of paraliel 
machines. The parallel computation thesis seems to have been first proposed 
in print in Chandra and Stockmeyer (1976). In 1978 appear other models with 
explicit parallelism: Fortune and Wyllie (1978) and Goldschlager (1978). The 
storage modification machines proposed in Schonhage (1980) are also a model 
of parallel machines. 
Let us consider in more detail the models appearing in this chapter. The 
Array machines are due to van Leeuwen and Wiedermann (1985). These ma-
chines are designed to model the existing vector and array processors. Those 
interested in "real" vector and array processors can read Chapter 3 of Hockney 
and Jesshope (1988). In Goldschlager (1978, 1982) the SIMDAG model is pre-
sented. This model is based on random access machines operating in parallel 
and sharing a common memory. The conglomerate model is also introduced in 
this paper. It is inspired by iterative arrays of processors and by binary trees of 
finite controls. It is also proved that SIMDAGs can be efficiently simulated by 
conglomerates. For concrete examples of arrays of processors look at Section 
8.3 of Mead and Conway (1980) or Chapter 5 of Ullman (1984). Savitch and 
Stimson (1979) extend the random access machine to permit recursive calls; 
the model obtained is the k-PRAM giving trees of processors. A more prac-
tical view of tree machines can be found in Section 8.4 of Mead and Conway 
(1980). Another extension of the random access memory machines with parallel 
processing capabilities is the Parallel RAM model due to Fortune and Wyllie 
(1978) where processors can wake up other processors. The Multiplication RAM 
model is due to Hartmanis and Simon (1974). In this model the random access 
memory is extended with a multiplication operation, having the added capabili-
ties of computing logical operations on bit vectors in parallel. The polynomial 
time PRAMs with both multiplication and shifts are considered by Trahan, Ra-
machandran, and Loui (1989). Parallel time is clearly a resource for parallel 
computation, but does not give complete insight in the cost of the computation; 
the number of processors and the amount of circuitry are also important. To deal 
with these problems Dymond and Cook (1980, 1989) have introduced two other 
models of computation, the aggregates and the hardware modification machines. 
The aggregates can be seen as combinational circuits with cycles. The hardware 
modification machines can be seen as a collection of variably connected finite 
state sequential machines. 

62 
The Parallel Computation Thesis 
A first attempt on the general theory of synchronous parallel computation is 
given in Cook (1981). Van Emde Boas (1985a,1985b) gives a survey on "the 
second machine class", referring to the models of parallel computation. Part 2 
of Hong (1986) is devoted to the parallel computation thesis. Parberry (1987) is 
also devoted to the study of parallel complexity theory, and in Chapter 11 raises a 
new parallel computation thesis. In this thesis time and word size on reasonable 
shared memory machines are simultaneously equivalent to alternations and time 
on an alternating Turing machine, the former to within a constant, and the latter 
a polynomial. Parberry (1988) characterizes parallel computations based on 
threshold functions. Chapter 11 of Wegener (1987) is devoted to parallel random 
access machines. From the point of view of the analysis of parallel algorithms 
Vishkin (1983) gives a nice short survey. A more complete survey on parallel 
algorithms for shared memory machines is given in Karp and Ramachandran 
(1988). Two books on parallel algorithms are Gibbons and Rytter (1988) and 
Akl (1989). 
More questions about parallel computation have been studied. Galil and 
Paul (1983) study the question of what is a good way to interconnect a large 
number of processors. Stockmeyer and Vishkin (1984) show how to simulate 
random access machines by circuits of unbounded fan-in. The simulation of 
ideal parallel computers is studied in Section 6.4 of Ullman. 

3 
Alternation 
3.1 Introduction 
In this chapter we shall study another computational model: the alternating 
Turing machine. This model combines the power of nondeterminism with par-
allelism by alternating the existential and universal states. Alternation connects 
time and space complexity rather well, in the sense that alternating polynomial 
time equals deterministic polynomial space, and alternating linear space equals 
deterministic exponential time. Analogous relationships hold for other time and 
space bounds. In the next chapter, we shall see how alternation links with the 
Parallel Computation Thesis presented in the previous chapters. 
In Section 2 we introduce the definition and properties of Alternating Turing 
Machines. In Section 3 we define the complexity classes for alternation and 
compare them with other complexity classes previously defined. Section 4 is 
devoted to introducing a special kind of graphs which model nondeterministic 
computation; although not specifically related to alternation, this section belongs 
to this chapter since it is the tool used in the last section to prove that the class of 
languages recognized by deterministic machines in linear time is strictly included 
in the class of languages recognized by nondeterministic machines in linear time. 
This is one of the few known results in which nondeterminism is shown to have 
strictly more power than determinism. 
3.2 Alternating Turing Machines 
Recall that in Chapter 1 of Volume I, we described the notion of an accepting 
computation of a nondeterministic Turing machine. An equivalent description 
of this concept is the following: For any input x, to see if a machine 111 accepts 
x, we define a bottom-up labeling of its computation tree (or part of it), by the 
following rules: 
• 
the accepting leaves are labeled 1; 
• 
any node is labeled 1 if at least one of its sons is labeled 1. 
The machine accepts x if and only if the root is labeled 1. We can strengthen 
the nondeterministic computation model by partitioning the nonterminal nodes 

64 
AJ~ation 
into two classes, depending on the state which the machine is in when reaching 
the node: the existential states and the universal states. We define for this new 
model of computation a bottom-up labeling l in a similar way as we have done 
for the nondeterministic computation tree: 
Definition 3.1 Given a tree in which internal nodes are either existential or 
universal, we denote by l the result of the following labeling procedure: 
1. 
the accepting leaves are labeled 1,· 
2. 
any existential node is labeled 1 if at least one of its sons has been 
labeled 1; 
3. 
any universal node is labeled 1 if all its sons are labeled 1. 
The machine accepts the input if and only if the root is labeled 1. This 
computation model corresponds to the alternating Turing machine (ATM) model. 
Formally, 
Definition 3.2 A k-tape alternating Turing machine AI is a five-tuple: 
AI = {Q,E,6,qa,g} 
where Q, E, qa and 6 are defined as for the nondeterministic Turing machine,· 
and 9 : Q -+ {A, V, acc, rej} is afunction which partitions the states of Q, into 
respectively universal, existential, accepting, rejecting. 
In the same way as we did with the Turing machine, we can define the 
concepts of configuration of the machine on an input, and computation. 
Definition 3.3 A configuration of a k-tape machine AI on input x, is a (2k+ 1)-
tuple giving }\([' s current state, its input tape contents and head position, the 
contents of its k - 1 work tapes and the head positions in the k - 1 work tapes. 
If the state associated with a given configuration is a universal (respectively, 
existential, accepting, rejecting) state, then the configuration is said to be a 
universal (respectively, existential, accepting, rejecting) configuration. 
The initial configuration of AI on input x is given by: 
ao = (qO, x,#, ... #,0, ... ,0) 
------~ 
k-l 
k 
The concept of step is defined as for Turing machines. A configuration a j 
is an immediate successor of & configuration ai, writen (ai I- aj), if aj follows 
from ai in one step according to the transition function 6. 
Definition 3.4 A computation path of AI on x is a sequence ao I- al I- ... l-
an where ao is the initial configuration of :AI on x. 

Alternating Turing Machines 
65 
A configuration aj is a successor of a cOllfiguration ai, written (ai 1-. aj), if 
aj is reachable from ai by a computation path. 
Notice that we have introduced the concept of an alternating machine from 
the concept of a nondeterministic machine via the computation tree of the non-
deterministic machine. Moreover, for an alternating machine to accept an input, 
it is required that all the computation paths going out from a universal node 
lead to an accepting leaf. Therefore, the concept of an accepting computation 
for alternating machines can only be formalized in terms of computation trees. 
Definition 3.5 
Given an ATM M and an input x, a computation tree of M on 
x is a possibly infinite tree whose nodes are configurations, the root being the 
initial configuration 0:0, and for any node O:i, its sons are those configurations 
which are immediate successors of O:i. 
To define acceptance we use the labeling procedure e of Definition 3.1. 
Definition 3.6 
111 accepts x if there is a computation tree of M on x in 
which the root gets label 1 under the labeling procedure e. The subtree of 
the computation tree which has all the nodes labeled 1 is called the accepting 
computation tree of M on x. 
Of course, the computation tree of every alternating machine must have 
bounded fan-out. The fan-out of a tree is the maximum number of sons any 
node has. 
As usual, define the language accepted by an alternating machine 111, by 
L(M) = {x 1111 accepts x} 
Summing up, an alternating Turing machine is just like a nondeterministic 
Turing machine such that some of its states are universal states: in these states 
the machine "must follow simultaneously" every one of the computations which 
are given by the transition function from the state under consideration and check 
that all of them accept. 
It is worth noticing that when no limits are put on the bounds of the resources, 
the alternating Turing machines accept exactly the same languages as the Turing 
machines: the recursively enumerable sets (see Exercise 1). 
In the same way as was done with Turing machines, alternating machines 
can be godelized, and we can define an enumeration of all the alternating Turing 
machines (see Volume 1 for the definition of godelization). 
As it was done with other models of machines, let us introduce the classes 
defined by bounding the time and space resources. Time bounded alternating 
machines are defined as follows. 
Definition 3.7 We say that the alternating machine M is time bounded by t, 
iffor any input x in L(1I1), with Ixl = n, there exists an accepting computation 
subtree of M on x with height at most t(n). 

66 
Alternation 
Let t be a time constructible function from IN to N. Determining whether an 
alternating machine M accepts x in time ten) is equivalent to constructing the 
computation tree of M on x, and pruning it at depth ten). Every non-accepting 
leaf is labeled 0, and the remaining nodes of the tree are labeled by the rules I, 
2 and 3 as before. 
Let us now define the space bound. For this case we must be careful that 
the space used for any configuration is not larger than the desired bound. The 
space used by a configuration of M is the maximum length of the contents of 
the work tapes of M in this configuration. 
Definition 3.8 We say an alternating machine M is space bounded by s, 
if for any input string x in L(M), with Ixl = n, there exists an accepting 
computation subtree of ],,1 on x such that the space used by each configuration 
in this accepting computation subtree is bounded above by sen). 
To determine whether an alternating machine M accepts an input string x 
within space bound s(lx j), we label with 0 all the nodes or leaves in the com-
putation tree, in which the corresponding configuration uses space larger than 
s(lxj), and label again the whole tree according to rules 1, 2 and 3. If the root 
is labeled 1, then it accepts, else it rejects. 
Definition 3.9 For any space boundfunction sen), and any time boundfunction 
ten), let ASPACE(s(n» denote the class of languages accepted by sen) space 
bounded alternating Turing machines; and denote by ATIME(t(n» the class of 
languages accepted by ten) time bounded alternating Turing machines. 
Another possible resource to bound on alternating machines is the number 
of alternations. 
Definition 3.10 Let M be an alternating Turing machine and let x be an input. 
We say ],,1 is A(n)-alternation bounded on x if any path of maximum length of 
any accepting computation tree of M on x alternates universal and existential 
configurations at most A(lxj) - 1 times. 
When the number of alternations is a constant, the machines receive the 
following particular names. 
Definition 3.11 For any k > 0, a Ek-machine (respectively ilk-machine) is a 
k-alternation bounded alternating machine ],,1 which starts with an existential 
(respectively universal) state. 
Notice that a E1-machine is exactly a nondeterministic Turing machine. By 
convention a Eo or a ilo machine is a deterministic Turing machine. 
For an example of the power of alternation, let us see how an alternating 
machine with one tape will recognize in linear time the language of all words 
which have a marked prefix equal to a marked suffix. It is well known that to 
solve this problem in linear time, with a deterministic machine, we need at least 
two tapes. 

Alternating Turing Machines 
67 
Theorem 3.1 For any finite alphabet E and marker # not in E, the language 
L = {x#y#z I x = z, where x, y, z E E·} 
is recognized by a one-tape alternating Turing machine in linear time. 
Proof Given the input w, with Iwl = n, proving that w E L is equivalent to 
testing the following three conditions: 
(a) 
w has the format x#y#z 
(b) x is a prefix of z 
(c) 
z is a prefix of x 
We construct an alternating one-tape machine M which tests (a), (b) and (c) 
using O(n) steps. 
To test (a), M deterministically scans the whole input, counting the number 
of # symbols. If it finds exactly two of these, M goes on to test conditions (b) 
and (c), otherwise it rejects. It takes O(n) steps to scan the input 
To test condition (b), M divides the tape in four tracks. Using O(n) steps, 
M copies the input on track 1. 
1. 
Universally, it chooses a tape square under x and prints a marker b on 
track 2 under the chosen square. 
2. 
For each of the resulting configurations, it chooses existentially a tape 
square under z and prints a marker c on track 2. 
Testing (b) is equivalent to testing that the following two conditions hold: 
(A) The symbol above b equals the symbol above c. 
(B) 
The distance dl from the left end of x to b equals the distance d2 
from the left end of z to c. 
It takes O(n) steps to test condition (A). In the remainder of the proof 
we shall show how to test condition (B) using only O(n) steps. Notice 
that the obvious way to do it would be to scan the tape back-and-forth, 
over-running the desired linear complexity, and that there is no other work 
tape where a counter could be held. 
To test (B): 
3. 
Existentially guess on track 3 nonoverlapping binary representations of 
natural numbers, ml, m2, ..• , mt> with t < n, and check that: 
(a) 
ml starts under the leftmost symbol of x and mt starts under the 
rightmost symbol of z, 
(b) Each mi is of length O(log n), and the distance between the leftmost 
symbols of consecutive mi and mi+l is O(n/ log n). 
Let [mil denote the number represented by mi. The idea is to use this 
sequence of representations as milestones, to test the situation of b and c. 

68 
Alternation 
4. 
Among all the sequences which agree with conditions (a) and (b) above, 
choose existentially the one for which the leftmost symbol of mi is on 
tape square [mil. 
To test that we made the appropriate selection, 
5. 
Branch universally on i E {2, 3, ... ,t}. Test deterministically if [m;1 -
[mi-tl equals the distance between the leftmost ends of mi and mi-l. 
This can be done in 0 COgn n - log n) steps. If the result of the test is 
negative, reject. 
6. 
Else, compute deterministically with the help of the milestones, the dis-
tance db and de. Again this can be done in 0 (J~ n - log n) steps. 
To test that (5) is correct: 
7. 
Existentially guess a representation among all the possible nonoverlapping 
binary representations nl, ... , nt of numbers such that for all i the first 
symbol of ni stands under the first symbol of mi; and such that for every 
t, ni = ni+I' 
To test that we have made the correct guess, 
8. 
Choose universally i E {2, 3, ... , t} and test deterministically that ni = 
ni-I. Reject if the test is negative. 
9. 
Compare deterministically db and de with some nearby ni. If they are 
equal accept; else reject. 
For an input of length n, any accepting subtree of the computation tree has 
depth O(n). 
0 
Notice that the previous theorem gives us a fast way of copying a string 
using a one-tape alternating machine: guess the copy and verify its correctness 
using the procedure described in the proof of Tneorem 3.1. 
It turns out that, contrary to what happens with Turing machines, for al-
ternating machines the increment in number of tapes does not bring a speed 
increment of the speed in recognizing languages. 
Let ATIME t (t(n» be the class of languages accepted by a t(n) time bounded 
alternating machine which has exactly one tape. The proof of the following result 
is given as a difficult exercise (Exercise 3). 
Theorem 3.2 For all t(n): ATIME(t(n» = ATIMEt(t(n» 
Most of the results that we presented in Chapter 2 of Volume I about bounded 
resources in Turing machines can also be proved for alternating Turing machines. 
In Exercise 5 we state a space hierarchy theorem, and in Exercise 6 we state a 
constant factor speed-up theorem. The following result states a time hierarchy 
theorem for alternating machines, similar to Theorem 2.11 of Volume I for 
Turing machines. This result will be used in the last section of this chapter. 
Theorem 3.3 Let tt be a time constructible junction, and let tl E W(t2)' Then 
ATIME(tl(n» contains a language which is not in ATIME(t2(n». 

Alternating Turing Machines 
69 
Proof We construct by diagonalization a set L in ATIME(tl(n» Which is not 
in ATIME(i2(n». For x in {O, 1}., let Mx denote a one-tape alternating Turing 
machine which has x as its GOdel number. Without loss of generality we shall 
give the proof for alternating machines on input alphabet {O, I}. 
input wand x 
let n be the length of w 
if w =I x#y for some y E {O, I}" then reject 
otherwise compute the binary representation of tl (n) 
using the time constructibility of tlo write it on tape 2 
comment: from now on this will be used as a counter, 
subtracting one at each step of the machine 
so that the machine will be clocked by tl steps 
simulate the computation of 1I1x on w 
interchanging the accepting and rejecting states 
as well as the existential and universal states 
if counter reaches 0, accept 
else reject 
end 
Figure 3.1 
Alternating machine M for the proof of Theorem 3.3 
Consider the two-tape alternating machine M given in Figure 3.1. Clearly 
the machine M accepts within time tl(n), therefore if L = L(M), then L belongs 
to the class ATIME(tI (n». 
Let us prove that L does not belong to ATIME(t2)' Assume on the contrary 
that L E ATIME(t2)' Then there will be an alternating machine Mx accepting 
every w in L within t2 steps. Moreover, as tl E W(t2) this would imply that M 
will have "enough time" to complete the simulation of lv[x on w, and therefore 
M will reject w, contradicting the way L has been defined. Therefore L rf. 
ATIME(t2). By the previous theorem, AI can be converted into a one-tape 
machine without changing the time bounds. 
0 
As a corollary to the proof of the theorem, we can state; 
Corollary 3.1 lft(n) is time constructible, then ATIME(t(n» is closed under 
complement. 
Proof Let M be the alternating machine of Figure 3.1, then for all x, w E 
L(Mx) if and only if (w, x) rt. L(M); thus a machine that on input w acts like 
M on input (w, x) accepts L(1I1x). 
0 

70 
Alternation 
3.3 Complexity Classes for Alternation 
In this section we study the complexity classes defined by alternating machines 
and establish the main relationships between alternating and deterministic com-
plexity classes. 
The following definition is the equivalent to Definition 3.1 in Volume I, for 
alternating Turing machines. 
Definition 3.12 Let us define the following complexity classes: 
ALOGSPACE = ASPACE(log n) 
APTIME = Uc>oATIME(nC) 
APSPACE = Uc>oASPACE(nC) 
AEXPTIME = Uc>oATIME(2n<) 
AEXPSPACE = Uc>oASPACE(2nC ) 
These classes are invariant under constant factors due to Exercises 5 and 6. 
Our first result will show how the power of alternation "jumps", alternating 
time to deterministic space modulo a polynomial. This statement will be proved 
in the next two theorems. 
Theorem 3.4 Let s be a space constructible function such that for all n E N 
we have sen) ;:::: n. Then: 
NSPACE(s(n» ~ ATlME(i(n» 
Proof Let L E NSPACE(s(n», then there exists a nondeterministic machine 
M accepting every x within space sen), which implies that there exists an 
accepting computation 10 r II r .,. r I, such that every configuration Ii can 
be written using as a maximum sClxi} space. Let k be the cardinality of the 
alphabet of A1; then the maximum number of different configurations is ks(n), 
which implies that the maximum length of the shortest accepting computation 
is ks(n). 
We define in Figure 3.2 an alternating machine MA which accepts an input 
x if and only if M accepts x. 
MA works as follows: it starts by marking off sen) blocks of length sen) 
in each one of the work tapes, which is done within time 0(s(n)2). Next, MA 
writes 10 on the first block of the first tape, and the guessed I, on the first block 
of the third tape. Then MA writes in the first block of the second tape, the k-ary 
expression c of p(n). From here on, MA calls the subroutine reachable which 
takes inputs of the form (II, c, 12), and checks whether II r* h in c or fewer 
steps, where II and h are configurations of A1 on x and c is a number in k-ary 
notation 0 S c S pen). The subroutine is the alternating version of the one used 
in Savitch's theorem (Theorem 2.9 of Volume I), and it is defined in Figure 3.3. 
(Also see Theorem 3.9 of Volume 1.) 

Complexity Classes for Alternation 
71 
input x 
let 10 be the initial configuration of M on x 
existentially guess a final accepting configuration I, 
c:= the k-ary expression of ks(n) 
if reachable (Io, c, I, ) 
end 
then halt accepting 
else halt rejecting 
end if 
Figure 3.2 Alternating machine MA for the proof of Theorem 3.4 
The successive recursive calls of the procedure reachable are stored in the 
cells marked off on tapes 1, 2 and 3 of MA • The entire computation requires 
log ks(n) = 0(8(n» recursive calls, and each call takes time 0(8(n», therefore 
the total complexity cost is 0(82(n». 
0 
Requirement 8(n) ~ n is necessary since classes AT/ME(t) have been defined 
for at least linear bounds t. Later we shall see how to get away with this linear 
lower bound. 
The next theorem indicates that every language recognized by an alternating 
Turing machine in time ten) can also be recognized by a deterministic Turing 
machine within the "roughly the same" bounds for the space. 
function reachable (II, c, h) returns boolean 
if c = 0 then 
if II = h or h is reached in one step from II 
then return true 
else return false 
else existentially guess a middle configuration 1m 
universally branch on the following two computations: 
(1) verify reachable(IJ, c/2, 1m) 
(2) verify reachable (Im, c/2, h) 
if both return true 
then return true 
else return false 
end 
Figure 3.3 The subroutine "reachable" 

72 
Alternation 
Theorem 3.5 Let t be a space constructible function such that for all n E N 
we have ten) ~ n. Then: 
ATIME(t(n» ~ DSPACE(t2(n» 
Proof Let L E AT I M E(t(n», then there exists an alternating machine MA 
which accepts every x E L, Ix I = n, within time ten). Therefore, the computation 
tree of AlA on x has depth less than or equal to ten), and total size ct(n) for 
some constant c. 
We construct a deterministic Turing machine M which is able to decide 
whether an input x is accepted by MA using only O(t2(n» space. The "brute 
force" approach, which needs to store the whole computation tree, will not work, 
as this tree is of size kt(n), and therefore does not fit into the space in which 
we wish to bound the computations of AIA • Instead we perform a depth first 
search of the tree, examining one path at a time. The machine Al is described 
in Figure 3.4. 
input x 
let 0:0 be the initial configuration of MA on x 
if label(o:o), then accept and halt 
else reject 
end if 
end 
Figure 3.4 Deterministic machine !If for Theorem 3.5 
M begins by writing 0:0 and ten) on one working tape: this can be done 
within space O(t(n», because each node of the computation tree can be repre-
sented in space less than or equal to ten), and ten) is space constructible. From 
here on, Al explores each path of the computation tree of AlA on x: AI calls 
recursively a subroutine "label" of each configuration of the computation path 
0:0' •• O:m, where O:m is a leaf of the computation tree. Al finds the sons of any 
given configuration using the transition function of MA • This is equivalent to 
testing in a postorder manner if that chain accepts the input x. 
At every instant of the simulation, the maximum space used by M is the 
maximum length of any computation chain, required to hold the path to the 
currently explored node, times the maximum size used by any configuration 0:, 
which is 0(t2(n». 
0 
Let us stress some remarks about the size of the computation trees in the 
nondeterministic Turing machine and in the alternating Turing machine. Let us 
consider a nondeterministic machine and an alternating machine, both operating 
in space sen) on inputs of size n and with alphabet of cardinality k. As we have 

function label(a) returns boolean 
if a is a leaf then 
if a is accepting 
then return true 
else return false 
if a is existential then 
b := false 
Complexity Classes for Alternation 
73 
while b = false and there are sons of a not explored yet 
choose a new son ai 
b = label(ai) 
erase ai 
end while 
if b = true then return true, else return false 
else 
comment: a is universal 
b = true 
end 
while b = true and there are sons of a not yet explored 
choose a new son ai 
b = label(ai) 
erase ai 
end while 
if b = true return true, else return false 
Figure 3.5 The subroutine "label" 
seen in the proof of the previous theorem, each machine has at most ks(n) distinct 
configurations, so their computation trees have height at most ks(n). Moreover 
an accepting computation of the nondeterministic machine can be described as a 
path from the root to a leaf, of length ks(n), while a computation of the alternating 
machine can be represented as a subtree in which each universal node may have 
as sons c configurations, where c is a constant. Such a subtree may involve as 
many as ck•Cft) nodes, but since only ks(n) different configurations exist, many of 
the nodes will be repeated, and therefore a large part of the computation tree 
will be redundant. 
The following theorem takes advantage of this redundancy of the compu-
tation tree of an alternating machine, to prove that an s(n)-alternating space 
bounded Turing machine is equivalent to a cs(n) deterministic space bounded 
Turing machine. For the sake of clarity in the exposition, to prove one of the 
inclusions, we use the single tape Turing machine model, which as the reader 
knows, is polynomially equivalent to the multitape model. 

74 
AJ~ation 
Theorem 3.6 Let sen) ~ log n be space constructible. Then: 
ASPACE(s(n» = U DTIME(c·(n» 
c>o 
Proof. Let us first prove that for any sen) ~ log n and any constant c > 0, 
then 
DTIME(s(n» ~ ASPACE(c ·logs(n» 
Let L E DTIME(s(n», then there exists a deterministic machine M which 
accepts L within time sen). By the remarks made in Section 2.6 of Volume I, 
there exists a single-tape deterministic machine MI which accepts L within time 
s'(n) = O(s2(n». Let Q and E be respectively the finite set of states and the 
finite alphabet of AI .. and let {qf} = F. 
Recall from Chapter I of Volume I, that for each x E L(MI), there exists 
a sequence of configurations Do, ... , Dt with t ::; s', such that Do is the initial 
configuration and D t is a final configuration. Without loss of generality we also 
assume that :All has a unique accepting state q/t and that at the right end of an 
input word, the tape is filled with an infinite number of blank symbols. Let us 
assume that the tape of MI is semi-infinite and there are no blanks at the left 
of the words. Also assume that position I contains a special endmarker symbol 
which the machine is not allowed to modify. 
As ~MI has only one tape, we can consider that the configurations are chains 
uqv, with u and v non-empty, and an infinite number of blank symbols to the 
right of v, where u, v E E*, q E Q and the symbol currently explored is the 
symbol to the right of q. The reader can observe that this notation is equivalent 
to the notation given in Chapter 1 of Volume I for multitape Turing machines, 
where q is used in place of #. The initial configuration Do is qox, where qo is on 
the first square of the semi-infinite tape of MI. Let wlaqbw2 be a configuration 
D i, with WI, W2 strings in E*, a, b symbols in E and q an element of Q. Let q 
be in the ph position, counting from the left, of the configuration Di. According 
to the transition function of All, the head of All can move one step towards the 
right, or one step to the left (if j =/1), or remain in the same position. In the 
first case, the configuration Di+1 will be wlab'q'w2 with symbol b' in the ph 
position; when All moves to the left, configuration Di+1 will be wlq'ab'w2 with 
a in the ph position; and finally if All does not move, configuration Di+1 will 
be wlaq'b'w2, and q' will be in the ph position. 
Therefore, the ph symbol of Di is completely determined by the (j -
l)th, 
the ph and the (j + l)th symbols of Di-I. This fact can be formalized as follows. 
Let Ll = QUE, and let ,i,j denote the ph symbol of configuration Di; then we 
define a function StepMl : Ll-+ Ll, depending on MI (but not on x), s!Jch that 
,iJ = StepM1hi-l,j-1, ,i-I,;' ,i-IJ+I) 
Moreover, All accepts input x if there is i and j with 1 ::; , ::; s'(n) and 
1 < j ::; s'(n) such that ,i,j = qf. 

input x 
existentially guess i, j 
call check (i,j,qj) 
end 
Complexity Classes for Alternation 
75 
Figure 3.6 Alternating machine for Theorem 3.6 
In Figure 3.6, we present an alternating machine MA which decides whether 
1111 accepts an input x by calling the recursive subroutine of Figure 3.7. MA 
accepts if 'i'i,; = qj' for some 1 ::; i ::; s'(n) and 1 < j ::; s'(n). MA starts 
by guessing, using an existential state, values i and j and checks that 'i'i,j = qj 
by working backwards through the computation of MI. At each stage of the 
checking procedure, MA has integers i, j and symbol a E Ll. To check whether 
'i'i,j = a is easy if j = 1 or i = O. Otherwise, to check that 'i'i,j = Z, 1I1A guesses, 
using existential states, three symbols a_I, ao, al in Ll and using the transition 
relation of 1111 checks whether a = StepM\ (a_I, ao, ad. If different, MA rejects; 
otherwise using a universal state, M A branches on the three values -1 ::; k ::; 1 
for integer k, and recursively calls itself to check whether 'i'i-I,;+k = ak. 
function check (i, j, a) 
if j = 1, then if in position 1 there is a 
then accept 
else reject 
if i = 0, and the jth symbol of qox is equal to a 
then accept 
else reject 
otherwise, existentially guess a_I, ao, al in Ll 
compute a' = StepM\ (a_I, ao, al) 
end 
if a =I a' then reject 
else universally for every k, (-1 ::; k ::; 1) 
call check (i - 1,j + k, ak) 
Figure 3.7 Subroutine "check" 
The space required for this procedure is dominated by the space required to 
write integers i and j. These integers are O(s'(n» = O(s2(n». Therefore, for 
each c > 0 there is an integer b such that space c· log(s2(n) suffices when the 
integers are written in b-ary notation. 

76 
Alternation 
Let us prove now that if s(n) ~ log n, then 
ASPACE(s(n» ~ U DTIME (cs(n») 
c>o 
If L E ASPACE(s(n», then for any x E L, with Ixl = n, there exists an 
alternating machine 1I1A accepting x using as maximum space s(n). Let T be 
the computation tree of 1I1A on x. By the remarks made after Theorem 3.5, 
T may have size dd·<n) and height ds(n) , for some constant d. To simulate the 
computation of 1I1A on x by a detenninistic machine within time cs(n), any kind 
of search will not work, since the time required to visit all the nodes would be 
o (dd.<ft»). However, as we already mentioned, the computation tree is highly 
redundant One way to get around this will be to construct a directed acyclic 
graph G, whose nodes are the ds(n) possible configurations and tWo nodes have 
an edge between them if the second node may be reached from the first in one 
step of the alternating machine. Each of the nodes will represent a configuration 
as in the computation tree. 
We can design a deterministic machine A1 which will construct such a graph, 
and afterwards label its nodes. The deterministic machine must decide if the 
initial node of the graph, which represents the initial configuration of MA on 
x, is labeled 1. 111 first constructs the initial configuration 0:0 of M A • Then 
it constructs all direct successors of 0:0, then all the direct successors of the 
direct successors of 0:0, and so on. This construction takes time o (ds(n) . ds(n» = 
O(cs(n». After the graph has been constructed, M labels the nodes of the 
constructed graph with {O, I} as follows: 
1. If the fan-out of a node is 0, then label it with 1 or ° according to whether 
the state of the configuration is accepting or rejecting. 
2. 
In other case, if all the direct successors of a node have already been 
labeled, then label this node according to the type of the state (existential 
or universal) of the node and the labels on its successors. 
The deterministic machine keeps on doing the above work until no more 
nodes can be labeled, accepting or rejecting the input acccording to whether 
the node corresponding to 0:0 is labeled with 1. The total time used by M is 
O(cs(n». 
0 
As an immediate corollary to Theorems 3.4, 3.5, and 3.6, we can state: 
Corollary 3.2 
EXPSPACE = AEXPTIME 
EXPTIME = APSPACE 
PSPACE = APT/ME 
P=AWGSPACE 

Complexity Classes for Alternation 
77 
The bounds in the statements of Theorems 3.4 and 3.5 are restricted to 
sen) ~ n and ten) ~ n, as we need time (Iwl> to read input w. By using a 
slightly modified model of alternating Turing machine, these restrictions can be 
relaxed to sen) ~ log nand ten) ~ log n. Let us introduce this new model. 
Definition 3.13 An indexing alternating Turing machine is an alternating ma-
chine which has a distinguished tape called the index tape and which does not 
read the input x directly, instead it writes down a number i (i :5 Ix I> in bi-
nary, and then enters a state requiring the ith input symbol to be visible to the 
alternating machine. 
Notice that this mechanism allows us to read any position, as it is needed, 
using log n steps. With the indexing alternating machine, we can define the 
following new alternating class; 
Definition 3.14 AWGTIME = ATIME(log n) 
To fully grasp the use of indexing machines, let us see how to recognize 
palindromes in alternating time log n. 
Lemma 3.1 {wwrlw E {a, 1}.} E AWGTIME 
Proof. Given an indexing machine, the procedure to recognize the above lan-
guage within log n time is the following: 
1. 
Guess n and verify that it is the length of the input word, by checking 
that the nth position contains some symbol but the (n + l)th does not. 
2. Using universal states, branch on all i between 1 and n. For each i, check 
that the ith and (n - i + l)th positions contain the same symbol. 
The cost of the whole process is writing nand i in binary, and computing 
n - i + 1. The remainder of the computation is a tree of height 2. By the remarks 
made at the beginning of the chapter about the need to have bounded fan-out 
trees, as the fan-out of the above computation tree is n, the conversion to a tree 
with fan-out 2 will increase the height of the tree to log(n). 
0 
Another example of a set in ALOGTIME is given in Exercise 12. 
It is should not be difficult for the reader to check that the proofs of Theorems 
3.4 and 3.5 still work for s(n.) ~ log n and t(n.) ~ log n if we use an indexing 
alternating machine. In this case, the input x need not appear explicitly in the 
configurations: the input appears only on the input tape and its symbols are 
accessed as needed as in the previous example. However, this does not imply 
that Corollary 3.2 can be extended with the addititonal equality AWGTIME = 
WGSPACE, due to the quadratic overheads. A more involved construction than 
that of Theorem 3.5 allows one to prove that ALOGTIME ~ WGSPACE. The 
converse inclusion is currently an open problem. 

78 
Alternation 
Up to now, we have defined the complexity classes for alternating machines 
as classes of sets or languages. We wish to extend these classes to include 
boolean functions. In order to do this, it is sufficient to associate with each 
boolean function f : {a, l}n -+ {a, l}m, the language 
AI = {(Xl' .. Xn, 01 the i th symbol of f(XI ••• xn) is I} 
Using this concept we can define the classes of functions computed by alter-
nating machines. 
Definition 3.15 A function f is in the alternating complexity class C if and 
only if its associate language AI is in C. 
These classes will be related to other concepts in the next chapter. In Exercise 
11, we give two examples of functions in the class AWGTlME. 
3.4 Computation Graphs of a Deterministic Turing Machine 
We introduce now the concept of the computation graph associated with the 
computation of a deterministic Turing machine. Some properties of these com-
putations graphs will play an important role in the next section, when proving 
that DUN INUN. 
We use the canonical notation G(V, A) for directed acyclic graphs (dags) 
where V denotes the set of vertices and A denotes the set of arcs. The type 
of dags we will be dealing with are multigraphs, i.e. they allow several edges 
between the same two nodes. Therefore, we should speak about the arcs as a 
multiset instead of a set. Keeping in mind this fact, we shall refer to them as 
dags. 
Given two dags GI (VI, AI) and G2(V2, A2), we define as the union of these 
graphs, a new dag G(V, A) where V = VI U V2, and for any two given vertices 
X, y in V, (x, y) belongs to A, if (x, y) belongs to Al or (x, y) belongs to A2. 
For any given dag G(V, A) and any set We V, we denote by G - W, the 
dag which results after taking from V all the elements of Wand taking from A 
all the arcs which begin or end at an element of W. 
Let {I, 2, ... ,n} be an ordered set of vertices, and denote by H(n) the 
class of acyclic directed graphs on vertices {I, 2, ... , n}, in which the edges are 
defined in such a way that if (i,j) is an arc, then i < j. We say that edges (i,j) 
and (i',i') cross if i < i' < j < j'. 
Let HI(n) denote the subclass of H(n) comprising those graphs in which 
every vertex has at most one immediate predecessor and at most one immediate 
successor, and in which no two edges cross, and denote by Hr(n) the subclass 
of H(n) comprising those graphs that can be expressed as the union of r graphs 
in HI (n). In Figure 3.8, we can see an example of an HI dag and of an H2 dag. 
Notice that a graph in Hr(n) cannot have more than rn edges. 

Computation Graphs of a Deterministic Turing Machine 
79 
( 
• 
'\ 
u 
II 
II 
(a) 
( " 
'\ 
) 
(b) 
Figure 3.8 (a) Graph in HI(6); (b) Graph in H2(4) 
Our goal will be to represent computations of Turing machines by the above 
kinds of graphs. In this way we will simplify some of the counting involved 
in the proof of the next section. To do so, we will introduce a variant of 
deterministic machine model. 
Let :Al be a deterministic machine, and let x be an input of length n. We 
partition the computation of AI on x into "time segments" of b(n) steps, and 
we partition the tapes of Minto a(n) blocks comprising b(n) contiguous cells, 
where a(n) and b(n) are suitable functions related to t(n). Consider the following 
definition: 
Definition 3.16 A deterministic machine M with k tapes is block-respecting 
if, during each time segment of its computation, each head of M visits cells in 
one (and only one) block. 
In other words, :Al is block-respecting if and only if each tape head only 
crosses boundaries between blocks at times which are multiples of b(n). The 
following result will be used later on: 
Proposition 3.1 If t is time constructible, then every set L accepted in time 
t(n) by a deterministic Turing machine with k tapes is also accepted in time 
O(t(n» by a block-respecting deterministic Turing machine with k + 1 tapes.' 
Proof. Let L be a language accepted by a machine M as described in the 
statement. Let x be an input of length n, and let b(n) be such that 1 :s; b(n) :s; 
t(n)/2 and b(n) can be space-constructed (Le. a block of b(n) cells can be laid 
off) in time t(n). 
We construct the following deterministic block-respecting machine M' which 
accepts x within time t(n). 
:Al' uses one of its tapes as a "clock", to indicate the time segments b(n); call 
it the "clock tape". To do this, we mark on this tape a block of size b(n) using 
the constructibility of b(n), and then during the computation the head moves 
back and forth over the block, indicating the times which are multiples of b(n) 
by reaching one end of the block. 

80 
Alternation 
M' must be constructed in such a way that it is block-respecting, and runs 
slower by at most a constant factor c. For this, using the clock, we mark the 
block boundaries on the tapes before starting the simulation. This can be done 
in time O(t(n». 
Divide each of the tapes of M' into 3 tracks. Let us number the blocks 
on each tape by I, 2, 3, ... Block j will contain in the middle track the actual 
simulation of the tape of M; moreover, when the tape head is on block j, it 
contains in the upper track the reversal of the contents of block j - 1, and in 
the lower track the reversal of the contents of block j + 1. See Figure 3.10. 
xr 
~r 
vr 
X 
W 
V 
U 
Iw r 
v r 
u r 
Figure 3.9 Part of the computation of a deterministic machine and its simulation by a 3-track 
block-respecting machine 
When the head of a tape arrives at a boundary at the end of a time segment, 
to enter block j, it first reads in a block-respecting manner each of the two 
blocks j - 1 and j + 1, then temporarily stores their contents in the clock tape 
(where they fit exactly) and copies them reversed into the upper and lower track, 
respectively. Then, if the head attempts to leave block j in the middle of a time 

Computation Graphs of a Deterministic Thring Machine 
81 
segment, it can continue the simulation without crossing, using the information 
in one of the upper or lower tracks. This allows it to simulate at least ben) + 1 
steps of the machine. 
When the tape head of the machine attempts to leave the region kept in the 
upper or lower track and tries to move further, the simulation stops for at most 
ben) - 1 steps (without halting the clock), awaiting the end of the time segment. 
Then it updates in a block-respecting manner the contents of the adjacent blocks 
with the information in the upper or lower track. For this, it may use again the 
empty space on the clock tape to copy the contents. Then it initializes all tracks 
of the block which the head attempts to enter, and the simulation continues for 
at least ben) + 1 steps again. 
In this way, at least ben) + 1 steps are simulated by at most 14· ben) steps. 
This introduces a constant factor of slowdown. In Figure 3.9 we can see the 
moves of a tape head of M, and the corresponding moves of the head of M'. In 
this case ben) = 5, and the time segments are indicated by short lines crossing 
the movement line. Each black dot represents a position of the head, and in (b) 
the black squares indicate that at that moment the head is scanning the lower 
track. 
0 
Let M be a k-tape Turing machine t(n)-bounded and block-respecting with 
block-size b(n), where n is the length of the input x. As indicated in the previous 
theorem, the computation of M on x can be partitioned into a(n) = r~l time-
segments, each one comprising ben) consecutive steps. 
Definition 3.17 The computation graph associated with the computation of a 
block-respecting machine 111 on x is a dag G(V, A) where V is the ordered set 
{ 1, 2, ... , a(n)}, and nodes i and j, with i < j, are connected by an edge if and 
only if j = i + 1, or for some tape of 111 a block is scanned at time-segment i 
and j, but riot at any time-segments in between. 
In Figure 3.10, we can see an example of the computation graph of a block-
respecting machine during ten time-segments of one of its computations. No-
tice that in general, the computation graph of any block-respecting, single-tape 
Turing machine on input of size n can be decomposed into the following: 
• 
A dag with vertices {I, 2, ... ,a(n)} and edges (i, i+l) for all 0 < i < a(n). 
This dag belongs to H1(a(n» and it represents the sequence of time-
segments where the head is allowed to change blocks. 
• 
A dag with vertices {I, 2, ... , a(n)} which represents the movements of the 
head on the tape. Every node in this dag can have at most one incoming 
edge from the last time-segment when the head was on the same block 
(no arc in case it is the first time the head visits that block), and every 
node has at most one outgoing edge which points to the node representing 
the next time-segment when the head will be on the same block (if the 

82 
Alternation 
z 
i + 10 
• 
... 
.. 
... 
•• 
... 
.. 
.. 
II 
... 
.. 
( 
( 
'\ l 
z 
[ 
"\ 
i + 10 
• 
• 
• 
• 
• 
• 
• 
~ " 
'-
) 
} 
)+10 
Figure 3.10 Movements of the head on a one-tape machine, with the resulting graph and the 
decomposition in three HI graphs 
block is visited again). To avoid the crossing of edges, this dag can be 
expressed as the union of two dags in HI(a(n». 
Therefore the resulting graph is the union of three dags in HI (a(n», which 
is a dag in H3(a(n». 
In general, when considering the case of k-tape block-respecting machines, it 
is a straightforward graph-theoretical exercise to prove the following Proposition 
(see Exercise 24): 
Proposition 3.2 Let M be a k-tape block-respecting deterministic machine, 
which on input x, Ixl = n, stops at time ten). Then the computation graph 
associated with the computation of 111 on x belongs to H2k+l(a(n», where a(n) 
and ben) are as given above. 

Computation Graphs of a Detenninistic Thring Machine 
83 
In Figure 3.11, we can see an example of a part of the computation graph 
associated with a computation of a three-tape block-respecting machine. The 
lines under the tapes indicate the movement of the heads at each step. 
, 
S2~ 
~: :: :::, 
1 
'--
, 
, 
9 
10 
12 
, 
%2 ,2
5 
1 
12 
10P11 
"-
3 
S' 
7 
9 
1, 
2 
3 
:6 
:5 
)4 
7C 
\.--
, 
;J::L)11 
8 
9 
12 
12 
Figure 3.11 Computation graph associated with the computation of a three-tape machine 
As we shall see, we will be interested in obtaining certain estimates of the 
number of predecessors of vertices in graphs Hk(a(n)). For this we state the 
following definitions. 
Definition 3.18 For any two nodes i and j in G, we say that node i is a 
predecessor of node j, if there is a path from i to j in G. 

84 
Alternation 
Definition 3.19 For any dag G(V, A) and any integer m, a set of nodes J c 
V is called an m-segregator for G if every node in G - J has at most m 
predecessors in G - J. 
In Figure 3.12 we can see an example of a 2-segregator in a dag. 
Notice that by removing an m-segregator from a dag, all the remaining nodes 
of the dag have at most m predecessors. 
Figure 3.12 A 2-segregator of size 3 (enclosed by circles) 
The following technical result will be used in the proof of Theorem 3.7 Let 
us recall from Chapter 2 of Volume I, that by definition, log· n denotes the 
number of times we have to raise 2, to get a number greater than or equal to n. 
The following notation will be used in the next lemma. For any two integers d; 
and dj , we write d;!dj if dj is a multiple of d;. 
Lemma 3.2 For any integer n ~ 256, there exist integers do, d!, ... ,db with 
k = r1og;nl, such that: 
1. 
1 = do I d1 I d2 I ... I dk :::; 2n 
2. 
Vi > 0, kni+l :::; nd k, where n; = r n/ d; 1 
(Condition 2 restricts the rate of growth of the sequence do, d!, ... ,db in 
the sense that n;+1 :::; logk n; - 1.) 
Proof We use the following notation: for k ~ 2 and or 1 :::; p :::; k - 1 define: 
e(k,O) = 1 and e(k,p + 1) = k2+1.e(k,p) 
From the definition and using induction it is easily proved that 
1 = e(k, 0) :::; ... :::; e(k, k - 1) :::; n 
Define do = 1; and for 1 :::; i :::; k define d; = 2!1og(n/e(k,k-i))1. This sequence 
agrees with condition 1 in the statement of the theorem. 
To verify condition 2, we show that Mn/di+ll :::; r n/ d;l / k. 
By the definition of d;, 
n/d;+1 = n/(2!1og(n/e(k,k-i-1»l) 
(3.1) 

Computation Graphs of a Deterministic Thring Machine 
85 
but 
2fIog(n/e(k,k-i-1»l ~ iog(n/e(k,k-i-l» = n/e(k, k - i-I) 
from (3.1), rn/di+1l ~ e(k, k - i-I), therefore 
(3.2) 
multiplying and dividing the right hand side of (3.2) by kZ, and using the defi-
niti<1n of e(k, k - i) we get: 
kZ+2e(k,k-i-l) 
krn/d,+ll < ---::--=--
-
k2 
but as e(k, k - i) ~ !7, the result follows. 
e(k, k - i) 
k2 
(3.3) 
o 
We use this lemma to prove the following result, which will be an important 
step in the proof of DUN =I NUN. 
Theorem 3.7 Let G be in Hr(n) and let n ~ 256. Then G has a 6n/ log* n-
segregator of size less than or equal to 15rn/ log* n. 
Proof We must prove that if a graph G(V, A) agrees with the statement of the 
theorem, then there must exist a J C V of size IJI = 15rn/ log* n, and such 
that every vertex in G - J has at most 671/ log* n predecessors in G - J. 
Let G E Hr(n), let k = pas;'! 1 and let do, dt, ... , dk be integers as in the 
statement of the previous lemma. Construct the following sequence of partitions 
on the vertices of G: For any 0 ~ j ~ k, Pj is constructed by taking blocks of 
exactly dj consecutive vertices of G (except the last block which may contain 
less than dj vertices). Clearly Po is the discrete partition and Pj +1 is coarser than 
Pj • Associate to each edge (i, j) of G the coarsest of the partitions Po" .. , Pk- 1 
such that i and j appear in different blocks. In Figure 3.13, we can see an 
example of the partitions for H3(9), with k = 3, do = 1, d1 = 2 and dz = 4. 
As G has at most rn edges, there must be a partition Pq with 0 ~ q ~ k - 1, 
that has at most rn/k edges associated with it. Fix this partition Pq• 
Let A. be the set of vertices i in G such that some edge (i,j) is associated with 
Pq• (In the example of Figure 3.13, assume that the Pq of the text corresponds 
to Pt, then the set A associated with Pt is {1,2,6}.) Then IAI ~ rn/k ~ 
3rn/ log* n. 
Construct a graph G* from the graph G - A by collapsing each block X of 
Pq into a single node X* and setting an arc from X* to y* whenever G - A 
contains an arc from a vertex in block X to a vertex in block Y. The number 
of vertices in G* will be the number of blocks of size dq that can be fonned 
with the n vertices of G; therefore we get IG*I = rn/dql = nq. In Figure 3.14 
we can see the graphs G - A and G* for the example of Figure 3.13. 
For any vertex X* in G* let D(X*) denote the number of immediate prede-
cessors of X* in G*, that is, the set of all vertices Y* in G* such that (Y*, X*) 

86 
Alternation 
2 
2 
Figure 3.13 Partition {Pj} for H3(9) with k = 3, d{) = 1, dl = 2, dz = 4 
is an edge of G". We make use of the following claim which will be proved 
later: 
Claim: 
E D(X") ~ 2rnq• 
X'EG' 
Let us say that a node X" in G" is bad if D(X") > k, and let B" denote 
the set of bad nodes in G". Then 
IB"I < EX'EG' D(X") < 2rnq 
-
k 
-
k 
Let B denote the set of vertices in G which belong to some block X of Pq , 
and for which the node X" belongs to B". Since each block of Pq contains 
at most dq vertices, we have IBI ~ 2rnqdq/k, and since dqnq ~ 2n, we get 
IBI ~ 4rn/k ~ 12rn/log"n. 
L :. 
~ 
... 
... 
3 
4 
5 
7 
8 
9 
• 
.. 
.. 
Xi 
Xi 
X; 
X: 
X; 
Figure 3.14 The graph G - A and the graph G* 

Computation Graphs of a Detenninistic Turing Machine 
87 
Let the segregator be J = A U B. Then as A and B are disjoint, 
A 
B 
3rn 
12rn 
ISrn 
IJI-I 1+1 1<--+-----
-
- log* n 
log* n - log* n' 
Therefore J has the desired size. 
Next we must show that every vertex in G - J has 6n/ log* n predecessors 
in G - J. 
Consider any arc in G* arising from an arc (i,j) of G. Then i and j must 
be in different blocks of Pq; otherwise both would belong to the same vertex of 
G*. Furthermore i and j must be in different blocks of Pq+t ; otherwise i E A, 
and hence (i,j) would be in G - A, which would imply that (i,j) gives rise to 
no edge in G*. Since there are just nq+t = rn/dq+tl blocks in Pq+t, the longest 
directed path in G* has length at most nq+t - 1. Moreover, since every node of 
G* - B* has at most k immediate predecessors, every node in G* - B* has at 
most kn ,+1 predecessors in G* - B*, but by the previous lemma kn,+1 :5 nq/k. 
So every node in G* - B* has nq/k or less predecessors in G* - B*. 
Fix a vertex in G - J; let us say it is in block Y. If it has a predecessor 
in G - J, say in block X, then X* will be a predecessor of y* in G* - B*. 
Therefore, each vertex in G - J has the predecessors in G - J distributed over 
at most nq / k blocks of Pq. Since each block of Pq contains at most dq vertices, 
the number of predecessors in G - J of a vertex in "G - J is bounded above by 
dq nq / k :5 2n / k :5 6n / log* n. Therefore, J is a 6n / log* n-segregator. 
To complete the proof, we just need to prove the claim. 
For this, notice that by definition G = U'.t Gio with each of the Gi in Ht(n). 
For 1 :5 i :5 r, we may construct the dag G: from Gi in the same way as we 
constructed G*. It is easy to check that G* = Ui.t G:. 
For each X* in G* let Di(X*) denote the number of immediate predecessors 
of X* in G:. Then 
(3.4) 
So we need to evaluate EXoEG! Di(X*). 
To do this we prove that distinct nodes of Gi have disjoint sets of immediate 
predecessors except maybe in their first immediate predecessor; for if X* is an 
immediate predecessor both of y* and of Z* in Gi with y* < Z*, then X* is the 
lowest numbered immediate predecessor of Y*, otherwise if there were another 
immediate predecessor W* of y* in G: with W* < X* then the arcs (W*, Y*) 
and (X*, Z*) would cross in G:, i.e. G: ¢ HI (nq). But this would imply that 
the edges of Gi which give rise to these arcs would also cross, contradicting the 
fact that Gi belongs to HI(n). This proves our statement 
As the possible number of vertices in G: is n q, then 
E (Di(X*) - 1) :5 2nq 
XOEG, 

88 
Alternation 
which together with (3.4) implies that 
E D(X·) ~ 2rnq • 
X"EG" 
o 
3.5 Determinism Versus Nondeterminism for Linear Time 
Define DUN to be the class Uc>oDTIME(c.n) and define NUN to be the class 
Uc>oNTIME(cn). In this section we shall show that nondeterministic linear time 
is more powerful than deterministic linear time, i.e., DUN C NUN. We begin 
by outlining the strategy: If NUN = DUN, then deterministic linear time equals 
alternating linear time (Lemma 3.3), moreover by a padding argument, this ex-
tends to non-linear time (Lemma 3.4). The crux of the proof is a simulation 
whereby any language recognized by a deterministic machine in non-linear time 
is recognized faster by an alternating machine (Theorem 3.9). A diagonaliza-
tion, namely the use of the hierarchy theorem for alternating Turing machines 
(Theorem 3.3) completes the proof. 
Let us start by proving the key result. For this, we shall use the tools devel-
oped in the previous section. We take advantage of the parallelism introduced 
by the universal states to speed up the computation of a deterministic Turing 
machine, and prove that alternating time with four alternations is more powerful 
than deterministic time. The idea of the proof is to simulate the computation 
of a deterministic machine working in linear time by an alternating machine 
in sublinear time. To do this simulation, the alternating machine guesses the 
computation of the deterministic machine; but to prove that the nondeterministic 
guess works correctly, it would take too long to test deterministically that the 
computation at every node follows naturally from every predecessor. However, 
as we know that there exists a segregator of a reasonable size, we just need to 
test in parallel the correctness of the computation for each node of the segregator 
and its predecessors (which by definition are not "too many"). First a definition. 
Definition 3.20 For any time bounded function ten), let Ek-T1ME(t(n» denote 
the class of languages accepted by a Ek-machine within time ten). 
Theorem 3.8 For any time-constructible function ten), with ten) ~ n log· n, 
we have that DTIME(t(n» ~ E4-TIME(t(n)/ log· n) 
Proof Let A be a language in DTIME(t(n», then there exists a deterministic 
Turing machine M which accepts A within time t(n~. Without loss of generality, 
assume that M has k tapes. Let ben) = rt(n)2/31 and a(n) = [t(n)I/31; by 
Proposition 3.2, there exists a deterministic block-respecting machine J.,1b, with 
k + 1 tapes, and with the blocks of length ben), which recognizes every x in A 
within time ten). Let us construct the following E4-machine M A : 

Determinism Versus Nondeterminism for Linear Time 
89 
Phase 0 Detenninistically on input x of length n, it computes ten), a(n), ben), 
and e(n) = r15(2k + 3)a(n)/ log- a(n)l. This can be done in time ten), as 
it is ten) time-constructible. 
. 
Phase 1 (existential) 
1. 
Existentially the machine guesses in which block 
are the heads of each tape at the end of each time-segment. This 
step has a time cost of O(a(n) log a(n». 
2. 
From the information previously guessed, it computes detenninisti-
cally the graph G associated with the block-respecting computation 
of M" on x. By Proposition 3.3, G E H2k+3(a(n». Notice that the 
nodes of G represent time-segments of length ben) in the computation 
of M" on x. Therefore, by Theorem 3.8, G has a 6a(n)/log-a(n)-
segregator J of size e(n). The computation graph will have a(n) 
nodes and at most (k + 2)a(n) arcs, therefore its construction takes 
time O(a(n) log a(n». 
3. Existentially it guesses a subset J of vertices of G, of size e(n).(This 
will be the segregator of G.) To do this, MA must explore all the 
a(n) nodes of G, selecting non-detenninistically e(n) among them. 
Therefore the cost of this step is O(a(n». 
4. For each node in the segregator J, and for the last node of G, say 
f, existentially it guesses the computation of Mb on x. Here we 
mean that MA existentially guesses the initial configuration at the 
beginning of the time-segment under consideration, guesses the next 
configuration and detenninistically checks for correctness with the 
help of the transition function of M". It does the same with the 
ben) - 1 configurations corresponding to the remaining steps inside 
the time-segment. Finally, it saves the initial and final configurations 
corresponding to the entrance and exit from the block under con-
sideration. The cost of guessing the computation for each node is 
O(b(n», as the maximal size of J is e(n). The total cost of this step 
is O(b(n) . e(n» = 0 (tJ(n)' (ti(n)/log- ti(n») which is equal to 
O(t(n)/ log- t(n». 
5. 
Detenninistically it checks that the computation for the last segment 
includes an accepting state. If this check fails, reject. Otherwise 
proceed to phase 2. This step has a constant cost. 
Therefore, the total cost of phase 1 is 
o (a(n) . log a(n) + (t(n)/ log- t(n») 
which is O(t(n)/ log- t(n» 
Phase 2 (universal) Let J = J U{f}. MA universally selects a vertex j in J. 
Let Ij denote the set of predecessors of j in G - J. Detenninistically 
it computes the set I j with a cost O(a(n) . log a(n». Notice that the set 
of predecessors of j in G - J cannot exceed e(n); otherwise, as they are 
connected to each other, one of them will have e(n) predecessors in G - J, 

90 
Alternation 
which contradicts the definition of a segregator. Therefore, if 11;1 > e(n), 
MA rejects, otherwise it proceeds to phase 3. As the universal selection 
has cost log e(n) (binary tree with e(n) + 1 internal nodes), this phase has 
cost O(a(n) . log a(n» 
Phase 3 (existential) 
MA existentially guesses the computation of Mb for each 
of the time-segments corresponding to vertices in I j , as was done in step 
4 of phase 1. The cost of this step is O(t(n)/ log- ten»~. 
Phase 4 (universal) 
MA universally selects a vertex i in Ij U j. It determin-
istically checks that the guessed initial configuration of time-segment i is 
consistent with the guessed final configuration of its immediate predeces-
sors of i in G, and with the transition function for Mb. If any of these 
checks fails, it rejects; otherwise MA accepts x. 
The running time of MA on x is O(t(n)/ log· t(n». 
o 
We need two more technical results to get our desired result. 
Lemma 3.3 If DUN = NUN, then for every k, DUN = Ek-TIME(n) 
Proof. 
From the definitions, it follows that DUN ~ Ek-TIME(n). 
To prove the other inclusion, we use induction on the number k of alterna-
tions. 
For k = 1, an alternating machine with 0 alternations and starting in a 
nondeterministic mode is just a nondeterministic machine. By the hypothesis of 
the lemma, the result follows. 
Let L be a language accepted by a alternating machine MA which starts in 
existential state and does k - 1 alternations. Modify the states of MA in such a 
way that MA counts the number of its alternations; this information can be kept 
in the finite control of the machine. 
Next modify MA in such a way that after making the first alternation, it 
enters a fixed state qe. Then the language accepted by MA started in state qe is 
in Ek_1-TIME(n) and by the induction hypothesis it is accepted in time O(n) by 
a deterministic machine M. Modify MA again by identifying qe with the initial 
state of M. Then }'1 makes 0 alternations and the theorem follows by the case 
k = 1. 
0 
The following result can be easily obtained from this last theorem, by a 
padding argument. 
Lemma 3.4 If DUN = NUN, thenfor every time-constructible ten) and every 
k, Ek-T1ME(t(n» = DTIME(t(n» 
Proof. 
Let A be recognized by a Ek-machine within time ten). Construct the 
following set A' = {x#t(lxi>lx E A} where # is a new symbol not in the alphabet 
of A. 

Exercises 
91 
Then A' E Ek-T1ME(n), but if DUN = NUN, by the previous lemma we 
have that A' E DTIME(n), which by the construction of the set A' implies that 
A E DTIME(t(n». 
0 
Now we are ready to prove our main result: 
Theorem 3.9 The class DUN is strictly contained in the class NUN. 
Proof The strategy will be to assume that DUN = NUN and to derive a 
contradiction. 
If NUN = DUN, by the previous lemma with ten) = n· log* nand k = 4 
we get that DTIME(n . log* n) = E4-TIME(n . log* n), but Theorem 3.9 with 
ten) = n . log* n would imply that E 4-TIME(n . log- 71) c E4-T1ME(71), which 
contradicts the time hierarchy theorem for alternating machines. 
Therefore DUN =I NUN. 
0 
3.6 Exercises 
1. 
Prove that the alternating Turing machines accept exactly the recursively 
enumerable sets. 
2. 
Prove that the language 
L = {x#y I y is the binary representation of Ix I} 
belongs to the class AT/ME I (n). 
3. 
·Prove Theorem 3.2. 
4. 
Prove that if L E ASPACE(s(n» then L is accepted by a one-tape alter-
nating machine within space sen). Hint: Use the same argument as for 
the Turing machines. 
5. 
(Space hierarchy theorem) Prove that if sl(n) and S2(71) are space con-
structible, s2(n) ;:::: log n, and SI E W(S2), then 
6. 
(Constant factor speed-up theorem). Let c be a positive rational number, 
and let k ;:::: 1. Assume t E woo(n). If L is accepted by a k-tape alternating 
Turing machine within time ten), then L is accepted by a k-tape alternating 
Turing machine within time max(n + 1, r 
c.t(n)l) 
7. Prove that if sen) is space constructible and greater than or equal to log n, 
then the class ASPACE(s(n» is closed under complements. 
8. 
Let 111 be an s(n)-space bounded and a(n)-alternation bounded machine, 
with sen) ;:::: log n. Then 111 can be simulated by a deterministic machine 
111' of space complexity a(n)s(n) + s(ni 
. 

92 
Alternation 
9. 
Prove that the set of boolean formulas with no satisfying assignment, SAT, 
is in APTIME. From this result, it is easy to see that co-N P ~ APTIME. 
10. Let Fn be the set of boolean functions from {a, l}n to {a, I}. Let men) 
be the maximum combinational complexity of a function in Fn. Prove 
that there is a language L E {O, I}·, which can be recognized by an alter-
nating machine in time 20 (n) and using a constant number of alternations, 
such that, for all n, the combinational complexity of L n {a, l}n is men). 
Obtain in this wayan alternative proof of Theorem 5.6 of Volume I. 
11. 
-Define the function counting COUNT: {a, l}n -+ N as follows; 
COUNT (x), . .. ,xn ) = the number of Xi equal to 1 
and define the function majority M AJ : {a, l}n -+ {a, I} as follows; 
. M AJ(x), ... , X n ) = 1 iff there are r nj21 or more Xi which have value 1 
Prove that 
(a) 
COUNTE AWGTIME 
(b) MAJ E AWGTIME 
12. 
Boolean formulas were defined in Chapter 1 of Volume I. Recall the 
definition, given the alphabet E = {v, A, --,,0,1, (,)}: 
(a) ° and 1 are boolean formulas; 
(b) if F) and Fz are boolean formulas, then so are (--,F), (F) v Fz) and 
(F) A Fz). 
Using Exercise 11(a), prove that the recognition problem for boolean for-
mulas is in AWGTlME. (In other words, construct an indexing alternating 
log time machine, which given as input a string of symbols from E, de-
termines if the input is a boolean formula.) 
13. Define AEf (respectively, AIIf> as the class of sets accepted by E,,-
machines (respectively lIt' -machines) which accept in polynomial time. 
Define {AEt' , All f} to be the polynomial time alternation hierarchy. 
Prove that 
Ef = AEf and lIt' = AlIt' 
where Ef, II t' denote the polynomial time hierarchy defined in Chapter 8 
of Volume r. 
14. Prove that the set: 
{(M,x, It) I AI is a E k- machine which accepts x in time t} 
is Ef -complete. 
15. Define the alternating logspace hierarchy by logspace alternating machines 
with a constant number of alternations. Prove: 
(a) AElog U AlIlog ~ AE1~\ n AlIt~ 

Exercises 
93 
(b) 
The set 1<log is AElog-complete; where 1<log is defined as: 
{(M,x) I AI is a Ek - machine which accepts x in space log Ixl} 
(c) The alternating logspace hierarchy collapses to level k > 0 if and 
only if its kth level is closed under complements. 
16. 
-Recall that a context-free grammar (denoted CFG), is a quadruple G = 
(V, T, P, S) where V and T are disjoint finite sets of variables and ter-
minals respectively. S is a special variable called start symbol. Finally, 
P is a finite set of productions; each production is of the form A -+ a 
where A is a variable and a is a string of symbols from (V U T)*. We 
say a language is context-free if it is generated by a context-free grammar. 
It should also be known to the reader that any context-free language not 
containing>. can be generated by a grammar in which all productions are 
of the form A -+ BC or A -+ a, where A., B and C are variables and a 
is a terminal. Such context-free grammars are known as Chomsky normal 
form grammars, and they have the characteristic that the derivation tree (or 
parse tree) of any word using such a grammar is a binary tree. Prove that 
any context-free language is recognized by an alternating Turing machine 
within time log2 n. Hint: Prove first that for any binary tree T with n 
leaves, there is a node s of T such that the subtree Ts rooted at s has m 
leaves, where n/3 ~ m ~ 2n/3. 
17. Let ASPTI(s(n), t(n» denote the class of languages recognized by alter-
nating machines within space s(n) and time t(n). Prove that CFL ~ 
ASPTI(log2 n, log n). Hint: Modify the algorithm in the previous prob-
lem by using the fact that for any three vertices in a binary tree such that 
any of them is a predecessor of the other two, there exists another vertex 
in the tree which is an immediate predecessor of exactly two of them. 
18. For s(n) = Q(log n), prove DSPACE(s(n» ~ ASPTI(s(n), s2(n». 
19. 
Say that a language L is accepted by an alternating machine M within 
tree-size bound z(n) if for every x E L, (Ixl = n), there is at least one 
accepting computation tree of J..1 on x which has size less than or equal to 
z(n). A language L is accepted within simultaneous tree-size and space 
bounds s(n) and z(n), L E ASPSZ(s(n), z(n», if L is accepted by an 
alternating machine M within tree-size bound z(n) and such that every 
configuration of the accepting computation tree uses space less than or 
equal to s(n). 
Prove that if L1 ~Iog L2 and L2 E ASPSZ(s(n), z(n» for s(n) ~ log n, 
then L2 E ASPSZ(s(n), z'(n», where the bound z'(n) = z(p(n» . p(n) for 
some polynomial p(n). 
20. Prove that every language in ASPSZ(s(n), z(n» is also in the class 
ASPTI(s(n), s(n) . log z(n» 

94 
Alternation 
21. 
Give a direct proof that any context-free language belongs to the tree-size 
and space bound class ASPSZ(1og n, n . log n). Notice that this exercise 
together with the previous one gives us other solution to Exercise 18. 
22. 
Define the class LOGCFL as the class of all sets which are log-space 
reducible to the class CFL of context-free languages. Prove that WGCFL 
can be characterized as the class of languages accepted by alternating 
Turing machines within simultaneous polynomial tree-size and logarithmic 
space bound. 
23. Prove that the following two problems are in WGCFL: 
• 
The graph accessibility problem 
• 
The monotone planar circuit value problem 
The graph accessibility problem was defined in Volume I. The monotone 
planar circuit value problem consists of the language of the codifications 
of all planar circuits with only AND and OR gates together with the inputs 
such that the output gate of the circuit evaluates to TRUE. 
24. 
Prove Proposition 3.3. 
25. 
How would you modify the proof of Theorem 3.9 to get the following 
result: DTIME(t(n» ~ ErTIME(t(n)/ log- n)? 
Hint: Construct the following machine: 1.- Existentially guess block-
respecting computation graph, segregator and segregator contents; 2.- Uni-
versally, for every node in the graph, mark the set of predecessors and 
simulate the deterministic machine on those nodes. 
26. 
Is it possible to extend the proof of DUN f NUN to DTIME(n3) f 
NTIME(n3)? Why can it not be extended to prove P f NP? 
3.7 Bibliographical Remarks 
The concept of alternation was developed independently by Kozen (1976) and 
by Chandra and Stockmeyer (1976). A later joint publication, Chandra, Kozen, 
and Stockmeyer (1981), gave a unified view of the theory. From these three 
papers we have taken our basic definitions, the section on complexity classes 
and Exercises 1, 5 to 7, and 13 to 15. In the joint paper, they credit Borodin for 
the result presented in Exercise 8. Further improvements in speedup between 
deterministic and alternating complexity classes are given in Paul and Reischuk 
(1980), Paul, PrauB, and Reischuk (1980) (from where we have taken Theorem 
3.1 and Exercises 2 and 3) and Dymond and Tompa (1985). In this last work, 
it is proved using a 2-person pebble game that DTIME(t) ~ ATIME(t/ log t). 
The concept of indexing machine is from Chandra, Kozen, and Stockmeyer 
(1981). Lemma 3.1 is from Ruzzo (1978). Exercise 10 is from L. Stockmeyer 
(personal communication); it is attributed to A. Meyer, and its use is described 
in Stockmeyer (1987). Exercises 11 and 12 are from Buss (1987). In this work, 

Bibliographical Remarks 
95 
Buss proves a stronger result, that the proQlem of determining the truth value of 
a boolean formula is in AWGTIME. 
In next chapter we will see the relationship, via uniform circuits, of alternat-
ing machines with the parallel models studied in Chapters 1 and 2. Moreover, 
direct characterizations in terms of alternating machines, of some of these mod-
els, exist in the literature. For example, SIMDAG's have nice characterizations 
in terms of alternating Turing machines in Stockmeyer and Vishkin (1984). The 
logarithmic alternation hierarchy was also defined in Kozen (1976) and in Chan-
dra, Kozen, and Stockmeyer (1981). Further characterizations of the logarithmic 
alternation hierarchy are given in Lange (1986). Several recent results prove 
the collapse of logarithmic space hierarchies; among them Jenner, Kirsig, and 
Lange (1989) and Schoning and Wagner (1988). These methods are quite in-
volved and do not seem to be easily extended to work in other classes such as 
the polynomial time hierarchy. A more recent result from which the collapse of 
the logspace hierarchies follows is the closure of nondeterministic space under 
complements, due independently to Immerman (1988) and Szelepcsenyi (1988). 
Immerman points out the collapse of the logspace alternating hierarchy and 
many other results, among them the closure under complementation of context-
sensitive languages, which was an open problem from Kuroda (1964). This 
result is described in the Appendix of this Volume. 
The results presented in Exercises 16 to 21 are due to Ruzzo (1978) and 
Ruzzo (1980). The idea of the tree-size bounded alternation is to define a mea-
sure with intermediate power between nondeterministic and alternating computa-
tions. The characterization of the class LOGCFL in terms of tree-size bounded 
alternation, which is given in Exercise 22, is also due to Ruzzo. Further re-
strictions on the size and shape of the computation trees of alternating Turing 
machines are given in King (1981), where he uses these results to characterize 
the languages accepted by Auxiliary Pushdown Automata. 
We shall observe the full importance of the class LOGCFL in the next chap-
ter. Besides the already mentioned characterization, the class LOGCFL has been 
characterized in terms of nondeterministic auxiliary pushdown automata work-
ing in polynomial time and logarithmic space, which was done by Sudborough 
(1978). Also LOGCFL has been characterized by means of a 2-person pebble 
game in Venkateswaran and Tompa (1986). Exercise 23 is from Cook (1985). 
This same work presents a collection of other problems in LOGCFL. 
One of the biggest fields of reseach in complexity theory has been the trade-
off between time and space. In Hopcroft, Paul, and Valiant (1977) it is proved 
that the class of sets recognizable by multitape Turing machines of time com-
plexity t(n) is strictly contained in the class of languages recognized by Turing 
machines with space complexity t(n); see also Theorem 5.32 of Paul (1978). 
Thus space is more valuable than time. The notion of block-respecting compu-
tations and Proposition 3.1 are from Hopcroft, Paul, and Valiant (1977). In Paul 
and Reischuk (1981) further time-space tradeoffs were obtained, and applied to 

96 
Alternation 
other machine models different from the Turing machine. Recently, in Halpern, 
Loui, Meyer, and Weise (1986), a general and simple simulation scheme to find 
space-time tradeoffs, for different kind of machines, has been developed. This 
line of research lies outside of the scope of this book. We refer to Chapter VIII 
of Wagner and Wechsung (1986) for a thorough survey of the results known in 
this line of research. 
We have already remarked in Volume I on the importance of the problem of 
comparing determinism versus nondeterminism, and in particular comparing de-
terministic complexity classes versus their non-deterministic counterparts. The 
first evidence that non-deterministic time-bounded machines are more powerful 
than deterministic time-bounded machines was provided by Hennie (1965), who 
proved a lower bound of at least n(n2) for a deterministic one-tape machine to 
recognize the non-palindromes of length n, while a nondeterministic machine 
can recognize them with a complexity O(n log n). See also Theorem 10.7 of 
Hopcroft, Ullmann (1969). Several subsequent attempts to extend these results 
to mUlti-tape machines were made. Among them'Paul and Reischuk (1980) as-
suming a key result in the proof a certain graph-theoretic hypothesis, proved that 
DUN =I NUN. Unfortunately, this hypothesis was later disproved by Schnitger 
(1982). However, using the same strategy and many of the techniques of Paul 
and Reischuk (1980), Paul, Pippenger, Szemeredi, and Trotter (1983) were fi-
nally able to prove the result. Our presentation of this result in Sections 5 and 6 
follows very closely this last article. The key of the proof lies in a simulation of 
deterministic Turing machines by alternating machines which uses the concept 
of a computation graph. This concept has been widely used in the simulation 
of different models of machines (see for example Paul (1978) or Chapter VIII 
of Wagner and Wechsung (1986». The result presented here is the only known 
separation among complexity classes other than those that follow from the time 
and space hierarchy theorems. 

4 
Uniform Circuit Complexity 
4.1 Introduction 
In the previous chapters we have studied some models of parallel machines. 
In this chapter, we are going to define some parallel complexity classes. In 
modelling parallel computation, we wish to model the situation in which the 
number of processors is greater than the length of the input; however it is 
clear that any "real world" parallel computer should have a feasible number 
of processors. For this reason most of the theoretical research on parallelism 
has focused on the case in which the number of processors is bounded by a 
polynomial on the size of the input. As we have mentioned in the Chapters 
one and two, the number of theoretical models of parallel computation is very 
large; we shall define complexity classes in terms of uniform families of circuits, 
which seems to be a unifying framework for most of the parallel models, as well 
as a natural measure to count the size of the hardware. In the second section, 
we introduce the basic definitions and properties. In the following section we 
prove the robustness of parallel complexity classes with respect to other models 
of parallel computers studied in Chapters 1 and 2; in particular we prove the 
equivalence of uniform circuits with vector machines. Then we discuss other 
measures of uniformity and its incidence in the definition of parallel classes. 
In section 4, we establish the relationship of uniform circuits with alternating 
machines, and prove a characterization of the defined parallel classes in terms 
of alternating machines. 
4.2 Uniform Circuits: Basic Definitions 
In this chapter, we will be dealing with families of boolean circuits, C = 
(Cl, C2, ••• ,Cn ••. ) where Cn has n inputs and one output. For the basic defi-
nitions on circuits, we refer the reader to Chapter 5 of Volume I; we just remark 
that unless otherwise stated the gates of the circuits will be {A, V, -,}, so the 
fan-in of the circuits will be 1 or 2. Let z(n) and t(n) denote the size and depth 
of circuit Cn. We number the gates of Cn in such a way that the output gate is 
numbered 0; the input gates are numbered from 1 to n; and the gate numbering 

98 
Uniform Circuit Complexity 
is restricted so that the largest gate number is z(n)O(l). In this way, any gate 
number can be written in binary using space O(log z(n». 
We can extend Definition 5.10 in Volume I to families of circuits: 
Definition 4.1 Given a set A E {O, 1}*, thefamily C recognizes A iffor each 
n, c,. computes the characteristic function of An {O, 1}*. If en has size zen) 
and depth t(n), then the size and depth complexity of C is zen) and ten). A set 
A has size and depth complexity zen) and ten) if there is a family of circuits of 
the corresponding complexity which recognizes A. 
Observe that the depth of the family is a measure of the time, while the size is 
a measure of the number processors needed. 
As we pointed out in Chapter 5 of Volume I, there are arbitrarily difficult sets 
with trivial circuit complexity. For example, let A be any nonrecursive subset of 
{O}*. Then A is accepted by the family C = (el, ... , en, .. . ), where Cn accepts 
on if on E A, otherwise Cn does not accept anything. The way to avoid this 
problem is consider only families of circuits in which c,. is "easily" constructed 
from n; families with this condition are said to be uniformly constructible. The 
uniformity condition amounts to require a pre-processing condition in which the 
circuits can be constructed using a certain amount of resources, previous to the 
actual "computing agent" function of the constructed circuit. The definition of 
"easy" affects the circuit complexity. If the uniformity condition is too strong, 
the circuits are very easy to compute, then the circuits will be too unrealistically 
simple. On the other hand, if we make the circuit constructor hard to compute, 
then we are in danger of trivializing the theory. In Exercises 9 and 10 we 
present a measure of uniformity where the pre-processing phase has as much 
computational power as is feasible. We shall begin by considering "weak" 
models of uniformity. 
Recall that in Chapter 5 of Volume I, we defined the standard encoding of 
a circuit Cn, denoted en, as the string of 4-tuples of the form (g, b, gl, gr), where 
g represents the gate number, b is the boolean operation performed by the gate, 
gl is the number of the gate which provides the left input to g, and gr is the 
number of the gate which provides the right input to g. The computation of the 
standard encoding of a circuit from the number of inputs will be the basis for 
our first definition of uniformity measure. 
Definition 4.2 Let UBc-SIZE,DEPTH(z(n), t(n» denote the class of sets A E 
{O, 1}* for which there exists a family C = (Cl, C2, •• • ) of size and depth com-
plexity zen) and ten) such that 
1. The family C recognizes exactly the set A; 
2. 
The function In -+ en is computable by a deterministic Turing machine in 
space log z(n), 
3. 
The circuit Cn has depth ten). 

Uniform Circuits: Basic Definitions 
99 
In the above definition, condition 2 is the uniformity condition; then~fore to say 
that a family C of circuits is UBc-unifonn means that for every n, en can be 
described within space log z(n). 
Definition 4.3 For any positive integer i, let NCi be the class of languages 
accepted by UBc-circuits of size n°(l) and depth logi n. Define 
NC = Ui~lNCi. 
From the definition it follows that for any i ~ 1 we have that NCi ~ NC. On 
the other hand, recall that the simulation done in Theorem 5.2 of Volume I, 
together with the uniformity condition which says that the whole circuit can be 
constructed using a log-space, enables us to state 
Proposition 4.1 NC ~ P 
It is an open problem whether the above inclusion is a strict one. Moreover, 
as in the case of P and NP, it is widely conjectured that NC =I P. This remark 
is important as if the conjecture is true, all P-complete problems will not be NC. 
In the exercises and the bibliographical remarks at the end of the chapter, the 
reader can find some references to P-complete problems under different types 
of reducibilities. In the last section of this chapter, we shall discuss more fully 
some of the implications of this conjecture. 
Proposition 4.2 For any i ~ 1 we have NCi ~ DSPACE(logi n). 
Proof. Let A be a language in NCi, then by definition there exists a family of 
circuits C which recognize A. Let en be the circuit in the family recognizing 
An {D, 1 In. Clearly, en can be represented as a dag of size n and height log; n. 
Moreover, each of the nodes of the dag will be labeled. Construct a deterministic 
Turing machine Un which from the node labeled D and using depth-first search, 
traverses the whole tree en. A direct implementation of this method will need 
space logi+l n. But we can use here the uniformity of C. Un can compute en 
within space log n; at each instant of the computation, Un needs to store only 
the node being considered (g, t, g" gd), which includes pointers to the next nodes 
to be considered. 
0 
As a corollary, using the space hierarchy theorem (Theorem 2.10 in Volume 
I), we can state 
Corollary 4.1 NC =I PSPACE 
We have defined the class NC as a class of languages. We also can extend 
the definition, so that NC is defined as a class of boolean functions. 
As we did with the alternating classes in the previous chapter, we state the 
following 

100 
Uniform Circuit Complexity 
Definition 4.4 
A boolean function I: {O, l}n -+ {O, l}m is in the class NC iff 
the language AI = {(Xl ... Xn, i)1 the ith symbol of I(XI .•. xn) is I} is in NC. 
Notice that the above definition coincides with the definition of function 
computed by a circuit, given as Definition 5.6 of Volume I. A circuit of depth 
log n and poly-size for I : {O,l}n -+ {O, l}m will consist of m output gates 
each of which will give respectively the l,t, 2nd, ... , mth bit of I(xt, ... , Xn). 
By the above definition, if I ENC, then each of the m circuits will have poly-
size, and log depth, and therefore the m circuits together in "parallel" will also 
have poly-size and log depth. 
Evidently, the same definition could be extended to any of the classes NCi . 
Let us see some examples of sets and functions in NC i• 
Example 4.1 
Let us see that the language {wwRlw E {O, 1}*} is in NC I • For 
even n, the circuit Cn computing An {O, l}n will consist of n gates -', n gates 
/\ and n/2 gates V, to test if positions i and n - i are equal. To the output of 
the V gates, we put an AND-TREE of polynomial size and height log n/2. See 
Figure 4.1. The "reverse" subcircuit has no gates and just swaps some wires 
to get a clearer drawing. The "equal" subcircuit tests equality, and their gates 
are numbered according to the same pattern. See Figure 4.2. The deterministic 
procedure described in Figure 4.3, given as input In, constructs in a bottom-
up fashion the circuit en using space log n. The algorithm is split into a main 
procedure and a subroutine. The main procedure tests if the input chain is even 
(otherwise it outputs 0, as the chain can not be a palindrome). If the input 
is even, it constructs the part of the circuits which compares bits i and n - i 
of the input, and output 1 if they are equal. So at this stage, there will be a 
circuit of size 5n/2, depth 4 and with n/2 outputs each of which must be 1 if 
the input is a palindrome. To test that indeed they are 1, the main procedure 
calls a subroutine AND-TREE (Figure 4.4) which constructs a tree of /\-nodes, 
in which the inputs are the n/2 outputs described above. The total size of the 
AND-TREE will be less than or equal to I-I and the depth log n/2. 
Recall from Chapter 5 of Volume I that the model of transducer computing in 
log-space only measures the amount of space used in the work tape. Therefore, 
in our procedure, in the working tapes we just write at each instant of the 
computation at most a single gate. If gate m has a codification (m, /\, u, z), 
then m, u, z are written in binary, and /\ is codified as a bit 1; therefore if the 
circuit has size zen), each gate can be writen using space 3·10g z(n)+c, where c is 
a constant. In the procedure of Figure 4.3, we will use four counters. A counter 
N will indicate the total number of gates written. Therefore the maximum size 
needed for this counter will be log zen). A counter F will indicate the size of 
the last "row" of generated gates; again the maximum size of this counter will 
be log zen). A counter S with a single bit will show if we have a gate which 
has been produced but not included as son of another gate (this happens when 
F is odd). Finally, a counter C contains, if S = 1, the label number of the odd 

EQUAL 
31 
21 
26 
11 
16 
EQUAL 
22 32 27 
12 
17 
EQUAL 
23 33 28 
13 
18 
Unifonn Circuits: Basic Definitions 
101 
EQUAL 
34 
24 
29 
14 
19 
REVERSE 
EQUAL 
35 
25 
30 
15 20 
Figure 4.1 A unifonn circuit accepting palindromes of length 10 
gate indicated by S. C will contain a gate at the time, so the space needed by 
C is log zen). Therefore, the total space used by the algorithm is 
3 . log z + c + 3 . log z + 1 E O(log z) 
In Figures 4.1 and 4.2, the numbers besides the gates indicate the enumeration 
produced by the procedure of Figure 4.3 on this circuit 
Example 4.2 Let us see that the matrix multiplication of two square n x n 
boolean matrices A = [ai,;] and B = [bi,;] is in Ne l • Recall that in Example 5.7 

102 
Unifonn Circuit Complexity 
Figure 4.2 Explanations for Figure 4.1 
I 
EQUAL 
31 
21 
26 
11 
16 
I 
I 
REVERSE 
of Volume I, we produced a circuit of polynomial size and logarithmic depth 
which computes this function. Such a circuit consists ofn independent OR-trees, 
each of depth flog n 1, and every terminal node in the trees will be an AND node 
having as inputs ai,j and bi,j' Looking at the previous example, it should be 
straightforward to prove Usc-uniformity. 
Example 4.3 From the previous example, it is easy to see that the transitive 
closure of a boolean square matrix A is in NC2• Notice the transitive closure 
which is defined as A· = V i>O Ai. The reader can easily check that the solution 
given in Example 5.8 of Volume I produces an algorithm of poly-size and log2 
depth. By the way, this example also shows that the problem of deciding, given 
a graph of n vertices, if there exists a path from a node i to a node j is also 
in NC 2; as this problem consists in checking if the entry i, j of the transitive 
closure of the adjacency matrix is 1. 
Again we leave to the reader the verification of the Usc uniformity condition. 

Relationship with General-Purpose Parallel Computers 
103 
Input In 
if n is odd, output ° 
else from I = I to n/2 do: 
Comment: Construct a first row of n gates ..,; 
write (n + i, .." i) 
write (n + I + i, .." 11 - i + 1) 
Comment: Construct a row of n gates 1\, 
Comment: The first n/2 will test if positions i and n - i are I's 
Comment: The other n/2 will test if positions i and n - i are O's 
from i = 1 to I do: 
write (2n + i, 1\, i, n - i + 1) 
write (2n + I + i, 1\, n + i, n + I + i) 
Comment: Construct a row of n/2 gates V which test if for every 
Comment: pair of 1\- gates coming from positions i and n - i 
Comment: at least one them has output I 
from i = 1 to I do: 
write (3n + i, V, 2n + i, 2n + I + i) 
Comment: Call the subroutine AND-TREE to test if the output of all the 
Comment: 71/2 V gates is 1 
call AND-TREE (71/2, 3n, 0, ologz(n» 
Figure 4.3 Procedure to construct Be-uniformly the circuit of Example 1 
4.3 Relationship with General-Purpose Parallel Computers 
We shall consider the relation between uniform circuits and other models of 
parallel computation. First of all we simulate vector machines with the aid of 
uniform circuits. This simulation is done with circuits of exponential size in 
the input. Secondly we consider a relation between uniform circuits and Turing 
machines. 
Let P be a vector program with vector registers Vi, V2, •• "Vk and index 
registers 11,[2, ... ,Iq. Suppose that Vi contains the input word wand all other 
registers are initially filled with zero. Suppose that P is bounded in time by 
T(n). By Lemma 1.1 there exist p and q depending on P such that the length 
of vector registers is bounded by 2P+T (n) + q and the length of index registers is 
bounded by T(n) + p. Redefine the time of P as pog(2P+T (n) + q)l. Then the 
length of vector registers is bounded by 2T(n) and the length of index register is 
bounded by T(n). 

104 
Uniform Circuit Complexity 
Procedure AND-TREE (F,N,S,C) 
While F > 3 do 
If F is even, let m = F /2; 
from i = 1 to m, do 
write (N + F + i, /I., N + 2i - 1, N + 2i) 
let N = N + F; and F = m; 
else let m = l F /2 J ; 
from i = 1 to m do 
write (N +F+ i,/I.,N +2i -1,N +2i) 
if S = 1 then 
write (N + F + 2m + 1, /I., N + F, C) 
let C = 0; S = 0; 
else let C = N + F; S = 1; N = N + F; and F = m; 
endif 
endif 
endwhile 
if F = 3 then write (N + 4, /I., N + 1, N + 2) 
if S = 1 then write (N + 5, /I., N + 3, C) 
write (0, /I., N + 4, N + 5) 
else write (0, /I., N + 4, N + 3) 
else if S = 1 then write (N + 3, /I., N + 2, C) 
write (0, /I., N + 1, N + 3) 
else write (0, /I., N + 1, N + 2) 
end 
Figure 4.4 Subroutine to construct an AND-TREE with input of size F 
In order to obtain circuits simulating vector instructions we will consider only 
the first 2T {n) components of any vector register and the first T(n) components 
of any index register. Let us consider the input-output conventions. Take a 
circuit simulating the instruction CA:-BAC' This circuit will have 2T {n)+1 inputs 
and 2T {n) outputs. The inputs from 1 to 2T{n) correspond to the vector B. The 
inputs from 2T {n) + 1 to 2T {n)+1 correspond to C. The outputs representing A 
are numbered from 2T {n)+1 + 1 to 2T {n)+2. The internal nodes are numbered from 
2T {n)+2. 
Lemma 4.1 Suppose that the Junction T(n) is space constructible. 
1. 
There exists a circuit CA-<l testing if a vector is equal to zero with depth 
O(T(n» and In -+ CA_O is computable in space T(n). 
2. 
There exists a circuit CA:-BAC giving the vector /I. oj two vectors with 
constant depth and In -+ CA:-BAC is computable in space T(n). 

Relationship with General-Purpose Parallel Computers 
105 
3. 
There exists a circuit CA:-B1C giving the shift of two vectors with depth 
O(T(n» and In -t CA:-B1C is computable in space T(n). 
Proof Consider the zero test. The circuit is an OR-TREE constructed in the 
same way as the AND-TREE given in Example 4.1. 
The circuit given the AND of the two vectors Band Cis: 
C . 
= {(2T(n)+1 + i /I. i 2T(n) + i) I 1 < i < 2T(n)} 
A.-BAC 
, , , 
_ 
_ 
This circuit has constant depth and can be computed in space T(n). 
Consider the shift operation. Suppose that the contents of A, Band C are 
respecti vel y: 
A = ... OOOa2T(n)'" a;··· al 
B = ... OOOb2T(n) ... b; ... bl 
C = ... OOOCT(n) ... cp ••• CI 
For every cp let us define the function shift 21'-1 as: 
where: 
for 2T(n) ~ i > 21'-1 
for 21'-1 ~ i ~ 0 
The shift can be obtained as the composition of the T(n) partial shifts as: 
The circuit giving the shift can be constructed as a circuit with T(n) levels. 
Each level takes care of one bit of the register C. This circuit can be computed 
in space T(n). 
o 
Theorem 4.1 If T(n) is space computable then VECIOR-TIME(T(n» is in-
cluded is U Bc-SIZE,DEPTH(O(2T(n», O(T2(n))). 
Proof Let P be a vector machine working in time T(n). This program is a 
list of labeled instructions: 
Let VIoV2, ... ,v;, be the registers appearing in P. Let us define a function 

106 
Uniform Circuit Complexity 
simulating one step of the computation. We will denote this function by: 
The function Cstep will be synthesized with the aid of two other functions. These 
functions are the function Coperation and the function Ccontrol: 
Ccontrol(ll, ..• , h, Vi, .. . , v,,) = (I;, ... , l~) 
Let us consider Coperation. Suppose that the instruction number i is Vi := V; II. Vk; 
by the preceding lemma we can construct a circuit: 
. {O 1}2T ( .. ).t 
{O 1}2T (") 
cVj:_v.l\v.·, 
--+, 
simulating the preceding instruction in space O(T(n». From this circuit we can 
easily construct another circuit simulating the preceding instruction but acting 
over all vector variables. Formally: 
where Vi = V; for j -I I and Vj' = v;. II. Va· The circuit CVj:aV.I\V.(VI, •.. , v,,) 
can be computed also in space O(T(n». In a similar way we can construct the 
circuits: 
CVi:R~'.TV. (Vi, ... , Vp) = (VI', ... , V;) 
Clr,:.V.W.(VI, ... , v,,) = (V(, ... , V;) 
clr,:-v.vv.(Vi, ... , v,,) = (V{, ... , V;) 
When we have a test instruction "go to I if V; = 0" this instruction does not 
change the content of the vector variables. In this case the associated function 
is the identity. 
Cgo /0 I if v;=o(Vi, ... , v,,) = (VI, .. . , Vp) 
In this way at the instruction number i we associate a circuit: 
Cinstructioni(VI, ... , v,,) = (V{, ... , V;) 
We conclude that every vector operation can be simulated by a circuit computable 
in space O(T(n». This circuit has a depth bounded by O(T(n». From these 
circuits we can construct the function coperation: 
Coperation(ll, .•. , lk, Vi, ... , v,,) = (V{, .•• , V;) 
(V{, ... , V;) = (II II. Cinstructiont) V •.. V (lk II. Cinstruction~) 
where the OR is taken component to component. 

Relationship with General-Purpose Parallel Computers 
107 
Let us consider the control function. Let Zj be a boolean variable taking the 
value one only when l!; = O. By the preceding lemma, Zj can be computed as 
Zj = cv;-o(l!;) in space O(T(n» and depth O(T(n». Then the following function 
can also be computed in space O(T(n» and depth O(T(n»: 
cVi-o(Vi, ••• , Vk ) = Zj 
Assuming that the variables Zj are constructed the function: 
l(ll,···, Ik' ZI, ••• , Zk) = (l~ ... I~) 
giving the label of the following active instruction to be executed can be com-
puted with a cost independent of the length of the input. Considering these 
results we can construct the following function in space O(T(n» and depth 
O(T(n»: 
ccontro/(/I, . .. ,Ik' VI, .. . , v;,) = (l~, ... ,l~) 
A circuit Cp for program P can be obtained as the composition of T(n) circuits 
Cstep. The circuit Cp has as variables II,. .. ,h,V), ... , v;,. The input to this circuit 
will be: 
VI = oT(n)-nw and V2 = ... = Vk = oT(n) 
II = 1 and 12 = ... = Ik = 0 
In order to obtain an input of length n the T(n)-n zeros of Vi and the T(n) zeros 
of other vector registers can be hardwired. A similar procedure can be employed 
with the values of the labels. Note that the circuit Cp has a depth bounded by 
0(T2(n», size 2T(n) * 0(T2(n» and is computable in space O(T(n». 
0 
In the sequel let us consider whether we can simulate a uniform circuit by a 
Turing machine. 
Theorem 4.2 For Sen) ~ log n we have that UBc-SIZE,DEPTH(2S(n), S(n» 
is contained in DSPACE(O(S(n») 
Proof Let Cn be a circuit belonging to UBc-SIZE,DEPTH(2S(n), S(n». A node 
can be coded with O(S(n» bits. Let us describe a machine Mn evaluating the 
circuit. The machine !lIn performs a depth-first search from the output node 
taking left descendants first. A straighforward implementation of this algorithm 
by a stack will be bounded in space by 0(S2(n» because the depth of the stack is 
bounded by Sen) and every stack frame is bounded by O(S(n». This algorithm 
can be improved to obtain a bound of O(S(n». To do this the machine Mn 
stores only the node currently examined and the stack keeps one symbol for 
each node on the path followed from the root to this node. This symbol is 1 if 
the search proceeds to the left input of the node, or is the value of the left input 
if this value has been determined and the search proceeds on to the right. 
0 
Considering the last two theorems we can state the following collorary 
Corollary 4.2 PSPACE = U Bc-SIZE,DEPTH(2nO(1), nO(I» 

108 
Uniform Circuit Complexity 
4.4 Other Uniformity Conditions 
We have seen in Example 1 that the proofs of uniformity via the standard 
encoding can be a bit cumbersome. In this section we introduce a new formalism 
for describing circuits, called the extended encoding language, which in many 
cases will simplify the proof of uniformity. First some notation. Let C = 
{Cl, ... , cn } be a family of circuits. If 9 is any gate in c;, and p a chain on 
{L, R} * , denote by g(p) the gate reached following the path indicated by p from 
the sons of g. For example, g().) is g, g(L) denotes the left son of gate g; g(R) 
denotes the right son of gate g; g(LRLL) denotes the gate which is the left son 
of the left son of the right son of the left son of g. 
Definition 4.5 
Given a family of circuits C, its extended connection lan-
guage Le 
is the set of strings of the form (n, g, p, y) with n, 9 E {O, I} *, 
Y E {x,/\, V,""} U {a, l}*, p E {L,R}* with Ipi ~ logz(n), such that in en 
either p = ). and the gate number 9 is y E {x, 1\, V,""} or p =I ). and the gate 
g(p) is numbered y E {0,1}*. 
From the definition, the extended connection language of a circuit is just a 
codification of the type of each gate, together with the labels of all predecessors 
within distance log z(n), where as usual z(n) denotes the size of the circuit. 
Definition 4.6 
The family of uniform circuits C = (Cl, C2, ••• ) of size and depth 
respectively z(n) and t(n) is U E-uniform, if there is a deterministic Turing ma-
chine recognizing Le in time logz(n). Similarly, C is UEo-uniform if there 
is an alternating Turing machine recognizing Le in time O(t(n» and space 
O(1og z(n». 
Let us define the complexity classes induced by the previous definition: 
Definition 4.7 Let UE-SIZE,DEPTH(z(n), t(n» denote the class of sets A E 
{a, 1}* for which there exists a UE-uniformfamily of circuits C = (CI, C2, ... ) of 
size and depth complexity z(n) and t(n) such that C recognizes exactly the set A. 
Similarly let UEo-SIZE,DEPTH(z(n), t(n» denote the class of sets A E {a, 1}* 
for which there exists a a UEo-uniformfamily of circuits C = (Cl, C2, ••• ) of size 
and depth complexity z(n) and t(n) such that C recognizes exactly the set A. 
Let us prove now that the extended connection language has exactly the same 
space complexity as the standard encoding function. 
Lemma 4.2 For 8(n) = il(log z(n» the following statements are equivalent: 
1. 
n -+ Cn is computable in DSPACE(s(n)) 
2. 
Le is recognizable in DSPACE(s(n)) 

extendedaccept (n,g,p,y) 
while pi>' do 
let p = Hp' with H E {L, R} 
Other Uniformity Conditions 
109 
if p' = >. then accept if g(H) = y otherwise reject 
else call extendedaccept (n, g(H),p', y) 
if p = >. then accept if y is the label of g, otherwise reject 
Figure 4.5 DTM which accepts L. within space .(n) 
Proof 
(1) =? (2). To decide whether (n, g,p, y) E L e , we simulate the deterministic 
Turing machine AI which computes n -+ en until it outputs (g, t, gL, gR) for some 
t, gL, gR. This machine is given in Figure 4.5. 
To obtain with M a 4-tuple (n, gH,p', y), we need space O(s(n». We 
need to repeat the process a maximum of Ipl :$ log z(n) times, but as we can 
always make the computations re-using the same space, we have that Le E 
DSPACE(s(n». 
(2) =? (1). Supposing that we can recognize Le within space s(n), let us 
prove we can compute n -+ en within space s(n). We compute first the number 
ofthe gate with greatest number in the circuit en. To do this, for each 9 = 0, 1, ... 
we test in space s(n), if for some y E {x, V,I\, ... }, we have that (n,g,>',y) 
belongs to Le. If so, we write the tuple as part of the output. Let k be the largest 
label of a gate in the circuit. Beginning with the gate 0, we can find its right (left) 
input by testing if for any gate gL :$ k, (n,g,L,gL) E Le «(n,g,R,gR) E Le) 
and repeating the process for all the gates from ° to k. 
Besides testing in space s(n) if the tuple is in the extended language, we 
must keep a count of the gates which have been found, but this can be done 
with a counter of size log z(n), which is less or equal to s(n). Therefore we can 
compute In -+ en within space s(n). 
0 
The previous result together with Exercise 18 of the previous Chapter give 
the following relationships between the various uniformity conditions. 
Proposition 4.3 
For any family C of circuits we have: 
1. If C is UE-uniform, then C is also UBc-uniform. 
2. If C is U E-uniform, then C is also U Eo -uniform. 
3. If t(n) ~ log2 z(n) then U Bc-uniformity implies U Eo -uniformity. 
Proof 
(1) If C is U E-uniform, then Le E DTIME(log z(n», so Le can be 
decided by a deterministic machine within space log z(n), so by Lemma 4.2, the 

110 
Unifonn Circuit Complexity 
encoding In -+ en can be computed within space O(log zen»~, and therefore by 
definition C is U Bc-uniform. 
(2) If C is U E-uniform, then Le can be decided deterministically within 
time O(log z(n» and space O(log zen»~; and therefore Le can be decided by an 
alternating machine working simultaneously in space log zen) and time log zen), 
as ten) ;::: log zen), it follows that C is UE.-uniform. 
(3) If C is U Bc-uniform then n -+ en is computable in space log zen). 
Applying Lemma 4.2 and Exercise 18 of Chapter 3, we see that Le can be rec-
ognized by an alternating machine working simultaneously within space log zen) 
and time log2(n), so using the hypothesis of the statement it follows that C is 
UE.-uniform. 
0 
From the previous proposition we can see that UE is the strongest uniformity 
condition. Intuitively, it says that a circuit is U E-uniform only if a deterministic 
Turing machine can follow such a path using an average of 0(1) time for each 
step in the path. Condition UBe is weaker in the sense that it allows polynomial 
time in zen) per step but within space logz(n). Condition UE' weakens UE in a 
different way; it allows the path-following to be done by an alternating machine, 
rather than a deterministic Turing machine, and also takes more time, but again 
within log z space. 
4.5 Alternating Machines and Uniformity 
Let us give a relationship between the model of an alternating machine studied 
in the previous chapter and uniform circuits. From this relationship will follow 
a new characterization of NC. For the sake of clearness in the explanation, we 
shall split the relationship into two theorems. 
Theorem 4.3 
Assume that ten) and sen) = il(log n) are computable by a 
deterministic Turing machine with input n (in binary) in time O(s(n». Then, 
ASPTI(s(n), t(n» ~ UE-SIZE,DEPTH(20(s(n», t(n» 
Proof Let A. be a language in ASPTI (s(n), t(n», then there exists an alter-
nating machine AI that recognizes every input Ixl = n in A within space sen) 
and time ten). To construct the family C = (el,"" en, . .. ), we consider each 
computation tree of AI on input x of size n, and construct a U E-uniform circuit 
en of size 20(s(n» and depth ten), which recognizes An {a, 1}*. 
Let us assume without loss of generality that at each step the alternating 
machine M does one of the following things: 
• 
universally or existentially it branches into exactly two successor config-
urations, 
• 
reads a cell on input tape and moves to another configuration accordingly, 
• 
accepts or rejects. 

Alternating Machines and Uniformity 
111 
We will define two types of gates for en; 'gates of type S, which will be usual 
gates V or A, and gates of type D which will be gates V or A but labeled (t, a) 
with 0 ~ t ~ ten) and a one of the 20 (.(n» possible configurations of M on 
x. The actual construction of the circuit from the computation tree is done as 
follows (see Figure 4.6): 
1. 
The output gate of the circuit will be (0, ao) where ao is the initial con-
figuration of M on x. 
2. 
If in configuration aj, M universally branches into aj and ako then aj 
will be substituted by a A-gate (t, aj), and its inputs will be (t + 1, a j) and 
(t+l,ak). 
3. 
If in configuration aj, AI existentially branches into aj or ako then aj will 
be substituted by a V-gate (t, aj), and its inputs will be (t + 1, aj) and 
(t+l,ak). 
4. 
If in configuration aj, AI reads input symbol Xj and it goes to configuration 
a/ or ak depending if Xj is 1 or 0, then aj will be substituted by a V-gate 
(f, aj): its left son will be a A type S gate which will have a left input 
(t + l,xj) and a right input (t + l,a/), the right son of (f,aj) will be a 
A type S gate which will have a left input (t + 1, ak) and a right input 
(t + l,x). 
5. 
If in configuration aj the machine M accepts, then (t, aj) will have as 
input the constant 1. If it rejects, it will have as input the constant O. 
As the number of type S gates is bounded above by 2n, and the number of 
type D gates is bounded above by t(n)· 20 (s(n», the total number of gates in the 
constructed circuit will be 2n + f(n)· 20 (s(n)) E 0 (20 (s(n»). 
Moreover, it follows from the construction than the height is f(n) + n E 
O(f(n». 
On the other hand, it is clear from the above procedure that the constructed 
circuit en accepts the language An {O, I}·. 
To complete the proof, we must see that Le can be decided within space 
O(s(n». This is done by the deterministic procedure given in Figure 4.7. 
As the alternating machine M is bounded in space sen), the decision process 
for (n, g, p, y) requires space sen). As by hypothesis the functions ten) and sen) 
are computable given n in time O(s(n», the deterministic machine of Figure 
4.7 can check that allowed time and space are not exceeded when following the 
steps indicated by p. 
0 
By considering Lemma 4.2 the previous theorem can be easily modified to 
change the statement to UBC uniformity. 
Let us now prove the relationship in the other direction, and consider the 
simulation of uniform circuits by alternating machines. 

112 
Uniform Circuit Complexity 
o ai (read Xi) 
v,a~/ ~o3,a. 
/\ /\ 
o 
0 
0 
0 
G)(t,ai ) 
t 
t+l 
/~ 
@ (type S) 
@ (type S) 
/ 
1\ 
@(t+ l,a;) Xi 
Xi 
0(t+ 1,ak) 
/\ 
/\ 
o 
0 
0 
0 
Figure 4.6 Above: Partial computation tree of an ATM. Below: The circuit constructed from 
the above tree 
Theorem 4.4 Let ten) = !2(1og n) and let zen) = n°(1), then 
UEo-SIZE,DEPTH(z(n), t(n» ~ ASPTI(log zen), t(n» 
Proof. Let C = (Cl, C2, ... , en, ... ) be a UEo-unifonn circuit recognizing the 
set A. Let Mu be an alternating machine recognizing the extended connection 
language Le of C, within time ten) and space log zen). The idea is to simulate 
the gates of each en by the nodes of an alternating computation tree, but the 
problem is that the circuit is too large. Instead, we construct an alternating 
machine M, which nondeterministically guesses the sequence of gates, and uses 
the universal and existential states to simulate the A and V gates respectively. 
The alternating machine },,f is given in Figure 4.8. 

Alternating Machines and Uniformity 
113 
input (n,g,p,y) 
decodify 9 -+ (t,O') 
if p = A then check that y corresponds to the type (V or /I.) of gate 
codified in 0' 
else, (P E {L, R}+), then simulate Ipl steps of the alternating machine M 
starting at 0'. At each step, we choose L or R according to p. 
check that y corresponds to the type of gate codified 
by the configuration at which we arrive 
Figure 4.7 Deterministic procedure to decide L. 
The alternating machine CV is a recursive "circuit value" procedure. The 
call CV (n, g, p) will accept if and only if gate g(p) in circuit en has value l. 
procedure eVen, g,p) : boolean 
while Ipl < log zen) do 
begin 
existentially guess t E {x, V, /I.} 
Comment: Do the following universally: 
Comment: Compute the gate number of g(p) and check that g(p) is of type t 
Comment: We cannot compute the gate number directly 
Comment: as we will use more space that we are allowed, 
existentially guess h E {O, 1 pogz(n) 
universally do the following two steps: 
using that (n,g,p,h) E L(Mu), check that h = g(p) 
using that (n,g,A,t) E L(Mu) check that his of type t 
Comment: Let us check the type of g(p) 
if t = V then return eV(n,g,pL) V eV(n,g,pR) 
else ift = /I. then return eV(n,g,pL)/I. eV(n,g,pR) 
else if t = x the return the hth symbol of the imput 
end Comment: In the case Ipl = log zen) 
existentially guess h 
using that (n,g,p, h) E L(Mu) check that h = g(p) 
set 9 = hand p = A 
call eVen, h, A) 
end 
Figure 4.8 
Alternating machine M which simulates a uniform circuit 

114 
Uniform Circuit Complexity 
The simulation begins by calling M(n, 0, >.) (recall that in our enumeration, 0 
corresponds to the label of the output gate). The idea of the procedure is to 
"get out" of the recursion as many instructions as possible. The routine at the 
end (case Ipl = log zen»~ will only be true once every log zen) recursive calls, 
and each of these times, will be O(log zen»~. It is left to the reader to check 
that as the number of recursive calls made is ten), the total running time of the 
procedure is O(t(n». 
Moreover, as we already saw previously in this chapter, we can re-use the 
same space for the check instructions, therefore the space complexity of the 
procedure is bounded above by O(log zen»~. 
0 
If we had used Unc-uniformity, the solution by alternating machines would 
need more time that in the previous theorem. See Exercise 6 at the end of the 
chapter. 
Putting together the two previous theorems, we get 
Corollary 4.3 
For all i ~ 2, NCi = ASPTJ(log n, login) 
Proof By definition, NC k = U Bc-SIZE,DEPTH(n°(1), logk n), which in tum, 
by Proposition 4.3, is a subset of U Eo-SIZE,DEPTH(n°(I), logk n), but using 
Theorem 4.4 we get NC k ~ ASPTI(log n, logk n). 
On the other hand, by Theorem 4.4 we have 
ASPTJ(log n, logk n) ~ UE.-SIZE,DEPTH(n°(I) , logk n) 
which is a subset of NC k • 
o 
Using this last characterization of the class NC, we can extend some of the 
results obtained in the exercises of the previous chapter into the hierarchy of 
parallel classes NC i• In particular, it follows immediately from Exercises 17 
and 22 of the previous chapter that: 
Corollary 4.4 
1. 
NC t ~ DSPACE(log n) ~LOGCFL~ NC 2• 
2. NC = ASPSZ(logn,2Iogn ). 
Part 1 of this corollary indicates that the structure between classes NC t and 
NC 2 is a rich one. In Exercise 7, a special type of reducibility is defined which 
helps to study such a structure. (See also bibliographical references at the end of 
the chapter.) The importance of the study of these two classes NC t and NC 2 lies 
in the fact that most of the known interesting problems in NC have been proved 
to belong to one of these two classes. As the reader could have guessed, it is an 
open problem whether the inclusions given in Part 1 of the previous corollary 
are strict or not. Of course, the conjecture is that strict inclusion holds. 
In general, proof of membership in NC t or NC 2 is quite involved, mainly 
due to the fact the most of the problems over candidates to be in these classes 
involve sophisticated techniques of modular arithmetic or algebraic theory. 

Robustness of NC and Conclusions 
115 
4.6 Robustness of NC and Conclusions 
We have seen in Proposition 4.3 that there is a sort of hierarchy in the strongness 
of the unifonnity conditions; namely UE implies UBC, U E implies U E' and 
for circuits with depth greater than the log-square of the size, UBC implies 
U E" We have used the U E and U E' unifonnities to prove the relation between 
alternating machines and circuits. The following result, which follows from that 
relationship, is left as an exercise (Exercise 13). 
Theorem 4.5 Let z(n) = nn(l) andt(n) = {}(logn) befunctions such that given 
as input n in binary, log z(n) and t(n) are computable by a deterministic Turing 
machine in time O(logt(n». For any language A E {a, 1}*, the following is 
true: 
1. If A E UBc-SIZE,DEPTH (z(n), t(n» then 
A E U E-SIZE,DEPTH (z°(1)(n), O(max(t(n), log2 z(n») 
2. If A E UE.-SIZE,DEPTH (z(n), t(n» then 
A E U E-SIZE,DEPTH (zO(J)(n), O(t(n») 
This result has an important consequence, that is, the robustness of N C under 
definitions UE, UBC and UE •. Moreover, from Theorem 4.5 it also follows that 
for k ~ 2, the class NC k is also identical under these three definitions. Therefore, 
NC can be defined either in terms of UE, UBC, and UE• circuits, or in terms of 
alternating Turing machines, as we have seen in Section 4.5. 
In the bibliographical references, the interested reader will find references 
to equivalent characterizations of the class N C in terms of other models of 
"parallel machines"; SIMDAG's, WRAM's, etc. Therefore we can consider 
NC to be the class of problems having "nice" parallel solutions, where nice 
means that a problem in the class NC can be solved in logarithmic space, using 
a polynomial number of processors. In the light of the remark made after 
Proposition 4.1, when a problem is shown to be P-complete, that means it 
cannot be nicely parallelized (in the sense of NC). For instance, the circuit value 
problem was shown to be P-complete in Exercise 15 in Chapter 5 of Volume I, 
which means that unless NC = P, this problem cannot be solved in log-time 
using a polynomial number of processors. As indicated in the bibliographical 
references, there are other concepts of what it means to be nicely parallelizable. 
4.7 Exercises 
1. 
Prove that the family of circuits described in Examples 1, 2 and 3 are also 
U E' -uniform. 

116 
Unifonn Circuit Complexity 
2. 
Define ACi as the class of languages recognized by U Bo-uniform circuits, 
with unbounded fan-in d, of polynomial size and logi space. (i.e. ACi 
is the equivalent to NCi , but allowing each gate to have fan-in up to the 
total size of the circuit). Prove, 
(a) 
For all i ~ 0, ACi ~ NCi+l • 
(b) 
LOGCFL~ ACI 
3. 
Show the following functions are in ACo: 
(a) 
Binary addition function. 
(b) 
Boolean matrix multiplication. 
4. 
Prove that integer binary multiplication is in ACI. 
5. 
·Prove that every symmetric function belongs to NC I • (This exercise 
requires some knowledge of complexity of arithmetical operations). 
6. 
Prove that if ten) = !/(log n) and zen) = n{](\), we have 
UBo-SIZE,DEPTH (z(n), t(n» ~ ASPTI(log zen), t 2(n» 
7. 
Given two sets A and B, say that A is NCI-reducible to B (A -::;,~Cl B) iff 
there is a U Bo-uniform family of circuits C of depth log 71. for A, where 
the circuits en of C are allowed to have oracle gates for B. An oracle 
gate for B is a node with some sequence (gl, g2, ... , gm) of input gates; 
the gate yields the output value 1 if the string ala2 ..• am E B where ai 
is the value output by gj. For the purpose of defining the depth of Cn, this 
oracle node counts as depth flog m 1. Prove the following: 
(a) 
-::;,~Cl is reflexive and transitive. 
(b) For any k ~ 1, the class NC k is closed under -::;,~Cl. 
8. 
Prove the following problems are P-complete under -::;,~Cl -reducibility: 
(a) 
The circuit value problem. 
(b) The monotone circuit value problem. 
(c) 
The problem of finding the lexicographically first maximum clique 
in an undirected graph. 
9. 
A family of circuits C = (el, C2, ••• ) is P-uniform iff the function In ~ en 
is computable by a deterministic Turing machine, in time polynomial to n. 
Define the class of P-uniform circuits (PUNC) as the class of functions 
(languages) computed by P-uniform circuits of depth 10gO(I) n and size 
71.0(1). Prove, 
(a) 
PUNC is closed under log-space reduction. 
(b) 
PUNC is closed under -::;,~cl-reduction. 
10. 
Prove that any language A is in PUNC iff A is recognized by a log-space 
alternating Turing machine, which accesses its input only during the first 
o (log n) steps. 

Bibliographical Remarks 
117 
11. 
·Prove PUNC = NC iff every tally language in P is also in NC 
12. ·Prove EXPTIME = NEXPTIME iff all sparse sets in NP are in PUNC 
(Hint: Look at Exercise 17 of Chapter 5 in Volume I). 
13. Prove Theorem 4.5 in the text. 
4.8 Bibliographical Remarks 
The notion of uniform circuit complexity was first suggested by Borodin (1977). 
In Chapter 5 of Volume I and in Schnorr (1976) can be found a dual notion which 
obtains non-uniformity on Turing machines by giving them "oracles" depending 
on n. Our first uniformity measure defined in Section 2 is from Cook (1979). 
It is an extension of the uniformity defined by Borodin, whence the name UBe. 
The simulation of a a vector machine by a uniform circuit, Theorem 4.1, 
is adapted from Chapter 7 of Hong (1986). However, the idea of simulating 
program computations by circuits is an old one, see for example Chapter 6 
of Savage (1976) or Theorem 5.1 of Volume 1. Theorem 4.2 is from Borodin 
(1977). 
The other uniformity measures described in Section 3, as well as the com-
parison among them, are from Ruzzo (1981). From this work, we have taken 
the characterization of NC by alternating machines. The proof of Theorem 4.3 
is an hybrid of Ruzzo's original work and the proof in Sipser (1985). 
The class NC was first described in Pippenger (1979). The name NC was 
coined in Cook (1979) as a mnemonic for "Nick's class" in recognition of 
Nicholas Pippenger's contribution. Lately, there have been quite a few papers 
proving NC problems. It is impossible to list them all here; for the interested 
reader, good surveys are given by Cook (1981), Cook (1985), and by Bertoni, 
Goldwurm, Mauri, and Sabadini (1986). The proof that CFL ~ NC 2 is due to 
Ruzzo (1981). Relationships between context-free languages and NC l are given 
in Ibarra, Jiang, and Ravikumar (1988). Problem 5 is from Muller and Preparata 
(1975); see also Theorem 4.1 on page 76 of Wegener (1987). 
Besides the characterization of NC in terms of uniform circuits, and alter-
nating machines, NC has also been characterized in terms of other models of 
parallel machines; hadware modification machines, Cook (1981); SIMDAG's, 
Goldschlager (1981); P-RAM's, Stockmeyer and Vishkin (1984), and others. 
For a good survey see Vishkin (1983). A very nice characterization of the class 
NC l in terms of bounded-width polynomial-size branching programs is made 
in Barrington (1989). This characterization is interesting in the sense that it is 
similar to the characterization of DSPACE(log n) in terms of polynomial-size 
branching programs without width restrictions, and it seems to add new interest 
to the relation NC l ~ DSPACE(logn). See also Cook and McKenzie (1987). 
Unbounded fan-in circuits were studied by Furst, Saxe, and Sipser (1984). 
In this work, they proved ACo i NC l by proving that the parity function does 
not belong to ACo. The unbounded fan-in classes ACi were implicitly defined in 

118 
Uniform Circuit Complexity 
Chandra, Stockmeyer, and Vishkin (1982, 1984). The classes were fully treated 
in Cook (1985). Further characterizations of the classes ACi have been given 
by Stockmeyer and Vishkin (1984) and Venkateswaran and Tompa (1986). In 
section 3 of Karp and Ramachandran (1988), the interested reader will find a 
very nice survey on the relations between circuits and PRAMs. 
The '5:~Cl -reducibility was defined in Cook (1985). From this work, we have 
taken Problem 8.c. 
The class PUNC was first implicitly defined in Beame, Cook, and Hoover 
(1984) and in von zur Gathen (1984). This class was studied and the name 
coined in Allender (1985) and Allender (1986). From these last two works we 
have taken Exercises 9 to 12. 
One class which is becoming increasily important is Random NC, (RNC), 
where probabilistic techniques are combined with parallelism. We shall not enter 
into this subject; we refer the interested reader to Cook (1985) and Borodin, von 
zur Gathen, and Hopcroft (1983), among others. As a curiosity, we should 
mention, that while the maximum flow problem with edge capacities expressed 
in binary is P-complete, (Goldschlager, Shaw, and Staples (1982), and Lengauer, 
Wagner (1987». In Cook (1985) is reported that in the M. Sc. thesis of Feather 
it is proved that the max-flow problem when capacities are expressed in unary 
is in RNC. Similar results have been obtained for several other "important" 
P-complete problems. 
Finally, some people have defined alternatives to NC as the class of par-
allelizable problems; for instance, Vitter and Simons (1986) proposed the class 
PC of problems for which the ratio between parallel time (with a polynomial 
number of processors) and sequential time tends to O. They show that certain 
P-complete problems also belong to this class. Kruskal, Rudolph, and Snir 
(1988) proposed another class PE of parallelizable problems: those problems 
for which there are parallel algorithms which achieve polynomial speedup using 
a number of processors proportional to the speedup achieved. There is still a 
lot to be done in this area from the point of view of structural complexity. 

5 
Isomorphism and NP-completeness 
5.1 Introduction 
An interesting area of research in Complexity Theory is the study of the structural 
properties of the complete problems in NP. The analogy with the class of the 
recursively enumerable sets, provided by the polynomial time hierarchy, suggests 
that they might have properties similar to those of the r.e.-complete sets, like 
being pairwise isomorphic, in the appropriate sense; in fact, sufficient conditions 
are known for certain sets being isomorphic under polynomial time computable, 
polynomially invertible bijections, and no NP-complete set has been proved to 
be non-isomorphic to SAT in this sense. It was conjectured that all NP-complete 
sets are polynomially isomorphic, a statement which is known as the Berman-
Hartmanis conjecture. Several consequences follow from this conjecture; among 
them, of course, P =I NP. 
We present in this chapter the strong motivations for this conjecture. We 
start from a construction allowing one to prove the existence of polynomial time 
isomorphisms among given sets, under certain very general sufficient conditions; 
this is described in Section 5.2. In Section 5.3 we consider the particular case 
in which one of these sets is SAT, and study the related concept of "poly-
nomial cylinder", obtaining sufficient conditions for an NP-complete set to be 
isomorphic to SAT. Then we present in Section 5.4 some of the consequences of 
the Berman-Hartmanis conjecture which can be proven independently, and have 
great interest on their own, namely the theorems of P. Berman and S. Mahaney 
showing that no NP-complete set can be tally (resp. sparse) unless P = NP. 
5.2 Polynomial Time Isomorphisms 
In Section 3.3 of Volume I, definitions are given regarding the possibility of 
computing in polynomial time inverses of polynomial time computable functions. 
During this chapter, we will be concerned with computing the unique inverse of 
one-to-one polynomial time computable functions. Thus, we restrict the concept 
of invertible function as follows. 
Definition 5.1 
A function I is said to be (polynomially) 1-invertible if it is 
one-to-one and 1-1 is computable in polynomial time. 

120 
Isomorphism and NP-completeness 
No condition is imposed on f itself; however, all our 1-invertible functions 
will be polynomial time computable as well. 
The reader should keep in mind throughout this chapter, as we did in Sec-
tion 3.3 of Volume I, that functions are not necessarily total. However, notice 
that if a partial function is computable in polynomial time, then it is possible to 
decide in polynomial time whether the function is undefined on an input: simply 
exhaust the allotted computation time and check that no output is produced. 
The following technical definition will be also used. 
Definition 5.2 A function f is said to be length increasing if for all w it holds 
that (If(w)1 > Iwl). 
Now we shall define the concept of isomorphic problems under polynomial 
time mappings, which is central to this chapter. 
Definition 5.3 Two languages A E ;;* and B E r* are p-isomorphic (written 
A == B) if and only if a bijection f : ;;* -t r* exists such that: 
1. f and f- I are computable in polynomial time; 
2. f is a reduction from A to B. 
Notice that f- I must be a reduction from B to A. The main result in 
this section proves the existence of a polynomial time isomorphism among two 
sets, provided that they are interreducible via reductions which fulfill certain 
properties. In the next section we will discuss to what extent reductions having 
these properties are frequent. Those readers who are familiar with the Cantor-
Schroeder-Bernstein argument in set theory will recognize its strong influence 
in the proof. 
Theorem 5.1 Let A C ;;* and B c r* be languages, and let f be a reduction 
from A to Band g a reductionfrom B to A, both of them I-invertible and length 
increasing. Then A and Bare p-isomorphic. 
Proof. 
Since f and g are I-invertible, there exists a polynomial pen) bounding 
the time needed to compute any of the four functions f, g, I-I, g-I. Let us 
divide each of both ;;* and r* into two disjoint sets in the following way: for 
each x E ;;* consider the sequence 
and for each y E r* consider the sequence 
Since f and g are length increasing, we know that Ixl > Ig-l(x)1 > ... and 
Iyl > If-l(y)1 > .... This implies that both sequences are finite, and that each 
one has at most Ixl + 1 and IYI + 1 elements respectively. 

Polynomial Time Isomorphisms 
121 
Let us define RI as the set of x E E* such that the above sequence ends in 
an element of E* (i.e., it reaches a point where g-I cannot be applied), and let 
R2 = E* - RI. In the same manner, let SI be the set of all y in r* such that 
the sequence ends in an element of r*, and let S2 = r* - SI. 
Then we have: 
1. 
RI n R2 = 0 and RI U R2 = E*. 
2. 
SI n S2 = 0 and SI U S2 = r*. 
3. 
Given x E E*, it is possible to decide whether x E RI or x E R2 within 
(Ixl + 2)p(lxi) steps. 
4. 
Given y E r*, it is possible to decide whether y E SI or y E S2 within 
(iyl + 2)p(lyi) steps. 
This decomposition of E* and of r* can be depicted as in Figure 5.1. 
R1 
S2 
R2 
S1 
y 
-1 
q (x) 
p(z) 
Figure 5.1 
Iterations of 1-1 and g-1 
Points (3) and (4) are consequence of the fact that the sequence starting at x 
has a maximum of Ixl + 1 elements, and for each element we need to compute 
either I-lor g-I, with a cost of at most p(lxi) steps, or to see that we are at the 
end of the sequence by checking that the corresponding function is undefined, 
also with a cost of at most p(lxi) steps. 
Let us define the isomorphism between A and B: 
h(x) = {I(x) 
if x E RI 
g-I(x) if x E R2 
We must prove that indeed h has the desired properties: 

122 
Isomorphism and NP-completeness 
1. 
h is a total function, because RI U R2 = E*; 
2. 
h is one-to-one, because f and g-I are one-to-one; 
3. 
h is onto, because h(RI) = 52 and h(R2) = 51; 
4. 
h is I-invertible, because its inverse h- I is defined as 
which is well-defined and total; 
5. 
h and h- I are computable in polynomial time; and finally, 
6. 
h reduces A to B, because for x E RI, h(x) = f(x), which is a reduction 
from A to B, and for x in R 2, h(x) = g-I(x), which is the (total and well-
defined) inverse of a reduction from B to A, and hence is a reduction 
from A to B. 
0 
5.3 Polynomial Cylinders 
The main theorem in the last section imposes two strong conditions which re-
strict its immediate application. In the following we study some concepts that 
make easier the testing of such conditions. These are the concept of "polynomial 
cylinder" and the three forms of "paddable sets". Some of them will be men-
tioned again in the next chapter. Here the notation A x B denotes the following 
form of cartesian product: A x B = {(x, y) I x E A 1\ Y E B}. 
Definition 5.4 For any set A, its cyfindrification is the set A x E*. 
Definition 5.5 
A set A is a polynomial cylinder (p-cylinder for short) if it is 
polynomially isomorphic to its cylindrification: A == A x E*. 
We shall see shortly that polynomial cylinders can be characterized by the 
property that, given a string in the set, there is a "quick" way of producing many 
other strings in the set. The function that provides these strings is usually called 
a padding function. Some extra conditions must be imposed on this function. 
We present below a more formal statement of this property. 
Definition 5.6 
Given a set A, a padding function for A is a polynomial time 
computable one-to-one function pad such that for all strings x, y, it holds that 
pad«(x, V)~ E A if and only if x E A. 
Recall that a function f is honest if and only if there is a polynomial p such 
that for every input x, Ixl ::; p(lf(x)l), i.e. f does not "shrink" the words too 
much (Definition 3.4 in Volume I). Of course, every polynomial time invertible 
function must be honest (proposition 3.3 of Volume I), because x must be 
computable from f(x) in polynomial time. 

Polynomial Cylinders 
123 
Definition 5.7 A set is paddable if and only if it has a padding Junction. A 
set is honestly paddable if and only if it has an honest padding Junction. A set 
is 1-invertibly paddable if and only if it has a i-invertible padding Junction. 
The following theorem clarifies the relationship between paddability and p-
cylinders and states the properties of polynomial cylinders which we will use. 
Theorem 5.2 The Jollowing statements are equivalent: 
(a) A is a p-cylinder. 
(b) There exists a set B such that A == B X E·. 
(c) A is i-invertibly paddable. 
(d) For every set B, if B ~m A then B ~m A via a i-invertible, length-
increasing Junction. 
(e) 
A x E· ~m A via a i-invertible, length-increasing Junction. 
Proof. 
Taking as B the same set A shows that (a) implies (b). To show that (b) 
implies (c), let A be p-isomorphic to B x E* via a polynomial time computable 
and invertible bijection h. By composing it with the projection functions, obtain 
two functions hi and h2 such that hex) = (hi (x), h2(x)} and define the function 
pad«(x, y}) = h-I«(hl(x), (h2(X), y}}) 
It is a padding function for A, since for every y, 
x E A if and only if 
hex) E B x E* 
if and only if 
h-I(x) E B 
if and only if 
(hi (x), (h2(x), y)} E B x E* 
if and only if 
h-I«(hl(x), (h2(X), y}}) E A 
To invert this padding function on a value z = pad«(x, y}), perform the 
algorithm of Figure 5.2. 
input z 
compute h(z), which is (hl(x), (h2(x), y}} 
recover from it the values of hl(x), h2(X), and y 
obtain x as h-I«(hl(x), h2(X)}) 
output (x, y) 
end 
Figure S.2 Inverting a padding function 
To prove that (c) implies (d), let pad be the I-invertible padding function 
for A, and let p be a nondecreasing polynomial bounding the time needed to 

124 
Isomorphism and NP-<:ompleteness 
compute pad-I. Let B be a set reducible to A via the function f. Define the 
following function: g(y) = pad«(f(y), yHY'(IIII>)). This function is a reduction 
from B to A, because yEA if and only if fey) E B, and by the properties of 
padding functions this holds if and only if g(y) E B. 
To invert g, compute g-I(z) as follows: compute pad-I(z), which equals 
(x, t) for some x, t; forget x, take t, which should be of the form wlOm , throw 
away the tail 10m , and output w. 
Finally, we show that 9 is length-increasing. First observe that, since p 
bounds the time to compute pad-I, we know that for every x, u, 
lui ~ \(x, u)\ = \pad-I{pad«(x, u))\ ~ p(\pad«(x, u)\ 
Thus, for u = yl()P(IIIi> and x = fey) we have 
p(\y\) ~ \y 1 ()p(11i I) \ ~ p(\pad«(f(y), y1()P<lIII»)\) = p(\g(y)\ 
Since p is nondecreasing, p(\y\) ~ p(\g(y)\) implies that \y\ ~ \g(y)\. 
It is always true that A x E* ~m A, and therefore the implication from (d) to 
(e) is immediate. Finally, to show that (e) implies (a), observe that A is always 
reducible to A x E* via a l-invertible, length-increasing reduction. Since by 
hypothesis the same holds for the converse reduction, we can apply Theorem 5.1 
to show that they are polynomially isomorphic. Thus A is p-isomorphic to its 
cylindrification, which is our definition of polynomial cylinder. This completes 
the proof of the theorem. 
0 
The interest of the notion of p-cylinder is made evident by the following 
theorem, which is easily derived from the above characterizations. Recall that 
the m-equivalence relation ==m was defined in Volume 1. 
Theorem 5.3 If A ==m B, and A and Bare p-cylinders, then A == B. 
Proof Since A ~m B, and B is a p-cylinder, using (d) in Theorem 5.2, 
there is a I-invertible, length-increasing reduction from A to B. Symmetrically, 
since B ~m A, and A is a p-cylinder, there is a I-invertible, length-increasing 
reduction from B to A. Thus, A and B are reducible to each other by such 
reductions, and by Theorem 5.1 they are p-isomorphic. 
0 
Now we can obtain a characterization of the sets which are p-isomorphic to 
SAT in terms of p-cylinders from the two following easy lemmas: 
Lemma 5.1 If A is a p-cylinder and A == B, then B is a p-cylinder. 
Proof Let pad be the I-invertible padding function for A, and let h be the 
isomorphism. Then it can easily be seen that the function 
pad'«(x, y) = h{pad«(h-I(x), y)) 
is a I-invertible padding function for B. 
o 

Sparse Complete Sets 
125 
Lemma 5.2 SAT is a p-cylinder. 
Proof We will construct a I-invertible padding function for SAT. Let us 
present a couple of auxiliary notations. Given a formula F, denote by reF) 
the maximum of the indices of the variables occurring in F, and denote by 9 the 
function defined as follows: for a word y over {O, I} of length n and a number 
r, the value of g(y, r) is the conjunction of literals 
g(y, r) = Zl A Z2 A •.. A Zn 
where each literal is 
. _ { Xr+l+i 
if the i th bit of y is 1 
Z, -
Xr+l+i 
if the i th bit of y is 0 
Then a I-invertible padding function for SAT is obtained as follows: 
pad«(x, y) = x A (Xr(x)+l V Xr(x)+l) A g(y, r(x» 
It is obvious that pad«(x, y) encodes a satisfiable formula if and only if 
x encodes a satisfiable formula, and that both x and y can be recovered easily 
from pad«(x, y) by looking for the separating disjunction (Xr(x)+l VXr(x)+I). 
0 
Now we can derive the following characterization: 
Theorem 5.4 An NP-complete set A is p-isomorphic to SAT if and only if it 
is a p-cylinder. 
Proof Since SAT is a p-cylinder, every set which is p-isomorphic to SAT must 
be a p-cylinder. Conversely, if a p-cylinder A is NP-complete, then A ==m SAT. 
Since both are p-cylinders, combining Theorem 5.1 and Theorem 5.2(d), we 
obtain A == SAT. 
0 
Therefore, to prove that an NP-complete problem is p-isomorphic to SAT, it 
is sufficient to show that B has a I-invertible padding function. In this manner, 
most of the known NP-complete problems have been proved to be p-isomorphic 
to SAT, thus adding support to the Berman-Hartmanis conjecture. 
5.4 Sparse Complete Sets 
If the Berman-Hartmanis conjecture is true then P i NP, because a finite lan-
guage cannot be isomorphic to any of the known NP-complete languages. This 
seems to indicate that a direct attack to prove the conjecture must be at least 
as hard as to prove that P i NP. On the other hand, it can be shown that one 
characteristic of the p-isomorphic NP-complete problems is that they have quite 

126 
Isomorphism and NP-completeness 
similar densities, in the sense presented in Chapter 1 of Volume I. Therefore, it 
is not surprising that people looked at families of "not too dense" languages in 
an effort to disprove the Berman-Hartmanis conjecture. 
Recall from Chapter I of Volume I the definition of the sparse sets. A set is 
sparse if the number of words in it up to length n is bounded by a polynomial. 
The interest of this notion in our present context arises from the fact that SAT 
is not sparse. In fact the following proposition proves a stronger result. 
Proposition 5.1 No nonempty p-cylinder is sparse. 
Proof Let A be sparse. Then there is a polynomial pen) bounding the census of 
A: CA(n) ::; pen). Assume also that A has a padding function pad, computable 
and I-invertible in polynomial time and such that for every x, y, pad«(x, y) E A 
if and only if x E A. 
Let Xo E A. Since pad«(xo, y) is computable in polynomial time, there is 
a polynomial q such that Ipad«(xo, y)1 ::; q(lyi) (otherwise there would be no 
time to write down the output!), which implies pad«(xo, E$n) C E$q{n). Of 
course, we also have pad«(xo, E$n) C A. Then 
which implies Ipad«(xo, E$n)1 < p(q(n»; this means that pad is a function 
mapping an exponential number of words in E$n into a polynomial number of 
images. Therefore pad cannot be one-to-one, which in turn implies its non-l-
invertibility. 
0 
As a corollary to the last proposition, together with Lemma 5.2, we can state 
the following: 
Corollary 5.1 No sparse set can be p-isomorphic to SAT. 
This last result led Berman and Hartmanis to state also the weaker conjecture 
that unless P = NP, there do not exist sparse NP-complete sets. We present in 
the remainder of this chapter the (positive) solution to this weaker conjecture. 
We start with a related result. 
Theorem 5.5 If SAT::;m S for a sparse set S, then P = NP. 
Proof Assume that SAT is reducible to some sparse set S via f. We will 
present an algorithm that using f solves SAT in polynomial time. We present 
first an intuitive description of the main ideas the algorithm uses. We formalize 
our argument afterwards. 
The algorithm is given a formula F to be decided. Then it constructs and 
traverses simultaneously a tree in a depth-first (or "preorder") manner. Each 
node in the tree is just a partial instance of F, i.e. a formula obtained from F 
by assigning values "false" or "true" to some of its variables. The sons of each 

Sparse Complete Sets 
127 
node G, named Go and Gl> are the result Of substituting the values "false" and 
"true", respectively, for the leftmost variable in G. The leaves of the tree are 
fonnulas without variables, and hence are either "false" or "true". (This requires 
that a simplification is made when each leaf is reached.) 
The algorithm just traverses the tree looking for a node "true". If it is found, 
then F is satisfiable. We have to guarantee that if a node "true" exists, then a 
node "true" is found by the algorithm; hence if F is satisfiable then this fact will 
be detected by the algorithm. This will prove the correctness of the algorithm. 
However, the tree has exponential size. To keep the running time of the 
algorithm bounded by a polynomial in IFI, we will prune it using the following 
property of the reduction: if node G has been shown unsatisfiable, and both G 
and G' are mapped to the same value f(G) = f(G'), then node G' is unsatisfiable. 
This is because G rt SAT implies f(G) E S, and then f(G') = f(G) E S implies 
G' rt SAT. 
Nodes already shown unsatisfiables will be called "dead". 
The remaining 
nodes which have been constructed are "alive" until they are declared dead by 
any of the following "three rules of death": 
1. 
If G is a false node then it is dead. 
2. 
If both sons Go and G1 of a node G are dead, then G is dead. 
3. 
If f(G) = f(G') and G' is dead, then G is dead. 
Note that the rules are correct in the sense that if at any moment all dead 
nodes are unsatisfiable, then new nodes declared dead are also un satisfiable. 
Hence, since at the beginning of the algorithm every node is alive, we know 
that only unsatisfiable nodes are declared dead during the computation. 
In order to keep the running time polynomially bounded, we restrict our 
algorithm so that no search is made below a dead node. Since dead nodes 
are unsatisfiable, we know that no node true can be skipped because of this 
restriction. This ensures the correcness of the algorithm. It is described fonnally 
in Figure 5.3. 
If F is accepted then a satisfying assignment can be obtained from the path 
to the node we are visiting at the moment of halting; thus F is satisfiable. If F 
is rejected then it is unsatisfiable, because only unsatisfiable nodes are declared 
dead. Thus the output of the algorithm is correct. To show that its running time 
is polynomial, we show that only polynomially many nodes are visited. This 
implies that visiting each node takes also polynomial time, because only visited 
nodes are examined during the "propagation of death"; thus the total time is 
polynomially bounded. 
Consider the subtree V of visited nodes. Its height is bounded by the number 
of different variables in F, which in tum is bounded by IFI. In V the following 
property holds: if G and G' are unsatisfiable internal nodes of V, and f(G) = 
f(G'), then G and G' are both in the same path from the root to a leaf. To 
prove this assertion, assume it is false and assume further that G is visited first; 

128 
Isomorphism and NP-<:ompleteness 
Algorithm P-time-satisfiability: 
input F 
explore-the-tree-below(F) 
end 
Explore-the-tree-below(G): 
visit(G) 
if G has been declared dead then return 
construct Go and explore-the-tree-below(Go) 
comment: the last exploration may declare G dead 
if G has been declared dead then return 
construct G) and explore-the-tree-below(G) 
return 
Visit(G): 
if G is "true" then halt in an accepting state 
compute f(G) 
apply the three rules of death until no more visited 
nodes can be declared dead 
if the root F is declared dead during this process 
then halt in a rejecting state 
return 
Figure 5.3 A polynomial time algorithm for SAT 
since it is unsatisfiable, G is declared dead before or during the exploration of 
the subtree below G, hence before visiting G'. The visit to G' will declare it 
dead by the third rule of death, and G' will remain a leaf. 
Call a node G of V penultimate if G is internal, unsatisfiable, and every son 
of G is a leaf of V. As no two penultimate nodes are in the same path from 
the root, by the property just stated, every two penultimate nodes must yield 
different values under f. Since there are only polynomially many values of f 
for unsatisfiable nodes (by the sparseness of S), there are only polynomially 
many penultimate nodes. 
We classify now the unsatisfiable nodes in V as follows: 
1. 
Polynomially many penultimate nodes; 
2. 
At most IFI other internal nodes in the path to each penultimate node: 
polynomially many such internal nodes (because the height of the tree is 
at most IFI>; 
3. 
Two leaves for each penultimate node: polynomially many such leaves; 

Sparse Complete Sets 
129 
4. 
At most one leaf for each non-penultimate internal node: polynomially 
many such leaves. This case arises when only one of the sons belongs to 
the path to a penultimate node. 
As every unsatisfiable node of V is in one of these four classes, there are 
polynomially many unsatisfiable nodes in V. Satisfiable nodes stop the search as 
soon as they are detected, and hence only one path of satisfiable nodes (from the 
root to a leaf "true") is fully constructed. No other satisfiable nodes are visited. 
Thus, IFI is a bound on the number of satisfiable nodes visited. We have proved 
that V has only polynomially many unsatisfiable nodes and polynomially many 
satisfiable nodes; hence V has polynomial size. 
0 
Thus, we have solved the "symmetric" statement of the Berman-Hartmanis 
conjecture: 
Corollary 5.2 If there is a co-sparse NP-complete set, then P = NP. 
Also, we have solved the Berman-Hartmanis conjecture for a subset of the 
sparse sets: the tally sets. 
Corollary 5.3 If there is a tally NP-complete set T, then P = NP. 
Proof Let T be NP-complete. Denote T the complement of T with respect 
to the one-letter alphabet over which T is written. Of course T is m-equivalent 
to the complement of T over any other bigger alphabet; hence, T is co-NP-
complete and sparse. By the last theorem, P = NP. 
0 
We have some more arguments before we can finally prove that there are no 
sparse NP-complete sets unless P = NP. Recall from Chapter 5 of Volume I 
the concept of P/log-m-reducibility: 
A -::::~/109 B if for a polynomial time 
computable function 9 and a logarithmically bounded advice function f, and for 
every x, x E A if and only if g({x,/(Olxl»)) E B. First we prove the following 
extension of Theorem 5.5. 
Theorem 5.6 If SAT -::::;/109 S for a sparse set S, then P = NP. 
Proof Let f be the reducing function. We know that for each n there is a 
word w of length logarithmic in n such that for x of length n, x E SAT if 
and only if f({x, w) E S. For each word w of length logarithmic in n, denote 
gw(x) = f«(x, w). 
Consider the algorithm P-time-satisfiability given in the proof of Theo-
rem 5.5. Its running time is a polynomial depending only on the polynomial 
bounds on the census of S and on the computing time of the reduction to S. 
Thus, there is a polynomial p, which can be fixed a priori, such that for the 
appropriate word w, and using gw as a reduction, the algorithm will correctly 
decide SAT within time p. This means that if no satisfying assignment has been 
found within p(n) steps, then the input is unsatisfiable. 

130 
Isomorphism and NP-completeness 
Of course our problem is that we do not know which one is the appropriate 
w. But we can cycle over all potential words w, because there are polynomially 
many of them. So we can design the algorithm presented in Figure 5.4. 
input F of length n 
for each word w of length logarithmic in n do 
perform p(n) steps of the algorithm P-time-satisfiability, 
using gw as a reduction 
if it halts in an accepting state then accept 
else continue with the next w 
reject 
end 
Figure 5.4 Iterating the algorithm of Figure 5.3 
Obviously, this algorithm runs in polynomial time. If it accepts an input, 
then it must have found a satisfying assignment, and hence the input is satisfi-
able. On the other hand, if an input has a satisfying assignment, then the loop 
corresponding to the correct w will find it within p(n) steps and lead us to ac-
ceptance. Thus, this algorithm accepts SAT in polynomial time, which implies 
p =NP. 
0 
Note the similarity of this argument with the proof of Theorem 5.7 in Vol-
ume I. 
The main arguments for the result have been presented. Now we just have 
to combine them. Recall from Theorem 5.9 of Volume I that a kind of "closure 
under complements" property of the Pjlog-m-reduction class of SAT holds, if 
restricted to the sparse sets in NP. We will use this result in the proof of the 
next theorem. 
Theorem 5.7 If NP has a sparse complete set, then P = NP. 
Proof. 
Let S be NP-complete. As S is in NP, we have that S Sm SAT, which 
implies S Sf,!log SAT. By Theorem 5.9 of Volume I, S S~/Iog SAT. 
On the other hand, S is NP-complete, and S is co-NP-complete. Thus, 
SAT < S <P/log SAT < S 
_m 
-m 
_m 
which implies by transitivity that SAT Sf,!log S. By Theorem 5.6, we obtain 
that P = NP, and the proof is finished. 
0 
It is not necessary to assume that S is NP-complete. Just the NP-hardness 
of S is enough to obtain the equality of P and NP. It can be shown as follows. 
Corollary 5.4 If there is a sparse NP-hard set S, then P = NP. 

Exercises 
131 
Proof We prove that if S is sparse and NP-hard, then a sparse set S' is NP-
complete. Let I be the reduction from SAT to S. Consider the following set: 
S' = {j(w)10Iwl ! w E SAT}. 
First observe that SAT is m-reducible to S' via the mapping 9 defined as 
g(w) = l(w)10Iwl. Second, observe that S' is also sparse, because the words 
of length up to n must have the form l(w)IOIWI where I(w) is a word in S of 
length up to n, and there are only polynomially many of them by the sparseness 
of S. And third, it only remains to show that S' is in NP. The algorithm in 
Figure 5.5 decides S in nondeterministic polynomial time; this completes the 
proof. 
0 
input z 
check that z = x IOn 
guess nondeterministically a word w of length n 
check that I(w) = x 
check nondeterministically that w E SAT 
if all the checks are successful then accept 
Figure 5.5 A nondeterministic algorithm for a sparse set 
5.5 Exercises 
1. 
Show that CLIQUE, the problem defined and proved NP-complete in 
Chapter 3 of Volume I, is a p-cylinder. Make a list of other NP-complete 
problems you know, and check for all of them whether they are p-cy linders. 
2. 
Show that, for each k, the problem k-QBF defined in Exercise 2, Chapter 8 
of Volume I, is a p-cylinder. 
3. 
Fix any natural number k. A set A E NP is k-creative if and only if there 
is a polynomial time computable function I (called a productive function 
for A) such that, for every nondeterministic machine M running in time 
k·n k + k, I(M) is accepted by M if and only if I(M) is in C. (We are 
identifying here the machine AI with its encoding as a word over E.) Show 
that for every k, every k-creative set C is NP-complete. Furthermore, 
show that if C has a 1-1 productive function then it is complete under 
1-1 reductions; that if C has a 1-1 and honest productive function then it 
is complete under 1-1, honest reductions; and that if C has an invertible 
productive function then it is complete under invertible reductions. 
4. 
Let I be an honest 1-1 function. For each natural number k, define: 
J«k, f) = {j(M) !.lIIis a nondeterministic machine which accepts I(M) 
in less than k'I/(M)jk + k steps} 
Show that J«k, f) is k-creative, with I as its productive function. 

132 
Isomorphism and NP-completeness 
5. 
Show that if C is a k-creative set for some k, and C has an invertible 
productive function, then C is a p-cylinder. 
6. 
The conjunctive truth-table reducibility was defined in Exercise 20 of 
Chapter 4 of Volume I. Show that if SAT is conjunctively tt-reducible 
to a sparse set then P = NP. 
7. 
Assume that SAT is in P(T) for some tally set T, and furthermore that 
the machine witnessing this fact queries its oracle at most k times on 
each input, where k is some fixed constant. Show that if this is true then 
P=NP. 
8. 
SN-m-reducibility was defined in Exercise 27 of Chapter 3 of Volume I. 
Show that if SAT is SN-m-reducible to a sparse set then NP = co-NP. 
9. 
Theorem 5.5 was proved in the text by constructing a recursive algorithm 
that explores depth-first a subtree of the computation tree of the given 
formula. Give a new proof by exploring the same tree of subformulas 
breadth-first. Hint: Find a polynomial r(n) such that no more than r(n) 
different labels can correspond to un satisfiable nodes. Keep a list of nodes 
to be expanded, all at the same level. At each loop, move down by one 
level, expanding all nodes; then prune those having repeated labels, and 
if more than r(n) of them are left then preserve only r(n) + 1 and discard 
the others. 
5.6 Bibliographical Remarks 
The study of polynomial time isomorphisms between NP-complete sets, mirror-
ing the similar work on L\-complete sets in Recursive Function Theory, was 
initiated by L. Berman and Hartmanis (1977). Definition 5.1 and Theorem 5.1 
appear in this work. Also, the sufficient condition of paddability is from this 
reference, as well as Theorem 5.4 and the results needed for its proof. Our 
presentation (in terms of p-cylinders) is based also on Young (1983). 
Many natural NP-complete problems have been shown to be p-cylinders, 
as we suggest in Exercise 1. In particular, L. Berman and Hartmanis (1977), 
and Hartmanis and L. Berman (1978), contain many proofs that known NP-
complete sets are p-cylinders. The conjecture that all the NP-complete sets are 
polynomially isomorphic appears in L. Berman and Hartmanis (1977). Advances 
in this direction have been obtained in Allender (1988), using a much more 
restrictive concept of reducibility. 
The weaker conjecture that no sparse NP-complete sets exist unless P = NP 
required a long trail of research. The basic construction is due to P. Berman 
(1978), who proved the statement that we have presented as Corollary 5.3. Our 
presentation follows Fortune (1979), who proved our Corollary 5.2, and Yap 
(1983). 

Bibliographical Remarks 
133 
It should be mentioned that the Berman-Hartmanis conjecture has been coun-
tered in Joseph and Young (1985) with the contrary conjecture that not all the 
NP-complete sets are poiynomially isomorphic. The basis for this counter-
conjecture is on the grounds of the k-creative sets defined in Exercises 3, 4, 
and 5, extracted from this reference. They prove that the existence of non-p-
cylinders among the k-creative sets can be related to the one-way (i.e. honest 
but not invertible) polynomial time computable functions (see Exercise 5). Ko, 
Long, and Du (1986) provide a stronger relationship along this line by consider-
ing complete sets for exponential time. However, as shown by relativizations in 
Hartmanis and Hemachandra (1987), these relationships are not strong enough to 
ensure that existence of one-way functions implies existence of non-isomorphic 
NP-complete sets. More information appears in Kurtz, Mahaney, and Royer 
(1989). 
The solution to the weaker conjecture about sparse NP-complete sets, that 
we present as Theorem 5.7, was achieved by S. Mahaney (1982). We have 
presented his proof in a somewhat different form, although the arguments remain 
essentially the same. He did not define the P/log reducibility, and hence could 
not state our Theorem 5.6 nor Theorem 5.9 of Volume I, that we have used in 
the proof. However, he actually proved these results, for all our proofs follow 
his arguments. Corollary 5.4 is also from Mahaney (1982). 
The same technique used in Fortune (1979) and Mahaney (1982) has been 
applied to obtain similar, stronger results about other reducibilities. In this way, 
Meyer and Paterson (1979), Ukkonen (1983), Yap (1983), and Yesha (1983) have 
shown several statements of this kind. We propose some of them as exercises: 
Exercise 6 appears in Ukkonen (1983) and Yap (1983), Exercise 7 in Ukkonen 
(1983), and Exercise 8 in Yap (1983). The idea of proving Fortune's theorem 
by breadth-first search, as proposed in Exercise 9, was suggested by Osamu 
Watanabe (personal communication). 

6 
Bi-Immunity and Complexity Cores 
6.1 Introduction 
In this chapter we present several interesting results regarding bi-immunity and 
related notions. We characterize the sets which are bi-immune for P by several 
different properties. A stronger notion is defined, and a construction of a set 
having this property is presented. Then it is shown that several properties of 
polynomial time m-reducibility and of the sets complete for this reducibility can 
be deduced from the existence of bi-immune sets. 
Complexity cores, proper complexity cores, and levelability are the other 
notions studied here. We present the most important results regarding complexity 
cores and prove related results about the structure of the "natural" problems one 
wishes to consider in the study of complexity classes. 
It is possible to characterize bi-immunity and complexity cores in terms of 
"special case solutions". Intuitively, an algorithm solves a special case of a set 
if every word accepted by the algorithm is in the set, and every word rejected by 
the algorithm is not in the set; however, the algorithm is allowed to output "?". 
We will use in the text only this intuitive meaning of "special case", which can 
be taken also as a form of approximation. The formalization of the relationship 
between this kind of approximation and bi-immunity, as well as other notions, 
is left for the exercises (Exercises 3 and 4). 
All these concepts tum out to be useful tools, and help to clarify the relation-
ship between our complexity classes and the lattice of the sets under inclusion 
modulo finite variants. 
6.2 Bi-Immunity, Complexity Cores, and Splitting 
Recall from Chapter 1 of Volume I that a set A is C-immune for a class C if 
and only if it is infinite and every subset of A belonging to C is finite, and that 
a set A is C-bi-immune if and only if both A and A are C-immune. 
Let us begin our study of bi-immunity by presenting one of the ways of 
characterizing bi-immunity: the notion of a complexity core for a set. 
Definition 6.1 Let A be any recursive set. An infinite set X is a complexity 
core for A if for every machine !If that accepts A and every polynomial p there 

Bi-Immunity, Complexity Cores, and Splitting 
135 
are at most finitely many x E X such that the number of steps of M on x is 
bounded by p(lx I). 
Let us discuss the interest and the meaning of this notion. Observe first that 
finite sets trivially fulfill the condition, and therefore would be complexity cores 
for every set. This is why the infinity condition is imposed on X, which is the 
nontrivial, interesting case. 
Thus, for an infinite X, to be a complexity core for A means just that the 
set X encompasses infinitely many instances in which any algorithm for A will 
find difficulties, and will need more than polynomial time. At first, it may be 
the case that no polynomial time algorithm solves A, but each algorithm can 
solve a "big piece" of it, so that every infinite set of instances is solved by some 
fast algorithm. Such a set cannot have a complexity core. However, we will 
show below that every set not in P has a complexity core. Thus, it cannot be 
the case that every set of instances can be solved fast: there must be a set of 
instances which is hard for every correct algorithm, forcing it to spend more 
than polynomial time. 
Let us point out the following relationship: 
Proposition 6.1 Let A be a recursive set. A is bi-immune for P if and only if 
E· is a complexity core for A. 
Proof Assume that A is not bi-immune for P. Hence, there is an infinite 
subset B of A or of its complement which is in P. Any complexity core of A 
must be contained in the complement of B. because there is an algorithm that 
decides A and needs only polynomial time to decide the infinite set B: if the 
input is in B, accept (or reject in the case B ~ A); else start any other algorithm 
for A. 
Conversely, if E· is not a core then there is an infinite set of words in which 
it is possible to decide A within polynomial time, say pen), by some machine 
A1. Either the set of inputs accepted by M within time pen), or the set of inputs 
rejected by A1 within time pen), or both, is infinite; hence A, or A, or both, has 
an infinite subset in P. 
0 
The existence of bi-immune sets will be shown in the next section, using a 
different characterization. 
We can also relate this notion to the notion of "splitting". Consider the lattice 
of set inclusion modulo finite variants. Incomparability of two sets requires that 
neither one is a finite extension of a subset of the other. The complement 
of a set is obviously incomparable with it. Splitting is a stronger form of 
incomparability, in which we require that both sets, and their complements, are 
pairwise incomparable. This notion is useful when studying classes closed under 
complements, like the class P. 
Definition 6.2 An infinite set A splits an infinite set B if and only if each of 
the four sets An B, An B, An B, and A n B is infinite. 

136 
Bi-Immunity and Complexity Cores 
It can be shown that for every recursive infinite set A there is a set B which 
splits A and is decidable simultaneously within logarithmic space and real time 
(see Exercise 2). The relation of this concept with bi-immunity is the following: 
Proposition 6.2 Let C be a class closed under complements and under finite 
variations. Then A is C-bi-immune if and only if A splits every infinite set in C. 
Proof. If A is not C-bi-immune then there is an infinite subset of A or of A in 
C, and this subset is not split by A. Conversely, if A does not split the infinite 
set Bin C, then one of the four intersections indicated in Definition 6.2 is finite. 
Assume that this holds for An B; since C is closed under complements, all of 
the other cases reduce to this one by renaming A and/or B. Then taking out 
from B this intersection we obtain an infinite subset of A, which is in C by the 
closure of C under finite variations. 
0 
A natural class to which to apply this proposition is P. We obtain another 
characterization of the P-bi-immune sets. 
Corollary 6.1 A is P-bi-immune if and only if A splits every infinite set in P. 
6.3 Bi-Immune Sets and Polynomial Time m-Reductions 
The main theorem in this section presents the important relationship between 
bi-immunity and polynomial time reductions: the functions reducing bi-immune 
sets to arbitrary sets must be finite-to-one. We will use the following notations. 
Definition 6.3 
(a) A function I is finite-to-one (briefly F-1) if and only if, for every element 
x in the range of I, 1-1 (x) is a finite set. 
(b) A function is 1-1 a.e. (almost everywhere) if and only if the set of pairs 
{(x, y) I x =I y, but I(x) = I(y)} 
is finite. 
Obviously, every 1-1 a.e. function is finite-to-one. The 1-1 a.e. functions are 
a particularly interesting case of the F-l functions. 
Theorem 6.1 A set A is P-bi-immune if and only if every polynomial time 
reductionfrom A to any other set B is F-1. 
Proof. 
If A (respectively A) contains an infinite subset D E P, then construct 
a polynomial time computable function that maps every element of D to some 
fixed point of A (respectively A), obtaining a reduction from A to itself which 
is not F-l. 

Bi-Immune Sets and Polynomial Time m-Reductions 
137 
Conversely, assume that f is a reduction from A to some set B, and that 
f- 1(b) is infinite for some word b. Observe that f- 1(b) E P, because in order to 
test membership of x it suffices to compute f(x) and test whether f(x) = b. If 
bE B, then f- 1(b) is an infinite subset of A; if b ¢ B, then f- 1(b) is an infinite 
subset of A; in either case, A is not bi-immune, because f- 1(b) is an infinite 
subset in P. 
0 
This motivates the following definition, which turns out to be a useful concept 
in that a built-in diagonalization is implicit in the definition. In many cases 
the use of one of these structural bi-immunity properties yields easy proofs of 
interesting results, as we shall see in the applications in this section. 
Definition 6.4 A set A is strongly bi-immune if and only if every polynomial 
time reduction f from A to any set B is 1-1 a.e. 
Observe that A is (strongly) bi-immune if and only if its complement A 
is also (strongly) bi-immune. Our main goal is now to show the existence of 
strongly bi-immune sets. 
Theorem 6.2 There are strongly bi-immune sets A decidable within exponen-
tial time. Such a set can be constructed so that all the words in A have different 
lengths. 
Proof The set A is constructed in a stage-by-stage way. The key idea consists 
of keeping a list R of still unsatisfied goals. Some of them will offer only 
finitely many chances to be satisfied, and may be left unsatisfied; however, the 
construction will make sure that every goal which is infinitely often satisfiable 
is eventually satisfied. 
Each of our goals consists of trying to prove that a polynomially computable 
function is not a reduction from the constructed set to any other set. This 
is achieved by finding two words having the same image under the function, 
adding one of the words to the constructed set A, and keeping the other out of 
A. Indeed, if f reduces A to any other set B then f(x) = f(y) implies that 
x and y are either simultaneously in A or simultaneously in A, depending on 
whether f(x) E B. Now, if a given polynomial time computable function f is 
not 1-1 a.e. then it presents infinitely many such pairs and we will have a chance 
to ensure that it is not a reduction from A to any other set. 
At stage n we decide whether the nth word of E*, w(n), enters A or stays 
out of A. We search through a finite but unboundedly increasing number of 
reductions for a reduction f and for a smaller word x having the same image 
under f as w(n). When we find it, then w(n) enters A if and only if x was 
prevented from entering A. 
Let Tko kEN, be an enumeration of polynomial time transducers computing 
every polynomial time computable function. Let tk be the polynomial time 
bound of Tk• We assume that the time bound tk grows at least as fast as tk-Io 
and that it is nondecreasing. The construction is as indicated in Figure 6.1. 

138 
Bi-Immunity and Complexity Cores 
stage 0 
A:= 0 
R:= 0 
k := 1 
stage n (n > 0) 
end 
iftk(lw(n)i)::; 2/w(n)/ then R:= Ru {k}; k:= k + 1 
if there is an i E R and a x < wen) such that Tj(x) = Tj(w(n» 
then let io be the smallest such i, and 
let Xo be the smallest corresponding x 
if Xo tt A then A := AU {wen)} 
R:= R - {io} 
Figure 6.1 Construction for the proof of Theorem 6.2 
Next we shall show that the construction can be performed in exponential 
time and that the constructed set only admits 1-1 a.e. reductions. First note that, 
in order to decide whether wen) is in A, we must perform the first n stages 
of the construction: this is a total of 2/w(n)/ stages. Each stage needs at most 
O(2/w(n)/) steps to be performed. The reason is that i only enters R when this 
is enough time to perform the computation of 1';. Thus, linear exponential time 
is enough for finishing the construction up to stage n. 
Assume now that Tk computes a function I which is not 1-1 a.e. Sooner or 
later, k will be the smallest index in R such that an example of "non-l-1-ness" 
is found. At this stage, one word is left in A and another out of A, both having 
the same image by I. Hence I cannot be a reduction from A to any other set. 
Thus, the only reductions allowed from A are 1-1 a.e. 
Finally, observe that every time a word w enters A there is no harm in 
skipping all the words of the same length. This allows one to perform a very 
similar construction fulfilling the last sentence in the statement of the theorem. 
o 
We will draw from the existence of such sets some consequences on the 
density of m-complete sets for DEXT and some other classes. To do this, we 
need a technical lemma. Recall from Definition 1.10 in Volume I that a set A 
has subexponential density if and only if for every polynomial p the census of 
A satisfies CA(P(n» E 0(2n) (see also Proposition 2.3 in Volume I) and that a 
set A has symmetric density if and only if neither A nor A have subexponential 
density. 
Lemma 6.1 If A is reducible to B via a I-I a.e. function I, and B has 
subexponential density, then A has subexponential density. 

Complexity Cores and Polynomial Time m-Reductions 
139 
Proof. 
Let p be a polynomial bounding the time needed to compute f. Let 
d be the number of pairs having the same image under f, which is finite by 
hypothesis. Then cA(n) ~ cB(P(n» + d, because every word of length less 
than n in A must be mapped to words in B of length less than pen), and each 
two of these words in B must be different, with at most the d exceptions. Now, 
as for every polynomial q, cB(p(q(n))) + d is o(2n), we have that cA(q(n» is 
o(2n). 
0 
Now we can prove the following property of DEXT-hard sets: 
Theorem 6.3 Every set B which is m-hardfor DEXT has symmetric density. 
Proof. 
Assume, to the contrary, that either B or B has subexponential density. 
Both are m-hard for DEXT, because this class is closed under complements. 
Thus both the strongly bi-immune set A constructed in Theorem 6.2 and its 
complement are reducible to a set with subexponential density. The strong bi-
immunity of A (and of A) implies that both reducibilities are 1-1 a.e., and 
hence, by the lemma, that both A and A have subexponential density; but this 
is a contradiction, because their union has exponential density. 
0 
As a corollary, we get a result similar to Mahaney's theorem (Theorem 5.7). 
Recall that this theorem shows that there are no sparse NP-complete sets unless 
P = NP. Now we have the following: 
Corollary 6.2 There are no sparse m,-complete sets for DEXT. 
Of course the same results apply to any other deterministic time class con-
taining DEXT. Still one more corollary can be obtained from the observation 
that strongly bi-immune sets can be constructed which have at most one word 
of each length (and are hence sparse) as indicated above. We can conclude the 
following important theorem, which distinguishes between Cook's reducibility 
and Karp's reducibility within DEXT. 
Theorem 6.4 There are sets in DEXT which are not m-reducible to their 
complements. Hence m-reducibility and T-reducibility differ in DEXT. 
Proof. 
Consider any strongly bi-immune set A having subexponential density, 
constructed as indicated in Theorem 6.2. Its complement A is also strongly 
bi-immune and cannot have subexponential density; hence A is not m-reducible 
to A. 
0 
6.4 Complexity Cores and Polynomial Time m-Reductions 
Let us turn now to the notion of complexity core defined above. Consider any 
recursive set A not in P. This implies that for every algorithm ,M for A there 
are some instances on which A1 spends more than polynomial time; moreover, 

140 
Bi-Immunity and Complexity Cores 
as P is closed under finite variations, there are infinitely many such instances. 
The following question is a natural one: to what extent is this set of difficult 
instances dependent of the algorithm M? Is there a set of instances which is 
uniformly hard, i.e. difficult for every algorithm? This is a complexity core for 
A. 
The answer is affirmative, and in fact swaps the quantifiers above: we show 
next that, if A is not in P, then there are infinitely many instances in which every 
algorithm for A spends more than polynomial time. Furthermore, we consider 
the problem of deciding whether a given instance is one of these difficult ones, 
i.e. we consider the complexity of the complexity cores for A. It is a little 
surprising that the cores for arbitrarily difficult sets can be easy to decide; An 
example of this is the bi-immune sets: Although they are difficult, they have a 
trivial core, E·. 
The decidability of the complexity core relies on a construction based on 
an enumeration of machines for deciding a given set A (although a much easier 
construction produces complexity cores if one does not care about the complexity 
of the core produced). We use in the construction the following notation: 
Definition 6.5 
Given a machine M and a time boundt, the set oft-hard inputs 
for Mis 
H(M, t) = {x I AI does not halt on x within the first t(lx D steps} 
The machines in the enumeration that we use for constructing a complexity 
core for a given set A. fulfill some conditions which are enumerated in the 
following lemma: 
Lemma 6.2 For every recursive set A there is an enumeration of total (i.e. halt-
ing on all inputs) Turing machines Afl , Ah ... , with the following properties: 
(a) For every i 2: 1, L(M;) is either A or a finite set. 
(b) For every i 2: 1, if L(M;) is finite then H(M;, q) is co-finite for every 
polynomial q. 
(c) For every total Turing machine 111 accepting A, and every polynomial q, 
there is a machine 111; in the enumeration and a polynomial r such that 
H(M;,r) ~ H(M,q). 
(d) The encoding of 111; can be computed from i in polynomial time. 
Proof Let M be a fixed total machine accepting A, and let t be a time con-
structible function bounding the running time of M. We assume that t(n) 2: 2n. 
Such a function is obtained from Lemma 2.3 in Volume 1. Consider an enumer-
ation of all Turing machines, clocked to halt within t steps; let T}, T2 • ... be 
this enumeration. Let M; be the machine defined by the program in Figure 6.2. 
For each machine T;. either it accepts A, and then .Hi also accepts A, or 
there is a non empty difference between A and the set accepted by Tj ; let y be the 

Complexity Cores and Polynomial Time m-Reductions 
141 
input x 
for each y such that t(lyi) ~ Ixl do 
test whether y is accepted by M if and only if y is accepted by 11 
if this holds for each such y then 
accept x if and only if x is accepted by 11 
else loop for 21xl steps and then reject 
Figure 6.2 Construction of an enumeration of machines 
smallest word in this difference. In this case no x of length greater than t(lyi) 
is accepted, and hence M; accepts only a finite set. This shows (a). Moreover, 
in this same case all but finitely many inputs x lead M; to spend 21xl steps in 
the "else" in the last line, and hence lIfi spends more than polynomial time in 
all but finitely many inputs. This shows (b). Any reasonable way of encoding 
machines will fulfill (d). Let us show (c). 
Let lIf be any total machine accepting A. Consider the machine M' which 
runs M and M in parallel, accepting if and when anyone of the two halts and 
accepts. Speed up M' by a constant factor so that it operates in time t. It is 
immediate that H(M', q) ~ H(M, q), because M' never needs more time than 
M. Clocking M' to stop within ten) steps does not modify it essentially. This 
clocked machine appears somewhere in the enumeration T], T2 , •••• Let i be 
its index, and consider the machine lIfi. The tests can be performed in time 
polynomial in lxi, because 21yI ~ t(lyi) ~ Ixl and hence the number of y's 
is linear, and each y requires time t(lyi) ~ Ixl to be tested. Hence for some 
polynomial r, H (lIfi , r) ~ H (11, q) ~ H (M, q), as was to be shown. 
0 
Now we proceed to the construction of a complexity core for any set A not 
in P. 
Theorem 6.5 Let t be any time constructible function such that for every k, 
n k E oCt). Let A be any recursive set not in P. Then A has an infinite complexity 
core C E DTIME(t). 
Proof Let lIfi be an enumeration such as the one in the previous lemma, and 
let Pi be a nondecreasing polynomial of degree i. Define the recursive function 
r as follows: r(k) is the length of the smallest word x such that 
1. Ixl > k; 
2. Pk(lxi) ~ t(~P; and 
3. for each i less than or equal to k, x E HCM;,Pk)' 

142 
Bi-Immunity and Complexity Cores 
Such a word x can be always found: for it is clear that (t / k) eventually 
majorizes every polynomial, and the intersection 
k 
nH(Mi,Pk) 
i-I 
must be infinite, since otherwise an algorithm consisting of running in parallel 
the k machines Mi , i S k, for Pk steps, would allow one to decide all but a 
finite part of A in time k . Ph contradicting the fact that A is not in P. 
Let s be a time constructible function majorizing r, obtained from Lemma 2.3 
of Volume I. Define a set C by its accepting machine, which is described in 
Figure 6.3. 
input x 
compute successively the values of s(O), s(2)(0) = s(s(O», s(3)(0) = s(s(s(O»), ... 
until m is found such that s(m)(o) < Ix I S s(m+I)(o) 
check that for k = s(m)(o), it holds that 
Pk(lxi) S t(~P, and x is in H(Mi,Pk) for every i S k 
if all these conditions hold then accept 
Figure 6.3 Machine accepting a complexity core 
The computation of m can be done in time polynomial in Ix I, because s is 
time constructible. It is easy to check the bound on Pk(lxi), and if this bound 
holds then each decision "x E H(Mi,pd" can be taken in time ttl>. Hence the 
full procedure takes time OCt). 
Finally, observe that the definition of rand s ensures that in each interval 
s(m)(o) ... s(m+l)(o) there is at least one x fulfilling the conditions to be in C, 
and hence C is infinite. Also, for every A1i and every polynomial q, all but 
finitely many words in C are in H(Mi , q), and hence, by the properties of the 
enumeration {Mi }, they are in H(M, q') for any AI deciding A and for any 
polynomial q'. Thus, each algorithm for A spends more than polynomial time 
on almost every word of C, which shows that C is a complexity core for A. 
0 
We end this section with an application of this notion to the study of sets 
characterized by some structural property; namely, complete DEXT-hard sets. 
We need a lemma relating cores to m-reductions: 
Lemma 6.3 Let A s~ B via the function f, and let C be a complexity core 
for A. Assume fCC) infinite. Then I(C) is a complexity core for B. 
Proof. 
Assume that D ~ I(C) is an infinite set in which membership in B 
is decidable in polynomial time, say by a machine M. By computing I and 
using AI to check whether the result is in B, we get that I-I (D) is a set in 

Levelability, Proper Cores, and Other Properties 
143 
which membership in A is decidable in polynomial time. As D is infinite and 
included in f(C), there are infinitely many words in C n f-I(D), and therefore 
C contains infinitely many words in which membership in A is decidable in 
polynomial time. Thus C cannot be a core for A. 
Thus if C is a core for A then no such D exists, and therefore f(C) is a 
core for B. 
0 
Now we can obtain the following result concerning the complexity cores of 
DEXT-hard sets. 
Theorem 6.6 Every set that is m-hard for DEXT has a complexity core of 
exponential density. 
Proof Let A. be m-hard for DEXT. Let B be a strongly bi-immune set in 
DEXT, and f be a reduction from B to A. By the characterization of bi-immune 
sets in terms of complexity cores, we know that E* is a complexity core for B. 
Hence, the range of f is a complexity core for A. On the other hand, f has to 
be 1-1 a.e. because of the strong bi-immunity of B. Hence the range of f has 
exponential density. 
0 
6.5 LeveIability, Proper Cores, and Other Properties 
It is natural also to consider the subset of hard instances which belong to the 
set under consideration. Given A not in P, and a core C for A, we call the set 
C n A a proper core for A. 
Observe, for example, that in the core constructed above for DEXT-hard sets 
both C n A and C n A are exponentially dense. On the other hand, bi-immune 
sets are those which have a maximal complexity core: E*. 
Our next natural question is: what about maximal proper cores? This mo-
tivates the definition of "almost P-immune" sets given below. To study this 
concept, the following characterization of complexity cores will be useful. It 
has interest on its own, also. 
Lemma 6.4 Let A be any recursive set. A subset B of A is a proper complexity 
core for A if and only if its intersection with every subset of A which belongs 
to P is finite. 
Proof Let B ~ A be a proper complexity core for A, and D E P with 
D ~ A. Since A is recursive, there is a total machine MI accepting it; let M2 
be a machine accepting D in polynomial time. Then the machine that simulates 
M2 and accepts if M2 does, else runs MI, accepts A and runs in polynomial 
time on D. Since no algorithm for A runs in polynomial time on infinitely 
many instances from B, the intersection of Band D must be finite. Conversely, 
suppose B ~ A is not a proper complexity core for A. Then there is a machine 
M accepting A and running in time p, a polynomial, on infinitely many inputs 

144 
Bi-Immunity and Complexity Cores 
from B. The machine J..{' that simulates J..{ for exactly p steps accepts a set in 
P with infinite intersection with B. 
0 
Next we define and characterize almost P-immune sets. Throughout this 
section, all sets are assumed to be infinite, since for finite sets the concepts 
make little sense. 
Definition 6.6 An infinite recursive set is almost P -immune if and only if it is 
the union of a set in P and a P-immune set. 
In the following, "maximal" means maximal in the preorder inclusion modulo 
finite variations. 
Proposition 6.3 Let A be an infinite recursive set. The following are equiva-
lent: 
1. 
A is almost P-immune. 
2. 
A has a maximal proper complexity core. 
3. 
A has a maximal subset in P. 
Proof 
1. =? 2. Let A be the union of a set B in P and a set C which is P-immune. 
Then C is a proper core for A, because otherwise the set of easily decidable 
instances of A would be an infinite subset of C in P. Let D be another proper 
core for A; then D n B must be finite, because it is a subset of D in which 
membership of A is polynomial time decidable. As D ~ A = B U C, we have 
that D is contained in C with finitely many exceptions, and the result follows. 
2. =? 3. We repeatedly use Lemma 6.4. Let C be the maximal proper core 
of A. We show next that A - C is in P. Furthermore, there is no essentially 
greater subset of A in P, because if there were such a set B, then B n C would 
be a subset of C in which membership of A is polynomial time decidable, 
contradicting the fact that C is a core. To show that A - C is in P, we will 
show first that any proper core for A - C is a proper core for A; then the 
existence of an infinite proper core D for A - C leads to contradiction, since 
D is a proper core for A and therefore CUD is also a proper core for A with 
infinitely many more words than C, contradicting its maximality. Since only 
sets in P lack infinite proper cores, A - C should be in P. 
Thus it only remains to show that any proper core for A - C is a proper core 
for A. Let H be a subset of A - C (and therefore a subset of C), and assume it 
is not a core for A: H n D is infinite for some subset D E P of A. However, 
C is a core for A, and hence C n D is finite. Thus D - C is in P and a subset 
of A - C, and using that H ~ C, we obtain H n (D - C) = H n D, which 
is infinite by hypothesis. Thus H has infinite intersection with a P subset of 
A - C, and is not a proper core for that set either. Hence all proper cores for 
A - C are also proper cores for A. 

Levelability, Proper Cores, and Other Properties 
145 
3. ~ 1. Let B be the maximal subset of A in P. Then C = A - B is 
P-immune: if C has an infinite subset D in P, then DUB would contradict 
the maximality of B. Thus A is the union of a set in P, B, and a P-immune 
wQ 
0 
By analogy with a recursion-theoretic concept, we define: 
Definition 6.7 An infinite recursive set A is P-Ievelable if and only if it is not 
almost P-immune. 
Almost P-immune sets are those having an optimal polynomial time decid-
able "approximation", or solvable special case, in the sense of the introduction: 
their maximal subset in P. Any other bigger subset in P is just a finite vari-
ation of the maximal set. In this sense, levelable sets are those that have no 
such optimal approximation. For every polynomial time solvable subset B of a 
P-Ievelable set A, there is another polynomial time solvable subset which in-
cludes B and is significantly better, because it is correct in infinitely many more 
instances of A. Other characterizations of almost P-immune sets are proposed 
in Exercises 4 and 8. 
The main interest of this notion is that all the known NP-complete and 
PSPACE-complete set are P-Ievelable, i.e. they have no optimal approximation. 
In particular, it is known that every set complete for PSPACE under logarithmic 
space reducibility is levelable. The same is true for sets that are complete for 
deterministic exponential time, and for sets having some particular form of self-
reducibility. In some sense, every naturally defined problem falls in some of 
these categories, and therefore turns out to be levelable. 
We prove this fact for paddable sets, which include all the known natural NP-
complete sets. We do not prove here the P-Ievelability of other NP-complete 
universal sets such as the honestly k-creative sets, nor of known PSPACE-
complete sets. See the references for proofs of these facts. 
Recall that the definitions of the various forms of paddable sets were given 
in Definition 5.5. 
Theorem 6.7 If A is an honestly paddable set not in P, then A is P-levelable. 
Proof Let A be paddable via the honest function pad as in the definition. The 
honesty hypothesis ensures that there is a polynomial p such that p(lpad(x, y)i) 2 
Ixl + Iyl for all x, y E E·. Define the function f(x) = pad(x, ()p<lxl)+l) Then 
If(x)1 > lxi, and for every x, f(x) E A if and only if x E A. Moreover, f is 
computable in polynomial time. 
Assume that A is almost P-immune, and let E be the maximal subset of A 
in P. We will show that there is an infinite subset B of A in P which does 
not intersect E; thus E U B is a P subset of A which witnesses that E is not 
maximal, contradicting the hypothesis; hence A is not almost P-immune, i.e. A 
is level able. 

146 
Bi-Immunity and Complexity Cores 
The set B is defined as the "boundary" of E with respect to the function I, 
as follows: 
B = {x I x ¢ E but I(x) E E} 
As E E P and I is computable in polynomial time, B E P. By definition, 
B n E is empty. It remains to show that B is infinite. 
For each x in A, consider the following sequence: x, I(x), 1(2)(x) = l(f(x», 
1(3)(x) = l(f(f(x»), .... Each of these values is in A, by the definition of I. 
The set 
Ex = {x,/(x),/(2)(x), ... ,/(n)(x), ... } 
is infinite and is in P, because I is strictly length-increasing. As E was assumed 
to be a maximal P subset of A, Ex must eventually be contained in E: it must 
"cross the boundary" B at some point. 
As A is not in P, the difference A - E is infinite. For each x in A - E, the 
sequence Ex reaches some member y of B, and Ixl ~ Iyl. Thus, only finitely 
many x may reach any fixed y, and hence there must be infinitely many such y. 
Therefore B is infinite. 
0 
Stronger results can be proved by additionally assuming invertibility of the 
padding function; these results can be found in the references. 
6.6 Exercises 
1. 
Show directly that no paddable set can be P-bi-immune. 
2. 
Use the Uniform Diagonalization technique (as in the proof of Theorem 7.1 
of Volume I) to show that for every recursive infinite set A there is a set 
B E P which splits A. 
3. 
A special case solution for a set A, sometimes also called a polynomial 
time approximation algorithm for A, is an algorithm that may answer three 
possible outputs: accept, reject, and "1" (or "I don't know"), such that 
every accepted word is in A and every rejected word is in A. Show that 
a set is bi-immune for P if and only if every approximation algorithm 
answers "?" on all but finitely many instances. 
4. 
Find a characterization of almost P-immune sets in terms of special case 
solutions. 
5. 
Show that there is a P-bi-immune set in DEXT which is not strongly 
P-bi-immune. 
6. 
Let d be a strictly increasing integer-valued function computable in ex-
ponential time such that r ~ d(on) ~ 2n+l - n - 1. Show that there is 
a strongly bi-immune set A in DEXT whose census is precisely cA(n) = 
d(on). 
7. 
Denote by CFL the class of context-free languages. Exhibit a CFL-bi-
immune set and a CFL-Ievelable set decidable by on-line Turing machines 

Bibliographical Remarks 
147 
using logarithmic space. Hint: Use the pumping and Ogden lemmas for 
context-free languages. 
8. 
A reduction I from A to B is P-to-finite if and only if I(E) is finite for 
every E ~ A, E E P. Show that A is almost P-immune if and only if 
there is a P-to-finite reduction from A to some recursive set B. 
9. 
Show that, if a bi-immune set for P exists in NP, then SAT has a com-
plexity core in P. 
10. A recursive set A is in the class APT if and only if there is a machine 
M that accepts A, which works in polynomial time on most instances; 
more formally, for some polynomial p, the set of instances x on which 
M spends more than p(lx I> steps is sparse. Show that A is in APT if and 
only if every complexity core for A is sparse. 
11. Consider the following asymmetric version of APT: a set A is in I-APT 
if and only if there is a machine .M that accepts A, which works in 
polynomial time on most accepted instances; more formally, for some 
polynomial p, the set of instances x E A (here is the difference with APT) 
on which AI spends more than p(lxl} steps is sparse. Show that A is in 
I-APT if and only if every proper complexity core for A is sparse. 
6.7 Bibliographical Remarks 
The starting point for the material we have presented in this chapter is in two 
independent results. The first one appears in Lynch (1975), where complexity 
cores were defined, and where it was proved that every recursive set not in P has 
a complexity core (an incomplete form of Theorem 6.5). The second one is from 
L. Berman (1976), where he proved our Corollary 6.2 using a set which was in 
fact strongly bi-immune (although he did not introduce the notion explicitly). 
The fact that the set constructed by Berman was bi-immune is observed 
in Ko and Moore (1981). Also, our concept of special case solution is from 
this reference, where it is termed "approximation algorithm". The result about 
splitting mentioned in the text (a stronger version of Exercise 2) was proved 
in Breidbart (1978) by a direct construction. A proof based on the Uniform 
Diagonalization technique appears in Schmidt (1985). 
The concept of a bi-immune set and its characterizations based on all these 
previous concepts, as well as the notion of strong bi-immunity and its characteri-
zations (proposition 6.1, Corollary 6.1, Theorem 6.1, and Exercises 3,5, and 6), 
are from Balcazar and Schoning (1985). The construction in Theorem 6.2 is an 
elaboration of the construction of Berman (1976). Theorem 6.4 is from Ladner, 
Lynch, and Selman (1975), where it is proved by a direct construction; our proof 
is from Balcazar and Schoning (1985). 
The concept of a complexity core was studied long after its definition by 
N. Lynch, specifically in Orponen and Schoning (1986), where our proof of 
the existence of complexity cores appears (Lemma 6.2 and Theorem 6.5), as 

148 
Bi-Immunity and Complexity Cores 
well as the exponentially dense cores for DEXT-complete sets (Lemma 6.3 and 
Theorem 6.6), and Exercises 10 and 11 about the classes APT and I-APT, defined 
previously in Meyer and Paterson (1979). An interesting application of the 
concept of complexity core to propositional logic appears in Schoning (1987b). 
Almost P-immunity, levelability, the characterization in Proposition 6.3, and the 
levelability of paddable sets (Theorem 6.7) are from Du, Isakowitz, and Russo 
(1984) and Orponen, Russo, and Schoning (1986). Exercise 8 is also from this 
last reference. Exercise 7 is from Balcazar, Diaz, and Gabarr6, (1985). The 
P-Ievelability of logspace-complete sets for PSPACE is shown in Russo (1986). 
The notion of complexity core as we have presented it is directly linked to the 
class P. An appealing idea is to define and prove existence of complexity cores 
with respect to other classes. Some interesting advances are in Even, Selman, 
and Yacobi (1985), where some specific classes are used instead of P, each 
requiring its own concept of complexity core. A clearer definition was given 
in Book and Du (1987), based on the characterization given in Lemma 6.4; in 
this way, the study of proper cores is made in an almost machine independent 
fashion that allows one to speak of complexity cores relative to the regular sets, 
in the low end, or relative to complexity classes, or to the recursive sets, or to 
the arithmetic sets, and beyond, always by means of the same definition. For 
other results about complexity cores and levelability, see also Du, Isakowitz, and 
Russo (1984), Ko (1985), Orponen (1986), Russo and Orponen (1987), and Du 
and Book (1989), and the interesting and complete overview given in Book, Du, 
and Russo (1988). 

7 
Relativization 
7.1 Introduction 
Many of the questions we address in this book can be thought of as questions 
about the properties of the resource-bounded reducibilities. As examples, the 
important concept of completeness (or incompleteness) in complexity classes 
is defined in terms of reducibility, and the polynomial time hierarchy is the 
closure of P under nondeterministic polynomial time Turing reducibility. We 
have shown in the previous chapter that polynomial time m-reducibility and T-
reducibility differ. It is time to address a similar question: Do the deterministic 
and nondeterministic polynomial time Turing reducibilities differ? Similarly: do 
deterministic polynomial time reducibility and deterministic polynomial space 
reducibility differ? 
By definition, A ~T B if and only if A E PCB), and A ~!JP B if and 
only if A E NP(B) (Volume I, Chapter 4). 
Suppose that for every set B, 
PCB) = NP(B). Then, for every A. and B, A ~T B if and only if A ~!f.P B. 
Thus A ~T B and A ~!JP B would be equivalent That is, showing that these 
reducibilities differ amounts to exhibiting a set B such that PCB) -I NP(B). 
Notice that in fact P is the zero degree (i.e. the class of sets reducible to 
the empty set) for deterministic polynomial time reducibility, and NP is the 
zero degree for nondeterministic polynomial time reducibility. If for every set 
B, P(B) = NP(B), then for B = 0 we would obtain P = NP; indeed, if the 
reducibilities coincide, so do their zero degrees. Thus the problem of whether 
these reducibilities coincide is in some sense a generalization of the P J NP 
problem. Similar considerations can be made about polynomial time versus 
polynomial space. 
In this chapter we solve this problem in the negative, showing that these 
two reducibilities differ. However, we will show also that there are "regions" in 
which these reducibilities coincide in some sense, thus indicating that the fact 
that the reducibilities differ does not imply that their zero degrees do so: the 
P J NP problem remains open. We investigate later further applications of the 
technique used, comparing other reducibilities and discussing the properties that 
allow us to apply this technique and enhanced versions of it. 

150 
Relativization 
7.2 Basic Results 
We show next that there exist sets A and B such that, respectively, peA) = 
NP(A) and PCB) 1 NP(B). Notice that the latter implies that A $;T B differs 
from A $;!J.P B. 
Theorem 7.1 
There exists a set A such that peA) = NP(A). 
Proof Let QBF be the T-complete set for PSPACE obtained in Theorem 3.9 
of Volume 1. Then every set in PSPACE is T-reducible in polynomial time to 
QBF. This implies that PSPACE ~ P(QBF). On the other hand, we know that 
for every oracle set B, 
PCB) ~ NP(B) ~ PSPACE(B) 
and since QBF is in PSPACE, PSPACE(QBF) ~ PSPACE. Thus we obtain: 
P(QBF) ~ NP(QBF) ~ PSPACE(QBF) = PSPACE ~ P(QBF) 
Since the first class and the last class coincide, all the classes in the chain 
coincide, and therefore P(QBF) = NP(QBF). 
0 
Observe that any PSPACE-complete set can be substituted for QBF in the 
preceding proof, since the only property of QBF used there is its PSPACE-
completeness. 
The following definition will be used in all the remaining constructions in 
this chapter. 
Definition 7.1 
Given a family of sets A(1), A(2), ... A(n), ... , we denote 
limn A(n) = {w I w E A(n) for all but finitely many n} 
Many constructions in this chapter will involve a process performed in stages 
and an infinite collection of sets A(n), each of them corresponding to the result 
of a stage n. The finally constructed set will be liffin A(n). 
We go on to show that the reducibilities differ. The construction in the proof 
involves a diagonal process, in which an effective enumeration of machines is 
used to diagonalize "against" (i.e. out of) a given class. Recall that in the proof of 
Lemma 7.1 of Volume I we constructed an effective enumeration PI, P2, P3, ••• 
of deterministic oracle machines with polynomial time clocks, which allows us to 
recursively present the class peA) for any recursive A. In the same way, we can 
construct an effective enumeration of nondeterministic oracle Turing machines 
with polynomial time clocks, NP l , NP2, ... which allows us to present NP(A) 
for any recursive set A. We assume that Pi is a polynomial bounding the running 
time of both Pi and NP i. Without loss of generality, we can assume that all the 
Pi are nondecreasing, and that for every i and n, Pi(n) $; Pi+l (n). 
Using these enumerations, we can prove the following: 

Basic Results 
151 
Theorem 7.2 There exists a set B such that PCB) =I NP(B). 
Proof. We shall construct a set B such that L(B) rt PCB), where 
L(B) = {on I there is an x E B with Ixl = n} 
Observe that L(B) E NP(B), via the nondeterministic oracle machine pre-
sented in Figure 7.1. 
input w 
if wE 0* then 
end 
guess x with Ix I = n 
if x E B then accept 
Figure 7.1 Nondeterministic decision procedure for L(B) 
The construction will be performed in stages. At stage n, we shall possibly 
expand the set constructed so far, adding at most one word in such a way 
that we diagonalize against machine Pn of the enumeration. To do this, we 
choose a word long enough to ensure that the previously simulated computations 
are not disturbed by the (potential) addition of this word to the oracle, and 
such that the currently simulated computation is not disturbed either. Through 
the construction, ken) denotes the length of the word chosen at stage n. The 
construction is shown in Figure 7.2. 
stage 0 
B(O) := 0 
k(O) := 0 
stage n 
let ken) be the smallest number such that 
Pn(k(n» < 2k(n) and Pn_,(k(n - 1» < ken) 
if ok(n) E L(Pn, B(n - 1» then B(n) := B(n - 1) 
else let wen) be the first word of length ken) such that Pn 
end stage 
has not queried it to B(n - 1) in the computation on ok(n) 
B(n) := B(n -
1) U {w(n)} 
Figure 7.2 Construction for Theorem 7.2 
Define B as B := limn B(n). 
Thus, at each stage, either we leave B unchanged, or we add the first word 
wen) of length ken), in the canonical order of {O, 1}*, such that wen) does not 

152 
Relativization 
appear in the query tape at any moment when Pn is in state QUERY during the 
computation of Pn on ok(n) relative to B(n - 1). 
We must show that the construction can be performed, and that the resulting 
set fulfills the required conditions. 
First observe that a number ken) exists fulfilling the conditions, because 2n 
grows faster than Pn. There are 2k(n) words of length ken), and the running time 
of Pn on ok(n) is bounded by Pn(k(n». By the way ken) has been chosen, this 
is less than 2k(n), and Pn "does not have the time" to query B(n - 1) about all 
the possible 2k(n) words of length ken). Hence the word wen) exists if needed. 
Moreover, for every n, the computation of Pn on ok(n) is the same relative 
to B as the computation relative to B(n - 1). Indeed, the word wen) added at 
stage n, if any, is not queried in this computation; therefore the computation is 
insensitive to the membership of wen) in the oracle. Observe that this is to say 
that wen) is "hidden" to the deterministic machine, in the sense that it is chosen 
such that Pn cannot "see" it. Also, by the way k(m) is chosen for m > n, words 
w(m,) added at later stages are too long to be queried by Pn on Ok(n); therefore 
this computation is again insensitive to the membership of w(m,) in the oracle. 
Thus, ok(n) E L(Pn, B) if and only if ok(n) E L(Pn, B(n - 1». 
Let us prove that L(B) rt PCB). We show that for every n, it holds that 
L(B) =I L(Pn, B). Fix n; then ok(n) E L(B) if and only if some wen) of 
length ken) has been included in B at stage n; this happens if and only if 
ok(n) rt L(Pn, B(n - 1», and by the argument above, this holds if and only if 
ok(n) rt L(Pn, B). Hence ok(n) is in L(B) if and only if it is not in L(Pn, B), 
which implies that L(B) =I L(Pn, B). 
Thus L(B) rt PCB) and since L(B) E NP(B), PCB) =I NP(B). 
0 
Notice that the time bound plays a crucial role in the above diagonalization, 
because it forces a bound in the number of oracle queries allowed in any compu-
tation. Also, note that this result shows that polynomial time Turing reducibility 
and polynomial space Turing reducibility differ: 
Corollary 7.1 There exists a set B such that PCB) =I PSPACE(B). 
Proof Immediate since NP(B) ~ PSPACE(B). 
0 
Since PCB) is closed under complementation, the following also holds: 
Corollary 7.2 There exists a set B such that PCB) =I co-NP(B). 
Polynomial time deterministic Turing reducibility is closed under comple-
mentation, in the sense that whenever A is reducible to B, A is also reducible 
to B. We know that this fact does not hold for polynomial time m-reducibility, 
from Theorem 6.4 of this volume. We show next that nondeterministic polyno-
mial time Turing reducibility is not closed under complementation, by exhibiting 
sets C and D such that D is in NP(C) but D is not. Notice that this amounts to 
exhibiting a separation of NP from co-NP under a suitable relativization. This 
can be done by a diagonalization similar to the one employed above. 

Basic Results 
153 
Theorem 7.3 There exists a set C such that NP(C) :/ co-NP(C). 
Proof We shall use the same technique as in the previous theorem. We con-
struct a set C such that L(C) does not coincide with the complement of a set 
in NP(C). To do this, we must make sure at each stage n that ok(n) is either 
simultaneously in L(C) and L(NP n, C), or simultaneously out of both sets. 
Let C := limn C(n), where each C(n) is constructed as indicated in Fig-
ure 7.3. 
stage 0 
C(O) := 0 
k(O) := 0 
stage n 
let ken) be the smallest number such that 
Pn(k(n» < 2k(n) and Pn_l(k(n - 1» < ken) 
if ok(n) E L(NP n, C(n - 1» 
end stage 
then 
fix an accepting computation of NP n on this word 
let wen) the first word of length ken) which has not 
been queried to C(n - 1) during the fixed computation 
C(n) := C(n -
1) U {w(n)} 
else C(n) := C(n - 1) 
Figure 7.3 Construction for the proof of Theorem 7.3 
Thus, at each stage, either we leave C unchanged, or we add the first word 
wen) of length ken) such that wen) does not appear in the query tape at any 
moment when NP n is on state QUERY, during the fixed accepting computation 
of NP n on ok(n) relative to C(n - 1). 
We show that the construction can be performed. First observe that the 
required number ken) always exists; this follows from the same argument as in 
the previous construction. There are 2k(n) words of length n. As the running 
time of NP n on ok(n) is bounded by Pn(k(n», NP n "does not have the time" to 
query C(n - 1) about all the possible 2k(n) words of length ken) in only one 
computation. Hence the word wen) exists if needed. Observe however that NP n 
might query about all the words of this length in the set of all computations. 
To show that the resulting set fulfills the required conditions, observe that 
for every n, the selected computation of NP n on ok(n) is the same relative to 
C as the computation relative to C(n - 1), by use of an argument analogous 
to the one in the previous construction. Therefore, the selected computation is 
preserved, and if ok(n) E L(NP n, C(n - 1» then ok(n) E L(NP n, C). Conversely, 

154 
Relativization 
if ok(n) ¢ L(NP n, C(n -1» then C is kept without change in the region accessible 
by NP n on input ok(n), and therefore ok(n) ¢ L(NP n, C). 
Let us prove that the complement of L(C) is not in NP(C). We show that for 
every n, it holds that L(C) :/ L(NP n, C). Fix n; then ok(n) E L(C) if and only 
if some w(n) of length k(n) has been included in C at stage n; this happens if 
and only if ok(n) E L(NP n, C(n - 1», and by the argument above, this holds if 
and only if ok(n) E L(NP n, C). Hence ok(n) is in L(C) if and only if it is also in 
L(NP n, C), which implies that L(C) cannot be the complement of L(NP n, C). 
Thus, L(C) ~ co-NP(C) and therefore NP(C):/ co-NP(C). 
0 
Notice that in a similar manner as before, this result shows that nonde-
terministic polynomial time Turing reducibility and polynomial space Turing 
reducibility differ: 
Corollary 7.3 There exists a set C such that NP(C) :/ PSPACE(C) and 
co-NP(C) :/ PSPACE(C). 
Proof Immediate since PSPACE(C) is closed under complementation. 
0 
Thus, regarding any pair of the classes P, NP, co-NP, and PSPACE, we 
have found an oracle under which this pair of classes coincide and an oracle 
under which this pair of classes differ. However, the oracle under which classes 
coincide is the same for all of them, and collapses all of them to P. In the next 
sections we show how to construct relativizations which collapse some classes, 
making them equal, while separating others. 
7.3 Encoding Sets in NP Relativized 
In this section we prove a lemma which allows the construction of oracles making 
classes equal to NP, but keeping NP different from P. The key to the proof 
will be again the same: to control the number and size of the queries that the 
machines are allowed to make. 
Lemma 7.1 Let 1If be a nondeterministic oracle machine which always halts. 
Assume that for every oracle B and on each input x, every word queried to B by 
11([ has length less than Ixi- Then there is an oracle A such that NP(A) :/ P(A), 
and furthermore L(M, A) E NP(A). 
Proof In the construction of A, we will use the words of odd length to perform 
a diagonalization as in Theorem 7.2 of the previous section, which separates 
P(A) from NP(A). The words of even length will be used to encode into A 
some information about the set LeAf, A), in such a way that a nondeterministic 
machine can retrieve it and decide L(AI, A) in polynomial time with oracle A. 
More precisely, we will force the set A to fulfill the following condition: 
Vu(u E L(M, A) if and only if 3vlvl = lui. uv E A) 

Encoding Sets in NP Relativized 
ISS 
It is clear that the right hand side of the implication is an NP(A) property. 
Therefore L(M, A) E NP(A). Notice also that this set is independent of the 
membership in the oracle of words having odd length. We will use these words 
to ensure that the set 
L(A) = {on I n is odd and there is an x E A with Ixl = n} 
is not in peA). As L(A) is in NP(A), this will prove our claim. 
The construction of A is given in Figure 7.4. 
stage 0 
A(O) := 0 
k(O) := 0 
stage n 
A(n) := A(n - 1) 
comment: other words will be added later 
let ken) be the smallest odd number such that 
Pn(k(n» < 2(k(n)-J)/2 and Pn-J(k(n - 1» < ken) 
comment: start of coding phase 
for each word u such that ken - 1) < 2·lul < ken) do 
if u E L(M, A(n» then 
let v the smallest word such that Ivl = lui, and 
uv has not been queried previously to the oracle 
A(n) := A(n) U {uv} 
comment: start of diagonalization phase 
if ok(n) ¢ L(Pn, A(n» then 
end stage 
let wen) be the first word of length ken) such that Pn has not 
queried it to A(n) in the computation on ok(n) 
A(n) := A(n) U {w(n)} 
Figure 7.4 Construction for the proof of Lemma 7.1 
We show that the construction can be performed. At the beginning of stage n, 
the only words of length between ken - 1) and ken) queried to the oracle were 
queried at stage k - 1 (they are too long to be queried at earlier stages). By the 
choice of ken - 1), fewer than 2(k(n-J)-J)/2 words were queried. That number 
is less than 21ul. As there are 21ul words uv to choose, there is always a word 
not queried to add to A in the coding phase of the construction. Notice that the 
bound on the length of the queries of M ensures that u is in L(M, A) if and 
only if, at the stage n that may add uv to A, it is in LeAf, A(n». 
Thus the coding part implies that A fulfills the condition indicated above 
for having L(Jf, A) in NP(A). The second part of the construction performs a 

156 
Relativization 
diagonalization identical to that of Theorem 3 to keep L(A) out of peA). The 
details are left to the reader (Exercise 3). 
0 
Now we present two applications of this lemma, constructing relativizations 
in which NP = co-NP but P ::/ NP, and NP = PSPACE but P ::/ NP. 
Theorem 7.4 
There is a set A such that NP(A) = co-NP(A) and peA) ::/ 
NP(A). 
Proof From the definition of the set J{(A) in Chapter 4 of Volume I, it is easy 
to see that it is possible to construct a deterministic machine which decides J{(A) 
deterministically under oracle A, in exponential time, and such that on input x 
every query has length at most IxI- By interchanging accepting and rejecting 
states of this deterministic machine, obtain a new machine M accepting the 
complement of J{(A), and subject to the same condition on the length of the 
queries. 
Therefore the lemma applies to this M, yielding a set A such that peA) ::/ 
NP(A) and J{(A) belongs to NP(A). Since J{(A) is m-complete in co-NP(A), 
and NP(A) is closed under m-reducibility, we obtain NP(A) = co-NP(A). 
0 
In a similar manner, we can produce an oracle making NP equal to PSPACE 
but different from P: 
Theorem 7.5 There is a set A such that NP(A) = PSPACE(A) and peA) ::/ 
NP(A). 
Proof Consider the set J{S(A) shown PSPACE-complete in Exercise 24 of 
Chapter 3 of Volume 1. It is easy to see that it can be recognized by a determin-
istic oracle machine M relative to oracle A such that on input x every query 
has length at most IxI-
Again the lemma applies to this M, yielding an oracle set A such that 
P(A)::/ NP(A) and moreover J{S(A) belongs to NP(A). By the m-completeness 
of J{ SeA) for the class PSPACE(A), we obtain that NP(A) = PSPACE(A). 
0 
Observe that the results presented in the last two sections cover all the in-
clusion possibilities among the three classes P, NP, and co-NP, in the follow-
ing sense: every possible relationship compatible with the known properties 
(P closed under complementation and included in both NP and co-NP) holds in 
some relativization. In the next section we present similar theorems regarding 
the probabilistic complexity classes. 
7.4 Relativizing Probabilistic Complexity Classes 
In this section we present some arguments similar to those of the preceding 
sections, distinguishing various probabilistic complexity classes from each other 
and from deterministic and nondeterministic classes. 

Relativizing Probabilistic Complexity Classes 
157 
In Chapter 6 of Volume I we proved some relationships between P and NP 
and some probabilistic classes (ZPP, R, BPP and PP). One of the inclusion 
relationships we did not settle there is that of NP versus BPP. Observe that BPP 
is included in PSPACE, so that for PSPACE-complete oracles both NP and BPP 
coincide with P and with PSPACE. We prove here that there are oracles relative 
to which neither one is included in the other, and obtain as consequences many 
other separations. We start with a theorem in which a diagonalization over NP 
is performed. The construction is a combination of the ideas in the proofs of 
Theorems 7.2 and 7.3. 
Theorem 7.6 There is an oracle B such that co-R(B) is not included in 
NP(B). 
Proof Observe that the statement is equivalent to saying that "R(B) is not 
included in co-NP(B)", by complementation. In a similar manner to Theorem 
7.3, we will diagonalize so that a certain set L(B) is not the complement of a 
set in NP(B). This set is defined as in Theorem 7.2: 
L(B) = {on I there is an x E B with Ixl = n} 
In order to ensure that L(B) is in R(B) we will build B fulfilling the follow-
ing condition: for each length n, either no word of this length is in B, or ~ of 
the words are. In this way, a probabilistic machine can be obtained from the 
nondeterministic algorithm in Figure 7.1 which witnesses that L(B) is in R(B). 
The construction is indicated in Figure 7.5. 
stage 0 
B(O) := 0 
keO) := 0 
stage n 
let ken) be the smallest number such that 
Pn(k(n» < 2k(n)-2 and Pn-l(k(n - 1» < ken) 
if ok(n) E L(NP n, B(n - 1» then 
fix an accepting computation of NP n on this word 
let Wen) be a set of 3·2k(n)-2 words of length ken) such that NP n 
has not queried any of them to B(n - 1) in the fixed 
accepting computation on ok(n) 
B(n) := B(n -
1) U Wen) 
else B(n) := B(n -
1) 
end stage 
Figure 7.S Construction for the proof of Theorem 7.6 
Now we prove that the construction can be performed and that it constructs 
a set B as indicated. The conditions on ken) ensure that at most 2k(n)-2 words 

158 
Relativization 
are queried in the fixed computation. As there are 2k(n) = 4· 2k(n)-2 words of 
this length, the set Wen) exists if needed. The fact that these words do not 
interfere with the diag6nalization is proved as in the other constructions in this 
chapter, and from this fact it follows that ok(n) E L(NP n, B(n - 1» if and only 
if ok(n) E L(NP n, B). An argument like that of Theorem 7.3 proves that L(B) 
intersects every language in NP(B), and therefore it is not the complement of a 
set in NP(B). Since L(B) is in R(B), L(B) is in co-R(B) but not in NP(B), as 
was to be shown. 
0 
Notice that, in this proof, the machine outlined which witnesses the fact 
that L(B) is in R(B) is a probabilistic machine with bounded error probability 
when it operates under the oracle set B, but it may lose this characteristic if 
the oracle set is changed. Thus, a probabilistic oracle machine can operate in 
an "R fashion" for some oracles, having bounded error probability, while for 
others it may be just a probabilistic machine with potentially unbounded error 
probability. A similar fact holds for BPP. We will use these facts later on. 
The following properties also hold for the oracle constructed in the last 
theorem: 
Corollary 7.4 There is an oracle B such that BPP(B) is not included in 
NP(B). 
Proof Immediate since co-R(B) is included in BPP(B). 
o 
Corollary 7.5 There is an oracle B such that R(B) is not closed under com-
plementation. 
Proof For the set B constructed above, there is a set in co-R(B) which is not 
in NP(B). Since R(B) is included in NP(B), such a set is not in R(B) either. 
Hence R(B) =I co-R(B). 
0 
Corollary 7.6 There is an oracle B such that R(B) and co-R(B) are different 
from both PCB) and BPP(B). 
Proof Follows from the previous corollary and the closure under complemen-
tation of PCB) and BPP(B). 
0 
This last corollary shows that the separation between P and NP can be 
strengthened to yield a separation between P and R. Moreover, as R is included 
in BPP, we have: 
Corollary 7.7 There is an oracle B such that PCB) =I BPP(B). 
All the remaining inequalities and non-inclusions among the classes R, co-R, 
P, and NP which are consistent with the known relationships will follow from 
our next theorem, in which an NP set diagonalizes over BPP. The diagonalization 

Relativizing Probabilistic Complexity Classes 
159 
over BPP needs some auxiliary concepts 'and lemmas, which we state in the 
following. 
Let 111 be a probabilistic oracle machine, A an oracle, and x a word. Let us 
say that 111 is an c;-error machine under A on x when the error probability of M 
operating with oracle A is bounded by c; < 1/2. To say c;-error machine under 
A means that 111 is an c;-error machine under A on every x. A similar concept 
for R is suggested in Exercise 9. As noticed above, this characteristic of M is 
oracle-dependent In fact, a machine which is c;-error under a given oracle may 
acquire unbounded error probability if a single word is thrown in or kept out of 
the oracle. See Exercise 11. 
The computation tree of a probabilistic oracle machine may contain expo-
nentially many queries. Furthermore, the subtree of accepting (or rejecting) 
computations may still contain exponentially many queries. This implies that 
the techniques used for the diagonalization in the previous sections must be re-
fined, since they rely on the fact that the acceptance of a word by a machine 
only depends on polynomially many queried words. 
We show first how to overcome this problem. The following definition is 
crucial for diagonalizing over bounded error probabilistic classes: 
Definition 7.2 Fix an oracle A, a machine 111, and a word x. The word w is 
c;-critical with respect to .71,1, A, and x, if and only if the following holds: 
(a) 
AI is an c;-error machine under A on x. 
(b) 
111 is also an c;-error machine under A.6. { w} on x. 
(c) x E (L(M, A).6.L(AI, A.6.{ w}» 
Thus, a word w is c;-critical for M, A, and x if the fact that M accepts 
or rejects x depends on whether w is in the oracle, whereas the fact that AI is 
an c;-error machine does not. 
Observe that the definition is symmetric, in the 
sense that w is c;-critical for M, A, and x if and only if w is c;-critical for M, 
A.6.{ w}, and x. 
We describe now the way this concept will be used. When we diagonalize 
over an c;-error machine 111 at a word x, with A the oracle constructed so far, 
only c;-critical words have to be controlled. The reason is that if a non-critical 
word v is added to (or taken out of) A, either v is not relevant to the computation 
of M on x (in the sense that M's answer on x is the same), or 111 is no longer 
an c;-error machine, and therefore there is no need to diagonalize over it. 
The key point for the use of this concept is that there can be "not that many" 
c;-critical words. 
Lemma 7.2 Let ~M be a probabilistic oracle machine whose running time is 
a polynomial pen), and A an oracle for M. Then there is a positive constant r 
such that, for each word x, there are at most r·p(lxl) c;-critical words for 111, 
A, and x. 

160 
Relativization 
Proof. 
Let w be an e-critical word for 111, A, and x. Conditions (a) and (b) in 
the definition of critical ensure that 111 answers differently on x for the oracles 
A and A~{ w}. Thus, w must be queried in some computations of 111 on x. 
Moreover, 111 is an c-error machine on x under both A and A~ { w }. 
To accept x, at least half the computations of 111 must accept x, whereas 
to reject x at most e of the computations may accept. Observe that the total 
of computations is 2p(j:cI>. Let 8 = «1/2) - e). Consider separately the set of 
queries made in each computation (note that many queried words are counted 
more than once). Each set contains at most p(lxi) queries. Therefore the total 
of queries is at most p(lx i). 2P(I:cI>. 
Since at least a fraction of 6 computations must change, a query about w 
must appear in the set corresponding to at least 8·2P(I:cI> computations. Now, let 
c be the number of e-critical words. Since the number of queries about e-critical 
words cannot be greater than the total number of queries, we obtain 
c·6·2p(j:cI> :::; p(lxi).2P(I:Cj) 
Therefore, c :::; 8- I ·p(lxi), and the lemma follows. 
o 
In the construction that follows, we will diagonalize over all sets in the 
relativization of BPP to an adequately constructed oracle. To do this, we first use 
Theorem 6.4 of Volume I to fix a uniform bound e on the error of probabilistic 
machines. The following lemma is an easy consequence of the mentioned result: 
Lemma 7.3 Fix a set A and any positive constant c. For every set B in 
BPP(A), there is a probabilistic machine accepting B in polynomial time with 
oracle A, which is an e-error machine under A. 
We are ready to state our next theorem: 
Theorem 7.7 There is an oracle G such that NP(G) is not included in BPP(G). 
Proof. 
Once more, we will diagonalize so that a certain set L(G) in NP(G) is 
not in BPP(G). This set is defined again as in Theorem 7.2: 
L(G) = {on I there is an x E G with Ixl = n} 
This set is in NP(G). The construction of G will ensure that L(G) diagonal-
izes over every set in BPP(G), by considering each machine in an enumeration 
of probabilistic oracle machines. 
Fix previously to the construction a value e < 1/2, and let r be the constant 
furnished by Lemma 7.2. By Lemma 7.3, every set in BPP(G) is accepted 
by an e-error machine under G. Thus, during the construction, each machine 
is considered. If it is an e-error machine, and rejects a word of the form on, 
then this word is added to L(G) by adding to G a non-critical word for on. 
Lemma 7.2 ensures that non-critical words exist. By the definition of c-critical, 

Relativizing Probabilistic Complexity Classes 
161 
this implies that either the machine is no lbnger an e-error machine, and we no 
longer care about it, or that the machine still rejects on, and we have performed 
one more step of the diagonalization. 
Let p ~ be an enumeration of all clocked polynomial time probabilistic 
machines (Le., nondeterministic machines fulfilling the conditions required in 
Chapter 6 of Volume I). Let Pi be the running time of P~. The construction 
is given in Figure 7.6. 
stage 0 
C(O) := 0 
k(O) := 0 
stage n 
let ken) be the smallest number such that 
r·Pn(k(n» < 2k(n) and Pn_l(k(n -
1» < ken) 
find the fraction of computations of P Rn 
which accept ok(n) with oracle C(n - 1) 
if this fraction is at most e then 
let wen) be the first word of length ken) which 
is not e-critical for P Ri, C(n - 1), and ok(n) 
C(n) := C(n -
1) U {w(n)} 
else C(n) := C(n - 1) 
end stage 
Figure 7.6 Construction for the proof of Theorem 7.7 
Define C as C := limn C(n). 
The conditions on ken) ensure that decisions at stage n do not disturb 
the goals attained at previous stages, and that non-critical words exist (by 
Lemma 7.2). Assume that a set L(C) belongs to BPP(C), and let P Rn be 
a machine witnessing this fact. By Lemma 7.3, we can assume that P Rn is an 
e-error machine under C. 
Consider the case that P Rn probabilistically accepts ok(n) using as oracle 
the set C(n -
1). In this case, at stage n the "else" branch is taken, and no 
word of length ken) is added to C. Therefore, ok(n) is not in L(C). But the 
computation of P Rn on this word is the same for C as for C(n - 1), and it must 
accept Ok(n) with oracle C, contradicting the fact that P Rn accepts L(C) under 
oracle C. 
Consider next the case that with oracle C(n - 1), more than e but at most 
half of the the computations of P Rn accept ok(n). In this case, P Rn is not an 
e-error machine under C(n - 1) on ok(n). Again the "else" branch is taken, no 
word is added to C, and the computation on ok(n) is the same under C. This 
contradicts the fact that P Rn is an e-error machine under C. 

162 
Relativization 
Finally, consider the case that under C(n - 1), at most e of the computations 
of P R,. accept ok(n). By definition, P R,. is e-error on ok(n) under C(n -
1). 
The "then" branch is taken at stage n, and a non-critical word wen) of length 
ken) is added to C, so that C(n) = C(n - 1)6{w(n)}. Since words added at 
later stages are too long, the computations of P Rn on ok(n) are the same under 
both C and C(n). This implies that ok(n) belongs to L(C), which by hypothesis 
coincides with the language accepted by P R,. with oracle C. We have: 
(a) 
P Rn is c:-error under C(n -
1) on Ok(n); 
(b) 
by hypothesis, P Rn is e-error under C = C(n - 1)6{ wen)} on Ok(n); 
(c) 
ok(n) is rejected by P R,. with oracle C(n -
1), since there are at most 
e < 1/2 accepting computations, but accepted by P R,. with oracle C 
since it belongs to L(C). 
But these three conditions imply that wen) is c:-critical for P Rn, C(n - 1), 
and Ok(n), contradicting the choice of wen) at stage n. Therefore all three cases 
lead to contradiction, and the hypothesis that P Rn probabilistic ally accepts L(C) 
must be false. Hence, L(C) does not belong to BPP(C), as was to be shown. 
o 
From this theorem, together with the fact that both Rand co-Rare always 
included in BPP, we obtain as corollaries: 
Corollary 7.8 There is an oracle C such that NP(C) is not included in co-
R(C). 
Corollary 7.9 There is an oracle C such that NP(C) =I R(C). 
It is easily seen that these corollaries, together with Theorem 7.2, Theo-
rem 7.6 and the corollaries to it, and Theorem 7.7, settle all the pairwise inclu-
sion relationships among the classes P, R, co-R, NP, and BPP, since for each 
ordered pair of these classes either the first is always included in the second, or 
there is an oracle relative to which the first is not included in the second. 
We have not discussed here the relationships of ZPP with the other classes; 
this subject is left for Exercises 5 and 6. No new techniques are required. 
Many other relativizations regarding probabilistic complexity classes can be con-
structed, exhibiting various properties. Some of them are proposed in Exercises 
5 to 11. 
7.5 Isolating the Crucial Parameters 
In this section we argue that the preceding proofs do not really speak about the 
relationship between deterministic, probabilistic, and nondeterministic computa-
tions. Rather, they show that the combination of relativization and nondetermin-
ism is more powerful than the plain juxtaposition of relativized (deterministic) 
computation with nondeterministic (unrelativized) computation. 

Isolating the Crucial Parameters 
163 
In fact, the key to the proofs of the preceding results is the fact that the combi-
nation of relativization and nondeterminism allows one to explore in polynomial 
time (in a nondeterministic manner) an exponentially broad space, such as the 
set of all words of a given length. Since deterministic polynomial time allows 
one to explore just a polynomially broad space (because there is no time to do 
more), we can "hide" in the oracle words that polynomial time deterministic 
machines do not see, but which can be found by nondeterministic machines. We 
show in the following that this fact is the crucial point, by proving that the time 
bound is unnecessary. 
To do this, we state and prove a theorem capturing the essential ideas from 
most of the previously presented constructions. The hypothesis no longer re-
quires time bounds on the machines over which we want to diagonalize: we 
show that it suffices to bound the number of queries performed by these ma-
chines. Of course, Theorem 7.2 will follow as a corollary, since time bounds 
do imply bounds on the number of queries. 
We will use the following notation: 
Definition 7.3 
Given a nondeterministic machine 111, an oracle A and an input 
x, denote by Q(.~I, A, x) the set of words which are queried in some computation 
of AI on x under oracle A .. 
We will show that bounding the cardinality of sets of the form Q(M, A, x) 
by "reasonable" bounding functions is enough to diagonalize over a class. 
Theorem 7.8 Let {Alj } be an effective enumeration of nondeterministic oracle 
machines and F be a class of time constructible functions. Suppose that the 
following conditions hold: 
(a) For each machine lIh there is a function f in F such that for every oracle 
A and input x the cardinality of Q(M, A, x) is bounded by f(lxi}. 
(b) There is an injective function t in F such that 2t(n) > fen) for every f in 
F and all but finitely many n. 
Then there exists an oracle B such that NTIME(F, B) is not included in 
{L I L = L(Mj, B) for some i}. 
Notice that condition (a) is precisely the formalization ofthe intuitive concept 
we have indicated above: the number of queries that machines 1I1i are allowed 
to make is bounded by a suitable family of functions. Condition (b) is technical, 
and it implies that among the functions in F there are no unbounded families 
of increasingly high "towers" of exponentials. This condition may be relaxed 
by allowing t not to be in F; however the conclusion of the theorem would be 
weaker. 
Proof 
The proof follows closely the guidelines of Theorem 7.2, adequately 
adjusted to the given query bounds. We shall construct a set B such that 

164 
Relativization 
NTIME(F, B) contains a set L(B) which is not in the class specified by the 
enumeration {.M;} relative to B. This particular set is defined as follows: 
L(B) = {on I there is an x E B with Ixl = ten)} 
where t is the function specified in condition (b) above. 
Observe that L(B) E NTIME(F, B) via a nondeterministic oracle machine 
working in time t, which nondeterministically constructs a word of length ten) 
and queries it to the oracle. The injectiveness of t ensures that the memberships 
of two different words On and om to L(B) are not interdependent. 
The construction will be performed in stages, in an almost identical fashion 
to that of Theorem 7.2. It is presented in Figure 7.7. Denote by In the function 
in F bounding the number of queries of machine Mn. 
stage 0 
B(O) := 0 
k(O) := 0 
stage n 
let ken) be the smallest number such that In(k(n» < 2t(n) and 
ken) is greater than the length 
of any query of a machine M; on Ok(i), with i < n 
if ok(n) E L(Mn, B(n - 1» then B(n) := B(n - 1) 
else 
end stage 
let wen) be the first word of length t(k(n» 
such that A1n has not queried it to B(n - 1) 
in any computation on ok(n) 
B(n) := B(n -
1) U {w(n)} 
Figure 7.7 Construction for the proof of Theorem 7.8 
Define B as B := limn B(n). 
Observe that condition (b) ensures the existence of ken). There are 2t(k(n» 
words of length t(k(n». By hypothesis, A1n queries at most In(k(n» words in 
the total of its computations. By the conditions imposed on ken), the word wen) 
exists if needed. 
The argument that the acceptance or rejection of ok(n) by lvfn is the same 
under B(n -
1) as under B, as well as the proof that L(B) differs from every 
set accepted by some J..1n under B, are exactly as in Theorem 7.2. 
0 
In an analogous manner, a more general version of Theorem 7.3 can be 
proven, showing that again the crucial parameter is the number of queries allowed 
in the computations. See the bibliographical remarks at the end of this chapter. 

Refining Nondeterminism 
165 
Notice that the construction diagonalizes against possibly nondeterministic 
machines. Such machines are able to make 2!(n) potential queries in time fen). 
The theorem shows that they must be allowed to do so in order to cover all of 
NTIME(F, B); as soon as this capability is restricted, a diagonalization can be 
perfonned which makes their loss of power apparent. 
Thus, this bound on the number of queries suffices to diagonalize out of (or 
against) a class, i.e. to produce a nonmember by a diagonalization process. On 
the other hand, it will be shown in the next chapter that this query bound is 
also in some sense necessary to diagonalize against a class, in the sense that, for 
many interesting classes, a relativized separation of determinism from bounded 
query nondetenninism amounts to separating the unrelativized classes. 
We close this section with some applications of Theorem 7.8. All of them 
can be seen to be instances of this theorem with easily identified families of 
functions F. Of course, the first one is Theorem 7.2. 
Corollary 7.10 There is an oracle B such that PCB) =I NP(B). 
Corollary 7.11 There is an oracle B Jor which some set in NP(B) cannot 
be accepted in nondeterministic polynomial time with a polynomial number oj 
potential queries to B. 
Corollary 7.12 There is an oracle B Jor which some set in NP(B) cannot be 
accepted in nondeterministic polynomial time with only nc.logn potential queries 
to B. 
Corollary 7.13 There is an oracle B separating DEXT(B) Jrom NEXT(B). 
Corollary 7.14 There is an oracle B Jor which some set in NEXT(B) cannot 
be accepted in nondeterministic exponential time with a polynomial number oj 
queries to B. 
Classes like those indicated in the corollaries, in which bounds are placed on 
the number of queries, playa role in the positive relativizations to be presented in 
the next chapter. In fact, other consequences of this theorem will be presented 
there. Many other similar corollaries can be derived from the theorem. The 
reader will have little trouble in finding some of them. 
7.6 Refining Nondeterminism 
Now we know that some of the constructions in the first sections of this chapter 
are particular cases of more general theorems (like Theorem 7.8) which show 
that the bound on the number of queries is the ground hypothesis allowing this 
kind of construction. Of course, time bounds imply such bounds on the number 
of queries. The following may now be asked: are there other "reasonable" 
resources such that bounding them implies bounding the number of queries? 

166 
Relativization 
Broad potential query spaces are obtained by nondeterministically guessing the 
queries. This suggests that bounding the amount of nondetenninism allowed in 
each computation should imply a bound on the number of queries. 
We present next a very interesting way of obtaining proper hierarchies in 
this manner. The combination of nondeterminism and relativization gives extra 
power to the computation by broadening the potential query space. Thus, let us 
bound the amount of nondeterminism used in each computation. We will show 
here that this bound on the number of nondeterministic steps implies a bound 
on the number of potential queries, allowing a diagonalization to be performed. 
A small increase in the number of nondeterministic steps is enough to perform 
a construction like the previous ones. 
Our statements and proofs are neither the only ones nor the strongest pos-
sible; we just want to give a feeling of the way the nondetenninism bound is 
used to diagonalize over a class. Some other results related to this construction 
are proposed as exercises (Exercises 14 and 15), and more powerful and very 
interesting ones are pointed out in the bibliographical remarks at the end of the 
chapter. 
Let g(n) be a nondecreasing function on the natural numbers. We define the 
following concept: 
Definition 7.4 A nondeterministic machine M operates in nondeterminism g(n) 
if for every input x, any computation of 111 on x has at most g(lxj) nondeter-
ministic steps. 
Definition 7.5 For any set A, let P(A)g(n) be the class of sets L such that 
L E NP(A), and this fact is witnessed by a polynomial time oracle machine that 
operates in nondeterminism g(n). 
It can be shown that a bound on the nondeterminism which is "too small" 
with respect to the time bounds can be useless; a clear example of this fact is 
presented in Exercise 15. 
We are interested in classes P(A)g(n) where g(n) = n( for some e > 0. 
Recall that in our machine model each nondeterministic step has at most two 
possible continuations, i.e. the nondeterministic steps have fan-out 2. Notice 
that this implies that the binary tree of computations of machines working in 
nondeterminism 7-/ branches r/ times, and hence describes at most 2n' many 
computations. 
The following result shows that incrementing by 1 the degree of the poly-
nomial bounding the amount of nondeterminism suffices to leave enough room 
to diagonalize. 
Theorem 7.9 For every integer e > 0, there exists a set B such that P(B) .. ( =I 
P(B) .. (+l' 

Refining Nondeterminism 
167 
Proof For any set B let 
L(B) = {on I there exists a z E B such that Iz I = nl +1} 
Using an argument like the one in Theorem 7.2, we obtain that this set 
L(B) E P(A) .. t+1. We shall prove that there is a B such that L(B) rj. P(B) .. t. 
It is clear that an "alarm clock" which counts the number of nondeterministic 
steps can be added to any nondeterministic machine, aborting the computation 
when nl is reached. This yields a machine working in nondeterminism nl. The 
process is the same as for adding polynomial time clocks, and is obviously 
effective. 
Let QJ. Q2, ... be an enumeration of nondeterministic polynomial time 
oracle machines which work in nondeterminism nl. For every i, let Pi be a 
polynomial which bounds the running time of Qi. The construction is described 
in Figure 7.8. 
stage 0 
B(O) := 0 
k(O) := 0 
stage n 
let ken) be the smallest number such that 
l 
l+1 
2n 
• Pi(n) < 2n 
and Pn-1(k(n - 1» < ken) 
if ok(n) E L(Qn, B(n - 1» then B(n) := B(n - 1) 
else let wen) be the first word of length k(n)l+1 
such that Qn has not queried it to B(n -
1) 
in any computation on Ok(n) 
B(n) := B(n -
1) U {w(n)} 
end stage 
Figure 7.8 
Construction for the proof of Theorem 7.9 
The existence of such a string wen) is guaranteed by the fact that there are 
at most 2nl possible computations of Qi on on, which query at most Pi(n) times 
(the length of the computation). Hence, the number of queries that can be made 
in the whole set of computations is at most 2nl . Pi(n), which by the choice of 
ken) is strictly less than 2nl+l; but this is the number of candidates for wen). 
The remainder of the proof is identical to that of Theorem 7.2 and is left to the 
reader. 
0 
This last theorem reinforces Theorem 5.8 in the sense that it shows that the 
cause of separation is not running time but rather the number of nondeterministic 
steps in which the machine writes on the query tape. Similar results for bounding 
nondeterminism by other functions different from polynomials are presented in 
Exercise 14. 

168 
Relativization 
7.7 Strong Separations 
The separations proven in many of the theorems in this chapter have the following 
form: given a relativized complexity class, a set is constructed which is not in 
the class. In this section, we address the following question: how different may 
this set be of the sets from the class? The construction ensures that the set 
differs from each set in the class by at least one word. Usually, the classes for 
which these constructions are presented are closed under finite variations, so that 
the set not in the class must differ from every set in the class on infinitely many 
words. 
However, it still may be the case that we can approximate a nontrivial part 
of the set by a set in the class. 
In fact, one can ask whether the set that witnesses the separation has no 
infinite subset in the smaller class. This leads us to look once more into Re-
cursive Function Theory, and borrow the concepts of immunity and simplicity, 
defined in Volume I, to study the structural properties of complexity classes. 
These concepts were of great interest in Recursive Function Theory, allowing 
the construction of noncomplete sets in the classes of the arithmetic hierarchy 
with respect to unbounded m-reducibility. We will show here that the construc-
tions in this chapter can be improved so that the separation among the classes 
are witnessed by immune or simple sets. Separations witnessed by immune or 
simple sets are usually called "strong separations". 
In our constructions in this section we will use a different diagonalization 
technique, usually called "slow diagonalization". It is quite similar to the so-
called "wait and see" arguments of Recursive Function Theory. 
This kind of diagonalization is characterized by not covering at each step a 
partial objective. Instead, the construction waits until it finds a step in which an 
objective can be met. It may be the case that some objectives are never met; 
then it is argued that it was unnecessary to attain these objectives. 
This technique can be applied to most of the cases developed earlier in this 
chapter, as indicated below. We start with the strong separation between P and 
NP. 
Theorem 7.10 There exists a recursive oracle A such that NP(A) has a set 
that is P(A)-immune. 
Proof The set to be proved to be P(A)-immune is defined exactly as in The-
orem 7.2: 
L(A) = {on I there is a word x E A with Ixl = n} 
As we have seen, for every A, L(A) is in NP(A). Let us observe that L(A) 
consist only of words of the form on. During the construction every set in peA) 
consisting only of such words is forced either to be finite, or to contain a word 
on which is not in L(A). This will imply that L(A) is P(A)-immune. 

Strong Separations 
169 
Through the construction, we will keep a list S of polynomial time deter-
ministic machines (more precisely, of indices of such machines) which are still 
"dangerous", i.e. may accept an infinite subset of L(A). Every index eventually 
enters this list. As soon as a machine is proved to accept a set which is not 
included in L(A), it is cancelled by removing it from the list S. Afterwards, 
it is proved that if a machine accepts an infinite set then its index is removed 
sooner or later from S. Of course, S changes with time; thus S(n) denotes the 
contents of S during stage n. 
The construction is shown in Figure 7.9. 
stage 0 
A(O) := 0 
S(O) := 0 
k(O) := 0 
stage n 
S(n) := S(n -
1) U {n} 
let k(n) be the smallest number such that 
Li<n Pi(k(n» < 2k(n) and Pi(k(n -
1» < k(n)Vi < n 
if there Is an i E S(n) such that 
then 
ok(n) E L(Pi • A(n - 1» 
let i(n) be the smallest such i 
S(n) := S(n) - {i(n)} 
A(n) := A(n -
1) 
else let w(n) be the first word of length k(n) 
end stage 
such that no Pi, i ~ n, has queried it to A(n - 1) 
in the computation on ok(n) 
A(n) := A(n -
1) U {w(n)} 
Figure 7.9 Construction for the proof of Theorem 7.10 
Notice that we are not diagonalizing over one Pi at each stage, as we did 
in the proof of Theorem 7.2; instead we wait until we find a stage in which Pi 
accepts Ok(n), maintaining meanwhile the index i in the list S(n). When we find 
this accepted word ok(n), then we diagonalize over Pi, leaving L(A) empty of 
words of that length. This ensures that L(Pi , A) is not included in L(A). 
The existence of a word w(n) as in the program is assured because on input 
ok(n), each Pi can query its oracle at most Pi(k(n» times, while the number of 
words of length k(n) in {O, 1}* is 2k(n). 
Moreover, the conditions on k(n) imply that k(n) is larger than the length 
of any string queried at any previous stage. Thus, the behavior of machines Pi, 

170 
Relativization 
i < n on ok(n) relative to A(n - 1) is the same as their behavior on ok(n) relative 
to A. 
We must prove that L(A) is infinite and that no infinite subset of L(A) is 
in peA). Since L(A) is infinite if and only if A is infinite, it suffices to prove 
the infinitude of A. Suppose that A is finite. Then there exists an no such that 
after stage no the construction never follows the "else" part of the algorithm. 
Thus, at each stage after stage no one index enters S and one index leaves S, 
and the size of S remains constant. But there are infinitely many indices i with 
the following property: for all sets B, L(Pi , B) = 0. Note that no such index 
may ever leave S, and hence the size of S must grow unboundedly. This is a 
contradiction. Hence, A must be infinite. 
Let j be any index of a polynomial time oracle machine. If 1(11, A) contains 
a word which is not of the form ok(n), then it cannot be included in L(A). 
Suppose that all the words of L(Pj , A) are of the form ok(n) and that L(Pj , A) 
is infinite. Let no be the first stage of the construction satisfying the following 
three conditions: 
(a) 
j:::; no (which implies that j already entered S), 
(b) 
no is great enough so that all of the finitely many indices less than j either 
have already been removed from S or will remain in S forever, and 
(c) 
Ok (no) E L(Pj , A). 
Since the computation of Pj on ok(no) is undisturbed by words added to A 
at stage no or later, we infer that ok(no) E L(Pj , A(n - 1». Thus, at stage no 
the construction follows the "then" part and makes i(n) equal to j; then goes 
on with no change in the oracle. This implies that no word of length keno) 
enters the oracle at any moment of the construction, and hence ok(no) ~ L(A). 
Therefore L(Pj , A) is not included in L(A), because the word Ok (no) is in the 
former but not in the latter. 
Therefore, the only way L(Pj , A) may be included in L(A) is by being a 
finite set. This shows that L(A) is P(A)-immune, as was to be shown. 
0 
In a similar manner, a relativization in which an NP-simple set exists can 
be constructed. The proof combines the slow diagonalization of the previous 
theorem with the proof of Theorem 7.3. Some technical problems appear, and 
we first discuss these problems and how to solve them. 
In the proof of Theorem 7.10, the P-immune set constructed has a very low 
density. This is used to show that potential subsets must consist of just very few 
words of a very particular form. In constructing an NP-immune set, we want to 
have the same property of low density. 
However, the way words are added to this immune set in the previous con-
struction is by adding one word of a given length to the oracle, or keeping it 
empty at this length. In fact, the immune set may be defined by the follow-
ing problem: "given a length n in unary notation, check whether the oracle is 

Strong Separations 
171 
nonempty at this length". But this nonemptiness property can be expressed by 
an existential quantifier, and we need a set in co-NP immune for NP. We should 
use a "universal" property of the oracle. The one that immediately pops into 
mind is the complementary property. We obtain the following candidate to be 
an NP-immune set relative to A: 
L(A) = {on I no word of length n is in A } 
However, this solution is not completely satisfactory. Consider a construction 
like the previous one: starting it with the empty oracle provides a set that 
from the very beginning contains all of 0·, which is far from our requirements. 
Therefore, the construction must start from something else-an oracle having 
words of every length. Our construction starts from 0·. 
Now the idea of the construction is as follows. At each stage, we choose a 
suitable length for the diagonalization. Then we add on to L(A) by removing 
it from A, and check whether on should stay in L(A). If we can diagonalize 
over a set in NP which accepts on, we remove it again from L(A) by adding an 
unqueried word of length n to A; otherwise, we leave on in L(A) for the sake 
of making it infinite. It can be seen that if these operations are performed in 
another order, the construction might not work. 
We proceed to the formalization of this argument. 
Theorem 7.11 There is a recursive oracle A. such that NP(A) contains a sim-
ple set. 
Proof. 
Define L(A) as above. Then L(A) is in co-NP(A). We construct A so 
that L(A) is NP(A)-immune, therefore the complement of L(A) is NP(A)-simple. 
As the constructed set may change twice in each stage, we introduce a tem-
porary ("local") variable T to hold the provisional value of A(n) through stage 
n. The meaning of the remaining variables is as in the previous construc-
tions. Construct A as limn A(n), where A(n) is obtained after performing stages 
1,2, ... ,n, ... as in Figure 7.10. 
The proof that the computations do not change from the set T to the full 
oracle A, and the consequent proof that L(A) becomes NP(A)-immune (i.e. L(A) 
is infinite but contains no infinite set in NP(A» follow the same guidelines as 
the previous theorem, and are left as an exercise (Exercise 16). 
0 
Thus we have a strong separation between NP and co-NP. Using similar 
ideas, all the statements presented up to now in this chapter can be reproduced 
in such a way that every separation between two classes is witnessed by a set in 
one of them which is immune for the other. Some of these results are proposed 
as exercises; see the references for the remaining results of this kind. 

172 
Relativization 
stage 0 
A(O) := O· 
S(O) := 0 
k(O) := 0 
stage n 
Sen) := Sen -
1) U in} 
let ken) be the smallest number such that 
Pi(k(n» < 2k(n)Yi < nand PiCken - 1» < k(n)Yi < n 
T := A(n -
1) - {Ok(n)} 
if there is an i E Sen) such that ok(n) E L(NP i, T) 
then 
else 
let i(n) be the smallest such i 
fix an accepting computation of NP;(n) on ok(n) 
Sen) := Sen) - {i(n)} 
let wen) be the first word of length ken) which has not 
been queried to T during the fixed computation 
A(n) := T U {wen)} 
A(n) := T 
end stage 
Figure 7.10 Construction for the proof of Theorem 7.11 
7.8 Further Results in Relativizations 
In the light of the previously presented results, we can conclude that relativized 
computations allow us to prove interesting results that explain the properties 
of nondeterministic computation. It is not surprising that the same paradigm 
has been widely used to prove many results about complexity classes and the 
structure of resource-bounded reducibilities. In this section we review some of 
the results that can be obtained by relativization. A warning must be issued 
here that we present a very reduced subset of the interesting results which can 
be found in the literature. Even the list of references provided for this chapter 
will be highly incomplete. A systematization of all of this material is needed, 
but this is too big a goal to be attempted here. We apologize to the reader if 
his/her favourite relativization result is missing from this very limited list. 
First we should indicate that all the possible inclusion relationships between 
P, NP, co-NP, and NP n co-NP, consistent with the known inclusions, hold in 
some relativization. Some of them have already been presented The others are: 
Theorem 7.12 There exist oracle sets such that: 
1. 
P(E)::/ NP(E) but peE) = NP(E) n co-NP(E). 

Further Results in Relativizations 
173 
2. 
P(F) =I NP(F) n co-NP(F) and NP(F) =I co-NP(F). 
Similarly, when the probabilistic complexity classes are added to the picture, 
every inclusion and every separation which is consistent with the known rela-
tionships can be obtained for the appropriately constructed oracle. Separations 
in higher levels of the polynomial time hierarchy have been also proved: 
Theorem 7.13 There exist oracle sets such that: 
1. 
NP(A) =I co-NP(A) but .d2(A) = 172 (A). 
2. 
.d2(A) =I 172(A). 
The separation at the second level was for long time the highest separation 
achieved: 
Theorem 7.14 There exists an oracle set such that 172(A) =l112(A). 
Very recently, a probabilistic argument applied to the boolean circuit com-
plexity of parity and other boolean functions allowed a proof of the following: 
Theorem 7.15 There exist oracle sets such that: 
1. 
The polynomial time hierarchy relative to A differs from PSPACE(A). 
2. 
The polynomial time hierarchy relative to A consists of infinitely many 
different levels. 
It is difficult, but possible, to combine the coding methods with these prob-
abilistic circuit arguments, and the following result has been obtained: 
Theorem 7.16 For any "consistent" set of strict inclusions and equalities 
among the classes of the polynomial time hierarchy, there is a relativization 
in which all these relationships hold. 
Here "consistent" means that it does not contradict the known relationships 
and implications in the polynomial time hierarchy, described in Chapter 8 of 
Volume r. "Strong" versions of all these theorems have been also proved, in 
which every separation is witnessed by some kind of immune set. 
In a different line of research, the question was investigated of "how frequent" 
are the oracles achieving a given goal. For example, it was first shown that the 
set of oracles which make P = NP is "meager" (a topological concept which 
can be interpreted as "such sets are infrequent"), and later that given an oracle at 
random the probability of having P =I NP for this oracle is 1. In view of these 
results, the conjecture was raised that, roughly speaking, "a relativized statement 
which is true with probability 1, is true for the empty oracle", which would imply 
P =I NP. This conjecture is known as the Random Oracle Hypothesis. However, 
the use of double relativizations (i.e., with two oracle sets available) allows one 

174 
Relativization 
to disprove certain versions of the conjecture. Whether any other more restricted 
form of this conjecture holds is a research problem. 
In a more philosophical vein, the contradictory answers to the P J NP 
problem and to similar problems in different relativizations lead to a discussion 
about their correct interpretation. Obviously, they imply that any answer to the 
problem in question must use a technique that does not translate directly to the 
relativized versions of the complexity classes. This suggest a screening of the 
techniques currently used in complexity theory, classifying them into relativizing 
and nonrelativizing. For example, results like the Uniform Diagonalization The-
orem or the arguments about NP-complete sets and p-cylinders are easily shown 
to relativize, while some combinatorial or counting arguments as those used 
to prove that nondeterministic linear time is more powerful than deterministic 
linear time in Chapter 3 of this volume may not relativize. 
However, more general statements like "diagonalization always relativizes" 
or "direct simulation always relativizes, but indirect simulation does not" are 
more difficult to argue. Although such statements can be found sometimes in 
the literature, most of them are countered with strong arguments in different 
papers. We prefer to avoid taking such a strong position; instead, we would 
like to point out results in the flavor of Theorem 7.8, which seem to us to say 
that the correct interpretation is as follows: relativization plus non determinism 
form too strong a combination to trust Their joint power is much greater then 
the sum of their individual powers. We mean by this that the "relativizing 
principle" of Recursive Function Theory ("if such and such statements hold for 
the recursive sets, they hold for the sets recursive in A."), which is true unless 
highly powerful concepts are in use, should be systematically questioned in 
Computational Complexity Theory. 
In the next chapter we will show that results like Theorem 7.8 can be com-
plemented with other results which suggest different ways of stating appropriate 
(and provable) weaker versions of relativizing principles. So, we will argue that 
relativizing techniques might still be useful to prove statements which relativize 
in conflicting ways, under the condition that the relativization of these techniques 
obbeys the restrictions indicated by the appropriate relativizing principles. This 
position will become clearer at the end of the next chapter, after presenting 
several interesting definitions and showing their properties. 
7.9 Exercises 
1. 
Show the existence of oracles making P = NP by constructing a set A 
such that A = K(A). Use the Recursion Theorem to prove it. Provide an 
alternative, constructive proof of such a set decidable in exponential time. 
2. 
Use theorem NP .; co-NP relativized to show that NP-T-reducibility, pre-
sented in Definition 4.4 of Volume I, is not transitive. 
3. 
Complete the proof of Lemma 7.1. 

Exercises 
175 
4. 
·Show that there is a relativization in which P is strictly included in 
NP n co-NP. Hint: Define L(A) so that membership in it depends on the 
existence of a word of even length in the oracle, while nonmembership 
depends on the existence of a word of odd length. Take care that the 
oracle is such that these two ways of defining L(A) are consistent. 
5. 
·Use the idea of the last exercise to show that there is a relativization in 
which P is strictly included in ZPP. 
6. 
Identify those oracles in the theorems presented in the text that make ZP P 
different from R. 
7. 
·Construct an oracle for which R is equal neither to P nor to NP. Hint: 
Alternate stages devoted to the first goal with stages devoted to the second. 
8. 
Use Lemma 7.1 to show that there is a relativization in which P =I NP = 
PP. 
9. Define a concept of one-sided c-error machine under A on x, suitable for 
applying to R the concept of critical word presented for BPP. Define such 
R-critical words, and prove an analog to Lemma 7.2 for these words. 
10. Use the previous exercise to present a direct proof of Corollary 7.9, adapt-
ing the construction of Theorem 5.7 to R. 
11. Find a machine 111, an oracle A, and a word w such that 111 witnesses the 
membership of L(M, A) to BP peA), but has unbounded probability of 
error under oracle A~{ w}. 
12. Let 111; be an effective enumeration of oracle machines that always halt, 
such that for every i and finite set F, a new index i' can be effectively 
found fulfilling: VA L(A1;" A) = L(M;, A~F). Let M be an oracle 
machine, and suppose that there is an arbitrary oracle A such that for 
every i, L(M, A) =I L(1I1;, A). Show that the same must hold for some 
recursive oracle. 
13. Use the techniques of Chapter 4 of Volume I to derive Corollary 7.13 
directly from the proof of Theorem 7.2. 
14. 
Show Theorem 7.9 for the classes defined by polynomial time and bounds 
of the form logl n, e > 1, on the nondeterministic steps. 
15. Show that the condition e > 1 in the previous exercise is necessary, by 
proving that polynomial time machines making at most log n nondeter-
ministic steps can be simulated by polynomial time deterministic ones, 
and that the proof relativizes to every oracle. 
16. Complete the proof of Theorem 7.11. 
17. • Show that there exists a recursive set A such that NP(A) contains simul-
taneously an NP(A)-simple set and a P(A)-immune set. Hint: Use even 
numbered stages to perform a diagonalization as in Theorem 7.10, and 
odd numbered stages to diagonalize as in Theorem 7.11. 
18. 
·Generalize Lemma 7.1 to witness P =I NP by a P(A)-immune set in 
NP(A). Use this "strong" version of Lemma 7.1 to show the existence of 
oracles such that NP(C) = co-NP(C) and NP(C) contains a P(C)-immune 

176 
Relativization 
set, PSPACE(C) = NP(C) and NP(C) contains a P(C)-immune set, and 
PP(C) = NP(C) and NP(C) contains a P(C)-immune set. 
7.10 Bibliographical Remarks 
The first results about relativizations were reported in Baker, Gill, and Solovay 
(1975). Theorems 7.1, 7.2, 7.3, and 7.4 appeared there, together with many of 
their corollaries and with other results showing, under the appropriate relativiza-
tion, all possible relationships between P, NP, co-NP, and NP n co-NP. The 
techniques used in in this paper became seminal tools to separate relativized 
complexity classes. Lemma 7.1 is a more abstract version of a construction of 
these authors, which allows one to obtain similar results using other classes. 
Exercise 1 is also from this reference. 
Relativizations of probabilistic classes have been studied by Rackoff (1982), 
Geske and GroHman (1986), and Balcazar and Russo (1988), among others. Our 
presentation builds on the basic ideas of Rackoff, exposed as in Balcazar and 
Russo. Additional results are presented in these references. We should remark 
that an oracle can be built for which NP equals EXPTIME, and that such oracle 
proves simultaneously several of the results presented here (Theorems 7.4, 7.5 
and 7.10). 
Theorems 7.8 and 7.10, which generalize the constructions using bounds 
on the number of queries or on the number of nondetenninistic steps, are the 
main representatives of a family of results presented in Kintala and Fischer 
(1980), Xu, Doner, and Book (1983), and SchOning and Book (1984). More 
information on classes defined by bounded nondetenninism can be found in Dfaz 
and Toran (1987). In particular, the first of these papers shows the existence 
of infinite hierarchies under relativization for increasing families of functions 
bounding the nondeterministic steps of polynomial time machines; the second 
shows how to translate these results to many other classes by proving a very 
general Separating Theorem, and the third uses slow diagonalization to obtain 
Theorem 7.10 and strong versions of Theorems 7.8 and 7.9 (the First and the 
Second Immunity Theorems). Theorem 7.11 and "simplicity" versions of these 
immunity theorems can be found in Balcazar (1985). Most of the exercises are 
from the same references. 
It should be pointed out that the study of strong separations has a particular 
interest when complexity theory is applied to cryptography-an application we 
do not deal with in this book. Let us just inform the reader that the nonexistence 
of P-immune sets in NP can be related to the question of whether every public 
key cryptosystem can be broken infinitely often, which is an undesirable property 
of these systems. Material related to public key cryptosystems can be found in 
Kranakis (1986), and results showing the applicability to cryptocomplexity of 
the relativizations of complexity classes are presented in Brassard (1983). 

Bibliographical Remarks 
177 
The remaining theorems in this chapter, stated without proof, have been taken 
from various sources. Theorem 7.12 appears already in Baker, Gill, and Solovay 
(1975). Theorem 7.13 appears in Heller (1984), and Theorem 7.14 in Baker and 
Selman (1979). Relationships to exponential time classes appear in Dekhtyar 
(1976), Gasarch and Homer (1983), Lischke (1986), and Lischke (1987). Strong 
separations up to the second level of the polynomial time hierarchy were first 
obtained in Torenvliet (1988), where relativized bi-immunity is also discussed. 
A stronger version of Exercise 17, in which both properties are exhibited by the 
same NP set, appears in Torenvliet and van Emde Boas (1989). Theorem 7.15 
has been obtained by Yao (1985), and is based on previous results of Furst, Saxe, 
and Sipser (1984); see also the results by Hastad (1987) and Ko (1988). Many 
of these papers contain interesting related results. "Meagerness" considerations 
of sets of oracles were studied in Mehlhorn (1973), and relativizations with 
probability 1 were originated in Bennet and Gill (1981). Many other results 
have been proved to hold with probability 1. This same paper raises the Random 
Oracle Hypothesis; results that disprove this conjecture (at least in some form) 
can be found, among others, in Kurtz (1983). The complete list of papers 
presenting some interesting property of complexity classes which holds or fails 
under relativization amounts to many dozens: we beg the permission of the 
reader to stop our listing here. 
Discussion on the philosophical implications of the results on relativizations 
can be found in virtually all the papers dealing with the subject. 

8 
Positive Relativizations 
8.1 Introduction 
This chapter presents an attempt to fully understand the implications of the 
results about contradictory ways of relativizing statements involving complexity 
classes. The key point of this chapter is given by the following question: what 
do the proofs in the previous chapter really say? What are the properties of 
complexity classes that make the proofs work? Are there intrinsic characteristics 
of complexity classes that explain the meaning of these results? 
We have seen that there are sets A such that peA) = NP(A) and there are 
sets B such that PCB) =I NP(B), and similarly for other complexity classes. One 
question that may come to our minds is whether the study of relativized com-
plexity classes can help us to answer questions about unrelativized complexity 
classes. Indeed, one might argue that these contradicting relativizations could 
be interpreted as saying that the study of relativization is useless for solving the 
main problems of complexity theory. 
Yet our aim in this chapter is to try to convince the reader that there are 
things to learn from relativization. As indicated in the last section of the previous 
chapter, the results regarding bounds on the number of queries and on the number 
of nondeterministic steps can be complemented with others yielding a clear 
interpretation of the separations presented there. In this chapter we present 
some of these complementing facts. 
In the proof of Theorem 5.2, the set B was constructed in such a way that 
for every i, the deterministic machine of index i rejects [f(i) relative to B if and 
only if there is a string of length k(i) in B, and this if and only if Ok(i) E L(B). 
On the other hand, for any set A, the set 
L(A) = {x I there exists yEA such that Iyl = Ixl} 
is in NP(A). Thus, the nondeterministic oracle machine can search for a word 
from a set of size 21xl in linear time, in a nondeterministic fashion. In general, 
this is not possible for a polynomial time deterministic oracle machine, due 
to the fact that the polynomial time bound implies a bound on the number of 
potential oracle queries. 
In the previous chapter we have shown that this property can be used for 
separating relativizations of complexity classes, and Theorems 7.8 and 7.9 show 

Introduction 
179 
that in fact the key to the technique is the bound on the number of queries, 
either explicit or implicit in the bound on the amount of nondeterminism. In 
this chapter we will show that this difference between the sizes of the oracle 
query spaces is not only sufficient, but also in some sense necessary to obtain 
relativized separations of complexity classes. 
The central concept is that of positive relativization. This is not a formal 
concept, but an intuitive property of complexity classes. Let us describe it 
briefly; several examples are shown in this chapter which will clarify it. Let C 
and V be complexity classes, defined by some kind of oracle machines. Assume 
that there is some restriction to be imposed on the machines defining C and V, 
and let CR and V R be the classes defined by machines obeying the restriction. 
Use C, V, CR, and V R also to denote their respective relativizations to the empty 
set (i.e. unrelativized classes), and use C(A), D(A), CR(A), and VR(A) to denote 
their relativizations to the oracle A. Assume that the following holds: 
1. CR = C 
2. 
V R = V 
3. 
CR = V R if and only if for every oracle set A, CR(A) = VR(A). 
Then we say that the restriction R provides a positive relativization of the 
? 
problem C == V. 
Intuitively, C and V are classes for which it is an open problem whether 
C = V, but there is a way of making them different under relativization. It may 
be the case that the construction of such relativization requires some particular 
property. Then we may raise the following question: is this property absolutely 
necessary for the result obtained, or is it just a requirement of the proof technique 
used? 
An answer may be obtained by placing on the machines some restriction 
that prevents the required property, and checking whether it is a positive rela-
tivization. If this is the case, to show a relativized separation of the restricted 
classes amounts to separate the unrelativized classes. Indeed, if an oracle A can 
be found for which CR(A) =lVR(A), then from the three properties above we 
obtain C =IV. In this way, we could learn interesting information about the 
characteristics of the techniques that would be able to show C =IV. 
Thus, later in this chapter we will show that the bounds on the number of 
queries which allow the constructions of the previous chapter to be performed 
are not a requirement of the proof techniques: they are necessary for obtaining 
the results in the sense that such bounds provide positive relativizations of the 
standard problems. 
In classical Recursive Function Theory there is a principle which holds unless 
highly powerful concepts are in use: if a nonrelativized statement holds, it will 
hold in every relativization. The results in the previous chapter show that no such 
relativizing principle holds directly in Complexity Theory. However, particular 

180 
Positive Relativizations 
fonns of this principle may hold if appropriate restrictions are placed on the 
oracle machines. Thus, it makes sense to add a fourth clause to the list above: 
4. CR =lVR if and only if for every oracle set A, CR(A) =lVR(A). 
If for a restriction R, yielding a positive relativization, also this fourth clause 
holds, then R will be called a relativizing principle. Such restrictions are more 
infrequent than positive relativizations, but we will present one here. A different 
kind of relativizing principle, in which the restriction is placed on the class of 
oracle sets instead of on the oracle machines, will be presented in the next 
chapter. 
8.2 A Positive Relativization of the P :l: PSPACE Problem 
Consider a nondetenninistic oracle machine 111 working in time ten). It has been 
said repeatedly that two parameters are bounded by the running time ten): the 
number of oracle queries allowed and the number of nondeterministic steps in 
which queries can be generated. Here we want to impose bounds on the number 
of queries directly, as opposed to bounding them indirectly by the running time. 
Oracle machines that have bounds on the number of times the oracle can be 
queried will be referred to as bounded query machines. We will use the set no-
tation Q(M, A, x) presented in Definition 7.3 to define bounded query machines 
and classes. 
Definition 8.1 Let A. be any oracle set. Define PQUERY (A) to be the class 
of sets accepted by those deterministic oracle machines using oracle A which 
use a polynomial amount of work space and which make a polynomial number 
of oracle queries, i.e. polynomial space machines M such that Q(M, A, x) has 
at most p(Jxi) elements for some polynomial p. 
Observe that such a machine may run for exponential time, since there are 
exponentially many configurations. Therefore, the query bound is meaningful, 
since PSPACE(A) machines may query an exponential number of times. On the 
other hand, polynomial time machines surely use polynomial space and make 
polynomially many queries, and of course polynomial space machines that never 
query the oracle also fulfill the condition. Therefore, we obtain: 
Proposition 8.1 For every set A, 
/PSPACE~ 
P 
PQUERY (A) -
PSPACE(A) 
~ 
peA) 
/ 
where each arrow denotes an inclusion. 

A Positive Relativization of the P l PSPACE Problem 
181 
For machines that do not query the oracle, it is irrelevant to bound.the number 
of queries or not to do so. Therefore: 
Proposition 8.2 PQUERY(0) = PSPACE. 
Observe that the restriction of making only polynomially many queries can be 
considered as indicated in the description of positive relativizations. Indeed, the 
unrelativized classes P and PSPACE directly obey the restriction: in the notation 
of the previous section, this is CR = C and 'DR = 'D. The class PQUERY(A) 
is then one of the restricted relativized classes, PQUERY(A) = 'DR (A), while 
the other is not affected by the restriction: P(A) = CR(A). Theorem 8.2 below 
corresponds to the third condition of the definition of positive relativization: 
CR = 'DR if and only if for every oracle set A, CR(A) = 'DR(A). 
Before proving the fact that this is a positive relativization of the P J, 
PSPACE problem, we need a technical characterization of the class PQUERY. 
The proof of Theorem 8.2 below is based on it. It uses the PSPACE-complete 
set QBF defined and studied in Chapter 3 of Volume 1. 
Theorem 8.1 For every oracle set A, PQUERY(A) = P(QBF EB A). 
The intuition behind this theorem is that a "pquery computation" can be 
split into polynomially many phases, each of them coresponding to non-oracle 
polynomial space computations (which can be simulated in polynomial time 
using the PSPACE-completeness of QBF), separated by queries to A (that are 
solved by directly querying A). Thus, given the compound oracle, polynomial 
time will suffice. A more formal proof follows. 
Proof. Consider the inclusion from right to left. Given a polynomial time 
machine with oracle QBF EB A, transform it into a machine M that uses a 
deterministic polynomial space machine to solve the oracle queries to QBF. M 
is a polynomial space machine, and queries .4. as many times as the original one 
did; since that was a polynomial time machine, it made only polynomially many 
queries. Thus we obtain a polynomial space and polynomial query bounded 
machine. 
Conversely, let L be a set in PQUERY(A). Let M be such that LCM, A) = L, 
and let p and q be the polynomials bounding respectively the amount of work 
space and the number of queries in each computation. Using Theorem 2.4 of 
Volume I, we can assume that every computation of AI halts, and is either ac-
cepting or rejecting. Use the alphabet {a, I} to encode configurations of M. 
For each configuration I of M, consider the segment of computation which be-
gins at I, ends at a configuration J which is either a query configuration, an 
accepting configuration, or a rejecting configuration, and has no query configu-
rations other than J. Since AI is deterministic, this segment of computation is 
uniquely defined. Define the function leap(I) as the configuration J which ends 
the corresponding segment of computation, and define the following set: 
B = {(I, x) I x is a prefix of leap(I)} 

182 
Positive Relativizations 
input x 
I := initial configuration of Al on x 
loop 
construct J := leap(I) using the subroutine 
exit the loop if J is a halting configuration 
comment: else, J is a query configuration 
let w be the word queried in J 
query A about w 
I := successor of J depending on the answer of A 
end loop 
comment: J is a halting configuration 
accept or reject according to J 
end 
Figure 8.1 Using "leap" to perform a simulation 
B can be decided in polynomial space as follows: starting at I, simulate 
M until a query or halting configuration is reached -i.e. compute leap(I)-
and check that x is a prefix of the obtained configuration. Thus, B is reducible 
to QBF in polynomial time, and a machine with an oracle for QBF can obtain 
information about B by using the reduction to compute the queries to QBF. 
Now the procedure of Figure 8.1, combined with the subroutine presented in 
Figure 8.2, accepts the same language as Af in polynomial time using oracle 
QBF Ef) A. 
construction of 1eap(l): 
y := >. 
while y does not encode a configuration and iyi ~ p(ixi) do 
if (I, yO) E B then y := yO 
else y := yl 
end while 
if y encodes a configuration then return it 
else reject 
comment: y is too long 
end subroutine 
Figure 8.2 Computing "leap" using oracle B 
The loop in the subroutine is obviously polynomially bounded. To show 
that the loop in the main procedure is also polynomial, notice that the number 
of times it is performed is one more than the number of queries M makes on 

A Positive Relativization of the P .? PSPACE Problem 
183 
input x, since each query is treated in one iteration of the loop. Thus, L belongs 
to P(QBF EB A). 
0 
From this characterization, we can derive the following theorem. Notice that 
it corresponds to the third condition in the definition of positive relativization. 
Theorem 8.2 
P = PSPACE if and only if for every oracle set A, peA) = 
PQUERY(A). 
Proof 
Since P(0) = P and PQUERY(0) = PSPACE, the proof from right 
to left is trivial: just take A to be empty. Let us prove the other direction. 
Assume that P = PSPACE. This implies that QBF E P, and therefore there is 
a polynomial time machine M deciding it. Now, every polynomial time oracle 
machine using as oracle any set of the form QBF EB A can be transformed into 
a polynomial time machine accepting the same language and using only the 
oracle A, by calling M whenever a query to QBF is made and solving it in 
polynomial time. Therefore, for every oracle A, P(QBF EB A) is included in 
peA). The converse inclusion is obvious. The result now follows immediately 
from Theorem 8.1. 
0 
Thus, the restriction of making polynomially many queries is a positive rel-
ativization of the P = PSPACE problem. Observe that in the previous chapter 
several oracles were exhibited separating P from PSPACE. Theorem 8.2 now 
indicates that if any of these constructions could be adapted to the case of 
PQUERY, constructing an oracle A for which peA) =I PQUERY (A), the separa-
tion of P from PSPACE unrelativized would follow immediately. 
We have claimed that this kind of result complements those obtained in the 
previous chapter. To support this view, we sketch the proof of the fact that, 
while separating relativizations of P from PQUERY is as difficult as separating 
P from PSPACE without relativization, separating PQUERY from PSPACE is 
nonetheless possible under relativization: 
Theorem 8.3 There is an oracle A such that PQUERY(A) =I PSPACE(A). 
Proof 
Apply Theorem 7.8 to an effective enumeration of machines describing 
PQUERY(A). Such an enumeration can be obtained by clocking machines with 
polynomials which stop them when a previously computed maximum number of 
queries is reached. Take as F the collection of polynomials. The hypotheses of 
Theorem 7.8 are easily seen to hold, and therefore there is an oracle B and a set 
in NP(B) which is not in PQUERY(B). Since NP(B) is included in PSPACE(B), 
the result follows. It is left to the reader (Exercise 1) to formalize and complete 
this proof. 
0 

184 
Positive Relativizations 
8.3 A Positive Relativization of the NP l PSPACE Problem 
We obtain in this section a positive relativization of the NP J. PSPACE problem, 
rather similar to that obtained in the previous section. We use nondeterministic 
machines and bound polynomially the number of queries and the work space. 
Although many arguments are very similar to those presented in the previous 
section, we consider that the differences between the proofs are interesting and 
instructive, and that it is worth presenting both here. However, in the parts that 
are similar enough to those of the preceding section, we will present the proofs 
in abbreviated form. 
Definition 8.2 Let A be any oracle set. Define NPQUERY(A) to be the class 
of sets accepted by those nondeterministic oracle machines using oracle A which 
use a polynomial amount of work space and make a polynomial number of oracle 
queries in each computation. 
Again such machines may run for exponential time, and the query bound 
is meaningful. Observe that a stronger condition might be imposed that only 
polynomially many queries appear in the full computation tree; this condition 
would be too restrictive, since we want to give the PSPACE machines the same 
capabilities of oracle access as polynomial time nondeterministic machines in 
order to obtain the desired positive relativization. 
The following relationships are immediate: 
Proposition 8.3 For every set A, PSPACE ~ PQUERY(A) ~ NPQUERY(A). 
Moreover, 
/PSPACE~ 
NP'-....... 
./ NPQUERY(A.) -
PSPACE(A.) 
"--.. NP(A) / 
where each arrow denotes an inclusion. 
The fact that NPQUERY(A) is included in PSPACE(A) follows from the 
relativized version of Savitch's Theorem (Theorem 2.9 of Volume J), since the 
machines defining NPQUERY(A) are nondeterministic polynomial space ma-
chines. 
Once more the unrelativized case equals PSPACE: 
Proposition 8.4 NPQUERY(0) = PSPACE. 
This again follows from Savitch's theorem since NPQUERY(0) is defined 
equivalently by nondeterministic polynomial space machines without oracle. 

A Positive Relativization of the NP ! PSPACE Problem 
185 
The restriction of making only polynomially many queries can be consid-
ered once more as indicated in the description of positive relativizations. The 
unrelativized classes NP and NPSPACE = PSPACE directly obey the restriction: 
in the notation of the previous section, CR = C and V R = V; and the restricted 
relativized classes are NP(A) and NPQUERY(A), since the former is not affected 
by the restriction. Theorem 8.5 below shows the third condition of the definition 
of positive relativization, and is based on a technical characterization of the class 
NPQUERY analogous to that of Theorem 8.1 in the preceding section. 
Theorem 8.4 For every oracle set A, NPQUERY(A) = NP(QBF $ A). 
Proof The inclusion from right to left is exactly as in Theorem 8.1. For the 
converse, let L be a set in NPQUERY(A) and let M witness this fact. For 
each configuration I of M, consider the possible computations which begin at 
I, end at a configuration J which is either a query configuration, an accepting 
configuration, or a rejecting configuration, and have no query configurations 
other than J. The only difference with the case of PQUERY is that now J is 
not unique. Thus, our polynomial time nondeterministic machine will guess it. 
Define the predicate leap(I, J) to be true if J is reachable from I under these 
conditions. Then the set: 
B = {(I, J) /leap(I, J)} 
can be decided in polynomial space by simulating all computation paths starting 
at I, and checking that one of them reaches J with no queries on the way. 
Use the oracle QBF to answer queries to B in the nondeterministic procedure 
presented in Figure 8.3. 
The guess requires polynomial time, and the polynomial query bound en-
sures that the loop is performed polynomially many times. Thus, L belongs to 
NP(QBF $ A). 
0 
From this characterization, we can obtain the third condition in the defini-
tion of positive relativization. We use properties of the strong nondeterministic 
Turing reducibility studied in Chapter 4 of Volume I. 
Theorem 8.S NP = PSPACE if and only if for every oracle set A, NP(.4.) = 
NPQUERY (A). 
Proof Since NP(0) = NP and NPQUERY(0) = PSPACE, the proof from right 
to left is trivial. Assume that NP = PSPACE. This implies that QBF E NP. 
Moreover, since PSPACE is closed under complements, NP would be as well, 
and therefore QBF E NP n co-NP = NP. This is equivalent to saying that 
QBF $.SN 0, and since A E P(A) we have that both QBF and A are strong 
nondeterministically reducible to .4.. By part 3 of Proposition 4.7 in Volume I, 
QBF $ A $.SN A. 

186 
Positive Relativizations 
input x 
I := initial configuration of 111 on x 
loop 
guess J of the appropriate (polynomial) length 
check that (I, J) E B, else reject 
exit the loop if J is a halting configuration 
comment: else, J is a query configuration 
let w be the word queried in J 
query A about w 
I := successor of J depending on the answer of A 
end loop 
comment: J is a halting configuration 
accept or reject according to J 
end 
Figure 8.3 A nondeterministic polynomial time simulation 
Now assume that L E NPQUERY (A) = NP(QBF ED A), and apply Theo-
rem 4.4 of Volume I: L is in NP relativized to a set which is strong non de-
terministically reducible to A, and therefore L is in NP(A). This completes 
the proof, since the converse inclusion has been already discussed in Proposi-
tion 8.3. 
0 
Thus the restriction of making polynomially many queries yields also a pos-
itive relativization of the NP = PSPACE problem. The oracle presented in the 
previous chapter exhibiting a class NP not closed under complements separates 
NP from PSPACE. Theorem 8.S now indicates that if the construction could be 
adapted to the case of NPQUERY, then the unrelativized separation of NP from 
PSPACE would follow. 
We should observe again that the class NPQUERY can be separated from both 
classes PSPACE and PQUERY in the appropriate relativizations. See Exercises 
3 and 4. On the other hand, notice that for the empty oracle the three classes 
PSPACE, PQUERY and NPQUERY coincide, and that QBF, considered as an 
oracle, collapses to P all the classes below PSPACE, including PQUERY and 
NPQUERY, thus making all of them also equal. 
8.4 A Positive Relativization of the P :b NP Problem 
We pursue in this section one of the positive relativizations known for our main 
problem, P J, NP. The technique for obtaining it will be quite similar to the 
ones presented in the previous sections in that it stems from a bound on the 

A Positive Relativization of the P .! NP Problem 
187 
queries of polynomial time nondeterministic machines. However, the details are 
more involved than those of the previous sections. 
We have studied classes defined by polynomial space machines whose access 
to the oracle is restricted. More precisely, the access to the oracle for "pquery" 
machines is as for polynomial time machines, while the access to the oracle 
for "npquery" machines is as for nondeterministic polynomial time machines. 
Thus, polynomial space machines with "P-like" oracle access provide a positive 
relativization of the P J PSPACE problem, while polynomial space machines 
with "NP-like" oracle access provide a positive relativization of NP J PSPACE. 
Therefore, in our quest for a positive relativization of P J NP, it is a natural 
idea to consider NP machines with P-like access to the oracle. It works. We 
define in this section this kind of machines and show the corresponding positive 
relativization result. 
Definition 8.3 Let A be any oracle set. Define NPb(A) to be the class of sets 
accepted by nondeterministic polynomial time oracle machines, using oracle A, 
that make a polynomial number of oracle queries; i.e. machines lv[ such that 
Q(M, A, x) has at most p(lxj) elements for some polynomial p. 
The subscript b stands for bounding the number of queries. Observe that 
this restriction does not look like that of NPQUERY, which is less restrictive 
since only the number of queries per computation is bounded. Comparing it to 
that of PQUERY is more exact. However, there is a difference of motivation: 
in the case of PQUERY the bound makes sense because the machines may run 
for long (exponential) time, while here the bound makes sense because the tree 
of the nondeterministic polynomial time computation may contain exponentially 
many queries. A clear example is the nondeterministic machine presented in 
Theorem 7.2 to accept the set L(A): every word of the appropriate length is 
queried in some computation. 
Again as in previous sections imposing a similar bound to P is meaning-
less, since a deterministic polynomial time machine cannot make more than 
polynomially many queries. It is easy to locate NP b: 
Proposition 8.5 For every oracle A, P(A) ~ NPb(A) ~ NP(A). 
Let us list some concepts that we will use in what follows. The NP(A)-
complete set K(A) has been defined in Volume I, Definition 4.8. K is just 
K(0), and is NP-complete. We will also use the relativized version of the class 
PF (Definition 3.2 of Volume I) and we denote by PF(A) the class of functions 
computable in polynomial time with oracle A. 
We also need a refinement of the notation Q(M, A, x). 
Definition 8.4 
1. 
Q(M, A, x, k) is the set of strings queried by Al on input x and with 
oracle A within the first k steps of some computation. 

188 
Positive Relativizations 
2. 
Q+(M, A, x, k) is the set of strings in Q(M, A, x, k) which received a 
positive answer from A, i.e. 
Q+(1I1, A, x, k) = Q(M, A, x, k) n A 
3. 
Q_(M, A, x, k) is the set of strings in Q(M, A, x, k) which received a 
negative answer from A, i.e. 
Q_(1I1, A, x, k) = Q(M, A,x, k) n A 
Thus, Q(M, A, x, k) contains all the queries in the first k levels of the com-
putation tree. 
Since all these sets are finite, we will assume that they are encoded into 
finite strings in such a way that set-theoretic operations can be performed in 
polynomial time on the strings encoding the sets. 
Respectively denote by 
q(M,A,x,k), q+(M,.4,x,k) and q_(M,A,x,k) the strings encoding the finite 
sets Q(M,A,x,k), Q+(M,A,x,k) and Q_(M,A,x,k). Usually, in the algo-
rithms below, q, q+, and q_ (without parameters) will denote local variables 
holding values which are expected to become q(M, A, x, k), q+(1I1, A, x, k) and 
q_(M, A, x, k) respectively. 
We need just one more technical definition before going into the results. 
Here, and later on as well, a partial computation is a sequence of legally reached 
configurations starting in an initial configuration. 
Definition 8.5 Let AI be any oracle machine, and let q+ and q_ be strings 
encoding finite disjoint sets. A partial computation of M is compatible with q+ 
and q_ if and only ifin every query configuration having a successor in the com-
putation, the queried word is either in q+, and then the successor configuration 
corresponds to a YES answer, or in q_, and then the successor configuration 
corresponds to a NO answer. 
Thus, a computation is compatible with q+ and q_ if and only if q+ contains 
all the words answered YES during the computation, and q_ contains all the 
words answered NO during the computation. Notice that the last configuration 
of the computation is the only one able to query for a word which is neither in 
q+ nor in q_. 
For the next lemma and onwards, M is any fixed nondeterministic polynomial 
time oracle machine, A is an oracle and x is an input 
Lemma 8.1 Assume that IQ(.~1, A, x)1 ~ p(lxi} for some polynomial p. Then 
there is a function nq in P F(K) such that 
nq(x, q+(M, A, x, k), q_(M, A, x, k), k) = q(M, A, x, k + 1) 

A Positive Relativization of the P ! NP Problem 
189 
The name nq stands for "new queries". Observe that the lemma says that, 
if all the words queried up to level k are given along with the corresponding 
answers, then it is not too difficult to compute the queries in the next level of the 
computation tree. Of course, computing the answers for these queries requires 
the help of the oracle. The requirement that IQ(M, A, x)1 ::; p(lxl) is necessary 
in order that the machine computing nq be able to write down q(M, A, x, k + 1) 
in polynomial time. 
input x, q+, q_, k 
q := 0 
if (x, q+, q_, k + 1,'x) E pref-level 
then construct-from('x) 
output q 
Figure 8.4 Starting the recursive computation of nq 
Proof. 
We use again a prefix searching technique. Assume for simplicity that 
the oracle tape alphabet is {O, I} (this assumption can be relaxed if necessary), 
and let # be a new symbol. Consider the following set: 
pref-Ievel = {{x, q+, q_, k, z} I z is a prefix of a word of the form 
w#, where w is queried within the first k steps 
of a partial computation compatible with q+ and q_} 
We claim that pref-Ievel belongs to NP. Indeed, to check if a tuple is in this 
set it is enough to guess a partial computation starting on x and polynomially 
long, check that the computation is compatible with q+ and q_, and check that 
z is a prefix of an appropriate word as indicated. 
Thus, pref-level is reducible in polynomial time to 1(, and machines using 
pref-Ievel as oracle may be transformed into machines using 1( as oracle with 
polynomial time overhead. 
We will show next that a function nq satisfying the condition of the lemma 
can be computed in polynomial time with pref-Ievel as oracle. This will complete 
the proof. Consider the algorithm of Figure 8.4, calling a doubly recursive 
subroutine presented in Figure 8.5. The output is held in the global variable q. 
The variable q is a string encoding a finite set, as indicated above. In this 
algorithm, every prefix of every word w satisfying the conditions of pref-Ievel 
generates a recursive call, and every recursive call receives as a parameter a 
prefix of such a word w. If the data q+ and q_ are precisely q+(1I1, A, x, k) and 
q_(M, A, x, k), then the computations in the definition of pref-Ievel correspond 

190 
Positive Relativizations 
construct-from(z) is 
comment: precondition (x, q+, q_, k + 1, z) E pref-Ievel holds 
if (x,q+,q_,k + l,zO) E pref-Ievel 
then construct-from(zO) 
if (x, q+, q_, k + 1, zl) E pref-Ievel 
then construct-from(zl) 
if (x, q+, q_, k + 1, z#) E pref-Ievel 
then q:= qU {z} 
end 
Figure 8.5 Computing nq recursively 
to computations of 111 on x with oracle A, and therefore by definition the 
algorithm will construct q(M, A, x, k + 1). Moreover, the size of this set is 
polynomial, and therefore the number of prefixes of words in it (which equals 
the number of recursive calls) is polynomial. Thus the algorithm works in 
polynomial time. 
0 
A last technical tool is given in the following definition and lemma: 
Definition 8.6 
]{' is the following set: 
]{' = {(M, x, It, F) I F is a finite set and M a nondeterministic oracle 
machine, and M accepts x in t steps with oracle F } 
The property we need is easy to show (Exercise 6). 
Lemma 8.2 ]{' E NP. 
Now we are ready to prove the corresponding positive relativization theorem. 
Theorem 8.6 P = NP if and only iffor every oracle set A, P(A) = NPb(A). 
Proof The implication from right to left is trivial. For the converse, one 
inclusion is clear, and only the other remains to be shown. Assume that P = 
NP, and let L be a language in NPb(A) for any oracle A. Let 111 be the 
machine witnessing this fact, and let r be a polynomial bounding its running 
time. Consider the algorithm given in Figure 8.6; by Lemmas 8.1 and 8.2 we 
find that the computation of nq and the query to ]{' can be done by queries 
to ]{, and therefore the algorithm works in polynomial time relative to ]{ EB A. 
From the hypothesis P = NP, we can substitute a polynomial time algorithm 
for the queries to ]{, and we obtain that the algorithm of Figure 8.6 works in 
polynomial time relative to A. 

input x 
q+ := 0 
q_ := 0 
k := 1; 
while k ~ r(lx I) do 
A Positive Relativization of the P l NP Problem 
191 
comment: construct queries up to level k 
q := nq(x, q+, q_, k) 
comment: and find out the corresponding answers 
for each word w in q, 
except those already in q+ or q_, do 
if w E A then q+ := q+ U {w } 
else q_ := q_ U {w} 
comment: ready for the next level 
k := k + 1 
end while 
comment: q+ = Q(M, A, x) n A, and q_ = Q(M, A, x) n A 
accept if and only if (AI, x, F<lxj), q+) E ]{' 
end 
Figure 8.6 A deterministic polynomial time simulation 
Lemma 8.1 guarantees that the while loop finds correctly all the queries 
made in the full computation tree of M, together with the correct answers, in 
polynomial time. Thus, for the machine AI working on input x the set encoded 
by q+ contains the same information as A does, and therefore we can use q+ as 
oracle. The set ](' saves us the simulation job. This algorithm accepts the same 
set as .M with oracle A, and therefore shows that L is in peA), as was to be 
shown. Observe, by the way, that the correctness of this algorithm proves that 
NPb(A) ~ P(1< EEl A) with no need of the hypothesis P = NPb(A), and that this 
fact is similar to the characterizations given in Theorems 8.1 and 8.4. 
0 
The same class NP b allows us to prove other positive relativizations. There 
are also other interesting positive relativizations of P and NP and of many other 
pairs of classes, including bounded query classes, classes specified by bounding 
functions other than polynomials, and classes specified by simultaneous resource 
bounds. See our comments about the others in the bibliographical remarks. Also, 
let us draw the attention of the reader to Exercise 7, which shows that NP can 
be separated from NPb by an oracle. Thus, in some sense we have decomposed 
the class NP relativized into two parts: one of them allowing the separation 
techniques to work (namely NP - NPb), the other making the separation as 
difficult as in the unrelativized case (namely NP b). 

192 
Positive Relativizations 
8.5 A Relativizing Principle 
A characteristic that is common to the previously presented restrictions providing 
positive relativizations is that only one of the two classes under consideration 
is affected by the restriction imposed on the machines. For instance, the query 
bound imposed on "npquery" machines affects "pspace" machines but does not 
affect the other class, NP, considered in the positive relativization result. 
However, the concept of positive relativization as presented in the introduc-
tion allows both classes to be restricted. There are several positive relativiza-
tions in which this happens; see the literature referred to in the bibliographical 
remarks. This is the case also for the relativizing principle presented in this 
section. 
The restriction presented here relates to the P ~ NP problem. Applying the 
concept ofrelativizing principle presented in the introduction to these classes, we 
obtain that we should present a restriction such that: first, it does not affect the 
nonrelativized classes; second, P = NP if and only if for every A, the restriction 
of the class peA) equals the restriction of the class NP(A); and third, P =I NP 
if and only if for every A, the restriction of the class peA) is different from the 
restriction of the class NP(A). 
Therefore, solving in either sense the problem of the equality of the restricted 
classes for any given oracle amounts to solving in the same sense the unrela-
tivized problem. Another consequence is that either for all oracles the restricted 
classes are equal, or for all oracles the restricted classes are different. 
The restriction to be imposed on the machines is very, very strong. No 
weaker restriction is known allowing one to obtain similar results, and therefore 
it is the strongest relativizing principle currently known. Thus it is an interest-
ing open problem whether some weaker condition yields a better relativizing 
principle for the P ~ NP question. 
The restriction we impose now will be strong enough to guarantee that non-
deterministic machines are allowed to make only polynomially many queries. 
We will thus be able to use the same techniques as in the preceding section 
to obtain the positive relativization of the equality P = NP. In order to show 
that inequality also relativizes, we will impose a restriction somewhat similar to 
that of logarithmic advice, and we will perform a polynomial time search for 
satisfying assignments for SAT in a manner similar to that of Theorem 5.7 of 
Volume I. 
The restriction to be imposed on the machines is the following: for each 
length n, the set of queries made in any computation on any input of length n 
has logarithmically many words. More formally, define the following sets: 
Definition 8.7 Given any oracle machine }'1, the set Q(}'1, x) is the union of 
all the sets Q(}'I, A, x) taken over all oracle sets A, and the set QU(}'1, n) is 
the union of all the sets Q(M, x) for Ix I = n. 

A Relativizing Principle 
193 
Here we are considering the computation of a nondeterministic machine as a 
tree with two types of internal nodes: nondeterministic steps and oracle queries. 
Then Q(M, x) is the set of all queries appearing in any path of the computation 
tree corresponding to input x. Next, we consider the family of computation 
trees over each word of length n, and define QU(M, n) as the set of all queries 
appearing in any path of any of these trees. The restriction we consider imposes 
a strong bound on the size of QU(M, n): 
Definition 8.8 Let A be any oracle set. Define P,CA) (respectively NP,(A») 
to be the class of sets accepted by deterministic (respectively, nondeterministic) 
polynomial time oracle machines, using oracle A, such that IQU(M, n)1 ~ 
c·log n for some constant c. 
The restriction is to machines that make o (log n) queries in the whole set 
of computation trees on inputs of length n. Since non-oracle machines are 
insensitive to bounds on the number of queries, it is easily seen that: 
Proposition 8.6 
P,(0) = P and NP,(0) = NP. 
input formula F 
check that F is syntactically correct 
while there remain variables in F do 
let x be the first variable in F 
F':= Flx:_o 
if F' E L(M, A) then F := F' 
else 
F' := Flx:_l 
if F' E L(A1, A) then F := F' 
else reject 
end while 
accept if and only if F simplifies to "true" 
end 
Figure 8.7 A "somewhat robust" algorithm 
To complete the proof that this restriction provides a relativizing principle 
of the P J, NP problem, we prove the following theorem: 
Theorem 8.7 
1. If there exists an oracle A such that P,CA) = NP,(A) then P =NP. 
2. If there exists an oracle A such that P,CA) =I NP I(A) then P =I NP. 

194 
Positive Relativizations 
Proof 
1. 
Assume that A is such that PICA) = NPICA). Notice that NP = NP/(0) 
is included in NPICA). Therefore it follows that SAT E PICA). We will 
show that this implies the existence of a polynomial time algorithm for 
deciding SAT. The proof uses the same technique as Theorem 5.7 of 
Volume I. We follow the tree given by the disjunctive self-reducibility of 
SAT to make sure that only satisfiable formulas are accepted; this yields 
a kind of robust algorithm which accepts SAT for the appropriate oracle 
and subsets of SAT for arbitrary oracles. Then we iterate the algorithm, 
trying every possible sequence of answers. The bound on the number of 
queries allows this to be done in polynomial time. As in Theorem 5.7 of 
Volume I, we assume that an encoding of SAT is available such that for 
any formula F, all the formulas obtained by substituting boolean constants 
for the variables of F have the same length as F. 
Let 111 be a deterministic oracle machine witnessing SAT E PICA). We 
construct the oracle machine 111' using oracle A as indicated in Figure 8.7. 
Notice that AI' only accepts after having found a satisfying assignment. 
Therefore, for every oracle E, L(1I1', E) is a subset of SAT. For the 
oracle A, LCM, A) is precisely SAT, and therefore if F is satisfiable then 
the search correctly identifies a satisfying assignment and accepts. Thus 
LCM', A) = SAT. Moreover, M' works in polynomial time. 
Next we show that the use of the oracle A is unnecessary. Instead, we use 
a string of bits of length o (log n). More precisely, the length of the string 
is the function that bounds the number of queries made by AI on inputs of 
length IFI. The following additional variables are used: i is an index to 
the string, indicating the bits already used; Y is the set of queries that are 
assumed to be in in the oracle, as indicated by the corresponding bit of 
w; and N is the set of the remaining queries made so far. The algorithm, 
named AI", is presented in Figure 8.8. 
Observe that for every oracle there is a word w of length o (log n) that 
yields the same answers, and that for each such word there is a corre-
sponding oracle. Thus, if M" accepts then the formula F is satisfiable, 
and if F is satisfiable then there is a word w that leads M" to acceptance. 
The polynomial time algorithm that completes the proof is obtained by 
repeatedly calling .~1" polynomially many times, one for each word w of 
the appropriate length, until receiving a positive answer (and then F is 
satisfiable) or running out of words w (and then F is unsatisfiable). 
2. 
We prove that if P = NP then for every oracle A, PI(A) = NPI(A). Let M 
be an oracle machine witnessing the fact that L(AI, A) belongs to NPI(A). 
We use Lemma 8.1 of the previous section. Again the hypothesis P = NP 
implies that the function nq of Lemma 8.1 is in P F. Thus we perform 
an algorithm analogous to that of Theorem 8.6 (Figure 8.6), constructing 
for each k, 1 $ k $ r(lx/), the set of queries made at level k and then 

input F, w 
i := I 
Y:= 0 
N:= 0 
loop 
simulate Af' until it stops or makes a query 
A Relativizing Principle 
195 
if it stops then stop and accept or reject accordingly 
else 
end loop 
end 
let x be the queried word 
if x E Y then continue with answer YES 
if x E N then continue with answer NO 
otherwise, if the i lh bit of w is I then 
else 
Y:= YU {x} 
i := i + I 
continue with answer YES 
N := N U {x} 
i := i + I 
continue with answer NO 
Figure 8.8 Using w instead of A 
querying those whose membership in A is not yet known. A glance at 
the algorithm shows that it is deterministic, makes the same set of queries 
to A as M does, and accepts L(M, A). Thus the same bound on the 
cardinality of queries holds, and L(M, A) is in PI(A). 
0 
Let us state the last theorem again in a different form. The corollary that 
follows is a combination of the theorem and its contrapositive: 
Corollary 8.1 
1. P:/ NP if and only if for every oracle A, PI(A) :/ NPI(A). 
2. 
P = NP if and only iffor every oracle A, PI(A) = NPI(A). 
Proof 
1. 
The implication from left to right follows taking as A the empty set. The 
converse is the contrapositive of part (a) of Theorem 8.7. 
2. 
The implication from left to right follows taking as A the empty set. The 
converse is the contrapositive of part (b) of Theorem 8.7. 
0 

196 
Positive Relativizations 
Parts (a) and (b) of the corollary match the requirements stated in the intro-
duction for the restriction to be considered a relativizing principle. 
A last observation is in order: 
Corollary 8.2 Either for every oracle A, ~(A) = NPI(A), or for every oracle 
A, Pl(A) =I NPl(A). 
It does not need a proof: the first case corresponds to the case that P = NP 
in the unrelativized world, and the second to the case that P =I NP in the 
unrelativized world. Still there are other ways of stating the same result, such 
as: P = NP if and only if there exists an oracle A such that PI (A) = NPI(A), if 
and only if for every oracle A, PI (A) = NPl(A). 
It should be emphasized once more that the restriction is very strong. Ex-
ercise 10 gives a reason for this judgement. As said before, finding a weaker 
relativizing principle is an interesting open problem. 
8.6 Exercises 
1. 
Complete the proof of Theorem 8.3. 
2. 
Use Theorem 8.1 to show that for every set A, peA) = PQUERY (A) if 
and only if A is PSPACE-hard. 
3. 
Show that there is a set A such that PSPACE(A) is not included in 
NPQUERY(A). 
4. 
Show that there is a set A such that PQUERY (A) =I NPQUERY (A). 
5. The polynomial query hierarchy is defined by iterating the NPQUERYO 
operator, in much the same way as the polynomial time hierarchy is defined 
by iterating the NPO operator (Definition 8.1 of Volume I). 
(a) Propose a definition of ErQ in the same way, for i ~ 1. Denote by 
PQH the union of all these classes. 
(b) 
Characterize each of these classes in a similar manner to Theorems 
8.1 and 8.4. 
(c) 
Show that PH = PSPACE if and only if for every A, PH(A) = 
PQH(A). 
6. 
Prove Lemma 8.2. 
7. 
Show that there is a set A such that NP(A) =I NPb(A». 
8. 
Show that the class P / log is included in the union of all the classes PleA). 
Hint: Design an oracle A containing prefixes of the advice. 
9. 
Show that the union of all the classes Pe(A) is included in Lhllog. Hint: 
Design an algorithm like that of Figure 8.6, using as oracle J{ EB A, and 
substitute a logarithmically long advice for the queries to A. 
10. 
·Show that there is a set A such that A tI PleA). Hint: Use the previous 
exercise. 
11. Prove: if P /log =I UA Pt(A) then P =I NP. Hint: Use Exercises 8 and 9. 

Bibliographical Remarks 
197 
12. Develop a restriction PSPACEl(A) on polynomial space machines anal-
ogous to that of the last section of this chapter. In this way, obtain a 
relativizing principle for the P J PSPACE problem. 
8.7 Bibliographical Remarks 
The first time a notion of positive relativization appears is, so far as we know, 
in the paper by Baker, Gill, and Solovay (1975), implicit in a remark regarding 
the machine model. They state that for a different model of oracle machines, 
in which the characteristic function of the oracle is given on a separate tape, 
their proof for separating P from NP does not work, and they credit a personal 
communication of P. Morris for the observation that, under this model, separating 
a relativization implies separating the unrelativized classes. To our knowledge, 
no complete proof of this fact was ever published by Morris. It is easy to see that 
this remark is a weak form of Theorem 8.6, since in this model the NP machines 
can query only the first polynomially many words of E* in lexicographic order, 
and therefore accept a subclass of NP b• 
The first positive relativization presented as such and proved was obtained in 
Book (1981), and was obtained in the study of the closure of complexity classes 
under certain algebraic operations. The proofs were language-theoretic in nature, 
and hard to translate into other classes. The main theorem of this reference is 
Theorem 8.5 in the text. Also some separations appear in this reference, such 
as Theorem 8.3 and Exercises 3 and 4. In a simultaneously published paper, 
Book and Wrathall (1981) presented the positive relativization of PH J PSPACE 
given in Exercise 5(c). The techniques were quite similar. 
The positive relativization by means of the class PQUERY (Theorem 8.2), 
raised as a conjecture in Book (1981), was later answered positively in Selman, 
Xu, and Book (1983); many other positive relativizations are developed there, 
modifying the classes according to different criteria (like "confluence"), includ-
ing positive relativizations of P J NP, and several results such as Exercise 2 
are shown. Also, important connections to classes of functions are presented 
there. To our knowledge, the term "positive relativization" was coined there. 
Applications of similar techniques to classes specified by simultaneous resource 
bounds, like time and work space, number of queries and work space, or time 
and length of the queries, are presented in Book, Wilson, and Xu (1982), to-
gether with related results. Our presentation of the results concerning PQUERY 
and NPQUERY is based on the characterizations of Theorems 8.1 and 8.4, which 
appeared, together with observations regarding the polynomial query hierarchy 
(Exercise 5), in Balcazar, Book, and Sch6ning (1985). 
Positive relativizations of space-bounded classes exist. In particular, Wil-
son (1985) and independently Kirsig and Lange (1987) have shown that space-
bounded oracle machines obeying the restrictions suggested by Ruzzo, Simon, 

198 
Positive Relativizations 
and Tompa (1984) provide a positive relativization of theDWG = NWG prob-
lem. 
The positive relativization of P 1 NP presented here is one among several 
interesting variations presented in Book, Long, and Selman (1984), where many 
separations and positive relativizations were presented. Applications of analo-
gous concepts to many other complexity classes may be found in Book, Long, 
and Selman (1985) and in Russo and Zachos (1983); see also Russo (1985). 
As we shall see in the next chapter, by imposing restrictions on the oracle sets 
instead of the machines other positive relativizations can be found; an interesting 
comparison is made in Long (1985). 
Finally, the relativizing principle presented in the last section is due to Book 
and Long (1985), and is based on previous work by Karp and Lipton (1980) 
for one of the directions and on Book, Long, and Selman (1984) for the other. 
Exercises 8 to 12 are from this reference (sometimes slightly modified). A survey 
of positive relativizations can be found in Book (1989). 

9 
The Low and the High Hierarchies 
9.1 Introduction 
In this chapter we shall study two hierarchies of classes of sets, obtained by 
fonnalizing the concept of the infonnation content of a set, considered as an 
oracle. This should not be confused with the infonnation contents of a string, or 
Kolmogorov complexity, which will be studied in the next chapter. We want to 
be able to evaluate the amount of infonnation provided by the set to the oracle 
Turing machines using it, so that questions such as "is this set useful?" or "is this 
set almost useless?" do make sense. The grading scale for comparing the power 
of different sets, used as oracles, will be the polynomial time hierarchy. We will 
compare classes 17j (.4.) with unrelativized classes 17j , looking for a nontrivial 
equality or inclusion. We define first a low and high hierarchy within NP. Later 
we present generalizations of this work. 
To motivate our approach, consider the following examples. Take any NP-
complete set like SAT; using it as oracle in a bounded alternation computation, 
say 17;, allows one to get one level higher in the polynomial time hierarchy. 
In this sense, SAT is a "useful" set in NP. Now consider a sparse set in NP; 
since it contains only polynomially many words, it seems rather intuitive that 
it should be able to encode only a limited quantity of information; it should 
not be "so useful". Moreover, recalling Mahaney's Theorem (Theorem 5.7), we 
can infer that the properties of NP-completeness and sparseness are of opposed 
nature. We can think of sparse sets as sets with low infonnation content, while 
the NP-complete sets can be thought of as sets with high infonnation content. 
In fact, we show below that for every sparse set S E NP, 172(S) = 172, i.e. a 
sparse oracle from NP does not allow E2 machines to go any further than the 
empty set. 
Hence the idea is to compare the power of a set as oracle with the power of 
a string of alternating (polynomially bounded) quantifiers. The upper bound on 
the infonnation obtained from an NP set will be set by the NP-complete sets, 
and therefore will be comparable to an additional quantifier; the lower bound 
is no information at all, and the string df quantifiers will be able to make the 
oracle unnecessary. The resulting definitions are presented in Section 9.2, with 
their basic properties, and the relationship with the polynomial time hierarchy is 

200 
The Low and the High Hierarchies 
developed in Section 9.3. Section 9.4 is devoted to showing the lowness implied 
by certain properties (like sparseness), and a theorem regarding self-reducible 
sets is proved. In Section 9.5 this theorem is used to establish properties that 
we call oracle-restricted positive relativizations, in the spirit of the theorems of 
Chapter 8. The way of generalizing the concepts of lowness and highness to 
sets outside NP is not unique: one of them is presented in Section 9.6, with 
some properties; another is suggested in the exercises. 
9.2 Definitions and Characterizations 
Definition 9.1 
For n ~ 0, define: 
Hn = {A E NP I En+1 ~ En(A)} 
We define also the unions of the low classes and of the high classes: 
Definition 9.2 
The idea behind the statement En(A) ~ En is that in some way we can 
distribute among the quantifiers in En the information given by A, thus rendering 
A useless. On the other hand, the statement En+1 ~ En(A) indicates that enough 
information can be encoded inside A to jump to the power of a new quantifier. 
Proposition 9.1 
1. 
Lo ~ LI ~ L2 ~ ... Ln ~ .. . NP. 
2. 
Ho ~ HI ~ H2 ~ ... Hn ~ ... NP. 
Proof We prove (1). If En(A) ~ En then NP(En(A» ~ NP(En), which is 
equivalent to EMI (A) ~ En+I' Therefore for every n, Ln ~ Ln+l. 
A similar argument works with (2). 
0 
We now present characterizations of the first two classes in each of the hier-
archies. The classes Lo and Ho are the zero degree and the NP-complete degree 
of the polynomial time Turing reducibility (Cook reducibility), while the classes 
LI and HI are the zero degree and the NP-complete degree of the polynomial 
time SN-reducibility, presented in Section 4.4 of Volume I. Theorem 4.4 of Vol-
ume I shows that for all sets A, B, and C, A E NP(B) and B 5,SN C implies 
A E NP(C). This fact will be used here. 

Definitions and Characterizations 
201 
Theorem 9.1 
1. 
Lo = P. 
2. 
Ho = {A E NP I for all B in NP, B ::;T A}. 
3. 
Ll = NP n co-NP 
4. 
HI = {A E N P I for all B in NP, B ::;SN A}. 
Proof. 
(1) and (2) are immediate from the definitions. In order to prove (3), 
we start with the inclusion from left to right. Let A ELI. Then A E NP and 
L't(A) ~ 171. which means NP(A) ~ NP. But both A and A E P(A), which in 
tum is included in NP(A) and hence in NP. Therefore both A and A are in NP. 
To prove the inclusion from right to left, let A E NP n co-NP. Then by 
definition A ::;SN 0. Let B E NP(A). By the fact above, B E NP(0). Hence 
El(A) ~ 171. 
Now we prove (4), starting again left to right. Let A E HI; then A E NP, and 
E 2(A) ~ NP(A). Let I< be any NP-complete set. Its complement I< belongs 
to 172; hence I< E NP(A), and then co-NP ~ NP(A) follows. We infer that 
NP ~ co-NP(A). Therefore NP ~ NP(A) n co-NP(A), which implies that A is 
NP-complete with respect to SN-reducibility. 
To prove the converse inclusion, assume that for all B in NP, we have 
B E NP(A) n co-NP(A). In particular, I< ::;SN A. But 172 = NP(I<), therefore 
every set C E 172 is also in NP(I<). By the fact above, C E NP(A). It follows 
that 172 ~ NP(A). 
0 
For the classes Ln and H n with n > 0, membership in the class can be 
equivalently expressed by means of m-reducibilities, using the iterations of the 
NP-complete set I< defined in Volume I. We present this characterization in the 
following lemma, which will be used in the next section. 
Lemma 9.1 For n > 0, A is in Ln if and only if ](n(A) ::;m I<n; and A is in 
Hn if and only if ](n+l ::;m ](n(.4.). 
Proof 
If A is in Ln, then En(.4.) ~ En. In particular, ](n(A), which belongs 
to En(A), is reducible to the complete set for En, which is ]{n. 
Conversely, if the complete set for En(A), which is ](n(A), is reducible to 
]{n, then every set in En(A) is reducible to ](n, and therefore belongs to En. 
which implies that A E Ln. 
An analogous argument proves the characterization of Hn. 
0 
It is interesting to note that the above lemma does not encompass the case 
n = 0. In the case of Lo, if ]{o and ](o(A) are defined respectively as 0 and A, 
then a trivial problem arises with the m-reducibility to the empty set. Defining 
]{o as any set in P different from 0 and r" (where r is the alphabet), the lemma 
holds also for Lo. 
It is not known whether the lemma holds for Ho: Ho are the NP-complete 
sets with respect to T-reducibility, whereas the characterization as in the lemma 
would give the NP-complete sets with respect to m-reducibility. 

202 
The Low and the High Hierarchies 
9.3 Relationship with the Polynomial Time Hierarchy 
This section studies the implications between equalities in the polynomial time 
hierarchy and equalities of high and low classes. We also present necessary and 
sufficient conditions for the high and low classes to cover all of NP. The main 
result relating these hierarchies is given in the next theorem and its corollary. 
Theorem 9.2 For all n ~ 0, 
1. If En = En+1, then for each i ~ n, Li = Ln = Hi = Hn = NP. 
2. If En =I En+l, then Ln n Hn = 0. 
Proof. 
To prove (1), let A E NP; for such A, En ~ En(A) ~ En+l' Assuming 
En = En+l' the first class equals the last class. It follows that A fulfills both the 
condition defining Ln and the condition defining Hn. Hence both classes are 
NP, and so must be all the classes Li containing Ln and Hi containing Hn. 
For the proof of (2), let A be in Ln n Hn (assumed nonempty). As A is in 
Hn, En+! ~ En (.4). As A is in Ln, En(A) ~ En. It follows that En+! ~ En. 
The converse inclusion always holds. Hence in this case En+! = En. Thus, if 
En+! =I En then such an .4 does not exist. 
0 
Corollary 9.1 
1. 
For all n ~ 0, either Ln n Hn = 0, or Ln = Hn = NP. 
2. 
The polynomial time hierarchy is infinite if and only if LH n HH = 0. 
If the polynomial time hierarchy is a proper hierarchy up to En+!' then the 
first part of this corollary says that Ln and H n are disjoint. But then the following 
question arises: do there exist sets in NP which are neither in Ln nor in Hn? 
In the same manner, if the polynomial hierarchy is infinite, then, by the second 
part of this corollary, the low and the high hierarchies are disjoint. Again we 
can ask: do there exist sets in NP which are neither in LH nor in HH? Answers 
to both questions are given by the Uniform Diagonalization Theorem. 
In order to apply it, we need the following technical lemma on the recursive 
presentability of the low and high classes: 
Lemma 9.2 For every n ~ 0, Ln, H n, LH and HH are recursively presentable. 
Proof. 
Recall that Lo is the class P, and Ho is the class of the NP-complete 
sets, which have already been proved to be recursively presentable in Proposi-
tion 7.2 of Volume 1. In order to show the presentability for n > 0, we use the 
characterization given in Lemma 9.1 in terms of m-reducibility to and from the 
nth application of the ]{ operator. 
Let {Mi I i ~ I} be a presentation of NP, and let {T; I i ~ I} be an 
enumeration of polynomially clocked transducers. Denote by T;(y) the output 
of transducer Ti on input y. For each index k = (i,j), we define the machines 
described in Figure 9.1. For k = (i, j), either L(Mk) is finite, which is the case 

M'· 
k' 
M "· 
k • 
Relationship with the Polynomial Time Hierarchy 
203 
input x 
for each y with Iyl < lxi, 
test if (y E J(n(L(Mj» if and only if Tj(Y) E J(n) 
if for all such y it is true, 
then accept x if and only if x E L(Mj ) 
else reject 
end 
input x 
for each y with Iyl < lxi, 
test if (y E J(n+l if and only if Tj(y) E J(n(L(Mj») 
if for all such y it is true, 
then accept x if and only if x E L(Mj ) 
else accept x if and only if x E SAT 
end 
Figure 9.1 
Auxiliary machines for the proof of Lemma 9.2 
if the function computed by Tj is not a reduction from J(n(L(Mj» to J{n, or 
L(MO = L(Mj), which happens exactly when Tj is a reduction from J{n(L(Mj» 
to J(n. In any case, for every k, L(MO E Ln. 
On the other hand, if A E Ln, then there exists a jo such that Tjo computes a 
reduction from J(n(A) to J(n. As A E NP, there is an io such that L(Mjo) = A. 
This implies that A = L(1I1ko) for ko = (io, jo), and we get that Ln is exactly 
{Mk I k ~ O}. 
By an analogous argument, which is left to the reader, we obtain that Hn is 
exactly {Mf I k ~ O}. Therefore, Ln and Hn are recursively presentable. 
Observe that both sequences of machines can be effectively constructed from 
the value of n. Hence, applying Exercise 5 of Chapter 7 of Volume I about the 
recursive presentability of the union of recursively presentable classes, HH and 
LH are recursively presentable. 
0 
From this lemma and the Uniform Diagonalization Theorem, we obtain the 
following result: 
Theorem 9.3 
1. For each n ~ 0, En .; En+l if and only if there exist sets in NP which are 
neither in Ln nor in Hn. 
2. If the polynomial hierarchy is infinite then there are sets in NP which are 
neither in LH nor in HH. 

204 
The Low and the High Hierarchies 
Proof. To prove (1), observe that if En 'I En+), then LnnH n = 0, which implies 
that SAT ¢ Ln and 0 ¢ H •. Using the Uniform Diagonalization Theorem 
(Theorem 7.1 of Volume I) with Al = SAT, A2 = 0, C. = Ln and C2 = Hn, 
we obtain a set A such that A ~m SAT $ 0, which implies that A E NP, but 
A ¢ Ln and A ¢ Hn. The converse is immediate from Theorem 9.2 above. 
To prove (2), observe that if the polynomial hierarchy is infinite, then for all 
n 2: 0, En 'I En+), so LH n HH = 0. Using again the Uniform Diagonalization 
Theorem with A. = SAT, A2 = 0, CI = LH and C2 = HH, we obtain a set A 
such that A E NP but A ¢ LH and A ¢ HH. 
0 
9.4 Some Classes of Low Sets 
At the beginning of this chapter, we mentioned that one of the .motivations for 
defining the low hierarchy was to study the use of oracles which do not contain 
too much information. The most important representatives of these sets are 
the sparse sets. In this section, we shall study the relation between sparse sets 
and the low hierarchy. The reader with a knowledge of Recursive Function 
Theory may find it amusing to compare our arguments in this section with the 
Tarski-Kuratowski algorithm for classifying sets in the arithmetic hierarchy. 
Theorem 9.4 Let S E NP be a sparse set. Then S E L2. 
By the definition of L2, we must prove that 1:2(S) ~ 1:2. Let D be any set 
with D E 1:2 (S). By Theorem 8.3 of Volume I, we have that xED if and 
only if 3PyvPz{x, y, z} E L(U, S), where U is a deterministic polynomial time 
oracle machine, and the quantifiers are bounded by a polynomial p of the length 
of x. On input {x, y, z}, :M can only query S about strings of length bounded 
by a polynomial of {x, y, z}. Moreover, as Iyl and Izl are also bounded bya 
polynomial in lxi, then we can find a polynomial q such that {x, y, z} E L(U, S) 
if and only if {x, y, z} E L(1I1, (S n r$q(lrl»); but IS n r$q(lrl)1 is polynomial in 
lxi, therefore ISnr$q(lrl)1 can be encoded into a string S of length r(lxl), where 
r is a polynomial. A slight modification of M gives a machine 111. which, on 
input {x,y,z,s}, simulates U on input {x,y,z}, with the following difference: 
when 111 queries its oracle about a word w, Ml checks if w is in the set encoded 
by s, and resumes the computation assuming a positive answer if w is in the 
set, and a negative answer otherwise. We have: 
{x, y, z} E L(1I1, S) ¢::::> 
3r s ("s encodes exactly IS n r$q(lrl)I" and {x, y, z, s} E L(1I1.» 
Let us see how to express the quoted predicate about the encoding s in terms 
of polynomially bounded quantifiers. On the one hand, we must say that every 
word u encoded by s is in S and has length less than q(lxl). The latter can be 
checked in polynomial time. To check that u E S, we will use the NP predicate 

Some Classes of Low Sets 
205 
describing S. Since S E NP, there is an R in P such that u E S if and only 
if 3Pv(u, v} E R. Hence, for each u encoded by 8 we will guess a v and then 
check that (u, v) E R. The guesses for all the u are encoded into a t which 
is guessed at the same time as 8. Since 8 is polynomially long, we can define 
a polynomial time predicate B such that (8, t) E B if and only if t contains 
proofs that all the words in 8 belong to S. On the other hand, we must say that 
every word in S having length less than q(\xD is encoded into 8, or, equivalently, 
Vqu(u is encoded into 8 or u ¢ S), where u ¢ S can be expressed using again 
the predicate R in the NP definition of S, as follows: VquV" v(u is encoded into 
8 or (u, v) ¢ R). 
Getting all the above together, there exists a polynomial p such that: 
xED ~ 
3Py 
31'8 
3Pt 
VPz 
VPu 
VPv 
which is a predicate in E2. 
(as in the E2(S) definition of D) 
(there exists an encoding 8) 
(and proofs that its words are in S) 
(as in the E 2(S) definition of D) 
(to check that all u in S are in 8) 
(to capture all the proofs that u E S) 
[(8, t) E B 1\ ("u is in 8" V (u, v) ¢ R) 
I\(x, y, z, 8} E L(MI )] 
o 
Corollary 9.2 If there is a sparse NP-complete set S (with respect to m-, T-, 
or SN-reducibility), then E2 = E3. 
Proof 
By Theorem 9.1, such complete sets are all in Hlo so that S E HI ~ H2• 
Moreover, by the last theorem S E L2. Therefore L2 and H2 intersect, and by 
Theorem 9.2, E2 = E3. 
0 
Similar but stronger statements regarding sparse T-complete sets for NP 
can be shown; see the bibliographical remarks. Most properties of sparse sets 
are shared by the sets having polynomial size circuits. Recall from Theorems 
5.4 and 5.5 of Volume I that these sets are precisely those sets which are T-
reducible in polynomial time to a sparse set. We prove in the following theorem 
that the sets in NP having polynomial size circuits are indeed in the low hierarchy. 
Theorem 9.5 Let S be any sparse set. For any set A in NP, if A E peS) then 
A E L3• 
Proof 
The proof is similar to that of Theorem 9.4. However, since S itself 
is not necessarily in NP, we need an extra level of quantifiers in order to check 
that the guessed part of S is correct. 
We must prove that E 3(A) ~ E3• The proof goes as follows: we first guess 
an encoding 8 of an initial part of the oracle S (this uses a 3), and then we check 

206 
The Low and the High Hierarchies 
that s is indeed an appropriate oracle for deciding A (using \/ and 3). Then, we 
can substitute computations with s for the queries to A. 
Let p be a large enough polynomial. Then, as A E NP, there is a predicate 
R in P such that x E A if and only if 3Py{x, y) E R; and, as A E peS), there 
exists a machine 1111 such that A = L(1Ill , S). Again a trivial modification of 
MI yields a machine M2 which has a word s as part of the input, and uses it to 
answer the queries instead of querying S. 
To prove that L'3(A) ~ L'3, let D be a set in L'3(A): xED if and only if 
3Py\JPz3Pt{x, y, z, t) E L(M3 , A) for some deterministic polynomial time oracle 
machine M 3• Define finally 1114 as the machine that receives s as input together 
with (x, y, z, t), and simulates 1113; and whenever a query u is made, answers it 
by simulating M2 on (u, s). Again it works in polynomial time. 
The relevant part of the oracle S can be encoded into a polynomially long 
word s. We may not be able to check that s is exactly S, but anyway we do not 
need so much: it suffices to check that this word allows us to correctly decide 
membership in A. Let us express this fact as follows: for every query u to A 
of polynomial length, (u E A <==> (u,s) E L(M2». The expression inside the 
parentheses is equivalent to 3Pv«(u, v) E R) <==> (u, s) E L(M2), which in 
turn is equivalent to: 
(ypv{u, v) ¢ R V (u, s) E L(M2» A «(u, s) ¢ L(M2) V 3Pw(u, w) E R) 
Therefore, (u E A <==> (u, s) E L(1I12» is equivalent to 
\fPv3Pw[«(u, v) ¢ R V (u, s) E L(M2» A «(u, s) ¢ L(M2) V (u, w) E R)] 
Denoting the predicate inside the square brackets by (u, v, w, s) E E, we see 
that E E P. Moreover, 
xED 
<==> 
3Py 
3Ps 
\JPz 
\JPu 
\JPv 
3Pt 
3Pw 
(as in the L'3(S) definition of D) 
(there exists an encoding s) 
(as in the L'3(S) definition of D) 
(to check that all u in A are correctly decided) 
(to check that u is not in A, when necessary) 
(as in the L'3(S) definition of D) 
(to check that u is in A, when necessary) 
«(u, v, w, s) E E A (x, y, z, t, s) E L(M4» 
which is a predicate in L'3. 
o 
Corollary 9.3 Let A be an NP-complete set (with respect to m-, T-, or SN-
reducibility). If there exists a sparse set S for which A E peS), then L'3 = L'4. 
Proof. If A is NP-complete for any of these reducibilities, then A E H 3• 
Therefore L3 and H3 intersect, which implies that L'3 = L'4. 
0 

Some Classes of Low Sets 
2CJ7 
Since sets having polynomial size circuits could be considered "feasible" 
even if no polynomial "uniform" algorithm is known, our last corollary still has 
some importance: it says that if NP-complete sets are feasible in this extended 
sense, then the polynomial hierarchy collapses to the third level- a consequence 
that would be at least surprising if shown to hold. However, some statements 
have been presented in the literature clearly accepting this possibility. 
A much less admissible hypothesis is the collapse to the second level. We 
are now going to strengthen our last corollary to one more level of collapse 
under the same hypothesis. The proof takes advantage of the self-reducibility 
structure of the NP-complete set SAT (which is also present, though sometimes 
less obvious, in other NP-complete sets). Let us show how to decrease the level 
in the low hierarchy of some self-reducible low sets. 
Recall from Chapter 4 of Volume I that a set A is self-reducible if and only if 
there is a polynomial time oracle machine 111, which always queries only about 
words strictly shorter than the input, such that A = L(M, A). Also, recall that 
SAT is self-reducible (Theorem 4.5 of Volume I). The following lemma states 
a key property of the self-reducible sets, which is a refinement of Exercise 30 
in Chapter 4 of Volume I. Again r denotes the alphabet. 
Lemma 9.3 Let A be self-reducible, and let AI witness this fact. For any set 
B and integer n, if B n r$n = L(M, B) n r$n then An r$n = B n r$n. 
Proof 
By induction on 71. For n = 0, M cannot query the oracle, and therefore 
L(Af, A) n r$n = L(M, B) n r$n. Thus An r$n = B n r$n. Assume it true 
up to size n and consider x of length n + 1. By hypothesis, all the queries 
made by AI to the oracle have length at most n, and therefore by the induction 
hypothesis both A and B answer alike. Thus x E B n r$n+l if and only if 
x E L(Af, (B n r$n», if and only if x E L(Af, (A n r$n», if and only if 
x E A n r$n+l . 
0 
Our next result gives an upper bound for the polynomial time hierarchy 
relative to a self-reducible set, provided that in tum the self-reducible set lies in 
the polynomial time hierarchy relative to a sparse set. 
Theorem 9.6 Let A be a self-reducible set such that A E Ek(S) for some 
k ~ 0 and some sparse set S. Then E2(A) ~ Ek+2' 
Proof The idea is similar to the above proof. An existential quantifier is used 
to guess a string s encoding the useful part of S. To test that the guessed s is 
correct, the self-reducibility structure of A is used. 
Let Afl be the self-reducing machine for A. We also know that A can be 
defined by a Ek predicate relative to S: 

208 
The Low and the High Hierarchies 
for some polynomial time deterministic oracle machine lI'h and k polynomially 
bounded alternating quantifiers. Note that there is a polynomial q such that 
x E A -¢=:} 3Yl VY2 •.. (x, Yl, . .. ) E L(M2, 5 n r~q(lxl» 
Consider now the following set, which corresponds to the definition of A 
but assuming that the oracle is given as part of the input. 
B = ((x,s) 13YlVY2 ... (X,Y ..... ,s) E L(M2)} 
where 1I1~ acts like 1112 but uses as oracle the set encoded by the last component 
of its input (here s). Obviously BE Ek • 
To show that E2(A) ~ Ek+2' consider an arbitrary set D E E2(A): 
xED -¢=:} 3zlVZ2(X,Zl,Z2) E L(M3,A) 
where both quantifiers are polynomially bounded. Again an initial segment of 
A, up to a polynomial length, can be used as oracle in this definition. Let r be 
this polynomial. Membership in D can be expressed as follows: 
xED 
(encoding 5) 
(x, Zl, Z2) E L(lIh {y I (y, s) E B}) 
Vw«(w, s) E B -¢=:} wE A) 
The quantifiers over the words Zi form a Ek+2 predicate, since it contains two 
quantifiers over a Llk+l predicate defined in terms of the Ek set B. It remains 
to show how to get rid of A in the process of checking the last equivalence, 
«(y, s) E B -¢=:} yEA). We use the self-reducing machine, and substitute for 
this predicate the following one: 
«(w,s) E B -¢=:} wE L(Ml,{y I (y,s) E B}» 
Observe that Lemma 9.3 guarantees that this holds if and only if yEA. It is 
a predicate in Llk+t. since B is in Eb and therefore the universal quantification 
yields Ih+l' When the whole predicate is put under the guess of s, it gets the 
form of an existential quantifier in front of a conjunction of a Ek+2 predicate 
and a ilk+l predicate, which yields once more a Ek+2 predicate. The proof is 
complete. 
0 
A number of corollaries follow from this theorem. We state here some of 
them, and will use the theorem again in the next section. 
Corollary 9.4 If A is self-reducible and A E Ek(5) for some k ~ 0 and some 
sparse set 5, then A E Ek+2 n ilk+2. 
Proof Follows from the fact that both A and A belong to E2(A). 
0 

Oracle-Restricted Positive Relativizations 
209 
Corollary 9.S If A E NP is self-reducible and A E P/poly, then A E L2. 
Proof. If A E P /poly then A E" P(S) for a sparse S (Theorem 5.5 in Vol-
ume I). Since P(S) = Eo(S) , from the theorem it follows that E2(A) ~ E2, 
which is the definition of L2. 
0 
Now we can tum to the proof of the following theorem, which states that it 
is unlikely that NP-complete sets are feasible even for nonuniform computation 
models, as announced above. 
Theorem 9.7 If there is a sparse set S such that the class NP is contained in 
the class P(S), then E2 = E3. Therefore, if any NP-complete set like SAT has 
polynomial size circuits, then the polynomial hierarchy collapses to E2. 
Proof. If NP ~ P(S), then SAT E P(S) (or, equivalently SAT has polynomial 
size circuits). But SAT is self-reducible; hence SAT E L2. On the other hand, 
since SAT is NP-complete, then SAT E H2. Therefore L2 and H2 intersect, 
which implies that E2 = E3. 
0 
9.S Oracle-Restricted Positive Relativizations 
This section is devoted to developing a series of results in the spirit of the 
positive relativizations presented in the previous chapter, which follow from the 
theorems of the previous section and from other somewhat similar arguments. 
Although lowness does not appear in an explicit manner, it will be easily seen 
that the concepts that are crucial to the proofs are similar to lowness properties. 
A positive relativization was defined in the previous chapter to be a restriction 
on the machines which allows one to show that equality among unrelativized 
complexity classes is equivalent to equality in any arbitrary relativization. Here 
we prove similar statements, but the restriction will no longer be on the oracle 
machines that define the classes, but on the class of permitted relativizations 
instead. Thus, our statements in this section will have the following format: 
such and such complexity classes are equal if and only if their relativizations to 
an oracle of such and such kind are equal. 
The first of our results will serve as a clarifying example. 
Theorem 9.8 P = NP if and only if for every tally set T, P(T) = NP(T). 
The proof will be based on the following lemma. 
Lemma 9.4 For every tally set T, NP(T) = NPb(T). 
Proof. Let A = L(M, T) for a nondeterministic oracle machine M, and let p 
be a polynomial bounding the running time of M. Thus the largest word that 
M can query on input x has length p(lxl). Consider the algorithm of Figure 9.2, 
which first scans all the reachable part of the oracle to figure out its precise 

210 
The Low and the High Hierarchies 
contents, and then uses this information to simulate the machine M. It is clear 
that this algorithm witnesses the fact that A E NPb(T) since it makes only 
polynomially many queries. 
0 
input x of length n 
F:= 0 
for each word 0;, 0 ~ i ~ p(n), 
if 0; E T then F := F U {Oi} 
comment: now F = T n r'5.p(n) 
simulate M with oracle F on input x 
if x E L(M, F) then accept 
end 
Figure 9.2 Few queries suffice when the oracle is tally 
Proof 
(Of Theorem 9.8) Assume P = NP. By Theorem 8.6 we have that 
for every oracle A, P(A) = NP b(.4). Let T be any tally set. By the lemma, 
P(T) = NPb(T) = NP(T). 
0 
Other positive relativizations restricted to tally oracle sets are presented in 
Exercises 5 and 6. We will prove in the remainder of this section several oracle-
restricted positive relativizations regarding the collapse of the polynomial time 
hierarchy and its equality to PSPACE. The class of oracles that we will consider 
is that of the sparse sets. The most interesting point in what follows is that the 
positive relativizations go both ways: if the unrelativized classes are equal, then 
they are equal for every sparse oracle set, while if they are different then again 
they are different for every sparse oracle set. Thus the results have a form close 
to the relativizing principles as described in Chapter 8. In the proofs we will 
use results from the preceding section. 
Our next theorem will be useful for proving oracle-restricted positive rei a-
tivizations of the equality of classes in the polynomial time hierarchy. 
Theorem 9.9 Let {M; I i ~ O} be a collection of deterministic oracle Turing 
machines, each AI; having the following properties: 
1. 
there is a polynomial p such that for all x and every oracle D, 
x E L(M;, D) {==? x E L(M;, D n r'5.p(lx/» 
2. 
there is another oracle machine 1111 such that for all strings y encoding 
a finite set Fy , 

Oracle-Restricted Positive Relativizations 
211 
3. 
andfurthermore L(M;, 0) E Ek for some fixed k independent ofi. 
Then, for every sparse set S and every i, L(M;, S) E E max{k,2}(S), 
Proof. 
Let A = LCM;, S) for an arbitrary sparse oracle S and an arbitrary i. 
We must show that A E Emax{k.2}(S), where k is a fixed constant given by the 
hypothesis. Let p be the polynomial corresponding to M;, so that x E A <===? 
x E L(M;, S n r:SP(lxl», and let M{ be as indicated in the hypothesis. Call 
B = L(U;, 0): from the last hypothesis it follows that B E Ek. 
Define the following set, which will be used to check the correctness of the 
guesses at the contents of S: 
N(S) = {(y,on) I y encodes S n r:Sn} 
Clearly 
(y, on) E N(S) <===? \t'u(lul :5 n)(u E S <===? u is in the set encoded by y) 
and therefore N(S) E III (S). 
Now, we have the following way of expressing membership in A: 
x E A <===? 3y«(y, ()l'(lxl» E N(S) A (x, y) E B) 
Since the first existential quantifier is polynomially bounded, the set N (S) E 
III (S), and B E Eko it follows that A E Emax{k.2}(S), 
0 
Observe that no bound is set on the time spent by M;, and that there is no 
need for the enumeration to be effective. Another technical result required in 
the proof is: 
Lemma 9.5 Every class Ek of the polynomial time hierarchy, k ;::: .1, has a 
complete self-reducible set. 
The proof is left as Exercise 7. We are ready to show that the collapse of the 
polynomial time hierarchy either holds in every sparse oracle, and then it holds 
in the unrelativized case, or in none, including again the unrelativized case. 
Theorem 9.10 
1. The polynomial time hierarchy collapses if and only ifitsrelativization to 
each sparse oracle collapses. 
2. The polynomial time hierarchy is infinite if and only if its relativization to 
each sparse oracle is infinite. 
Proof We prove the left to right implication of each statement. Both right to 
left implications are immediate, being the empty set sparse by definition. 
To prove the first statement, we show that for each k ;::: 2, PH = Ek if and 
only if for every sparse set S, PH(S) = Ek(S). Consider an enumeration {Mi I 

212 
The Low and the High Hierarchies 
i ~ O} of oracle machines such that for every oracle D, {L(M;. D) I i ~ O} = 
Ih(D), and such that it fulfills the conditions of the previous theorem; such a 
family can be easily constructed, using the hypothesis that PH = Ek to show 
the fact that L(M;. 0) E Ih ~ E k • 
Now, from the theorem it follows directly that for sparse sets S, Ih(S) ~ 
E rnax{k,2}(S) = Ek(S), since k ~ 2, and then the polynomial time hierarchy 
relative to S collapses to Ek(S). 
To prove the second statement, assume that there is a sparse set S such 
that the hierarchy relative to S collapses to level k: Ih(S) = Ek(S) = PH(S). 
Consider a self-reducible set A. complete for the class Ek+l ~ Ek+l (S) = Ek(S). 
By Theorem 9.6, since A. is self-reducible, E 2(A) ~ Ek+2' However, by the 
completeness of A. for the class Ek+t. we know that E 2(A) = Ek+3. Thus, we 
obtain that Ek+3 ~ Ek+2 and the polynomial time hierarchy collapses. 
0 
The following corollary is immediate. 
Corollary 9.6 
Either the polynomial time hierarchy col/apses for every sparse 
oracle, or is infinite for every sparse oracle. 
Analogous results hold with respect to the problem PH J, PSPACE. 
Theorem 9.11 
1. 
PH = PSPACE if and only if for every sparse oracle S, PH(S) 
PSPACE(S). 
2. 
PH =I PSPACE if and only if for every sparse oracle S, PH(S) =I 
PSPACE(S). 
Proof Assume that PH = PSPACE. Then the PSPACE-complete set QBF is 
in the polynomial time hierarchy, and thus QBF E Ek for some k. Therefore 
PSPACE = E k. It suffices to apply Theorem 9.9 to a family of oracle machines 
that accept exactly the class PSPACE(D) with oracle D, and this proves the first 
statement. 
To prove the second statement, notice that QBF is self-reducible. Thus, if 
PSPACE ~ PSPACE(S) = Ek(S) for some sparse set S then QBF E Ek(S), and 
by Corollary 9.4 we obtain QBF E Ek+2. which implies the equality PSPACE = 
PH. Therefore, if they are different then they PSPACE(S) =I Ek(S), for each k 
and each S, and hence PSPACE(S) =I PH (S). 
0 
Again a corollary follows. 
Corollary 9.7 
Either PH(S) = PSPACE(S) for every sparse oracle S, or 
PH(S) =I PSPACE(S) for every sparse oracle S. 
The theorems are quite general, and can be used to show similar results for 
other complexity classes. Some of them are proposed in Exercises 8 and 9. 

Lowness Outside NP 
213 
9.6 Lowness Outside NP 
The aim of this section is to generalize the concepts of low and high to sets not 
necessarily in NP. We consider that the results obtained in the previous sections 
were interesting enough to motivate this question; for example: in the proof that 
sparse sets in NP are in L2, the fact that the set is in NP is used. Is it possible to 
avoid this use, showing that sparse sets must be low regardless of whether they 
are in NP? The same question may be proposed for sets with polynomial size 
circuits, and for other sets. However, an extension of the concept of "low" must 
be defined before answering this, because the low and high hierarchies defined 
so far in this chapter are contained in NP. 
We present here such a generalization of the concepts of low and high, and 
show that sparse sets are always low, no matter how complex they are (even 
nonrecursive!). We show also that if a set has self-producible circuits (as defined 
in Chapter 5 of Volume I) then it is in the very first level of this extended low 
hierarchy. We will apply this result to some particular kind of sets in the next 
chapter. A different generalization of the concepts of lowness and highness is 
proposed and studied in Exercises 12 to 18. 
Definition 9.3 For each n 2:: 1, we define the classes extended low and ex-
tended high, noted ELn and EHn, as follows: 
ELn = {A I 17n(A) ~ 17n_1(A 67 SAT)} 
EHn = {A I 17n(A 67 SAT) ~ 17n(A)} 
(Observe, however, that EHn also makes sense for n = 0.) 
Thus, we define the classes by comparing a string of n quantifiers over the 
oracle A, with a string of n -
1 quantifiers over an oracle having the power 
of A, plus the power of one extra quantifier. Intuitively, if the inclusion as in 
the definition of extended lowness holds, it means that one of the quantifiers 
does not need to access A, and can be simulated just by a query to SAT. In a 
similar way, the intuitive meaning of the condition of extended highness is that 
a string of n quantifiers is able to extract from A enough information to render 
the oracle SAT useless. 
It should be noted that NP-hard sets whose relativized polynomial hierarchy 
collapses (as, for instance, PSPACE-complete sets) are simultaneously low and 
high under this definition. We present only results about low sets; the definition 
of extended high sets is presented here only for the sake of completeness. Next 
we show that the lowness of sparse sets is an inherent property, and not just a 
property of sparse sets in NP. 
Theorem 9.12 If S is sparse then S E EL3. 
Proof Let B E 173(S): x E B if and only if 3yVz3t(x, y, z, t) E R, where 
R is in P(S). Thus x is in B if and only if there is a string s coding S up 

214 
The Low and the High Hierarchies 
to some length polynomial in lxi, such that the same property holds when ora-
cle queries are solved by looking them up in s instead of querying S. Let M 
be the machine that performs this modified computation. Then, the predicate 
3t(x, y, z, t, s} E L(Af) is in NP and hence solvable by querying SAT about 
some value f«(x,y,z,s}) computable in polynomial time (f stands for the re-
duction to SAT). We can now express membership in B as follows: 
x E B 
~ 3Py3Ps\iPz\iPu 
[(u is coded into s if and only if u E S) 
/\f«(x, y, z, s}) E SAT] 
which is a predicate in E2 (S EEl SAT), since the predicate inside the square 
brackets is in P(S EEl SAT). 
0 
In a similar manner, the following can be proved: 
Theorem 9.13 
Every set having polynomial size circuits is in EL3. 
The proof is left as an exercise (Exercise 11). We finish this chapter with 
an example of a family of sets which lie in the bottom class of the extended 
low hierarchy. Recall from Chapter 4 the concept of self-producible circuits: 
A set A has self-producible circuits if there is a function h from {ot into r· 
computable in polynomial time with the help of an oracle for A, and there is a 
set B in P such that for all x, x E A if and only if (x, h(Olxl)} E B. 
This notion has some relevance here, since this property characterizes the sets 
T-equivalent to tally sets. For instance, from Lemma 9.4 we infer immediately 
that if A has self-producible circuits then NPb(.4) = NP(A). We can now show 
the following: 
Theorem 9.14 If the set A has self-producible circuits, then A is in ELI' 
Proof Let A have self-producible circuits. We prove that NP(A) is included 
in P(A EEl SAT). 
Let D = L(Ml , A) for a nondeterministic polynomial time oracle machine 
M l ; let p be the time bound. First we construct a second machine A12 which 
works as described in Figure 9.3, where B is as in the definition of self-
producible circuits. We have that x E A if and only if (x, h(Olxl)} E B. Hence, 
if we could ensure z = h(Op(lyll), then the simulation would proceed correctly, 
and accept (y, z) if and only if y ED. 
This machine }'12 works in nondeterministic polynomial time; therefore, 
L(M2) E NP. Let g be a polynomial time reduction from L(M2) to SAT. Recall 
from the definition of self-producible circuits that h is computable in polynomial 
time relative to A. Now, y E D if and only if (y, h(Op(lyl») E L(M2), if and 
only if g«(y, h(Op(lyll») E SAT; and this fact can be decided by a polynomial 
time oracle machine using A EEl SAT as oracle. (The oracle A is used in the 
evaluation of h.) Hence D E P(A EEl SAT). 
0 

input (y, z) 
simulate 111, on input y 
if and when AI, queries A about a word x, 
continue with answer YES if (x, z) E B, 
and continue with answer NO otherwise 
accept if and only if AI, accepts 
Figure 9.3 Using a "circuit" instead of an oracle 
Exercises 
215 
Finally, we show that the lowness of the sets in EL, implies an oracle-
restricted positive relativization such as those of Section 9.5. 
Proposition 9.2 P = NP if and only if/or every set A. EEL,. peA) = NP(A). 
Proof 
Since the empty set is in EL" the implication from right to left is 
immediate. For the converse, assume that P = NP and consider the class NP(A) 
for a set A in EL,. By the definition of EL" NP(A) is included in P(AEBSAT). 
But if P = NP, the answer for the oracle queries to SAT can be computed in 
polynomial time, and hence peA EB SAT) = peA). Thus NP(A) ~ peA). 
0 
Since the empty set has self-producible circuits, the following consequence 
follows from Theorem 9.14 and the proof of Proposition 9.2: P = NP if and 
only if, for every set A having self-producible circuits, peA) = NP(A). 
9.7 Exercises 
1. 
An infinite family of reducibilities can be defined as follows: set A is 
reducible to set B via the nih reducibility if and only if En(A) ~ En(B). 
Characterize, in terms of lowness and highness, those sets in NP that 
belong to the zero degree of the nih reducibility and those that are NP-
complete with respect to the nih reducibility. 
2. 
-The sets in the class R were shown in Corollary 6.4 of Volume I to 
have polynomial size circuits. Hence, they are in the class L3. Using the 
particular structure of the circuits for such sets, improve this by showing 
that R ~ L2• 
3. 
Consider the following alternative definition of the low and high hierar-
chies within NP: 
fIn = {A E NP I Lln+' ~ Lln(A)} 
Ln = {A E NP I Lln(A) ~ Lln} 
Find and prove the inclusion relationships among them and to the classes 
Ln and H n. Which classes collapse in case of collapse of the polynomial 
hierarchy? Show that they are recursively presentable, and that if the 

216 
The Low and the High Hierarchies 
polynomial hierarchy extends n levels then there are sets in NP which are 
neither in fIn nor in Ln. 
4. 
·Show that the sparse sets in NP and the sets in R are in the class L2, 
and that the co-sparse sets in NP are in the class L3• Use the first of these 
statements to improve Corollary 9.2. 
5. 
Show that NP = co-NP if and only if, for every tally set T, NP(T) = 
co-NP(T). 
6. 
Strong nondeterministic oracle machines were defined in Volume I. Say 
that an oracle machine is robustly strong if it is strong under every oracle, 
and define the class NPRS(A) as the class of all the sets accepted by 
robustly strong machines using oracle A.. Show that P = NP n co-NP if 
and only if, for every tally set T, peT) = NPRS(T). 
7. 
Show that the complete sets for each level of the polynomial time hierarchy 
presented in Exercise 2 of Chapter 8 in Volume I are self-reducible. 
8. 
Using Exercise 6 of Chapter 6 in Volume I, show that PP ~ PH if and 
only if, for every sparse oracle S, PP(S) ~ PH(S). 
9. 
Show that PP ~ PH if and only if, for every sparse oracle S, PP(S) ~ 
PH(S). 
10. 
Show that ELn n NP is Ln, and that EHn n NP is Hn. 
11. 
Show that every set (not necessarily in NP) having polynomial size circuits 
is in EL3 (Theorem 9.13). Show also that if a set having polynomial size 
circuits is in EHk then the polynomial time hierarchy collapses to Ek+2. 
12. 
Consider the following different generalization of high and low sets: 
L(i,j) = {A I Ei(A) ~ Ei } 
H(k,€) = {A I Ek ~ Ec(A)} 
We will call them generalized high and generalized low respectively. 
Discuss the relationships that must hold between i and j (respectively k 
and f) in order to obtain meaningful definitions (Le. nontrivial classes). 
Discuss the inclusion relationships between these classes. 
13. 
Show that for every j, L(1,j) = Ei n IIi' 
14. 
Show that for every k, H(k,O) are the T-hard sets for Eko and H(k, 1) 
are the SN-hard sets for Ek. 
15. 
Show that if L(i, j) and H(k, €) intersect, and j + € < i + k, then the 
polynomial hierarchy collapses to Emax{i,l}+i-i' Hint: Compare that class 
to Ek+max{i,i}-l' 
16. 
Using the tools developed for the proof of Theorem 8.6 of Volume I to 
show that BPP is included in the second level of the polynomial time hi-
erarchy, show that all the sets in BPP are generalized low; more precisely, 
show that BPP is included in L(2, 2). 
17. 
A set is pseudo-sparse if and only if it is in PH(S) for some sparse 
set S. Show that every pseudo-sparse set in the polynomial hierarchy is 
generalized low. Conclude that if a pseudo-sparse set is SN-complete for 

Bibliographical Remarks 
217 
some level of the polynomial time hierarchy, higher than its level relative 
to S, then the polynomial time hierarchy collapses. 
18. 
Show that if any sparse set is generalized high then the polynomial time 
hierarchy collapses. Conclude that either no sparse set is generalized high, 
or every sparse set is generalized high. 
19. Consider the class DEXT, and define a set A to be DEXT-Iow if and only 
if DEXT(A) = DEXT. Show that every set in Pis DEXT-Iow. 
20. 
Show that A is DEXT-Iow if and only if there are no tally sets in P(A)- P. 
(Observe that Corollary 5.2 of Volume I shows that the DEXT-Iow sets 
with self-producible circuits are in P.) 
9.8 Bibliographical Remarks 
The definition and the initial work on the low and the high hierarchies in com-
plexity theory is due to Schoning (1983), and is inspired by similar ideas from 
Recursive Function Theory; see Soare (1977). The translation to the polynomial 
time world presents quite different properties than those of the original concept 
of Recursive Function Theory. 
We have followed the cited work of Sch6ning (1983) in our Definitions 
9.1 and 9.2 and Theorems 9.1, 9.2, and 9.3; Proposition 9.1, Lemmas 9.1 and 9.2, 
and Corollary 9.1 are also from this work. The natural continuation of this work 
is presented in Ko and Schoning (1985), where an extensive study of low sets 
is presented. We have taken from this work Theorems 9.4 and 9.5, together 
with Corollaries 9.2 and 9.3. A stronger statement than Corollary 9.2 was 
proved before by Mahaney (1982): if NP has a sparse T-complete set then 
the polynomial time hierarchy collapses to .:12 (see Exercise 4). This statement 
has been improved again by Kadin (1987), who shows that under the same 
hypothesis the hierarchy collapses to the class pNP[log) of the sets polynomial 
time Turing reducible to NP by machines that ask only logarithmically many 
queries. 
The same work by Ko and Schoning (1985) was also the basis for The-
orem 9.6, which we have presented following Balcazar, Book, and Sch6ning 
(1986a). Its consequence, Theorem 9.7, was originally stated in Karp and Lip-
ton (1980), where a contribution of M. Sipser is also acknowledged. From Ko 
and Sch6ning (1985) we have also taken Exercises 2 and 3. 
Our discussion on the possibility of SAT having polynomial size circuits 
considers the relativized separation of the second level of the polynomial time 
hierarchy presented in Baker and Selman (1979). In this reference the possibility 
of a collapse at the third level was expressly conjectured. 
We have coined here the term "oracle-restricted positive relativization" by 
analogy with the positive relativizations of the previous chapter. The first ones 
appear in Long and Selman (1986), where they prove Theorem 9.8 and parts 
of Theorems 9.10 and 9.11, as well as Exercises 5 and 6. Our proofs here 

218 
The Low and the High Hierarchies 
follow Balcazar, Book, and Schoning (l986a), where the same results are proved 
in a different manner and completed with the other statements of Theorems 
9.10 and 9.11, thus obtaining the relativizing principles. Exercises 8 and 9 are 
from this reference. 
In Balcazar, Book, and Schoning (1986b) two generalizations of the low 
and high hierarchies are proposed and studied. One of them is presented in 
the text as Definition 9.3 and Theorems 9.12 and 9.13. Other properties of 
this generalization, and the other generalization and its properties, constitute 
Exercises 12 to 18. Lowness of sets with self-producible circuits, shown in 
Theorem 9.14, is from Balcazar and Book (1986). 
The application of the lowness concept to the exponential time hierarchy 
is due to Book, Orponen, Russo, and Watanabe (1988). Exercises 19 and 20 
are from this reference, where in addition several relationships are established 
between the low sets in the exponential time hierarchy and the low sets in 
the polynomial time hierarchy. Exercise 16 is from Zachos and Heller (1986), 
although they do not use the "lowness" terminology. Other applications of the 
lowness concept and related notions have been found; an example of these is 
Watanabe (1988). 
One of the most natural problems in NP, not known to be neither in P 
nor NP-complete, is the problem of determining whether two given graphs are 
isomorphic. A recent result of Schoning (1987a) shows that this problem is in L 2• 

10 
Resource-Bounded Kolmogorov Complexity 
10.1 Introduction 
There is a classical approach to the problem of classifying strings with respect to 
the difficulty of computing them; in some sense, we look for a definition of the 
amount of information coded by the string. Intuitively, a difficult string contains 
a high amount of compactly coded information, and one has to know all this 
information in order to write down the string. Conversely, an easy string has low 
infonnation content, and one can describe the string in a compact way, so that 
from this small description one can obtain enough infonnation to reconstruct 
the string and write it down. This idea can be formalized as we do in this 
chapter, giving rise to a different concept of complexity, known as Kolmogorov 
complexity, which has been succesfully used for proving lower bounds in a more 
concrete approach to complexity theory. 
A related (and, under the appropriate definitions, equivalent) concept is that 
of random strings, which contain so much information that there is no possibility 
of predicting new bits given the knowledge of the previous ones, because the 
new bits must encode different information. 
The aim of this chapter is to present relationships between this approach 
to complexity, based on the infonnation content, and the resource-bounded ap-
proach developed in this book until now. We start by defining (unbounded) 
Kolmogorov complexity, and indicating some interesting facts about the strings 
of high complexity. We omit here two variants of this complexity measure, 
the conditional and the uniform measures, which can be found in the original 
paper of Kolmogorov and in later works of other authors. We then move to 
the resource-bounded versions of Kolmogorov complexity, which are closer to 
our resource-bounded complexity classes, and characterize the sets consisting 
only of words of small complexity, via resource-bounded reductions. Finally, 
we characterize in tenns of nonuniform complexity measures the class of sets 
whose characteristic function has low Kolmogorov complexity. 
Throughout this chapter our alphabet is fixed to E = {O, I}. Given any 
Turing machine AI which computes a function, we denote by M(x) the result 
of the computation of M on input x (if defined). 

220 
Resource-Bounded Kolmogorov Complexity 
10.2 Unbounded Kolmogorov Complexity 
Let U be a fixed universal Turing machine. Hence U is a machine that is given 
as input a pair x = (M, y), where M is (the encoding of) a machine, and y is an 
input for machine M. U accepts if y E L(M). This is to say that the input to U 
is a program together with data for this program; the machine U will execute the 
program on the data. By the well-known s-m-n theorem of Recursive Function 
Theory, we can assume that both the program and the data are given just as a 
program, in which the data appear as constants. The Kolmogorov complexity 
of a string w (relative to U) is the length of the shortest such program (i.e. pair 
(program, data) which, when given as input to U, will lead U to write down 
w as output. Formally: 
Definition 10.1 
I<u(w) = min{lxll U(x) = w}. 
The set {Ixll U(x) = w} is never empty: we can obtain w by giving w itself 
as input to a program for the identity function. Thus, the following holds: 
Proposition 10.1 There is a constant c (depending only on U) such that Jor 
all w, (I<u(w) :S Iwl + c). 
Proof. Let M be an encoding of a machine that computes the identity function, 
copying its input into the output tape. Let c = 1M I· Then, for any string w, 
giving x = (M, w) as input to U allows one to obtain w. The length of x is 
Iwl+c. 
0 
Our next result indicates that, modulo an additive constant, the choice of U 
as the base universal machine is irrelevant. 
Proposition 10.2 Let U and U' be universal machines. Then there is a con-
stant c (depending only on U and U') such thatJor all w, (I<u'(w) :S I<u(w)+c). 
Proof. 
In this case, c is just the length of the program p that allows U' to 
simulate U. 
0 
Thus, from now on, the universal machine is assumed to be fixed, and the 
sUbscript U dropped from the definition. We prove later similar properties of 
the resource-bounded version of Kolmogorov complexity. 
The main observation is that, by an easy counting argument, we can prove 
that most words have high Kolmogorov complexity. 
Proposition 10.3 For each length n, and each natural number m, there are 
at most 2n - m+1 -
1 strings w, with Iwl :S n, having Kolmogorov complexity 
I«w) :S n - m. 

Unbounded Kolmogorov Complexity 
221 
Proof The number of possible programs (with the input incorporated as before) 
of length at most n - m is at most 2n- m+1 -1, and each of these programs allows 
the universal machine to write down at most one string. 
0 
As an example of a string with low complexity, take any string which consists 
just of zeros. It is enough to give to the universal machine the length of the 
string and a constant program that reads this length and writes the corresponding 
number of zeros. Hence, the complexity of such a string is a constant plus the 
base 2 logarithm of its length. This fact will be used later. Examples of 
strings of high complexity are much harder to produce; one can just write a 
"random" string of zeros and ones, and there is a high probability that the string 
has high complexity, by the previous proposition; but, as we shall see below 
(Example 10.2), for long enough strings it is not possible to prove that this is 
the case. 
Let us present now, in a very intuitive way, some uses of this definition of 
complexity. The mathematically mature reader will have no problem in formal-
izing our exposition. 
Example 10.1 As first example, we use these concepts to argue that there are 
infinitely many prime numbers. Recalling that each natural number uniquely 
defines and is uniquely defined by its prime factorization, if only k primes exist 
then each natural number n can be reconstructed from much smaller information: 
it suffices to design a program with a built-in table containing the k primes, 
which is a constant size, and give it as data the exponents of these primes in 
the prime factorization of n. The program raises the primes to their exponents 
and computes the product, obtaining n from the exponents. Observe that the 
exponents of n are all less than or equal to log2 n, and therefore their length in 
binary is logarithmic in the length of n. A universal machine can reconstruct 
n from this program (of constant size, including the table of primes) and the 
exponents (of total size less than k ·log2 In I). But the last proposition shows that 
not every number can be reconstructed from a so small quantity of information; 
in fact, most numbers cannot. Hence an infinite quantity of primes is required. 
Example 10.2 As a second example we give the information-theoretic proof 
of the GOdel incompleteness theorem. It is based on the Berry paradox: "Find 
the smallest positive integer whose specification requires more characters than 
there are in this sentence". 
Pick any consistent formal system F powerful enough to formalize recur-
siveness concepts, like machines and computations, and producing a recursively 
enumerable set of theorems. Peano arithmetic will do. Consider the program 
that enumerates theorems of the formal system, and let c be a constant much 
greater than "the size of this program. We know that there are only finitely many 
words having complexity smaller than c; hence, many words have complexity 
higher than c. Let w be one of these words: this fact, though true, cannot be 

222 
Resource-Bounded Kolmogorov Complexity 
proved by F. To show this, assume that F I- "]«u) > c" for some words u, and 
consider the u having the smallest proof (in the ordering induced by the alpha-
bet). The following program will produce u: enumerate the proofs of theorems 
of F, and if and when a proof is found of the fact "](u) > c" for some u, then 
output this u. Assume that it halts. If c was chosen large enough, this program 
is shorter than c, and hence "](u) > c" is not true; but, since F I- "](u) > c", 
the inconsistency of F would follow. As a result, this program will never halt, 
which implies that the set of the u such that F I- "J(u) > c" is empty; the 
incompleteness of F follows. 
10.3 Resource-Bounded Kolmogorov Complexity 
Let us tum now to resource-bounded Kolmogorov complexity. Recall that U is 
a fixed universal machine. Let I, 9 be functions on the natural numbers. Define 
the sets J([f, g] and J( S[f, g] as follows: 
Definition 10.2 
1. 
The Kolmogorov time-bounded complexity set J([f, g] is the set formed 
by the strings u such that there is a word w, of length Iwl $ I(lul}, such 
that U(w) = u and this result is obtained in at most g(lul} steps. 
2. 
The Kolmogorov space-bounded complexity set J( S[f, g] is the set formed 
by the strings u such that there is a word w, of length Iwl $ I(lul}, such 
that U(w) = u and this result is obtained using at most g(luj) work space. 
Thus, the set J([f, g] is the set of strings which can be compressed by a factor 
of I, and which can also be recovered from their compressed form within the 
time bound g. Similarly, J(S[f,g] is the set of strings that can be compressed 
by a factor of I, and can be recovered from this compressed form within the 
space bound g. Observe that the difference compared with classical Kolmogorov 
complexity is that we require not only that the strings are generated by a suitably 
short program, but that they are generated by a short and fast (or at least short 
and feasible) program. 
Sometimes it is useful to assume that the universal machine used for mea-
suring the Kolmogorov complexity can query oracles. In this case, we denote 
by J(A[f,g] the set of words with complexity I, bounded in time g, as before, 
but allowing the universal machine to query oracle A. 
Notice that the use of a fixed universal Turing machine increases the length 
of the necessary data for writing down w by at most an additive constant; the 
running time by at most a factor of the logarithm of the running time; and the 
space by at most a multiplicative constant. See Exercise 1. 
Our first use of time-bounded Kolmogorov complexity is to present an al-
ternative proof of the relativized separation of NP(A) from P(A) under the 
appropriate A, which was shown in Theorem 7.2. 

Resource-Bounded Kolmogorov Complexity 
223 
Theorem 10.1 There is a set A such that NP(A) =I P(A). 
Proof Let T be a tally set in EXPTIME - P, consisting only of words whose 
length is a power of 2_ Such a set can be obtained as in the time hierarchy 
theorem (Theorem 2.11 in Volume 1)_ Construct the set A as follows: for each 
length m = 2n, if T has a word of this length, om, then put into A the first string 
of Em, in lexicographic order, which is not in K[log2 n, n 1o&2 log2 n] (i.e. the first 
string that cannot be constructed in time nlog2lo&2 n from a string of logarithmic 
length); else keep A empty for this length. 
Note that A is decidable in exponential time: on an input y of length m, 
check (in exponential time) that om E T and check (in exponential time also) 
that y is the first word of length m not in K[log2 n, n 1o&2 log2 n]. 
Of course, T is in NP(A): on input om, just guess a word of length m 
and check that it is in A. To show that T is not in P(A), suppose that the 
deterministic oracle machine M, with oracle A, is able to decide T. On input 
om, where m is a power of 2, 111 must decide whether it is in T. We claim that 
the oracle cannot help 111 in deciding it. 
Consider the strings queried by A1. If their length is not a power of 2, then 
we know that the answer is NO. If their length is a power of 2 less than m, 
then M can compute the answer in time polynomial in m, using the exponential 
time machine for A. Finally, if their length is m, then 111 has generated it from 
om in polynomial time: thus it is in ]([log2 n, nk] and we also know that the 
answer is NO. 
Therefore, computations on short strings can be substituted for some of the 
queries made by M, and the remaining ones, being too large, must be always 
answered NO; thus T is decidable in polynomial time without any oracle. So it 
must be in P, which contradicts its definition. 
0 
Recall from Chapter 7 that, in the constructions there, the word in the oracle 
was hidden to the deterministic machines by means of a search over all the 
words queried by these machines. Now this search has been substituted by the 
property that the string is hard to generate, and hence cannot be queried by 
the deterministic machines on the "easy" strings of the tally set to be decided. 
Exercise 11 shows that the existence of these difficult strings in the oracle set 
A is necessary to separate P from NP, unless P = NP in the unrelativized case. 
Several variations of the concept of Kolmogorov complexity can be defined. 
A quite natural and interesting variant is relative Kolmogorov complexity, in 
which a string is given for free and not counted for the complexity. We de-
fine only the time-bounded version; the space-bounded version can be defined 
analogously. 
Definition 10.3 The Kolmogorov time-bounded complexity set relative to the 
given string z, denoted K[(f,g)lz], is the set/ormed by all strings u such that 
there is a string w, 0/ length Iwl :5: f(lul), such that U«(w, z) = u and this 
result is obtained in at most g(lul} steps. 

224 
Resource-Bounded Kolmogorov Complexity 
Thus the only difference is that the universal machine U can use for free 
the information provided by the string z during its construction of the output u; 
but its computation time is restricted. Relative Kolmogorov complexity gives 
an interesting necessary and sufficient condition for the equality P = NP. The 
result establishes that NP differs from P exactly when polynomiaIIy clocked 
nondeterministic machines can "manufacture" randomness, i.e. when the wit-
nesses (or certificates, as defined next) they have to find are "hard" with respect 
to the input in the sense of relative Kolmogorov complexity. Before formally 
stating the theorem, let us define certificates. 
Definition 10.4 
Given a nondeterministic machine 111; and an input x accepted 
by 1I1i , the certificates Ctfu/x) are the words encoding accepting paths of the 
computation of 1I1i on x. 
We assume that all the words encoding the accepting paths on x have the 
same length, a polynomial on Ixl depending only on the machine, and that they 
encode the input x in a form which is easy to retrieve. 
Theorem 10.2 P = NP if and only if NP has simple certificates relative to 
the input. More precisely, P = NP if and only if for every polynomial time 
nondeterministic machine 1I1i there exists a constant c such that 
\Ix E L(M;)(CtfM, (x) n J([(c log n, nC)lx] =10) 
Proof Assume P = NP and consider a nondeterministic machine 1I1i. Consider 
the function that maps each certificate to the corresponding input word: by 
the above assumption on the words encoding computations, this function is 
polynomial time computable and honest. By Theorem 3.4 in Volume I we 
obtain that it is possible to invert it in polynomial time, and therefore from x 
it is possible to construct a certificate in polynomial time, if one exists. Thus, 
certificates are in a sense trivialized, since the information required to construct 
them is the knowledge of the machine inverting the function, which can be 
described in a constant number of symbols, and the input x, which does not 
count due to the definition of relative Kolmogorov complexity. 
Conversely, assume now that every set L in N P has simple certificates. By 
definition, there are only 20 (logn) simple strings relative to the input; that is a 
polynomial number. Moreover, such simple strings can be found deterministi-
cally, in polynomial time, by just feeding short words into the universal machine 
to produce them; in tum, we check for each of them whether it is a true cer-
tificate, and if found we accept. All accepted words must be in L because a 
certificate exists; moreover, as all words in L have a simple certificate, the de-
terministic algorithm will find it. Therefore we can decide L in deterministic 
polynomial time. 
0 

Tally Sets, Printability, and Ranking 
225 
10.4 Tally Sets, Printability, and Ranking 
In this section we study the sets composed almost completely of easy strings, 
and characterize them in various ways. These sets are interesting in that all but 
a finite number of their words can be described in a much shorter way, and the 
words recovered quickly from the description. 
Since the number of available words of such low Kolmogorov complexity 
is small, all these sets will be sparse. More particularly, typical examples of 
such sets are tally sets: tally words can be recovered in linear time from a much 
smaller amount of information, namely their length. Among other characteriza-
tions, we will show here that tally sets are typical representatives of this family 
of sets in the following sense: this class is precisely the closure of the tally sets 
under polynomial-time computable isomorphisms. 
The sparseness of these sets has yet another significance: the position of 
a string in the set (which below is called its ranking) can be written down 
with a small number of bits, and in certain cases this information is enough 
to recover the string. Thus, the properties of ranking functions will be very 
useful for technical purposes, and play an important role in the proof of the 
above-mentioned characterization. This fact will be related also to "printability": 
given a sparse set in P, is it possible to produce in polynomial time the list of 
its contents up to a certain length? Those sets for which it is possible are related 
in a similar way to the tally sets in P. Let us start by introducing the following 
definition: 
Definition 10.5 The ranking function rL of a given language L is defined as 
follows: rL(x) is the cardinality of the set {y ELI y ~ x}. 
Observe that when x E L, rL(x) is precisely the position of x in L. Ranking 
functions are related to compression functions, which are one-one functions 
which map strings in a language L into smaller words coding them. In particular, 
the ranking function is an optimal compression. See Exercises 4 and 5. 
Usually ranking functions are not injective on E*, although they are injective 
on L. Thus their inverses are not uniquely defined. However, one of the ways 
of defining inverses of ranking functions is particularly important, and we will 
choose it, by convention, as what we mean by the inverse of a ranking function. 
Definition 10.6 Let rL be the ranking function of L. We will denote by ri l 
the following function, which will be called the inverse of rL: 
riley) = min{x I rL(x) = y} 
The importance of the inverse of a ranking function comes from the following 
observation: ril(O is precisely the ith word of L in the standard ordering. The 
complexity of the inverse of a ranking function, relative to the complexity of 
the ranking function itself, is characterized in the following useful result. 

226 
Resource-Bounded Kolmogorov Complexity 
Lemma 10.1 If r L is computable in polynomial time, then ri1 is computable 
in time polynomial on its output. 
Proof Given i, compute ril(i) using rL as follows: first, to find the length 
of ri1(i), iteratively compute rL(A), rL(I), rLCll) ... , until a word In is found 
such that rL(In-l) < i ~ rL(1n). This process requires n evaluations of the 
polynomially computable function rL, and informs us that Irr:1(i)1 = n. Since 
rL is nondecreasing, a binary search can be performed now to identify rr:1(i) 
among all the 2n words of length n, with at most n additional evaluations of rL. 
The total amount of time is polynomial on n, the length of the output of rr:1 
on i. 
0 
To ilustrate the use of the idea of ranking to help prove facts related to 
Kolmogorov complexity, we formalize now the argument given above regarding 
the coding of words of sparse sets by their index in the language. 
Proposition 10.4 Let S be a sparse set in DTIME(T(n)). Then 
S ~ J{[k log n, kT(n)2n ] 
for some constant k. 
Proof For each nand i, the i 1h among the words of length n in S may be 
coded by a word (AI, n, i) where M is a constant word encoding the following 
machine: on input nand i, it cycles through all the strings of length n and, for 
each one, it decides whether it is in S. A counter is kept of the number of words 
found in S. The output is the i lh word among those found to be in S. 
0 
Observe that the index i which encodes each w in the proof is exactly r LC w)-
rL(llwl-l). An alternative statement, adequate for all sparse sets (even nonre-
cursive), is given in Exercise 2. Observe that as a consequence, all the sparse 
sets in DEX P, and in particular those in P, are included in J{[O(log n), 20 (n)], 
where the constants hidden in the 00 notation depend only on the set. Cur-
rently, it is an open problem whether all the sparse sets in P are included in 
J{[O(log n), 71,0(1)]. We will define now one of the main concepts in this section: 
the class of sets containing only simple words, with finitely many exceptions. 
Definition 10.7 For given functions I, g, the class SETS-J{[j, g] consists of 
all sets A such that all the words in A, but finitely many of them, are in ]{[j, g]. 
For sets of functions F, G, SEIS-J([F, G] is the class of sets which are in 
SETS-J{[j, g] for some I E F, 9 E G. 
Thus, SETS-J{[j, g] is the class of finite extensions of subsets of J{[j, g]. 
Sets defined in this way can be characterized in an interesting manner. Typical 
families to be used in the definition of Kolmogorov complexity will be the 
families 

Tally Sets, Printability, and Ranking 
227 
log = {f I fen) = c· log2 n for some constant c} 
lin = {f I fen) = c· n for some constant c} 
poly 
= {f I fen) = c . n k for constants c, k} 
polylog = {f I fen) = c . (log2 n)k for constants c, k} 
According to this definition, SETS-]{[log, poly] denotes the class of all sets 
A such that constants c, k exist (depending only on A), for which every word in 
A is in ]{[c log2 n, cnk], with at most finitely many exceptions. Natural exam-
ples of this family are the tally sets, since all their words can be reconstructed in 
a short time from much less information, namely the length of the word. There 
are other words of similar Kolmogorov complexity, but many of the natural ex-
amples that come to mind are usually just lexical variations of tally sets. This 
is not casual: we will prove a strong relationship (namely polynomial time iso-
morphism) between tally sets and arbitrary sets in the class SETS-]{[log,poly]. 
For now, let us observe the following. 
Proposition 10.5 
The class SETS-]{[log, poly] is closed under polynomial 
time isomorphism. 
Proof. 
Let A be a subset of ]([c log2 n, cnk] U F, where c and k are constants 
and F is finite. Let B be a set isomorphic to A via isomorphism h, so that 
x E A <==? hex) E B. For each word x E A - F, let f(x) be a word of length 
c.log2lxl such that, on input f(x), the universal machine prints x in c'lxl k steps. 
Consider a constant size encoding ,Tl.1 of a machine that operates as follows: on 
input y, simulate the universal machine on y and then compute h on the result. 
Let p be a polynomial bounding the length of h-1• Now, for all but finitely 
many words z E B, the universal machine prints z on input (AI, f(h-1(z»), 
which has length c . log2 Ih-l(z)1 + IMI ~ c .IOg2P(lzi) + IMI ~ c'.log21z1. The 
time required is Ih-1(z)ik to recover h-I(z), and a polynomial on Ih-.l(z)1 to 
compute h recovering z. This whole process is polynomial in Izl, and therefore 
BE SETS-]{[log, poly]. 
0 
Later, we will show that the following intuitive statement can be formalized: 
the sets consisting only of strings of low complexity are themselves, in some 
sense (related to the lowness concept of the previous chapter), of low complexity. 
A first, easy approximation is given in the following proposition. 
Proposition 10.6 
For constants c and k, ]([clog n, nk] ~ P. 
Proof To determine whether x E ]([c log n, nk], just cycle over all the poly-
nomially many words of length at most c log Ix I and run the universal machine 
on each of them for at most Ixl k steps; accept if and only if x itself is produced 
by any of these runs. 
0 

228 
Resource-Bounded Kolmogorov Complexity 
This easy proof is worth some comments. Notice that the way of deciding the 
set is quite specific, and does not limit itself to computing its bit-valued output; 
instead, the whole list of words of the set ]{[c log n, nk] up to length n can be 
obtained in time polynomial on n. Moreover, if A is a subset of ]([c log n, nk] 
and it is available as an oracle, we can perform the same procedure querying 
each obtained word, and thus list in polynomial time all the words in A up to 
length n. This process appeared in the previous chapter, e.g. in the proof of 
Lemma 9.4. If we additionally assume that A E P, then no oracle is needed. 
Are there other sets for which this listing can be produced in polynomial time? 
Let us give a name for the concept. 
Definition 10.8 A set A. is p-printable if and only if there is a polynomially 
computable function f from {O}· to E· such that f(on) is a word coding the 
set An E$n. 
An alternative concept, which is also interesting, is obtained for sets not 
necessarily in P, under the condition that the set itself is available as an oracle for 
the process of printing. Those sets are called "self-p-printable", and are studied 
in the exercises. Another observation is that p-printable sets are afortiori sparse, 
and that their ranking functions are easily seen to be computable in polynomial 
time. 
Our next result is the key step for the announced characterization of SETS-
]{[log, poly] as the closure of the class of tally sets under polynomial time 
computable isomorphisms: it proves the particular case of the sets in SETS-
]{[log, poly] n P. It also shows that "p-printable" is just another name for these 
sets, and thus the way of being p-printable described in the presentation of the 
concept is essentially the only one. Further, it shows that no other sparse sets 
have polynomial time computable ranking functions. 
Theorem 10.3 The following are equivalent: 
(a) 
S E SETS-]{[log,poly] n P. 
(b) 
S is p-prinrable. 
(c) S is sparse and its ranking function is computable in polynomial time. 
(d) 
S is polynomial time isomorphic to a tally set in P. 
Proof. 
That (a) implies (b) is just the formalization of the discussion following 
Proposition 10.6, and that (b) implies (c) is clear (see the observation following 
Definition 10.6). Let us show that (c) implies (d), which is the hardest part of 
this theorem. 
Let rl be the polynomial time computable ranking function of S. By sub-
tracting rl(x) from the number of words in E· less than or equal to x, we 
obtain the polynomial time computable ranking function for S. Denote it by r2. 
Observe also that the cardinality of S n En is rl(1n) - rl(1n-I), and assume 
that it is bounded by a nondecreasing polynomial p(n). We first describe how 

Tally Sets, Printability, and Ranking 
229 
the isomorphism is defined. We will set aside infinitely many blocks of pen) 
consecutive tally words to correspond to the at most pen) words of length n 
in 5. Thus, 5 n En can be mapped into {on.p(n) ..• o(n+I)'p(n)-I} by the function 
f: 5 -+ {O}*, defined as follows: f(x) = on.p(n)+i-I where n = Ixl and x is the 
ith word of length n in 5. Observe that f is injective, but as 5 may have less 
than pen) words at some lengths n, not all the words in the blocks are exhausted 
by f. 
Let T = f(5). T will be the tally set isomorphic to 5, and the isomorphism 
h is defined like f within 5; it is completed by mapping the i lh word of S 
to the ilh word of T = E* - T, so that it is obviously bijective. Observe 
also that Ixl and Ih(x)1 are polynomially interrelated. Indeed, on the one hand, 
Ixl :::; If(x)1 :::; (Ixl + l)p(lxi), and on the other hand, since both S and T have 
exponential density, both of their ilh words have length 8 (log i) and therefore 
are linearly related. 
It just remains to exhibit a polynomial time decision procedure for T and 
polynomial time algorithms for hand h- I • Since ranking functions allow us to 
talk concisely about such concepts as "the ilh word of 5", we define T and h in 
terms of ranking functions. 
First observe that on.p(nl+i E T if and only if 15 n EOI > i. Thus, T = 
{on.p(n)+i I 0 :::; i :::; rl (1 n) - rl (1 n-I)}, which is polynomial time decidable 
since rl is polynomial time computable. Moreover, T's ranking function is also 
computable in polynomial time; we will use the ranking function of T, denoted 
here r3, which can be computed as follows: on input y, find n and i < pen) 
with Iyl = n· pen) + i, and then output r3(y) = min{rl(ln-I) + i, rl(ln)}. 
Now, on x of length n, h can be defined as follows: 
The first case is clearly polynomial time computable. For the second case, notice 
that the length of hex) = r3"l(r2(x» is polynomial in lxi, and therefore r3"1 can 
be computed in time polynomial in Ixl by Lemma 10.1. 
Finally, to check the polynomial time invertibility of h, notice that if yET, 
say y = on.p(n)+i with i < p(n), then h-I(y) is the ilh word of length 11 in 5, 
which is rll(i + rl(1n-I», computable in polynomial time by Lemma 10.1; and 
if y rt T, then h-I(y) is the (r3(y»th word of S, which is r2"l(r3(Y»' Again 
by Lemma 10.1, it is computable in polynomial time. This completes the proof 
that (c) implies (d). 
The remaining implication from (d) to (a) is easy since tally sets are in SETS-
J([log,poly], and both this class and the class P are closed under polynomial 
time isomorphism (proposition 10.5). 
0 
We give next the full isomorphism characterization for arbitrary sets not 
necessarily in P. 

230 
Resource-Bounded Kolmogorov Complexity 
Theorem 10.4 S is in SETS-l{[log,poly] if and only if S is isomorphic to a 
tally set. 
Proof Let c and k be constants and F a finite set such that 
S ~ ]{[c·log n, nk] U F 
as given by the hypothesis that S E SETS-l{[log,poly]. By Proposition 10.6, 
the set l{[c·log n, nk] belongs to P. Thus, by the previous theorem, there is a 
tally set T E P isomorphic to l{[c·log n, nk] U F. Let h be the isomorphism 
from l{[c·log n, nk] U F to T. Then A is isomorphic to h(A) ~ T via h itself, 
and therefore is isomorphic to a tally set. The converse is immediate from the 
closure of SETS-l{[log,poly] under polynomial time isomorphism. 
0 
The p-printable sets have polynomial size circuits. Recalling the concept of 
self-producible circuits from Section 5.7 of Volume I, we obtain: 
Corollary 10.1 Every set A in SETS-l{[log, poly] has self-producible circuits. 
Proof This follows from the result proven in Theorem 5.10 of Volume I that 
a set A has self-producible circuits if and only if A =~ T for some tally set T. 
This is the case for sets polynomially isomorphic to tally sets. 
0 
Also, lowness of these sets follows from the result proven in Theorem 9.14: 
sets with self-producible circuits are extended low. This is the formal statement 
of the announced fact that sets containing only simple words are also themselves 
simple in some sense. 
Corollary 10.2 Every set A in SETS-l{[log,poly] is in ELI' 
From this property of lowness, an oracle-restricted positive relativization 
follows immediately, applying Proposition 9.2: P = NP if and only if for every 
set A E SETS-l{[log, poly], P(A) = NP(A). Other interesting results regarding 
the concepts studied in this section are proposed in the exercises. 
10.5 Kolmogorov Complexity of Characteristic Functions 
To end this chapter, we present a different relationship between Kolmogorov 
complexity and nonuniform complexity. We investigate the consequences of 
a set having a characteristic function of low Kolmogorov complexity on the 
complexity of the set. We show that precise relationships can be found between 
the Kolmogorov complexity of the characteristic function of the set and the 
nonuniform complexity of the set. 
In order to speak of the Kolmogorov complexity of the characteristic function 
of a set, we have to identify it with some sort of string. We do it in the following 
standard way: we identify the characteristic function of a set A with the infinite 
word having as nth bit a 0 or a 1, according to whether the nth word of E*, in 
its standard ordering, is in A. We denote this infinite string as XA' 

Kolmogorov Complexity of Characteristic Functions 
231 
For such infinite words, we define their Kolmogorov complexity as follows: 
Definition 10.9 
1. 
An infinite string w is in K[f, g] if and only iffor all but finitely many n, 
the word formed by the first n symbols of w is in K[f, g]; 
2. An infinite string w is in K S[f, g] if and only if for all but finitely many 
n, the word formed by the first n symbols of w is in K S[f, g]; 
3. If F and G are classes of functions, K[F, G] denotes the union of K[f, g] 
for f E F and 9 E G. 
4. If F and G are classes of functions, J( S[F, G] denotes the union of 
KS[f,g] for f E F and 9 E G. 
Now we can prove the following characterization: 
Theorem 10.5 A E PSPACE/poly if and only if its characteristic string XA 
is in KS[polylog,polylog]. 
Proof Let A be a set in PSPACE /poly, via the advice function f and the 
PSPACE set B. Consider an initial segment of XA, of length m, and let n be 
the smallest integer such that m ~ 2n, so that the m th word of E* has length n. 
This initial segment can be generated in polylog space, given m and the advice 
for A, by the program in Figure 10.1. 
for each word w up to the m1h word of E* loop 
if (w, fClwl») E B 
then output 1 
else output a 
Figure 10.1 Generation of an initial segment of XA 
Observe that the input has length log2 m + nk for some constant k depending 
on the size of the advice; note that the advices for all the lengths up to n are 
needed, but this amount is still polynomial in the maximal length to be reached, 
n. Since n is within a constant of log2 m, the size of the data is polynomial in 
log2 m. Finally, the space needed to run this program is again polynomial in n. 
Therefore, the initial segment under consideration is in J( S[polylog, polylog]. 
Conversely, if each initial segment of length m = 2n can be constructed by 
the universal machine on some input Un of length (lOg2 m)k = nk, then A is 
in PSPACElpoly via the advice function fen) = Un, and the machine M that 
first simulates the universal machine on the advice, and then uses the output XA 
to decide A in the length 7l. Af is a PSPACE machine because the universal 
machine uses only space poly log on m, i.e. polynomial in n. 
0 

232 
Resource-Bounded Kolmogorov Complexity 
10.6 Exercises 
L 
Show that there is a universal Turing machine U such that for any other 
Turing machine V, J{v[f, g] ~ J{u[f +c, c· 9 ·log2 g+c] and J{Sv[f, g] ~ 
J{ Su[f + c, c· 9 + c] for some fixed constant c depending only on V. 
2. 
Let J{A(w) denote the unbounded Kolmogorov complexity of w with or-
acle A. Show that if S is sparse then, for each w E S, J{s(w) ~ log2lwl. 
3. 
The sets J{[f, g] for finite strings are defined and used only for particular 
functions I and g, and not for families of functions as in the case of 
SETS-J{[log, poly] or the infinite strings. Find a convincing argument to 
show that the class of finite strings 
J{[log,poly] = U J{[f,g] 
is not worth particular study. 
I Elo, 
gEpoly 
4. 
Prove that a function is the ranking function of some set if and only if it 
is nondecreasing and surjective. 
5. 
A function I is a compression lunction for a language L if and only if I is 
one-to-one on L and for all but finitely many words x E L, I/(x)1 < Ixl. 
If I/(x)1 ~ flogcL(lxl)l then I is said to be an optimal compression. Find 
an argument to justify this name and show that the ranking function of L 
is an optimal compression. 
6. 
Find and prove the closure properties of the classes SETS-J{[1og,poly] 
and SETS-J{S[log,poly]. 
7. 
Show that DEXT = NEXT if and only if every sparse set in NP is p-
printable. 
8. 
A set A is se/f-p-prinrable if and only if there is a function I from {O}· 
to E· computable in polynomial time with oracle A, such that 1(0") is a 
word coding the set A n E~n. The class SETS-J{A[log,poly] is defined 
like SETS-J{[log,poly] but allowing the universal machine to access the 
oracle A. A is self-p-rankable if and only if rAE P F(A). Show that the 
following are equivalent: 
(a) 
S E SETS-J{A[1og, poly] n P. 
(b) 
S is self-p-printable. 
(c) 
S is sparse and self-p-rankable. 
(d) 
S is isomorphic to a tally set via a bijection in P F(S). 
9. 
From the previous exercise, it follows that all the sets in the class SETS-
J{[log,poly] are self-p-printable. Show that the converse is not true. 
10. 
Show that all the self-p-printable sets have self-producible circuits, but 
there are sets with self-producible circuits which are not self-p-printable. 
11. 
Obtain an oracle-restricted positive relativization of the P 1 NP prob-
lem by showing that P = NP if and only if for every set A in SETS-

Bibliographical Remarks 
233 
J(A[iog,poiy], peA) = NP(A). Notice that this means that if a set A sep-
arates peA) from NP(A), and the unrelativized separation does not follow 
immediately, then A must have difficult strings, and that the strings stay 
difficult even if the oracle A can be used to generate them. Observe that 
this is precisely the essential ingredient of the proof of Theorem 10.1. 
12. 
In Exercise 10, the counterexample found by the reader was probably not 
sparse. If it was, congratulations -
you will be famous! Indeed, show that 
if there is a sparse set which is not self-p-printable, but has self-producible 
circuits, then P is different from NP. Hint: Show that if P = NP then 
every sparse set in ELI is self-p-printable. 
13. 
Recall from Chapter 8 the definition of NPb(A). 
Show that if A is 
self-p-printable then NPb(A) = NP(A), by a direct proof such as that 
of Lemma 9.4. Also, use Lemma 9.4 itself and the characterization of 
sets with self-producible circuits (Theorem 5.10 in Volume I) to extend 
to these sets the property that NPb(A) = NP(A). 
14. Discuss the possibility of proving a characterization of P/poly similar to 
Theorem 10.5. 
10.7 Bibliographical Remarks 
The first ideas about measuring the amount of information in a string of bits were 
expressed in a description given by M. Minsky of the work of R. Solomonoff, 
in Minsky (1962). 
A few years later, the Kolmogorov complexity of finite strings, as we have 
defined it, was introduced independently in Kolmogorov (1965), (1968), and 
Chaitin (1966), (1969), (1974), while the latter was a student at Columbia Uni-
versity. Our proofs of the existence of infinitely many primes and of the G6del 
incompleteness theorem are from Chaitin (1977) and Chaitin (1974), respec-
tively. Another formalization of the Berry paradox to prove the G6del incom-
pleteness theorem is given in Boolos (1989), where descriptions are built from 
arithmetic formulas. 
Sipser (1983), Hartmanis (1983), and Ko (1986) modified the original idea 
to include not only the length of the program but also the running time of 
the program. In the text of this chapter we have followed the approach taken 
in Hartmanis (1983). Theorem 10.1, Exercise 1, and many explanations and 
remarks are from this reference. 
Theorem 10.2 was independently obtained by Levin (1973) and Adleman 
(1979). More discussion on Levin's proof can be found in Trakhtenbrot (1984), 
and more discussion of Adleman's version appears in Hemachandra and Wech-
sung (1989). In this last article, a nice characterization of the question p N P = 
pNP[1og) in terms of Kolmogorov complexity is given. 
Here pNP[log) is the 
class of sets Turing reducible to NP sets by polynomial time oracle machines 
that only query logarithmically many times. 

234 
Resource-Bounded Kolmogorov Complexity 
Ranking functions were introduced by Allender (1985) and by Goldberg and 
Sipser (1985). Exercise 5 introduces the subject of compression; the interested 
reader should read these references to find more interesting results regarding 
these concepts. Lemma 10.1 is from these references. The concept of p-
printability and Exercise 7 are from Hartmanis and Yesha (1984). Theorems 
10.3 and 10.4 were obtained by Allender and Rubinstein (1986). Related mate-
rial appears in Balcazar and Book (1986) and in Hartmanis and Hemachandra 
(1986), such as the results presented at the end of Section 10.4 and several parts 
of Exercises 8 to 13. The relationship between the complexity of a set and the 
Kolmogorov complexity of its characteristic function is from Balcazar, Diaz, 
and Gabarr6 (1987), and Exercise 2 is from Sipser (1983). 
Another important use of Kolmogorov complexity has been for obtaining 
sharp lower bounds. The technique was pioneered by Paul (1979). From then 
on, a variety of new results and improvements of old results has been obtained; 
see for example Paul, Seiferas, and Simon (1980), and Li (1985). This line 
of work beyond of the scope of this book. A good survey of applications of 
Kolmogorov complexity is Li and Vitanyi (1988). 

11 
Probability Classes and Proof-Systems 
11.1 Introduction 
In this last chapter, we bring together many of the concepts previously treated 
in this volume and in Volume I, to introduce proof-systems and frame them in 
the more general theory of probability classes. In the second section, taking as 
starting point the concept of NP, we introduce the interactive proof-systems. In 
the following two sections, we define another kind of proof-system, the Arthur-
Merlin games, and prove some structural properties of these proof-systems, by 
considering them as probabilistic classes. The reader will recognize some of the 
arguments presented in Section 11.4 as generalizations of results from Chapters 6 
and 8 in Volume I. Finally, in the last section, we prove that for a bounded num-
ber of interactions, interactive proof-systems classes and Arthur-Merlin classes 
coincide. 
The reader is advised that this chapter contains only the basic theory of a 
subject which is still being actively developed. For those who wish to delve 
deeper into the subject, the bibliographical references at the end of the chapter 
contain references to most of the recent results on interactive proof systems 
and related topics. It also should be remarked that a minimum knowledge of 
probability theory is assumed. It will be advisible to keep at hand Volume I of 
Feller's book, or some other similar book on probability theory. 
We should indicate that, unless otherwise stated, all through this chapter, 
sets, words and symbols are always defined on alphabet {O, I}. 
11.2 Interactive Proof-Systems: Basic Definitions and Examples 
Recall that, when in Chapter 3 of Volume I we defined the class NP, we remarked 
that NP was the class of problems for which there is a known way to check in 
polynomial time whether a potential solution is an actual solution. Following this 
remark, let us introduce the following "verifying" machine, which will define 
the class NP. 
Definition 11.1 An NP proof-system consists of two deterministic Turing ma-
chines Alp and _'ft.1v , respectively called the prover, which has unlimited com-
puting power, and the verifier, which is polynomially time bounded. The two 

236 
Probability Classes and Proof-Systems 
machines share a common read-only input tape; they also share a communica-
tion tape~ where Mp can only write a chain and Afv can only read the chain 
written by M p, and both !lIp and lIIv have independent read-write working tapes. 
An NP proof-system defines the class NP as follows: for any language LEN P, 
given as input a chain x in L, the prover computes a chain y, with the length of 
y polynornially bounded in the length of x, and writes it on the communication 
tape. V reads y and checks in polynomial time that IA(Y, x) = 1, where IA is 
as a polynomial-time computable function which can be thought of a proof of 
membership of x in L. So in this way the class NP could be re-defined as the 
class of sets which have short proofs of membership, where here short means 
polynomial-time. Let us see some examples to clarify the idea. 
Example 11.1 Let us consider the problem of the clique. Given a graph 
G(V, E), with IVI = n, and a positive integer k, decide whether G contains 
a complete subgraph of k or more vertices. (See Definition 3.8 of Volume 1.) 
So L will be the set of chains which codify a graph G and a constant k, such 
that G contains a clique with k vertices. Our NP proof-system will work as 
follows: Mp will read the input x which will consist of the graph G properly 
codified, and the constant k. As usual, we may consider all our codifications to 
be over alphabet {O, I}. Let 711, denote the length of x. Then Mp will compute 
a clique of size k of G, and write it codified as chain y. Then Mv will compute 
the function lA, defined by 
f (X 
) = { 1 if y is ~ subgraph of x, it is complete and Iyl = k 
A 
, Y 
0 otherWise 
Example 11.2 Let us consider now the problem of the graph isomorphism. 
Given two graphs G1(Vi,E) and G2(V2,E2), with VI = V2 = {1,2, ... ,n}, say 
that G1 is isomorphic to G2 (G1 ';:! G2), if there exists a permutation 7r from 
{1,2, ... ,n} to {1,2, ... ,n}, such that for any vertices i andj, (i,j) E El if 
and only if (7r(i), 7r(j» E E". The graph isomorphism problem (ISO), will be 
defined by 
ISO = {(Gt, G2)IG1 9! G2} 
Again an NP proof-system for ISO will consist of the prover reading (Gt, G2), 
properly codified, computing the correct permutation 7r, and sending it to the 
verifier. Then V checks in polynomial time that !Iso (Gt, G2), 7r) = 1, where 
/Iso is computed by checking that for every i and j, (i, j) E EI if and only if 
(7r(i),7r(j» E E2. 
The above proof-system has an inconvenience; it does not allow us any inter-
action between the prover and the verifier. The NP-proof system is like a dumb 
and inexpressive teacher writing a proof for a class of mute students. In normal 

Interactive Proof-Systems: Basic Definitions and Examples 
237 
conditions, we would like to capture a more general way of communicating; for 
instance, we would like to build in the possibility of students asking questions 
at key steps of the proof, and receiving an answer. This makes teaching easier! 
To capture this wider concept of communicating a proof, we define the more 
general notion of an interactive proof-system. We do this in stages, defining first 
the concept of an interactive Turing machine, then the concept of an interactive 
protocol and finally the notion of an interactive proof-system. 
Definition 11.2 An interactive Turing machine (ITM) is a Turing machine with 
a read-only input tape, a read-only random tape, a read-write work-tape, a 
read-only communication tape, a write-only communication tape, and a write-
only output tape. The random tape contains an infinite sequence of bits from 
{a, I}, which can be thought of as the outcome of unbiased coin tosses. When 
we say that an interactive machine "flips the coin", we mean it reads the next 
bit from the random tape. 
The contents of the write-only communication tape can be thought of as mes-
sages sent by the machine; while the contents of the read-only communication 
tape can be thought of as messages received by the machine. Notice that the 
random bits read by the machine are secret in the sense that no other machine 
can decide which is the random bit read by the machine. 
An interactive protocol is an ordered pair CMp, ~Mv) of iteractive Turing 
machines sharing the same input tape, with the write-only communication tape of 
lIlv as the lIlp read-only communication tape and vice versa. The machines take 
turns in being active, with Mv starting the computation. When each machine is 
active, it can perform internal computation, read and write on its tapes and send a 
message to the other machine by writing on the appropriate communication tape. 
The ith message of Mv is the entire string that Mv writes on the communication 
tape during its ith tum. At this point, Mv is deactivated and lIlp becomes active, 
unless the protocol has terminated. The ith message of lIlp is similarly defined. 
Either machine can terminate the protocol by not sending any message in its 
active stage. Machine Mv accepts (rejects) the input by entering an accepting 
(rejecting) state and ending the protocol. Mp is a computationally unbounded 
Turing machine. The computation time of Mv is defined as the sum of lIlv's 
computation time during its active stages. We require that the computation time 
of Mv be bounded by a polynomial in the length of the input string. A move is 
a message from Alv to !llp or vice versa. The i th round of the protocol is the ith 
move from lIlv to lIlp together with the answer from lIlp to lIlv. (Notice that the 
protocols always begin with Mv sending a message and end with Mv accepting 
or rejecting.) The number of rounds of an interactive protocol (!llp, Mv) on input 
x is defined as the total number of rounds exchanged. The text or history of the 
computation is the sequence hn = {VhP\, ... , vn,Pn}, where Vi (respectively 
Pi) denotes the message sent from Mv to Mp (from Mp to lIlv) in the ith round. 
Pn = >. if lIlp halts the computation on the nth round. We may consider also 

238 
Probability Classes and Proof-Systems 
the history at the ith round hi = {VI, PI, ... , Vi, Pi}. The size of an interactive 
protocol (1111" Mv) is defined as the total length of the text of the computation. 
We shall mainly use the concept of move for most of the material in the chapter. 
It is important for the reader to notice that a round is equivalent to two moves. 
As the two interactive Turing machines are random, we can define a proba-
bility space on the set of texts of all possible computations of (1111" Mv) on input 
x, in such a way that the probability of each computation of (Mp , Mv) on input 
x is taken over the coin tosses of machines Mp and Mv' This leads us to make 
the following definitions. 
Definition 11.3 
Given the set L E {O, I}·, we say that L has an interactive 
proof-system if there exists an interactive poly-time Turing machine 1I1v such 
that 
1. 
There exists an interactive Turing machine 1111' such that the pair (1111" 1I1v) 
is an interactive protocol and for every x in L with Ix I sufficiently large, 
the probability that Mv accepts x is greater than 2/3. 
2. 
For every possible interactive Turing machine 1111' such that the pair 
(1111" 1I1v) forms an interactive protocol, andfor every x not in L, with Ixl 
sufficiently large, the probability that Mv accepts x is less than 1/3. 
where the probabilities are taken over the coin tosses of 1111' and 1I1v. 
We say that (Afp, 1I1v) is an interactive proof-system for L. 
As in Definition 11.1, Mp will be called the prover, and Afv will be called 
the verifier. 
In the previous definition, Condition I says that for every input in the lan-
guage L, there exists an easy way to prove this fact to 1I1v which succeeds with 
high probability. Condition 2 says that for every input not in the language, the 
probability of success of any strategy to fool Mv into believing that x belongs 
to L is small. Another interpretation of this definition is to say that Condition I 
means that it is possible to prove a true theorem so that the proofs are "easily" 
(in polynomial time) verified; and Condition 2 means that with high probability, 
no one can be convinced of the proof of a false theorem. 
Notice that it does not suffice that the verifier cannot be fooled by the 
predetermined prover, as such a condition would presupposed that the prover is 
a trusted oracle. 
Definition 11.4 
Given a nondecreasing function q from IN to IN, we say that 
(Mp,1Ilv ) is a q-round interactive proof-system for the set L E {O, I}*, iffor 
every x in L, the length of the history of the computation of (1111" 1Itv) on x is 
bounded by q(lx I). 
Notice that the above definition is equivalent to saying that the number of moves 
in the computation is bounded by q(lxl}. 

Interactive Proof-Systems: Basic Definitions and Examples 
239 
We will consider this function to be either bounded by a polynomial of n, 
and write it q(n), or to be bounded by a constant. Then we can define the 
following complexity classes; 
Definition 11.5 Define IP[q(n)] to be the class of languages for which there 
exists a q(n)-move interactive proof-system. For any positive constant c, define 
IP[c] to be the class of languages for which there exists a c-move interactive 
proof-system. In generalIP[poly] or IP is defined as IP = Uk~oIP[nk]. 
For instance, IP[2] is the class of languages for which there exists an inter-
active proof-system of the following type: on input x, M" flips the coin and 
sends a message to Mp (one move), then after doing whatever it does Alp sends 
a message back to 111" (second move). Finally M" accepts or rejects the input. 
Example 11.3 presents a detailed protocol to show that a language is in IP[2]. 
Notice that the class NP which was discussed at the beginning of the section is 
a particular case of a trivial interactive proof-system where the verifier tosses 
no coins. 
From the definition, we can state, 
Proposition 11.1 
NP ~ IP[2] ~ IP[q(n)] ~ IP[q(n) + 1] 
where q(n) is a nonconstant polynomial. 
Let us see an example of interactive proof system. Another example of a 
problem in IP[2] is presented in Exercise 1 at the end of the chapter. 
Example 11.3 We can define graph non-isomorphism by: 
that is, NONISO is the set of pairs of graphs with vertices {I, 2, ... , n}, which 
are not isomorphic. 
Given as common input two graphs Go(V, Eo) and Gl(V, Ed, with IVI = n, 
let us consider the following protocol: 
1. 
AI" reads the input and chooses at random n integers Ci E {O, I}. This is 
done by tossing the coin n times. For each Ci, 1 ~ i ~ n, 111" computes a 
graph Hi(V, Fi) which is isomorphic to Gc;(V, Ec;)' At the end, M" sends 
to Mp the n produced graphs Hl(V,Fl)'''' , Hn{V, Fn). 
2. 
The prover reads the input and tests if Go is isomorphic to Gt. It can do 
that, as it has unlimited power. In case Go is not, the prover reads the 
message sent by the verifier. For each Hi, Mp tests whether Hi 9! Go or 
Hi 9! G l . In the first case it makes ai = 0, in the second it makes ai = 1. 
If Go 9! GJ, then for each i, Mp chooses ai E {O, I} with probability 1/2. 
In any case, when Alp has produced {at, ... an}, it sends them to AI". 

240 
Probability Classes and Proof-Systems 
3. The verifier reads the message {O'I ••• O'n} from the prover. 111" compares 
each O'i with the corresponding Ci. If for some i it gets that O'i is different 
from Ci, which means that either the graphs are isomorphic, or the prover 
has made a mistake, and the verifier rejects the input. Otherwise, the 
verifier accepts the input (M" verifies that Go 1 Gt>. 
Let us prove that this protocol really works. In the case that Go 1 G), 1I1p 
will find that each Hi is isomorphic only to one of Go or G" but not to both. 
Therefore 1I1p will be able to return the correct value of O'i, and in step 3, M" 
will find for each i that O'i = Ci. 
On the other hand, if Go '?t Gl> then any prover will be able to decide if M" 
has constructed each Hi from Go or from G" and may return an arbitrary O'i 
with probability 1/2 of error. Therefore, the probability that M" does not find 
at least one i for which O'i =I Ci is less than or equal to 1 /2n. 
So from the definition we have that NON/SO belongs to the class /P[2]. 
It is interesting to note that NON/SO is not known to be in NP. Also notice 
that in the last example, the secrecy of the tosses is essential to the protocol. 
In the next section, we present another proof-system, where the coin tosses are 
made public. 
11.3 Arthur Against Merlin Games 
The proof-systems presented in this section consist of a game where the wizard 
Merlin, who possesses supernatural intellectual abilities, tries to convince the 
intelligent human King Arthur that a string x belongs to a given language L. 
In the same way as in the previous section, we define an Arthur-Merlin 
protocol <-~1, A) on common input x as an interactive protocol in which A's 
messages consist merely of tossing a coin at most a polynomial number of times 
and sending the outcomes to Merlin. After the interchange of moves between 
A and 111 is terminated, A evaluates a polynOlllial-time function defined over 
the contents of its tapes to decide whether to accept or reject the input x. If 
A accepts the input, then Merlin wins, otherwise Arthur wins. Similarly to 
Definition 11.2 we have 
Definition 11.6 A set L has an Arthur-Merlin proof-system if there exists an 
interactive Turing machine A called Arthur such that, 
1. There exists an interactive Turing machine 111, called Merlin, such that 
the pair (AI, A) is an Arthur-Merlin protocol; and for every x in L the 
probability that A accepts x is greater that 2/3. 
2. 
For every possible interactive Turing machine M such that the pair (111, A) 
form an interactive protocol, and for every x not in L, the probability that 
A accepts x is less than 1/3. 

Probabilistic Complexity Classes and Proof-Systems 
241 
As the reader will notice, the difference between the Arthur-Merlin proof-
system and the interactive proof-system as defined in Section 11.2 is the re-
stricted way that Arthur is allowed to use his coin tosses during the protocol. 
Thus the Arthur-Merlin proof-system is a special case among interactive proof-
systems. On the other hand, from the example of graph nonisomorphism, it 
looks as if the ability of the verifier to maintain the secrecy of its coin tosses is 
the crucial ingredient which makes the proof work. Surprisingly, it turns out that 
both classes coincide. Arthur-Merlin proof systems also give rise to a hierarchy 
of complexity classes: 
Definition 11.7 
Given a nondecreasing function q from N to lN, we say that 
(111, A) is a q-move Arthur-Merlin proof-system if for every x in L, the number 
of rounds in the computation of (AI, A) to recognize x is bounded by q. 
Then we can define the following complexity classes: 
Definition 11.8 Define AM [q(n)] to be the class of languages for which there 
exists a q(n)-move Arthur-Merlin proof-system. 
In the case where we have a positive constant c, we define AM[c] to be 
the class of languages for which there exists a c-move Arthur-Merlin proof-
system. For instance AM[2] will consist in Arthur tossing the coin and sending 
the result to Merlin, and Merlin answering to Arthur (in total two moves). After 
the protocol, Arthur decides if he accepts the input or not Therefore, AM[2] 
can be represented as AM. In the same way, AM[3] will consist of a move 
from A to Af, another move from Al back to A, and again a move from A 
to 111. After this protocol is finished, A decides if he accepts the input or 
not. AM[3] can be represented as AMA. Notice that the last move from A to 
111 seems a bit superfluous; its meaning will become clear in the next section, 
when we interpret the Arthur-Merlin protocols in terms of probabilistic and 
nondeterministic operators. Does the reader recognize AM[I]? 
11.4 Probabilistic Complexity Classes and Proof-Systems 
Recall that in Chapter 6 of Volume I, we introduced a model of a probabilistic 
machine. We can strengthen this model by introducing also nondeterminism. 
Define a probabilistic, nondeterministic, Turing machine N as we did the 
probabilistic machine in Chapter 6 of Volume I, except that N has two kinds of 
nondeterministic states; random states and guess states. Given a configuration 
a of such a machine, we assign it a probability p(a) of accepting as follows: if 
a is an accepting configuration, then p(a) = 1; if a is a rejecting configuration, 
then p(a) = 0; if a is a deterministic configuration, then p(a) = p(a') where a' 
is the successor of a; if a is a random configuration, then p(a) is the average of 
p(a') over all the successor configurations a'; and if a is a guess configuration, 
then p(a) is the maximum of p(a') over all successor configurations a' of a. 

242 
Probability Classes and Proof-Systems 
We say that ProbeN accepts input xl = p(o:o) where 0:0 is the initial con-
figuration of N on x. In the same way that in Definition 6.4 of Volume I we 
introduced the class BPP, we can use the machines just described to define the 
class BPNP as follows; a language L ~ {0,1}· belongs to the class BPNP if 
Vx E {O, 1}·: 
if x E L then ProbeN accepts xl > 2/3 
if x rt L then probeN accepts xl < 1/3 
Moreover, from the definition of BPP given in Chapter 6 of Volume I, we 
get that BPP ~ BPNP. 
Let us re-study probabilistic classes in a more general framework. Recall 
from Chapter 6 of Volume I, that BPP was defined as the class of sets L such 
that for some set B E P, some 8 > 0, some polynomial p and for all inputs x 
of length 71, we have 
Prob[(x,y) E B iff x E Ll > ~+8 
where y E {a, 1 }p(n) is randomly chosen with probability ~. This definition 
can be generalized to other classes; such as the class NP above, or any class C 
of the polynomial-time hierarchy, 
Definition 11.9 Given a class C of sets, let the bounded-error probabilistic- C 
class (BP . C) be defined as the class of all sets L such that for some set B in 
C,for some 8 > O,for some polynomial p and for all inputs x of length 71, we 
have 
Prob[(x,y) E B iff x E Ll > ~ +8 
where y E {a, 1 }p(n) is randomly chosen with probability ~. 
The reason we write BP . C is because BP can be seen as an operator, 
the BP- operator, which transforms the complexity class C into a probabilistic 
complexity class BPC. 
Although we have defined the classes BPNP and BPP via probabilistic Turing 
machines, it should be clear to the reader that Definition 11.9 could also be used 
to define those classes, and that BP . P and BP P are the same class, as well as 
BP . NP and BPNP. From now on, for simplicity of notation we shall drop the 
middle dot and write BPC, but the reader should remember to see it as the class 
resulting from applying the BP-operator to the class C. The reader may recall 
that in Chapter 8 of Volume I, for any class C we implicitly defined the NP-
operator, which carried any class C to NP . C, denoted 3C in Volume 1. For 
instance this was the way we tharacterized the classes {Ed of the polynomial-
time hierarchy. (See Section 8.3 in Volume 1.) 
Therefore, in the Arthur-Merlin protocols, Arthur corresponds to the BP-
operator, and Merlin to the NP-operator. In particular AM[2l = BPNP, and as 

Probabilistic Complexity Classes and Proof-Systems 
243 
the reader probably guessed at the end of the previous section AM [1] = BP P! 
Moreover, in the framework of the BP and NP operators, it makes sense to 
define a Merlin-Arthur hierarchy, MA[c], although these MA classes are not 
proof-systems. It should be easy for the reader to convince him/herself that MA 
coincides with NPBPP. Why? 
In the rest of the section, we will prove general properties of the classes 
arising from Definition 11.9 , and place a special emphasis on the consequences 
for BPNP, i.e. AM. Let us start by proving that it is possible to amplify the 
probability in Definition 11.9 from ~ + 6 to something as close as we wish to 
1. To do this, we shall recall from Exercise 15 in Chapter 4 of Volume I, the 
concept of positive Turing reducibility. An oracle machine },f is positive iff 
L ~ B implies that L(M, L) ~ L(U, B). Then L is positive Turing reducible 
to B, L ~po. B, iff L = L(M, B) where !If is a positive oracle machine. If M 
is deterministic then we shall write L ~:o. B. In the exercise referred to above, 
it was proved than the class NP is closed under ~:o. reducibility. It is left to 
the reader to prove that classes P, co-NP, PSPACE, Ef and lIf for i ~ 1, are 
closed under ~:o. reducibility. Let us first prove a technical result, which was 
implicitly done in Theorem 6.4 of Volume I. 
Lemma 11.1 Let E be an event that occurs with probability greater than ~+6, 
for 0 < 8 < 1/2. Then for any t which is odd, the probabiLity than E occurs 
t/2 times, within t independent triaLs, is greater than 
1 ( 
2) t/2 
1 -"2' 1 - 46 
Proof Let qi denote the probability that E occurs exactly i times in t indepen-
dent trials. By the basic result on binomial distributions (see e.g. Feller (1968), 
page 148), 
qi 
G) . (1/2 + 8i . 0/2 - 8)t-i 
< 
(~). (1/2 + 8i . (1/2 _ 8)t-i . ( 1/2 + 8) t/2-i 
z 
1/2 - 8 
which for every i ~ t/2 gives us 
Let p denote the probability of E occurring more than t/2 times. Then 
applying the above result, 
t/2 
P 
= 1- Lqi 
i-O 

244 
Probability Classes and Proof-Systems 
> 1 - E 
(~) . 
(1/4 - fl)t/2 
i-O 
t 
1 - 2t- 1 • (1/4 _ 62)t/2 
1 -
~ . (1 - 462i/2 
2 
o 
This last bound will be used in the proof of the amplification lemma , which 
which will play an important role in the remainder of the chapter. 
Lemma 11.2 Let C be any class of sets which is closed under ::;:03 reducibility. 
Then for any set L E BPC and any polynomial q. there is a set B E C and a 
polynomial p such that for all x with Ix I = n, 
Prob[(x, y) E B iff x E L] > 1 -
2q~n) 
where the yare randomly chosen from {a, l}p(n) under a uniform distribution. 
Proof. 
Let L be in the class BPC, then by definition there exists a set Lo in 
the class C, a polynomial p and a small positive constant 0 such that 
Prob[(x, y) E Lo iff x E L] > 1/2 + 0 
Let us see how to reduce this error probability, making it as close to ° as 
we wish, by the technique of taking a majority vote. Define B as the set of 
all (x, YlY2 ••• Yn'" Yt(n» such that the majority of all (x, Yi) are in Lo, for 
1 ::; i ::; t(n). 
If x E L, then by the previous lemma the probability than the majority of 
(x, Yi) are in Lo is at least 1 - !(1 - 462)t/2. Therefore to prove the statement, 
t has to be chosen such that 
1 -
~(1 - 402)t/2 ~ 1 - 2-q(n) 
Changing signs and inverting the terms, 
2( 
1 
)t/2 > 2q(n) 
(1 - 402 ) 
-
Taking logarithms and solving for t we get 
2q(n) 
t> 
l' 
-
log( 1-4.5z) 
Therefore, making t(n) = c· q(n) where c = log(l/L46l ) we get that 
Prob[(x, YIY2' .. Yt(n» E B if x E L] ~ 1 - 2-q(n) 

Probabilistic Complexity Classes and Proof-Systems 
245 
In a similar way, we can prove that for x {j. L, the probability that the 
majority of (x, Yj) will be in Lo is at most 2- q(n), which proves 
Prob[(x, Y!Y2'" Yt(n» E B iff x E L] > 1 - 2- q(n). 
To end the proof, it remains to show that B is in C; but this is a consequence 
of the facts that B ::;:06 Lo and Lo E C together with the hypothesis of C being 
closed under ::;:06' 
0 
Re-stating the amplification lemma for C = NP we get a particular case of 
the statement above: 
Corollary 11.1 For every set L E AM[2] and every polynomial q, there is a 
set B E NP and a polynomial p such that for all x with Ixl = n, if x E L the 
fraction of strings Y E {O, 1 }p(n) for which (x, y) E B is at least 1 - 2- q(n), and 
at most 2- q(n) for x {j. L. 
In Exercise 2 at the end of the chapter, the reader will find a stronger am-
plification lemma for Arthur-Merlin classes. 
Let us state a lemma which is a generalization of a result already proved in 
Chapter 8 of Volume I (Lemma 8.3), which was used to prove the containment 
of BP P in E2 n lIz. This lemma is known as the quantifier simulation lemma. It 
will be very useful to change expressions defined in terms of probabilities, such 
as the classes BPC, into equivalent expressions defined in terms of universal 
and existential bounded quantifiers. Throughout the rest of the chapter, for 
u, v E {O, l}n, U ffi v denotes the bitwise addition of U and v, i.e., the ith bit of 
U ffi v is the exclusive OR of the ith bit of vectors U and v. 
Lemma 11.3 Let E ~ {O, l}m such that lEI> (1- 2-k)2m , where 2k > 1n. 
Then the following statements are true: 
1. 
Prob[3v, Ivl = m, Ai .. 1 (Uj ffi v) E E] = 1 
2. 
ProbrYv, Ivl = m, Vi,:I(Uj ffi v) E E] > 1 - 2- m(k-l) 
where (UI, ... , um) E {O, l}m X ... x {O, l}m (m times) is chosen uniformly at 
random. 
Proof 
1. 
The proof of statement 1 is done by contradiction. Let (UI, •.. , um) be 
such that for all v in {O,l}m there exists at least one i ::; m such that 
Uj ffi v is not in E. For this to be true, at least 2m 1m elements w of 
{O, l}m must be out of E. Therefore, 
which contradicts the statement of the lemma. 

246 
Probability Classes and Proof-Systems 
2. 
Let us compute Prob['v'v V~I(Ui EB v) E E]. Notice that events v and Ui 
are independent 
m 
m 
Prob['v'v V (u; EB v) E E] = 1 - Prob[3v V (u; EB v) ¢ E] 
m 
> 
1 - EProb[V(ui EEl v) ¢ E] 
tJ 
;-1 
= 1- EIIProb[(u; EEl v) ¢ E]. 
tJ 
; 
Given Ui and v the probability that Ui EEl v is not in E is at most the ratio of 
chains in {a, 1}m which are not in E. Therefore, Prob[uiEBv ¢ E] ~ 1/2k, 
and we get 
m 
Prob['v'v V(Ui EB v) E E] > 1 - E IT 2-k = 1 - E 2-km = 1 - 2-m(k-l). 
i-I 
11 
i 
tI 
which proves statement 2. 
o 
The following weaker version of the above corollary will be sometimes easier 
to apply. 
Corollary 11.2 Let E ~ {a, l}m be such that lEI> (1-2-k)2m with 2k > m. 
Then the following is true: 
1. For every U = (UI, ... ,um) E {a, l}m x ... x {a, l}m there exists a v such 
that for every i ~ m, Ui EB vEE. 
2. 
There exists at least one U = (Ul, ... , um) E {a, l}m x ... x {a, l}m such 
that for every v E {O, 1} m, there exists i ~ m such that Ui EB vEE. 
The reader should underestand fully the meaning of the previous result, as 
it will be a key point in the forthcoming proofs. Lemma 11.3 and its corol-
lary allow us to simulate a probability assertion by means of two alternating 
quantifiers; either V 3 or 3 V. Let us see a first application of this lemma. 
Theorem 11.1 For every k ~ 1 we have 
1. BP Ek ~ Ilk+l 
2. BP Ilk ~ Ek+l 
Proof. 
The proof of (1) will use the amplification lemma to get the correct 
probability, then we shall simulate the probability by quantifiers using the quan-
tifier simulation lemma. Let L be in BP E k • Then by the amplification lemma, 
there exists a set B in Ek and polynomials pen) and q(n) such that for every 
x E {a, 1}* with length Ixl = 71, 
Prob[(x, y) E B iff x E L] > 1 - 2-q(n) 

Probabilistic Complexity Classes and Proof-Systems 
247 
where y is randomly chosen from {a, 1 }I,(n), and where pen) and q(n) are related 
as in the proof of Lemma 11.2, and therefore pen) > q(n). Then, if x E L, we 
have that 
Prob[(x, y) E B] > 1 - 2- q(n) 
but notice that for fixed x, the number of possible of pairs (x, y) in B is 2Q(n), 
so that for fixed x, the number of chains y such that (x, y) E B is greater 
than 2P(n)(1 - 2-q(n». Applying Corollary 11.2 (1) we can express the above 
probability by 
pen) 
Vu 3v /\ (x, Uj ED v) E B 
jsl 
where U = (UI, .•. , Up(n» with each Uj E {a, l}p(n) and v E {a, l}p(n). 
If x rf. L, then Prob[(x, y) rf. B] > 1 - 2- p(n), and using part 2 of Corol-
lary 11.2 we can express the above probability by 
pen) 
3uVv V (x, Uj ED v) rf. B 
jsl 
where U = (UI, ... , Up(n», each Uj E {O,l}p(n) and v E {O,I},,(n). As B is 
in Eb and the quantifiers Vf;~) and Af~n) have a polynomial range, both of the 
expressions Vf;~)(x, UjEDv) E B and Af;~\x, UjEDv) rf. B are also in Eko therefore 
by the characterization of the polynomial time hierarchy, given in Theorem 8.3 
of Volume I, L E Ilk+I' 
The proof of (2) in the statement of the theorem is performed in exactly the 
same way and is left to the reader. 
0 
Considering the specific case of the proof-system AM, we can state: 
Corollary 11.3 
AM[2] ~ Il2, and 
co-AM ~ E2 
Notice that the previous theorem also gives us a proof thatBPP ~ E2, which was 
Theorem 8.6 in Volume 1. The reader may check the similarity of the arguments 
in Lemma 11.3 and those in Lemma 8.3 of Volume 1. 
Other results about probabilistic classes which will allow us to derive im-
portant consequences for the class AM[2] are the following: 
Theorem 11.2 For all k ~ 1 we have 
1. if Ilk ~ BP Ek then Ek+1 = Ilk+1 
2. if Ek ~ BP Ilk then Ek+1 = Ilk+1 
Proof Let L be a set in Ek+t. then by the definition of the polynomial time 
hierarchy, there exists a set B E Ilk such that 
L = {xI3P(n)y such that (x,y) E B} 

248 
Probability Classes and Proof-Systems 
But by hypothesis Ilk ~ BP Eb therefore B E BP Eb so by Definition 11.9, 
there exists a set D E Ek and small 6 > ° such that for every (x, y) 
Prob[(x, y, z) E D iff x E B] > ~ + 6 
where z E {a, 1 p{n) is uniformly taken. But as we have done previously, we 
can use the amplification lemma to make this last probability approach 1. This 
allows us to use the quantifier simulation lemma and conclude that the set L can 
be characterized by 
pen) 
L = {xlVu 3v 3y /\ (x, y, Ui Gl v) ED}. 
i-I 
Again notice that A runs over a polynomial range; therefore as D is in Ek> the 
whole expression M~~)(x, y, Ui Gl v) E D is also in Eb and by the basic results 
on the polynomial time hierarchy in Chapter 6 of Volume I, we get that L is in 
Ilk+l. In the same way, we can prove part 2 of the statement 
0 
Rewriting, as we have done previously, this theorem in terms of AM we have 
Corollary 11.4 If either co-NP ~ AM[2] or NP ~ co-AM[2], then the poly-
nomial time hierarchy collapses to Il2• 
Further results about the relationship between probabilistic classes and the 
polynomial time hierarchy are given in Exercises 5 and 6 at the end of the 
chapter. 
After having seen the relation of the probabilistic classes to the polynomial 
time hierarchy, in the remainder of the section we shall prove the fact that for 
every constant c ~ 2, we have AM[2] = AM[c]. We start with two technical 
lemmas. The first of them deals with one-side error, and it arises from the 
non-symmetry in the statement of the amplification lemma. Notice that 1 in 
Lemma 11.3 holds with probability 1 for all U = (UI, ••• , urn). This motivates 
the following result: 
Lemma 11.4 Let C be any class of sets which is closed under ~:os reducibility. 
Then for any set L E BPC and any polynomial q, there is a set B E C and a 
polynomial p such that for all x with Ixl = n, 
if x E L then Prob[(x, y) E B] = 1 
if x rt L then Prob[(x, y) E B] ~ 2-q{n) 
where the yare randomly chosen from {a, l}p{n) under a uniform distribution. 

Probabilistic Complexity Classes and Proof-Systems 
249 
Proof. Given the set L in BPC and a polynomial q, we apply the amplification 
lemma to obtain a set B' E C and a polynomial p ~ q, such that for all words 
x of length n we have 
Prob[(x, y) E B iff x E L] > 1 - rq(n) 
Using the quantifier simulation lemma with k = q(n), m = pen), once with 
E = {y E EP(n)l(x, y) E B} and again with E = {y E EP(n)l(x, y) ¢ B} we 
obtain 
x E L if Prob[3v, Ivl = pen), I\f~i)(x, Ui 61 v) E B'] = 1, and 
x ¢ L if Prob[3v, Ivl = p(n),Ar~~)(x, Ui 61 v) E B'] < 2-q(n). 
Let us define the set 
pen) 
B = {(x, y) : 3v, Ivl = pen), /\ (x, Ui 61 v) E B'} 
then BEN Ppos(B'), so BEe and the statement of the theorem follows. 
0 
The second technical lemma roughly shows how to interchange probabilistic 
and nondeterministic operators. 
Lemma 11.5 Given a predicate P(x, y, z) in NP, and a polynomial p, consider 
the expression 
I 
Yy Prob[z E EP1zIIP(x,y,z)] > '2+8 
There exists a polynomial q such that the above expression can be substituted 
by 
1 
Prob[C ~ Eq(lzl)'p(lzl> I Yy V P(x, y, Ci)] > '2 + 8 
ciEC 
where C is a set of q(lxl> strings of length p(lxl>. 
Proof. Notice that, given the polynomial-time predicate P(x, y, z), the bounded 
conjunction V ciEC P(z, y I Ci) is also a polynomial-time predicate. 
Assume that we have Yy Prob[z E EPIP(x , y, z)] > ! + 8. For every n, let 
the polynomial q be sufficiently large. We estimate the probability for the set 
of chains C ~ EP(n).q(n) such that ....,Yy V ciEC P(x, y, Ci) 
Prob[C ~ EP·q 
....,Yy, V P(x, y, Ci)] 
c;EC 
Prob[C ~ EP·q13y /\ ....,P(x, y, c;)}] 
c;EC 
= Probe U {C ~ EP·ql /\ ....,P(x, y, Cj)}] 
lI~p(lzl) 
c;EC 
< E Prob[{C ~ ENI /\ ....,P(X,y,Ci)}] 
lI~p(lrl) 
c;EC 
q(lzl> 1 
I 
< E IT'2:::;4 
31~p(lrl) i.l 
where q is choosed such that q(n) = pen) + 3. 

250 
Probability Classes and Proof· Systems 
Therefore for most of the sets of chains C we have that Vv VCiEC P(x, v, e;) 
and the statement of the theorem follows. 
0 
The next theorem gives the core of the result. 
Theorem 11.3 MAM ~ AM. 
Proof. Let L be a language in the class MAM. Since by definition MAM is the 
class NPAM, L can be characterized as 
Vx E E·(x E L iff (3y (x, y) E B» 
where B is a set in AM. But as AM = BPNP, by Lemma 11.4, BEAM if there 
exists a polynomial p, a set DEN P and a small 8 > 0 such that 
if (x, y) E B then Prob[z E Eql(x, v, z) ED] = 1 
if (x, V) f/. B then Prob[z E Eql(x, v, z) f/. D] ~ t + 8 
The above statements can be rewritten as 
if x E L then 3y with Prob[z E Eql(x, y, z) ED] = 1 
if x f/. L then 3y with Prob[z E Eql(x, y, z) f/. D] ~ ! + 8. 
Applying the previous lemma with P(x, y, z) = (x, y, z) f/. D to the second 
statement, and considering the fact that the probability is 1 for the first statement, 
we get 
if x E L then 3y Vz E Eql(x,v,z) ED 
if x f/. L then Prob[C ~ Eq·PIVyVciEC(X,y,Ci) f/. D] ~ 1/2+8 
Let D' = {(x, V, Ct, ••• ,cq) I M.l (x, v, Ci) ED}. From Section 8.3 of Volume I, 
D' E N P, so we can rewrite the above statements as 
if x E L then 3yVz E Eq.pl{x, v, z) E D' 
if x f/. L then Prob[z E Eq·PI(x, v, z) f/. D'] ~ ! + 8. 
Interchanging quantifiers in the first statement 
if x E L then Vz3y I{x, v, z) ED' 
if x f/. L then Prob[z E Eq·PI{x, y, z) f/. D'] ~ ! + 8. 
If we define the set E = {(x, z)13v (x, y, z) ED'} then E E N P and moreover 
we get 
if x E L then Prob[zl{x, z) E E] = 1 
if x f/. L then Prob[zl{x, z) E E] < t - 8 
and we can conclude that L E AM. 
o 

Equivalence of AM and IP 
251 
To obtain the announced result, just notice that from Chapter 8 of Volume I, 
it is known that two or more NP operators collapse into a single one. On the 
other hand, using the amplification lemma, it is easy to prove that the same 
happens with the BP operators. Therefore A··· AlIJ··· M = AM. Using these 
remarks together with the last theorem we can obtain the desired result: 
Corollary 11.5 For constant c ~ 2, AM [2] = AM [c]. 
As a consequence, from now on we can talk about AM to denote any class 
AM[c] for constant c. In the bibliographical references, the reader will find 
further improvements on this result. 
11.5 Equivalence of AM and IP 
We already mentioned that from the definitions it follows that Arthur-Merlin 
proof-systems are just a particular case of interactive proof-systems. Moreover, 
in Example 11.3, it looked as if the ability of the verifier to keep his coin 
tosses secret was the crux of the proof. Therefore the next result looks a bit 
surprising: it states that proof-system classes Uc>2IP[c] and AM coincide. (See 
the bibliographical remarks at the end of the chapter.) Moreover, in the light of 
Corollary 11.5, we have that for any constant c ~ 2, IP[2] = IP[c]. 
We could relax the error probability of Definition 11.2, to describe other 
probabilistic classes which will be used in next theorem, 
Definition 11.10 Given a set L and an interactive proofsystem (Mp , Mv) for 
L, we say that (Mp , Mv) has an error probability € if: 
1. 
For all w in L, Prob[Mv accepts w] > 1 -
€ 
2. 
For all w not in L, Prob[Mv accepts w] < € 
Notice that for the particular interactive proof-system classes, € = ~. 
Theorem 11.4 For constant c > 0, AM = IP[c]. 
Proof Let A be a language in IP[c], and let (!lIp, Mv) be an interactive proof-
system which on inputs of length n makes c moves. Mv flips the coin at most 
fen) times and the error probability is fen). Without loss of generality we can 
assume that the messages of .li.Jv and lIJp are one bit long and that lIJv outputs 
its entire string of coin tosses at the end of the protocol. (See Exercise 7 at the 
end of the Chapter.) 
We may think of an interactive proof protocol as a pair of functions: 
lIJv : E* x E* x E* -+ E* U {accept, reject} 
1I1p : E* -+ E* 

252 
Probability Classes and Proof-Systems 
We write AI,,(w, r, hi) = Vi+1 to mean that M" on input w, with random 
sequence r and current history hi, produces as its next message Vi+l. We say 
Alp(hi , Vi+l) = pi+l to mean that Alp produces its next message Pi+l given current 
history (hi, Vi+I)' 
Let AI" be a given fixed verifier, and consider the optimal prover Mp 
for that verifier. 
For each random sequence r, define prefix (w, r) as the 
set of prefixes s of the histories h = {VI, PI, ••• , Vt(n), Pt(n)} such that Vi = 
AI,,(w, r, VI, PI, •.. ,Pi-I) and Pi = Alp(vl, PI,· •• ,Vi-I). 
Given s E prefix(w, r), define by o(s, w) the total number of random se-
quences r such that s E prefix(w,r) with AI,,(w,r,vI,PI, ... ,Vt(n)'Pt(n» = 
accept, Vi+1 = m,,(w, r, V!,Pl, ... ,Pi) and Pi+l = Alp(vl,pl,"" Vi+I)' 
It follows that o(s,w) = maxa{o(sOa,w)} +maxa{o(sla,w)}, and as we 
agreed that the last message of M" should specify its coin tosses, then for any 
prefix s of length ten) we have that o(s, w) ~ 1. 
If A denotes the empty word, then the probability that M" accepts input w 
is by definition o(A, w )/2J(n). Thus to prove that w E L, it suffices for Merlin 
to prove to Arthur that 
o(A, w) > 2J(n)(2f(n» 
The following Arthur-Merlin protocol achieves this goal. On input w, s, kO 
which initially is s = A, kO > 2J(n)(2f(n», for every i, 1 ~ i ~ ten), 
1. 
Merlin sends to Arthur kb, kl such that kb = o(sO, w) and ki = o(s 1, w) 
2. 
Arthur replies to Merlin a bit Vi E {O, I} with bias kb/(kb + ki> 
3. 
Merlin sets Pi = _Hp(s) for s = SViPi and sends s to Arthur. 
Once the protocol is finished, Arthur accepts if and only if the following condi-
tions hold: 
AI,,(w, r, s) = accepts 
For all i 2 < i < ten) ki + ki = ki- I 
, 
-
'0 
I 
";_1 
kA + kf = kO and j.·b(n) + kl(n) = 1 
From the above protocol it follows that if w E L then the probability that 
Arthur accepts on w, s, k is exactly 1. To agree with Definition 11.6, it remains 
to be shown that if w rt. L then for every possible game of Merlin, there is a 
small probability that Arthur will accept on w, s, k. We leave the proof as an 
exercise. (See Problem 8.) 
0 
This last result together with Theorem 11.2 implies that we can consider IP[c] 
as the class IP[2] for constant c. Also, the result implies that IP[c] = BPNP, 
and so all the results which we have proved for AM apply equally to IP[c]. 
In general, it is more comfortable to use IP for classifying particular prob-
lems, as we have done with NONISO and as we do in Exercise 1 at the end of 
the chapter; but it is handy to use the operator type characterization of AM to 

Exercises 
253 
consider structural properties of these classes. (See also remarks made at the 
end of the bibliographical remarks.) 
To finish, let us present evidence that the graph isomorphism problem is 
probably not NP-complete. 
Theorem 11.5 If ISO is NP-complete, then the polynomial-time hierarchy col-
lapses to Ih. 
Proof Notice that NONISO is in IP and that by the previous theorem NON ISO 
is in AM, which implies that ISO belongs to co-AM. Suppose ISO is also NP-
complete, then all sets in NP will also belong to co-AM. But NP ~ co-AM and 
by Corollary 11.4 this implies that the polynomial-time hierarchy collapses to 
~. 
0 
11.6 Exercises 
1. 
Consider the following sets, 
z~ = {x < nix is relatively prime with n} 
QR = {(x,n)lx E z~ and 3y: l == x mod n} 
Define Q N R (quadratic nonresidue set) as the set of all integers x E Z: 
such that (x, n) rt QR. Prove QN R E IP. 
2. 
Prove the following strong version of the amplification lemma for AM. 
For any set A E AM and any polynomial q there is a set B E NP and a 
polynomial p such that for all n 
Prob[Vx, Ixl ~ n, «x, y) E B iff x E A)] > 1 - 2- q(n) 
where y is randomly chosen under a uniform distribution from {O, l}p(n). 
3. 
Prove that AM ~ NP jpoly. (This means that AM has small generators, in 
the sense defined in Chapter 5 of Volume 1. Such circuits are also called 
nondeterministic circuits.) 
4. 
Prove that the quantifier simulation lemma can be deduced from the fol-
lowing strong hashing lemma. Let S ~ {O, l}m, and let A be a ran-
dom boolean m x n matrix. Let i : {O,l}m -+ {O,l}n be defined by 
i(x) = A.x, and let is be the restriction of ito S. The following is true: 
If S is large, lSI ~ 2n, then is is likely to be onto most of {O, l}n, 
and most elements of {O, l}n will have many preimages. 
If S is small, lSI ~ 2n , then the range of is is a small subset of 
{O, l}n, and most elements of is(S) have only one inverse in S. 
5. 
Prove the following: 
(a) 
E2(BP Ed = Ek+2 

254 
Probability Classes and Proof-Systems 
(b) 
E2(BP Ek n BP Ih) = Ek+l 
6. 
From part (b) of the previous exercise, deduce that: 
(a) BPP ~ L2 
(b) NP n co-AM ~ L2 
(c) If ISO E Hk then the polynomial-time hierarchy collapses to the 
level E max{2,k}' 
7. 
Prove that every ten) round proof system (Mp , lvfv) whose messages are 
men) bits long, with error probability fen), and in which Afv does not 
output its coin tosses at the end of the protocol, can be simulated by a 
proof system (M;, M~) such that the messages between M; and M~ are 
one bit long and such that M~ outputs its string of coin tosses at the 
end of the protocol, achieving the same error probability fen), and using 
ten) . men) + fen) rounds, where fen) is the maximum number of coin 
flips made by M v • 
8. 
Complete the proof of Theorem 11.4 (i.e. prove that if w rt L then for 
every algorithm of Merlin, the probability that Arthur accepts on input 
w, s, k is less than or equal to o(:w). Do that using backward induction 
on the length of s. Then set kO > 2f(n)2J(n) to get the desired result.) 
9. 
Given languages A and B over EO, A is said to be majority reducible to 
B (A ~!:'aj B) if there exists a function f computable in polynomial time 
such that for each x E EO, f(x) = Yl#Y2#" '#Ym (m 2: 1) and x E A if 
and only if the majority of y/s are in B. 
Rework the proof of the amplification lemma (Lemma 11.2) using ~:;'aj 
instead of ~:08' 
10. Prove the following statements: 
(a) 
N P is closed under ~:;'aj' 
(b) 
For any complexity class C, if C is closed under ~:;'aj reduction, then 
BP . BPC = BPC. 
11. 
Given a BPP machine working in time pen), we say that a word Y of 
length pen) is a bad computation if it leads the machine to an erroneous 
result on some input of length n. Prove that the set of bad computations 
of a fixed BPP machine is in MA. 
11.7 Bibliographical Remarks 
Interactive proof-systems were developed by Goldwasser, Micali and Rackoff 
(1985). Their main motivation was the application to cryptographic protocols. 
Exercise 1 is also taken from this work. The interactive proof-system for NON-
ISO is due to Goldreich, Micali and Wigderson (1986). Arthur versus Merlin 
games were introduced by Babai (1985). The motivation of Babai was to create 
complexity classes as small as possible to accommodate a bunch of algebraic 

Bibliographical Remarks 
255 
problems which are not known to be NP. (See the the introduction of the article 
by Babai.) Arthur-Merlin games are restricted cases of the Games Against Na-
ture introduced by Papadimitriou (1983) to describe complexity classes arising 
from polynomially bounded games against an indifferent, randomized adversary. 
A good fonnal description of probabilistic games is given in Babai and Moran 
(1988). This paper also gives a direct AM protocol for NON/SO and also a proof 
that the coset intersection problem belongs to NPn co-AM. 
The AM and MA classes can also be defined in tenns of quantifiers. This 
was implicit in Definition 11.9 and the subsequent remarks. The NP operator 
is a bounded 3 quantifier, as seen in Chapter 8 of Volume I, and the BP-
operator was defined to be the probabilistic quantifier 3+ by Zachos and Heller 
(1986). Zachos (1988) gives a nice survey of characterizations of classes using 
probabilistic quantifiers. 
The development in Section 11.4 of the structural properties of probabilistic 
classes closely follows Schoning (1987). To give the history of some of the 
results in this section, the amplification lemma was developed by Bennett and 
Gill (1981) for the class BPP. They used the well known Chernoff bound. 
Lemma 11.1 is a refinement of that lemma due to Schoning (1985). The fact 
that AM has small generators (Exercise 3) is attributed to Sipser in Goldwasser 
(1988). Quantifier simulation techniques were used by Lautemann (1983). (See 
Section 8.5 in Volume 1.) The equivalent hashing lemma (Exercise 4) was orig-
inally due to Carter and Wegman (1979), and can be used in many situations, 
especially to show that a set is large (see for example Sipser (1983), Goldwasser 
and Sipser (1986), Babai and Moran (1988» and also to show that a set is small, 
Fortnow (1987). We have chosen Lautemann's approach due to the fact that it 
was already introduced in Volume 1. Theorem 11.1 is a generalization of the 
result presented in Chapter 8 of Volume I, that BPP E Ih Its corollary was 
proved in Babai (1985). Theorem 11.2 is a generalization, due to Schoning 
(1987), of a result of Boppana, Hastad and Zachos (1987), which is presented 
here as Corollary 11.4. The characterization of BPP as a class in the second 
level of the low hierarchy, Exercise 6 (a), was presented for first time in Zachos 
and Heller (1986). Exercises 6 (a) and (b) are from Sch6ning (1986). In that 
paper, Theorem 11.5 was proved. 
The equivalence the collapse of AM[c] to AM[2] for constant c was first 
shown by Babai (1985). Our proof is basically inspired by the work of Zachos 
and Heller (1986). Lemma 11.4 is from Sch6ning (1986). Recently Babai and 
Moran (1988) have proved the following speed-up theorem: For all polynomially 
bounded f(n) ~ 2, we get AM [J(n)] = AM [J(n) + 1]. The proof is beyond the 
scope of the chapter. 
The equivalence between IP and AM was shown by Goldwasser and Sipser 
(1986). Due to the complexity of their argument, we have presented a simpler 
proof, which in Goldwasser (1988) is attributed to J. Kilian. As we already 
remarked, this proof works only for a constant number of interactions. Maybe 

256 
Probability Classes and Proof-Systems 
the equivalence between all the levels of the Arthur-Merlin and interactive proof-
system hierarchies is the reason for the fact that people tend to define these 
classes in a rather sloppy way. In this chapter, we have tried to clarify somewhat 
the differences between round and move and their use in the definitions of the 
classes. Exercise 11 is from a personal comunication of U. Schoning. 
As stated at the beginning of the chapter, we have just given a small survey of 
certain aspects of probabilistic classes. For the interested reader, we present some 
references to further work in the subject. There is a whole bunch of separation 
results for the unbounded levels of the Arthur-Merlin hierarchy. Aiello, Hastad, 
and Goldwasser (1986) prove that for any unbounded fen) there exists an oracle 
B such that AM(f(n»B ~ PHB; Fortnow and Sipser (1987) have constructed 
an oracle B such that CO-NpB ~ AMB. These two results seems to indicate 
that the unbounded levels of the Arthur-Merlin hierarchy are not believed to be 
contained in a finite level of the polynomial-time hierarchy. Recently, M. Santha 
(1989) has found an oracle C such that MAC =I AMc , solving an open problem 
posed in Babai (1985). 
Another related area in which an incredible amount of research is going on 
is that of zero-knowledge proofs. In the field of zero-knowledge proofs, Arthur-
Merlin protocols seem to be useless, and so far only the IP type protocols 
are used. Zero-knowledge proofs were introduced by Goldwasser, Micali and 
Rackoff (1985). Since then, the list of contributions has become very large and 
is beyond the scope of this short remark. We refer the interested reader to the 
survey papers by Blum (1986) and Goldwasser (1989) and to the last section of 
Goldreich (1988). In these, many other references can be found. 

Appendix 
Complementation via Inductive Counting 
1 Nondeterministic Space is Closed Under Complement 
The closure under complementation of nondeterministic complexity classes is 
still a research problem. Several computational notions, in particular those re-
garding the power of alternating quantifiers (as in the polynomial time hierar-
chy) or of resource-bounded alternation-bounded alternating machines, will not 
be well understood without solving this problem: for instance, if NP is closed 
under complementation then the polynomial time hierarchy collapses to L't. 
We present in this appendix a technique due to N. Immerman to show that 
nondeterministic space classes are closed under complement. The technique 
consists of nondeterministically counting, in an inductive manner, the number of 
accessible configurations and the number of accepting accessible configurations. 
The fundamental property of the technique is that knowing the number N 
of accessible configurations allows one to check, when it is the case, that no 
accepting accessible configuration exists, by nondeterministically finding N non-
accepting accessible configurations. 
The main tool is presented in the next lemma, which shows how to decide 
whether a given configuration is accessible in d + 1 steps provided that the 
number of configurations accessible in d steps is known. The key property of 
the nondeterministic machine that does so is that on the appropriate inputs it,is 
a strong nondeterministic machine (see Exercise 24 in Chapter 4 of Volume I). 
We say that a nondeterministic machine with accepting and rejecting final states 
is strong on input x if either some computation accepts and no computation 
rejects, or some computation rejects and no computation accepts. Thus, it does 
not yield contradictory answers. The interest of strong machines is that they are 
always able to provide a correct answer, if the nondeterminism works properly, 
and if it fails they can detect it and abort the computation. This fact is used in 
the proof. 
Our notion of configuration in this appendix is that presented in Defini-
tion 1.32 of Volume I, in which the contents of the input tape is omitted; the 
context will make clear at each moment what the input is. Let AI be a non-
deterministic space-bounded machine with space bound s(n). If it is at least 

258 
Appendix 
logarithmic, then O(s(n)) space suffices for describing each configuration; thus 
we identify each configuration with a word of length O(s(n)) which encodes it. 
Cycling over all the words of this length, e.g. in lexicographical order (by repeat-
edly adding 1 in binary), allows one to cycle over all the possible configurations 
of 111 on inputs of length n. Cycling in this manner will be very useful in the 
algorithms in the proof. 
Denote by 10 the (encoding of the) initial configuration of 111, which is in-
dependent of the input. For an input x, denote by RchM,r(t) the number of 
configurations reachable by computations on input x in at most t steps starting 
at 10. Again, when AI and x are easily identifiable from the context, the sub-
script will be omitted. Finally, let t(n) be a time bound for 111 obtained as in 
Theorem 2.8 of Volume I. 
We now describe a nondeterministic decision procedure for reach ability of 
configurations. The major property is that when the input provides the correct 
number RchM,At) of configurations, then the procedure is strong: some com-
putation does not abort, and all nonaborting computations give a correct answer. 
Lemma 1 Given AI as before, there is a nondeterministic machine AI0 working 
in space s(n), which on each input (x, n, t, u), where n = Rch(t), is strong and 
accepts if and only if u encodes a configuration reachable in at most t + 1 steps 
for 10, 
Proof 
The machine is presented in Figure 1. Assume that some computation 
Afo: 
input x, n, t, u 
m :=0 
for each word v coding a configuration of 111 do 
nondeterministically simulate 111 on x during at most t steps, 
checking whether v is reached 
if so, then 
m:= m+ 1 
if AI on input x goes in one step from v to u then accept 
end for 
if m = n then reject 
else abort by halting in a nonfinal ("?") state 
end 
Figure 1 A machine to test reachability 
reaches u within t + 1 steps, and let v be a predecessor of u in this computation. 

Nondeterministic Space is Closed Under Complement 
259 
Then the nondeterministic choices that hit on the the right way of reaching v 
in at most t steps will accept; any other nondeterministic choice will fail to 
count v as accessible, and therefore will end up with m < n, thus aborting. No 
computation rejects. 
Conversely, if u is not reachable then no computation will accept; moreover, 
some nondeterministic choices will find all the right ways of reaching each 
configuration v reachable in at most t steps, and thus the machine will reach 
m = n and reject. Thus if n = Rch(t) then Mo is strong. 
0 
The machine works properly for every t ~ O. In particular for t = 0 the only 
configuration reachable in 0 steps is 10, n is expected to have value 1, and Mo 
will answer YES if and only if u is reachable in one step from 10• 
In the main proof, this lemma is used for two purposes: (1) to inductively 
count the number of reachable configurations, by computing Rch(t + 1) from 
each Rch(t) up to the time bound t(n); and (2) to check whether any accepting 
configuration is reachable within the time bound. The important point is that 
the "negative" information provided by Mo is complete, in the sense that its 
status as a strong nondeterministic machine for the proper inputs guaranteees 
that the answer is NO if and only if the configuration u received as input is 
NOT accessible from the starting configuration. 
We now present the machine that uses 1110 to inductively count the number of 
accessible configurations Rch(t(lxi». It computes it nondeterministicaIly; this 
means that there are always some nondeterministic paths that correctly compute 
the number, and that all the others detect that they fail to compute the correct 
number and halt without answer. 
Lemma 2 On the same hypothesis as the previous lemma, there is a machine 
which on input x nondeterministically computes the number of accessible con-
figurations Rch(t(lx I». 
Proof This machine repeatedly calls Mo, but is deterministic otherwise. It is 
presented in Figure 2. The comments indicate the assertions required to verify 
that it meets the statement of the lemma. 
The assertion n = Rch(t) guarantees that all the calls to Mo are correct, and 
therefore it either aborts (and then .HI aborts as well) or answers correctly. Thus 
at the end of the extemalloop t the value of 711, has correctly counted how many 
configurations were accessible in at most t + 1 steps, and the last assignment 
updates n so as to maintain the invariant. Thus, if AIl does not abort then it 
computes Rch(t(lxl» as required. 
0 
The information computed by !If, allows us to use Mo again to check the 
reachability of accepting configurations, with the certainty that AI rejects if and 
only if some sequence of nondeterministic choices leads 1110 to reject all of them, 
and no sequence of nondeterministic choices leads Mo to accept any of them. 
This can be tested in nondeterministic space s(n). 

260 
Appendix 
input x 
n:= 1 
t := 0 
for each t from 1 to t(lx I> do 
comment: assertion n = Reh(t) is invariant for the loop 
for each u encoding a configuration do 
if u is reachable from 10 in at most t + 1 steps then m := m + 1 
comment: this is tested by calling Mo on (x, n, t, u) 
end internal loop 
comment: reestablish invariant by updating n. 
n.:= m, 
comment: ready for the next t 
end 
Figure 2 Computing the number of accessible configurations 
Theorem 1 If s(n.) ~ log nand s is space constructible then NSPACE(s) is 
closed under complementation. 
Proof Let M accept L in nondeterministic space s and in time t. Consider a 
machine that on input x calls Ml on x to compute n = RehM,x(tlxl}. and then 
for each accepting configuration u of M calls Mo on input (x, n, t(lxl), u) to 
check whether any of them is accessible. accepting if no accessible accepting 
configuration exists. This machine accepts L. 
0 
A complete machine for the complement of L is presented in Figure 3; it 
combines the previously presented machines. There. the testing for accessibility 
of accepting configurations is done simultaneously with the inductive counting. 
The algorithm uses several counters. These can be implemented by laying 
out s(n.) cells in any of the working tapes of the machine. This can be done since 
sen) is space constructible. [( will count the number of steps we are dealing 
with at any given moment of the algorithm; D will keep track of the number 
of reachable configurations at each value of [(. As usual. the fact that there 
exists a path of [( or less steps. from configuration 10 to configuration I. will 
be denoted by 10 r:5k I. Counters C1 and C2 will contain at some moment the 
number of configurations reachable in at most [( - 1 and [( steps respectively. 
The following corollary follows now from Exercise 15 of Chapter 3. 
Corollary 1 The logs pace alternating hierarchy col/apses to AL'log. 

Nondeterministic Space is Closed Under Complement 
261 
comment: Machine III and space bound s are given 
input x 
create counters D, Cit C2, I<, and C3 of size s(lxi) 
compute initial ID 10 
deterministically compute the number II of configurations 
reachable in one step from 10 
C2:= 0 
for each I do 
if deterministically 101-1 I then C2 := C2 + 1 
D :=C 
comment: I< counts up to the maximum number of steps 
J(:= 2 
while J( < 2s(n) 
C2 := 0 
comment: C2 counts up to Reh(l() 
for each configuration 12 do 
comment: check whether h is reachable in k steps 
CI:= 0 
comment: CI counts up to Reh(I{ -
1) 
for each configuration II do 
comment: check whether II is reachable in k - 1 steps and precedes 12 
guess path 10 l-:5 k II 
if there exists a correct path, then CI := CI + 1 
if II 1-1 h then 
if 12 is accepting, REJECT and STOP 
else 
C2 := C2 + 1 
exit the inner loop 
end for loop on II 
comment: check that the nondeterminism worked correctly 
if CI :/ D then STOP 
end for loop on h 
if D = C2 then ACCEPT 
comment: because no new configurations were reached 
else 
end while 
end 
l( := J( + 1 
D:= C2 
Figure 3 Nondeterministic machine accepting L(M) 

262 
Appendix 
2 Bibliographical Remarks 
The closure of nondeterministic space classes under complementation via induc-
tive counting was proved independently and almost simultaneously by Immer-
man (1988) and Szelepcsenyi (1988); both results were distributed as internal 
research reports in 1987. See also Hromkovic (1988). The result followed sev-
eral previous results proving the collapse of logarithmic space hierarchies, shown 
in Jenner, Kirsig, and Lange (1989), Toda (1987), and Schoning and Wagner 
(1988). Similar techniques for counting words in certain sets (e.g. sparse sets) 
were already known; see e.g. Mahaney (1982). Related results are the closure of 
NWG under NC reductions-attributed to S. Buss in Immerman (1988)-and 
the closure under complements of the class LOG(CFL) of the sets :::;~g-reducible 
to context-free languages-attributed to M. Tompa in Immerman (1988). 
Also in Immerman (1988), some consequences are stated regarding closure 
properties of classes of sets definable in certain first-order languages. The tech-
nique of inductive counting has been used also· to show properties of classes 
corresponding to symmetric logarithmic space, probabilistic logarithmic space, 
and circuits with semi-unbounded fan-in. See Borodin, Cook, Dymond, Ruzzo, 
and Tompa (1989). Other applications of the technique are given in Buntrock, 
Hemachandra, and Siefkes (1988). 

Errata 
EATCS Monographs on Theoretical Computer Science, Vol. 11 
J. L. Balcazar, J. Diaz, J. Gabarr6: 
Structural Complexity I 
ISBN 3-540-18622-0 
ISBN 0-387-18622-0 
Page 2, line -11. Chapter 5, not 4. 
Page 10, line 6. The sum should be (n· (n + 1))/2. 
Page 10, just after Definition 1.11. The formula in the sentence "It is immediately clear 
that ... " is incorrect. Should be CA(nr2 ) ~ rl ·2n • 
Page 12, line 23. A partition of the power set of ~*, not of ~* itself. 
Page 12, line -5. The term "preorder" is defined in Proposition 3.5a, page 65. 
Page 15, Example 1.4. The indices in the first disjunct should run from Xl up to Xr(l), 
in the second up to X r(2), ••• , and in the last up to Xr(k). 
Page 17, line 1. Conjunctive. 
Page 18, line 6. Should say" ... how to remove the constants." 
Page 18, lines 21 and 22. Permute "assigned I" with "assigned 0". 
Page 37, Proposition 2.7. The logarithm must be to base 2. 
Page 42, lines 8 and 9. O(t) should be 9(1), and 0(8) should be 9(8). 
Page 49, proof of (d). For "from (b)" read ''from (c)". 
Page 51, Figure 2.3, line 13. The "return false" instruction is incorrect here; it has to 
be executed upon exit of the "for" loop, if no middle configuration was found. 
Page 65, line 11. Delete "it". 
Page 65, line -14. For "as" read "an". 
Page 71, line 18. For "xn " read "xm ". 

Page 75, Theorem 3.11. Substitute fen) for nand f(lwl) for Iwl throughout the proof. 
Page 80, Exercise 9. To prove that :::;~p is not transitive requires complex techniques, 
such as an oracle construction as in Chapter 7 of Volume II. 
Page 86, Definition 4.6, item 3: For "nondeterministic" read "deterministic". 
Page 89, line 4. The expression n E A refers to the word associated with integer n as 
explained in Chapter 1, page 8. 
Page 90. The algorithm of Figure 4.6 does not accept tally(bits(S)). The set T to be 
used in the proof is the set accepted by this algorithm, and not tally(bits(S)) as defined. 
Page 92, line 1. For "on" read "on the". 
Page 92, line 2. Theorem 4.2, not Corollary 4.2. 
Page 95, line 5 (Figure 4.8). For "smaller" read "smallest". 
Page 96, Exercise 19. The last deduction requires the hypothesis that C is closed under 
polynomial time Turing reducibility. 
Page 101, Definitions 5.3, 5.4. Every occurrence of gj should be gi. 
Page 103, line -8. Not O(n2 ) but O(n). 
Page 104, Example 5.8. For "transitive closure" read "reflexive transitive closure". 
Page 105, Figure 5.3. C stands for (A + 1). 
Page 115, proof of Theorem 5.7. The assumption of line 6 of the proof requires paddabil-
ity in addition to disjunctive self-reducibility. 
Page 121, Figure 5.13. The topmost horizontal wire connecting the output of C1 to the 
output of C2 should not be there. The output of C1 should be useless. 
Page 125, Exercise 6: It is not transitive. The composition of P/log-m-reducibility and 
polynomial time m-reducibility is again P/log-m-reducibility. 
Page 133, Figure 6.2. This is both a nondeterministic algorithm for SAT and a proba-
bilistic algorithm for MAJ. 
Page 133, Theorem 1. Time O(2t) should be O(t22t). 
Page 135. Between lines 7 and 8 it should be shown that PP can also be defined with 
> (1/2) ratio of accepting computations for acceptance, and < (1/2) (instead of :::;) for 
rejection. 

Page 138, line -16. For "a a" read "a". 
Page 149, line 1. Add "Exercises 2 and 3 are from Russo (1985). Theorem 6.4 appears, 
together with related material, in Zachos (1982)." (See below.) 
Page 158, Exercise 7. Sentence 1: for "a recursively presentable class closed" read "a 
class closed under join and". Sentence 2: for "classes," read "classes closed under finite 
variations," . 
Page 158. Exercise 8. Sentence 1: for "recursively presentable classes" read "nonempty 
recursively presentable classes closed under finite variations". Sentence 2: for "under" 
read "under join and under". Sentence 3: for "not recursively presentable" read "not 
recursively presentable, unless it is empty" . 
Page 163, line 12. Should read " ... in terms of ... ". 
Page 167, line -11. D should be D'. 
Page 172, line 3. F should be the set of all w of length p( n) such that (y, w) does not 
belong to E. 
Page 179, under Hinman, P., Zachos, E. (1984). Oberwolfach. 
Page 183, under Stearns, R.E., Hartmanis, J., Lewis, P.M. (1965). "Conference Record" 
should read "Proc. 6th Annual IEEE Symp." The page numbers should be 179-190. 
Page 183. Add reference: Zachos, S. (1982): "Robustness of probabilistic computational 
complexity classes under definitional perturbations". Information and Control 54, 143-
154. 
Page 188. Index should include: "Degree, 65, 85" and "Preorder, 65". 
Springer-Verlag 
Berlin 
Heidelberg 
New York 
London 
Paris 
Tokyo 
Hong Kong 
November 1989 

References 
Adleman, L. (1979): "Time, space, and randomness". Technical Report MlT/LCSrrM-
131, Massachusetts Institute of Technology. 
Aiello, W., Goldwasser, S., Hastad, J. (1986): "On the power of interaction". In: Proc. 
27th. IEEE Symp. on Foundations of Computer Science, 368-379. 
Akl, S.G. (1989): The Design and Analysis of Parallel Algorithms. Prentice-Hall Inter-
nacional, Inc. 
Allender, E.W. (1985): "Invertible functions". Ph. D. Dissertation, Georgia Institute of 
Technology. 
Allender, E.W. (1986): "Characterizations of PUNC and precomputation". In: Proc. 
13th. Int. Coli. on Automata. Languages. and Programming (L. Kott, ed.), 1-10. 
Lecture Notes in Computer Science 226, Springer-Verlag. 
Allender, E.W. (1988): "Isomorphisms and l-L reductions". Journal of Computer and 
System Sciences 36, 336-350. 
Allender, E.W., Rubinstein, R. (1988). "P-printable sets". SIAM Journal on Computing 
17, 1193-1202. 
Babai, L. (1985): "Trading group theory for randomness". In: Proc. 17th. ACM Symp. 
on Theory of Computing, 421-429. 
Babai, L., Moran, S. (1988): "Arthur-Merlin games: a randomized proof-system, and 
a hierarchy of complexity classes". Journal of Computer and System Sciences 36, 
254-276. 
Baker, T., Gill, J., Solovay, R. (1975): "Relativizations of the P =?NP question". SIAM 
Journal on Computing 4, 431-442. 
Baker, T., Selman, A.L. (1979): "A second step toward the polynomial time hierarchy". 
Theoretical Computer Science 8, 177-187. 
BaIclizar, J.L. (1985): "Simplicity, relativizations, and nondeterminism". SIAM Journal 
on Computing 14, 148-157. 
BaIclizar, J.L., Book, R.V. (1986): "Sets with small generalized Kolmogorov complex-
ity". Acta Informatica 23, 679-688. 
BaIcazar, J.L., Book, R.V., Sch6ning, U. (1985): "On bounded query machines". The-
oretical Computer Science 40, 237-243. 
Balcazar, I.L., Book, R.V., Sch6ning, U. (1986a): ''The polynomial time hierarchy and 
sparse oracles". Journal of the ACM 33,603-617. 
Balcazar, J.L., Book, R.V., Sch6ning, U. (1986b): "Sparse sets, lowness, and highness". 
SIAM Journal on Computing 15, 739-747. 
BaIclizar, J.L., D1az, 1., Gabarr6, 1. (1985): "Examples of CF-bi-immune and CF-
level able sets in LOGSPACE". Bulletin of the EIirCS 25, 11-14. 

264 
References 
Balcazar, J.L., Dlaz, J., Gabarro, J. 
(1987): 
"On characterizations of the class 
PSPACE/poly". Theoretical Computer Science 52, 1-17. 
Balcazar, J.L., Russo, D.A. (1988): "Immunity and simplicity in relativizations of prob-
abilistic complexity classes". Informatique Theorique et Applications 22, 227-244. 
Balcazar, J.L., Schening, U. (1985): "Bi-immune sets for complexity classes". Mathe-
matical Systems Theory 18, 1-10. 
Barrington, D.A. (1989): "Bounded-width polynomial-size branching programs recog-
nize exactly those languages in N Cl. Journal of Computer and System Sciences 38, 
150-164. 
Beame, P.W., Cook, S.A., Hoover, HJ. (1984): "Log depth circuits for division and 
related problems". In: Proc. 25th. Symp. on Foundations of Computer Science, 1-11. 
Bennett, c., Gill, J. (1981): "Relative to a random oracle A, pA i NpA with probabil-
ity I". SIAM Journal on Computing 10, 96-113. 
Bennan, L. (1976): "On the structure of complete sets: almost everywhere complexity 
and infinitely often speed-up". In: 17th. IEEE Symp. on Foundations of Computer 
Science, 76-80. 
Bennan, L., Hartmanis, 1. (1977): "On isomorphism and density of NP and other com-
plete sets". SIAM Journal on Computing 6, 305-322. 
Bennan, P. (1978): "Relationship between density and detenninistic complexity of NP-
complete languages". In: Proc. 5th. Int. Coli. on Automata. Languages. and Program-
ming (G. Ausiello, ed.), 63-71. Lecture Notes in Computer Science 62, Springer-
Verlag. 
Bertoni, A, Goldwunn, M., Mauri, G., Sabadini, N. (1986): "Parallel algorithms and 
the classification of problems". Technical Report, University of Milano. 
Blum, M. (1986): "How to prove a theorem so no one else can claim it". International 
Congress of Mathematicians, Berkeley. 
Book, R.V. (1981): "Bounded query machines: on NP and PSPACE'. Theoretical Com-
puter Science 15, 27-39. 
Book, R.V. (1989): "Restricted relativizations of complexity classes". In: Computational 
Complexity Theory, Proc. Symposia in Applied Mathematics 38 O. Hartmanis, ed.), 
American Mathematical Society, 47-74. 
Book, R.V., Du, Ding-Zhu (1987): "The existence and density of generalized complexity 
cores". Journal of the ACM 34,718-730. 
Book, R.V., Du, Ding-Zhu, Russo, D.A (1988): "On polynomial and generalized com-
plexity cores". In: Proc. Structure in Complexity Theory Third Annual Conference, 
236-250. 
Book, R.V., Long, TJ. (1985): "Relativizing the P =?NP question". Unpublished 
manuscript, Dept Mathematics, Univ. California Santa Barbara. 
Book, R.V., Long, T.J., Selman, A.L. (1984): "Quantitative relativizations of complexity 
classes". SIAM Journal on Computing 13 461-487. 
Book, R.V., Long, TJ., Selman, A.L. (1985): "Qualitative relativizations of complexity 
classes". Journal of Computer and System Sciences 30, 395-413. 
Book, R.V., Orponen, P., Russo, D.A, Watanabe, O. (1988): "On exponential lowness". 
SIAM Journal on Computing 17 504-516. 
Book, R.V., Wilson, C., Xu, Mei-Rui (1982): "Relativizing time, space, and time-space". 
SIAM Journal on Computing 11,571-581. 

References 
265 
Book. R.V., Wrathall, C. (1981): "Bounded query machines: on NPO and NPQUERYO". 
Theoretical Computer Science 15 41-50. 
Boolos, G. (1989): "A new proof of the Godel incompleteness theorem". Notices of the 
American Mathematical Society 30, 388-390. 
Boppana, R., Hastad, 1., Zachos, S. (1987): "Does co-NP have shon interactive proofs?". 
Information Processing Letters 25, 127-132. 
Borodin, A. (1977): "On relating time and space to size and depth". SIAM Journal on 
Computing 6, 733-744. 
Borodin, A., Cook, S.A., Dymond, P., Ruzzo, W., and Tompa, M. (1989): ''Two ap-
plications of complememation via inductive counting". SIAM Journal on Computing 
18, 559-578. 
Borodin, A., von zur Gathen, 1., Hopcroft, 1. (1982): "Fast parallel matrix and GDC 
computations". Information and Control 52, 241-256. 
Brassard, G. (1983): "Relativized cryptography". IEEE Transactions on Information 
Theory IT-29, 877-894. 
Breidbart, S. (1978): "On splitting recursive sets". Journal of Computer and System 
Sciences 17, 56-64. 
Buntrock, G., Hemachandra, L.A., Siefkes, A. (1988): "Unambiguous computation may 
be better than determinism at simulating nondeterminism". Manuscript. 
Buss, S.R. (1987): ''The boolean formula value problem is in ALOGTIME". In: Proc. 
19th. ACM Symp. on Theory of Computing, 123-131. 
Caner, J.L., Wegman, M.N. (1979): "Universal classes of hash functions". Journal of 
Computer and System Sciences 18, 143-154. 
Chaitin, G.J. (1966): "On the length of programs for computing finite binary sequences". 
Journal of the ACM 13, 547-569. 
Chaitin, G.J. (1969): "On the simplicity and speed of programs for computing definite 
sets of natural numbers". Journal of the ACM 16, 407-412. 
Chaitin, G.J. (1974): "Information-theoretic limitations of formal systems". Journal of 
the ACM 21, 403-424. 
Chaitin, G.J. (1977): "An information-theoretic proof that there are infinitely many 
primes". IBM-RC 6833. 
Chandra, A.K., Kozen, D., Stockmeyer, LJ. (1981): "Alternation". Journal of the ACM 
28, 114-133. 
Chandra, A.K., Stockmeyer, L.1. (1976): "Alternation". In: Proc. 17th. IEEE Symp. on 
Foundations of Computer Science, 98-108. 
Chandra, A.K., Stockmeyer, LJ., Vishkin, U. (1982): "Complexity theory for un-
bounded fan-in parallelism". In: Proc. 23rd.IEEE Symp. on Foundations of Computer 
Science, 1-13. 
Chandra, A.K., Stockmeyer, L.J., Vishkin, U. (1984): "Constam depth reducibility". 
SIAM Journal on Computing 13, 423-439. 
Cook, S.A. (1979): "Deterministic CFL's are accepted simultaneously in polynomial 
time and log squared space". In: Proc. 11th. ACM Symp. on Theory of Computing, 
338-345. 
Cook, S.A. (1981): ''Towards a complexity theory of synchronous parallel computation". 
L'Enseignement Mathematique 27,99-124. 

266 
References 
Cook, S.A. (1985): "A taxonomy of problems with fast parallel algorithms". Information 
and Control 64, 2-22. 
Cook, S.A., McKenzie, P. (1987): "Problems complete for deterministic logarithmic 
space". Journal of Algorithms 8, 385-394. 
Dekhtyar, M. (1976): 
"On the relativization of deterministic and nondeterminis-
tic complexity classes". In: Mathematical Foundations of Computer Science (A. 
Mazurkiewicz, ed.), 255-259. Lecture Notes in Computer Science 45, Springer-
Verlag. 
Diaz, J., Toran, J. (1987): "Classes of bounded nondeterminism". To appear in Mathe-
matical Systems Theory. 
Du, Ding-Zhu, Book, R. (1989): "On inefficient special cases of NP-complete prob-
lems". Theoretical Computer Science 63, 239-252. 
Du, Ding-Zhu, Isakowitz, T., Russo, D.A. (1984): "Structural properties of complexity 
cores". Unpublished manuscript. Dept Mathematics, Univ. California Santa Barbara. 
Dymond, P.W., Cook, S.A. (1980): "Hardware complexity and parallel computation". 
In: Proc. 21th. IEEE Symp. on Foundations of Computer Science, 360-371. 
Dymond, P.W., Cook, S.A. (1989): "Complexity theory of parallel time and hardware". 
Information and Computation 80, 205-226. 
Dymond, P.W., Tompa, M. (1985): "Speedups of deterministic machines by synchronous 
parallel machines"., Journal of Computer and System Sciences 30, 146-161. 
van Emde Boas, P. (1985a): "The second machine class: models of parallelism". In: Par-
allel Computers and Computations, CWI Syllabus 9 (J. van Leeuwen, J.K. Lenstra, 
eds.), 133-161. Center for Mathematics and Computer Science, Amsterdam. 
van Emde Boas, P. (1985b): "The second machine class 2: an encyclopedic view of 
the parallel computation thesis". In: Proc. Stefan Banach International Mathematical 
Center. 
Even, S., Selman, A.L., Yacobi, Y. (1985): "Hard-core theorems for complexity classes". 
Journal of the ACM 32,205-217. 
Feller, W. (1968): An introduction to probability theory and its applications. Volume 1. 
3rd. edition. Wiley, New York. 
Fortnow, L. (1987): "The complexity of perfect zero-knowledge". In: Proc. 19th. ACM 
Symp. on Theory of Computing, 204-209. 
Fortnow, L., Sipser, M. (1987): "Interactive proof systems with a log space verifier". 
Preprint Massachusetts Institute of Technology. 
Fortune, S. (1979): "A note on sparse complete sets". SIAM Journal on Computing 8, 
431-433. 
Fortune, S., Wyllie, J. (1978): "Parallelism in random access machines". In: Proc. 10th. 
ACM Symp. on Theory of Computing, 114-118. 
Furst, M., Saxe, J., Sipser, M. (1984): "Parity, circuits, and the polynomial-time hierar-
chy". Mathematical Systems Theory 17, 13-27. 
Galil, Z., Paul, W.J. (1983): "An efficient general-purpose parallel computer". Journal 
of the ACM 30, 360--387. 
Gasarch, W.I., Homer, S. (1983): "Relativizations comparing NP and exponential time". 
Information and Control 58, 88-100. 
von zur Gathen, J. (1984): "Parallel powering". In: Proc. 25th. ACM Symposium on 
Theory of Computing, 31-36. 

References 
267 
Geske, J., GroUmann, J. (1986): "Relativizations of unambiguous and random polyno-
mial time classes". SIAM Journal on Computing IS, 511-519. 
Gibbons, A., Rytter, W. (1988): Efficient Parallel Algorithms. Cambridge University 
Press. 
Goldberg, A.V., Sipser, M. (1985): "Compression and ranking". Proc. 17th. ACM Sym-
posium on Theory of Computing, 440-448. 
Goldreich, O. (1988): "Randomness, interactive proof, and zero-knowledge. A survey". 
In: The Universal Turing Machine. A Half-Century Survey. (R. Herken, ed.), 377-405. 
Kammerer und Unverzagt, Berlin. 
Goldreich, 0., Micali, S., Wigderson, A. (1986): "Proofs that yield nothing but their 
validity and a methodology for protocol design". In: Proc. 27th. IEEE Symp. Foun-
dations of Computer Science, 174-187. 
Goldschlager, L.M. (1978): "A unified approach to models of synchronous parallel 
machines". In: Proc. 10th. ACM Symp. on Theory of Computing, 89-94. 
Goldschlager, L.M. (1982): "A universal interconnection pattern for paraIlel computers". 
SIAM Journal of Computing 29, 1073-1086. 
Goldschlager, L.M., Shaw, R.A., Staples, J. (1982): "The maximum flow problem is 
log space complete for P". Theoretical Computer Science 21,105-111. 
Goldwasser, S. (1988): Lecture Notes on Interactive Proof Systems. Laboratory for 
Computer Science. Massachusetts Institute of Technology. 
Goldwasser, S., Micali, S., Rackoff, C. (1985): "The knowledge complexity of interac-
tive proofs". In: Proc. 17th. ACM Symp. on Theory of Computing, 291-305. 
Goldwasser, S., Sipser, M. (1986): "Private coins versus public coins in interactive proof 
systems". In: Proc. 18th. ACM Symp. on Theory of Computing, 59-68. 
Halpern, 1.Y., Loui, M.C., Meyer, A.R., Weise, D. (1986): "On time versus space III". 
Mathematical Systems Theory 19, 13-28. 
Hartmanis, J. (1983): "Generalized Kolmogorov complexity and the structure of feasible 
computations". In: 24th. IEEE Symp. Foundations of Computer Science, 439-445. 
Hartmanis, J., Berman, L. (1978): "On polynomial time isomorphism of some new 
complete sets". Journal of Computer and System Sciences 16, 418-422. 
Hartmanis, J., Hemachandra, L.A. (1986): "On sparse oracles separating feasible com-
plexity classes". In: Symp. Theoretical Aspects of Computer Science (B. Monien and 
G. Vidal-Naquet, eds.), 321-331. Lecture Notes in Computer Science 210, Springer-
Verlag. 
Hanmanis, 1., Hemachandra, L.A. (1987): "One-way functions, robustness, and the non-
isomorphism of NP-complete sets". In: Proc. Structure in Complexity Theory Second 
Annual Conference, 160-174. 
Hanmanis, 1., Simon, J. (1974): "On the power of multiplication in random access 
machines". In: Proc. 15th. IEEE Symp. on Switching and Automata Theory, 13-23. 
Hanmanis, J., Yesha, Y. (1984): "Computation times of NP sets of different densities". 
Theoretical Computer Science 34, 17-32. 
Hastad, J. (1987): "Computational limitations for smaIl-depth circuits". Ph.D. Dissena-
tion, Massachusetts Institute of Technology. 
Heller, H. (1984): "Relativized polynomial hierarchies extending two levels". Mathe-
matical Systems Theory 17, 71-84. 

268 
References 
Hemachandra, L.A., Wechsung, G. (1989): "Using Randomness to Characterize the 
Complexity of Computation". In: Information Processing 89 (Proc. IFIP 11th World 
Computer Congress, G.X. Ritter, ed.), 281-285 (participant edition), North-Holland. 
Hennie, F.C. (1965): "One-tape off-line Thring machine computations". Information 
and Control 8, 553-578. 
Hockney, R.W., Jesshope, c.R. (1988): Parallel Computers 2. Adam Hilger Ltd, Bristol. 
Hong, J.W. (1986): Computation: Computability, Similarity and Duality. Pitman, Lon-
don and John Wiley, New York, Toronto. 
Hopcroft, lE., Paul, W.J., Valiant, L.G. (1977): "On time versus space". Journal of the 
ACM 24,332-337. 
Hopcroft, J.E., Ullman, 1 (1969): Formal languages and their relation to automata. 
Addison-Wesley, Reading, Mass. 
Hromkovic, J. (1988): "Two independent solutions of the 23 years old open problem 
in one year or NSPACE is closed under complementation by two authors". Bulletin 
of the EATCS 34,310-312. 
Ibarra, a., Jiang, T., Ravikumar, B. (1988): "Some subclasses of context-free languages 
in Net". Information Processing Letters 29, 111-117. 
Immerman, N. (1988): "Nondeterministic space is closed under complementation". 
SIAM Journal on Computing 17,935-938. 
Jenner, B., Kirsig, B., Lange, K.-J. (1989): "The logarithmic alternation hierarchy col-
lapses: AL'f = Allf". Information and Computation 80,269-288. 
Joseph, D., Young, P. (1985): "Some remarks on witness functions for nonpolynornial 
and noncomplete sets in NP". Theoretical Computer Science 39, 225-237. 
Kadin, J. (1987): "pNP[logn) and sparse Turing complete sets for NP". In: Proc. Structure 
in Complexity Theory Second Annual Conference, 33-40. 
Karp, R., Lipton, R. (1980): "Some connections between non-uniform and uniform 
complexity classes". In: 12th. ACM Symp. on Theory of Computing, 302-309. See 
also: ''Turing machines that take advice", L'Enseignement Mathematique 28, series 2 
(1982), 191-209. 
Karp, R., Ramachandran, V. (1988): "A survey of parallel algorithms for shared-memory 
machines". Technical Report UCB/CSD 88/408. To appear in Handbook ofTheoret-
ical Computer Science, to be published by North-Holland. 
King, K.N. (1981): "Measures of parallelism in alternating computation trees". In: Proc. 
13th. ACM Symp. on Theory of Computing, 189-201. 
Kintala, C.M.R., Fischer, P. (1980): "Refining nondeterminism in relativized polynornial-
time bounded computations". SIAM Journal on Computing 9, 46-53. 
Kirsig, B., Lange, K.-J. (1987): "Separation with the Ruzzo, Simon, and Tompa rela-
tivization implies DSPACE(log n) i NSPACE(log n)".lnformation Processing Letters 
25, 13-15. 
Ko, Ker-I (1986): "On the notion of infinite pseudorandom sequence". Theoretical 
Computer Science 39, 9-33. 
Ko, Ker-I (1985): "Non-level able sets and immune sets in the accepting density hierar-
chy in NP". Mathematical Systems Theory 18, 189-205. 
Ko, Ker-I (1988): "Relativized polynomial-time hierarchies having exactly K levels". 
In: Proc. 20th. ACM Symp. Theory of Computing, 245-253. 

References 
269 
Ko, Ker-I, Long, TJ., Du, Ding-Zhu (1986): "A note on one-way functions and poly-
nomial time isomorphisms". Theoretical Computer Science 47, 263-276. 
Ko, Ker-I, Moore, D. (1981): "Completeness, approximation, and density". SIAM Jour-
nal on Computing 10, 787-796. 
Ko, Ker-I, Schoning, U. (1985): "On circuit-size complexity and the low hierarchy in 
NP". SIAM Journal on Computing 14,41-51. 
Krentel, M. W. (1988): "The complexi ty of optimization problems". J oumal of Computer 
and System Sciences 36, 490-509. 
Kolmogorov, A.N. (1965): "Three approaches for defining the concept of information 
quantity". Probl. Information Transmission 1, 1-7. 
Kolmogorov, A.N. (1968): "Logical basis for information theory and probability the-
ory". IEEE Transactions on Information Theory 14, 662-664. 
Kozen, D. (1976): "On parallelism in 'lUring machines". In: Proc. 17th. IEEE Symp. 
on Foundations of Computer Science, 89-97. 
Kranakis, E. (1986): Primaliry and cryptography. Wiley-Teubner Series in Computer 
Science, Stuttgart. 
Kruskal, C.P., Rudolph, L. and Snir, M. (1988): "A complexity theory of efficient paral-
lel algorithms". In: Proc. 15th. Int. Coli. on Automata, Languages. and Programming 
(T. Lepisto and A. Salomaa, eds.), 333-346. Lecture Notes in Computer Science 317, 
Springer-Verlag. 
Kuroda, S.Y. (1964): "Classes of languages and linear-bounded automata". Information 
and Control 7,207-223. 
Kurtz, S.A. (1983): "On the random oracle hypothesis". Information and Control 57, 
40-47. 
Kurtz, S.A., Mahaney, S.R., Royer, 1.S. (1989): "The isomorphism conjecture fails 
relative to a random oracle". In: Proc. 21th. ACM Symp. Theory of Computing. 
Ladner, R.E., Lynch, N., Selman, A.L. (1975): "A comparison of polynomial time 
reducibilities". Theoretical Computer Science 1, 103-123. 
Lange, K.-J. (1986): "Two characterizations of the logarithmic alternation hierarchy". 
In: Proc. 12th. Symp. on Mathematical Foundations of Computer Science (J. Gruska, 
B. Rovan, and 1. Wiedermann, eds.), 518-526. Lecture Notes in Computer Science 
233, Springer-Verlag. 
Lautemann, C. (1983): "BPP and the polynomial hierarchy". Information Processing 
Letters 17, 215-217. 
van Leeuwen, J., Wiedermann, 1. (1985): "Array processing machines". In: Fundamen-
tals of Computation Theory (L. Budach, ed.), 257-268. Lecture Notes in Computer 
Science 199, Springer-Verlag. 
Lengauer, T., Wagner, K.W. (1987): "The binary network flow problem is logs pace 
complete for P". Technical Report 138 of the Institut rur Mathematik, Universitat 
Augsburg. 
Levin, L. (1973): "Universal sequential search problems" (translated from Russian). 
Probl. Information Transmission 9,265-266. Reprinted in Trakhtenbrot (1984). 
Li, Ming (1985): "Lower bounds by Kolmogorov complexity". In: Proc. 12th. Int. Coil. 
on Automata, Languages, and Programming (W. Brauer and C. Papadimitriou, eds.), 
383-393. Lecture Notes in Computer Science 194, Springer-Verlag. 

270 
References 
Li, Ming, Viuinyi, P.M.B. (1988): ''Two decades of applied Kolmogorov complexity". 
In: Proc. Structure in Complexity Theory Third Annual Conference, 80-101. 
Lischke, G. (1986): "Oracle constructions to prove all possible relationships between 
relativizations of P, NP, EL, NEL, EP and NEP". Zeitschr. Math. Logik und Grund-
lagen Math. 32, 257-270. 
Lischke, G. (1987): "Relativizations of NP and EL, strongly separating, and sparse sets". 
J. Inf. Process. Cybern. ElK 23, 99-112. 
Long, T.J. (1985): "On restricting the size of oracles compared with restricting access 
to oracles". SIAM Journal on Computing 14,585-597. Erratum in: SIAM Journal on 
Computing 17 (1988), p. 628. 
Long, T.J., Selman, AL. (1986): "Relativizing complexity classes with sparse sets". 
Journal of the ACM 33, 618-628. 
Lynch, N. (1975): "On reducibility to complex or sparse sets". Journal of the ACM 22, 
341-345. 
Mahaney, S.R (1982): "Sparse complete sets for NP: solution of a conjecture by Berman 
and Hartmanis". Journal of Computer and System Sciences 25, 130-143. 
Mead, C., Conway, L. (1980): Introduction to VLSI systems. Addison-Wesley, Reading, 
Mass. 
Mehlhorn, K. (1973): "On the size of sets of computable functions". In: Proc. IEEE 
Symp. Switching and Automata Theory, 190-196. 
Meyer, AR, Paterson, M. (1979): "With what frequency are apparently intractable 
problems difficult?". Technical Report TM-126, Massachusetts Institute of Technol-
ogy. 
Minsky, M. (1962): "Problems of formulation for anificial intelligence". In: Mathemati-
cal Problems in the Biological Sciences (RE. Bellman, ed.), American Mathematical 
Society. 
Muller, D.E., Preparata, F. (1975): "Bounds to complexities of networks for sorting and 
switching". Journal of the ACM 22, 195-201. 
Orponen, P. (1986): "A classification of complexity core lattices". Theoretical Computer 
Science 47 (1986), 121-130. 
Orponen, P., Russo, D.A, Sch6ning, U. (1986): "Optimal approximations and poly no-
mially levelable sets". SIAM Journal on Computing 15, 399-408. 
Orponen, P., Sch6ning, U. (1986): "On the density and complexity of polynomial cores 
for intractable sets". Information and Control 70, 54-68. 
Papadimitriou, C.H. (1983): "Games against nature". In: Proc. 24th. IEEE Symp. on 
Foundations of Computer Science, 446-450. 
Parberry, I. (1987): Parallel Complexity Theory. Pitman and John Wiley. 
Parberry, I. (1988): "Parallel complexity with threshold functions". Journal of Computer 
and System Sciences 36, 278-302. 
Paul, W.J. (1978): Komplexitiitstheorie. Teubner, Stuttgart. 
Paul, W.J. (1979): "Kolmogorov complexity and lower bounds". In: Proc. 2nd. Int. 
Con! on Fundamentals of Computation Theory (L. Budach, ed.), 325-335. Akademie-
Verlag. 
Paul, W.l, Pippenger, N., Szemeredi, E., Trotter, W.T. (1983): "On determinism versus 
non-determinism and related problems". In: Proc. 24th. IEEE Symp. on Foundations 
of Computer Science, 429-438. 

References 
271 
Paul, W.J., PrauB, E.J., Reischuk, R. (1980): "On alternation". Acta Informatica 14, 
243-255. 
Paul, W.J., Reischuk, R. (1980): "On alternation II". Acta Informatica 14, 391-403. 
Paul, W.I., Reischuk, R. (1981): "On time versus space II". Journal of Computer and 
System Sciences 22, 312-327. 
Paul, W.I., Seiferas, J., Simon, I. (1980): "An information-theoretic approach to time 
bounds for on-line computations". In: 12th. ACM Symp. on Theory of Computing, 
357-367. 
Petersen, W.P. (1983): "Vector Fortran for numerical problems on CRAY-l". Commu-
nications of the ACM 26, 1008-102l. 
Pippenger, N. (1979): "On simultaneous resource bounds". In: Proc. 20th. IEEE Symp. 
on Foundations of Computer Science, 307-31l. 
Pratt, V., Rabin, M., Stockmeyer, L.I. (1974): "A characterisation of the power of vector 
machines". In: 6th ACM Symp. on the Theory of Computing, 122-134. 
Pratt, V., Stockmeyer, LJ. (1976): "A characterisation of the power of vector machines". 
Journal of Computer and System Sciences 12 198-22l. 
Rackoff, C. (1982): "Relativized questions involving probabilistic algorithms". Journal 
of the ACM 29, 261-268. 
Russo, D.A. (1985): "Structural properties of complexity classes". Ph. D. Dissertation, 
Univ. California Santa Barbara. 
Russo, D.A. (1986): "Optimal approximations of complete sets". In: Proc. Structure in 
Complexity Theory First Annual Conference (A.L. Selman, ed.), 311-324. Lecture 
Notes in Computer Science 223, Springer-Verlag. 
Russo, D.A., Orponen, P. (1987): "On P-subset structures". Mathematical Systems The-
ory 20, 129-136. 
Russo, D.A., Zachos, S. (1983): "Positive relativizations of probabilistic polynomial 
time". Manuscript. 
Ruzzo, W.L. (1978): "General context-free language recognition". Ph. D. Dissertation, 
University of California Berkeley. 
Ruzzo, W.L. (1980): "Tree-size bounded alternation". Journal of Computer and System 
Sciences 21, 218-235. 
Ruzzo, W.L. (1981): "On uniform circuit complexity". Journal of Computer and System 
Sciences 22, 365-383. 
Ruzzo, W.L., Simon, I., and Tompa, M. (1984): "Space-bounded hierarchies and prob-
abilistic computations". Journal of Computer and System Sciences 28, 216-230. 
Samha, M. (1989): Relativized AM versus MA games. Information and Computation 
80,44-49. 
Savage, I.E. (1976): The complexity of computing. I. Wiley, New York. 
Savitch, W.J., Stimson, M. (1979): "Time bounded random access machines with par-
allel processing". Journal of the ACM 26, 103-118. 
Schnorr, C.P. (1976): ''The network complexity and the Turing machine complexity of 
finite functions". Acta Informatica 7, 95-107. 
Schonhage, A. (1980): "Storage modification machines". SIAM Journal on Computing 
9,490-508. 
Schoning, U. (1983): "A low and a high hierarchy within NP". Journal on Computer 
and System Sciences 27, 14-28. 

272 
References 
Schoning, U. (1985): Complexity and Structure. Lecture Notes in Computer Science 
211, Springer-Verlag. 
Schoning, U. (1987a): "Graph isomorphism is in the low hierarchy". In: 4th. Sympo-
sium on Theoretical Aspects of Computer Science (P. Brandenburg, G. Vidal-Naquet, 
and M. Wirsing, eds.), 114-124. Lecture Notes in Computer Science 247, Springer-
Verlag. 
Schoning, U. (1987b): "Complexity cores and hard-to-prove formulas". In: 1st Workslwp 
on Computer Science Logic (E. Borger, H. Kleine Buning, M.M. Richter, eds.), 273-
280. Lecture Notes in Computer Science 329, Springer-Verlag. 
SchOning, U. (1987c): "Probabilistic complexity classes and lowness". In: Proc. Struc-
ture in Complexity Theory Second Annual Conference, 2-8. 
Schoning, U., Book, R.V. (1984): "Immunity, relativizations, and nondeterminism". 
SIAM Journal on Computing 13, 329-337. 
Schoning, U., Wagner, K. (1988): "CoIlapsing oracle hierarchies, census functions, 
and 10garithmicaIly many queries". In: Proc. 5th. Symp. on Theoretical Aspects of 
Computer Science (R. Cori, M. Wirsing, eds.), 91-98. Lecture Notes in Computer 
Science 294, Springer-Verlag. 
Schmidt, D. (1985): "The recursion-theoretic structure of complexity classes". Theoret-
ical Computer Science 38, 143-156. 
Schnitger, G. (1982): "A family of graphs with expensive depth-reduction". Theoretical 
Computer Science 18, 89-93. 
Selman, A.L., Xu, Mei-Rui, Book, R.V. (1983): "Positive relativizations of complexity 
classes". SIAM Journal on Computing 12, 565-579. 
Sipser, M. (1983): "A complexity-theoretic approach to randomness". In: Proc. 15th. 
ACM Symp. Theory of Computing, 330-335. 
Sipser, M. (1985): Lecture Notes on Complexity Theory. Computer Science Division, 
University of California Berkeley. 
Soare, R. (1977): "Computational complexity, speedability, and levelable sets". Journal 
of Symbolic Logic 42, 545-563. 
Stockmeyer, L.J. (1987): "Classifying the computational complexity of problems". Jour-
nal of Symbolic Logic 52, 1-43. 
Stockmeyer, LJ., Vishkin, U. (1984): "Simulation of parallel random access machines 
by circuits". SIAM Journal on Computing 13,409-422. 
Stone, H.S. (1971): "ParaIlel processing with the perfect shuffle". Communications of 
the ACM 20, 153-161. 
Sudborough, I.H. (1978): "On the tape complexity of deterministic context-free lan-
guages". Journal of the ACM 25,405-414. 
Szelepcsenyi, R. (1988): "The method of forced enumeration for nondeterministic au-
tomata". Acta Informatica 26, 279-284. 
Toda, S. (1987): "E2-SPACE is closed under complement". Journal of Computer and 
System Sciences 35, 145-152. 
Torenvliet, L. (1986): "Structural concepts in relativised hierarchies". Ph. D. Disserta-
tion, Univ. Amsterdam. 
Torenvliet, L. (1988): "A second step towards the strong polynomial time hierarchy". 
Mathematical Systems Theory 21, 99-123. 

References 
273 
Torenvliet, L., van Emde Boas, P. (1989): "Simplicity, immunity, relativizations, and 
nondetenninism".lnformation and Computation 80, 1-17. 
Trakhtenbrot, B.A. (1984): "A survey of Russian approaches to perebor (brute. force 
search) algorithms". Annals of the History of Computing 6 (1984),384-401. 
Trahan, J.L., Ramachandran, V., Loui, M.C. (1989): ''The power of paraIlel random 
access machines with augmented instruction sets". In: Proc. Structure in Complexity 
Theory Fourth Annual Conference, 97-103. 
Ukkonen, E. (1983): ''Two results on polynomial time truth-table reductions to sparse 
sets". SIAM Journal on Computing 12, 580-587. 
UIlman, J.D. (1984): Computational Aspects of VLSI. Computer Science Press. 
Venkateswaran, H., Tompa, M. (1986): "A new game that characterizes parallel com-
plexity classes". In: Proc. 27th. IEEE Symp. on Foundations of Computer Science, 
348-360. 
Vishkin, U. (1983): "Synchronous parallel computation-a survey". Courant Institute, 
New York University. 
Vitter, J.S., Simons, R.A. (1986): "New classes for parallel complexity: a study of 
unification and other complete problems". IEEE Transactions on Computers 35, 403-
418. 
Wagner, K., Wechsung, G. (1986): Computational Complexity. Reidel, Dordrecht 
Watanabe, O. (1988): "On hardness of one-way functions". Information Processing 
Letters 27 (1988), 151-157. 
Wegener, I. (1987): The Complexity of Boolean Functions. Wiley-Teubner Series in 
Computer Science, Stuttgart. 
Wilson, C. (1985): "Relativized circuit complexity". Ph. D. Dissertation, Univ. Toronto. 
Xu, Mei-Rui, Doner, J., Book, R.V. (1983): "Refining nondetenninism in relativizations 
of complexity classes". Journal of the ACM 30, 677-685. 
Yao, A.C.C. (1985): "Separating the polynomial time hierarchy by oracles". In: Proc. 
26th. IEEE Symp. on Foundations of Computer Science, 1-10. 
Yap, C. (1983): "Some consequences of nonunifonn conditions on unifonn classes". 
Theoretical Computer Science 27, 287-300. 
Yesha, Y. (1983): "On certain polynomial time truth-table reducibilities of complete 
sets to sparse sets". SIAM Journal on Computing 12,411-425. 
Young, P. (1983): "Some structural properties of polynomial reducibilities and sets in 
NP". In: Proc. 15th. ACM Symposium on Theory of Computing, 392-401. 
Zachos, S. (1988): "Probabilistic quantifiers and games". Journal of Computer and 
System Sciences 36, 433-451. 
Zachos, S., HeIler, H. (1986): "A decisive characterization of BPP". Information and 
Control 69, 125-135. 

Author Index 
Adleman, L., 233 
AieIlo, W., 256 
Aid, S.G., 62 
AIlender, E.W., 118, 132, 234 
Babai, L.; 254, 255, 256 
Baker, T., 176, 177, 197,217 
Bald.zar, J.L., 147, 148, 176, 197,217, 
218, 234 
Barrington, D.A., 117 
Beame, P.W., 118 
Bennett, c., 177, 255 
Berman, L., 132, 147 
Berman, P., 132 
Bertoni, A., 117 
Blum, M., 256 
Book, R, 148, 176, 197, 198,217, 
218,234 
Boppana, R, 255 
Bbrodin, A., 94, 117, 118,262 
Brassard, G., 176 
Breidbart, S. 147 
Buntrock, G., 262 
Buss, S., 94, 262 
Carter, J.L., 255 
Chandra, A.K., 61, 94, 95, 118 
Chaitin, G.J., 233 
Conway, L., 61 
Cook, S.A., 61, 62, 95, 117, 118,262 
Diaz, J., 148, 176,234 
Doner, J., 176 
Du, D.Z., 133, 148 
Dymond, P.W., 61, 94,262 
van Emde Boas, P., 62, 177 
Even, S., 148 
Feather, T., 118 
Fischer, P., 176 
Fortnow, L., 255 
Fortune, S., 61, 132, 133 
Furst, M., 117, 177 
Gabarr6, J., 148, 234 
GaIil, Z., 62 
Gasarch, W.I., 177 
von zur Gathen, J., 118 
Geske, J., 176 
Gibbons, A., 62 
Gill, J., 176, 177, 197,255 
Goldberg, A., 234 
Goldschlager, L.M., 61, 117, 118 
Goldreich, 0., 254, 256 
Goldwasser, S., 254, 255, 256 
Goldwurm, M., 117 
GroIlman, J., 176 
Halpern, J.Y., 96 
Hastad, J., 177,255 
Hartmanis, J., 61, 132, 133, 
233, 234 
Heller, H., 177,218,255 
Hemachandra, L.A., 133, 233, 234, 262 
Hennie, F.C., 96 
Hockney, RW., 32, 61 

Author Index 
275 
Homer, S., 177 
Mahaney, S.R., 133,217,262 
Hong, J. W., 32, 62, 117 
Mauri, G., 117 
Hoover, H.l, 118 
McKenzie, P. 117 
Hopcroft, J., 95, 96, 118 
Mead, C., 61 
Hromkovi~, I, 262 
Mehlhorn, K., 177 
Ibarra, D., 117 
Meyer, A.R., 94,96, 133, 148 
Immerman, N., 95, 262 
Micali, S., 254, 256 
Isakowitz, T., 148 
Minsky, M., 233 
Jenner, B., 95, 262 
Muller, D.E., 117 
Jesshope, C.R., 32, 61 
Moore, D., 147 
Jiang, T., 117 
Moran, S., 254, 255 
Joseph, D., 133 
Oeponen, P., 147, 148, 218 
Kadin, 1, 217 
Parberry, I., 62 
Karp, R., 62, 118, 198,217 
Papadimitriou, C.H., 155 
King, K.N., 95 
Paterson, M., 133, 148 
Kintala, C.M.R., 176 
Paul, W.I., 62, 94, 95, 96, 234 
Kirsig, B., 95, 197,262 
Petersen, W.P., 32 
Kilian, I, 255 
Pippenger, N., 96, 117 
Ko, Ker-I, 133, 147, 148, 177, 
Pratt, V., 32, 61 
217,233 
PrauB, E.I., 94 
Kolmogorov, A.N., 233 
Preparata, F., 117 
Kozen, D., 61, 94, 95 
Rabin, M.D., 32, 61 
Kranakis, E., 176 
Rackoff, C., 176, 254, 256 
Kruskal, C.P., 118 
Ramachandran, V., 61, 62, 118 
Kuroda, S.Y., 95 
Ravikumar, B., 117 
Kurtz, S.A., 133, 177 
Reischuk, R., 94, 95, 96 
Ladner, R.I., 147 
Royer, I, 133 
Lange, K.-1., 95, 197,262 
Rubinstein, R., 234 
Lautemann, C., 255 
Rudolph, L., 118 
van Leeuwen, J., 61 
Russo, D.A., 148, 176, 198,218 
Lengauer, T., 118 
Ruzzo, W., 94,95, 117, 197,262 
Levin, L.A., 233 
Rytter, W., 62 
Li, M., 234 
Sabadini, N., 117 
Lipton, R.l, 198, 217 
Santha, M., 256 
Lischke, G., 177 
Savage, S.E., 117 
Long, T., 133, 198, 217 
Savitch, W.1., 61 
Loui, M.C., 61, 96 
Saxe, 1., 117, 177 
Lynch, N., 147 
Schmidt, D., 147 

276 
Author Index 
Schnitger, G., 96 
Schnorr, C.P., 117 
Schi:inhage, A., 61 
Schi:ining, U., 95, 147, 148, 176, 197, 
217, 218, 255,262 
Seiferas, J., 234 
Selman, A., 147, 148, 177, 197, 
198, 217 
Shaw, RA., 118 
Siefkes, D., 262 
Simon,1. 61, 197,234 
Simons, R.A., 118 
Sipser, M., 117, 177,233, 
234,255 
Snir, M., 118 
Soare, R., 217 
Solovay, R, 176, 177, 197 
Staples, J., 118 
Stimson, M., 61 
Stone, H.S., 32 
Stockmeyer, L., 32, 61, 62, 
94,95, 117, 118 
Sudborough, I.H., 95 
Szelepcsenyi, R, 95, 262 
Szemeredi, E., 96 
Toda, S., 262 
Tompa, M., 94, 95, 119, 198,262 
Toran, J., 176 
Torenvliet. L., 177 
Trahan, 1., 61 
Trakhtenbrot, B.A., 233 
Trotter, W.T., 96 
Ullman, 1., 61, 62, 96 
Ukkonen, E., 133 
Valiant, L., 95 
Venkateswaran, H., 95, 118 
Vishkin, U., 62, 95, 117, 118 
Vitanyi, P.M.B., 234 
Vitter, 1.S., 118 
Wagner, K.W., 95, 96, 118,262 
Watanabe, 0., 133,218 
Wegener, I., 62, 117 
Wechsung, G., 96, 233 
Weise, D., 96 
Wegman, M.N., 255 
Wiedermann, 1., 61 
Wilson, C., 197 
Wigderson, A., 254 
Wrathall, C., 197 
Wyllie, 1., 61 
Xu, M.R., 176, 197 
Yacobi, Y., 148 
Yap, C.K., 132, 133 
Yao, A.C.C., 177 
Yesha, Y., 133, 234 
Young, P., 132, 133 
Zachos, S., 198,218,255 

Symbol Index 
P,99 
NP,236 
NP/poiy, 253 
co-NP, 92 
PSPACE, 54, 99 
PSPACElpoly, 231 
pNP[logJ, 233 
pNP, 233 
PI(A), 193 
NPb,209 
NPb,214 
NPb(A), 187,233 
NPI(A), 193 
PQUERY(A), 180 
NPQUERY(A), 184 
NP-operator, 255 
NP proof-system, 236 
PQH,196 
AWGSPACE, 70 
APTIME,70 
APSPACE,70 
AEXPTlME, 70 
AEXPSPACE, 70 
AWGTIME,77 
CFL,93 
WGCFL, 94, 95,114,116 
AM[q(n)], 241 
AM,241 
co-AM,254 
AEr, 92 
Allf,92 
Era, 196 
APT, 147 
I-APT, 147 
PC, 118 
PE,118 
BPNP, 242 
BPP, 254 
BP-operator, 255 
BP ·C, 242 
IP[2],239 
IP[poly], 239 
IP[q(n}}, 239 
IP,239 
NPRS(A),216 
ASPACE(S(n», 66 
ASPSZ(z(n),s(n»,93, 114 
ASPTI(s(n), t(n», 93, 110, 114 
ATIMEI(n), 91 
ATIME1(t(n», 68 
DSPACE(O(S(n))), 99, 107, 109 
DTIME(T(n», 226 
NSPACE(S(n», 25 
SPACE(S(n», 27, 53 
TIME(t(n», 66 
ACI , 116 
ACi, 116 
NC,99 
NCi , 99 
Random NC, 118 
PUNC,116 
NC-junclions, 100 
APM-TIME(O(T(n))), 38, 44, 45 
:F-TIME(T(n», 33 
M-RAM-NTIME(nO(l), 54 
NP-RAM-TIME(O(T(n))), 60 
P-RAM-TIME(T(n»,53 
SIMDAG-TIME(O(T(n», 42. 44. 45 

278 
Symbol Index 
UAG-HARDWARE(S(n». 57 
U E-SIZE,DEPTH(20(.(n», t(n». I/O 
UBc-SIZE,DEPTH(z(n), t(n». 99. 107 
VECTOR-TIME(T(n». 21. 25. 27. 38. 45. 
105 
UE.115 
UBc.115 
UE·.115 
UE-uniformity.108 
UBc-uniformity.114 
U E. -uniformity. 108 
ilk-machine. 66 
Ek-machine. 66 
CRCW P-RAM. 54 
CREW P-RAM. 54 
EREW P-RAM. 53 
M-RAM. 54. 61 
P-RAM. 52. 61 
<NCl 116 
-T • 
NCl-reducibility. 116 
~T. 149.150 
~r;p. 149. 150 
~:o .. 243 
~~aj' 254 
<P/log 129 130 
_m 
, 
, 
P / log-m-reducibility. 129 
SN-m-reducibility. 132 
SN-reducibility. 200 
Ln. 200 
LB. 200 
ELlt 230 
ELn.213 
L(i,j).216 
Ln. 215 
Hn. 200. 254 
HH.200 
EHn.213 
H(k,l).216 
fIn. 215 
DEXT-low set. 217 
U.220 
KS[/,g]. 222. 231 
I(S[polyiog,polylog].231 
1([(f,g)lz].223 
KA(w).232 
I(A[f,g].222 
Ku(w).220 
SETS-KS[log, poly]. 232 
SETS-l([/,g].226 
SETS-K[log,poiy]. 227.228.230.232 
SETS-l(A[log,poly].232 
SAT. 92 
k-QBF.131 
CliQUE. 131 
ISO. 236. 254 
NONISO. 239. 255 
QBF.150 
QNR.253 
QR.253 
COUNT. 92 
MAJ. 92 
Q(~f,A,x). 163. 180 
Q(M,A,x,k).187 
Q(M,n).192 
QU(M, n). 192 
Q+(M,A,x,k).188 
Q_(M,A,x,k).188 
A/.78 
CtfMj(x).224 
Fn .92 
Gl ~ G2. 236 
RchM,z(t). 258 
Rch(t(lx\».259 
XA.230 
l(k]Og.93 
3+.255 
(n,g,p, y). 108 
Le./08 
limn A(n). 150. 151. 153. 161. 164. 171 
$.245 
A-gate. 111 
V-gate. 111 
{wwrlw E to, 1}*}. 77. 100 

lin,227 
log, 227 
pad«(x, y}), 122, 123, 124, 125 
poly, 227 
polylog, 227 
ri l , 225 
rL,225 
H r(n),81 
Z~,253 
Step function, 74 
it! round, 237 
l (labeling), 64 
leap(/), 181 
m-segregator, 84 
pre/-level, 189 
prefix(w, r), 252 
SynrnDol Index 
279 

Subject Index 
1-1 a.e. function, 136, 137, 138 
I-invertible function, 119, 120, 123 
1-invertibly paddable set, 123 
Accepting computation tree of an ATM, 
65 
Accessible configuration, 257, 258 
Aggregates, 55, 56, 61 
Alive node, 127 
Almost P-immune set, 144, 145, 146, 
147 
Alternating Turing machine (ATM), 61, 
64, 110 
Alternation bounded alternating Turing 
machine, 66 
Amplification lemma, 244, 254 
AND-TREE, 100 
Approximation algorithm, 146 
Array instructions, 34, 36,37, 57 
Array processing machine (APM) , 34, 
35,37,38,44,45,58,59,61 
Arthur-Merlin proof-system, 241 
Arthur-Merlin protocol, 240 
Bad computation, 254 
Berry paradox, 221 
Binary addition, 116 
Bitwise addition, 245 
Block-respecting machine, 79 
Boolean circuit complexity, 173 
Boolean formula recognition problem, 92 
Boolean formulas with no satisfying 
assignment, 92 
Boolean function in an alternating 
complexity class, 78 
Boolean matrices, 11 
Boolean matrix multiplication, 116 
Bounded-error probabilistic-C class, 242 
BP-operator, 242 
Breadth-first search, 132 
Certificates of a nondeterministic 
machine, 224 
Characteristic function, 230 
Circuit Value Problem, 113, 115, 116 
Clique, 236 
Closure under 5.~Cl, 116 
Closure under complements of 
ASPACE(s(n», 91 
Co-sparse set, 129 
Combinational complexity, 92 
Complexity core, 134, 135, 140, 141, 
142, 143, 147 
Compression function, 232 
Computation graph associated to a 
block-respecting machine, 81 
Computation path for an ATM, 65 
Computation tree of an ATM, 65 
Configuration, 258 
Configuration of an ATM on an input, 64 
Conglomerates, 54, 55, 61 
Conjunctive truth-table reducibility, 132 
Constant Factor Speed-Up for ATM, 91 
Context-free language, 93 
Coset Intersection Problem, 255 
Counting function, 92 
Cylindrification, 122 
C-bi-immune set, 134, 135, 136 
C-immune set, 134 

Dags,78 
Dead node, 127 
Depth-first search, 126, 132 
Detenninistic polynomial space Thring 
reducibility, 149 
Detenninistic polynomial time Thring 
reducibility, 149 
Directed acyclic graphs, 78 
Error probability, 158 
Error probability of an IPS, 251 
Existential states, 63 
Expand, 18, 19 
Exponential density, 143 
Extended connection language, 108 
Extended high, 213 
Extended low, 213 
!-critical word, 159, 160, 162 
!-error machine, 159, 160, 161, 162 
Fan-out of a tree, 65 
Finite-to-one function, 136 
Fork instruction, 53 
Fonnal system, 221 
Godel Incompleteness Theorem, 221 
GOdelization of ATM, 65 
Games against Nature, 255 
Generalized high, 216 
Generalized low, 216 
Graph Hr(n}, 81 
Graph Accessibility Problem, 94 
Graph Isomorphism Problem, 236 
Graph Non-Isomorphism, 239 
Hardware modification machine, 61 
Hardware size, 56 
Hashing lemma, 253 
History of the computation of an IPS, 
252,237 
Honest function, 122, 131 
Honestly paddable set, 145 
Immune set, 168, 170, 171, 173, 175 
Index variables, 6 
Indexing alternating Thring machine, 77 
Subject Index 
281 
Inductive counting, 257 
Inequality of NUN and DUN, 91 
Information contents of a set, 199 
Infonnation contents of a string, 199 
Integer binary multiplication, 116 
Interactive Thring machine (ITM), 237 
Interactive proof-system, 238 
Interactive protocol, 237 
Inverse of a ranking function, 225 
Kolmogorov complexity, 199,220 
Kolmogorov space-bounded complexity, 
222 
Kolmogorov time-bounded complexity, 
222 
k-creative set, 131 
k-offspring parallel random access 
machine, 46 
Language accepted by an ATM, 65 
Language associated with a function, 78 
Length increasing function, 120, 123 
Lexicographically first maximum clique, 
116 
Linear recurrence, 9, 10,32 
Logspace alternating hierarchy, 93, 260 
Majority function, 92 
Majority reduction, 254 
Masking, 16,36,37 
Matrix algebra, 11 
Matrix product, 18, 19,20, 32 
Matrix transposition, 12, 13, IS, 16, 24, 
30,31 
Meager class, 173 
MIMD, 33, 46 
Monotone Circuit Value Problem, 116 
Monotone Planar Circuit Value Problem, 
94 
Move, 237, 240 
Multiplication RAM, 54, 61 
Nondeterministic circuits, 253 
Nondeterministic polynomial time Thring 
reducibility, 149, 152, 154 

282 
Subject Index 
NP-operator, 242 
Number of rounds, 237 
One-side error, 248 
One-sided c-error machine, 175 
Operating in nondeterminism g(n), 166, 
167 
Oracle gates, ll6 
Oracle-restricted positive relativization, 
209,215 
Paddable set, 122, 123, 146 
Padding function, 122 
Parallel Computation Thesis, 4, 33, 45, 
52,61,62 
Parallel RAM, 61 
P-bi-immune set, 146, 147 
P-complete, 116 
p-cylinders, 174 
Peano arithmetic, 221 
Penultimate node, 128 
Perfect-shuffle, 31, 32 
P-immune set, 144 
p-isomorphic sets, 120 
P-Ievelable set, 145 
Polynomial cylinder (or p-cylinder), 122, 
123, 124, 125, 126 
Polynomial query hierarchy, 196 
Polynomial size circuits, 205, 214, 216 
Polynomial space lUring reducibility, 
152, 154 
Polynomial time lUring reducibility, 152 
Polynomial time alternation hierarchy, 92 
Polynomial time hierarchy, 242, 248, 254 
Positive lUring reducibility, 243 
Positive relativization, 179, 209 
p-printable set, 228, 232 
Predecessor node, 84 
Prime factorization, 221 
Prime numbers, 221 
Probabilistic nondeterministic lUring 
machine, 241 
Probabilistic quantifiers, 255 
Probability 1 class, 173 
Product of two numbers, 16 
Productive function, 131 
Proper complexity core, 143, 147 
Prover, 236, 238 
P-to-finite function, 147 
Pseudo-sparse set, 216 
P-uniform circuits, 116 
Quadratic nonresidue, 253 
Quantifier simulation lemma, 245 
q-round Arthur-Merlin proof-system, 241 
q-round interactive proof-system, 238 
Random Oracle Hypothesis, 173 
Ranking function, 225, 228 
Recursive doubling, 6, 9, 10, 16, 19, 20, 
21, 24, 36, 37,39,5 
Recursive folding, 9, ll, 16, 36 
Recursively enumerable sets, 91 
Reduce, 18 
Relativizing principle, 174, 180,210 
Robustly strong oracle machines, 216 
Savitch's Theorem, 70, 184 
Self-p-printable set, 228, 232 
Self-p-rankable set, 232 
Self-producible circuits, 213, 214, 217, 
230 
Self-reducible set, 207,208, 216 
Separations for the Arthur-Merlin 
hierarchy, 256 
Signature register, 39, 41, 42 
SIMD,4 
SIMDAG, 33, 39, 40, 41, 42, 45, 52, 55, 
59,61, 115 
Simple certificates, 224 
Simple set, 170, 171, 175 
Size and depth complexity of a set, 98 
Size, 238 
Slow diagonalization, 168, 170 
SmalI generators, 253 
Space Hierarchy Theorem for ATM, 91 
Space bounded ATM, 66 
Sparse set, 126, 129, 130, 139. 147, 199, 
204,213,216,226 
Special case solution, 146 
Speed-up theorem for the AM hierarchy, 
255 

Splitting, 135, 136 
Standard encoding, 98, 108 
Step for an ATM, 64 
Storage modification machines, 61 
Strong amplification lemma, 253 
Strong nondeterministic machine, 257 
Strong nondeterministic oracle machine, 
216 
Strong separation, 168, 171, 173 
Strongly bi-immune set, 137, 139, 146 
Subexponeruial density, 138, 139 
Symmetric density, 139 
Tally set, 129, 228, 230 
Tape Compression for block-respecting 
machines, 79 
Tarski-Kuratowski algorithm, 204 
Text of the computation, 237 
Time Hierarchy Theorem for ATM, 68 
Time bounded ATM, 66 
Transitive closure, 12, 20, 26, 102 
Tree machine k-PRAM, 46, 47, 48, 49, 
50,51,52,60,61 
Tree-size bound, 93 
t-hard inputs, 140 
Uniform Diagonalization Theorem, 174, 
202 
Uniform circuits, 98 
Union of dags, 78 
Universal Thring machine, 220 
Universal states, 63 
Vector instructions,S 
Vector machine,S, 103 
Vector variables, 6 
Vectorization, 57 
Verifier, 236, 238 
Wait and see argument, 168 
Zero-knowledge proofs, 256 
Subject Index 
283 

EATCS Monographs on Theoretical Computer Science 
Vol. 1: K. Mehlhorn 
Data Structures and Algorltbms 1: 
Sorting and Searching 
Vol.2: K. Mehlhorn 
Data Structures and Algorithms 2: 
Graph Algorithms and NP·Completeness 
Vol.3: K.Mehlhom 
Data Structures and Algorltbms 3: 
Multidimensional Searching and 
Computational Geometry 
Vol.4: W. Reisig 
Petri Nels 
An Introduction 
Vol.5: W. Kuicb, A. Salomaa 
Semirings, Automata, Languages 
Vol.6: H. Ehrig, B. Mahr 
Fundamentals or Algebraic Specification 1 
Equations and Initial Semantics 
Vol. 7: F. G&seg 
Products or Automata 
Vol.8: F. KrOger 
Temporal Logic or Programs 
Vol. 9: K. Weihrauch 
Computability 
Vol. 10: H. EdelsbruMeI' 
Algorithms in Combinatorial Geometry 
Vol. 11: J. L. Balcmr, J. Di'az. J. Gabarr6 
Structural Complexity I 
Vol. 12: J. Berste!, C. Reutenaucr 
Rational Series and Their Languages 
Vol. 13: E. Best, C. Fernandez C. 
Nonsequential Processes 
A Petri Net View 
Vol. 14: M. Jantzen 
Connuent String Rewriting 
Vol. 15: S. Sippu, E. Soisalon·Soininen 
Parsing Theory 
Volume I: Languages and Parsing 
Vol. 16: P. PadawilZ 
Computing in Hom Clause Theories 
Vol. 17: J. Paredaens, P. De Bra, M. GyssCllS, 
D. VanGucht 
The Structure or the Relational Database 
Model 
Vol. 18: J. Dassow, G. Piun 
Regulated Rewriting in Formal Language 
Theory 
Vol. 19: M. Tofte 
Compiler Generators 
Wbat tbey can do, wbat they mlgbt do, 
and wbat tbey will probably never do 
Vol.20: S. Sippu, E. Soisalon-Soinincn 
Parsing Theory 
Volume ll: LR(k) and LL(k) Parsing 
Vol.21: H. Ehrig, B. Mahr 
Fundamentals or Algebraic Specification 1 
Module Speclflcatlons and Constraints 
Vol. 22: J. L. Balcmr, J, Draz. J. Gabarr6 
Structural Complexity II 


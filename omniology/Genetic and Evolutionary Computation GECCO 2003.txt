Lecture Notes in Computer Science
2723
Edited by G. Goos, J. Hartmanis, and J. van Leeuwen

3
Berlin
Heidelberg
New York
Hong Kong
London
Milan
Paris
Tokyo

Erick Cant´u-Paz James A. Foster
Kalyanmoy Deb Lawrence David Davis
Rajkumar Roy Una-May O’Reilly
Hans-Georg Beyer Russell Standish
Graham Kendall Stewart Wilson
Mark Harman Joachim Wegener
Dipankar Dasgupta Mitch A. Potter
Alan C. Schultz Kathryn A. Dowsland
Natasha Jonoska Julian Miller (Eds.)
Genetic and
Evolutionary Computation –
GECCO 2003
Genetic and Evolutionary Computation Conference
Chicago, IL, USA, July 12-16, 2003
Proceedings, Part I
1 3

Series Editors
Gerhard Goos, Karlsruhe University, Germany
Juris Hartmanis, Cornell University, NY, USA
Jan van Leeuwen, Utrecht University, The Netherlands
Main Editor
Erick Cant´u-Paz
Center for Applied Scientiﬁc Computing (CASC)
Lawrence Livermore National Laboratory
7000 East Avenue, L-561, Livermore, CA 94550, USA
E-mail: cantupaz@llnl.gov
Cataloging-in-Publication Data applied for
A catalog record for this book is available from the Library of Congress
Bibliographic information published by Die Deutsche Bibliothek
Die Deutsche Bibliothek lists this publication in the Deutsche Nationalbibliograﬁe;
detailed bibliographic data is available in the Internet at <http://dnb.ddb.de>.
CR Subject Classiﬁcation (1998): F.1-2, D.1.3, C.1.2, I.2.6, I.2.8, I.2.11, J.3
ISSN 0302-9743
ISBN 3-540-40602-6 Springer-Verlag Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer-Verlag. Violations are
liable for prosecution under the German Copyright Law.
Springer-Verlag Berlin Heidelberg New York
a member of BertelsmannSpringer Science+Business Media GmbH
http://www.springer.de
© Springer-Verlag Berlin Heidelberg 2003
Printed in Germany
Typesetting: Camera-ready by author, data conversion by PTP Berlin GmbH
Printed on acid-free paper
SPIN 10928998
06/3142
5 4 3 2 1 0

Preface
These proceedings contain the papers presented at the 5th Annual Genetic and
Evolutionary Computation Conference (GECCO 2003). The conference was held
in Chicago, USA, July 12–16, 2003.
A total of 417 papers were submitted to GECCO 2003. After a rigorous
doubleblind reviewing process, 194 papers were accepted for full publication and
oral presentation at the conference, resulting in an acceptance rate of 46.5%.
An additional 92 submissions were accepted as posters with two-page extended
abstracts included in these proceedings.
This edition of GECCO was the union of the 8th Annual Genetic Program-
ming Conference (which has met annually since 1996) and the 12th International
Conference on Genetic Algorithms (which, with its ﬁrst meeting in 1985, is the
longest running conference in the ﬁeld). Since 1999, these conferences have mer-
ged to produce a single large meeting that welcomes an increasingly wide array
of topics related to genetic and evolutionary computation.
Possibly the most visible innovation in GECCO 2003 was the publication of
the proceedings with Springer-Verlag as part of their Lecture Notes in Computer
Science series. This will make the proceedings available in many libraries as well
as online, widening the dissemination of the research presented at the conference.
Other innovations included a new track on Coevolution and Artiﬁcial Immune
Systems and the expansion of the DNA and Molecular Computing track to
include quantum computation.
In addition to the presentation of the papers contained in these proceedings,
the conference included 13 workshops, 32 tutorials by leading specialists, and
presentation of late-breaking papers.
GECCO is sponsored by the International Society for Genetic and Evolutio-
nary Computation (ISGEC). The ISGEC by-laws contain explicit guidance on
the organization of the conference, including the following principles:
(i) GECCO should be a broad-based conference encompassing the whole ﬁeld
of genetic and evolutionary computation.
(ii) Papers will be published and presented as part of the main conference
proceedings only after being peer-reviewed. No invited papers shall be published
(except for those of up to three invited plenary speakers).
(iii) The peer-review process shall be conducted consistently with the prin-
ciple of division of powers performed by a multiplicity of independent program
committees, each with expertise in the area of the paper being reviewed.
(iv) The determination of the policy for the peer-review process for each of
the conference’s independent program committees and the reviewing of papers
for each program committee shall be performed by persons who occupy their
positions by virtue of meeting objective and explicitly stated qualiﬁcations based
on their previous research activity.

VIII
Preface
(v) Emerging areas within the ﬁeld of genetic and evolutionary computation
shall be actively encouraged and incorporated in the activities of the conference
by providing a semiautomatic method for their inclusion (with some procedural
ﬂexibility extended to such emerging new areas).
(vi) The percentage of submitted papers that are accepted as regular full-
length papers (i.e., not posters) shall not exceed 50%.
These principles help ensure that GECCO maintains high quality across the
diverse range of topics it includes.
Besides sponsoring the conference, ISGEC supports the ﬁeld in other ways.
ISGEC sponsors the biennial Foundations of Genetic Algorithms workshop on
theoretical aspects of all evolutionary algorithms. The journals Evolutionary
Computation and Genetic Programming and Evolvable Machines are also sup-
ported by ISGEC. All ISGEC members (including students) receive subscriptions
to these journals as part of their membership. ISGEC membership also includes
discounts on GECCO and FOGA registration rates as well as discounts on other
journals. More details on ISGEC can be found online at http://www.isgec.org.
Many people volunteered their time and energy to make this conference a
success. The following people in particular deserve the gratitude of the entire
community for their outstanding contributions to GECCO:
James A. Foster, the General Chair of GECCO for his tireless eﬀorts in organi-
zing every aspect of the conference.
David E. Goldberg and John Koza, members of the Business Committee, for
their guidance and ﬁnancial oversight.
Alwyn Barry, for coordinating the workshops.
Bart Rylander, for editing the late-breaking papers.
Past conference organizers, William B. Langdon, Erik Goodman, and Darrell
Whitley, for their advice.
Elizabeth Ericson, Carol Hamilton, Ann Stolberg, and the rest of the AAAI staﬀ
for their outstanding eﬀorts administering the conference.
Gerardo Valencia and Gabriela Coronado, for Web programming and design.
Jennifer Ballentine, Lee Ballentine and the staﬀof Professional Book Center, for
assisting in the production of the proceedings.
Alfred Hofmann and Ursula Barth of Springer-Verlag for helping to ease the
transition to a new publisher.
Sponsors who made generous contributions to support student travel grants:
Air Force Oﬃce of Scientiﬁc Research
DaimlerChrysler
National Science Foundation
Naval Research Laboratory
New Light Industries
Philips Research
Sun Microsystems

Preface
IX
The track chairs deserve special thanks. Their eﬀorts in recruiting program
committees, assigning papers to reviewers, and making diﬃcult acceptance de-
cisions in relatively short times, were critical to the success of the conference:
A-Life, Adaptive Behavior, Agents, and Ant Colony Optimization,
Russell Standish
Artiﬁcial Immune Systems, Dipankar Dasgupta
Coevolution, Graham Kendall
DNA, Molecular, and Quantum Computing, Natasha Jonoska
Evolution Strategies, Evolutionary Programming, Hans-Georg Beyer
Evolutionary Robotics, Alan Schultz, Mitch Potter
Evolutionary Scheduling and Routing, Kathryn A. Dowsland
Evolvable Hardware, Julian Miller
Genetic Algorithms, Kalyanmoy Deb
Genetic Programming, Una-May O’Reilly
Learning Classiﬁer Systems, Stewart Wilson
Real-World Applications, David Davis, Rajkumar Roy
Search-Based Software Engineering, Mark Harman, Joachim Wegener
The conference was held in cooperation and/or aﬃliation with:
American Association for Artiﬁcial Intelligence (AAAI)
Evonet: the Network of Excellence in Evolutionary Computation
5th NASA/DoD Workshop on Evolvable Hardware
Evolutionary Computation
Genetic Programming and Evolvable Machines
Journal of Scheduling
Journal of Hydroinformatics
Applied Soft Computing
Of course, special thanks are due to the numerous researchers who submitted
their best work to GECCO, reviewed the work of others, presented a tutorial,
organized a workshop, or volunteered their time in any other way. I am sure you
will be proud of the results of your eﬀorts.
May 2003
Erick Cant´u-Paz
Editor-in-Chief GECCO 2003
Center for Applied Scientiﬁc Computing
Lawrence Livermore National Laboratory

Table of Contents
Volume I
A-Life, Adaptive Behavior, Agents, and
Ant Colony Optimization
Swarms in Dynamic Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
T.M. Blackwell
The Eﬀect of Natural Selection on Phylogeny Reconstruction
Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
Dehua Hang, Charles Ofria, Thomas M. Schmidt, Eric Torng
AntClust: Ant Clustering and Web Usage Mining . . . . . . . . . . . . . . . . . . . . .
25
Nicolas Labroche, Nicolas Monmarch´e, Gilles Venturini
A Non-dominated Sorting Particle Swarm Optimizer for
Multiobjective Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
Xiaodong Li
The Inﬂuence of Run-Time Limits on Choosing Ant System
Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
Krzysztof Socha
Emergence of Collective Behavior in Evolving Populations of
Flying Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
Lee Spector, Jon Klein, Chris Perry, Mark Feinstein
On Role of Implicit Interaction and Explicit Communications in
Emergence of Social Behavior in Continuous Predators-Prey
Pursuit Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
Ivan Tanev, Katsunori Shimohara
Demonstrating the Evolution of Complex Genetic
Representations: An Evolution of Artiﬁcial Plants . . . . . . . . . . . . . . . . . . . . .
86
Marc Toussaint
Sexual Selection of Co-operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
M. Afzal Upal
Optimization Using Particle Swarms with Near Neighbor Interactions . . .
110
Kalyan Veeramachaneni, Thanmaya Peram, Chilukuri Mohan,
Lisa Ann Osadciw

XXVI
Table of Contents
Revisiting Elitism in Ant Colony Optimization . . . . . . . . . . . . . . . . . . . . . . .
122
Tony White, Simon Kaegi, Terri Oda
A New Approach to Improve Particle Swarm Optimization . . . . . . . . . . . . .
134
Liping Zhang, Huanjun Yu, Shangxu Hu
A-Life, Adaptive Behavior, Agents, and Ant Colony
Optimization – Posters
Clustering and Dynamic Data Visualization with Artiﬁcial Flying
Insect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
S. Aupetit, N. Monmarch´e, M. Slimane, C. Guinot, G. Venturini
Ant Colony Programming for Approximation Problems . . . . . . . . . . . . . . . .
142
Mariusz Boryczka, Zbigniew J. Czech, Wojciech Wieczorek
Long-Term Competition for Light in Plant Simulation . . . . . . . . . . . . . . . . .
144
Claude Lattaud
Using Ants to Attack a Classical Cipher . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
Matthew Russell, John A. Clark, Susan Stepney
Comparison of Genetic Algorithm and Particle Swarm Optimizer When
Evolving a Recurrent Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
Matthew Settles, Brandon Rodebaugh, Terence Soule
Adaptation and Ruggedness in an Evolvability Landscape . . . . . . . . . . . . . .
150
Terry Van Belle, David H. Ackley
Study Diploid System by a Hamiltonian Cycle Problem Algorithm . . . . . .
152
Dong Xianghui, Dai Ruwei
A Possible Mechanism of Repressing Cheating Mutants
in Myxobacteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
Ying Xiao, Winfried Just
Tour Jet´e, Pirouette: Dance Choreographing by Computers . . . . . . . . . . . .
156
Tina Yu, Paul Johnson
Multiobjective Optimization Using Ideas from the Clonal Selection
Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
Nareli Cruz Cort´es, Carlos A. Coello Coello
Artiﬁcial Immune Systems
A Hybrid Immune Algorithm with Information Gain for the Graph
Coloring Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
Vincenzo Cutello, Giuseppe Nicosia, Mario Pavone

Table of Contents
XXVII
MILA – Multilevel Immune Learning Algorithm . . . . . . . . . . . . . . . . . . . . . .
183
Dipankar Dasgupta, Senhua Yu, Nivedita Sumi Majumdar
The Eﬀect of Binary Matching Rules in Negative Selection . . . . . . . . . . . . .
195
Fabio Gonz´alez, Dipankar Dasgupta, Jonatan G´omez
Immune Inspired Somatic Contiguous Hypermutation for Function
Optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
Johnny Kelsey, Jon Timmis
A Scalable Artiﬁcial Immune System Model for Dynamic
Unsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
Olfa Nasraoui, Fabio Gonzalez, Cesar Cardona, Carlos Rojas,
Dipankar Dasgupta
Developing an Immunity to Spam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
Terri Oda, Tony White
Artiﬁcial Immune Systems – Posters
A Novel Immune Anomaly Detection Technique Based on Negative
Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
F. Ni˜no, D. G´omez, R. Vejar
Visualization of Topic Distribution Based on Immune Network Model . . .
246
Yasufumi Takama
Spatial Formal Immune Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
248
Alexander O. Tarakanov
Coevolution
Focusing versus Intransitivity (Geometrical Aspects of Co-evolution) . . . .
250
Anthony Bucci, Jordan B. Pollack
Representation Development from Pareto-Coevolution . . . . . . . . . . . . . . . . .
262
Edwin D. de Jong
Learning the Ideal Evaluation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
Edwin D. de Jong, Jordan B. Pollack
A Game-Theoretic Memory Mechanism for Coevolution . . . . . . . . . . . . . . . .
286
Sevan G. Ficici, Jordan B. Pollack
The Paradox of the Plankton: Oscillations and Chaos in
Multispecies Evolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
Jeﬀrey Horn, James Cattron

XXVIII
Table of Contents
Exploring the Explorative Advantage of the Cooperative
Coevolutionary (1+1) EA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
Thomas Jansen, R. Paul Wiegand
PalmPrints: A Novel Co-evolutionary Algorithm for Clustering
Finger Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
322
Nawwaf Kharma, Ching Y. Suen, Pei F. Guo
Coevolution and Linear Genetic Programming for Visual Learning . . . . . .
332
Krzysztof Krawiec and Bir Bhanu
Finite Population Models of Co-evolution and Their Application to
Haploidy versus Diploidy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
344
Anthony M.L. Liekens, Huub M.M. ten Eikelder, Peter A.J. Hilbers
Evolving Keepaway Soccer Players through Task Decomposition . . . . . . . .
356
Shimon Whiteson, Nate Kohl, Risto Miikkulainen, Peter Stone
Coevolution – Posters
A New Method of Multilayer Perceptron Encoding . . . . . . . . . . . . . . . . . . . .
369
Emmanuel Blindauer, Jerzy Korczak
An Incremental and Non-generational Coevolutionary Algorithm . . . . . . . .
371
Ram´on Alfonso Palacios-Durazo, Manuel Valenzuela-Rend´on
Coevolutionary Convergence to Global Optima . . . . . . . . . . . . . . . . . . . . . . .
373
Lothar M. Schmitt
Generalized Extremal Optimization for Solving Complex Optimal
Design Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
Fabiano Luis de Sousa, Valeri Vlassov, Fernando Manuel Ramos
Coevolving Communication and Cooperation for Lattice
Formation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
Jekanthan Thangavelautham, Timothy D. Barfoot,
Gabriele M.T. D’Eleuterio
DNA, Molecular, and Quantum Computing
Eﬃciency and Reliability of DNA-Based Memories . . . . . . . . . . . . . . . . . . . .
379
Max H. Garzon, Andrew Neel, Hui Chen
Evolving Hogg’s Quantum Algorithm Using Linear-Tree GP . . . . . . . . . . . .
390
Andr´e Leier, Wolfgang Banzhaf
Hybrid Networks of Evolutionary Processors. . . . . . . . . . . . . . . . . . . . . . . . . .
401
Carlos Mart´ın-Vide, Victor Mitrana, Mario J. P´erez-Jim´enez,
Fernando Sancho-Caparrini

Table of Contents
XXIX
DNA-Like Genomes for Evolution in silico . . . . . . . . . . . . . . . . . . . . . . . . . . .
413
Michael West, Max H. Garzon, Derrel Blain
DNA, Molecular, and Quantum Computing – Posters
String Binding-Blocking Automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
425
M. Sakthi Balan
On Setting the Parameters of QEA for Practical Applications: Some
Guidelines Based on Empirical Evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
427
Kuk-Hyun Han, Jong-Hwan Kim
Evolutionary Two-Dimensional DNA Sequence Alignment . . . . . . . . . . . . . .
429
Edgar E. Vallejo, Fernando Ramos
Evolvable Hardware
Active Control of Thermoacoustic Instability in a Model Combustor
with Neuromorphic Evolvable Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
431
John C. Gallagher, Saranyan Vigraham
Hardware Evolution of Analog Speed Controllers for a DC Motor . . . . . . .
442
David A. Gwaltney, Michael I. Ferguson
Evolvable Hardware – Posters
An Examination of Hypermutation and Random Immigrant Variants of
mrCGA for Dynamic Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
454
Gregory R. Kramer, John C. Gallagher
Inherent Fault Tolerance in Evolved Sorting Networks . . . . . . . . . . . . . . . . .
456
Rob Shepherd and James Foster
Evolutionary Robotics
Co-evolving Task-Dependent Visual Morphologies in Predator-Prey
Experiments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
458
Gunnar Buason, Tom Ziemke
Integration of Genetic Programming and Reinforcement Learning for
Real Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
470
Shotaro Kamio, Hideyuki Mitsuhashi, Hitoshi Iba
Multi-objectivity as a Tool for Constructing Hierarchical Complexity . . . .
483
Jason Teo, Minh Ha Nguyen, Hussein A. Abbass
Learning Biped Locomotion from First Principles on a Simulated
Humanoid Robot Using Linear Genetic Programming. . . . . . . . . . . . . . . . . .
495
Krister Wolﬀ, Peter Nordin

XXX
Table of Contents
Evolutionary Robotics – Posters
An Evolutionary Approach to Automatic Construction of
the Structure in Hierarchical Reinforcement Learning . . . . . . . . . . . . . . . . . .
507
Stefan Elfwing, Eiji Uchibe, Kenji Doya
Fractional Order Dynamical Phenomena in a GA . . . . . . . . . . . . . . . . . . . . .
510
E.J. Solteiro Pires, J.A. Tenreiro Machado, P.B. de Moura Oliveira
Evolution Strategies/Evolutionary Programming
Dimension-Independent Convergence Rate for Non-isotropic
(1, λ) −ES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
512
Anne Auger, Claude Le Bris, Marc Schoenauer
The Steady State Behavior of (µ/µI, λ)-ES on Ellipsoidal Fitness
Models Disturbed by Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
Hans-Georg Beyer, Dirk V. Arnold
Theoretical Analysis of Simple Evolution Strategies in Quickly
Changing Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
537
J¨urgen Branke, Wei Wang
Evolutionary Computing as a Tool for Grammar Development . . . . . . . . . .
549
Guy De Pauw
Solving Distributed Asymmetric Constraint Satisfaction Problems
Using an Evolutionary Society of Hill-Climbers . . . . . . . . . . . . . . . . . . . . . . .
561
Gerry Dozier
Use of Multiobjective Optimization Concepts to Handle Constraints
in Single-Objective Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
573
Arturo Hern´andez Aguirre, Salvador Botello Rionda,
Carlos A. Coello Coello, Giovanni Liz´arraga Liz´arraga
Evolution Strategies with Exclusion-Based Selection Operators
and a Fourier Series Auxiliary Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
585
Kwong-Sak Leung, Yong Liang
Ruin and Recreate Principle Based Approach for the Quadratic
Assignment Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
598
Alfonsas Misevicius
Model-Assisted Steady-State Evolution Strategies . . . . . . . . . . . . . . . . . . . . .
610
Holger Ulmer, Felix Streichert, Andreas Zell
On the Optimization of Monotone Polynomials by the (1+1) EA and
Randomized Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
622
Ingo Wegener, Carsten Witt

Table of Contents
XXXI
Evolution Strategies/Evolutionary Programming –
Posters
A Forest Representation for Evolutionary Algorithms Applied to
Network Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
634
A.C.B. Delbem, Andre de Carvalho
Solving Three-Objective Optimization Problems Using Evolutionary
Dynamic Weighted Aggregation: Results and Analysis . . . . . . . . . . . . . . . . .
636
Yaochu Jin, Tatsuya Okabe, Bernhard Sendhoﬀ
The Principle of Maximum Entropy-Based Two-Phase Optimization of
Fuzzy Controller by Evolutionary Programming. . . . . . . . . . . . . . . . . . . . . . .
638
Chi-Ho Lee, Ming Yuchi, Hyun Myung, Jong-Hwan Kim
A Simple Evolution Strategy to Solve Constrained Optimization
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
640
Efr´en Mezura-Montes, Carlos A. Coello Coello
Eﬀective Search of the Energy Landscape for Protein Folding . . . . . . . . . . .
642
Eugene Santos Jr., Keum Joo Kim, Eunice E. Santos
A Clustering Based Niching Method for Evolutionary Algorithms . . . . . . .
644
Felix Streichert, Gunnar Stein, Holger Ulmer, Andreas Zell
Evolutionary Scheduling Routing
A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing
Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
646
Jean Berger, Mohamed Barkaoui
An Evolutionary Approach to Capacitated Resource Distribution by
a Multiple-agent Team. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
657
Mudassar Hussain, Bahram Kimiaghalam, Abdollah Homaifar,
Albert Esterline, Bijan Sayyarodsari
A Hybrid Genetic Algorithm Based on Complete Graph
Representation for the Sequential Ordering Problem . . . . . . . . . . . . . . . . . . .
669
Dong-Il Seo, Byung-Ro Moon
An Optimization Solution for Packet Scheduling: A Pipeline-Based
Genetic Algorithm Accelerator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
681
Shiann-Tsong Sheu, Yue-Ru Chuang, Yu-Hung Chen, Eugene Lai
Evolutionary Scheduling Routing – Posters
Generation and Optimization of Train Timetables Using Coevolution . . . .
693
Paavan Mistry, Raymond S.K. Kwan

XXXII
Table of Contents
Genetic Algorithms
Chromosome Reuse in Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . .
695
Adnan Acan, Y¨uce Tekol
Real-Parameter Genetic Algorithms for Finding Multiple Optimal
Solutions in Multi-modal Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
706
Pedro J. Ballester, Jonathan N. Carter
An Adaptive Penalty Scheme for Steady-State Genetic Algorithms . . . . . .
718
Helio J.C. Barbosa, Afonso C.C. Lemonge
Asynchronous Genetic Algorithms for Heterogeneous Networks
Using Coarse-Grained Dataﬂow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
730
John W. Baugh Jr., Sujay V. Kumar
A Generalized Feedforward Neural Network Architecture and Its
Training Using Two Stochastic Search Methods . . . . . . . . . . . . . . . . . . . . . . .
742
Abdesselam Bouzerdoum, Rainer Mueller
Ant-Based Crossover for Permutation Problems . . . . . . . . . . . . . . . . . . . . . . .
754
J¨urgen Branke, Christiane Barz, Ivesa Behrens
Selection in the Presence of Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
766
J¨urgen Branke, Christian Schmidt
Eﬀective Use of Directional Information in Multi-objective
Evolutionary Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
778
Martin Brown, R.E. Smith
Pruning Neural Networks with Distribution Estimation Algorithms . . . . . .
790
Erick Cant´u-Paz
Are Multiple Runs of Genetic Algorithms Better than One? . . . . . . . . . . . .
801
Erick Cant´u-Paz, David E. Goldberg
Constrained Multi-objective Optimization Using Steady State
Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
813
Deepti Chafekar, Jiang Xuan, Khaled Rasheed
An Analysis of a Reordering Operator with Tournament Selection on
a GA-Hard Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
825
Ying-Ping Chen, David E. Goldberg
Tightness Time for the Linkage Learning Genetic Algorithm . . . . . . . . . . . .
837
Ying-Ping Chen, David E. Goldberg
A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem . . . . . . .
850
Heemahn Choe, Sung-Soon Choi, Byung-Ro Moon

Table of Contents
XXXIII
Normalization in Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
862
Sung-Soon Choi and Byung-Ro Moon
Coarse-Graining in Genetic Algorithms: Some Issues and Examples. . . . . .
874
Andr´es Aguilar Contreras, Jonathan E. Rowe,
Christopher R. Stephens
Building a GA from Design Principles for Learning Bayesian Networks. . .
886
Steven van Dijk, Dirk Thierens, Linda C. van der Gaag
A Method for Handling Numerical Attributes in GA-Based Inductive
Concept Learners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
898
Federico Divina, Maarten Keijzer, Elena Marchiori
Analysis of the (1+1) EA for a Dynamically Bitwise Changing
OneMax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
909
Stefan Droste
Performance Evaluation and Population Reduction for a Self
Adaptive Hybrid Genetic Algorithm (SAHGA) . . . . . . . . . . . . . . . . . . . . . . .
922
Felipe P. Espinoza, Barbara S. Minsker, David E. Goldberg
Schema Analysis of Average Fitness in Multiplicative Landscape . . . . . . . .
934
Hiroshi Furutani
On the Treewidth of NK Landscapes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
948
Yong Gao, Joseph Culberson
Selection Intensity in Asynchronous Cellular Evolutionary Algorithms . . .
955
Mario Giacobini, Enrique Alba, Marco Tomassini
A Case for Codons in Evolutionary Algorithms . . . . . . . . . . . . . . . . . . . . . . .
967
Joshua Gilbert, Maggie Eppstein
Natural Coding: A More Eﬃcient Representation for Evolutionary
Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
979
Ra´ul Gir´aldez, Jes´us S. Aguilar-Ruiz, Jos´e C. Riquelme
Hybridization of Estimation of Distribution Algorithms with a
Repair Method for Solving Constraint Satisfaction Problems . . . . . . . . . . .
991
Hisashi Handa
Eﬃcient Linkage Discovery by Limited Probing . . . . . . . . . . . . . . . . . . . . . . . 1003
Robert B. Heckendorn, Alden H. Wright
Distributed Probabilistic Model-Building Genetic Algorithm . . . . . . . . . . . . 1015
Tomoyuki Hiroyasu, Mitsunori Miki, Masaki Sano, Hisashi Shimosaka,
Shigeyoshi Tsutsui, Jack Dongarra

XXXIV
Table of Contents
HEMO: A Sustainable Multi-objective Evolutionary Optimization
Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1029
Jianjun Hu, Kisung Seo, Zhun Fan, Ronald C. Rosenberg,
Erik D. Goodman
Using an Immune System Model to Explore Mate Selection in Genetic
Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1041
Chien-Feng Huang
Designing A Hybrid Genetic Algorithm for the Linear
Ordering Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1053
Gaofeng Huang, Andrew Lim
A Similarity-Based Mating Scheme for Evolutionary Multiobjective
Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1065
Hisao Ishibuchi, Youhei Shibata
Evolutionary Multiobjective Optimization for Generating an
Ensemble of Fuzzy Rule-Based Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1077
Hisao Ishibuchi, Takashi Yamamoto
Voronoi Diagrams Based Function Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . 1089
Carlos Kavka, Marc Schoenauer
New Usage of SOM for Genetic Algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . 1101
Jung-Hwan Kim, Byung-Ro Moon
Problem-Independent Schema Synthesis for Genetic Algorithms . . . . . . . . . 1112
Yong-Hyuk Kim, Yung-Keun Kwon, Byung-Ro Moon
Investigation of the Fitness Landscapes and Multi-parent
Crossover for Graph Bipartitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1123
Yong-Hyuk Kim, Byung-Ro Moon
New Usage of Sammon’s Mapping for Genetic Visualization . . . . . . . . . . . . 1136
Yong-Hyuk Kim, Byung-Ro Moon
Exploring a Two-Population Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . 1148
Steven Orla Kimbrough, Ming Lu, David Harlan Wood, D.J. Wu
Adaptive Elitist-Population Based Genetic Algorithm for
Multimodal Function Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1160
Kwong-Sak Leung, Yong Liang
Wise Breeding GA via Machine Learning Techniques for Function
Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1172
Xavier Llor`a, David E. Goldberg

Table of Contents
XXXV
Facts and Fallacies in Using Genetic Algorithms for Learning
Clauses in First-Order Logic. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1184
Flaviu Adrian M˘arginean
Comparing Evolutionary Computation Techniques via
Their Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1196
Boris Mitavskiy
Dispersion-Based Population Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . 1210
Ronald W. Morrison
A Parallel Genetic Algorithm Based on Linkage Identiﬁcation . . . . . . . . . . 1222
Masaharu Munetomo, Naoya Murao, Kiyoshi Akama
Generalization of Dominance Relation-Based Replacement Rules for
Memetic EMO Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1234
Tadahiko Murata, Shiori Kaige, Hisao Ishibuchi
Author Index
Volume II
Genetic Algorithms (continued)
Design of Multithreaded Estimation of Distribution Algorithms . . . . . . . . . 1247
Jiri Ocenasek, Josef Schwarz, Martin Pelikan
Reinforcement Learning Estimation of Distribution Algorithm . . . . . . . . . . 1259
Topon Kumar Paul, Hitoshi Iba
Hierarchical BOA Solves Ising Spin Glasses and MAXSAT . . . . . . . . . . . . . 1271
Martin Pelikan, David E. Goldberg
ERA: An Algorithm for Reducing the Epistasis of SAT Problems . . . . . . . 1283
Eduardo Rodriguez-Tello, Jose Torres-Jimenez
Learning a Procedure That Can Solve Hard Bin-Packing Problems:
A New GA-Based Approach to Hyper-heuristics . . . . . . . . . . . . . . . . . . . . . . 1295
Peter Ross, Javier G. Mar´ın-Bl´azquez, Sonia Schulenburg, Emma Hart
Population Sizing for the Redundant Trivial Voting Mapping . . . . . . . . . . . 1307
Franz Rothlauf
Non-stationary Function Optimization Using Polygenic Inheritance . . . . . . 1320
Conor Ryan, J.J. Collins, David Wallin

XXXVI
Table of Contents
Scalability of Selectorecombinative Genetic Algorithms for
Problems with Tight Linkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1332
Kumara Sastry, David E. Goldberg
New Entropy-Based Measures of Gene Signiﬁcance and Epistasis . . . . . . . 1345
Dong-Il Seo, Yong-Hyuk Kim, Byung-Ro Moon
A Survey on Chromosomal Structures and Operators for Exploiting
Topological Linkages of Genes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1357
Dong-Il Seo, Byung-Ro Moon
Cellular Programming and Symmetric Key Cryptography Systems . . . . . . 1369
Franciszek Seredy´nski, Pascal Bouvry, Albert Y. Zomaya
Mating Restriction and Niching Pressure: Results from Agents and
Implications for General EC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1382
R.E. Smith, Claudio Bonacina
EC Theory: A Uniﬁed Viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1394
Christopher R. Stephens, Adolfo Zamora
Real Royal Road Functions for Constant Population Size. . . . . . . . . . . . . . . 1406
Tobias Storch, Ingo Wegener
Two Broad Classes of Functions for Which a No Free Lunch Result
Does Not Hold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1418
Matthew J. Streeter
Dimensionality Reduction via Genetic Value Clustering . . . . . . . . . . . . . . . . 1431
Alexander Topchy, William Punch
The Structure of Evolutionary Exploration: On Crossover,
Buildings Blocks, and Estimation-of-Distribution Algorithms . . . . . . . . . . . 1444
Marc Toussaint
The Virtual Gene Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1457
Manuel Valenzuela-Rend´on
Quad Search and Hybrid Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 1469
Darrell Whitley, Deon Garrett, Jean-Paul Watson
Distance between Populations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1481
Mark Wineberg, Franz Oppacher
The Underlying Similarity of Diversity Measures Used in
Evolutionary Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1493
Mark Wineberg, Franz Oppacher
Implicit Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1505
Alden H. Wright, Michael D. Vose, Jonathan E. Rowe

Table of Contents
XXXVII
Finding Building Blocks through Eigenstructure Adaptation . . . . . . . . . . . . 1518
Danica Wyatt, Hod Lipson
A Specialized Island Model and Its Application in
Multiobjective Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1530
Ningchuan Xiao, Marc P. Armstrong
Adaptation of Length in a Nonstationary Environment . . . . . . . . . . . . . . . . 1541
Han Yu, Annie S. Wu, Kuo-Chi Lin, Guy Schiavone
Optimal Sampling and Speed-Up for Genetic Algorithms on the
Sampled OneMax Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1554
Tian-Li Yu, David E. Goldberg, Kumara Sastry
Building-Block Identiﬁcation by Simultaneity Matrix . . . . . . . . . . . . . . . . . . 1566
Chatchawit Aporntewan, Prabhas Chongstitvatana
A Uniﬁed Framework for Metaheuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1568
J¨urgen Branke, Michael Stein, Hartmut Schmeck
The Hitting Set Problem and Evolutionary Algorithmic Techniques
with ad-hoc Viruses (HEAT-V) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1570
Vincenzo Cutello, Francesco Pappalardo
The Spatially-Dispersed Genetic Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . 1572
Grant Dick
Non-universal Suﬀrage Selection Operators Favor Population
Diversity in Genetic Algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1574
Federico Divina, Maarten Keijzer, Elena Marchiori
Uniform Crossover Revisited: Maximum Disruption in
Real-Coded GAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1576
Stephen Drake
The Master-Slave Architecture for Evolutionary Computations
Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1578
Christian Gagn´e, Marc Parizeau, Marc Dubreuil
Genetic Algorithms – Posters
Using Adaptive Operators in Genetic Search . . . . . . . . . . . . . . . . . . . . . . . . . 1580
Jonatan G´omez, Dipankar Dasgupta, Fabio Gonz´alez
A Kernighan-Lin Local Improvement Heuristic That Solves Some Hard
Problems in Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1582
William A. Greene
GA-Hardness Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1584
Haipeng Guo, William H. Hsu

XXXVIII
Table of Contents
Barrier Trees For Search Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1586
Jonathan Hallam, Adam Pr¨ugel-Bennett
A Genetic Algorithm as a Learning Method Based on Geometric
Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1588
Gregory A. Holiﬁeld, Annie S. Wu
Solving Mastermind Using Genetic Algorithms. . . . . . . . . . . . . . . . . . . . . . . . 1590
Tom Kalisker, Doug Camens
Evolutionary Multimodal Optimization Revisited . . . . . . . . . . . . . . . . . . . . . 1592
Rajeev Kumar, Peter Rockett
Integrated Genetic Algorithm with Hill Climbing for Bandwidth
Minimization Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1594
Andrew Lim, Brian Rodrigues, Fei Xiao
A Fixed-Length Subset Genetic Algorithm for the p-Median Problem . . . . 1596
Andrew Lim, Zhou Xu
Performance Evaluation of a Parameter-Free Genetic Algorithm for
Job-Shop Scheduling Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1598
Shouichi Matsui, Isamu Watanabe, Ken-ichi Tokoro
SEPA: Structure Evolution and Parameter Adaptation in
Feed-Forward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1600
Paulito P. Palmes, Taichi Hayasaka, Shiro Usui
Real-Coded Genetic Algorithm to Reveal Biological Signiﬁcant
Sites of Remotely Homologous Proteins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1602
Sung-Joon Park, Masayuki Yamamura
Understanding EA Dynamics via Population Fitness Distributions . . . . . . 1604
Elena Popovici, Kenneth De Jong
Evolutionary Feature Space Transformation Using Type-Restricted
Generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1606
Oliver Ritthoﬀ, Ralf Klinkenberg
On the Locality of Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1608
Franz Rothlauf
New Subtour-Based Crossover Operator for the TSP . . . . . . . . . . . . . . . . . . 1610
Sang-Moon Soak, Byung-Ha Ahn
Is a Self-Adaptive Pareto Approach Beneﬁcial for Controlling
Embodied Virtual Robots? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1612
Jason Teo, Hussein A. Abbass

Table of Contents
XXXIX
A Genetic Algorithm for Energy Eﬃcient Device Scheduling in
Real-Time Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1614
Lirong Tian, Tughrul Arslan
Metropolitan Area Network Design Using GA Based on Hierarchical
Linkage Identiﬁcation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1616
Miwako Tsuji, Masaharu Munetomo, Kiyoshi Akama
Statistics-Based Adaptive Non-uniform Mutation for Genetic
Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1618
Shengxiang Yang
Genetic Algorithm Design Inspired by Organizational Theory:
Pilot Study of a Dependency Structure Matrix Driven
Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1620
Tian-Li Yu, David E. Goldberg, Ali Yassine, Ying-Ping Chen
Are the “Best” Solutions to a Real Optimization Problem Always
Found in the Noninferior Set? Evolutionary Algorithm for Generating
Alternatives (EAGA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1622
Emily M. Zechman, S. Ranji Ranjithan
Population Sizing Based on Landscape Feature . . . . . . . . . . . . . . . . . . . . . . . 1624
Jian Zhang, Xiaohui Yuan, Bill P. Buckles
Genetic Programming
Structural Emergence with Order Independent Representations . . . . . . . . . 1626
R. Muhammad Atif Azad, Conor Ryan
Identifying Structural Mechanisms in Standard Genetic Programming . . . 1639
Jason M. Daida, Adam M. Hilss
Visualizing Tree Structures in Genetic Programming . . . . . . . . . . . . . . . . . . 1652
Jason M. Daida, Adam M. Hilss, David J. Ward, Stephen L. Long
What Makes a Problem GP-Hard? Validating a Hypothesis of
Structural Causes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1665
Jason M. Daida, Hsiaolei Li, Ricky Tang, Adam M. Hilss
Generative Representations for Evolving Families of Designs . . . . . . . . . . . . 1678
Gregory S. Hornby
Evolutionary Computation Method for Promoter Site Prediction
in DNA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1690
Daniel Howard, Karl Benson
Convergence of Program Fitness Landscapes . . . . . . . . . . . . . . . . . . . . . . . . . 1702
W.B. Langdon

XL
Table of Contents
Multi-agent Learning of Heterogeneous Robots by Evolutionary
Subsumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1715
Hongwei Liu, Hitoshi Iba
Population Implosion in Genetic Programming. . . . . . . . . . . . . . . . . . . . . . . . 1729
Sean Luke, Gabriel Catalin Balan, Liviu Panait
Methods for Evolving Robust Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1740
Liviu Panait, Sean Luke
On the Avoidance of Fruitless Wraps in Grammatical Evolution . . . . . . . . 1752
Conor Ryan, Maarten Keijzer, Miguel Nicolau
Dense and Switched Modular Primitives for Bond Graph Model Design . . 1764
Kisung Seo, Zhun Fan, Jianjun Hu, Erik D. Goodman,
Ronald C. Rosenberg
Dynamic Maximum Tree Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1776
Sara Silva, Jonas Almeida
Diﬃculty of Unimodal and Multimodal Landscapes in Genetic
Programming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1788
Leonardo Vanneschi, Marco Tomassini, Manuel Clergue,
Philippe Collard
Genetic Programming – Posters
Ramped Half-n-Half Initialisation Bias in GP . . . . . . . . . . . . . . . . . . . . . . . . . 1800
Edmund Burke, Steven Gustafson, Graham Kendall
Improving Evolvability of Genetic Parallel Programming Using
Dynamic Sample Weighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1802
Sin Man Cheang, Kin Hong Lee, Kwong Sak Leung
Enhancing the Performance of GP Using an Ancestry-Based Mate
Selection Scheme. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1804
Rodney Fry, Andy Tyrrell
A General Approach to Automatic Programming Using Occam’s Razor,
Compression, and Self-Inspection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1806
Peter Galos, Peter Nordin, Joel Ols´en, Kristofer Sund´en Ringn´er
Building Decision Tree Software Quality Classiﬁcation Models
Using Genetic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1808
Yi Liu, Taghi M. Khoshgoftaar
Evolving Petri Nets with a Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 1810
Holger Mauch

Table of Contents
XLI
Diversity in Multipopulation Genetic Programming . . . . . . . . . . . . . . . . . . . 1812
Marco Tomassini, Leonardo Vanneschi, Francisco Fern´andez,
Germ´an Galeano
An Encoding Scheme for Generating λ-Expressions in
Genetic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1814
Kazuto Tominaga, Tomoya Suzuki, Kazuhiro Oka
AVICE: Evolving Avatar’s Movernent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1816
Hiromi Wakaki, Hitoshi Iba
Learning Classiﬁer Systems
Evolving Multiple Discretizations with Adaptive Intervals for a
Pittsburgh Rule-Based Learning Classiﬁer System . . . . . . . . . . . . . . . . . . . . . 1818
Jaume Bacardit, Josep Maria Garrell
Limits in Long Path Learning with XCS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1832
Alwyn Barry
Bounding the Population Size in XCS to Ensure Reproductive
Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1844
Martin V. Butz, David E. Goldberg
Tournament Selection: Stable Fitness Pressure in XCS . . . . . . . . . . . . . . . . . 1857
Martin V. Butz, Kumara Sastry, David E. Goldberg
Improving Performance in Size-Constrained Extended Classiﬁer
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1870
Devon Dawson
Designing Eﬃcient Exploration with MACS: Modules and Function
Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1882
Pierre G´erard, Olivier Sigaud
Estimating Classiﬁer Generalization and Action’s Eﬀect:
A Minimalist Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1894
Pier Luca Lanzi
Towards Building Block Propagation in XCS: A Negative Result and
Its Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1906
Kurian K. Tharakunnel, Martin V. Butz, David E. Goldberg
Learning Classiﬁer Systems – Posters
Data Classiﬁcation Using Genetic Parallel Programming . . . . . . . . . . . . . . . 1918
Sin Man Cheang, Kin Hong Lee, Kwong Sak Leung
Dynamic Strategies in a Real-Time Strategy Game . . . . . . . . . . . . . . . . . . . . 1920
William Joseph Falke II, Peter Ross

XLII
Table of Contents
Using Raw Accuracy to Estimate Classiﬁer Fitness in XCS . . . . . . . . . . . . . 1922
Pier Luca Lanzi
Towards Learning Classiﬁer Systems for Continuous-Valued Online
Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1924
Christopher Stone, Larry Bull
Real World Applications
Artiﬁcial Immune System for Classiﬁcation of Gene Expression Data . . . . 1926
Shin Ando, Hitoshi Iba
Automatic Design Synthesis and Optimization of Component-Based
Systems by Evolutionary Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1938
P.P. Angelov, Y. Zhang, J.A. Wright, V.I. Hanby, R.A. Buswell
Studying the Advantages of a Messy Evolutionary Algorithm for
Natural Language Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1951
Lourdes Araujo
Optimal Elevator Group Control by Evolution Strategies . . . . . . . . . . . . . . . 1963
Thomas Beielstein, Claus-Peter Ewald, Sandor Markon
A Methodology for Combining Symbolic Regression and Design of
Experiments to Improve Empirical Model Building . . . . . . . . . . . . . . . . . . . . 1975
Flor Castillo, Kenric Marshall, James Green, Arthur Kordon
The General Yard Allocation Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1986
Ping Chen, Zhaohui Fu, Andrew Lim, Brian Rodrigues
Connection Network and Optimization of Interest Metric for
One-to-One Marketing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1998
Sung-Soon Choi, Byung-Ro Moon
Parameter Optimization by a Genetic Algorithm for a Pitch
Tracking System
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2010
Yoon-Seok Choi, Byung-Ro Moon
Secret Agents Leave Big Footprints: How to Plant a Cryptographic
Trapdoor, and Why You Might Not Get Away with It
. . . . . . . . . . . . . . . . 2022
John A. Clark, Jeremy L. Jacob, Susan Stepney
GenTree: An Interactive Genetic Algorithms System for Designing
3D Polygonal Tree Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2034
Clare Bates Congdon, Raymond H. Mazza
Optimisation of Reaction Mechanisms for Aviation Fuels Using a
Multi-objective Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2046
Lionel Elliott, Derek B. Ingham, Adrian G. Kyne, Nicolae S. Mera,
Mohamed Pourkashanian, Chritopher W. Wilson

Table of Contents
XLIII
System-Level Synthesis of MEMS via Genetic Programming and Bond
Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2058
Zhun Fan, Kisung Seo, Jianjun Hu, Ronald C. Rosenberg,
Erik D. Goodman
Congressional Districting Using a TSP-Based Genetic Algorithm . . . . . . . 2072
Sean L. Forman, Yading Yue
Active Guidance for a Finless Rocket Using Neuroevolution . . . . . . . . . . . . 2084
Faustino J. Gomez, Risto Miikkulainen
Simultaneous Assembly Planning and Assembly System Design Using
Multi-objective Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2096
Karim Hamza, Juan F. Reyes-Luna, Kazuhiro Saitou
Multi-FPGA Systems Synthesis by Means of Evolutionary
Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2109
J.I. Hidalgo, F. Fern´andez, J. Lanchares, J.M. S´anchez, R. Hermida,
M. Tomassini, R. Baraglia, R. Perego, O. Garnica
Genetic Algorithm Optimized Feature Transformation –
A Comparison with Diﬀerent Classiﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2121
Zhijian Huang, Min Pei, Erik Goodman, Yong Huang, Gaoping Li
Web-Page Color Modiﬁcation for Barrier-Free Color Vision with
Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2134
Manabu Ichikawa, Kiyoshi Tanaka, Shoji Kondo, Koji Hiroshima,
Kazuo Ichikawa, Shoko Tanabe, Kiichiro Fukami
Quantum-Inspired Evolutionary Algorithm-Based Face Veriﬁcation . . . . . 2147
Jun-Su Jang, Kuk-Hyun Han, Jong-Hwan Kim
Minimization of Sonic Boom on Supersonic Aircraft Using an
Evolutionary Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2157
Charles L. Karr, Rodney Bowersox, Vishnu Singh
Optimizing the Order of Taxon Addition in Phylogenetic Tree
Construction Using Genetic Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2168
Yong-Hyuk Kim, Seung-Kyu Lee, Byung-Ro Moon
Multicriteria Network Design Using Evolutionary Algorithm . . . . . . . . . . . 2179
Rajeev Kumar, Nilanjan Banerjee
Control of a Flexible Manipulator Using a Sliding Mode
Controller with Genetic Algorithm Tuned Manipulator Dimension . . . . . . 2191
N.M. Kwok, S. Kwong
Daily Stock Prediction Using Neuro-genetic Hybrids . . . . . . . . . . . . . . . . . . 2203
Yung-Keun Kwon, Byung-Ro Moon

XLIV
Table of Contents
Finding the Optimal Gene Order in Displaying Microarray Data . . . . . . . . 2215
Seung-Kyu Lee, Yong-Hyuk Kim, Byung-Ro Moon
Learning Features for Object Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2227
Yingqiang Lin, Bir Bhanu
An Eﬃcient Hybrid Genetic Algorithm for a Fixed Channel
Assignment Problem with Limited Bandwidth
. . . . . . . . . . . . . . . . . . . . . . . 2240
Shouichi Matsui, Isamu Watanabe, Ken-ichi Tokoro
Using Genetic Algorithms for Data Mining Optimization in an
Educational Web-Based System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2252
Behrouz Minaei-Bidgoli, William F. Punch
Improved Image Halftoning Technique Using GAs with Concurrent
Inter-block Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2264
Emi Myodo, Hern´an Aguirre, Kiyoshi Tanaka
Complex Function Sets Improve Symbolic Discriminant Analysis of
Microarray Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2277
David M. Reif, Bill C. White, Nancy Olsen, Thomas Aune,
Jason H. Moore
GA-Based Inference of Euler Angles for Single Particle Analysis . . . . . . . . 2288
Shusuke Saeki, Kiyoshi Asai, Katsutoshi Takahashi, Yutaka Ueno,
Katsunori Isono, Hitoshi Iba
Mining Comprehensible Clustering Rules with an Evolutionary
Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2301
Ioannis Saraﬁs, Phil Trinder, Ali Zalzala
Evolving Consensus Sequence for Multiple Sequence Alignment with
a Genetic Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2313
Conrad Shyu, James A. Foster
A Linear Genetic Programming Approach to Intrusion Detection . . . . . . . . 2325
Dong Song, Malcolm I. Heywood, A. Nur Zincir-Heywood
Genetic Algorithm for Supply Planning Optimization under
Uncertain Demand
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2337
Tezuka Masaru, Hiji Masahiro
Genetic Algorithms: A Fundamental Component of an Optimization
Toolkit for Improved Engineering Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2347
Siu Tong, David J. Powell
Spatial Operators for Evolving Dynamic Bayesian Networks from
Spatio-temporal Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2360
Allan Tucker, Xiaohui Liu, David Garway-Heath

Table of Contents
XLV
An Evolutionary Approach for Molecular Docking . . . . . . . . . . . . . . . . . . . . 2372
Jinn-Moon Yang
Evolving Sensor Suites for Enemy Radar Detection . . . . . . . . . . . . . . . . . . . . 2384
Ayse S. Yilmaz, Brian N. McQuay, Han Yu, Annie S. Wu,
John C. Sciortino, Jr.
Real World Applications – Posters
Optimization of Spare Capacity in Survivable WDM Networks . . . . . . . . . 2396
H.W. Chong, Sam Kwong
Partner Selection in Virtual Enterprises by Using Ant
Colony Optimization in Combination with the Analytical
Hierarchy Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2398
Marco Fischer, Hendrik J¨ahn, Tobias Teich
Quadrilateral Mesh Smoothing Using a Steady State Genetic
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2400
Mike Holder, Charles L. Karr
Evolutionary Algorithms for Two Problems from the Calculus of
Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2402
Bryant A. Julstrom
Genetic Algorithm Frequency Domain Optimization of an
Anti-Resonant Electromechanical Controller . . . . . . . . . . . . . . . . . . . . . . . . . . 2404
Charles L. Karr, Douglas A. Scott
Genetic Algorithm Optimization of a Filament Winding Process . . . . . . . . 2406
Charles L. Karr, Eric Wilson, Sherri Messimer
Circuit Bipartitioning Using Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . 2408
Jong-Pil Kim, Byung-Ro Moon
Multi-campaign Assignment Problem and Optimizing Lagrange
Multipliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2410
Yong-Hyuk Kim, Byung-Ro Moon
Grammatical Evolution for the Discovery of Petri Net Models of
Complex Genetic Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2412
Jason H. Moore, Lance W. Hahn
Evaluation of Parameter Sensitivity for Portable Embedded Systems
through Evolutionary Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2414
James Northern, Michael Shanblatt
An Evolutionary Algorithm for the Joint Replenishment of
Inventory with Interdependent Ordering Costs . . . . . . . . . . . . . . . . . . . . . . . . 2416
Anne Olsen

XLVI
Table of Contents
Beneﬁts of Implicit Redundant Genetic Algorithms for Structural
Damage Detection in Noisy Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2418
Anne Raich, Tam´as Liszkai
Multi-objective Traﬃc Signal Timing Optimization Using
Non-dominated Sorting Genetic Algorithm II . . . . . . . . . . . . . . . . . . . . . . . . . 2420
Dazhi Sun, Rahim F. Benekohal, S. Travis Waller
Exploration of a Two Sided Rendezvous Search Problem Using
Genetic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2422
T.Q.S. Truong, A. Stacey
Taming a Flood with a T-CUP – Designing Flood-Control Structures
with a Genetic Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2424
JeﬀWallace, Sushil J. Louis
Assignment Copy Detection Using Neuro-genetic Hybrids
. . . . . . . . . . . . . 2426
Seung-Jin Yang, Yong-Geon Kim, Yung-Keun Kwon, Byung-Ro Moon
Search Based Software Engineering
Structural and Functional Sequence Test of Dynamic and
State-Based Software with Evolutionary Algorithms . . . . . . . . . . . . . . . . . . . 2428
Andr´e Baresel, Hartmut Pohlheim, Sadegh Sadeghipour
Evolutionary Testing of Flag Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2442
Andre Baresel, Harmen Sthamer
Predicate Expression Cost Functions to Guide Evolutionary Search
for Test Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2455
Leonardo Bottaci
Extracting Test Sequences from a Markov Software Usage Model
by ACO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2465
Karl Doerner, Walter J. Gutjahr
Using Genetic Programming to Improve Software Eﬀort Estimation
Based on General Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2477
Martin Leﬂey, Martin J. Shepperd
The State Problem for Evolutionary Testing . . . . . . . . . . . . . . . . . . . . . . . . . . 2488
Phil McMinn, Mike Holcombe
Modeling the Search Landscape of Metaheuristic Software
Clustering Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2499
Brian S. Mitchell, Spiros Mancoridis

Table of Contents
XLVII
Search Based Software Engineering – Posters
Search Based Transformations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2511
Deji Fatiregun, Mark Harman, Robert Hierons
Finding Building Blocks for Software Clustering . . . . . . . . . . . . . . . . . . . . . . 2513
Kiarash Mahdavi, Mark Harman, Robert Hierons
Author Index

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1–12, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Swarms in Dynamic Environments
T.M. Blackwell
Department of Computer Science, University College London, Gower Street, London, UK
tim.blackwell@ieee.org
Abstract. Charged particle swarm optimization (CPSO) is well suited to the
dynamic search problem since inter-particle repulsion maintains population
diversity and good tracking can be achieved with a simple algorithm. This
work extends the application of CPSO to the dynamic problem by considering
a bi-modal parabolic environment of high spatial and temporal severity. Two
types of charged swarms and an adapted neutral swarm are compared for a
number of different dynamic environments which include extreme ‘needle-in-
the-haystack’ cases.  The results suggest that charged swarms perform best in
the extreme cases, but neutral swarms are better optimizers in milder envi-
ronments.
1   Introduction
Particle Swarm Optimization (PSO) is a population based optimization technique
inspired by models of swarm and flock behavior [1]. Although PSO has much in
common with evolutionary algorithms, it differs from other approaches by the inclu-
sion of a solution (or particle) velocity. New potentially good solutions are generated
by adding the velocity to the particle position. Particles are connected both temporally
and spatially to other particles in the population (swarm) by two accelerations. These
accelerations are spring-like: each particle is attracted to its previous best position, and
to the global best position attained by the swarm, where ‘best’ is quantified by the
value of a state function at that position. These swarms have proven to be very suc-
cessful in finding global optima in various static contexts such as the optimization of
certain benchmark functions [2].
The real world is rarely static, however, and many systems will require frequent re-
optimization due to a dynamic environment. If the environment changes slowly in
comparison to the computational time needed for optimization (i.e. to within a given
error tolerance), then it may be hoped that the system can successfully re-optimize. In
general, though, the environment may change on any time-scale (temporal severity),
and the optimum position may change by any amount (spatial severity). In particular,
the optimum solution may change discontinuously, and by a large amount, even if the
dynamics are continuous [3]. Any optimization algorithm must therefore be able to
both detect and respond to change.

2         T.M. Blackwell
Recently, evolutionary techniques have been applied to the dynamic problem [4, 5,
6]. The application of PSO techniques is a new area and results for environments of
low spatial severity are encouraging [7, 8]. CPSO, which is an extension of PSO,  has
also been applied to more demanding environments, and found to outperform the
conventional PSO [9, 10]. However,  PSO can be improved or adapted by incorporat-
ing change detecting mechanisms [11]. In this paper we compare adaptive PSO with
CPSO for various dynamic environments, some of which are severe both spatially and
temporally. In order to do this, we use a model which enables simple testing for the
three types of dynamism defined by Eberhart, Shi and Hu [7, 11].
2   Background
The problem of optimization within a general and unknown dynamic environment can
be approached by a classification of the nature of the environment and a quantification
of the difficulty of the problem. Eberhart, Shi and Hu [7, 11] have defined three types
of dynamic environment. In type I environments, the optimum position xopt, defined
with respect to a state function f, is subject to change. In type II environments, the
value of f at xopt varies and, in type III environments, both xopt and f (xopt) may change.
These changes may occur at any time, or they may occur at regular periods, corre-
sponding, for example, to a periodic sensing of the environment. Type I problems
have been quantified with a severity parameter s, which measures the jump in opti-
mum location.
Previous work on PSO in dynamic environments has focused on periodic type I en-
vironments of small spatial severity. In these mild environments, the optimum position
changes by an amount sI, where I is the unit vector in the n-dimensional search space
of the problem. Here, ‘small’ is defined by comparison with the dynamic range of the
internal variables x. Comparisons of CPSO and PSO have also been made for severe
type I environments, where s is of the order of the dynamic range [9]. In this work, it
was observed that the conventional PSO algorithm has difficulty adjusting in spatially
severe environments due to over specialization. However, the PSO can be adapted by
incorporating a change detection and response algorithm [11].
A different extension of PSO, which solves the problem of change detection and re-
sponse, has been suggested by Blackwell and Bentley [10]. In this extension (CPSO),
some or all of the particles have, in analogy with electrostatics, a ‘charge’. A third
collision-avoiding acceleration is added to the particle dynamics, by incorporating
electrostatic repulsion between charged particles. This repulsion maintains population
diversity, enabling the swarm to automatically detect and respond to change, yet does
not diminish greatly the quality of solution. In particular, it works well in certain spa-
tially severe environments [9].
Three types of particle swarm can be defined: neutral, atomic and fully-charged.
The neutral swarm has no charged particles and is identical with the conventional
PSO. Typically, in PSO, there is a progressive collapse of the swarm towards the best
position, with each particle moving with diminishing amplitude around the best posi-

Swarms in Dynamic Environments         3
tion. This ensures good exploitation, but diversity is lost.  However, in a swarm of
‘charged’ particles, there is an additional collision avoiding acceleration. Animations
for this swarm reveal that the swarm maintains an extended shape, with the swarm
centre close to the optimum location [9, 10]. This is due to the repulsion which works
against complete collapse. The diversity of this swarm is high, and response to envi-
ronment change is quick. In an ‘atomic’ swarm, 50% of the particles are charged and
50% are neutral. Animations show that the charged particles orbit a collapsing nucleus
of neutral particles, in a picture reminiscent of an atom. This type of swarm therefore
balances exploration with exploitation. Blackwell and Bentley have compared neutral,
fully charged and atomic swarms for a type-I time-dependent dynamic problem of
high spatial severity [9]. No change detection mechanism is built into the algorithm.
The atomic swarm performed best, with an average best values of f some six orders of
magnitude less than the worst performer (the neutral swarm).
One problem with adaptive PSO [11], is the arbitrary nature of the algorithm (there
are two detection methods and eight responses) which means that specification to a
general dynamic environment is difficult. Swarms with charge do not need any adap-
tive mechanisms since they automatically maintain diversity. The purpose of this
paper is to test charged swarms against a variety of environments, to see if they are
indeed generally applicable without modification.
In the following experiments we extend the results obtained above by considering
time-independent problems that are both spatially and temporally severe.  A model of
a general dynamic environment is introduced in the next section. Then, in section 4,
we define the CPSO algorithm. The paper continues with sections on experimental
design, results and analysis. The results are collecting together in a concluding section.
3   The General Dynamic Search Problem
The dynamic search problem is to find xopt for a state function f(x, u(t)) so that f(xopt,
t) fopt is the instantaneous global minimum of f. The state variables are denoted x and
the influence of the environment is through a (small) number of control variables u
which may vary in time. No assumptions are made about the continuity of u(t), but
note that even smooth changes in u can lead to discontinuous change in xopt.  (In prac-
tice a sufficient requirement may be to find a good enough approximation to xopt i.e. to
optimize f to within some tolerance f in timescales t. In this case, precise tracking of
xopt may not be necessary.)
This paper proposes a simple model of a dynamic function with moving local
minima,
f = min {f1 (x, u1 ), f2(x, u2),…, fm (x, um)}
(1)
where the control variables ua = {xa, ha
2} are defined so that fa has a single minimum at
xa, with an optimum value ha
2  0 at fa(xa). If the functions fa themselves have individ-
ual dynamics, f can be used to model a general dynamic environment.

4         T.M. Blackwell
A convenient choice for fa, which allows comparison with other work on dynamic
search with swarms [4, 7, 8, 9, 11], is the parabolic or sphere function in n dimensions,
fa =
2
1
)
(
ai
n
i
i
x
x −
∑
=
+ ha
2
(2)
which differs from De Jong’s f1 function [12] by the inclusion of a height offset ha and
a position offset xia. This model satisfies Branke’s conditions for a benchmark problem
(simple, easy to describe and analyze, and tunable) and is in many respects similar to
his “moving peaks” benchmark problem, except that the widths of each optimum are
not adjustable, and in this case we seek a minimization (“moving valleys”) [6]. This
simple function is easy to optimize with conventional methods in the static mono-
modal case. However the problem becomes more acute as the number m of moving
minima increases.
Our choice of f also suggests a simple interpretation. Suppose that all ha are zero.
Then fa is the Euclidean ‘squared distance’ between vectors x and xa. Each local opti-
mum position xa can be regarded as a ‘target’. Then, f is the squared distance of the
nearest ‘target’ from the set {xa} to x. Suppose now that the vectors x are actually
projections of vectors y in R
n+1, so that y = (x, 0) and targets ya have components (xa,
ha) in this higher dimensional space. In other words, ha are height offsets in the n+1th
dimension. From this perspective, f is still the squared distance to the nearest target,
except that the system is restricted to R
n. For example, suppose that x is the 2-
dimensional position vector of a ship, and {xa} are a set of targets scattered on the sea
bed at depths {ha}. Then the square root of f at any time is the distance to the closest
target and the depth of the shallowest object is
)
(
opt
f x
. The task for the ship’s navi-
gator is to position the ship at xopt, directly over the shallowest target, given that all the
targets are in independent motion along an uneven sea bed.
Since no assumptions have been made about the dynamics of the environment, the
above model describes the situation where the change can occur at any time. In the
periodic problem, we suppose that the control variables change simultaneously at
times ti and are held fixed at ui for the corresponding intervals   [ ti, ti+1]:
i
i
i
i
t
t
t
u
u
))
(
)
(
(
)
(
1
+
Θ
−
Θ
=∑
(3)
where (t) is the unit step function.
The PSO and CPSO experiments of [9] and [11] are time-dependent type I experi-
ments with a single minimum at x1 and with h1 = 0. The generalization to more diffi-
cult type I environments is achieved by introducing more local minima at positions xa,
but fixing the height offsets ha. Type II environments are easily modeled by fixing the
positions of the targets, but allowing ha to change at the end of each period. Finally, a
type III environment is produced by periodically changing both xa and ha.
Severity is a term that has been introduced to characterize problems where the op-
timum position changes by a fixed amount s at a given number of iterations [4, 7]. In
[7, 11] the optimum position changes by small increments along a line. However,

Swarms in Dynamic Environments         5
Blackwell and Bentley have considered more severe dynamic systems whereby the
optimum position can jump randomly within a target cube T which is of dimension
equal to twice the dynamic range vmax [9]. Here severity is extended to include dynamic
systems where the target jumps may be for periods of very short duration.
4   PSO and CPSO Algorithms
Table 1 shows the particle update algorithm. The PSO parameters g1, g2 and w govern
convergence. The electrostatic acceleration ai, parameterized by pcore, p and Qi, is
j
i
ij
ij
core
ij
ij
j
i
i
p
r
p
r
Q
Q
x
x
r
r
a
−
=
<
<
∑
=
≠
    
,
,
i
j
3
(4)
The PSO and CPSO search algorithm is summarized below in Table 2. To begin, a
swarm of M particles, where each particle has n-dimensional position and velocity
vectors {xi, vi,}, is randomized in the box T = D
n =[-vmax, vmax]
n where D is the ‘dynamic
range’ and vmax is the clamping velocity. A set of period durations {ti} is chosen; these
are either fixed to a common duration, or chosen from a uniform random distribution.
A single iteration is a single pass through the loop in Table 2.
Denoting the best value position and value found by the swarm as xgb and fgb, change
detection is simply invoked by comparing f(xgb) with fgb. If these are not equal, the
inference is that f has changed since fgb was last evaluated. The response is to re-
randomize a fraction of the swarm in T, and to re-set fgb to f(xgb). The detection and
response algorithm is only applied to neutral swarms.
The best position attained by a particle, xpb,i, is updated by comparing  f(xi) with
f(xpb,i): if  f(xi) < f(xpb,i), then xpb,i  xi. Any new xpb,i is then tested against xgb, and a
replacement is made, so that at each particle update f(xgb) = min{f(xpb,i )}. This speci-
fies update best(i).
Table 1. The particle update algorithm
update particle(i)
vi  wvi + g1(xpb,i – xi) + g2(xgb-xi) + ai
if  vi vmax  
          vi  (vmax / vivi
    xi  xi + vi

6         T.M. Blackwell
Table 2. Search algorithm for charged and neutral particle swarm optimization
(C)PSO search
initialize swarm { xi, vi} and periods{tj}
loop:
     if  t = tj
           update function
     if (neutral swarm)
          detect and respond to change
     for i = 1 to M
          update best (i)
          update particle(i)
     endfor
     tt+1
until stopping criterion is met
5   Experiment Design
Twelve experiments of varying severity were conceived, for convenience arranged in
three groups. The parameters and specifications for these experiments are summarized
in Tables 3 and 4. In each experiment, the dynamic function has two local minima at
xa, a = 1, 2; the global minimum is at x2. The value of f at x1 is fixed at 100 in all ex-
periments. The duration of the function update periods, denoted , is either fixed at
100 iterations, or is a random integer between 1 and 100. (For simplicity, random
variables drawn from uniform distribution with limits a, b will be denoted x ~ [a, b]
(continuous distribution) and x ~ [a…b] (discrete distribution).
In the first group (A) of experiments, numbers 1 – 4, x2 is moved randomly in T
(‘spatially severe’)  or is moved randomly in a smaller box 0.1T. The optimum value,
f(x2), is fixed at 0. These are all type I experiments, since the optimum location moves,
but the optimum value is fixed. Experiments 3 and 4 repeat the conditions of 1 and 2
except that x2 moves at random intervals ~ [1…100] (temporally severe).
Experiments 5 – 8 (Group B) are type II environments. In this case, x1 and x2 are
fixed at ±r, along the body diagonal of T, where r = (vmax/3) (1, 1, 1). However, f (x2)
varies, with h2 ~ [0, 1], or h2 ~ [0, 100]. Experiments 7 and 8 repeat the conditions of 5
and 6 but for high temporal severity.
In the last group (C) of experiments (9 – 12), both x1 and x2 jump randomly in T. In
the type III case, experiments 11 and 12, f (x2) varies. For comparison, experiments 9

Swarms in Dynamic Environments         7
and 10 duplicate the conditions of 11 and 12, but with fixed f (x2). Experiments 10 and
12 are temporally severe versions of 9 and 11.
Each experiment, of 500 periods, was performed with neutral, atomic (i.e. half the
swarm is charged) and fully charged swarms (all particles are charged) of 20 particles
(M = 20). In addition, the experiments were repeated with a random search algorithm,
which simply, at each iteration, randomizes the particles within T. A spatial dimension
of   n = 3 was chosen. In each run, whenever random numbers are required for target
positions, height offsets and period durations, the same sequence of pseudo-random
numbers is used, produced by separately seeded generators. The initial swarm configu-
ration is random in T, and the same configuration is used for each run.
Table 3. Spatial, electrostatic and PSO Parameters
Table 4. Experiment Specifications
Group
Expt
Targets
{x1, x1}
Local Opt
{f(x1),  f(x2)}
Period 
1
{O,  ~0.1T}
2
{O,  ~T}
100
3
{O,  ~0.1T}
A
4
{O,  ~T}
{100, 0}
~[1, 100]
5
{100,  ~[0, 1]}
6
{100,~[0,100]}
100
7
{100,  ~[0, 1]}
B
8
{O– r, O+r}
{100,~[0,100]}
~[1, 100]
9
100
10
{100, 0]}
~[1,100]
11
100
C
12
{~T,  ~T}
{100,~[0,100]}
~[1,100]
The search (C)PSO algorithm has a number of parameters (Table 3) which have
been chosen to correspond to the values used in previous experiments [5, 9, 11]. These
choices agree with Clerc’s analysis for convergence [13]. The spatial and electrostatic
parameters are once more chosen for comparison with previous work on charged par-
ticle swarms [9]. An analysis that explains the choice of the electrostatic parameters is
Spatial
Electrostatic
PSO
vmax
n
M
T
pcore
p
Qi
g1, g2
w
32
3
20
[-32,32]
3
1
2	3vmax
16
~[0,1.49]
~[0.5, 1]

8         T.M. Blackwell
given in [14]. Since we are concerned with very severe environments, the response
strategy chosen here is to randomize the positions of 50% of the swarm [11]. This also
allows for comparisons with the atomic swarm which maintains a diverse population
of 50% of the swarm.
6   Results and Analysis
The chief statistic is the ensemble average best value, <f(x2) - fgb >; this is positive and
bounded by zero. A further statistic, the number of ‘successes’, nsuccesses,, was also col-
lected to aid analysis. Here, the search is deemed a success if xgb is closer, at the end of
each period, to target 2 (which always has the lower value of f) than it is to target 1.
The results for the three swarms and for random search are shown in Figs 1 and 2. The
light grey boxes in Figure 1, experiment 6, indicate an upper bound to the ensemble
average due to the precision of the floating-point representation: for these runs, f(x2) -
fgb  = 0 at the end of each period, but this is an artifact of the finite-precision arithmetic.
Group A. Figure 1 shows that all swarms perform better than random search except
for the neutral swarm in spatially severe environments (2 and 4) and the atomic swarm
in a spatially and temporally severe environment (4). In the least severe environment
(1), the neutral swarm performs very well, confirming previous results. This swarm
has the least diversity and the best exploitation. The order of performance for this
experiment reflects the amount of diversity; neutral (least diversity, best), atomic,
fully charged, and random (most diversity, worst). When environment 1 is made tem-
porally severe (3), all swarms have similar performance and are better than random
search. The implication here is that on average the environment changes too quickly
for the better exploitation properties of the neutral swarm to become noticeable. Ex-
periments 2 and 4 repeat the conditions of 1 and 2, except for higher spatial severity.
Here the order of performance amongst the swarms is in increasing order of diversity
(fully charged best and neutral worst). The reason for the poor performance of the
neutral swarm in environments 2 and 4 can be inferred from the success data. The
success rate of just 5% and ensemble average close to 100 (= f(x1)) suggests that the
neutral swarm often gets stuck in the false minimum at x1. Since fgb does not change at
x1, the adapted swarm cannot register change, does not randomize, and so is unlikely
to move away from x1 until x2 jumps to a nearby location. In fact the neutral swarm is
worse than random search by an order of magnitude. Only the fully charged swarm
out-performs random search appreciably for the spatially severe type I environments
(2 and 4) and this margin diminishes when the environment is temporally severe too.
Group B. Throughout this group, all swarms are better than random and the number of
successes shows that there no problems with the false minimum. The swarm with the
least diversity and best exploitation (neutral) does best since the optimum location

Swarms in Dynamic Environments         9
Fig. 1. Ensemble average <f(x2)-fgb> for all experiments
Fig. 2. Number of successes nsuccesses for all experiments

10         T.M. Blackwell
does not change from period to period. The effect of increasing temporal severity can
be seen by comparing 7 to 5 and 8 to 6. Fully charged and random are almost unaf-
fected by temporal severity in these type II environments, but the performance of the
neutral and atomic swarms worsens. Once more the explanation for this is that these
are the only two algorithms which can significantly improve their best position over
time because only these two contain neutral particles which can converge unimpeded
on the minimum. This advantage is lessened when the average time between jumps is
decreased. The near equality of ensemble averages for random search in 5 and 6, and
again in 7 and 8, is due to the fact that random search is not trying to improve on a
previous value – it just depends on the closest randomly generated points to x2 during
any period. Since x1 and x2 are fixed, this can only depend on the period size and not
on f(x2).
Group C. The ensemble averages for the four experiments in this group (9-12) are
broadly similar but the algorithm with the most successes in each experiment is ran-
dom search. However random search is not able to exploit any good solution, so al-
though the swarms have more failures, they are able to improve on their successes
producing ensemble averages close to random search. In experiments 9 and 10, which
are type I cases, all swarms perform less well than random search. These two experi-
ments differ from environments 2 and 4, which are also spatially severe, by allowing
the false minimum at x1 to jump as well. The result is that the performance of the
neutral swarm improves since it is no longer caught by the false minimum at x1; the
number of successes improves from less than 25 in 2 and 4, to over 350 in 9 and 10.
In experiments 11 and 12 (type III) when fopt changes in each period, the fully charged
swarm marginally out-performs random search. It is worth noting that 12 is a very
extreme environment: either minimum can jump by arbitrary amounts, on any time
scale, and with the minimum value varying over a wide range.  One explanation for
the poor performance of all swarms in 9 and 10 is that there is a higher penalty (<f (x1)
- fopt> = 100) for getting stuck on the false minimum at x1, than the corresponding
penalty in 11 and 12 (<f (x1) - fopt> = 50). The lower success rate for all swarms com-
pared to random search supports this explanation.
7   Conclusions
A dynamic environment can present numerous challenges for optimization. This paper
has presented a simple mathematical model which can represent dynamic environ-
ments of various types and severity. The neutral particle swarm is a promising algo-
rithm for these problems since it performs well in the static case, and can be adapted
to respond to change. However, one draw back is the arbitrary nature of the detection
and response algorithms. Particle swarms with charge need no further adaptation to
cope with the dynamic scenario due to the extended swarm shape. The neutral and two
charged particle swarms have been tested, and compared with random search, with
twelve environments which are classified by type. Some of these environments are
extreme, both in the spatial as well as the temporal domain.

Swarms in Dynamic Environments         11
The results support the intuitive idea that type II environments (those in which the
optimum location is fixed, but the optimum value may vary) present few problems to
evolutionary methods since a population diversity is not important. In fact the algo-
rithm with the lowest diversity performed best. Increasing temporal severity dimin-
ishes the performance of the two swarms with neutral particles, but does not affect the
fully charged swarm.
However, environments where the optimum location can change (types I and III)
are much harder to deal with, especially when the optimum jumps can be to an arbi-
trary point within the search space, and can happen at very short notice. This is the
dynamic equivalent of the needle in a haystack problem. A type I environment has
been identified which poses considerable problems for the adapted PSO algorithm: a
stationary false minimum and a mobile true minimum with large spatial severity.
There is a tendency for the neutral swarm to become trapped by the false minimum. In
this case, the fully charged swarm is the better option.
Finally, the group C environments proved to be very challenging for all swarms.
These environments are distinguished by two spatially severe minima with a large
difference in function value at these minima. In other words, there is a large penalty
for finding the false minimum rather than the true minimum. All swarms struggled to
improve upon random search because of this trap.
Despite this, all swarms have been shown, for dynamic parabolic functions, to offer
results comparable to random search in the worst cases, and considerably better than
random in the more benign situations. As with static search problems, if some prior
knowledge of the dynamics is known, a preferable algorithm can be chosen. Accord-
ing to the classification of Eberhart and Wu [7, 11], and for the examples studied here,
the adapted neutral swarm is the best performer for mild type I and II environments.
However, it can be easily fooled in type I and III environments where a false mini-
mum is also dynamic. In this case, the charged swarms are better choices. As the envi-
ronment becomes more extreme, charge, which is a diversity increasing parameter,
becomes more useful. In short, if nothing is known about an environment, the fully
charged swarm has the best average performance.
It is possible that different adaptations to the neutral swarm can lead to better per-
formance in certain environments, but it remains to be seen if there is a single adapta-
tion which works well over a range of environments. On the other hand, the charged
swarm needs no further modification since the collision avoiding accelerations ensure
exploration the space around a solution.
References
1.
Kennedy J. and Eberhart, R.C.: Particle Swarm Optimization. Proc of the IEEE Interna-
tional Conference on Neural Networks  IV  (1995) 1942–1948
2.
Eberhart R.C. and Shi Y.: Particle swarm optimization: Developments, applications and
resources. Proc Congress on Evolutionary Computation (2001) 81–86
3.
Saunders P.T.: An Introduction to Catastrophe Theory. Cambridge University Press (1980)

12         T.M. Blackwell
4.
Angeline P.J.: Tracking extrema in dynamic environments. Proc Evolutionary Program-
ming IV. (1998) 335–345
5.
Bäck T.: On the behaviour of evolutionary algorithms in dynamic environments. Proc Int.
Conf. on Evolutionary Computation. (1998) 446–451
6.
Branke J.: Evolutionary algorithms for changing optimization problems. Proc Congress on
Evolutionary Computation. (1999) 1875–1882
7.
Eberhart R.C. and Shi Y.: Tracking and optimizing dynamic systems with particle swarms.
Proc Congress on Evolutionary Computation. (2001) 94–97
8.
Carlisle A. and Dozier G.: Adapting particle swarm optimization to dynamic environments.
Proc of Int Conference on Artificial Intelligence. (2000) 429–434
9.
Blackwell and Bentley P.J.: Dynamic search with charged swarms. Proc Genetic and Evo-
lutionary Computation Conference. (2002) 19–26
10. Blackwell and Bentley P.J.: Don’t push me! Collision avoiding swarms. Proc Congress on
Evolutionary Computation. (2002) 1691–1696
11. Hu X. and Eberhart R.C.: Adaptive particle swarm optimization: detection and response to
dynamic systems. Proc Congress on Evolutionary Computation. (2002) 1666–1670
12. De Jong K: An analysis of the behavior of a class of genetic adaptive systems. PhD thesis,
University of Michigan (1975)
13. Clerc M.: The swarm and the queen: towards a deterministic and adaptive particle swarm
optimization. Proc Congress on Evolutionary Computation. (1999) 1951–1957
14. Blackwell and Bentley P.J.: Improvised Music with Swarms, Proc Congress on Evolution-
ary Computation. (2002) 1462–1467

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 13–24, 2003.
© Springer-Verlag Berlin Heidelberg 2003
The Effect of Natural Selection on Phylogeny
Reconstruction Algorithms
Dehua Hang
1, Charles Ofria
1, Thomas M. Schmidt
2, and Eric Torng
1
1Department of Computer Science & Engineering
Michigan State University, East Lansing, MI  48824 USA
2Department of Microbiology and Molecular Genetics
Michigan State University, East Lansing, MI  48824 USA
 {hangdehu, ofria, tschmidt, torng}@msu.edu
Abstract. We study the effect of natural selection on the performance of
phylogeny reconstruction algorithms using Avida, a software platform that
maintains a population of digital organisms (self-replicating computer
programs) that evolve subject to natural selection, mutation, and drift.  We
compare the performance of neighbor-joining and maximum parsimony
algorithms on these Avida populations to the performance of the same
algorithms on randomly generated data that evolve subject only to mutation and
drift.  Our results show that natural selection has several specific effects on the
sequences of the resulting populations, and that these effects lead to improved
performance for neighbor-joining and maximum parsimony in some settings.
We then show that the effects of natural selection can be partially achieved by
using a non-uniform probability distribution for the location of mutations in
randomly generated genomes.
1   Introduction
As researchers try to understand the biological world, it has become clear that
knowledge of the evolutionary relationships and histories of species would be an
invaluable asset.  Unfortunately, nature does not directly track such changes, and so
such information must be inferred by studying extant organisms. Many algorithms
have been crafted to reconstruct phylogenetic trees - dendrograms in which species
are arranged at the tips of branches, which are then linked successively according to
common evolutionary ancestors. The input to these algorithms are typically traits of
extant organisms such as gene sequences. Often, however, the phylogenetic trees
produced by distinct reconstruction algorithms are different, and there is no way of
knowing which, if any, is correct.
In order to determine which reconstruction algorithms work best, methods for
evaluating these algorithms need to be developed. As documented by Hillis [1], four
principal methods have been used for assessing phylogenetic accuracy: working with
real lineages with known phylogenies, generating artificial data using computer
simulations, statistical analyses, and congruence studies. These last two methods tend
to focus on specific phylogenetic estimates; that is, they attempt to provide
independent confirmations or probabilistic assurances for a specific result rather than

14         D. Hang et al.
evaluate the general effectiveness of an algorithm. We focus on the first two methods,
which are typically used to evaluate the general effectiveness of a reconstruction
algorithm: computer simulations [2] and working with lineages with known
phylogenies [3].
In computer simulations, data is generated according to a specific model of
nucleotide or amino acid evolution. The primary advantages of the computer
simulation technique are that the correct phylogeny is known, data can be collected
with complete accuracy and precision, and vast amounts of data can be generated
quickly. One commonly used computer simulation program is seq-gen [4]. Roughly
speaking, seq-gen takes as input an ancestral organism, a model phylogeny, and a
nucleotide substitution model and outputs a set of taxa that conforms to the inputs.
Because the substitution model and the model phylogeny can be easily changed,
computer simulations can generate data to test the effectiveness of reconstruction
algorithms under a wide range of conditions.
Despite the many advantages of computer simulations, this technique suffers
from a “credibility gap’’ due to the fact that the data is generated by an artificial
process. That is, the sequences are never expressed and thus have no associated
function. All genomic changes in such a model are the result of mutation and genetic
drift; natural selection does not determine which position changes are accepted and
which changes are rejected. Natural selection is only present via secondary
relationships such as the use of a model phylogeny that corresponds to real data.  For
this reason, many biologists disregard computer simulation results.
Another commonly used evaluation method is to use lineages with known
phylogenies.  These are typically agricultural or laboratory lineages for which records
have been kept or experimental phylogenies generated specifically to test
phylogenetic methods. Known phylogenies overcome the limitation of computer
simulations in that all sequences are real and do have a relation to function. However,
working with known phylogenies also has its limitations. As Hillis states, “Historic
records of cultivated organisms are severely limited, and such organisms typically
have undergone many reticulations and relatively little genetic divergence.” [1].
Thus, working with these lineages only allows the testing of reconstructions of
phylogenies of closely related organisms. Experimentally generated phylogenies were
created to overcome this difficulty by utilizing organisms such as viruses and bacteria
that reproduce very rapidly.
However, even research with experimentally generated lineages has its
shortcomings. First, while the organisms are natural and evolving, several artificial
manipulations are required in order to gather interesting data. For example, the
mutation rate must be artificially increased to produce divergence and branches are
forced by explicit artificial events such as taking organisms out of one petri dish and
placing them into two others. Second, while the overall phylogeny may be known, the
data captured is neither as precise nor complete as that with computer simulations.
That is, in computer simulations, every single mutation can be recorded whereas with
experimental phylogenies, only the major, artificially induced phylogenetic branch
events can be recorded. Finally, even when working with rapidly reproducing
organisms, significant time is required to generate a large amount of test data; far
more time than when working with computer simulations.
Because of the limitations of previous evaluation methods, important questions
about the effectiveness of phylogeny reconstruction algorithms have been ignored in

The Effect of Natural Selection on Phylogeny Reconstruction Algorithms        15
the past. One important question is the following: What is the effect of natural
selection on the accuracy of phylogeny reconstruction algorithms?
Here, we initiate a systematic study of this question. We begin by generating two
related data sets.  In the first, we use a computer program that has the accuracy and
speed of previous models, but also incorporates natural selection. In this system, a
mutation only has the possibility of persisting if natural selection does not reject it.
The second data set is generated with the same known phylogenetic tree structure as
was found in the first, but this time all mutations are accepted regardless of the effect
on the fitness of the resulting sequence (to mimic the more traditional evaluation
methodologies). We then apply phylogeny reconstruction algorithms to the final
genetic sequences in both data sets and compare the results to determine the effect of
natural selection.
To generate our first data set, we use Avida, a digital life platform that maintains
a population of digital organisms (i.e. programs) that evolve subject to mutation, drift,
and natural selection.  The true phylogeny is known because the evolution occurs in a
computer in which all mutation events are recorded.  On the other hand, even though
Avida populations exist in a computer rather than in a petri dish or in nature, they are
not simulations but rather are experiments with digital organisms that are analogous
to experiments with biological organisms. We describe the Avida system in more
detail in our methods section.
2   Methods
2.1   The Avida Platform [5]
The major difficulty in our proposed study is generating sequences under a variety of
conditions where we know the complete history of all changes and the sequences
evolve subject to natural selection, not just mutation and drift. We use the Avida
system, an auto-adaptive genetic system designed for use as a platform in
digital/artificial life research, for this purpose.
A typical Avida experiment proceeds as follows. A population of digital
organisms (self-replicating computer programs with a Turing-complete genetic basis)
is placed into a computational environment. As each organism executes, it can
interact with the environment by reading inputs and writing outputs.  The organisms
reproduce by allocating memory to double their size, explicitly copying their genome
(program) into the new space, and then executing a divide command that places the
new copy onto one of the CPU’s in the environment “killing” the organism that used
to occupy that CPU. Mutations are introduced in a variety of ways. Here, we make the
copy command probabilistic; that is, we can set a probability that the copy command
fails by writing an arbitrary instruction rather than the intended instruction.
The crucial point is that during an Avida experiment, the population evolves
subject to selective pressures. For example, in every Avida experiment, there is a
selective pressure to reproduce quickly in order to propagate before being overwritten
by another organism. We also introduce other selective pressures into the
environment by rewarding organisms that perform specific computations by
increasing the speed at which they can execute the instructions in their genome. For
example, if the outputs produced by an organism demonstrate that the organism can

16         D. Hang et al.
perform a Boolean logic operation such as “exclusive-or” on its inputs, then the
organism and its immediate descendants will execute their genomes at twice their
current rate.  Thus there is selective pressure to adapt to perform environment-specific
computations. Note that the rewards are not based on how the computation is
performed; only the end product is examined. This leads to open-ended evolution
where organisms evolve functionality in unanticipated ways.
2.2   Natural Selection and Avida
Digital organisms are used to study evolutionary biology as an independent form of
life that shares no ancestry with carbon-based life. This approach allows general
principles of evolution to be distinguished from historical accidents that are particular
to biochemical life. As Wilke and Adami state, “In terms of the complexity of their
evolutionary dynamics, digital organisms can be compared with biochemical viruses
and bacteria”, and “Digital organisms have reached a level of sophistication that is
comparable to that of experiments with bacteria or viruses” [6].
The limitation of working with digital organisms is that they live in an artificial
world, so the conclusions from digital organism experiments are potentially an artifact
of the particular choices of that digital world. But by comparing the results across
wide ranges of parameter settings, as well as results from biochemical organisms and
from mathematical theories, general principles can still be disentangled. Many
important topics in evolutionary biology have been addressed by using digital
organisms including the origins of biological complexity [7], and quasi-species
dynamics and the importance of neutrality [8]. Some work has also compared
biological systems with those of digital organisms, such as a study on the distribution
of epistemic interactions among mutations [9], which was modeled on an earlier
experiment with E. coli [10], and the similarity of the results were striking, supporting
the theory that many aspects of evolving systems are governed by universal
principles.
Avida is a well-developed digital organism platform. Avida organisms are self-
replicating computer programs that live in, and adapt to, a controlled environment.
Unlike other computational approaches to studying evolution (such as genetic
algorithms or numerical simulations), Avida organisms must explicitly create a copy
of their own genome to reproduce, and no particular genomic sequence is designated
as the target or optimal sequence. Explicit and implicit mutations occur in Avida.
Explicit mutations include point mutations incurred during the copy process and the
random insertions and/or deletions of single instructions.  Implicit mutations are the
result of flawed copy algorithms. For example, an Avida organism might skip part of
its genome during the replication, or replicate part of its genome more than once.  The
rates of explicit mutations can be controlled during the setup process, whereas
implicit mutations cannot typically be controlled. Selection occurs because the
environment in which the Avida organisms live is space limited. When a new
organism is born, an older one is removed from the population.

The Effect of Natural Selection on Phylogeny Reconstruction Algorithms        17
2.3   Determining Correctness of a Phylogeny Reconstruction: The Four Taxa
Case
Even when we know the correct phylogeny, it is not easy to measure the quality of a
specific phylogeny reconstruction. A phylogeny can be thought of as an edge-
weighted tree (or, more generally, an edge-weighted graph) where the edge weights
correspond to evolutionary time or distance. Thus, a reconstruction algorithm should
not only generate the correct topology or structure but also must generate the correct
evolutionary distances.
Like many other studies, we simplify the problem by ignoring the edge weights
and focus only on topology [11]. Even with this simplification, measuring correctness
is not an easy problem. If the reconstructed topology is identical to the correct
topology, then the reconstruction is correct. However, if the reconstructed topology is
not identical, which will often be the case, it is not sufficient to say that the
reconstruction is incorrect. There are gradations of correctness, and it is difficult to
state that one topology is closer to the correct topology than a second one in many
cases.
We simplify this problem so that there is an easy answer of right and wrong.  We
focus on reconstructing topologies based on populations with four taxa.  With only
four taxa, there really is only one decision to be made: Is A closest to B, C, or D?  See
the following diagram for an illustration of the three possibilities.  Focusing on
situations with only four taxa is a common technique used in the evaluation of
phylogeny reconstruction algorithms [2,11,12].
Fig. 1. Three possible topologies under four taxa model tree.
2.4   Generation of Avida Data
We generated Avida data in the following manner. First, we took a hand-made
ancestor S1 and injected it into an environment E1 in which four simple computations
were rewarded.  The ancestor had a short copy loop and its genome was padded out to
length 100 (from a simple 15-line self-replicator) with inert no-op instructions.  The
only mutations we allowed during the experiments were copy mutations and all size
changes due to mis-copies were rejected; thus the lengths of all genome sequences
throughout the execution are length 100. We chose to fix the length of sequences in
order to eliminate the issue of aligning sequences. The specific length 100 is
somewhat arbitrary. The key property is that it is enough to provide space for
mutations and adaptations to occur given that we have disallowed insertions. All
environments were limited to a population size of 3600. Previous work with avida
(e.g. [16]) has shown that 3600 is large enough to allow for diversity while making
large experiments practical.
A
C
B
C
B
D
A
D
C
B
A
D

18         D. Hang et al.
After running for L1 updates, we chose the most abundant genotype S2 and
placed S2 into a new environment E2 that rewarded more complex computations.
Two computations overlapped with those rewarded by E1 so that S2 retained some of
its fitness, but new computations were also rewarded to promote continued evolution.
We executed two parallel experiments of S2 in E2 for 1.08 × 10
10 cycles, which is
approximately 10
4 generations. In each of the two experiments, we then sampled
genotypes at a variety of times L2 along the line of descent from S2 to the most
abundant genotype at the end of the execution.  Let S3a-x denote the sampled
descendant in the first experiment for L2 = x while S3b-x denotes the same
descendant in the second experiment.
Then, for each value x of L2, we took S3a-x and S3b-x and put them each into a
new environment E3 that rewards five complex operations.  Again, two rewarded
computations overlapped with the computations rewarded by E2 (and there was no
overlap with E1), and again, we executed two parallel experiments for each organism
for a long time.  In each of the four experiments, we then sampled genotypes at a
variety of times L3 along the line of descent from S3a-x or S3b-x to the most
abundant genotype at the end of the execution.  For each value of L3, four taxa A, B,
C and D were used for reconstruction. This experimental procedure is illustrated in
the following diagram. Organisms A and B share the same ancestor S3a-x while
organisms C and D share the same ancestor S3b-x.
Fig. 2. Experimental procedure diagram.
We varied our data by varying the sizes of L2 and L3.  For L2, we used values 3,
6, 10, 25, 50, and 100. For L3, we used values 3, 6, 10, 25, 100, 150, 200, 250, 300,
400, and 800. We repeated the experimental procedure 10 times.
The tree structures that we used for reconstruction were symmetric (they have
the shape implied by Fig. 1). The internal edge length of any tree structure is twice
the value of L2. The external edge length of any tree structure is simply L3. With six
values of L2 and eleven values of L3, we used 66 different tree structures with 10
distinct copies of each tree structure.
E1
E2
E3
L1
L2
L3
S1
S2
S3a-x
S3b-x
A
B
C
D

The Effect of Natural Selection on Phylogeny Reconstruction Algorithms        19
2.5   Generation of Random Data
We developed a random data generator similar to seq-gen in order to produce data
that had the same phylogenetic topology as the Avida data, but where the evolution
occurred without any natural selection. Specifically, the generator took as input the
known phylogeny of the corresponding Avida experiment, including how many
mutations occurred along each branch of the phylogenetic tree, as well as the
ancestral organism S2 (we ignored environment E1 as its sole purpose was to distance
ourselves from the hand-written ancestral organism S1). The mutation process was
then simulated starting from S2 and proceeding down the tree so that the number of
mutations between each ancestor/descendant is identical to that in the corresponding
Avida phylogenetic tree. The mutations, however, were random (no natural selection)
as the position of the mutation was chosen according to a fixed probability
distribution, henceforth referred to as the location probability distribution, and the
replacement character was chosen uniformly at random from all different characters.
In different experiments, we employed three distinct location probability distributions.
We explain these three different distributions and our rationale for choosing them in
Section 3.3.  We generated 100 copies of each tree structure in our experiments.
2.6   Two Phylogeny Reconstruction Techniques (NJ, MP)
We consider two phylogeny reconstruction techniques in this study.
Neighbor-Joining. Neighbor-joining (NJ) [13,14] was first presented in 1987
and is popular primarily because it is a polynomial-time algorithm, which means it
runs reasonably quickly even on large data sets. NJ is a distance-based method that
implements a greedy strategy of repeatedly clustering the two closest clusters (at first,
a pair of leaves; thereafter entire subtrees) with some optimizations designed to
handle non-ultrametric data.
Maximum Parsimony. Maximum parsimony (MP) [15] is a character-based
method for reconstructing evolutionary trees that is based on the following principle.
Of all possible trees, the most parsimonious tree is the one that requires the fewest
number of mutations.  The problem of finding an MP tree for a collection of
sequences is NP-hard and is a special case of the Steiner problem in graph theory.
Fortunately, with only four taxa, computing the most parsimonious tree can be done
rapidly.
2.7    Data Collection
We assess the performance of NJ and MP as follows.  If NJ produces the same tree
topology as the correct topology, it receives a score of 1 for that experiment.  For each
tree structure, we summed together the scores obtained by NJ on all copies (10 for
Avida data, 100 for randomly generated data) to get NJ’s score for that tree structure.
Performance assessment was more complicated for MP because there are cases
where multiple trees are equally parsimonious.  In such cases, MP will output all of
the most parsimonious trees. If MP outputs one of the three possible tree topologies
(given that we are using four taxa for this evaluation) and it is correct, then MP gets a

20         D. Hang et al.
score of 1 for that experiment.  If MP outputs two tree topologies and one of them is
correct, then MP gets a score of 1/2 for that experiment.  If MP outputs all three
topologies, then MP gets a score of 1/3 for that experiment.  If MP fails to output the
correct topology, then MP gets a score of 0 for that experiment.  Again, we summed
together the scores obtained by MP on all copies of the same tree structure (10 for
Avida data, 100 for random data) to get MP’s score on that tree structure.
3   Results and Discussions
3.1.   Natural Selection and Its Effect on Genome Sequences
Before we can assess the effect of natural selection on phylogeny reconstruction
algorithms, we need to understand what kind of effect natural selection will have on
the sequences themselves.  We show two specific effects of natural selection.
Fig. 3. Location probability distribution                Fig. 4. Hamming distances between branch A
from one Avida run (length 100). Probability        and B from Avida data and randomly
data are normalized to their percentage.                 generated data. Internal edge length is 50.
We first show that the location probability distribution becomes non-uniform
when the population evolves with natural selection. In a purely random model, each
position is equally likely to mutate. However, with natural selection, some positions
in the genome are less subject to accepted mutations than others. For example,
mutations in positions involved in the copy loop of an Avida organism are typically
detrimental and often lethal. Thus, accepted mutations in these positions are relatively
rare compared to other positions. Fig. 2 shows the non-uniform position mutation
probability distribution from a typical Avida experiment. This data captures the
frequency of mutations by position in the line of descent from the ancestor to the most
abundant genotype at the end of the experiment. While this is only one experiment,
similar results apply for all of our experiments. In general, we found roughly three
types of positions: fixed positions with no accepted mutations in the population

The Effect of Natural Selection on Phylogeny Reconstruction Algorithms        21
(accepted mutation rate = 0%); stable positions with a low rate of accepted mutations
in the population (accepted mutation rate < 1%), and volatile positions with a high
rate of accepted mutations (accepted mutation rate > 1%).
Because some positions are stable, we also see that the average hamming distance
between sequences in populations is much smaller when the population evolves with
natural selection. For example, in Fig. 4, we show that the hamming distance between
two specific branches in our tree structure nears 96 (almost completely different)
when there is no natural selection while the hamming distance asymptotes to
approximately 57 when there is natural selection. While this is only data from one
experiment, all our experiments show similar trends.
3.2   Natural Selection and Its Effect on Phylogeny Reconstruction
The question now is, will natural selection have any impact, harmful or beneficial, on
the effectiveness of phylogeny reconstruction algorithms. Our hypothesis is that
natural selection will improve the performance of phylogeny reconstruction
algorithms. Specifically, for the symmetric tree structures that we study, we predict
that phylogeny reconstruction algorithms will do better when at least one of the two
intermediate ancestors will have incorporated some mutations that significantly
improve its fitness. The resulting structures in the genome are likely to be preserved
in some fashion in the two descendant organisms making their pairing more likely.
Since the likelihood of this occurring increases as the internal edge length in our
symmetric tree structure increases, we expect to see the performance difference of
algorithms increase as the internal edge length increases.
The results from our experiments support our hypothesis. In Fig. 5, we show that
MP does no better on the Avida data than the random data when the internal edge
length is 6.  MP does somewhat better on the Avida data than the random data when
the internal edge length grows to 50. Finally MP does significantly better on the
Avida data than the random data when the internal edge length grows to 200.
3.3   Natural Selection via Location Probability Distributions
Is it possible to simulate the effects of natural selection we have observed by the
random data generator? In part 1, we observed that natural selection does have some
effect on the genome sequences. For example, mutations are frequently observed only
on part of the genome. If we tune the random data generator to use non-uniform
location probability distributions, is it possible to simulate the effects of natural
selection?
To answer this question, we collected data from 20 Avida experiments to
determine what the location probability distribution looks like with natural selection.
We first looked at how many positions typically are fixed (no mutations). Averaging
the data from the 20 Avida experiments, we saw that 21 % are fixed in a typical run.
We then looked further to see how many positions were stable (mutation rate <= 1%)
and how many positions were volatile (mutation rate > 1%) in a typical experiment.
Our results show that 35% of the positions are stable, and 44% of the positions are
volatile.

22         D. Hang et al.
Fig. 5. MP scores vs log of external edge length. The internal edge lengths of a, b and c are 6,
50 and 200.
From these findings, we set up our random data generator with three different
location probability distributions.  The first is the uniform distribution.  The second is
a two-tiered distribution where 20 of the positions are fixed (no mutations) and the
remaining 80 positions are equally likely. Finally, the third is a three-tiered
distribution where 21 of the positions were fixed, 35 were stable (mutation rates of
0.296%), and 44 were volatile (mutation rates of 2.04%).
Results from using these three different location probability distributions are
shown in Fig. 6. Random dataset A uses the three-tier location probability
distribution. Random dataset B uses the uniform location probability distribution.
Random dataset C uses the two-tier location probability distribution. We can see that
MP exhibits similar performance on the Avida data and the random data with the
three-tier location probability distribution.
Why does the three-tier location probability distribution seem to work so well?
We believe it is because of the introduction of the stable positions (low mutation
rates).  Stable positions with a low probability are more likely to remain identical in
the two final descendants that will make their final pairing more likely.
4   Future Work
While we feel that this preliminary work shows the effectiveness of using Avida to
evaluate the effect of natural selection on phylogeny reconstruction, there are several
important extensions that we plan to pursue in future work.
    1. Our symmetric tree structure has only four taxa. Thus, there is only one internal
edge and one bipartition. While this simplified the problem of determining if a
reconstruction was correct or not, the scenario is not challenging and the full power

The Effect of Natural Selection on Phylogeny Reconstruction Algorithms        23
a
   
              b
                   c
Fig. 6. MP scores from Avida data and 3 random datasets. The internal edge lengths of a, b and
c are 6, 50 and 200.
of algorithms such as maximum parsimony could not be applied. In future work, we
plan to examine larger data sets. To do so, we must determine a good method for
evaluating partially correct reconstructions.
2. We artificially introduced branching events. We plan to avoid this in the
future. To do so, we must determine a method for generating large data sets with
similar characteristics in order to derive statistically significant results.
3. We used a fixed-length genome, which eliminates the need to align sequences
before applying a phylogeny reconstruction algorithm. In our future work, we plan to
perform experiments without fixed length, and we will then need to evaluate sequence
alignment algorithms as well.
4. Finally, our environments were simple single niche environments. We plan to
use more complex environments that can support multiple species that evolve
independently.
Acknowledgements. The authors would like to thank James Vanderhyde for
implementing some of the tools used in this work, and Dr. Richard Lenski for useful
discussions. This work has been supported by National Science Foundation grant
numbers EIA-0219229 and DEB-9981397 and the Center for Biological Modeling at
Michigan State University.
References
1.
Hillis D.M.: Approaches for Assessing Phylogenetic Accuracy, Syst. Biol. 44(1) (1995) 3–
16
2.
Huelsenbeck J.P.: Performance of Phylogenetic Methods in Simulation, Syst. Biol. 44(1)
(1995) 17–48

24         D. Hang et al.
3.
Hillis D., Bull J.J., White M.E., Badgett M.R., Molineux L.J.: Experimental Phylogenetics:
Generation of a Known Phylogeny. Science 255 (1992) 589–592
4.
Ramnaut A. and Grassly N. C.: Seq-Gen: An application for the Monte Carlo simulation of
DNA sequence evolution along phylogenetic trees. Comput. Appl. Biosci. 13 (1997) 235–
238
5.
Ofria C., Brown C.T., and Adami C.: The Avida User‘s Manual, 297–350 (1998)
6.
Wilke C.O., Adami C.: The biology of digital organisms. TRENDS in Ecology and
Evolution, 17:11 (2002) 528–532
7.
Adami C., Ofria C., and Collier T.C.: Evolution of Biological Complexity. Proc. Natl.
Acad. Sci. USA 97 (2000) 4463–4468
8.
Wilke C.O., et. al.: Evolution of Digital Organisms at High Mutation Rates Leads to
Survival of the Flattest. Nature, 412 (2001) 331–333
9.
Lenski R.E., et. al.: Genome Complexity, Robustness, and Genetic Interactions in Digital
Organisms. Nature 400 (1999) 661–664
10. Elena S.F. and Lenski, R.E.: Test of Synergistic Interactions Among Deleterious Mutations
in Bacteria. Nature 390 (1997) 395–398
11. Gaut B.S. and Lewis P.O.: Success of Maximum Likelihood Phylogeny Inference in the
Four-Taxon Case, Mol. Biol. Evol 12(1) (1995) 152–162
12. Tateno Y., Takezaki N., and Nei M.: Relative Efficiencies of the Maximum-Likelihood,
Neighbor-joining, and Maximum Parsimony Methods When Substitution Rate Varies with
Site, Mol. Biol. Evol. 11(2) (1994) 261–277
13. Saitou N. and Nei M.,: The Neighbor-Joining Method: A New Method for Reconstructing
Phylogenetic Trees, Mol. Biol. Evol. 4 (1987) 406–425
14. Studier J. and Keppler K.: A Note on the Neighbor-Joining Algorithm of Saitou and Nei,
Mol. Biol. Evol. 5 (1988) 729–731
15. Fitch W.: Toward Defining the Course of Evolution: Minimum Change for a Specified
Tree Topology, Systematic Zoology, 20 (1971) 406–416
16. Lenski E., Ofria C., Collier C. and Adami C.: Genome Complexity, Robustness and
Genetic Interactions in Digital Organisms, Nature, 400 (1999) 661–664

AntClust: Ant Clustering and Web Usage
Mining
Nicolas Labroche, Nicolas Monmarch´e, and Gilles Venturini
Laboratoire d’Informatique de l’Universit´e de Tours,
´Ecole Polytechnique de l’Universit´e de Tours-D´epartement Informatique,
64, avenue Jean Portalis 37200 Tours, France
{labroche,monmarche,venturini}@univ-tours.fr
http://www.antsearch.univ-tours.fr/
Abstract. In this paper, we propose a new ant-based clustering algo-
rithm called AntClust. It is inspired from the chemical recognition sys-
tem of ants. In this system, the continuous interactions between the
nestmates generate a “Gestalt” colonial odor. Similarly, our clustering
algorithm associates an object of the data set to the odor of an ant and
then simulates meetings between ants. At the end, artiﬁcial ants that
share a similar odor are grouped in the same nest, which provides the
expected partition. We compare AntClust to the K-Means method and
to the AntClass algorithm. We present new results on artiﬁcial and real
data sets. We show that AntClust performs well and can extract mean-
ingful knowledge from real Web sessions.
1
Introduction
Numbers of computer scientists have proposed novel and successful approaches
for solving problems by reproducing biological behaviors. For instance, genetic
algorithms have been used in many research ﬁelds, such as clustering problems
[1],[2] and optimization [3]. Other examples can be found in the modeling of col-
lective behaviors of ants as in the well-known algorithmic approach Ant Colony
Optimization (ACO)([4]) in which pheromone trails are used. Similarly, ant-
based clustering algorithms have been proposed ([5], [6], [7]). In these studies,
researchers have modeled real ants abilities to sort their brood. Artiﬁcial ants
may carry one or more objects and may drop them according to given probabili-
ties. These agents do not communicate directly with each other’s, but they may
inﬂuence themselves through the conﬁguration of objects on the ﬂoor. Thus, af-
ter a while, these artiﬁcial ants are able to construct groups of similar objects,
a problem which is known as data clustering. We focus in this paper on an-
other important collective behavior of the real ants, namely the construction of
a colonial odor and its use to determine the ant nest membership.
Introduced in [8], the AntClust algorithm reproduces the main principles
of this recognition system. It is able to ﬁnd automatically a good partition
over artiﬁcial and real data sets. Furthermore, it does not need the number of
expected clusters to converge. It can also be easily adapted to any type of data
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 25–36, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

26
N. Labroche, N. Monmarch´e, and G. Venturini
(from numerical vectors to character strings and multimedia), since a distance
measure can be deﬁned between the vectors of attributes that describe each
object of the data set.
In this paper, we propose a new version of AntClust that does not need to be
parameterized to produce the ﬁnal partition. The paper is organized as follows:
the section 2 gives a detailed description of the AntClust algorithm. The section
3 presents the experiments that have been conducted to set the parameters
of AntClust regardless of the data sets. The section 4 compares the results of
AntClust to those of the K-Means method (initialized with the expected number
of clusters) and those of AntClass, an ant-based clustering algorithm. In the
section 5, we present some of the clustering algorithms already used in the Web
mining context and our very ﬁrst results when we apply AntClust to real Web
sessions. The last section concludes and discusses future evolutions of AntClust.
2
The AntClust Algorithm
The goal of AntClust is to solve the unsupervised clustering problem. It ﬁnds a
partition, as close as possible to the natural partition of the data set, without any
assumption concerning the deﬁnition of the objects or the number of expected
clusters. The originality of AntClust is to model the chemical recognition system
of ants to solve this problem. Real ants solve a similar problem in their every day
life, when the individuals that wear the same cuticular odor gather in the same
nest. AntClust associates an object of the data set to the genome of an artiﬁcial
ant. Then, it simulates meetings between artiﬁcial ants to exchange their odor.
We present hereafter the main principles of the chemical recognition system of
ants. Then, we describe the representation and the coding of the parameters of
an artiﬁcial ant and also the behavioral rules that allow the method to converge.
2.1
Principles of the Chemical Recognition System of Ants
AntClust is inspired from the chemical recognition system of ants. In this
biological system, each ant possesses its own odor called label that is spread
over its cuticle (its “skin”). The label is partially determined by the genome
of the ant and by the substances extracted from its environment (mainly the
nest materials and the food). When they meet other individuals, ants compare
the perceived label to their template that they learned during their youth. This
template is then updated during all their life by the mean of trophallaxies,
allo-grooming and social contacts. The continuous chemical exchanges between
the nestmates lead to the establishment of a colonial odor that is shared and
recognized by every nestmates, according to the “Gestalt theory” [9,10].

AntClust: Ant Clustering and Web Usage Mining
27
2.2
The Artiﬁcial Ants Model
An artiﬁcial ant can be considered as a set of parameters that evolve according
to behavioral rules. These rules reproduce the main principles of the recognition
system and apply when two ants meet.
For one ant i, we deﬁne the parameters and properties listed hereafter.
The label Labeli indicates the belonging nest of the ant and is simply coded
by a number. At the beginning of the algorithm, the ant does not belong to
a nest, so Labeli = 0. The label evolves until the ant ﬁnds the nest that best
corresponds to its genome.
The genome Genomei corresponds to an object of the data set. It is not
modiﬁed during the algorithm. When they meet, ants compare their genome to
evaluate their similarity.
The template Templatei or Ti is an acceptance threshold that is coded by a
real value between 0 and 1. It is learned during an initialization period, similar
to the ontogenesis period of the real ants, in which each artiﬁcial ant i meets
other ants, and each time evaluates the similarity between their genomes. The
resulting acceptance threshold Ti is a function of the maximal Max(Sim(i, ·))
and mean Sim(i, ·) similarities observed during this period. Ti is dynamic and
is updated after each meeting realized by the ant i, as the similarities observed
may have changed. The following equation shows how this threshold is learned
and then updated:
Ti ←Sim(i, ·) + Max(Sim(i, ·))
2
(1)
Once artiﬁcial ants have learned their template, they use it during their meetings
to decide if they should accept the encountered ants. We deﬁne the acceptance
mechanism between two ants i and j as a symmetric relation A(i, j) in which
the genomes similarity is compared to both templates as follows:
A(i, j) ⇔(Sim(i, j) > Ti) ∧(Sim(i, j) > Tj)
(2)
We state that there is “positive meeting” when there is acceptance between ants.
The estimator Mi indicates the proportion of meetings with nestmates. This
estimator is set to 0 at the beginning of the algorithm. It is increased each time
the ant i meets another ant with the same label (a nestmate) and decreased in
the opposite case. Mi enables each ant to estimate the size of its nest.
The estimator M +
i reﬂects the proportion of positive meetings with nestmates
of the ant i. In fact, this estimator measures how well accepted is the ant i in
its own nest. It is roughly similar to Mi but add the “acceptance notion”. It is
increased when ant i meets and accepts a nestmate and decreased when there is
no acceptance with the encountered nestmate.
The age Ai is set to 0 and is increased each time the ant i meets another
ant. It is used to update the maximal and mean similarities values and thus the
value of the acceptance threshold of the ant Templatei.
At each iteration, AntClust randomly selects two ants, simulates meetings
between them and applies a set of behavioral rules that enable the proper con-
vergence of the method.

28
N. Labroche, N. Monmarch´e, and G. Venturini
The 1st rule applies when two ants whith no nest meet and accept each other.
In this case, a new nest is created. This rule initiates the gathering of similar
ants in the very ﬁrst clusters. These clusters “seeds” are then used to generate
the ﬁnal clusters according to the other rules.
The 2nd rule applies when an ant with no nest meets and accepts an ant that
already belongs to a nest. In this case, the ant that is alone joins the other in
its nest. This rule enlarges the existing clusters by adding similar ants.
The 3rd rule increments the estimators M and M + in case of acceptance
between two ants that belong to the same nest. Each ant, as it meets a nestmate
and tolerates it, imagines that its nest is bigger and, as there is acceptance, feels
more integrated in its nest.
The 4th rule applies when two nestmates meet and do not accept each other.
In this case, the worst integrated ant is ejected from the nest. That rule permits
to remove non-optimally clustered ants to change their nest and try to ﬁnd a
more appropriate one.
The 5th rule applies when two ants that belong to a distinct nest meet and
accept each other. This rule is very important because it allows the gathering of
similar clusters, the small one being progressively absorbed by the big one.
The AntClust algorithm can be summarized as follows:
Algorithm 1: AntClust main algorithm
AntClust()
(1)
Initialization of the ants:
(2)
∀ants i ∈[1, N]
(3)
Genomei ←ith object of the data set
(4)
Labeli ←0
(5)
Templatei is learned during NApp iterations
(6)
Mi ←0, M +
i ←0, Ai ←0
(7)
NbIter ←75 ∗N
(8)
Simulate NbIter meetings between two randomly chosen ants
(9)
Delete the nests that are not interesting with a probability Pdel
(10)
Re-assign each ant that has no more nest to the nest of the most
similar ant.
3
AntClust Parameters Settings
It has been shown in [8] that the quality of the convergence of AntClust mainly
depends on three major parameters, namely the number of iterations ﬁxed to
learn the template NApp, the number of iterations of the meeting step NbIter
and ﬁnally, the method that is used to ﬁlter the nests. We describe hereafter how
we can ﬁx the value of these parameters regardless of the structure of the data
sets. First, we present our measure of the performance of the algorithm and the
data sets used for evaluation.

AntClust: Ant Clustering and Web Usage Mining
29
3.1
Performance Measure
To express the performance of the method we deﬁne Cs as 1−Ce, where Ce is the
clustering error. We choose an error measure adapted from the measure devel-
oped by Fowlkes and Mallows as used in [11]. The measure evaluates the diﬀer-
ences between two partitions by comparing each pair of objects and by verifying
each time if they are clustered similarly or not. Let Pi be the expected partition
and Pa the output partition of AntClust. The clustering success Cs(Pi, Pa) can
be deﬁned as follows:
Cs(Pi, Pa) = 1 −
2
N(N −1) ×

(m,n)∈[1,,N]2,m<n
ϵmn
(3)
where:
ϵmn =



0 if (Pi(m) = Pi(n) ∧Pa(m) = Pa(n))∨
(Pi(m) ̸= Pi(n) ∧Pa(m) ̸= Pa(n))
1 otherwise
(4)
with N the number of objects in the original data set. Pi(ob) (resp. Pa(ob)) is
the cluster number of the object ob in the partition Pi (resp. Pa).
We use artiﬁcial data sets named Art1,2,3,4,5,6 for our evaluations. We gener-
ate them according to gaussian or uniform laws with distinct diﬃculties (irrele-
vant attributes, clusters overlap).
3.2
How Many Meetings?
For the time being, the user has to specify the number of iterations during which
artiﬁcial ants meet, if she wants to adapt the algorithm to the size of the data
set that is explored. AntClust has a default value that is set to 50000 iterations.
Nevertheless, in the Web mining context, it is usual to work with larger data
sets. In this case, AntClust cannot guarantee a proper convergence since some
ants are assigned to a nest without having realized any meeting. Our idea is
to consider that the number of iterations may be linearly linked to the number
of ants (i.e. the number of objects in the data set). In the method, at each
iteration, 2 ants are randomly and uniformly selected in the population. Let α
denote the minimal number of iterations each ant has to perform to ensure the
convergence of the algorithm. The following equation shows how we associate
the total number of iterations NbIter to the number of ants N:
NbIter = 1
2 ∗α ∗N
(5)
We evaluate the performance of the method for several values of α between
10 and 500 meetings per ant. The goal is to verify that α can be initialized
regardless of the size of the data set. The ﬁgure 1 shows the results in term of
mean clustering success. We have conducted 10 tests for each value of α and
each artiﬁcial data set.

30
N. Labroche, N. Monmarch´e, and G. Venturini
Mean clustering success percentage versus number of 
meetings par ant (α)
0
20
40
60
80
100
120
10
40
70
100
130
160
190
220
250
280
310
340
370
400
430
460
490
Number of meetings per ant (α)
Mean clustering success 
percentage (10 runs)
art3
art5
art6
Fig. 1. Mean clustering success over 10 runs for each value of α ∈[10, 500].
According to this ﬁgure, we can see that the values of the mean clustering
success converge quite quickly regardless of the data set. Although the data sets
size ranges from 200 to 1100 objects, the convergence seems to operate for the
same value of α in every case. Thus, we may consider that there is a minimal
number of meetings that each ant should realize to ensure the proper convergence
of the method. We consider that we can set experimentally the value of α to 150.
As a consequence, the meetings step of AntClust can be solved in linear time
with the number of objects in the data set.
3.3
How Many Iterations to Learn the Template?
We now focus on the number of iterations needed to learn the template. We
think that the template learning process cannot be longer, in term of number
of iterations, than the meetings step studied before. Let β be the number of
meetings needed to learn the template per ant. As for the number of iterations
of the meeting step, we test several values of β expressed as a percentage of the
value of α ranging from 0% to 100%. The ﬁgure 2 presents the results that we
obtained for each artiﬁcial data sets in term of mean clustering success according
to the value of β.
As with the previous experiment, the results of mean clustering success con-
verges even if the limit is not as clearly expressed in the plotted graph. It is
important to notice that β is not necessarily positively linked to the perfor-
mance. For example, the mean clustering success of Art6 remains stable as β
increases, whereas the performances of Art4 decrease. This can be explained by
the fact that the number of clusters is taken into account to compute Cs and
that as β increases the error in the estimated number of clusters also increases

AntClust: Ant Clustering and Web Usage Mining
31
Mean clustering success percentage versus number of 
template learning iterations (expressed as a % of α =
150)
0
20
40
60
80
100
120
1
7
13
19
25
31
37
43
49
55
61
67
73
79
85
91
97
Number of template learning iterations (expressed as a % of 
α = 150)
Mean clustering success 
percentage (10 runs)
art4
art5
art6
Fig. 2. Mean clustering success over 10 runs for each β ∈[1, 100]% of α.
for Art4. In fact, by increasing β, artiﬁcial ants become too sensitive and ﬁnd
too much clusters. As a conclusion, we set β as 0.5∗α, that is to say, NApp ←75.
3.4
The Nest Deletion Method
At the end of AntClust, the nests that are not suﬃciently “interesting” are
deleted and the ants are reassigned to the nest of the most similar ant. This
method allows suppressing noise in the ﬁnal partition. For the time being, the
nest deletion criterion is only based on the number of ants that belong to this nest
and a threshold ﬁxed by the user. The default value of this threshold is equal to a
percentage of the total number of ants (generally 15%). This approach, although
eﬃcient, is limited, because the algorithm could not ﬁnd more than a ﬁxed
number of clusters of the same size. Our optimization replaces this deterministic
method by a probabilistic one that is more adaptive. We compute for each nest
η a probability Pdel to be deleted. This probability depends mainly on the mean
integration of the ants in the nest η, M +
η , and the number of ants in the nest
Nη as in the previous version. For the nest η, Pdel(η) is given as follows:
Pdel(η) ←(1 −ν) ∗M +
η + ν ∗Nη
N
(6)
Several experimentations have been conducted and revealed that the value
of ν = 0.2 provides the best partitions for the set of data we tested. Hence, this
method is more interesting than the previous one, because it allows to better
appreciate the number of clusters thanks to its probabilistic nature.

32
N. Labroche, N. Monmarch´e, and G. Venturini
4
Experiments and Results
In this section, we compare AntClust to the K-Means method and the AntClass
algorithm to evaluate the performance of our method. First, we will brieﬂy de-
scribe both methods. Second, we introduce the main properties of the data sets
that are used for the comparison. Then, we present our results over artiﬁcial and
real data sets.
4.1
K-Means and AntClass
We use two clustering algorithms to evaluate AntClust. First, we choose to apply
a traditional K-Means approach because, it can perform well in a short time.
This method needs an initial K-partition of the data set that is reﬁned gradually.
At each iteration, the objects are assigned to the most similar center of the
clusters. The algorithm stops when the intra class inertia becomes stable. In our
test, we generate randomly the initial partitions with the number of clusters that
is expected in the data set, to get the best results that a K-Means approach can
give.
Second, we compare AntClust to AntClass, an other ant-based clustering
algorithm. This is an hybrid algorithm in which artiﬁcial ants create a ﬁrst
partition of the data set that is used by a K-Means algorithm as initial partition.
This scheme is repeated twice in order to get the ﬁnal partition. The ant-based
part of AntClass relies on the establishment of a 2 dimensional grid. The objects
and the artiﬁcial ants are randomly placed on the grid. At each iteration, ants
move and have a probability to pick-up or to drop an object depending if they
have already one or not. Artiﬁcial ants generate heaps of similar objects that
deﬁne the partition.
4.2
Data Sets and Experimental Protocol
We evaluate the clustering methods over several data sets that represent distinct
clustering diﬃculties in the same experimental conditions in order to better
appreciate the performance of each of them. First, we test K-Means, AntClass
and AntClust over the artiﬁcial data sets that have been previously introduced
in the section 3.1, namely Art1,2,3,4,5,6.
Second, we test the algorithms over real data sets such as Iris, Pima, Soybean,
Glass and Thyroid. We expect these data sets to be more diﬃcult to be cluster,
since they may be noisier than artiﬁcial data sets. We introduce in the table 1
the parameters that characterize the artiﬁcial and real data sets used for our
evaluations. The ﬁelds for each data set are: the number of objects (N ), their
associated number of attributes (M ), and the number of clusters (K).
For each data set, we run 50 times each method and compute the mean
clustering success (see section 3.1), the mean number of clusters found and their
respective standard deviations. The next section presents our results.

AntClust: Ant Clustering and Web Usage Mining
33
Table 1. Main characteristics of the data sets.
Art1 Art2
Art3
Art4
Art5
Art6
N 400
1000
1100
200
900
400
M
2
2
2
2
2
8
K
4
2
4
2
9
4
Iris Glass Pima Soybean Thyroid
N 150
214
798
47
215
M
4
9
8
35
5
K
3
7
2
4
3
4.3
Results with Artiﬁcial and Real Data Sets
The table 2 shows the results obtained over the artiﬁcial and real data sets.
We can see in this table that K-Means gives, in general, the best results in
term of clustering success and obviously in term of number of clusters found
as they are provided to the algorithm. AntClust has the best clustering results
only twice for the data sets Soybean and Thyroid that are little data sets
(with respectively 47 and 215 objects). Nevertheless, AntClust shows its ability
to treat large data sets with Art2 and Art3. AntClass performs better than
AntClust three times, for Art1, Art5 and Iris. For Art1 and Art5, AntClass
better estimates the number of expected clusters, but for Iris, the situation is
reversed. This means that in this case, AntClust creates poorer quality clusters
than those of AntClass. In all the other experiments K-Means and AntClust are
more eﬃcient than AntClass. One thing to point out is that when K-Means fails
(for example in the data sets Glass and Pima), AntClass (which is partially
based on K-Means method) and AntClust also behaves poorly. Nevertheless,
AntClust performs well in general and can be even more eﬃcient than K-Means
for which the number of clusters is provided.
To complete the evaluation of AntClust, we compare its complexity to those
of K-Means and AntClass. Let Cx be the complexity of the algorithm x. AntClust
can be splitted in several steps: the template learning process (θ(N × NApp)),
the creation of the nests (θ(N × NbIter)) and ﬁnally the nests deletion process
and the re-assignment of the ants to a nest. This step runs in quadratic time
since each ant that has no more nest has to ﬁnd the the most similar ant that
belongs to a nest. The complexity of AntClust is CAntClust = θ(N 2) in the worst
case. The K-Means algorithm’s complexity is known to be CKMeans = θ(N) and
AntClass has also a linear complexity CAntClass = θ(N). Experimentally, our
tests revealed that K-Means is the quickest method and that AntClust runs
faster than AntClass.

34
N. Labroche, N. Monmarch´e, and G. Venturini
Table 2. Mean number of clusters (# clusters) and mean success (Success) and their
standard deviation for each data set and each method computed over 50 runs.
# clusters
Success
K-Means AntClass AntClust K-Means AntClass AntClust
Data sets mean [std] mean [std] mean [std] mean [std] mean [std] mean [std]
Art1
3.98
[0.14] 4.22
[1.15] 4.70
[0.95] 0.89
[0.00] 0.85
[0.05] 0.78
[0.03]
Art2
2.00
[0.00] 12.32 [2.01] 2.30
[0.51] 0.96
[0.00] 0.59
[0.01] 0.93
[0.02]
Art3
3.84
[0.37] 14.66 [2.68] 2.72
[0,88] 0.78
[0.02] 0.65
[0.01] 0.85
[0.02]
Art4
2.00
[0.00] 1.68
[0.84] 4.18
[0.83] 1.00
[0.00] 0.71
[0.23] 0.77
[0.05]
Art5
8.10
[0.75] 11.36 [1.94] 6.74
[1.66] 0.91
[0.02] 0.92
[0.01] 0.74
[0.02]
Art6
4.00
[0.00] 3.74
[1.38] 4.06
[0.24] 0.99
[0.04] 0.89
[0.13] 0.95
[0.01]
Iris
2.96
[0.20] 3.52
[1.39] 2.82
[0.75] 0.86
[0.03] 0.81
[0.08] 0.78
[0.01]
Glass
6.88
[0.32] 5.60
[2.01] 5.90
[1.23] 0.68
[0.01] 0.60
[0.06] 0.64
[0.02]
Pima
2.00
[0.00] 6.10
[1.84] 10.66 [2.33] 0.56
[0.00] 0.53
[0.02] 0.54
[0.01]
Soybean 3.96
[0.20] 1.60
[0.49] 4.16
[0.55] 0.91
[0.08] 0.46
[0.17] 0.93
[0.04]
Thyroid 3.00
[0.00] 5.84
[1.33] 4.62
[0.90] 0.82
[0.00] 0.78
[0.09] 0.84
[0.03]
5
AntClust for Web Usage Mining
For the time being, a lot of research eﬀorts have been conducted to cluster
user sessions extracted from Web servers log ﬁles. The recurrent problem in this
area is that clustering algorithms must be able to treat large data sets with an
aﬀordable computing time. Actually, a single Web server log ﬁle may contain
several hundred thousand requests for Web pages. As a consequence, researchers
try to use algorithms that run fast in their ﬁrst approaches.
In [12], Yan et al. use a “First leader clustering algorithm”. The sessions
are expressed as numerical vectors containing for each Web page, the number of
recorded impacts. Although results are very promising and quickly computed,
this method is limited since the ﬁnal partition depends on the order of the
sessions in the data set.
Heer and Chi introduce in [13] the Wavefront algorithm that improves the
initialization step of the K-Means method. The cluster seeds are randomly gen-
erated according to an estimated center of gravity of the data set. According
to the authors, this method allows a quicker convergence. In their work, the
Web sessions are expressed as multi-modal vectors that take into account the
navigation of the users (the time spent for each page) and model a page as a
combination of structure and content information.
Finally, Estivill-Castro et al. propose in [14] a robust clustering algorithm
that is mainly a K-Means algorithm in which the median estimator is used
instead of the mean estimator. The algorithm is then more resistant to the noise
in the data sets.
There are two major limitations to use “K-Means like” algorithms in the
Web usage mining context. First, the number of clusters K has to be provided
to ensure a good convergence of the method, but it can’t be easily set, unless

AntClust: Ant Clustering and Web Usage Mining
35
the user is able to guess how people navigate on her Web site, which is exactly
the goal of the clustering process. Second, mean values have to be computed
and limit the coding of the Web sessions to numerical expressions (diﬃculties
for keywords or multimedia contents coding). Finally, mean values may not have
any meaning in the Web sessions context.
5.1
Web Session Data
The last data set that we explore with AntClust is a Web server log ﬁle. We sort
and ﬁlter this raw ﬁle to get a data set composed of users sessions. A session
captures the activity of a user on a Web site during a speciﬁed period of time.
The sessions have been recorded for one month in October 2001, on a web site
of the University of Tours that contains computer sciences courses. The 1064
reconstructed sessions contain 667 unique sessions, that is to say, sessions that
come from an IP number that has been seen only once in the requests log ﬁle.
Consequently, our sessions may reﬂect a lot of distinct behaviors from the users,
and thus may be very noisy. Nevertheless, as there are few hyperlinks between
the online courses, we expect the clusters to be representative of a minimum of
courses in order to be valuable and understandable. Similarly to other works,
we coded the Web sessions as vectors where each component corresponds to the
number of hits recorded for each page.
5.2
Results
When applied on Web sessions, AntClust ﬁnds 17 clusters. The 3 largest clusters
contain the half of the sessions. Although the two largest clusters refer to 2 or 3
online computer sciences courses, the others generally reﬂect interests for only
one course. This probably means that a majority of users went to the Web site
with no speciﬁc goal and look at several courses to evaluate the content of the
diploma. The other users were certainly already students and were searching for
lecture notes relative to one topic. This little experiment proves that AntClust is
able to generate a non-noisy partition of Web sessions that can help understand-
ing the interests of the Web users. AntClust spends approximately 1.33 minutes
at 650 MHz to cluster the user sessions, which is an aﬀordable time.
6
Conclusion and Perspectives
AntClust is a new clustering algorithm that models the chemical recognition
system of real ants. It associates one object of the data set to one ant and deﬁnes
the expected partition as a set of nests. In this paper, we show how the internal
parameters of the method can be set, regardless of the structure of the data set
to be explored. Furthermore, we develop a non-deterministic approach to delete
the uninteresting nest and to re-aﬀect the ants which nest has been deleted. We
evaluate its performance against K-Means and AntClass with artiﬁcial and real
data sets. We show that AntClust can even do better than K-Means for two data

36
N. Labroche, N. Monmarch´e, and G. Venturini
sets. When applied to Web sessions, AntClust ﬁnds meaningful clusters that can
help understanding the interests of the users of the Web site. Our future works
will try to apply AntClust to larger Web data sets to evaluate its robustness. In
these experiments we will compare AntClust to other clustering algorithms in
the Web context. We also plan to develop an incremental version of AntClust
and our ﬁrst results seem to be promising.
References
1. Y. Chiou and L. W. Lan, “Genetic clustering algorithms,” European journal of
Operational Research, no. 135, pp. 413–427, 2001.
2. L. Y. Tseng and S. B. Yang, “Genetic clustering algorithms,” European journal of
Operational Research, no. 135, pp. 413–427, 2001.
3. N. Monmarch´e, G. Venturini, and M. Slimane, “On how Pachycondyla apicalis ants
suggest a new search algorithm,” Future Generation Computer Systems, vol. 16,
no. 8, pp. 937–946, 2000.
4. A. Colorni, M. Dorigo, and V. Maniezzo, “Distributed optimization by ant
colonies,” in Proceedings of the First European Conference on Artiﬁcial Life
(F. Varela and P. Bourgine, eds.), pp. 134–142, MIT Press, Cambridge, Mas-
sachusetts, 1991.
5. E. Lumer and B. Faieta, “Diversity and adaptation in populations of clustering
ants,” in Cliﬀet al. [15], pp. 501–508.
6. P. Kuntz and D. Snyers, “Emergent colonization and graph partitioning,” in Cliﬀ
et al. [15], pp. 494–500.
7. N. Monmarch´e, M. Slimane, and G. Venturini, “On improving clustering in nu-
merical databases with artiﬁcial ants,” in Lecture Notes in Artiﬁcial Intelligence
(D. Floreano, J. Nicoud, and F. Mondala, eds.), (Swiss Federal Institute of Tech-
nology, Lausanne, Switzerland), pp. 626–635, Springer-Verlag, 13–17 September
1999.
8. N. Labroche, N. Monmarch´e, and G. Venturini, “A new clustering algorithm based
on the chemical recognition system of ants,” in Proc. of 15th European Conference
on Artiﬁcial Intelligence (ECAI 2002), Lyon FRANCE, pp. 345–349, 2002.
9. B. H¨olldobler and E. Wilson, The Ants. Springer Verlag, Berlin, Germany, 1990.
10. N.
Carlin
and
B.
H¨olldobler,
“The
kin
recognition
system
of
carpenter
ants(camponotus spp.). i. hierarchical cues in small colonies,” Behav Ecol Sociobiol,
vol. 19, pp. 123–134, 1986.
11. J. Heer and E. Chi, “Mining the structure of user activity using cluster stability,” in
Proceedings of the Workshop on Web Analytics, SIAM Conference on Data Mining
(Arlington VA, April 2002)., 2002.
12. T. Yan, M. Jacobsen, H. Garcia-Molina, and U. Dayal, “From user access patterns
to dynamic hypertext linking,” in Proc. of 5th WWW, pp. 1007–1014, 1996.
13. J. Heer and E. Chi, “Identiﬁcation of web user traﬃc composition using multi-
modal clustering and information scent,” 2001.
14. V. Estivill-Castro and J. Yang, “Categorizing visitors dynamically by fast and
robust clustering of access logs,” Lecture Notes in Computer Science, vol. 2198,
pp. 498–509, 2001.
15. D. Cliﬀ, P. Husbands, J. Meyer, and S. W., eds., Third International Conference
on Simulation of Adaptive Behavior: From Animals to Animats 3, MIT Press,
Cambridge, Massachusetts, 1994.

A Non-dominated Sorting Particle Swarm
Optimizer for Multiobjective Optimization
Xiaodong Li
School of Computer Science and Information Technology
RMIT University, VIC 3001, Melbourne, Australia
xiaodong@cs.rmit.edu.au
http://www.cs.rmit.edu.au/˜xiaodong
Abstract. This paper introduces a modiﬁed PSO, Non-dominated Sort-
ing Particle Swarm Optimizer (NSPSO), for better multiobjective opti-
mization. NSPSO extends the basic form of PSO by making a better
use of particles’ personal bests and oﬀspring for more eﬀective non-
domination comparisons. Instead of a single comparison between a par-
ticle’s personal best and its oﬀspring, NSPSO compares all particles’
personal bests and their oﬀspring in the entire population. This proves
to be eﬀective in providing an appropriate selection pressure to propel
the swarm population towards the Pareto-optimal front. By using the
non-dominated sorting concept and two parameter-free niching methods,
NSPSO and its variants have shown remarkable performance against a
set of well-known diﬃcult test functions (ZDT series). Our results and
comparison with NSGA II show that NSPSO is highly competitive with
existing evolutionary and PSO multiobjective algorithms.
1
Introduction
Multiobjective optimization problems represent an important class of real-world
problems. Typically such problems involve trade-oﬀs. For example, a car man-
ufacturer may wish to maximize its proﬁt, but meanwhile also to minimize its
production cost. These objectives are typically conﬂicting to each other. A higher
proﬁt would increase the production cost. There is no single optimal solution.
Often the manufacturer needs to consider many possible “trade-oﬀ” solutions
before choosing the one that suits its need. The curve or surface (for more than
2 objectives) describing the optimal trade-oﬀsolutions between objectives is
known as the Pareto front. One of the major goals in multiobjective optimiza-
tion is to ﬁnd a set of well distributed optimal solutions along the Pareto front.
In recent years, population-based optimization methods such as Evolution-
ary Algorithms have become increasingly popular for solving multiobjective op-
timization problems [1][2]. EA’s success is due to its generic ability to han-
dle large complex real-world problems. Since EAs maintain a population of
solutions, this allows exploration of diﬀerent parts of the Pareto front simul-
taneously. However not until recently, another population-based optimization
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 37–48, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

38
X. Li
technique Particle Swarm Optimization (PSO) has been only applied to sin-
gle objective optimization tasks. PSO technique is inspired by studies of so-
cial behavior of insects and animals [3]. The social behavior is modeled in a
PSO to guide a population of particles (so-called swarm) moving towards the
most promising area of the search space. In PSO, Each particle represents a
candidate solution, Xi = (xi1, xi2, . . . , xid). d is the dimension of the search
space. The i-th particle of the swarm population knows: a) its personal best
position Pi = (pi1, pi2, . . . , pid), i.e., the best position this particle has vis-
ited so far that yields the highest ﬁtness value; and b) the global best posi-
tion, Pg = (pg1, pg2, . . . , pgd), i.e., the position of the best particle that gives
the best ﬁtness value in the entire population; and c) its current velocity,
Vi = (vi1, vi2, . . . , vid), which represents its position change; The following equa-
tion (1) uses the above information to calculate the new updated velocity for
each particle in the next iteration step. Equation (2) updates the each particle’s
position in the search space.
vid = wvid + c1r1(pid −xid) + c2r2(pgd −xid)
(1)
xid = xid + χvid ,
(2)
where d = 1, 2, . . . , D; i = 1, 2, . . . , N; N is the size of the swarm population; χ
is a constriction factor which controls and constricts the velocity’s magnitude;
w is the inertia weight, which is often used as a parameter to control explo-
ration/exploitation in the search space; c1 and c2 are two coeﬃcients (positive
constants); r1 and r2 are two random numbers within the range [0, 1]. There is
also a VMAX, which sets the upper and lower bound for velocity values.
PSO has proved to be an eﬃcient optimization method for single objective
optimization, and more recently has also shown promising results for solving
multiobjective optimization problems [4][5] [6][7]. What is in common among
these works is the use of a basic form of PSO ﬁrst introduced by Kennedy and
Eberhart [3]. However the basic form of PSO has some serious limitations in
particular when dealing with multiobjective optimization problems. In PSO, a
particle is modiﬁed only through its personal best and global best to produce
its oﬀspring. At each iteration step, if the ﬁtness of the oﬀspring is better than
the parent’s personal best, then the personal best is updated with this oﬀspring,
however, there is no sharing of information with other particles in the population,
except that each particle can access the global best. For multiobjective optimiza-
tion, we argue that such sharing of information among all the individuals in a
population is crucial in order to introduce the necessary selection pressure to
propel the population moving towards the true Pareto-optimal front.
This paper introduces a modiﬁed PSO, Non-dominated Sorting Particle
Swarm Optimizer (NSPSO), which is able to increase such “sharing” among
all particles in a swarm population especially concerning how to allow the popu-
lation as a whole to progress towards the true Pareto-optimal front. For clarity,
in this paper, we use P ∗to denote the true Pareto-optimal front, and Q the
found non-dominated solution set.

A Non-dominated Sorting Particle Swarm Optimizer
39
Fig. 1. Dominance relationships among 4 particles, including the personal best P t
1 of
a particle Xt
1, and its potential oﬀspring Xt+1
1
, plus P t
2 and Xt+1
2
for a second particle
Xt
2, assuming minimization of f1 and f2.
2
Modifying PSO for Better Dominance Comparison
One problem that can be identiﬁed with the basic form PSO is that dominance
comparisons are not fully utilized in the process of updating the personal best
Pi of each particle. This can be illustrated via the following example shown in
Fig. 1. Note that F(·) denotes the evaluation of a particle in the objective space.
Fig.1 shows that the personal best P t
1 is mutually non-dominating with Xt+1
1
,
and P t
2 is non-dominating with Xt+1
2
, however, Xt+1
1
is dominated by Xt+1
2
and
P t
2 , and furthermore, P t
1 is also dominated by P t
2. In a standard PSO, the i-th
particle only has a single P t
i , which is used to compare with its potential oﬀspring
Xt+1
i
at time step t+1. If P t
i is non-dominating with its potential oﬀspring such
as the situation shown in Fig. 1, then P t
i will remain the same. The consequence
of this kind of comparison is that the useful non-domination relationships among
all the four particles will not be captured. Fig. 1 shows that if we allow all 4
particles to be compared, then we would have found P t
2 and Xt+1
2
to be the
better two to retain. This illustration shows that in a standard PSO, valuable
non-domination comparisons are not eﬀectively used. In other words, as a result
of having only a single comparison between a particle’s P t
i and Xt+1
i
, there is
only a very weak selection pressure with respect to the non-dominated front that
exists in the current population.
This would naturally lead to the next two questions - what if we allow all
the personal bests of all particles and as well as these particles’ oﬀspring to be
compared for non-domination relationships? How are we going to choose the
global best for each particle in order to propel the population to move towards
P ∗, while also maintaining a diverse set of solutions?
3
Non-dominated Sorting PSO
Two major goals in multiobjective optimization are to obtain a set of non-
dominated solutions as closely as possible to the true Pareto front P ∗, and to

40
X. Li
maintain a well-distributed solution set along the Pareto front. To achieve this
using PSO, a Non-dominated Sorting Particle Swarm Optimizer (NSPSO) is
proposed. In NSPSO, we adopt the non-dominated sorting concept used in NSGA
II [2], where the entire population is sorted into various non-domination levels.
This provides the means for selecting the individuals in the better fronts, hence
providing the necessary selection pressure to push the population towards P ∗.
To maintain population diversity, we use a widely-used niching method [8], and
also the crowding distance assignments adopted by NSGA II [9]. The following
two sections describe these methods.
3.1
Selection Pressure towards P ∗
Instead of comparing solely on a particle’s personal best with its potential oﬀ-
spring, the entire population of N particles’ personal bests and N of these parti-
cles’ oﬀspring are ﬁrst combined to form a temporary population of 2N particles.
After this, domination comparisons among all the 2N individuals in this tem-
porary population are carried out. This “combine-then-compare” approach will
ensure more non-dominated solutions can be discovered through the domination
comparison operations. Since updating the personal bests of the N particles rep-
resents “collectively” a step towards a better region in the search space, retaining
them for domination comparison provides the needed selection pressure towards
P ∗. By comparing the combined 2N particles for non-domination relationships,
we will be able to sort the entire population in diﬀerent non-domination levels as
used in NSGA II. This type of sorting can then be used to introduce the selection
bias to the individuals in the populations, in favour of individuals closer to the
true Pareto front P ∗. At each iteration step, we choose only N individuals out
of the 2N to the next iteration step, based on the non-domination levels. This
process can be illustrated in Fig. 2. First the entire population is sorted into
Fig. 2. Particles of a swarm population of 10 are classiﬁed into 4 successive non-
dominated fronts
two sets, the non-dominated set and the remaining dominated set. In Fig.2, the
non-dominated set is Front 1, which contains 3 particles labeled as 1, 2 and 3.
Front 1 is the best non-dominated set, since all particles in Front 1 are not dom-
inated by any other particles in the entire population. To obtain the next front,

A Non-dominated Sorting Particle Swarm Optimizer
41
Front 2, we temporarily remove Front 1 from the population, then ﬁnd the non-
dominated solutions of the remaining population, which is Front 2. Then again
we remove Front 2 as well, in order to identify the non-dominated solutions of
the next level. This procedure continues until all particles in the population are
classiﬁed into diﬀerent non-dominated front levels. For each particle, there are
O(mN) comparisons are required to ﬁnd out if it is dominated by other part-
ciles in a population of size N (m is the number of objectives). The complexity
to ﬁnd the ﬁrst non-dominated front (i.e., Front 1) for the whole population is
O(mN 2). In the worst case where there is only one particle in each front, the
complexity of the above procedure for classifying diﬀerent non-dominated fronts
is O(mN 3).
Now we create the new particle population for the next iteration step, by
selecting particles from fronts in ascending order, e.g., ﬁrst from Front 1, then
Front 2, etc, until N particles (or a speciﬁed threshold) are selected. Since the
particles in the ﬁrst few fronts get chosen ﬁrst, this selection pressure will eﬀec-
tively drive the particle population towards the best front over many iteration
steps. Note that Front 1 could have more than N particles (since the combined
2N particles are sorted), especially after a number of steps in a run. Setting
a threshold may be necessary as it would allow opportunities of particles from
other fronts to be selected as well, i.e., maintaining “lateral diversity” [2].
3.2
Parameter-Free Niching Methods to Maintain a Diverse Q
Niching methods have been extensively studied and used as means of maintaining
population diversity in Genetic Algorithms. One commonly used niching method
is a sharing function model introduced by Goldberg and Richardson [10], where
a niche is treated as a resource shared among other individuals in the niche.
In NSPSO, to achieve the second goal of maintaining a diverse non-dominated
solution set, two niching methods are tried, in order to see how eﬀective each
method is in maintaining solution diversity. The ﬁrst method requires calculating
a niche count for each particle, whereas the second method requires calculating
a crowding distance value for each particle.
Using Niche Count. In NSPSO, the niche count mi of a particle i is simply
calculated as the number of other particles within a σshare distance (i.e., Eu-
clidean distance) from i. Note that σshare can be calculated dynamically at each
iteration step. Fig.3 shows how niche counts are calculated for two candidate
solution A and B. Both A and B are on the current non-dominated front. How-
ever since A has a smaller niche count than B, A will be preferred over B. This
choice has the eﬀect of emphasizing a more diverse non-dominated front.
One of the undesirable features of the niching method employing σshare is
that σshare has to be speciﬁed by a user, and the model’s performance is highly
dependant on the choice of value for this parameter. We adopted the dynamic
update of σshare proposed by Fonseca and Fleming [11] so that we do not have
to specify σshare. For two objective functions, the following equation is used to
determine the σshare dynamically [2]:

42
X. Li
Fig. 3. Niche counts are calculated for particle A and B on the non-dominated front
Q (indicated by circles), assuming minimization of f1 and f2.
σshare = u2 −l2 + u1 −l1
N −1
,
(3)
where ui and li are the upper and lower bounds for each of the two objective
values for the entire population. Note that as population size increases, the
σshare is reduced to accommodate more niches. At each iteration step, we select
from the current non-dominated solution set Q those particles with smallest
niche counts. Then Pg for each particle is randomly chosen among these “less
crowded” non-dominated particles.
Using Crowding Distance Assignments. The 2nd niching method employ-
ing crowding distance [9] is also free of choosing such a parameter. Deb et al. [9]
in their NSGA II introduced this niching method that makes use of the density
of solutions around a particular point on the non-dominated front. The density
is estimated by calculating the so-called crowding distance of a point i , which
is the average distance of the two point i-1 and i+1 on either side of this point i
along each of the objectives. When we use crowding distance values for niching,
we simply sort all the particles of the current Q in descending order, and then
choose a particle randomly from the top part of the sorted list (i.e., a particle
in the least crowded areas in the Pareto region) as Pg for each particle. This
process is repeated for each particle in the population, and over many itera-
tion steps. The complexity of this procedure in the worst case is O(mNlogN),
when all particles are in one front. For more information on crowding distance
assignment, the readers can refer to [9].
Replacement of “Overcrowded” Particles with New Particles. Another
method that can further promote diversity is to remove particles from over-
crowded areas on the current non-dominated front Q, and replace them with
new particles. We implement this in NSPSO by removing the particle with the
largest niche count (or the smallest crowding distance value), and replace it with
a randomly generated new particle, at each iteration step. Since the new par-
ticle chooses the particle with the smallest niche count, i.e., least crowded (or

A Non-dominated Sorting Particle Swarm Optimizer
43
the largest crowding distance value), from the current Q as its Pg, this particle
should have a better chance to land somewhere “less crowded” on the current
front Q.
3.3
NSPSO Algorithm
NSPSO can be summarized in the following steps:
1. Initialize the population and store the population in a list P SOList:
a) The current position of the i-th particle, Xi and its current velocity Vi, are initialized with
random real numbers within the speciﬁed decision variable range; Vi has a probability of
0.5 being speciﬁed in a diﬀerent direction; The personal best position Pi, is set to Xi ;
VMAX is set to the upper and lower bounds of the decision variable range.
b) Evaluate each particle in the population; iteration counter t := 0.
2. t := t + 1.
3. Identify particles that give non-dominated solutions in the population and store them in a list
nonDomP SOList.
4. Calculate - a) niche count, or b) crowding distance value, for each particle.
5. Resort the nonDomP SOList according to a) niche counts, or b) crowding distance values.
6. F or(i := 0; i < numP articles; i++) (step through P SOList):
a) Select randomly a global best Pg for the i-th particle from a speciﬁed top part (e.g. top
5%) of the sorted nonDomP SOList.
b) Calculate the new velocity Vi, based on the equation (1), and the new Xi by equation (2).
c) Add the i-th particle’s Pi
and the new Xi
to a temporary population, stored in
nextP opList. Note that Pi and Xi now coexist. Also note that nextP opList now has
a size of 2N.
d) Go to a) if i < numP articles.
7. Identify particles that give non-dominated solutions from nextP opList, and store them in
nonDomP SOList. Particles other than non-dominated ones from nextP opList are stored in a
list nextP opListRest.
8. Empty P SOList for the next iteration step.
9. Select randomly members of nonDomP SOList and add them to P SOList (not to exceed
numP articles).
10. Loop if P SOList size < numP articles:
a) Identify
non-dominated
particles
from
nonDomListRest
and
store
them
in
nextNonDomList.
b) Add
members
of
nextNonDomList
to
P SOList,
if
still
the
P SOList
size
<
numP articles.
c) Copy nextP opListRest to nextP opListRestCopy, then empty nextP opListRest.
d) Assign the vacant nextP opListRest with the remaining particles other than non-
dominated ones from nextP opListRestCopy.
e) Go back to a), if still the P SOList size < numP articles.
11. If t < maxIterations, go to 2.
12. Obtain Q from the ﬁnal population, and calculate the performance metric values (see section
4).
4
Performance Metrics
Diversity of Q. We measure the diversity of solutions along the Pareto front
in the ﬁnal population by comparing the uniform distribution and the deviation
of solutions as described by Deb [2]:
∆= ΣM
m=1de
m + Σ|Q|
i=1|di −¯d|
ΣM
m=1dem + |Q| ¯d
,
(4)
where di is the distance between two neighbouring solutions in the non-
dominated solution set Q, ¯d is the mean value of all the di. de
m is the distance
between the extreme solutions of the true Pareto-optimal set P ∗and Q on the

44
X. Li
m-th objective. de
m is calculated by using Schott’s diﬀerence distance measure
[2]. By using de
m, equation (4) also takes into account of the extent of the spread.
For an ideal distribution of solutions (uniform and de
m =0.0), ∆is 0.0. For func-
tions with disconnected sectors on the Pareto front, e.g., ZDT3, ∆is calculated
within each continuous sector and then averaged.
Number of non-dominated solutions found. The above diversity metric
does not take into account the number of optimal solutions found. An ideal
uniform distribution of Q with ∆= 0.0 could have very few solutions. Obviously
we prefer a uniform distribution with a larger number of solutions found in the
ﬁnal step of a run.
Closeness to P ∗. Generational distance (GD) metric is used to measure the
closeness of solutions in Q to P ∗. The GD metric ﬁnds the average distance of
the solutions of Q from P ∗[2]:
GD = (Σ|Q|
i=1dp
i )
1/p
|Q|
.
(5)
For a two objective problem (p = 2), di is the Euclidean distance between the
solution i ∈Q and the nearest member of P ∗.
5
Experiments
Four test functions ZDT1, ZDT2, ZDT3 and ZDT4 were used [2]. These functions
are considered to be diﬃcult because of the large number of decision variables,
disconnectedness of P ∗, and multiple local fronts. The initial population of the
NSPSO was set to 200. c1 and c2 were set to 2.0. w was gradually decreased from
1.0 to 0.4. VMAX was set to be the bounds of decision variable ranges, and χ
simply to 1.0. NSPSO was run for 100 iteration steps. At the ﬁnal iteration, the
diversity metric ∆and closeness metric GD values were calculated according to
equation (4) and (5), and also the number of non-dominated solutions found.
A set of |P ∗| = 500 uniformly distributed Pareto-optimal solutions is used to
calculate the GD values. The results of NSPSO were compared with the real-
parameter NSGA II. NSGA II also had an initial population of 200, and it was
run for 100 generations. As suggested in [2], a crossover probability of 0.9 and
a mutation probability of 1/n (n is the number of real-variables) were used.
The SBX and real-parameter mutation operators, ηc and ηm, were set to 20
respectively. NSPSO and NSGA II were both run 10 times. The results are
averaged and summarised in Table 1 - 3. Four NSPSO variants were used: NC:
using niche count only; NC-R: using niche count with replacement; CD: using
crowding distance only; and CD-R: using crowding distance with replacement.
Refer to section 3.2 about these four niching variants.

A Non-dominated Sorting Particle Swarm Optimizer
45
Table 1. Mean and variance values of the GD metric measuring convergence.
ZDT1
ZDT2
ZDT3
ZDT4
Algorithm
GD
δ2
GD
δ2
GD
δ2
GD
δ2
NC
7.97E-04 8.13E-05 8.05E-04 3.05E-05 3.40E-03 2.54E-04 7.82E-04 6.91E-05
NC-R
7.53E-04 4.18E-05 2.93E-02 9.03E-02 3.22E-03 5.31E-04 7.36E-04 5.16E-05
CD
1.05E-03 1.71E-04 8.48E-04 4.57E-05 3.54E-03 4.98E-04 9.03E-04 1.66E-04
CD-R
8.61E-04 1.55E-04 2.93E-02 9.02E-02 3.53E-03 3.79E-04 8.42E-04 1.50E-04
NSGA II 1.14E-03 7.64E-05 8.25E-04 3.26E-05
N/A
N/A
2.92E-02 4.67E-02
Table 2. Mean and variance values of the ∆metric measuring diversity.
ZDT1
ZDT2
ZDT3
ZDT4
Algorithm
∆
δ2
∆
δ2
∆
δ2
∆
δ2
NC
7.67E-01 3.00E-02 7.58E-01 2.77E-02 9.14E-01 3.93E-02 7.68E-01 3.57E-02
NC-R
7.72E-01 3.58E-02 7.67E-01 4.36E-02 9.04E-01 6.89E-02 8.06E-01 5.05E-02
CD
7.62E-01 3.83E-02 7.48E-01 4.66E-02 8.69E-01 5.81E-02 7.36E-01 1.84E-02
CD-R
7.62E-01 3.17E-02 7.60E-01 4.86E-02 8.65E-01 5.94E-02 7.89E-01 3.98E-02
NSGA II 3.86E-01 1.63E-02 3.90E-01 2.01E-02
N/A
N/A
6.55E-01 1.98E-01
6
Results and Discussion
Table 1 shows that for all 4 functions, NSPSO has no trouble reaching P ∗for al-
most all of the 10 runs within 100 iterations. However an “outlier” was identiﬁed
for NC-R (ZDT2) and CD-R (ZDT2) each. Closely examining the data, it was
found that the poor GD value was the result of a single poor run, but 9 others
were in fact very good. Note that NSGA II failed to converge in 100 iteration
steps on ZDT3. NSGA II (ZDT4) has the worst variance and GD values, which
are due to two runs reaching only local fronts (Fig. 4 (last on the bottom row).
Table 2 shows that NSGA II has a better ∆value overall, whereas NSPSO’s ∆
values are higher. However from a typical run as shown in Fig. 4, we can see that
NSPSO has a coverage of P ∗just as good as NSGA II. The higher ∆values can
be attributed to a larger number of diﬀerent Q found by NSPSO than NSGA II,
as shown in Table 3. Since we can obtain the best front (Front 1) with possibly
Table 3. Number of non-dominated solutions found in the ﬁnal iteration step.
Algorithm ZDT1 ZDT2 ZDT3 ZDT4
NC
274.3 297
201
276.4
NC-R
277.2 282.6 186.9 286.4
CD
187.9 239.7 134.1 159.6
CD-R
247.8 290.1 194
254.5
NSGA II
200
200
N/A
175.5

46
X. Li
more than N non-dominated solutions in the combined population of 2N per-
sonal bests and oﬀspring, NSPSO is often able to obtain more than 200 diﬀerent
non-dominated solutions in the ﬁnal step. In contrast, NSGA II generally has
a constant number of non-dominated solutions, which is its initial population
size (except for ZDT4). To our surprise, NC-R and CD-R did not seem to make
much diﬀerence in terms of ∆and GD values. However in Table 3, we can note
that CD-R managed to ﬁnd many more diﬀerent non-dominated solutions than
CD. For NC and NC-R, however, there is not much diﬀerence. Fig. 4 presents
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f2
f1
NSPSO
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f2
f1
NSPSO
-1
-0.5
0
0.5
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
f2
f1
NSPSO
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f2
f1
NSPSO
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f2
f1
NSGA II
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
f2
f1
NSGA II
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
f2
f1
NSGA II-best
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
0.2
0.4
0.6
0.8
1
f2
f1
NSGA II-best
NSGA II-worst1
NSGA II-worst2
Fig. 4. Non-dominated solutions found by NSPSO (top row) and NSGA II (bottom
row) for ZDT1, 2, 3, and 4 (from left to right).
that for a typical NSPSO and NSGA II run, the non-dominated solutions found
in the ﬁnal iteration step for all 4 test functions. Overall, the results show that
NSPSO is very competitive in terms of solution spread, coverage and closeness to
P ∗. Fig. 4 (3rd on the top row) shows that NSPSO has no trouble converging on
the diﬀerent disconnected sectors of P ∗for ZDT3, whereas in this case, NSGA
II failed completely (3rd on the bottom row). For ZDT4, NSPSO’s 10 runs have
all converged to P ∗with a good spread and coverage. In contrast, NSGA II had
2 out of 10 runs reaching only a local front (Fig. 4 (last on the bottom row)).
0
5
10
15
20
25
30
35
40
45
50
0
0.2
0.4
0.6
0.8
1
f2
f1
NSPSO - ZDT4 step1
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
f2
f1
NSPSO - ZDT4 step5
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
f2
f1
NSPSO - ZDT4 step9
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
f2
f1
NSPSO - ZDT4 step15
Fig. 5. Snapshots of a NSPSO run showing the entire particle population at step 1, 3,
9 and 15, for solving ZDT4.
Fast and better convergence towards the global front. Fig. 5 presents 4
snapshots of the ﬁrst few steps in a NSPSO’s run for solving ZDT4. It can be

A Non-dominated Sorting Particle Swarm Optimizer
47
noted that even in step 3, there were no particles that had found P ∗, but by
step 9 there were quite a few, though the majority of the particles were stuck at
the 2nd best front. By step 15, all particles had reached P ∗without diﬃculty.
Considering the majority of current multiobjective evolutionary algorithms (with
the exception of NSGA II) are unable to converge to P ∗for ZDT4, this is a
remarkable result. NSPSO also demonstrates its consistency as 10 out of 10
runs have all converged to P ∗, better than the NSGA II counterpart. NSPSO’s
impressive performance is due to the fact that we allow all particles’ personal
bests and oﬀspring to be combined for non-domination comparison. Updating
of the particles is then based on their mutual non-domination relationships with
respect to the current Q in the population, not with respect to just another
single particle (like the basic PSO). Each time, the best N particles in terms of
non-domination levels of the entire population are selected for the next iteration
step. Even if a single particle has found P ∗, the particle’s personal best and
its oﬀspring will be emphasised favourably in the next step. Since this personal
best and the oﬀspring coexist and they are most likely to be located near P ∗, it
is likely they will be selected again for the next step. This is more like a “self-
proliferation” of good particles. The aggregated eﬀect of this over many steps
would produce a large number of ﬁtter particles along P ∗. Since at each step
we only select the best N particles, particles on those local fronts are gradually
phased out, replaced by the ﬁtter ones on the global front.
Using a larger population size. It was found that a reasonably large popu-
lation is necessary for a good convergence. If population size was too small (e.g.,
20), NSPSO could converge to sub-optimal fronts, or to P ∗but with a limited
number of non-dominated solutions, which is insuﬃcient in terms of solution
spread and coverage. This is because a smaller number of particles do not suﬃ-
ciently sample the search space, and as a result, certain existing particles could
quickly become too dominant early on, and they would prevent other potentially
good particles (in terms of diﬀerent non-dominated solutions) from being pro-
duced. A large initial population size would allow for a better sampling of the
search space, and from there onwards allow NSPSO to better use domination
comparison operations to ﬁnd a wide spread of solutions along P ∗.
7
Conclusion
With an aim to improve PSO’s eﬀectiveness in utilising the domination com-
parison operations for solving multiobjective optimization problems, this paper
has proposed a modiﬁed PSO, NSPSO. The model is able to discover more non-
dominated relations by comparing the personal bests and oﬀspring of all particles
in a combined swarm population, thereby providing a more appropriate selection
pressure for the population to approach the true Pareto-optimal front. NSPSO
adopts the non-dominated sorting concept, and uses two parameter-free nich-
ing techniques to promote solution diversity. It has been shown that NSPSO
and its variants are able to perform remarkably well against some diﬃcult test

48
X. Li
functions (functions with high-dimensional decision variables and multiple lo-
cal fronts) found in the literature. NSPSO is also fast, more reliable, and often
converging to the true Pareto-optimal front with a good solution spread and
coverage within just a few steps. This once again proves that PSO is a powerful
optimization technique that can be used eﬃciently not only for single objective,
but also for multiobjective optimization. Future work will look into application
of NSPSO to problems with more than 2 objectives, and also real-world multi-
objective optimization problems.
References
1. Zitzler, E., Deb, K. and Thiele, L.: Comparison of multiobjective evolutionary algo-
rithms: Empirical results. Evolutionary Computation, 8(2):173–195, April (2000).
2. Deb, K.: Multi-Objective Optimization using Evolutionary Algorithms, John Wiley
& Sons, Chichester, UK (2001).
3. Kennedy, J. and Eberhart, R.: Particle Swarm Optimization. In Proceedings of
the Fourth IEEE International Conference on Neural Networks, Perth, Australia.
IEEE Service Center(1995) 1942–1948.
4. Coello, C.A.C. and Lechuga, M.S.: MOPSO: A Proposal for Multiple Objective
Particle Swarm Optimization, in Proceedings of Congress on Evolutionary Com-
putation (CEC’2002), Vol. 2, IEEE Press (2002) 1051–1056.
5. Hu, X. and Eberhart, R.: Multiobjective Optimization Using Dynamic Neighbour-
hood Particle Swarm Optimization. In Proceedings of the IEEE World Congress
on Computational Intelligence, Hawaii, May 12–17, 2002. IEEE Press (2002).
6. Parsopoulos, K.E. and Vrahatis, M.N.: Particle Swarm Optimization Method in
Multiobjective Problems, in Proceedings of the 2002 ACM Symposium on Applied
Computing (SAC’2002) (2002) 603–607.
7. Fieldsend, E. and Singh, S.: A Multi-Objective Algorithm based upon Parti-
cle Swarm Optimisation, an Eﬃcient Data Structure and Turbulence, Proceed-
ings of the 2002 U.K. Workshop on Computational Intelligence, Birmingham,
UK(2002) 37–44.
8. Horn, J., Nafpliotis, N., and Goldberg, D.E.: A Niched Pareto Genetic Algorithm
for Multiobjective Optimization. In Proceedings of the First IEEE Conference on
Evolutionary Computation, IEEE World Congress on Computational Intelligence,
vol: 1, Piscataway, New Jersey. IEEE Service Center.(1994) 82–87.
9. Deb, K., Agrawal, S. Pratap, A. and Meyarivan, T.: A Fast Elitist NonDominated
Sorting Genetic Algorithm for Multi-Objective Optimization: NSGA-II. In Pro-
ceedings of Parallel Problem Solving from Nature - PPSN VI, Springer(2000) 849–
858.
10. Goldberg, D.E., and Richardson, J.J.: Genetic Algorithms with sharing for multi-
modal function optimization. Genetic Algorithms and Their Applications: Proceed-
ings of the Second ICGA, Lawrence Erlbaum Associates, Hillsdale, NJ, (1987) 41–
49.
11. Fonseca, C.M. and Fleming, P.J.: Genetic algorithms for multiobjective optimiza-
tion: Formulation, discussion, and generalization. In Proceedings of the Fifth In-
ternational Conference on Genetic Algorithms (1993) 355–365.

The Inﬂuence of Run-Time Limits on Choosing
Ant System Parameters
Krzysztof Socha
IRIDIA, Universit´e Libre de Bruxelles, CP 194/6,
Av. Franklin D. Roosevelt 50, 1050 Bruxelles, Belgium
ksocha@ulb.ac.be
http://iridia.ulb.ac.be
Abstract. The inﬂuence of the allowed running time on the choice of
the parameters of an ant system is investigated. It is shown that diﬀer-
ent parameter values appear to be optimal depending on the algorithm
run-time. The performance of the MAX-MIN Ant System (MMAS)
on the University Course Timetabling Problem (UCTP) – a type of con-
straint satisfaction problem – is used as an example. The parameters
taken into consideration include the type of the local search used, and
some typical parameters for MMAS – the τmin and ρ. It is shown that
the optimal parameters depend signiﬁcantly on the time limits set. Con-
clusions summarizing the inﬂuence of time limits on parameter choice,
and possible methods of making the parameter choice more independent
from the time limits, are presented.
1
Introduction
Ant Colony Optimization (ACO) is a metaheuristic proposed by Dorigo et al. [1].
The inspiration of ACO is the foraging behavior of real ants. The basic ingredient
of ACO is the use of a probabilistic solution construction mechanism based
on stigmergy. ACO has been applied successfully to numerous combinatorial
optimization problems including the traveling salesman problem [2], quadratic
assignment problem [3], scheduling problems [4], and others. In this paper we
focus on the MAX-MIN Ant System [5] – a version of ACO.
ACO, similarly to any other metaheuristic, may be parameterized. This
means that in order to be able to deliver optimal performance, the ant algorithm
uses a number of parameters that precisely deﬁne its operation. The parame-
ters are usually chosen speciﬁcally for the class of problems the ant algorithm
is to tackle. Usually, the optimal (or near optimal) parameters are chosen by
the trial-and-error procedure. As the number of trials necessary to ﬁne-tune the
parameters is usually quite high, it is often the case that limited time is used for
each algorithm run. The best parameters found are then used for tackling actual
problems. In turn, the actual problem solving runs tend to be much longer.
We do not focus on the actual results obtained by the algorithm, but only
on choosing the best set of parameters optimizing algorithm performance within
allowed run-time limit. This paper attempts to show that optimal parameters
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 49–60, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

50
K. Socha
for the ant algorithm depend signiﬁcantly on the time given it to run. Hence,
the parameter ﬁne-tuning done with run-times signiﬁcantly shorter than actual
problem solving runs may cause choosing suboptimal parameters. It is assumed
that for the time limits investigated, the algorithm does not reach optimality –
it does not ﬁnd the optimal solution.
The remaining part of the paper is organized as follows: Section 2 brieﬂy
presents the example problem used for evaluating the performance of the ant
algorithm. Section 3 presents the ant system used to solve the problem – the
MAX-MIN Ant System. The main design considerations and parameters used
are highlighted. Section 4 presents the local search used by the algorithm, and
discusses how time limits imposed determine the choice of the local search type.
Section 5 discusses some other parameters used by the algorithm, and presents
the relationship between their optimal values and the time limits imposed. Fi-
nally, Section 6 summarizes the ﬁndings and presents the conclusions drawn.
2
Example Problem
The problem used to illustrate the thesis of this paper, is the University Course
Timetabling Problem (UCTP) [6,7,8]. It is a type of constraint satisfaction prob-
lem. It consists of a set of n events E = {e1, . . . , en} to be scheduled in a set of i
timeslots T = {t1, . . . , ti}, and a set of j rooms R = {r1, . . . , rj} in which events
can take place. Additionally, there is deﬁned a set of students S who attend
the events, and a set of features F satisﬁed by rooms and required by events.
Each student is already preassigned to a subset of events. A feasible timetable
is one in which all events have been assigned a timeslot and a room, so that the
following hard constraints are satisﬁed:
– no student attends more than one event at the same time;
– the room is big enough for all the attending students and satisﬁes all the
features required by the event;
– only one event is taking place in each room at a given time.
In addition, a feasible candidate timetable is penalized equally for each oc-
currence of the following soft constraint violations:
– a student has a class in the last slot of the day;
– a student has more than two classes in a row (one penalty for each class
above the ﬁrst two);
– a student has exactly one class during a day.
The infeasible timetables are worthless and are considered equally bad re-
gardless of the actual level of infeasibility. The objective is to minimize the
number of soft constraint violations (#scv) in a feasible timetable. The solution
to the UCTP is a mapping of events into particular timeslots and rooms. Fig. 1
shows an example of a timetable.

The Inﬂuence of Run-Time Limits on Choosing Ant System Parameters
51
Fig. 1. Timetable of i timeslots and j rooms (k = i · j places). Some events from the
set E have already been placed.
Two instances of the UCTP are used for illustrating the performance of
the ant algorithm in this paper – competition04 and competition07. These
instances have been proposed as a part of the International Timetabling Com-
petition1. Note that these instances are known to have a perfect solution, i.e. a
solution where no hard or soft constraints are violated.
3
Algorithm Description
The basic mode of operation of the MAX-MIN Ant System used for the
experiments is as follows. At each iteration of the algorithm, each of the m ants
constructs a complete assignment C of events into timeslots and rooms. Following
a pre-ordered list of events, the ants choose the timeslot and room for the given
event probabilistically, guided by stigmergic information. This information is in
the form of a matrix of pheromone values τ : E × T × R →R+, where E is
the set of events, T is the set of timeslots, and R is a set of rooms. In order to
maintain simplicity of the notation, let us call the timeslot-room combination a
place. The pheromone matrix becomes then of form: τ : E × P →R+, where P
is the set of k places, and k = |P| = |T| · |R| = i · j.
Also, some problem speciﬁc knowledge (heuristic information) is used by
the algorithm. The place for an event (i.e. the timeslot-room combination) is
chosen only from the ones that are suitable for the given event - placing the
event there will not violate any hard constraint. If at some point of time during
the construction of the assignment there is no such a place available, a list
of timeslots is extended by one, and the event is placed in one of the rooms
of this additional timeslot. This of course results in an infeasible solution2 as
the number of timeslots used from now on exceeds i. This also means that the
pheromone matrix has to be extended as well. It is done by creating an extended
set T ′ of i′ timeslots, and consequently a new extended set P ′ of k′ places. The
new pheromone matrix is deﬁned as τ : E × P ′ →R+. Note that initially it is
assumed that i′ = i and k′ = k.
1 http://www.idsia.ch/Files/ttcomp2002/
2 Only for this particular ant, and only in this iteration.

52
K. Socha
Once all the ants have constructed their assignment of events into places, a
local search routine is used to further improve the solutions. More details about
local search routine are provided in Sec. 4. Finally the best solution of each
iteration is compared to the global best solution found so far. If the iteration
best solution is better than the global best, it is replaced. Only the global best
solution is used for the pheromone update.
If the diﬀerences between extreme pheromone values were too large, all ants
would almost always generate the same solutions, which would mean algorithm
stagnation. The MAX-MIN Ant System introduces upper and lower limits on
the pheromone values – τmax and τmin respectively [5] – that prevent this. The
maximal diﬀerence between the extreme levels of pheromone may be controlled,
and thus the search intensiﬁcation versus diversiﬁcation may be balanced. The
pheromone update rule is as follows (for the particular case of assigning events
e into places p):
τ(e,p) ←

(1 −ρ) · τ(e,p) + τfixed
if (e, p) is in Cglobal best
(1 −ρ) · τ(e,p)
otherwise,
(1)
where ρ ∈[0, 1] is the evaporation rate, and τfixed is the pheromone update
value. Pheromone update is completed using the following:
τ(e,p) ←



τmin
if τ(e,p) < τmin,
τmax
if τ(e,p) > τmax,
τ(e,p)
otherwise.
(2)
The pheromone update value τfixed is a constant that has been established
after some experiments with the values calculated based on the actual quality
of the solution. The function q measures the quality of a candidate solution C
by counting the number of constraint violations. According to the deﬁnition of
MMAS, τmax = 1
ρ ·
g
1+q(Coptimal), where g is a scaling factor. Since it is known
that q(Coptimal) = 0 for the considered test instances, we set τmax to a ﬁxed value
τmax = 1
ρ. We observed that the proper balance of the pheromone update and
the evaporation rate was achieved with a constant value of τfixed = 1.0, which
was also more eﬃcient than the calculation of exact value based on quality of
the solution.
4
Inﬂuence of Local Search
It has been shown in the literature that ant algorithms perform particularly
well, when supported by a local search (LS) routine [2,9,10]. There were also
attempts to design the local search for the particular problem tackled here (the
UCTP) [11]. Here, we try to show that although adding an LS to an algorithm
improves the results obtained, it is important to carefully choose the type of such
LS routine, especially with regard to algorithm running time limits imposed.
The LS used here by the MMAS solving the UCTP consists of two major
modules. The ﬁrst module tries to improve an infeasible solution (i.e. a solution

The Inﬂuence of Run-Time Limits on Choosing Ant System Parameters
53
that uses more than i timeslots), so that it becomes feasible. Since its main
purpose is to produce a solution that does not contain any hard constraint
violations and that ﬁts into i timeslots, we call it HardLS. The second module
of the LS is run only if a feasible solution is available (either generated by an
ant directly, or obtained after running HardLS). This module tries to increase
the quality of the solution by reducing number of the soft constraint violations
(#scv), and hence is called SoftLS. It does so by rearranging the events in
the timetable, but any such rearrangement must never produce an infeasible
solution.
The HardLS module is always called before calling the SoftLS module, if the
solution found by an ant is infeasible. Also, it is not parameterized in any way,
so in this paper we will not go into details of its operation.
SoftLS rearranges the events aiming at increasing the quality of the already
feasible solution, without introducing infeasibility. This means that an event may
only be placed in timeslot tl:l≤i. In the process of ﬁnding the most eﬃcient LS,
we developed the following three types of SoftLS:
– type 0 – The simplest and the fastest version. It tries to move one event at
a time to an empty place that is suitable for this event, so that after such
a move the quality of the solution is improved. The starting place is chosen
randomly, and then the algorithm loops through all the places trying to put
the events in empty places until a perfect solution is found, or until in the
last k = |P| iterations there was no improvement.
– type 1 – Version similar to the SoftLS type 0, but also enhanced by the
ability to swap two events in one step. The algorithm not only checks, if an
event may be moved to another empty suitable place to improve the solution,
but also checks, if this event could perhaps be swapped with any other event.
Only moves (or swaps) that do not violate any hard constraints and improve
the overall solution are accepted. This version of SoftLS usually provides
a greater solution improvement than the SoftLS type 0, but also a single
run takes signiﬁcantly more time.
– type 2 – The most complex version. In this case, as a ﬁrst step, the SoftLS
type 1 is run. After that, the second step is executed: the algorithm tries to
further improve the solution by changing the order of timeslots. It attempts
to swap any two timeslots (i.e. move all the events from one timeslot to the
other without changing the room assignment), so the solution is improved.
The operation continues until no swaps of any two timeslots may further
improve the solution. The two steps are repeated until a perfect solution is
found, or neither of them has produced any improvement. This version of
SoftLS is the most time consuming.
4.1
Experimental Results
We ran several experiments in order to establish, which of the presented SoftLS
types is best suited for the problem being solved. Fig. 2 presents the performance
of our ant algorithm with diﬀerent versions of SoftLS, as a function of time limit

54
K. Socha
1
2
5
20
50
200
400
500
600
700
800
competition04
t [s]
q [#scv]
LS type 0
LS type 1
LS type 2
probabilistic LS
1
2
5
20
50
200
200
300
400
500
600
competition07
t [s]
q [#scv]
LS type 0
LS type 1
LS type 2
probabilistic LS
Fig. 2. Mean value of the quality of the solutions (#scv) generated by the MMAS
using diﬀerent versions of local search on two instances of the UCTP – competition04
and competition07.
imposed on the algorithm run-time. Note that we initially focus here on the three
basic types of SoftLS. The additional SoftLS type – probabilistic LS – that is
also presented on this ﬁgure, is described in more detail in Sec. 4.2.
We ran 100 trials for each of the SoftLS types. The time limit imposed on
each run was 672 seconds (chosen with the use of benchmark program supplied
by Ben Peachter as part of the International Timetabling Competition). We
measured the quality of the solution throughout the duration of each run. All
the experiments were conducted on the same computer (AMD Athlon 1100 MHz,
256 MB RAM) under a Linux operating system.
Fig. 2 clearly indicates the diﬀerences in performance of the MMAS, when
using diﬀerent types of SoftLS. While the SoftLS type 0 produces ﬁrst results
already within the ﬁrst second of the run, the other two types of SoftLS produce
ﬁrst results only after 10-20 seconds. However, the ﬁrst results produced by
either the SoftLS type 1 or type 2 are signiﬁcantly better than the results
obtained by the SoftLS type 0 within the same time. With the increase of
allowed algorithm run-time, the SoftLS type 0 quickly outperforms SoftLS
type 1, and then type 2. While in case of competition07, the SoftLS type
0 remains the best within the imposed time limit (i.e. 672 seconds), in case
of competition04, the SoftLS type 2 apparently eventually catches up. This
may indicate that if more time was allowed for each version of the algorithm to
run, the best results may be obtained by SoftLS type 2, rather than type 0.
It is also visible that towards the end of the search process, the SofLS type 1
appears to converge faster than type 0 or type 2 for both test instances. Again,
this may indicate that – if longer run-time was allowed – the best SoftLS type
may be diﬀerent yet again.

The Inﬂuence of Run-Time Limits on Choosing Ant System Parameters
55
It is hence very clear that the best of the three presented types of local
search for the UCTP may only be chosen after deﬁning the time limit for a
single algorithm run. The examples of time limits and appropriate best LS type
are summarized in Tab. 1.
Table 1. Best type of the SoftLS depending on example time limits.
Time Limit
Best SoftLS Type
[s]
competition04
competition07
5
type 0
type 0
10
type 1
type 1
20
type 2
type 2
50
type 0
type 2
200
type 0
type 0
672
type 0/2
type 0
4.2
Probabilistic Local Search
After experimenting with the basic types of SoftLS presented in Sec. 4, we
realized that apparently diﬀerent types of SoftLS work best during diﬀerent
stages of the search process. We wanted to ﬁnd a way to take advantage of all
of the types of SoftLS.
First, we thought of using a particular type of SoftLS depending on the time
spent by the algorithm on searching. However this approach, apart from having
an obvious disadvantage of the necessity of measuring time and being dependent
on the hardware used, had some additional problems. We found that the solution
(however good it was) generated with the use of any basic type of SoftLS, was
not always easy to be further optimized by another type of SoftLS. When the
type of SoftLS used changed, the algorithm spent some time recovering from the
previously found local optimum. Also, the sheer necessity of deﬁning the right
moments, when the SoftLS type was to be changed was a problem. It had to be
done for each problem instance separately, as those times diﬀered signiﬁcantly
from instance to instance.
In order to overcome these diﬃculties, we came up with the idea of probabilis-
tic local search. Such local search would probabilistically choose the basic type
of the SoftLS to be used. Its behavior may be controlled by proper adjustment
of the probabilities of running the diﬀerent basic types of SoftLS. After some
initial tests, we found that rather small probability of running the SoftLS type
1 and type 2 comparing to the probability of running the SoftLS type 0, pro-
duced best results within the time limit deﬁned. Fig. 2 also presents the mean
values obtained by 100 runs of this probabilistic local search. The probabilities
of running each type of the basic SoftLS types that were used to obtain these
results, are listed in Tab. 2.
The performance of the probabilistic SoftLS is apparently the worst for
around ﬁrst 50 seconds of the run-time for both test problem instances. After

56
K. Socha
Table 2. Probabilities of running diﬀerent types of the SoftLS.
SoftLS Type
Probabilities
competition04
competition07
type 0
0.90
0.94
type 1
0.05
0.03
type 2
0.05
0.03
that, it improves faster than the performance of any other type of SoftLS, and
eventually becomes the best. In case of the competition04 problem instance, it
becomes the best already after around 100 seconds of the run-time, and in case
of the competition07 problem instance, after around 300 seconds.
It is important to note that the probabilities of running the basic types of
SoftLS have been chosen in such a way that this probabilistic SoftLS is in
fact very close to the SoftLS type 0. Hence, its characteristics are also similar.
However, by appropriately modifying the probability parameters, the behavior
of this probabilistic SoftLS may be adjusted, and hence provide good results for
any given time limits. In particular, the probabilistic SoftLS may be reduced to
any of the basic versions of SoftLS.
5
ACO Speciﬁc Parameters
Having shown in Sec. 4 that choice of the best type of local search very much
depends on the time the algorithm is run, we wanted to see if this also applies
to other algorithm parameters. Another aspect of the MAX-MIN Ant System
that we investigated with regard to the imposed time limits, was a subset of the
typical MMAS parameters: evaporation rate ρ and pheromone lower bound
τmin. We chose these two parameters among others, as they have been shown in
the literature [12,10,5] to have signiﬁcant impact on the results obtained by a
MAX-MIN Ant System.
We generated 110 diﬀerent sets of these two parameters. We chose the evapo-
ration rate ρ ∈[0.05, 0.50] with the step of 0.05, and the pheromone lower bound
τmin ∈[6.25 · 105, 6.4 · 103] with the logarithmic step of 2. This gave 10 diﬀerent
values of ρ and 11 diﬀerent values of τmin – 110 possible pairs of values. For each
such pair, we ran the algorithm 10 times with the time limit set to 672 seconds.
We measured the quality of the solution throughout the duration of each run
for all the 110 cases. Fig. 3 presents the gray-shade-coded grid of ranks of mean
solution values obtained by the algorithm with diﬀerent sets of the parameters
for four diﬀerent run-times allowed (respectively 8, 32, 128, and 672 seconds)3.
The results presented, were obtained for the competition04 instance.
The results indicate that the best solutions – those with higher ranks (darker)
– are found for diﬀerent sets of parameters, depending on the allowed run-time
3 The ranks were calculated independently for each time limit studied.

The Inﬂuence of Run-Time Limits on Choosing Ant System Parameters
57
pheromone lower bound
pheromone evaporation rate
time:008[s]
0.1
0.2
0.3
0.4
0.5
time:032[s]
2^−162^−142^−122^−10 2^−8
0.1
0.2
0.3
0.4
0.5
2^−162^−142^−122^−10 2^−8
time:128[s]
time:672[s]
−0
−20
−40
−60
−80
−100
Fig. 3. The ranks of the solution means for the competition04 instance with regard
to the algorithm run-time. The ranks of the solutions are depicted (gray-shade-coded)
as function of the pheromone lower bound τmin, and pheromone evaporation rate ρ.
limit. In order to be able to analyse the relationship between the best solutions
obtained and the algorithm run-time more closely, we calculated the mean value
of the results for 16 best pairs of parameters, for several time limits between 1
and 672 seconds. The outcome of that analysis is presented on Fig. 4. The ﬁgure
presents respectively: the average best evaporation rate as a function of algorithm
run-time: ρ(t), the average best pheromone lower bound as a function of run-
time: τmin(t), and also how the pair of the best average ρ and τmin, changes with
run-time. Additionally, it shows how the average best solution obtained with the
current best parameters change with algorithm run-time: q(t).
It is clearly visible that the average best parameters change with the change
of run-time allowed. Hence, similarly as in case of the local search, the choice
of parameters should be done with close attention to the imposed time limits.
At the same time, it is important to mention that the probabilistic method
of choosing the conﬁguration that worked well in the case of the SoftLS, is
rather diﬃcult to implement in case of the MMAS speciﬁc parameters. Here,
the change of parameters’ values has its eﬀect on algorithm behavior only after
several iterations, rather than immediately as in case of LS. Hence, rapid changes

58
K. Socha
1
2
5
10
50
200
0.25
0.30
0.35
0.40
0.45
ρ(t)
t [s]
ρ
1
2
5
10
50
200
2   e−05
1   e−04
5   e−04
τmin(t)
t [s]
τmin
2   e−05
1   e−04
5   e−04
2   e−03
0.25
0.30
0.35
0.40
0.45
ρ(τmin)
τmin
ρ
1
2
5
10
50
200
400
500
600
700
q(t)
t [s]
q
Fig. 4. Analysis of average best ρ and τmin parameters as a function of time assigned
for the algorithm run (the upper charts). Also, the relation between best values of
ρ and τmin, as changing with running time, and the average quality of the solutions
obtained with the current best parameters as a function of run-time (lower charts).
of these parameters may only result in algorithm behavior that would be similar
to simply using the average values of the probabilistically chosen ones.
More details about the experiments conducted, as well as the source code of
the algorithm used, and also results for other test instances that could not be
included in the text due to the limited length of this paper, may be found on
the Internet4.
6
Conclusions and Future Work
Based on the examples presented, it is clear that the optimal parameters of the
MAX-MIN Ant System may only be chosen with close attention to the run-
4 http://iridia.ulb.ac.be/˜ksocha/antparam03.html

The Inﬂuence of Run-Time Limits on Choosing Ant System Parameters
59
time limits. Hence, the time-limits have to be clearly deﬁned before attempting
to ﬁne-tune the parameters. Also, the test runs used to adjust the parameter
values should be conducted under the same conditions as the actual problem
solving runs.
In case of some parameters, such as the type of the local search to be used,
a probabilistic method may be used to obtain very good results. For some other
types of parameters (τmin and ρ in our example) such a method is not so good,
and some other approach is needed. The possible solution is to make the parame-
ter values variable throughout the run of the algorithm. The variable parameters
may change according to a predeﬁned sequence of values, or they may be adap-
tive – the changes may be a derivative of a certain algorithm state.
This last idea seems especially promising. The problem however is to deﬁne
exactly how the state of the algorithm should inﬂuence the parameters. To make
the performance of the algorithm independent from the time limits imposed on
the run-time, several runs are needed. During those runs, the algorithm (or at
least algorithm designer) may learn what is the relation between the algorithm
state, and the optimal parameter values. It remains an open question how diﬃ-
cult it would be to design such a self-ﬁne-tuning algorithm, or how much time
such an algorithm would need in order to learn.
6.1
Future Work
In the future, we plan to investigate further the relationship between diﬀerent
ACO parameters and run-time limits. This should include the investigation of
other test instances, and also other example problems. We will try to deﬁne
a mechanism that would allow a dynamic adaptation of the parameters. Also,
it is very interesting to see if the parameter-runtime relation is similar (or the
same) regardless of the instance or problem studied (at least for some ACO
parameters). If so, this could permit proposing a general framework of ACO
parameter adaptation, rather than a case by case approach.
We believe that the results presented in this paper may also be applicable
to other combinatorial optimization problems solved by ant algorithms. In fact
it is very likely that they are also applicable to other metaheuristics as well5.
The results presented in this paper do not yet allow to simply jump to such
conclusions however. We plan to continue the research to show that it is in fact
the case.
Acknowledgments. Our work was supported by the Metaheuristics Network,
a Research Training Network funded by the Improving Human Potential Pro-
gramme of the CEC, grant HPRN-CT-1999-00106. The information provided
is the sole responsibility of the authors and does not reﬂect the Community’s
opinion. The Community is not responsible for any use that might be made of
data appearing in this publication.
5 Of course with regard to their speciﬁc parameters.

60
K. Socha
References
1. Dorigo, M., Maniezzo, V., Colorni, A.: The ant system: Optimization by a colony
of cooperating agents. IEEE Transactions on Systems, Man, and Cybernetics 26
(1996) 29–41
2. St¨utzle, T., Dorigo, M.: Aco algorithms for the traveling salesman problem. In
Makela, M., Miettinen, K., Neittaanm¨aki, P., P´eriaux, J., eds.: Proceedings of
Evolutionary Algorithms in Engineering and Computer Science: Recent Advances
in Genetic Algorithms, Evolution Strategies, Evolutionary Programming, Genetic
Programming and Industrial Applications (EUROGEN 1999), John Wiley & Sons
(1999)
3. St¨utzle, T., Dorigo, M. In: ACO Algorithms for the Quadratic Assignment Prob-
lem. McGraw-Hill (1999)
4. Merkle, D., Middendorf, M., Schmeck, H.: Ant colony optimization for resource-
constrained project scheduling. In: Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO 2000), Morgan Kaufmann Publishers (2000)
893–900
5. St¨utzle, T., Hoos, H.H.: MAX-MIN Ant System. Future Generation Computer
Systems 16 (2000) 889–914
6. Rossi-Doria, O., Sampels, M., Chiarandini, M., Knowles, J., Manfrin, M., Mastro-
lilli, M., Paquete, L., Paechter, B.: A comparison of the performance of diﬀerent
metaheuristics on the timetabling problem. In: Proceedings of the 4th International
Conference on Practice and Theory of Automated Timetabling (PATAT 2002) (to
appear). (2002)
7. Socha, K., Knowles, J., Sampels, M.: A MAX-MIN Ant System for the Uni-
versity Timetabling Problem.
In Dorigo, M., Di Caro, G., Sampels, M., eds.:
Proceedings of ANTS 2002 – Third International Workshop on Ant Algorithms.
Lecture Notes in Computer Science, Springer Verlag, Berlin, Germany (2002)
8. Socha, K., Sampels, M., Manfrin, M.: Ant Algorithms for the University Course
Timetabling Problem with Regard to the State-of-the-Art.
In: Proceedings of
EvoCOP 2003 – 3rd European Workshop on Evolutionary Computation in Com-
binatorial Optimization, LNCS 2611. Volume 2611 of Lecture Notes in Computer
Science., Springer, Berlin, Germany (2003)
9. Maniezzo, V., Carbonaro, A.: Ant Colony Optimization: an Overview. In Ribeiro,
C., ed.: Essays and Surveys in Metaheuristics, Kluwer Academic Publishers (2001)
10. St¨utzle, T., Hoos, H. In: The MAX-MIN Ant System and Local Search for Combi-
natorial Optimization Problems: Towards Adaptive Tools for Combinatorial Global
Optimisation. Kluwer Academic Publishers (1998) 313–329
11. Burke, E.K., Newall, J.P., Weare, R.F.: A memetic algorithm for university exam
timetabling. In: Proceedings of the 1st International Conference on Practice and
Theory of Automated Timetabling (PATAT 1995), LNCS 1153, Springer-Verlag
(1996) 241–251
12. St¨utzle, T., Hoos, H.:
Improvements on the ant system: A detailed report on
max-min ant system. Technical Report AIDA-96-12 – Revised version, Darmstadt
University of Technology, Computer Science Department, Intellectics Group (1996)

Emergence of Collective Behavior in Evolving
Populations of Flying Agents
Lee Spector1, Jon Klein1,2, Chris Perry1, and Mark Feinstein1
1 School of Cognitive Science, Hampshire College
Amherst, MA 01002, USA
2 Physical Resource Theory, Chalmers U. of Technology and G¨oteborg University
SE-412 96 G¨oteborg, Sweden
{lspector, jklein, perry, mfeinstein}@hampshire.edu
http://hampshire.edu/lspector
Abstract. We demonstrate the emergence of collective behavior in two
evolutionary computation systems, one an evolutionary extension of a
classic (highly constrained) ﬂocking algorithm and the other a relatively
un-constrained system in which the behavior of agents is governed by
evolved computer programs. We describe the systems in detail, docu-
ment the emergence of collective behavior, and argue that these systems
present new opportunities for the study of group dynamics in an evolu-
tionary context.
1
Introduction
The evolution of group behavior is a central concern in evolutionary biology
and behavioral ecology. Ethologists have articulated many costs and beneﬁts of
group living and have attempted to understand the ways in which these factors
interact in the context of evolving populations. For example, they have consid-
ered the thermal advantages that warm-blooded animals accrue by being close
together, the hydrodynamic advantages for ﬁsh swimming in schools, the risk of
increased incidence of disease in crowds, the risk of cuckoldry by neighbors, and
many advantages and risks of group foraging [4]. Attempts have been made to
understand the evolution of group behavior as an optimization process operat-
ing on these factors, and to understand the circumstances in which the resulting
optima are stable or unstable [6], [10]. Similar questions arise at a smaller scale
and at an earlier phase of evolutionary history with respect to the evolution of
symbiosis, multicellularity, and other forms of aggregation that were required to
produce the ﬁrst large, complex life forms [5], [1].
Artiﬁcial life technologies provide new tools for the investigation of these is-
sues. One well-known, early example was the use of the Tierra system to study
the evolution of a simple form of parasitism [7]. Game theoretic simulations,
often based on the Prisoner’s Dilemma, have provided ample data and insights,
although usually at a level of abstraction far removed from the physical risks and
opportunities presented by real environments (see, e.g., [2], about which we say
a bit more below). Other investigators have attempted to study the evolution of
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 61–73, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

62
L. Spector et al.
collective behavior in populations of ﬂying or swimming agents that are similar
in some ways to those investigated here, with varying degrees of success [8], [13].
The latest wave of artiﬁcial life technology presents yet newer opportunities,
however, as it is now possible to conduct much more elaborate simulations on
modest hardware and in short time spans, to observe both evolution and behav-
ior in real time in high-resolution 3d displays, and to interactively explore the
ecology of evolving ecosystems.
In the present paper we describe two recent experiments in which the emer-
gence of collective behavior was observed in evolving populations of ﬂying agents.
The ﬁrst experiment used a system, called SwarmEvolve 1.0, that extends a
classic ﬂocking algorithm to allow for multiple species, goal orientation, and
evolution of the constants in the hard-coded motion control equation. In this
system we observed the emergence of a form of collective behavior in which
species act similarly to multicellular organisms. The second experiment used
a later and much-altered version of this system, called SwarmEvolve 2.0, in
which the behavior of agents is controlled by evolved computer programs instead
of a hard-coded motion control equation.1 In this system we observed the emer-
gence of altruistic food-sharing behaviors and investigated the link between this
behavior and the stability of the environment.
Both SwarmEvolve 1.0 and SwarmEvolve 2.0 were developed within
breve, a simulation package designed by Klein for realistic simulations of de-
centralized systems and artiﬁcial life in 3d worlds [3]. breve simulations are
written by deﬁning the behaviors and interactions of agents using a simple
object-oriented programming language called steve. breve provides facilities
for rigid body simulation, collision detection/response, and articulated body sim-
ulation. It simpliﬁes the rapid construction of complex multi-agent simulations
and includes a powerful OpenGL display engine that allows observers to ma-
nipulate the perspective in the 3d world and view the agents from any location
and angle. The display engine also provides several “special eﬀects” that can
provide additional visual cues to observers, including shadows, reﬂections, light-
ing, semi-transparent bitmaps, lines connecting neighboring objects, texturing
of objects and the ability to treat objects as light sources. More information
about breve can be found in [3]. The breve system itself can be found on-line
at http://www.spiderland.org/breve.
In the following sections we describe the two SwarmEvolve systems and the
collective behavior phenomena that we observed within them. This is followed
by some brief remarks about the potential for future investigations into the
evolution of collective behavior using artiﬁcial life technology.
1 A system that appears to be similar in some ways, though it is based on 2d cellular
automata and the Santa Fe Institute Swarm system, is described at
http://omicrongroup.org/evo/.

Emergence of Collective Behavior in Evolving Populations of Flying Agents
63
2
SwarmEvolve 1.0
One of the demonstration programs distributed with breve is swarm, a simu-
lation of ﬂocking behavior modeled on the “boids” work of Craig W. Reynolds
[9]. In the breve swarm program the acceleration vector for each agent is de-
termined at each time step via the following formulae:
V = c1V1 + c2V2 + c3V3 + c4V4 + c5V5
A = m( V
|V|)
The ci are constants and the Vi are vectors determined from the state of the
world (or in one case from the random number generator) and then normalized
to length 1. V1 is a vector away from neighbors that are within a “crowding”
radius, V2 is a vector toward the center of the world, V3 is the average of the
agent’s neighbors’ velocity vectors, V4 is a vector toward the center of gravity
of all agents, and V5 is a random vector. In the second formula we normalize
the resulting velocity vector to length 1 (assuming its length is not zero) and
set the agent’s acceleration to the product of this result and m, a constant that
determines the agent’s maximum acceleration. The system also models a ﬂoor
and hard-coded “land” and “take oﬀ” behaviors, but these are peripheral to the
focus of this paper. By using diﬀerent values for the ci and m constants (along
with the “crowding” distance, the number of agents, and other parameters) one
can obtain a range of diﬀerent ﬂocking behaviors; many researchers have explored
the space of these behaviors since Reynolds’s pioneering work [9].
SwarmEvolve 1.0 enhances the basic breve swarm system in several
ways. First, we created three distinct species2 of agents, each designated by a
diﬀerent color. As part of this enhancement we added a new term, c6V6, to the
motion formula, where V6 is a vector away from neighbors of other species that
are within a “crowding” radius. Goal-orientation was introduced by adding a
number of randomly moving “energy” sources to the environment and imposing
energy dynamics. As part of this enhancement we added one more new term,
c7V7, to the motion formula, where V7 is a vector toward the nearest energy
source. Each time an agent collides with an energy source it receives an energy
boost (up to a maximum), while each of the following bears an energy cost:
– Survival for a simulation time step (a small “cost of living”).
– Collision with another agent.
– Being in a neighborhood (bounded by a pre-set radius) in which represen-
tatives of the agent’s species are outnumbered by representatives of other
species.
– Giving birth (see below).
2 “Species” here are simply imposed, hard-coded distinctions between groups of agents,
implemented by ﬁlling “species” slots in the agent data structures with integers
ranging from 0 to 2. This bears only superﬁcial resemblance to biological notions of
“species.”

64
L. Spector et al.
The numerical values for the energy costs and other parameters can be ad-
justed arbitrarily and the eﬀects of these adjustments can be observed visually
and/or via statistics printed to the log ﬁle; values typical of those that we used
can be found in the source code for SwarmEvolve 1.0.3
As a ﬁnal enhancement we leveraged the energy dynamics to provide a ﬁtness
function and used a genetic encoding of the control constants to allow for evolu-
tion. Each individual has its own set of ci constants; this set of constants controls
the agent’s behavior (via the enhanced motion formula) and also serves as the
agent’s genotype. When an agent’s energy falls to zero the agent “dies” and is
“reborn” (in the same location) by receiving a new genotype and an infusion
of energy. The genotype is taken, with possible mutation (small perturbation of
each constant) from the “best” current individual of the agent’s species (which
may be at a distant location).4 We deﬁne “best” here as the product of energy
and age (in simulation time steps). The genotype of the “dead” agent is lost,
and the agent that provided the genotype for the new agent pays a small en-
ergy penalty for giving birth. Note that reproduction is asexual in this system
(although it may be sexual in SwarmEvolve 2.0).
The visualization system presents a 3d view (automatically scaled and tar-
geted) of the geometry of the world and all of the agents in real time. Commonly
available hardware is suﬃcient for ﬂuid action and animation. Each agent is a
cone with a pentagonal base and a hue determined by the agent’s species (red,
blue, or purple). The color of an agent is dimmed in inverse proportion to its
energy — agents with nearly maximal energy glow brightly while those with
nearly zero energy are almost black. “Rebirth” events are visible as agents ﬂash
from black to bright colors.5 Agent cones are oriented to point in the direction
of their velocity vectors. This often produces an appearance akin to swimming
or to “swooping” birds, particularly when agents are moving quickly. Energy
sources are ﬂat, bright yellow pentagonal disks that hover at a ﬁxed distance
above the ﬂoor and occasionally glide to new, random positions within a ﬁxed
distance from the center of the world. An automatic camera control algorithm
adjusts camera zoom and targeting continuously in an attempt to keep most of
the action in view.
Figure 1 shows a snapshot of a typical view of the SwarmEvolve world. An
animation showing a typical action sequence can be found on-line.6
SwarmEvolve 1.0 is simple in many respects but it nonetheless exhibits
rich evolutionary behavior. One can often observe the species adopting diﬀerent
strategies; for example, one species often evolves to be better at tracking quickly
moving energy sources, while another evolves to be better at capturing static en-
3 http://hampshire.edu/lspector/swarmevolve-1.0.tz
4 The choice to have death and rebirth happen in the same location facilitated, as an
unanticipated side eﬀect, the evolution of the form of collective behavior described
below. In SwarmEvolve 2.0, among many other changes, births occur near parents.
5 Birth energies are typically chosen to be random numbers in the vicinity of half of
the maximum.
6 http://hampshire.edu/lspector/swarmevolve-ex1.mov

Emergence of Collective Behavior in Evolving Populations of Flying Agents
65
Fig. 1. A view of SwarmEvolve 1.0 (which is in color but will print black and white
in the proceedings). The agents in control of the pentagonal energy source are of the
purple species, those in the distance in the upper center of the image are blue, and a
few strays (including those on the left of the image) are red. All agents are the same
size, so relative size on screen indicates distance from the camera.
ergy sources from other species. An animation demonstrating evolved strategies
such as these can be found on-line.7
3
Emergence of Collective Behavior in SwarmEvolve 1.0
Many SwarmEvolve runs produce at least some species that tend to form static
clouds around energy sources. In such a species, a small number of individuals
will typically hover within the energy source, feeding continuously, while all
of the other individuals will hover in a spherical area surrounding the energy
source, maintaining approximately equal distances between themselves and their
neighbors. Figure 2 shows a snapshot of such a situation, as does the animation
at http://hampshire.edu/lspector/swarmevolve-ex2.mov; note the behavior of
the purple agents.
We initially found this behavior puzzling as the individuals that are not
actually feeding quickly die. On ﬁrst glance this does not appear to be adaptive
behavior, and yet this behavior emerges frequently and appears to be relatively
stable. Upon reﬂection, however, it was clear that we were actually observing
the emergence of a higher level of organization.
When an agent dies it is reborn, in place, with a (possibly mutated) version
of the genotype of the “best” current individual of the agent’s species, where
7 http://hampshire.edu/lspector/swarmevolve-ex2.mov

66
L. Spector et al.
Fig. 2. A view of SwarmEvolve 1.0 in which a cloud of agents (the blue species) is
hovering around the energy source on the right. Only the central agents are feeding;
the others are continually dying and being reborn. As described in the text this can be
viewed as a form of emergent collective organization or multicellularity. In this image
the agents controlling the energy source on the left are red and most of those between
the energy sources and on the ﬂoor are purple.
quality is determined from the product of age and energy. This means that the
new children that replace the dying individuals on the periphery of the cloud will
be near-clones of the feeding individuals within the energy source. Since the cloud
generally serves to repel members of other species, the formation of a cloud is a
good strategy for keeping control of the energy source. In addition, by remaining
suﬃciently spread out, the species limits the possibility of collisions between its
members (which have energy costs). The high level of genetic redundancy in the
cloud is also adaptive insofar as it increases the chances that the genotype will
survive after a disruption (which will occur, for example, when the energy source
moves).
The entire feeding cloud can therefore be thought of as a genetically coupled
collective, or even as a multicellular organism in which the peripheral agents
act as defensive organs and the central agents act as digestive and reproductive
organs.
4
SwarmEvolve 2.0
Although SwarmEvolve 2.0 was derived from SwarmEvolve 1.0 and is su-
perﬁcially similar in appearance, it is really a fundamentally diﬀerent system.

Emergence of Collective Behavior in Evolving Populations of Flying Agents
67
Fig. 3.
A view of SwarmEvolve 2.0 in which energy sources shrink as they are
consumed and agents are “fatter” when they have more energy.
The energy sources in SwarmEvolve 2.0 are spheres that are depleted (and
shrink) when eaten; they re-grow their energy over time, and their signals (sensed
by agents) depend on their energy content and decay over distance according to
an inverse square law. Births occur near mothers and dead agents leave corpses
that fall to the ground and decompose. A form of energy conservation is main-
tained, with energy entering the system only through the growth of the energy
sources. All agent actions are either energy neutral or energy consuming, and the
initial energy allotment of a child is taken from the mother. Agents get “fatter”
(the sizes of their bases increase) when they have more energy, although their
lengths remain constant so that length still provides the appropriate cues for
relative distance judgement in the visual display. A graphical user interface has
also been added to facilitate the experimental manipulation of system parame-
ters and monitoring of system behavior.
The most signiﬁcant change, however, was the elimination of hard-coded
species distinctions and the elimination of the hard-coded motion control for-
mula (within which, in SwarmEvolve 1.0, only the constants were subject to
variation and evolution). In SwarmEvolve 2.0 each agent contains a computer
program that is executed at each time step. This program produces two values
that control the activity of the agent:
1. a vector that determines the agent’s acceleration,
2. a ﬂoating-point number that determines the agent’s color.

68
L. Spector et al.
Agent programs are expressed in Push, a programming language designed
by Spector to support the evolution of programs that manipulate multiple data
types, including code; the explicit manipulation of code supports the evolution
of modules and control structures, while also simplifying the evolution of agents
that produce their own oﬀspring rather than relying on the automatic application
of hand-coded crossover and mutation operators [11], [12].
Table 1. Push instructions available for use in SwarmEvolve 2.0 agent programs
Instruction(s)
Description
DUP, POP, SWAP, REP, =, NOOP,
Standard Push instructions
PULL, PULLDUP, CONVERT, CAR,
(See [11])
CDR, QUOTE, ATOM, NULL, NTH,
+, ∗, /, >, <, NOT, AND, NAND
OR, NOR, DO*, IF
VectorX, VectorY, VectorZ,
Vector access, construction,
VPlus, VMinus, VTimes, VDivide,
and manipulation
VectorLength, Make-Vector
RandI, RandF, RandV, RandC
Random number, vector, and
code generators
SetServoSetpoint, SetServoGain,
Servo-based persistent
Servo
memory
Mutate, Crossover
Stochastic list manipulation
(parameters from stacks)
Spawn
Produce a child with code
from code stack
ToFood
Vector to energy source
FoodIntensity
Energy of energy source
MyAge, MyEnergy, MyHue,
Information about self
MyVelocity, MyLocation,
MyProgram
ToFriend, FriendAge, FriendEnergy,
Information about closest
FriendHue, FriendVelocity,
agent of similar hue
FriendLocation, FriendProgram
ToOther, OtherAge, OtherEnergy,
Information about closest
OtherHue, OtherVelocity,
agent of non-similar hue
OtherLocation, OtherProgram
FeedFriend, FeedOther
Transfer energy to closest
agent of indicated category
The Push instructions available for use in agent programs are shown in Table
1. In addition to the standard Push instructions, operating on integers, ﬂoating
point numbers, Boolean values, and code expressions, instructions were added for
the manipulation of vectors and for SwarmEvolve-speciﬁc sensors and actions.
Note that two sets of instructions are provided for getting information about

Emergence of Collective Behavior in Evolving Populations of Flying Agents
69
other agents in the world, the “friend” instructions and the “other” instructions.
Each “friend” instruction operates on the closest agent having a color similar to
the acting agent (currently deﬁned as having a hue within 0.1 in a hue scale that
ranges from 0.0 to 1.0). Each “other” instruction operates on the closest agent
having a color that is not similar to the acting agent.8 In some cases, in particular
when an agent sets its color once and never changes it, the “friend” instructions
will be likely to operate on relatives, since presumably these relatives would set
their colors similarly. But since agents can change their colors dynamically each
time-step, a “friend” is not necessarily a relative and a relative is not necessarily
a “friend.” The term “friend” here should be taken with a grain of salt; the
friend/other distinction provides a way for agents to distinguish among each
other based on color, but they may use this capability in a variety of ways.
SwarmEvolve 2.0 is an “autoconstructive evolution” system, in which agents
are responsible for producing their own oﬀspring and arbitrary reproductive
mechanisms may evolve [11]. Whenever an agent attempts to produce a child
(by executing the Spawn instruction), the top of its code stack is examined. If
the expression is empty (which happens rarely once the system has been running
for some time) then a newly generated, random program is used for the child.
If the expression is not empty then it is used as the child’s program, after a
possible mutation. The probability of mutation is also determined by the parent.
A random number is chosen from a uniform distribution from zero to the absolute
value of the number on top of the Integer stack; if the chosen number is zero
then a mutation is performed. The mutation operation is similar to that used
in traditional genetic programming: a random sub-expression is replaced with a
newly generated random expression. Note that the program access instructions
provide the means for agents to produce their children asexually or sexually,
potentially using code from many “mates.”
At the beginning of a SwarmEvolve 2.0 run most of the agents, which
will have been generated randomly, will not have programs that cause them
to seek food and produce oﬀspring; they will therefore die rather quickly and
the population will plummet. Whenever the population drops below a user-
deﬁned threshold the system injects new random agents into the world. With
the parameters used here, however, it usually takes only a few hundred time-steps
before “reproductive competence” is achieved — at this point the population is
self-sustaining as there are a large number of agents capable of reproducing.
SwarmEvolve 2.0 is a complex program with many parameters, not all of
which can be addressed in the scope of this short paper. However, the source
code for the system (including the parameters used in the experiments described
below) is available on-line.9 Figure 3 shows a typical scene from SwarmEvolve
2.0; an animation of a typical action sequence can be found on-line.10
8 If there are no other agents meeting the relevant criterion then each of these instruc-
tions operates on the acting agent itself.
9 http://hampshire.edu/lspector/swarmevolve-2.0.tz
10 http://hampshire.edu/lspector/swarmevolve2-ex1.mov

70
L. Spector et al.
5
Emergence of Collective Behavior in SwarmEvolve 2.0
The last two instructions listed in Table 1, FeedFriend and FeedOther, provide a
means for agents to transfer energy to one another (to share food). Each of these
instructions transfers a small increment of energy (0.01 out of a possible total
of 1.0), but only under certain conditions which we varied experimentally (see
below). Ordinarily, the use of these instructions would seem to be maladaptive,
as they decrease the energy of the acting agents. The use of a Feed instruction
thereby makes the feeding agent both more vulnerable and less likely to produce
children.
Might there nonetheless be some circumstances in which it is adaptive for
agents to feed one another? We set out to investigate this question by conducting
runs of SwarmEvolve 2.0 and monitoring the proportion of agents that feed or
attempt to feed other agents.11 Because the Feed instructions will occasionally
occur in randomly generated code and in mutations, we expect every run to
produce some number of calls to these instructions. We expect, however, that
the proportion of food sharing agents, when averaged over a large number of
runs, will reﬂect the extent to which food sharing is adaptive.
We hypothesized, for example, that dynamic, unstable environments might
provide a breeding ground for altruistic feeding behavior. We reasoned as follows,
from the perspective of a hypothetical agent in the system: “If the world is stable,
and everyone who’s smart enough to ﬁnd food can reliably get it, then I should
get it when I can and keep it to myself. If the world is unstable, however, so that
I’ll sometimes miss the food despite my best eﬀorts, then it’d be better for me
if everyone shared. Food from others would help to buﬀer the eﬀects of chance
events, and I’d be willing to share food when I have it in order to secure this
kind of insurance.” Of course one shouldn’t put too much faith in such “just so
stories,” but they can sometimes be a guide for intuitions. In the present case
they led us to conduct a simple experiment in which we varied the stability of the
energy sources and the sharing conditions in SwarmEvolve 2.0 and measured
the proportion of food-sharing agents that resulted.
We conducted a total of 1,625 runs under a variety of stability and sharing
conditions. We used values of the “stability” parameter ranging from 20 (unsta-
ble) to 2,000 (highly stable). The stability parameter governs the frequency with
which energy sources begin to drift to new, random locations; the probability
that a particular energy source will begin to drift to a new location in any partic-
ular time step is
1
stability. We collected data on four diﬀerent sharing conditions.
In all of the conditions the potential recipient is the closest agent of similar or
dissimilar color, depending on whether the agent is executing the FeedFriend or
FeedOther instruction respectively. In all cases the feeding is conditional on the
recipient having less energy than the provider. In “waste” sharing the energy
is all lost in the transfer, and the recipient receives nothing; we included this
11 For the analyses presented below we did not distinguish between FeedFriend and
FeedOther executions; we explored the distinction brieﬂy but there were no obvious
patterns in the data.

Emergence of Collective Behavior in Evolving Populations of Flying Agents
71
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
10
100
1000
10000
Environmental Stability
Waste Sharing
3
3
3
3
3
Charity Sharing
+
+
+
+
+
Mutual Sharing
2
2
2
2
2
Noop Sharing
×
×
×
×
×
Fig. 4. Proportion of agents that share food (on the y axis) graphed vs. environmental
(energy source) stability (on the x axis) for four sharing conditions (see text).
sharing condition as a control. In “charity” sharing the recipient receives all of
the energy, regardless of whether or not the recipient itself shares energy. In
“mutual” sharing the recipient receives all of the energy, but only if it has itself
shared energy at least once in its life. Finally, in “noop” sharing no energy is
transferred or lost; this is another control.
We collected data only from runs with at least 5,300 consecutive time-steps
of reproductive competence — there were 936 runs meeting this condition. For
qualifying runs we then collected data over the last 5,000 of time-steps, divided
into 100-time-step “epochs.” At each epoch boundary we took a census, recording
the proportion of living agents that have attempted to share energy with another
agent on at least one occasion.
Our results are graphed in Figure 4. Our hypothesis that dynamic, unstable
environments might provide a breeding ground for altruistic feeding behavior was
only partially conﬁrmed; indeed, the most stable environments also appear to be
conducive to food sharing. To the extent that the hypothesis is conﬁrmed it is
interesting to note that a similar eﬀect, involving the preference for cooperation
in unpredictable environments, has been observed in a radically diﬀerent, game-
theoretic context [2].
The “waste” sharing control produced less food sharing than all of the other
sharing conditions; this is what one would expect, as “waste” sharing has costs
but no possible beneﬁts. The “noop” sharing control, on the other hand, which
has no costs and no beneﬁts, produced slightly more sharing than all other con-
ditions at low stability. Note, however, that both “charity” sharing and “mutual”

72
L. Spector et al.
sharing, which have both costs and potential beneﬁts, produced more sharing
than both of the controls at several stability settings. There is substantial vari-
ance in the data, and the statistical signiﬁcance of some of the diﬀerences visible
in the graph is questionable. In any event we can say that the amount of sharing
in the “charity” and “mutual” conditions was, under several stability settings,
either greater than or at least not signiﬁcantly less than the amount of sharing
in the “noop” control. This by itself is evidence that altruistic feeding behavior
is adaptive in the environment under study. Many of the other diﬀerences in the
data are clearly signiﬁcant, and the trends indicate that collective feeding behav-
iors do arise in some circumstances and not in others. These simulations provide
a rich framework for investigating the relations between collective behavior and
evolution, which we have only begun to explore.
6
Conclusions and Future Work
The emergence of collective behavior is an intriguing and at times counter-
intuitive phenomenon, an understanding of which will have signiﬁcant impacts
on the study of living systems at all levels, from symbiotic microbes to hu-
man societies. The work presented in this paper demonstrates that new artiﬁcial
life technologies provide new tools for the synthetic investigation of these phe-
nomena, complementing the well-established analytic methods of evolutionary
biology and behavioral ecology.
In particular we demonstrated the emergence of a simple form of multicellu-
lar organization in evolving populations of agents based on a traditional ﬂocking
algorithm. We also demonstrated the emergence of altruistic feeding behavior
in a system that is considerably less constrained, as the agents are controlled
by evolved computer programs. We believe that this latter system provides sig-
niﬁcant new avenues of study by allowing for agents of arbitrary complexity to
evolve within complex, dynamic worlds.
Our own plans for this work in the near future include a systematic explo-
ration of the eﬀects of various parameter changes on the emergence of collec-
tive behavior. We are making the source code for SwarmEvolve 1.0 and 2.0
freely available in the hopes that others will also contribute to this process; see
http://hampshire.edu/lspector/swarmevolve-1.0.tz and
http://hampshire.edu/lspector/swarmevolve-2.0.tz.
Acknowledgments. Raymond Coppinger, Rebecca Neimark, Wallace Feurzeig,
Oliver Selfridge, and three anonymous reviewers provided comments that helped
to improve this work. This eﬀort was supported by the Defense Advanced Re-
search Projects Agency (DARPA) and Air Force Research Laboratory, Air Force
Materiel Command, USAF, under agreement number F30502-00-2-0611, and by
NSF grant EIA-0216344.

Emergence of Collective Behavior in Evolving Populations of Flying Agents
73
References
1. Bonner, J.T.: The evolution of complexity by means of natural selection. Princeton
University Press, Princeton, NJ (1988)
2. Eriksson, A., Lindgren, K.: Cooperation in an Unpredictable Environment. Proc.
Eighth Intl. Conf. on Artiﬁcial Life. The MIT Press, Cambridge, MA (2002) 394–
399.
3. Klein, J.: breve: a 3D Environment for the Simulation of Decentralized Systems
and Artiﬁcial Life. Proc. Eighth Intl. Conf. on Artiﬁcial Life. The MIT Press,
Cambridge, MA (2002) 329–334. http://www.spiderland.org/breve/breve-klein-
alife2002.pdf
4. Krebs, J.R., Davies, N.B.: An Introduction to Behavioural Ecology. 3rd edn. Black-
well Scientiﬁc Publications LTD, Oxford London Edinburgh Boston Melbourne
Paris Berlin Vienna (1981)
5. Maynard Smith, J., Szathm´ary, E.: The origins of life. Oxford University Press
(1999)
6. Pulliam, H.R., Caraci, T.: Living in groups: is there an optimal group size? In J.R.
Krebs and N.B. Davies (eds), Behavioral Ecology: An Evolutionary Approach,
2nd edn. Blackwell Scientiﬁc Publications LTD, Oxford London Edinburgh Boston
Melbourne Paris Berlin Vienna (1984) 122–147
7. Ray, T.S.: Is it Alive or is it GA. Proc. Fourth Intl. Conf. on Genetic Algorithms.
Morgan Kaufmann, San Mateo, CA (1991) 527–534
8. Reynolds, C.W.: An Evolved, Vision-Based Behavioral Model of Coordinated
Group Motion. In From Animals to Animats 2: Proc. Second Intl. Conf. on Simu-
lation of Adaptive Behavior. The MIT Press, Cambridge, MA (1993) 384–392
9. Reynolds, C.W.: Flocks, Herds, and Schools: A Distributed Behavioral Model.
Computer Graphics 24:4 (1987) 25–34
10. Silby, R.M.: Optimal group size is unstable. Anim. Behav. 31 (1983) 947–948
11. Spector, L., Robinson, A.: Genetic Programming and Autoconstructive Evolution
with the Push Programming Language. Genetic Programming and Evolvable Ma-
chines 3:1 (2002) 7–40
12. Spector, L.: Adaptive Populations of Endogenously Diversifying Pushpop Organ-
isms are Reliably Diverse. Proc. Eighth Intl. Conf. on Artiﬁcial Life. The MIT
Press, Cambridge, MA (2002) 142–145
13. Zaera, N., Cliﬀ, D., Bruten, J.: (Not) Evolving Collective Behaviours in Synthetic
Fish. From Animals to Animats 4: Proc. Second Intl. Conf. on Simulation of Adap-
tive Behavior. The MIT Press, Cambridge, MA (1996)

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 74–85, 2003.
© Springer-Verlag Berlin Heidelberg 2003
On Role of Implicit Interaction and Explicit
Communications in Emergence of Social Behavior in
Continuous Predators-Prey Pursuit Problem
Ivan Tanev
1 and Katsunori Shimohara
1, 2
1 ATR Human Information Science Laboratories,
2-2-2 Hikaridai, “Keihanna Science City”, Kyoto 619-0288, Japan
2 Graduate School of Informatics, Kyoto University,
Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan
{i_tanev, katsu}@atr.co.jp
Abstract. We present the result of our work on use of genetic programming for
evolving social behavior of agents situated in inherently cooperative environ-
ment. We use predators-prey pursuit problem to verify our hypothesis that rela-
tively complex social behavior may emerge from simple, implicit, locally de-
fined, and therefore – robust and highly-scalable interactions between the
predator agents. We propose a proximity perception model for the predator
agents where only the relative bearings and the distances to the closest predator
agent and to the prey are perceived. The instance of the problem we consider is
more realistic than commonly discussed in that the world, the sensory and
moving abilities of agents are continuous; and the sensors of agents feature lim-
ited range of “visibility”. The results show that surrounding behavior, evolved
using proposed strongly typed genetic programming with exception handling
(STGPE) emerges from local, implicit and proximity-defined interactions be-
tween the predator agents in both cases when multi-agents systems comprises (i)
partially inferior predator agents (with inferior moving abilities and superior
sensory abilities) and with (ii) completely inferior predator agents. In the latter
case the introduction of short-term memory and explicit communication con-
tributes to the improvement of performance of STGPE.
1   Introduction
Over the past few years, multi-agent systems (MAS) have become more and more
important in many aspects of computer science such as distributed artificial intelli-
gence, distributed computing systems, robotics, artificial life, etc. MAS introduce the
issue of collective intelligence and of the emergence of behavior through interactions
between the agents. An agent is a virtual entity that can act, perceive the proximity of
its environment and communicate with others; it is autonomous and has abilities to
achieve its objectives. MAS contain a world (environment), entities (agents), relations
between the entities, a way the world is perceived by the entities, a set of operations
that can be performed by the entities and the changes of the world as a result of these
actions. Currently, the main application areas of MAS are problem solving, simula

On Role of Implicit Interaction and Explicit Communications         75
tion, collective robotics, software engineering, and construction of synthetic worlds
[4]. Considering the latter application area and focusing on the autonomy of agents
and the interactions that link them together [14], the following important issues can be
raised: What is minimum amount of perception information needed to agents in order
to perceive the world? How can agents cooperate? What are the methods, and what are
the lower bounds of communications, required for them to coordinate their actions?
What is the architecture they should feature so that they can achieve their goals? What
approaches can be applied to automatically construct the agents’ functionality, with
the quality of such a design being competitive to the design handcrafted by human?
These issues are of special interest, since the aim is to create MAS which is scalable,
robust, flexible, and the able to automatically adapt to changes. These features of
MAS are believed to be particularly important in real world applications where the
approaches to construct synthetic worlds can be viewed as a practical methods, a tech-
niques towards creating complex “situational aware” multi-computer, multi-vehicle, or
multi-robot systems based on the concepts of agents, communication, cooperation and
coordination of actions.
Within considered context, the objective of our research is an automatic design of
autonomous agents which situated in inherently cooperative environment are capable
of accomplishing complex tasks through interaction. We adhere to the methodological
holism based on the belief that any complex system or society (Heraclitus, Aristotle,
Hegel, and more recently – [12][13]), and multi-agent society in particular [7] is more
than the sum of its individual entities, more than the sum of the parts that compose it.
The social behavior, needed to accomplish the complex task might emerge in MAS
from relatively simply defined interactions between the agents. We are particularly
interested in the ultimate case of such simplicity – local, implicit, proximity defined,
and therefore, robust, flexible and highly scalable interactions between the agents,
situated in more realistic (than commonly considered), inherently cooperative envi-
ronments. This document is intended to highlight the issues of applying genetic pro-
gramming for investigating the sufficiency of implicit interaction and the role of ex-
plicit communication in emergence of social behavior in MAS.
The remaining of the document is organized as follows. Section 2 introduces the
task which we use to test our hypotheses – an instance of the general, well-defined yet
difficult to solve predator-prey pursuit problem. The same section addresses the issue
of developing the software architecture of the agents. Section 3 elaborates the strongly
typed genetic programming with exceptions handling (STGPE), proposed as an algo-
rithmic paradigm used to evolve the functionality of agents. Empirical results are
presented in Section 4 and conclusion is drawn in Section 5.
2   The Problem and the Agents Architecture
2.1   Instance of Predator Prey Pursuit Problem
The general, well-defined and well-studied yet difficult to solve predator-prey pursuit
problem [2] is used to verify our hypothesis that relatively complex social behavior

76         I. Tanev and K. Shimohara
might emerge from simple, local, implicit, proximity-defined, and therefore – robust
and highly-scalable interactions between the predator agents. The problem comprises
four predator agents whose goals are to capture a prey by surrounding it on all sides in
a world.  In our work we consider an instance of the problem, which is more realistic
than commonly considered in the previous work [5][6][9]. The world is a simulated
two-dimensional continuous torus 1600mm x 1000mm. The moving abilities of four
predator agents are continuous too – the predators can turn left and right to any angle
from their current heading and can run with speed equal to 0, 0.25, 0.5, 0.75 and 1.0 of
maximum speed. In addition, we introduce an proximity perception model for predator
agents in that they can see the prey and only the closest predator agent, and only when
they are within the limited range of visibility of their simulated (covering an area of
360 degrees) sensors. The prey employs random wandering if there is no predator in
sight and a priory handcrafted optimal escaping strategy as soon as predator(s) become
“visible”. The maximum speed of prey is higher than the maximum speed of predator
(i.e. predator-agents feature inferior moving abilities). In order to allow for predators
to stalk and collectively approach the prey, in the first of the two considered cases the
range of visibility of predators is more than the range of visibility of the prey (i.e.
superior are only the sensory abilities of predators, therefore they can be considered
partially inferior). In the second case the range of visibility of predators is equal to the
range of visibility of the prey (i.e. completely inferior predator agents). We consider
these two cases in order to create an inherently cooperative environment in that the
mission of predators is nearly impossible unless they collaborate with each other. We
are not interested in cases when predators are superior in their moving abilities, since
the capturing of the prey in this case seems trivial, can be accomplished by single
agent, and therefore does not require collective behavior from MAS. Analogically, the
situation comprising completely inferior agents who besides being slower feature
more myopic (rather than equal) sensory abilities than the prey is intractable for the
conditions give above, and therefore is beyond our current consideration.
2.2   Architecture of the Agents
We adopted the subsumption architecture [3] of the agents comprising of functional
modules distributed in three levels, corresponding to the three different aspects (“lev-
els of competence“) of agent’s behavior: wandering/exploring, greedy chase and social
behavior - surrounding (Figure 1a). Given that we focus our attention on evolving the
top-level, highest priority module – surrounding the prey (assuming that the rest two
modules are handcrafted), our objective of automatic design of autonomous agents via
simulated evolution, which we declared earlier, can be rephrased as evolving the sur-
rounding module in subsumption architecture of the agents.
In order to coordinate the functionalities of each of architectural modules we intro-
duce the notion of agent‘s state. At every instant, the agent can be in one of the three
states, corresponding to the module which is currently governing the agent’s behavior:
surrounding, greedy chase and wandering/exploring. The agent is in surrounding state
if and only if there is a match (response) for the currently perceived proximity of the
world (stimuli) in the functionality of evolved surrounding module. Being in sur-

On Role of Implicit Interaction and Explicit Communications         77
rounding state, the agent is fully controllable by the functionality of the surrounding
module, while functionalities of hunting and wandering/exploring modules are inhib-
ited. If there is no match of the perceived proximity of the world in the functionality of
evolved surrounding module, the agents state switches to greedy chase if prey is in
sight or to wandering/exploring state otherwise (Figure 1b).
We would like to emphasize that the proposed implicit interstate-transition scheme
is fully controllable by evolved functionality of the surrounding module, which allows
to simultaneously evolve (i) the capability of agents to resolve social dilemmas, de-
termined by the way social behavior overrides greedy chase when prey is in sight, and
(ii) the capability to resolve the exploration-exploitation dilemma, determined by the
ability of social behavior to override wandering/exploring when prey is invisible.
3   Algorithmic Paradigm Employed to Evolve Predator Agents
3.1   Strongly-Typed Genetic Programming with Exception Handling
Limiting the Search Space of Genetic Programming. We consider a set of stimulus-
response rules as a natural way to model the reactive behavior of predator agents [7]
which in general can be evolved using artificial neural networks, genetic algorithms,
and genetic programming (GP). GP is a domain-independent problem solving ap-
proach in which a population of computer programs (individuals) is evolved to solve
problems [8]. The simulated evolution in GP is based on the Darwinian principle of
reproduction and survival of the fittest. In GP genetic programs (individuals) can be
represented as parsing trees whose nodes are functions, variables or constants. The
nodes that have sub-trees are non-terminals - they represent functions where the sub-
trees represent the arguments to function of that node. Variables and constants are
terminals – they take no arguments and they always are leaves in the parsing tree. The
set of terminals for evolving agent’s behavior includes the perceptions (stimuli), and
the actions (response) which the agent is able to perform. The set of functions com-
prises the arithmetical and logical operators, and the IF-THEN function, establishing
the relationship between certain stimulus and corresponding response(s). Without
touching the details of representation of genetic programs, which will be elaborated
later in Section 3.2, a human readable form of sample stimulus-response rule is shown
in Figure 2. It expresses a reactive behavior of turning to the bearing of the peer agent
(Peer_a) plus 10 (degrees) as a result of stimulus of its own speed being less than 20
(mm/s).
The strength of GP to automatically evolve a set of stimulus-response rules of arbi-
trary complexity without the need to a priory specify the extent of such complexity
might imply an enormous computational effort caused by the need to discover a huge
search space while looking for potential solution to the problem. Agreeing with [13]
that for huge and multidimensional search spaces the introduction of “pruning algo-
rithms” is a critical step towards efficient solution search, we impose a restriction on
the syntax of evolved genetic programs based on some a priory known semantics. The

78         I. Tanev and K. Shimohara
approach is known as strongly typed genetic programming and its advantage over
canonical GP in achieving better computational effort is well proven [11].
                               a)                                                                              b)
Fig. 1. Subsumption architecture of the agents: functional structure (a) and states (b)
     IF (Speed<20) THEN Turn(Peer_a+10)
Fig. 2. Sample stimulus-response rule
Considering the sample rule, shown in Figure 2 it is noticeable that both the func-
tions and their operands are associated with data types such as speed (e.g. Speed, 20),
angle of visibility (bearing)  (Peer_a, 10), and  Boolean (Speed<20). An eventual
arbitrary creation or modification of genetic program semantically would make little
sense: indeed, it is unfeasible to maintain Boolean expressions comparing operands of
different data types at least because they have different physical units. Moreover, since
we introduce sensor’s range limits, there is a clear possibility of maintaining introns in
genetic programs when, for example Boolean expressions include comparison of per-
ception variable of certain data type with constant value beyond the limits of the data
type of that variable (e.g. Peer_d>1000, in case that sensor range is only 400).
Analogically, the semantics of action Turn() implies a parameter of data type angle.
And allowing just addition and subtraction as arithmetical operations implies that all
the operands involved in the expression which defines the turning parameter should
have the same data type angle. Addressing the mentioned concerns, the grammar of
STGPE establishes generic data types of visible angle, distance, speed, and Boolean
with the corresponding allowed ranges of the values for their respective instances
(variables and ephemeral constants). In addition, it stipulates the data type of the re-
sults of arithmetical and logical expressions, and the allowed data type of operands
(perception variables and ephemeral constants) involved in these expressions.
    We would like to emphasize that proposed approach is not based on domain-
specific knowledge, and therefore STGPE can not be considered as a “stronger” ap-
proach compromising the domain-neutrality of the very GP paradigm itself. The limi-
tations imposed to the syntax of genetic programs are solely based (i) on the natural
presumption that the predator agents are fully aware of their physically reasonable
limits of his perception- and moving abilities; and (ii) on the common rule in strongly-
Stimuli
Responses
Wandering/exploring
Surrounding
Greedy chase
Stimuli
Responses
Wandering/exploring
Surrounding
Greedy chase
Prey visible
Prey invisible
Wandering/exploring
(handcrafted)
Greedy chase
(handcrafted)
Surrounding
(evolved)
High priority
Low priority
Prey visible
Prey invisible
Wandering/exploring
(handcrafted)
Greedy chase
(handcrafted)
Surrounding
(evolved)
High priority
Low priority

On Role of Implicit Interaction and Explicit Communications         79
typed 3G algorithmic languages that all the operands in addition, subtraction and com-
parison operations should have the same data types. In no way these limitations incor-
porate a priory obtained knowledge, specific for the domain or for the world where the
agents are situated.
Exception Handling. The notion of exception handling is introduced in a way, much
similar to the 3G algorithmic languages. In our approach, an exception is an event,
raised when a runtime error occurs in an evaluation of the Boolean expression in the
conditional part of IF-THEN rule. Due to the limited range of simulated sensors of
predator agents, such an error would happen when the Boolean condition involving
perception variable(s) related to perceiving another entity in the world (e.g. closest
predator agent and/or the prey) can not be evaluated because the corresponding entity
is currently “invisible”. In addition to IF-THEN we introduce IF-THEN-NA (IF-THEN-
“not available”) type of stimulus-response rule with exception handling capabili-
ties. The human readable syntax and the corresponding semantics of sample stimulus
response rule with exception are shown in Figure 3.
3.2   Main Attributes of STGPE
Function and Terminal Sets. Function and terminal sets of adopted STGPE are
summarized in Table 1. Notice the local, proximity defined sensory abilities of agents.
Representation of Genetic Programs. Inspired by flexibility and recently emerged
widespread adoption of document object model (DOM) and extensible markup lan-
guage (XML), we represent genetic programs as a DOM-parsing trees featuring corre-
sponding flat XML text. Our additional motivation stems from the fact that despite of
the recently reported use of DOM/XML for representing computer architectures,
source codes, and agents’ communication languages we are not aware about any at-
tempts to employ XML technology for representing evolvable structures such as ge-
netic programs in generic, standard, and portable way. Our approach implies per-
forming genetic operations on DOM-parsing tree using off-the shelf, platform- and
language neutral DOM-parsers, and using XML-text representation (rather than S-
expression) as a flat format, feasible for migration of genetic programs among the
computational nodes in eventual distributed implementation of STGPE. The fragment
of XML representation of the above discussed sample stimulus-response rule (refer to
Figure 3) is shown in Figure 4. The benefits of using DOM/XML-based representa-
tions of genetic programs, as documented in [15] can be briefly summarized as fol-
lows: (i) XML tags offer a generic support for maintaining data types in STGPE; (ii)
W3C-standard XML schema offers generic way for representing the grammar of
STGPE; (iii) using standard built-in API of DOM-parsers for maintaining and ma-
nipulating genetic programs; (iv) OS neutrality of parsers; (v) algorithmic language
neutrality of DOM-parsers, and (vi) inherent Web-compliance of eventual parallel
distributed implementation of STGPE.
Genetic Operations.  Binary tournament selection is employed – a robust, commonly
used selection mechanism, which has proved to be efficient and simple to code.

80         I. Tanev and K. Shimohara
TRY IF (Peer_d<20)
THEN Turn(Peer_a+10)
EXCEPT  Turn(10);
IF (Peer is Visible) THEN BEGIN
  IF (Peer_d<20) THEN Turn(Peer_a+10);
END ELSE Turn(10); // invisible preda-
tor agent
                a)                                                              b)
Fig. 3. Syntax (a) and semantics (b) of sample stimulus-response rule with exception handling
Table 1. Function Set and Terminal Set of STGPE
Category
Designation
Remarks
Function set
IF-THEN,  IF-THEN-
NA
LE, GE, WI, EQ, NE, +,
-
IF-THEN without/with exception handling
 	

Sensory abilities
Prey_d;  Peer_d
Prey_a;   Peer_a
PreyVisible; PeerVisible
Distance to the prey and to the closest agent, mm.
Bearing of the prey and of the closest agent,
degrees
True if prey /  agent  is “visible”, false otherwise
State variable
Speed
Speed of the agent, mm/s
Ephemeral
constants
Integer
Terminal set
Moving abilities
 
Stop, Go_1.0
Go_0.25, Go_0.5,
Go_0.75
   
Sets speed to 0, Sets speed to maximum
Sets speed to 25%, 50%, 75% of maximum
<IF-THEN-NA>
   <COND-THEN-NA><COND_TDistance>
       <VAR_TDistance>Peer_d</VAR_TDistance >
       <OPER TDistance>LE</OPER_TDistance >
       <CONST TDistance>20</CONST_TDistance >
   </COND_TDistance ></COND-THEN-NA>
   <THEN> ... </THEN>
   <NA>   ... </NA>
</IF-THEN-NA>
Fig. 4.  Fragment of XML representation of sample stimulus-response rule
Crossover operation is defined in a strongly typed way in that only the nodes (and
corresponding subtrees) of the same data type (i.e. labeled with the same tag) from
parents can be swapped. The sub-tree mutation is also allowed in strongly typed way
in that a random node in genetic program is replaced by syntactically correct sub-tree.
The routine refers to the type of node it is going to currently alter and applies the ran-
domly chosen rule from the set of applicable rules as defined in the grammar of
STGPE. The transposition mutation also operates on single genetic program by swap-
ping two random nodes having the same data type.
Breeding Strategy. We adopted a homogeneous breeding strategy in which the per-
formance of single genetic program, cloned to all the agents is evaluated. Anticipating
that the symmetrical nature of the world, populated with identical predator agents is

On Role of Implicit Interaction and Explicit Communications         81
unlikely to promote any specialization in the behavior of agents, we consider the fea-
tures of such a homogeneous multi-agent society as (i) adequate to the world and (ii)
consistent with our previously declared intention to create robust and well scalable
multi-agent system.
Fitness Function. In order to obtain more general solutions to the problem the fitness
of the genetic program is evaluated as average of the fitness measured over 10 differ-
ent initial situations. However, based on the empirically proven data that on the initial
stages of evolution agents are hardly able to successfully resolve more than few (out
of 10) initial situations in order to enhance the computational performance of STGPE
we applied the approach of noisy evaluation of the fitness function [10].  The amount
of initial situations used to evaluate the genetic programs in population gradually in-
creases as population evolves. Starting from 4 for the first generation of each run, the
amount of situations is revised on completion of each generation and it is set to exceed
by number of 2 the amount of situations, successfully solved by the best-of-generation
genetic program. Given that with addition of another initial situation(s) they have to
resolve, the agents would perform either better or, most probably worse, the fitness of
the best-of-current generation could be occasionally somewhat worse than fitness of
best genetic program from previous generation. Therefore, it is reasonable to expect
non-monotonous fitness convergence characteristics of STGPE.
The fitness F measured for the trial starting with particular initial situation is
evaluated as a length of the radius vector of the derived agents’ behavior in the virtual
energy-distance-time space as:
F 
 = 
2
2
2
T
D
dE
A
A
+
+
(1)
where dEA is the average energy loss during the trial, DA is the average distance to the
prey by the end of the trial, and T is the elapsed time of the trial. The quantities dEA
and DA are averaged over the all predator agents. The energy loss estimation dE for
each of predator agents takes into account both the basal metabolic rate, equal to 0.05
units per second, and the energy loss for moving activities equal to 0.01 units per mm
of path traversed during the trial. The trial is limited to 300s of “real” time or to the
instance when prey is captured; and with sampling rate of 500ms it is simulated with
up to 600 time steps. Smaller values of fitness function correspond to better perform-
ing predator agents.
4    Empirical Results
4.1   Parameter Values of STGPE
The parameters of STGPE used in our experiments are as follows: the population size
is 400 genetic programs, the selection ratio is 0.1 (including 0.01 elitism), and the
mutation ratio is 0.02, equally divided between sub-tree mutation and transposition.
The termination criterion is defined as a disjunction of the following three termination
conditions: (i) fitness of the best genetic program in less than 300 and the amount of

82         I. Tanev and K. Shimohara
initial situations in which the prey is captured (successful situations) equals 10 (out of
10), (ii) the amount of elapsed generations is more than a 100, and (iii) the amount of
recent generations without fitness improvement is more than 16.
4.2   Partially Inferior Predator Agents
In the first case a superior sensory abilities of predators (range of visibility 400mm vs.
200mm for prey) and inferior moving abilities are considered (20mm/s vs. 24mm/s).
The computational effort (amount of genetic programs needed to be evaluated in order
to obtain the solution with specified probability, e.g. 0.95), is obtained from the prob-
ability of success p(t) by each of 20 independent runs in a way as suggested in [8]. The
result, shown in Figure 5a indicates that p(t)=0.95 by generation 80 which yields a
computational effort of about 32,000 genetic programs. Typical fitness convergence
characteristic is shown in Figure 5b.
                                      a)                                                                      b)
Fig. 5.  Probability of success (a) and typical fitness convergence characteristic (b)
Human-readable representation of sample best-of-run genetic program is shown in
Figure 6, and the traces of the entities in the world for one of the 10 initial situations is
shown in Figure 7. The prey, originally situated in the center of the world, is captured
by time step 140. The emergence of following behavioral traits of predator agents are
noticeable: (i) switch from greedy chase into surrounding approach of agents #3 (time
step 65, on right part of the world) and agent #2 (time step120, top left) as soon as
other agents appear in sight; (ii) zigzag move by agent #0 which results a lower chas-
ing speed indicating “intention” to trap the prey (after time step 100, far right and far
left) and (iii) surrounding approach demonstrated by agents #1 and #3 during the final
stages of the trial (top left).
4.3   Completely Inferior Predator Agents
In this case the same range of visibility of predators (400mm vs. 400mm respectively)
and inferior moving abilities are considered (20mm/s vs. 24mm/s). These conditions
definitely render the task more difficult for predator agents. As empirical results indi-
cate that for the same values of GP-parameters (as sated in Section 4.1) a probability
0.0
0.2
0.4
0.6
0.8
1.0
0
20
40
60
80
100
Generations
p(t)
Fitness
Successful  situations              
Evaluated situations
Generations
60
40
20
0
Fitness
700
600
500
400
300
200
100
0
Situations
10
8
6
4
2
0

On Role of Implicit Interaction and Explicit Communications         83
of success of 0.95 is hardly achievable. In order to illustrate the very ability of STGPE
to discover the solution in these conditions we present plotted values of fitness of the
best-of-run genetic program and the amount of successfully resolved situations for 20
independent runs. The results are shown in Figure 8a.
Program Main;
type TDistance =    0..400;
     TVisAngle = -180..180;
     TSpeed    =    0..22;
var  Peer_d, Prey_d           : TDistance;
     Peer_a, Prey_a           : TVisAngle;
     Speed                    : TSpeed;
     PreyVisible, PeerVisible : Boolean;
Procedure GP;
begin
 try if (Prey_a >= -26)
   then try if (Prey_a within -5)
     then begin
       if (not PeerVisible) then Turn(Prey_a);
       try if (Prey_a <= -139)
         then begin
           Null;
           Go_0.25;
         end;
       except Turn(-24-Peer_a+7-24);
     end;
   except Null;
 except try if (Peer_d <= 136) then Null; except Turn(Prey_a);
end;
begin // main program
  GP;
      end.
Fig. 6.  Human-readable representation of sample best-of-run genetic program
Fig. 7.  Traces of the entities with agents governed by the genetic program shown in Figure 6.
The prey is captured in 140 simulated time steps (top left). Larger white and small black circles
denote the predator agents in their initial and final position respectively. The small white circle
indicates the prey, initially situated in the center of the world. The numbers in rectangles show
the timestamp information.

84         I. Tanev and K. Shimohara
Fig. 8. Fitness and amount of successful situation of the best-of-run genetic program in 20
independent runs when agents are implicitly interacting (a) and when agents are employing
short term memory with explicit communication.
As figure illustrates, only 25% of runs have been successfully completed (i.e. termi-
nated by criteria (i) as described in Section 4.1) and the average and the standard de-
viation of fitness and successful situations are FA=362, 
2
F=63, SA=8 and 
2
S=1.9. The
results with introduced short term (working) memory [1], storing the direction where
in which prey has been recently seen and explicit communication allowing the ex-
change the currently or recently seen direction to the prey  (i.e. predator agents still
remain mechanically inferior) are shown in Figure 8b. Although the solution can be
evolved by STGPE even with implicit interactions between the predator agents, intro-
ducing short term memory and explicit communication improves the performance of
simulated evolution: 35% of 20 runs have been successfully completed with more
favorable statistical results of FA=346, 
2
F=53, SA=9 and 
2
S=1.1.
5 Conclusion
We presented the result of our work on use of genetic programming for evolving so-
cial behavior of agents situated in inherently cooperative environment. We use preda-
tors-prey pursuit problem to verify our hypothesis that relatively complex social be-
havior may emerge from simple, implicit, locally defined, and therefore – robust and
highly-scalable interactions between the predator agents. We proposed a proximity
perception model for the predator agents where only the relative bearings and the
distances to the closest predator agent and to the prey are perceived. The instance of
the problem we consider is more realistic than commonly discussed in that the world,
the sensory and moving abilities of agents are continuous; and the sensors of agents
feature limited range of “visibility”. Adopted subsumption architecture and developed
implicit inter-state transition model allow for simultaneous evolution of the capabili-
ties of predator agents to resolve both the social dilemma and the dilemma between
exploration and exploitation. The empirical results show that surrounding behavior,
evolved using proposed strongly typed genetic programming with exception handling
(STGPE) emerges from local, implicit and proximity-defined interactions between the
predator agents in both cases when multi-agents systems comprises (i) partially infe-
rior predator agents (with inferior moving abilities and superior sensory abilities) and
with (ii) completely inferior predator agents. In the latter case the introduction of

On Role of Implicit Interaction and Explicit Communications         85
short-term memory and explicit communication contributes to the improvement of
performance of STGPE.
Acknowledgements. This research was conducted as part of “Research on Human
Communication” with funding from the Telecommunications Advancement Organi-
zation of Japan.
References
1.
Barborica, A., Ferrera, V.P., Estimating Invisible Target Speed from Neuronal Activity in
Monkey Frontal Eye Field, Nature Neuroscience Vol.6, No.1 (2003) 66–74
2.
Benda, M., Jagannathan, B. , Dodhiawala, R.: On Optimal Cooperation of Knowledge
Sources. Technical Report BCS-G2010-28, Boeing AI Center, Boeing Computer Services,
Bellevue,WA (1986)
3.
Brooks, R.A.: A Robust Layered Control System for a Mobile Robot. IEEE Journal of
Robotics and Automation, Vol.2, No.1  (1986) 14–23
4.
Ferber, J.: Multi-Agent Systems: An Introduction to Distributed Artificial Intelligence,
Harlow: Addison Wesley Longman (1999)
5.
Haynes, T., Sen, S.: Evolving Behavioral Strategies in Predators and Prey (1996)
6.
Haynes, T., Wainwright, R., Sen, S., Schoenefeld, D.: Strongly Typed Genetic Program-
ming in Evolving Cooperation Strategies (1997)
7.
Holand, J.H.: Emergence: From Chaos to Order, Cambridge, Perseus Books (1999)
8.
Koza, J.R.: Genetic Programming: On the Programming of Computers by Means of Natu-
ral Selection, Cambridge, MA, MIT Press (1992)
9.
Luke, S., Spector, L.: Evolving Teamwork and Coordination with Genetic Programming
10. Miller, B.,L., Goldberg, D., E.,: Genetic Algorithms, Tournament Selection, and the Ef-
fects of Noise, Illigal Report No. 95006, University of Illinois (1995).
11. Montana, D.: Strongly Typed Genetic Programming, Evolutionary Computation, Vol.3,
No.2 (1995) 199–230
12. Morgan, C.: Emergent Evolution, New York (1923)
13. Morowitz, H.,J.: The Emergence of Everything: How the World Became Complex, Oxford
University Press, New York (2002)
14. Parunak, H. Van D., Brueckner, S., Fleischer, M., Odell, J.: Co-X: Defining what Agents
Do Together, Proceedings of the AAMAS 2002 Workshop on Teamwork and Coalition
Formation, Onn Shehory, Thomas R. Ioerger, Julita Vassileva, John Yen, eds., Bologna, It-
aly, (2002)
15. Tanev, I.: DOM/XML-Based Portable Genetic Representation of Morphology, Behavior
and Communication Abilities of Evolvable Agents, Proceedings of the 8
th International
Symposium on Artificial Life and Robotics (AROB’03), Beppu, Japan (2003), 185–188 

Demonstrating the Evolution of
Complex Genetic Representations:
An Evolution of Artiﬁcial Plants
Marc Toussaint
Institut f¨ur Neuroinformatik, Chair for Theoretical Biology
Ruhr-Universit¨at Bochum ND-04, 44780 Bochum, Germany
toussaint@neuroinformatik.rub.de
Abstract. A common idea is that complex evolutionary adaptation is
enabled by complex genetic representations of phenotypic traits. This pa-
per demonstrates how, according to a recently developed theory, genetic
representations can self-adapt in favor of evolvability, i.e., the chance of
adaptive mutations. The key for the adaptability of genetic representa-
tions is neutrality inherent in non-trivial genotype-phenotype mappings
and neutral mutations that allow for transitions between genetic repre-
sentations of the same phenotype. We model an evolution of artiﬁcial
plants, encoded by grammar-like genotypes, to demonstrate this theory.
1
Introduction
In ordinary evolutionary systems, including natural evolution and standard Ge-
netic Algorithms (GAs), the search strategy is determined by uncorrelated mu-
tations and recombinations on the level of genes. In view of this trial-and-error
search strategy, it might seem surprising that natural evolution was that eﬀec-
tive in ﬁnding highly structured, complex solutions. One might have expected
that the search strategy needs to be more sophisticated, more structured and
adapted to the “objective function” to accomplish this eﬃciency.
An example for sophisticated, adapted, and structured search strategies are
recent developments in evolutionary computation, which can be collected un-
der the name of Estimation-of-Distribution Algorithms (EDAs, see [10] for a
review). These algorithms structure search by learning probabilistic models of
the distribution of good solutions.
Now, it hardly seems that ordinary evolutionary systems like natural evolu-
tion or GAs learn “models about the distribution of good solutions” and adapt
their mutational exploration accordingly. However, it is possible for ordinary evo-
lutionary systems to adapt their search strategy and natural evolution certainly
exploits this possibility. The key to understand this possibility is to consider
a non-trivial relation between genotype and phenotype, between the genes and
the phenes (phenotypic traits that are ﬁtness-relevant). In this case it is possible
that the same phenotype can be represented by diﬀerent genotypes. In other
words, evolution could principally adapt the way it represents solutions.
Adapting the genetic representation of phenes while keeping mutation opera-
tors ﬁxed is in some sense dual to adapting the mutation operators while keeping
E. Cant’u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 86−97, 2003.
 Springer-Verlag Berlin Heidelberg 2003

the representation of phenes ﬁxed. Especially in the biology literature it is often
argued that this is the way natural evolution adapts its search strategy: The way
genes encode phenotypic traits is surely not an incident but the outcome of a
long adaptative process in favor of evolvability [15]. Much research is spend to
understand these principles of gene interactions (like epistasis or canalization)
[1, 16, 17, 4, 5].
But there would be another open question. Do genetic representations really
adapt such that the search strategy becomes “better”? Is the induced adaptation
of the search strategy similar to the adaptation in the case of EDAs, such that
indeed a model of the distribution of good solutions is indirectly learned by
adapting the representations of solutions? Recently, a theory on the adaptation
of phenotypic search distributions (exploration distributions) in the case of non-
trivial genotype-phenotype mappings was proposed [14]. Since the present paper
aims to illustrate this process and thereby to open a more intuitive access to this
theory, we want, at this place, brieﬂy review the main results.
A non-trivial genotype-phenotype mapping is such that (1) the same pheno-
type is encoded by more than one genotype, and (2), at least for some of these
phenotypes, the induced phenotypic variability depends on the genotype it is
encoded with. In this case, every genotype g may be written as a tuple (x, σ),
where x is its phenotype and σ generally represents any kinds of neutral traits
of that genotype (strategy parameters are only a special case). Diﬀerent σ for
the same x mean diﬀerent genetic representations of the same phenotype. In
[14] it is shown that a σ is one-to-one identiﬁable with a mutation distribution
(i.e., search distribution) “around” g. Hence, the tuple (x, σ) actually lives in
the product space of phenotype space and the space of distributions.
Now, the main approach is the following: Instead of investigating the evolu-
tion of genotypes g in a given evolutionary process, one can also analyze how x
and σ evolve in that same process. The most interesting result is that the equa-
tion that describes the evolution of σ’s turns out to be itself an equation similar
to evolutionary dynamics. The equation allows to associate a quality measure
to each σ and says that σ’s evolve as to increase this quality measure just as
normal evolutionary processes evolve as to increase the ﬁtness of genotypes. This
quality measure of σ’s can be interpreted as a measure of “how good the mu-
tational phenotypic variability (the phenotypic search distribution) matches the
distribution of good organisms”—just as for EDAs. The results are summarized
in following corollary:
Corollary 1 ([14]). The evolution of genetic representations, or equivalently, of
exploration distributions (σ-evolution) naturally has a selection pressure towards
– minimizing the Kullback-Leibler divergence between exploration and the ex-
ponential ﬁtness distribution, and
– minimizing the entropy of exploration.
Here, the exponential ﬁtness distribution describes the distribution of good
solutions and the Kullback-Leibler divergence is a distance measure between two
distributions and thus measures the match between the search strategy (as given
by the exploration distribution) and this exponential ﬁtness distribution.
This paper will present a ﬁrst case study for the evolution of genetic represen-
tations by modeling an evolution of artiﬁcial plants. We chose this case study for
87
Demonstrating the Evolution of Complex Genetic Representations

several reasons: First, the genotype-phenotype mapping that we will introduce
shortly is highly non-trivial in the strict sense we deﬁned it for σ-evolution, and
it is interpretable from an (abstract) biological point of view. Further, this kind
of encoding and the possibility to visually display the phenotypes allows to really
grasp the genetic encodings, how completely diﬀerent genetic representations of
the same phenotype are possible, that complex features like gene interactions
and modularity of the genetic representations can emerge.
The next section introduces the genetic encoding and the mutability we as-
sume on this encoding. The major novelty here are 2nd-type mutations that
allow for neutral transitions between equivalent genetic representations within
the grammar-like encoding. Section 3 then explains the experimental setup and
presents the experiments that demonstrate the evolution of complex genetic rep-
resentations. Conclusions follow thereafter.
2
A Genotype Model and 2nd-Type Mutations
The genotype-phenotype mapping we will assume is a variant of the L-systems
proposed by [11] to encode plant-like structures. Let us introduce this encod-
ing from another point of view that puts more emphasis on the principles of
ontogenesis and gene interactions.
Let us consider the development of an organism as an interaction of its state
Ψ (describing, e.g., its cellular conﬁguration) and a genetic system Π, neglecting
environmental interactions. Development begins with an initial organism in state
Ψ (0), the “egg cell”, which is also inherited and which we model as part of the
genotype. Then, by interaction with the genetic system, the organism develops
through time, Ψ (1), Ψ (2), .. ∈P, where P is the space of all possible organism
states. Hence, the genetic system may be formalized as an operator Π : P →
P modifying organism states such that Ψ (t) = Πt Ψ (0). We make a speciﬁc
assumption about this operator: we assume that Π comprises a whole sequence
of operators, Π = ⟨π1, π2, .., πr⟩, each πi : P →P. A single operator πi (also
called production rule) is meant to represent a transcription module, i.e., a single
gene or an operon. Based on these ideas we deﬁne the general concept of a
genotype and a genotype-phenotype mapping for our model:
A genotype consists of an initial organism Ψ (0) ∈P and a sequence Π =
⟨π1, π2, .., πr⟩, πi : P →P of operators. A genotype-phenotype mapping φ devel-
ops the ﬁnal phenotype Ψ (T ) by recursively applying all operators on Ψ (0).
This deﬁnition is somewhat incomplete because it does not explain the stop-
ping time T of development and in which order operators are applied. We keep
to the simplest options: we apply the operators in sequential order and will ﬁx
T to some chosen value.
For the experiments we need to deﬁne how we represent an organism state
Ψ and how operators are applied. We represent an organism by a sequence of
symbols ⟨ψ1, ψ2, ..⟩, ψi ∈A. Each symbol may be interpreted, e.g., as the state
of a cell; we choose the sequence representation as the simplest spatially orga-
nized assembly of such states. Operators are represented as replacement rules
⟨a0:a1, a2, ..⟩, ai ∈A, that apply on the organism by replacing all occurrences
of a symbol a0 by the sequence ⟨a1, a2, ..⟩. If the sequence ⟨a1, a2, ..⟩has length
88
M. Toussaint

greater than 1, the organism is growing; if it has length 0, the organism shrinks.
Calling a0 promoter and ⟨a1, a2, ..⟩the structural genes gives the analogy to
operons in natural genetic systems. For example, if the initial organism is given
by Ψ (0) = ⟨a⟩and the genetic system is Π =

⟨a:ab⟩, ⟨a:cd⟩, ⟨b:adc⟩

, then the
organism grows as: Ψ (0) = ⟨a⟩, Ψ (1) = ⟨cdadc⟩, Ψ (2) = ⟨cdcdadcdc⟩, etc.
The general idea is that these operators are basic mechanisms which intro-
duce correlating eﬀects between phenotypic traits. [13] already claimed that the
essence of the operon is to introduce correlations between formerly independent
genes in order to adopt the functional dependence between the genes and their
phenotypic eﬀects and thereby increase the probability of successful variations.
The proposed model is very similar to the models by [8], [3], and [9], who
use grammar-encodings to represent neural networks. It is also comparable to
new approaches to evolve complex structures by means of so-called symbiotic
composition [6, 7, 18].
The crucial novelty in our model are 2nd-type mutations . These allow for ge-
netic variations that explore the neutral sets which are typical for any grammar-
like encoding. Without these neutral variations, self-adaptation of genetic rep-
resentations and of exploration distributions is not possible.
Consider the three genotypes given in the ﬁrst column,
genotype
phenotype phenotypic neighbors
Ψ (0) = ⟨a⟩, Π =

⟨a:bcbc⟩

⟨bcbc⟩
⟨*⟩, ⟨*cbc⟩, ⟨b*bc⟩, ⟨bc*c⟩, ⟨bcb*⟩
Ψ (0) = ⟨a⟩, Π =

⟨a:dd⟩, ⟨d:bc⟩

⟨bcbc⟩
⟨*⟩, ⟨*bc⟩, ⟨bc*⟩, ⟨*c*c⟩, ⟨b*b*⟩
Ψ (0) = ⟨bcbc⟩, Π =
 
⟨bcbc⟩
⟨*cbc⟩, ⟨b*bc⟩, ⟨bc*c⟩, ⟨bcb*⟩
All three genotypes have, after developing for at least two time steps, the same
phenotype Ψ (t) = ⟨bcbc⟩, t ≥2. The third genotype resembles what one would
call a direct encoding, where the phenotype is directly inherited as Ψ (0). Assume
that, during mutation, all symbols, except for the promoters, mutate with ﬁxed,
small probability. By considering all one-point mutations of the three genotypes,
we get the phenotypic neighbors of ⟨bcbc⟩as given in the third column of the
table, where a star * indicates the mutated random symbol. This shows that,
although all the three genotypes represent the same phenotype, they induce
completely diﬀerent phenotypic variabilities, i.e., completely diﬀerent phenotypic
search distributions. Note that, wherever a phenotypic neighbor comprises two
stars, these two phenotypic variations are completely correlated.
In order to enable a variability of genetic representations within such a neu-
tral set we need to allow for mutational transitions between phenotypically equiv-
alent genotypes. A transition from the 1st to the 3rd genotype requires a genetic
mutation that applies the operator ⟨a:bcbc⟩on the egg cell ⟨a⟩and deletes it
thereafter. Both, the application of an operator on some sequence (be it the egg
cell or another operator’s) and the deletion of operators will be mutations we
provide in our model. The transition from the 2nd to the 1st genotype is similar:
the 2nd operator ⟨d:ab⟩is applied on the sequence of the ﬁrst operator ⟨a:dd⟩and
deleted thereafter. But we must also account for the inverse of these transitions.
A transition from the 3rd genotype to the 1st is possible if a new operator is
created by randomly extracting a subsequence (here bcbc from the egg cell) and
encoding it in a new operator (here ⟨a:bcbc⟩). The original subsequence is then
replaced by the promoter. Similarly, a transition from the 1st to the 2nd geno-
89
Demonstrating the Evolution of Complex Genetic Representations

• First type mutations are ordinary symbol mutations that occur in every sequence
(i.e., promoter or rhs of an operator) of the genotype; namely symbol replacement,
symbol duplication, and symbol deletion, which occur with equal probabilities. The
mutation frequency for every sequence is Poisson distributed with the mean num-
ber of mutations given by (α · sequence-length), where α is a global mutation rate
parameter.
• The second type mutations aim at a neutral restructuring of the genetic system.
A 2nd-type mutation occurs by randomly choosing an operator π and a sequence p
from the genome, followed by one of the following operations:
– application of an operator π on a sequence p,
– inverse application of an operator π on a sequence p; this means that all matches
of the operator’s rhs sequence with subsequences of p are replaced in p by the
operator’s promoter,
– deletion of an operator π, only if it was never applied during ontogenesis,
– application of an operator π on all sequences of the genotype followed by dele-
tion of this operator,
– generation of a new operator ν by extracting a random subsequence of stochastic
length 2 + Poisson(1) from a sequence p and encoding it in an operator with
random promoter. The new operator ν is inserted in the genome behind the
sequence p, followed by the inverse application of ν on p.
All these mutations occur with equal probabilities. The total number of second
type mutations for a genotype is Poisson distributed with mean β. The second type
mutations are not necessarily neutral but they are neutral with suﬃcient probability
to enable an exploration of neutral sets.
• A genotype is mutated by ﬁrst applying second type mutations and thereafter ﬁrst
type mutations, the frequencies of which are given by β and α, respectively.
Table 1. The mutation operators in our model.
type occurs when the subsequence bc is extracted from the operator ⟨a:bcbc⟩
and encoded in a new operator ⟨d:bc⟩.
Basically, the mutations we will provide in our model are the generation of
new operators by extraction of subsequences (deﬂation), and the application
and deletion of existing operators (inﬂation). Technical details can be found in
table 1; the main point of these mutation operators though is not their details
but that they in principle enable a transition between phenotypically equivalent
representations in our encoding. We call these mutations 2nd-type mutations to
distinguish them from ordinary symbol mutation, deletion, and insertion.
3
Evolving Plants
The symbols for encoding plants. The sequences we are evolving are strings of
the alphabet {A,..,P} which are mapped on plant-describing strings according
to {A,..,I} →{F,+,-,&,^,\,/,[,]} and {J,..,P} →{.}. The meanings of the
symbols of the plant-describing strings are summarized in table 2. For example,
the sequence ⟨FF[+F][-F]⟩represents a plant for which the stem grows two units
upward before it branches in two arms, one to the right, the other to the left,
each of which has one unit length and a leave attached at the end.
Table 3 demonstrates the implications of this encoding. The plant on the left
is an examples taken from [12]. To the right of this “original” you ﬁnd three
90
M. Toussaint

F
attach a unit length stem
+,-,&
^,\,/
rotations of the “local coordinate system” (that apply on following attached
units): four rotation of δ degree in all four directions away from the stem
(+,-,&,^) and two rotations of ±δ degree around the axis of the stem (\,/).
[
a branching (instantiation of a new local coordinate system)
]
the end of that branch: attaches a leave and proceeds with the previous branch
(return to the previous local coordinate system); also if the very last symbol
of the total sequence is not ], another leave is attached
.
does nothing
Table 2. Description of the plant
b
grammars sym ols, cf. [11].
⟨F:FF-[-F+F+F]+[+F-F-F]⟩⟨F:FF-[+F+F+F]+[+F-F-F]⟩⟨F:FF-[-F-F+F]+[+F-F-F]⟩
Table 3. An example for a 2D plant structure and its phenotypic variabilit induced b
y
y
(0)
single
bol
in the
of them T = 4 δ
and Ψ
sym
mutations
genot
F
ype.
or all
= 20,
=⟨F⟩.
,
Belo
h
w eac
illustration, the single
duction rule that
pro
was mutated is given.
diﬀerent variations, each of which was produced by a single symbol mutation
in the genotype. Obviously, there are large-scale correlations in the phenotypic
variability induced by uncorrelated genetic mutations.
The ﬁtness function. Given a sequence we evaluate its ﬁtness by ﬁrst drawing
the corresponding 3D plant in a virtual environment (the OpenGL 3D graphics
environment). We chop oﬀeverything of the plant that is outside a bounding
cube of size b × b × b. Then we grab a bird’s view perspective of this plant and
measure the area of green leaves as observed from this perspective. The mea-
surement is also height dependent: the higher a leave (measured by OpenGL’s
depth buﬀer in logarithmic scale, where 0 corresponds to the cube’s ﬂoor and 1
to the cube’s ceiling), the more it contributes to the green area integral
L =

x∈bird’s view area

color of x = green

·

height at x
 dArea
b2
∈[0, 1] . (1)
This integral is the positive component of a plant’s ﬁtness. The negative compo-
nent is related to the number and “weight” of branch elements: To each element
i we associate a weight wi which is deﬁned recursively. The weight of a leave is
1; the total weight of a subtree is the sum of weights of all the elements of that
subtree; and the weight of a branch element i is 1 plus the total weight of the
subtree that is attached to this branch element. E.g., a branch that has a single
leave attached has weight 1+1 = 2, a branch that has two branches each with a
single leave attached has weight 1+(2+1)+(2+1) = 7, etc. The idea is that wi
91
Demonstrating the Evolution of Complex Genetic Representations

roughly reﬂects how “thick” i has to be in order to carry the attached weight.
The total weight of the whole tree,
W =

i
wi ,
gives the negative component of a plant’s ﬁtness. In our experiments we used
f = L −ϱ W
as the ﬁtness function, where the penalty factor ϱ was chosen ϱ ∈{10−6, 10−7}
in the diﬀerent experiments.
Details of the implementation. Evolving such plant structures already gets close
to the limits of today’s computers, both, with respect to memory and compu-
tation time. Hence, we use some additional techniques to improve eﬃciency:
First, we impose diﬀerent limits on the memory resource that a genotype and
phenotype is allowed to allocate, namely three: (1) The number of symbols in
a phenotype was limited to be lower or equal than a number Mmax. This holds
in particular during ontogenesis: If the application of an operator results in a
phenotype with too many symbols, then the operator is simply skipped. (2) The
number of operators in a genotype is limited to be ≤Rmax. If a mutation oper-
ator would lead to more chromosomes, this mutation operator is simply skipped
(no other mutation is made in place). (3) There is a soft limit on the number of
symbols in a single chromosome: A duplication mutation is skipped if the chro-
mosome already has length ≥Umax. The limit though does not eﬀect 2nd-type
mutations; an inﬂative mutation π · p may very well lead to chromosomes of
length greater than Umax.
Second, we adopt an elaborated technique of self-adaptation of the mutation
frequencies. We used the scheme similar to the self-adaptation of strategy pa-
rameters proposed by [2]. Every genome i additionally encodes two real numbers
αi and βi. Before any other mutations are made, they are mutated by
αi ←αi (S N(0, τ) + τ ′) ,
(2)
βi ←βi (S N(0, τ) + τ ′) ,
where S N(0, τ) is a random sample (independently drawn for αi and βi) from
the Gaussian distribution N(0, τ) with zero mean and standard deviation τ.
The parameter τ ′ allows to induce a pressure towards increasing mutation rates.
After this mutation, αi and βi determine the mutation frequencies of 1st- and
2nd-type mutations respectively.
The 1st trial. Let us discuss two of the trials made with diﬀerent parameters.
Table 4 summarizes the experimental setup. See ﬁgure 1. For the 1st trial, the
curves show some sudden changes at generation ∼4000 where the ﬁtness, the
number of phenotypic elements, the number of operators in the genomes, and the
total genome length explodes. Between generation ∼4000 and ∼5400, the most
signiﬁcant curve in the graph is the repeatedly decaying genome size. Indeed we
will ﬁnd that the genomes in this period are too large and mutationally unsta-
ble. The innovations extinct and genome size decays until at generation ∼5400
a comparably large number of phenotypic elements can be encoded by much
smaller genomes that comprise more operators. In table 5, the illustrations of
92
M. Toussaint

1st trial
2nd trial
δ, b
20,20
←
L-system angle δ, size of the bounding cube
T
1
←
stopping time of development
ϱ
10−7
10−6
factor of the weight term W in a plant’s ﬁtness f
α
.01
←
(initial) frequency of ﬁrst type mutations
β
.01
.3
(initial) frequency of second type mutations
τ (τ ′)
.5 (.1)
0 (0)
rate of self-adaptation of α and β
µ, λ
←
type of selection
no
←
crossover turned on?
⟨AAAFFFJ⟩
←
initialization of Ψ (0) of all genotypes in the ﬁrst popula-
tion (they have no operators)
A
{A,..,P}
←
symbol alphabet
λ
100
←
(oﬀspring) population size
µ
30
←
(selected) parent population size
Mmax
100 000
1 000 000 maximal number of symbols in a phenotype
Rmax
100
←
maximal number of operators allowed in one genotype
Umax
40
←
symbol duplication mutations are allowed only if a se-
quence has less than Umax symbols
“
”
Table 4. Parameter settings of the t
for
wo trials. The
←
means “same as
the
previous trial” and shows that only few parameters are changed from trial to trial.
the best individual in selected generations explain in more detail what happened.
For a long time this is not much until, in generation 4000, a couple of leaves turn
up at certain places of the phenotype. Then, very rapidly, more leaves pop up
until, in generation 4025, every phenotypic segment has a leave attached. This
is exactly what we call a correlated phenotypic adaptation and was enabled by
encoding all the segments that now carry a leave within one operator, namely
the A-operator. The resulting “long-arm-building-block” triggers a revolution in
phenotypic variability and leads to the large structures as illustrated for genera-
tion 4400 (3467 elements). However, these structures are not encoded eﬃciently,
the genome size is too large (512) and phenotypic variability becomes chaotic.
The species almost extinguishes until, in generation 5100, evolution ﬁnds a much
better structured genome to encode large phenotypes. The J-operator becomes
dominant and allows to encode 1479 phenotypic elements with a genome size of
217. This concept is further improved and evolves until, in generation 8000, a
genome of size 141 with 2 operators encodes a regularly structured phenotype
of 3652 elements.
The 2nd trial. For the 2nd trial we turned oﬀthe self-adaptation mechanism for
the mutation frequencies (based on the experience with previous trials we can
now estimate a good choice of α = .01 and β = .3 for the mutation frequen-
cies and ﬁx it) and increase the limit Mmax to maximally 1 000 000 elements
per phenotype. The severe change in the resulting structures is also due to the
increase of the weight penalty factor ϱ to 10−6—the ﬁnal structure of the 1st
trial has a weight of about .3 · 10−6 which would now lead to a crucial penalty.
The weight punishing factor ϱ enforces structures that are regularly branched
instead of long curling arms. Table 6 presents the results of the 2nd trial. Com-
paring the illustrations for generation 950 and 1010 we see that evolution very
quickly developed a fan-like structure that is attached at various places of the
93
Demonstrating the Evolution of Complex Genetic Representations

0
2000
4000
6000
8000
0
0.2
0.4
0.6
0.8
1
1.2
1st trial
fitness
#elements /10000
genome size /1000
*operator usage /5
0
500
1000
1500
2000
0
0.2
0.4
0.6
0.8
1
1.2
2nd trial
fitness
#elements /10000
genome size /1000
*operator usage /10
Fig. 1. The graphs display the curves of the ﬁtness, the number of phenotypic elements,
the genome size, and the operators usage of the best individual in all generations of the
trials. Note that every quantity has been rescaled to ﬁt in the range of the ordinate,
e.g., the number of phenotype elements has been divided by 10 000 as indicated by the
notation “#elements/10000”. The * for the operator usages indicates that the curve
has been smoothed by calculating the running average over an interval of about a
hundredth of the abscissae (80 and 20 in the respective graphs).
phenotype. The fans arise from an interplay of two operators: The N-operator
encodes the fan-like structures while the F-operator encodes the spokes of these
fans. Adaptation of these fans is a beautiful example for correlated exploration.
The N-operator encodes more and more spokes until the fan is complete in gen-
eration 1010, while the F-operator makes the spokes longer. Elongation proceeds
and results in the “hairy”, long-armed structures. Note that, in generation 1650,
one N- and two B-operators are redundant. Until generation 1900, leaves are
attached to each segment of the arms, similar to generation 4025 of the 1st trial.
At that time, the plant’s weight is already 105 099 and probably prohibits to
make the arms even longer (since weight would increase exponentially). Instead
a new concept develops: At the tip of each arm two leaves are now attached
instead of one and this quickly evolves until there are three leaves, in generation
1910, and eventually a complete fan of six leaves attached at the tip of each arm.
In generation 2100, a comparably short genome with 10 used operators encodes
a very dense phenotype structure of 9483 elements.
More trials, data, and source code can be found at the author’s home page.
4
Conclusions
Let us brieﬂy discuss whether similar result could have been produced with a
more conventional GA that uses, instead of our non-trivial genotype-phenotype
mapping, a direct encoding of sequences in {F,+,-,&,^,\,/,[,]} that describe
the plants. For example, setting β = 0 in our model corresponds to such a
94
M. Toussaint

3800: f=.0034 e=49 w=612 o=2 g=66/2
Π=⟨⟨N:NNAA⟩,⟨M:MMMPNNAA⟩⟩
4025: f=.0087 e=156 w=1813 o=1 g=114/1
Π=⟨⟨A:IMAJA⟩⟩
4400: f=.28 e=3467 w=92031 o=5 g=512/5
Π=⟨⟨K:KA⟩,⟨J:64⟩,⟨B:BF⟩,
⟨G:GIFJCIJAIJA⟩,⟨O:29⟩⟩
5100: f=.20 e=1479 w=57134 o=2 g=217/2
Π=⟨⟨K:⟩,⟨J:77⟩⟩
5400: f=.22 e=1410 w=59219 o=3 g=159/3
Π=⟨⟨I:AJ⟩,⟨C:JJCJ⟩,⟨J:64⟩⟩
8000: f=.31 e=3652 w=379288 o=2 g=141/2
Π=⟨⟨I:39⟩,⟨J:60⟩⟩
Table 5. The 1st trial. The illustrations displa
he
y t
phenot
at selected
ypes
generations.
The two squared pictures in the lower right corner of each illustration display exactly
the bird’s view perspective that is used to calculate the ﬁtness: The lower (colored)
picture displa
the
ys
plant as seen from above and determines which area enters the
i
green area ntegral in equation (1), and the
icture
upper gray-scale p
displa
the
ys
height
value of each element which enters the same equation (where white and black refer to
height 0 and 1,
ectiv
resp
ely). Below each illustration
data
you ﬁnd some
corresponding
to this phenotype: ⟨generation⟩: f
ﬁtness
=⟨
⟩e=⟨number of elements⟩w=⟨plant’s total
wei ht
of used
g
⟩o=⟨number
o
size
perators⟩g=⟨genome
⟩/⟨number of
the
operators in
(0)
The
ed.
Ψ
is
genome⟩
genetic system Π is also display
(
generally too large to be
.
displayed here.) For some operators, the size of the rhs is given instead of the sequence.
See the text for a discussion of this evolution.
GA since no operators will be created and the evolution takes places solely
on the “egg cell” Ψ (0), which is equal to the ﬁnal phenotype in the absence
of operators. We do not need to present the results of such a trial—not much
happens. The obvious reason is the unsolvable dilemma of long sequences in
a direct encoding: On the one hand, mutability must be small such that long
sequences can be represented stably below the error threshold of reproduction;
on the other hand mutability should not vanish in order to allow evolutionary
progress. This dilemma becomes predominant when trying to evolve sequences
of length ∼104, as it is the case for the plants evolved in the 2nd trial. Also
elaborate methods of self-adaptation of the mutation rate cannot circumvent
95
Demonstrating the Evolution of Complex Genetic Representations

950: f=.018 e=144 w=2681 o=3 g=163/3
Π=⟨⟨N:IP⟩,⟨P:JF⟩,⟨F:IIAJJF⟩⟩
1010: f=.032 e=506 w=10250 o=2 g=211/2
Π=⟨⟨N:NIJFFFFFFFFF⟩,
⟨F:IIAAJJF⟩⟩
1650: f=.052 e=1434 w=31476 o=5 g=180/8
Π=⟨⟨B:IJNN⟩,⟨N:IAAJFIAAJF⟩,
⟨N:4⟩,⟨B:3⟩,⟨B:9⟩,⟨F:IINFNNKK
KCBJF⟩,⟨B:IJNNAJ⟩,⟨N:IJBA⟩⟩
1900: f=.17 e=4915 w=105099 o=10 g=230/12
Π=⟨⟨B:NN⟩,⟨N:IABFIAAJF⟩,⟨N:
33⟩,⟨J:MOFJ⟩,⟨D:KNME⟩,⟨B:BB
BBJ⟩,⟨B:FB⟩,⟨B:MNNLDDM⟩,⟨F:
32⟩,⟨B:28⟩,⟨N:NLA⟩,⟨L:IJ⟩⟩
1910: f=.20 e=4340 w=89996 o=12 g=226/14
Π=⟨⟨B:NN⟩,⟨N:IABFIAAJF⟩,⟨N:
33⟩,⟨J:MMJ⟩,⟨J:NJ⟩,⟨D:KNME⟩,
⟨B:BBBBJ⟩,⟨B:FB⟩,⟨B:MNNLD
DM⟩,⟨F:30⟩,⟨B:18⟩,⟨C:KCKKC
LK⟩,⟨N:NLA⟩,⟨L:IJ⟩⟩
2100: f=.33 e=9483 w=192235 o=10 g=261/15
Π=⟨⟨B:NN⟩,⟨N:IAABFIAAJF⟩,
⟨N:57⟩,⟨J:B⟩,⟨J:JJ⟩,⟨D:DOIE⟩,
⟨B:FBFBFBFBHJ⟩,⟨B:ENNDD⟩,
⟨F:36⟩,⟨B:25⟩,⟨B:6⟩,⟨C:CCGLB⟩,
⟨B:CLK⟩,⟨N:NNLA⟩,⟨L:IJ⟩⟩
Table 6. The 2nd trial. Please see the caption of table 5 for explanations.
this problem completely; the only way to solve the dilemma is to allow for
an adaptation of genetic representations. The key novelty in our model that
enabled the adaptation of genetic representations are the 2nd-type mutations
we introduced.
In our example, two important features of the genetic representations co-
incide. First, this is the capability to ﬁnd compact representations that allow
to encode large phenotypes with small genotypes solving the error threshold
dilemma. Second, this is the ability for complex adaptation, i.e., to induce highly
structured search distributions that incorporate large-scale correlations between
phenotypic traits. For example, the variability of one leave is, in certain rep-
resentations, not independent of the variability of another leave. A GA with
direct encoding would have to optimize each single phenotypic element by itself,
step by step. The advantage of correlated exploration is that many phenotypic
elements can be adapted simultaneously in dependence of each other.
96
M. Toussaint

Our experiments demonstrated the theory of σ-evolution which mainly states
that the evolution of genetic representations is guided by a fundamental princi-
ple: they evolve such that the match between the evolutionary search distribution
and the distribution of good solutions becomes better. The way genetic systems
are organized is a mirror of what evolution has learned about the problem.
References
1. L. Altenberg. Genome growth and the evolution of the genotype-phenotype map.
In W. Banzhaf and F. H. Eeckman, editors, Evolution and Biocomputation: Com-
putational Models of Evolution, pages 205–259. Springer, Berlin, 1995.
2. T. B¨ack.
Evolutionary Algorithms in Theory and Practice.
Oxford University
Press, 1996.
3. F. Gruau. Automatic deﬁnition of modular neural networks. Adaptive Behaviour,
3:151–183, 1995.
4. T. F. Hansen and G. P. Wagner. Epistasis and the mutation load: A measurement-
theoretical approach. Genetics, 158:477–485, 2001.
5. T. F. Hansen and G. P. Wagner.
Modeling genetic architecture: A multilinear
model of gene interaction. Theoretical Population Biology, 59:61–86, 2001.
6. G. S. Hornby and J. B. Pollack. The advantages of generative grammatical en-
codings for physical design. In Proceedings of the 2001 Congress on Evolutionary
Computation (CEC 2001), pages 600–607. IEEE Press, 2001.
7. G. S. Hornby and J. B. Pollack. Evolving L-systems to generate virtual creatures.
Computers and Graphics, 25:1041–1048, 2001.
8. H. Kitano. Designing neural networks using genetic algorithms with graph gener-
ation systems. Complex Systems, 4:461–476, 1990.
9. S. Lucas. Growing adaptive neural networks with graph grammars. In Proc. of
European Symp. on Artiﬁcial Neural Netw. (ESANN 1995), pages 235–240, 1995.
10. M. Pelikan, D. E. Goldberg, and F. Lobo. A survey of optimization by building
and using probabilistic models. Technical Report IlliGAL-99018, Illinois Genetic
Algorithms Laboratory, 1999.
11. P. Prusinkiewicz and J. Hanan. Lindenmayer Systems, Fractals, and Plants, vol-
ume 79 of Lecture Notes in Biomathematics. Springer, New York, 1989.
12. P. Prusinkiewicz and A. Lindenmayer. The Algorithmic Beauty of Plants. Springer,
New York, 1990.
13. R. Riedl. A systems-analytical approach to macro-evolutionary phenomena. Quar-
terly Review of Biology, 52:351–370, 1977.
14. M. Toussaint.
On the evolution of phenotypic exploration distributions.
In
C. Cotta, K. De Jong, R. Poli, and J. Rowe, editors, Foundations of Genetic Al-
gorithms 7 (FOGA VII). Morgan Kaufmann, 2003. In press.
15. G. P. Wagner and L. Altenberg. Complex adaptations and the evolution of evolv-
ability. Evolution, 50:967–976, 1996.
16. G. P. Wagner, G. Booth, and H. Bagheri-Chaichian. A population genetic theory
of canalization. Evolution, 51:329–347, 1997.
17. G. P. Wagner, M. D. Laubichler, and H. Bagheri-Chaichian. Genetic measurement
theory of epistatic eﬀects. Genetica, 102/103:569–580, 1998.
18. R. Watson and J. Pollack. A computational model of symbiotic composition in
evolutionary transitions. Biosystems, Special Issue on Evolvability, 2002.
97
Demonstrating the Evolution of Complex Genetic Representations

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 98–109, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Sexual Selection of Co-operation
M. Afzal Upal
Opal Rock Technologies
42741 Center St, Chantilly, VA
afzal@opalrock.com
Abstract. Advocates of sexual selection theory have argued that various male
traits, such as male co-operative behavior towards females, can evolve through
female preference for mating with those males who possess that trait. This paper
reports on the results of a simulation performed to test the hypothesis that
female preference for mating with co-operative males can lead to an increase in
the proportions of males in a population who co-operate with females.  We
simply model the sex differences using a single variable measuring the cost of
reproduction. Our results show that even in such a simple environment there are
a large number of interacting variables, which complicate the relationship
between the sexual selection of co-operative males by females and the
proportion of males actually co-operating with females. In fact, in most
situations we modeled, sexual selection of co-operative males by females ended
up causing the proportion of females that co-operate with males to increase
while the proportion of males co-operating with females showed no significant
increase over the random selection experiments.
1   Introduction and Background
Co-operation is a fundamental part of the repertoire of animal behavior. It has been
extensively documented among various social animals such as primates. Co-operative
behavior has been observed between members of a group who defend themselves
against predators (Corning 1998), between pair-bonded males and females, between
parents and children, between infants and non-parent adults (Brown 1970), between
challengers to the authority of the dominant male, and between second and third
ranking males and the harem females (de Waal 1982, Noe 1992). Evolutionary theo-
rists have identified a number of mechanisms that can confer evolutionary advantages
to co-operating individuals in certain situations. However, none of these mechanisms
can satisfactorily explain the prevalence of co-operation among a variety of animals in
a variety of situations.
Theorists study this problem by using an abstract model known as the Prisoner’s Di-
lemma (Hamilton 1964).  Prisoner’s Dilemma models a world in which two agents
that have to decide whether to co-operate or defect without knowing about the other
agent’s decision.  If both agents decide to co-operate then they get a reward R, if both

Sexual Selection of Co-operation         99
decide to defect then both get the mutual defection punishment P, if one decides to
co-operate while the other defects then the defector gets the defection reward D while
the co-operator gets the sucker’s payoff S. When R > average(D, S) (e.g., as in Table
1) a game-theoretic analysis reveals that defection is the best strategy if the two play-
ers never have to play each other again. However, if the agents repeatedly play against
each other, different results emerge. Robert Axelrod (1984) held two tournaments to
find out the winning strategy in a repeated Prisoner’s Dilemma game. He invited oth-
ers to submit strategies for playing the game and ran the submitted strategies against
one another in a round robin fashion to find out which strategy can earn the largest
number of points. A strategy called Tit-For-Tat (TFT) turned out to be better than all
other strategies including some selfish-looking strategies. TFT strategists start out co-
operating and then do what the other player did on the previous move (Axelrod 1984).
Table 1.  Common values used for the payoff in the Prisoner’s  Dilemma. The matrix entries list
the player’s payoffs as row column.
However, this does not explain how co-operation evolves in the first place.  To answer
this question, Axelrod and Hamilton (1987) set up a simulation in which they ran-
domly generated 100 strategies, allowed them to play against one another.  At the end
of the playing phase, the relatively more successful strategies were allowed to repro-
duce while the less successful ones were not. The strategies of the offsprings were
generated by crossing-over the strategies of the parents and through random mutation.
Axelrod and Hamilton (1987) report that after a significant number of generations, co-
operative strategies such as TFT come to dominate. However, subsequent detailed
analysis of repeated Prisoner’s Dilemma has shown that neither TFT nor any other
pure or mixed strategy is evolutionarily stable (Nowak et al. 1995). Which strategy
emerges as winner depends on the prevalence of the competing strategies in the popu-
lation.  This has lead researchers to consider other factors that can enhance the evolu-
tion of co-operative behavior. Key and Aiello (2000) hypothesized that the differences
in male and reproductive cost may be responsible for evolution of co-operation in a
mixed sex environment.
Trivers (1971) defines reproductive cost as the cost of reproduction measured to the
extent that it detracts from an individual's ability to invest in future offspring. It con-
sists of the parental investment and the mating costs. Parental investment measures the
cost of parent's behavior that directly increases their offspring's reproductive success.
For female mammals this includes the high cost of gestation, lactation, and rearing of
the offspring. Mating costs are also significantly higher for females (because of the
relatively higher costs of producing an egg, and not being able to reproduce while
carrying a fetus to term). However, males spend more energy on maintaining their
relatively larger bodies and on acquiring mates to reproduce with.
Decision
Co-operate
Defect
Co-operate
R=3, R=3
S=0, D=5
Defect
D=5, S=0
D=1, D=1

100         M.A. Upal
Key and Aiello (2000) modified Axelrod and Hamilton’s simulation set up so that the
agents were divided into two groups; males and females. The only difference between
a male and a female agent was their cost of reproduction. The sex of each one of the
650 players was selected randomly and in most trials roughly half of the agents were
males and the other half females. Similar to Axelrod and Hamilton's simulation, initial
strategies of the players were randomly selected. A number of rounds of prisoner's
dilemma games were played. In each round, two random players (regardless of their
sex) were chosen to play prisoner's dilemma game a fixed number of times. However,
only relatively successful players of opposite sex were allowed to mate to reproduce
off springs. Key and Aiello (2000) ran their simulation by varying the male reproduc-
tive cost (MRC) from 1 to 600 and keeping the female reproductive cost (FRC) fixed
at 1000.  They concluded that at low costs of reproduction, males co-operate more
with females than females do with them.
However, Key and Aiello's agents were not able to choose their mates. Mate selection
is known to be significant factor affecting the evolution of various male traits. In a
number of bird species, females are known to select males by visiting them at their
special gathering places (known as leks) where males gather to show off their capa-
bilities.  Sexual selection of males by females has been used to explain various male
features such as a male peacock's tail, and even speciation itself (Darwin 1900). Re-
cently, some researchers (Miller 2000, Tallamy 2000) have argued that co-operation
among males may also have evolved through sexual selection by females. This paper
reports on a simulation based study we performed to test the hypothesis that female
preference for mating with co-operative males can enhance the evolution of male co-
operative behavior.
2   Experimental Setup
Our model involved building a heterosexual population of 100 agents. Each agent was
modeled as a Java object with the attributes of sex, reproductive cost, and four 21-bit
binary strings encoding the game playing strategy of the player. As shown in Fig.1, the
first 21-bits of the 84-bit strategy string are used to encode the strategy that is to be
used when a male is playing against another male. The second part when a male is
playing against a female, the third when a female is playing against a male and the
final part to cover the situations in which a female is playing against another female.
If the first bit of the 21-bit string is 1, the player co-operates with its opponent on the
first move and defects otherwise. On the second move, the player uses the knowledge
of its own last move and its opponent’s last move to decide what to do.  Since, there
are four possibilities (both co-operated, both defected, it co-operated and the opponent
defected, it defected and the opponent co-operated) the second move strategy requires
4-bits to encode.  Similarly, the player uses the knowledge of its own last two moves
and its opponent’s last two move to decide what to do on the third and the following
moves.  This means that 16-bits are needed to encode the strategy for the third and the
following moves. As shown by Ikegami (1994), this two-step history is all that an
agent needs to encode in order to learn a strategy.

Sexual Selection of Co-operation         101
Fig. 1. 84-bit strategy string encoding a player’s strategy for playing Prisoner’s Dilemma game
in a mixed sex environment.
The sex of an agent was randomly chosen to be male or female. Game playing strate-
gies of the first generation of agents were also randomly generated. Next, two players
were randomly selected to play 100 rounds of Prisoner’s Dilemma game. After 150
game playing rounds, relatively successful players of the opposite sex were allowed to
mate and reproduce children. Strategies of the off springs were produced by crossing
over the strategies of their parents and through random mutation as shown in Fig. 2.
The chances of a strategy gene randomly mutating from 0 to 1 or vice versa were 1 in
5000. Similar to Key and Aiello’s experiments, the only difference between a male
and female agent was the cost of reproduction. We ran the experiment by varying
MRC from 1 to 1000 and keeping FRC fixed at 1000. The metrics we measured were
the proportion of males and females receiving co-operation and defection from players
of the same and opposite sex.
Similar to Key and Aiello (2000), interactions of a the players of a sex G with a player
A were deemed co-operative if the average number of points per game gained by
player A from players of sex G (i.e., sum of the points obtained by A from players of
sex G divided by the number of Prisoner’s Dilemma games A played with players of
sex G) exceeded 2.25.  If A obtained less than 2.75 points per game from players of
sex G, the players of sex G were considered to have been weakly co-operative to A.  If
A on the other hand, fared less than 1.75 points per game with players of sex G, its
interactions with players of sex G, were considered to have been dominated by defec-
tions.  Those managing between 1.75 and 2.25 points per game from players of sex G
were considered to have received weak defections from players of that sex.  During
each experiment, we computed the proportion of males and females who received co-
operation/weak-co-operation/defection/weak-defection from players of the same and
the opposite sex.  The experiment was run 20 times and the average proportions cal-
culated.
We performed this experiment with two mate selection strategies; random selection
and selection of most co-operative male by females. In the first experiment, females
randomly selected a male agent to mate with. In the second experiment, a female
preferred to mate with the male player who had been the most co-operative to her
during their game playing round.  If the most co-operative player was not able to mate
(because of not having accumulated enough points to mate), the next most co-
0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0
MaleMale
MaleFem ale
Fem aleMale
Fem aleFem ale
1st
move
2nd move
Moves after the 2nd move

102         M.A. Upal
operative player was chosen. The purpose of conducting these two experiments was to
test the hypothesis that males co-operate significantly more with females when fe-
males sexually select males for co-operation. We performed t-tests to see if the differ-
ences between the mean proportions obtained from the two experiments indicated two
different distributions and if the mean proportion of the females receiving co-operation
from males in the sexual selection experiment was significantly (with 0.05 being the
level of significance) larger than the mean proportion of females receiving co-
operation from males in the random selection experiment.
Fig. 2. (a) A random partition point P between 1 and the length of the strategy string N is se-
lected and the first P bits are copied from the first string while the remaining N-P bits are cop-
ied from the second bit-string to create the cross-over bit-string. (b) A random bit location L is
selected.  Another random number R between 1 and 5000 is generated. If R equals 2500 the
value of the Lth bit is flipped from 1 to 0 or vice versa.
3   Results and Analysis
Fig. 3 and Fig. 4 show the mean proportion of males who receive co-operation/weak-
co-operation/weak-defection/defection from other males and females under the ran-
dom selection and sexual selection conditions for various values of MRC. Fig. 5 and
Fig. 6 show the mean proportion of females who receive co-operation/weak-co-
operation/weak-defection/defection from males and from other females.  The results
show that in the random selection experiments, males and females co-operate more
with members of the opposite sex than they do with members of the same sex. Simi-
larly, when females randomly select mates, they receive more co-operation from
males than they offer in turn (especially at low values of MRC) i.e., at low values of
MRC males can afford to be suckers.  This confirms the results obtained by Key and
Aiello (2000).
0 0 1 0 1 0 0 1 0 0 0
1 1 1 0 0 0 0 1 0 1 1 1
0
0 0 1 0 1 0 0 1 0 1 1 1
Cross-over
M utation
0 0 1 0 1 0 1 1 0 1 1 1
(a)
(b)

Sexual Selection of Co-operation         103
Fig. 3. The mean proportion of males in the 100
th generation who co-operate/weakly co-
operate/defect/weakly-defect with other males  in random and sexual selection experiments for
various values of MRC.
Comparing the sexual selection experiment results with random selection results
shows that sexual selection of co-operative males by females results in an increase of
co-operation between members of the same sex for most values of MRC.  However,
co-operation between members of opposite sex increases under some conditions while
it decreases under different conditions. At very small values of MRC (MRC < 200)
proportion of females receiving co-operation from males increases as predicted by the
null hypothesis but then it declines as MRC increases further.  We performed t-tests to
see if the increase in male co-operation with females was statistically significant. The
graph shown in Fig. 7 plots the t-test values of the difference in the proportion of
males co-operating with females between the sexual selection and the random selec-
tion populations.  The t-test values initially increase and then drop but the increases
are not statistically significant at any point.  These results contradict our null hypothe-
sis that the sexual selection by females of the most co-operative mate leads to a sig-
nificant increase in the proportion of females receiving co-operation from males.
MRC=1
Co-operation
Weak co-operation
Weak Defection
Defection
MRC=100
MRC=200
MRC=300
MRC=400
MRC=500
MRC=600
MRC=700
MRC=800
MRC=900
MRC=1000
Random Selection
Sexual Selection 
Random Selection
Sexual Selection

104         M.A. Upal
The main reason why the proportion of males co-operating with females does not
increase (especially at large values of MRC where it actually decreases) is that the
male population in the sexual selection environment faces two conflicting selection
pressures. Besides the selection pressure exerted by female selection favoring co-
operative males, value of male reproductive cost (MRC) also exerts a selection pres-
sure by allowing only those males to mate and pass their genes if they have collected
enough points during their game playing phase. The reproductive cost pressure selects
for those males who have more competitive strategies and are better point collectors.
The two selection pressures would pull in the same direction if co-operation with
females also improved a male player’s point collection. This would happen if all fe-
males adopted reciprocal co-operation strategies (such as tit-for-tat) in which female
co-operation with males depended on male co-operation with them.
Fig. 4. The mean proportion of males in the 100
th generation who co-operate/weakly co-
operate/defect/weakly-defect with females  in random and sexual selection experiments for
various values of MRC.
Increase in co-operation by males in response to other simple female strategies such as
always co-operate or always defect would decrease a male’s point collection and
hence the two selection pressures will pull in the opposite directions. When the two
MRC=1
Random Selection
Sexual Selection 
Random Selection
Sexual Selection
Co-operation
Weak co-operation
Weak Defection
Defection
MRC=100
MRC=200
MRC=300
MRC=400
MRC=500
MRC=600
MRC=700
MRC=800
MRC=900
MRC=1000

Sexual Selection of Co-operation         105
Fig. 5. The mean proportion of females in the 100
th generation who co-operate/weakly co-
operate/defect/weakly-defect with males  in random and sexual selection experiments for vari-
ous values of MRC.
pressures select for different strategies more complicated interactions ensue. However,
even when both selection pressures favor more co-operative male strategies,males
may never evolve to adopt the strategy of “always co-operate with females”. This is
because such strategies are not evolutionarily stable. If males become mostly co-
operative towards females, females would learn the strategy of always defect because
that allows them to collect more points.  Females have a stronger selection pressure on
them to be competitive because of a higher reproductive cost in most cases. Faced
with the “always defect with males” strategy, reproductive pressure on males will
favor “always defect with females” strategy which runs counter to the co-operative
strategies favored by sexual selection pressure. When male reproductive cost is low,
male co-operation with females will arise because the sexual selection pressure domi-
nates the reproductive selection pressure.  This is what happens as the graph in Fig. 7
illustrates. However, as the male reproductive cost increases, co-operative males de-
spite being favored by the females are simply not able to collect enough points to
enable them to mate and propagate their genes.
MRC=1
Random Selection
Sexual Selection 
Random Selection
Sexual Selection
Co-operation
Weak co-operation
Weak Defection
Defection
MRC=100
MRC=200
MRC=300
MRC=400
MRC=500
MRC=600
MRC=700
MRC=800
MRC=900
MRC=1000

106         M.A. Upal
Fig. 6. The mean proportion of females in the 100
th generation who co-operate/weakly co-
operate/defect/weakly-defect with other females  in random and sexual selection experiments
for various values of MRC.
There is another solution to the male point collection problem namely to collect more
points from other males to compensate for the loss of points that males incur by be-
coming more co-operative with females. As Fig. 3 shows, this is indeed what happens;
male-male co-operation increases at most points. Female-female co-operation also
increases at most points as shown in Fig. 6 because females also need to compensate
for the loss of points resulting from a drop in male co-operation. However, regardless
of how competitive females are against other females, they cannot obtain all their
points from females simply because players cannot select the sex of their opponent
during the game playing phase (which is done randomly). The reproductive cost se-
lection pressure assures that females must be good at collecting point from both males
as well as females. Even though males become less co-operative in the sexual
selection population than they were in the random selection population, they are still
more co-operative to females than females are to other females.
MRC=1
Random Selection
Sexual Selection 
Random Selection
Sexual Selection
Co-operation
Weak co-operation
Weak Defection
Defection
MRC=100
MRC=200
MRC=300
MRC=400
MRC=500
MRC=600
MRC=700
MRC=800
MRC=900
MRC=1000

Sexual Selection of Co-operation         107
Fig. 7. Student t-test values for the difference between the mean proportion of males co-
operating with females between the sexual selection and the random selection population plotted
against the value of MRC varied from 1 to 1000.
As male strategies become more competitive, they evolve from being mostly co-
operative towards females to more complicated strategies such as reciprocal co-
operation strategies that co-operate with only those females who co-operate with them.
This means that female strategies of mostly defection gather less points. This means
that females must co-operate more with males in order to gain points from them.  This
is what happens as shown in Fig. 5. Initially, at very small values of MRC (MRC <
200) as the male co-operation with females increases (as discussed earlier) female co-
operation with them declines. However, at larger values of MRC, as male strategies
become more competitive, female co-operation with males increases. We performed t-
tests to see if the increase in female co-operation with males in the sexual selection
population over the random selection population was statistically significant. As Fig. 8
shows, the t-test values are above the 0.05 threshold at most points (especially at
points with larger values of MRC). This shows that there is an emergent selection
pressure on the female population favoring those females who are more co-operative
to males even though no such selection pressure were explicitly programmed (nor
expected when we began this study).
4   Conclusion
Advocates of sexual selection theory have argued that various male traits such as male
co-operative behavior towards females can evolve through female preference for
males who co-operate with them. This paper presents the results of simulation experi-
t-test 
values
MRC
-3
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1
100
200
300
400
500
600
700
800
900
1000

108         M.A. Upal
Fig. 8. Student t-test values for the difference between the two population means plotted against
the increasing value of MRC from 1 to 1000.
ment we performed to test this hypothesis in a simple population of male and female
agents only distinguished by their reproductive costs. Our results show that sexual
selection of co-operative males by females does not lead to a significantly larger pro-
portion of males co-operating with females. What we found instead was that it is the
proportions of females co-operating with males that ends up increasing. It remains to
be seen if these results will hold up in other environments such as n-person Prisoner’s
Dilemma and the game of Chicken (Nowak 1995).
References
1. 
Axelrod, R. (1984) The Evolution of Co-operation, Basic Books, New York.
2. 
Axelrod, R. and Hamilton, W. (1987) “The Evolution of Co-operation”, In Axelrod R. The
Complexity of Co-operation, Princeton University Press, Princeton.
3. 
Brown, J. L. (1970) “Cooperative Breeding and Altruistic Behaviour in the Mexican Jay”.
Animal Behaviour, vol. 18, pp. 366–378. 1970.
4. 
Corning, P. (1998) “The Cooperative Gene: On the Role of Synergy in Evolution”. Evolu-
tionary Theory, vol. 11,  pp. 183–207. 1998.
5. 
Darwin, C. (1900) The Decent of Man and Selection in Relation to Sex, Werner Company,
Akron, Ohio.
6. 
De Waal, F. (1982) Chimpanzee Politics: Power and Sex Among Apes, John Hopkins
University Press, Baltimore MD.
7. 
Dugatkin, L. (1999) Cheating Monkeys and Citizen Bees: The Nature of Cooperation in
Animals and Humans. New York: Free Press.
t-test 
values
MRC
1
100
200
300
400
500
600
700
800
900
1000
-2
-1
0
1
2
3

Sexual Selection of Co-operation         109
8. 
Hamilton, W. (1964) The Genetic Evolution of Social Behaviour, Journal of Theoretical
Biology, vol. 7, pp. 1–52.
9. 
Ikegami, T. “From Genetic Evolution to Emergence of Game Strategies”, Physica D, vol.
75, pp.310--327. 1994
10. Key, C. and Aiello, L. (2000), A Prisoner’s Dilemma Model of the Evolution of Paternal
Care, Folia Primatologica, vol. 71, pp. 77–92.
11. Miller, (2000), The Mating Mind: How Sexual Choice Shaped the Evolution of Human
Nature, Doublesday, London.
12. Noe, R. “Alliance Formation Among Male Baboons”, In Harcourt A. & de Waal F. Coali-
tions and Alliances in Human and Other Animals, Oxford University Press, 1992, pp. 285–
321.
13. Nowak, M., May, R., and SIGMUND, K. (1995), The Arithmetics of Mutual Help, Scien-
tific American, June 1995, pp. 50–55.
14. Tallamy, D. (2000) Sexual Selection and the Evolution of Exclusive Paternal Care in
Arthropods, Animal Behaviour, vol. 60, pp. 559–567.
15. Trivers, R. (1972) “Parental Investment and Sexual Selection”. In Sexual Selection and the
Decent of Man, London, Heinemann, 1972, pp. 136–179.

Optimization Using Particle Swarms with Near Neighbor
Interactions
Kalyan Veeramachaneni, Thanmaya Peram, Chilukuri Mohan, 
 and Lisa Ann Osadciw
Department of Electrical Engineering and Computer Science
Syracuse University
Syracuse, NY 13244-1240
(315)443-3366(office)/(315)443-2583(fax)
kveerama,tperam,mohan,laosadci@ecs.syr.edu
Abstract. This paper presents a modification of the particle swarm optimization
algorithm (PSO) intended to combat the problem of premature convergence
observed in many applications of PSO. In the new algorithm, each particle is
attracted towards the best previous positions visited by its neighbors, in addition to
the other aspects of particle dynamics in PSO. This is accomplished by using the
ratio of the relative fitness and the distance of other particles to determine the direc-
tion in which each component of the particle position needs to be changed. The
resulting algorithm, known as Fitness-Distance-Ratio based PSO (FDR-PSO), is
shown to perform significantly better than the original PSO algorithm and several
of its variants, on many different benchmark optimization problems. Avoiding pre-
mature convergence allows FDR-PSO to continue search for global optima in diffi-
cult multimodal optimization problems, reaching better solutions than PSO and
several of its variants.
1  Introduction
The Particle Swarm Optimization algorithm (PSO), originally introduced in terms of
social and cognitive behavior by Kennedy and Eberhart in 1995 [1], [2], has proven to be
a powerful competitor to other evolutionary algorithms such as genetic algorithms [3].
The PSO algorithm simulates social behavior among individuals (particles) “flying”
through a multidimensional search space, each particle representing a single intersection
of all search dimensions[7]. The particles evaluate their positions relative to a goal (fit-
ness) at every iteration, and particles in a local neighborhood share memories of their
“best” positions, then use those memories to adjust their own velocities and positions as
shown in equations (1) and (2) below. The PSO formulae define each particle as a poten-
tial solution to a problem in a D-dimensional space, with the ith particle represented as
, .. ... .. 
. Each particle also remembers its previous best position,
designated 
as
pbest, 
,.....
 
and 
its 
velocity
,..........
 [7]. In each generation, the velocity of each particle is
updated, being pulled in the direction of its own previous best position (pi) and the best of
all positions (pg) reached by all particles until the preceding generation.
Xi
xi1 xi2 xi3
,
,
(
=
xiD)
Pi
pi1 pi2 pi3
,
,
(
=
piD)
Vi
vi1 vi2 vi3
,
,
(
=
viD)
E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 110−121, 2003.
 Springer-Verlag Berlin Heidelberg 2003

The original PSO formulae developed by Kennedy and Eberhart were modified by Shi
and Eberhart [4] with the introduction of an inertia parameter,
, that was shown empir-
ically to improve the overall performance of PSO.
(1)
(2)
Several interesting variations of the PSO algorithm have recently been proposed by
researchers in [12], [13], [14], [15], [16], [17]. Many of these PSO improvements are
essentially extrinsic to the particle dynamics at the heart of the PSO algorithm and can be
applied to augment the new algorithm presented in this paper. By contrast to most other
PSO variations, this paper proposes a significant modification to the dynamics of parti-
cles in PSO, moving each particle towards other nearby particles with a more successful
search history, instead of just the best position discovered so far. This is in addition to the
terms in the original PSO update equations.
Section 2 motivates and describes the new Fitness- Distance-Ratio based PSO (FDR-
PSO) algorithm. Section 3 defines the benchmark continuous optimization problems used
for experimental comparison of the algorithms, and the experimental settings for each
algorithm. Section 4 presents and discusses the results. Conclusions and future work are
presented in Section 5.
2  FDR-PSO Algorithm
Theoretical results [10][13] have shown that the particle positions in PSO oscillate in
damped sinusoidal waves until they converge to points in between their previous best
positions and the global best positions discovered by all particles so far. If some point vis-
ited by a particle during this oscillation has better fitness than its previous best position
(as is very likely to happen in many fitness landscapes), then particle movement contin-
ues, generally converging to the global best position discovered so far. All particles fol-
low the same behavior, quickly converging to a good local optimum of the problem.
However, if the global optimum for the problem does not lie on a path between original
particle positions and such a local optimum, then this convergence behavior prevents
effective search for the global optimum. It may be argued that many of the particles are
wasting computational effort in seeking to move in the same direction (towards the local
optimum already discovered), whereas better results may be obtained if various particles
explore other possible search directions. This paper explores an alternative in which each
particle is influenced by several other particles, not just moving towards or away from the
best position discovered so far.
The most logical choices, for deciding which other particles ought to influence a given
particle, are drawn from natural observations and expectations of animal behavior: 
1. An organism is most likely to be influenced by others in its neighborhood.
ω
Vid
t(
1)
+
ω
Vid
×
t )
(
ψ1
pid
Xid
t( )
–
(
)
×
ψ2
pgd
Xid
t( )
–
(
)
×
+
+
=
Xid
t
1
+
(
)
Xid
t( )
Vid
t(
1)
+
+
=
111
Optimization Using Particle Swarms with Near Neighbor Interactions

2. Among the neighbors, those that have been more successful ( than itself) are likely to
affect its behavior.
 Attempting to introduce the effects of multiple other (neighboring) particles on each
particle must face the possibility of crosstalk effects encountered in neural network learn-
ing algorithms. In other words, the pulls experienced in the directions of multiple other
particles may mostly cancel each other, reducing the possible benefit of all the associated
computations. To counteract this possibility, the FDR-PSO algorithm selects only one
other particle when updating each velocity dimension, which is chosen to satisfy two cri-
teria:
 1. It must be near the particle being updated.
 2. It should have visited a position of higher fitness.
Experiments have been conducted with several possible ways of selecting particles that
satisfy these criteria, without significant difference in the performance of the resulting
algorithm. The simplest and most robust variation was to update each velocity dimension
by selecting a particle that maximizes the ratio of the fitness difference to the one-dimen-
sional distance. In other words, the dth dimension of the ith particle’s velocity is updated
using a particle called the nbest, with prior best position Pj, chosen to maximize 
(3)
where |...| denotes the absolute value, and it is presumed that the fitness function is to
be maximized. The above expression is called the Fitness-Distance-Ratio, suggesting the
name FDR-PSO for the algorithm; for a minimization problem, we would instead use
(Cost (Pj) - Cost(Xi)) in the numerator of the above expression.
This version of the algorithm has been more successful than variations such as select-
ing a single particle in whose direction all velocity components are updated. The
pseudocode for this algorithm is given in Figure 1.
3  Experimental Settings and Benchmark Problems
Experiments were conducted with several variations of FDR-PSO, obtained by changing
the parameter values
, 
, 
. The results in the tables and figures use the notation
“FDR-PSO(
,
, 
)”. Note that FDR-PSO(1,1,0) is the same as the usual PSO algo-
rithm described by Kennedy and Eberhart. On the other hand, FDR-PSO(0,1,
) and
FDR-PSO(1,0,
) correspond to the variations in which one of the main components of
the old PSO algorithm is completely deleted.
“FDR-PSO (1,1,
)” refers to an instance of the new algorithm in which the relative
weightage of the new term is “
” and the terms of the old PSO algorithm remain
unchanged. In all the implementations, the inertia parameter is decremented with number
of iterations as in [11].
FDR j i d
, ,
(
)
Fitness Pj
(
)
Fitness Xi
(
)
–
Pjd
Xid
–
-----------------------------------------------------------------
=
ψ1
ψ2
ψ3
ψ1
ψ2
ψ3
ψ3
ψ3
ψ3
ψ3
112
K. Veeramachaneni et al.

(4)
where 
=0.9;
where gsize is the maximum number of generations for which the algorithm runs, i is
the present generation number. 
 FDR-PSO was compared against two variants of random search algorithms, to verify
whether the particle dynamics are of any use at all. In the “Random Velocity Update algo-
rithm” the new velocity term = old velocity term + a number chosen from the interval [-
width/10, width/10], where “width” is the difference between the max. and min. possible
values for that dimension. In the “Random Position Update algorithm”, with no explicit
velocity contributing, new position = old position + a random number chosen in the same
manner.
Algorithm FDR-PSO:
For t= 1 to the max. bound of the number on generations,
For i=1 to the population size,
    For d=1 to the problem dimensionality,
   Apply the velocity update equation: 
+ 
 where Pi is the best position visited so far by Xi,
 Pg is the best position visited so far by any particle
 and Pn is chosen by maximizing
;
Limit magnitude:
;
Update Position:
;
End- for-d;
Compute fitness of (
);
If needed, update historical information regarding Pi and Pg;
End-for-i;
Terminate if Pg meets problem requirements;
End-for-t;
End algorithm.
Fig. 1. Pseudocode for FDR-PSO algorithm
All the experiments were conducted using a population size of 10, with each algorithm
executed for a maximum of 1000 generations. Experiments were conducted with the fol-
lowing benchmark problems for a dimensionality of n=20. All the benchmarks have glo-
bal minima at the origin.
ω i( )
ω
0.4
–
(
)
gsize
i
–
(
)
×
gsize
0.4
+
------------------------------------------------------
=
ω
Vid
t
1
+
ω
Vid
×
t
ψ1
pid
Xid
–
(
)
×
ψ2
pgd
Xid
–
(
)
×
+
+
=
ψ3
pnd
Xid
–
(
)
×
Fitness Pj
(
)
Fitness Xi
(
)
–
Pjd
Xid
–
-----------------------------------------------------------------
Vid
t
1
+
(
)
min Vmax max
Vmax Vid
t
1
+
(
)
,
–
(
)
,
(
)
=
Xid
t
1
+
(
)
min Maxd max
Mind Xid
t( )
Vid
t
1
+
(
)
+
,
–
(
))
,
(
=
Xi
t
1
+
(
)
113
Optimization Using Particle Swarms with Near Neighbor Interactions

3.1 De Jong’s function 1
where 
(5)
3.2 Axis parallel hyper-ellipsoid
where
(6)
3.3 Rotated hyper-ellipsoid
where 
(7)
3.4 Rosenbrock’s Valley (Banana function)
where 
(8)
3.5 Griewangk’s function
where 
(9)
3.6 Sum of different powers 
where 
(10)
4  Results and Discussion
Figures 2 through 7 present the results on the optimization functions defined in the previ-
ous section. The graphs show results averaged over 30 trials. In each trial, the population
is randomly initialized and the same population is used for PSO and FDR-PSO. 
As shown in Figures 2, 3, 4, 5, 6, 7 and Table 1, the new FDR-PSO algorithm outper-
forms the classic PSO algorithm on each of the benchmark problems on which the exper-
iments have been conducted so far. In each case, the original PSO algorithm performs
well in initial iterations but fails to make further progress in later iterations.
The significant improvement achieved by the FDR-PSO algorithm can be attributed to
the near neighbor interactions. Population diversity is achieved by allowing particles to
learn from their nearest best neighbor which may be of poorer fitness than the global best.
FDR-PSO’s learning is consistent with the social behavior of the individuals in groups,
i.e., learning from the nearest best neighbors with successful search history rather than
learning from only the global best. In some cases, the nearest best neighbors can be the
f x
( )
xi
2
i
1
=
n
∑
=
5.12
–
xi
5.12
≤
≤
f x
( )
i
xi
2
×
i
1
=
n
∑
=
5.12
–
xi
5.12
≤
≤
f x
( )
xj
j
1
=
i
∑







2
i
1
=
n
∑
=
65.536
–
xi
65.536
≤
≤
f x
( )
100
xi
1
+
xi
2
–
(
)
2
1
xi
–
(
)2
+
×
i
1
=
n
1
–
∑
=
2.048
–
xi
2.048
≤
≤
f x
( )
xi
2
4000
------------
xi
i
-----




cos
1
+
i
1
=
n
∏
–
i
1
=
n
∑
=
600
–
xi
600
≤
≤
f x
( )
xi
i
1
+
i
1
=
n
∑
=
1
–
xi
1
≤
≤
114
K. Veeramachaneni et al.

global best itself and hence can imply re-emphasizing the social learning. However, the
probability of the nearest best neighbor being the global best decreases with the increase
in population as well as the dimensionality of the problem. The algorithm in this paper
has been implemented for a population of 10. The probability of the global best being the
nearest best neighbor was observed to be as low as 0.4. Increasing the population size
would result in a more robust implementation of this algorithm and is expected to result
in a more diverse population. Such an implementation can be used for a more difficult
multimodal search space where a diverse and localized PSO is a requirement.
The population diversity that is achieved can be demonstrated by the fact that the best
fitness and average population fitness became identical within 500 generations when the
PSO algorithm was applied to Rosenbrock’s problem, whereas this did not occur until
about 1000 generations when the FDR-PSO algorithm was applied. Similar results were
observed for all the other benchmark problems, this shows that the new algorithm is less
plagued by the premature convergence problem faced by the PSO.
Table 1. Minima achieved for different optimization functions using different algorithms
The results also show that the FDR-PSO algorithm can perform well in the absence of
the social or cognitive terms. By ranking the algorithm in the decreasing order of their
performance, it can be seen that the top three positions are shared by FDR-PSO(112),
FDR-PSO(012), FDR-PSO(102) with FDR-PSO(112) being the best in most of the
benchmark problems. It is interesting to note that FDR-PSO(111) has always been a poor
performer in the FDR-PSO family. This demonstrates the sensitivity of the algorithm to
the weight given to the “nbest” term. The “near neighbor” term, however, remains the
most important term of the new algorithm with PSO related terms adding a little more to
the performance. This can be seen from the fact that the FDR-PSO(002) outperforms the
standard PSO and the FDR-PSO(111) in four benchmark problems. The versions, random
velocity update and random position updates are worst performers of all. 
Algorithm
De
Jong’s
Rosenbr
ock’s
Axis 
Parallel 
Hyper-
Ellipsoid
Rotated 
Hyper-
Ellipsoid
Griewang
k’s
Sum of
Powers
 PSO
0.0239
 6.8309
0.1250
55.85
 5.0501
1.8e-7
FDR(111)
0.0027
6.0802
0.0230
20.5686
 3.6946
7.32e-11
FDR (112)
2.02e-5
4.8717
1.07e-5
1.2776
0.0475
5.3e-19
FDR (102)
2.63e-7
5.7389
7.6e-5
365.0034
0.4172
4.8e-17
FDR (012)
8.36e-6
5.0130
3.6e-4
0.9080
0.0308
3.8e-11
FDR(002)
0.0010
8.2869
 0.0035
1513.2
 2.1735
3.3e-12
115
Optimization Using Particle Swarms with Near Neighbor Interactions

Fig. 2. Best minima plotted against the number of generations for each algorithm, for
DeJong’s function, averaged over 30 trials
Fig. 3. Best minima plotted against the number of generations for each algorithm, for
Axis parallel hyper-ellipsoid, averaged over 30 trials
0
100
200
300
400
500
600
700
800
900
1000
-7
-6
-5
-4
-3
-2
-1
0
1
2
GENERATIONS------->
LOG (BEST MINIMA)----->
Minima Achieved Vs Number of Iterations
PSO
FDR-PSO(111)
FDR-PSO(112)
FDR-PSO(102)
FDR-PSO(012)
FDR-PSO(002)
Random Velocity
Random Postion Update
0
100
200
300
400
500
600
700
800
900
1000
-5
-4
-3
-2
-1
0
1
2
3
GENERATIONS------->
LOG (BEST MINIMA)----->
Minima Achieved Vs Number of Iterations
PSO
FDR-PSO(111)
FDR-PSO(112)
FDR-PSO(102)
FDR-PSO(012)
FDR-PSO(002)
Random Velocity
Random Postion Update
116
K. Veeramachaneni et al.

Fig. 4. Best minima plotted against the number of generations for each algorithm, for
Rotated hyper-ellipsoid, averaged over 30 trials
Fig. 5. Best minima plotted against the number of generations for each algorithm, for
Rosenbrock’s Valley, averaged over 30 trials
0
100
200
300
400
500
600
700
800
900
1000
-0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
GENERATIONS------->
LOG (BEST MINIMA)----->
Minima Achieved Vs Number of Iterations
PSO
FDR-PSO(111)
FDR-PSO(112)
FDR-PSO(102)
FDR-PSO(012)
FDR-PSO(002)
Random Velocity
Random Postion Update
0
100
200
300
400
500
600
700
800
900
1000
0.5
1
1.5
2
2.5
3
GENERATIONS------->
LOG (BEST MINIMA)----->
Minima Achieved Vs Number of Iterations
PSO
FDR-PSO(111)
FDR-PSO(112)
FDR-PSO(102)
FDR-PSO(012)
FDR-PSO(002)
Random Velocity
Random Postion Update
117
Optimization Using Particle Swarms with Near Neighbor Interactions

Fig. 6. Best minima plotted against the number of generations for each algorithm, for
Griewangk’s Function, averaged over 30 trials
Fig. 7. Best minima plotted against the number of generations for each algorithm, for
Sum of Powers, averaged over 30 trials
0
100
200
300
400
500
600
700
800
900
1000
-2
-1.5
-1
-0.5
0
0.5
1
1.5
GENERATIONS------->
LOG (BEST MINIMA)----->
Minima Achieved Vs Number of Iterations
PSO
FDR-PSO(111)
FDR-PSO(112)
FDR-PSO(102)
FDR-PSO(012)
FDR-PSO(002)
Random Velocity
Random Postion Update
0
100
200
300
400
500
600
700
800
900
1000
-20
-15
-10
-5
0
5
GENERATIONS------->
LOG (BEST MINIMA)----->
Minima Achieved Vs Number of Iterations
PSO
FDR-PSO(111)
FDR-PSO(112)
FDR-PSO(102)
FDR-PSO(012)
FDR-PSO(002)
Random Velocity
Random Postion Update
118
K. Veeramachaneni et al.

 Several other researchers have proposed different variations of PSO. For example,
ARPSO[17] uses a diversity measure to have the algorithm alternate between two phases
i.e., attraction and repulsion. In this algorithm, 95% of the fitness improvements were
achieved in the attraction phase and the repulsion phase merely increases the diversity. In
the attraction phase the algorithm runs as the basic PSO, while in the repulsion phase the
particles are merely pushed in opposite direction of the best solution achieved so far.
The random restart mechanism has also been proposed under the name of “PSO with
Mass Extinction”[15]. In this, after every “Ie” generations, called the extinction interval,
the velocities of the swarm are reinitialised with random numbers. Researchers have also
explored increasing diversity by increasing randomness associated with velocity and
position updates, thereby discouraging swarm convergence, in the “Dissipative
PSO”[16]. Lovbjerg and Krink have explored extending the PSO with “Self Organized
Criticality”[14], aimed at improving population diversity. In their algorithm, a measure,
called “criticality”, describing how close to each other are the particles in the swarm, is
used to determine whether to relocate particles. Lovbjerg, Rasmussen, and Krink also
proposed in [6], an idea of splitting the population of particles into subpopulations and
hybridizing the algorithm, borrowing the concepts from Genetic algorithms. All these
variations perform better than the PSO. These variations however seem to add new con-
trol parameters, such as, extinction interval in [15], diversity measure in [17], criticality
in[14], and various genetic algorithm related parameters in [6], which can be varied and
have to be carefully decided upon. The beauty of FDR-PSO lies in the fact that it has no
more additional parameters than the PSO and achieves the objectives achieved by any of
these variations and reaches a better minima. Table 2 compares the FDR-PSO algorithm
with these variations. The comparisons were performed by experimenting FDR-PSO(1,
1, 2) on the benchmark problems with approximately the same settings as reported in the
experiments of those variations. In all the cases the FDR-PSO outperforms the other vari-
ations.
Table 2. Minima achieved by different variations of PSO and FDR-PSO
Algorithm 
Dimensions
Generations
Griewangk’s
Function
Rosenbrock’s
Function
PSO
20
2000
0.0174
11.16
GA
20
2000
0.0171
107.1
ARPSO
20
2000
0.0250
2.34
FDR-PSO(112)
20
2000
0.0030
1.7209
PSO
10
1000
0.08976 
43.049
GA
10
1000
283.251
109.81
Hybrid(1)
10
1000
0.09078
43.521
119
Optimization Using Particle Swarms with Near Neighbor Interactions

5  Conclusions 
This paper has proposed a new variation of the particle swarm optimization algorithm
called FDR-PSO, introducing a new term into the velocity component update equation:
particles are moved towards nearby particles’ best prior positions, preferring positions of
higher fitness. The implementation of this idea is simple, based on computing and maxi-
mizing the relative fitness-distance-ratio. The new algorithm outperfoms PSO on many
benchmark problems, being less susceptible to premature convergence, and less likely to
be stuck in local optima. FDR-PSO algorithm outperforms the PSO even in the absence
of the terms of the original PSO.
From one perspective, the new term in the update equation of FDR-PSO is analogous
to a recombination operator where recombination is restricted to individuals in the same
region of the search space. The overall evolution of the PSO population resembles that of
other evolutionary algorithms in which offspring are mutations of parents, whom they
replace. However, one principal difference is that algorithms in the PSO family retain his-
torical information regarding points in the search space already visited by various parti-
cles; this is a feature not shared by most other evolutionary algorithms.
In current work, a promising variation of the algorithm, with the simultaneous influ-
ence of multiple other neighbors on each particle under consideration, is being explored.
Future work includes further experimentation with parameters of FDR-PSO, testing the
new algorithm on other benchmark problems, and evaluating its performance relative to
EP and ES algorithms.
References
1.  Kennedy, J. and Eberhart, R., “Particle Swarm Optimization”, IEEE International
Conference on Neural Networks, 1995, Perth, Australia.
2.   Eberhart, R. and Kennedy, J., “A New Optimizer Using Particles Swarm Theory”,
Sixth International Symposium on Micro Machine and Human Science, 1995,
Nayoga, Japan.
3.
 Eberhart, R. and Shi, Y., “Comparison between Genetic Algorithms and Particle
Swarm Optimization”, The 7th Annual Conference on Evolutionary Programming,
1998,  San Diego, USA.
Hybrid(2)
10
1000
0.46423
51.701
Hybrid(4)
10
1000
0.6920
63.369
Hybrid(6)
10
1000
0.74694
81.283
HPSO1
10
1000
0.09100
70.41591
HPSO2
10
1000
0.08626
45.11909
FDR-PSO(112)
10
1000
0.0148
9.4408
Algorithm 
Dimensions
Generations
Griewangk’s
Function
Rosenbrock’s
Function
120
K. Veeramachaneni et al.

4.   Shi, Y. H., Eberhart, R. C., “A Modified Particle Swarm Optimizer”, IEEE Interna-
tional Conference on Evolutionary Computation, 1998, Anchorage, Alaska.
5.
 Kennedy J., “Small Worlds and MegaMinds: Effects of Neighbourhood Topology on
Particle Swarm Performance”, Proceedings of the 1999 Congress of Evolutionary
Computation, vol. 3, 1931-1938. IEEE Press.
6.  Lovbjerg, M., Rasmussen, T. K., Krink, T., “ Hybrid Particle Swarm Optimiser with
Breeding and Subpopulations”, Proceedings of Third Genetic Evolutionary Compu-
tation, (GECCO 2001). 
7.
Carlisle, A. and Dozier, G.. “Adapting Particle Swarm Optimization to Dynamic
Environments”, Proceedings of International Conference on Artificial Intelligence,
Las Vegas, Nevada, USA, pp. 429-434, 2000.
8.   Kennedy, J., Eberhart, R. C., and Shi, Y. H., Swarm Intelligence, Morgan Kaufmann
Publishers, 2001.
9. GEATbx: Genetic and Evolutionary Algorithm Toolbox for MATLAB, Hartmut Pohl-
heim, 
http://www.systemtechnik.tu-ilmenau.de/~pohlheim/GA_Toolbox/
index.html.
10.   E. Ozcan and C. K. Mohan, “Particle Swarm Optimzation: Surfing the Waves”, Pro-
ceedings of Congress on Evolutionary Computation (CEC’99), Washington D. C.,
July 1999, pp 1939-1944.
11.  Particle Swarm Optimization Code, Yuhui Shi, www.engr.iupui.edu/~shi
12.  van den Bergh, F., Engelbrecht, A. P., “Cooperative Learning in Neural Networks
using Particle Swarm Optimization”, South African Computer Journal, pp. 84-90,
Nov. 2000.
13.  van den Bergh, F., Engelbrecht, A. P., “Effects of Swarm Size on Cooperative Parti-
cle Swarm Optimisers”, Genetic and Evolutionary Computation Conference, San
Francisco, USA, 2001.
14.  Lovbjerg, M., Krink, T., “Extending Particle Swarm Optimisers with Self-Organized
Criticality”, Proceedings of Fourth Congress on Evolutionary Computation, 2002,
vol. 2, pp. 1588-1593.
15.   Xiao-Feng Xie, Wen-Jun Zhang, Zhi-Lian Yang, “Hybrid Particle Swarm Optimizer
with Mass Extinction”, International Conf. on Communication, Circuits and Sys-
tems (ICCCAS), Chengdu, China, 2002.
16. 
Xiao-Feng Xie, Wen-Jun Zhang, Zhi-Lian Yang, “A Dissipative Particle Swarm
Optimization”, IEEE Congress on Evolutionary Computation, Honolulu, Hawaii,
USA, 2002.
17.   Jacques Riget, Jakob S. Vesterstorm, “A Diversity-Guided Particle Swarm Opti-
mizer - The ARPSO”, EVALife Technical Report no. 2002-02.
121
Optimization Using Particle Swarms with Near Neighbor Interactions

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 122–133, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Revisiting Elitism in Ant Colony Optimization
Tony White, Simon Kaegi, and Terri Oda
School of Computer Science, Carleton University
1125 Colonel By Drive, Ottawa, Ontario, Canada K1S 5B6
arpwhite@scs.carleton.ca, skaegi@sympatico.ca, terri@zone12.com
Abstract. Ant Colony Optimization (ACO) has been applied successfully in
solving the Traveling Salesman Problem. Marco Dorigo et al. used Ant System
(AS) to explore the Symmetric Traveling Salesman Problem and found that the
use of a small number of elitist ants can improve algorithm performance. The
elitist ants take advantage of global knowledge of the best tour found to date and
reinforce this tour with pheromone in order to focus future searches more
effectively. This paper discusses an alternative approach where only local
information is used to reinforce good tours thereby enhancing the ability of the
algorithm for multiprocessor or actual network implementation. In the model
proposed, the ants are endowed with a memory of their best tour to date. The
ants then reinforce this “local best tour” with pheromone during an iteration to
mimic the search focusing of the elitist ants. The environment used to simulate
this model is described and compared with Ant System.
Keywords: Heuristic Search, Ant Algorithm, Ant Colony Optimization, Ant
System, Traveling Salesman Problem.
1 Introduction
Ant algorithms (also known as Ant Colony Optimization) are a class of heuristic
search algorithms that have been successfully applied to solving NP hard problems
[1]. Ant algorithms are biologically inspired from the behavior of colonies of real ants,
and in particular how they forage for food. One of the main ideas behind this approach
is that the ants can communicate with one another through indirect means by making
modifications to the concentration of highly volatile chemicals called pheromones in
their immediate environment.
The Traveling Salesman Problem (TSP) is an NP complete problem addressed by
the optimization community having been the target of considerable research [7]. The
TSP is recognized as an easily understood, hard optimization problem of finding the
shortest circuit of a set of cities starting from one city, visiting each other city exactly
once, and returning to the start city again. Formally, the TSP is the problem of finding
the shortest Hamiltonian circuit of a set of nodes. There are two classes of TSP
problem: symmetric TSP, and asymmetric TSP (ATSP). The difference between the

Revisiting Elitism in Ant Colony Optimization         123
two classes is that with symmetric TSP the distance between two cities is the same
regardless of the direction you travel, with ATSP this is not necessarily the case.
Ant Colony Optimization has been successfully applied to both classes of TSP
with good, and often excellent, results. The ACO algorithm skeleton for TSP is as
follows [7]:
procedure ACO algorithm for TSPs
   Set parameters, initialize pheromone trails
   while (termination condition not met) do
      ConstructSolutions
      ApplyLocalSearch % optional
      UpdateTrails
   end
end ACO algorithm for TSPs
The earliest implementation, Ant System, was applied to the symmetric TSP
problem initially and as this paper presents a proposed improvement to Ant System
this is where we will focus our efforts.
While the ant foraging behaviour on which the Ant System is based has no central
control or global information on which to draw, the use of global best information in
the Elitest form of the Ant System represents a significant departure from the purely
distributed nature of ant-based foraging. Use of global information presents a
significant barrier to fully distributed implementations of Ant System algorithms in a
live network, for example. This observation motivates the development of a fully
distributed algorithm – the Ant System Local Best Tour (AS-LBT) – described in this
paper. As the results demonstrate, it also has the by-product of having superior
performance when compared to the Elitest form of the Ant System (AS-E). It also has
fewer defining parameters.
The remainder of this paper consists of 5 sections. The next section provides
further detail for the algorithm shown above. The Ant System Local Best Tour (AS-
LBT) algorithm is then introduced and the experimental setup for its evaluation
described. An analysis section follows, and the paper concludes with an evaluation of
the algorithm with proposals for future work.
2 Ant System (AS)
Ant System was the earliest implementation of Ant Colony Optimization meta
heuristic. The implementation is built on top of the ACO algorithm skeleton shown
above. A brief description of the algorithm follows. For a comprehensive description
of the algorithm, see [1, 2, 3 or 7].

124         T. White, S. Kaegi, and T. Oda
2.1 
Algorithm
Expanding upon the algorithm above, an ACO consists of two main sections:
initialization and a main loop. The main loop runs for a user-defined number of
iterations. These are described below:
Initialization
 
Any initial parameters are loaded.
 
Each of the roads is set with an initial pheromone value.
 
Each ant is individually placed on a random city.
Main Loop Begins
Construct Solution
 
Each ant constructs a tour by successively applying the probabilistic choice
function and randomly selecting a city it has not yet visited until each city has
been visited exactly once.
[
] [ ]
[
]
[
]
∑∈
⋅
⋅
=
k
i
N
l
il
il
ij
ij
k
ij
t
t
t
p
β
α
β
α
η
τ
η
τ
)
(
)
(
)
(
 
The probabilistic function,
)
(t
pk
ij
, is designed to favor the selection of a road
that has a high pheromone value,τ , and high visibility value, η , which is
given by: 
ij
d
/
1
, where
ij
d
is the distance to the city. The pheromone scaling
factor,α , and visibility scaling factor, β , are parameters used to tune the
relative importance of pheromone and road length in selecting the next city.
Apply Local Search
 
Not used in Ant System, but is used in several variations of the TSP problem
where 2-opt or 3-opt local optimizers [7] are used.
Best Tour Check
 
For each ant, calculate the length of the ant’s tour and compare to the best tour’s
length. If there is an improvement, update it.
Update Trails
 
Evaporate a fixed proportion of the pheromone on each road.
 
For each ant perform the “ant-cycle” pheromone update.
 
Reinforce the best tour with a set number of “elitist ants” performing the “ant-
cycle” pheromone update.
In the original investigation of Ant System algorithms, there were three versions
of Ant System that differed in how and when they laid pheromone. The “Ant-density”
heuristic updates the pheromone on a road traveled with a fixed amount after every
step. The “Ant-quantity” heuristic updates the pheromone on a road traveled with an
amount proportional to the inverse of the length of the road after every step. Finally,

Revisiting Elitism in Ant Colony Optimization         125
the “Ant-cycle” heuristic first completes the tour and then updates each road used with
an amount proportional to the inverse of the total length of the tour.
Of the three approaches “Ant-cycle” was found to produce the best results and
subsequently receives the most attention. It will be used for the remainder of this
paper.
2.2 
Discussion
Ant System in general has been identified as having several good properties related to
directed exploration of the problem space without getting trapped in local minima [1].
The initial form of AS did not make use of elitist ants and did not direct the search as
well as it might. This observation was confirmed in our experimentation performed as
a control and used to verify the correctness of our implementation.
The addition of elitist ants was found to improve ant capabilities for finding better
tours in fewer iterations of the algorithm, by highlighting the best tour. However, by
using elitist ants to reinforce the best tour the problem now takes advantage of global
data with the additional problem of deciding on how many elitist ants to use. If too
many elitist ants are used the algorithm can easily become trapped in local minima [1,
3]. This represents the dilemma of exploitation versus exploration that is present in
most optimization algorithms.
There have been a number of improvements to the original Ant System algorithm.
They have focused on two main areas of improvement [7]. First, they more strongly
exploit the globally best solution found. Second, they make use of a fast local search
algorithm like 2-opt, 3-opt, or the Lin-Kernighan heuristic to improve the solutions
found by the ants.
The algorithm improvements to Ant System have produced some of the highest
quality solutions when applied to the TSP and other NP complete (or NP hard)
problems [1].
As described in section 2.1, augmenting AS with a local search facility would be
straightforward; however, it is not considered here. The area of improvement
proposed in this paper is to explore an alternative to using the globally best tour
(GBT) to reinforce and focus on good areas of the search space. The Ant System
Local Best Tour algorithm is described in the next section.
3 Ant System Local Best Tour (AS-LBT)
The use of an elitist ant in Ant System exposes the need for a global observer to watch
over the problem and identify what the best tour found to date is on a per iteration
basis. As such, it represents a significant departure from the purely distributed AS
algorithm.
The idea behind the design of AS-LBT is specifically to remove this notion of a
global observer from the problem. Instead, each individual ant keeps track of the best
tour it has found to date and uses it in place of the elitist ant tour to reinforce tour
goodness.

126         T. White, S. Kaegi, and T. Oda
It is as if the scale of the problem has been brought down to the ant level and each
ant is running its individual copy of the Ant System algorithm using a single elitist ant.
Remarkably, the ants work together effectively even if indirectly and the net effect is
very similar to that of using the pheromone search focusing of the elitist ant approach.
In fact, AS-E and AS-LBT can be thought of as extreme forms of a Particle Swarm
algorithm. In Particle Swarm Optimization (PSO), particles (effectively equivalent to
ants in ACO) have their search process moderated by both local and global best
solutions.
3.1 
Algorithm
The algorithm used is identical to that described for Ant System with the replacement
of the elitist ant step with the ant’s local best tour step. Referring, once again, to the
algorithm described in section 2.1, the following changes are made:
That is, where the elitist ant step was:
 
Reinforce the best tour with a set number of “elitist ants” performing the “ant-
cycle” pheromone update.
For Local Best Tour we now do the following:
 
For each ant perform the “ant-cycle” pheromone update using its local best tour.
The rest of the Ant System algorithm is unchanged, including the newly explored
tour’s “ant-cycle” pheromone update.
3.2 
Experimentation and Results
For the purposes of demonstrating AS-LBT we constructed an Ant System simulation
and applied it to a series of TSP Problems from the TSPLIB95 collection [6].  Three
symmetric TSP problems were studied: eil51, eil76 and kro101. The eil51 problem is
a 51-city TSP instance set up in a 2 dimensional Euclidean plane for which the
optimal tour is known. The weight assigned to each road comes from the linear
distance separating each pair of cities. The problems eil76 and kro101 represent
symmetric TSP problems of 76 and 101 cities respectively.
The simulation created for this paper was able to emulate the behavior of the
original Ant System (AS), Ant System with elitist ants (AS-E), and finally Ant System
using the local best tour (AS-LBT) approach described in section 2.
3.2.1 Parameters and Settings
Ant System requires you to make a number of parameter selections. These parameters
are:
Pheromone sensitivity (α ) = 1
Pheromone additive constant
Visibility sensitivity ( β ) = 5
Number of ants
Pheromone decay rate ( ρ ) = 0.5
Number of elitist ants
Initial pheromone (
0
τ ) = 10-6

Revisiting Elitism in Ant Colony Optimization         127
In his original work on Ant System Marco Dorigo performed considerable
experimentation to tune and find appropriate values for a number of these parameters
[3]. The values Dorigo found that provide for the best performance when averaged
over the problems he studied were used in our experiments. These best-practice values
are shown in the list above.
For those parameters that depend on the size of the problem our simulation made
an effort to select good values based on knowledge of the problem and number of
cities. Recent work [5] on improved algorithm parameters was unavailable to us when
developing the LBT algorithm. We intend to explore the performance of the new
parameters settings and will report the results in a future communication.
The Pheromone additive constant (Q) was eliminated altogether as a parameter by
replacing it with the global best tour (GBT) length in the case of standard Ant System
and the local best tour (LBT) length for the approach in this paper. We justify this
decision by noting that Dorigo found that differences in the value of Q only weakly
affected the performance of the algorithm and a value within an order of magnitude of
the optimal tour length was acceptable. This means that the pheromone addition on an
edge becomes:
ant
best
L
L
For a normal “ant-cycle” pheromone update
1
=
best
best
L
L
For an elitist or LBT “ant-cycle” pheromone update
The key factor in the pheromone update is that it remains inversely proportional
to the length of the tour and this still holds with our approach. The ants now are not
tied to a particular value of Q in the event of a change in the number of cities in the
problem. We consider the removal of a user-defined parameter another attractive
feature of the LBT algorithm and a contribution of the research reported here.
For the number of ants, we set this equal to the number of cities, as this seems to
be a reasonable selection according to the current literature [1, 3, 7].
For the number of elitist ants we tried various values dependent on the size of the
problem and used a value of 1/6th of the number of cities for the results reported in this
paper.   This value worked well for the relatively low number of cities we used in our
experimentation but for larger problems this value might need to be tuned, possibly
using the techniques used in [5]. The current literature is unclear on the best value of
the number of elitest ants to be used.
With AS-LBT, all ants perform the LBT “ant-cycle” update so subsequently the
number of elitist ants is not needed. We consider the removal of the requirement to
specify a value for the number of elitest ants an advantage. Hereafter, we refer to AS
with elitest ants as AS-E.
3.2.2 Results
Using the parameters from the previous section, we performed 100 experiments for
eil51, eil76 and kro101; the results are shown in Figures 1, 2 and 3 respectively. In the
case of eil51 and eil76, 2000 iterations of each algorithm were performed, whereas

128         T. White, S. Kaegi, and T. Oda
3500 iterations were used for kro101. The results of the experimentation showed
considerable promise for AS-LBT. While experiments for basic AS were performed,
they are not reported in detail here as they were simply undertaken in order to validate
the code written for AS-E and AS-LBT.
Fig. 1. Difference between LBT and Elitest Algorithms (eil51)
Fig. 2. Difference between LBT and Elitest Algorithms (eil76)
Figures 1, 2 and 3, each containing 4 curves, require some explanation. Each
curve in each figure is the difference between the AS-LBT and AS-E per-iteration
average of the 100 experiments performed. Specifically, the “Best Tour” curve
represents the difference in the average best tour per iteration between AS-LBT and
AS-E. The “Avg. Tour” curve represents the difference in the average tour per
iteration between AS-LBT and AS-E. The “Std. Dev. Tour” curve represents the
difference in the standard deviation of all tours per iteration between AS-LBT and AS-

Revisiting Elitism in Ant Colony Optimization         129
Fig. 3. Difference between LBT and Elitest Algorithms (kro101)
E. Finally, the “Global Tour” curve represents the difference in the best tour found per
iteration between AS-LBT and AS-E. As the TSP is a minimization problem, negative
difference values indicate superior performance for AS-LBT. The most important
measure is the “Global Tour” measure, at least at the end of the experiment. This
information is summarized in Table 1, below.
Table 1. Difference in Results for AS-LBT and AS-E
Best Tour
Average Tour
Std. Dev Tour
Global Tour
eil51
-33.56
-39.74
4.91
-3.00
eil76
-29.65
-41.25
1.08
-10.48
Kro101
-19.97
-12.86
3.99
-1.58
The results in Table 1 clearly indicate the superior nature of the AS-LBT
algorithm. The “Global Tour” is superior, on average, in all 3 TSP problems at the
end of the experiment. The difference between AS-E and AS-LBT is significant for all
3 problems for a t-test with an  value of 0.05. Similarly, the “Best Tour” and
“Average Tour” are also better, on average, for AS-LBT. The results for eil76 are
particularly impressive, owing much of their success to the ability of AS-LBT to find
superior solutions at approximately 1710 iterations.
The one statistic that is higher for AS-LBT is the average standard deviation of
tour length on a per-iteration basis. This, too, is an advantage for the algorithm in that
it means that there is still considerable diversity in the population of tours being
explored. It is, therefore, more effective at avoiding local optima.

130         T. White, S. Kaegi, and T. Oda
4 Analysis
Best Tour Analysis: As has been shown in the Results section, AS-LBT is superior to
the AS-E approach as measured by the best tour found. In this section we take a
comparative look at the evolution of the best tour in all three systems and then a look
at the evolution of the best tour found per iteration.
Fig. 4. Evolution of Best Tour Length
In Figure 4, which represents a single typical experiment, we can see the key
difference between AS-E and AS-LBT. Whereas AS-E quickly finds a few good
results, holds steady and then improves in relatively large pronounced steps, AS-LBT
improves more gradually at the beginning but continues its downward movement at a
steadier rate. In fact, if one looks closely at the graph one can see that even the
classical AS system has found a better result during the early stages of the simulation
when compared to AS-LBT. However, by about iteration 75, AS-LBT has overtaken
the other two approaches and continues to gradually make improvements and
maintains its overall improvement until the end of the experiment. This is confirmed in
Figure 1, which is the average performance of AS-LBT for eil51 over 100
experiments.
Overall, the behavior of AS-LBT could be described as slower but steadier. It
takes slightly longer at the beginning to focus pheromone on good tours but after it
has, it improves more frequently and steadily and on average will overtake the other
two approaches given enough time. Clearly this hypothesis is supported by
experimentation with the eil76 and kro101 TSP problem datasets as shown in Figures
2 and 3.
Average Tour Analysis: In the Best Tour Analysis we saw that there was a tendency
for the AS-LBT algorithm to gradually improve in many small steps. With our
analysis of the average tour we want to confirm that the relatively high deviation of ant
EIL51.TSP - Best Tour Length
400
420
440
460
480
500
520
540
560
1
51
101
151
201
251
301
351
401
451
501
551
601
651
701
751
801
851
901
951
Iteration
Tour Length
Ant System (Classic)
Ant System (Elitist Ants)
Ant System (Local Best Tour)

Revisiting Elitism in Ant Colony Optimization         131
algorithms is working in the average case meaning that we are continuing to explore
the problem space effectively. In this section we look at the average tour length per
iteration to see if we can identify any behavioural trends.
In Figure 5 we see a very similar situation to that of the Best Tour Length per
Iteration. The AS-LBT algorithm is on average exploring much closer to the optimal
solution. Perhaps more importantly, the AS-LBT graph trend line is behaving very
similarly in terms of its deviation as that with the other two systems. This suggests that
the AS-LBT system is working as expected and is in fact searching in a better-focused
fashion closer to the optimal solution.
EIL51.TSP - Iteration Average Tour Length
400
420
440
460
480
500
520
540
560
580
600
1
51
101
151
201
251
301
351
401
451
501
551
601
651
701
751
801
851
901
951
Iteration
Tour Length
Ant System (Classic)
Ant System (Elitist Ant)
Ant System (Local Best Tour)
Fig. 5. Average Tour Length for Individual Iterations
Evolution of the Local Best Tour: The Local Best Tour approach is certainly very
similar to the notion of elitist ants; only it is applied at the local level instead of at the
global level. In this section we look at the evolution of the local best tour in terms of
the average and worst tours, and compare them with the global best tour used by elitist
ants.
From Figure 6 we can see that over time both the average and worst LBTs
approach the value of global best tour. In fact the average in this simulation is virtually
the same as the global best tour. From this figure, it is clear that the longer the
simulation runs the closer the LBT “ant-cycle” pheromone update becomes to that of
an elitist ant’s update scheme.
5 Discussion and Future Work
Through the results and analysis shown in this paper, Local Best Tour has proven to
be an effective alternative to the use of the globally best tour for focusing ant search
through pheromone reinforcement. In particular, the results show that AS-LBT has

132         T. White, S. Kaegi, and T. Oda
EIL51.TSP - Comparing Local Best Tour
400
420
440
460
480
500
520
540
560
580
600
1
51
101
151
201
251
301
351
401
451
501
551
601
651
701
751
801
851
901
951
Iteration
Tour Length
Worst Local Best Tour
Average Local Best Tour
Global Best Tour
Fig. 6. Evolution of the Local Best Tour
excellent average performance characteristics. By removing the need for the global
information required for AS-E, we have improved the ease with which a parallel or
live network implementation can be achieved; i.e. a completely distributed
implementation of the TSP is possible.
Analysis of the best tour construction process shows that AS-LBT, while initially
converging more slowly than AS-E, is very consistent at incrementally building a
better tour and on average will overtake the AS-E approach early in the search of the
problem space.
Average and best iteration tour analysis has shown that AS-LBT shares the same
variability characteristics of the original Ant System that make it resistant to getting
stuck in local minima. Furthermore, AS-LBT is very effective in focusing its search
towards the optimal solution.
Finally, AS-LBT follows in the notion that the use of best tours to better focus an
ant’s search is an effect optimization. The emergent behaviour of a set of autonomous
LBT ants is to, in effect, become elitist ants over time.
As described earlier in this paper, a relatively straightforward way to further
improve the performance of AS-LBT would be to add a fast local search algorithm
like 2-opt, 3-opt or the Lin Kernighan heuristic. Alternatively, the integration of recent
network transformation algorithms [4] should prove useful as local search operators.
Finally, future work should include the application of the LBT algorithm to other
problems such as: the asymmetric TSP, the Quadratic Assignment Problem (QAP), the
Vehicle Routing Problem (VRP) and other problems to which ACO has been applied
[1].

Revisiting Elitism in Ant Colony Optimization         133
6 Conclusions
This paper has demonstrated that an ACO algorithm using only local information can
be applied to the TSP. The AS-LBT algorithm is truly distributed and is characterized
by fewer parameters when compared to AS-E. Considerable experimentation has
demonstrated that significant improvements are possible for 3 TSP problems. We
believe that AS-LBT with the improvements outlined in the previous section will
further enhance our confidence in the hypothesis and look forward to reporting on
these improvements in a future research paper.  Finally, we believe that a Particle
Swarm Optimization algorithm, where search is guided by both local best tour and
global best tour terms may yield further improvements in performance for ACO
algorithms.
References
1. 
Bonabeau E., Dorigo M., and Theraulaz G. Swarm Intelligence From Natural to Artificial
Systems. Oxford University Press, New York NY, 1999.
2. 
Dorigo M. and L.M. Gambardella. Ant Colony System: A Cooperative Learning Approach
to the Traveling Salesman Problem. IEEE Transactions on Evolutionary Computation,
1(1):53–66, 1997.
3. 
Dorigo M., V. Maniezzo and A. Colorni. The Ant System: Optimization by a Colony of
Cooperating Agents. IEEE Transactions on Systems, Man, and Cybernetics-Part B,
26(1):29–41, 1996.
4. 
Dumitrescu A. and Mitchell J., Approximation Algorithms for Geometric Optimization
Problems, in the Proceedings of the 9th Canadian Conference on Computational Geometry,
Queen's University, Kingston, Canada, August 11-14, 1997, pp. 229–232.
5. 
Pilat M. and White T., Using Genetic Algorithms to optimize ACS-TSP. In Proceedings of
the 3rd International Workshop on Ant Algorithms, Brussels, Belgium, September 12–14
2002.
6. 
Reinelt G. TSPLIB, A Traveling Salesman Problem Library. ORSA Journal on
Computing, 3:376–384, 1991.
7. 
Stützle T. and Dorigo M. ACO Algorithms for the Traveling Salesman Problem. In K.
Miettinen, M. Makela, P. Neittaanmaki, J. Periaux, editors, Evolutionary Algorithms in
Engineering and Computer Science, Wiley, 1999. 

  
E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 134–139, 2003.
© Springer-Verlag Berlin Heidelberg 2003
A New Approach to Improve Particle Swarm
Optimization
Liping Zhang, Huanjun Yu, and Shangxu Hu
College of Material and Chemical Engineering, Zhejiang University,
Hangzhou 310027, P.R. China
zhanglp@infotech.zju.edu.cn
yuhj@zjuem.zju.edu.cn
sxhu@mail.hz.zj.cn
Abstract. Particle swarm optimization (PSO) is a new evolutionary computa-
tion technique. Although PSO algorithm possesses many attractive properties,
the methods of selecting inertia weight need to be further investigated. Under
this consideration, the inertia weight employing random number uniformly dis-
tributed in [0,1] was introduced to improve the performance of PSO algorithm
in this work. Three benchmark functions were used to test the new method. The
results were presented to show that the new method is effective.
1   Introduction
Particle swarm optimization (PSO) is an evolutionary computation technique intro-
duced by Kennedy and Eberhart in 1995[1-3]. The underlying motivation for the de-
velopment of PSO algorithm was social behavior of animals such as bird flocking,
fish schooling, and swarm [4]. Initial simulations were modified to incorporate near-
est-neighbor velocity matching, eliminate ancillary variable, and acceleration in
movement. PSO is similar to genetic algorithm (GA) in that the system is initialized
with a population of random solutions. However, in PSO, each individual of the
population, called particle, has an adaptable velocity, according to which it moves
over the search space. Each particle keeps track of its coordinate in hyperspace, which
are associated with the solution (fitness) it has achieved so far. This value is called
pbest. Another “best” value is called gbest that is obtained so far by any particle in
the population and stored the overall best value.
Suppose that the search space is D-dimensional, then the i-th particle of the swarm
can be represented by a D-dimensional vector, Xi=(xi1, xi2,...,xiD). The velocity of this
particle, can be represented by another D-dimensional vector Vi=(vi1, vi2,...,viD). The
best previously visited position of the i-th particle is denoted as Pi=(pi1, pi2,...,piD). De-
fining g as the index of the best particle in the swarm, then the velocity of particle and
its new position will be assigned according to the following two equations:
)
(
)
(
2
2
1
1
id
gd
id
id
id
id
x
p
r
c
x
p
r
c
v
v
−
+
−
+
=
(1)

A New Approach to Improve Particle Swarm Optimization          135
id
id
id
v
x
x
+
=
(2)
where c1 and c2 are positive constant, called acceleration, and r1 and r2 are two random
numbers, uniformly distributed in [0,1].
    Velocities of particles on each dimension are clamped by a maximum velocity Vmax.
If the sum of accelerations would cause the velocity on that dimension to exceed Vmax,
which is a parameter specified by the user, then the velocity on that dimension is lim-
ited to Vmax. Vmax influences PSO performance sensitively. A larger Vmax facilitates
global exploration, while a smaller Vmax encourages local exploitation [5].
    The PSO algorithm is still far from mature, many authors have modified the origi-
nal version. Firstly, in order to better control exploration, an inertia weight in the PSO
algorithm was first introduced in 1998 [6]. Recently, for insuring convergence, Clerc
proposed the use of a constriction factor in the PSO [7]. Equation (3), (4), and (5) de-
scribes the modified algorithm.
))
(
)
(
(
2
2
1
1
id
gd
id
id
id
id
x
p
r
c
x
p
r
c
wv
v
−
+
−
+
= χ
(3)
id
id
id
v
x
x
+
=
(4)
ϕ
ϕ
ϕ
χ
4
2
2
2 −
−
−
=
(5)
where w is the inertia weight, and χ  is a constriction factor, and 
4
,
2
1
>
+
=
ϕ
ϕ
c
c
.
    The use of the inertia weight for controlling the velocity has resulted in high effi-
ciency for PSO. Suitable selection of the inertia weight provides a balance between
global and local explorations. The performance of PSO using an inertia weight was
compared with performance using a constriction factor [8], and Eberhart et al. con-
cluded that best approach is to use the constriction factor while limiting the maximum
velocity Vmax to the dynamic range of the variable Xmax on each dimension. For exam-
ple, Vmax= Xmax.
    In this work, we proposed a method using random number inertia weight called
RNM to improve the performance of PSO.
2   The Ways to Determine the Inertia Weight
As mentioned precedingly, the inertia weight was found to be an important parameter
to PSO algorithms. However, the determination of inertia weight is still an unsolved
problem. Shi et al. provided methods to determine the inertia weight. In their earlier
work, inertia weight was set as constant [6]. By setting maximum velocity to be 2.0, it
was found that PSO with an inertia weight in the range [0.9, 1.2] on average has a
better performance. In a later work, inertia weight was set to be continuously de-
creased linearly during run [9]. Still later, a time decreasing inertia weight from 0.9 to
0.4 was found to be better than a fixed inertia weight. The linearly decreasing inertia

136          L. Zhang, H. Yu, and S. Hu
weight (LDW) was used by many authors so far [10-12]. Recently another approach
was suggested to use a fuzzy variable to adapt the inertia weight [12,13]. The results
reported in their papers showed that the performance of PSO can be significantly im-
proved. However, it is relatively complicated.
    The right side of equation (1) consists of three parts: the first part is the previous
velocity of the particle; the second and third parts are contributing to the change of the
velocity of a particle. Shi and Eberhart concluded that the role of the inertia weight w
is considered to be crucial for the convergence of PSO [6]. A larger inertia weight fa-
cilitates global exploration (searching new areas), while a smaller one tends to facili-
tate local exploitation. A general rule of thumb suggests that it is better to initially set
the inertia weight to a larger value, and gradually decrease it. Unfortunately, the phe-
nomenon that the global search ability is decreasing when inertia weight is decreasing
to zero indicates that inertia weight may exit some unclear mechanism [14]. However,
the deceased inertia weight is subject to trap the algorithms into the local optima and
slows the convergence speed when it is near a minimum. Under this consideration,
many cases were tested, and we finally set the inertia weight as random numbers uni-
formly distributed in [0,1], which is more capable of escaping from the local optima
than LDW, therefore better results were obtained. Our motivation is that local ex-
ploitation combining with global exploration can be processing parallel. The new ver-
sion is:
)
(
)
(
2
2
1
1
0
id
gd
id
id
id
id
x
p
r
c
x
p
r
c
v
r
v
−
+
−
+
=
(6)
where r0 is a random number uniformly distributed in [0,1], and the other parameters
are same as before.
    Our method can overcome two drawbacks of LDW. For one thing, decreasing the
dependence of inertial weight on the maximum iteration that is difficultly predicted
before experiments. Another is avoiding the lacks of local search ability at early of
run and global search ability at the end of run.
3   Experimental Studies
In order to test the influence of inertia weight on the PSO performance, three non-
linear benchmark functions reported in literature [15,16] were used since they are well
known problems. The first function is the Rosenbrock function:
∑
=
+
−
+
−
=
n
i
i
i
i
x
x
x
x
f
1
2
2
2
1
1
)
)1
(
)
(
100
(
)
(
(7)
where x=[x1, x2,...,xn] is an n-dimensional real-valued vector.
    The second is the generalized Rastrigrin function:
∑
=
+
−
=
n
i
i
i
x
x
x
f
1
2
2
)
10
)
2
cos(
10
(
)
(
π
(8)
    The third is the generalized Griewank function:

A New Approach to Improve Particle Swarm Optimization        137
∑
∏
=
=
+
−
=
n
i
n
i
i
i
i
x
x
x
f
1
1
2
3
1
)
cos(
4000
1
)
(
(9)
Three different amounts dimensions were tested: 10, 20 and 30. The maximum num-
bers of generations were set as 1000, 1500 and 2000 corresponding to the dimensions
10, 20 and 30, respectively. For investigation the scalability of PSO algorithm, three
population sizes 20, 40 and 80 were used for each function with respect to different
dimensions. Acceleration constants took the values c1=c2=2. Constriction factor χ
=1. For the purpose of comparison, all the Vmax and Xmax were assigned by same pa-
rameter settings as in literature [13] and listed in table 1. 500 trial runs were taken for
each case.
Table 1. Xmax and Vmax values used for tests
Function
Xmax
Vmax
f1
100
100
f2
10
10
f3
600
600
4   Results and Discussions
Table 2, 3 and 4 listed the mean best fitness value of the best particle found for the
Rosenbrock, Rastrigrin, and Griewank function with two inertia weight selecting
methods, LDW and RNW respectively.
Table 2. Mean best fitness value for the Rosenbrock function
By comparing the results of two methods, it is clearly to see that the performance of
PSO can be improved with random number inertia weight for Rastrigrin and Ro-
Population
Size
No. of
Dimensions
No. of
Generations
LDW
Method
RNW
Method
10
1000
106.63370
65.28474
20
1500
180.17030
147.52372
20
30
2000
458.28375
409.23443
10
1000
61.36835
41.32016
20
1500
171.98795
95.48422
40
30
2000
289.19094
253.81490
10
1000
47.91896
20.77741
20
1500
104.10301
82.75467
80
30
2000
176.87379
156.00258

138          L. Zhang, H. Yu, and S. Hu
senbrock function, while for the Griewank function, results of two methods are com-
parable.
Table 3. Mean best fitness value for the Rastrigrin function
Population
Size
No. of
Dimensions
No. of
Generations
LDW
Method
RNW
Method
10
1000
5.25230
5.04258
20
1500
22.92156
20.31109
20
30
2000
49.21827
42.58132
10
1000
3.56574
3.22549
20
1500
17.74121
13.84807
40
30
2000
38.06483
32.15635
10
1000
2.37332
1.85928
20
1500
13.11258
9.95006
80
30
2000
30.19545
25.44122
Table 4.  Mean best fitness value for the Griewank function
Population
Size
No. of
Dimensions
No. of
Generations
LDW
Method
RNW
Method
10
1000
0.09620
0.09926
20
1500
0.03000
0.03678
20
30
2000
0.01674
0.02007
10
1000
0.08696
0.07937
20
1500
0.03418
0.03014
40
30
2000
0.01681
0.01743
10
1000
0.07154
0.06835
20
1500
0.02834
0.02874
80
30
2000
0.01593
0.01718
5   Conclusions
In this work, the performance of the PSO algorithm with random number inertia
weight has been extensively investigated by experimental studies of three non-linear
functions. Because local exploitation combining with global exploration can be proc-
essing parallel, random number inertia weight (RNW) method can obtain better re-
sults than linearly decreasing inertia weight (LDW) method. Lacks of local search
ability at early stage of run and global search ability at the end of run using linearly
decreasing inertia weight method were overcomed. However, only three benchmark
problems had been tested. To fully claim the benefits of the random number inertia
weight to PSO algorithm, more problems need to be tested.

A New Approach to Improve Particle Swarm Optimization        139
References
1. 
J. Kennedy and R. C. Eberhart. Particle swarm optimization. Proc. IEEE Int. Conf. on
Neural Networks (1995) 1942–1948
2. 
R. C. Eberhart and J. Kennedy. A new optimizer using particle swarm theory. Proceedings
of the Sixth International Symposium on Micro Machine and Human Science. Nagoya,
Japan (1995) 39–43
3. 
R. C. Eberhart, Simpson, P. K., and Dobbins, R. W. Computational Intelligence PC Tools.
Boston, MA: Academic Press Professional (1996)
4. 
M. M. Millonas. Swarm, phase transition, and collective intelligence. In C.G. Langton,
Eds., Artificial life III. Addison Wesley, MA (1994)
5. 
K. E. Parsopoulos and M. N. Vrahatis. Recent approaches to global optimization problems
through particle swarm optimization. Natural Computing 1 (2002) 235–306
6. 
Y. Shi and R. Eberhart. A modified particle swarm optimizer. IEEE Int. Conf. on Evolu-
tionary Computation (1997) 303–308
7. 
M. Clerc. The swarm and queen: towards a deterministic and adaptive particle swarm op-
timization. Proc. Congress on Evolutionary Computation, Washington, DC,. Piscataway,
NJ:IEEE Service Center (1999) 1951–1957
8. 
R. C. Eberhart and Y. Shi. Comparing Inertia weight and constriction factors in particle
swarm optimization. In Proc. 2000 Congr. Evolutionary Computation, San Diego, CA
(2000) 84–88
9. 
H. Yoshida, K. Kawata, Y. Fukuyama, and Y. Nakanishi.  A particle swarm optimization
for reactive power and voltage control considering voltage stability. In G. L. Torres and A.
P. Alves da Silva, Eds., Proc. Int. Conf. on Intelligent System Application to Power Sys-
tems, Rio de Janeiro, Brazil (1999) 117–121
10. C. O. Ouique, E. C. Biscaia, and J. J. Pinto. The use of particle swarm optimization for
dynamical analysis in chemical processes. Computers and Chemical Engineering 26
(2002) 1783–1793
11. Y. Shi and R. Eberhart. Parameter selection in particle swarm optimization. Proc. 7
th An-
nual Conf. on Evolutionary Programming (1998) 591–600
12. Y. Shi, and Eberhart, R. Experimental study of particle swarm optimization. Proc.
SCI2000 Conference, Orlando, FL (2000)
13. Y. Shi and R. Eberhart. Fuzzy adaptive particle swarm optimization. 2001. Proceedings of
the 2001 Congress on Evolutionary Computation, vol. 1 (2001) 101–106
14. X. Xie, W. Zhang, and Z. Yang.  A dissipative particle swarm optimization.  Proceedings
of the 2002 Congress on Evolutionary Computation, Volume: 2 (2002) 1456–1461
15. J. Kennedy. The particle swarm: social adaptation of knowledge. Proc. IEEE International
Conference on Evolutionary Computation (Indianapolis, Indiana), IEEE Service Center,
Piscataway, NJ (1997) 303–308
16. P. J. Angeline. Using selection to improve particle swarm optimization. IEEE International
Conference on Evolutionary Computation, Anchor age, Alaska, May (1998) 4–9
17. J. Kennedy, R.C. Eberhart, and Y. Shi. Swarm Intelligence, San Francisco: Morgan
Kaufmann Publishers (2001)

Clustering and Dynamic Data Visualization with
Artiﬁcial Flying Insect
S. Aupetit1, N. Monmarch´e1, M. Slimane1, C. Guinot2, and G. Venturini1
1 Laboratoire d’Informatique de l’Universit´e de Tours,
´Ecole Polytechnique de l’Universit´e de Tours - D´epartement Informatique
64, Avenue Jean Portalis, 37200 Tours, France.
{monmarche,oliver,venturini}@univ-tours.fr
sebastien.aupetit@etu.univ-tours.fr
2 CE.R.I.E.S.,
20 rue Victor Noir, 92521 Neuilly sur Seine C´edex.
christiane.guinot@ceries-lab.com
Abstract. We present in this paper a new bio-inspired algorithm that
dynamically creates and visualizes groups of data. This algorithm uses
the concepts of ﬂying insects that move together in complex manner with
simple local rules. Each insect represents one datum. The insect moves
aim at creating homogeneous groups of data that evolve together in a
2D environment in order to help the domain expert to understand the
underlying class structure of the data set.
1
Introduction
Many clustering algorithms are inspired from biology like genetic algorithms [1,
2] or artiﬁcial ant algorithms [3,4] for instance. The main advantages of these
algorithms are that they are distributed and they generally do not need an initial
partition of data as it can be often needed. This study takes its inspiration from
diﬀerent kinds of animals that use social behavior for their movement (clouds of
insects, schooling ﬁshes or bird ﬂocks) that have not been applied and extensively
tested on clustering problems yet. Models of these behaviors that can be found
in literature are characterized by a “swarm intelligence” which consists in the
appearance of macroscopic patterns obtained with simple entities obeying to
simple local coordination rules [6,5].
2
Principle
In this work, we use the notion of ﬂying insect/entity in order to treat dynamic
visualization and data clustering problems. The main idea is to consider that
insects represent data to cluster and that they move following local behavior
rule in a way that, after few movements, homogeneous insect clusters appear
and move together. Cluster visualization allow the domain expert to perceive
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 140–141, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Clustering and Dynamic Data Visualization with Artiﬁcial Flying Insect
141
the partitioning of the data. Another algorithm can analyze these clusters and
give precise classiﬁcation as output.
An example can be observed in the following pictures :
(a)
(b)
(c)
where (a) corresponds to the initial step for 150 objects (Iris dataset), (b) and
(c) are screen shots showing the dynamic formation of clusters.
3
Conclusion
This work has demonstrated that ﬂying animals can be used to visualize data
structure in a dynamic way. Future work will concerns an application of these
principles to present results obtained by a search engine.
References
1. R. Cucchiara. Analysis and comparison of diﬀerent genetic models for the clustering
problem in image analysis. In R.F. Albrecht, C.R. Reeves, and N.C. Steele, editors,
International Conference on Artiﬁcial Neural Networks and Genetic Algorithms,
pages 423–427. Springer-Verlag, 1993.
2. D.R. Jones and M.A. Beltrano. Solving partitioning problems with genetic algo-
rithms. In Belew and Booker, editors. Fourth International Conference on Genetic
Algorithms. Morgan Kaufmann, San Mateo, CA, 1991., pages 442–449.
3. E.D. Lumer and B. Faieta. Diversity and adaptation in populations of clustering
ants. In D. Cliﬀ, P. Husbands, J.A. Meyer, and Stewart W., editors, Proceedings
of the Third International Conference on Simulation of Adaptive Behavior, pages
501–508. MIT Press, Cambridge, Massachusetts, 1994.
4. N. Monmarch´e, M. Slimane, and G. Venturini. On improving clustering in numerical
databases with artiﬁcial ants. In D. Floreano, J.D. Nicoud, and F. Mondala, editors,
5th European Conference on Artiﬁcial Life (ECAL’99), Lecture Notes in Artiﬁcial
Intelligence, volume 1674, pages 626–635, Swiss Federal Institute of Technology,
Lausanne, Switzerland, 13-17 September 1999. Springer-Verlag.
5. G. Proctor and C. Winter. Information ﬂocking: Data visualisation in virtual worlds
using emergent behaviours. In J.-C. Heudin, editor, Proc. 1st Int. Conf. Virtual
Worlds, VW, volume 1434, pages 168–176. Springer-Verlag, 1998.
6. C. W. Reynolds. Flocks, herds, and schools: A distributed behavioral model. Com-
puter Graphics (SIGGRAPH ’87 Conference Proceedings), 21(4):25–34, 1987.

Ant Colony Programming for Approximation
Problems⋆
Mariusz Boryczka1, Zbigniew J. Czech2, and Wojciech Wieczorek1
1 University of Silesia, Sosnowiec, Poland, {boryczka,wieczor}@us.edu.pl
2 University of Silesia, Sosnowiec and Silesia University of Technology, Gliwice,
Poland, zjc@us.edu.pl
Abstract. A method of automatic programming, called genetic pro-
gramming, assumes that the desired program is found by using a genetic
algorithm. We propose an idea of ant colony programming in which in-
stead of a genetic algorithm an ant colony algorithm is applied to search
for the program. The test results demonstrate that the proposed idea
can be used with success to solve the approximation problems.
1
Introduction
Approximation problems which consist in a choice of an optimum function from
some class of functions are considered. While solving an approximation problem
by ant colony programming the desired approximating function is built as a
computer program, i.e. a sequence of assignment instructions which evaluates
the function.
2
Ant Colony Programming for Approximation Problems
The ant colony programming system consists of: (a) the nodes of set N of graph
G = (N, E) which represent the assignment instructions out of which the desired
program is built; the instructions comprise the terminal symbols, i.e. constants,
input and output variables, temporary variables and functions; (b) the tabu
list which holds the information about the path pursued in the graph; (c) the
probability of moving ant k located in node r to node s in time t which is equal
to:
Here ψs = 1/e, where e is an approximation error given by the program while
expanded by the instruction represented by node s ∈N.
⋆This work was carried out under the State Committee for Scientiﬁc Research (KBN)
grant no 7 T11C 021 21.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 142–143, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Ant Colony Programming for Approximation Problems
143
3
Test Results
The genetic (GP) and ant colony programming (ACP) methods to solve approx-
imation problems were implemented and compared on the real-valued function
of three variables:
t = (1 + x0.5 + y−1 + z−1.5)2
(1)
where x, y, z ∈[1.0, 6.0]. The experiments were conducted in accordance to
the learning model. Both methods were ﬁrst run on a training set, T, of 216
data items, and then on a testing set, S, of 125 data items. The results of the
Table 1. (a) The Average Percentage Error, eT , eS, and the Standard Deviation, σT ,
σS, for the Training, T, and Testing, S, Data; (b) Comparison of Results
a)
Method
eT
σT
eS
σS
100 experiments, 15 min each
GP
1.86 1.00 2.15 1.35
ACP
6.81 2.60 6.89 2.61
10 experiments, 1 hour each
GP
1.07 0.58 1.18 0.60
ACP
2.60 2.17 2.70 2.28
b)
Model/method
eT
eS
GMDS model
4.70 5.70
ACP (this work) 2.60 2.70
Fuzzy model 1
1.50 2.10
GP (this work)
1.07 1.18
Fuzzy model 2
0.59 3.40
FNN type 1
0.84 1.22
FNN type 2
0.73 1.28
FNN type 3
0.63 1.25
M-Delta
0.72 0.74
Fuzzy INET
0.18 0.24
Fuzzy VINET
0.08 0.18
experiments are summarized in Table 1. It can be seen (Table 1a) that the
average percentage errors (eT and eS) for the ACP method are larger than
those for the GP method. The range of this error for the training process and
100 experiments was 0.0007...9.9448 for the ACP method, and 0.0739...6.6089
for the GP method. The error 0.0007 corresponds to a perfect ﬁt solution with
respect to function (1). Such a solution was found 8 times in the series of 100
experiments by the ACP method, and was not found at all by the GP method.
Table 1b compares our GP and ACP experimental results (for function (1)) with
the results cited in the literature.
4
Conclusions
The idea of ant colony programming for solving approximation problems was
proposed. The test results demonstrated that the method is eﬀective. There are
still some issues which remain to be investigated. The most important is the
issue of establishing the set of instructions, N, which deﬁnes the solution space
explored by the ACP method. On the one hand this set should be as small as
possible so that the searching process is fast. On the other hand it should be
large enough so that the large number of local minima, and hopefully the global
minimum, are encountered.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 144–145, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Long-Term Competition for Light in Plant Simulation
Claude Lattaud
Artificial Intelligence Laboratory of Paris V University (LIAP5)
45, rue des Saints Pères
75006 Paris, France
Claude.Lattaud@math-info.univ-paris5.fr
Abstract. This paper presents simulations of long-term competition for light
between two plant species, oaks and beeches. These artificial plants, evolving in
a 3D environment, are based on a multi-agent model. Natural oaks and beeches
develop two different strategies to exploit light. The model presented in this
paper uses these properties during the plant growth. Most of the results are
close to those obtained in natural conditions on long-term evolution of forests.
1   Introduction
The study of ecosystems is now deeply related to economic resources and their
comprehension becomes an important field of research since the last century. P.
Dansereau in [1] says that “An ecosystem is a limited space where resource recycling
on one or several trophic levels is performed by a lot of evolving agents, using
simultaneously and successively mutually compatible processes that generate long or
short term usable products”. This paper tries to focus on one aspect of this co-
evolution in the ecosystem, the competition for a resource between two plant species.
In nature, most of the plants compete for light. Photosynthesis being one of the
main factors for plant growth, trees, in particular, tend to develop several strategies to
optimize the quantity of light they receive. This study is based on the observation of a
French forest composed mainly of oaks and beeches. In [2] B. Boullard says : “In the
forest of Chaux […] stands were, in 1824, composed of 9/10 of oaks and 1/10 of
beeches. In 1964, proportions were reversed […] Obviously, under the oak grove of
temperate countries, the decrease of light can encourage the rise of beeches to the
detriment of oaks, and slowly the beech grove replaces the oak grove”.
2   Plant Modeling
The plant model defined in this paper is based on multi-agent systems [3]. The main
idea of this approach is to decentralize all the decisions and processes on several
autonomous entities, the agents, able to communicate together, instead of on a unique
super-entity. A plant is then determined by a set of agents, representing the plant
organs, which allow the emergence of plant global behaviors by their cooperation.

Long-Term Competition for Light in Plant Simulation         145
Each of these organs have their own mineral and carbon storage with a capacity
proportional to its volume. These storages stock plant resources and are used for its
survival and its growth at each stage.
During each stage, an organ receives and stocks resources,
directly from ground minerals or sunlight, or indirectly from
other organs, and uses them for its survival, organic functions
and development. The organ is then able to convert carbon
and mineral resources in structural mass for the growth
process or to distribute them to nearby organs.
The simulations presented in this paper focus on the light
resource. Photosynthesis is the process by which the plants
increase their carbon storage by converting light they receive
from the sky. Each point of the foliage can receive light from
the sky according to three directions in order to simulate a
simple daily sun movement.
As simulations are performed on the long-term, a reproduction process has been
developed. At each stage, if a plant reaches its sexual maturity, the foliage assigns a
part of its resources to its seeds, then eventually spreads them in the environment.
All the plants are disposed in a virtual environment, defined as a particular agent,
composed with the ground and the sky. The environment manages synchronously all
the interactions between plants, like mineral extraction from the ground, competition
for light and physical encumbrance.
3   Conclusion
Two sets of simulations were performed to understand the evolution of oak and beech
populations. They exhibit a global behavior of plant communities close to that of
those observed in nature : oaks competing for light against beeches slowly disappear.
Artificial oaks develop a short-term strategy to exploit light, while artificial beeches
tend to develop a long-term strategy. The main factor to be considered in this
competition was the foliage and stack properties of virtual plants, but simulation
showed that another unexpected phenomenon occurred. The competition for light did
not only happen in altitude at the foliage level, but also on the ground where seeds
grow. Shadow generated by plants played a capital role in the seed growth dynamics,
especially in the seed sleeping phase. In this competition, beeches always outnumber
oaks on the long-term.
References
1. Dansereau, P. : Repères «Pour une éthique de l'environnement avec une méditation sur la
paix.» In Bélanger, R., Plourde S. (eds.) : Actualiser la morale: mélanges offerts à René
Simon, Les Éditions Cerf, Paris (1992).
2. Boullard, B.: «Guerre et paix dans le règne végétal», Ed. Ellipse (1990).
3. Ferber, J., « Les systèmes multi-agents », Inter Editions, Paris (1995).
Fig. 1. Plant organs

Using Ants to Attack a Classical Cipher
Matthew Russell, John A. Clark, and Susan Stepney
Department of Computer Science, University of York, York, YO10 5DD, U.K.
{matthew,jac,susan}@cs.york.ac.uk
1
Introduction
Transposition ciphers are a class of historical encryption algorithms based on
rearranging units of plaintext according to some ﬁxed permutation which acts
as the secret key. Transpositions form a building block of modern ciphers, and
applications of metaheuristic optimisation techniques to classical ciphers have
preceded successful results on modern-day cryptological problems. In this pa-
per we describe the use of Ant Colony Optimisation (ACO) for the automatic
recovery of the key, and hence the plaintext, from only the ciphertext.
2
Cryptanalysis of Transposition Ciphers
The following simple example of a transposition encryption uses the key 31524:
31524 31524 31524 31524 31524
THEQU ICKBR OWNFO XJUMP EDXXX
⇒HQTUE CBIRK WFOON JMXPJ DXEXX
Decryption is straightforward with the key, but without it the cryptanalyst has
a multiple anagramming problem, namely rearranging columns to discover the
plaintext:
H
C
W
J
D
Q
B
F
M
X
T
I
O
X
E
U
R
O
P
X
E
K
N
U
X
⇒
T
I
O
X
E
H
C
W
J
D
E
K
N
U
X
Q
B
F
M
X
U
R
O
P
X
Traditional cryptanalysis has proceeded by using a statistical heuristic for the
likelihood of two columns being adjacent. Certain pairs of letters, or bigrams,
occur more frequently than others. For example, in English, ‘TH’ is very common.
Using some large sample of normal text an expected frequency for each bigram
can be inferred. Two columns placed adjacently create several bigrams. The
heuristic dij is deﬁned as the sum of their probabilities; that is, for columns i
and j, dij = 
r P(irjr), where ir and jr denote the rth letter in the column
and P(xy) is the standard probability for the bigram “xy”.
Maximising the sum of dij over a permutation of the columns can be enough
to reconstruct the original key, and a simple greedy algorithm will often suﬃce.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 146–147, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Using Ants to Attack a Classical Cipher
147
However, the length of the ciphertext is critical as short ciphertexts have large
statistical variation, and two separate problems eventually arise: (1) the greedy
algorithm fails to ﬁnd the global maximum, and, more seriously, (2) the global
maximum does not correspond to the correct key.
In order to attempt cryptanalysis on shorter texts, a second heuristic can
be employed, based on counting dictionary words in the plaintext, weighted by
their length. This typically solves problem (2) for much shorter ciphertexts, but
the ﬁtness landscape it deﬁnes is somewhat discontinuous and diﬃcult to search,
while the original heuristic yields much useful, albeit noisy, information.
3
Ants for Cryptanalysis
A method has been found that successfully deals with problems (1) and (2),
combining both heuristics using the ACO algorithm Ant System [2]. In the ACO
algorithm, ants construct a solution by walking a graph with a distance matrix,
reinforcing with pheromone arcs that correspond to better solutions. An ant’s
choice at each node is aﬀected by both the distance measure and the amount of
pheromone deposited in previous iterations.
For our cryptanalysis problem the graph nodes represent columns, and the
distance measure used in the ants’ choice of path is given by the dij bigram-
based heuristic, essentially yielding a maximising Asymmetric Travelling Sales-
men Problem. The update to the pheromone trails, however, is determined by the
dictionary heuristic, not the usual sum of the bigram distances. Therefore both
heuristics have inﬂuence on an ant’s decision at a node: the bigram heuristic is
used directly, and the dictionary heuristic provides feedback through pheromone.
In using ACO with these two complementary heuristics, we found that less
ciphertext was required to completely recover the key, compared both to a greedy
algorithm, and also to other metaheuristic search methods previously applied to
transposition ciphers: genetic algorithms, simulated annealing and tabu search
[4,3,1]. It must be noted that these earlier results make use of only bigram fre-
quencies, without a dictionary word count, and they could conceivably be modi-
ﬁed to use both heuristics. However, ACO provides an elegant way of combining
the two heuristics.
References
1. Andrew Clark. Optimisation Heuristics for Cryptology. PhD thesis, Queensland
University of Technology, 1998.
2. Marco Dorigo, Vittorio Maniezzo, and Alberto Colorni. The Ant System: Optimiza-
tion by a colony of cooperating agents. IEEE Transactions on Systems, Man, and
Cybernetics Part B: Cybernetics, 26(1):29–41, 1996.
3. J. P. Giddy and R. Safavi-Naini. Automated cryptanalysis of transposition ciphers.
The Computer Journal, 37(5):429–436, 1994.
4. Robert A. J. Matthews. The use of genetic algorithms in cryptanalysis. Cryptologia,
17(2):187–201, April 1993.

Comparison of Genetic Algorithm and Particle
Swarm Optimizer When Evolving a Recurrent
Neural Network
Matthew Settles1, Brandon Rodebaugh1, and Terence Soule1
Department of Computer Science, University of Idaho,
Moscow, Idaho U.S.A
Abstract. This paper compares the performance of GAs and PSOs in
evolving weights of a recurrent neural network. The algorithms are tested
on multiple network topologies. Both algorithms produce successful net-
works. The GA is more successful evolving larger networks and the PSO
is more successful on smaller networks.1
1
Background
In this paper we compare the performance of two population based algorithms, a
genetic algorithm (GA) and particle swarm optimization (PSO), in training the
weights of a strongly recurrent artiﬁcial neural network (RANN) for a number of
diﬀerent topologies. The goal is to develop a recurrent network that can repro-
duce the complex behaviors seen in biological neurons [1]. The combination of a
strongly connected recurrent network and an output with a long period makes
this a very diﬃcult problem. Previous research in using evolutionary approaches
to evolve RANNs have either evolved the topology and weights or used a hybrid
algorithm that evolved the topology and used a local search or gradient descent
search for the weights (see for example [2]).
2
Experiment and Results
Our goal is to evolve a network that produces a simple pulsed output when an
activation ‘voltage’ is applied to the network’s input. The error is the sum of
the absolute value of the diﬀerence between the desired output and the actual
output at each time step plus a penalty (0.5) if the slope of the desired output
diﬀers in direction from the slope of the actual output.
The neural network is strongly connected with a single input node and a
single output node. The nodes use a symmetric sigmoid activation function. The
activation levels are calculated synchronically.
The GA uses a chromosomes consisting of real values. Each real value corre-
sponds to the weight between one pair of nodes.
1 This work supported by NSF EPSCoR EPS-0132626. The experiments were per-
formed on a Beowulf cluster built with funds from NSF grant EPS-80935 and a
generous hardware donation from Micron Technologies.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 148–149, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Comparison of Genetic Algorithm and Particle Swarm Optimizer
149
The GA is generational, 250 generations, 500 individuals per generation. The
two best individuals are copied into the next generation (elitism). Tournament
selection is used, with a tournament of size 3. The initial weights were randomly
chosen in the range (-1.0,1.0). The mutation rate is 1/(LN)2. Mutation changes
a weight by up to 25% of the weight’s original value. Crossover is applied to two
individuals at the same random (non-input) node. The crossover rate is 0.8.
The PSO uses position and velocity vectors which refer to the particles’
position and velocity within the search space. They are real valued vectors, with
one value for each network weight. The PSO is run for 250 generations on a
population of 500 particles. The initial weights were randomly chosen in the
range (-1.0,1.0). The position vector was allowed to explore values in the range
of (-2.0,2.0). The inertial weight is reduced linearly from 0.9 to 0.4 each epoch [3].
Tables 1 and 2 show the number of successful trials out of the ﬁfty. Successful
trials evolve a network that produces periodic output with the desired frequency.
Unsuccessful trials fail to produce periodic behavior.
Both the GA and PSO perform well for medium sized networks. The GAs
optimal network size is around 3-4 layers with 5 nodes per layer. The PSOs
optimal network is approximately 2x5. The GA is more successful with larger
networks, whereas the PSO is more successful with smaller networks. A two-
tailed z-test (α of 0.05) conﬁrms that these diﬀerences are statistically signiﬁcant.
Table 1. Number of successful trials
(out of ﬁfty) trained using GA.
Layers
1
2
3
4
1 Node/Layer
0
0
0
0
3 Nodes/Layer
0
17
44
49
5 Nodes/Layer
5
41
50
50
7 Nodes/Layer
22
48
46
41
9 Nodes/Layer
36
49
40
–
Table 2. Number of successful trials
(out of ﬁfty) trained using PSO.
Layers
1
2
3
4
1 Node/Layer
0
4
23
38
3 Nodes/Layer
17
43
49
47
5 Nodes/Layer
39
50
40
32
7 Nodes/Layer
46
46
36
19
9 Nodes/Layer
49
41
17
–
3
Conclusions and Future Work
In this paper we demonstrated that GA and PSO can be used to evolve the
weights of strongly recurrent networks to produce long period, pulsed output
signals from a constant valued input. Our results also show that both approaches
are eﬀective for a variety of diﬀerent network topologies. Future work will include
evolving a single network that can produce a variety of biologically relevant
behaviors depending on the input signals.
References
1. Shepherd, G.M.: Neurobiology. Oxford University Press, New York, NY (1994)
2. Angeline, P.J., Saunders, G.M., Pollack, J.P.: An evolutionary algorithm that con-
structs recurrent neural networks. IEEE Transactions on Neural Networks 5 (1994)
54–65
3. Kennedy, J., Eberhart, R.: Swarm Intelligence. Morgan Kaufmann Publishers, Inc.,
San Francisco, CA (2001)

Adaptation and Ruggedness in an Evolvability
Landscape
Terry Van Belle and David H. Ackley
Department of Computer Science
University of New Mexico
Albuquerque, New Mexico, USA
{vanbelle, ackley}@cs.unm.edu
Evolutionary processes depend on both selection—how ﬁt any given individual
may be, and on evolvability—how and how eﬀectively new and ﬁtter individ-
uals are generated over time. While genetic algorithms typically represent the
selection process explicitly by the ﬁtness function and the information in the
genomes, factors aﬀecting evolvability are most often implicit in and distributed
throughout the genetic algorithm itself, depending on the chosen genomic rep-
resentation and genetic operators. In such cases, the genome itself has no direct
control over evolvability except as determined by its ﬁtness.
Researchers have explored mechanisms that allow the genome to aﬀect not
only ﬁtness but also the distribution of oﬀspring, thus opening up the potential
of evolution to improve evolvability. In prior work [1] we demonstrated that eﬀect
with a simple model focusing on heritable evolvability in a changing environment.
In our current work [2], we introduce a simple evolvability model, similar in
spirit to those of Evolution Strategies. In addition to genes that determine the
ﬁtness of the individual, in our model each individual contains a distinct set of
‘evolvability genes’ that determine the distribution of that individual’s potential
oﬀspring. We also present a simple dynamic environment that provides a canon-
ical ‘evolvability opportunity’ by varying in a partially predictable manner.
That evolution might lead to improved evolvability is far from obvious, be-
cause selection operates only on an individual’s current ﬁtness, but evolvability
by deﬁnition only comes into play in subsequent generations. Two similarly-ﬁt
individuals will contribute about equally to the next generation, even if their
evolvabilities vary drastically. Worse, if there is any ﬁtness cost associated with
evolvability, more evolvable individuals might get squeezed out before their ad-
vantages could pay oﬀ. The basic hope for increasing evolvability is circumstances
where weak selective pressure allows diverse individuals to contribute oﬀspring
to the next generation, and then those individuals with better evolvability in the
current generation will tend to produce oﬀspring that will dominate in subse-
quent ﬁtness competitions. In this way, evolvability advantages in the ancestors
can lead to ﬁtness advantages in the descendants, which then preserves the in-
herited evolvability mechanisms.
A common tool for imagining evolutionary processes is the ﬁtness landscape,
a function that maps the set of all genomes to a single-dimension real ﬁtness
value. Evolution is seen as the process of discovering peaks of higher ﬁtness,
while avoiding valleys of low ﬁtness. If we can derive a scalar value that plau-
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 150–151, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Adaptation and Ruggedness in an Evolvability Landscape
151
sibly captures the notion of evolvability, we can augment the ﬁtness landscape
conception with an analogous notion of an evolvability landscape. With our al-
gorithm possessing variable and heritable evolvabilities, it is natural to wonder
what the evolution of a population will look like on the evolvability landscape
as well as the ﬁtness landscape.
We adopt as an evolvability metric the online ﬁtness of a population: The
average ﬁtness value of the best of population from the start of the run until
a ﬁxed number of generations have elapsed. The online ﬁtness of a population
with a ﬁxed evolvability gives us the ‘height’ of the evolvability landscape at
that point. In cases where evolvability is adaptive, we envision the population
moving across the evolvability landscape as evolution proceeds, which in turn
modiﬁes the ﬁtness landscape. Figures 1 and 2 show some of our results.
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
0
1
10
100
1000
10000
Online Fitness
Generation
Fixed/Target
Adaptive
Fixed/Independent
Fig. 1. Fixed/Independent is standard
GA evolvability, in which all gene muta-
tions are independent. Fixed/Adaptive,
with
an
evolvable
evolvability,
does
signiﬁcantly better. Fixed/Target does
best, but assumes advance knowledge of
the environmental variation pattern.
-0.7
-0.6
-0.5
-0.4
-0.3
-0.2
-0.1
0
1
10
100
1000
10000
Online Fitness
Generation
Fixed/Target
Fixed/NearMiss1
Fixed/NearMiss2
Fig. 2. Evidence of a ‘cliﬀ’ in the evolv-
ability landscape. Fixed evolvabilities
that are close to optimal, but not ex-
act, can produce extremely poor perfor-
mance.
Acknowledgments. This research was supported in part by DARPA contract
F30602-00-2-0584, and in part by NSF contract ANI 9986555.
References
[1] Terry Van Belle and David H. Ackley. Code factoring and the evolution of evolv-
ability. In Proceedings of GECCO-2002, New York City, July 2002. AAAI Press.
[2] Terry Van Belle and David H. Ackley. Adaptation and ruggedness in an evolv-
ability landscape. Technical Report TR-CS-2003-14, University of New Mexico,
Department of Computer Science, 2003.
http://www.cs.unm.edu/colloq-bin/tech reports.cgi?ID=TR-CS-2003-14.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 152–153, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Study Diploid System by a Hamiltonian Cycle Problem
Algorithm
Dong Xianghui and Dai Ruwei
System Complexity Research Center
Institute of Automation, Chinese Academy of Science, Beijing 100080
xianghui.dong@mail.ia.ac.cn
Abstract. Complex representation in Genetic Algorithms and pattern in real
problems limits the effect of crossover to construct better pattern from sporadic
building blocks. Instead of introducing more sophisticated operator, a diploid
system was designed to divide the task into two steps: in meiosis phase,
crossover was used to break two haploid of same individual into small units and
remix them thoroughly. Then better phenotype was rebuilt from diploid of
zygote in development phase. We introduced a new representation for
Hamiltonian Cycle Problem and implemented an algorithm to test the system.
Our algorithm is different from conventional GA in several ways:
 The edges of potential solution are directly represented without coding.
 Crossover is only part of meiosis, working between diploid of same individual.
 Instead of mutation, the population size guarantees the diversity of genes.
Since Hamiltonian Cycle Problem is a NP-Complete problem, we can design a
search algorithm for Non-deterministic Turing Machine.
Table 1. A graph with a Hamiltonian Cycle of (0, 3, 2, 1, 4, 5, 0), and two representation of
Hamiltonian cycle
To find the Hamiltonian Cycle, our Non-deterministic Turing Machine will:

Study Diploid System by a Hamiltonian Cycle Problem Algorithm
153
 Check the first row. Choose a vertex from vertices connected to current first row
vertex. These two vertices designate an edge.
 Process other rows in the same way. If there is a Hamiltonian Cycle and every
choice is right, these n edges construct a valid cycle.
Therefore, we designed an evolutionary algorithm to simulate it approximately:
 Every individual represents a group of n edges got by a selecting procedure made
by random or genetic operators.
 The fitness of an individual is the maximal length of contiguous path can extend
from start in the edge group.
Fig. 1. Expression of genotype. Dashed edges are edges in genotype. Numbers in edges note
the order of expression. The path terminated in 4-0 because of repetition
Since Hamiltonian Cycle Problem highly depend on the internal relation among
vertices, it will be very hard for crossover to keep the validity of path and pattern
formed at the same time. If edges are represented in path order, crossover may
produce edge group with duplicate vertices; if edges are represented in the same fixed
order, the low-order building blocks cannot be kept after crossover. Fortunately, the
meiosis and diploid system in biology provide a solution for this problem. It can be
divided into two steps:
1. Meiosis. Every chromosome got in gamete can come from either haploidy.
Crossover and linkage occurred between corresponding chromosomes.
2. Diploid expression. No matter how thoroughly recombination had been conduct in
meiosis, broken patterns can be recovered, and a better phenotype can be obtained
with two options in every alleles. Our algorithm tests all the possible options in
new searching branch and keeps the maximal contiguous path. The search space is
not too much because many branches will be pruned for repeated vertex. Of
course, we limited the size of searching branches pool.
It was proved that the algorithm usually solves graph with 16 vertices immediately.
For larger scale (1000 ~ 5000) it had steady search capability only restrained by
computing resource (mainly in space, not in time). Java codes and data are available
from http://ai.ia.ac.cn/english/people/draco/index.htm.
Acknowledgments. The authors are very grateful to Prof. John Holland for
invaluable encouragement and discussions.

A Possible Mechanism of Repressing Cheating
Mutants in Myxobacteria
Ying Xiao and Winfried Just
Department of Mathematics, Ohio University, Athens, OH 45701, U.S.A.
Abstract. The formation of fruiting bodies by myxobacteria colonies
involves altruistic suicide by many individual bacteria and is thus vulner-
able to exploitation by cheating mutants. We report results of simulations
that show how in a structured environment with patchy distribution of
cheating mutants the wild type might persist.
This work was inspired by experiments on myxobacteria Myxococcus xanthus
reported in [1]. Under adverse environmental conditions individuals in an M.
xanthus colony aggregate densely and form a raised “fruiting body” that consists
of a stalk and spores. During this process, many cells commit suicide in order to
form the stalk. This “altruistic suicide” enables spore formation by other cells.
When conditions become favorable again, the spores will be released and may
start a new colony.
Velicer et al. studied in [1] some mutant strains that were deﬁcient in their
ability to form fruiting bodies and had lower motility but higher growth rates
than wild-type bacteria. When mixed with wild-type bacteria, these mutant
strains were signiﬁcantly over-represented in the spores in comparison with their
original frequency. Thus these mutants are cheaters in the sense that they reap
the beneﬁts of the collective action of the colony while paying a disproportionally
low cost of altruistic suicide during fruiting body formation. The authors of [1]
ask which mechanism insures that the wild-type behavior of altruistic suicide is
evolutionarily stable against invasion by cheating mutants.
We conjecture that a clustered distribution of mutants at the time of sporu-
lation events could be a suﬃcient mechanism for repressing those mutants. One
possible source of such clustering could be lower motility of mutants.
A detailed description of the program written to test this conjecture, the
source code, as well as all output ﬁles, can be found at the following URL:
www.math.ohiou.edu/˜just/Myxo/. The program simulates growth, develop-
ment, and evolution of ten M. xanthus colonies over 500 seasons (sporulation
events). Each season consists on average of 1,000 generations (cell divisions).
Each colony is assumed to live on a square grid, and growth of the colony is
modeled by expansion into neighboring grid cells. At any time during the sim-
ulation, each grid cell is characterized by the number of wild-type and mutant
bacteria that it holds. At the end of each season, fruiting bodies are formed in
regions where suﬃciently many wild type bacteria are present. After each sea-
son, the program selects randomly ten fruiting bodies formed in this season and
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 154–155, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

A Possible Mechanism of Repressing Cheating Mutants in Myxobacteria
155
seeds the new colonies with a mix of bacteria in the same proportions as the
proportions found in the fruiting body that was chosen for reproduction.
The proportion of wild-type bacteria in excess of carrying capacity that move
to neighboring grid cells in the expansion step was set to 0.024. We run ten
simulations each for parameter settings where mutants in excess of carrying
capacity move to neighboring grid cells at rates of 0.006, 0.008, 0.012, and 0.024
and grow 1%, 1.5%, or 2% faster than wild-type bacteria. In the following table,
the column headers show the movement rates for the mutants, row headers show
by how much mutants grow faster than wild type bacteria, and the numbers
in the body of the table show how many of the simulations in each run of ten
simulations reached the cutoﬀof 500 seasons without terminating due to lack of
fruiting body formation.
Table 1. Number of simulations that run for 500 seasons
0.006
0.008
0.012
0.024
1%
9
7
5
0
1.5%
6
5
2
0
2%
5
4
2
0
These results show that for many of our parameter settings, wild-type bac-
teria can successfully propagate in the presence of cheating mutants. Successful
propagation of wild-type bacteria over many seasons is more likely the less the
discrepancy in growth rates of mutants and wild type is, and the less mobile the
mutants are. This can be considered as a proof of principle for our conjecture.
All our simulations in which mutants have the same motility as wild-type
bacteria terminated prematurely due to lack of fruiting body formation. The
authors of [2] report that motility of mutant strains that are deﬁcient in their
ability to form fruiting bodies can be (partially) restored in the laboratory. If
such mutants do occur in nature, then our ﬁndings suggest that another defense
mechanism is necessary for the wild-type bacteria to prevail against them.
References
1. Velicer, G. J., Kroos, L., Lenski, R. E.: Developmental cheating in the social bac-
terium Myxococcus xanthus. Nature 404 (2000) 598–601.
2. Velicer, G. J., Lenski, R. E., Kroos, L.: Rescue of Social Motility Lost during Evolu-
tion of Myxococcus xanthus in an Asocial Environment. J. Bacteriol. 184(10) (2002)
2719–2727.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 156–157, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Tour Jeté, Pirouette: Dance Choreographing by
Computers
Tina Yu1 and Paul Johnson2
1 ChevronTexaco Information Technology Company
6001 Bollinger Canyon Road
San Ramon, CA 94583
tiyu@chevrontexaco.com
http://www.improvise.ws
2 Department of Political Science
University of Kansas
Lawrence, Kansas 66045
pauljohn@ukans.edu
http://lark.cc.ku.edu/~pauljohn
Abstract. This project is a “proof of concept” exercise intended to demonstrate
the workability and usefulness of computer-generated choreography. We have
developed a framework that represents dancers as individualized computer ob-
jects that can choose dance steps and move about on a rectangular dance floor.
The effort begins with the creation of an agent-based model with the Swarm
simulation toolkit. The individualistic behaviors of the computer agents can cre-
ate a variety of dances, the movements and positions of which can be collected
and animated with the Life Forms software.  While there are certainly many ad-
ditional elements of dance that could be integrated into this approach, the initial
effort stands as evidence that interesting, useful insights into the development of
dances can result from an integration of agent-based models and computerized
animation of dances.
1   Introduction
Dance might be one of the most egoistic art forms ever created. This is partly due to
the fact that human bodies are highly unique. Moreover, it is very difficult to record
dance movements in precise details, no matter what method one uses. As a result,
dances are frequently associated with the name of their choreographers, who not only
create but also teach and deliver these art forms with ultimate authority. Such tight
bonds between a dance and its creator gives the impression that dance is an art that can
only be created by humans.
Indeed, creativity is one of the human traits that set us apart from other organisms.
Random House Unabridged Dictionary defines creativity as “the ability to transcend
traditional ideas, rules, patterns, relationships or the like, and to create meaningful
new ideas, forms, methods, interpretations, etc.,” With the ability to create, humans

Tour Jeté, Pirouette: Dance Choreographing by Computers         157
carry out the creation process in many different ways. One avenue is trial-and-error. It
starts with an original idea and imagination. Through the process of repeated trying
and learning from the failure, those that are unknown previously can be discovered
and new things created.
Is creativity a quality that belongs to humans only? Do computers have the ability
to create? We approach this question in two steps. First, can computers have original
ideas and imagination? Second, can computers carry out the creation process?
Ideas and imagination seem to be something come and go on their own that no one
has control over. Frequently, we heard artists discussing about where they find their
ideas and what can simulate their imagination. What is computers’ source of ideas and
imagination? One answer is “randomness”; computers can be programmed to generate
as many random numbers as needed. Such random numbers can be mapped into new
possibilities of doing things, hence a source of ideas and imagination.
Creation process is very diverse in that different people have different approaches.
For example, some dance choreographers like to work out the whole piece first and
then teach them to their dancers. Others prefer working with their dancers to generate
new ideas. Which style of creation process that computers can have? One answer is
trial-and-error; computers can be programmed to repeat an operation as many times as
needed. By applying such repetition to new/old ways of doing things, new possibilities
can be discovered.
When equipped with a source of ideas and a process of creation, computers seem to
become creative. This also suggests that computers might be able to create the art
forms of dance. We are interested in computer-generated choreography and the
possibility of incorporating that with human dancers to create a new kind of stage
production. This paper describes the project and reports the progress we have made so
far.
We started the project with a conversation with professional dancers and
choreographers about their views of computer-generated choreography. Based on the
discussion, we selected two computer tools (Swarm and Life Forms) for the project.
We then implemented the “randomness” and “trial-and-error”  abilities in the Swarm
computer software to generate a sequence of dance steps. The music for this dance is
then considered and selected. With a small degree of improvisation (according to the
rhythm of the music), we put the dance sequences in animation. The initial results are
then shown to a dance company’s artistic director. The feedback is very encouraging,
although the piece needs more work to be able to put into production. All of these lead
us to conclude that computer-generated choreography can produce interesting
movements that might lead to a new type of stage production.
The Swarm code: http://lark.cc.ku.edu/~pauljohn/Swarm/MySwarmCode/Dancer.
The Life Forms dance animiation: http://www.improvise.ws/Dance.mov.zip.

Multiobjective Optimization Using Ideas from the
Clonal Selection Principle
Nareli Cruz Cort´es and Carlos A. Coello Coello
CINVESTAV-IPN
Evolutionary Computation Group
Depto. de Ingenier´ıa El´ectrica
Secci´on de Computaci´on
Av. Instituto Polit´ecnico Nacional No. 2508
Col. San Pedro Zacatenco
M´exico, D. F. 07300, MEXICO
nareli@computacion.cs.cinvestav.mx,
ccoello@cs.cinvestav.mx
Abstract. In this paper, we propose a new multiobjective optimization approach
based on the clonal selection principle. Our approach is compared with respect
to other evolutionary multiobjective optimization techniques that are represen-
tative of the state-of-the-art in the area. In our study, several test functions and
metrics commonly adopted in evolutionary multiobjective optimization are used.
Our results indicate that the use of an artiﬁcial immune system for multiobjective
optimization is a viable alternative.
1
Introduction
Most optimization problems naturally have several objectives to be achieved (normally
conﬂicting with each other), but in order to simplify their solution, they are treated as if
they had only one (the remaining objectives are normally handled as constraints). These
problems with several objectives, are called “multiobjective” or “vector” optimization
problems, and were originally studied in the context of economics. However, scientists
and engineers soon realized that such problems naturally arise in all areas of knowledge.
Over the years, the work of a considerable number of operational researchers has pro-
duced a wide variety of techniques to deal with multiobjective optimization problems
[13]. However, it was until relatively recently that researchers realized of the potential
of evolutionary algorithms (EAs) and other population-based heuristics in this area [7].
The main motivation for using EAs (or any other population-based heuristics) in
solving multiobjective optimization problems is because EAs deal simultaneously with
a set of possible solutions (the so-called population) which allows us to ﬁnd several
members of the Pareto optimal set in a single run of the algorithm, instead of having to
perform a series of separate runs as in the case of the traditional mathematical program-
ming techniques [13].Additionally, EAs are less susceptible to the shape or continuity of
the Pareto front (e.g., they can easily deal with discontinuous and concave Pareto fronts),
whereas these two issues are a real concern for mathematical programming techniques
[7,3].
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 158–170, 2003.
c⃝Springer-Verlag Berlin Heidelberg 2003

Multiobjective Optimization Using Ideas from the Clonal Selection Principle
159
Despite the considerable amount of research on evolutionary multiobjective opti-
mization in the last few years, there have been very few attempts to extend certain
population-based heuristics (e.g., cultural algorithms and particle swarm optimization)
[3]. Particularly, the efforts to extend an artiﬁcial immune system to deal with multiob-
jective optimization problems have been practically inexistent until very recently. In this
paper, we precisely provide one of the ﬁrst proposals to extend an artiﬁcial immune sys-
tem to solve multiobjective optimization problems (either with or without constraints).
Our proposal is based on the clonal selection principle and is validated using several test
functions and metrics, following the standard methodology adopted in this area [3].
2
The Immune System
One of the main goals of the immune system is to protect the human body from the
attack of foreign (harmful) organisms. The immune system is capable of distinguishing
between the normal components of our organism and the foreign material that can cause
us harm (e.g., bacteria). Those molecules that can be recognized by the immune system
are called antigens that elicit an adaptive immune response.
The molecules called antibodies play the main role on the immune system response.
The immune response is speciﬁc to a certain foreign organism (antigen).When an antigen
is detected, those antibodies that best recognize an antigen will proliferate by cloning.
This proccess is called clonal selection principle [5].
The new cloned cells undergo high rate somatic mutations or hypermutation. The
main roles of that mutation process are twofold: to allow the creation of new molecular
patterns for antibodies, and to maintain diversity. These mutations experienced by the
clones are proportional to their afﬁnity to the antigen. The highest afﬁnity antibodies
experiment the lowest mutation rates, whereas the lowest afﬁnity antibodies have high
mutation rates. After this mutation process ends, some clones could be dangerous for
the body and should therefore be eliminated.
After these cloning and hypermutation processes ﬁnish, the immune system has
improved the antibodies’ afﬁnity, which results on the antigen neutralization and elimi-
nation.At this point, the immune system must return to its normal condition, eliminating
the excedent cells. However, some cells remain circulating throughout the body as mem-
ory cells. When the immune system is later attacked by the same type of antigen (or
a similar one), these memory cells are activated, presenting a better and more efﬁcient
response. This second encounter with the same antigen is called secondary response.
The algorithm proposed in this paper is based on the clonal selection principle previously
described.
3
Previous Work
The ﬁrst direct use of the immune system to solve multiobjective optimization problems
reported in the literature is the work of Yoo and Hajela [20]. This approach uses a
linear aggregating function to combine objective function and constraint information
into a scalar value that is used as the ﬁtness function of a genetic algorithm. The use of
different weights allows the authors to converge to a certain (pre-speciﬁed) number of

160
N. Cruz Cort´es and C.A. Coello Coello
points of the Pareto front, since they make no attempt to use any speciﬁc technique to
preserve diversity. Besides the limited spread of nondominated solutions produced by
the approach, it is well-known that linear aggregating functions have severe limitations
for solving multiobjective problems (the main one is that they cannot generate concave
portions of the Pareto front [4]). The approach of Yoo & Hajela is not compared to any
other technique.
de Castro and Von Zuben [6] proposed an approach, called CLONALG, which is
based on the clonal selection principle and is used to solve pattern recognition and
multimodal optimization problems. This approach can be considered as the ﬁrst attempt
to solve multimodal optimization problems which are closely related to multiobjective
optimization problems (although in multimodal optimization, the main emphasis is to
preserve diversity rather than generating nondominated solutions as in multiobjective
optimization).
Anchor et al. [1] adopted both lexicographic ordering and Pareto-based selection in
an evolutionary programming algorithm used to detect attacks with an artiﬁcial immune
system for virus and computer intrusion detection. In this case, however, the paper is
more focused on the application rather than on the approach and no proper validation of
the proposed algorithms is provided.
The current paper is an extension of the work published in [2]. Note however, that our
current proposal has several important differences with respect to the previous one. In
our previous work, we attempted to follow the clonal selection principle very closely, but
our results could not be improved beyond a certain point. Thus, we decided to sacriﬁce
some of the biological metaphor in exchange for a better performance of our algorithm.
The result of these changes is the proposal presented in this paper.
4
The Proposed Approach
Our algorithm is the following:
1. The initial population is created by dividing decision variable space into a certain
number of segments with respect to the desired population size. Thus, we generate
an initial population with a uniform distribution of solutions such that every
segment in which the decision variable space is divided has solutions. This is done
to improve the search capabilities of our algorithm instead of just relying on the
use of a mutation operator. Note however, that the solutions generated for the initial
population are still random.
2. Initialize the secondary memory so that it is empty.
3. Determine for each individual in the population, if it is (Pareto) dominated or not.
For constrained problems, determine if an individual is feasible or not.
4. Determine which are the “best antibodies”, since we will clone them adopting the
following criterion:

Multiobjective Optimization Using Ideas from the Clonal Selection Principle
161
– If the problem is unconstrained, then all the nondominated individuals are
cloned.
– If the problem is constrained, then we have two further cases: a) there are
feasible individuals in the population, and b) there are no feasible individuals
in the population. For case b), all the nondominated individuals are cloned.
For case a), only the nondominated individuals that are feasible are cloned
(nondominance is measured only with respect to other feasible individuals in
this case).
5. Copy all the best antibodies (obtained from the previous step) into the secondary
memory.
6. We determine for each of the “best” antibodies the number of clones that we want
to create. We wish to create the same number of clones of each antibody, and we
also that the total number of clones created amounts the 60% of the total population
size used. However, if the secondary memory is full, then we modify this quantity
doing the following:
– If the individual to be inserted into the secondary memory is not allowed access
either because it was repeated or because it belongs to the most crowded region
of objective function space, then the number of clones created is zero.
– When we have an individual that belongs to a cell whose number of solutions
containedisbelowaverage(withrespecttoalltheoccupiedcellsinthesecondary
memory), then the number of clones to be generated is duplicated.
– When we have an individual that belongs to a cell whose number of solutions
contained is above average (with respect to all the occupied cells in the adaptive
grid), then the number of clones to be generated is reduced by half.
7. We perform the cloning of the best antibodies based on the information from the
previous step. Note that the population size grows after the cloning process takes
place. Then, we eliminate the extra individuals giving preference (for survival) to
the new clones generated.
8. A mutation operator is applied to the clones in such a way that the number of
mutated genes in each chromosomic string is equal to the number of decision
variables of the problem. This is done to make sure that at least one mutation occurs
per string, since otherwise we would have duplicates (the original and the cloned
string would be exactly the same).
9. We apply a non-uniform mutation operator to the “worst” antibodies (i.e., those not
selected as “best antibodies” in step 4). The initial mutation rate adopted is high
and it is decreased linearly over time (from 0.9 to 0.3).
10. If the secondary memory is full, we apply crossover to a fraction of its contents (we
proposed 60%). The new individuals generated that are nondominated with respect
to the secondary memory will then be added to it.

162
N. Cruz Cort´es and C.A. Coello Coello
11. After that cloning process ends, the population size is increased. Later on, it is
necessary to reset the population size to its original value.At this point, we eliminate
the excedent individuals, allowing the survival of the nondominated solutions.
12. We repeat this process from step 3 during a certain (predetermined) number of times.
Note that in the previous algorithm there is no distinction between antigen and
antibody. In contrast, in this case all the individuals are considered as antibodies, and
we only distinguish between “better” antibodies and “not so good” antibodies.
The reason for using an initial population with a uniform distribution of solutions
over the allowable range of the decision variables is to sample the search space uniformly.
This helps the mutation operator to explore the search space more efﬁciently.
We apply crossover to the individuals in the secondary memory once this is full so
that we can reach intermediate points between them. Such information is used to improve
the performance of our algorithm.
Note that despite the similarities of our approach with CLONALG, there are im-
portant differences such as the selection strategy, the mutation rate and the number of
clones created by each approach. Also, note that our approach incorporates some op-
erators taken from evolutionary algorithms (e.g., the crossover operator applied to the
elements of the secondary memory (step 10 from our algorithm). Despite that fact, the
cloning process (which involves the use of a variable-size population) of our algorithm
differs from the standard deﬁnition of an evolutionary algorithm.
4.1
Secondary Memory
We use a secondary or external memory as an elitist mechanism in order to maintain
the best solutions found along the process. The individuals stored in this memory are
all nondominated not only with respect to each other but also with respect to all of the
previous individuals who attempted to enter the external memory. Therefore, the external
memory stores our approximation to the true Pareto front of the problem.
In order to enforce a uniform distribution of nondominated solutions that cover the
entire Pareto front of a problem, we use the adaptive grid proposed by Knowles and
Corne [11] (see Figure 1).
Ideally, the size of the external memory should be inﬁnite. However, since this is not
possible in practice, we must set a limit to the number of nondominated solutions that we
want to store in this secondary memory. By enforcing this limit, our external memory
will get full at some point even if there are more nondominated individuals wishing
to enter. When this happens, we use an additional criterion to allow a nondominated
individual to enter the external memory: region density (i.e., individuals belonging to
less densely populated regions are given preference).
The algorithm for the implementation of the adaptive grid is the following:
1. Divide objective function space according to the number of subdivisions set by the
user.
2. For each individual in the external memory, determine the cell to which it belongs.
3. If the external memory is full, then determine which is the most crowded cell.

Multiobjective Optimization Using Ideas from the Clonal Selection Principle
163
0                      1                   2                     3                     4                    5
0
4
3
5
2
1
Space covered by the grid for objective 1
Space covered by the grid for objective 2
The lowest fit individual for objective 1
and the fittest individual for objective 2
The lowest fit individual for objective 2
and the fittest individual for objective 1
Fig. 1. An adaptive grid to handle the secondary memory
– To determine if a certain antibody is allowed to enter the external memory, do
the following:
• If it belongs to the most crowded cell, then it is not allowed to enter.
• Otherwise, the individual is allowed to enter. For that sake, we eliminate
a (randomly chosen) individual that belongs to the most crowded cell in
order to have an available slot for the antibody.
5
Experiments
In order to validate our approach, we used several test functions reported in the standard
evolutionary multiobjective optimization literature [18,3]. In each case, we generated
the true Pareto front of the problem (i.e., the solution that we wished to achieve) by
enumeration using parallel processing techniques. Then, we plotted the Pareto front
generated by our algorithm, which we call the multiobjective immune system algorithm
(MISA).
The results indicated below were found using the following parameters for MISA:
Population size = 100, number of grid subdivisions = 25, size of the external memory =
100 (this is a value normally adopted by researchers in the specialized literature [3]). The
number of iterations to be performed by the algorithm is determined by the number of
ﬁtness function evaluations required. The previous parameters produce a total of 12,000
ﬁtness function evaluations.

164
N. Cruz Cort´es and C.A. Coello Coello
MISA was compared against the NSGA-II [9] and against PAES [11]. These two
algorithms were chosen because they are representative of the state-of-the-art in evolu-
tionary multiobjective optimization and their codes are in the public domain.
The Nondominated Sorting Genetic Algorithm II (NSGA-II) [8,9] is based on the
use of several layers to classify the individuals of the population, and uses elitism and
a crowded comparison operator that keeps diversity without specifying any additional
parameters. The NSGA-II is a revised (and more efﬁcient) version of the NSGA [16].
The Pareto Archived Evolution Strategy (PAES) [11] consists of a (1+1) evolution
strategy (i.e., a single parent that generates a single offspring) in combination with a
historical archive that records some of the nondominated solutions previously found.
This archive is used as a reference set against which each mutated individual is being
compared.
All the approaches performed the same number of ﬁtness function evaluations as
MISA and they all adopted the same size for their external memories.
In the following examples, the NSGA-II was run using a population size of 100, a
crossover rate of 0.75, tournament selection, and a mutation rate of 1/vars, where vars =
number of decision variables of the problem.
PAES was run using a mutation rate of 1/L, where L refers to the length of the chro-
mosomic string that encodes the decision variables. Besides the graphical comparisons
performed, the three following metrics were adopted to allow a quantitative comparison
of results:
– Error Ratio (ER): This metric was proposed by Van Veldhuizen [17] to indicate
the percentage of solutions (from the nondominated vectors found so far) that are
not members of the true Pareto optimal set:
ER =
n
i=1 ei
n
,
(1)
where n is the number of vectors in the current set of nondominated vectors
available; ei = 0 if vector i is a member of the Pareto optimal set, and ei = 1
otherwise. It should then be clear that ER = 0 indicates an ideal behavior, since
it would mean that all the vectors generated by our algorithm belong to the Pareto
optimal set of the problem.
– Spacing (S): This metric was proposed by Schott [15] as a way of measuring the
range (distance) variance of neighboring vectors in the Pareto front known. This
metric is deﬁned as:
S ≜




1
n −1
n

i=1
(d −di)2 ,
(2)
where di = minj(| f i
1(x) −f j
1(x) | + | f i
2(x) −f j
2(x) |), i, j = 1, . . . , n, d
is the mean of all di, and n is the number of vectors in the Pareto front found
by the algorithm being evaluated. A value of zero for this metric indicates all the
nondominated solutions found are equidistantly spaced.

Multiobjective Optimization Using Ideas from the Clonal Selection Principle
165
– Generational Distance (GD): The concept of generational distance was introduced
by Van Veldhuizen & Lamont [19] as a way of estimating how far are the elements
in the Pareto front produced by our algorithm from those in the true Pareto front of
the problem. This metric is deﬁned as:
GD =
n
i=1 d2
i
n
(3)
where n is the number of nondominated vectors found by the algorithm being ana-
lyzed and di is the Euclidean distance (measured in objective space) between each
of these and the nearest member of the true Pareto front. It should be clear that a
value of GD = 0 indicates that all the elements generated are in the true Pareto
front of the problem. Therefore, any other value will indicate how “far” we are from
the global Pareto front of our problem.
In all the following examples, we performed 20 runs of each algorithm. The graphs
shown in each case were generated using the average performance of each algorithm
with respect to generational distance.
Example 1
Our ﬁrst example is a two-objective optimization problem proposed by Schaffer [14]:
Minimize f1(x) =







−x
if x ≤1
−2 + x if 1 < x ≤3
4 −x
if 3 < x ≤4
−4 + x if x > 4
(4)
Minimize f2(x) = (x −5)2
(5)
and −5 ≤x ≤10.
0
2
4
6
8
10
12
14
16
18
-1
-0.5
0
0.5
1
1.5
f2
f1
PF true
MISA
0
2
4
6
8
10
12
14
16
18
-1
-0.5
0
0.5
1
1.5
f2
f1
PF true
NSGA2
0
2
4
6
8
10
12
14
16
18
-1
-0.5
0
0.5
1
1.5
f2
f1
PF true
PAES
Fig. 2. Pareto front obtained by MISA (left), the NSGA-II (middle) and PAES (right) in the ﬁrst
example. The true Pareto front of the problem is shown as a continuous line (note that the vertical
segment is NOT part of the Pareto front and is shown only to facilitate drawing the front).
The comparison of results between the true Pareto front of this example and the
Pareto front produced by MISA, the NSGA-II, and PAES are shown in Figure 2. The
values of the three metrics for each algorithm are presented in Tables 1 and 2.

166
N. Cruz Cort´es and C.A. Coello Coello
Table 1. Spacing and Generational Distance for the ﬁrst example.
Spacing
GD
MISA
NSGA-II
PAES
MISA
NSGA-II
PAES
Average 0.236345 0.145288 0.268493 0.000375 0.000288 0.002377
Best
0.215840 0.039400 0.074966 0.000199 0.000246 0.000051
Worst
0.256473 0.216794 1.592858 0.001705 0.000344 0.034941
Std. Dev. 0.013523 0.079389 0.336705 0.000387 0.000022 0.007781
Median 0.093127 0.207535 0.137584 0.000387 0.000285 0.000239
In this case, MISA had the best average value with respect to generational distance.
The NSGA-II had both the best average spacing and the best average error ratio. Graph-
ically, we can see that PAES was unable to ﬁnd most of the true Pareto front of the
problem. MISA and the NSGA-II were able to produce most of the true Pareto front and
their overall performance seems quite similar from the graphical results with a slight ad-
vantage for MISA with respect to closeness to the true Pareto front and a slight advantage
for the NSGA-II with respect to uniform distribution of solutions.
Table 2. Error ratio for the ﬁrst example.
MISA
NSGA-II
PAES
Average 0.410094 0.210891 0.659406
Best
0.366337 0.178218 0.227723
Worst
0.445545 0.237624 1.000000
Std. Dev. 0.025403 0.018481 0.273242
Median 0.410892 0.207921 0.663366
7.4
7.6
7.8
8
8.2
8.4
8.6
-8
-6
-4
-2
0
2
4
6
8
f2
f1
PF true
MISA
7.2
7.4
7.6
7.8
8
8.2
8.4
8.6
-3
-2
-1
0
1
2
3
4
5
6
7
f2
f1
PF true
NSGA2
7.4
7.6
7.8
8
8.2
8.4
8.6
-8
-6
-4
-2
0
2
4
6
8
f2
f1
PF true
PAES
Fig. 3. Pareto front obtained by MISA (left), the NSGA-II (middle) and PAES (right) in the second
example. The true Pareto front of the problem is shown as a continuous line.
Example 2
The second example was proposed by Kita [10]: Maximize F = (f1(x, y), f2(x, y))
where: f1(x, y) = −x2 + y, f2(x, y) = 1
2x + y + 1, x, y ≥0, 0 ≥1
6x + y −13
2 ,

Multiobjective Optimization Using Ideas from the Clonal Selection Principle
167
0 ≥1
2x + y −15
2 , 0 ≥5x + y −30. The comparison of results between the true Pareto
front of this example and the Pareto front produced by MISA, the NSGA-II and PAES
are shown in Figure 3. The values of the three metrics for each algorithm are presented
in Tables 3, and 4.
Table 3. Spacing and Generational Distance for the second example.
Spacing
GD
MISA
NSGA-II
PAES
MISA
NSGA-II
PAES
Average 0.905722 0.815194 0.135875 0.036707 0.049669 0.095323
Best
0.783875 0.729958 0.048809 0.002740 0.004344 0.002148
Worst
1.670836 1.123444 0.222275 0.160347 0.523622 0.224462
Std. Dev. 0.237979 0.077707 0.042790 0.043617 0.123888 0.104706
Median 0.826587 0.173106 0.792552 0.019976 0.066585 0.018640
In this case, MISA had again the best average value for the generational distance.
The NSGA-II had the best average error ratio and PAES had the best average spacing
value. Note however from the graphical results that the NSGA-II missed most of the true
Pareto front of the problem. PAES also missed some portions of the true Pareto front of
the problem. Graphically, we can see that MISA found most of the true Pareto front and
therefore, we argue that it had the best overall performance in this test function.
Table 4. Error ratio for the second example.
MISA
NSGA-II
PAES
Average 0.007431 0.002703 0.005941
Best
0.000000 0.000000 0.000000
Worst
0.010000 0.009009 0.009901
Std. Dev. 0.004402 0.004236 0.004976
Median 0.009901
0.0000
0.009901
Example 3
Our third example is a two-objective optimization problem deﬁned by Kursawe [12]:
Minimize f1(x) =
n−1

i=1

−10 exp

−0.2

x2
i + x2
i+1

(6)
Minimize f2(x) =
n

i=1

|xi|0.8 + 5 sin(xi)3
(7)
where: −5 ≤x1, x2, x3 ≤5

168
N. Cruz Cort´es and C.A. Coello Coello
-12
-10
-8
-6
-4
-2
0
2
-20
-19
-18
-17
-16
-15
-14
f2
f1
PF true
MISA
-12
-10
-8
-6
-4
-2
0
2
-20
-19
-18
-17
-16
-15
-14
f2
f1
PF true
NSGA2
-12
-10
-8
-6
-4
-2
0
2
-20
-19
-18
-17
-16
-15
-14
-13
f2
f1
PF true
PAES
Fig. 4. Pareto front obtained by MISA (left), and the NSGA-II (middle) and PAES (right) in the
third example. The true Pareto front of the problem is shown as a continuous line.
The comparison of results between the true Pareto front of this example and the
Pareto front produced by MISA, the NSGA-II and PAES are shown in Figure 4. The
values of the three metrics for each algorithm are presented in Tables 5 and 6.
Table 5. Spacing and Generational Distance for the third example.
Spacing
GD
MISA
NSGA-II
PAES
MISA
NSGA-II
PAES
Average 3.188819 2.889901 3.019393 0.004152 0.004164 0.009341
Best
3.177936 2.705087 2.728101 0.003324 0.003069 0.002019
Worst
3.203547 3.094213 3.200678 0.005282 0.007598 0.056152
Std. Dev. 0.007210 0.123198 0.133220 0.000525 0.001178 0.013893
Median 3.186680 2.842901 3.029246 0.004205 0.003709 0.004468
For this test function, MISA had again the best average generational distance (this
value was, however, only marginally better than the average value of the NSGA-II). The
NSGA-II had the best average spacing value and the best average error ratio. However,
by looking at the graphical results, it is clear that the NSGA-II missed the last (right
lowerhand) portion of the true Pareto front, although it got a nice distribution of solutions
along the rest of the front. PAES missed almost entirely two of the three parts that make
the true Pareto front of this problem. Therefore, we argue in this case that MISA was
practically in a tie with the NSGA-II in terms of best overall performance, since MISA
covered the entire Pareto front, but the NSGA-II had a more uniform distribution of
solutions.
Based on the limited set of experiments performed, we can see that MISA provides
competitive results with respect to the two other algorithms against which it was com-
pared. Although it did not always ranked ﬁrst when using the three metrics adopted, in
all cases it produced reasonably good approximations of the true Pareto front of each
problem under study (several other test functions were adopted but not included due to
space limitations), particularly with respect to the generational distance metric. Never-
theless, a more detailed statistical analysis is required to be able to derive more general
conclusions.

Multiobjective Optimization Using Ideas from the Clonal Selection Principle
169
Table 6. Error ratio for example 3
MISA
NSGA-II
PAES
Average 0.517584 0.262872 0.372277
Best
0.386139 0.178218 0.069307
Worst
0.643564 0.396040 0.881188
Std. Dev. 0.066756 0.056875 0.211876
Median 0.504951 0.252476 0.336634
6
Conclusions and Future Work
We have introduced a new multiobjective optimization approach based on the clonal
selection principle. The approach was found to be competitive with respecto to other
algorithms representative of the state-of-the-art in the area. Our main conclusion is that
the sort of artiﬁcial immune system proposed in this paper is a viable alternative to
solve multiobjective optimization problems in a relatively simple way. We also believe
that, given the features of artiﬁcial immune systems, an extension of this paradigm for
multiobjective optimization (such as the one proposed here) may be particularly useful
to deal with dynamic functions and that is precisely part of our future research. Also,
it is desirable to reﬁne the mechanism to maintain diversity that our approach currently
has, since that is its main current weakness.
Acknowledgements. We thank the comments of the anonymous reviewers that greatly
helped us to improve the contents of this paper. The ﬁrst author acknowledges support
from CONACyT through a scholarship to pursue graduate studies at the Computer
Science Section of the Electrical Engineering Department at CINVESTAV-IPN. The
second author gratefully acknowledges support from CONACyT through project 34201-
A.
References
1. Kevin P. Anchor, Jesse B. Zydallis, Gregg H. Gunsch, and Gary B. Lamont. Extending
the Computer Defense Immune System: Network Intrusion Detection with a Multiobjective
Evolutionary Programming Approach. In Jonathan Timmis and Peter J. Bentley, editors,
First International Conference on Artiﬁcial Immune Systems (ICARIS’2002), pages 12–21.
University of Kent at Canterbury, UK, September 2002. ISBN 1-902671-32-5.
2. Carlos A. Coello Coello and Nareli Cruz Cort´es. An Approach to Solve Multiobjective Opti-
mization Problems Based on an Artiﬁcial Immune System. In Jonathan Timmis and Peter J.
Bentley, editors, First International Conference on Artiﬁcial Immune Systems (ICARIS’2002),
pages 212–221. University of Kent at Canterbury, UK, September 2002. ISBN 1-902671-32-
5.
3. Carlos A. Coello Coello, David A. Van Veldhuizen, and Gary B. Lamont.
Evolutionary
Algorithms for Solving Multi-Objective Problems. Kluwer Academic Publishers, New York,
May 2002. ISBN 0-3064-6762-3.

170
N. Cruz Cort´es and C.A. Coello Coello
4. Indraneel Das and John Dennis. A Closer Look at Drawbacks of Minimizing Weighted Sums
of Objectives for Pareto Set Generation in Multicriteria Optimization Problems. Structural
Optimization, 14(1):63–69, 1997.
5. Leandro N. de Castro and JonathanTimmis. Artiﬁcial Immune Systems:A New Computational
Intelligence Approach. Springer, London, 2002.
6. Leandro Nunes de Castro and F. J. Von Zuben. Learning and Optimization Using the Clonal
Selection Principle. IEEE Transactions on Evolutionary Computation, 6(3):239–251, 2002.
7. Kalyanmoy Deb. Multi-Objective Optimization using Evolutionary Algorithms. John Wiley
& Sons, Chichester, UK, 2001. ISBN 0-471-87339-X.
8. Kalyanmoy Deb, Samir Agrawal, Amrit Pratab, and T. Meyarivan.
A Fast Elitist Non-
Dominated Sorting Genetic Algorithm for Multi-Objective Optimization: NSGA-II. In Marc
Schoenauer, Kalyanmoy Deb, G¨unter Rudolph, XinYao, Evelyne Lutton, Juan Julian Merelo,
and Hans-Paul Schwefel, editors, Proceedings of the Parallel Problem Solving from Nature
VI Conference, pages 849–858, Paris, France, 2000. Springer. Lecture Notes in Computer
Science No. 1917.
9. Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and T. Meyarivan. A Fast and Elitist Mul-
tiobjective Genetic Algorithm: NSGA–II. IEEE Transactions on Evolutionary Computation,
6(2):182–197, April 2002.
10. Hajime Kita, Yasuyuki Yabumoto, Naoki Mori, and Yoshikazu Nishikawa. Multi-Objective
Optimization by Means of the Thermodynamical Genetic Algorithm. In Hans-Michael Voigt,
Werner Ebeling, Ingo Rechenberg, and Hans-Paul Schwefel, editors, Parallel Problem Solv-
ing from Nature—PPSN IV, Lecture Notes in Computer Science, pages 504–512, Berlin,
Germany, September 1996. Springer-Verlag.
11. Joshua D. Knowles and David W. Corne. Approximating the Nondominated Front Using the
Pareto Archived Evolution Strategy. Evolutionary Computation, 8(2):149–172, 2000.
12. Frank Kursawe. A Variant of Evolution Strategies for Vector Optimization. In H. P. Schwefel
and R. M¨anner, editors, Parallel Problem Solving from Nature. 1stWorkshop, PPSN I, volume
496 of Lecture Notes in Computer Science, pages 193–197, Berlin, Germany, oct 1991.
Springer-Verlag.
13. Kaisa M. Miettinen. Nonlinear Multiobjective Optimization. Kluwer Academic Publishers,
Boston, Massachusetts, 1998.
14. J.DavidSchaffer. MultipleObjectiveOptimizationwithVectorEvaluatedGeneticAlgorithms.
PhD thesis, Vanderbilt University, 1984.
15. Jason R. Schott. Fault Tolerant Design Using Single and Multicriteria Genetic Algorithm
Optimization. Master’s thesis, Department of Aeronautics and Astronautics, Massachusetts
Institute of Technology, Cambridge, Massachusetts, May 1995.
16. N. Srinivas and Kalyanmoy Deb. Multiobjective Optimization Using Nondominated Sorting
in Genetic Algorithms. Evolutionary Computation, 2(3):221–248, Fall 1994.
17. DavidA.VanVeldhuizen. Multiobjective Evolutionary Algorithms: Classiﬁcations, Analyses,
and New Innovations. PhD thesis, Department of Electrical and Computer Engineering.
Graduate School of Engineering. Air Force Institute of Technology, Wright-Patterson AFB,
Ohio, May 1999.
18. David A. Van Veldhuizen and Gary B. Lamont. MOEA Test Suite Generation, Design &
Use. In Annie S. Wu, editor, Proceedings of the 1999 Genetic and Evolutionary Computation
Conference. Workshop Program, pages 113–114, Orlando, Florida, July 1999.
19. David A. Van Veldhuizen and Gary B. Lamont. On Measuring Multiobjective Evolutionary
Algorithm Performance. In 2000 Congress on Evolutionary Computation, volume 1, pages
204–211, Piscataway, New Jersey, July 2000. IEEE Service Center.
20. J. Yoo and P. Hajela.
Immune network simulations in multicriterion design.
Structural
Optimization, 18:85–94, 1999.

A Hybrid Immune Algorithm with Information
Gain for the Graph Coloring Problem
Vincenzo Cutello, Giuseppe Nicosia, and Mario Pavone
University of Catania,
Department of Mathematics and Computer Science
V.le A. Doria 6, 95125 Catania, Italy
{cutello,nicosia,mpavone}@dmi.unict.it
Abstract. We present a new Immune Algorithm that incorporates a
simple local search procedure to improve the overall performances to
tackle the graph coloring problem instances. We characterize the algo-
rithm and set its parameters in terms of Information Gain. Experiments
will show that the IA we propose is very competitive with the best
evolutionary algorithms.
Keywords: Immune Algorithm, Information Gain, Graph coloring prob-
lem, Combinatorial optimization.
1
Introduction
In the last ﬁve years we have witnessed an increasing number of algorithms,
models and results in the ﬁeld of Artiﬁcial Immune Systems [1,2]. Natural Im-
mune System provide an excellent example of bottom up intelligent strategy, in
which adaptation operates at the local level of cells and molecules, and useful
behavior emerges at the global level, the immune humoral response. From an
information processing point of view [3] the Immune System (IS) can be seen
as a problem learning and solving system. The antigen (Ag) is the problem to
solve, the antibody (Ab) is the generated solution. At the beginning of the pri-
mary response the antigen-problem is recognized by poor candidate solution. At
the end of the primary response the antigen-problem is defeated-solved by good
candidate solutions. Consequently the primary response corresponds to a train-
ing phase while the secondary response is the testing phase where we will try to
solve problems similar to the original presented in the primary response [4].
Recent studies show that when one faces the Graph Coloring Problem (GCP)
with evolutionary algorithms (EAs), the best results are often obtained by hybrid
EAs with local search and specialized crossover [5]. In particular, the random
crossover operator used in a standard genetic algorithm performs poorly for
combinatorial optimization problem and, in general, the crossover operator must
be designed carefully to identify important properties, building blocks, which
must be transmitted from parents population to oﬀspring population. Hence the
design of a good crossover operator is crucial for the overall performance of the
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 171–182, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

172
V. Cutello, G. Nicosia, and M. Pavone
EAs. The drawback is that is might happen to recombine good individuals from
diﬀerent regions of the search space, having diﬀerent symmetries, producing poor
oﬀspring [6]. For this reason, we use an Immunological Algorithm (IA) to tackle
the GCP. IAs do not have a crossover operator, and the crucial task of designing
an appropriate crossover operator is avoided at once. The IA we will propose
makes use of a particular mutation operator and a local search strategy without
having to incorporate speciﬁc domain knowledge.
For sake of clarity, we recall some basic deﬁnitions. Given an undirected graph
G = (V, E) with vertex set V, edge set E and a positive integer K ≤| V |, the
Graph Coloring Problem asks whether G is K–colorable, i.e. whether there exists
a function f : V →{1, 2, ..., K} such that f(u) ̸= f(v) whenever {u, v} ∈E. The
GCP is a well-known NP–complete problem [7]. Exact solutions can be found
for simple or medium instances [8,9]. Coloring problems are very closely related
with cliques [10] (complete subgraphs). The size of the maximum clique is a
lower bound on the minimum number of colors needed to color a graph, χ(G).
Thus, if ω(G) is the size of the maximum clique: χ(G) ≥ω(G).
2
Immune Algorithms
We work with a simpliﬁed model of the natural immune system. We will see
that the IA presented in this work is very similar to De Castro, Von Zuben’s
algorithm, CLONALG [11,12] and to Nicosia et al. immune algorithm [4,13].
We consider only two entities: Ag and B cells. Ag is the problem and the B
cell receptor is the candidate solution. Formally, Ag is a set of variables that
models the problem; and, B cells are deﬁned as strings of integers of ﬁnite length
ℓ
=
|
V
|. The input is the antigen–problem, the output is basically the
candidate solutions–B cells that solve–recognize the Ag.
By P (t) we will denote a population of d individuals of length ℓ, which repre-
sent a subset of the space of feasible solutions of length ℓ, Sℓ, obtained at time
t. The initial population of B cells, i.e. the initial set P (0), is created randomly.
After initialization, there are three diﬀerent phases.
In the Interaction phase the population P (t) is evaluated. f(x) = m is the
ﬁtness function value of B cell receptor x. Hence for the GCP, the ﬁtness function
f(x) = m indicates that there exists a m–coloring for G, that is, a partition of
vertices V = S1 ∪S2 ∪. . . ∪Sm such that each Si ⊆V is a subset of vertices
which are pairwise not adjacent (i.e. each Si is an independent set).
The Cloning expansion phase is composed of two steps: cloning and hyper-
mutation. The cloning expansion events are modeled by cloning potential V and
mutation number M, which depend upon f. If we exclude all the adaptive mech-
anisms [14] in EA’s (e.g., adaptive mutation and adaptive crossover rates which
are related to the ﬁtness function values), the immune operators, contrary to
standard evolutionary operators, depend upon the ﬁtness function values[15].
Cloning potential is a truncated exponential: V (f(x)) = e−k(ℓ−f(x)), where the
parameter k determines the sharpness of the potential. The cloning operator
generates the population P clo. The mutation number is a simple straight line:

A Hybrid Immune Algorithm with Information Gain
173
M(f(x)) = 1 −(ℓ/f(x)) , and this function indicates the number of swaps be-
tween vertices in x. The mutation operator chooses randomly M(f(x)) times
two vertices i and j in x and then swaps them. The hypermutation function from
population P clo generates the population P hyp. The cell receptor mutation mech-
anism is modeled by the mutation number M, which is inversely proportional to
the ﬁtness function value. The cloning expansion phase triggers the growth of a
new population of high–value B cells centered around a higher ﬁtness function
value.
In the Aging phase, after the evaluation of P hyp at time t, the algorithm elim-
inates old B cells. Such an elimination process is stochastic, and, speciﬁcally, the
probability to remove a B cell is governed by an exponential negative law with
parameter τB, (expected mean life for the B cells): Pdie(τB) = (1−e(−ln(2)/τB)).
Finally, the new population P (t+1) of d elements is produced. We can use two
kinds of Aging phases: pure aging phase and elitist aging phase. In the elitist
aging, when a new population for the next generation is generated, we do not
allow the elimination of B cells with the best ﬁtness function. While in the pure
aging the best B cells can be eliminate as well. We observe that the exponen-
tial rate of aging, Pdie(τB), and the cloning potential, V (f(x)), are inspired by
biological processes [16].
Sometimes it might be useful to apply a birth phase to increase the population
diversity. This extra phase must be combined with an aging phase with a longer
expected mean life τB. For the GCP we did not use the birth phase because it
produced a higher number of ﬁtness function evaluation to solutions.
Assignment colors. To assign colors, the vertices of the solution represented
by a B cell are examined and assigned colors, following a deterministic scheme
based on the order in which the graph vertices are visited. In details, vertices
are examined according to the order given by the B cell and assigned the ﬁrst
color not assigned to adjacent vertices. This method is very simple. In literature
there are more complicated and eﬀective methods [5,6,10]. We do not use those
methods because we want investigate the learning and solving capability of our
IA. In fact, the IA described does not use speciﬁc domain knowledge and does
not make use of problem-dependent local searches. Thus, our IA can be improved
simply including ad hoc local search and immunological operators using speciﬁc
domain knowledge.
2.1
Termination Condition by Information Gain
To analyze the learning process, we use the notion of Kullback information, also
called information gain [17], an entropy function associated to the quantity of
information the system discovers during the learning phase. To this end, we
deﬁne the B cells distribution function f (t)
m as the ratio between the number,
Bt
m, of B cells at time t with ﬁtness function value m, (the distance m from the
antigen–problem) and the total number of B cells:
f (t)
m =
Bt
m
h
m=0 Btm
= Bt
m
d .
(1)

174
V. Cutello, G. Nicosia, and M. Pavone
It follows that the information gain can be deﬁned as:
K(t, t0) =

m
f (t)
m log(f (t)
m /f (t0)
m ).
(2)
The gain is the amount of information the system has already learned from the
given Ag–problem with respect to initial distribution function (the randomly
generated initial population P (t0=0)). Once the learning process starts, the in-
formation gain increases monotonically until it reaches a ﬁnal steady state (see
ﬁgure 1). This is consistent with the idea of a maximum information-gain prin-
ciple of the form dK
dt ≥0. Since dK
dt = 0 when the learning process ends, we use
it as a termination condition for the Immune Algorithms. We will see in section
3 that the information gain is a kind of entropy function useful to understand
the IA’s behavior and to set the IA’s parameters.
0
5
10
15
20
25
5
10
15
20
25
30
35
40
45
50
Information Gain
Generations
K(t0,t)
7
7.5
8
8.5
9
9.5
5 101520253035404550
Clones’ avg fit.
Pop’s avg fit.
Best fit.
Fig. 1. Information Gain versus generations for the GCP instance queen6 6.
In ﬁgure 1 we show the information gain when the IA faces the GCP instance
queen6 6 with vertex set | V |= 36, edge set | E |= 290 and optimal coloring
7. In particular, in the inset plot one can see the corresponding average ﬁtness
of population P hyp, the average ﬁtness of population P (t+1) and the best ﬁtness
value. All the values are averaged on 100 independent runs. Finally, we note that
our experimental protocol can have other termination criteria, such as maximum
number of evaluations or generations.
2.2
Local Search
Local search algorithms for combinatorial optimization problems generally rely
on a deﬁnition of neighborhood. In our case, neighbors are generated by swapping
vertex values. Every time a proposed swap reduces the number of used colors,
it is accepted and we continue with the sequence of swaps, until we explore the
neighborhood of all vertices. Swapping all pair of vertices is time consuming,
so we use a reduced neighborhood: all n =| V | vertices are tested for a swap,
but only with the closer ones. We deﬁne a neighborhood with radius R. Hence

A Hybrid Immune Algorithm with Information Gain
175
we swap all vertices only with their R nearest neighbors, to left and to right.
A possible value for radius R is 5. Given the large size of neighborhood and n,
we found it convenient to apply the previous local search procedure only on the
population’s best B cell. We note that if R = 0 the local search procedure is
not executed. This case is used for simple GCP instances, to avoid unnecessary
ﬁtness function evaluations. The local search used is not critical to the searching
process. Once a maximum number of generations has been ﬁxed, the local search
procedure increases only the success rate on a certain number of independent
runs and, as drawback, it increases the average number of evaluations to solu-
tions. However, if we omit it, the IA needs more generations, hence more ﬁtness
function evaluations, to obtain the same results of IA using local search.
Table 1. Pseudo–code of Immune Algorithm
Immune Algorithm(d, dup, τB, R)
1. t := 0;
2. Initialize P (0) = {x1, x2, ..., xd} ∈Sℓ
3. while ( dK
dt ̸= 0 ) do
4.
Interact(Ag, P (t));
/* Interaction phase */
5.
P clo := Cloning (P (t), dup);
/* First step Cloning expansion */
6.
P hyp := Hypermutation (P clo);
/* Second step Cloning expansion */
7.
Evaluate (P hyp);
/* Compute P hyp ﬁtness function */
8.
P ls:=Local Search(P hyp, R);
/* LS procedure */
9.
P (t+1):=aging(P hyp ⊔P (t) ⊔P ls, τB); /* Aging Phase */
10.
K(t, t0):=InformationGain();
/* Compute K(t, t0) */
11.
t := t + 1;
12. end while
In ﬁgure 2 we show the ﬁtness function value dynamics. In both plots, we show
the dynamics of average ﬁtness of population P hyp, P (t+1), and the best ﬁt-
ness value of population P (t+1). Note that the average ﬁtness of P hyp shows
the diversity in the current population, when this value is equal to average
ﬁtness of population P (t+1), we are close a premature convergence or in the
best case we are reaching a sub–optimal or optimal solution. It is possible
to use the diﬀerence between P hyp average ﬁtness and P (t+1) average ﬁtness,
| avgfitness(P hyp)−avgfitness(P (t+1) |= Popdiv as a standard to measure popu-
lation diversity. When Popdiv rapidly decreases, this is considered as the primary
reason for premature convergence. In the left plot we show the IA dynamic when
we face the DSCJ250.5.col GCP instance (| V |= 250 and | E |= 15, 668).
We execute the algorithm with population size d = 500, duplication parameter
dup = 5, expected mean life τB = 10.0 and neighborhood’s radius R = 5. For
this instance we use pure aging and obtain the optimal coloring. In the right plot

176
V. Cutello, G. Nicosia, and M. Pavone
26
28
30
32
34
36
38
40
42
44
0
200
400
600
800
1000
Fitness values
Generations
Graph coloring instance: DSJC250.5.col
Clones’ average fitness
Population’s average fitness
Best fitness
20
25
30
35
40
45
50
0
100
200
300
400
500
600
Fitness values
Generations
Graph coloring instance: flat_300_20_0
Clones’ average fitness
Population average fitness
Best fitness
Fig. 2. Average ﬁtness of population P hyp, average ﬁtness of population P (t+1), and
best ﬁtness value vs generations. Left plot: IA with pure aging phase. Right plot: IA
with elitist aging
we tackle the flat 300 20 GCP instance (| V |= 300 and | E |= 21, 375), with
the following IA’s parameters: d = 1000, dup = 10, τB = 10.0 and R = 5. For
this instance the optimal coloring is obtained using elitist aging. In general, with
elitist aging the convergence is faster, even though it can trap the algorithm
in a local optimum. Although, with pure aging the convergence is slower and
the population diversity is higher, our experimental results indicate that elitist
aging seems to work well. We can deﬁne the ratio Sp =
1
dup as the selective
pressure of the algorithm: when dup = 1, obviously we have that Sp = 1 and
the selective pressure is low, while increasing dup we increase the IA’s selective
pressure. Experimental results show that high values of d denote high clones
population average ﬁtness and, in turn, high population diversity but, also, a
high computational eﬀort during the evolution.
3
Parameters Tuning by Information Gain
To understand how to set the IA parameters, we performed some experiments it
with the GCP instance queen6 6. Firstly, we want to set the B cell’s mean life,
τB. We ﬁx the population size d = 100, duplication parameter dup = 2, local
search radius R = 2 and total generations gen = 100. For each experiment we
performed runs = 100 independent runs.
3.1
B Cell’s Mean Life, τB
In ﬁgure 3 we can see the best ﬁtness values (left plot) and the Information Gain
(right plot) with respect the following τB values {1.0,5.0,15.0,25.0,1000.0}. When
τB = 1.0 the B cells have a shorter mean life, only one time step, and with this
value the IA performed poorly. With τB = 1.0 the maximum information gain
obtained at generation 100 is about 13. As τB increases, the best ﬁtness values
decreases and the Information Gain increases. The best value for τB is 25.0.
With τB = 1000.0, and in general when τB is greater than a number of ﬁxed

A Hybrid Immune Algorithm with Information Gain
177
7
7.2
7.4
7.6
7.8
8
0
20
40
60
80
100
Best Fitness
Generations
tauB =    1.0
tauB =    5.0
tauB =   15.0
tauB =   25.0
tauB = 1000.0
0
5
10
15
20
25
0
10
20
30
40
50
60
70
80
90
100
Information Gain
Generations
tauB =    1.0
tauB =    5.0
tauB =   15.0
tauB =   25.0
tauB = 1000.0
Fig. 3. Best ﬁtness values and Information Gain vs generations.
generations gen, we can consider B cells mean life inﬁnite and obtain a pure
elitist selection scheme. In this special case, the behavior of IA shows slower
convergence in the ﬁrst 30 generations in both plots. For values of τB greater
than 25.0 we obtain slightly worse results. Moreover, when τB ≤10 the success
rate (SR) on 100 independent runs is less than 98 while when τB ≥10 the
IA obtains a SR=100 with a lower Average number of Evaluations to Solution
(AES) located when τB = 25.0.
3.2
Duplication Parameter Dup
Now we ﬁx τB = 25.0 and vary dup. In ﬁg.4 (left plot) we note that the IA obtains
quickly more Information Gain at each generation with dup = 10, moreover it
reaches faster the best ﬁtness value with dup = 5. With both values of dup the
0
5
10
15
20
25
0
10
20
30
40
50
60
70
80
90
100
Information Gain
Generations
dup =  2
dup =  3
dup =  5
dup = 10
7
7.2
7.4
7.6
7.8
8
0
10
20
30
40
50
60
dup =  5
dup = 10
7
7.5
8
8.5
9
9.5
0
5
10
15
20
25
30
35
40
45
50
Fitness
Generations
Clones’ average fitness, dup =  5
Clones’ average fitness, dup = 10
Pop(t)’s average fitness, dup =  5
Pop(t)’s average fitness, dup = 10
Fig. 4. Left plot, Information Gain and Best ﬁtness value for dup. Right plot, average
ﬁtness of Clones and Pop(t) for dup ∈{5, 10}.
largest information gain is obtained at generation 43. Moreover, with dup = 10
the best ﬁtness is obtained at generation 22, whereas with dup = 5 at generation
40. One may deduce that dup = 10 is the best value for the cloning of B cells

178
V. Cutello, G. Nicosia, and M. Pavone
since we obtain faster more information gain. This is not always true. Indeed,
if we observe ﬁgure 4 (right plot) we can see how the IA with dup = 5 obtains
a larger amount of clones average ﬁtness and hence a greater diversity. This
characteristic can be useful in avoiding premature convergence and in ﬁnding
more optimal solutions for a given combinatorial problem.
3.3
Dup and τB
In 3.1 we saw that for dup = 2, the best value of τB is 25.0. Moreover, in 3.2
experimental results show better performance for dup = 5. If we set dup = 5
and vary τB, we obtain the results in ﬁg.5. We can see that for τB = 15 we reach
the maximum Information Gain at generation 40 (left plot) and more diversity
(right plot). Hence, when dup = 2 the best value of τB is 25.0, i.e. on average we
need 25 generations for the B cells to reach a mature state. On the other hand,
when dup = 5 the correct value is 15.0 Thus, increasing dup the average time
for the population of B cells to reach a mature state decreases.
0
5
10
15
20
25
0
10
20
30
40
50
60
70
80
90
100
Information Gain
Generations
tauB = 15
tauB = 20
tauB = 25
tauB = 50
17
18
19
20
21
22
23
24
20
25
30
35
40
45
50
tauB = 15
tauB = 20
tauB = 25
tauB = 50
7
7.5
8
8.5
9
9.5
0
5
10
15
20
25
30
35
40
45
50
Fitness
Generations
Clones’ average fitness, tauB = 25
Clones’ average fitness, tauB = 20
Clones’ average fitness, tauB = 15
Pop(t)’s average fitness, tauB = 25
Pop(t)’s average fitness, tauB = 20
Pop(t)’s average fitness, tauB = 15
Fig. 5. Left plot Information Gain for τb ∈{15, 20, 25, 50}. Right plot average ﬁtness
of population P hyp and population P (t) for τb ∈{15, 20, 25}
3.4
Neighborhood’s Radius R, d and Dup
Local search is useful for large instances (see table 2). The cost of local search,
though, is high. In ﬁgure 6 (left plot) we can see how the AES increases as
the neighborhood radius increases. The plot reports two classes of experiments
performed with 1000 and 10000 independent runs. In ﬁgure 6 (right plot) we
show the values of parameters d and dup as functions of the Success Rate (SR).
Each point has been obtained averaging 1000 independent runs. How we can see
there is a certain relation between d and dup in order to reach a SR = 100. For
the queen6 6 instance, for low values for the population we need a high value
of dup to reach SR = 100. For d = 10, dup = 10 is not suﬃcient to obtain the
maximum SR. On the other hand, as the population number increases, we need
smaller values for dup. Small values of dup are a positive factor.

A Hybrid Immune Algorithm with Information Gain
179
Table 2. Mycielsky and Queen graph instances. We ﬁxed τB = 25.0, and the number
of independent runs 100. OC denotes the Optimal Coloring.
Instance G
| V |
| E |
OC
(d,dup,R)
Best Found
AES
Myciel3
11
20
4
(10,2,0)
4
30
Myciel4
23
71
5
(10,2,0)
5
30
Myciel5
47
236
6
(10,2,0)
6
30
Queen5 5
25
320
5
(10,2,0)
5
30
Queen6 6
36
580
7
(50,5,0)
7
3750
Queen7 7
49
952
7
(60,5,0)
7
11,820
Queen8 8
64
1,456
9
(100,15,0)
9
78,520
Queen8 12
96
2,736
12
(500,30,0)
12
908,000
Queen9 9
81
1,056
10
(500,15,0)
10
445,000
School1 nsh
352
14,612
14
(1000,5,5)
15
2,750,000
School1
385
19,095
9
(1000,10,10)
14
3,350,000
We recall that dup is similar to the temperature in Simulated Annealing [18].
Low values of dup corresponds to a system that cools down slowly and has a
high EAS.
6000
8000
10000
12000
14000
16000
18000
20000
22000
24000
26000
1
5
10
15
20
25
30
35
AES
Neighbourhood’s Radius
runs =  1000
runs = 10000
10
20
30
40
50
Population size
1 2 3 4 5 6 7 8 9 10
Dup
10
20
30
40
50
60
70
80
90
100
SR
Fig. 6. Left plot: Average number of Evaluations to Solutions versus neighborhood’s
radius. Right plot: 3D plot of d, dup versus Success Rate (SR).
4
Results
In this section we report our experimental results. We worked with classical
benchmark graph [10]: the Mycielski, Queen, DSJC and Leighton GCP
instances. Results are reported in Tables 2 and 3. In these experiments the IA’s
best found value is always obtained SR = 100. For all the results presented in
this section, we used elitist aging. In tables 4 and 5 we compare our IA with two of
the best evolutionary algorithms, respectively Evolve AO algorithm [19] and the

180
V. Cutello, G. Nicosia, and M. Pavone
Table 3. Experimental results on subset instances of DSJC and Leighton graphs. We
ﬁxed τB = 15.0, and the number of independent runs 10.
Instance G
| V |
| E |
OC
(d,dup,R)
Best Found
AES
DSJC125.1
125
736
5
(1000,5,5)
5
1,308,000
DSJC125.5
125
3,891
12
(1000,5,5)
18
1,620,000
DSJC125.9
125
6,961
30
(1000,5,10)
44
2,400,000
DSJC250.1
250
3,218
8
(400,5,5)
9
1,850,000
DSJC250.5
250
15,668
13
(500,5,5)
28
2,500,000
DSJC250.9
250
27,897
35
(1000,15,10)
74
4,250,000
le450 15a
450
8,168
15
(1000,5,5)
15
5,800,000
le450 15b
450
8,169
15
(1000,5,5)
15
6,010,000
le450 15c
450
16,680
15
(1000,15,10)
15
10,645,000
le450 15d
450
16,750
9
(1000,15,10)
16
12,970,000
HCA algorithm [5]. For all the GCP instances we ran the IA with the following
parameters: d = 1000, dup = 15, R = 30, and τB = 20.0. For these classes of
experiments the goal is to obtain the best possible coloring, no matter the value
of AES. Table 4 shows how the IA outperform the Evolve AO algorithm, while
is similar in results to HCA algorithm and better in SR values (see table 5).
Table 4. IA versus Evolve AO Algorithm. The values are averaged on 5 independent
runs.
Instance G
χ(G)
Best–Known
Evolve AO
IA
Diﬀerence
DSJC125.5
12
12
17.2
18.0
+ 0.8
DSJC250.5
13
13
29.1
28.0
-0.9
ﬂat300 20 0
≤20
20
26.0
20.0
-6.0
ﬂat300 26 0
≤26
26
31.0
27.0
-4.0
ﬂat300 28 0
≤28
29
33.0
32.0
-1.0
le450 15a
15
15
15.0
15.0
0
le450 15b
15
15
15.0
15.0
0
le450 15c
15
15
16.0
15.0
-1.0
le450 15d
15
15
19.0
16.0
-3.0
mulsol.i.1
–
49
49.0
49.0
0
school1 nsh
≤14
14
14.0
15.0
+1.0
5
Conclusions
We have designed a new IA that incorporates a simple local search procedure
to improve the overall performances to tackle the GCP instances. The IA pre-
sented has only four parameters. To set correctly these parameters we use the
Information Gain function, a particular entropy function useful to understand

A Hybrid Immune Algorithm with Information Gain
181
Table 5. IA versus Hao et al.’s HCA algorithm. The number of independent runs is
10.
Instance G
HCA’s Best–Found and (SR)
IA’s Best–Found and (SR)
DSJC250.5
28 (90)
28 (100)
ﬂat300 28 0
31 (60)
32 (100)
le450 15c
15 (60)
15 (100)
le450 25c
26 (100)
25 (100)
the IA’s behavior. The Information Gain measures the quantity of information
that the system discovers during the learning process. We choose the parameters
that maximize the information discovered and that increases moderately the in-
formation gain monotonically. To our knowledge, this is the ﬁrst time that IAs,
and in general the EAs, are characterized in terms of information gain. We deﬁne
the average ﬁtness of population P hyp as the diversity in the current population,
when this value is equal to average ﬁtness of population P (t+1), we are close
a premature convergence. Using a simple coloring method we have investigated
the IA’s learning and solving capability. The experimental results show how the
proposed IA is comparable to and, in many GCP instances, outperforms the best
evolutionary algorithms. Finally, the designed IA is directed to solving GCP in-
stances although the solutions’ representation and the variation operators are
applicable more generally, for example Travelling Salesman Problem.
Acknowledgments. The authors wish to thank the anonymous referees for
their excellent revision work. GN wishes to thank the University of Catania
project “Young Researcher” for partial support and is grateful to Prof. A. M.
Anile for his kind encouragement and support.
References
1. Dasgupta, D. (ed.): Artiﬁcial Immune Systems and their Applications. Springer-
Verlag, Berlin Heidelberg New York (1999)
2. De Castro L.N., Timmis J.: Artiﬁcial Immune Systems: A New Computational
Intelligence Paradigm. Springer-Verlag, UK (2002)
3. Forrest, S., Hofmeyr, S. A.: Immunology as Information Processing. Design Princi-
ples for Immune System & Other Distributed Autonomous Systems. Oxford Univ.
Press, New York (2000)
4. Nicosia, G., Castiglione, F., Motta, S.: Pattern Recognition by primary and sec-
ondary response of an Artiﬁcial Immune System. Theory in Biosciences 120 (2001)
93–106
5. Galinier, P., Hao, J.: Hybrid Evolutionary Algorithms for Graph Coloring. Journal
of Combinatorial Optimization Vol. 3 4 (1999) 379–397
6. Marino, A., Damper, R.I.: Breaking the Symmetry of the Graph Colouring Prob-
lem with Genetic Algorithms. Workshop Proc. of the Genetic and Evolutionary
Computation Conference (GECCO’00). Las Vegas, NV: Morgan Kaufmann (2000)

182
V. Cutello, G. Nicosia, and M. Pavone
7. Garey, M.R., Johnson, D.S.: Computers and Intractability: a Guide to the Theory
of NP-completeness. Freeman, New York (1979)
8. Mehrotra, A., Trick, M.A.: A Column Generation Approach for Graph Coloring.
INFORMS J. on Computing 8 (1996) 344–354
9. Caramia, M., Dell’Olmo, P.: Iterative Coloring Extension of a Maximum Clique.
Naval Research Logistics, 48 (2001) 518–550
10. Johnson, D.S., Trick, M.A. (eds.): Cliques, Coloring and Satisﬁability: Second DI-
MACS Implementation Challenge. American Mathematical Society, Providence,
RI (1996)
11. De Castro, L. N., Von Zuben, F. J.: The Clonal Selection Algorithm with Engineer-
ing Applications. Proceedings of GECCO 2000, Workshop on Artiﬁcial Immune
Systems and Their Applications, (2000) 36–37
12. De Castro, L.N., Von Zuben, F.J.: Learning and optimization using the clonal
selection principle. IEEE Trans. on Evolutionary Computation Vol. 6 3 (2002)
239–251
13. Nicosia, G., Castiglione, F., Motta, S.: Pattern Recognition with a Multi–Agent
model of the Immune System. Int. NAISO Symposium (ENAIS’2001). Dubai,
U.A.E. ICSC Academic Press, (2001) 788–794
14. Eiben, A.E., Hinterding, R., Michalewicz, Z.: Parameter control in evolutionary
algorithms. IEEE Trans. on Evolutionary Computation, Vol. 3 2 (1999) 124–141
15. Leung, K., Duan, Q., Xu, Z., Wong, C.W.: A New Model of Simulated Evolu-
tionary Computation – Convergence Analysis and Speciﬁcations. IEEE Trans. on
Evolutionary Computation Vol. 5 1 (2001) 3–16
16. Seiden P.E., Celada F.: A Model for Simulating Cognate Recognition and Response
in the Immune System. J. Theor. Biol. Vol. 158 (1992) 329–357
17. Nicosia, G., Cutello, V.: Multiple Learning using Immune Algorithms. Proceedings
of the 4th International Conference on Recent Advances in Soft Computing, RASC
2002, Nottingham, UK, 12–13 December (2002)
18. Johnson, D.R., Aragon, C.R., McGeoch, L.A., Schevon, C.: Optimization by sim-
ulated annealing: An experimental evaluation; part II, graph coloring and number
partitioning. Operations Research 39 (1991) 378–406
19. Barbosa, V.C., Assis, C.A.G., do Nascimento, J.O.: Two Novel Evolutionary For-
mulations of the Graph Coloring Problem. Journal of Combinatorial Optimization
(to appear)

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 183–194, 2003.
© Springer-Verlag Berlin Heidelberg 2003
MILA – Multilevel Immune Learning Algorithm
DipankarDasgupta, Senhua Yu, and Nivedita Sumi Majumdar
Computer Science Division, University of Memphis, Memphis, TN 38152, USA
{dasgupta, senhuayu, nmajumdr}@memphis.edu
Abstract. The biological immune system is an intricate network of specialized
tissues, organs, cells, and chemical molecules. T-cell-dependent humoral im-
mune response is one of the complex immunological events, involving interac-
tion of B cells with antigens (Ag) and their proliferation, differentiation and
subsequent secretion of antibodies (Ab). Inspired by these immunological prin-
ciples, we proposed a Multilevel Immune Learning Algorithm (MILA) for novel
pattern recognition. It incorporates multiple detection schema, clonal expansion
and dynamic detector generation mechanisms in a single framework. Different
test problems are studied and experimented with MILA for performance
evaluation. Preliminary results show that MILA is flexible and efficient in de-
tecting anomalies and novelties in data patterns.
1   Introduction
The biological immune system is of great interest to computer scientists and engineers
because it provides a unique and fascinating computational paradigm for solving com-
plex problems. There exist different computational models inspired by the immune
system. A brief survey of some of these models may be found elsewhere [1]. Forrest et
al. [2–4] developed a negative-selection algorithm (NSA) for change detection based
on the principles of self-nonself discrimination. This algorithm works on similar prin-
ciples, generating detectors randomly, and eliminating the ones that detect self, so that
the remaining detectors can detect any non-self. If any detector is ever matched, a
change (non-self) is known to have occurred. Obviously, the first phase is analogous
to the censoring process of T cells maturation in the immune system. However, the
monitoring phase is logically (not biologically) derivable.
The biological immune system employs a multilevel defense against invaders
through nonspecific (innate) and specific (adaptive) immunity. The problems for
anomaly detection also need multiple detection mechanisms to obtain a very high
detection rate with a very low false alarm rate. The major limitation of binary NSA is
that it generates a higher false alarm rate when applied to anomaly detection for some
data sets. To illustrate this limitation, some patterns, for example, 110, 100, 011, 001,
are considered as normal samples. Based on these normal samples, 101, 111, 000, 010
become abnormal. A partial matching rule is usually used to generate a set of detec-
tors. As described in [5], with matching threshold (r = 2), two strings (one represents

184          D. Dasgupta, S. Yu, and N.S. Majumdar
candidate detector, another is a pattern) match if and only if they are identical in at
least 2 contiguous positions. Because the detector must fail to match any string in
normal samples, for the above example, the detectors cannot be generated at all, and
consequently anomalies cannot be detected; except for r = 3 (length of the string),
which results in exact match and requires all non-self strings as detectors.
In order to alleviate these difficulties, we proposed an approach, called Multilevel
Immune Learning Algorithm (MILA). There are several features which distinguish this
algorithm from the NSA; in particular, multilevel detection and immune memory. In
this paper, we describe this approach and show the advantages of using new features
of MILA in the application of anomaly detection.
The layout of this paper is as follows. Section 2 outlines the proposed algorithm.
Section 3 briefly describes the application of MILA to anomaly detection. Section 4
reports some experimental results with different testing problems. Section 5 discusses
new features of MILA indicated in the application of anomaly detection. Section 6
provides concluding remarks.
2   Multilevel Immune Learning Algorithm (MILA)
This approach is inspired by the interaction and processes of T cell-dependent humoral
immune response. In biological immune systems, some B cells recognize antigens
(foreign protein) via immunoglobulin receptors on their surface but are unable to pro-
liferate and differentiate unless prompted by the action of lymphokines secreted by T
helper cells. Moreover, in order for T helper cells to become stimulated to release
lymphokines, they must also recognize specific antigens. However, while T helper
cells recognize antigens via their receptors, they can only do so in the context of MHC
molecules. Antigenic peptides must be extracted by several types of cells called anti-
gen-presenting cells (APCs) through a process called “Ag presentation.” Under certain
conditions, however, B-cell activation is suppressed by T suppressor cells, but specific
mechanisms for such suppression are yet unknown. The activated B cells and T cells
migrate to the primary follicle of the cortex in lymph nodes, where a complex interac-
tion of the basic cell kinetic process of proliferation (cloning), mutation, selection,
differentiation, and death of B-cells occurs through germinal center reaction [6] and
finally secretes antibodies. These antibodies function as effectors to the humoral re-
sponse by binding to antigens and facilitating their elimination. The proposed artificial
immune system is an abstract of complex multistage immunological events in humoral
immune response. The algorithm consists of initialization phase, recognition phase,
evolutionary phase and response phase. As shown in Fig.2, the main features of each
phase can be summarized as follows:
 
In initialization phase, the detection system is “trained” by giving the knowl-
edge of “self”. The outcome of the initialization is to generate sets of detec-
tors, analogous to the populations of T helper cells (Th), T suppressor cells
(Ts) and B cells, which participate in T cell dependent humoral immune re-
sponse.

MILA – Multilevel Immune Learning Algorithm         185
 
In recognition phase, B cells, together with T cells (Th, Ts) and antigen pre-
senting cells (APCs), form a multilevel recognition. APC is an extreme high-
level detector, which acts as a default detector (based on environment) identi-
fying visible damage signals from the system. For example, while monitoring
a computer system, screen turning black, too many lining-up printing jobs
and so on may provide visible signals captured by APC. Thus, APC is not de-
fined based on particular normal behavior in input data. It is to be noted that
T cells and B cells recognize antigens at different levels. The recognition of
Th is defined as a bit-level (lowest level) recognition, such as using consecu-
tive windows of data pattern. Importantly, B cells in the immune system only
recognize particular sites called epitope on the surface of the antigen, as
shown in Fig.1. Clearly, the recognition (matching) sites are not contiguous
when we stretch out the 3-dimension folding of the antigen protein. Thus, the
B cell is considered as feature-level recognition at different non-contiguous
(occasionally contiguous) positions of antigen strings. Accordingly, MILA
can provide multilevel detection in hierarchical fashion, starting with APC
detection, B-cell detection and T-cell detection. However, Ts acts as suppres-
sion and is problem dependent. As shown in fig. 2, the logical operator can
be set to ∧(AND) or ∨(OR) to make the system more fault-tolerant or
more sensitive as desired.
 
In evolutionary phase, the activated B cells clone to produce memory cells
and plasma cells. Cloning is subject to very high mutation rates called so-
matic hypermutation with a selective pressure. In addition to passing negative
selection, for each progeny of the activated B cell (parent B cell), only the
clones with higher affinity are selected. This process is known as positive
selection. The outcome of evolutionary phase is to generate high-quality de-
tectors with specificity to the exposed antigens for future use.
 
Response phase involves primary response to initial exposure and secondary
response to the second encounter.
Accordingly, the above mechanism steps, as shown in the Fig.2, give a general de-
scription of MILA, however, based on applications and timeliness of execution, some
detection phase may not be considered.
Fig. 1. B Cell Receptor Matches an antigenic protein in its surface

186          D. Dasgupta, S. Yu, and N.S. Majumdar
Fig. 2. Overview of Multilevel Immune Learning Algorithm (MILA)
3 Application of MILA to Anomaly Detection Problems
Detecting anomaly in a system or in a process behavior is very important in many
real-world applications. For example, high-speed milling processes require continuous
monitoring to assure high quality production; jet engines also require continuous
monitoring to assure safe operation. It is essential to detect the occurrence of unnatural
events as quickly as possible before any significant performance degradation results
[5]. There are many techniques for anomaly detection, and depending on application
domains, these are referred to as novelty detection, faulty detection, surprise pattern
detection, etc. Among these approaches, the detection algorithm with better discrimi-
nation ability will have a higher detection rate. In particular, it can accurately dis-
criminate the normal data and the observed data during monitoring. The decision-
making systems for detection usually depend on learning the behavior of the moni-
tored environment from a set of normal (positive) data. By normal, we mean usage
data that have been collected during the normal operation of the system or a process.
In order to evaluate the performance, MILA is applied to the anomaly detection prob-

MILA – Multilevel Immune Learning Algorithm         187
lem. For this problem, the following assumptions are made to simplify the implemen-
tation:
 
In Initialization phase and Recognition phase, Ts detectors employ more strin-
gent threshold than Th detectors and B detectors.
 
Ts detector is regarded as a special self-detecting agent. In Initialization
phase, Ts detector will be selected if it still matches the self-antigen under
more stringent threshold, whereas in Recognition phase the response will be
terminated when Ts detector matches a special antigen resembling self-data
pattern. Similar to Th and B cells, the activated Ts detector undergoes cloning
and positive selection after being activated by a special Ag.
 
APC-detectors, as shown in Fig.2, are not used in this application.
 
The lower the antigenic affinity, the higher the mutation rate. From a com-
putational perspective, the purpose of this assumption is to increase the prob-
ability of producing effective detectors.
 
For each parent cloning, only ONE clone whose affinity is the highest among
all clones is kept. The selected clone will be discarded if it is similar to the
existing detectors. This assumption solves the problem using minimal re-
sources without compromising the detection rate.
 
Currently, the response phase is dummy as we are only dealing with anomaly
detection tasks.
This application employs a distance measure (Euclidean distance) to calculate the
affinity between the detector and the self/nonself data pattern along with a partial
matching rule. Overall, the implementation of MILA for anomaly detection can be
summarized as follows:
1. Collect Self data sufficient to exhibit the normal behavior of a system and
choose a technique to normalize the raw data.
2. Generate different types of detectors, e.g., B, Th, Ts detectors. Th and B de-
tectors should not match any of self-peptide strings according to the partial
matching rule. The sliding window scheme [5] is used for Th partial match-
ing. The random position pick-up scheme is used for B partial matching. For
example, suppose that a self string is <s1, s2, …, sL> and the window size is
chosen as 3, then the self peptide strings can be <s1, s3, sL>, < s2, s4, s9 >, < s5,
s7, s8 > and so on by randomly picking up the attribute at some positions. If
the candidate B detector represented as <m1, m2, m3 > fails to match Any self-
feature indexed as <1, 3, L1> in self-data patterns, the candidate B detector is
selected and represented as <(1, m1), (3, m2), (L1, m3)>. Two important pa-
rameters, Th threshold and B threshold, are employed to measure the match-
ing. If the value for the distance between the Th (or B) detector and the self
string is greater than Th (or B) threshold, then it is considered as matching. Ts
detector, however, is selected if it can match the special self strings by em-
ploying more stringent suppressor threshold called Ts threshold.

188          D. Dasgupta, S. Yu, and N.S. Majumdar
3. When monitoring the system, the logical operator shown in Fig.1 is chosen as
“AND ( ∧)” in this application. The unseen pattern is tested by Th, Ts, B de-
tector, respectively. If any Th and B detector is ever activated (matched with
current pattern) and all of the Ts detectors are not activated, a change in be-
havior pattern is known to have occurred and an alarm signal is generated in-
dicating an abnormality.  The same matching rule is adopted as used in gen-
erating detectors. We calculate the distance between the Th / Ts detector and
the new sample as described in [5]. B detector is actually an information
vector with the information of binding sites and values of attributes in these
sites. For the B detector <(1, m1), (3, m2), (L, m3)> in the above example, if
an Ag is represented as <n1, n2, n3, …, nL>,  then the distance is calculated
only between points <m1, m2, m3> and < n1, n3, nL >.
4. Activated Th, Ts, B detectors are cloned with a high mutation rate and only
one clone with the highest affinity is selected. Detectors that are not activated
are kept in detector sets.
5. Employ the optimized detectors generated after the detection phase to test the
unseen patterns, repeat from step 3.
4   Experiments
4.1   Data Sets
We experimented with different datasets to investigate the performances of MILA for
detecting anomalous patterns. The paper only reported results of using speech-
recording time series dataset (see reference [8]) because of space limitations. We
normalized the raw data (total 1025 time steps) at the range 0~1 for training the sys-
tem. The testing data (total 1025 time steps) are generated that contain anomalies
between 500 and 700 and some noise after 700 time steps.
4.2   Performance Measures
Using a sliding (overlapping) window of size L (in our case, L =13), if normal series
have the values: x1, x2, …, xm, self-patterns are generated as follows:
                                <x1,         x2,       …   xL>
                                <x2,         x3,       …   xL+1>
                                   .            .          .      .
                                <xm-L+1,  xm-L+2, …,  xm>
Similarly, Ag-patterns are generated from the samples shown in Fig.4b. In this ex-
periment, we used real-valued strings to represent Ag and Ab molecules, which is
different from binary Negative Selection Algorithm [4, 5, 9] and Clone Selection
Principle application [10]. Euclidean distance measure is used to model the complex

MILA – Multilevel Immune Learning Algorithm         189
chemistry of Ag/Ab recognition as a matching rule. Two measures of effectiveness for
detecting anomaly are calculated as follows:
Detection rate 
TP
TP
FN
=
+
False alarm rate
FP
TN
FP
=
+
Where TP (true positives), anomalous elements identified as anomalous; TN (true
negatives), normal elements identified as normal; FP (false positives), normal ele-
ments identified as anomalous; FN (false negatives), anomalous elements identified as
the normal [11].
The MILA algorithm has a number of tuning parameters. Different detector
thresholds that determine whether a new sample is normal or abnormal control the
sensitivity of the system. Employing various strategies to change threshold values,
different values for detection rate and false alarm rate are obtained that are used for
plotting the ROC (Receiver Operating Characteristics) curve, which reflects tradeoff
between false alarm rate and detection rate.
4.3   Experimental Results
The following test cases are studied and some results are reported in this paper:
1. For different threshold changing strategies the influence on ROC curves is
studied. In this paper, we report the results obtained from three different
cases: (1) changing B threshold at fixed Th threshold (0.05 if B threshold is
less than 0.16, otherwise 0.08) and Ts threshold (0.02); (2) changing B
threshold at fixed Th threshold (0.1) and Ts threshold (0.02); (3) changing Th
threshold at fixed B threshold (0.1) and Ts threshold (0.02). The results
shown in Fig.3 indicate that the first case obtain a better ROC curve. There-
fore, this paper uses this strategy to obtain different values for detection and
false alarm rate for MILA based anomaly detection.
2. The comparison of performances illustrated in ROC curves between single
level detection and multilevel detection (MILA) is studied. We experimented
and compared the efficiency of anomaly detection in three cases: (1) only
using Th detectors; (2) only using B detectors; (3) combining Th, Ts, B detec-
tors as indicated in MILA. ROC curves in these cases are shown in Fig.4.
Moreover, Fig.5 show how detection and false alarm rates change when
threshold is modified in these three cases. Since detectors are randomly gen-
erated, different values for detection and false alarm rates are observed. Con-
sidering this issue, we run the system ten iterations to obtain the average of
the values for detection and false alarm rate, as shown in Fig.4 and Fig.5.

190          D. Dasgupta, S. Yu, and N.S. Majumdar
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
Threshold
False Alarm Rate
T Detection
B Detection
MILA
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
Threshold
Detection Rate
T Detection
B D etection
MILA
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
False Alarm Rate
Detection Rate
Strategy 1
Strategy 2
Strategy 3
Fig. 3. ROC curves obtained by employing different thresholds changing strategy as described
in the section 4.3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
False Alarm Rate
Detection Rate
T Detection
B Detection
MILA
Fig. 4. Comparison of ROC curves between single level detection (e.g., Th detection or B detec-
tion) and multilevel detection (MILA)
(a)
                     (b)
Fig. 5. Evolution of detection rate in Fig. 5(a) and false alarm rate in Fig. 5(b) based on single
level detection and multilevel detection (MILA) with changing threshold values
3. The efficiency of the detector for detecting anomaly is studied. Once detec-
tors, e.g., Th detectors, Ts detectors and B detects, are generated in Initializa-
tion phase, we repeatedly tested the same abnormal samples for 5 iterations

MILA – Multilevel Immune Learning Algorithm         191
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
Threshold
Detection Rate
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0
0.05
0.1
0.15
0.2
0.25
Threshold
False Alarm Rate
with same parameter settings. Since the detector in MILA undergoes cloning,
mutation and selection after Recognition phase, the elements in the detector
set changes after each iteration in detecting phase, although the same abnor-
mal samples and conditions are employed in Recognition phase. So, for each
iteration, different values for detection and false alarm rate are observed, as
shown in Fig. 6 through Fig.7.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
False Alarm Rate
Detection Rate
1
2
3
4
5
Fig. 6. ROC curves for MILA based anomaly detection in each detecting iteration. 1, 2, 3, …in
the ROC curves denote the iterations of detecting same Ag samples. For each iteration, the
detector sets are those that are generated in the detect phase of previous iteration.
                     (a)      
                           (b)
Fig. 7. Evolution of detection rate in Fig. 7(a) and false alarm rate in Fig. 7(b) for MILA based
anomaly detection in each detecting iteration as described in Fig.6 when threshold is varied.
5   New Features of MILA
The algorithm presented here takes its inspiration from T-cell-dependent humoral
immune response. Considering the application to anomaly detection, one of the key
features of MILA is its multilevel detection; that is, multiple strategies are used to
generate detectors, which are combined to detect anomalies in new samples.
    Preliminary experiments show that MILA is flexible and unique. The generation and
recognition of various detectors in this algorithm can be implemented in different
ways depending on the application. Moreover, the efficiency of anomaly detection can

192          D. Dasgupta, S. Yu, and N.S. Majumdar
be improved by tuning threshold values for different detection scheme. Fig.3 shows
this advantage of MILA and indicates that better performance can be obtained (shown
in the ROC curves) by employing different threshold changing strategies. Compared to
the Negative Selection Algorithm (NSA), which uses single level detection scheme,
Fig.4 shows that the performance of multilevel detection of MILA is better. Further
results shown in Fig.5 also support the superior performance of MILA. Specifically,
when comparing multilevel detection (MILA) with single detection scheme (NSA),
the varying trend for detection rate when the threshold is modified is similar as illus-
trated in Fig.5(a) (at least relative to false alarm rate as shown in Fig.5(b); however,
the false alarm rate for multilevel detection (when the threshold is modified) is much
lower.
For anomaly detection using NSA, the detector set remains constant once generated
in the training phase. However, the detector set is dynamic for MILA based anomaly
detection. MILA involves a process of cloning, mutation and selection after successful
detection, and some detectors with high affinity for a given anomalous pattern will be
selected. This constitutes an on-line learning and detector optimization process. The
outcome is to update the detector set and affinity of those detectors that have proven
themselves to be valuable by having recognized more frequently occurring anomalies.
Fig.6 shows improved performance by using the optimized detector set being gener-
ated after the detection phase. This can be explained by the fact that some of the
anomalous data employed in our experiment are similar, but generally anomaly is
much different from normal series. Thus, when we reduce the distance between a
detector and a given abnormal pattern, that is, increase the detector affinity for this
pattern, the distances between this detector and other anomalies similar to the given
abnormal pattern are also reduced so that those anomalies which formerly failed to be
detected by this detector become detectable. However, the distances between the de-
tector and most of the “self”, except for some “self” very similar to “non-self” (anom-
aly), are still exceeds allowable variation. Therefore, the number of detectors having
high affinity increases with the increase in the times of detecting the antigens that are
encountered before (at least in a certain range) and thus the detection rate at certain
thresholds becomes higher and higher. The experimental results confirm this explana-
tion. Under the same threshold values, Fig.7(a) shows that the detector set produced
later has a higher detection rate than the previous detector set, whereas the false alarm
rate is almost unchanged as shown in Fig.7(b). In the application to anomaly detection,
because of the random generation of the pre-detector, the generated detector set is
always different, even if the absolutely same conditions are applied. We cannot guar-
antee the efficiency of the initial detector set. However, MILA based anomaly detec-
tion can optimize the detector during on-line detection and thus we can finally obtain
more efficient detectors for given samples for monitoring.
As a summary of our proposed principle and initial experiments, the follow-
ing features of MILA have been observed on anomaly detection:
 
Unites several different immune system metaphors rather than implementing
the immune system metaphors in a piecemeal manner.

MILA – Multilevel Immune Learning Algorithm         193
 
Uses multilevel detection to find and patch the security hole in a large com-
puter system as much as possible.
 
MILA is more flexible than single detection scheme (e.g. Negative Selection
Algorithm). The implementation for detector generation is problem depend-
ent. More thresholds and parameters may be modified for tuning the system
performance.
 
Detector set in MILA is dynamic whereas detector set in Negative Selection
Algorithm remains constant once it is generated in training phase. MILA in-
volves cloning, mutation and selection after detect phase, which is similar but
not equal to Clone Selection Theory. The process of cloning in MILA is tar-
geted (not blind) cloning. Only those detectors that are activated in recogni-
tion phase can be cloned. The process of cloning, mutation and selection in
MILA is actually a process of detector on-line learning and optimization.
Only those clones with high affinity can be selected. This strategy ensures
that both the speed and accuracy of detection become successively higher
after each detecting.
 
MILA is initially inspired by humoral immune response but spontaneously
unites the main feature of Negative Selection Algorithm and Clone Selection
Theory. It imports their merits but has its own features
6   Conclusions
In this paper, we outlined a proposed change detection algorithm inspired by the T-
cell-dependent humoral immune response. This algorithm is called Multilevel Immune
Learning Algorithm (MILA), which involves four phases: Initialization phase, Recog-
nition phase, Evolutionary phase and Response phase. The proposed method is tested
with an anomaly detection problem. MILA based anomaly detection is characterized
by multilevel detection and on-line learning technique. Experimental results show that
MILA based anomaly detection is flexible and the detection rate can be improved at
the range of allowable false alarm rate by applying different threshold changing
strategies. In comparison with single level based anomaly detection, the performance
of MILA is clearly better. Experimental results show that detectors have been opti-
mized during the on-line testing phase as well. Moreover, by busing different logical
operators, it is possible to make the system very sensitive to any changes or robust to
noise. Reducing complexity of the algorithm, proposing appropriate suppression
mechanism, implementing response phase and experimenting with different data sets
are the main directions of our future work.

194          D. Dasgupta, S. Yu, and N.S. Majumdar
Acknowledgement. This work is supported by the Defense Advanced Research
Projects Agency (no. F30602-00-2-0514). The authors would like to thank the source
of the datasets: Keogh, E. & Folias, T. (2002). The UCR Time Series Data Mining
Archive [http://www.cs.ucr.edu/~eamonn/TSDMA/index.html]. Riverside CA. Univer-
sity of California – Computer Science & Engineering Department.
References
1.
Dasgupta, D., Attoh-Okine, N.: Immunity-Based Systems: A Survey. In the proceedings of
the IEEE International Conference on Systems, Man and Cybernetics, Orlando, October
12–15, 1997
2.
Forrest, S., Hofmeyr, S., Somayaji, A.: Computer Immunology. Communications of the
ACM 40(10) (1997)  pp 88–96.
3.
Forrest, S., Somayaji, A., Ackley, D.: Building Diverse Computer Systems. Proc. of the
Sixth Workshop on Hot Topics in Operating Systems (1997).
4.
Forrest, S., Perelson, A. S., Allen, L., Cherukuri, R.: Self-nonself discrimination in a com-
puter. Proc. of the IEEE Symposium on Research in Security and Privacy, IEEE Computer
Society Press, Los Alamitos, CA, (1994) 202–212
5.
Dasgupta, D., Forrest, S.: An Anomaly Detection Algorithm Inspired by the Immune Sys-
tem. In: Dasgupta D (eds) Artificial Immune Systems and Their Applications, Springer-
Verlag, (1999) 262–277
6.
Hollowood, K., Goodlad, J.R.: Germinal centre cell kinetics. J. Pathol.185(3) (1998) 229–
33
7.
Perelson, A. S., Oster, G. F.: Theoretical studies of clonal selection: Minimal antibody
repertoire size and reliability of self- non-self discrimination. J. Theor.Biol. 81(4) (1979)
645–670
8.
 Keogh, E., Folias, T.: The UCR Time Series Data Mining Archive 
[http://www.cs.ucr.edu/~eamonn/TSDMA/index.html]. Riverside CA. University of Cali-
fornia – Computer Science & Engineering Department. (2002)
9.
D'haeseleer, P., Forrest, S., Helman, P.: An immunological approach to change detection:
algorithms, analysis, and implications. Proceedings of the 1996 IEEE Symposium on Com-
puter Security and Privacy, IEEE Computer Society Press, Los Alamitos, CA, (1996) 110–
119
10. de Castro, L. N., Von Zuben, F. J.: Learning and optimization using the clonal selection
principle. IEEE Transactions on Evolutionary Computation 6(3) (2002) 239–251
11. Gonzalez, F., Dasgupta, D.: Neuro-Immune and SOM-Based Approaches: A Comparison.
Proceedings of 1st International Conference on Artificial Immune Systems (ICARIS-
2002), University of Kent at Canterbury, UK, September 9
th–11
th, 2002

The Effect of Binary Matching Rules in Negative
Selection
Fabio Gonz´alez1, Dipankar Dasgupta2, and Jonatan G´omez1
1 Division of Computer Science, The University of Memphis , Memphis TN 38152
and Universidad Nacional de Colombia, Bogot´a, Colombia
{fgonzalz,jgomez}@memphis.edu
2 Division of Computer Science, The University of Memphis, Memphis TN 38152
dasgupta@memphis.edu
Abstract. Negative selection algorithm is one of the most widely used techniques
in the ﬁeld of artiﬁcial immune systems. It is primarily used to detect changes in
data/behavior patterns by generating detectors in the complementary space (from
given normal samples). The negative selection algorithm generally uses binary
matching rules to generate detectors. The purpose of the paper is to show that the
low-level representation of binary matching rules is unable to capture the structure
of some problem spaces. The paper compares some of the binary matching rules
reported in the literature and study how they behave in a simple two-dimensional
real-valued space. In particular, we study the detection accuracy and the areas
covered by sets of detectors generated using the negative selection algorithm.
1
Introduction
Artiﬁcial immune systems (AIS) is a relatively new ﬁeld that tries to exploit the mech-
anisms present in the biological immune system (BIS) in order to solve computational
problems. There exist many AIS works [5,8], but they can roughly be classiﬁed into two
major categories: techniques inspired by the self/non-self recognition mechanism [12]
and those inspired by the immune network theory [9,22].
The negative selection (NS) algorithm was proposed by Forrest and her group [12].
This algorithm is inspired by the mechanism of T-cell maturation and self tolerance in the
immune system. Different variations of the algorithm have been used to solve problems
of anomaly detection [4,16], fault detection [6], to detect novelties in time series [7],
and even for function optimization [3].
A process that is of primary importance for the BIS is the antibody-antigen matching
process, since it is the basis for the recognition and selective elimination mechanism that
allows to identify foreign elements. Most of the AIS models implement this recognition
process, but in different ways. Basically, antigens and antibodies are represented as
strings of data that correspond to the sequence of aminoacids that constituting proteins
in the BIS. The matching of two strings is determined by a function that produces a
binary output (match or not-match).
The binary representation is general enough to subsume other representations; after
all, any data element, whatever its type is, is represented as a sequence of bits in the
memory of a computer (though, how they are treated may differ). In theory, any matching
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 195–206, 2003.
c⃝Springer-Verlag Berlin Heidelberg 2003

196
F. Gonz´alez, D. Dasgupta, and J. G´omez
rule deﬁned on a high-level representation can be expressed as a binary matching rule.
However, in this work, we restrict the use of the term binary matching rule to designate
thoserulesthattakeintoaccountthematchingofindividualbitsrepresentingtheantibody
and the antigen.
Most works on the NS algorithm have been restricted to binary matching rules like
r-contiguous [1,10,12]. The reason is that efﬁcient algorithms that generate detectors
(antibodies or T-cell receptors) have been developed, exploiting the simplicity of the
binary representation and its matching rules [10]. On the other hand, AIS approaches
inspired by the immune network theory often use real vector representation for antibodies
and antigens [9,22], as this representation is more suitable for applications in learning and
data analysis. The matching rules used with this real-valued representation are usually
based on Euclidean distance, (i.e. the smaller the antibody-antigen distance, the more
afﬁnity they have).
TheNSalgorithmhasbeenappliedsuccessfullytosolvedifferentproblems;however,
some unsatisfactory results have also been reported [20].As it was suggested by Balthrop
et al. [2], the source of the problem is not necessarily the NS algorithm itself, but the
kind of matching rule used. The same work [2] proposed a new binary matching rule,
r-chunk matching (Equation 2 in Section 2.1), which appears to perform better than
r-contiguous matching.
The starting point of this paper is to address the question: do the low-level repre-
sentation and its matching rules affect the performance of NS in covering the non-self
space? This paper provides some answers to this issue. Speciﬁcally, it shows that the
low-level representation of the binary matching scheme is unable to capture the structure
of even simple problem spaces. In order to justify our argument, we use some of the
binary matching rules reported in the literature and study how they behave in a simple
bi-dimensional real space. In particular, we study the shape of the areas covered by
individual detectors and by a set of detectors generated by the NS algorithm.
2
The Negative Selection Algorithm
Forrest et al. [12] developed the NS algorithm based on the principles of self/non-self
discrimination in the BIS. The algorithm can be summarized as follows (taken from [5]):
– Deﬁne self as a collection S of elements in a representation space U (also called
self/non-self space), a collection that needs to be monitored.
– Generate a set R of detectors, each of which fails to match any string in S.
– Monitor S for changes by continually matching the detectors in R against S.
2.1
Binary Matching Rules in Negative Selection Algorithm
The previous description is very general and does not say anything about what kind of
representation space is used or what the exact meaning of matching is. It is clear that the
algorithmic problem of generating good detectors varies with the type of representation
space (continuous, discrete, hybrid, etc.), the detector representation, and the process
that determines the matching ability of a detector.

The Effect of Binary Matching Rules in Negative Selection
197
A binary matching rule is deﬁned in terms of individual bit matchings of detectors
and antigens represented as binary strings. In this section, some of the most widely used
binary matching rules are presented.
r-contiguous matching. The ﬁrst version of the NS algorithm [12] used binary strings
of ﬁxed length, and the matching between detectors and new patterns is determined by
a rule called r-contiguous matching. The binary matching process is deﬁned as follows:
given x = x1x2...xn and a detector d = d1d2...dn,
d matches x ≡∃i ≤n −r + 1 such that xj = dj for j = i, ..., i + r −1,
(1)
that is, the two strings match if there is a sequence of size r where all the bits are identical.
The algorithm works in a generate-and-test fashion, i.e. random detectors are generated;
then, they are tested for self-matching. If a detector fails to match a self string, it is
retained for novel pattern detection.
Subsequently, two new algorithms based on dynamic programming were proposed
[10], the linear and the greedy NS algorithm. Similar to the previous algorithm, they are
also speciﬁc to binary string representation and r-contiguous matching. Both algorithms
run in linear time and space with respect to the size of the self set, though the time and
space are exponential on the size of the matching threshold, r.
r-chunk matching.
Another binary matching scheme called r-chunk matching was
proposed by Balthrop et al. [1]. This matching rule subsumes r-contiguous matching,
that is, any r-contiguous detector can be represented as a set of r-chunk detectors. The
r-chunk matching rule is deﬁned as follows: given a string x = x1x2...xn and a detector
d = (i, d1d2...dm), with m ≤n and i ≤n −m + 1,
d matches x ≡xj = dj for j = i, ..., i + m −1,
(2)
where i represents the position where the r-chunk starts. Preliminary experiments [1]
suggest that the r-chunk matching rule can improve the accuracy and performance of
the NS algorithm.
Hammingdistancematchingrules. OneoftheﬁrstworksthatmodeledBISconceptsin
developing pattern recognition was proposed by Farmer et al. [11].Their work proposed a
computational model of the BIS based on the idiotypic network theory of Jerne [19], and
compared it with the learning classiﬁer system [18]. This is a binary model representing
antibodies and antigens and deﬁning a matching rule based on the Hamming distance. A
Hamming distance based matching rule can be deﬁned as follows: given a binary string
x = x1x2...xn and a detector d = d1d2...dn,
d matches x ≡

i
xi ⊕di ≥r,
(3)
where ⊕is the exclusive-or operator, and 0 ≤r ≤n is a threshold value.

198
F. Gonz´alez, D. Dasgupta, and J. G´omez
Different variations of the Hamming matching rule were studied, along with other
rules like r-contiguous matching, statistical matching and landscape-afﬁnity matching
[15]. The different matching rules were compared by calculating the signal-to-noise
ratio and the function-value distribution of each matching function when applied to
a randomly generated data set. The conclusion of the study was that the Rogers and
Tanimoto (R&T) matching rule, a variation of the Hamming distance, produced the
best performance. The R&T matching rule is deﬁned as follows: given a binary string
x = x1x2...xn and a detector d = d1d2...dn,
d matches x ≡

i
xi ⊕di

i
xi ⊕di + 2

i
xi ⊕di
≥r,
(4)
where ⊕is the exclusive-or operator, and 0 ≤r ≤1 is a threshold value.
It is important to mention that a good detector generation scheme for this kind of
rules is not available yet, other than the exhaustive generate-and-test strategy [12].
3
Analyzing the Shape of Binary Matching Rules
Usually, the self/non-self space (U) used by the NS algorithm corresponds to an ab-
straction of a speciﬁc problem space. Each element in the problem space (e.g. a feature
vector) is mapped to a corresponding element in U (e.g. a bit string).
A matching rule deﬁnes a relation between the set of detectors1 and U. If this re-
lationship is mapped back to the problem space, it can be interpreted as a relation of
afﬁnity between elements in this space. In general, it is expected that elements that are
matched by the same detector have some common property. So, a way to analyze the
ability of a matching rule to capture this ‘afﬁnity’ relationship in the problem space is
to take the subset of U corresponding to the elements matched by a speciﬁc detector,
and map this subset back to the problem space. Accordingly, this set of elements in the
problem space is expected to share some common properties.
In this section, we apply the approach described above to study the binary matching
rules presented in section 2.1. The problem space used corresponds to the set [0.0, 1.0]2.
One reason for choosing this problem space is that multiple problems in learning, pattern
recognition, and anomaly detection can be easily expressed in an n-dimensional real-
valued space. Also, it makes easier to visualize the shape of different matching rules.
All the examples and experiments in this paper use a self/non-self space composed of
binary strings of length 16.An element (x, y) in the problem space is mapped to the string
b0, ..., b7, b8, ..., b15,wheretheﬁrst8bitsencodetheintegervalue⌊255 · x + 0.5⌋andthe
last 8 bits encode the integer value ⌊255 · y + 0.5⌋. Two encoding schemes are studied:
conventional binary representation and Gray encoding. Gray encoding is expected to
favor binary matching rules, since the codiﬁcations of two consecutive numbers only
differs by one bit.
1 In some matching rules, the set of detectors is same as U (e.g. r-contiguous matching). In other
cases, it is a different set that usually contains or extends U (e.g. r-chunk matching).

The Effect of Binary Matching Rules in Negative Selection
199
Figure 1 shows some typical shapes generated by different binary matching rules.
Each ﬁgure represents the area (in the problem space) covered by one detector located
at the center, (0.5,0.5) (1000000010000000 in binary notation). In the case of r-chunk
matching, the detector does not correspond to an entire string representing a point on
the problem space, rather, it represents a substring (chunk). Thus, we chose an r-chunk
detector that matches the binary string corresponding to (0.5,0.5), ****00001000****.
The area covered by a detector is drawn using the following process: the detector is
matched against all the binary strings in the self/non-self space; then, all the strings
that match are mapped back to the problem space; ﬁnally, the corresponding points are
painted in gray color.
(a)
(b)
(c)
(d)
Fig. 1. Areas covered in the problem space by an individual detector using different matching
rules. The detector corresponds to 1000000010000000, which is the binary representation of the
point (0.5,0.5). (a) r-contiguous matching, r = 4, (b) r-chunk matching, d = ****00001000****,
(c) Hamming matching, r = 8, (d) R&T matching, r = 0.5.
The shapes generated by the r-contiguous rule (Figure 1(a)) are composed by vertical
and horizontal stripes that constitute a grid-like shape. The horizontal and vertical stripes
correspond to sets of points having identical bits at least at r contiguous positions in the
encoded space. Some of these points, however, are not close to the detector in the decoded
(problem) space. The r-chunk rule generates similar, but simpler shapes (Figure 1(b)).
In this case, the area covered is composed of vertical or horizontal sets of parallel strips.
The orientation depends on the position of the r-chunk: if it is totally contained in the
ﬁrst eight bits, the strips are vertically going from top to bottom; if it is contained on the
last eight bits, the strips are oriented horizontally; ﬁnally, if it covers both parts, it has
the shape shown in Figure 1(b).
The area covered by Hamming and R&T matching rules has a fractal-like shape,
shown in Figure 1(c) and 1(d), i.e. it exhibits self-similarity. It is composed of points
that have few interconnections. There is no signiﬁcant difference between the shapes
generated by the R&T rule and those generated by the Hamming rule, which is not a
surprise, considering the fact that the R&T rule is based on Hamming distance.
The shape of the areas covered by r-contiguous and r-chunk matching is not affected
by the change in codiﬁcation from binary to Gray (as shown in Figures 2(a) and 2(b)).
This is not the case with the Hamming and the R&T matching rule (Figures 2(c) and

200
F. Gonz´alez, D. Dasgupta, and J. G´omez
2(d)).The reason is that the Gray encoding represents consecutive values using bit strings
with small Hamming distance.
(a)
(b)
(c)
(d)
Fig. 2. Areas covered in the problem space by an individual detector using Gray encoding for
the self/non-self space. The detector corresponds to 1100000011000000, which is the Gray rep-
resentation of the point (0.5,0.5). (a) r-contiguous matching, r = 4, (b) r-chunk matching, d =
******0011******, (c) Hamming matching, r = 8, (d) R&T matching, r = 0.5.
The different matching rules and representations generate different types of detector
coveringshapes.Thisreﬂectsthebiasintroducedbyeachrepresentationandthematching
scheme. It is clear that the relation of proximity exhibited by these matching rules in
the binary self/non-self space does not coincide with the natural relation of proximity
in a real-valued, two-dimensional space. Intuitively, this seems to make the task harder
of placing these detectors to cover the non-self space without covering the self set. This
fact is further investigated in the next section.
4
Comparing the Performance of Binary Matching Rules
This section shows the performance of the binary matching rules (as presented in section
2.1) in the NS algorithm. A generate-and-test NS algorithm is used. Experiments are
performed using two synthetic data sets shown in Figure 3.
The ﬁrst data set (Figure 3(a)) was created by generating random vectors (1000) in
[0, 1]2 with the center in (0.5,0.5) and scaling them to a norm less than 0.1, so that the
points lies within a single circular cluster. The second set (Fig. 3(b)) was extracted from
the Mackey-Glass time series data set, which has been used in different works that apply
AIS to anomaly detection problems [7,14,13]. The original data set has four features
extracted by a sliding window. We used only the ﬁrst and the fourth feature. The data
set is divided in two sets (training and testing), each one with 497 samples. The training
set has only normal data, and the testing set has mixed normal and abnormal data.
4.1
Experiments with the First Data Set
Figure 4 shows a typical coverage of the non-self space corresponding to a set of detectors
generated by the NS algorithm with r-contiguous matching for the ﬁrst data set. The
non-covered areas in the non-self space are known as holes [17] and are due to the

The Effect of Binary Matching Rules in Negative Selection
201
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
(a)
(b)
Fig. 3. Self data sets used as input to the NS algorithm shown in a two-dimensional real-valued
problem space. (a) First data set composed of random points inside of a circle of radius 0.1 (b)
Second data set corresponding to a section of the Mackey-Glass data set [7,14,13].
characteristics of r-contiguous matching. In some cases, these holes can be good: since
they are expected to be close to self strings, the set of detectors will not detect small
deviations from the self set, making the NS algorithm robust to noise. However, when
we map the holes from the representation (self/non-self) space to the problem space,
they are not necessarily close to the self set, as shown in Figure 4. This result is not
surprising; as we saw in the previous section (section 3), the binary matching rules fail
to capture the concept of proximity in this two-dimensional space.
Fig. 4. Coverage of space by a set of detectors generated by NS algorithm using r-contiguous
matching (with r = 7). Black dots represent self-set points, and gray regions represent areas
covered by the generated detectors (4446).
We run the NS algorithm using different matching rules and varying the r value.
Figure 5 shows the best coverage generated using standard (no Gray) binary represen-
tation. The improvement in the coverage generated by r-contiguous matching (Figure
5(a)) is due to the higher value of r (r = 9), which produces more speciﬁc detectors.
The coverage with the r-chunk matching rule (Figure 5(b)) is more consistent with the
shape of the self set because of the high speciﬁcity of r-chunk detectors. The outputs
produced by the NS algorithm with Hamming and R&T matching rules are the same.

202
F. Gonz´alez, D. Dasgupta, and J. G´omez
These two rules do not seem to do as well as the other matching rules (Figure 5(c)).
However, by changing the encoding from binary to Gray (Figure 5(d)), the performance
can be improved, since the Gray encoding changes the detector shape, as was shown in
the previous section (Section 3). The change in the encoding scheme, however, does not
affect the performance of the other rules for this particular data set.
(a)
(b)
(c)
(d)
Fig. 5. Best space coverage by detectors generated with NS algorithm using different matching
rules. Black dots represent self-set points, and gray regions represent areas covered by detectors. (a)
r-contiguous matching, r = 9, binary encoding, 36,968 detectors. (b) r-chunk matching, r = 10,
binary encoding, 6,069 detectors. (c) Hamming matching, r = 12, binary encoding (same as R&T
matching, r = 10/16), 9 detectors. (d) Hamming matching, r = 10, Gray encoding (same as
R&T matching, r = 7/16), 52 detectors.
The r-chunk matching rule produced the best performance in this data set, followed
closely by the r-contiguous rule. This is due to the shape of the areas covered by r-chunk
detectors which adapt very well to the simple structure of this self set, one localized,
circular cluster of data points.
4.2
Experiments with the Second Data Set
The second data set has a more complex structure than the ﬁrst one, where the data
are spread in a certain pattern. The NS algorithm should be able to generalize the self
set with incomplete data. The NS algorithm was run with different binary matching
rules, with both encodings (binary and Gray), and varying the value parameter r (the
different values are shown in Table 1). Figure 6 shows some of the best results produced.
Clearly, the tested matching rules were not able to produce a good coverage of the non-
self space. The r-chunk matching rule generated satisfactory coverage of the non-self
space (Figure 6(b)); however, the self space was covered by some lines resulting in
erroneously detecting the self as non-self (false alarms). The Hamming-based matching
rules generated an even more stringent result (Figure 6(d)) that covers almost the entire
self space.The parameter r, which works as a threshold, controls the detection sensitivity.
A smaller value of r generates more general detectors (i.e. covering a larger area) and
decreases the detection sensitivity. However, for a more complex self set, changing the
value of r from 8 (Figure 6(b)) to 7 (Figure 6(c)) generates a coverage with many holes
in the non-self area, and still with some portions of the self covered by detectors. So, this

The Effect of Binary Matching Rules in Negative Selection
203
problem is not with the setting of the correct value for r, but a fundamental limitation
on of the binary representation that is not capable of capturing the semantics of the
problem space. The performance of the Hamming-based matching rules is even worse;
it produces a coverage that overlaps most of the self space (Figure 6(d)).
(a)
(b)
(c)
(d)
Fig. 6. Best coverage of the non-self space by detectors generated with negative selection. Different
matching rules, parameter values and codings (binary and Gray) were tested. The number of
detectors is reported in Table 1. (a) r-contiguous matching, r = 9, Gray encoding. (b) r-chunk
matching, r = 8, Gray encoding. (c) r-chunk matching, r = 7, Gray encoding. (d) Hamming
matching, r = 13, binary encoding (same as R&T matching, r = 10/16).
A better measure to determine the quality of the non-self space coverage with a set of
detectors can be produced by matching the detectors against a test data set. The test data
set is composed of both normal and abnormal elements as described in [13]. The results
are measured in terms of the detection rate (percentage of abnormal elements correctly
identiﬁed as abnormal) and the false alarm rate (percentage of the normal detectors
wrongly identiﬁed as abnormal). An ideal set of detectors would have a detection rate
close to 100%, while keeping a low false alarm rate. Table 1 accounts the results of
experiments that combine different binary matching rules, different threshold or window
size values (r), and two types of encoding. In general, the results are very poor. None of
the conﬁgurations managed to deliver a good detection rate with a low false alarm rate.
The best performance, which is far from good, is produced by the coverage depicted in
Figure 6(b) (r-chunk matching, r = 8, Gray encoding), with a detection rate of 73.26%
and a false alarm rate of 47.47%. These results are in contrast with other previously
reported [7,21]; however, it is important to notice that in those experiments, the normal
data in the test set is same to the normal data in the training set; so, no new normal data
was presented during testing. In our case, the normal samples in the test data are, in
general, different from those in the training set, though they are generated by the same
process. Hence, the NS algorithm has to be able to generalize the structure of the self
set in order to be able to classify correctly previously unseen normal patterns. But, is
this a problem with the matching rule or a more general issue in the NS algorithm? In
fact, the NS algorithm can perform very well on the same data set if the right matching
rule is employed. We used a real value representation matching rule and followed the
approach proposed in [14] on the second data set. The performance over the test data set

204
F. Gonz´alez, D. Dasgupta, and J. G´omez
was detection rate, 94%, false alarm, 3.5%. These results are clearly superior to all the
results reported in Table 1.
Table 1. Results of different matching rules in NS using the the second test data set. (r: threshold
parameter, ND: number of detectors, D%: detection rate, FA%: false alarm rate). The results in
bold correspond to the sets of detectors shown in Figure 6.
Binary
Gray
r
ND
D%
FA%
ND
D%
FA%
r-contiguous
7
0
40
3.96%
1.26%
8
343
15.84% 16.84%
361
16.83% 16.67%
9
4531 53.46% 48.48% 4510 66.33% 48.23%
10
16287 90.09% 77.52% 16430 90.09% 75.0%
11
32598 95.04% 89.64% 32609 98.01% 90.4%
r-chunk
4
0
2
0.0%
0.75%
5
4
0.0%
0.75%
8
0.0%
0.75%
6
18
3.96%
4.04%
22
3.96%
2.52%
7
98
14.85% 16.16%
118
18.81% 13.13%
8
549
54.45% 48.98%
594
73.26% 47.47%
9
1942 85.14% 72.97% 1959 88.11% 67.42%
10
4807 98.01% 86.86% 4807 98.01% 86.86%
11
9948
100% 92.92% 9948
100%
92.92%
12
18348 100% 94.44% 18348 100%
94.44%
Hamming
12
1
0.99%
3.03%
7
10.89% 8.08%
13
2173
99%
91.16% 3650
99.0% 91.66%
14
29068 100%
95.2% 31166 100%
95.2%
Rogers & Tanimoto 9/16
1
0.99%
3.03%
7
10.89% 8.08%
10/16 2173
99%
91.16% 3650
99%
91.66%
11/16 29068 100%
95.2% 31166 100%
95.2%
12/16 29068 100%
95.2% 31166 100%
95.2%
5
Conclusions
In this paper, we discussed different binary matching rules used in the negative selection
(NS) algorithm. The primary applications of NS have been in the ﬁeld of change (or
anomaly) detection, where the detectors are generated in the complement space which
can detect changes in data patterns. The main component of NS is the choice of a
matching rule, which determines the similarity between two patterns in order to classify
self/non-self (normal/abnormal) samples. There exists a number of matching rules and
encoding schemes for the NS algorithm. This paper examines the properties (in terms
of coverage and detection rate) of each binary matching rule for different encoding
schemes.
Experimental results showed that the studied binary matching rules cannot produce
a good generalization of the self space, which results in a poor coverage of the non-

The Effect of Binary Matching Rules in Negative Selection
205
self space. The reason is that the afﬁnity relation implemented by the matching rule
at the representation level (self/non-self ) space cannot capture the afﬁnity relationship
at the problem space. This phenomenon is observed in our experiments with a simple
real-valued two-dimensional problem space.
The main conclusion of this paper is that the matching rule for NS algorithm needs
to be chosen in such a way that it accurately represents the data proximity in the problem
space. Another factor to take into account is the type of application. For instance, in
change detection applications (integrity of software or data ﬁles), where the complete
knowledge of the self space is available, the generalization of the data may not be
necessary. In contrast, in anomaly detection applications, like those in computer security
where a normal behavior model needs to be build using available samples in a training
set, it is crucial to count on matching rules that can capture the semantics of the problem
space [4,20].
Other types of representation and detection schemes for the NS algorithm have been
proposed by different researchers [4,13,15,21,23]; however, they have not been studied
as extensively as binary schemes. The ﬁndings in this paper provide motivation to further
explore matching rules for different representations. Particularly, our effort is directed
to investigate methods to generate good sets of detectors in real valued spaces. This type
of representation also opens the possibility to integrate NS with other AIS techniques
like those inspired by the immune memory mechanism [9,22].
Acknowledgments. This work was funded by the DefenseAdvanced Research Projects
Agency (no. F30602-00-2-0514) and National Science Foundation (grant no. IIS-
0104251). The authors would like to thank Leandro N. de Castro and the anonymous
reviewers for their valuable corrections and suggestions to improve the quality of the
paper.
References
1. J. Balthrop, F. Esponda, S. Forrest, and M. Glickman. Coverage and generalization in an
artiﬁcial immune system. In GECCO 2002: Proceedings of the Genetic and Evolutionary
Computation Conference, pages 3–10, New York, 9-13 July 2002. Morgan Kaufmann Pub-
lishers.
2. J. Balthrop, S. Forrest, and M. R. Glickman. Revisting lisys: Parameters and normal behavior.
In Proceedings of the 2002 Congress on Evolutionary Computation CEC2002, pages 1045–
1050. IEEE Press, 2002.
3. C. A. C. Coello and N. C. Cortes. A parallel implementation of the artiﬁcial immune system
to handle constraints in genetic algorithms: preliminary results. In Proceedings of the 2002
Congress on Evolutionary Computation CEC2002, pages 819–824, Honolulu, Hawaii, 2002.
4. D. Dagupta and F. Gonz´alez. An immunity-based technique to characterize intrusions in
computer networks. IEEE Transactions on Evolutionary Computation, 6(3):281–291, June
2002.
5. D. Dasgupta. An overview of artiﬁcial immune systems and their applications. In D. Dasgupta,
editor, Artiﬁcial immune systems and their applications, pages pp 3–23. Springer-Verlag, Inc.,
1999.

206
F. Gonz´alez, D. Dasgupta, and J. G´omez
6. D. Dasgupta and S. Forrest. Tool breakage detection in milling operations using a negative-
selection algorithm. Technical Report CS95-5, Department of Computer Science, University
of New Mexico, 1995.
7. D. Dasgupta and S. Forrest. Novelty detection in time series data using ideas from immunol-
ogy. In Proceedings of the International Conference on Intelligent Systems, pages 82–87,
June 1996.
8. L. N. de Castro and J. Timmis. Artiﬁcial Immune Systems: A New Computational Approach.
Springer-Verlag, London, UK, 2002.
9. L. N. de Castro and F. J. Von Zuben. An evolutionary immune network for data clustering.
Brazilian Symposium on Artiﬁcial Neural Networks (IEEE SBRN’00), pages 84–89, 2000.
10. P. D’haeseleer, S. Forrest, and P. Helman. An immunological approach to change detection:
algorithms, analysis and implications.
In Proceedings of the 1996 IEEE Symposium on
Computer Security and Privacy, pages 110–119, Oakland, CA, 1996.
11. J. D. Farmer, N. H. Packard, andA. S. Perelson. The immune system, adaptation, and machine
learning. Physica D, 22:187–204, 1986.
12. S. Forrest,A. Perelson, L.Allen, and R. Cherukuri. Self-nonself discrimination in a computer.
In Proc. IEEE Symp. on Research in Security and Privacy, pages 202–212, 1994.
13. F. Gonz´alez and D. Dagupta. Neuro-immune and self-organizing map approaches to anomaly
detection: A comparison. In Proceedings of the 1st International Conference on Artiﬁcial
Immune Systems, pages 203–211, Canterbury, UK, Sept. 2002.
14. F. Gonz´alez, D. Dasgupta, and R. Kozma. Combining negative selection and classiﬁcation
techniques for anomaly detection. In Proceedings of the 2002 Congress on Evolutionary
Computation CEC2002, pages 705–710, Honolulu, HI, May 2002. IEEE.
15. P. Harmer, G. Williams, P.D.and Gnusch, and G. Lamont. An Artiﬁcial Immune System
Architecture for Computer Security Applications. IEEE Transactions on Evolutionary Com-
putation, 6(3):252–280, June 2002.
16. S. Hofmeyr and S. Forrest. Architecture for an artiﬁcial immune system. Evolutionary
Computation, 8(4):443–473, 2000.
17. S.A. Hofmeyr. An interpretative introduction to the immune system. In I. Cohen and L. Segel,
editors, Design principles for the immune system and other distributed autonomous systems.
Oxford University Press, 2000.
18. J. H. Holland, K. J. Holyoak, R. E. Nisbett, and P. R. Thagard. Induction: Processes of
Inference, Learning, and Discovery. MIT Press, Cambridge, 1986.
19. N. K. Jerne. Towards a network theory of the immune system. Ann. Immunol. (Inst. Pasteur),
125C:373–389, 1974.
20. J. Kim and P. Bentley. An evaluation of negative selection in an artiﬁcial immune system for
network intrusion detection. In GECCO 2001: Proceedings of the Genetic and Evolutionary
Computation Conference, pages 1330–1337, San Francisco, California, USA, 2001. Morgan
Kaufmann.
21. S. Singh. Anomaly detection using negative selection based on the r-contiguous matching rule.
In Proceedings of the 1st International Conference on Artiﬁcial Immune Systems (ICARIS),
pages 99–106, Canterbury, UK, sep 2002.
22. J. Timmis and M. J. Neal. A resource limited artiﬁcial immune system for data analysis. In
Research and development in intelligent systems XVII, proceedings of ES2000, pages 19–32,
Cambridge, UK, 2000.
23. P. D. Williams, K. P. Anchor, J. L. Bebo, G. H. Gunsch, and G. D. Lamont. CDIS: Towards
a computer immune system for detecting network intrusions. Lecture Notes in Computer
Science, 2212:117–133, 2001.

Immune Inspired Somatic Contiguous
Hypermutation for Function Optimisation
Johnny Kelsey and Jon Timmis
Computing Laboratory, University of Kent
Canterbury. Kent. CT2 7NF. UK
{jk34,jt6}@kent.ac.uk
Abstract. When considering function optimisation, there is a trade oﬀ
between quality of solutions and the number of evaluations it takes to
ﬁnd that solution. Hybrid genetic algorithms have been widely used for
function optimisation and have been shown to perform extremely well
on these tasks. This paper presents a novel algorithm inspired by the
mammalian immune system, combined with a unique mutation mech-
anism. Results are presented for the optimisation of twelve functions,
ranging in dimensionality from one to twenty. Results show that the im-
mune inspired algorithm performs signiﬁcantly fewer evaluations when
compared to a hybrid genetic algorithm, whilst not sacriﬁcing quality of
the solution obtained.
1
Introduction
The problem of function optimisation has been of interest to computer scientists
for decades. Function optimisation can be characterised as, given an arbitrary
function, how can the maximum (or minimum) value of the function be found.
Such problems can present a very large search space, particularly when dealing
with higher-dimensional functions. Genetic algorithms (GAs) though not ini-
tially designed for such a purpose, however, they soon began to grow in favour
with researchers for this task. Whilst the standard GA performs well in terms
of ﬁnding solutions, it is typical that for more complex problems, some form of
hybridisation of the GA is performed: typically, an extra search mechanism is
employed as part of the hybridisation, for example hill climbing, to help the GA
perform a more eﬀective local search near the optimum [10].
In recent years, interest has been growing in the use of other biologically in-
spired models: in particular the immune system, as witnessed by the emergence
of the ﬁeld of Artiﬁcial Immune Systems (AIS). AIS can be deﬁned as adaptive
systems inspired by theoretical immunology and observed immune functions and
principles, which are applied to problem solving [5]. This insight into the immune
system has led to an ever increasing body of research in a wide variety of do-
mains. To review the whole area would be outside the scope of this paper, but
work pertinent to this paper is work on function optimisation [4], extended with
an immune network approach in [6] and applied to multi-modal optimisation.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 207–218, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

208
J. Kelsey and J. Timmis
Other germane and signiﬁcant papers include [19], where work there is con-
sidered on multi-objective optimisation. However, work proposed in this paper
varies signiﬁcantly in terms of population evolution and mutation mechanisms
employed.
This paper presents initial work into the investigation of immune inspired
algorithms for function optimisation. A novel mutation mechanism has been de-
veloped, loosely inspired by the mutation mechanism found in B-cell receptors
in the immune system. This coupled with evolutionary pressure observed in the
immune system, leads to the development of a novel algorithm for function opti-
misation. Experiments with twelve diﬀerent functions have shown the algorithm
to perform signiﬁcantly fewer evaluations when compared to a standard hybrid
GA, whilst maintaining high accuracy on the solutions found.
This paper ﬁrst outlines a hybrid genetic algorithm which might typically be
used for function optimisation. Then there follows a short discussion on immune
inspired algorithms which outlines the basis of the theoretical framework under-
pinning AIS. The focus of the paper then turns to the novel B-cell algorithm,
followed with the presentation and initial analysis of the ﬁrst empirical results
obtained. Conclusions are drawn and future research directions are explored.
2
Hybrid Genetic Algorithms
Hybrid genetic algorithms (HGAs) have, over the last decade, become almost
standard tools for function optimisation and combinatorial analysis: according
to Goldberg et. al., real-world business and engineering applications are typically
undertaken with some form of hybridisation between the GA and a specialised
search [10]. The reason for this is that HGAs generally have an improved per-
formance, as has been demonstrated in such diverse areas as vehicle routing [2]
and multiple protein sequence alignment [16]).
As an example, within a HGA a population P is given as candidates to
optimised an objective function g(x). Each member of the population can be
thought of as a vector v of bit strings of length l = 64 (to represent double-
precision ﬂoating point numbers, although this does not have to be the case)
where v ∈P and P is the population. Hybrid genetic algorithms employ an extra
operator working in conjunction with crossover and mutation which improves the
ﬁtness of the population. This can come in many diﬀerent guises: sometimes it is
speciﬁc to the particular problem domain; when dealing with numerical function
optimisation, the HGA is likely to employ a variant of local search. The basic
procedure of a HGA is given in ﬁgure 1. The local search mechanism functions by
examining the neighbourhood of the ﬁtness individuals within a given landscape
of the population. This allows for a more speciﬁc search around possible solutions
that results in a faster convergence rate to a possible solution. The local search
typically operates as described in ﬁgure 2. Notice that there are two distinct
mutation rates utilised: the standard genetic algorithm typically uses a very low
level of mutation, and the local search function h(x) uses a much higher one, so

Immune Inspired Somatic Contiguous Hypermutation
209
we have δ << ρ. This process outlined in ﬁgure 2. This method of hybridising a
GA is adopted as the model for the HGA used in this paper.
1. Initialisation: create an initial random population (P) of individuals v;
a) Fitness evaluation: ∀v ∈P: evaluate ﬁtness of P(v) with objective function
g(x);
b) Diversity
i. Selection and crossover: Select n number of ﬁttest individuals and with
probability p perform crossover between selected individuals;
ii. Mutation: subject t number of individuals of the population to a low level
of mutation with an equally low probability;
c) Utilise hybrid function: subject s members of the population to a hybrid search
technique h(x); if a higher-ﬁtness member results, return this to the popula-
tion;
d) Cycle: repeat from step (a) until a certain stopping criterion is met.
Fig. 1. Generic Hybrid GA algorithm
1. Select: copy v to v′;
2. Explore neighbourhood: apply mutation to v′ with probability ρ;
a) Generate number of mutations: Subject v′ to mutation: Nmut = f(ρ);
b) Generate mutation sites: for Nmut, randomly select sites on v′ and perturb bit
string;
3. Fitness Evaluation: if g(v′) > g(v), replace v so that v′ ∈P;
Fig. 2. Example of local search mechanism for a HGA
3
Artiﬁcial Immune Systems
There has been a growing interest in the use of the biological immune system
as a source of inspiration to the development of computational systems [5]. The
natural immune system protects our bodies from infection and this is achieved by
a complex interaction of white blood cells called B Cells and T Cells. Essentially,
AIS is concerned with the use of immune system components and processes as
inspiration to construct computational systems. This insight into the natural im-
mune system has led to an increasing body of work in a wide variety of domains.
Much of this work emerged from early work in theoretical immunology [13], [8]
and where mathematical models of immune system process were developed in
an attempt to better understand the function of the immune system. This acted
as a mini-catalyst for computer scientists, examples being work on on computer

210
J. Kelsey and J. Timmis
security [9] and virus detection [14]. Researchers realised that, although the com-
puter security metaphor was a natural ﬁrst choice for AIS, there are many other
potential application areas that could be explored such as machine learning [18],
scheduling [12] and optimisation [4].
Recent work in [5] has proposed a framework for the construction of AIS. This
framework can described in three layers. The ﬁrst layer is one of representation
of the system, this is termed shape space and deﬁne the components for the
system. A typical shape space for a system may be binary, where elements within
each component can take either a zero or one value. The second layer is one
of aﬃnity measures: this allows for the measurement of the goodness of the
component when measured against the problem. In terms of optimisation, this
would be in terms of how well the values in the component performed with
respect to the function being optimised. Finally, immune algorithms control the
interactions of these components in terms of population evolution and dynamics.
Such basic algorithms include negative selection, clonal selection and immune
network models. These can be utilised as building blocks for AIS and augmented
and adapted as desired. At present, clonal selection based algorithms have been
typically used to build AIS for optimisation. This is the approach adopted in
this paper. Work in this paper can be considered as an augmentation to the
framework in the area of immune algorithms, rather than oﬀering anything new
in terms of representation and aﬃnity measures.
3.1
An Immune Algorithm for Optimisation
Pertinent to work in this paper is work in [4]. Here the authors proposed an
algorithm inspired by the workings of the immune system, in a process known
as clonal selection. There are other examples of immune inspired optimisation
such as [11], however these will not be discussed here. The reader is directed to
[5] for a full review of these techniques. Clonal selection is the process by which
the immune system is said to respond to invading organisms (pathogens, which
then become antigens). The process is conceptually simple: the immune system
is made up of cells known as T-cells and B-cells (all of which have receptors
on them which are capable of recognising antigens, via a binding mechanisms
analogous to a lock and key). When an antigen enters the host, receptors on
B-cells and T-cells attach themselves to the antigens. These cells become stim-
ulated through this interaction, with B-cells receiving stimulation from T-cells
that attach themselves to similar antigen. Once a certain level of stimulation
is reached, B-cells begin to clone at a rate proportional to their aﬃnity to the
antigen. These clones undergo a process of aﬃnity maturation: this is achieved
by the mutation of the clones at a high rate (known as somatic hypermutation)
and selection of the strongest cells, some of which are retained as memory cells.
At the end of each iteration, a certain number of random individuals are inserted
into the population, to maintain an element of diversity.
Results reported for CLONALG (CLONal ALGorithm), which captures the
above process, seem to indicate that it performs well on function optimisation [4].
However, from the paper it was hard to extract an exact number of evaluations

Immune Inspired Somatic Contiguous Hypermutation
211
and solutions found, as these were not presented other than in graphical form.
Additionally, a detailed comparison between alternative techniques was never
undertaken, so it has proved diﬃcult to fully assess the potential of the algorithm.
The work presented in this paper (undertaken independently of and contem-
poraneously to the above work) is a variation of clonal selection, which applies
a novel mutation operator and a diﬀerent selection mechanism, which has been
found to greatly improve on optimisation performance on a number of functions.
4
The B-Cell Algorithm
This paper proposes a novel algorithm, called the B-cell algorithm (BCA), which
is also inspired by the clonal selection process. An important feature of the BCA
is its use of a unique mutation operator, known as contiguous somatic hyper-
mutation. Evidence for this in the immunological literature is sparse, but such
examples are [17], [15]. Here the authors argue that mutation occurs in clusters
of regions within cells: this is analogous to contiguous regions. However, in the
spirit of biologically inspired computing, it is not necessary for the underlying
biological theory to be proven, as computer scientists are interested in taking
inspiration from these theories to help improve on current solutions. As will be
shown the BCA is diﬀerent to both CLONALG and HGAs in a number of ways.
The BCA and motivation for the algorithm will now be discussed.
The representation employed in the BCA is one of a N-dimensional vector
of 64-bit strings (as in the HGA above), known as Binary Shape Space within
AIS, which represents bit-encoded double-precision numbers. These vectors are
considered to be the B-cells within the system. Each B-cell within the population
are evaluated by the objective function, g(x). More formally, the B-cells are
deﬁned as a vector v ∈P of bit strings of length l = 64 where P is the population.
Empirical evidence indicates that an eﬃcient population size for many functions
is low in contrast with genetic algorithms; a typical size would be ♯P ∈[3..5].
The BCA can ﬁnd solutions with higher P, but it converges more rapidly to the
solution (using less evaluations of g(x)) with a smaller value for P. Results were
obtained regarding this observation, but are not presented in this paper.
After evaluation by the objective function, a B-cell (v) is cloned to produce
a clonal pool, C. It should be noted that there exists a clonal pool C for each
B-cell within the population and also that all the adaptation takes place within
C. The size of C is typically the same size as the population P (but this does not
have to be the case). Therefore, if P was of size 4 then each B-cell would produce
4 clones. In order to maintain diversity within the search, one clone is selected
at random and each element in vector undergo a random change, subject to a
certain probability. This is akin to the metadynamics of the immune system,
a technique also employed in CLONALG, but here a separate random clone
is produced, rather than utilising an existing one. Each B-cell v′ ∈C is then
subjected to a novel contiguous somatic hypermutation mechanism. The precise
form of this mutation operator will be explored in more detail below.

212
J. Kelsey and J. Timmis
The BCA uses a distance function as its stopping criterion for the empirical
results presented below: when it is within a certain prescribed distance from the
optimum, the algorithm is considered to have converged. The BCA is outlined
in ﬁgure 4.
1. Initialisation: create an initial random population of individuals P;
2. Main loop: ∀v ∈P:
a) Aﬃnity Evaluation: evaluate g(v);
b) Clonal Selection and Expansion:
i. Clone each B-cell: clone v and place in clonal pool C;
ii. Metadynamics: randomly select a clone c ∈C; randomise the vector;
iii. Contiguous mutation: ∀c ∈C, apply the contiguous somatic hypermuta-
tion operator;
iv. Aﬃnity Evaluation: evaluate each clone by applying g(v); if a clone has
higher aﬃnity than its parent B-cell v, then v = c;
3. Cycle: repeat from step (2) until a certain stopping criterion is met.
Fig. 3. Outline of the B-Cell Algorithm
The unusual feature of the BCA is the form of the mutation operator. This
operates by subjecting contiguous regions of the vector to mutation. The biolog-
ical motivation for this is as follows: when mutation occurs on B-cell receptors,
it focuses on complementarity determining regions, which are small regions on
the receptor. These are sites that are primarily responsible for detecting and
binding to their targets. In essence a more focused search is undertaken. This is
in contrast to the method employed by CLONALG and the local search func-
tion h(x), whereby although multiple mutations take place, they are uniformly
distributed across the vector, rather than being targeted at a contiguous region
(see ﬁgure 4). Contrastingly, as also shown in ﬁgure 4, the contiguous mutation
operator, rather than selecting multiple random sites for mutation, a random
site (or hotspot) is chosen within the vector, along with a random length; the
vector is then subjected to mutation from the hotspot onwards, until the length
of the contiguous region has been reached.
5
Results
Both the HGA and BCA were tested on a number of functions ranging in com-
plexity from one to twenty dimensions, taken from [1] and [7]. It was not possi-
ble to obtain results for all functions for the CLONALG, but results for certain
functions were taken from [4] for comparative purposes. In total twelve functions
were tested. The parameters for the HGA were derived according to standard
heuristics, with a crossover rate of 0.6 and a mutation rate of 0.001: the local
search function h(x) incorporated a mutation rate of δ ∈{2, 3, 4, 5} per vector.
The BCA had a clonal pool size equal to the population size. It should be noted

Immune Inspired Somatic Contiguous Hypermutation
213
h2
h1
h3
length
hotspot
Fig. 4. Multiple-point and contiguous mutation
that all vectors consisted of bit strings of length 64 (i.e double-precision ﬂoating
point numbers) and no Gray encoding was used on either the HGA or BCA.
Each experiment was run for 50 iterations and the results averaged over the
runs. The functions to be optimised are given in table 1. Some of the functions
may seem quite simple e.g. f1, f9 with one and two dimensions respectively.
However, f12 is of twenty dimensions. An interesting characteristic of function
f11 is the presence of a second best minimum away from the global minimum.
Function f12 has a product term introducing an interdependency between the
variables; this is intended to disrupt optimisation techniques that work on one
function variable at a time [7].
5.1
Overview of Results
When monitoring the performance of the algorithms, two measures were em-
ployed: these were the quality of the solution found, and the number of eval-
uations taken to ﬁnd the solution. The number of evaluations of the objective
function is a measure adopted in many papers for assessing the performance
of an algorithm; in case the algorithm does not converge on the optimum, the
distance measure can give an estimate of how proximity to the solution. Table
2 provides a set of results averaged over 50 runs for the optimised functions.
It it noteworthy that the results presented are for a population size of only 4
individuals, in order to allow for direct comparisons to be made; it should also
be noted that results were obtained for population sizes ranging from 4 to 40 for
both algorithms. It was found that the performance diﬀerence between the two
algorithms was similar as the population size was increased. As the population
sizes increased for both algorithms, the number of evaluations increased, with
occasional eﬀect on the quality of the result obtained (in terms of quality of solu-
tion found).
As can be seen from table 2 both the hybrid GA and BCA perform
well in ﬁnding the optimal solutions for the majority of functions. Notable ex-
ceptions are f7 and f9 where neither algorithm found a minimal value. In terms
of the metric for quality of solutions then there seems little to distinguish the

214
J. Kelsey and J. Timmis
Table 1. Functions to be Optimised
Function ID
Function
Parameters
f1
f(x) = 2(x −0.75)2 + sin(5πx −0.4π)
0 ≤x ≤1
- 0.125
f2
f(x, y) = (4 −2.1x2 + x4
3 )x2+
−3 ≤x ≤3 and −2 ≤x ≤2
(Camelback)
xy + (−4 + 4y2)y2
f3
f(x) = −5
j=1[j sin((j + 1)x + j)]
−10 ≤x ≤10
f4
f(x, y) = a(y −bx2 + cx −d)2+
a = 1, b =
5.1
4π2 , c = 5
π ,
(Branin)
h(1 −f) cos(x) + h
d = 6, f =
1
8π , h = 10
−5 ≤x ≤10, 0 ≤y ≤15
0 ≤y ≤15
f5
f(x, y) = 5
j=1 j cos[(j + 1)x + j]
−10 ≤x ≤10 and −10 ≤y ≤10
(Pshubert 1)
and β = 0.5
f6
5
j=1 j cos[(j + 1)y + j]
as above but β = 1
(Pshubert 2)
+β[(x + 1.4513)2 + (y + 0.80032)2
f7
f(x, y) = x sin(4πx) −y sin(4πyπ) + 1
−10 ≤x ≤10 and −10 ≤y ≤10
f8
y = sin6(5πx)
−10 ≤x ≤10 and −10 ≤y ≤10
f9
f(x, y) = x4
4 −x2
2 +
x
10 + y2
2
−10 ≤x ≤10 and −10 ≤y ≤10
(quartic)
f10
f(x, y) = 5
j=1 j cos[(j + 1)x + j]
−10 ≤x ≤10 and −10 ≤y ≤10
(Shubert)
5
j=1 j cos[(j + 1)y + j]
f11
f(−→
x ) = 418.9829n−
−512.03 ≤xi ≤511.97, n = 3.
(Schwefel)
n
i=1 xi sin(

|xi|)
f12
f(−→
x ) = 1 + n
i=1
x2
i
4000−
n = 20 and −600 ≤xi ≤600
(Griewangk)

cos( xi
√
i)
two algorithms. This at least conﬁrms that the BCA is performing sensibly on
the functions. However, when the number of evaluations are taken into account,
then a diﬀerent picture emerges. These are highlighted in the table 2 and are
presented as a compression rate, so the lower the rate, the fewer the number of
evaluations the BCA algorithm performs when compared to the HGA. As can
be seen from the table, for the majority of the functions reported, the BCA per-
formed signiﬁcantly fewer evaluations on the objective function than the HGA,
but without compromising quality of the solution.

Immune Inspired Somatic Contiguous Hypermutation
215
Table 2. Averaged results over 50 runs, for a population size of 4. Standard deviations
are given where it was non-zero
f(x)
Min.
Minimum Found
No. Eval. of g(x)
Compression Rate
BCA
HGA
BCA
HGA
f1
-1.12
-1.08(±.49)
-1.12
1452
6801
21.35
f2
-1.03
-1.03
-0.99(±.29)
3016
12658
23.81
f3
-12.03
-12.03
-12.03
1219
3709
32.87
f4
0.40
0.40
0.40
4921
30583
16.09
f5
-186.73
-186.73
-186.73
46433
78490
59.16
f6
-186.73
-186.73
-186.73
42636
76358
55.84
f7
1
0.92
0.92(±.03)
333
870
38.28
f8
1
1.00
1.00
132
484
27.27
f9
-0.35
-0.91
-0.99(±.29)
2862
15894
18.01
f10
-186.73
-186.73
-186
14654
52581
27.87
f11
0
0.04
0.04
67483
131147
51.46
f12
1
1
1
44093
80062
55.07
The diﬀerence between the number of evaluations is striking. The BCA takes
fewer evaluations to converge on the optimum in every case, as the percentage
diﬀerence in number of evaluations illustrates. On average, it would appear that
the BCA performs at least half as many evaluations as the HGA. Further exper-
iments need to be done in comparison with other techniques, in order to further
gauge evaluation performance. This is outside the scope of this paper, but is
earmarked for future research.
Clearly, the BCA is not performing like the HGA. When compared to the
CLONALG results, it should be noted that CLONALG also found optimal so-
lutions for f7 but the number of evaluations was not available.
5.2
Why Does the BCA Have Fewer Evaluations?
The question of why the BCA converges on a solution with relatively few evalu-
ations of the objective function is one which has not yet been fully explored as
part of this work, but is clearly a major avenue for investigation. It is possible
that the performance of this algorithm is problem dependant (as is the case with
GA’s) and that the mutation operator is speciﬁcally well suited to the nature of
the data representation.
It is possible that the responsibility for rapid convergence lies with the con-
tiguous somatic hypermutation operator. Consider a ﬁtness landscape with a
number of local optima and one global optimum. Now consider a B-cell that
is trapped on a local optimum; a purely local search mechanism would be un-
able to extricate the B-cell, since that would mean ﬁrst moving to a point of
lower ﬁtness. If the mutation regime were limited to a small number of point
mutations, it would only be able to explore its immediate neighbourhood in the
ﬁtness landscape, and so it is unlikely that it would be able to escape the local
optimum.

216
J. Kelsey and J. Timmis
However, the random length utilised by the contiguous somatic hypermuta-
tion operator means that it is possible for the B-cell to explore a much wider
area of the ﬁtness landscape than just its immediate neighbourhood. The B-cell
may be able to jump oﬀof a local optimum and onto the slopes of the global op-
timum. In much the same way, the contiguous somatic hypermutation operator
can also function in a more narrow sense, analogous to local search, exploring
local points in the ﬁtness space, depending on the value of length.
Despite their intuitive appeal, these are far from formal arguments; more
work will need to be undertaken to verify this hypothesis.
5.3
Diﬀerences between HGA, BCA, and CLONALG
It is important to identify, at least at a conceptual level, diﬀerences in these
approaches. It should be noted that, although the BCA is clearly an evolution-
ary algorithm, the authors do not consider it to be a genetic or hybrid genetic
algorithm: a canonical GA employs a deliberately low mutation rate, and em-
phasises crossover as the primary operator. Similarly, the authors do not con-
sider the BCA to be a memetic algorithm, despite superﬁcial similarities. It is
noted that a more rigorous analysis of diﬀerences is required, but that has been
earmarked for future research. It is the aim of this section to merely highlight
conceptual diﬀerences for the reader. Table 3 summarises the main similarities
and diﬀerences. However, it is worth expanding on these slightly.
Table 3. Summarising the main similarities and diﬀerences between BCA, HGA and
CLONALG
Algorithm
Diversity
Selection
Population
BCA
Somatic Contiguous
Replacement
Introduction of random
mutation
B-cell. Fixed size.
HGA
Point mutation,
Replacement
Fixed size.
crossover and
local search
CLONALG
Aﬃnity proportional
Replacement by
Introduction of random
somatic mutation
n ﬁttest clones
cells, ﬂexible population
ﬁxed size memory population
Two major diﬀerences are the mutation mechanisms and the frequency of
mutation that is employed. Both BCA and CLONALG have high levels of mu-
tation, when compared to the HGA. However, the BCA mutates a contiguous
region of the vector, whereas the other two select multiple random points in the
vector space. As hypothesised above, this may give the BCA a more focused
search, which helps the algorithm to converge with fewer evaluations. It is also
noteworthy that neither AIS algorithms employ crossover, as this does not occur
within the immune system.

Immune Inspired Somatic Contiguous Hypermutation
217
The replacement of individuals within the population also varies between al-
gorithms. Within both the HGA and BCA, when a new clone has been evaluated
and is found to be better than an existing member of the population, the exist-
ing member is simply replaced with the new clone. Alternatively, in CLONALG
a number n of the memory set are replaced, rather than just one. However, it
should be noted that within the HGA the concept of a clone does not exist,
as crossover rather than cloning is employed. This means that within the BCA
there is a certain amount of enhanced parallelism, since copies of the cloned
B-cell have a chance to explore the immediate neighbourhood within the vector
space, by providing extra coverage of the neighbourhood. In contrast, it is again
hypothesised that the HGA loses this extra parallelism through the crossover
mechanism.
6
Conclusions and Future Work
This work has presented an algorithm inspired by how the immune system cre-
ates and matures B-cells, called the B-cell algorithm. A striking feature of the
B-cell algorithm is its performance in comparison to a hybrid genetic algorithm.
A unique aspect of the BCA is its use of a contiguous hypermutation operator,
which, it has been hypothesised, is responsible for its enhanced performance. A
ﬁrst test would be to use this operator in a standard GA to assess the perfor-
mance gain (or not) that the operator brings. This will allow for useful conclu-
sions to be drawn about the nature of the mutation operator. A second useful
direction for future work would be to further test the BCA against other algo-
rithms and widen the scope and type of functions tested; another would be to
test its inherent ability to optimise multimodal functions. It has been noted that
CLONALG is suitable for multimodal optimisation [4] as an inherent property of
the algorithm; it would be worthwhile evaluating if this is the case for the BCA.
Perhaps the most illuminating piece of work would be to test the hypothesis
regarding the eﬀect of the contiguous hypermutation operator on convergence of
the algorithm.
References
1. Andre, J., Siarry, P. and Dognon, T. An improvement of the standard genetic
algorithm ﬁghting premature convergence in continuous optimisation. Advances in
Engineering Software. 32. p. 49–60, 2001.
2. Berger, J., Sassi, J and Salois, M. A Hybrid Genetic Algorithm for the Vehicle
Routing Problem with Time Windows and Itinerary Constraints, Proceedings of
the Genetic and Evolutionary Computation Conference, 1999, 1, 44–51, Orlando,
Florida, USA, Morgan Kaufmann. 1-55860-611-4,
3. Burke E.K., Elliman D.G. and Weare R.F., A hybrid genetic algorithm for highly
constrained timetabling problems, 6th International Conference on Genetic Algo-
rithms (ICGA’95, Pittsburgh, USA, 15th-19th July 1995), Morgan Kaufmann, San
Francisco, CA, USA, pages 605–610, 1995

218
J. Kelsey and J. Timmis
4. de Castro L. Von Zuben F. Clonal selection principle for learning and optimisation.
IEEE Transactions on Evolutionary Computation. 2002.
5. de Castro L and Timmis J. Artiﬁcial immune systems: a new computational intel-
ligence approach Springer-Verlag. ISBN 1-85233-594-7. 2002
6. de Castro L and Timmis J. An artiﬁcial immune network for multimodal opti-
misation In 2002 Congress on Evolutionary Computation. Part of the 2002 IEEE
World Congress on Computational Intelligence, pages 699–704, Honolulu, Hawaii,
USA, May 2002. IEEE.
7. Eiben, A and van Kemenade, C. Performance of multi-parent crossover operators
on numerical function optimization problems Technical Report TR-9533, Leiden
University, 1995.
8. Farmer, J.D., Packard, N.H., and Perelson, A. The Immune System, Adaptation
and Machine Learning. Physica, 1986. 22(D): p. 187-204
9. Forrest S., Hofmeyr S. and Somayaji S. Computer Immunology. Communications
of the ACM. 40(10). pages 88–96. 1997
10. Goldberg, D. and Voessner, S. Optimizing global-local search hybrids, Proceed-
ings of the Genetic and Evolutionary Computation Conference, 1, 13–17, Morgan
Kaufmann, Orlando, Florida, USA, 1-55860-611-4, 220–228, 1999.
11. Hajela, P. and Yoo, J. Immune network modelling in design optimisation. In New
Ideas in Optimisation. D. Corne, M. Dorigo and F. Glover (eds), McGraw-Hill. pp.
203–215, 1999.
12. Hart, E. and Ross, P. The evolution and analysis of a potential antibody library
for use in job-shop scheduling. In New Ideas in Optimisation. Corne, D., Dorigo,
M. and Glover, F.(eds), p. 185–202, 1999.
13. Jerne, N.K. Towards a network theory of the immune system. Annals of Immunol-
ogy, 1974. 125C: p. 373–389.
14. Kephart, J. A biologically inspired immune system for computers. Artiﬁcial Life
IV. 4th International Workshop on the Synthesis and Simulation of Living Systems.
MIT Press, 1994.
15. Lamlum, H., et. al. The type of somatic mutation at APC in familial adenoma-
tous polyposis is determined by the site of the germline mutation: a new facet to
Knudson’s ’two-hit’ hypothesis. Nature Medicine, 1999, 5: pages 1071–1075.
16. Nguyen, H. Yoshihara, I., Yamamori, M. and Yasunaga, M. A parallel hybrid ge-
netic algorithm for multiple protein sequence alignment, Proceedings of the 2002
Congress on Evolutionary Computation CEC2002, 309–314, 2002, IEEE Press.
17. Rosin-Arbesfeld, R., Townsley, F. and Bienz, M. The APC tumour suppressor has
a nuclear export function. Letters to nature, 2000, 406: pages 1009–1012.
18. Timmis, J. and Neal, M. A resource limited artiﬁcial immune system for data
analysis. Knowledge Based Systems. 14(3-4): p. 121–130, 2001.
19. Coello, C. Coello and Cruz Cortes, N. An approach to solve multiobjective op-
timization problems based on an artiﬁcial immune system, Proceedings of the 1st
International Conference on Artiﬁcial Immune Systems (ICARIS) 1, 212–221, 2002

A Scalable Artiﬁcial Immune System Model for Dynamic
Unsupervised Learning
Olfa Nasraoui1, Fabio Gonzalez2, Cesar Cardona1, Carlos Rojas1, and
Dipankar Dasgupta2
1 Department of Electrical and Computer Engineering, The University of Memphis
Memphis, TN 38152
{onasraou, ccardona, crojas}@memphis.edu
2 Division of Computer Sciences, The University of Memphis
Memphis, TN 38152
{fgonzalz, ddasgupt}@memphis.edu
Abstract. Artiﬁcial Immune System (AIS) models offer a promising approach
to data analysis and pattern recognition. However, in order to achieve a desired
learning capability (for example detecting all clusters in a dat set), current models
require the storage and manipulation of a large network of B Cells (with a number
often exceeding the number of data points in addition to all the pairwise links
between these B Cells). Hence, current AIS models are far from being scalable,
which makes them of limited use, even for medium size data sets.
We propose a new scalable AIS learning approach that exhibits superior learning
abilities, while at the same time, requiring modest memory and computational
costs. Like the natural immune system, the strongest advantage of immune based
learning compared to current approaches is expected to be its ease of adaptation
in dynamic environments. We illustrate the ability of the proposed approach in
detecting clusters in noisy data.
Keywords. Artiﬁcial immune systems, scalability, clustering, evolutionary com-
putation, dynamic learning
1
Introduction
Natural organisms exhibit powerful learning and processing abilities that allow them
to survive and proliferate generation after generation in ever changing and challenging
environments. The natural immune system is a powerful defense system that exhibits
many signs of cognitive learning and intelligence [1,2]. Several Artiﬁcial Immune Sys-
tem (AIS) models [3,4] have been proposed for data analysis and pattern recognition.
However, in order to achieve a desired learning capability (for example detecting all
clusters in a dat set), current models require the storage and manipulation of a large
network of B Cells (with a number of B Cells often exceeding the number of data points,
and for network based models, all the pairwise links between these B Cells). Hence,
current AIS models are far from being scalable, which makes them of limited use, even
for medium size data sets. In this paper, we propose a new AIS learning approach for
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 219–230, 2003.
c⃝Springer-Verlag Berlin Heidelberg 2003

220
O. Nasraoui et al.
clustering, that addresses the shortcomings of currentAIS models. Our approach exhibits
improved learning abilities and modest complexity. The rest of the paper is organized
as follows. In Section 2, we review some current artiﬁcial immune system models that
have been used for clustering. In Section 3, we present a new dynamic AIS model and
learning algorithm designed to address the challenges of Data Mining. In Section 4, we
illustrate using the proposed Dynamic AIS model for robust cluster detection. Finally,
in Section 5, we present our conclusions.
2
Artiﬁcial Immune System Models
Artiﬁcial Immune Systems have been investigated and practical applications developed
notably by [5,6,7,3,8,1,4,9,10,11,12,13,14]. The immune system (lymphocyte elements)
can behave as an alternative biological model of intelligent machines, in contrast to the
conventional model of the neural system (neurons). Of particular relevence to our work,
is theArtiﬁcial Immune Network (AIN) model. In their attempt to apply immune system
metaphorstomachinelearning,HuntandCookebasedtheirmodel[3]onJerne’sImmune
Network theory [15].The system consisted of a network of B cells used to create antibody
strings that can be used for DNA classiﬁcation. The resource limited AIN (RLAINE)
model [9] brought improvements for more general data analysis. It consisted of a set of
ARBs (Artiﬁcial Recognition Balls), each consisting of several identical B cells, a set of
antigen training data, links betweenARBs, and cloning operations. EachARB represents
a single n−dimensional data item that could be matched by Euclidean distance to an
antigen or to another ARB in the network. A link was created if the afﬁnity (distance)
between 2 ARBs was below a Network Afﬁnity Threshold parameter, NAT, deﬁned as
the average distance between all data items in the training set. Other immune network
models have been proposed, notably by De Castro and Von Zuben[4].
It is common for theARB population to grow at a proliﬁc rate inAINE [3,16], as well
as other derivatives of AINE, though to a lesser extent [9,11]. It is also common for the
ARB population to converge rather prematurely to a state where a few ARBs matching a
small number of antigens overtake the entire population. Hence, any enhancement that
can reduce the size of this repertoire while still maintaining a reasonable approxima-
tion/representation of the antigen population (data) can be considered a signiﬁcant step
in immune system based data mining.
3
Proposed Artiﬁcial Immune System Model
In all existing artiﬁcial immune network models, the number of ARBs can easily reach
the same size as the training data, and even exceed it. Hence, storing and handling the
network links between all ARB pairs makes this approach unscalable. We propose to
reduce the storage and computational requirements related to the network structure.
3.1
A Dynamic Artiﬁcial B-Cell Model Based on Robust Weights:
The D-W-B-Cell Model
In a dynamic environment, the antigens are presented to the immune network one at a
time, with the stimulation and scale measures re-updated with each presentation. It is

A Scalable Artiﬁcial Immune System Model for Dynamic Unsupervised Learning
221
more convenient to think of the antigen index, j, as monotonically increasing with time.
That is, the antigens are presented in the following chronological order: x1, x2, · · · , xN.
The Dynamic Weighted B-Cell (D-W-B-cell) represents an inﬂuence zone over the do-
main of discourse consisting of the training data set. However, since data is dynamic in
nature, and has a temporal aspect, data that is more current will have higher inﬂuence
compared to data that is less current/older. Quantitatively, the inﬂuence zone is deﬁned
in terms of a weight function that decreases not only with distance from the antigen/data
location to the D-W-B-cell prototype / best exemplar as in [11], but also with the time
since the antigen has been presented to the immune network. It is convenient to think
of time as an additional dimension that is added to the D-W-B-Cell compared to the
classical B-Cell, traditionally statically deﬁned in antigen space only. For the ith D-W-
B-cell, DWBi, we deﬁne the following weight/membership function after J antigens
have been presented:
wij = wi

d2
ij

= e
−

d2
ij
2σ2
i
+ (J−j)
τ

(1)
where d2
ij is the distance from antigen xj (jth antigen encountered by the immune
network) to D-W-B-cell, DWBi. The stimulation level, after J antigens have been
presented to DWBi, is deﬁned as the density of the antigen population around DWBi:
sai,j =
J
j=1 wij
σ2
i
,
(2)
The scale update equations are found by setting ∂sai,j
∂σ2
i
= 0, and deriving incre-
mental update equations, to obtain the following approximate incremental equations for
stimulation and scale, after J antigens have been presented to DWBi.
sai,J = e−1
τ Wi,J−1 + wiJ
σ2
i,J
,
(3)
σ2
i,J =
e−1
τ σ2
i,J−1Wi,J−1 + wiJd2
iJ
2

e−1
τ Wi,J−1 + wiJ

.
(4)
where Wi,J−1 = J−1
j=1 wij is the sum of the contributions from the (J −1) previous
antigens, x1, x2, · · · , xJ−1, to D-W-B-Cell i, and σ2
i,J−1 is its previous scale value.
3.2
Dynamic Stimulation and Suppression
We propose incorporating a dynamic stimulation factor, α (t), in the computation of
the D-W-B-cell stimulation level. The static version of this factor is a classical way to
simulate memory in an immune network by adding a compensation term that depends
on other D-W-B-cells in the network [3]. In other words, a group of intra-stimulated
D-W-B-cells can self-sustain themselves in the immune network, even after the antigen
that caused their creation disappears from the environment. However, we need to put a
limit on the time span of this memory so that truly outdated patterns do not impose an

222
O. Nasraoui et al.
additional superﬂuous (computational and storage) burden on the immune network. We
propose to do this by an annealing schedule on the stimulation factor. This is done by
allowing each group of D-W-B-cells to have their own stimulation coefﬁcient, and to
have this stimulation coefﬁcient decrease with the age of the sub-net). In the absence of
a recent antigen that succeeds in stimulating a given subnet, the age of the D-W-B-cell
increases by 1 with each antigen presented to the immune system. However, if a new
antigen succeeds in stimulating a given subnet, then the age calculation is modifed by
refreshing the age back to zero.This makes extremely old sub-nets die gradually, if not re-
stimulated by more recent relevent antigens. Incorporating a dynamic suppression factor
in the computation of the D-W-B-cell stimulation level is also a more sensible way to
take into account internal interactions.The suppression factor is not intended for memory
management, but rather to control the proliferation and redundancy of the D-W-B-cell
population. In order to understand the combined effect of the proposed stimulation and
suppression mechanism, we consider the following two extreme cases: (i) When there is
positive suppression (competition), but no stimulation. This results in good population
control and no redundancy. However, there is no memory, and the immune network
will forget past encounters. (ii) When there is positive stimulation, but no suppression,
there is good memory but no competition. This will cause the proliferation of the D-W-
B-cell population or maximum redundancy. Hence, there is a natural tradeoff between
redundancy/memory and competition/reduced costs.
3.3
Organization and Compression of the Immune Network
We deﬁne external interactions as those occuring between an antigen (external agent)
and the D-W-B-cell in the immune network. We deﬁne internal interactions as those
occuring between one D-W-B-cell and all other D-W-B-cells in the immune network.
Figure 1(a) illustrates internal (relative to D-W-B-cellk) and external interactions (caused
by an external agent called “Antigen"). Note that the number of possible interactions
is immense, and this is a serious bottleneck in the face of all existing immune network
based learning techniques [3,9,11]. Suppose that the immune network is compressed by
clustering the D-W-B-cells using a linear complexity approach such as K Means. Then
the immune network can be divided into several subnetworks that form a parsimonious
view of the entire network. For global low resolution interactions, such as the ones be-
tween D-W-B-cells that are very different, only the inter-subnetwork interactions are
germane. For higher resolution interactions such as the ones between similar D-W-B-
cells, we can drill down inside the corresponding subnetwork and afford to consider all
the intra-subnetwork interactions. Similarly, the external interactions can be compressed
by considering interactions between the antigen and the subnetworks instead of all the
D-W-B-cells in the immune network. Note that the centroid of the D-W-B-cells in a
given subnetwork/cluster is used to summarize this subnetwork, and hence to compute
the distance values that contribute in the internal and external interaction terms. This
divide and conquer strategy can have signiﬁcant impact on the number of interactions
that need to be processed in the immune network. Assuming that the network is divided
into roughly K equal sized subnetworks, then the number of internal interactions in
an immune network of NB D-W-B-cells, can drop from N 2
B in the uncompressed net-

A Scalable Artiﬁcial Immune System Model for Dynamic Unsupervised Learning
223
work, to
 NB
K
2 intra-subnetwork interactions and K −1 inter-subnetwork interactions
in the compressed immune network. This clearly can approach linear complexity as
K →√NB. Figure 1(c) illustrates the reduced internal (relative to D-W-B-cellk) inter-
actions in a compressed immune network. Similarly the number of external interactions
relative to each antigen can drop from NB in the uncompressed network to K in the com-
pressed network. Figure 1(b) illustrates the reduced external (relative to external agent
“Antigen") interactions. Furthermore, the compression rate can be modulated by choos-
ing the appropriate number of clusters, K ≈√NB, when clustering the D-W-B-cell
population, to maintain linear complexity, O(NB).
Sufﬁcient summary statistics for each cluster of D-W-B-cells are computed, and can
later be used as approximations in lieu of repeating the computation of the entire sup-
pression/stimulation sum. The summary statistics are in the form of average dissimilarity
within the group, cardinality of the group (number of D-W-B-cells in the group), and
density of the group.
(a)
(b)
(c)
Fig. 1. Immune network interactions: (a) without compression, (b) with compression, (c) Internal
Immune network interactions with compression
3.4
Effect of the Network Compression on Interaction Terms
The D-W-B-cell speciﬁc computations can be replaced by subnet computations in a
compressed immune network. The stimulation and scale values become
si = sai,J + α (t)
N i
B
l=1 wil
σ2
i,J
−β (t)
N i
B
l=1 wil
σ2
i,J
,
(5)

224
O. Nasraoui et al.
where sai,J is the pure antigen stimulation given by (3 ) for D-W-B-celli; and N i
B is the
number of B-cells in the subnetwork that is closest to the Jth antigen. This will modify
the D-W-B-cell scale update equations to become
σ2
i,J = 1
2
e−1
τ σ2
i,J−1Wi,J−1 + wiJd2
iJ + α (t) N i
B
l=1 wild2
il −β N i
B
l=1 wild2
il
2

e−1
τ Wi,J−1 + wiJ

+ α (t) N i
B
l=1 wil −β N i
B
l=1 wil
.
(6)
3.5
Cloning in the Dynamic Immune System
The D-W-B-cells are cloned (i..e, duplicated together with all their intrinsic properties
such as scale value) in proportion to their stimulation levels relative to the average
stimulation in the immune network. However, to avoid preliminary proliferation of good
B-Cells, and to encourage a diverse repertoire, new B-Cells do not clone before they
are mature (their age, ti exceeds a lower limit tmin). They are also not removed from
the immune network regardless of their stimulation level. Similarly, B-cells with age
ti > tmax are frozen, or prevented from cloning, to give a fair chance to newer B-Cells.
This means that
Nclonesi = Kclone
si
ND−W −B−cell
k=1
sk
if tmin ≤ti ≤tmax.
(7)
3.6
Learning New Antigens and Relation to Outlier Detection
Somatic hypermutation is a powerfull natural exploration mechanism in the immune
system, that allows it to learn how to respond to new antigens that have never been seen
before. However, from a computational point of view, this is a very costly operation
since its complexity is exponential in the number of features. Therefore, we model
this operation in the artiﬁcial immune system model by an instant antigen duplication
whenever an antigen is encountered that fails to activate the entire immune network. A
new antigen, xj is said to activate the ith B-Cell, if its contribution to this B-Cell, wij
exceeds a minimum threshold wmin. Antigen duplication is a simpliﬁed rendition of the
action of a special class of cells called dendritic cells whose main purpose is to teach
other immune cells such as B-cells to recognize new antigens. Dendritic cells (which
have long been mistaken to be part of the nervous system), and their role in the immune
system, have only recently been understood. We refer to this new antigen duplication, a
dendritic injection, since it essentially injects new information in the immune system.

A Scalable Artiﬁcial Immune System Model for Dynamic Unsupervised Learning
225
3.7
Proposed Scalable Immune Learning Algorithm for Clustering Evolving
Data
Scalable Immune Based Clustering for Evolving Data
Fix the maximal population size NB;
Initialize D-W-B-cell population and σ2
i = σinit using the ﬁrst batch of the input anti-
gens/data;
Compress immune network into K subnets using 2-3 iterations of K Means;
Repeat for each incoming antigen xj {
Present antigen to each subnet centroid in network and determine the closest subnet;
IF antigen activates closest subnet Then {
Present antigen to each D-W-B-cell, D-W-B-celli, in closest immune subnet;
Refresh this D-W-B-cell’s age (t = 0) and update wij using (1);
Update the compressed immune network subnets incrementally;
}
ELSE
Create by dendritic injection a new D-W-B-cell = xj and σ2
i = σinit;
Repeat for each D-W-B-celli in closest subnet only {
Increment age (t) for D-W-B-celli;
Compute D-W-B-celli’s stimulation level using (5);
Update D-W-B-celli’s σ2
i using (6);
}
Clone and mutate D-W-B-cells;
IF population size > NB Then
Kill worst excess D-W-B-cells, or leave only subnetwork representatives of oldest subnet-
works in main memory;
Compress immune network periodically (after every T antigens), into K subnets using 2-3
iterations of K Means;
}
3.8
Comparison to Other Immune Based Clustering Techniques
Because of paucity of space, we review only some of the most recent and most related
methods. The Fuzzy AIS [11] uses a richer knowledge representation for B-cells as
provided by fuzzy memberships that not only model different areas of the same cluster
differently, but are also robust to noise and outliers, and allow a dynamic estimation of
scale unlike all other approaches. The Fuzzy AIS obtains better results than [9] with a
reduced immune network size. However, its batch style processing required storing the
entire data set and all intra-network interaction afﬁnities.
The Self Stabilizing AIS (SSAIS) algorithm [12] maintains stable immune networks
that do not proliferate uncontrollably like in previous versions. However, a single NAT
threshold is not realistic for data with clusters of varying size and separation, and SSAIS
is rather slow in adapting to new/emerging patterns/clusters. Even though SSAIS does
not require storage of the entire data set, it still stores and handles interactions between
all the cells in the immune network. Because the size of this network is comparable to
that of the data set, this approach is not scalable.
The approach in [13] relies exclusively on the antigen input and not on any internal
stimulation or suppression. Hence the immune network has no memory, and would not be

226
O. Nasraoui et al.
able to adapt in an incremental scenario. Also, the requirement to store the entire dataset
(batch style) and the intense computations of all pairwise distances to get the intial NAT
value, make this approach unscalable. Furthermore, a single NAT value and a drastic
winner-takes-all pruning strategy may impact diversity and robustness on complex and
noisy data sets.
In [14], an approach is presented that exploits the analogy between immunology and
sparse distributed memories. The scope of this approach is different from most otherAIS
based methods for clustering because it is based on binary strings, and clusters represent
different schemas. This approach is scalable, since it has linear complexity, and works
in an incremental fashion.Also, the gradual inﬂuence of data inputs to all clusters avoids
undesirable winner-take-all effects of most other techniques.
Finally, the aiNet algorithm [4] evolves a population of antibodies using clonal
selection, hypermutation and apoptosis, and then uses a computationally expensive graph
theoretic technique to organize the population into a network of clusters.
Table 1 summarizes the charateristics of several immune based approaches to clus-
tering, in addition to the K Means algorithm. The last row lists typical values reported in
the experimental results in these papers. Note that all immune based techniques, as well
as most evolutionary type clustering techniques are expected to beneﬁt from insensitivity
to initial conditions (reliability) by virtue of being population based. Also, techniques
that require storage of the entire data set or a network of immune cells with a size that is
comparable to that of the data set in main memory, are not scalable in memory. The cri-
terion Density/distance/Partition/ refers to whether a density type of ﬁtness/stimulation
measure is used or one that is based on distance/error. Unlike Distance and Partitioning
based methods, Density type methods directly seek dense areas of the data space, and
can ﬁnd more good clusters, while being robust to noise.
Table 1. Comparison of proposed Scalable Immune LearningApproach with Other Immune Based
Approaches for Clustering and K Means
Approach →
Proposed AIS Fuzzy AIS [11]
RLAINE [9]
SSAIS [12]
Wierzchon [13]
SOSDM [14]
aiNet [4]
K Means
Reliabibilty/Insensitivity
yes
yes
yes
yes
yes
yes
yes
no
to initialization
Robustness to noise
yes
yes
no
no
no
moderately
no
no
Scalability in time (linear)
yes
no
no
no
no
yes
no
yes
Scalability in space (memory)
yes
no
no
no
no
yes
no
no
Maintains Diversity
yes
yes
no
yes
not clear
yes
yes
N/A
does not requires No. Clusters
yes
yes
yes
yes
yes
yes
yes
no
Quickly Adapts to New Patterns
yes
no
no
no
no
yes
yes
no
Robust Individualized Scale Estimation
yes
yes
no
no
no
no
no
no
Density/Distance/Partition based?
Density
Density
Distance
Distance
Distance/
Distance/
Distance/
Distance/
Partition
Partition
Partition
Partition
batch/incremental: passes(size of data)
incremental:
batch: 39 ( 600) batch: 20 ( 150)
incremental:
batch: 15 ( 100)
incremental:)
batch: 10 (50) batch: typically
(for incremental: required passes
1(2000)
10,000(25)
1-10(40)
a few passes
over entire data set
(Fig. 2 in [11]) (Fig. 5(b) in [9]) (Fig. 10 in [12]) (Fig. 5 in [12]) (Fig. 5 in [13]) (Fig. 6 in [4])
data to learn new cluster)

A Scalable Artiﬁcial Immune System Model for Dynamic Unsupervised Learning
227
4
Experimental Results
Clean and noisy 2-dimensional sets, with roughly 1000 to 2000 points, and between
3 and 5 clusters, are used to illustrate the performance of the proposed immune based
approach. The implementation parameters were as follow: The ﬁrst 0.02% of the data
are used to create an initial network. The initial value for the scale was σinit = 0.0025
(an upper radial bound derived based on the range of normalized values in [0, 1]). B-cells
were only allowed to clone past the age of tmin = 2, and the cloning coefﬁcient was 0.97.
The maximum B-cell population size was 30 (an extremely small number considering
the size of the data), the mutation rate was 0.01, τ = 1.5, and the compression rate, K
varied between 1 and 7. The network compression was performed after every T = 40
antigens have been processed. The evolution of the D-W-B-cell population for 3 noisy
clusters, after a single pass over the antigens, presented in random order, is shown in
Figure 2, superimposed on the original data set. The results for the same data set, but with
antigens presented in the order of the clusters is shown in Figure 3, with the results of
RLAINE [9] in Fig. 3 (d). This scenario is the most difﬁcult (worst) case for single-pass
learning, as it truly tests the ability of the system to memorize the old patterns, adapt
to new patterns, and still avoid excessive proliferation. Unlike the proposed approach,
RLAINE is unable to adapt to new patterns, given the same amount of resources. Similar
experiments are shown for a data set of ﬁve clusters in Figure 4 and 5. Since this is an
unsupervised clustering problem, it is not important that a cluster is modeled by one
or several D-W-B-cells. In fact, merging same-cluster cells is trivial since we have not
only their location estimates, bue also their individual robust scale estimates. Finally, we
illustrate the effect of the compression of the immune network by showing the ﬁnal D-
W-B-cell population for different compression rates corresponding to K = 1, 3, 5 on the
data set with 3 clusters, in Fig. 6. In the last case (K = 5), the immune interactions have
been practically reduced from quadratic to linear complexity by using K ≈

(NB).
It is worth mentioning that despite the dramatic reduction in complexity, the results
are virtually indistinguishable in terms of quality. The effect of compression is further
illustrated for the data set with 5 clusters, in Fig. 7. The antigens were presented in the
most challenging order (one cluster at a time), and in a single pass. In each case, the
proposed immune learning approach succeeds in detecting dense areas after a single
pass, while remaining robust to noise.
5
Conclusion
We have introduced a new robust and adaptive model for immune cells, and a scalable
immune learning process.The D-W-B-cell, modeled by a robust weight function, deﬁnes
a gradual inﬂuence region in the antigen, antibody, and time domains. This is expected
to condition the search space. The proposed immune learning approach succeeds in
detecting dense areas/clusters, while remaining robust to noise, and with a very modest
D-W-B-cell population size. Most existing methods work with B-cell population sizes
often exceeding the size of the data set, and can suffer from premature loss of good
detected immune cells. The proposed approach is favorable from the points of view
of scalability, as well as quality of learning. Quality comes in the form of diversity

228
O. Nasraoui et al.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
(a)
(b)
(c)
Fig. 2. Single Pass Results on a Noisy antigen set presented one at a time in random order: Location
of D-W-B-cells and estimated scales for data set with 3 clusters after processing (a) 100 antigens,
(b) 700 antigens, and (c) all 1133 antigens
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
(a)
(b)
(c)
(d)
Fig. 3. Single Pass Results on a Noisy antigen set presented one at a time in the same order as
clusters, (a, b, c): Location of D-W-B-cells and estimated scales for data set with 3 clusters after
processing (a) 100 antigens, (b) 300 antigens, and (c) all 1133 antigens, (d) RLAINE’s ARB
locations after presenting all 1133 antigens
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
(a)
(b)
(c)
(d)
Fig. 4. Single Pass Results on a Noisy antigen set presented one at a time in random order: Location
of D-W-B-cells and estimated scales for data set with 5 clusters after processing (a) 400 antigens,
(b) 1000 antigens, and (c) 1300 antigens, (d) all 1937 antigens

A Scalable Artiﬁcial Immune System Model for Dynamic Unsupervised Learning
229
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
(a)
(b)
(c)
(d)
Fig. 5. Single Pass Results on a Noisy antigen set presented one at a time in the same order as
clusters: Location of D-W-B-cells and estimated scales for data set with 5 clusters after processing
(a) 100 antigens, (b) 700 antigens, and (c) 1300 antigens, (d) all 1937 antigens
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
(a)
(b)
(c)
Fig. 6. Effect of Compression rate on Immune Network: Location of D-W-B-cells and estimated
scales for data set with 3 clusters (a) K = 1, (b) K = 3, (c) K = 5
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
(a)
(b)
(c)
Fig. 7. Effect of Compression rate on Immune Network: Location of D-W-B-cells and estimated
scales for data set with 5 clusters (a) K = 3, (b) K = 5, (c) K = 7

230
O. Nasraoui et al.
and continuous adaptation as new patterns emerge. We are currently investigating the
use of our scalable immune learning approach to extract patterns from evolving Web
clickstream and text data for Web data mining applications.
Acknowledgment. This work is partially supported by a National Science Foundation
CAREER Award IIS-0133948 to Olfa Nasraoui and support from Universidad Nacional
de Colombia for Fabio Gonzalez.
References
1. D. Dasgupta, Artiﬁcial Immune Systems and Their Applications, Springer Verlag, 1999.
2. I. Cohen, Tending Adam’s Garden, Academic Press, 2000.
3. J. Hunt and D. Cooke, “An adaptative, distributed learning system, based on immune system,”
in IEEE International Conference on Systems, Man and Cybernetics, LosAlamitos, CA, 1995,
pp. 2494–2499.
4. L. N. De Castro and F. J. Von Zuben, “An evolutionary immune network for data clustering,”
in IEEE Brazilian Symposium on Artiﬁcial Neural Networks, Rio de Janeiro, 2000, pp. 84–89.
5. J.D. Farmer and N.H. Packard, “The immune system, adaptation and machne learning,”
Physica, vol. 22, pp. 187–204, 1986.
6. F.J. Varela H. Bersini, “The immune recruitment mechanism: a selective evolutionary strat-
egy,” in Fourth International Conference on Genetic Algorithms, San Mateo, CA, 1991, pp.
520–526.
7. S. Forrest, A. S. Perelson, L. Allen, and R. Cherukuri, “Self-nonself discrimination in a
computer,” in IEEE Symposium on Research in Security and Privacy, Los Alamitos, CA,
1994.
8. D. Dasgupta and S. Forrest, “Novelty detection in time series data using ideas from immunol-
ogy,” in 5th International Conference on Intelligent Systems, Reno, Nevada, 1996.
9. J. Timmis and M. Neal, “A resource limited artiﬁcial immune system for data analysis,”
Knowledge Based Systems, vol. 14, no. 3, pp. 121–130, 2001.
10. T Knight and J Timmis,
“Aine: An immunological approach to data mining,”
in IEEE
International Conference on Data Mining, San Jose, CA, 2001, pp. 297–304.
11. O. Nasraoui, D. Dasgupta, and F. Gonzalez, “An artiﬁcial immune system approach to robust
data mining,” in Genetic and Evolutionary Computation Conference (GECCO) Late breaking
papers, New York, NY, 2002, pp. 356–363.
12. M. Neal, “An artiﬁcial immune system for continuous analysis of time-varying data,” in 1st
International Conference on Artiﬁcial Immune Systems, Canterbury, UK, 2002, pp. 76–85.
13. Wierzchon and U. Kuzelewska, “Stable clusters formation in an artiﬁcial immune system,”
in 1st International Conference on AIS, Canterbury, UK, 2002, pp. 68–75.
14. E Hart and P Ross, “Exploiting the analogy between immunology and spares distributed
memories: A system for clustering non-stationary data,” in 1st International Conference on
Artiﬁcial Immune Systems, Canterbury, UK, 2002, pp. 49–58.
15. N. K. Jerne, “The immune system,” Scientiﬁc American, vol. 229, no. 1, pp. 52–60, 1973.
16. J. Timmis, M. Neal, and J. Hunt, “An artiﬁcial immune system for data analysis,” Biosystems,
vol. 55, no. 1, pp. 143–150, 2000.

Developing an Immunity to Spam
Terri Oda and Tony White
Carleton University
terri@zone12.com,
arpwhite@scs.carleton.ca
Abstract. Immune systems protect animals from pathogens, so why not
apply a similar model to protect computers? Several researchers have
investigated the use of an artiﬁcial immune system to protect computers
from viruses and others have looked at using such a system to detect
unauthorized computer intrusions. This paper describes the use of an
artiﬁcial immune system for another kind of protection: protection from
unsolicited email, or spam.
1
Introduction
The word “spam” is used to denote the electronic equivalent of junk mail. This
typically includes advertisements (unsolicited commercial email or UCE) or other
messages sent in bulk to many recipients (unsolicited bulk email or UBE). Al-
though spam may also include viruses, typically the term is used to refer to the
less destructive classes of email.
In small quantities, spam is simply an annoyance but easily discarded. In
larger quantities, however, it can be time-consuming and costly. Unlike tradi-
tional junk mail, where the cost is borne by the sender, spam creates further
costs for the recipient and for the service providers used to transmit mail. To
make matters worse, it is diﬃcult to detect all spam with the simple rule-based
ﬁlters commonly available.
Spam is similar to computer viruses because it keeps mutating in re-
sponse to the latest “immune system” response. If we don’t ﬁnd a techno-
logical solution to spam, it will disable Internet email as a useful medium,
just as viruses threatened to disable the PC revolution. [1]
Although many people would consider this statement a little over-dramatic, there
is deﬁnitely real need for methods of controlling spam (unsolicited email).
This paper will look at a new mechanism for controlling spam: an artiﬁcial
immune system (AIS). The authors of this paper have found no other research
involving creation of a spam-detector based on the function of the mammalian
immune system, although the immune system model has been applied to the
similar problem of virus detection [2].
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 231–242, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

232
T. Oda and T. White
2
The Immune System
To understand how an artiﬁcial immune system functions, we need to consider
the mammalian immune system upon which it is based. This is only a very
general overview and simpliﬁcation of the workings of the immune system which
uses information from several sources [3], [4]. A more complete and accurate
description of the immune system can be found in many biology texts.
In essence, the job of an immune system is to distinguish between self and
potentially harmful non-self elements.
The harmful non-self elements of particular interest are the pathogens. These
include viruses (e.g. Herpes simplex), bacteria (e.g. E. coli), multi-cellular par-
asites (e.g. Malaria) and fungi. From the point of view of the immune system,
there are several features that can be used to identify a pathogen: the cell surface,
and soluble proteins called antigens.
In order to better protect the body, an immune system has many layers
of defence: the skin, physiological defences, the innate immune system and the
acquired immune system. All of these layers are important in building a full viral
defence system, but since the acquired immune system is the one that this spam
immune system seeks to emulate, it is the only one that we will describe in more
detail.
2.1
The Acquired Immune System
The acquired immune system is comprised mainly of lymphocytes, which are
types of white blood cells that detect and destroy pathogens. The lymphocytes
detect pathogens by binding to them.
There are around 1016 possible varieties of antigen, but the immune system
has only 108 diﬀerent antibody types in its repertoire at any given time. To
increase the number of diﬀerent antigens that the immune system can detect,
the lymphocytes bind only approximately to the pathogens. By using this ap-
proximate binding, the immune system can respond to new pathogens as well as
pathogens that are similar to those already encountered. The higher aﬃnity the
surface protein receptors (called antibodies) have for a given pathogen, the more
likely that lymphocyte will bind to it. Lymphocytes are only activated when the
bond reaches a threshold level, that may be diﬀerent for diﬀerent lymphocytes.
Creating the detectors. In order to create lymphocytes, the body uses a
“library” of genes that are combined randomly to produce diﬀerent antibodies.
Lymphocytes are fairly short-lived, living less than 10 days, usually closer to 2
or 3. They are constantly replaced, with something on the order of 100 million
new lymphocytes created daily.
Avoiding Auto-immune Reactions. An auto-immune reaction is one where
the immune system attacks itself. Obviously this is not desirable, but if lympho-
cytes are created randomly, why doesn’t the immune system detect self?

Developing an Immunity to Spam
233
This is done by self-tolerization. In the thymus, where one class of lympho-
cytes matures, any lymphocyte that detects self will either be killed or simply
not selected. These specially self-tolerized lymphocytes (known as T-helper cells)
must then bind to a pathogen before the immune system can take any destructive
action. This then activates the other lymphocytes (known as B-cells).
Finding the Best Fit. (Aﬃnity maturation) Once lymphocytes have been
activated, they undergo cloning with hypermutation. In hypermutation, the mu-
tation rate is 109 times normal. Three types of mutations occur:
– point mutations,
– short deletions,
– and insertion of random gene sequences.
From the collection of mutated lymphocytes, those that bind most closely to
the pathogen are selected. This hypermutation is thought to make the coverage
of the antigen repertoire more complete. The end result is that a few of these
mutated cells will have increased aﬃnity for the given antigen.
3
Spam as the Common Cold
Receiving spam is generally less disastrous than receiving an email virus. To
continue the immune system analogy, one might say spam is like the common
cold of the virus world – it is more of an inconvenience than a major infection,
and most people just deal with it. Unfortunately, like the common cold, spam
also has so many variants that it is very diﬃcult to detect reliably, and there are
people working behind the scenes so the “mutations” are intelligently designed
to work around existing defences.
Our immune systems do not detect and destroy every infection before it has
a chance to make us feel miserable. They do learn from experience, though,
remembering structures so that future responses to pathogens can be faster.
Although ﬁghting spam may always be a diﬃcult battle, it seems logical to ﬁght
an adaptive “pathogen” with an adaptive system.
We are going to consider spam as a pathogen, or rather a vast set of varied
pathogens with similar results, like the common cold. Although one could say
that spam has a “surface” of headers, we will use the entire message (headers
and body) as the antigen that can be matched.
4
Building a Defence
4.1
Layers Revisited
Like the mammalian immune system, a digital immune system can beneﬁt from
layers of defence [5]. The layers of spam defence can be divided into two broad
categories: social and technological. The proposed spam system is a technolog-
ical defence, and would probably be expected to work alongside other defence
strategies. Some well-known defences are outlined below.

234
T. Oda and T. White
Social Defences. Many people are attempting to control spam through social
methods, such as suing senders of spam [6], legislation prohibiting the send-
ing of spam [7], or more grassroots methods [8].
Technological Defences. To defend against spam, people will attempt to
make it diﬃcult for spam senders to obtain their real email address, or
use clever ﬁltering methods. These include two of particular interest for this
paper:
SpamAssassin [9] uses a large set of heuristic rules.
Bayesian/Probabilistic Filtering [10] [11] uses “tokens” that are rated
depending on how often they appear in spam or in real mail. Probabilistic
ﬁlters are actually the closest to the proposed spam immune system, since
they learn from input.
Some solutions, such as the Mail Abuse Prevention System (MAPS) Realtime
Blackhole List (RBL) fall into both the social and the technological realms. RBL
provides a solution to spam through blocking mail from networks known to be
friendly or neutral to spam senders [12]. This helps from a technical perspective,
but also from a social perspective since users, discovering that their mail is being
blocked, will often petition their service providers to change their attitudes.
4.2
Regular Expressions as Antibodies
Like real lymphocytes, our digital lymphocytes have receptors that can bind to
more than one email message. This is done by using regular expressions (patterns
that match a variety of strings) as antibodies. This allows use of a smaller gene
library than would otherwise be necessary, since we do not need to have all
possible email patterns available. This has the added advantage that, given a
carefully-chosen library, a digital immune system could be able to detect spam
with only minimal training.
The library of gene sequences is represented by a library of regular expressions
that are combined randomly to produce other regular expressions. Individual
“genes” can be taken from a variety of sources:
– a set of heuristic ﬁlters (such as those used by SpamAssassin)
– an entire dictionary
– several entire dictionaries for diﬀerent languages
– a set of strings used in code, such as HTML and Javascript, that appears in
some messages
– a list of email addresses and URLs of known spam senders
– a list of words chosen by a trained or partially-trained Bayesian Filter
The combining itself can be done as a simple concatenation, or with wild-
cards placed between each “gene” to produce antibodies that match more general
patterns.
Unfortunately, though this covers the one-to-many matching of antibodies
to antigens, there is no clear way to choose which of our regular expression
antibodies has the best match, since regular expressions are handled in a binary
(matches/does not match) way. Although an arbitrary “best match” function
could be applied, it is probably just as logical to treat all the matching antibodies
equally.

Developing an Immunity to Spam
235
4.3
Weights as Memory
Theories have proposed that there may be a longer-lived lymphocyte, called a
memory B-cell, that allows the immune system to remember previous infections.
In a digital immune system, it is simple enough to create a special subclass of
lymphocytes that is very long-lived, but doing this may not give the desired
behaviour.
While a biological immune system has access to all possible self-proteins, a
spam immune system cannot be completely sure that a given lymphocyte will not
match legitimate messages in the future. Suppose the user of the spam immune
system buys a printer for the ﬁrst time. Previously, any message with the phrase
“inkjet cartridges” was spam (e.g. “CHEAP INKJET CARTRIDGES ONLINE
– BUY NOW!!!”), but she now emails a friend to discuss ﬁnding a store with
the best price for replacement cartridges. If her spam immune system had long-
lived memory B-cells, these would continue to match not only spam, but also
the legitimate responses from her friend that contain that phrase.
In order to avoid this, we need a slightly more adaptive memory system in
that it can unlearn as well as learn things. A simple way to model this is to use
weights for each lymphocyte.
In the mammalian immune system, pathogens are detected partially because
many lymphocytes will bind to a single pathogen. This could easily be dupli-
cated, but matching multiple copies of a regular expression antibody is needlessly
computationally intensive. As such, we use the weights as a representation of the
number of lymphocytes that would bind to a given pathogen.
When a lymphocyte matches a message that the user has designated as spam,
the lymphocyte’s weight is then incremented (e.g. by a set amount or a multiple
of current weight) Similarly, when a lymphocyte matches something that the
user indicates is not spam, then the weight is decremented.
Although the lymphocyte weights can be said to represent numbers of lym-
phocytes, it is important to note that these weights can be negative, representing
lymphocytes which, eﬀectively, detect self. Taking a cue from SpamAssassin, we
use the sum of the positive and negative weights as the ﬁnal weight of the mes-
sage. If the ﬁnal weight is larger than a chosen threshold, it can be declared as
spam. (Similarly, messages with weights smaller than a chosen threshold can be
designated non-spam.)
The system can be set to learn on its own from existing lymphocytes. If a new
lymphocyte matches a message that the immune system has designated spam,
then the weight of the new lymphocyte could be incremented. This increment
would probably be less than it would have been with a human-conﬁrmed spam
message, since it is less certain to be correct. Similarly, if it matches a message
designated as non-spam, its weight is decremented.
When a false positive or negative is detected, the user can force the system
to re-evaluate the message and update all the lymphocytes that match that mes-
sage. These incorrect choices are handled using larger increments and decrements
so that the automatic increment or decrement is overridden by new weightings

236
T. Oda and T. White
based on the correction. Thus, the human feedback can override the adaptive
learning process if necessary.
In this way, we create an adaptive system that learns from a combination of
human input and automated learning.
An Algorithm for Aging and Cell Death. Lymphocytes “die” (or rather,
are deleted) if they fall below a given weight and a given age (e.g. a given
number of days or a given number of messages tested). This simulates not only
the short lifespan of real lymphocytes, but also the negative selection found in
the biological immune system.
We beneﬁt here from being less directly related to the real world. Since there
is no good way to be absolutely sure that a given lymphocyte will not react
to the wrong messages, co-stimulation by lymphocytes that are guaranteed not
to match legitimate messages would be diﬃcult. Attempting to simulate this
behaviour might even be counter-productive with a changing ”self.” For this
prototype, we chose to keep the negatively-weighted, self-detecting lymphocytes
in this prototype to help balance the system without co-stimulation as it occurs
in nature. Thus, cell death occurs only if the absolute value of the weight falls
below a threshold. It should be possible to create a system which ”kills” oﬀthe
self-matching lymphocytes as the self changes, but this was not attempted for
this prototype.
How legitimate is removing those with weights with small absolute values?
Consider a antibody that never matches any messages (e.g. antidisestab-
lishmentarianism.* aperient.* kakistocracy). It will have a weight of 0,
and there is no harm in removing it since it does not aﬀect detection. Even a
lymphocyte with a small absolute weight is not terribly useful, since small abso-
lute weights mean that the lymphocyte has only a small eﬀect on the ﬁnal total.
It is not a useful indicator of spam or non-spam, and keeping it does not beneﬁt
the system.
A simple algorithm for artiﬁcial lymphocyte death would be:
if (cell is past “expiry date”) {
decrement weight magnitude
if (abs(cell weight) < threshold) {
kill cell
} else {
increment expiry date
}
}
The decrement of the weight is to simulate forgetfulness, so that if a lym-
phocyte has not had a match in a very long time, it can eventually be recycled.
This decrement should be very small or could even be none, depending on how
strong a memory is desired.

Developing an Immunity to Spam
237
4.4
Mutations?
Since we have no algorithm deﬁned to say that one regular expression is a better
match than another, we cannot use mutation easily to ﬁnd matches that are more
accurate. Despite this, there could still be a beneﬁt to mutating the antibodies of
a digital immune system, since it would be possible (although perhaps unlikely)
that some of the new antibodies created would match more spam, even if there
was no clear way to deﬁne a better match with the current message. Mutations
could be useful for catching words that spam senders have hyphenated, mis-
spelled intentionally, or otherwise altered to avoid other ﬁlters. At the very least,
mutations would have a higher chance of matching with similar messages than
lymphocytes created by random combinations from the gene library.
Mutations could occur in two ways:
1. They could be completely random, in which case some of the mutated regular
expressions will not parse correctly and will not be usable.
2. They could be mutated according to a scheme similar to that of Automat-
ically Deﬁned Functions (ADF) in genetic programming [13]. This would
leave the syntax intact so that the result is a legitimate regular expression.
It would be simpler to write code that would do random mutations, but then
harder to check the syntax of the mutated regular expressions if we wanted to
avoid program crashing when lymphocytes with invalid antibodies try to bind
to a message. These lymphocytes would simply die through negative selection
during the hypermutation process, since they are not capable of matching with
anything. Conversely, it would be harder to code the second type, but it would
not require any further syntax-checking.
Another variation on mutation is an adaptive library. In some cases, no lym-
phocytes will match a given message. If this message is tagged as spam by the
user, then the system will be unable to “learn” more about the message because
no weights will be updated. To avoid this situation, the system could generate
new gene sequences based upon the message. These could be “tokens” as de-
scribed by Graham [11], or random sections of the email. These new sequences,
now entered into the gene pool, will be able to match and learn about future
messages.
5
Prototype Implementation
Our implementation has been done in Perl because of its great ﬂexibility when
it comes to working with strings.
The gene library and lymphocytes are stored in simple text ﬁles. Figure 1
shows the contents of a short library ﬁle. In the library, each line is a regular
expression. Each “gene” is on a separate line.
Figure 2 shows the contents of a short lymphocytes ﬁle. For the lymphocytes,
each line contains the weight, the cell expiry date and the antibody regular ex-
pression. The format uses the string ”###” (that does not occur in the library)

238
T. Oda and T. White
remove.{1,15}subject
Bill.{0,10}1618.{0,10}TITLE.{0,10}(III|\#3)
check or money order
\s+href=[’"]?www\.
money mak(?:ing|er)
(?:100%|completely|totally|absolutely) (?-i:F)ree
Fig. 1. Sample Library Entries
-5###1040659390###result of
10###1040659390###\<BODY.*bgcolor="#?[ˆf]
-98###1040659390###(?:\$|US\$|usd?).?\d{2,3}(?:\.\d)?
52###1040661344##trial version of CommuniGate.*opt-in
33###1040661344###click .{0,30}(?:here|below)
Fig. 2. Sample Lymphocytes
as a separator, so each lymphocyte deﬁnition string has <weight>###<expiry
time>###<regular expression antibody>.
Our AIS spam detector has three phases:
1. Generation of Lymphocytes
2. Application of Lymphocytes
3. Culling of Lymphocytes
Figure 3 gives an overview of the life cycle of a spam lymphocyte. The gen-
eration and culling scripts are run on a regular schedule, and the application
script is called with appropriate arguments to increment, decrement, or simply
detect, as appropriate. For example, if the target message has been tagged as
spam by the user, the application script is called with a large increment so that
all the lymphocytes that match that message are incremented. The target mes-
sage could fall into six potential classes: Tagged as spam or legitimate by the
user, tagged as spam or legitimate by the immune system, or tagged as a false
positive or a false negative by the user.
5.1
Generation
This script reads all the “genes” in the library and combines them randomly to
produce the number of antibodies speciﬁed as the ﬁrst argument.
The antibodies are created by choosing a gene randomly, then eﬀectively
ﬂipping a coin to see if another gene should be appended to the resulting reg-
ular expression antibody. The probability of appending to an antibody can be
changed relatively easily. For the purposes of testing, we chose 50% as a the
likelihood for each gene addition. (Although an antibody using fewer genes will
tend to match more frequently, we wished to to see if some longer antibodies
matched spam more exclusively.) To add another gene, another gene is chosen

Developing an Immunity to Spam
239
Fig. 3. Life cycle of a digital lymphocyte
randomly and the original string is concatenated with the string .* (match any
number of characters) followed by the new gene sequence. This intermediary .*
is used to increase the number of possible matches.
The lymphocyte is then given a weight of 0 and an expiry date. The resulting
lymphocytes are written to disk in the order of generation.
There is no attempt made to avoid duplicates since doing so could be costly
and having duplicates should not overly adversely aﬀect results, only giving
some antibodies a stronger eﬀect by duplicating. This eﬀect should be balanced
by other antibodies.
5.2
Application
The application script reads in all the lymphocytes and applies them one by one
to each message in the ﬁle speciﬁed as ﬁrst argument. The total weighted sums
are outputted for each message, along with the Subject: and From: lines to give
some context.
The antibodies are applied in a case-insensitive manner. This was chosen so
that more matches would be found, although a quick comparison using the same
lymphocytes showed that matching in a case-sensitive manner did not make a
signiﬁcant diﬀerence in the weightings.
This script takes a second argument that indicates the increment or decre-
ment to be applied to each lymphocyte that matches messages in this ﬁle. The
updated lymphocytes are then re-written to the lymphocytes ﬁle.
5.3
Culling
The culling script reads in all the lymphocytes and kills oﬀcells based on the
algorithm described earlier in this paper. Ideally, the culling script would call
the generation script to replace those lymphocytes removed, but for the moment
this is done manually.
6
Results
6.1
Initial Parameters and Data
We initially used a set of 150 regular expression ”genes” as a library. Many of
these were simpliﬁed SpamAssassin heuristics. SpamAssassin contains over 700

240
T. Oda and T. White
very complex heuristics, but our gene library contained only pieces of those,
typically pieces that matched only a few shorter phrases. These heuristics were
used to reduce the training time required for the lymphocytes, since many people
would prefer a solution which required little training time. (If individual users
were not interested in training their own artiﬁcial immune systems at all, it
could be possible give them spam-trained lymphocytes, much like users currently
download known virus patterns for their computer virus scanners.)
To test and train the lymphocytes, a set of mailbox ﬁles (in mbox format)
were used with messages compiled from personal mail, Grant Taylor’s collection
of spam [14], and mailing list archives of lists known to contain both legitimate
posts and spam.
In a quick test with 100 lymphocytes generated, it was soon apparent that
either this size of library was insuﬃcient to detect the selection of spam being
used to train the lymphocytes or more lymphocytes needed to be generated.
Over 40% of the messages known to be spam were not detected by any of the
lymphocytes, and thus the immune system could not learn about these messages.
This may indicate a need for a process akin to aﬃnity maturation where, knowing
a message to be spam, the digital immune system produces lymphocytes that
match it.
With a larger library of around 300 genes and set of around 1000 lymphocytes
generated, the tests take much longer to run, so only a few iterations were run
after the initial testing phase.
Increment and decrement values were chosen so that a false positive or nega-
tive was given a more signiﬁcant weight than either training values or automated
learning values. Because missed messages are more irritating to users, we wanted
to encourage faster learning from these messages so that users would not have
to keep reporting similar missed messages. The following values were used:
Determined by
User
AIS
User
(training) (automated) (false positive/negative)
Spam/non-spam
weight adjustment
5/-5
1/-1
10/-10
6.2
Output
The initial spam training gave weights ranging from 0 to over 105. As such, some
lymphocytes had a much larger ability than others to aﬀect the ﬁnal sum. Some
of these were balanced after initial “self-tolerization” training against legitimate
messages.
After the initial training phase of over 1600 spam messages and 1000 legiti-
mate messages was done, we forced a cell death on lymphocytes with absolute
weights less than 4. Out of the 1000 lymphocytes originally generated, less than
100 were saved, leading credence to the idea that a larger lymphocyte population
would be necessary to do proper tests.
These remaining 100 lymphocytes were used as a quick test against a het-
erogeneous (spam and non-spam) mailbox with approximately 1200 messages.

Developing an Immunity to Spam
241
The results were very promising. We set the threshold at 10, but even with this
small threshold, only two false positives occurred (where a legitimate message
was found as spam).
– One was a message sent by a random stranger and was vastly diﬀerent in
content and style from the messages used initially to set the weights.
– The other was a message from a current spam control system. It contained
a listing of the Subject: and From: lines of the messages blocked that day.
Although the second false positive seems reasonably explicable, this be-
haviour is problematic for layering spam defences. Some sort of special exception
may have to be found so that messages of this sort from a reporting system will
not be caught. This could be through manual addition of an overriding lympho-
cyte (one with a very large negative weight), for example. Training the system
normally should be possible, but could neutralise otherwise-useful lymphocytes.
Similarly, the spam messages not tagged by the system were unusual spams,
unlike those that had been seen earlier by the system.
There were some negatively-valued messages, but most of the messages were
given weights of 0, as would be expected given that the library was drawn from
spam-matching regular expressions used by SpamAssassin. If a more broad-based
library were used, then there would be more negatively-weighted lymphocytes,
and thus it would be possible to have more negatively-valued messages.
Further tests after more iterations and re-running of false positives yielded
similar results to those initial ones: approximately 1% of the messages scanned
were false positives. Overall, the 1000 lymphocyte system was correctly identi-
fying approximately 90% of the spam messages.
7
Conclusions
This model has some potential: even with empirically-chosen values for incre-
ments and decrements and a relatively small library of regular expressions, emer-
gent behaviour has been observed. It is detecting spam with very few false pos-
itives in the test mailboxes. Due to the small size of the library space, it is still
missing approximately 10% of the spam messages entirely. While false negatives
are less damaging than false positives [15], they are still an indicator of a detec-
tor or library space that is still too small to cover the range of potential spam
messages.
This initial 90% accuracy is not as good as Graham’s probabilistic ﬁlter,
which can work at over 99% accuracy [11]. However, while we looked at 1000
lymphocytes, Graham’s system recognises over 23,000 tokens, and his algorithm
has had signiﬁcantly more time to mature.
7.1
Future Work
Several directions for future research are planned. A mutation model is to be
included, perhaps with a function to determine better matching or preferential

242
T. Oda and T. White
weightings of lymphocytes (e.g. One could use the length of the string matched by
a given regular expression as an indicator of better matching[16]). Larger and al-
ternative gene libraries are to be created and tested. A variation self-tolerization
could be used to generate only lymphocytes which match spam rather than
relying upon weight-balancing through self-detecting lymphocytes. Finally, al-
ternative weighting schemes are to be designed and compared to the performance
of the algorithm described in this paper.
References
1. Hellweg, E.: What price spam? Business 2.0 (1999)
2. White, S.R., Swimmer, M., Pring, E.J., Arnold, W.C., Chess, D.M., Morar, J.F.:
Anatomy of a commercial-grade immune system. Technical report, IBM Thomas
J. Watson Research Center (2002)
3. Nunn, I.: Immunocomputing: The natural immune system and its computational
metaphors. (2002)
http://www.scs.carleton.ca/ arpwhite/courses/95590Y/notes/Immunocom-
puting.ppt.
4. Hofmeyr, S.A.: An overview of the immune system (1997)
http://www.cs.unm.edu/˜immsec/html-imm/immune-system.html.
5. Sakkis, G., Androutsopoulos, I., Paliouras, G., Karkaletsis, V., Spyropoulos, C.D.,
Stamatopoulos, P.: Stacking classiﬁers for anti-spam ﬁltering of E-mail. In: Pro-
ceedings of EMNLP-01, 6th Conference on Empirical Methods in Natural Language
Processing, Pittsburgh, US, Association for Computational Linguistics, Morris-
town, US (2001)
6. Jesdanun, A.: AOL wins $7 million spam lawsuit. Salon.com (2002)
http://www.salon.com/tech/wire/2002/12/17/aol spam/.
7. Email, C.A.U.: Pending legislation (2002) http://www.cauce.org/legislation.
8. Wendland, M.: Internet spammer can’t take what he dishes out. Detroit Free Press
(2002)
9. SpamAssassin: Spamassassin website (2002) http://spamassassin.org/.
10. Sahami, M., Dumais, S., Heckerman, D., Horvitz, E.:
A bayesian approach to
ﬁltering junk E-mail. In: Learning for Text Categorization: Papers from the 1998
Workshop, Madison, Wisconsin, AAAI Technical Report WS-98-05 (1998)
11. Graham, P.: A plan for spam (2002) http://www.paulgraham.com/spam.html.
12. Vixie, P.: MAPS RBL rationale (2000)
http://mail-abuse.org/rbl/rationale.html.
13. Poli, R.: Introduction to evolutionary computation: Automatically deﬁned func-
tions in genetic programming (1996)
http://www.cs.bham.ac.uk/˜rmp/slide book/node7.html.
14. Taylor, G.: A collection of spam (2002)
http://www2.picante.com:81/˜{}gtaylor/download/spam.tar.gz.
15. Androutsopoulos, I., Koutsias, J., Chandrinos, K.V., Paliouras, G., Spyropoulos,
C.D.: An evaluation of naive bayesian anti-spam ﬁltering. In Potamias, G., Mous-
takis, V., van Someren, M., eds.: Proceedings of the Workshop on Machine Learn-
ing in the New Information Age, 11th European Conference on Machine Learning
(ECML 2000), Barcelona, Spain (2000) 9–17
16. O’Byrne, K.: Private communication (2002)

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 243–245, 2003.
© Springer-Verlag Berlin Heidelberg 2003
A Novel Immune Anomaly Detection Technique Based on
Negative Selection
F. Niño, D. Gómez, and R. Vejar
Department of Computer Science
National University of Colombia
Bogotá, Colombia
lfnino@ing.unal.edu.co, {djgomezb, ravejaru}@unal.edu.co
1 Introduction
In this paper, a novel immune based anomaly detection technique is proposed.
Specifically an Artificial Immune System (AIS), based on negative selection, is used
to detect some particular kind of computer attacks (Denial of Service attacks) on a
typical LAN environment.
2 AIS Architecture
In this work, the general model of an AIS consists of the following phases: data
collection, data pre-processing, learning (training), post-processing of the detectors
and the detection phase. The learning and detection phases are described next.
2.1 
Learning Phase
Here, a representation of the environment, the antigens (negative samples) and the
detectors (antibodies) is chosen. Besides, a matching rule between detectors and
antigens is specified; an antigen matches a detector if the distance between them is
less than a threshold value. The goal of the training process of the AIS is to find an
optimum covering of the self space with a suitable set of antibodies (hyper-spheres),
which are generated based on the set of sample antigens. In intrusion detection, such
set of antigens will correspond to DoS attacks.
• 
Generation of Antibodies. The goal is to produce a new population of antibodies
in the self space from the current population. The radius of a new antibody
depends on the distance from its center to its nearest antigen. The generation of
new antibodies is determined by its overlapping with existing detectors. The
overlapping between two antibodies is a measure of the intersection between
their corresponding hyper-spheres. Hence, the goal is to make such overlapping
measure as small as possible.
• 
Selection of Antibodies. The best antibodies are selected according to the
following criteria: the fittest antibodies are determined according to their radii

244         F. Niño, D. Gómez, and R. Vejar
and their overlapping with other antibodies. Detectors with larger radius are more
likely to be selected. Besides, if its overlapping measure is greater than a
specified threshold, then such antibody will be discarded. Otherwise, it will
become part of the next generation.
• 
Cloning Antibodies. A new set of antibodies is generated from the existing
antibodies. A clone of an antibody is generated by making a copy of such
antibody and moving its center at random; its radius is adjusted based on its
distance to the nearest antigen.
• 
Post-processing Antibodies. The goal is to cover small regions of the self space
that may remain uncovered after the learning process, and that may produce false
negatives. The radius of a new antibody is adjusted using the distance to the
nearest antibody.
2.2 
Detection Phase
A set of unseen patterns is presented to the AIS to be classified as either
corresponding to normal traffic or to DoS attacks. The degree of abnormality of a
pattern is proportional to the distance between the input pattern and the set of
detectors. If the distance from a detector to an input pattern is smaller than a threshold
then such pattern belongs to the self set. Otherwise, it will be considered as abnormal
traffic. Four fields of the header information were used as input to the AIS, namely,
source and target port, packet size and protocol ID. This data was used to
completely characterize the information that would allow distinguishing packets
belonging to DoS attacks and normal traffic.
3 Experimental Results
Network traffic was collected and used during the AIS learning process. The captured
data was divided into two sets: the training set, which consisted of DoS attacks, and
the test set that consisted of both, normal traffic data and DoS attacks. An input
pattern consisted of a sequence of several consecutive packets. The AIS was able to
detect all the attacks in the training set (it consisted of 4516 patterns), 99% of the
attacks were detected in the test set, and the AIS performed well in classifying normal
traffic. However, some patterns corresponding to normal traffic were misclassified as
possible attacks. In the best experimental results, the AIS correctly classified 88% of
normal traffic.
4   Conclusions
In this paper, a novel general anomaly detection technique based on immunology was
developed. One main advantage of the AIS is that it starts with a small number of
detectors and a new set of antibodies is generated through an iterative process that
improves the covering of the self space. The number of detectors generated during the
training process was smaller than the size of the input data set because the learning
process allows the detectors to have variable radius and it is possible to cover the self

A Novel Immune Anomaly Detection Technique Based on Negative Selection         245
space with a small number of detectors. The post-processing of the antibodies
improves the performance of the AIS. In future work, the immune technique may be
applied to detect other types of intrusions in computer systems, or to solve any other
anomaly detection problem.
References
1.
R. Bolaños and C. Cadena. Intrusion detection in Linux using neural networks (in spanish).
Computer Science Thesis. National University of Colombia. Bogotá, Colombia 2002.
2.
D. Dasgupta (Editor), Artificial Immune Systems and Their Applications, Publisher:
Springer-Verlag, Inc. Berlin, January 1999.
3.
D. Dasgupta and F. Nino. Comparison of Negative and Positive Selection Algorithms in
Novel Pattern Detection, In the Proceedings of the IEEE International Conference on
Systems, Man and Cybernetics (SMC), Nashville, 2000.
4.
IBM. Denial of service attacks: Understanding network Vulnerabilities in www.ibm.com.
2002.

Visualization of Topic Distribution Based on
Immune Network Model
Yasufumi Takama
Tokyo Metropolitan Institute of Technology
6-6 Asahigaoka, Hino, Tokyo 191-0065 JAPAN
PREST, Japan Science and Technology Corporation, JAPAN
ytakama@cc.tmit.ac.jp
Abstract. A method is presented that applies the properties of immune
system for visualizing the topic distribution over a document set, as well
as the topic stream through a sequence of document sets.
1
Introduction
As the Web provides us with new topics constantly, it is diﬃcult for us to grasp
the trends or change of topics. In particular, there exists the information, which
cannot be found from a single document but from a sequence of document sets.
We are developing a Web information visualization method that can ﬁnd the
topic distribution over the document set(s)[1,2]. As the fundamental method,
a plastic clustering method[2] has been proposed for generating a topic-
sensitive keyword map. One of the characteristic features of the plastic clustering
method is the generation of keyword map as well as document clustering. The
keyword’s activation value is calculated based on the immune network model,
which is also useful as the visualization metaphor to improve the understand-
ability of the keyword map. The model of memory cell is also incorporated, so
that it can ﬁnd the topical relation among diﬀerent document sets.
2
Examples of Extracted Topic Stream
The experiment shown here is performed on the sequences of Yahoo! Japan online
news article sets that were issued from 17 to 21 September 2001. Fig. 1 shows
the landmarks extracted from the sequence. The landmarks extracted by both
methods (with/without memory cell) are indicated with dotted texture. In Fig.
1, only one landmark is reused more than once, among 18 landmarks (5.56%)
when no memory cell is used. On the contrary, 3 landmarks are reused among
17 landmarks (17.6%) with using memory cell.
As the news articles within the sequence was issued just after the tragedy in
N.Y., many articles contain the related topics. Two landmarks, ‘Performance’,
‘Multiple’, which are reused more than once, concern the topics related to this
N.Y. tragedy. That is, ‘Multiple’ literally concerns the simultaneous multiple ter-
rorist attacks occurred in New York. After that disaster, many concerts, events,
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 246–247, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Visualization of Topic Distribution Based on Immune Network Model
247
Sep.17
Sep.18
Sep.19
Sep.20
Sep.21
Performance
Fukada
Guest
Ginza
Interview
Comedy
Donation Danger
Major
Osaka
Carry
News
Marriage
Sale
Expla-
nation
Perform-
ance
Society
Number
New-work
Sep.17
Sep.18
Sep.19
Sep.20
Sep.21
Performance
Fukada
Guest
Ginza
Interview
Release
Donation
Major
District
Court
News
Marriage
Germany
Photo
Metro-
politan
Charity
Hero
Multiple
(a)Without Memory Cell
Date
Date
(b)With Memory Cell
Fig. 1. Landmarks Extracted from Online News Sequence
etc., were performed/held for the purpose of charity, donation, or encouragement
for the victims. The landmark ‘Performance’ mainly refer to this kind of topics.
Performance
Guest
Fukada
Ginza
Fig. 2. Keyword Map Generated From Document Set of 17 Sep. 2001.
3
Conclusion
A method for extracting topic distribution over document space based on im-
mune network model is presented. We are now developing the keyword map-
based visualization tool that can visualize the extracted topic distribution as
shown in Fig.2.
References
1. Takama, Y. and Hirota, K., “Web Information Visualization Method Employing
Immune Network Model for Finding Topic Stream from Document-Set Sequence,”
J. of New Generation Computing, Vol. 21, No. 1, pp. 49–59, 2003.
2. Takama, Y., Hirota, K., “Web Information Visualization Based on Immune Network
Metaphor,” J. of Japan Society for Fuzzy Theory and Systems, 14, 5, pp. 472–481,
2002.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 248–249, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Spatial Formal Immune Network
Alexander O. Tarakanov
St. Petersburg Institute for Informatics, Russian Academy of Sciences
14-line 39, St. Petersburg 199178, Russia
sasha_tar@hotmail.com
Abstract. A notion of Spatial Formal Immune Network is proposed for pattern
recognition applications.
In our previous works [4], [6], we have proposed a rigorous mathematical notion of
Formal Immune Network (FIN) inspired by N. Jerne’s network theory of the immune
system [3]. That FIN has been introduced as Integer valued (IFIN), where B-cells are
coded by integers and formed ordered sequences (populations). Such IFIN provides a
discrete mathematical model of immune response, which properties described by a
row of theorems. However, IFIN is insufficient for real-world applications with multi-
dimensional and real valued data, like those in surveillance of the natural plague or
information security.
The present work makes an attempt to solve the above problem by introducing
Spatial FIN (SFIN) as an expanded FIN for pattern recognition applications.
Define SFIN as a tuple
SFIN = < R
N, m, w, ∆h, B-cells >,
where
B-cell = <S, P>,
R
N is N-dimensional Euclidean space considered as the shape space of SFIN,
according to [1];
P∈R
N is N-dimensional vector (point of the shape space) considered as the Formal
Protein (FP) as well as the receptor of the B-cell (see, e.g., [6]);
m is the number of the nearest neighbors of any B-cell;
w is the Euclidean distance considered as the binding energy between FPs:
w: R
N×R
N→R
1, wij = |Pi −Pj|;
h is real valued mutation step: h∈R
1;
S is the state indicator of B-cell: S = {ltm, stm, del}, where ltm is long-term
(memory) cell,  stm is short-term (memory) cell, and  del is deleted cell.
The behavior of the SFIN is determined by the following rules.
All B-cells update their states simultaneously in a discrete time t = 0,1,2, … .
Let {Bi} is current set (population) of B-cells: Si≠ del, i = 1,…,n.
Any B-cell Bi has no more than m nearest neighbors, which correspond to the m
nearest points in the shape space (rule B_neighbor).

Spatial Formal Immune Network
249
Any B-cells Bi, Bj are close if wij<h .
Any B-cell  Bi  updates its state as follows.
Short-term cell Bj becomes deleted if it is close to any long-term cell Bi (rule
B_delete).
In case of the deletion, consider that long-term cell Bi recognizes short-term cell Bj.
Short-term cell makes m copies (proliferates) by the mutation step to the directions
of the nearest neighbors if it isn’t close to any cell (rule B_prolif).
Short-term cell becomes long-term cell if it is close to any short-term cell but isn’t
close to any long-term cell (rule B_interf).
Long-term cell drifts by the mutation step to the direction of the nearest long-term
cell if the cells are close (rule B_drift).
Two long-term cells Bi , Bj fuse if they are identical (rule B_fusion).
If any new FP (antigen) PA intrudes to the SFIN, then new short-term cell (antigen
presenting cell) appears, which expose the intruder as the receptor (rule A_pres).
The following conclusions can be done based on simple theoretical examples of
immune response of SFIN, as well as on the applications of SFIN to predict risk of the
plague infection and to detect intrusions in computer networks.
SFIN differs essentially from immune algorithms proposed by [2]. The algorithms
represent special case of genetic algorithms, because immune cells are coded by bit
strings and mutate by random rules. Unlike these features, cells of SFIN are coded by
real-valued vectors and their behavior is deterministic.
Actually, SFIN represents a modification of hybrid cellular automata, which have
been applied successfully to the modeling of rather complex dynamical process [5].
References
1. De Boer, R.J., Segel, L.A., Perelson, A.S.: Pattern Formation in One and Two-Dimensional
Shape Space Models of the Immune System. J. Theoret. Biol. 155 (1992) 295–333
2. De Castro, L.N., Timmis, J.: Artificial Immune Systems: A New Computational Intelligence
Paradigm. Springer-Verlag, Berlin Heidelberg New York (2002)
3. Jerne, N.K.: Toward a Network Theory of the Immune System. Ann. Immunol. 125C Paris
(1974) 373–389
4. Tarakanov, A.O.: Information Security with Formal Immune Networks. Lecture Notes in
Computer Science, Vol. 2052. Springer-Verlag, Berlin Heidelberg New York (2001) 115–
126
5. Tarakanov, A., Adamatzky, A.: Virtual Clothing in Hybrid Cellular Automata. Kybernetes
(Int. J. of Systems & Cybernetics) 31 (7/8) (2002) 1059–1072
6. Tarakanov, A.O., Skormin, V.A., Sokolova, S.P.: Immunocomputing: Principles and
Applications. Springer-Verlag, Berlin Heidelberg New York (2003)

Focusing versus Intransitivity
Geometrical Aspects of Co-evolution
Anthony Bucci⋆and Jordan B. Pollack
Brandeis University, Waltham MA 02454, USA
{abucci,pollack}@cs.brandeis.edu
http://demo.cs.brandeis.edu/
Abstract. Recently, a minimal domain dubbed the numbers game has
been proposed to illustrate well-known issues in co-evolutionary dynam-
ics. The domain permits controlled introduction of features like intran-
sitivity, allowing researchers to understand failings of a co-evolutionary
algorithm in terms of the domain. In this paper, we show theoretically
that a large class of co-evolution problems closely resemble this minimal
domain. In particular, all the problems in this class can be embedded into
an ordered, n-dimensional Euclidean space, and so can be construed as
greater-than games. Thus, conclusions derived using the numbers game
are more widely applicable than might be presumed. In light of this ob-
servation, we present a simple algorithm aimed at remedying focusing
problems and relativism in the numbers game. With it we show empir-
ically that, contrary to expectations, focusing in transitive games can
be more troublesome for co-evolutionary algorithms than intransitivity.
Practitioners should therefore be just as wary of focusing issues in ap-
plication domains.
1
Introduction
[1] discusses a minimal substrate which can be used to illustrate several issues
plaguing co-evolutionary dynamics. Individuals in this substrate are simply tu-
ples of numbers. Because of the simplicity of the individuals, we are able to see
more clearly what goes wrong when an algorithm fails to work as hoped. The
authors explored two variants of the numbers game, a transitive and an intransi-
tive one. The intransitive numbers game proved problematic for a conventional,
ﬁtness proportionate co-evolutionary algorithm. One issue which arose in the
numbers game experiments which is of particular interest to us is the problem
of overspecialization. Individuals had multiple dimensions on which they could
vary. It was observed that some individuals would focus on one dimension at the
expense of another. We will refer to this issue as the focusing problem. A detailed
discussion of this problem can be found in [2].
In this paper, we will show that a large class of co-evolutionary domains, even
intransitive ones, can be viewed as n-dimensional, transitive numbers games,
⋆Corresponding author
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 250–261, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Focusing versus Intransitivity
251
for some unknown dimension n. This observation raises an important question:
where have the intransitivities of the original domain gone? As we will see, the
mathematics of Pareto co-evolution [3],[4] turns intransitive cycles into sets of
non-dominated individuals [5]; what remains then are the transitive relations
among individuals.
Our main mathematical tool will be the partial order decomposition theo-
rem. This theorem states that any ﬁnite partially ordered set (poset) can be
decomposed into an intersection of n linear orders, for some number n called
the partial order dimension. This dimension n is meaningful geometrically: one
corollary of the theorem is that an n-dimensional poset can be embedded mono-
tonically into IRn. As shown in [5], co-evolution problems expressible with a
payoﬀfunction p : S × T →R (R preordered) can be regarded as preorderings
on the set S of candidate solutions. Modulo technicalities caused by preordering,
such co-evolution problems can therefore also be viewed as suborders of IRn. Or,
to put it simply: as n-dimensional transitive numbers games.
While this result is purely theoretical, our empirical results suggest that we
may reasonably approximate the mathematics in algorithms, at least for the
problems we have tried. In other words, a na¨ive implementation of Pareto co-
evolution can eﬀectively solve intransitive problems.
Surprisingly, we also ﬁnd that a co-evolutionary algorithm with a diversity
maintenance mechanism can tackle the intransitive numbers game, similarly call-
ing into question the importance of intransitivity in assessing co-evolutionary al-
gorithm failure. Indeed, with a transitive variant of the numbers game designed
to emphasize the focusing problem, co-evolution with diversity maintenance fails,
and the full power of Pareto co-evolution is required to solve the problem. The
conclusion we draw from these results is that focusing, rather than intransitivity,
is the more important domain feature to consider in some applications.
We will be considering co-evolution problems which can be expressed with
a payoﬀfunction of form p : S × T →R, where R is assumed to be partially
ordered. For simplicity we will assume S, T and R are ﬁnite sets; though this
assumption is not strictly necessary, it does simplify arguments.1 We make no
further assumptions about structure on R. This set might consist of numbers,
or it might contain symbolic values like lose and win.
This paper is organized as follows. In section 2, we set up and state the partial
order decomposition theorem. Making use of the mathematical notation and
terminology established in [5], we apply this theorem to co-evolution problems
and prove and important corollary, the preorder decomposition theorem, which
makes up the backbone of our claim that co-evolution problems have signiﬁcant
geometric aspects. In section 3 we describe our experiments.
1 What we really need for most results is that the induced preorder on S be ﬁnite-
dimensional, which is true of many inﬁnite preorders too.

252
A. Bucci and J.B. Pollack
2
Orders and Co-evolution
In this section we set up and state the poset decomposition theorem. Our pre-
sentation borrows from [6], which should be consulted for details and proofs.
We next prove a corollary which we call the preorder decomposition theorem.
Finally, we apply our results to co-evolution problems of form p : S × T →R,
showing that any such problem can be embedded into Euclidean n-space for some
unknown n. Finally, we work through an example to illustrate the concepts.
2.1
Poset Decomposition
We will write ≤for orders. We may subscript like ≤R to emphasize we are in
the poset R. Recall that a monotone function between two posets Q and R is
a function f : Q →R such that whenever q1 ≤Q q2, then f(q1) ≤R f(q2).
Such a function thus preserves order relations. Recall also that f is injective if,
whenever q1 ̸= q2 then f(q1) ̸= f(q2). f is an embedding of Q into R if it is both
monotone and injective. An embedding f essentially realizes Q as being part of
R.
In this section, assume all posets are ﬁnite. We begin with the notion of linear
extension:
Deﬁnition 1 (Linear Extension). A linear extension of R is a total ordering
L of the elements of R which is consistent with R’s order. In other words, if
S is the underlying set of both R and L, the identity function 1S : S →S is
monotone with respect to R in the domain and L in the range.
Example 2. Let R have elements {a, b, c} and relations a ≤c, b ≤c (i.e., a and b
are incomparable). Then one linear extension of R puts these elements in order
a ≤b ≤c; call it L1. R has a second linear extension L2 putting the elements in
order b ≤a ≤c. In light of this example, we have the following:
Deﬁnition 3 (Linear Realizer). A linear realizer of a (ﬁnite) poset R is a
set {L1, . . . , Ln} of linear extensions of R such that
n
i=1
Li = R. The intersection
means that the only comparisons in
n
i=1
Li are the ones which are in all the Li;
all other pairs of elements are incomparable.
Example 4. In example 2, L1 and L2 constitute a linear realizer {L1, L2} of R.
To see this, notice that a ≤c and b ≤c in both L1 and L2, whereas a ≤b in
L1 while b ≤a in L2. Thus, in L1
 L2, a ≤c and b ≤c whereas a and b are
incomparable. These relations are exactly the ones in R; hence, R = L1
 L2.
We have been leading up to the following fundamental fact about posets
which we state without proof.2
2 See [6] for details. The crux of the proof is to show that R has at least one ﬁnite
linear realizer.

Focusing versus Intransitivity
253
Theorem 5 (Poset Decomposition Theorem). Every ﬁnite poset has a
minimal realizer.
The “minimal” in the name “minimal realizer” means the linear realizer
contains a minimum number of linear extensions. This minimum, call it n, is the
dimension of R; alternately, R is called an n-dimensional poset. For instance,
the poset in examples 2 and 4 is two-dimensional. The justiﬁcation for using the
word “dimension” is via the following two lemmas:
Lemma 6. Every linear extension L of R gives rise to an embedding x : R →IN.
Proof. A linear extension of R is essentially a choice for putting the elements of
R into a list. If R = {s1, . . . , sm}, then L will be sσ(1) ≤sσ(2) ≤· · · ≤sσ(m),
σ being some permutation of 1, . . . , m. Let us reindex R by deﬁning ti = sσ(i).
Then, the mapping x : R →IN deﬁned by ti →i, is monotonic by construction.
It is also injective, since we only index distinct elements of R.
⊓⊔
Lemma 7 (Embedding Lemma). Every linear realizer {L1, . . . , Ln} of R
gives rise to an embedding φ : R →INn.
Proof. By lemma 6, each Li gives rise to an injective, monotone function xi :
R →IN. These functions deﬁne a sort of coordinate system for R. Deﬁne the
map φ : R →INn by s →(x1(s), . . . , xn(s)) for all s ∈R. Each coordinate xi of
φ is injective and monotone; thus φ itself is too.
⊓⊔
Remark 8. In particular, if R is an n-dimensional poset, it has a minimal realizer
{L1, . . . , Ln}. By lemma 7, there is thus an embedding φ : R →INn. INn embeds
into IRn, and so we see that φ can be regarded as embedding R into ordered, n-
dimensional Euclidean space. n is minimal in this case, so R cannot be embedded
into m-dimensional space for some smaller m. Thus the name “n-dimensional
poset.”
2.2
Applications to Co-evolution
To complete the picture, we need to see how to apply the results of the previous
section to co-evolution problems. First, let us recall some important deﬁnitions
and notation from [5]. For any function f : S×T →R, where S is a set and R is a
poset, write Sf for the preordering induced on S by pullback, and write the order
on Sf as ≤f. This deﬁnition means that s1 ≤f s2 exactly when f(s1) ≤R f(s2).3
Any function of form S × T →R can be curried to a function of form
S →[T →R], where [T →R] stands for the set of all functions from T to R. If
p : S × T →R, write λt.p for the corresponding curried function.
Consequently, starting from a co-evolution problem p : S × T →R, with
R a poset, there is a corresponding preorder structure on the set S, namely
3 We can interpret this deﬁnition in terms of ﬁtness functions: s1 ≤f s2 if s2’s ﬁtness
is at least as high as s1’s.

254
A. Bucci and J.B. Pollack
Sλt.p. The basic idea is that two candidate solutions s1 and s2 lie in the relation
s1 ≤λt.p s2 exactly when s2’s array of outcomes covers s1’s. In other words, s2
does at least as well as s1 does against every possible opponent. We refer the
reader to [5] for details and examples.
The poset decomposition theorem applies to partial orders, not preorders, so
we need to adjust it slightly. Our approach is to observe that every preorder R
comes with an equivalence relation deﬁned: s1 ∼s2 if and only if s1 ≤R s2 and
s2 ≤R s1. One way to think about this relation is in the context of an objective
function f : S →IR. s1 ∼s2 exactly when the individuals s1 and s2 have the
same ﬁtness. In a multi-objective context, s1 ∼s2 when s1 and s2 have the
same objective vector. Given this equivalence relation, we can then prove the
following:
Lemma 9. Let R be a (ﬁnite) preorder. There is a canonical partial order Q =
R/ ∼and a surjective, monotone function π : R →Q (called the projection)
such that π(s1) = π(s2) if and only if s1 ∼s2, for all s1, s2 ∈R. Q is called the
quotient of R.
Proof. The proof that Q is a well-deﬁned partial order can be found in [5]. Let
us show that π is surjective and monotone. Deﬁne π : R →Q by π(s) = [s] for
all s ∈R, where [s] is the equivalence class of s under ∼. π is surjective trivially,
since the only equivalence classes in Q are of form [s] for some s ∈R. To see π
is monotone, observe the order on Q is [s1] ≤Q [s2] if and only if s1 ≤R s2. An
equivalent way to state this deﬁnition is: π(s1) ≤Q π(s2) if and only if s1 ≤R s2
(this is just rewriting [si] as π(si)). The “if” part proves the monotonicity of
π.
⊓⊔
Lemma 9 permits us to adapt the poset decomposition theorem (theorem 5)
to preorders. If R is a preorder, form the quotient Q = R/ ∼, which will be a
partial order. Apply the embedding lemma (lemma 7) to yield an embedding
φ : Q →INn of Q in INn. Composing φ with the projection π gives a monotone
function φ ◦π : R →INn which we call a pseudo-embedding of R into INn. By
this we mean the following. Write ψ for φ ◦π. ψ is such that ψ(s1) = ψ(s2)
exactly when s1 ∼s2. Consequently, ψ behaves like an embedding, except that
it sends equivalent individuals in R to the same point in INn. Otherwise, it sends
non-equivalent elements in R to diﬀerent points in INn while preserving the order
relations between them. Let us record these observations as:
Theorem 10 (Preorder Decomposition Theorem). Every ﬁnite preorder
can be pseudo-embedded into INn (and thus into IRn). Moreover, every ﬁnite
preorder has a minimum n, the dimension of the preorder, for which such pseudo-
embedding is possible.
Let us examine an example to help visualize the deﬁnitions.
Example 11. Consider the following game:.

Focusing versus Intransitivity
255
p
rock stone paper scissors
rock
0
0
−1
1
stone
0
0
−1
1
paper
1
1
0
−1
scissors
−1
−1
1
0
This game is rock-paper-scissors with a clone of rock called “stone.” In this
case, S = T = {rock, stone, paper, scissors}, and R = {−1, 0, 1} with −1 ≤0 ≤
1.
By comparing the rows of this matrix, we can see that none of the strategies
dominates any of the others. Every strategy does well against at least one oppo-
nent; likewise, every strategy does poorly against at least one opponent. However,
rock ∼stone because their rows are identical. Consequently, the induced pre-
order on {rock, stone, paper, scissors} contains only the relations rock ≤stone
and stone ≤rock. We show this preorder in ﬁgure 1.
Fig. 1. The preorder rock-stone-paper-scissors displayed as a graph. An arrow between
two individuals indicates a ≤relationship; absence of an arrow indicates the individuals
are incomparable.
When we mod out the equivalence relation ∼, we arrive at a partial order
consisting of the three equivalence classes {[rock], [paper], [scissors]}. In this
partial order, all three elements are incomparable to one another. This is, in
fact, a 2-dimensional partial order, with linear realizer L =
{([rock], [paper], [scissors]), ([scissors], [paper], [rock])} Figure 2 shows a plot of
the corresponding pseudo-embedding.
2.3
Discussion
We began with a co-evolution problem which may have had intransitive cycles
and other pathologies. We ended up with what amounts to an embedding of the
problem into INn for some unknown dimension n. INn is a particularly simple
partial order; in particular, it is transitive. How did we turn a pathological
problem into a nice transitive one?
The ﬁrst point to note is that a pseudo-embedding, while well-deﬁned math-
ematically, is not easily computable. Indeed, it is has been known for some time
that even learning the dimension of a poset is NP-complete [7]. Furthermore,

256
A. Bucci and J.B. Pollack
Fig. 2. The preorder rock-stone-paper-scissors pseudo-embedded into the plane IN2
in order to compute a poset decomposition, we need to have all the elements of
the poset on hand. While in many problems it is possible to enumerate all solu-
tions,4 it is typically intractable to do so. Thus, while pseudo-embeddings exist
as mathematical objects, they are at least as expensive to compute as solving the
problem by brute force. This is not surprising; if we had a pseudo-embedding in
hand, we could treat our problem as a greater-than game and solve it relatively
easily. Pseudoembeddings thus encode a lot of information about the original
problem which we have to “pay for” somehow. In essence, what we have done is
reorganize the information in a problem.
A second point is that we can at least hope that the preorder decomposition
of a co-evolution problem can be approximated by a more practical algorithm.
In the next section, we will present a simple algorithm aimed at approximating
the preorder decomposition of a problem and show that it works reasonably well
on two instances of the numbers game.
3
Experiments
In this section we present our experimental results. We begin by recalling the
numbers game, from [1]. Next, we describe the algorithms we employ and our
implementation choices. Finally, we present and discuss results.
3.1
The Numbers Game
“The numbers game” [1] actually refers to a class of games. Common among
them is that the set of individuals, call it S, consists of n-tuples of natural
numbers. How we choose to compare individuals, and what choice we make for
n, deﬁne an instance of the game. In our experiments, we will be considering two
instances. In both, we will deviate somewhat from the score functions deﬁned
in [1]. Instead of returning a score, we will construct our functions to have form
p : S ×S →{0, 1}, where S = IN2 is the set of ordered pairs of natural numbers,
4 Exceptions include problems with real-valued parameters.

Focusing versus Intransitivity
257
and the function p simply says which individual is bigger (i.e., gets a bigger score
or “wins” the game).
In our experiments we only present data for 2-dimensional problems, since
the issues we wish to emphasize are already visible at this low dimensionality.
For simplicity of presentation, we deﬁne these games for 2 dimensions only.
The Intransitive Game [IG] In this game, we ﬁrst decide which dimensions of
the individuals are most closely matched, and then we decide which individual
is better on that dimension. The payoﬀfunction we use is:
pIG((i1, j1), (i2, j2)) =





1
if |i1 −i2| > |j1 −j2| and j1 > j2
1
if |j1 −j2| > |i1 −i2| and i1 > i2
0
otherwise
(1)
This game is intransitive; one cycle is (1, 6), (4, 5), (2, 4) [1]. (1, 6) and (4, 5) are
closest on the second dimension, so (1, 6) > (4, 5). (4, 5) and (2, 4) are closest on
the second dimension also, so (4, 5) > (2, 4). However, (2, 4) and (1, 6) are closest
on the ﬁrst dimension, meaning (2, 4) > (1, 6). Nevertheless, an individual with
high values on both dimensions will tend to beat more individuals than one
without, and it seems, intuitively, that the best solutions to this game are such
individuals.
The Focusing Game [FG] In this game, the ﬁrst and second individuals are
treated asymmetrically. The second individual is scanned to see on which dimen-
sion it is highest. Then, it is compared to the ﬁrst individual. The ﬁrst individual
is better if it is higher on the best dimension of the second individual. As a payoﬀ
function:
pF G((i1, j1), (i2, j2)) =





1
if i2 > j2 and i1 > i2
1
if j2 > i2 and j1 > j2
0
otherwise
(2)
Note that this game is transitive. However, the emphasis on one dimension at the
expense of others encourages individuals to race on one of the two and neglect
the second. Nevertheless, an individual which is high on both dimensions will
beat more individuals than one which is focused on a single dimension.
This game is closely related to the compare-on-one game described in [2].5
3.2
Algorithms and Setup
We will compare two algorithms, a population based co-evolutionary hillclimber
(P-CHC) and a population based Pareto hillclimber (P-PHC). The P-CHC has
5 But note that pF G has range {0, 1} and requires strict inequalities for a 1 output,
whereas compare-on-one has range {−1, 1} and does not require strict inequalities.

258
A. Bucci and J.B. Pollack
a single population. To assess the ﬁtness of an individual, we sum how it does
against all other individuals according to the game we are testing. The wrinkle
is that each individual produces exactly one oﬀspring, and the oﬀspring can
only replace its parent if it is strictly better. This algorithm uses a subjective
ﬁtness measure to assess individuals, but the constraint that an oﬀspring can
only replace its own parent is a simple form of diversity maintenance resembling
deterministic crowding [8].
Our population based Pareto hillclimbing algorithm is similar in spirit to the
DELPHI algorithm presented in [2]. Our P-PHC operates as follows. There are
two populations, candidates and tests. The candidates are assessed by playing
against all the tests. Rather than receiving a summed ﬁtness, they receive an
outcome vector, as in evolutionary multi-objective optimization [9]. The out-
come vectors are then compared using Pareto dominance: candidate a is better
than candidate b if a does at least as well as b does versus all tests, and does
better against at least one. The tests are assessed diﬀerently, using an approx-
imation of informativeness [10]. Since the outcome order is 0 < 1, the infor-
mativeness measure presented in that paper collapses to simply counting how
many pairs of candidates a test says are equal.6 In other words, each test has
a score f(t) = 
s1,s2∈Si δ(p(s1, t), p(s2, t)), where Si is the current population,
and δ(p(s1, t), p(s2, t)) returns 1 if p(s1, t) = p(s2, t), 0 otherwise. For our ex-
periments, p will be one of pIG, or pF G. As in the co-evolutionary hillclimber,
in the Pareto hillclimber individuals receive only one oﬀspring, and an oﬀspring
can only replace its parent. However, the climbing is done separately in the two
populations.7
In order to focus more closely on domain-speciﬁc problems, we do away with
a bitstring genotype. In our experiments, genotype=phenotype. Individuals are
simply pairs of numbers. To create a mutant, we add random noise to each coor-
dinate with some probability. Notice there is no mutation bias in any particular
direction.
We will be using a population size of 100 for all experiments. In the co-
evolutionary hillclimber, the single population will be 100 individuals; in the
Pareto hillclimber, there will be 50 candidates and 50 tests. The mutation rate
is 100%; mutation adds +1 or -1 to each dimension. No form of crossover is used.
We ran each simulation for 500 time steps.
3.3
Results
Figures 3 and 4 show performance versus evolutionary time for both P-CHC
and P-PHC on the payoﬀfunctions pIG and pF G. Note that the co-evolutionary
hillclimber out paces the Pareto hillclimber. The Pareto hillclimber must adjust
not only its candidates to make an improvement, but also its tests. Updating the
6 Strictly speaking, this statement is not true; however, two tests which give diﬀerent
counts of equal candidate pairs are incomparable; thus we use it as a heuristic.
7 We should remark that neither of these algorithms was intended to be practical;
rather, they are intended to test our ideas.

Focusing versus Intransitivity
259
tests causes a time lag which slows down progress. The graphs are intended to
be qualitative, however; what is important is that both algorithms make steady
progress.
Note ﬁgure 3, the intransitive game. Unlike the algorithm used in [1], P-CHC
made continuous progress on the intransitive game. Since P-CHC essentially adds
only a diversity maintenance mechanism, it seems the diversity is important to
the success or failure of co-evolution on this problem.
Fig. 3. Performance versus time of P-CHC and P-PHC on IG (intransitive game). Plot
shows a single, typical run. Performance is measured as the sum of the coordinates;
the plot shows this value for the best individual of the population at each time step.
At ﬁrst glance, the Pareto co-evolution mechanism of P-PHC does not seem to
be adding anything. To understand more completely what is happening, we plot
in ﬁgure 5 the ﬁnal candidates P-CHC found on a typical run on pF G, together
with the ﬁnal candidates and tests which P-PHC found on a typical run. Notice
how P-CHC has focused entirely on the horizontal dimension. While it made
great progress there, it neglected the vertical dimension entirely. By contrast,
P-PHC has maintained progress on both dimensions equally. While it did not
move as far as P-CHC, it did remain balanced. Most important are the tests
P-PHC found. In the plot, the tests appear to be “corralling” the candidates,
keeping them in a tight group near the main diagonal. An animation of a typical
run of P-PHC reveals this is indeed the case. The tests keep step behind the
candidates. The same conﬁguration of tests and candidates persists, but the
group of them move slowly up and to the right, towards the better values of
this game. Intuitively, we imagine the individuals found by P-CHC are brittle
specialists, whereas the individuals found by P-PHC are more robust generalists.

260
A. Bucci and J.B. Pollack
Fig. 4. Performance of P-CHC and P-PHC on FG (focusing game). Plot shows a
single, typical run. Performance is measured as the sum of the coordinates of the
best individual.
Fig. 5. Position in plane of ﬁnal candidates from P-CHC run on pF G (lower right),
together with ﬁnal candidates and tests from P-PHC (lower left). P-CHC has focused
on the horizontal dimension, whereas P-PHC has improved in both dimensions. The
tests which P-PHC found are arranged to “corral” the candidates along the main
diagonal.
4
Conclusion
To sum up, we have shown mathematically that a wide class of co-evolution
problems, if properly construed, can be looked upon as n-dimensional, transi-

Focusing versus Intransitivity
261
tive, greater-than games. The trouble is that discovering n is an NP-complete
problem in general, let alone the embedding which would permit us to convert
our favorite problem domain to a nicely-behaved transitive domain. Neverthe-
less, we feel the mathematical result changes the face of intransitivity. We exam-
ined intransitivity experimentally, using insights gained from our mathematics,
and saw that it may not be the demon it has been made out to be. Some al-
gorithms fail because they are overspecializing on some dimensions of a game
at the expense of others. While this observation is not new, the mathematical
derivation of it sheds new light on the interpretation of co-evolution and suggests
new algorithms which might overcome both intransitivity and overspecialization
diﬃculties.
References
1. Watson, R., Pollack, J.: Coevolutionary Dynamics in a Minimal Substrate. In
Spector, L., Goodman, E., Wu, A., Langdon, W., Voigt, H.M., Gen, M., Sen,
S., Dorigo, M., Pezeshk, S., Garzon, M., Burke, E., eds.: Proceedings of the Ge-
netic and Evolutionary Computation Conference, GECCO-2001, San Francisco,
CA, Morgan Kaufmann Publishers (2001)
2. de Jong, E., Pollack, J.B.: Ideal Evaluation from Coevolution. Evolutionary Com-
putation (to appear)
3. Ficici, S.G., Pollack, J.B.:
Pareto Optimality in Coevolutionary Learning.
In:
European Conference on Artiﬁcial Life. (2001) 316–325
4. Noble, J., Watson, R.A.: Pareto coevolution: Using performance against coevolved
opponents in a game as dimensions for Pareto selection. In Spector, L., Goodman,
E., Wu, A., Langdon, W., Voigt, H.M., Gen, M., Sen, S., Dorigo, M., Pezeshk, S.,
Garzon, M., Burke, E., eds.: Proceedings of the Genetic and Evolutionary Com-
putation Conference, GECCO-2001, San Francisco, CA, Morgan Kaufmann Pub-
lishers (2001) 493–500
5. Bucci, A., Pollack, J.B.: A Mathematical Framework for the Study of Coevolu-
tion. In: FOGA 2002: Foundations of Genetic Algorithms VII, San Francisco, CA,
Morgan Kaufmann Publishers (2002)
6. Scheinerman, E.R.: Mathematics: A Discrete Introduction. 1st edn. Brooks/Cole,
Paciﬁc Grove, CA (2000)
7. Yannakakis, M.: The Complexity of the Partial Order Dimension Problem. SIAM
Journal on Algebraic and Discrete Methods 3 (1982) 351–358
8. Mahfoud, S.W.: Crowding and Preselection Revisited. In M¨anner, R., Mander-
ick, B., eds.: Parallel Problem Solving from Nature 2, Amsterdam, North-Holland
(1992) 27–36
9. Fonseca, C.M., Fleming, P.J.: An Overview of Evolutionary Algorithms in Multi-
objective Optimization. Evolutionary Computation 3 (1995) 1–16
10. Bucci, A., Pollack, J.B.: Order-Theoretic Analysis of Coevolution Problems: Co-
evolutionary Statics. In Barry, A.M., ed.: GECCO 2002: Proceedings of the Bird
of a Feather Workshops, Genetic and Evolutionary Computation Conference, New
York, AAAI (2002) 229–235

Representation Development from
Pareto-Coevolution
Edwin D. de Jong
DSS Group, Utrecht University, The Netherlands
dejong@cs.uu.nl, http://www.cs.uu.nl/˜dejong/
Abstract. Genetic algorithms generally use a ﬁxed problem representa-
tion that maps variables of the search space to variables of the problem,
and operators of variation that are ﬁxed over time. This limits their
scalability on non-separable problems. To address this issue, methods
have been proposed that coevolve explicitly represented modules. An
open question is how modules in such coevolutionary setups should be
evaluated.
Recently,
Pareto-coevolution
has
provided
a
theoretical
basis
for
evaluation in coevolution. We deﬁne a notion of functional modularity,
and objectives for module evaluation based on Pareto-Coevolution. It
is shown that optimization of these objectives maximizes functional
modularity. The resulting evaluation method is developed into an algo-
rithm for variable length, open ended development of representations
called DevRep. DevRep successfully identiﬁes large partial solutions and
greatly outperforms ﬁxed length and variable length genetic algorithms
on several test problems, including the 1024-bit Hierarchical-XOR
problem.
Keywords: Development of representations, hierarchical modularity,
Pareto-coevolution, Evolutionary Multi-Objective Optimization
1
Introduction
Most genetic algorithms employ a single, ﬁxed representation that is given as
part of the problem speciﬁcation. To apply a genetic algorithm to a problem,
a mapping has to be chosen between genotypes (sequences of binary or other
variables) and the actual individuals they represent, phenotypes. We call this
mapping the representation of the problem. For most genetic algorithms, the
representation is chosen once, and does not change during the algorithm’s op-
eration. The operators of variation are typically constant too. Thus, the search
space and the operators specifying the possible moves in this space cannot be
adapted by the algorithm.
In combination, the use of a ﬁxed representation and ﬁxed operators of varia-
tion make it unlikely that diﬀerent combinations of large partial solutions will be
explored, see [14]. Partial solutions take the form of schemata, i.e. partial speci-
ﬁcations of a genotype. Both forming large partial solutions and mixing existing
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 262–273, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Representation Development from Pareto-Coevolution
263
genotypes can be achieved in isolation. However, the standard genetic algorithm
makes it unlikely for mutually exclusive partial solutions1 to persist, since no
mechanism for protecting mutually exclusive partial solutions is present. Meth-
ods that cannot represent information about mutually exclusive partial solutions
are unlikely to address non-separable problems2.
When niching is used, mutually exclusive partial solutions can persist. Still,
unless they are explicitly represented, crossover is unlikely to respect the bound-
aries of such partial solutions. This limits the potential to combine large partial
solutions. A solution is to represent partial solutions, modules, explicitly. Several
methods taking this approach have been investigated so far, including GLiB [1],
ADF’s [8], ARL [12], ADSN [7], and SEAM [20].
The development of modules allows algorithms to start searching in terms
of combinations of variables. Thus, such algorithms can be viewed as adapting
the representation of the problem during search [1,12,3]. H-BOA, an apparently
quite diﬀerent approach to address hierarchical problems, also represents partial
solutions explicitly, by using decision trees [10].
So far, there has been a lack of theory to guide the development of algo-
rithms forming modules, in particular regarding module evaluation. Methods
that simultaneously evolve modules and assemblies are instances of coevolution,
as module evaluation is based on interactions with assemblies. Recently, the
paradigm of Pareto-coevolution has provided a theoretical basis for evaluation
in coevolution [6,19,2,4].
Here, we will apply Pareto-coevolution to the question of how modules may
be evaluated. The ﬁrst algorithm to use Pareto-coevolution for module evaluation
is Watson’s SEAM algorithm [20]. SEAM is designed for ﬁxed length problems.
We study the question of how combinations of large partial solutions may be
explored for variable length problems in an open-ended setup where modules
can be combined into new modules recursively and indeﬁnitely. The problems
we are interested in are large search problems that have structure. A search
problem is large if it requires a long solution, in terms of the original variables.
If information from part of the search space can be used to predict (better than
random) information about other parts of the search space, we will say the
problem has structure.
The structure of the paper is as follows. First, our algorithm is gradually in-
troduced, by discussing structural and functional modularity (2), coevolution of
modules and assemblies and Pareto-coevolution (Section 3), evaluation of mod-
ules based on Pareto-coevolution (Section 4), and ﬁnally the algorithm following
from this principle (Section 5). The test problems are described in Section 6.
Experimental results are reported in Section 7, followed by discussion and con-
clusions.
1 Two partial solutions are mutually exclusive if they assign conﬂicting values to one
or more variables.
2 A problem is separable if each variable has a single optimal setting, independent of
the other variables [16].

264
E.D. de Jong
2
Structural versus Functional Modularity
We will distinguish between structural modularity, a characteristic of algorithms,
and functional modularity, the modularity present in a problem. Watson [17] de-
ﬁnes notions of structural and functional modularity based on the structure and
behavior of a dynamical system. Here, we will say any method that represents
partial solutions explicitly features structural modularity. While any grouping of
elements leads to structural modularity, the value of a modular representation
strongly depends on the particular grouping that is chosen. Ideally, the struc-
tural modules constructed by an algorithm should correspond to the functional
modules present in the problem.
The primitives of a problem are the basic elements that may occur in a geno-
type, typically numerical values, e.g. {0, 1}, or actions or operators. A module
is a sequence of primitives, and has a unique identiﬁer. We will assume that for
every primitive there is a module containing only that primitive. Thus, without
loss of generality, individuals can be viewed as sequences of modules. Candidate
solutions are sequences of modules, and are called assemblies.
The compact representation of an assembly describes the assembly in terms
of the modules of which it consists, using the modules’ unique ID’s. The assem-
bly’s expressed form consists of the concatenation of the sequences of primitives
represented by its modules. The sizes of the compact and expressed representa-
tions of an assembly are called its compact and expressed size. Since an assembly
is expressed by concatenating the expressed forms of its modules, the position
of a module, and hence of its primitives, is determined by the number and size
of the modules that precede it in the assembly.
The functional modularity of a module is considered with respect to some set
of assemblies, called the context set. We will use the operation of replacing the
module at a given position within an assembly by another module. A position
used in this way will be called an insertion point.
Using the above concepts, a deﬁnition for functional modularity in variable
length search problems can now be stated. Let S be a context set and let C be
a set of comparison modules.
Deﬁnition 1 (Functional Modularity). A module A is functionally modular
with respect to S, insertion points iS, and C iﬀ:
∀S ∈S : ∀C ∈C : f(S(A, iS)) ≥f(S(C, iS))
where S(A, i) speciﬁes placing module A at the ith position of S, and f(S)
returns the ﬁtness of an assembly S. Functionally modular modules are also
simply called functional modules.
3
Coevolution of Modules and Assemblies
We study how the development of functional modules can be achieved in a setup
where modules and assemblies coevolve. A coevolutionary approach to module

Representation Development from Pareto-Coevolution
265
formation is obtained by using two populations, one containing modules and one
containing assemblies. Assemblies are candidate solutions for the problem, and
hence their evaluation is given by the ﬁtness function of the problem.
In Evolutionary Multi-Objective Optimization (EMOO [13]), individuals are
evaluated on multiple objectives instead of a single ﬁtness function, and the
values of these objectives are treated separately; for an introduction, see e.g. [5].
The central idea in Pareto-coevolution is to view the outcomes of interactions
with other evolving individuals as objectives. Treating the outcomes of interac-
tions separately provides more speciﬁc information about individuals than a
single value such as the average outcome, permiting better-informed methods of
selection.
4
Module Evaluation: Assemblies Provide Objectives
This section shows how the Pareto-coevolution view leads to a principle for
module evaluation. It will be seen that optimization of the objectives corresponds
to the optimization of functional modularity. This provides a connection between
Pareto-coevolution and functional modularity.
4.1
Objectives from Pareto-Coevolution
Assemblies provide situations in which modules can perform useful roles, and
thereby implicitly deﬁne objectives. The maximal set of objectives that can be
considered for a module therefore consists of the union of the objectives deﬁned
by some set of assemblies, e.g. a subset of the coevolving assembly population.
For an individual assembly, the objective of a module in it is to contribute
to the assembly’s ﬁtness by performing a useful role in it. Assemblies deﬁne a
number of positions at which modules can perform a useful role. Thus, in an
assembly containing n modules, each of the n positions deﬁnes an objective. For
a module A, the value of the ith objective of assembly S is obtained by using i
as an insertion point, and considering the ﬁtness of the assembly resulting from
placing A at the insertion point: f(S(A, i)).
The numerical value of this objective equals the ﬁtness of the complete assem-
bly, and is not informative by itself. The logic behind this choice of objectives
becomes clear when comparing the values these objectives assign to diﬀerent
modules. Intuitively, a module A is more valuable than another module B for a
given assembly if using A instead of B has a positive eﬀect on the overall ﬁtness
of the assembly. This is precisely what is measured when the objective values of
two modules A and B are compared; A has a higher objective value than B for
position i of an assembly S if f(S(A, i)) > f(S(B, i)). This comparison turns
out positive for A if A, when replacing B at the ith position of S, results in a
higher overall ﬁtness for the assembly.
Using individual assemblies as objectives for the modules they contain allows
for the identiﬁcation of many diﬀerent specialized roles or tasks. A module can
in principle be valuable even if it is only used by a small number of assemblies,

266
E.D. de Jong
or if only some of the assemblies employing it have high ﬁtness, while an average
ﬁtness approach would not detect the value of such a module.
Evaluation by replacing a module with other modules and comparing the
overall ﬁtness is a form of diﬀerential ﬁtness comparison. This principle has been
used in various forms, e.g. Cooperative Coevolution [11], COIN’s Wonderful Life
Utility [15], and SEAM [20].
4.2
Correspondence between Pareto-Coevolution and Functional
Modularity
The previous subsection has shown how using Pareto-coevolution, assemblies
can provide objectives for modules. An important question is how the resulting
objectives relate to the earlier notion of functional modularity. We show that
there is a direct correspondence between these.
Let A be a candidate module, chosen from a set of all possible modules C,
and let S be a set of assemblies. Then A is functionally modular with respect to
S, insertion points iS, and C if and only if:
∀S ∈S : ∀C ∈C : f(S(A, iS)) ≥f(S(C, iS))
(1)
Now consider the objectives speciﬁed by the same assemblies S and insertion
points iS. As deﬁned in the previous section, these are given by f(S(A, iS)) for
all S ∈S. A maximizes these objectives simultaneously over C if and only if:
∀S ∈S : ∀C ∈C : f(S(A, iS)) ≥f(S(C, iS))
(2)
Equation 1 and 2 are identical. Thus, a module is functionally modular for a set
of assemblies and corresponding insertion points if and only if it maximizes the
objectives represented by these assemblies and insertion points.
4.3
Practical Issues in Module Evaluation
For a candidate module AB, we can replace all its occurrences in assemblies by a
compact representation of the candidate module X = AB. After doing so, each
occurrence of X in an assembly deﬁnes an objective (assembly and insertion
point) that is likely to be relevant in evaluating X. For eﬃciency reasons, we
limit module evaluation to these objectives, i.e. the objectives speciﬁed by the
positions where X actually occurs.
Candidate modules are identiﬁed by considering consecutive pairs of modules
that occur frequently in assemblies. One of the assemblies in which such a fre-
quent candidate module occurs is selected, and deﬁnes an objective value for the
candidate module. The candidate module is evaluated by comparing its value
for the objective to that of other possible candidate modules in a comparison
set C. It is only accepted as a new module if its value for the objective is equal
or greater than all alternatives in C, and strictly greater than some alternatives.

Representation Development from Pareto-Coevolution
267
This condition ensures that no better candidate is available, and that the can-
didate is an improvement over alternative combinations of modules. Thus, for
given C, S, and insertion point iS:
∀C ∈C : f(S(A, iS)) ≥f(S(C, iS))
∧
∃C ∈C : f(S(A, iS)) > f(S(C, iS))
A possible concern is to what extent evaluation based on a single objective is
suﬃcient. In contrast with most EMOO work, the aim here is to identify modules
maximizing performance in one or more objectives. Thus, we are only interested
in the extremes of the tradeoﬀfront. Since modules are added incrementally
rather than evolved, the objectives can at least be optimized independently.
To furthermore promote modules that maximize multiple objectives, candidate
modules are combinations that occur frequently in assemblies.
To avoid unnecessary material, a candidate module is compared to alterna-
tives of the same or smaller expressed length. An eﬃciency improvement can be
made by viewing the constituents A and B of a module AB as modules. We
thus compare a candidate module AB to all modules A* and *B, observing the
length requirement, and only accept it if it obtains at least as high ﬁtness as all
of these and higher ﬁtness than at least one of these. A ﬁnal requirement is that
its ﬁtness is at least as high as when either or both constituent modules are left
out; that is, the module is also compared to A, B, and [].
5
The DevRep Algorithm
DevRep()
1.
modules:=primitives;
2.
assemblies:=generate random sequences(modules);
3.
while(¬stop criterion)
4.
modules := create modules(assemblies, modules);
5.
for i=1:interval
6.
assemblies := evolve assemblies(assemblies, modules);
7.
end
8.
end
Basic cycle of the DevRep algorithm.
The choices that have been made regarding module construction and evalu-
ation lead to an algorithm that Develops a Representation for the problem as
part of the search, and is therefore called DevRep. This method is based on ear-
lier work presented in [3]. The population of modules is initialized to the set of
primitives. The assembly population is initialized to random sequences of these
modules of a given length. Next, the following loop is repeated until a stop cri-
terion: pairs of existing modules occurring consecutively in the assemblies are

268
E.D. de Jong
considered for consolidation into new modules, and a generation of evolving the
assemblies is performed.
Create modules does the following. Let AB be the pair of existing mod-
ules consecutively occurring most frequently in the assembly population. One
assembly in which AB occurs is selected randomly. Let us write this assembly
as XABY , where X and Y represent sequences of modules. We now consider all
assemblies XA·Y and X ·BY in which either A or B has been replaced by some
other module, whose expressed length does not exceed that of XABY . Then
the ﬁtness f(XABY ) must be at least as high as that of all of these modiﬁed
assemblies, and higher than that of at least one of the modiﬁed assemblies:
∀Z :
f(XABY ) ≥f(XAZY )
∧
f(XABY ) ≥f(XZBY )
∃Z :
f(XABY ) > f(XAZY )
∨
f(XABY ) > f(XZBY )
If this is the case, we furthermore require that the compact representation of AB
contains no unnecessary elements:
f(XABY ) > f(XAY )
∧
f(XABY ) > f(XBY )
∧
f(XABY ) > f(XY )
If these requirements are met, the new module is given a unique ID, and added
to the module population. Furthermore, all occurrences of AB in the current
assemblies are replaced by the new module, followed by a null module to maintain
the same assembly length.3 If not, the next max-modules-to-consider most
frequent pairs of modules are considered in order for consolidation until at most
max-modules-per-gen new modules are found.
Evolve assemblies is based on deterministic crowding [9]. The following cy-
cle is repeated a number of times equal to the assembly population size. Two
assemblies are selected randomly to function as parents. Oﬀspring are produced
by crossover with probability pcross, and by copying otherwise. The resulting
assemblies are mutated at each element with probability pmut. Mutation replaces
a module by a randomly selected element of the module population. Next, the
parents are paired up with the oﬀspring, such that the sum of the Hamming
distances between the compact representations of the parent-oﬀspring pairs is
minimized. Each oﬀspring replaces its matched parent if its ﬁtness is equal or
higher than that of its parent.
6
Test Problems
6.1
Hierarchical Test Problems
Several authors have recently studied the scalability of evolutionary algorithms
[14,20,10]. As part of this, diﬃcult hierarchical test problem were designed that
yet contain structure, such as Hierarchical IF-and-only-iF (H-IFF) [18]. Prelim-
inary experiments showed H-IFF is no longer diﬃcult when modules can be
3 If AB occurs more than once, all occurrences are replaced.

Representation Development from Pareto-Coevolution
269
Table 1. Target modules for the H-IFF and H-XOR problems. The target modules at
each level are composed of those at the previous level, and are each other’s inverse. By
using XOR instead of IFF, repetition of a single type of module is no longer suﬃcient
for solving the problem.
Level
H-IFF
H-XOR
4
3
2
1
0
A
B
0000000000000000 1111111111111111
00000000
11111111
0000
1111
00
11
0
1
A
B
0110100110010110 1001011001101001
01101001
10010110
0110
1001
01
10
0
1
repeated; 64-bit H-IFF was solved within a few generations, or a fraction of a
second. We therefore test performance on the analogous H-XOR problem [18],
which uses XOR instead of IF and only iF as its basic function, see Table 1. This
problem is much more diﬃcult for variable length methods due to its reduced
potential for exploiting repetitiveness.
         


         


         


         


         


         


         


      


      


      


      


      


      


      


      


      


      


      


      


      


      


      


      


      


         


      


      


      


      


      


      


      


      


        



      


      


      


      


        



         


         


         


         


         


         


            



         


      


      


      


      


      


      


         


      


         


        



      


        



      


      


      


      


        



      


      


      


      


      


      


         


      


         


         


      


         


      


      


      


      


        



         


        



         


        



      


      


      


      


         


      


      


      


      


      


      


      


      


      


      


      


         


      


      


      


      


      


      


      


      


      


      


      


      


      


         


      


      


      


      


      


      


      


      


      


      


      


         


      


      


      


      


      


      


      


      


      


      


        



        



        



            



        



        



        



        



        



        



        



        



      


      


      


         


      


      


      


      


      


      


      


      


      


      


        



      


      


            



        



        



      


        



        



      


        



        



        



        



        



            



        



        



        



        



        



        



        



        



Fig. 1. Target images used in the experiments: squares, octagon, and path.
6.2
Pattern Generation
In the second test problem, the goal is to generate a picture using turtle graphics
on a toroidal grid. The primitives for this problem are the following commands:
turn left, turn right, move, and put pixel. The expressed form of an
assembly is a sequence of these primitives. The interpretation of a sequence of
primitives produces a bitmap, representing the concept speciﬁed by the assembly.
The starting point for the interpretation of a sequence is the point from which
the ﬁgure is drawn; thus, locating the target ﬁgure on the grid is not part of the
task.
The target images are 16x16 bitmaps containing simple line drawings, see
Figure 1. The two objectives are the number of black and white pixels correctly
produced. Perfect solutions for these four problems require between 70 and 160
primitive operators.

270
E.D. de Jong
0
2
4
6
8
10
x 10
7
0
2000
4000
6000
8000
10000
11264
Computational expense (bits evaluated) 
Best fitness
The 1024−bit HXOR problem
Theoretical maximum
DevRep             
GA, pop size 100   
GA, pop size 500   
0
1
2
3
4
5
x 10
7
0
0.04
0.08
0.12
0.16
Computational expense (bits evaluated) 
Best Error
Squares problem
GA, length 200     
GA length 100      
GA, variable length
DevRep             
Fig. 2. Performance on the 1024-bit H-XOR problem (left) and the squares (right)
problem.
0
1
2
3
4
5
x 10
7
0
0.1
0.2
0.3
0.4
Computational expense (bits evaluated) 
Best Error
Path problem
GA, length 200     
GA length 100      
GA, variable length
DevRep             
0
1
2
3
4
5
x 10
7
0
0.02
0.04
0.06
0.08
0.1
Computational expense (bits evaluated) 
Best Error
Hexagon problem
GA, length 200     
GA length 100      
GA, variable length
DevRep             
Fig. 3. Performance on the path (left) and octagon (right) problems.
7
Experimental Results
Here, we report experiments with the DevRep algorithm. Assemblies are of
length 2 (H-XOR) or 10 (pattern generation). Other parameters are as follows:
max-modules-to-consider = 5, max-modules-per-gen = 2, interval = 50,
pcross = .9, pmut = .1. All curves are averaged over ten runs.
A genetic algorithm variant of DevRep is obtained simply by omitting the
module formation procedure. Furthermore, while in DevRep a child replaces
its parent when all of its objective values are equal or higher, for the genetic
algorithm we employ the standard Pareto-dominance criterion. We employ two
genetic algorithm methods using ﬁxed length representations of 100 and 200
primitives, and a variable length method, initialized with size 10 assemblies.
The experimental results are as follows. On the 64-bit H-XOR problem (not
shown), all algorithms progressed substantially, while only DevRep reached the
maximum score within the given number of bit evaluations and for all runs. On
the 1024-bit version of the same problem, see ﬁgure 2, the diﬀerence in scalability

Representation Development from Pareto-Coevolution
271
between the methods becomes clear; while the genetic algorithm variants all stall
at a low ﬁtness level, DevRep is able to progress by repeatedly forming larger
modules, and subsequently searching in terms of these modules. DevRep thereby
again achieves maximum performance on the problem for all runs. Inspection of
a run showed that the modules formed over time correspond precisely to the
target modules for the problem, shown in table 1.
On the pattern generation tasks, ﬁg. 2-3, all genetic algorithm variants per-
formed poorly. Apparently, the biases of the genetic algorithm do not correspond
well to those required for these problems, which are characterized by long range
dependencies and by sequential structure. DevRep greatly improves over this per-
formance. The average score substantially improved over the genetic algorithm
methods, and while the GA methods were unable to ﬁnd a correct solution for
any of the problems, DevRep found perfect solutions for all problems, and in all
runs except three of the ’path’ problem runs.
8
Discussion
The DevRep algorithm shares several features with SEAM [20], most importantly
the use of recursive module formation, leading to hierarchy, and the use of Pareto-
coevolution for module evaluation; the latter distinguishes these algorithms from
other methods for representation development. Compared to SEAM, the main
new contribution is the application of this evaluation principle in a variable
length setting. A related diﬀerence is the use of assemblies evolved on ﬁtness,
rather than random assemblies; in additional experiments, ﬁtness based selection
was found to be a necessary component.
A crucial idea in deﬁning modularity for variable length problems was to
consider modularity relative to speciﬁc subsets of all possible assemblies. This
possibility is required to address non-separable problems such as H-IFF and
H-XOR.
Search algorithms can be characterized by the types of patterns they are able
to discover. For DevRep, these include the following:
– Functional modularity. By maximizing the module objectives, the al-
gorithm searches for modules that are functionally modular, as deﬁned in
section 2.
– Hierarchical modularity. By looking for useful modules recursively, the
algorithm searches for modules of hierarchical structure.
– Repetitive modularity. Modules can be used repeatedly, i.e. a combina-
tion of two consecutive primitives or operators can be used multiple times
within a single individual, due to the use of position-independent coding.
9
Conclusions
Problems that require long solutions pose diﬃculties to standard genetic algo-
rithms, due to the size of the associated search spaces. Still, if such problems

272
E.D. de Jong
have structure, they can in principle be addressed. While several authors have
investigated the simultaneous development of modules and assemblies, princi-
pled evaluation of such partial solutions has long been an open issue. Here, we
have derived objectives for module evaluation in variable length problems from
Pareto-coevolution.
Functional modularity is deﬁned, and it is shown that optimization of the
objectives derived from Pareto-coevolution corresponds to optimization of func-
tional modularity. Based on this evaluation principle, the DevRep algorithm for
variable length problems is developed.
DevRep was tested on Hierarchical XOR (H-XOR) up to size 1024, and on
pattern generation tasks. It was found to develop large and ideal partial solutions
and greatly improve performance compared to a genetic algorithm approach.
DevRep is able to exploit structure in certain large search problems, in particular
functional modularity, hierarchical modularity, and repetitive modularity. This is
achieved by recursively forming modules and searching the space of combinations
of such modules, thus forming modules in an open ended way.
We conclude that certain forms of structure in large search problems can
be exploited by gradually consolidating learned information. Here the patterns
that are detected are templates, but in principle any type of detectable pattern
can be considered. According to this view, a challenge for research into large
search problems is to identify the patterns present in problems of interest, and
to develop corresponding algorithms exploiting those patterns.
Acknowledgements. The author wishes to thank the Agents Systems Group
at the Vrije Universiteit and the members of the DEMO Lab at Brandeis, par-
ticularly Sevan Ficici, Richard Watson, and Anthony Bucci, who gave valuable
feedback on this work. A talent-fellowship from the Netherlands Organisation
for Scientiﬁc Research (nwo) is gratefully acknowledged.
References
1. Peter J. Angeline and Jordan B. Pollack. Coevolving high-level representations.
In Christopher G. Langton, editor, Artiﬁcial Life III, volume XVII of SFI Studies
in the Sciences of Complexity, pages 55–71, Redwood City, CA, 1994. Addison-
Wesley.
2. Anthony Bucci and Jordan B. Pollack.
Order-theoretic analysis of coevolution
problems: Coevolutionary statics. In Proceedings of the GECCO-2002 Workshop
on Coevolution: Understanding Coevolution, 2002.
3. Edwin D. De Jong and Tim Oates. A coevolutionary approach to representation
development. In E.D. de Jong and T. Oates, editors, Proceedings of the ICML-
2002 Workshop on Development of Representations, Sydney NSW 2052, 2002. The
University of New South Wales. Online proceedings:
http://www.demo.cs.brandeis.edu/icml02ws.
4. Edwin D. De Jong and Jordan B. Pollack. Learning the ideal evaluation function.
In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO-
2003, 2003.

Representation Development from Pareto-Coevolution
273
5. Kalyanmoy Deb.
Multi-Objective Optimization Using Evolutionary Algorithms.
Wiley & Sons, New York, NY, 2001.
6. Sevan G. Ficici and Jordan B. Pollack. Pareto optimality in coevolutionary learn-
ing. In Jozef Kelemen, editor, Sixth European Conference on Artiﬁcial Life, Berlin,
2001. Springer.
7. Frederic Gruau. Neural Network Synthesis Using Cellular Encoding and the Ge-
netic Algorithm.
PhD thesis, PhD Thesis, Ecole Normale Sup´erieure de Lyon,
1994.
8. John R. Koza. Genetic Programming II: Automatic Discovery of Reusable Pro-
grams. The MIT Press, Cambridge, MA, May 1994.
9. Samir W. Mahfoud. Niching Methods for Genetic Algorithms. PhD thesis, Uni-
versity of Illinois at Urbana-Champaign, Urbana, IL, May 1995. IlliGAL Report
95001.
10. Martin Pelikan and David E. Goldberg. Escaping hierarchical traps with competent
genetic algorithms. In L. Spector, E.D. Goodman, A. Wu, W. B. Langdon, H.-M.
Voigt, M. Gen, S. Sen, M. Dorigo, S. Pezeshk, M. H. Garzon, and E. Burke, editors,
Proceedings of the Genetic and Evolutionary Computation Conference, GECCO-
2001, pages 511–518, San Francisco, CA, 2001. Morgan Kaufmann.
11. Mitchell A. Potter and Kenneth A. De Jong. Cooperative coevolution: An architec-
ture for evolving coadapted subcomponents. Evolutionary Computation, 8(1):1–29,
2000.
12. Justinian P. Rosca and Dana H. Ballard.
Discovery of subroutines in genetic
programming. In P.J. Angeline and K. E. Kinnear, Jr., editors, Advances in Genetic
Programming 2, chapter 9, pages 177–202. The MIT Press, Cambridge, MA, 1996.
13. J. David Schaﬀer. Multiple objective optimization with vector evaluated genetic
algorithms. In John J. Grefenstette, editor, Proceedings of the First International
Conference on Genetic Algorithms and their Applications, pages 93–100, Hillsdale,
NJ, 1985. Lawrence Erlbaum Associates.
14. Dirk Thierens. Scalability problems of simple genetic algorithms. Evolutionary
Computation, 7(4):331–352, 1999.
15. Kagan Tumer and David Wolpert. Collective intelligence and Braess’ paradox. In
Proceedings of the 7th Conference on Artiﬁcial Intelligence (AAAI-00) and of the
12th Conference on Innovative Applications of Artiﬁcial Intelligence (IAAI-00),
pages 104–109, Menlo Park, CA, 2000. AAAI Press.
16. Richard A. Watson. Compositional Evolution: Interdisciplinary Investigations in
Evolvability, Modularity, and Symbiosis. PhD thesis, Brandeis University, 2002.
17. Richard A. Watson. Modular interdependency in complex dynamical systems. In
Bilotta et al., editor, Workshop Proceedings of the 8th International Conference on
the Simulation and Synthesis of Living Systems. UNSW Australia, 2003.
18. Richard A. Watson, Gregory S. Hornby, and Jordan B. Pollack. Modeling building-
block interdependency. In A.E. Eiben, Th. B¨ack, M. Schoenauer, and H.-P. Schwe-
fel, editors, Parallel Problem Solving from Nature, PPSN-V., volume 1498 of LNCS,
pages 97–106, Berlin, 1998. Springer.
19. Richard A. Watson and Jordan B. Pollack. Symbiotic combination as an alter-
native to sexual recombination in genetic algorithms. In M. Schoenauer, K. Deb,
G. Rudolph, X. Yao, E. Lutton, J. Julian Merelo, and H.-P. Schwefel, editors, Par-
allel Problem Solving from Nature, PPSN-VI, volume 1917 of LNCS, Berlin, 2000.
Springer.
20. Richard A. Watson and Jordan B. Pollack. A computational model of symbiotic
composition in evolutionary transitions. Biosystems, 69(2-3):187–209, May 2003.
Special Issue on Evolvability, ed. Nehaniv.

Learning the Ideal Evaluation Function
Edwin D. de Jong⋆and Jordan B. Pollack
DEMO Lab, Volen National Center for Complex Systems, Brandeis University
MS018, 415 South street, Waltham MA 02454-9110, USA
{edwin|pollack}@cs.brandeis.edu, http://demo.cs.brandeis.edu
Abstract. Designing an adequate ﬁtness function requires substantial
knowledge of a problem and of features that indicate progress towards
a solution. Coevolution takes the human out of the loop by dynamically
constructing the evaluation function based on interactions between
evolving individuals. A question is to what extent such automatic
evaluation can be adequate. We deﬁne the notion of an ideal evaluation
function. It is shown that coevolution can in principle achieve ideal eval-
uation. Moreover, progress towards ideal evaluation can be measured.
This observation leads to an algorithm for coevolution. The algorithm
makes stable progress on several challenging abstract test problems.
Keywords: Coevolution, Pareto-Coevolution, Complete Evaluation
Set, ideal evaluation, underlying objectives, Pareto-hillclimber, over-
specialization
Designing an adequate ﬁtness function requires substantial domain knowledge
and can be a critical factor in evolution, see e.g. [9]. Often though, tests reveal-
ing information about the qualities of individuals can readily be performed. In
chess for example, absolute evaluation of strategies is extremely diﬃcult, while
comparing individuals only requires knowledge of the rules of the game. If indi-
viduals can be evaluated based on tests, coevolution can be used to circumvent
the problem of deﬁning a ﬁtness function.
Coevolution has already produced a number of promising results [10,19,12,
17]. However, there are various ways in which evaluation in coevolution can
become inaccurate [21,2,16]. As a step towards accurate evaluation, Juill´e deﬁnes
a domain-speciﬁc ideal trainer [11]. Rosin provides an automatic mechanism for
accurate evaluation, but the approach is based on a single-objective perspective,
and likely to stall for problems with multiple underlying objectives. Pareto-
coevolution [6,20] uses the outcomes of a learner against coevolving evaluators
(tests) as objectives in the sense of Evolutionary Multi-Objective Optimization.
By combining Rosin’s complete set of tests with Ficici’s important notion
of distinctions [7], we arrive at the concept of a Complete Evaluation Set. The
complete evaluation set was ﬁrst described in [3], and detects all diﬀerences
between learners relevant to selection.
⋆Current address: DSS Group, Utrecht University. dejong@cs.uu.nl
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 274–285, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Learning the Ideal Evaluation Function
275
We prove that given a complete evaluation set as evaluators, Pareto-
coevolution leads to ideal evaluation, i.e. evaluation according to all underlying
objectives of a problem. Using order theory, Bucci has deﬁned a set of maximally
informative evaluators [1]. While this set also makes all distinctions necessary
for learner selection, it is diﬀerent, as the complete evaluation set is a maximally
informative set of evaluators. By virtue of this, the complete evaluation set has
the property that its required size is bounded and small.
The complete evaluation set provides a practical way for coevolution meth-
ods to approximate ideal evaluation. An algorithm based on this principle is
described, and found to achieve stable progress on a number of test problems
that could not be addressed by standard coevolution methods used for compar-
ison. This paper summarizes the results described in our technical report [3]. A
more extensive account of this work is to appear in [4].
1
Evaluation in Coevolution
We consider problems where multiple objectives may underly performance. This
includes as a special case single ﬁtness value problems. The theoretical ideal eval-
uation function speciﬁes which individuals would be preferred over which other
individuals if the underlying objectives would be available. We demonstrate that
using the outcomes of interactions between coevolving individuals as objectives,
it is possible to construct an evaluation function that is precisely equivalent to
the ideal evaluation function.
1.1
An Ideal Evaluation Function
The problem of evaluating individuals according to multiple objectives is studied
in Evolutionary Multi-Objective Optimization (EMOO), see e.g. [8,5]. We follow
EMOO in using the Pareto-dominance relation to compare individuals:
Deﬁnition 1 (Pareto-dominance). An individual a dominates another indi-
vidual b with respect to a set of objectives O if:
dom
O (a, b)
⇐⇒
∀i : O(a, i) ≥O(b, i) ∧∃i : O(a, i) > O(b, i)
(1)
where O(x, i) returns the value of the ith objective of x, 1 ≤i ≤n, and n is the
number of objectives contained in O.
To obtain an evaluation function Fideal that determines for any pair of individuals
a and b whether a is to be preferred over b, we can directly employ the Pareto-
dominance relation based on the (unknown) underlying objectives U:
Fideal(a, b) = dom
U (a, b)
(2)
U(x, i) = xi
(3)
In general, the solution to a multi-objective problem is a tradeoﬀfront of in-
dividuals that achieve the diﬀerent objectives to diﬀerent degrees. If a single
optimum exists, as in problems with scalar ﬁtness functions, this individual is
also the solution of the corresponding EMOO problem.

276
E.D. de Jong and J.B. Pollack
1.2
Coevolution: Interactions as a Basis for Evaluation
The diﬃculty of evaluation in coevolution is that selection does not have access
to the ideal evaluation function. Instead, selection decisions must be based on
the outcomes of interactions between individuals. We will demonstrate that these
interactions can provide suﬃcient information for ideal evaluation.
We distinguish between learners, and evaluators. Learners are to address the
problem at hand. The aim of the evaluators is to distinguish between learners.
The set of all possible learners is denoted as L, and the set of all possible eval-
uators as E. Particular sets of learners and evaluators are denoted as L and
E.
All interactions are assumed to be pairwise. An interaction is a function G :
LxE →O that accepts a learner and an evaluator. It returns an outcome for the
learner from some ordered set of values O, e.g. real numbers or game outcomes.
An interaction G(a, e) may be thought of as a two-player game between a and
e, or as a test or test-case that e poses to a. The interaction between a and e
reveals some information about a’s underlying objectives, while it is unknown
what this information is, or what the underlying objectives are.
Clearly, in order for the interaction function G to be useful in evaluating in-
dividuals, it must bear some relation to the underlying objectives that determine
the quality of individuals. Speciﬁcally, we require that any increase in an under-
lying objective of an individual a must be reﬂected in an increased outcome of
its interaction with some player b. Conversely, the information contained in G
should not provide misleading information by indicating an improvement when
there is none.
Formally, the interaction requirement speciﬁes that for any pair of learners
a, b ∈L:
∃i : ai > bi
⇐⇒
∃e ∈E : G(a, e) > G(b, e)
(4)
Each learner is evaluated based on its outcomes against the current set of
evaluators. Following Pareto-coevolution [6,20], these outcomes are treated as
objectives. This results in the following evaluation function Fcoev for learners:
Fcoev = dom
OE
G
(a, b)
(5)
where a, b ∈L are learners, and the kth objective of a learner Li ∈L is the
outcome of its interaction G with the kth evaluator Ek ∈E:
OE
G(Li, k) = G(Li, Ek)
(6)
2
Principled Evaluation in Coevolution
An evaluator e ∈E distinguishes between two learners a, b ∈L if a’s outcome
against e is higher than b’s outcome:
dist(e, a, b) ⇐⇒G(a, e) > G(b, e)
(7)

Learning the Ideal Evaluation Function
277
We deﬁne a Complete Evaluation Set to be a set of evaluators E that make all
distinctions that can be made between the learners in L:
Deﬁnition 2 (Complete Evaluation Set). An evaluation set E ⊆E is com-
plete for an interaction function G and a set of learners L if and only if:
∀a, b ∈L : [∃e ∈E : G(a, e) > G(b, e) =⇒∃e′ ∈E : G(a, e′) > G(b, e′)]
(8)
We will write E∗
L to denote an evaluation set that satisﬁes this property for a set
of learners L. The theoretical result of this paper is that the use of a complete
evaluation set E∗
L as objectives for a set of learners L renders the coevolutionary
evaluation function equivalent to the ideal evaluation function:
Theorem 1 (Equivalence with the ideal evaluation function). Let
Fcoev(a, b) = dom
O
E∗
L
G
(a, b) be a coevolutionary evaluation function for L based on
a complete evaluation set E∗
L. Let Fideal(a, b) = dom
U (a, b) be the ideal evalu-
ation function for L, based on the underlying objectives U. Furthermore, let
G satisfy the interaction requirement for U. Then for any pair of learners
a, b ∈L : Fcoev(a, b) = Fideal(a, b) .
A proof is given in appendix A. The ﬁnding implies that by treating the outcomes
of learners against evaluators as objectives, ideal evaluation can in principle be
achieved. Thus, it may be seen as a motivation for Pareto-Coevolution.
2.1
Approximating the Complete Evaluation Set
We now consider how algorithms may approximate the complete evaluation set.
This is surprisingly tractable, since the number of potential distinctions is the
square of the number of learners. Thus, we can treat all potential distinctions
between learners as objectives, resulting in a setup where evaluators strive to
ﬁnd all possible distinctions between learners:
O(Ek, nl · i + j) =

1
if
G(Li, Ek) > G(Lj, Ek)
0
otherwise
(9)
where O(Ek, n) is the nth objective of an evaluator Ek ∈E, Li is a learner,
nl = |L| is the number of learners and G(l, e) is the interaction function accept-
ing a learner and an evaluator. A convenient representation of the objectives of
evaluators is as the entries in a square matrix, where the columns and rows rep-
resent the learners, and each entry represents a distinction between two learners,
see ﬁgure 1 and eq. 7.
3
An Algorithm for Pareto-Coevolution
The above idea can be translated into an outline for algorithms by combining
a current population of learners and a set of oﬀspring into a single set of learn-
ers. To obtain an evaluation set for this set of learners, we invoke a secondary
evolutionary process. This leads to an outline for algorithms, see ﬁgure 2.

278
E.D. de Jong and J.B. Pollack
L1       0
1
0
L2
0
0
1
L3
1
1
0
E1
E2
E3
G(Li,Ek)
G(L2,E3)>G(L3,E3)
Resulting distinctions
dist(Li,Lj)
0
1
0
1
0
1
1
1
0
L1
L2
L3
L1
L2
L3
Interaction outcomes
Fig. 1. Matrix representation of the possible distinctions that can be made between a
set of learners (example). A distinction between learners Li and Lj can be made (1) if
an evaluator Ek exists such that the outcome of Li against Ek exceeds that of Lj.
Convergence to ideal evaluation can be guaranteed in the limit by generating
every possible evaluator with non-zero probability, and collecting any evaluator
making a new distinction; for n learners, this leads to a set of at most n2 evalua-
tors. In practice, we iterate the inner loop for a single step only, so as to balance
the computational eﬀort spent on evolving learners and evaluators.
Concerning learner selection, preliminary experiments led to the ﬁnding that
non-dominance is not strict enough as a selection criterion for learners and can
result in regress. Therefore, a learner may replace an existing individual only if
it dominates that individual. This simple technique is suﬃcient when a global
optimum exists. For an algorithm also striving towards a balanced distribution
of individuals over the tradeoﬀfront, see [14].
1.
Lpop:=random population()
2.
Epop:=random population()
3.
while ¬ performance-criterion
4.
Ltot := Lpop ∪generate(Lpop)
5.
while ¬ distinctions-criterion
6.
Etot := Epop ∪generate(Epop)
7.
∀i, k : G[i, k] := G(Li, Ek)
8.
∀k, i, j : d[k, i, j] := (G[i, k] >G[j, k])
9.
evaluate(Etot,d)
10.
Epop := select (Etot)
11.
end
12.
evaluate(Ltot,G)
13.
Lpop := select (Ltot)
14.
end
Fig. 2. Outline for coevolution algorithms that approximate the ideal evaluation func-
tion.

Learning the Ideal Evaluation Function
279
The strict selection consideration also applies to evaluator selection. In addi-
tion, diverse evaluators must be maintained, representing all underlying objec-
tives. Therefore, an evaluator will be replaced by its oﬀspring only, and only if
this oﬀspring dominates it. This is similar to the deterministic crowding method
for diversity maintenance, see [15]. We call such individuals Pareto-hillclimbers;
the PAES algorithm [13] is another example of a Pareto-hillclimber.
We have arrived at a setup where, given a population of learners L and a
population of evaluators E, new learners are evaluated based on the evaluators
in E and can replace any learner they dominate, while evaluators are Pareto-
hillclimbers that use the distinctions between the learners in L as their objec-
tives. This method will be called delphi, which stands for Dominance-based
Evaluation of Learners on Pareto-Hillclimbing Individuals.
4
Test Problems and Experimental Setup
We will now investigate the algorithm derived from the ideal evaluation principle
in experiments. The test problems employed are variants of the Numbers Game
[21]. Individuals are vectors of real valued variables. The underlying objectives
for the problems correspond precisely to these variables. Hence, the aim should
be to maximize each of the individual’s variables. However, as we aim to study
coevolution, the selection mechanism may not use knowledge of the underlying
objectives, but is based on the outcomes of interactions between individuals. The
diﬃculty of the task is determined among other factors by the information the
interaction function G provides about the underlying objectives of an individual.
The purpose of the test problems is to test to what extent coevolution algo-
rithms are able to provide accurate evaluation, i.e. evaluation according to all
underlying objectives. To this end, the problems should make accurate evalua-
tion diﬃcult. This is achieved by making it likely for evaluators to represent only
a subset of the dimensions or objectives in the problem. When this occurs, learn-
ers can only progress on a subset of the underlying objectives, a phenomenon
called over-specialization or focusing [21]. In this case the minimum value of
learners will not increase further. By using the minimum value of individuals
as a performance measure, we can detect whether progress is being made on all
underlying objectives.
The ﬁrst test-problem is called compare-on-all. In this problem, the
learner and the evaluator are compared based on all of the evaluator’s dimen-
sions. The outcome of the interaction function for this problem is positive (1) if
and only if the learner’s values are all at least as high as those of its evaluator:
Gall(a, e) =

1
if
∀i : ai ≥ei
−1
otherwise
(10)
where a is a learner, e is an evaluator, and xi denotes the value of individual x
in dimension i. In the compare-on-one problem, the learner and the evalua-
tor are compared based on only one of the evaluator’s dimensions, namely the

280
E.D. de Jong and J.B. Pollack
                                                       




      

Learner
Evaluators
The compare-on-all game
                                             








                                                                                                                                                                                              









      

Learner
Evaluators
                                                                                                                                                                                                                                 














The compare-on-one game
Fig. 3. The grey areas show all evaluators that are solved by the learner in the ﬁgure.
Left: the compare-on-all game. A learner receives a positive outcome if it is equal
or greater than the evaluator in every dimension. Right: the compare-on-one game.
A learner receives a positive outcome if it is equal or greater in the evaluator’s highest
dimension.
evaluator’s dimension with the highest value. The games are illustrated in ﬁgure
3.
m = arg max
i
ei
(11)
Gone(a, e) =

1
if
am ≥em
−1
otherwise
(12)
While evaluators in the compare-on-all game can compare learners based
on all of their dimensions, this is not possible in the compare-on-one game.
Therefore, evaluators in diﬀerent regions of the space must be maintained. This
results in a strong risk of maintaining evaluators for only some of the underlying
objectives, as desired.
5
Experimental Results
The setup is as follows. Initial values in each dimension are chosen uniformly from
[0, 0.05]. A new generation of individuals is created using mutation. Mutation
adds a value chosen uniformly from [−d −b, d −b] to a dimension i, where
d = 0.1 is the mutation distance and b = 0.05 (where used) is the mutation
bias. Mutation is applied to two randomly chosen dimensions. Thus, an increase
in one dimension will often be accompanied by a decrease in another, and an
improved interaction outcome does not imply improvement on all objectives. The
size of learner and evaluator populations and of new generations is 50, resulting
in learner and evaluator sets of size 100. All experiments (except the trajectory
graph) are averaged over 100 runs.
We performed experiments with the compare-on-all and compare-on-
one game in 2-dimensional and 5-dimensional form, with and without mutation
bias. Due to space limits, we present results for the easiest and most diﬃcult
variants in the problem set: 2-dimensional compare-on-all without mutation

Learning the Ideal Evaluation Function
281
0
250
500
750
1000
0
5
10
15
Generations
Minimum value
DELPHI
avg E, avg L
dom E,dom L
P−HC E, P−HC L
half E, half L
prob E, prob L
0
250
500
750
1000
0
0.5
1
Generations
Minimum value
DELPHI
P−HC E, P−HC dist L
dom E, spread dist L
dom E,dom dist L
Fig. 4. Left: Performance of delphi and a number of competitive methods on the
2-dimensional compare-on-all problem. All methods achieve some progress on this
problem. Right: delphi and comparison methods on 5-dimensional compare-on-one
with mutation bias. Only the methods employing Pareto-Hillclimbing still achieve sus-
tained progress; the other methods overspecialize, and neglect one or more objectives.
bias and 5-dimensional compare-on-one with mutation bias. For the latter
problem, 86% of the mutations that produce an increase in some dimension
cause a (typically larger) decrease in some other dimension.
We ﬁrst compare delphi to several competitive coevolution methods. In
avg E, avg L, the ﬁtness of learners is the average score against evaluators,
vice versa. Individuals are selected into the next population with a ﬁtness-
proportional probability. prob E, prob L views the outcomes as objectives,
and employs a standard EMOO method [8] sorting individuals based on the
number of individuals they are dominated by and using the normalized rank as
the probability of selection. A stricter variant half E, half L selects the best
half of the population. Still more strict is a method replacing an existing indi-
vidual by any new individual that dominates it (dom E, dom L). Finally, we
require that the replacer must be the oﬀspring of the replacee(P-HC E, P-HC
L), so that both learners and evaluators are Pareto-hillclimbers.
Figure 4 shows the average minimum value for the two-dimensional
compare-on-all problem. All competitive methods are able to achieve some
progress. delphi outperforms all of these, and makes remarkably constant
progress.
To test whether choices made in developing delphi are necessary, we perform
several control experiments. This time, the much more diﬃcult compare-on-
one problem is used with ﬁve dimensions and with mutation bias. All methods
use the outcomes of interactions with evaluators as the objectives for learners,
and use the distinctions between learners as objectives for evaluators. dom E,
spread dist L attempts to make evaluators spread over the possible distinc-
tions. The ﬁtness contribution for making a distinction is shared with other
evaluators making the distinction. This competitive ﬁtness sharing [18] method
was the most successful of several methods used in [7] when applied to distinc-

282
E.D. de Jong and J.B. Pollack
−10
−5
0
5
10
15
0
5
10
15
20
25
Value in dimension 1
Value in dimension 2
Learners
Evaluators
Fig. 5. Trajectories in version of the compare-on-one problem where the underlying
objectives have been rotated 30 degrees anti-clockwise. The evaluators still identify the
underlying dimensions when these do not correspond to the variables of the problem.
tions, as it is here. P-HC E, P-HC L tests whether learners may also beneﬁt
from the parent criterion; both learners and evaluators are Pareto-hillclimbers.
To test if the parent criterion is necessary in evaluator selection, dom E, dom
dist L uses dominance for both learner and evaluator selection.
For this diﬃcult test problem, only methods employing Pareto-Hillclimbing
for the evaluation of evaluators achieve sustained progress on all objectives, see
ﬁg. 4. The comparison methods are unable to do so, and even deteriorate due to
overspecialization, i.e. values are not maintained or improved for all objectives
simultaneously. In summary, only delphi displays consistent and considerable
progress across all test problems.
Finally, we investigate whether evaluators identify the underlying objectives
when these have no direct correspondence to the variables of the problem. To
test this, individuals in compare-on-one are projected onto a rotated coor-
dinate system. The variables and operators of variation remain unchanged. As
the trajectories in ﬁgure 5 show, the evaluators approximately identify the new
underlying objectives of the problem, while learners progress evenly in both of
the extracted underlying dimensions. Thus, the identiﬁcation of the underlying
objectives was not merely due to a correspondence between the variables and
objectives of the problem.
6
Conclusions
Coevolution in principle oﬀers a potential for learning in problems where no
adequate evaluation function is known. We began by considering what the ideal
evaluation function would be if one would have access to the underlying objec-
tives of a problem. Since these underlying objectives are not available, actual
evaluation in coevolution must be based on interactions between individuals.
The theoretical result of the article is that in the limit of ﬁnding all possible
distinctions, this evaluation becomes equal to the ideal evaluation function.

Learning the Ideal Evaluation Function
283
The result immediately suggests a practical operational criterion for approx-
imating the ideal evaluation function in the form of Ficici’s distinctions [7].
We have developed an algorithm based on this principle called delphi. The al-
gorithm evaluates learners by using coevolving evaluators as objectives, while
these evaluators are evaluated by using their ability to make distinctions be-
tween learners as objectives. Strict criteria for learner and evaluator selection
are found to be instrumental in delphi’s ability to achieve sustained progress.
delphi was found to substantially outperform comparison methods on sev-
eral abstract test problems of varying diﬃculty. Experimental evidence was pre-
sented indicating that the evaluators identify the underlying objectives of the
problem. While the current article has explored one particular algorithm, the
idea of approximating the ideal evaluation function can be taken up in many
diﬀerent ways, and provides a principled approach to evaluation in coevolution.
We therefore hope that this work may stimulate the development of new, reliable
algorithms for coevolution.
Acknowledgements. The authors would like to thank Kenneth Stanley and
the anonymous reviewers for helpful comments and suggestions, and the mem-
bers of the DEMO-lab, in particular Anthony Bucci, Sevan Ficici and Richard
Watson, for this and for discussions that were vital in the development of this
work. EdJ gratefully acknowledges a talent fellowship from the Netherlands
Organization for Scientiﬁc Research (NWO).
References
1. Anthony Bucci and Jordan B. Pollack.
Order-theoretic analysis of coevolution
problems: Coevolutionary statics. In Proceedings of the GECCO-2002 Workshop
on Coevolution: Understanding Coevolution, 2002.
2. D. Cliﬀand G. F. Miller. Tracking the Red Queen: Measurements of adaptive
progress in co-evolutionary simulations. In F. Mor´an, A. Moreno, J. J. Merelo,
and P. Chac´on, editors, Proceedings of the Third European Conference on Artiﬁcial
Life: Advances in Artiﬁcial Life, volume 929 of LNAI, pages 200–218, Berlin, 1995.
Springer.
3. Edwin D. De Jong and Jordan B. Pollack. Principled Evaluation in Coevolution.
Technical Report CS-02-225, Brandeis University, May 31, 2002.
4. Edwin D. De Jong and Jordan B. Pollack.
Ideal evaluation from coevolution.
Evolutionary Computation, accepted for publication.
5. Kalyanmoy Deb.
Multi-Objective Optimization Using Evolutionary Algorithms.
Wiley & Sons, New York, NY, 2001.
6. Sevan G. Ficici and Jordan B. Pollack. A game-theoretic approach to the simple
coevolutionary algorithm. In M. Schoenauer, K. Deb, G. Rudolph, X. Yao, E. Lut-
ton, J. Julian Merelo, and H.-P. Schwefel, editors, Parallel Problem Solving from
Nature, PPSN-VI, volume 1917 of LNCS, Berlin, 2000. Springer.
7. Sevan G. Ficici and Jordan B. Pollack. Pareto optimality in coevolutionary learn-
ing. In Jozef Kelemen, editor, Sixth European Conference on Artiﬁcial Life, Berlin,
2001. Springer.

284
E.D. de Jong and J.B. Pollack
8. Carlos M. Fonseca and Peter J. Fleming. Genetic Algorithms for Multiobjective
Optimization: Formulation, Discussion and Generalization. In Stephanie Forrest,
editor, Proceedings of the Fifth International Conference on Genetic Algorithms,
ICGA-93, pages 416–423, San Francisco, CA, 1993. Morgan Kaufmann.
9. Pablo Funes. Evolution of Complexity in Real-World Domains. PhD thesis, Bran-
deis University, Waltham, MA, 2001.
10. D. W. Hillis. Co-evolving parasites improve simulated evolution in an optimization
procedure. Physica D, 42:228–234, 1990.
11. Hugues Juille. Methods for Statistical Inference: Extending the Evolutionary Com-
putation Paradigm. PhD thesis, Brandeis University, 1999.
12. Hugues Juille and Jordan B. Pollack. Co-evolving intertwined spirals. In L.J. Fogel,
P.J. Angeline, and T. Baeck, editors, Evolutionary Programming V: Proceedings of
the Fifth Annual Conference on Evolutionary Programming, pages 461–467, Cam-
bridge, MA, 1996. The MIT Press.
13. Joshua D. Knowles, Richard A. Watson, and David W. Corne. Reducing Local Op-
tima in Single-Objective Problems by Multi-objectivization. In E. Zitzler, K. Deb,
L. Thiele, C.A. Coello Coello, and D. Corne, editors, First International Confer-
ence on Evolutionary Multi-Criterion Optimization, volume 1993 of LNCS, pages
268–282. Springer-Verlag, 2001.
14. Marco Laumanns, Lothar Thiele, Kalyanmoy Deb, and Eckart Zitzler. Combining
convergence and diversity in evolutionary multi-objective optimization. Evolution-
ary Computation, 10(3):263–282, 2002.
15. Samir W. Mahfoud. Niching Methods for Genetic Algorithms. PhD thesis, Uni-
versity of Illinois at Urbana-Champaign, Urbana, IL, May 1995. IlliGAL Report
95001.
16. Ludo Pagie and Paulien Hogeweg. Information integration and Red Queen dynam-
ics in coevolutionary optimization. In Proceedings of the 2000 Congress on Evo-
lutionary Computation, CEC-00, pages 1260–1267, Piscataway, NJ, 2000. IEEE
Press.
17. Jordan B. Pollack and Alan D. Blair. Co-evolution in the successful learning of
backgammon strategy. Machine Learning, 32(1):225–240, 1998.
18. Christopher D. Rosin.
Coevolutionary Search among Adversaries.
PhD thesis,
University of California, San Diego, CA, 1997.
19. Karl Sims. Evolving 3D morphology and behavior by competition. In R. Brooks
and P. Maes, editors, Artiﬁcial Life IV, pages 28–39, Cambridge, MA, 1994. The
MIT Press.
20. Richard A. Watson and Jordan B. Pollack. Symbiotic combination as an alter-
native to sexual recombination in genetic algorithms. In M. Schoenauer, K. Deb,
G. Rudolph, X. Yao, E. Lutton, J. Julian Merelo, and H.-P. Schwefel, editors, Par-
allel Problem Solving from Nature, PPSN-VI, volume 1917 of LNCS, Berlin, 2000.
Springer.
21. Richard A. Watson and Jordan B. Pollack. Coevolutionary dynamics in a mini-
mal substrate. In L. Spector, E. Goodman, A. Wu, W.B. Langdon, H.-M. Voigt,
M. Gen, S. Sen, M. Dorigo, S. Pezeshk, M. Garzon, and E. Burke, editors, Pro-
ceedings of the Genetic and Evolutionary Computation Conference, GECCO-2001,
pages 702–709, San Francisco, CA, 2001. Morgan Kaufmann.

Learning the Ideal Evaluation Function
285
Appendix A: Proof of the Equivalence
Proof (Equivalence with the ideal evaluation function). To prove the equivalence
theorem, we show that given the interaction requirement for G, the coevolu-
tionary evaluation function Fcoev equals the ideal evaluation function Fideal:
Fcoev(a, b) ⇐⇒Fideal(a, b)
(13)
dom
O
E∗
L
G
(a, b) ⇐⇒dom
O (a, b) (by (5) and (2))
(14)
[∀e ∈E∗
L : G(a, e) ≥G(b, e) ∧∃e ∈E∗
L : G(a, e) > G(b, e)]
(15)
⇐⇒[∀i : ai ≥bi ∧∃i : ai > bi] (by (6) and (3))
(16)
Assume: ∀e ∈E∗
L : G(a, e) ≥G(b, e) ∧∃e ∈E∗
L : G(a, e) > G(b, e) (17)
Assume: ∃i : bi > ai
(18)
⇒∃e ∈E : G(b, e) > G(a, e) (by (4))
(19)
⇒∃e ∈E∗
L : G(b, e) > G(a, e) (by (8))
(20)
This contradicts (17). Therefore (18) cannot hold, so:
(21)
∄i : bi > ai
(22)
⇒∀i : ai ≥bi
(23)
Furthermore: ∃i : ai > bi (by (17, right) and (4))
(24)
Combining (23) and (24) proves the implication. To show the reverse implication:
Assume: ∀i : ai ≥bi ∧∃i : ai > bi
(25)
Assume: ∃e ∈E : G(b, e) > G(a, e)
(26)
∃i : bi > ai (by (4))
(27)
This contradicts (25). Therefore (26) cannot hold, so:
(28)
∄e ∈E : G(b, e) > G(a, e)
(29)
⇒∀e ∈E : G(a, e) ≥G(b, e)
(30)
And since E∗
L is a subset of E:
(31)
⇒∀e ∈E∗
L : G(a, e) ≥G(b, e)
(32)
∃e ∈E : G(a, e) > G(b, e) (by (25, right) and (4))
(33)
∃e ∈E∗
L : G(a, e) > G(b, e) (by (33) and (8))
(34)
Combining (32) and (34) proves the reverse implication, and completes the
proof.

 	

	


	




 


		





	

 

	














   

	





 




 



!


	
 
"
 




#








$






 












	!!





!%
"

&
'
#
$
(
)'
*

%



	
 !

+
,-
.




	









"







	



$
-
(
/
!




#

	

'
#

!



!"


!


)
*

$
0-
(








1-








$
-
(

%






1
2


-$




'

!



3

45







!"




	






)6
	
7
*








 
(

$
 

	





	




	






 






	

 
	
!





"















!

 	
	



	










!

	



	







#




	
	









$










 	





%	


 









$










&





		









 

"'
(








	
 










	


	



	






	



#	


"








	










)!

 *





	





	










	
)

	


+
	











	


	








,-."
&


	




	

	


E. Cant’u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 286−297, 2003.
 Springer-Verlag Berlin Heidelberg 2003

 

 	
	 
 







	 

 





   


 

	 




	   

 

	 

	 
 


 

 
 



 




	


 



   



	 
 
		 




 

 








 


 


 




 
   



 




 


 	
  

	 




 

 	
	


	
!
	


 Æ


 
 


	 










	

 

 
 
 
"	


! 
 
	
    
 
	

!

 







 	


	 
	 

		
	 
 

 
 #

  	
$

%

	 	

 

 



 
	  


$




 	 
! 



%

	 
! 

 

	


$

 
!	

	
 

 
 



	


	 

 

	
	
	


 


	
 

	

	

  


	  
&	

 '

	


	
 
 
	  

 

	

  
!
 
 

!

	
	 

&	

	'!
 

	
	



 
(

  
!
 

	 
 


 	
	
&		'
 

!
	
	 


 
 


(
)
	  
!
	

 

 
 
 
	 

!

 
	





  



!

	 
	  
	
!



 	
		

 


 






 	
	 
*
!





	

 
 
!
	 " 
	  

!



 

	

 

 
 (


 
!

 !

	

 





	

	 #

	 

	

	 
 

	

 
 
 
&!
	'
	

	  
	 


	
	 " 
	  
+
 

	 
!

 

	

 
,

  	 
	

	
 




	

	


	 " 
	  
	

!

	

 
	



 
- 
.


%







	


/012 
! 

 
	 
! 
 
	  

 
!

	

  



	
&,
	'

 

	
 

 

 


	



,

  	 




3

		 
!


!





&


	 '
	

 
 
$ 

4!





/52 

!




6  
 
	
	

	







	


/002

  


	





 

	 

		


,

  	 



	
 
	 "


!
 
7
	 !



	

	

 
1

 


! 
  






	





 
8

	 !

 	
	






	


	 
 
9
 

	
 


	 


	
,

	

 
 
:


 


	
	 


 
	  





	

 
;
	


	

	 




	

 
<
=	

 
		
287
A Game-Theoretic Memory Mechanism for Coevolution

 
	



 




	

	





	




	



	





		
	
	




	











 
		
	
	




		

	
	








!
		

	
 "




 




 


	


#

#




$



	





 
 
	





	









"
#


##

#



 
%






##

#


	&




  


'


()



*


+










	&


+
,
 
 -
% 

.
$



#







	&
/








	
#









	
 
"

&



0

#






#





1




&



0

#










2

	
 
	
1


3
1


,
.
*
	

1

,
.
$



	&

  




(#


)




	


4


  

 
0

	&

-
1  
  

1
  
*








  





0
##
#









#




  
*


 





	&

-
1  


.







 




	&	






#

#


	
	5








&



0






6



*






'






	
/


	

"
 





 
7#	



*





	&













&



0



7




2
,
 
 -
1
 

.
8






	&



	&6





	&

  



	&




0
2

+
$






/





	&
 

  
2  

+  

2  
,
 
*





	&



		





 




-
9
,
2
:


	



0
 
 -
1
 

1

 

 
 -
1
 

1

 
!
"





 


 

  
1  
 

.






	

 
	

#

 



$


	
	
 



	




 

;

&	






#



	





<










	
#


		
	
	

	




(

)



	


288
S.G. Ficici and J.B. Pollack

 
 
	



	

	








 











 



 
	
	










 	
















	



 


 








 












 




 





 



 
!!"
#



	









$


% 	


 


	
&"




 







'"


	





 


(



 







 




)



*
+,-




!


.
	

 


 

 











 





 
  


	





 





	


 





/$



 




#
!0"
 1
!

1
!
2

+
3"

 1


1
0

.

4

 "
 1
!0

1
!0
5

 



 
#

6
!!"





 

	

78






 
	




8 









 


 






.
	


 












9
	

	










)

 *




 









 



	











 



:
 







	

  
 
$









(





	

 




;





 









5


	
















 
	







 
$


	



















 
	



 
	
	


5



 
	
9





 
	
	



$
 








 
 








 
	

+








	

















	





	 










 

<"
(

9


 

	


 
	
	













=















  




 



	















 





 










 


=




;

 
(









  

















9



















.

4

 "
5


 



	



;









 
	






% 	



 

8

$

.8






 




 















	





	



 	


	











 

9












 



	

  

289
A Game-Theoretic Memory Mechanism for Coevolution

 


	

	









	

 




	





	

	




	







	

 




		
	



	
 	




	




 

	









	















	
	



	


	
	


	





	



 


	
	

	







	
	







 

 	




	
 



	

	











	
 



 ! "

 

!#


 	
	


$


	

	





	


"

	 %

	
 
	








	

!"



	






	










		
&	
 
 !"
	

			




	
 
	

 

 


	
	
	



	







	
	



	




	
	
	

	






	



	
  









 








	

	










	
	

 
	
	





	
 



	

	







	


	




 






	
	
	

	

#




 


	



		


	



	
 








 



	


#


 

	
 





'

 


	



 




	

	







 




	




 


(! "



 ! "








	
	

	


	

	



	
	
 
)
	
 *



	









	


 	


 

*


 


 
	








	

	


















	 

!	





	
 

"

	
*
+,-.
(	*



/!

 "

0
	






 
	 
	








	

	


	


	
	
	





 

 



	




	





	

#

	




/!

 "
 











	
	
 








	
		




 


	  

 	
	

1
/!

 "
2
    

3! "/!

"
290
S.G. Ficici and J.B. Pollack

 

	
 	
  



 
 

	

 































 


 
 

 




!
 
	
 


"


#





 
 




		








$
%
$





&


$
'
()*+
,

!
 


&




!

-
.$
)








 

%








"

$

 
&

#

/

$



&
#




$

&
#







&
#






 
0




 


&


 









 



&




 

$



#






$



 "



&





























 
N
M
N’
M’
W
recall
Pre-Update State
Post-Update State
discard
From Search Heuristic
support
retain
release
~support
retain
 

 

	




 

	



	
	
 

1

23




	

	
()4+




&





#
&



&
5
$3
$



&
%



&
&"












,
&



		




$&







#

$












.




$





$
$&




$
$





6
#








$
7
&





$





'






	
291
A Game-Theoretic Memory Mechanism for Coevolution

  



 




 	


   

 
	


 
   

 

 



	


 
   

 

  

 


 	



	

		





	

	
	


 

	



		

	







 !
"	#



	



	$
	#

	

%
	
	

&


	


'
		



(
	
#



	


)'

#*



	
	
+
 

	


&






	'

	
	#


	



,
	

	
	




	#

,


-	

'
.
#

	

	'


&



#

%
&


&$
#


	,



&
	



'
,

%
'

,
	


'
%		#

	
"


''
		#




	

	'

&

%	
,
	




&
	#



%
	
	'	


,



 $
	
	

/


	
'
	
	
 

	

0



&

,

	

	

(	
&
	









'	


	
	



#
1


(	
	
	2
'

	


0
(	
	

	

'
	


,





		#
#







	
	
	



3
	#

	


	

	'




'
(	
	


	

(	
	
	
4	


(	
&

	
5%	

	#

(	
	






	

	




	


+


#

(	
	

	

&
	

	$

	#

(	
	
	


	
	
6#

	
'	

(	
&




+


#


,
#

	
'	
'



,



	
7
	
	#

(	

	

	
	


,


,





	
0
	
	



(	

	


	


%	
	'
0
	#
	



	#

'





	
	
	'
''
	
8#
,






&


	
#

%

	
	


	
'



(,
		#







#

'





	
	
	
	

8
0
	#


,


	'





	
	

	
1
'



,
	




0
	#
	





	



	









&



	
	
7
'#

(	


	
	
	

	

&
$

'#




		#





(	
	
&











#







&
	





 




	




292
S.G. Ficici and J.B. Pollack

 
 

 	

 
 	

 

	

	

	

 




	 



 	

 
 

	
 
	 
 
 	
	
 	



	
 

 

		


	
	


 


	
 
	 
 


		

	
	



	
	




	









	
 




	

	









 

 




 






 





	
 







	

	
	





 






	


 	
	



!	




	

!

 	


 	


	
	








"			
	
 


	
#

	!
$!

!

$


$

	

	
 	


 	

	



$

	
 

 	
	


!


		




$

	

	$


	%
	

!
	




$	


&

$$!	
	
 





"	'	$

	

$	
	

 	$

 

!





!
!	

	
 

	
&

 	

!
$

	





'
	

	
	



	

	






(

	
)$	
*
 	








 



!


	
		
!


	

 	

	'
	






$



%!

	
     

 
    

 
&



		
!

 

!!
	
	' 	

	
 	

!
'	

'
	
!		



$
	
	

!







$
$






	

$




	



 	

!		


+
	
	

%!
$


 +
+
&
,
$!$	

	



&

	
!
$
	
	




 

-
			.



!'

 	





-
			.
!
!	

	.

 	


	

/


!
!	
	



0

	



!

$

0
-
	'$	

	
		
1

	
!
!	





!


 	
1

2
3
)!

/$


	
	
)!

!
$


 

/

$

	
		
!


	

!
!	
	



$
	
 '
	



$

	
		

	
	



4
5

&
(


	
		

	

 
	
 
	
	

6


(






$
7!



'(
	
		


%

	
		

	

	
	

8
!
		

 	
9!	

	

:(
'!!
	
;
$	


%
!
$	

 
		
	

	

<

 




	
$
	





(
	
		
1






$

!

$




+
	


%!
	

=



	

	



%!
	



	
!
)!





	
(

!
	

	

%	

$


 
!


	



293
A Game-Theoretic Memory Mechanism for Coevolution

  
 	

 


	
	









	
	




	
	





		
		



	
 



	

	
  
 
 
		



 











 

	




	
 

	




	
 		







	
 

	

		



 

 
!

  

"
#	
$



%
&
 





	


 
	
'
	


	
	
	

	
 
&
	
		
 	
	


 	



	
	
		
		


	


	





	





 
	
	


 




		



	

 	
	


&
 


(! %
	
)







	
*
	

	
	
	

	
 
! 

%
50
100
150
200
250
300
350
400
450
500
-0.2
0
0.2
0.4
0.6
0.8
1
Epoch
Score
Mean
Median
50
100
150
200
250
300
350
400
450
500
-0.2
0
0.2
0.4
0.6
0.8
1
Epoch
Mean Score
 

 

	



	








	









 
 


  

	

!

"
+

,


	





	

	
!+




  
	
	




 	
	



	
 	




 


 %
&
  
		
  
	
	
	


		
,
*-
!	


	
 


+

,
.
	%
&


	
	
!#	
$



%

/



		

*0
		

	
  
1





	
	
  



		
 


*0

 


,
*-
	
	
  
	




)



$
	


	
	



	


		
*2
$



	
 

#



	
 	


 
	


	
	

 

*2
$
	
	

	
	








	 
 

&

*2
$

	

*0


/
		
	

	
,
*-
!	
		



%
&




/


		

'

	

			


  	
!

	
.

#3


 %





		





	





		
	
	
!


%
&

	
	


		
	
	
		


	











 

		



	

	
	
	
	
	


#


	


		


 	


	

	
!
#	
,



%
 
 
	



 	


 

&

	
!

	

  %


	



 
		
294
S.G. Ficici and J.B. Pollack

-1
-0.5
0
0.5
1
Score
50
100
150
200
250
300
350
400
450
500
0
10
20
30
40
50
Epoch
|C(N)|
0
20
40
60
80
100
0
20
40
60
80
100
X Location
Y Location
Epoch 1
Epoch 21
Epoch 62
Epoch 500
 

 
	





	
	
























	
	
 

	





















  



			








!

"

	





 


			


 



#$
%#$
&%$
	
'((
 




	



	


 


	

  





		



 
	





 



 







		



	









	


 





 
!
 










		








 
"
 
 



 













#
$

 


Æ









	


 
	


%!




&
 	



'
 








	



 
 


	


() (%
*&+




 
!
#





,	



%
 


	



&

#
&

 





 
!

() (%


		

 
	









-






&	






$$



 

"
 
"
%..

/
0



!
1








+














	




$







	













.


)
 
!

 



 

!





2


3







	






0



&	








#





3



 
	



).

..


.(


.%.!


&	


 

"
%.. 
"
%..


 !

&	



3


$


.(4


.%04!
#



5
		


67
	



8


-

&



1
#









#




















	

$
295
A Game-Theoretic Memory Mechanism for Coevolution

 

	

	
	
	

	


	



	
	
	



	

	
	













	
	
	

	
	


	

	
	

	






	



	

	






	






 

	


		 
	
	
	


	




	

 




	



	

	





	

		 

	


	
	


	
	


	



		 


	







	

		 


 

	
	
	


	

		 

	







 


 

	


	
	







 

	
	

	
		
 
  !! ""
#
 ""  !!$
%	
&'
	



	



(	
	

)
  !!  !!



	
  !! ""
	


	

	
	
*
	



	


		 

	









 

	

)

		 



	

	

	

	

	



	
 
	









	



	

%	
+,




	
	
	

)



	



		 

	
	


 	

	
	


	
	



	



 

	

		




	


	



	


	

	
-
	
	


		 

	





	
	

)


	





 




	


	



	

	





%	
+ 
	

	

	


	


	
		


		
 	
	
	
	





.

	


	
	


	


 

	

!!/
 	


0!
	










 
	




	
	1
	


	2


	



	



	
	1

	



	





#
	

	

	

	

 '
%
3

	

	

	

	
	

		 


	




	
	
	

	

	

	
		



	1	




	


	
	
	
	



 
	

)


.




	


 

	
	


)
		 


 
#



  !!  !!'



	

	

#	
 ,4

		 


	'
	
	
	
	
	

	

		 
#
5

	


!!0'
	
	

	
)
#
5


!4,4'
	








	
	
	

	
)
		 





3
	


	

	




	

	
	
	

	
)
		 


	
	
 

3
	



	
 

 
.


	




	

		




 .
	


6

5

		





 2
)


	7	




	
.


	

	
	
 
	
	
	

87





	



	







	
 	



	

87





	


 
9"
+:
.




		

	





	


	
 1	
296
S.G. Ficici and J.B. Pollack

 





		

	












	










		







	

	


	







	





	





	



	











	



	



 












!




"#
	



	























$

%




&


"#












	
'
('
	

		





	









%
$



%










	
	


	



	



('
	





 

	


'




%
)


"
*

+
,
%	
-


.


 

	




 



	





 



	

	







 



	





	


 	!
 "	

#

$%

$
	
&%
 	'
"
 	
	

	








	

	




$
(() *
+$	
,

 -
.
+


/
	
	
0




1
"&
	
%

	
#
&
	
"


 
	
%
 	'
"
	
1
23




 




	

	
	
	
$
**()**4
555

(((

+


	
0




	
	

%
 	'
		!
1"&

'	"
$
	&	

	
	
"


6

	

1
"




 





	

	




$
.*)4


 *
-


 
	
6

	
0

 
!



 *
7


6

 	


	


"
#$	"
	
%	


		
1

	
8
'

 *
4
+
9:
	

	
&%
%	
$

	
$'
6
!

;"
<

	
:
%
 	=




>?!.  )..-
 *
*

8
'	

"$
	

%
 	
	

"
#

	


	

@ A
$
-)7


	
	

B
9B
"


#
"$
%

&%
 	
	
	
-> ?! )
 4
 (
C
+"
5%
%	
.
"$'
	
6

%
6
'
"$
	
	


	

@ A
$
*).
  
C
/
+	'
	

 
		



"		
 	"	

"

#
"	&
	
$
	

%
 	
	
1
'




 	
	
	
	
	

	

&'
 



$
)*
((
 





	
(	

	

	

 


	
	
 
!

0	
8'
	
+	
 **
 .

1
8
	
	
0





%
 	'

'	"
	

"	"
 6
	
D
+$





 


)
 	
	
	
	
	

$
4()4(
	
C #"		
(( 
297
A Game-Theoretic Memory Mechanism for Coevolution

The Paradox of the Plankton: Oscillations and
Chaos in Multispecies Evolution
Jeﬀrey Horn and James Cattron
Department of Mathematics and Computer Science
Northern Michigan University
1401 Presque Isle Avenue
Marquette, Michigan, 49855, USA
jhorn@nmu.edu, jcattron@alumni.nmu.edu
http://cs.nmu.edu/{˜jeffhorn,˜jcattron}
Abstract. Two theoretical ecologists have recently discovered that even
under the simplest models of competition, three species are suﬃcient
to generate permanent oscillations, and ﬁve species can generate chaos
(Huisman & Weissing, 2001). We can show that these results carry over
into genetic algorithm (GA) resource sharing after making one minor
change in the “usual” sharing methods. We also bring together previous,
scattered results showing oscillatory and chaotic behavior in the “usual”
GA sharing methods themselves. Thus one could argue that oscillations
and chaos are fairly easy to generate once individuals are allowed to
inﬂuence each other, even if such interactions are extremely simple, nat-
ural, and indirect, as they are under resource sharing. We suggest that
great care be taken before assuming that any particular implementation
of resource sharing leads to a unique and stable equilibrium.
1
Introduction and Background
Population biologists have long known about oscillations and chaotic behavior
in multispecies competition models. But in the much simpler and more abstract
models of evolution employed in genetic algorithms, we usually assume smooth
convergence to stable equilibria (especially under selection alone). In particular,
the simple and natural method of niching via the sharing of common resources
has been shown to induce stable, long-term equilibria with multiple species co-
existing (e.g., Horn, 1997). Yet even under the widely-used technique of simply
dividing up ﬁnite resources among competing individuals, we can ﬁnd evidence
of oscillatory behavior and non-monotonic approaches to equilibrium.
1.1
Resource Sharing
A natural niching eﬀect is implicitly induced by competition for limited resources
(i.e., ﬁnite rewards). The sharing procedure we discuss here is common to the
models of theoretical ecologists as well as to those of GA practitioners. The
fundamental steps of resource sharing are intuitive:
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 298–309, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

The Paradox of the Plankton: Oscillations and Chaos
299
1. For each of the ﬁnite resources ri, divide it up among all individuals contend-
ing for it, in proportion to the strengths of their claims. (Thus two equally
deserving individuals should be allocated equal amounts of the resource.)
2. For each individual, add all rewards/credits earned in the ﬁrst step, and use
this amount (perhaps scaled) as the ﬁtness for GA selection.
3. After a new generation is produced, replenish/renew the resources and start
over at the ﬁrst step above.
The notions of competition and niche overlap are easy to visualize in the case
of resource sharing. In Figure 1, the circle represents the resources covered by
the corresponding species. In a learning classiﬁer system, for example, the circles
would represent the subset of examples that are correctly classiﬁed by individuals
of that species1. The resources in the overlapped niches are covered by multiple
species, and must be shared among the individuals of all such species. (The
resources covered by only one species must also be shared, but only among
members of that one species.)
A
f
B
f
AB
f
B
A
AB
f
B
f
A
f
AC
f
BC
f
C
f
ABC
f
A
B
C
C
f
A
f
B
f
AB
f
AC
f
BC
f
B
A
C
k  =  3
niches
k  =  2    niches
k  =  3
niches
Fig. 1. Diﬀerent situations of overlapping resource coverage.
1 For the purposes of this paper, we will use the term “niche” to refer to such a subset
of resources covered by a species.

300
J. Horn and J. Cattron
To be explicit about the actual sharing mechanism, we calculate the shared
ﬁtness for the members of a species A in the situation shown in Figure 1, bottom
(i.e., only pairwise niche overlaps, with fABC = 0). Let fA, fB, and fC be the
objective (i.e., unshared) ﬁtnesses for rules A, B, and C respectively2. Let fAB
be the amount of resources in the overlapping coverage of species A and B. That
is, fAB is the amount of resources shared by A and B. Let nA, nB, nC be the
number of members of each of the three species, in our population of size N
(thus N = nA + nB + nC). We calculate the shared ﬁtness of A:
fsh,A = fA −fAB −fAC
nA
+
fAB
nA + nB
+
fAC
nA + nC
.
(1)
Similarly for fsh,B and fsh,C.
To simulate an actual experiment (i.e., a run of a GA), we use the well-
known method of expected proportion equations. We assume a generational GA
with proportionate selection and no crossover or mutation:
PA,t+1 =
nA,tfsh,A,t

∀speciesX(nX,tfsh,X,t)
=
PA,tfsh,A,t

∀speciesX(PX,tfsh,X,t),
(2)
where PA,t means the proportion of the population taken up by copies of species
A at time (generation) t (i.e., nA,t+1/N), and fsh,A,t is the shared ﬁtness (e.g.,
Equation 1) of A at time t.
Resource sharing is often incorporated in adaptive, or simulated, systems,
including: learning classiﬁer systems (LCS) (Booker, 1982; Wilson, 1994; Horn,
Goldberg, & Deb, 1994), immune system models (Smith, Forrest, & Perelson,
1993), evolving cellular automata (Werfel, Mitchell, & Crutchﬁeld, 2000; Juill´e
& Pollack, 1998), and ecological simulations (Huberman, 1988). It is known by
other names, such as example sharing (McCallum & Spackman, 1990) and shared
sampling (Rosin & Belew, 1997).
2
Oscillations in Traditional Models of Resource Sharing
2.1
Two-Species Oscillations
Oei, Goldberg, and Chang (1991) showed how the “na¨ıve” combination of ﬁtness
sharing and tournament selection leads to oscillations and chaos. In their model,
the population consisted solely of two species with no niche overlap3. If tour-
nament selection is applied to the shared ﬁtness values calculated at generation
t, then the species with the lower shared ﬁtness will lose every competition in
which it is paired with the other species. This will result in an overcompensa-
tion by selection, in which the more crowded (and therefore less ﬁt) species will
2 In this paper we use the term objective ﬁtness to mean the credit or reward earned by
an individual for covering each resource, summed over all covered resources, before
any sharing method is applied.
3 Since resource sharing and ﬁtness sharing are equivalent methods when there is no
niche overlap (Horn, 1997), the following result applies to resource sharing as well.

The Paradox of the Plankton: Oscillations and Chaos
301
suddenly become the under-represented and more highly ﬁt species in the next
generation.
5
10
15
20
25
30
t (generations)
0
20
40
60
80
100
n
NAIVE TOURNAMENT SHARING OSCILLATIONS
Fig. 2. Lotka-Volterra predator-prey oscillations under na¨ıve tournament sharing.
These predicted swings in population are borne out in the plot of expected
proportions shown in Figure 2. We trace the expected proportion of As in a
size N = 100 population under naive tournament sharing with two species of
equal objective ﬁtness: fA = fB. We start with one copy of A at t = 0, the
initial generation. We observe a rapid convergence toward equilibrium, followed
by an overshooting, then an undershooting, and so on; an apparently periodic
oscillation. This seems reminiscent of predator-prey oscillations generated by
Lotka-Volterra growth equations.
Although the oscillations in Figure 2 seem periodic, we note that this is an
artifact of the model. By looking only at expected values, we are averaging out
the randomness of an individual run. An actual run would involve the stochastic
nature of tournament selection (in the random selection of tournament com-
petitors). Oei, et. al. (1991) predict chaotic behavior, calculating a Lyapunov
exponent of approximately 0.21, even with only two niches. In the case of k > 2
niches, we can imagine that the oscillations would be so coupled as to always
result in chaotic behavior.
2.2
Non-monotonic Convergence with Three Species
Horn (1997) analyzed the behavior and stability of resource sharing under pro-
portionate selection. He looked at the existence and stability of equilibrium for
all situations of overlap, but most of this analysis was limited to the case of only

302
J. Horn and J. Cattron
two species. Horn did take a brief look at three overlapping niches, and found
the following interesting result.
If all three pairwise niche overlaps are present (as in Figure 1), then it is
possible to have non-monotic convergence to equilibrium. That is, one or more
species can “overshoot” its equilibrium proportion, as in Figure 3. This overshoot
is expected, and is not due to stochastic eﬀects of selection during a single run.
We speculate that this “error” in expected convergence is related to the increased
complexity of the niching equilibrium equations. For three mutually overlapped
niches, the equilibrium condition yields a system of cubic equations to solve.
Furthermore, the complexity of such equations for k mutually overlapping niches
can be shown to be bounded from below: the equations must be polynomials of
degree 2k −3 or greater (Horn, 1997).
species   C
species   A
species   B
2
4
6
8
10
12
0
0.2
0.4
0.6
0.8
1
 
t  (generation)
proportion
initial overshoot
initial overshoot
EXPECTED BEHAVIOR FOR THREE OVERLAPPED NICHES
Fig. 3. Small, initial oscillations even under traditional “summed ﬁtness”.
3
Phytoplankton Models of Resource Sharing
Recent work by two theoretical ecologists (Huisman & Weissing, 1999; 2001),
has shown that competition for resources by as few as three species can result
in long-term oscillations, even in the traditionally convergent models of plank-
ton species growth. For as few as ﬁve species, apparently chaotic behavior can
emerge. Huisman and Weissing propose these phenomena as one possible new
explanation of the paradox of the plankton, in which the number of co-existing
plankton species far exceeds the number of limiting resources, in direct con-
tradiction of theoretical predictions. Continuously ﬂuctuating species levels can

The Paradox of the Plankton: Oscillations and Chaos
303
support more species than a steady, stable equilibrium distribution. Their re-
sults show that external factors are not necessary to maintain non-equilibrium
conditions; the inherent complexity of the “simple” model itself can be suﬃcient.
Here we attempt to extract the essential aspects of their models and duplicate
some of their results in our models of resource sharing in GAs. We note that
there are major diﬀerences between our model of resource sharing in a GA
and their “well-known resource competition model that has been tested and
veriﬁed extensively using competition experiments with phytoplankton species”
(Huisman & Weissing, 1999). For example, where we assume a ﬁxed population
size, their population size varies and is constrained only by the ﬁnite resources
themselves. Still, there are many similarities, such as the sharing of resources.
3.1
Diﬀerential Competition
First we try to induce oscillations among multiple species by noting that Huis-
man and Weissing’s models allow diﬀerential competition for overlapped re-
sources. That is, one species I might be better than another species J when
competing for the resources in their overlap fIJ. Thus species I would obtain a
greater share of fIJ than would J. In contrast, our models described above all
assume equal competitiveness for overlapped resources, and so we have always
divided the contested resources evenly among species.
Now we try to add this diﬀerential competition to our model. In the phyto-
plankton model, cij denotes the content of resource i in species j. In our model
we will let cI,IJ denote the competitive advantage of species I over species J in
obtaining the resource fIJ. Thus cA,AB = 2.0 means that A is twice as good as
B at obtaining resources from the overlap fAB, and so A will receive twice the
share that B gets from this overlap:
fsh,A = fA −fAB
nA
+
cA,AB ∗fAB
cA,AB ∗nA + nB
fsh,B = fB −fAB
nB
+
fAB
cA,AB ∗nA + nB
.
(3)
This generalization4 seems natural. What can it add to the complexity of
multispecies competition? We looked at the expected evolution of ﬁve species,
with pairwise niche overlaps and diﬀerent competitive resource ratios. After some
experimentation, the most complex behavior we were able to generate is a “dou-
ble overshoot” of equilibrium by a species, similar to Figure 3. This is a further
step away from the usual monotonic approach to equilibrium, but does not seem
a promising way to show long-term oscillations and non-equilibrium dynamics.
3.2
The Law of the Minimum
Diﬀerential competition does not seem to be enough to induce long-term oscil-
lations in our GA model of resource sharing. We note another major diﬀerence
4 Note that we get back our original shared ﬁtness formulae by setting all competitive
factors cI,IJ to one.

304
J. Horn and J. Cattron
between our model and the Plankton model. Huisman and Weissing (2000) “as-
sume that the speciﬁc growth rates follow the Monod equation, and are deter-
mined by the resource that is the most limiting according to Liebig’s ‘law of the
minimum’:
µi(R1, ..., Rk) = min

riR1
K1i + R1
, ...,
riR1k
Kki + Rk

”
(4)
where Ri are the k resources being shared.
Since a min function can sometimes introduce “switching” behavior, we at-
tempt to incorporate it in our model of resource sharing. Whereas we simply
summed the diﬀerent components of the shared ﬁtness expression (Equation 1),
we might instead take the minimum of the components:
fsh,A = min
fA −fAB −fAC
nA
,
cA,AB ∗fAB
cA,AB ∗nA + nB
,
cA,AC ∗fAC
cA,AC ∗nA + nC

.
(5)
Note that we have added the competitive factors introduced in Equation 3 above.
We want to use diﬀerential competition to induce a rock-paper-scissors relation-
ship among the three overlapping species, as in (Huisman & Weissing, 1999).
To do so, we set our competitive factors as follows: cA,AB = 2, cB,BC = 2, and
cC,AC = 2, with all other cI,IJ = 1. Thus A “beats” B, B beats C, and C beats
A. These settings are meant to induce a cyclical behavior, in which an increase
in the proportion of species A causes a decline in species B which causes an
increase in C which causes a decline in A, and so on.
Plugging the shared ﬁtness of Equation 5 into the expected proportions of
Equation 2, we plot the time evolution of expected proportions in Figure 4,
assuming starting proportions of PA,0 = 0.2, PB,0 = 0.5, PC,0 = 0.3. Finally,
we see the “non-transient” oscillations that Huisman and Weissing were able to
ﬁnd. These follow the rock-paper-scissors behavior of sequential ascendency of
each species in the cycle.
3.3
Five Species and Chaos
Huisman and Weissing were able to induce apparently chaotic behavior with as
few as ﬁve species (in contrast to the seemingly periodic oscillations for three
species). Here we attempt to duplicate this eﬀect in our modiﬁed model of GA
resource sharing.
In (Huisman & Weissing, 2001), the authors set up two rock-paper-scissors
“trios” of species, with one species common to both trios. This combination
produced chaotic oscillations. We attempt to follow their lead by adding two
new species D and E in a rock-scissors-paper relationship with A. In Figure 5
we can see apparently chaotic oscillations that eventually lead to the demise of
one species, C. The loss of a species seems to break the chaotic cycling, and it
appears that immediately a stable equilibrium distribution of the four remaining
species is reached.

The Paradox of the Plankton: Oscillations and Chaos
305
Fig. 4. Permanent oscillations.
We consider the extinction of a member species to signify the end of a trio.
We can then ask which trio will win, given a particular initial population dis-
tribution. Huisman and Weissing found in their model that the survival of each
species, and hence the success of the trios, was highly dependent on the initial
conditions, such as the initial species counts. They proceeded to generate fractal-
like images in graphs in which the independent variables are the initial species
counts and the dependent variable, dictating the color at that coordinate, is the
identity of the winning (surviving) trio.
Here we investigate whether our model can generate a fractal-like image based
on the apparently chaotic behavior exhibited in Figure 5. We choose to vary the
initial proportions of species B (x-axis), and D (y-axis). Since we assume a ﬁxed
population size (unlike Huisman and Weissing), we must decrease other species’
proportions as we increase another’s. We choose to set PC,0 = 0.4 −PB,0 and
PE,0 = 0.4 −PD,0, leaving PA,0 = 0.2. Thus we are simply varying the ratio of
two members of each trio, on each axis. Only the initial proportions vary. All
other parameters, such as the competitive factors and all of the ﬁtnesses, are
constant.
Since our use of proportions implies an inﬁnite population, we arbitrarily
choose a threshold of 0.000001 to indicate the extinction of a species, thus sim-
ulating a population size of one million. If PX,t falls below
1
N = 0.000001, then
species X is considered to have gone extinct, and its corresponding trio(s) is
considered to have lost. In Figure 6 we plot the entire range of feasible values
of PB,0 and PC,0. The resolution of our grid is 400 by 400 “pixels”. We color
each of the 160,000 pixels by iterating the expected proportions equations (as in
Equation 5) until a species is eliminated or until a maximum of 300 generations
is reached. We then color the pixel as shown in the legend of Figure 6: red for

306
J. Horn and J. Cattron
Fig. 5. Chaotic, transient oscillations leading to extinction.
a win by trio ABC, blue for an ADE win, and yellow if neither trio has been
eliminated by the maximum number of generations5.
Figure 6 exhibits fractal characteristics, although further analysis is needed
before we can call it a fractal. But we can gain additional conﬁdence by plotting a
much narrower range of initial proportion values and ﬁnding similar complexity.
In Figure 7 we look at a region from Figure 6 that is one one hundredth the
range along both axes, thus making the area one ten thousandth the size of the
plot in Figure 6. We still plot 400 by 400 pixels, and at such resolution we see
no less complexity.
3.4
Discussion
How relevant are these results? The most signiﬁcant change we made to GA
resource sharing was the substitution of the min function for the usual Σ (sum)
function in combining the components of shared ﬁtness. How realistic is this
change?
For theoretical ecologists, Liebig’s law of the minimum is widely accepted
as modeling the needs of organisms to reproduce under competition for a few
limited resources. In the case of phytoplankton, resources such as nitrogen, iron,
phosphorus, silicon, and sunlight are all critical for growth, so that the least
available becomes the primary limiting factor of the moment. We could imagine
a similar situation for simulations of life, and for artiﬁcial life models. Instances
from other ﬁelds of applied EC seem plausible. For example, one could imagine
the evolution of robots (or robot strategies) whose ultimate goal is to assemble
“widgets” by obtaining various widget parts from a complex environment (e.g.,
5 We also use green to signify that species A, a member of both trios, was the ﬁrst to
go. But that situation did not arise in our plots.

The Paradox of the Plankton: Oscillations and Chaos
307
Fig. 6. An apparently fractal pattern.
a junkyard). The number of widgets that a robot can assemble is limited by
the part which is hardest for the robot to obtain. If the stockpile of parts are
“shared” among the competing robots, then indeed the law of the minimum
applies.
4
Conclusions and Future Work
There seem to be many ways to implement resource sharing with oscillatory and
even chaotic behavior. Yet resource (and ﬁtness) sharing are generally associated
with unique, stable, steady-state populations of multiple species. Indeed, the
oscillations and chaos we have seen under sharing are better known and studied
in the ﬁeld of evolutionary game theory (EGT), in which species compete pair-
wise according to a payoﬀmatrix, and selection is performed based on each
individual’s total payoﬀ.
For example, Ficici, et. al. (2000) found oscillatory and chaotic behavior
similar to that induced by na¨ıve tournament sharing, but for other selection

308
J. Horn and J. Cattron
Fig. 7. Zooming in on
1
10,000
th of the previous plot.
schemes (e.g., truncation, linear-rank, Boltzmann), when the selection pressure
was high. Although they did not analyze ﬁtness or resource sharing speciﬁcally,
their domain, the Hawk-Dove game, induces a similar coupling (Lotka-Volterra)
between two species.
Another example of a tie-in with EGT is the comparison of our rock-paper-
scissors, ﬁve-species results with the work of Watson and Pollack (2001). They
investigate similar dynamics arising from “intransitive superiority”, in which a
species A beats species B which beats species C which beats A, according to
the payoﬀmatrix.
Clearly there is a relationship between the interspecies dynamics introduced
by resource sharing and those induced by pairwise games. There are also clear
diﬀerences, however. While resource sharing adheres to the principal of conser-
vation of resources, EGT in general involves non-zero-sum games. Still, it seems
that a very promising extension of our ﬁndings here would be mapping resource
sharing to EGT payoﬀmatrices.
It appears then that some of the unstable dynamics recently analyzed in
theoretical ecology and in EGT can ﬁnd their way into our GA runs via resource
sharing, once considered a rather weak, passive, and predictable form of species
interaction. In future, we as practitioners must be careful not to assume the
existence of a unique, stable equilibrium under every regime of resource sharing.

The Paradox of the Plankton: Oscillations and Chaos
309
References
Booker, L. B. (1989). Triggered rule discovery in classiﬁer systems. In J. D. Schaﬀer,
(Ed.), Proceedings of the Third International Conference on Genetic Algorithms
(ICGA 3). San Mateo, CA: Morgan Kaufmann. 265–274.
Ficici, S. G., Melnik, O., & Pollack, J. B. (2000). A game-theoretic investigation of
selection methods used in evolutionary algorithms. In A. Zalzala, et al (Ed.s),
Proceedings of the 2000 Congress on Evolutionary Computation. IEEE Press.
Horn, J. (1997). The Nature of Niching: Genetic Algorithms and the Evolution of
Optimal, Cooperative Populations. Ph.D. thesis, University of Illinois at Urbana-
Champaign, (UMI Dissertation Services, No. 9812622).
Horn, J., Goldberg, D. E., & Deb, K. (1994). Implicit niching in a learning classiﬁer
system: nature’s way. Evolutionary Computation, 2(1). 37–66.
Huberman, B. A. (1988). The ecology of computation. In B. A. Huberman (Ed.), The
Ecology of Computation. Amsterdam, Holland: Elsevier Science Publishers B. V.
1–4.
Huisman, J., & Weissing, F. J. (1999). Biodiversity of plankton by species oscillations
and chaos. Nature, 402. November 25, 1999, 407–410.
Huisman, J., & Weissing, F. J. (2001). Biological conditions for oscillations and chaos
generated by multispecies competition. Ecology, 82(10). 2001, 2682–2695.
Juill´e, H., & Pollack, J. B. (1998). Coevolving the “ideal” trainer: application to
the discovery of cellular automata rules. In J. R. Koza, et. al., (Ed.s), Genetic
Programming 1998. San Francisco, CA: Morgan Kaufmann. 519–527.
McCallum, R. A., & Spackman, K. A. (1990). Using genetic algorithms to learn dis-
junctive rules from examples. In B. W. Porter & R. J. Mooney, (Ed.s), Machine
Learning: Proceedings of the Seventh International Conference. Palo Alto, CA:
Morgan Kaufmann. 149–152.
Oei, C. K., Goldberg, D. E., & Chang, S. (1991) Tournament selection, niching, and
the preservation of diversity. IlliGAL Report No. 91011. Illinois Genetic Algo-
rithms Laboratory, University of Illinois at Urbana-Champaign, Urbana, IL. De-
cember, 1991.
Rosin, C. D., & Belew, R. K. (1997). New methods for competitive coevolution.
Evolutionary Computation, 5(1). Spring, 1997, 1–29.
Smith, R. E., Forrest, S., & Perelson, A. S. (1993). Searching for diverse, cooperative
populations with genetic algorithms. Evolutionary Computation, 1(2). 127–150.
Watson, R.A., & Pollack, J.B. (2001). Coevolutionary dynamics in a minimal sub-
strate. In L. Spector, et. al. (Ed.s), Proceedings of the 2001 Genetic and Evolu-
tionary Computation Conference, Morgan Kaufmann.
Werfel, J., Mitchell, M., & Crutchﬁeld, J. P. (1999). Resource sharing and coevolution
in evolving cellular automata. IEEE Transactions on Evolutionary Computation,
4(4). November, 2000, 388–393.
Wilson, S. W. (1994). ZCS: A zeroth level classiﬁer system. Evolutionary Computa-
tion, 2(1). 1–18.

Exploring the Explorative Advantage of the
Cooperative Coevolutionary (1+1) EA
Thomas Jansen1⋆and R. Paul Wiegand2
1 FB 4, LS2, Univ. Dortmund, 44221 Dortmund, Germany
Thomas.Jansen@udo.edu
2 Krasnow Institute, George Mason University, Fairfax, VA 22030
paul@tesseract.org
Abstract. Using a well-known cooperative coevolutionary function op-
timization framework, a very simple cooperative coevolutionary (1+1)
EA is deﬁned. This algorithm is investigated in the context of expected
optimization time. The focus is on the impact the cooperative coevolu-
tionary approach has and on the possible advantage it may have over
more traditional evolutionary approaches. Therefore, a systematic com-
parison between the expected optimization times of this coevolutionary
algorithm and the ordinary (1+1) EA is presented. The main result is
that separability of the objective function alone is is not suﬃcient to
make the cooperative coevolutionary approach beneﬁcial. By present-
ing a clear structured example function and analyzing the algorithms’
performance, it is shown that the cooperative coevolutionary approach
comes with new explorative possibilities. This can lead to an immense
speed-up of the optimization.
1
Introduction
Coevolutionary algorithms are known to have even more complex dynamics than
ordinary evolutionary algorithms. This makes theoretical investigations even
more challenging. One possible application common to both evolutionary and
coevolutionary algorithms is optimization. In such applications, the question of
the optimization eﬃciency is of obvious high interest. This is true from a theoret-
ical, as well as from a practical point of view. While for evolutionary algorithms
such run time analyses are known, we present results of this type for a coevolu-
tionary algorithm for the ﬁrst time. Coevolutionary algorithms may be designed
for function optimization applications in a wide variety of ways. The well-known
cooperative coevolutionary optimization framework provided by Potter and De
Jong (7) is quite general and has proven to be advantageous in diﬀerent applica-
tions (e.g., Iorio and Li (4)). An attractive advantage of this framework is that
any evolutionary algorithm (EA) can be used as a component of the framework.
⋆The research was partly conducted during a visit to George Mason University. This
was supported by a fellowship within the post-doctoral program of the German
Academic Exchange Service (DAAD).
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 310–321, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Exploring the Explorative Advantage
311
However, since these cooperative coevolutionary algorithms involve several
EAs working almost independently on separate pieces of a problem, one of the
key issues with the framework is the question of how a problem representation
can be decomposed in productive ways. Since we concentrate our attention on
the maximization of pseudo-Boolean functions f: {0, 1}n →IR, there are very
natural and obvious ways we can make such representation choices. A bit string
x ∈{0, 1}n of length n is divided into k separate components x(1), . . . , x(k).
Given such a decomposition, there are then k EAs, each operating on one of
these components. When a function value has to be computed, a bit string of
length n is reconstructed from the individual components by picking representa-
tive individuals from the other EAs. Obviously, the choice of the EA that serves
as underlying search heuristic has great impact on the performance of this co-
operative coevolutionary algorithm (CCEA). We use the well-known (1+1) EA
for this purpose because we feel that it is perhaps the simplest EA that still
shares many important properties with more complex EAs, which makes it an
attractive candidate for analysis.
Whether this mechanism of dividing the optimization problem f into k sub-
problems and treating them almost independently of one another is an advan-
tage strongly depends on properties of the function f. In applications, a priori
knowledge about f is required in order to deﬁne an appropriate division. We
neglect this problem here and investigate only problems where the division in
sub-problems matches the objective function f. The investigation of the impact
of the separation of inseparable parts is beyond the scope of this paper. Intu-
itively, separability of f seems to be necessary for the CCEA to have advantages
over that EA this is used as underlying search heuristic. After all, we could solve
linearly separable blocks with completely independent algorithms and then con-
catenate the solutions, if we like. Moreover, one expects that such an advantage
should grow with the degree of separability of the objective function f. Indeed,
in the extreme we could imagine a lot of algorithms simultaneously solving lots
of little problems, then aggregating the solutions. Linear functions like the well-
known OneMax problem have a maximal degree of separability. This makes them
natural candidates for our investigations. Regardless of our intuition, however, it
will turn out that separability alone is not suﬃcient to make the CCEA superior
to the “stand-alone EA.”
Another aspect that comes with the CCEA are increased explorative possibil-
ities. Important EA parameters, like the mutation probability, are often deﬁned
depending on the string length, i.e., the dimension of the search space. For bi-
nary mutations, 1/n is most often recommended for strings of length n. Since the
components have shorter length, an increased mutation probability is the con-
sequence. This diﬀers from increased mutation probabilities in a “stand-alone”
EA in two ways. First, one can have diﬀerent mutation probabilities for diﬀerent
components of the string with a CCEA in a natural way. Second, since mutation
is done in the components separately, the CCEA can search in these components
more eﬃciently, while the partitioning mechanism may aﬀord the algorithm some
added protection from the increased disruption. The components that are not

312
T. Jansen and R.P. Wiegand
“active” are guaranteed not to be changed in that step. We present a class of
example functions where this becomes very clear.
In the next section we give precise formal deﬁnitions of the (1+1) EA, the
CC (1+1) EA, the notion of separability, and the notion of expected optimiza-
tion time. In Section 3 we analyze the expected optimization time of the CC
(1+1) EA on the class of linear functions and compare it with the expected
optimization time of the (1+1) EA. Surprisingly, we will see that in spite of the
total separability of linear functions the CC (1+1) EA has no advantage over the
(1+1) EA. This leads us to concentrate on the eﬀects of the increased mutation
probability. In Section 4, we deﬁne a class of example functions, CLOB, and an-
alyze the performance of the (1+1) EA and the CC (1+1) EA. We will see that
the cooperative coevolutionary function optimization approach can reduce the
expected optimization time from super-polynomial to polynomial or from poly-
nomial to a polynomial of much smaller degree. In Section 5, we conclude with
a short summary and a brief discussion of possible directions of future research.
2
Deﬁnitions and Framework
The (1+1) EA is an extremely simple evolutionary algorithm with population
size 1, no crossover, standard bit-wise mutations, and plus-selection known from
evolution strategies. Due to its simplicity it is an ideal subject for theoreti-
cal research. In fact, there is a wealth of known results regarding its expected
optimization time on many diﬀerent problems (M¨uhlenbein (6), Rudolph (9),
Garnier, Kallel, Schoenauer (3), Droste, Jansen, and Wegener (2)). Since we are
interested in a comparison of the performance of the EA alone as opposed to its
use in the CCEA, known results and, even more importantly, known analytical
tools and methods (Droste et. al. 1) are important aspects that make the (1+1)
EA the ideal choice for us.
Algorithm 1. ((1+1) Evolutionary Algorithm ((1+1) EA))
1.
Initialization
Choose x ∈{0, 1}n uniformly at random.
2.
Mutation
Create y by copying x and, independently for each bit
flip this bit with probability 1/n.
3.
Selection
If f(y) ≥f(x), set x := y.
4.
Continue at line 2.
We do not care about ﬁnding an appropriate stopping criterion and let the
algorithm run forever. In our analysis we are interested in the ﬁrst point of time
when f(x) is maximal, i.e., a global maximum is found. As a measure of time
we count the number of function evaluations.
For the CC (1+1) EA, we have to divide x into k components. For the sake
of simplicity, we assume that x can be divided into k components of equal length

Exploring the Explorative Advantage
313
l, i. e., l = n/k ∈IN. The generalization of our results to the case n/k /∈IN with
k −1 components of equal length ⌊n/k⌋and one longer component of length
n −(k −1) · ⌊n/k⌋is trivial. The k components are denoted as x(1), . . . , x(k)
and we have x(i) = x(i−1)·l+1 · · · xi·l for each i ∈{1, . . . , k}. For the functions
considered here, this is an appropriate way of distributing the bits to the k
components.
Algorithm 2. (Cooperative Coevolutionary (1+1) EA (CC (1+1)
EA))
1.
Initialization
Independently for each i ∈{1, . . . , k},
choose x(i) ∈{0, 1}l uniformly at random.
2.
a := 1
3.
Mutation
Create y(a) by copying x(a) and, independently for each bit,
flip this bit with probability min{1/l, 1/2}.
4.
Selection
If f(x(1) · · · y(a) · · · x(k)) ≥f(x(1) · · · x(a) · · · x(k)), set x(a) := y(a).
5.
a := a + 1
6.
If a > k, then continue at line 2, else continue at line 3.
We use min{1/l, 1/2} as mutation probability instead of 1/l in order to deal
with the case k = n, i. e., l = 1. We consider 1/2 to be an appropriate upper
bound on the mutation probability. The idea of mutation is to create small
random changes. A mutation probability of 1/2 is already equivalent to pure
random search. Indeed, larger mutation probabilities are against this basic “small
random changes” idea of mutation. This can be better for some functions and is
in fact superior for the functions considered here. Since this introduces annoying
special cases that have hardly any practical relevance, we exclude this extreme
case.
The CC (1+1) EA works with k independent (1+1) EAs. The i-th (1+1) EA
operates on x(i) and creates the oﬀspring y(i). For the purpose of selection the
k strings x(i) are concatenated and the function value of this string is compared
to the function value of the string that is obtained by replacing x(a) by y(a). The
(1+1) EA with number a is called active. Again, we do not care about a stopping
criterion and analyze the ﬁrst point of time until the function value of a global
maximum is evaluated. Here we also use the number of function evaluations as
time measure.
Consistent with existing terminology in the literature (Potter and De Jong 8),
we call one iteration of the CC (1+1) EA where one mutation and one selection
step take place a generation. Note, that it takes k generations until each (1+1)
EA was active once. Since this is an event of interest, we call k consecutive
generations a round.
Deﬁnition 1. Let the random variable T denote the number of function eval-
uations until for some x ∈{0, 1}n with f(x) = max {f(x′) | x′ ∈{0, 1}n} the

314
T. Jansen and R.P. Wiegand
function value f(x) is computed by the considered evolutionary algorithm. The
expectation E (T) is called expected optimization time.
When analyzing the expected run time of randomized algorithms, one ﬁnds
bounds of this expected run time depending on the input size (Motwani and
Raghavan 5). Most often, asymptotic bounds for growing input lengths are given.
We adopt this perspective and use the dimension of the search space n as mea-
sure for the “input size.” We use the well-known O, Ω, and Θ notions to express
upper, lower, and matching upper and lower bounds for the expected optimiza-
tion time.
Deﬁnition 2. Let f, g: IN0 →IR be two functions. We say f = O(g), if
∃n0 ∈IN, c ∈IR+: ∀n ≥n0: f(n) ≤c · g(n)
holds. We say f = Ω(g), if g = O(f) holds. We say f = Θ(g), if f = O(g) and
f = Ω(g) both hold.
As discussed in Section 1, an important property of pseudo-Boolean functions
is separability. For the sake of clarity, we give a precise deﬁnition.
Deﬁnition 3. Let f: {0, 1}n →IR be any pseudo-Boolean function. We say
that f is s-separable if there exists a partition of {1, . . . , n} into disjoint sets
I1, . . . , Ir, where 1 ≤r ≤n, and if there exists a matching number of pseudo-
Boolean functions g1, . . . , gr with gj: {0, 1}|Ij| →IR such that
∀x = x1 · · · xn ∈{0, 1}n : f(x) =
r

j=1
gj

xij,1 · · · xij,|Ij|

holds, with Ij =

ij,1, . . . , ij,|Ij|

and |Ij| ≤s for all j ∈{1, . . . , r}.
We say that f is exactly s-separable, if f is s-separable but not (s −1)-
separable.
If a function f is known to be s-separable, it is possible to use the sets Ij for a
division of x for the CC (1+1) EA. Then each (1+1) EA operates on a function
gj and the function value f is the sum of the gj-values. If the decomposition
into sub-problems is expected to be beneﬁcial, it should be so if s is small
and the decomposition matches the sets Ij. Obviously, the extreme case s = 1
corresponds to linear functions, where the function value is the weighted sum of
the bits, i. e., f(x) = w0 +w1 ·x1 +· · ·+wn ·xn with w0, . . . , wn ∈IR. Therefore,
we investigate the performance of the CC (1+1) EA on linear functions ﬁrst.
3
Linear Functions
Linear functions, or 1-separable functions, are very simple functions. They can
be optimized bit-wise without any interaction between diﬀerent bits. It is easy
to see that this can be done in O(n) steps. An especially simple linear function

Exploring the Explorative Advantage
315
is OneMax, where the function value equals the number of ones in the bit-
string. It is long known that the (1+1) EA has expected optimization time
Θ(n log n) on OneMax (M¨uhlenbein 6). The same bound holds for any linear
function without zero weights, and the upper bound O(n log n) holds for any
linear function (Droste, Jansen, and Wegener 2). We want to compare this with
the expected optimization time of the CC (1+1) EA.
Theorem 1. The expected optimization time of the CC (1+1) EA for a linear
function f: {0, 1}n →IR with all non-zero weights is Ω(n log n) regardless of the
number of components k.
Proof. According to our discussion we have k ∈{1, . . . , n} with n/k ∈IN. We
denote the length of each component by l := n/k. First, we assume k < n. We
consider (n−k) ln n generations of the CC (1+1) EA and look at the ﬁrst (1+1)
EA operating on the component x(1). This EA is active in each k-th generation.
Thus, it is active in ((n −k) ln n)/k = (l −1) ln n of those generations. With
probability 1/2, at least half of the bits need to ﬂip at least once after random
initialization. This is true since we assume that all weights are diﬀerent from 0.
Therefore, each bit has an unique optimal value, 1 for positive weights and 0 for
negative weights. The probability that among l/2 bits there is at least one that
has not ﬂipped at all is bounded below by
1 −

1 −

1 −1
l
(l−1) ln n	l/2
≥1 −

1 −e−ln nl/2 = 1 −

1 −1
n
l/2
≥1 −e−1/(2k) ≥1 −
1
1 + 1/(2k) =
1
2k + 1 ≥1
3k .
Since the k (1+1) EA are independent, the probability that there is one that has
not reached the optimum is bounded below by 1 −(1 −1/(3k))k ≥1 −e−1/3.
Thus, the expected optimization time of the CC (1+1) EA with k < n on a
linear function without zero weights is Ω(n log n).
For k = n we have n (1+1) EA with mutation probability 1/2 operating on
one bit each. Each bit has an unique optimal value. We are waiting for the ﬁrst
point of time when each bit has had this optimal value at least once. This is
equivalent to throwing n coins independently and repeating this until each coin
came up head at least once. On average, the number of coins that never came
up head is halved in each round. It is easy to see that on average this requires
Ω(log n) rounds with all together Ω(n log n) coin tosses.
⊓⊔
We see that the CC (1+1) EA has no advantage over the (1+1) EA at all on
linear functions in spite of their total separability. This holds regardless of the
number of components k. We conjecture that the expected optimization time
is Θ(n log n), i. e., asymptotically equal to the (1+1) EA. Since this leads away
from our line of argumentation we do not investigate this conjecture here.

316
T. Jansen and R.P. Wiegand
4
A Function Class with Tunable Advantage for the CC
(1+1) EA
Recall that there were two aspects of the CC (1+1) EA framework that could
lead to potential advantage over a (1+1) EA: partitioning of the problem and
increased focus of the variation operators on the smaller components created by
the partitioning. However, as we have just discussed, we now know that separa-
bility alone is not suﬃcient to make the cooperative coevolutionary optimization
framework advantageous. Now we turn our attention to the second piece of the
puzzle: increased explorative attention on the smaller components. More specif-
ically, dividing the problem to be solved by separate (1+1) EAs results in an
increased mutation probability in our case.
Let us consider one round of the CC (1+1) EA and compare this with k
generations of the (1+1) EA. Remember that we use the number of function
evaluations as measure for the optimization time. Note that both algorithms
make the same number of function evaluations in the considered time period. We
concentrate on l = n/k bits that form one component in the CC (1+1) EA, e. g.,
the ﬁrst l bits. In the CC (1+1) EA the (1+1) EA operating on these bits is active
once in this round. The expected number of b bit mutations, i. e., mutations
where exactly b bits in the bits x1, . . . , xl ﬂip, equals

l
b
 
 1
l
b 
1 −1
l
l−b. For the
(1+1) EA in one generation the expected number of b bit mutations in the bits
x1, . . . , xl equals

l
b
 
 1
n
b 
1 −1
n
l−b. Thus, in one round, or k generations, the
expected number of such b bit mutations equals k·

l
b
 
 1
n
b 
1 −1
n
l−b. For b = 1
we have

1 −1
l
l−1 for the CC (1+1) EA and

1 −1
n
l−1 for the (1+1) EA which
are similar values. For b = 2 we have ((l −1)/(2l))

1 −1
l
l−2 for the CC (1+1)
EA and ((l −1)/(2n))

1 −1
n
l−2 for the (1+1) EA, which is approximately a
factor 1/k smaller. For small b, i. e., for the most relevant cases, the expected
number of b bit mutations is approximately a factor of kb−1 larger for the CC
(1+1) EA than for the (1+1) EA. This may result in an huge advantage for the
CC (1+1) EA.
In order to investigate this, we deﬁne an objective function, which is separable
and requires b bit mutations in order to be optimized. Since we want results for
general values of b, we deﬁne a class of functions with parameter b. We use the
well-known LeadingOnes problem as inspiration (Rudolph 9).
Deﬁnition 4. For n ∈IN and b ∈{1, . . . , n} with n/b ∈IN we deﬁne the
function LOBb: {0, 1}n →IR (short for LeadingOnesBlocks) by
LOBb(x) :=
n/b

i=1
b·i

j=1
xj
for all x ∈{0, 1}n.
LOBb is identical to the so-called Royal Staircase function (van Nimwegen
and Crutchﬁeld 10) which was deﬁned and used in a diﬀerent context. Obviously,

Exploring the Explorative Advantage
317
the function value LOBb(x) equals the number of consecutive blocks of length
b with all bits set to one (scanning x from left to right). Consider the (1+1)
EA operating on LOBb. After random initialization the bits have random values
and all bits right of the left most bit with value 0 remain random (see Droste,
Jansen, and Wegener 2 for a thorough discussion). Therefore, it is not at all
clear that b bit mutations are needed. Moreover, LOBb is not separable, i. e., it
is exactly n-separable. We resolve both issues by embedding LOBb in another
function deﬁnition. The diﬃculty with respect to the random bits is resolved
by taking a leading ones block of a higher value and subtracting OneMax in
order to force the bits right of the left most zero bit to become zero bits. We
achieve separability by concatenating k independent copies of such functions,
which is a well-known technique to generate functions with a controllable degree
of separability.
Deﬁnition 5. For n ∈IN, k ∈{1, . . . , n} with n/k ∈IN, and b ∈{1, . . . , n/k}
with n/(bk) ∈IN, we deﬁne the function CLOBb,k: {0, 1}n →IR (short for Con-
catenated LOB) by
CLOBb,k(x) :=
 k

h=1
n · LOBb

x(h−1)·l+1 · · · xh·l

	
−OneMax(x)
for all x = x1 · · · xn ∈{0, 1}n, with l := n/k.
We have k independent functions, the i-th function operates on the bits
x(i−1)·l+1 · · · xi·l. For each of these functions the function value equals n times
the number of consecutive leading ones blocks (where b is the size of each block)
minus the number of one bits in all its bit positions. The function value CLOBb,k
is simply the sum of all these function values.
Since we are interested in ﬁnding out whether the increased mutation prob-
ability of the CC (1+1) EA proves to be beneﬁcial we concentrate on CLOBb,k
with b > 1. We always consider the case where the CC (1+1) EA makes com-
plete use of the separability of CLOBb,k. Therefore, the number of components
or sub-populations equals the function parameter k. In order to avoid technical
diﬃculties we restrict ourselves to values of k with k ≤n/4. This excludes the
case k = n/2 only, since k = n is only possible with b = 1. We start our in-
vestigations with an upper bound on the expected optimization time of the CC
(1+1) EA.
Theorem 2. The expected optimization time of the CC (1+1) EA on the func-
tion CLOBb,k: {0, 1}n →IR is O

klb 
 l
b + ln k

with l := n/k, where the number
of components of the CC (1+1) EA is k, and 2 ≤b ≤n/k, 1 ≤k ≤n/4, and
n/(bk) ∈IN hold.
Proof. Since we have n/(bk) ∈IN we have k components x(1), . . . , x(k) of length
l := n/k each. In each component the size of the blocks rewarded by CLOBb,k
equals b and there are exactly l/b ∈IN such blocks in each component.
We consider the ﬁrst (1+1) EA operating on x(1). As long as x(1) diﬀers from
1l, there is always a mutation of at most b speciﬁc bits that increases the function

318
T. Jansen and R.P. Wiegand
value by at least n −b. After at most l/b such mutations x(1) = 1l holds. The
probability of such a mutation is bounded below by (1/l)b(1 −1/l)l−b ≥1/(elb).
We consider k · 10e · lb((l/b) + ln k) generations. The ﬁrst (1+1) EA is active
in 10e · lb((l/b) + ln k) generations. The expected number of such mutations is
bounded below by 10((l/b)+ln k). Chernoﬀbounds yield that the probability not
to have at least (l/b) + ln k such mutations is bounded above by e−4((l/b)+ln k) ≤
min{e−4, k−4}. In the case k = 1, this immediately implies the claimed bound
on the expected optimization time. Otherwise, the probability that there is a
component diﬀerent from 1l is bounded above by k · (1/k4) = 1/k3. This again
implies the claimed upper bound and completes the proof.
⊓⊔
The expected optimization time O(klb((l/b)+ln k)) grows exponentially with
b as could be expected. Note, however, that the basis is l, the length of each
component. This supports our intuition that the exploitation of the separability
together with the increased mutation probability help the CC (1+1) EA to be
more eﬃcient on CLOBb,k. We now prove this belief to be correct by presenting
a lower bound for the expected optimization time of the (1+1) EA.
Theorem 3. The expected optimization time of the (1+1) EA on the function
CLOBb,k: {0, 1}n →IR is Ω

nb (n/(bk) + ln k)

, if 2 ≤b ≤n/k, 1 ≤k ≤n/4,
and n/(bk) ∈IN holds.
Proof. The proof consists of two main steps. First, we prove that with probability
at least 1/8 the (1+1) EA needs to make at least ⌈k/8⌉·l/b mutations of b speciﬁc
bits to ﬁnd the optimum of CLOBb,k. Second, we estimate the expected waiting
time for this number of mutations.
Consider some bit string x ∈{0, 1}n. It is divided into k pieces of length
l = n/k each. Each piece contains l/b blocks of length b. Since each leading
block that contains 1-bits only contributes n −b to the function value, these
1-blocks are most important.
Consider one mutation generating an oﬀspring y. Of course, y is divided into
pieces and blocks in the same way as x. But the bit values may be diﬀerent. We
distinguish three diﬀerent types of mutation steps that create y from x. Note
that our classiﬁcation is complete, i. e., no other mutations are possible.
First, the number of leading 1-blocks may be smaller in y than in x. We can
ignore such mutations since we have CLOBb,k(y) < CLOBb,k(x) in this case.
Then y will not replace its parent x.
Second, the number of leading 1-blocks may be the same in x and y. Again,
mutations with CLOBb,k(y) < CLOBb,k(x) can be ignored. Thus, we are only
concerned with the case CLOBb,k(y) ≥CLOBb,k(x). Since the number of leading
1-blocks is the same in x and y, the number of 0-bits cannot be smaller in y
compared to x. This is due to the −OneMax part in CLOBb,k.
Third, the number of 1-blocks may be larger in y than in x. For blocks with
at least two 0-bits in x the probability to become a 1-block in y is bounded
above by 1/n2. We know that the −OneMax part of CLOBb,k leads the (1+1)
EA to all zero blocks in O(n log n) steps. Thus, with probability O((log n)/n)
such steps do not occur before we have a string of the form

Exploring the Explorative Advantage
319
1j1·b0((l/b)−j1)·b1j2·b0((l/b)−j2)·b · · · 1jk·b0((l/b)−jk)·b
as current string of the (1+1) EA.
The probability that we have at least two 0-bits in the ﬁrst block of a speciﬁc
piece after random initialization is bounded below by 1/4. It is easy to see that
with probability at least 1/4 we have at least ⌈k/8⌉such pieces after random
initialization. This implies that with probability at least 1/8 we have at least
⌈k/8⌉pieces which are of the form 0l after O(n log n) generations. This completes
the ﬁrst part of the proof.
Each 0-block can only become a 1-block by a speciﬁc mutation of b bits
all ﬂipping in one step. Furthermore, only the leftmost 0-block in each piece is
available for such a mutation leading to an oﬀspring y that replaces its parent x.
Let i be the number of 0-blocks in x. For i ≤k, there are up to i blocks available
for such mutations. Thus, the probability for such a mutation is bounded above
by i/nb in this case. For i > k, there cannot be more than k 0-blocks available
for such mutations, since we have at most one leftmost 0-block in each of the k
pieces. Thus, for i > k, the probability for such a mutation is bounded above by
k/nb. This yields
1
8 ·


k

i=1
nb
i +
⌈k/8⌉l/b

i=k+1
nb
k

≥nb
8 ·

ln k + kl
8bk

= Ω

nb ·
 n
bk + log n

as lower bound on the expected optimization.
⊓⊔
We want to see the beneﬁts the increased mutation probability due to the
cooperative coevolutionary approach can cause. Thus, our interest is not speciﬁ-
cally concentrated on the concrete expected optimization times of the (1+1) EA
and the CC (1+1) EA on CLOBb,k. We are much more interested in a compar-
ison. When comparing (expected) run times of two algorithms solving the same
problem it is most often sensible to consider the ratio of the two (expected) run
times. Therefore, we consider the expected optimization time of the (1+1) EA
divided by the expected optimization time of the CC (1+1) EA. We see that
Ω

nb ·

 n
bk + log n

O (lb ((l/b) + log k)) = Ω

kb−1
holds. We can say that the CC (1+1) EA has an advantage of order at least kb−1.
The parameter b is a parameter of the problem. In our special setting, this holds
for k, too, since we divide the problem as much as possible. Using c components,
where c ≤k, would reveal that this parameter c inﬂuences the advantage of the
CC (1+1) EA in a way k does in the expression above. Obviously, c is a parameter
of the algorithm. Choosing c as large as the objective function CLOBb,k allows
yields the best result. This conﬁrms our intuition that the separability of the
problem should be exploited as much as possible. We see that for some values of
k and b this can decrease the expected optimization time from super-polynomial
for the (1+1) EA to polynomial for the CC (1+1) EA. This is, for example, the
case for k = n(log log n)/(2 log n) and b = (log n)/ log log n.

320
T. Jansen and R.P. Wiegand
It should be clear that simply increasing the mutation probability in the
(1+1) EA will not resolve the diﬀerence. Increased mutation probabilities lead
to a larger number of steps where the oﬀspring y does not replace its parents
x, since the number of leading ones blocks is decreased due to mutations. As
a result, the CC (1+1) EA gains clear advantage over the (1+1) EA on this
CLOBb,k class of functions. Moreover, this advantage is drawn from more than a
simple partitioning of the problem. The advantage stems from the coevolutionary
algorithm’s ability to increase the focus of attention of the mutation operator,
while using the partitioning mechanism to protect the remaining components
from the increased disruption.
5
Conclusion
We investigated a quite general cooperative coevolutionary function optimiza-
tion framework that was introduced by Potter and De Jong (7). One feature
of this framework is that it can be instantiated using any evolutionary algo-
rithm as underlying search heuristic. We used the well-known (1+1) EA and
presented the CC (1+1) EA, an extremely simple cooperative coevolutionary al-
gorithm. The main advantage of the (1+1) EA is the multitude of known results
and powerful analytical tools. This enabled us to present the run time or opti-
mization time analysis for a coevolutionary algorithm. To our knowledge, this is
the ﬁrst such analysis of coevolution published. The focus of our investigation
was on separability. Indeed, when applying the Potter and De Jong 7 cooper-
ative coevolutionary approach, practitioners make implicit assumptions about
the separability of the function in order to come up with appropriate divisions
of the problem space. Given such a static partition of a string into components,
the CCEA is expected to exploit the separability of the problem and to gain
an advantage over the employed EA when used alone. We were able to prove
that separability alone is not suﬃcient to give the CCEA any advantage. We
compared the expected optimization time of the (1+1) EA with that of the CC
(1+1) EA on linear functions that are of maximal separability. We found that
the CC (1+1) EA is not faster. Motivated by this ﬁnding we discussed the ex-
pected frequency of mutations for both algorithms. The main point is that b bit
mutations occur noticeably more often for the CC (1+1) EA for b > 1 only. The
expected frequency of mutations changing only one single bit is asymptotically
the same for both algorithms. This leads to the deﬁnition of CLOBb,k, a family
of separable functions where b bit mutations are needed for successful optimiza-
tion. For this family of functions we were able to prove that the cooperative
coevolutionary approach leads to an immense speed-up. The advantage of the
CC (1+1) EA over the (1+1) EA can be of super-polynomial order. Moreover,
this advantage stems not only from the ability of the CC (1+1) EA to partition
the problem, but because coevolution can use this partitioning to concentrate
increased variation on smaller parts of the problem.
Our results are a ﬁrst and important step towards a clearer understanding
of coevolutionary algorithms. But there are a lot of open problems. An upper
bound for the expected optimization time of the CC (1+1) EA on linear functions

Exploring the Explorative Advantage
321
needs to be proven. Using standard arguments the bound O(n log2 n) is easy
to show; however, we conjecture that the actual expected optimization time is
O(n log n) for any linear function and Θ(n log n) for linear functions without zero
weights. For CLOBb,k we provided neither a lower bound proof of the expected
optimization time of the CC (1+1) EA nor an upper bound proof of the expected
optimization time of the (1+1) EA. A lower bound for the CC (1+1) EA that is
asymptotically tight is not diﬃcult to prove. A good upper bound for the (1+1)
EA is slightly more diﬃcult.
Furthermore, it is obviously desirable to have more comparisons for more gen-
eral parameter settings and other objective functions. The systematic investiga-
tion of the eﬀects of running the CC (1+1) EA with partitions into components
that do not match the separability of the objective function is also the subject
of future research. A main point of interest is the analysis of other coopera-
tive coevolutionary algorithms where more complex EAs that use a population
and crossover are employed as underlying search heuristics. The investigation of
such CCEAs that are more realistic leads to new, interesting, and much more
challenging problems for future research.
References
S. Droste, T. Jansen, G. Rudolph, H.-P. Schwefel, K. Tinnefeld, and I. Wegener (2003).
Theory of evolutionary algorithms and genetic programming. In H.-P. Schwefel,
I. Wegener, and K. Weinert (Eds.), Advances in Computational Intelligence, Berlin,
Germany, 107–144. Springer.
S. Droste, T. Jansen, and I. Wegener (2002). On the analysis of the (1+1) evolutionary
algorithm. Theoretical Computer Science 276, 51–81.
J. Garnier, L. Kallel, and M. Schoenauer (1999). Rigorous hitting times for binary
mutations. Evolutionary Computation 7(2), 173–203.
A. Iorio and X. Li (2002). Parameter control within a co-operative co-evolutionary
genetic algorithm.
In J. J. Merelo Guerv´os, P. Adamidis, H.-G. Beyer, J.-L.
Fern´andez-Villaca˜nas, and H.-P. Schwefel (Eds.), Proceedings of the Seventh Con-
ference on Parallel Problem Solving From Nature (PPSN VII), Berlin, Germany,
247–256. Springer.
R. Motwani and P. Raghavan (1995). Randomized Algorithms. Cambridge: Cambridge
University Press.
H. M¨uhlenbein (1992). How genetic algorithms really work. Mutation and hillclimbing.
In R. M¨anner and R. Manderick (Eds.), Proceedings of the Second Conference on
Parallel Problem Solving from Nature (PPSN II), Amsterdam, The Netherlands,
15–25. North-Holland.
M. A. Potter and K. A. De Jong (1994). A cooperative coevolutionary approach to
function optimization. In Y. Davidor, H.-P. Schwefel, and R. M¨anner (Eds.), Pro-
ceedings of the Third Conference on Parallel Problem Solving From Nature (PPSN
III), Berlin, Germany, 249–257. Springer.
M. A. Potter and K. A. De Jong (2002). Cooperative coevolution: An architecture for
evolving coadapted subcomponents. Evolutionary Computation 8(1), 1–29.
G. Rudolph (1997). Convergence Properties of Evolutionary Algorithms. Hamburg,
Germany: Dr. Kovaˇc.
E. van Nimwegen and J. P. Crutchﬁeld (2001). Optimizing epochal evolutionary search:
Population-size dependent theory. Machine Learning 45(1), 77–114.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 322–331, 2003.
© Springer-Verlag Berlin Heidelberg 2003
PalmPrints: A Novel Co-evolutionary Algorithm for
Clustering Finger Images
Nawwaf Kharma, Ching Y. Suen, and Pei F. Guo 
Departments of Electrical & Computer Engineering and Computer Science,
Concordia University, 1455 de Maisonneuve Blvd. West, Montreal, QC, H3G 1M8, Canada
kharma@ece.concordia.ca
Abstract. The purpose of this study is to explore an alternative means of hand
image classification, one that requires minimal human intervention. The main
tool for accomplishing this is a Genetic Algorithm (GA). This study is more
than just another GA application; it introduces (a) a novel cooperative co-
evolutionary clustering algorithm with dynamic clustering and feature selection;
(b) an extended fitness function, which is particularly suited to an integrated
dynamic clustering space. Despite its complexity, the results of this study are
clear: the GA evolved an average clustering of 4 clusters, with minimal overlap
between them.
1   Introduction
Biometric approaches to identity verification offer a mostly convenient and potentially
effective means of personal identification. All such techniques, whether palm-based or
not, rely on the individual’s most-unique and stable, physical or behavioural
characteristics.
The use of multiple sets of features requires feature selection as a prerequisite for
the subsequent application of classification or clustering [5, 8]. In [5], a hybrid genetic
algorithm (GA) for feature selection resulted in (a) better convergence properties; (b)
significant improvement in terms of final performance; and (c) the acquisition of
subset-size feature control. Again, in [8], a GA, in combination with a k-nearest
neighbour classifier, was successfully employed in feature dimensionality reduction.
Clustering is the grouping of similar objects (e.g. hand images) together in one set.
It is an important unsupervised classification technique. The simplest and most well
known clustering algorithm is the k-means algorithm. However, this algorithm
requires that the user specifies, before hand, the desired number of clusters. An
evolutionary strategy implementing variable length clustering in the x-y plane was
developed to address the problem of dynamic clustering [3]. Additionally, a genetic
clustering algorithm was used to determine the best number of clusters, while
simultaneously clustering objects [9].

PalmPrints: A Novel Co-evolutionary Algorithm for Clustering Finger Images         323
Genetic algorithms are randomized search and optimization techniques guided by
the principles of evolution and natural genetics, and offering a large amount of
implicit parallelism. GAs perform search in complex, large and multi-modal
landscapes. They have been used to provide (near-)optimal solutions to many
optimization problems [4].
Cooperative co-evolution refers to the simultaneous evolution of two or more
species with coupled fitness. Such evolution allows the discovery of complex
solutions wherever complex solutions are needed. The fitness of an individual
depends on its ability to collaborate with individuals from other species. In this way,
the evolutionary pressure stemming from the difficulty of the problem favours the
development of cooperative individual strategies [7].
In this paper, we propose a cooperative co-evolutionary clustering algorithm,
which integrates dynamic clustering, with (hand-based) feature selection. The co-
evolutionary part is defined as the problem of partitioning a set of hand objects into a
number of clusters without a priori knowledge of the feature space. The paper is
organized as follows. In section 2, hand feature extraction is described. In section 3,
cooperative co-evolutionary clustering and feature selection are presented, along with
implementation results. Finally, the conclusions are presented in section 4.
2   Feature Extraction
Hand geometry refers to the geometric structure of the hand. Shape analysis requires
the extraction of object features, often normalized, and invariant to various geometric
transformations such as translation, rotation and (to a lesser degree) scaling. The
features used may be divided into two sets: geometric features and statistical features.
2.1   Geometric Features
The geometrical features measured can be divided into six categories:
- 
Finger Width(s): the distance between the minima of the two phalanges at either
side of a finger. The line connecting those two phalanges is termed the finger
base-line.
- 
Finger Height(s): the length of the line starting at the fingertip and intersecting (at
right angles) with the finger base-line.
- 
Finger Circumference(s): The length of the finger contour.
- 
Finger Angle(s): The two acute angles made between the finger base-line and the
two lines connecting the phalange minima with the finger tip.
- 
Finger Base Length(s): The length of the finger base-lines.
- 
Palm Aspect Ratio:  the ratio of the ‘palm width’ to the ‘palm height’. Palm
width is (double) the distance between the phalange joint of the middle finger,
and the midpoint of the line connecting the outer points of the base lines of the
thumb and pinkie (call it mp). Palm length is (double) the shortest distance
between mp and the right edge of the palm image.

324         N. Kharma, C.Y. Suen, and P.F. Guo
2.2   Statistical Features
Before any statistical features are measured, the fingers are re-oriented (see Fig. 1),
such that they are standing upright by using the Rotation and Shifting of the
Coordinate Systems. Then, each 2D finger contour is mapped onto a 1D contour (see
Fig. 2), taking the finger midpoint centre as its reference point. The shape analysis for
four fingers (excluding the thumb) is measured using: (1) Central moments; (2)
Fourier descriptors; (3) Zernike moments.
Fig. 1. Hand Fingers (vertically re-oriented) using the Rotation and Shifting of the Coordinate
Systems
Fig. 2. 1D Contour of a Finger. The y-axis represents the Euclidean distance between the
contour point and the finger midpoint centre (called the reference point)
Central Moments. For a digital image, the pth order regular moment with respect to a
one-dimensional function F[n] is defined as:
]
[
0
n
F
n
N
n
p
p
R
⋅
= ∑
=
The normalized one-dimensional pth order central moments are defined as:
]
[
)
(
0
n
F
n
n
M
p
N
n
p
⋅
−
=∑
=
            
0
1 R
R
n =
Little finger to the thumb
0
50
100
150
0
50
100
150
200
250
distance
0
20
40
60
point index

PalmPrints: A Novel Co-evolutionary Algorithm for Clustering Finger Images         325
F[n]:  with n[0,N]; the Euclidean distance between point n and the finger reference
point.
N:      the total number of pixels.
Fourier Descriptors. We define a normalized cumulative function * as an
expanding Fourier series to obtain descriptive coefficients (Fourier Descriptors or
FD’s). Given a periodic 1D digital function F[n] in [0, N] points (periodic), the
expanding Fourier series is:
                                
)
2
2
cos
(
2
)
(
1
0
*
t
N
k
sin
b
t
N
k
a
a
t
k
k
k
⋅
+
⋅
+
=
∑
Φ
∞
=
π
π
              
n
N
k
n
F
N
a
N
n
k
⋅
⋅
=
∑
=1
2
cos
]
[
2
π
 ,             
n
N
k
sin
n
F
N
b
N
n
k
⋅
⋅
=
∑
=1
2
]
[
2
π
The kth harmonic amplitudes of the Fourier Descriptors are:
                       
b
a
k
k
k
A
2
2
+
=
  
             k = 1,2, ..
Zernike Moments. For a digital image with a polar form function 
)
,
(
ϕ
ρ
f
, the
normalized (n+m)th order Zernike moments  is approximated by:
)
,
(
)
,
(
1
*
j
j
nm
j
j
j
nm
V
f
N
n
Z
ϕ
ρ
ϕ
ρ
⋅
+
≈
∑
ϕ
ρ
ϕ
ρ
jm
nm
nm
e
R
V
⋅
=
)
(
)
,
(
 , 
1
2
2
≤
+
j
j
y
x
∑
−
=
−
−
−
−
+
−
−
=
)
2
/|
|
(
0
2
)!
2
/
|)
|
((
)!
2
/
|)
|
((
!
)!
(
)1
(
)
(
m
n
s
s
n
s
nm
s
m
n
s
m
n
s
s
n
R
ρ
ρ
n:     a positive integer.
m:   a positive or negative integer subject to the constraints that  n-|m| is even, |l|
)
,
(
j
j
f
ϕ
ρ
: the length of vector between point j and the finger reference point.
3   Co-evolution in Dynamic Clustering and Feature Selection
Our clustering application involves the optimization of three quantities, which
together form a complete solution, (1) the set of features (dimensions) used for
clustering; (2) the actual cluster centres; and (3) the total number of clusters. Since
this is the case, and since the relationship between the three quantities is
complementary (as opposed to adversarial), it makes sense to use cooperative (as

326         N. Kharma, C.Y. Suen, and P.F. Guo
opposed to competitive) co-evolution as the model for the overall genetic
optimization process. Indeed, it is our hypothesis that whenever a (complete) potential
solution (i) is comprised of a number of complementary components; (ii) has a
medium-high degree of dimensionality; and (iii) features a relatively low level of
coupling between the various components; then attempting a cooperative co-
evolutionary approach is justified.
In similarity-based clustering techniques, a number of cluster centres are proposed.
An input pattern (point) is assigned to the cluster whose centre is closest to the point.
After all the points are assigned to clusters, the cluster centres are re-computed. Then,
the points are re-assigned to the (new) clusters based (again) on their distance from
the new cluster centres. This process is iterative, and hence it continues until the
locations of the cluster centres stabilize.
During co-evolutionary clustering, the above occurs, but in addition, less
discriminatory features are eliminated, leaving a more efficient subset for use. As a
result, the overall output of the genetic optimization process is a number of
traditionally good (i.e. tight and well-separated) clusters, which also exist in the
smallest possible feature space.
The co-evolutionary genetic algorithm used entails that we have two populations
(one of cluster centres and another of dimension selections: more on this below), each
going through a typical GA process. This process is iterative and follows these steps:
(a) fitness evaluation; (b) selection; (c) the application of crossover and mutation (to
generate the next population); (d) convergence testing (to decide whether to exit or
not);  (e) back to (a).
This continues until the convergence test is satisfied and the process is stopped.
The GA process is applied to the first population and in parallel (but totally
independently) to the second population. The only difference between a GA applied
to one (evolving population) and a GA applied to two cooperatively co-evolving
populations is that fitness evaluation of an individual in one population is done after
that individual is joined to another individual in the other population. Hence, the
fitness of individuals in one population is actually coupled with (and is evaluated with
the help of) individuals in the other population.
Below, is a description of the most important aspects of the genetic algorithm
applied to the co-evolving populations that make-up PalmPrints. First, the way
individuals are represented (as chromosomes) is described. This is followed by an
explanation of step (a) to step (e), listed above. Finally, a discussion of the results is
presented.
3.1  Chromosomal Representation
In any co-evolutionary genetic algorithm, two (or more) populations co-evolve. In our
case, there are only two populations, (a) a population of cluster centres (Cpop), each
represented by a variable-length vector of real numbers; and (b) a population of
‘dimension-selections’, or simply dimensions (Dpop), each represented by a vector of
bits. Each individual in Cpop represents a (whole) number of cluster centre
coordinates. The total number of coordinates equals the number of clusters. On the
other hand, each individual (‘dimension-selection’) in Dpop indicates, via its ‘1’ bits,

PalmPrints: A Novel Co-evolutionary Algorithm for Clustering Finger Images         327
which dimensions will be used and which, via its ‘0’ bits, will not be used. Splicing
an individual (or chromosome) from Cpop with an individual (or chromosome) from
Dpop will give us an overall chromosome that has the following form:
{(A1, B1, … , Z1), (A2, B2, ... , Z2), ... (An, Bn, ... ,  Zn), 10110…0 }
Taken as a single representational unit, this chromosome determines:
(1) The number of clusters, via the number of cluster centres in the left-hand side of
the chromosome;
(2) The actual cluster centres, via the coordinates of cluster centres, also presented in
the left-hand side of the chromosome; and
(3) The number of dimensions (or features) used to represent the cluster centres, via
the bit vector on the right-hand side of the chromosome.
As an example, the chromosome presented above has n clusters in three
dimensions: the first, third and fourth dimensions. (This is so because the bit vector
has 1 in its first bit location, 1 in its third bit location and 1 in its fourth bit location.)
The maximum number of feature dimensions (allowed in this example) is equal to the
number of letters in the English alphabet: 26, while the minimum is 1. And, the
maximum number of clusters (which is not shown) is m>n.
3.2   Crossover and Mutation, Generally
In our approach, the crossover operators need to (a) deal with varying-length
chromosomes; (b) allow for a varying number of feature dimensions;  (c) allow for a
varying number of clusters; and (d) to be able to adjust the values of the coordinates
of the various cluster centres. This is not a trivial task, and is achieved via a host of
crossover operators, each tuned for its own task. This is explained below.
Crossover and Mutation for Cpop. Cpop needs crossover and mutation operators
suited for variable-length clusters as well as real-valued parameters. When crossing
over two parent chromosomes to produce two new child chromosomes, the algorithm
follows a three-step procedure:
(a) The length of a child chromosome is randomly selected from the range: [2,
MaxLength], where MaxLength is equal to the total number of clusters in both
parent chromosomes;
(b) Each child chromosome picks up copies of cluster centre coordinates, from each
of the two parents, in proportion to the relative fitness of the parents (to each
other); and finally,
(c) The actual values of the cluster coordinates are modified using the following
(mutation) formula for ith feature with  randomly selected from the range [0,1]:
f i  = min(Fi Fi)  -  min (Fi) ] .
(1)
Fi:
the ith feature dimension, i= 0,1,2….
	

min(f i ) / max(f i ):   minimum  / maximum value that feature i can take.

328         N. Kharma, C.Y. Suen, and P.F. Guo
With  changed within [0,1], the function of equation (1) varies the ith feature
dimension in its own distinguished feature range [min(Fi), max(Fi)] as for the
variation of actual values of the cluster coordinates (see Fig. 3).
Fig. 3. Variation of the ith feature dimension within [min(Fi), max(Fi)] with a random value 
ranged [0,1]
In addition to crossover, mutation is applied, with a probability 
c to one set of
cluster centre coordinates. The value of c used is 0.2 (or 20%).
Crossover and Mutation for Dpop. Dpop needs one crossover operator suited for
fixed length binary-valued parameters. For a binary representation of Dpop
chromosomes, single-point crossover is applied. Following that, mutation is applied
with a mutation rate of d. The value of d used is 0.02.
3.3  Selection and Generation of Future Generations
For both populations, elitism is applied first, and causes copies of the fittest
chromosomes to be carried over (without change) from the current generation to the
next generation. Elitism is set at 12% of Cpop and 10% of Dpop. Another 12% of
Cpop and 10% of Dpop are generated via the crossing over of pairs of elite
individuals, to generate an equal number of children. The rest (76% of Cpop and 80%
of Dpop ) of the next generation is generated through the application of crossover and
mutation (in that order) to randomly selected individuals from the non-elite part of the
current generation. Crossover is applied with a probability of 1 (i.e. all selected
individuals are crossed over), while mutation is applied with a probability of 20% for
Cpop and 2% for Dpop.
3.4   Fitness Function
Since the Mean Square Error (MSE) can always be decreased by adding a data point
as a cluster centre, fitness was a monotonically decreasing function of cluster
numbers. The fitness function (MSE) was poorly suited for comparing clustering
situations that had a different numbers of clusters. A heuristic MSE was chosen with
dynamic cluster n, based on the one given by [3].

PalmPrints: A Novel Co-evolutionary Algorithm for Clustering Finger Images         329
In our own approach of dynamic clustering with feature selection in a co-
evolutionary GA, there are two dynamic variables interchanged with the two
populations: dynamic clustering and dynamic feature dimensions. Hence, a new
extended MSE fitness is proposed for our model, which measures quantities of both
object tightness (fT ) and cluster separation (fS ):
n:     dynamic no. of clusters 
k:     dynamic no. of features
ci:    the ith cluster centre 
Ave(A):  the average value of A
mi:   the number of data points belonging to the ith cluster
i
jx :  the jth data point belonging to the ith cluster
d(a,b):    the Euclidean distance between points a and b
The square root of the number of clusters and the square root of the number of
dimension in MSE extended fitness are chosen to be unbiased in the dynamic co-
evolutionary environment. The point of the MSE extended fitness is to optimize of the
distance criterion by minimizing the within-cluster spread and maximizing the inter-
cluster separation.
3.5   Convergence Testing
The number of generations prior to termination depends on whether an acceptable
solution is reached or a set number of iterations are exceeded. Most genetic
algorithms keep track of the population statistics in the form of population maximum
and mean fitness, standard deviation of (maximum or mean) fitness, and minimum
cost. Any of these or any combination of these can serve as a convergence test. In
PalmPrints, we stop the GA when the maximum fitness does not change by more than
.001 for 10 consecutive generations.
3.6   Implementation Results
The Dpop population is initialized with 500 members, from which 50 parents were
paired from top to bottom. The remaining 400 offspring are produced randomly using
)
1
(
1
S
T
f
f
n
fitness
extended
MSE
+
+
=
,
/)
,
(
1
1
n
x
c
d
f
n
i
mi
j
i
j
i
T ∑∑
=
=
=
})
(
,
{
1
1
,1
∑
∑
=
≠
=
+
=
n
i
n
i
j
j
j
i
S
c
Ave
c
d
k
f

330         N. Kharma, C.Y. Suen, and P.F. Guo
single-point crossover and a mutation rate (
d) of 0.02. Cpop is initialized at 88
individuals, from which 10 members are selected to produce 10 direct new copies in
the next generation. The remaining 68 are generated randomly, using the dimension
fine-tuning crossover strategy and a mutation rate (
c) of 0.2.
The experiment presented here uses 100 hand images and 84 normalized features.
Termination occurred at a maximum of 250 generations, since it is discovered that
fitness converged to less than 0.0001 variance prior. The results are promising; the
average co-evolutionary clustering fitness is 0.9912 with a significantly low standard
deviation of 0.1108. The average number of clusters is 4, with a very low standard
deviation of 0.4714. Average hand image misplacement rate is 0.0580, with a low
standard deviation of 2.044. Following convergence, the dimension of the feature
space is 41, with zero standard deviation. Hence, half of the original 84 features are
eliminated. Convergence results are shown in Fig. 4.
Fig. 4. Convergence results
4   Conclusions
This study is the first to use a genetic algorithm to simultaneously achieve
dimensionality reduction and object (hand image) clustering. In order to do this, a
cooperative co-evolutionary GA is crafted, one that uses two populations of part-
solutions in order to evolve complete highly fit solutions for the whole problem. It
does succeed in both its objectives. The results show that the dimensionality of the
clustering space is cut in half. The number (4) and quality (0.058) of clusters
0
0.2
0.4
0.6
0.8
1
1
50
99
148
197
246
generation
fitness
Maximum fitness
Meam fitness
Minimum fitness

PalmPrints: A Novel Co-evolutionary Algorithm for Clustering Finger Images         331
produced are also very good. These results open the way towards other cooperative
co-evolutionary applications, in which 3 or more populations are used to co-evolve
solutions and designs consisting of 3 or more loosely-coupled sub-solutions or
modules.
In addition to the main contribution of this study, the authors introduce a number
of new or modified structural (e.g. palm aspect ratio) and statistical features (e.g.
finger 1D contour transformation) that may prove equally useful to others working on
the development of biometric-based technologies.
References
1.
Fogel, D.B.: Evolutionary Computation: Toward A New Philosophy Of Machine
Intelligence. IEEE Press, New York, (1995) 5
2.
Haupt, R.L. and Haupt, S.E.: Practical Genetic Algorithms. Wiley Interscience, New York
(1998)
3.
Lee, C.-Y.: Efficient Automatic Engineering Design Synthesis Via Evolutionary
Exploration.  PhD thesis (2002), California Institute of Technology, Pasadena, California
4.
Maulik, U., Bandyopadhyay S.: Genetic Algorithm-based Clustering Technique. Pattern
Recognition 33 (2000) 1455–1465
5.
Oh, I.-S., Lee, J.-S. and Moon, B.-R.: Local Search-embedded Genetic Algorithms For
Feature Selection. Proc. of International Conf. on Pattern Recognition  (2002) 148–151
6.
Paredis, J.: Coevolutionary Computation.  Artificial Life 2 (1995) 355–375
7.
Pena-Reyes, C.A., Sipper M.: Fuzzy CoCo: A Cooperative-Coevolutionary Approach To
Fuzzy Modeling. IEEE Transaction on Fuzzy Systems Vol. 9, No.5 (October 2001) 727–
737
8.
Raymer, M.L., Punch, W.F., Goodman, E.D., Kuhn, L.A. and Jain, A.K.: Dimensionality
Reduction Using Genetic Algorithms. IEEE Transaction on Evolutionary Computation,
Vol. 4, No.2 (July 2000) 164–171
9.
Tseng, L.Y., Yang, S.B.: A Genetic Approach To The Automatic Clustering Problem.
Pattern Recognition 34 (2001) 415–424

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 332–343, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Coevolution and Linear Genetic Programming for
Visual Learning
Krzysztof Krawiec* and Bir Bhanu
Center for Research in Intelligent Systems
University of California, Riverside, CA 92521-0425, USA
{kkrawiec,bhanu}@cris.ucr.edu
Abstract. In this paper, a novel genetically-inspired visual learning method is
proposed. Given the training images, this general approach induces a
sophisticated feature-based recognition system, by using cooperative
coevolution and linear genetic programming for the procedural representation
of feature extraction agents. The paper describes the learning algorithm and
provides a firm rationale for its design. An extensive experimental evaluation,
on the demanding real-world task of object recognition in synthetic aperture
radar (SAR) imagery, shows the competitiveness of the proposed approach with
human-designed recognition systems.
1   Introduction
Most real-world learning tasks concerning visual information processing are
inherently complex. This complexity results not only from the large volume of data
that one usually needs to process, but also from its spatial nature, information
incompleteness, and, most of all, from the vast number of hypotheses that have to be
considered in the learning process and the ‘ruggedness’ of the fitness landscape.
Therefore, the design of a visual learning algorithm mostly consists in modeling its
capabilities so that it is effective in solving the problem. To induce useful hypotheses
on one hand and avoid overfitting to the training data on the other, some assumptions
have to be made, concerning training data and hypothesis representation, known as
inductive bias and representation bias, respectively. In visual learning, these biases
have to be augmented by an extra ‘visual bias’, i.e., knowledge related to the visual
nature of the information being subject to the learning process. A part of that is
general knowledge concerning vision (background knowledge, BK), for instance,
basic concepts like pixel proximity, edges, regions, primitive features, etc. However,
usually a more specific domain knowledge (DK) related to a particular
task/application (e.g., fingerprint identification, face recognition, etc.) is also
required.
Currently, most recognition methods make intense use of DK to attain a
competitive performance level. This is, however, a double-edged sword, as the more
DK the method uses, the more specific it becomes and the less general and
                                                          
* On a temporary leave from Institute of Computing Science,   	
 
Technology,  

Coevolution and Linear Genetic Programming for Visual Learning          333
transferable is the knowledge it acquires. The contribution of such over-specific
methods to the overall body of knowledge is questionable.
Therefore, in this paper, we propose a general-purpose visual learning method that
requires only BK and produces a complete recognition system that is able to classify
objects in images. To cope with the complexity of the recognition task, we break it
down into components. However, the ability to identify building blocks is a
necessary, but not a sufficient, precondition for a successful learning task. To enforce
learning in each identified component, we need an evaluation function that spans over
the space of all potential solutions and guides the learning process. Unfortunately,
when no a priori definition of module’s ‘desired output’ is available, this requirement
is hard to meet. This is why we propose to employ here cooperative coevolution [10],
as it does not require the explicit specification of objectives for each component.
2   Related Work and Contributions
No general methodology has been developed so far that effectively automates the
visual learning process. Several methods have been reported in the literature; they
include blackboard architecture, case-based reasoning, reinforcement learning, and
automatic acquisition of models, to mention the most predominant. The paradigm of
evolutionary computation (EC) has also found applications in image processing and
analysis. It has been found effective for its ability to perform global parallel search in
high-dimensional search spaces and to resist the local optima problem. However, in
most approaches the learning is limited to parameter optimization. Relatively few
results have been reported [5,8,13,14], that perform visual learning in the deep sense,
i.e., with a learner being able to synthesize and manipulate an entire recognition
system.
The major contribution of this paper is a general method that, given only a set of
training images, performs visual learning and yields a complete feature-based
recognition system. Its novelty consists mostly in (i) procedural representation of
features for recognition, (ii) utilization of coevolutionary computation for induction of
image representation, and (iii) a learning process that optimizes the image feature
definitions, prior to classifier induction.
3   Coevolutionary Construction of Feature Extraction Procedures
We pose visual learning as the search of the space of image representations (sets of
features). For this purpose, we propose to use cooperative coevolution (CC) [10],
which, besides being appealing from the theoretical viewpoint, has been reported to
yield interesting results in some experiments [15]. In CC, one maintains many
populations, with individuals in populations encoding only a part of the solution to
the problem. To undergo evaluation, individuals have to be (temporarily) combined
with individuals from the remaining populations to form an organism (solution). This
joint evaluation scheme forces the populations to cooperate. Except for this evaluation
step, other steps of evolutionary algorithm proceed in each population independently.

334         K. Krawiec and B. Bhanu
According to Wolpert’s ‘No Free Lunch’ theorem [17], the choice of this particular
search method is irrelevant, as the average performance of any metaheuristic search
over a set of all possible fitness functions is the same. In the real world, however, not
all fitness functions are equally probable. Most real-world problems are characterized
by some features that make them specific. The practical utility of a search/learning
algorithm depends, therefore, on its ability to detect and benefit from those features.
The high complexity and decomposable nature of the visual learning task are such
features. Cooperative coevolution seems to fit them well, as it provides the possibility
of breaking up a complex problem into components without specifying explicitly the
objectives for them. The manner in which the individuals from populations cooperate
emerges as the evolution proceeds. In our opinion, this makes CC especially
appealing to the problem of visual learning, where the overall object recognition task
is well defined, but there is no a priori knowledge about what should be expected at
intermediate stages of processing, or such knowledge requires an extra effort from the
designer.
In [3], we provide experimental evidence for the superiority of CC-based feature
construction over standard EC approach in the standard machine learning setting;
here, we extend this idea to visual learning. Following the feature-based recognition
paradigm, we split the object recognition process into two modules: feature extraction
and decision making. The algorithm learns from a finite training set of examples
(images) D in a supervised manner, i.e. requires D to be partitioned into finite number
of pairwise disjoint decision classes Di.
In the coevolutionary run, n populations cooperate in the task of building the
complete image representation, with each population responsible for evolving one
component. Therefore, the cooperation here may be characterized as taking place at
the feature level. In particular, each individual I from a given population encodes a
single feature extraction procedure. For clarity, details of this encoding are provided
in Section 4.
 	


1

i

n

I1
*

In
*










f(O,D) 
 
Y(X)
X∈D

D



Cfit


I1
*



 O
 Ii
Fig. 1. The evaluation of an individual Ii from i
th population.
The coevolutionary search proceeds in all populations independently, except for
the evaluation phase, shown in Fig. 1. To evaluate an individual Ij from population #j,
we first provide for the remaining part of the representation. For this purpose,

Coevolution and Linear Genetic Programming for Visual Learning          335
representatives I
*
i are selected from all the remaining populations ij. A representative
I
*
i  of i
th population is defined here in a way that has been reported to work best [15]: it
is the best individual w.r.t. the previous evaluation. In the first generation of
evolutionary run, since no prior evaluation data is given, it is a randomly chosen
individual.
Subsequently, Ij is temporarily combined with representatives of all the remaining
populations to form an organism
*
*
1
*
1
*
1
,
,
,
,
,
,
n
j
j
j
I
I
I
I
I
O


+
−
=
.
(1)
Then, the feature extraction procedures encoded by individuals from O are ‘run’ (see
Section 4) for all images X from the training set D. The feature values y computed by
them are concatenated, building the compound feature vector Y:
)
,
(
,
),
,
(
),
,
(
),
,
(
,
),
,
(
)
(
*
*
1
*
1
*
1
X
I
X
I
X
I
X
I
X
I
X
n
j
j
j
y
y
y
y
y
Y


+
−
=
.
(2)
Feature vectors Y(X), computed for all training images XD, together with the
images’ decision class labels constitute the dataset:
}
,
:
),
(
{
i
i
D
D
X
i
X
∀
∈
∀
Y
(3)
Finally, cross-validation, i.e. multiple train-and-test procedure is carried out on these
data. For the sake of speed, we use here a fast classifier Cfit that is usually much
simpler than the classifier used in the final recognition system. The resulting
predictive recognition ratio (see equation 4) becomes the evaluation of the organism
O, which is subsequently assigned as the fitness value to f ( ) the individual Ij,
concluding its evaluation process:
{
}
(
)
)
(
,
,
))
(
(
,
),
(
)
,
(
)
,
(
/
D
card
D
i
X
C
D
X
i
X
card
D
O
f
D
I
f
i
i
j
∀
=
∧
∈
∀
=
=
=
Y
Y
(4)
where card() denotes cardinality of a set. Using this evaluation procedure, the
coevolutionary search proceeds until some stopping criterion (usually considering
computation time) is met. The final outcome of the coevolutionary run is the best
found organism/representation O
*.
4   Representation of Feature Extraction Procedures
For representing the feature extraction procedures as individuals in the evolutionary
process, we adopt a variety of Linear Genetic Programming (LGP) [1], a hybrid of
genetic algorithms (GA) and genetic programming (GP). The individual’s genome is
a fixed-length string of bytes, representing a sequential program composed of
(possibly parameterized) basic operations that work on images and scalar data. This
representation combines advantages of both GP and GA, being both procedural and
more resistant to the destructive effect of crossover that may occur in ‘regular’ GP
[1].

336         K. Krawiec and B. Bhanu
A feature extraction procedure accepts an image X as input and yields a vector y of
scalar values as the result. Its operations are effectively calls to image processing and
feature extraction functions. They work on registers, and may use them for both input
as well as output arguments. Image registers store processed images, whereas real-
number registers keep intermediate scalar results features. Each image register has
single channel (grayscale), the same dimensions as the input image X, and maintains a
rectangular mask that, when used by an operation, limits the processing to its area. For
simplicity, the numbers of both types of registers are controlled by the same
parameter m.
Each chunk of four consecutive bytes in the genome encodes a single operation
with the following components:
(a)  operation code,
(b)  mask flag – decides whether the operation should be global (work on the entire
image) or local (limited to the mask),
(c)  mask dimensions (ignored if the mask flag is ‘off’),
(d)  arguments: references to registers to fetch input data and store the result.
	
	


	
 
 
 	

	
 


  


 


 	

 	

 	
	

	
	
	
	
	


	
	


	

	
morph_open(R1,R2)
	

	
r1
r2
rm

 	
!	"#"	I
$% &	'
	()
(	!	*	
		X 
+*				
"(#	!
R1
R2
Rm

,	#
yi(X), i=1,…,m
!(*"	!	*
!	-(	
!		
% &	
Fig. 2. Execution of LGP code contained in individual’s I genome (for a single image X).
Fig. 2 shows the execution at the moment of executing the following operation:
morphological opening (a), applied locally (b) to the mask of size 1414 (c) to the
image fetched from image register pointed by argument #1, and storing the result in
image register pointed by argument #2 (d). There are currently 70 operations
implemented in the system. They mostly consist of calls to functions from Intel Image
Processing and OpenCV libraries, and encompass image processing, mask-related
operations, feature extraction, and arithmetic and logic operations.

Coevolution and Linear Genetic Programming for Visual Learning          337
The processing of a single input image X  D by the LGP procedure encoded in an
individual I proceeds as follows (Fig. 2):
1. Initialization: Each of the m image registers is set to X. The masks of images are
set to the m most distinctive local features (here: bright ‘blobs’) found in the
image. Real-number registers are set to the center coordinates of corresponding
masks.
2. Execution: the operations encoded by I are carried out one by one, with
intermediate results stored in registers.
3. Interpretation: the scalar values yj(I,X), j=1,…,m, contained in the m real-value
registers are interpreted as the output yielded by I for image X.  The values are
gathered to form an individual’s output vector
)
,
(
,
),
,
(
)
,
(
1
X
I
y
X
I
y
X
I
m

=
y
,
(5)
that is subject to further processing described in Section 3.
5   Architecture of the Recognition System
The overall recognition system consists of:  (i) the best feature extraction procedures
O* constructed using the approach described in Sections 3 and 4, and (ii) classifiers
trained using those features.
We incorporate a multi-agent methodology that aims to compensate for the
suboptimal character of representations elaborated by the evolutionary process and
allows us to boost the overall performance.
 	
		nsub
 	
		2



X
 	
		1
		
C
	
	O*
Y(X)
C(Y(X))


	

Fig. 3. The top-level architecture of recognition system.
The basic prerequisite for the agents’ fusion to become beneficial is their
diversification. This may be ensured by using homogenous agents with different
parameter settings, homogenous agents with different training data (e.g., bagging [4]),
heterogeneous agents, etc. Here, the diversification is naturally provided by the
random nature of the genetic search. In particular, we run many genetic searches that
start from different initial states (initial populations). The best representation O
*
evolved in each run becomes a part of a single subsystem in the recognition system’s
architecture (see Fig. 3). Each subsystem has two major components: (i) a
representation O
*, and (ii) a classifier C trained using that representation. As this

338         K. Krawiec and B. Bhanu
classifier training is done once per subsystem, a more sophisticated classifier C may
be used here (as compared to the classifier Cfit used in the evaluation function).
The subsystems process the input image X independently and output recognition
decisions that are further aggregated by a simple majority voting procedure into the
final decision. The subsystems are therefore homogenous as far as the structure is
concerned; they only differ in the features extracted from the input image and the
decisions made. The number of subsystems nsub is a parameter set by the designer.
6   Experimental Results
The primary objective of the computational experiment is to test the scalability of the
approach with respect to the number of decision classes and its sensitivity to various
types of object distortions. As an experimental testbed, we choose the demanding task
of object recognition in synthetic aperture radar (SAR) images. There are several
difficulties that make recognition in this modality extremely hard:
 poor visibility of objects – usually only prominent scattering centers are visible,
 low persistence of features under rotation, and
 high levels of noise.
The data source is the MSTAR public database [12] containing real images of several
objects taken at different azimuths and at 1-foot spatial resolution. From the original
complex (2-channel) SAR images, we extract the magnitude component and crop it to
4848 pixels. No other form of preprocessing is applied.
 
	

 

Fig. 4. Selected objects and their SAR images used in the learning experiment.
The following parameter settings are used for each coevolutionary run: number of
subsystems nsub: 10; classifier Cfit used for feature set evaluation: decision tree inducer
C4.5 [11]; mutation operator: one-point, probability 0.1; crossover operator: one-
point, probability 1.0, cutting allowed at every point; selection operator: tournament
selection with tournament pool size = 5; number of registers (image and
numeric) m: 2; number of populations n: 4; genome length: 40 bytes (10 operations);

Coevolution and Linear Genetic Programming for Visual Learning          339
single population size: 200 individuals; time limit for evolutionary search: 4000
seconds (Pentium PC 1.4 GHz processor).
A compound classifier C is used to boost the recognition performance. In
particular, C implements the ‘1-vs.-all’ scheme, i.e. it is composed of l base
classifiers (where l is the number of decision classes), each of them working as a
binary (two-class) discriminator between a single decision class and all the remaining
classes. To aggregate their outputs, a simple decision rule is used that yields final
class assignment only if the base classifiers are consistent and indicate a single
decision class. With this strict rule, any inconsistency among the base classifiers (i.e.,
no class indicated or more than one class indicated) disables univocal decision and the
example remains unclassified (assigned to ‘No decision’ category).
The system’s performance is measured using different base classifiers (if not stated
otherwise, the classifier uses default parameter settings as specified in [16]):
 support vector machine with polynomial kernels of degree 3 (trained using
sequential minimal optimization algorithm [9] with complexity parameter set
to 10),
 nonlinear neural networks with sigmoidal units trained using backpropagation
algorithm with momentum,
 C4.5 decision tree inducer [11].
Scalability. To investigate the scalability of the proposed approach w.r.t. to the
problem size, we use several datasets with increasing numbers of decision classes for
a 15-deg. depression angle, starting from l=2 decision classes: BRDM2 and ZSU.
Consecutive problems are created by adding the decision classes up to l=8 in the
following order: T62, Zil131, a variant A04 of T72 (T72#A04 in short), 2S1,
BMP2#9563, and BTR70#C71.
For i
th decision class, its representation Di in the training data D consists of two
subsets of images sampled uniformly from the original MSTAR database with respect
to a 6-degree azimuth step. Training set D, therefore, always contains 2*(360/6)=120
images from each decision class, so its total size is 120*l. The corresponding test set T
contains all the remaining images (for a given object and elevation angle) from the
original MSTAR collection. In this way, the training and test sets are strictly disjoint.
Moreover, the learning task is well represented by the training set as far as the
azimuth is concerned. Therefore, there is no need for multiple train-and-test
procedures here and the results presented in the following all use this single particular
partitioning of MSTAR data.
Let nc, ne, and nu, denote respectively the numbers of test objects correctly
classified, erroneously classified, and unclassified by the recognition system.
Figure 5(a) presents the true positive rate, i.e. Ptp=nc/(nc+ne+nu), also known as
probability of correct identification (PCI), as a function of the number of decision
classes. It can be observed, that the scalability depends heavily on the base classifier,
and that SVM clearly outperforms its rivals. For this base classifier, as new decision
classes are added to the problem, the recognition performance gradually decreases.
The major drop-offs occur when T72 tank and 2S1 self-propelled gun (classes 5 and
6, respectively), are added to the training data; this is probably due to the fact that
these objects are visually similar to each other (e.g., both have gun turrets) and
significantly resemble the T62 tank (class 3). On the contrary, introducing

340         K. Krawiec and B. Bhanu
consecutive classes 7 and 8 (BMP2 and BTR60) did not affect the performance much;
more than this, an improvement of accuracy is even observable for class 7.
 
 

  


 
!
"#
$
%
&
'
(

 
)		


*	+
 


#
'
 #
 '
!#
!'
"##
#
#$
#&
#(
# 
"
,	+

*	+
	

	

	

	

	

	

Fig. 5. (a) Test set recognition ratio as a function of number of decision classes. (b) ROC
curves for different number of decision classes (base classifier: SVM).
Figure 5(b) shows the receiver operating characteristics (ROC) curves obtained, for
the recognition systems using SVM as a base classifier, by modifying the confidence
threshold that controls whether the classifier votes. The false positive rate is defined
here as Pfp=ne/(nc+ne+nu). Again, the results support our method: the curves do not
drop rapidly as the false positive rate decreases. Therefore, very high accuracy of
classification, i.e., nc/(nc+ne), may be obtained when accepting a reasonable rejection
rate nu/(nc+ne+nu). For instance, for 4 decision classes, when Pfp=0.008, Ptp=0.885 (see
marked point in Fig. 5(b)), and, therefore, rejection rate is 1-(Pfp+Ptp)=0.107, the
accuracy of classification equals 0.991.
Object variants. A desirable property of an object recognition system is its ability to
recognize different variants of the same object. This task may pose some difficulties,
as configurations of vehicles often vary significantly. To provide a comparison with
human-designed recognition system, we use the conditions of the experiment reported
in [2]. In particular, we synthesized recognition systems using:
 2 objects: BMP2#C21, T72#132,
 4 objects: BMP2#C21, T72#132, BTR70#C71, and ZSU23/4.
For both of these cases, the testing set includes two other variants of BMP2 (#9563
and #9566), and two other variants of T72 (#812 and #s7).
The results of the test set evaluation shown in the confusion matrices (Table 1)
suggest that, even when the recognized objects differ significantly from the models
provided in the training data, the approach is still able to maintain high performance.

Coevolution and Linear Genetic Programming for Visual Learning          341
Here the true positive rate Ptp equals 0.804 and 0.793, for 2- and 4-class systems,
respectively. For the cases where a decision can be made (83.3% and 89.2%,
respectively), the values of classification accuracy, 0.966 and 0.940, respectively, are
comparable to the forced recognition results of the human-designed recognition
algorithms reported in [2], which are 0.958 and 0.942, respectively. Note that in the
test, we have not used ‘confusers’, i.e. test images from different classes that those
present in the training set, as opposed to [2], where BRDM2 armored personnel
carrier has been used for that purpose.
Table 1. Confusion matrices for recognition of object variants.
BMP2
T72
No
BMP2
T72
BTR
ZSU
No
Object
Serial #
[#C21] [#132] decision [#C21] [#132] [#C71] [#d08] decision
BMP2
[#9563,9566]
295
18
78
293
27
27
1
43
T72
[#812,s7]
4
330
52
12
323
1
9
41
Test objects
2-class system
Predicted class 
4-class system
7   Conclusions
In this contribution, we provide experimental evidence for the possibility of
synthesizing, without or with little human intervention, a feature-based recognition
system which recognizes 3D objects at the performance level that can be comparable
to handcrafted solutions. Let us emphasize that these encouraging results are obtained
in the demanding field of SAR imagery, where the acquired images only roughly
depict the underlying 3D structure of the object.
There are several major factors that contribute to the overall high performance of
the approach. First of all, the paradigm of coevolution allows us to decompose the
task of representation (feature set) construction into several semi-independent,
cooperating subtasks. In this way, we exploit the inherent modularity of the learning
process, without the need of specifying explicit objectives for each developed feature
extraction procedure. Secondly, the approach manipulates LGP-encoded feature
extraction procedures, as opposed to most approaches which are usually limited to
learning meant as parameter optimization. This allows for learning sophisticated
features, which are novel and sometimes very different from expert’s intuition, as
may be seen from example shown in Figure 6. And thirdly, the fusion at feature and
decision level helps us to aggregate sometimes contradictory information sources and
build a recognition system that is comparable to human-designed system performance
with a bunch of simple components at hand.

342         K. Krawiec and B. Bhanu
Fig. 6. Processing carried out by one of the evolved procedures shown as a graph (small
rectangles in images depict masks; boxes: local operations; rounded boxes: global operations).
Acknowledgements. This research was supported by the grant F33615-99-C-1440.
The contents of the information do not necessarily reflect the position or policy of the
U. S. Government. The first author is supported by the Polish State Committee for
Scientific Research, research grant no. 8T11F 006 19. We would like to thank the
authors of software packages: ECJ [7] and WEKA [16] for making their software
publicly available.
References
1. 
Banzhaf, W., Nordic, P., Keller, R., Francine, F.: Genetic Programming. An Introduction.
On the automatic Evolution of Computer Programs and its Application. Morgan
Kaufmann, San Francisco, Calif. (1998)
2. 
Bhanu, B., Jones, G.: Increasing the discrimination of SAR recognition models. Optical
Engineering 12 (2002) 3298–3306
3. 
Bhanu, B. and Krawiec, K.: Coevolutionary construction of features for transformation of
representation in machine learning. Proc. Genetic and Evolutionary Computation
Conference (GECCO 2002). AAAI Press, New York (2002) 249–254
4. 
Breiman, L.: Bagging predictors. Machine Learning 24 (1996) 123–140
5. 
Draper, B., Hanson, A., Riseman, E.: Knowledge-Directed Vision: Control, Learning and
Integration. Proc. IEEE 84 (1996) 1625–1637
6. 
Krawiec, K.: On the Use of Pair wise Comparison of Hypotheses in Evolutionary Learning
Applied to Learning from Visual Examples. In: Perner, P. (ed.): Machine Learning and
Data Mining in Pattern Recognition. Lecture Notes in Artificial Intelligence, Vol. 2123.
Springer Verlag, Berlin (2001) 307–321.

Coevolution and Linear Genetic Programming for Visual Learning          343
7. 
Luke, S.: ECJ Evolutionary Computation System. http://www.cs.umd.edu/projects/plus/
ec/ecj/ (2002)
8. 
Peng, J., Bhanu, B.: Closed-Loop Object Recognition Using Reinforcement Learning.
IEEE Trans. on PAMI 20 (1998) 139–154
9. 
Platt, J.: Fast Training of Support Vector Machines using Sequential Minimal
Optimization. In: Schölkopf, B., Burges, C., Smola, A. (eds.): Advances in Kernel
Methods - Support Vector Learning. MIT Press, Cambridge, Mass. (1998)
10. Potter, M.A., De Jong, K.A.: Cooperative Coevolution: An Architecture for Evolving
Coadapted Subcomponents. Evolutionary Computation 8 (2000) 1–29
11. Quinlan, J.R.: C4.5: Programs for machine learning. Morgan Kaufmann, San Mateo, Calif.
(1992)
12. Ross, T., Worell, S., Velten, V., Mossing, J., Bryant, M.: Standard SAR ATR Evaluation
Experiments using the MSTAR Public Release Data Set. SPIE Proc.: Algorithms for
Synthetic Aperture Radar Imagery V, Vol. 3370, Orlando, FL (1998) 566–573
13. Segen, J.: GEST: A Learning Computer Vision System that Recognizes Hand Gestures. In:
Michalski, R.S., Tecuci, G., (eds.): Machine Learning. A Multistrategy Approach. Volume
IV. Morgan Kaufmann, San Francisco, Calif. (1994) 621–634
14. Teller, A., Veloso, M.: A Controlled Experiment:  Evolution for Learning Difficult Image
Classification. Proc. 7
th Portuguese Conference on Artificial Intelligence. Springer Verlag,
Berlin, Germany (1995) 165–176
15. Wiegand, R.P., Liles, W.C., De Jong, K.A.: An Empirical Analysis of Collaboration
Methods in Cooperative Coevolutionary Algorithms. Proc. Genetic and Evolutionary
Computation Conference (GECCO 2001). Morgan Kaufmann, San Francisco, Calif.
(2001) 1235–1242
16. Witten, I.H., Frank, E.: Data Mining: Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, San Francisco, Calif. (1999)
17. Wolpert, D., Macready, W.G.: No Free Lunch Theorems for Search. Tech. Report SFI-TR-
95-010, The Santa Fe Institute (1995)

Finite Population Models of Co-evolution and
Their Application to Haploidy versus Diploidy
Anthony M.L. Liekens, Huub M.M. ten Eikelder, and Peter A.J. Hilbers
Department of Biomedical Engineering
Technische Universiteit Eindhoven
P.O. Box 513, 5600MB Eindhoven, The Netherlands
{a.m.l.liekens, h.m.m.t.eikelder, p.a.j.hilbers}@tue.nl
Abstract. In order to study genetic algorithms in co-evolutionary en-
vironments, we construct a Markov model of co-evolution of populations
with ﬁxed, ﬁnite population sizes. In this combined Markov model, the
behavior toward the limit can be utilized to study the relative perfor-
mance of the algorithms. As an application of the model, we perform an
analysis of the relative performance of haploid versus diploid genetic al-
gorithms in the co-evolutionary setup, under several parameter settings.
Because of the use of Markov chains, this paper provides exact stochastic
results on the expected performance of haploid and diploid algorithms
in the proposed co-evolutionary model.
1
Introduction
Co-evolution of Genetic Algorithms (GA) denotes the simultaneous evolution
of two or more GAs with interdependent or coupled ﬁtness functions. In com-
petitive co-evolution, just like competition in nature, individuals of both algo-
rithms compete with each other to gather ﬁtness. In cooperative co-evolution,
individuals have to cooperate to achieve higher ﬁtness. These interactions have
previously been modeled in Evolutionary Game Theory (EGT), using replica-
tor dynamics and inﬁnite populations. Similar models have, for example, been
used to study equilibriums [2] and comparisons of selection methods [1]. Simula-
tions of competitive co-evolution have previously been used to evolve solutions
and strategies for small two-player games, i.e., in [3,4], sorting networks [5], or
competitive robotics [6].
In this paper, we provide the construction of a Markov model of co-evolution
of two GAs with ﬁnite population sizes. After this construction we calculate
the relative performances in such a setup, in which a haploid and diploid GA
co-evolve with each other.
Commonly, GAs are based on the haploid model of reproduction. In this
model, an individual is assumed to carry a single genotype to encode for its
phenotype. When two parents are selected for reproduction, recombination of
these two genotypes takes place to construct a child for the next generation.
Most higher order species in nature, however, have the characteristic of car-
rying two sets of alleles that both can encode for the individual’s phenotype.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 344–355, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Finite Population Models of Co-evolution
345
For each of the genes, two (possibly diﬀerent) alleles are thus present. A dom-
inance relation is deﬁned on each pair of alleles. In a heterozygous gene, i.e.,
in a gene with 2 diﬀerent alleles, this dominance relation deﬁnes which allele is
expressed. A dominance relation can be pure, such that either one of the alleles
is always expressed in heterozygous individuals, or it can be partial, such that
the result of phenotypic expression is a probability distribution over the alleles.
When two diploid parents are selected to reproduce, they produce haploid ga-
mete cells through meiosis, in which each parent’s genes are recombined. The
haploid gametes are then merged, or fertilized, to form a new diploid child.
In dynamic environments, diploid GAs are hypothesized to perform better
than haploid algorithms, since they can build up an implicit long time memory of
previously encountered solutions in the recessive parts of the populations’ allele
pool. These alleles are kept safe from harmful selection. Under the assumption
that co-evolution mimics a dynamic environment, we will test this hypothe-
sis with a small problem in this paper, using co-evolution as a special form of
dynamic optimization. The Markov model approach yields exact stochastic ex-
pectations of performance of haploid and diploid algorithms. Previous accounts
of research on the use of diploidy for dynamic optimization, and results of its
performance as compared with haploid algorithms, can be found in [5,7,8,9,10].
The methods used in these papers diﬀer from our approach in the fact that we
consider exact probability distributions whereas others perform simulation ex-
periments or equilibrium analyses of inﬁnite models. The stochastic method of
Markov models, as used in this paper, allows us to provide exact stochastic re-
sults and performance expectations, instead of empirical data which is, as we will
show later, subject to a large standard deviation. A similar model to the model
presented in this paper, discussing stochastic models for dynamic optimization
problems, is discussed in [11].
In this study, haploid and diploid populations face one another in co-
evolution, which creates a simulation of a comparable situation in the history of
life on Earth: The ﬁrst diploid organisms to appear on Earth had to face haploid
life forms in a competition for resources. The dynamics of the co-evolutionary
competitive games played by these prehistoric cells are similar to the models
presented in this paper. Correct interpretation of the results can give insights
whether the earliest diploid life forms were able to compete with haploid life
forms.
In this paper, co-evolution, of two competing populations and their govern-
ing GAs, is used as a “test bed” to test two algorithms’ relative performance in
dynamic environments. Indeed, since the ﬁtness of an individual in one of the co-
evolving populations is based on the conﬁguration of the opponent population,
the ﬁtness landscapes of both populations constantly change, thereby simulating
dynamic environments through both populations’ interdependent ﬁtness func-
tions. Note that the results can only be used to discuss the algorithms’ relative
performance since the dynamics of one algorithm is explicitly determined by the
other algorithm.

346
A.M.L. Liekens, H.M.M. ten Eikelder, and P.A.J. Hilbers
2
Models and Methods
In this section, we construct a ﬁnite population Markov model of co-evolution.
Two ﬁnite population Markov chains of simple genetic algorithms, based on
the simple GA as described by [12,13], are intertwined through interdependent
ﬁtness functions. A discussion of the resulting Markov chain’s behavior toward
the limit and the interpretation of the limit behavior is also provided.
2.1
Haploid and Diploid Reproduction Schemes
The following constructions are based on the deﬁnition of haploid and diploid
simple genetic algorithms with ﬁnite population sizes as described in [13].
Haploid Reproduction. Let ΩH be the space of binary bit strings with length
l. The bit string serves as a genotype with l loci, that each can hold the alleles
0 or 1. ΩH serves as the search space for the Haploid Simple Genetic Algorithm
(HSGA). Let PH be a haploid population, PH = {x0, x1, . . . , xrH−1}, a multi set
with xi ∈ΩH for 0 ≤i < rH, and rH = |PH| the population size. Let πH denote
the set of all possible populations PH of size rH.
Let fH : ΩH →R+ denote the ﬁtness function. Let ςfH : πH →ΩH represent
stochastic selection, proportional to ﬁtness function fH. Crossover is a genetic
operator that takes two parent individuals, and results in a new child individual
that shares properties of these parents. Mutation slightly changes the genotype
of an individual. Crossover and mutation are represented by the stochastic func-
tions χ : ΩH × ΩH →ΩH and µ : ΩH →ΩH respectively.
In a HSGA, a new generation of individuals is created through sexual repro-
duction of selected parents from the current population. The probability that a
haploid individual i ∈ΩH is generated from a population PH can be written
according to this process as
Pr [i is generated from PH] =
(1)
Pr [µ (χ (ςfH (PH) , ςfH (PH))) = i]
where it has been shown in [13] that the order of mutation and crossover may
be interchanged in equation (1).
Diploid Reproduction. In the Diploid Simple Genetic Algorithm (DSGA),
an individual consists of two haploid genomes. An individual of the diploid pop-
ulation is represented by a multi set of two instances of ΩH, e.g. {i, j} with
i, j ∈ΩH. The set of all possible diploid instances is denoted by ΩD, the search
space of the DSGA. A diploid population PD with population size rD is deﬁned
over ΩD, similar to the deﬁnition of a haploid population. Let πD denote the set
of possible populations.
Haploid selection, mutation and crossover are reused in the diploid algo-
rithm. Two more speciﬁc genetic operators must be deﬁned. δ : ΩD →ΩH

Finite Population Models of Co-evolution
347
is the dominance operator. A ﬁtness function fH deﬁned for the haploid algo-
rithm, can be reused in a ﬁtness function fD for the diploid algorithm with
fD({i, j}) = fH(δ({i, j})) for any {i, j} in ΩD. Another diploid-speciﬁc opera-
tor is fertilization, which merges two gametes (members of ΩH) into one diploid
individual: φ : ΩH × ΩH →ΩD. Throughout this paper we will assume that
φ(i, j) = {i, j} for all i, j in ΩH. Diploid reproduction can now be written as
Pr [{i, j} is generated from PD] =
(2)
Pr [φ (µ (χ (ςfD(PD))) , µ (χ (ςfD(PD)))) = {i, j}] .
2.2
Simple Genetic Algorithms
In the simple GA (SGA), a new population P ′ of ﬁxed size r over search space
Ωfor the next generation is built according to population P with
Pr [τ(P) = P ′] =
(3)
r!

i∈Ω(
j∈P ′[i=j])!

i∈P ′ Pr [i is generated from P]
where τ : π →π represents the stochastic construction of a new population
from and into population space π of the SGA, and P ′(i) denotes the number of
individuals i in P ′. Since the system to create a new generation P ′ only depends
on the previous state P, the SGA is said to be Markovian. The SGA can now be
written as a Markov chain with transition matrix T with TP ′P = Pr [τ(P) = P ′].
If mutation can map any individual to any other individual, all elements of T
become strictly positive, and T becomes irreducible and aperiodic. The limit
behavior of the Markov chain can then be studied by ﬁnding the eigenvector,
with corresponding eigenvalue 1, of T.
We will assume uniform crossover, bitwise mutation according to a mutation
probability µ, and selection proportional to ﬁtness throughout the paper.
This completes the formal construction of haploid and diploid simple genetic
algorithms. More details of this construction can be found in [13].
2.3
Co-evolution of Finite Population Models
Next, we consider the combined co-evolutionary process of two SGAs, respec-
tively deﬁned by population transitions τ1 and τ2, over population search spaces
π1 and π2. We assume that the population sizes of both algorithms are ﬁxed and
ﬁnite, and their generational transitions are executed at the same rate.
In order to make the representative GAs – and thus their ﬁtness functions –
interdependent, we need to override the ﬁtness evaluation f : Ω→R+ of any
one of the co-evolving GAs with fi : Ωi × πj →R+ where Ωi is the search space
of the GA, and πj is the population state space of the co-evolving GA. As such,
the ﬁtness function of an individual in one population becomes dependent on
the conﬁguration of the population of the co-evolving GA. Consequently, the

348
A.M.L. Liekens, H.M.M. ten Eikelder, and P.A.J. Hilbers
generation probabilities of equation (3) now also depend on the population of
the competing algorithm.
The state space πco of the resulting Markov chain of the co-evolutionary
algorithm is deﬁned as the Cartesian product of spaces π1 and π2, i.e., πco = π1×
π2. All (P, Q), with P ∈π1, Q ∈π2, are states of the co-evolutionary algorithm.
Generally, the transition τco : πco →πco in the co-evolutionary Markov chain of
two interdependent Markov chains is deﬁned by
Pr [τco((P, Q)) = (P ′, Q′)] = Pr [τ1(P) = P ′|Q] · Pr [τ2(Q) = Q′|P]
(4)
where populations P and Q are states of π1 and π2 respectively. The dependence
of τ1 and τ2 on Q and P respectively, gives way for the implementation of a
coupled ﬁtness function for either algorithm.
2.4
Limit Behavior
One can show that the combination of irreducible and aperiodic interdependent
Markov chains, as deﬁned above, does not generally result in an irreducible and
aperiodic Markov chain. Therefore, we cannot simply assume that the Markov
chain that deﬁnes the co-evolutionary process converges to a unique ﬁxed point.
We can, however, make the following assumptions: If mutation can map any
individual – in both of the co-evolving GAs – to any other individual in the
algorithm’s search space with a strictly positive probability, then all elements in
the transition matrices of both co-evolving Markov chains are always nonzero
and strictly positive. As a result from multiplying the transition probabilities in
equation (4), all transition probabilities of the co-evolutionary Markov chain are
thus strictly positive. This makes the combined Markov chain irreducible and
aperiodic, such that the limit behavior of the whole co-evolutionary process can
be studied by ﬁnding the unique eigenvector, with corresponding eigenvalue 1,
of the transition matrix as deﬁned by equation (4), due to the Perron-Frobenius
theorem [14].
2.5
Expected Performance
The eigenvector, with corresponding eigenvalue 1, of the co-evolutionary Markov
chain describes the ﬁxed point distribution over all possible states (P, Q) of the
Markov chain in the limit. As a result, toward the limit, the Markov chain
converges to the distribution that describes the overall mean behavior of the
co-evolutionary system. If a simulation is run that starts with an initial popu-
lation according to this distribution, the distribution over the states at all next
generations are also according to this ﬁxed point distribution. For each of the
states, we can compute the mean ﬁtness of the constituent populations of that
state. With this information, and the distribution over all states in the limit, we
can make a weighted mean to ﬁnd the mean ﬁtness of both algorithms in the
co-evolutionary system at hand.

Finite Population Models of Co-evolution
349
More formally, let T denote the |πco| × |πco| transition matrix of the co-
evolutionary system with transition probabilities
T(P ′,Q′),(P,Q) = Pr [τco((P, Q)) = (P ′, Q′)]
as deﬁned by equation (4). Let ξ denote the eigenvector, with corresponding
eigenvalue 1, of T. ξ denotes the distribution of states of the co-evolutionary
algorithm in the limit, with component ξ(P,Q) denoting the probability of ending
up in state (P, Q) ∈πco in the limit. If f1(P, Q) gives the mean ﬁtness of the
individuals in population P, given an opponent population Q, then
f1 =

(P,Q)∈πco
ξ(P,Q) · f1(P, Q)
with
f1(P, Q) =
1
|P|

i∈P
f1(i, Q),
(5)
gives the mean ﬁtness of the populations governing the dynamics of the ﬁrst
algorithm toward the limit, in relation to its co-evolving algorithm. Similarly,
the mean ﬁtness of the second algorithm can be computed. We use the mean
ﬁtness in the limit as an exact measure of performance of the algorithm, in
relation to the co-evolving algorithm. Equation (5) also gives the expected mean
ﬁtness of the co-evolving algorithms if simulations of the model are executed.
We will also calculate the variance and standard deviation in order to dis-
cuss the signiﬁcance of the exact results. The variance of the ﬁtness of the ﬁrst
algorithm, according to distribution ξ, is equal to
σ2
f1 =

(P,Q)∈πco
ξ(P,Q) ·

f1(P, Q) −f1
2 .
(6)
Similarly to the mean ﬁtness, the variance of the ﬁtness gives an expectation of
the variance for simulations of the model.
Given the parameters for ﬁtness determination, selection and reproduction
of both co-evolving GAs in the co-evolutionary system, we can now estimate
the mean ﬁtness, and discuss the performance of both genetic algorithms, in the
context of their competitors’ performance.
3
Application
3.1
Competitive Game: Matching Pennies
In order to construct interdependent ﬁtness functions, we can borrow ideas of
competitive games from Evolutionary Game Theory (EGT, overviews can be
found in [15,16]). EGT studies the dynamics and equilibriums of games played
by populations of players. The strategies players employ in the games determine
their interdependent ﬁtness.
A common model to study the dynamics – of frequencies of strategies adopted
by the populations – is based upon replicator dynamics. This model makes a
couple of assumptions, some of which will be discarded in our model. Replicator

350
A.M.L. Liekens, H.M.M. ten Eikelder, and P.A.J. Hilbers
dynamics assumes inﬁnite populations, asexual reproduction, complete mixing,
i.e., all players are equally likely to interact in the game, and strategies breed
true, i.e., strategies are transmitted to oﬀspring proportionally to the payoﬀ
achieved. In our ﬁnite population model, where two GAs compete against each
other, we maintain the assumption that strategies breed true. We also main-
tain complete mixing, although the stochastic model also represents incomplete
mixing with randomly chosen opponent strategies. We now consider ﬁnite ﬁxed
population sizes with variation and sexual reproduction of strategies.
In the scope of our application, we focus on a family of 2 × 2 games called
“matching pennies.” Consider the payoﬀmatrices for the game in Table 1. Each
of the two players in the game either calls ‘heads’ or ‘tails.’ Depending on the
players’ calls and their representative values in the payoﬀmatrices, the players
receive a payoﬀ. More speciﬁcally, the ﬁrst player receives payoﬀ1−L if the calls
match, and L otherwise. The second player receives 1 minus the ﬁrst player’s
payoﬀ. If L ranges between 0 and 0.5, the ﬁrst player’s goal therefore is to call
the same as the second player, whose goal in turn is to do the inverse. Hence the
notion of competition in the game.
Table 1. Payoﬀmatrices of the matching pennies game. One population uses payoﬀ
matrix f1, where the other players use payoﬀmatrix f2. Parameter L denotes the payoﬀ
received when the player loses the game, and can range from 0 to 0.5
f1
heads tails
heads 1 −L
L
tails
L
1 −L
f2
heads tails
heads
L
1 −L
tails 1 −L
L
Let a population of players denote a ﬁnite sized population consisting of in-
dividuals who either call ‘heads’ or ‘tails.’ In our co-evolutionary setup, two GAs
evolving such populations P and Q are put against one another. The ﬁtnesses
of individuals in population P and Q are based on f1 and f2, from Table 1,
respectively. We use complete mixing to determine the ﬁtness of each individual
in either of the populations: Let pheads denote the proportion of individuals in
population P who call ‘heads,’ and qheads the proportion of individuals in Q
to call ‘heads.’ Deﬁne ptails and qtails similarly for the proportion of ‘tails’ in
the populations. The ﬁtness of an individual i of population P, regarding the
constituent strategies of population Q, can now be deﬁned as
f1(i, Q) =

qheads · (1 −L) + qtails · L
if i calls ‘heads’
qtails · (1 −L) + qheads · L
if i calls ‘tails’
(7)
and that of an individual j in population Q as
f2(j, P) =

pheads · L + ptails · (1 −L)
if j calls ‘heads’
ptails · L + pheads · (1 −L)
if j calls ‘tails’
(8)

Finite Population Models of Co-evolution
351
It can easily be veriﬁed that the mean ﬁtness of population P always equals 1
minus the mean ﬁtness of population Q, i.e., f1(P, Q) = 1 −f2(Q, P). Similarly,
the mean ﬁtness of both algorithms sum up to 1, with f1 = 1 −f2, c.f. equation
(5). If we assume 0 ≤L < 0.5, then there exists a unique Nash equilibrium of
this game, where both populations call ‘heads’ or ‘tails,’ each with probability
0.5. In this equilibrium, both populations receive a mean ﬁtness of 0.5. No player
can beneﬁt by changing her strategy while the other players keep their strategies
unchanged. Any deviation from this indicates that one algorithm relatively per-
forms better at the co-evolutionary task at hand than the other. As we want to
compare the performance of algorithms in a competitive co-evolutionary setup,
this is a viable null hypothesis.
3.2
Haploid versus Diploid
For the matching pennies game, we construct a co-evolutionary Markov chain in
which a haploid and diploid GA compete with each other. With this construc-
tion, and their transition matrices, we can determine the performance of both
algorithms according to the limit behavior of the Markov chain. Depending on
the results, either algorithm can be elected as a relatively better algorithm.
Let the length of binary strings in both algorithms be l = 1. This is referred
to as the single locus, two allele problem, a common, yet small, setup in popu-
lation genetics. An individual with phenotype 0 calls ‘heads,’ and ‘tails’ if the
phenotype is 1. Note that uniform crossover will not recombine genes since there
is only one locus, but will rather select one of both parent gametes.
Let πco be the search space of the co-evolutionary system, deﬁned by the
Cartesian product of the haploid populations’ search space πH and diploid pop-
ulations πD, such that πco = πH × πD. Depending on a ﬁxed population size r
for both competing algorithms, |πco| = ((r + 2)(r + 1)2)/2 denotes the size of
the co-evolutionary state space.
For any state (P, Q) ∈πco, let equations (7) and (8) be the respective ﬁtness
functions for the individuals in the haploid and diploid algorithms. Since we
want to compare the algorithms’ performance under comparable conditions, both
populations are assumed to have the same parameters for recombination and
mutation.
3.3
Limit Behavior and Mean Fitness
According to the deﬁnition of the co-evolutionary system in equation (4), the
transition matrix for a given set of parameters can be calculated. The eigen-
vector, with corresponding eigenvalue 1, of this transition matrix can be found
through iterated multiplication of the transition matrix with an initially dis-
tributed stochastic vector. From the resulting eigenvector we can ﬁnd the mean
ﬁtness of the co-evolutionary GAs toward the limit. These means are discussed
in the following sections.
We split the presentation of the limit behavior results into two separate
sections. In the ﬁrst section, we discuss the results given the assumption of pure

352
A.M.L. Liekens, H.M.M. ten Eikelder, and P.A.J. Hilbers
dominance, i.e., one of both alleles, either 0 or 1 is strictly dominant over the
other allele. In the second part, we discuss the results in the case of partial
dominance. In this setting, the phenotype of the diploid heterozygous genotype
{0, 1} is deﬁned by a probability distribution over 0 and 1.
Pure dominance. Let 1 be the dominant allele, and 0 the recessive allele
in diploid heterozygous individuals. This implies that diploid individuals with
genotype {0, 1} have phenotype 1 1.
Figure 1 shows the mean ﬁtness of the haploid algorithm, which is derived
from the co-evolutionary systems’ limit behavior, using equation (5). The pro-
portion of parameter settings for which diploidy performs better, increases as
the population size of the algorithms becomes bigger.
Fig. 1. Exact mean ﬁtness of the haploid GA in the co-evolutionary system, for variable
mutation rate µ and payoﬀparameter L. The mean ﬁtness of the diploid algorithm
always equals 1 minus the mean ﬁtness of the haploid algorithm. Population size of
both algorithms is ﬁxed to 5 in (a) and 15 in (b). The mesh is colored light as the mean
ﬁtness is below 0.4975, i.e. when the diploid algorithm performs better, and dark as
the mean ﬁtness is over 0.5025, i.e. for parameters where haploidy performs better.
In our computations, we found a fairly large standard deviation near µ = 0
and L = 0. The standard deviation goes to zero as either of the parameters go to
0.5. We discuss the source of this fairly large standard deviation in section 3.4.
Because of the large standard deviation, it is very hard to obtain these results
with empirical runs of the model. However, it is hard to compute the exact limit
behavior of large population systems, since this implies that we need to ﬁnd the
eigenvector of a matrix with O(r6) elements for population size r.
Partial dominance. Instead of using a pure dominance scheme in the diploid
GA, we can also assign a partial dominance scheme to the dominance operator. In
1 If we would choose 0 as the dominant allele instead of 1, the co-evolutionary sys-
tem would yield the exact same performance results, because of symmetries in the
matching pennies game. The same holds for exchanging ﬁtness functions f1 and f2.

Finite Population Models of Co-evolution
353
this dominance scheme, the heterozygous genotype {0, 1} has phenotype 0 with
probability h, and phenotype 1 with probability 1−h. h is called the dominance
degree or coeﬃcient. The dominance degree is the measure of dominance of the
recessive allele in the case of heterozygosity. Since our model is stochastic, we
could also state that the ﬁtness of an heterozygous individual is an intermediate
of the ﬁtnesses of both homozygous phenotypes.
The performance results are summarized in Figure 2. The ﬁgures show signif-
icantly better performance results for the diploid algorithm under small muta-
tion and high selection pressure (small L), in relation to the haploid algorithm.
Indeed, if we consider partial dominance instead of pure dominance, the memo-
rized strategies in the recessive alleles of a partial dominant diploid population
are tested against the environment, even in heterozygous individuals. The fact
that this could lead to lower ﬁtnesses in heterozygous individuals because of in-
terpolation of high and low ﬁtness, does not restrict the diploid algorithm from
obtaining a higher mean ﬁtness in the co-evolutionary algorithm. The standard
deviation is smaller than in the pure dominance case. This is explained in section
3.4.
Fig. 2.
Mean ﬁtness in the limit of the haploid algorithm similar to Figure 1 for
diﬀerent dominance coeﬃcients, with r = 15. Figure (a) applies dominance degree
h = 0.5 and (b) has dominance degree h = 0.01. Figure 1 applies dominance degree
h = 0
3.4
Source of High Variance
In order to ﬁnd where the high variance originates, we analyze the distribution
of ﬁtness at the ﬁxed point. Dissecting the stable ﬁxed point shows that there
are a small number of states with high probability, and many other states with a
small probability. More speciﬁcally, of these states with a high probability, about
half of them have an extremely high mean ﬁtness for one algorithm, where the
other half have an extremely low mean ﬁtness. This explains the high variance
in the ﬁtness distribution. If we would run a simulation of the model, we would

354
A.M.L. Liekens, H.M.M. ten Eikelder, and P.A.J. Hilbers
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
0.3
fitness
distribution
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
0.3
fitness
distribution
(a)
(b)
Fig. 3. Histogram showing the distribution of ﬁtness of the haploid genetic algorithm,
in the limit. Both ﬁgures have parameters r = 10, µ = 0.01, L = 0. Figure (a) shows the
distribution for h = 0 and h = 0.5 for (b). f1 = 0.4768 and σf1 = 0.4528 in histogram
(a) and f1 = 0.3699 and σf1 = 0.3715 in (b)
see that the algorithm alternately visits high and low ﬁtness states, and switches
relatively fast between these sets of states.
Figure 3 shows that, toward the limit, the mean ﬁtness largely depends on
states with both extremely low and high ﬁtnesses, which corresponds with the
high standard deviation. Note that the standard deviation is smaller in the case
of a higher dominance degree. This is also due to average ﬁtnesses being smeared
out in heterozygous individuals because of the higher dominance degree. The
relative diﬀerence between frequencies of extremely low and high ﬁtnesses also
results in a lower variance, as the dominance degree increases.
4
Discussion
This paper shows how a co-evolutionary model of two GAs with ﬁnite population
size can be constructed. We also provide ways to measure and discuss the relative
performance of the algorithms at hand. Because of the use of Markov chains,
exact stochastic results can be computed.
The analyses presented in the application of this paper show that, given the
matching pennies game, and if pure dominance is assumed, the results are only
in favor of diploidy in case of speciﬁc parameter settings. Even then, the results
are not signiﬁcant and subject to a large standard deviation. A diploid GA with
partial dominance and a strictly positive dominance degree can outperform a
haploid GA, if similar conditions hold for both algorithms. These results are ex-
pressed best under low mutation pressure and high selection pressure, i.e., when
a deleterious mutation has an almost lethal eﬀect on the individual. Diploidy
performs relatively better as the population size increases.
Based on these results, we suggest that further research should be under-
taken on the usage of diploidy in co-evolutionary GAs. This paper studies a

Finite Population Models of Co-evolution
355
small problem and small search spaces. Empirical evidence might prove to be
a useful tool in studying complexer problems, or larger populations. Scaled up
versions – of small situations which can be analyzed exactly – could be used
as empirical evidence to support exact predictions. Low signiﬁcance and high
standard deviations might prove that the study of relative performance of GAs
in competitive co-evolutionary situations is, however, empirically hard.
References
1. S. G. Ficici, O. Melnik, and J. B. Pollack. A game-theoretic investigation of selec-
tion methods used in evolutionary algorithms. In Proceedings of the 2000 Congress
on Evolutionary Computation, 2000.
2. S. G. Ficici and J. B. Pollack. A game-theoretic approach to the simple coevolu-
tionary algorithm. In Parallel Problem Solving from Nature VI, 2000.
3. C. D. Rosin. Coevolutionary search among adversaries. PhD thesis, San Diego,
CA, 1997.
4. A. Lubberts and R. Miikkulainen. Co-evolving a go-playing neural network. In
Proceedings of the Genetic and Evolutionary Computation Conference (GECCO),
pages 14–19, 2001.
5. D. Hillis. Co-evolving parasites improve simulated evolution as an optimization
procedure. In Artiﬁcial Life II. Addison Wesley, 1992.
6. D. Floreano, F. Mondada, and S. Nolﬁ. Co-evolution and ontogenetic change in
competing robots. In Robotics and Autonomous Systems, 1999.
7. D. E Goldberg and R. E. Smith. Nonstationary function optimization using genetic
algorithms with dominance and diploidy. In Second International Conference on
Genetic Algorithms, pages 59–68, 1987.
8. J. Lewis, E. Hart, and G. Ritchie. A comparison of dominance mechanisms and
simple mutation on non-stationary problems. In Parallel Problem Solving from
Nature V, pages 139–148, 1998.
9. K. P. Ng and K. C. Wong. A new diploid scheme and dominance change mechanism
for non-stationary function optimization. In 6th Int. Conf. on Genetic Algorithms,
pages 159–166, 1995.
10. R. E. Smith and D. E. Goldberg.
Diploidy and dominance in artiﬁcial genetic
search. Complex Systems, 6:251–285, 1992.
11. A. M. L. Liekens, H. M. M. ten Eikelder, and P. A. J. Hilbers. Finite population
models of dynamic optimization with alternating ﬁtness functions.
In GECCO
Workshop on Evolutionary Algorithms for Dynamic Optimization Problems, 2003.
12. A. E. Nix and M. D. Vose.
Modelling genetic algorithms with markov chains.
Annals of Mathematics and Artiﬁcial Intelligence, pages 79–88, 1992.
13. A. M. L. Liekens, H. M. M. ten Eikelder, and P. A. J. Hilbers.
Modeling and
simulating diploid simple genetic algorithms. In Foundations of Genetic Algorithms
VII, 2003.
14. D. P. Bertsekas and J. N. Tsitsiklis.
Parallel and Distributed Computation.
Prentice-Hall, 1989.
15. J. W. Weibull. Evolutionary Game Theory. MIT Press, Cambridge, Massachusetts,
1995.
16. J. Hofbauer and K. Sigmund.
Evolutionary Games and Population Dynamics.
Cambridge University Press, 1998.

Evolving Keepaway Soccer Players through Task
Decomposition
Shimon Whiteson, Nate Kohl, Risto Miikkulainen, and Peter Stone
Department of Computer Sciences
The University of Texas at Austin
1 University Station C0500
Austin, Texas 78712-1188
{shimon,nate,risto,pstone}@cs.utexas.edu
http://www.cs.utexas.edu/{˜shimon,nate,risto,pstone}
Abstract. In some complex control tasks, learning a direct mapping
from an agent’s sensors to its actuators is very diﬃcult. For such tasks,
decomposing the problem into more manageable components can make
learning feasible. In this paper, we provide a task decomposition, in the
form of a decision tree, for one such task. We investigate two diﬀerent
methods of learning the resulting subtasks. The ﬁrst approach, layered
learning, trains each component sequentially in its own training environ-
ment, aggressively constraining the search. The second approach, coevo-
lution, learns all the subtasks simultaneously from the same experiences
and puts few restrictions on the learning algorithm. We empirically com-
pare these two training methodologies using neuro-evolution, a machine
learning algorithm that evolves neural networks. Our experiments, con-
ducted in the domain of simulated robotic soccer keepaway, indicate
that neuro-evolution can learn eﬀective behaviors and that the less con-
strained coevolutionary approach outperforms the sequential approach.
These results provide new evidence of coevolution’s utility and suggest
that solution spaces should not be over-constrained when supplementing
the learning of complex tasks with human knowledge.
1
Introduction
One of the goals of machine learning algorithms is to facilitate the discovery
of novel solutions to problems, particularly those that might be unforeseen by
human problem-solvers. As such, there is a certain appeal to “tabula rasa learn-
ing,” in which the algorithms are turned loose on learning tasks with no (or
minimal) guidance from humans. However, the complexity of tasks that can be
successfully addressed with tabula rasa learning given current machine learning
technology is limited.
When using machine learning to address tasks that are beyond this com-
plexity limit, some form of human knowledge must be injected. This knowledge
simpliﬁes the learning task by constraining the space of solutions that must be
considered. Ideally, the constraints simply enable the learning algorithm to ﬁnd
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 356–368, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Evolving Keepaway Soccer Players through Task Decomposition
357
the best solutions more quickly. However there is also the risk of eliminating the
best solutions from the search space entirely.
In this paper, we consider a multi-agent control task that, given current
methods, seems infeasible to learn via a tabula rasa approach. Thus, we provide
some structure via a task decomposition in the form of a decision tree. Rather
than learning the entire task from sensors to actuators, the agents now learn a
small number of subtasks that are combined in a predetermined way.
Providing the decision tree then raises the question of how training should
proceed. For example, 1) the subtasks could be learned sequentially, each in its
own training environment, thereby adding additional constraints to the solution
space. On the other hand, 2) the subtasks could be learned simultaneously from
the same experiences. The latter methodology, which can be considered coevo-
lution of the subtasks, does not place any further restrictions on the learning
algorithms beyond the decomposition itself.
In this paper, we empirically compare these two training methodologies us-
ing neuro-evolution, a machine learning algorithm that evolves neural networks.
We attempt to learn agent controllers for a particular domain, namely keepaway
in simulated robotic soccer. Our results indicate that neuro-evolution can learn
eﬀective keepaway behavior, though constraining the task beyond the tabula
rasa approach proves necessary. We also ﬁnd that the less constrained coevolu-
tionary approach to training the subtasks outperforms the sequential approach.
These results provide new evidence of coevolution’s utility and suggest that so-
lution spaces should not be over-constrained when supplementing the learning
of complex tasks with human knowledge.
The remainder of the paper is organized as follows. Section 2 introduces the
keepaway task as well as the general neuro-evolution methodology. Section 3 fully
speciﬁes the diﬀerent approaches that we compare in this paper. Detailed empir-
ical results are presented in Section 4 and are evaluated in Section 5. Section 6
concludes and discusses future work.
2
Background
This section describes simulated robotic soccer keepaway, the domain used for all
experiments reported in this paper. We also review the fundamentals of neuro-
evolution, the general machine learning algorithm used throughout.
2.1
Keepaway
The experiments reported in this paper are all in a keepaway subtask of robotic
soccer [15]. In keepaway, one team of agents, the keepers, attempts to maintain
possession of the ball while the other team, the takers, tries to get it, all within
a ﬁxed region. Keepaway has been used as a testbed domain for several previous
machine learning studies. For example, Stone and Sutton implemented keepaway
in the RoboCup soccer simulator [14]. They hand-coded low-level behaviors and
applied learning, via the Sarsa(λ) method, only to the high-level decision of when

358
S. Whiteson et al.
and where to pass. Di Pietro et al. took a similar approach, though they used
genetic algorithms and a more elaborate high-level strategy [8]. Machine learning
was applied more comprehensively in a study that used genetic programming,
though in a simpler grid-based environment [6].
We implement the keepaway task within the SoccerBots environment [1].
SoccerBots is a simulation of the dynamics and dimensions of a regulation game
in the RoboCup small-size robot league [13], in which two teams of robots ma-
neuver a golf ball on a ﬁeld built on a standard ping-pong table. SoccerBots is
smaller in scale and less complex than the RoboCup simulator [7], but it runs ap-
proximately an order of magnitude faster, making it a more convenient platform
for machine learning research.
To set up keepaway in SoccerBots, we increase the size of the ﬁeld to give
the agents enough room to maneuver. To mark the perimeter of the game, we
add a large bounding circle around the center of the ﬁeld. Figure 1 shows how a
game of keepaway is initialized. Three keepers are placed just inside this circle
at points equidistant from each other. We place a single taker in the center of
the ﬁeld and place the ball in front of a randomly selected keeper.
Keepers
Taker
Ball
K
K
K
K
T
T
Fig. 1. A game of keepaway after
initialization. The keepers try to
complete as many passes as possi-
ble while preventing the ball from
going out of bounds and the taker
from touching it.
After initialization, an episode of keep-
away proceeds as follows. The keepers re-
ceive one point for every pass completed.
The episode ends when the taker touches
the ball or the ball exits the bounding circle.
The keepers and the taker are permitted to
go outside the bounding circle.
In this paper, we evolve a controller for
the keepers, while the taker is controlled
by a ﬁxed intercepting behavior. The keep-
away task requires complex behavior that
integrates sensory input about teammates,
the opponent, and the ball. The agents must
make high-level decisions about the best
course of action and develop the precise con-
trol necessary to implement those decisions.
Hence, it forms a challenging testbed for ma-
chine learning research.
2.2
Neuro-evolution
We train a team of keepaway players using neuro-evolution, a machine learning
technique that uses genetic algorithms to train neural networks [11]. In its sim-
plest form, neuro-evolution strings the weights of a neural network together to
form an individual genome. Next, it evolves a population of such genomes by
evaluating each one in the task and selectively reproducing the ﬁttest individuals
through crossover and mutation.

Evolving Keepaway Soccer Players through Task Decomposition
359
Sub−Populations
A Complete Network
Neurons
Fig. 2. The Enforced Sub-Populations Method (ESP). The population of neurons is
segregated into sub-populations, shown here as clusters of grey circles. One neuron,
shown in black, is selected from each sub-population. Each neuron consists of all the
weights connecting a given hidden node to the input and output nodes, shown as white
circles. The selected neurons together form a complete network which is then evaluated
in the task.
The Enforced Sub-Populations Method (ESP) [4] is a more advanced neuro-
evolution technique. Instead of evolving complete networks, it evolves sub-pop-
ulations of neurons. ESP creates one sub-population for each hidden node of
the fully connected two-layer feed-forward networks it evolves. Each neuron is
itself a genome which records the weights going into and coming out of the
given hidden node. As Figure 2 illustrates, ESP forms networks by selecting one
neuron from each sub-population to form the hidden layer of a neural network,
which it evaluates in the task. The ﬁtness is then passed back equally to all the
neurons that participated in the network. Each sub-population tends to converge
to a role that maximizes the ﬁtness of the networks in which it appears. ESP
is more eﬃcient than simple neuro-evolution because it decomposes a diﬃcult
problem (ﬁnding a highly ﬁt network) into smaller subproblems (ﬁnding highly
ﬁt neurons).
In several benchmark sequential decision tasks, ESP outperformed other
neuro-evolution algorithms as well as several reinforcement learning methods [2,
3,4]. ESP is a promising choice for the keepaway task because the basic skills
required in keepaway are similar to those at which ESP has excelled before.
3
Method
The goals of this study are 1) to verify that neuro-evolution can learn eﬀective
keepaway behavior, 2) to show that decomposing the task is more eﬀective than
tabula rasa learning, and 3) to determine whether coevolving the component
tasks can be more eﬀective than learning them sequentially.
Unlike soccer, in which a strong team will have forwards and defenders spe-
cialized for diﬀerent roles, keepaway is symmetric and can be played eﬀectively
with homogeneous teams. Therefore, in all these approaches, we develop one
controller to be used by all three keeper agents. Consequently, all the agents
have the same set of behaviors and the same rules governing when to use them,

360
S. Whiteson et al.
though they are often using diﬀerent behaviors at any time. Having identical
agents makes learning easier, since each agent learns from the experiences of its
teammates as well as its own. In the remainder of this section, we describe the
three diﬀerent methods that we consider for training these agents.
3.1
Tabula Rasa Learning
In the tabula rasa approach, we want our learning method to master the task
with minimal human guidance. In keepaway, we can do this by training a sin-
gle “monolithic” network. Such a network attempts to learn a direct mapping
from the agent’s sensors to its actuators. As designers, we need only specify the
network’s architecture (i.e. the inputs, hidden units, outputs, and their connec-
tivity) and neuro-evolution does the rest. The simplicity of such an approach
is appealing though, in diﬃcult tasks like keepaway, learning a direct mapping
may be beyond the ability of our training methods, if not simply beyond the
representational scope of the network.
To implement this monolithic approach with ESP, we train a fully connected
two-layer feed-forward network with nine inputs, four hidden nodes, and two out-
puts, as illustrated in Figure 3. This network structure was determined, through
experimentation, to be the most eﬀective. Eight of the inputs specify the posi-
tions of four crucial objects on the ﬁeld: the agent’s two teammates, the taker,
and the ball. The ninth input represents the distance of the ball from the ﬁeld’s
bounding circle. The inputs to this network and all those considered in this pa-
per are represented in polar coordinates relative to the agent. The four hidden
nodes allow the network to learn a compacted representation of its inputs. The
network’s two outputs control the agent’s movement on the ﬁeld: one alters its
heading, the other its speed. All runs use sub-populations of size 100.
r
Taker
Ball
Taker
r
Teammate2
Teammate1
r
Teammate1
r
Ball
Speed
Heading
edge
Teammate2
Distance
Fig. 3. The monolithic net-
work for controlling keepers.
White circles indicate inputs
and outputs while black cir-
cles indicate hidden nodes.
Since learning a robust keepaway controller di-
rectly is so challenging, we facilitate the process
through incremental evolution. In incremental evo-
lution, complex behaviors are learned gradually,
beginning with easy tasks and advancing through
successively more challenging ones. Gomez and Mi-
ikkulainen showed that this method can learn more
eﬀective and more general behavior than direct
evolution in several dynamic control tasks, includ-
ing prey capture [2] and non-Markovian double
pole-balancing [3].
We apply incremental evolution to keepaway by
changing the taker’s speed. When evolution begins,
the taker can move only 10% as quickly as the keep-
ers. We evaluate each network in 20 games of keep-
away and sum its scores (numbers of completed
passes) to obtain its ﬁtness. When the population’s
average ﬁtness exceeds 50 (2.5 completed passes per episode), the taker’s speed

Evolving Keepaway Soccer Players through Task Decomposition
361
is incremented by 5%. This process continues until the taker is moving at full
speed or the population’s ﬁtness has plateaued.
3.2
Learning with Task Decomposition
If learning a monolithic network proves infeasible, we can make the problem
easier by decomposing it into pieces. Such task decomposition is a powerful,
general principle in artiﬁcial intelligence that has been used successfully with
machine learning in the full robotic soccer task [12]. In the keepaway task, we
can replace the monolithic network with several smaller networks: one to pass
the ball, another to receive passes, etc.
Near Ball?
Teammate #1 Safer?
Pass To
Teammate #1
Pass To
Teammate #2
Passed To?
Intercept
Get Open
Yes
No
Yes
No
No
Yes
Fig. 4. A decision tree for controlling keepers in the keepaway task. The behavior at
each of the leaves is learned through neuro-evolution. A network is also evolved to
decide which teammate the agent should pass to.
To implement this decomposition, we developed a decision tree, shown in
Figure 4, for controlling each keeper. If the agent is near the ball, it kicks to
the teammate that is more likely to successfully receive a pass. If it is not near
the ball, the agent tries to get open for a pass unless a teammate announces its
intention to pass to it, in which case it tries to receive the pass by intercepting
the ball. The decision tree eﬀectively provides some structure (based on human
knowledge of the task) to the space of policies that can be explored by the
learners.
To implement this decision tree, four diﬀerent networks must be trained.
The networks, illustrated in Figure 5, are described in detail below. As in the
monolithic approach, these network structures were determined, through exper-
imentation, to be the most eﬀective.
Intercept: The goal of this network is to get the agent to the ball as quickly as
possible. The obvious strategy, running directly towards the ball, is optimal
only if the ball is not moving. When the ball has velocity, an ideal interceptor
must anticipate where the ball is going. The network has four inputs: two
for the ball’s current position and two for the ball’s current velocity. It has

362
S. Whiteson et al.
Distanceedge
Taker
r
Taker
Ball
r
Ball
Get Open
Heading
Speed
Target
Angle
r
Ball
Ball
Speed
Heading
r
Taker
Ball
r
Ball
Taker
Intercept
Pass
r
Pass Evalulate
Confidence
Teammate
Teammate
Heading
Speed
r
Ball
Ball
r
Velocity
Velocity
Ball
Ball
Fig. 5. The four networks used to implement the decision tree shown in Figure 4.
White circles indicate inputs and outputs while black circles indicate hidden nodes.
two hidden nodes and two outputs, which control the agent’s heading and
speed.
Pass: The pass network is designed to kick the ball away from the agent at a
speciﬁed angle. Passing is diﬃcult because an agent cannot directly specify
what direction it wants the ball to go. Instead, the angle of the kick depends
on the agent’s position relative to the ball. Hence, kicking well requires a
precise “wind-up” to approach the ball at the correct speed from the correct
angle. The pass network has three inputs: two for the ball’s current position
and one for the target angle. It has two hidden nodes and two outputs, which
control the agent’s heading and speed.
Pass Evaluate: Unlike the other networks, which correspond to behaviors at
the leaves of the decision tree, the pass evaluator implements a branch of
the tree: the point when the agent must decide which teammate to pass to.
It analyzes the current state of the game and assesses the likelihood that
an agent could successfully pass to a speciﬁc teammate. The pass evaluate
network has six inputs: two each for the position of the ball, the taker, and
the teammate whose potential as a receiver it is evaluating. It has two hidden
nodes and one output, which indicates, on scale of 0 to 1, its conﬁdence that
a pass to the given teammate would succeed.
Get Open: The get open network is activated when a keeper does not have
a ball and is not receiving a pass. Clearly, such an agent should get to a
position where it can receive a pass. However, an optimal get open behavior
would not just position the agent where a pass is most likely to succeed.
Instead, it would position the agent where a pass would be most strategically
advantageous (e.g. by considering future pass opportunities as well). The get
open network has ﬁve inputs: two for the ball’s current position, two for the
taker’s current position, and one indicating how close the agent is to the
ﬁeld’s bounding circle. It has two hidden nodes and two outputs, which
control the agent’s heading and speed.
After decomposing the task as described above, we need to evolve networks
for each of the four subtasks. These networks can be trained in sequence, through
layered learning, or simultaneously, through coevolution. The remainder of this
section details these two alternatives.

Evolving Keepaway Soccer Players through Task Decomposition
363
Layered Learning. One approach to training the components of a task de-
composition is layered learning, a bottom-up paradigm in which low-level be-
haviors are learned prior to high-level ones [16]. Since each component is trained
Pass Evaluate
Get Open
Pass
Intercept
Fig. 6. A layered learning hierarchy for the
keepaway task. Each box represents a layer
and arrows indicate dependencies between
layers. A layer cannot be learned until all
the layers it depends on have been learned.
separately, the learning algorithm op-
timizes over several small solution
spaces, instead of one large one. How-
ever, since some sub-behaviors must
be learned before others, it is not
usually possible to train each compo-
nent in the actual domain. Instead, we
must construct a special training envi-
ronment for each component. The hi-
erarchical nature of layered learning
makes this construction easier: since
the components are learned from the
bottom-up, we can use the already
completed sub-behaviors to help con-
struct the next training environment.
In the original implementation of layered learning, each sub-task was learned
and frozen before moving to the next layer [16]. However, in some cases it is
beneﬁcial to allow some of the lower layers to continue learning while the higher
layers are trained [17]. For simplicity, here we freeze each layer before proceeding.
Figure 6 shows one way in which the components of the task decomposi-
tion can be trained using layered learning. An arrow from one layer to another
indicates that the latter layer depends on the former. A given task cannot be
learned until all the layers that point to it have been learned. Hence, learning
begins at the bottom, with intercept, and moves up the hierarchy step by step.
The training environment for each layer is described below.
Intercept: To train the interceptor, we propel the ball towards the agent at
various angles and speeds. The agent is rewarded for minimizing the time
it takes to touch the ball. As the interceptor improves, the initial angle and
speed of the ball increase incrementally.
Pass: To train the passer we propel the ball towards the agent and randomly
select at which angle we want it to kick the ball. The agent employs the
intercept behavior learned in the previous layer until it arrives near the ball,
at which point it switches to the pass behavior being evolved. The agent’s
reward is inversely proportional to the diﬀerence between the target angle
and the ball’s actual direction of travel. As the passer improves, the range
of angles at which it is required to pass increases incrementally.
Pass Evaluate: To train the pass evaluator, the ball is placed in the center of
the ﬁeld and the pass evaluator is placed just behind it at various angles. Two
teammates are situated near the edge of the bounding circle on the other side
of the ball at a randomly selected angle. A single taker is placed similarly
but nearer to the ball to simulate the pressure it exerts on the passer. The
teammates and the taker use the previously learned intercept behavior. We

364
S. Whiteson et al.
run the evolving network twice, once for each teammate, and pass to the
teammate who receives the higher evaluation. The agent is rewarded only if
the pass succeeds.
Get Open: When training the get open behavior, the other layers have already
been learned. Hence, the get open network can be trained in a complete game
of keepaway. Its training environment is identical to that of the monolithic
approach with one exception: during a ﬁtness evaluation the agents are con-
trolled by our decision tree. The tree determines when to use each of the
four networks (the three previously trained components and the evolving
get open behavior).
At each layer, the results of previous layers are used to assist in training. In
this manner, all the components of the task decomposition can be trained and
assembled into an eﬀective keepaway controller. However, the behaviors learned
with this method are optimized for their training environment, not the keepaway
task as a whole. It may sometimes be possible to learn more eﬀective behaviors
through coevolution, which we discuss next.
Coevolution. A much less constrained method of learning the keepaway agents’
sub-behaviors is to evolve them all simultaneously, a process called coevolution.
In general, coevolution can be competitive [5,10], in which case the components
are adversaries and one component’s gain is another’s loss. Coevolution can also
be cooperative [9], as when the various components share ﬁtness scores.
In our case, we use an extension of ESP designed to coevolve several cooper-
ating components. This method, called Multi-Agent ESP, has been successfully
used to master multi-agent predator-prey tasks [18]. In Multi-Agent ESP, each
component is evolved with a separate, concurrent run of ESP. During a ﬁtness
evaluation, networks are formed in each ESP and evaluated together in the task.
All the networks that participate in the evaluation receive the same score. There-
fore, the component ESPs coevolve compatible behaviors that together solve the
task.
The training environment for this coevolutionary approach is very similar to
that of the get open layer described above. The decision tree still governs each
keeper’s behavior though the four networks are now all learning simultaneously,
whereas three of them were ﬁxed in the layered approach.
4
Empirical Results
To compare monolithic learning, layered learning, and coevolution, we ran seven
trials of each method, each of which evolved for 150 generations. In the layered
approach, the get open behavior, trained in a full game of keepaway, ran for
150 generations. Additional generations were used to train the lower layers. Fig-
ure 7 shows what task diﬃculty (i.e. taker speed) each method reached during
the course of evolution, averaged over all seven runs. This graph shows that
decomposing the task vastly improves neuro-evolution’s ability to learn eﬀective

Evolving Keepaway Soccer Players through Task Decomposition
365
controllers for keepaway players. The results also demonstrate the eﬃcacy of
coevolution. Though it requires fewer generations to train and less eﬀort to im-
plement, it achieves substantially better performance than the layered approach
in this task.
How do the networks trained in these experiments fair in the hardest version
of the task? To determine this, we tested the evolving networks from each method
against a taker moving at 100% speed. At every ﬁfth generation, we selected the
strongest network from the best run of each method and subjected it to 50 ﬁtness
evaluations, for a total of 1000 games of keepaway for each network (recall that
one ﬁtness evaluation consists of 20 games of keepaway). Figure 8, which shows
the results of these tests, further veriﬁes the eﬀectiveness of coevolution. The
learning curve of the layered approach appears ﬂat, indicating that it was unable
to signiﬁcantly improve the keepers’ performance through training the get open
network. However, the layered approach outperformed the monolithic method,
suggesting that it made substantial progress when training the lower layers. It is
essential to note that neither the layered nor monolithic approaches trained at
this highest task diﬃculty, whereas the best run of coevolution did. Nonetheless,
these tests provide additional conﬁrmation that neuro-evolution can truly master
complex control tasks once they have been decomposed, particularly when using
a coevolutionary approach.
0
10
20
30
40
50
60
70
80
90
100
0
20
40
60
80
100
120
140
160
Average Task Difficulty (% Full Speed)
Generations
Average Task Difficulty Over Time
Coevolution
Layered Learning
Monolithic Learning
Fig. 7. Task diﬃculty (i.e. taker speed) of each method over generations, averaged over
seven runs. Task decomposition proves essential for reaching the higher diﬃculties. Only
coevolution reaches the hardest task.

366
S. Whiteson et al.
0
20
40
60
80
100
120
140
0
20
40
60
80
100
120
140
160
Average Score per Fitness Evaluation
Generations
Average Score Over Time
Coevolution
Layered Learning
Monolithic Learning
Fig. 8.
Average score per ﬁtness evaluation for the best run of each method over
generations when the taker moves at 100% speed. These results demonstrate that task
decomposition is important in this domain and that coevolution can eﬀectively learn
the resulting subtasks.
5
Discussion
The results described above verify that given a suitable task decomposition
neuro-evolution can learn a complex, multi-agent control task that is too diﬃcult
to learn monolithically. Given such a decomposition, layered learning developed
a successful controller, though the less-constrained coevolutionary approach per-
formed signiﬁcantly better.
By placing fewer restrictions on the solution space, coevolution beneﬁts from
greater ﬂexibility, which may contribute to its strong performance. Since co-
evolution trains every sub-behavior in the target environment, the components
have the opportunity to react to each other’s behavior and adjust accordingly.
In layered learning, by contrast, we usually need to construct a special training
environment for most layers. If any of those environments fail to capture a key
aspect of the target domain, the resulting components may be sub-optimal. For
example, the interceptor trained by layered learning is evaluated only by how
quickly it can reach the ball. In keepaway, however, a good interceptor will ap-
proach the ball from the side to make the agent’s next pass easier. Since the
coevolving interceptor learned along with the passer, it was able to learn this
superior behavior, while the layered interceptor just approached the ball directly.
Though it is possible to adjust the layered interceptor’s ﬁtness function to en-
courage this indirect approach, it is unlikely that a designer would know a priori
that such behavior is desirable.
The success of coevolution in this domain suggests that we can learn complex
tasks simply by providing neuro-evolution with a high-level strategy. However,
we suspect that in extremely diﬃcult tasks, the solution space will be too large

Evolving Keepaway Soccer Players through Task Decomposition
367
for coevolution to search eﬀectively given current neuro-evolution techniques. In
these cases, the hierarchical features of layered learning, by greatly reducing the
solution space, may prove essential to a successful learning system.
Layered learning and coevolution are just two points on a spectrum of possi-
ble methods which diﬀer with respect to how aggressively they constrain learn-
ing. At one extreme, the monolithic approach tested in this paper places very
few restrictions on learning. At the other extreme, layered learning conﬁnes the
search by directing each component to a speciﬁc sub-goal. The layered and co-
evolutionary approaches can be made arbitrarily more constraining by replacing
some of the components with hand-coded behaviors. Similarly, both methods
can be made less restrictive by requiring them to learn a decision tree, rather
than giving them a hand-coded one.
6
Conclusion and Future Work
In this paper we verify that neuro-evolution can master keepaway, a complex,
multi-agent control task. We also show that decomposing the task is more eﬀec-
tive than training a monolithic controller for it. Our experiments demonstrate
that the more ﬂexible coevolutionary approach learns better agents than the
layered approach in this domain.
In ongoing research we plan to further explore the space between uncon-
strained and highly constrained learning methods. In doing so, we hope to shed
light on how to determine the optimal method for a given task. Also, we plan to
test both the layered and coevolutionary approaches in more complex domains
to better assess the potential of these promising methods.
Acknowledgments. This research was supported in part by the National Sci-
ence Foundation under grant IIS-0083776, and the Texas Higher Education Co-
ordinating Board under grant ARP-0036580476-2001.
References
1. T. Balch. Teambots domain: Soccerbots, 2000.
http://www-2.cs.cmu.edu/˜trb/TeamBots/Domains/SoccerBots.
2. F. Gomez and R. Miikkulainen. Incremental evolution of complex general behavior.
Adaptive Behavior, 5:317–342, 1997.
3. F. Gomez and R. Miikkulainen. Solving non-Markovian control tasks with neu-
roevolution. Denver, CO, 1999.
4. F. Gomez and R. Miikkulainen. Learning robust nonlinear control with neuroevo-
lution. Technical Report AI01-292, The University of Texas at Austin Department
of Computer Sciences, 2001.
5. T. Haynes and S. Sen. Evolving behavioral strategies in predators and prey. In
G. Weiß and S. Sen, editors, Adaptation and Learning in Multiagent Systems, pages
113–126. Springer Verlag, Berlin, 1996.

368
S. Whiteson et al.
6. W. H. Hsu and S. M. Gustafson. Genetic programming and multi-agent layered
learning by reinforcements. In Genetic and Evolutionary Computation Conference,
New York, NY, July 2002.
7. I. Noda, H. Matsubara, K. Hiraki, and I. Frank. Soccer server: A tool for research
on multiagent systems. Applied Artiﬁcial Intelligence, 12:233–250, 1998.
8. A. D. Pietro, L. While, and L. Barone.
Learning in RoboCup keepaway using
evolutionary algorithms. In GECCO 2002: Proceedings of the Genetic and Evo-
lutionary Computation Conference, pages 1065–1072, New York, 9-13 July 2002.
Morgan Kaufmann Publishers.
9. M. A. Potter and K. A. D. Jong. Cooperative coevolution: An architecture for
evolving coadapted subcomponents. Evolutionary Computation, 8:1–29, 2000.
10. C. D. Rosin and R. K. Belew.
Methods for competitive co-evolution: Finding
opponents worth beating. In Proceedings of the Sixth International Conference on
Genetic Algorithms, pages 373–380, San Mateo,CA, July 1995. Morgan Kaufman.
11. J. D. Schaﬀer, D. Whitley, and L. J. Eshelman. Combinations of genetic algorithms
and neural networks: A survey of the state of the art. In D. Whitley and J. Schaf-
fer, editors, International Workshop on Combinations of Genetic Algorithms and
Neural Networks (COGANN-92), pages 1–37. IEEE Computer Society Press, 1992.
12. P. Stone. Layered Learning in Multiagent Systems: A Winning Approach to Robotic
Soccer. MIT Press, 2000.
13. P. Stone, (ed.), M. Asada, T. Balch, M. Fujita, G. Kraetzschmar, H. Lund, P. Scerri,
S. Tadokoro, and G. Wyeth. Overview of RoboCup-2000. In RoboCup-2000: Robot
Soccer World Cup IV. Springer Verlag, Berlin, 2001.
14. P. Stone and R. S. Sutton. Scaling reinforcement learning toward RoboCup soccer.
In Proceedings of the Eighteenth International Conference on Machine Learning,
pages 537–544. Morgan Kaufmann, San Francisco, CA, 2001.
15. P. Stone and R. S. Sutton.
Keepaway soccer: a machine learning testbed.
In
RoboCup-2001: Robot Soccer World Cup V. Springer Verlag, Berlin, 2002.
16. P. Stone and M. Veloso. Layered learning. In Machine Learning: ECML 2000, pages
369–381. Springer Verlag, Barcelona,Catalonia,Spain, May/June 2000. Proceedings
of the Eleventh European Conference on Machine Learning (ECML-2000).
17. S. Whiteson and P. Stone. Concurrent layered learning. In Second International
Joint Conference on Autonomous Agents and Multiagent Systems, July 2003. To
appear.
18. C. H. Yong and R. Miikkulainen. Cooperative coevolution of multi-agent systems.
Technical Report AI01-287, The University of Texas at Austin Department of
Computer Sciences, 2001.

A New Method of Multilayer Perceptron
Encoding
Emmanuel Blindauer and Jerzy Korczak
Laboratoire des Sciences de l’Image, de l’Informatique et de la T´el´ed´etection,
UMR7005, CNRS, 67400 Illkirch, France. {blindauer,jjk}@lsiit.u-strasbg.fr
1
Evolving Neural Networks
One of the central issues in neural network research is how to ﬁnd an optimal
MultiLayer Perceptron architecture. The number of neurons, their organization
in layers, as well as their connection scheme have a considerable inﬂuence on
network learning, and on the capacity for generalization [7]. A solution to ﬁnd
out these parameters is needed: The neuro-evolution ([1,2,4,5]).
The novelty is to emphasize the network performance aspects, and the net-
work simpliﬁcation achieved by reducing the network topology. All these genetic
manipulations on the network architecture should not decrease the neural net-
work performance.
2
Network Representation and Encoding Schemes
The main goal of an encoding scheme is to represent neural networks in a pop-
ulation as a collection of chromosomes. There are many approaches to genetic
representation of neural networks [4], [5]. Classical method use to encode the
network topology into a single string. But frequently, for large-size problems,
these methods do not generate satisfactory results: computing new weights to
get satisfactory networks is very costly.
A new encoding method based on the matrix encoding is proposed: A matrix
where every element represents a weight of the neural network.
Several operators for a genotype have been proposed: crossover operators
and mutation operators. For the classical crossover operation, a new matrix
is created from two splitted matrix: the oﬀspring get two diﬀerent parts, one
from each parent. This can be considered as the one point crossover, in a two
dimension space A second crossover operator is deﬁned: an exchange of a sub-
matrix between the parents is done. For the mutation, several operators are
availables. The ﬁrst is the ablation operator. Setting one or several zero in the
matrix, we are removing these connections. Setting to zero a partial row or
column, we delete several incoming or outgoing connections from the neuron.
The second is the grown operator: connection are added. Again, we can control
where the connections are added, and know if a neuron is fully connected or not.
With these operators, as matrix elements are the weights of the network,
some learning is required to get a new optimal network. As only a few weights
have changed, the learning will be faster.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 369–370, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

370
E. Blindauer and J. Korczak
3
Experimentation
The performance have been evalued on several classical problems. These case
studies have been chosen based on the growing complexity of the problem to
solve. Each population had 200 individuals. For each individual, 100 epochs
were carried out for training. For the genetics parameters, the crossover percent
is set to 80%, with a elitist model. 5% of the population can fall under mutation.
Compared with other results from [3], this new method has shown the best,
not only in term of network complexity, but also in quality of learning
Table 1. Results of experimentations
XOR Parity 3 Parity 4 Parity 5
Heart
Sonar
Number of hidden neurons
2
3
5
8
12
30
Number of connections
6
11
23
38
354
1182
Number of epochs (error)
13
23
80
244
209 (9%) 120 (13%)
4
Conclusion
The experiments have conﬁrmed that, ﬁrstly by encoding the network topology
and weights the search space is aﬃned; secondly, by the inheritence of connection
weights, the learning stage is speeded up considerably. The presented method
generates eﬃcient networks in a shorter time compared to actual methods. The
new encoding scheme improves the eﬀectiveness of evolutionary process: weights
of the neural network included in the genetic encoding scheme and good genetics
operators give acceptable results.
References
1. J. Korczak and E. Blindauer, An Approach to Encode Multilayer Perceptrons, [In]
Proceedings of the International Conference on Artiﬁcial Neural Networks, 2002
2. E.Cant´u-Paz, C.Kamath, Evolving Neural Networks For The Classiﬁcation of Galax-
ies, [In] Proceedings of the Genetic and Evolutionary Computation Conference, 2002
3. M.A. Gr¨onroos, Evolutionary Design Neural Networks, PhD thesis, Department of
Mathematical Sciences, University of Turku, 1998.
4. F. Gruau, Neural networks synthesis using cellular encoding and the genetic algo-
rithm, PhD thesis, LIP, Ecole Normale Superieure, Lyon, 1992.
5. H. Kitano, Designing neural networks using genetic algorithms with graph genera-
tion system, Complex Systems, 4: 461–476, 1990.
6. F. Radlinski, Evolutionary Learning on Structured Data for Artiﬁcial Neural Net-
works, MSC Thesis, Dep. of Computer Science Australian National University, 2002
7. X. Yao, Evolving artiﬁcial neural networks. Proceedings of the IEEE, 1999.

An Incremental and Non-generational
Coevolutionary Algorithm
Ram´on Alfonso Palacios-Durazo1 and Manuel Valenzuela-Rend´on2
1 Lumina Software, apd@luminasoftware.com
http://www.luminasoftware.com/apd
Washington 2825 Pte C.P. 64040, Monterrey N.L., Mexico
2 ITESM, Monterrey
Centro de Sistemas Inteligentes
valenzuela@itesm.mx,
http://www-csi.mty.itesm.mx/˜mvalenzu
C.P. 64849 Monterrey, N.L., Mexico
The central idea of coevolution lies in the fact that the ﬁtness of an individ-
ual depends on its performance against the current individuals of the opponent
population. However, coevolution has been shown to have problems [2,5]. Meth-
ods and techniques have been proposed to compensate the ﬂaws in the general
concept of coevolution [2]. In this article we propose a diﬀerent approach to
implementing coevolution, called incremental coevolutionary algorithm (ICA) in
which some of these problems are solved by design. In ICA, the importance of
the coexistance of individuals in the same population is as important as the
individuals in the opponent population. This is similar to the problem faced by
learning classiﬁer systems (LCSs) [1,4]. We take ideas from these algorithms and
put them into ICA.
In a coevolutionary algorithm, the ﬁtness landscape depends on the opponent
population, therefore it changes every generation. The individuals selected for
reproduction are those more promising to perform better against the ﬁtness
landscape represented by the opponent population. However, if the complete
population of parasites and hosts are recreated in every generation, the oﬀspring
of each new generation face a ﬁtness landscape unlike the one they where bred
to defeat. Clearly, a generational approach to coevolution can be too disruptive.
Since the ﬁtness landscape changes every generation, it also makes sense to
incrementally adjust the ﬁtness of individuals in each one. These two ideas deﬁne
the main approach of the ICA: the use of a non-generational genetic algorithm
and the incremental adjustment of the ﬁtness estimation of an individual. The
formal deﬁnition of ICA can be seen in ﬁgure 1.
ICA has some interesting properties. First of all, it is not generational. Each
new individual faces a similar ﬁtness landscape than its parents. The ﬁtness
landscape changes gradually, allowing an arms race to occur. Since opponents are
chosen proportional to their ﬁtness, an individual has a greater chance of facing
good opponents. If a particular strength is found in a population, individuals
that have it will propagate and will have a greater probability of coming into
competition (both because more individuals carry the strength, and because a
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 371–372, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

372
R.A. Palacios-Durazo and M. Valenzuela-Rend´on
greater ﬁtness produces a higher probability of being selected for competition).
If the population overspecializes, another strength will propagate to maintain
balance. Thus, a natural sharing occurs.
(*Deﬁne A(x, f)*): A(x, f) = tanh(x/f)
Generate random host and parasite population
Initialize ﬁtness of all parasites Sp ←Mp/Cp and hosts Sh ←As/Ma
repeat
(* Competition cycle*)
for c ←1 to Nc
Select parasite p and host h proportionally to ﬁtness.
error ←abs(Result of competition between h and p )
Sp ←Sp + MpA(error, Eerror) −CpSp(t)
Sh ←Sh + As(1 −A(error, Eerror)) −MaSh
end-for c
(* 1 step of a GA*)
Select two parasite parents (p1 and p2) proportionally to Sp
Create new individual p0 by doing crossover and mutation
Sp0 ←(Sp1 + Sp2)/2
Delete parasite with worst ﬁtness and substitute with p0
Repeat above for host population
until termination criteria met
Fig. 1. Incremental coevolutionary algorithm
The equations for incrementally adjusting ﬁtness can be proven to be stable
doing an analysis similar to the one used for LCSs [3]. ICA was tested ﬁnding
trigonometric identities and was found to be robust, able to generate specializa-
tion niches and to consistently outperform traditional genetic programming.
References
1. John Holland. Escaping brittleness: The possibilities of general-purpose learning
algorithms applied to parallel rule-based systems. Machine Learning: An Artiﬁcial
Intelligence Approach, 2, 1986.
2. Christopher D. Rosin and Richard K. Belew. Methods for competitive co-evolution:
Finding opponents worth beating. In Larry Eshelman, editor, Proceedings of the
Sixth International Conference on Genetic Algorithms, pages 373–380, San Fran-
cisco, CA, 1995. Morgan Kaufmann.
3. Manuel Valenzuela-Rend´on. Two Analysis Tools to Describe the Operation of Clas-
siﬁer Systems. PhD thesis, The University of Alabama, Tuscaloosa, Alabama, 1989.
4. Manuel Valenzuela-Rend´on and E. Uresti-Charre. A nongenerational genetic algo-
rithm for multiobjective optimization. In Proceedings of the Seventh International
Conference on Genetic Algorithms, pages 658–665. Morgan Kaufmann, 1997.
5. Richard A. Watson and Jordan B. Pollack. Coevolutionary dynamics in a minimal
substrate. In Proceedings of the Genetic and Evolutionary Computation Conference
(GECCO-2001), pages 702–709, San Francisco, California, USA, 7-11 2001. Morgan
Kaufmann.

Coevolutionary Convergence to Global Optima
Lothar M. Schmitt
The University of Aizu, Aizu-Wakamatsu City, Fukushima Prefecture 965-8580, Japan
lothar@u-aizu.ac.jp
Abstract. We discuss a theory for a realistic, applicable scaled genetic
algorithm (GA) which converges asymptoticly to global optima in a
coevolutionary setting involving two species. It is shown for the ﬁrst
time that coevolutionary arms races yielding global optima can be
implemented successfully in a procedure similar to simulated annealing.
Keywords: Coevolution; convergence of genetic algorithms; simulated
annealing; genetic programming.
In [2], the need for a theoretical framework for coevolutionary algorithms and
possible convergence theorems in regard to coevolutionary optimization (“arms
races”) was pointed out. Theoretical advance for coevolutionary GAs involving
two types of creatures seems very limited thus far. [6] largely ﬁlls this void1 in
the case of a ﬁxed division of the population among the two species involved
even though there is certainly room for improvement. For a setting involving
two types of creatures, [6] satisﬁes all goals advocated in [1, p. 270] in regard to
ﬁnding a theoretical framework for scaled GAs similar to simulated annealing.
[4,5] contain recent substancial advances in theory of coevolutionary GAs for
competing agents/creatures of a single type. In particular, the coevolutionary
global optimization problem is solved under the condition that (a group of)
agents exist that are strictly superior in every population they reside in.
Here and in [6], we continue to use the well-established notation of [3,4,5].
The setup considers two sets of creatures C(0) and C(1). Elements of C(0) can,
e.g., be thought of as sorting programs while C(1) can be thought of as unsorted
tuples. The two types of creatures C(j), j∈{0, 1}, involved in the setup of the
coevolutionary GA are being encoded as ﬁnite-length strings over arbitrary-size
alphabets Aj. Creatures c∈C(0), d∈C(1) are evaluated by a duality <c, d> ∈IR.
In case of the above example, this expression may represent execution time of a
sorting program c on an unsorted tuple d. Any population p is a tuple consisting
of s0≥4 creatures of C(0) followed by s1≥4 creatures of C(1). This ﬁxed division
of the population is done here simply for practical purposes but is, in eﬀect, in
accordance with the evolutionary stable strategy in evolutionary game theory.
In particular, the model in [6] does not refer to the multi-set model [7].
1 Possibly, there exist signiﬁcant theoretical results unknown to the author. Referee
753 claims in regard to [6]: “this elaborate mathematical framework that doesn’t
illuminate anything we don’t already know” without giving further reference.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 373–374, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

374
L.M. Schmitt
The GA considered in [6] employs very common GA operators which are given
by detailed, almost procedural deﬁnitions including explicit annealing schedules:
multiple-spot mutation, practically any known crossover, and scaled proportional
ﬁtness selection. Thus, the GA considered in [6] is standard and by no means
“out of the blue sky”. Work by the authors of [1] and [4, Thms. 8.2–6] show that
the annealing procedure considered in [6] is absolutely necessary for convergence
to global optima and not “highly contrived”. The mutation operator allows for a
scalable compromise on the alphabet level between a neighborhood-based search
and pure random change (the latter as in [4, Lemma 3.1]). The population-
dependent ﬁtness function is deﬁned as follows: if p = (c1, . . . , cs0, d1, . . . , ds1),
ϕ1=±1, then f(dι, p) = exp(ϕ1
s0
σ=1 <cσ, dι> ). The ﬁtness function is deﬁned
similarly for c1, . . . , cs0. The factors ϕ0,1=±1 are used to adjust whether the
two types of creatures have the same or opposing goals. Referring to the above
example, one would set ϕ0=−1 and ϕ1=1 since good sorting programs aim for
a short execution time while ‘diﬃcult’ unsorted tuples aim for a long execution
time. The ﬁtness function is then scaled with logarithmic growth in the exponent
as in [4, Thm. 8.6] or [5, Thm. 3.4.1] with similar lower bounds for the factor
B>0 determining the growth.
Under the assumption that a group of globally strictly maximal creatures
exists that are evaluated superior in any population they reside in, an analogue
of [4, Thm. 8.6] [5, Thm. 3.4.1] with similar restriction on population size is
shown in [6]. In particular, the coevolutionary GA in [6] is strongly ergodic and
converges to a probability distribution over uniform populations containing only
globally strictly maximal creatures. [6] is available from this author. As indicated
above, this author ﬁnds the concerns of referees unacceptable to a large degree.
References
1. Davis, T.E.; Principe, J.C.: A Markov Chain Framework for the Simple GA. Evol.
Comput. 1 (1993) 269–288
2. DeJong, K.: Lecture on Coevolution. In: Beyer H.-G. et al. (chairs): Seminar ‘Theory
of Evolutionary Computation 2002’, Max Planck Inst. Comput. Sci. Conf. Cent.,
Schloß Dagstuhl, Saarland, Germany (2002)
3. Schmitt, L.M. et al.: Linear Analysis of Genetic Algorithms. Theoret. Comput. Sci.
200 (1998) 101–134
4. Schmitt, L.M.: Theory of Genetic Algorithms. Theoret. Comput. Sci. 259 (2001)
1–61
5. Schmitt, L.M.: Asymptotic Convergence of Scaled Genetic Algorithms to Global
Optima —A gentle introduction to the theory—. In: Menon A. (ed.). The Next
Generation Research Issues in Evolutionary Computation. (in preparation), Kluwer
Ser. in Evol. Comput. (Goldberg D.E., ed.). Kluwer, Dordrecht, The Netherlands
(2003) (to appear)
6. Schmitt, L.M.: Coevolutionary Convergence to Global Optima. Tech. Rep. 2003-2-
001, The University of Aizu, Aizu-Wakamatsu, Japan (2003) 1–12
7. Vose M.D.: The Simple Genetic Algorithm: Foundations and Theory. MIT Press,
Cambridge, MA, USA (1999)

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 375–376, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Generalized Extremal Optimization for Solving Complex
Optimal Design Problems
Fabiano Luis de Sousa1, Valeri Vlassov1, and Fernando Manuel Ramos2
1 Instituto Nacional de Pesquisas Espaciais – INPE/DMC – Av. dos Astronautas, 1758
12227-010 São José dos Campos,SP – Brazil
{fabiano,vlassov}@dem.inpe.br
2 Instituto Nacional de Pesquisas Espaciais – INPE/LAC – Av. dos Astronautas, 1758
12227-010 São José dos Campos,SP – Brazil
fernando@lac.inpe.br
Recently, Boettcher and Percus [1] proposed a new optimization method, called
Extremal Optimization (EO), inspired by a simplified model of natural selection
developed to show the emergence of Self-Organized Criticality (SOC) in ecosystems
[2]. Although having been successfully applied to hard problems in combinatorial
optimization, a drawback of the EO is that for each new optimization problem
assessed, a new way to define the fitness of the design variables has to be created [2].
Moreover, to our knowledge it has been applied so far to combinatorial problems with
no implementation to continuous functions.
In order to make the EO easily applicable to a broad class of design optimization
problems, Sousa and Ramos [3,4] have proposed a generalization of the EO that was
named the Generalized Extremal Optimization (GEO) method. It is of easy
implementation, does not make use of derivatives and can be applied to unconstrained
or constrained problems, non-convex or disjoint design spaces, with any combination
of continuous, discrete or integer variables. It is a global search meta-heuristic, as the
Genetic Algorithm (GA) and the Simulated Annealing (SA), but with the a priori
advantage of having only one free parameter to adjust. Having been already tested on
a set of test functions, commonly used to assess the performance of stochastic
algorithms, the GEO proved to be competitive to the GA and the SA, or variations of
these algorithms [3,4].
The GEO method was devised to be applied to complex optimization problems,
such as the optimal design of a heat pipe (HP). This problem has difficulties such as
an objective function that presents design variables with strong non-linear
interactions, subject to multiple constraints, being considered unsuitable to be solved
by traditional gradient based optimization methods [5]. To illustrate the efficacy of
the GEO on dealing with such kind of problems, we used it to optimize a HP for a
space application with the goal of minimizing the HP’s total mass, given a desirable
heat transfer rate and boundary conditions on the condenser. The HP uses a mesh type
wick and is made of Stainless Steel. A total of 18 constraints were taken into account,
which included operational, dimensional and structural ones.  Temperature dependent
fluid properties were considered and the calculations were done for steady state
conditions, with three fluids being considered as working fluids: ethanol, methanol
and ammonia.  Several runs were performed under different values of heat transfer

376         F.L. de Sousa, V. Vlassov, and F.M. Ramos
rate and temperature at the condenser. Integral optimal characteristics were obtained,
which are presented in Figure 1.
20.0
40.0
60.0
80.0
100.0
Heat transfer rate (W)
0.0
4.0
8.0
12.0
16.0
20.0
Total mass of the HP (kg)
  Tsi = -15.0 oC
 Tsi = 0.0 oC
  Tsi = 15.0 oC
 Tsi = 30.0 oC
Ethanol
20.0
40.0
60.0
80.0
100.0
Heat transfer rate (W)
0.0
4.0
8.0
12.0
16.0
20.0
Total mass of the HP (kg)
  Tsi = -15.0 oC
 Tsi = 0.0 oC
  Tsi = 15.0 oC
 Tsi = 30.0 oC
Methanol
20.0
40.0
60.0
80.0
100.0
Heat transfer rate (W)
0.0
4.0
8.0
12.0
16.0
20.0
Total mass of the HP (kg)
  Tsi = -15.0 oC
 Tsi = 0.0 oC
  Tsi = 15.0 oC
 Tsi = 30.0 oC
Ammonia
Fig. 1. Minimum HP mass found for ethanol, methanol and ammonia, at different operational
conditions.
It can be seen from these results, that for moderate heat transfer rates (up to 50
W), the ammonia and methanol HPs display similar results in terms of optimal mass,
while for high heat transfer rates (as for Q = 100 W), the HP filled with ammonia
shows considerably better performance. In practice, this means that for applications
which require the transport of moderate heat flow rates, cheaper methanol HPs can be
used, whereas at higher heat transport rates, the ammonia HP should be utilized. It can
be also seen, that the higher the heat to be transferred, the higher the HP total mass.
Although this is an expected result, the apparent non-linearity of the HP mass with Q
(more pronounced as the temperature on the external surface of the condenser Tsi is
increased), means that for some applications there is a theoretical possibility that the
use of two HPs of a given heat transfer capability can yield a better performance, in
terms of mass optimization,  than the use of an single HP with double capability. This
non-linearity of the optimal characteristics has an important significance in design
practice and, thus, should be further investigated. These results highlight the potential
of the GEO to be used as a design tool. In fact, it can be said that the GEO method is a
good candidate to be incorporated to the designer’s tools suitcase.
References
1. 
Boettcher, S. and Percus, A. G.: Optimization with Extremal Dynamics, Physical Review
Letters, Vol. 86 (2001) 5211–5214.
2. 
Bak, P. and Sneppen, K., “Punctuated Equilibrium and Criticality in a Simple Model of
Evolution”, Physical Review Letters, Vol. 71, Number 24, pp. 4083–4086, 1993.
3. 
Sousa, F.L. and Ramos, F.M.: Function Optimization Using Extremal Dynamics.
Proceedings of the 4th International Conference on Inverse Problems in Engineering, Rio
de Janeiro, Brazil, (2002).
4. 
Sousa, F.L., Ramos, F.M., Paglione, P. and Girardi, R.M.: A New Stochastic Algorithm
for Design Optimization. Accepted for publication in the AIAA Journal.
5. 
Rajesh, V.G. and Ravindran K.P.: Optimum Heat Pipe Design: A Nonlinear Programming
Approach. International Communications in Heat and Mass Transfer, Vol. 24, No. 3,
(1997) 371–380.

Coevolving Communication and Cooperation for
Lattice Formation Tasks
Jekanthan Thangavelautham, Timothy D. Barfoot, and
Gabriele M.T. D’Eleuterio
Institute for Aerospace Studies University of Toronto
4925 Duﬀerin Street, Toronto, Ontario, Canada, M3H 5T6
thangav@ecf.utoronto.ca, {tim.barfoot,gabriele.deleuterio}@utoronto.ca
Abstract. Reactive multi-agent systems are shown to coevolve with ex-
plicit communication and cooperative behavior to solve lattice formation
tasks. Comparable agents that lack the ability to communicate and coop-
erate are shown to be unsuccessful in solving the same tasks. The control
system for these agents consists of identical cellular automata lookup ta-
bles handling communication, cooperation and motion subsystems.
1
Introduction
In nature, social insects such as bees, ants and termites collectively manage to
construct hives and mounds, without any centralized supervision [1]. The agents
in our simulation are driven by a decentralized control system and can take
advantage of communication and cooperation strategies to produce a desired
‘swarm’ behavior. A decentralized approach oﬀers some inherent advantages, in-
cluding fault tolerance, parallelism, reliability, scalability and simplicity in agent
design [2].
Our initial test has been to evolve a homogenous multi-agent system able
to construct simple lattice structures. The lattice formation task involves redis-
tributing a preset number of randomly scattered objects (blocks) in a 2-D grid
world into a desired lattice structure. The agents move around the grid world
and manipulate blocks using reactive control systems with input from simulated
vision sensors, contact sensors and inter-agent communication. A global consen-
sus is achieved when the agents arrange the blocks into one indistinguishable
lattice structure (analogous to the heap formation task [3]). The reactive control
system triggers one of four basis behaviors, namely move, manipulate object,
pair-up (link) and communicate based on the state of numerous sensors.
2
Results and Discussion
For the GA run, the 2-D world size was a 16 × 16 grid with 24 agents, 36 blocks
and a training time of 3000 time steps. Shannon’s entropy function was used as
a ﬁtness evaluator for the 3 × 3 tilling pattern task. After 300 generations, the
GA run converged to a reasonably high average ﬁtness value (about 99). The
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 377–378, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

378
J. Thangavelautham, T.D. Barfoot, and G.M.T. D’Eleuterio
agents learn to explicitly cooperate within the ﬁrst 5-10 generations. From our
ﬁndings, it appears the evolved solution perform well for much larger problem
sizes of up to 100 × 100 grids as expected, due to our decentralized approach.
Within a coevolutionary process it would be expected for competing popu-
lations (or subsystems) to spur an ‘arms race’ [4]. The steady convergence in
physical behaviors appears to exhibit this process. The communication protocol
that had evolved from the GA run consists of a set of non-coherent signals with
a mutually agreed upon meaning. A comparable agent was developed which
lacked the ability to communicate and cooperate for solving the 3 × 3 tiling
pattern task. Each agent had 7 vision sensors, which meant 4374 lookup table
entries compared to the 349 entries for the agent discussed earlier. After having
modiﬁed various genetic parameters, it was found the GA run never converged.
For this particular case, techniques employing communication and cooperation
have reduced the lookup table size by a factor 12.5 and have made the GA run
computational feasible.
Fig. 1. Snapshot of the system taken at various time steps (0, 100, 400, 1600 ). The 2-D
world size is a 16 × 16 grid with 28 agents and 36 blocks. At time step 0, neighboring
agents are shown ‘unlinked’ (light gray) and by 100 time steps all 28 agents manage to
‘link’ (gray or dark gray). Agents shaded in dark gray carry a block. After 1600 time
steps (far right), the agents come to a consensus and form one lattice structure.
References
1. Kube, R., Zhang, H.: Collective Robotics Intelligence : From Social Insects to robots.
In Proc. Of Simulation of Adaptive Behavior (1992) 460–468
2. Cao, Y.U., Fukunaga, A., Kahng, A. : Cooperative Mobile Robotics : Antecedents
and Directions. : In Autonomous Robots, Vol.4. Kluwer Academic Pub., Boston
(1997) 1–23
3. Barfoot, T., D’Eleuterio, G.M.T.: An Evolutionary Approach to Multi-agent Heap
Formation. In proceedings of the Congress on Evolutionary Computation (1999)
4. Koza, J.R.: Genetic Programming: On the Programming of Computers by Means
of Natural Selection. The MIT Press. Cambridge, MA, (1992)
5. J. Thangavelautham, T.D. Barfoot, G. M. T. D’Eleuterio: Coevolving Communica-
tion and Cooperation for Lattice Formation Tasks. University of Toronto Institute
for Aerospace Studies Technical Report. Toronto, Ont. (2003)

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 379–389, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Efficiency and Reliability of DNA-Based Memories
Max H. Garzon, Andrew Neel, and Hui Chen
Computer Science, University of Memphis
373 Dunn Hall, Memphis, TN 38152-3240
 {mgarzon, aneel, hchen2}@memphis.edu
Abstract. Associative memories based on DNA-affinity have been proposed
[2]. Here, the performance, efficiency, reliability of DNA-based memories is
quantified through simulations in silico. Retrievals occur reliably (98%) within
very short times (milliseconds) despite the randomness of the reactions and re-
gardless of the number of queries. The capacity of these memories is also ex-
plored in practice and compared with previous theoretical estimates. Advantages
of implementations of the same type of memory in special purpose chips in
silico is proposed and discussed.
1   Introduction
DNA olignucleotides have demonstrated to be a feasible and useful medium for com-
puting applications since Adleman’s original work [1], which created a field now
known as biomolecular computing (BMC). Potential applications range from increas-
ing speed through massively parallel computations [13], to new manufacturing tech-
niques in nanotechnology [18], and to the creation of memories that can store very
large amounts of data and fit into minuscule spaces [2], [15]. The apparent enormous
capacity of DNA (over million fold compared to conventional electronic media) and
the enormous advances in recombinant biotechnology to manipulate DNA in vitro in
the last 20 years make this approach potentially attractive and promising. Despite
much work in the field, however, difficulties still abound in bringing these applica-
tions to fruition due to inherent difficulties in orchestrating a large number of individ-
ual molecules to perform a variety of functions in the environment of virtual test
tubes, where the complex machinery of the living cell is no longer present to organize
and control the numerous errors pulling computations by molecular populations away
from their intended targets.
In this paper, we initiate a quantitative study of the potential, limitations, and actual
capacity of memories based or inspired by DNA. The idea of using DNA to create
large associative memories goes back to Baum [2], where he proposed to use DNA
recombination as the basic mechanism for content-addressable storage of information
so that retrieval could be accomplished using the basic mechanism of DNA hybridiza-
tion affinity.  Content is to be encoded in single stranded molecules in solution (or
their complements.) Queries can be obtained by dropping in the tube a DNA primer

380
M.H. Garzon, A. Neel, and H. Chen
Watson-Crick complement of the (partial) information known about a particular rec-
ord using the same coding scheme as in the original memory, appropriately marked
(e.g., using magnetic beads, or fluorescent tags). Retrieval is completed by extension
and/or retrieval (e.g., by sequencing) of any resulting double strands after appropriate
reaction times have been allowed for hybridization to take effect. As pointed out by
Baum [2], and later Reif & LaBean [15], many questions need to be addressed before
an associative memory based on this idea can be regarded as feasible, let alone actu-
ally built.
Further methods were proposed in [15] for input/output from/to databases repre-
sented in wet DNA (such as genomic information obtained from DNA-chip optical
readouts, or synthesis of strands based on such output) and suggested methods to im-
prove the capabilities and performance of the queries of such DNA-based memories.
The proposed hybrid methods, however, require major pre-processing of the entire
database contents (through clustering and vector quantization) and post-processing to
complete the retrieval by the DNA memory (based on the identification of the clusters
centers.) This is a limitation when the presumed database approaches the expected
sizes to be an interesting challenge to conventional databases, or when the data already
exists in wet DNA, because of the prohibitive (and sometimes even impossible) cost
of the transduction process to and from electronics. Inherent issues in the retrieval per
se, such as the reliability of the retrieval in-vitro and the appropriate concentrations for
optimal retrieval times and error rates remain unclear.
We present an assessment of the efficiency and reliability of queries in DNA-based
memories in Section 3, after a description of the experimental design and the data
collected for this purpose in Section 2. In Section 3, we also present very preliminary
estimates of their capacity. Finally, section 4 summarizes the results and discusses the
possibility of building analogous memories in silico inspired by the original ideas in
vitro, as suggested by the experiments reported here. A preliminary analysis of some
of these results has been presented in [7], but here we present further results and a
more complete analysis.
2 
Experimental Design
The experimental data used in this paper has been obtained by simulations in the vir-
tual test tube of Garzon et al [9]. Recently, driven by efficiency and reliability consid-
erations, the ideas of BMC have been implemented in silico by using computational
analogs of DNA and RNA molecules [8]. Recent results show that these protocols
produce results that closely resemble, and in many cases are indistinguishable from,
the protocols they simulate in wet tubes [7]. For example, Adleman’s experiment has
been experimentally reproduced and scaled in virtual test tubes with random graphs of
up to 15 vertices while producing results correct with no probability of a false positive
error and a probability of a false negative of at most 0.4%. Virtual test tubes have also
matched very well the results obtained in vitro by more elaborate and newer protocols,
such as the selection protocol for DNA library design of Deaton et Al. [4]. Therefore,

Efficiency and Reliability of DNA-Based Memories
381
there is good evidence that virtual test tubes provide a reasonable and reliable estimate
of the events in wet tubes (see [7] for a more detailed discussion.)
Virtual test tubes thus can serve as a reasonable pre-requisite methodology to esti-
mate the performance and experimental validation prior to construction of such a
memory, a validation step that is now standard in the design of conventional solid-
state memories. Moreover, as will be seen below in the discussion of the results, vir-
tual test tubes offer a much better insight into the nature of the reaction kinetics than
corresponding experiments in vitro, which, when possible (such as Cot curves to
measure the diversity of a DNA pool), incur much larger cost and effort.
2.1   Virtual Test Tubes
Our experimental runs were implemented using the virtual test tube Edna of Garzon et
al. [7],[8],[9] that simulates BMC protocols in silico. Edna provides an environment
where DNA analogs can be manipulated much more efficiently, can be programmed
and controlled much more easily, at much lower costs, and produce comparable results
to those obtained in a real test tube [7]. Users simply need to create object-oriented
programming classes (in C++) specifying the objects to be used and their interactions.
The basic design of the entities that were put in Edna represent each nucleotide within
the DNA as a single character and the entire strand of DNA as a string, which may
contain single- or double-stranded sections, bulges, and loops or higher secondary
structures. An unhybridized strand represents a strand of DNA from the 5’-end to the
3’-end. These strands encode library records in the database, or queries containing
partial information that identify the records to be retrieved.
The interactions among objects in Edna represent chemical reactions by hybridiza-
tion and ligation resulting in new objects such as dimers, duplexes, double strands, or
more complicated complexes. They can result in one or both entities being destroyed
and a new entity possibly being created. In our case, we wanted to allow the entities
that matched to hybridize to each other to effect a retrieval, per Baum’s design  2].
Edna simulates the reactions in successive iterations. One iteration moves the objects
randomly in the tube’s container (the RAM really) and updates their status according
to the specified interactions with neighbor objects, based on proximity parameters that
can be varied within the interactions. The hybridization reactions between strands
were performed according to the h-measure [8] of hybridization likelihood. Hybridi-
zation was allowed if the h-measure was under a given threshold, which is the number
of mismatches allowed (including frame-shifts) and so roughly codes for stringency in
the reaction conditions. A threshold of zero enforces perfect matches in retrieval,
whereas a larger value permits more flexible and associative retrieval. These require-
ments essentially ensured good enough matches along the sections of the DNA that
were relevant for the associative recall.
The efficiency of the test tube protocols (in our case, retrievals) can be measured by
counting the number of iterations necessary to complete the reactions or achieve the
desired objective; alternatively, one can measure the wall clock time. The number of
iterations taken until a match is found has the advantage of being indifferent to the

382
M.H. Garzon, A. Neel, and H. Chen
speed of the machine(s) running the experiment. This intrinsic measure was used be-
cause one iteration is representative of a unit of real-time for in vitro experiments. The
relationship between simulation results in simulation and equivalent results in vitro
has been discussed in [7]. Results of the experiments in silico can be used to yield
realistic estimates of those in vitro. Essentially, one iteration of the test tube corre-
sponds to the reaction time of one hybridization in the wet tube, which is of the order
of one millisecond [17]. However, the number of iterations cannot be a complete pic-
ture because iterations will last longer as more entities are put in the test tube. For this
reason, processor time (wall clock) was also measured. The wall clock time depends
on the speed and power of the machine(s) running Edna and ranged anywhere from
seconds to days for the single processors and 16 PC cluster that were used to run the
experiments used below.
2.2   Libraries and Queries
We assume we have at our disposal a library of non-cross hybridizing (nxh) strands
representing the records in the databases. The production of such large libraries has
been addressed elsewhere [4], [10]. Well-chosen DNA word designs that will make
this perfectly possible in large numbers of DNA strands directly, even in real test
tubes, will likely be available within a short time. The exact size of such a library will
be discussed below. The nxh property of the library will also ensure that retrievals will
be essentially noise-free (no false positives), module the flexibility built into the re-
trieval parameters (here h-distance). We will also assume that a record may also con-
tain an additional segment (perhaps double-stranded [2]) encoding supplementary
information beyond the label or segment actively used for associative recall, although
this is immaterial for assumptions and results in this paper. The library is assumed to
reside in the test tube, where querying takes place.
Queries are strings objects encoding, and complementary of, the available informa-
tion to be searched for. The selection operation uses probes to mark strands by hy-
bridizing part of the probe with part of the “probed” strand. The number of unique
strands available to be probed is, in principle, the entire library, although we consider
below more selective retrieval modes based on temperature gradients. Strictly speak-
ing, the probe consists of two logical sections: the query and tail. The tail is the por-
tion of the strand that is used with in vitro experiments to physically retrieve the
marked DNA from the test tube (e.g., biotin-streptavidin-coated beads or fluorescent
tags [16]). The query is the portion of the strand that is expected to hybridize with
strands from the library to form a double-stranded entity. We will only be concerned
with the latter below, as the former becomes important only at the implementation
stage, or just be identical to the duplex formed during retrieval.
When a probe comes close enough to a library or probe strand in the tube so that
any hybridization between the two strands is possible, an encounter (which triggers a
check for hybridization) is said to have occurred. The number of encounters can vary
greatly depending directly on the concentration of probes and library strands. It ap-
pears that higher concentration reduce retrieval time, but this is only true to a point

Efficiency and Reliability of DNA-Based Memories
383
since results below show that too much concentration will interfere with the retrieval
process. In other words, a large number of encounters may cause unnecessary hybridi-
zation attempts that will slow down the simulation.  Further, too many neighbor
strands may hinder the movement of the probe strands in search of their match. Prob-
ing is considered complete when probe copies have formed enough retrieval duplexes
with library strands that should be retrieved (perhaps none) according to stringency of
the retrieval (here the h-distance threshold.)  In single probes with high stringency
(perfect matches), probing can be halted when one successful hybridization occurs.
Lesser stringency and multiple simultaneous probes require longer times to complete
the probe. The question arises how long is long enough to complete the probes with
high reliability.
2.3   Test Libraries and Experimental Conditions
The experiments used mostly a library consisting of the full set of 512 non-
complementary 5-mer strands, although other libraries obtained through the software
package developed based on the thermodynamic model of Deaton et Al. [5] were also
tried with consistent results. This is a desirable situation to benchmark retrieval per-
formance since the library is saturated (maximum size) and retrieval times would be
worst-case. The probes were chosen to be random probes of 5-mers. The stringency
was highest (h-distance 0), so exact matches were required. The experiment began by
placing variable concentrations (number of copies) of the library and the probes into
the tube of constant size. Once placed in the tube, the simulation begins. It stops when
the first hybridization is detected. For the purposes of these experiments, there existed
no error margin thus preventing close matches from hybridizing. Introduction of more
flexible thresholds does not affect the results of the experiments.
In the first batch of experiments, we collected data to quantify the efficiency of the
retrieval process (time, number of encounters, and attempted hybridizations) with
single queries between related strands and its variance in hybridization attempts until
successful hybridization. Three successive batches of experiments were designed to
determine the optimal concentrations with which the retrieval was both successful and
efficient, as well as to determine the effect on retrieval times of multiple probes in a
single query. The experiments were performed between 5 and 100 times each and the
results averaged. The complexity and variety of experiments has limited the quantity
of runs possible for each experiment. Over a total of over 2000 experiments were run
continuously over the course of many weeks.
3   Analysis of Results
Below are the results of the experiments, with some analysis of the data gathered.

384
M.H. Garzon, A. Neel, and H. Chen
3.1   Retrieval Efficiency
Figure 1 shows the results of the first experiment at various concentrations averaged
over five runs.  The most hybridization attempts occurred when the concentration of
probes is between 50-60 copies and the concentration of library strands was between
20-30 copies. Figure 2 represents the variability (as measured by the standard devia-
tion) of the experimental data. Although, there exists an abnormally high variance in
some deviations in the population, most data points exist with deviations less than
5000. This high variance can be partially explained by the probabilistic chance of any
two matching strands encountering each other by following a random walk. Interest-
ingly enough, the range of 50-60 probe copies and 20-30 library copies exhibits mini-
mum deviations.
Fig. 1. Retrieval difficulty (hybridization attempts) based on concentration.
Fig. 2. Variability in retrieval difficulty (hybridization attempts) based on concentration.

Efficiency and Reliability of DNA-Based Memories
385
3.2   Optimal Concentrations
Figure 3 shows the average retrieval times as measured in tube iterations. The number
of iterations decreases as the number of probes and library strands increase, to a point.
One might think at first that the highest available probe and library concentration is
desirable. However, Fig. 1 indicates a diminishing return in that the number of hy-
bridization attempts increases as the probe and library concentration increase. In order
for the experiments in silico to be representative of the wet test tube experiments, a
compromise must be made. Therefore, if the ranges of concentrations determined
from Fig. 1 are used, the number of tube iterations remains under 200. Fig. 4 shows
only minimum deviations once the optimal concentration has been achieved. The
larger deviations at the lower concentrations can be accounted for by the highly ran-
domized nature of the test tube simulation. These results on optimal concentration are
consistent and further supported by comparison with the results in Fig. 1.
Fig. 3. Retrieval times (number of iterations) based on concentration.
As a comparison, in a second batch of experiments with a smaller (much sparser)
library of 64 32-mers obtained by a genetic algorithm [9], the same dependent meas-
ures were tested. The results (averaged over 100 runs) are similar, but are displayed in
a different form below. In Figure 5, the retrieval times ranged from nearly 0 through
5,000 iterations. For low concentrations, retrieval times were very large and exhibited
great variability. As the concentration of probe strands exceeds a threshold of about
10, the retrieval times drop under 100 iterations, assuming a library strand concentra-
tion of about 10 strands.
Finally, Figure 6 shows that the retrieval time increases only logarithmically with
the number of multiple queries and tends to level off in the range within which probes
don’t interfere with one another.

386
M.H. Garzon, A. Neel, and H. Chen
Fig. 4. Variability in retrieval times (number of iterations) based on concentration.
    
Fig. 5. Retrieval times and optimal concentration on sparser library.
In summary, these results permit a preliminary estimate of optimal and retrieval times
for queries in DNA associative memories. For a library of size N, a good concentra-
tion of library for optimal retrieval time appears to be in the order of O(logN). Probe
strands require the same order, although probably a smaller number will suffice. The
variability in the retrieval time also decreases for optimal concentrations. Although not
reported here in detail due to space constraints, similar phenomena were observed for
multiple probes. We surmise that this hold true up to O(logN) simultaneous probes,
past which probes begin to interfere with one another causing a substantial increase in
retrieval time. Based on benchmarks obtained by comparing simulations in Edna with

Efficiency and Reliability of DNA-Based Memories
387
Fig. 6. Retrieval times (number of iterations) based on multiple simultaneous of queries.
wet tube experiments [7], we can estimate the actual retrieval time itself in all these
events to be in the order of 1/10 of a second for libraries in the range of 1 to 100 mil-
lions strands in a wet tube.
  It is worth noticing that similar results may be expected for memory updates.
Adding a record is straightforward in DNA-based memories (assuming that the new
record is noncrosshybridizing with the current memory), one can just drop it in the
solution. Deleting a record requires making sure that all copies of the records are re-
trieved (full stringency for perfect recall) and expunged, which reduces deletion to the
problem above. Additional experiments were performed that verified this conclusion.
The problem of adding new crosshybridizing records is of a different nature and was
not addressed in this project.
3.3   DNA-Based Memory Capacity
An issue of paramount importance is the capacity of the memories considered in this
paper. Conventional memories and even memories developed with other technologies
have impressive sizes despite apparent shortcomings such as address-based indexing
and sequential search retrievals. DNA-based memories need to offer a definitive ad-
vantage to make them competitive. Candidates are massive size, associative retrieval,
and straightforward implementation by recombinant biotechnology. We address below
only the first aspect.
Baum [2] claimed that it seemed DNA-based memories could be made with a ca-
pacity larger than the brain, but warned that preventing undesirable cross-
hybridization may reduce the potential capacity of 4
n strands for a library made of n-
mers. Later work on error-prevention has confirmed that the reduction will be orders
of magnitude smaller [6]. Based on combinatorial constraints, [14] combinatorially
obtained some theoretical lower bounds and upper bounds of the number of equi-
length DNA strands. However, from the practical point of view, the question still
remains of determining the size of the largest memories based on oligonucleotides in
effective use (20 to 150-mers).

388
M.H. Garzon, A. Neel, and H. Chen
A preliminary estimation of the runs has been made in several ways. First, a greedy
search of small DNA spaces (up to 9-mers) in [10] by exhaustive searches averaged a
number of 100 code words or less at a minimum h-distance apart of 4 or more, in a
space of at least 4
10 strands, regardless of the random order in which they the entire
spaces were searched. Using the more realistic (but still approximate) thermodynamic
model of Deaton et Al. [5], similar greedy searches turned up libraries of about 1,400
10-mers with nonnegative pairwise Gibbs energies (given by the model.) An in vitro
selection protocol proposed by Deaton et Al. [4] has been tested experimentally and is
expected to produce large libraries. The difficulty is that quantifying the size of the
libraries obtained by the selection protocol is yet an unresolved problem given the
expected size for 20-mers. In a separate experiment simulating this selection protocol,
Edna has produced libraries of about 100 to 150 n-mers (n=10, 11, 12) starting with a
full size DNA space of all n-mers (crosshybridizying) as the seed populations. Further
several simulations of the selection protocol with random seeds of 1024 20-mers as
initial population have consistently produced libraries of no more than 150 20-mers. A
linear extrapolation to the size of the entire population is too risky because the greedy
searches show that sphere packing allows high density in the beginning, but tends to
add more strands very sparsely toward the end of the process. The true growth rate of
the library size as a function of strand size n remains a truly intriguing question.
4   Summary and Conclusions
The reliability and efficiency of DNA-based associative memories has been explored
quantitatively through simulation of reactions in silico on a virtual test tube. They
show that there the region of optimal concentrations for library and probe strands to
minimize retrieval time and avoid excessive concentrations (which tend to lengthen
retrieval times) is about O(logN), where N is the size of the library. Further the re-
trieval time is highly dependent on reactions conditions and the probe, but tends to
stabilize at optimal concentrations. Furthermore, these results remain essentially un-
changed for simultaneous multiple queries if they remain small compared to the li-
brary size (within O(log N).) Previous benchmarks of the virtual tube provide a good
level of confidence that these results extrapolate well to wet tubes with real DNA. The
retrieval times in that case can be estimated in the order of 1/10 of a second. The im-
portant question of how the memory capacity grows as a function of strand size is
certainly sub-exponential, but remains a truly intriguing open question.
An interesting possibility is suggested by the results presented here. The experi-
ments were run in simulation. It is thus conceivable that conventional memories could
be designed in hardware using special-purpose chips of the software simulations. The
chips would run according to the parallelism inherent in VLSI circuits. One iteration
could be run in nanoseconds with current technology. Therefore, once can obtain the
advantages of DNA-based associative recall at varying threshold of stringency in
silico, while retaining the speed, implementation, and manufacturing facilities of
solid-state memories. A further exploration of this idea will be fleshed out elsewhere.

Efficiency and Reliability of DNA-Based Memories
389
References
1. 
L.M. Adleman: Molecular Computation of Solutions to Combinatorial Problems. Science
266 (1994) 1021–1024
2. 
E. Baum, Building An Associative Memory Vastly Larger Than The Brain. Science 268
(1995) 583–585.
3. 
A. Condon, G. Rozenberg (eds.): DNA Computing (Revised Papers). In: Proc. of the 6
th
International Workshop on DNA-based Computers, 2000. Springer-Verlag Lecture Notes
in Computer Science 2054 (2001)
4. 
R. Deaton, R., J. Chen, H. Bi, M. Garzon, H. Rubin, D.H. Wood. A PCR-Based Protocol
for In-Vitro Selection of Non-Crosshybridizing Oligonucleotides (2002). In [11], 105–114
5. 
R.J. Deaton, J. Chen, H. Bi, J.A. Rose: A Software Tool for Generating Non-
crosshybridizing Libraries of DNA Oligonucleotides. In [11], pp. 211–220.
6. 
R. Deaton, M. Garzon, R. E. Murphy, J. A. Rose, D. R. Franceschetti, S.E. Stevens, Jr.
The Reliability and Efficiency of a DNA Computation. Phys. Rev. Lett. 80 (1998) 417–420
7. 
M. Garzon, D. Blain, K. Bobba, A. Neel, M. West: Self-Assembly of DNA-like structures
in silico. Journal of Genetic Programming and Evolvable Machines 4:2 (2003), in press.
8. 
M. Garzon: Biomolecular Computation in silico. Bull. of the European Assoc. For Theo-
retical Computer Science EATCS (2003), in press.
9. 
M. Garzon, C. Oehmen: Biomolecular Computation on Virtual Test Tubes. In: N. Jonoska
and N. Seeman (eds.): Proc. of the 7
th International Workshop on DNA-based Computers,
2001. Springer-Verlag Lecture Notes in Computer Science 2340 (2002) 117–128
10. M. Garzon, R. Deaton, P. Neathery, R.C. Murphy, D.R. Franceschetti, E. Stevens Jr.: On
the Encoding Problem for DNA Computing. In: Proc. of the Third DIMACS Workshop on
DNA-based Computing, U of Pennsylvania. (1997) 230–237
11. M. Hagiya, A. Ohuchi (eds.): Proceedings of the 8
th Int. Meeting on DNA Based
Computers, Hokkaido University, 2002, Springer-Verlag Lecture Notes in Computer Sci-
ence 2568 (2003)
12. J. Lee, S. Shin, S.J. Augh, T.H. Park, B. Zhang: Temperature Gradient-Based DNA Com-
puting for Graph Problems with Weighted Edges. In [11], pp. 41–50.
13. R. Lipton: DNA Solutions of Hard Computational Problems. Science 268 (1995) 542–544
14. A. Marathe, A. Condon, R. Corn: On Combinatorial Word Design. In: E. Winfree and  D.
Gifford (eds.): DNA Based Computers V, DIMACS Series in Discrete Mathematics and
Theoretical Computer Science. 54 (1999) 75–89
15. J.H. Reif, T. LaBean. Computationally Inspired Biotechnologies: Improved DNA Synthe-
sis and Associative Search Using Error-Correcting Codes and Vector Quantization In [3],
pp. 145–172
16. K.A. Schmidt, C.V. Henkel, G. Rozenberg: DNA computing with single molecule detec-
tion. In [3], 336.
17. J.G. Wetmur: Physical Chemistry of Nucleic Acid Hybridization. In: H. Rubin and D.H.
Wood (eds.): Proc. DNA-Based Computers III, U. of Pennsylvania, 1997. DIMACS series
in Discrete Mathematics and Theoretical Computer Science 48 (1999) 1–23
18. E. Winfree, F. Liu, L.A. Wenzler, N.C. Seeman: Design and self-assembly of two-
dimensional DNA crystals. Nature 394 (1998) 539–544

Evolving Hogg’s Quantum Algorithm Using
Linear-Tree GP
Andr´e Leier and Wolfgang Banzhaf
University of Dortmund, Dept. of Computer Science, Chair of Systems Analysis,
44221 Dortmund, Germany
{andre.leier, wolfgang.banzhaf}@cs.uni-dortmund.de
Abstract. Intermediate measurements in quantum circuits compare to
conditional branchings in programming languages. Due to this, quan-
tum circuits have a natural linear-tree structure. In this paper a Genetic
Programming system based on linear-tree genome structures developed
for the purpose of automatic quantum circuit design is introduced. It
was applied to instances of the 1-SAT problem, resulting in evidently
and “visibly” scalable quantum algorithms, which correspond to Hogg’s
quantum algorithm.
1
Introduction
In theory certain computational problems can be solved on a quantum computer
with a lower complexity than possible on classical computers. Therefore, in view
of its potential, design of new quantum algorithms is desirable, although no
working quantum computer beyond experimental realizations has been built so
far. Unfortunately, the development of quantum algorithms is very diﬃcult, since
they are highly non-intuitive and their simulation on conventional computers is
very expensive.
The use of genetic programming to evolve quantum circuits is not a novel
approach. It was elaborated ﬁrst in 1997 by Williams and Gray [21]. Since then,
various other papers [5,1,15,18,17,2,16,14,20] dealt with quantum computing as
an application of genetic programming or genetic algorithms, respectively. The
primary goal of most GP experiments, described in this context, was to demon-
strate the feasibility of automatic quantum circuit design. Diﬀerent GP schemes
and representations of quantum algorithms were considered and tested on vari-
ous problems.
The GP system described in this paper uses linear-tree structures and was
build to achieve more “degrees of freedom” in the construction and evolution
of quantum circuits compared to stricter linear GP schemes (like in [14,18]).
A further goal was to evolve quantum algorithms for the k-SAT problem (only
for k = 1 up to now). In [9,10] Hogg has already introduced quantum search
algorithms for 1-SAT and highly constrained k-SAT. An experimental imple-
mentation of Hogg’s 1-SAT algorithm for logical formulas in three variables is
demonstrated in [13].
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 390–400, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Evolving Hogg’s Quantum Algorithm Using Linear-Tree GP
391
The following section brieﬂy outlines some basics of quantum computing
essential to understand the mathematical principles on which the simulation of
quantum algorithms depends. Section 3 of this paper discusses previous work
on automatic quantum circuit design. Section 4 describes the linear-tree GP
scheme used here. The results of evolving quantum algorithms for the 1-SAT
problem are presented in Sect. 5. The last section summarizes our results and
draws conclusions.
2
Quantum Computing Basics
Quantum computing is the result of a link between quantum mechanics and infor-
mation theory. It is computation based on quantum principles, that is quantum
computers use coherent atomic-scale dynamics to store and to process informa-
tion [19]. The basic unit of information is the qubit which, unlike a classical
bit can exist in a superposition of the two classical states 0 and 1, i. e. with a
certain probability p, resp. 1−p, the qubit is in state 0, resp. 1. In the same way
an n-qubit quantum register can be in a superposition of its 2n classical states.
The state of the quantum register is described by a 2n-dimensional complex
vector (α0, α1, . . . , α2n−1)t, where αk is the probability amplitude correspond-
ing to the classical state k. The probability for the quantum register being in
state k is |αk|2 and from the normalization condition of probability measures it
follows 2n−1
k=0 |αk|2 = 1. It is common usage to write the classical states (the so-
called computational basis states) in the ‘ket’ notation of quantum computing,
as |k⟩= |an−1an−2 . . . a0⟩, where an−1an−2 . . . a0 is the binary representation of
k. Thus, the general state of an n-qubit quantum computer can be written as
|ψ⟩= 2n−1
k=0 αk|k⟩.
The quantum circuit model of computation describes quantum algorithms as
a sequence of unitary – and therefore reversible – transformations (plus some
non-unitary measurement operators), also called quantum gates, which are ap-
plied successively to an initialized quantum state. Usually this state to an n-qubit
quantum circuit is |0⟩⊗n. A unitary transformation operating on n qubits is a
2n × 2n matrix U, with U †U = I. Each quantum gate is entirely determined by
it’s gate type, the qubits, it is acting on, and a certain number of real-valued
(angle) parameters. Figure 1 shows some basic gate types working on one or
two qubits. Similar to the universality property of classical gates, small sets of
quantum gates are suﬃcient to compute any unitary transformation to arbitrary
accuracy. For example, single qubit and CNOT gates are universal for quantum
computation, just as H, CNOT, Phase[π/4] and Phase[π/2] are. In order to be
applicable to an n-qubit quantum computer (with a 2n-dimensional state vector)
quantum gates operating on less than n qubits have to be adapted to higher di-
mensions. For example, let U be an arbitrary single-qubit gate applied to qubit
q of an n-qubit register. Then the entire n-qubit transformation is composed of
the tensor product
I ⊗. . . ⊗I



n−(q+1)
⊗U ⊗I . . . ⊗I



q

392
A. Leier and W. Banzhaf
H = 1/
√
2

1 1
1 −1

Phase[φ] =

1 0
0 eφ

CNOT =



1 0 0 0
0 1 0 0
0 0 0 1
0 0 1 0



Rx[φ] =

cos φ i sin φ
i sin φ cos φ

Ry[φ] =

cos φ sin φ
−sin φ cos φ

Rz[φ] =

e−iφ 0
0
eiφ

Fig. 1. Some basic unitary 1- and 2-qubit transformations: Hadamard-gate H, a Phase-
gate with angle parameter φ, a CNOT-gate, some rotation gates Rx[φ], Ry[φ], Rz[φ]
with rotation angle φ.
Calculating the new quantum state requires 2n−1 matrix-vector-multiplications
of the 2×2 matrix U. It is easy to see, that the costs of simulating quantum cir-
cuits on conventional computers grow exponentially with the number of qubits.
Input gates sometimes known as oracles enable the encoding of problem in-
stances. They may change from instance to instance of a given problem, while the
“surrounding” quantum algorithm remains unchanged. Consequently, a proper
quantum algorithm solving the problem has to achieve the correct outputs for
all oracles representing problem instances. In quantum algorithms like Grover’s
[6] or Deutsch’s [3,4], oracle gates are permutation matrices computing Boolean
functions (Fig. 2, left matrix). Hogg’s quantum algorithm for k-SAT [9,10] uses
a special diagonal matrix, encoding the number of conﬂicts in assignment s,
i. e. the number of false clauses for assignment s in the given logical formula at
position (s, s) (Fig. 2, right matrix).










1 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 1
0 0 0 0 0 0 1 0




















1 0 0 0 0 0
0
0
0 i 0 0 0 0
0
0
0 0 i 0 0 0
0
0
0 0 0 −1 0 0
0
0
0 0 0 0 i 0
0
0
0 0 0 0 0 −1 0
0
0 0 0 0 0 0 −1 0
0 0 0 0 0 0
0 −i










Fig. 2. Examples for oracle matrices. Left matrix: implementation of the AND func-
tion of two inputs. The right-most qubit is ﬂipped, if the two other qubits are ‘1’.
This gate is also called a CCNOT. Right matrix: a diagonal matrix with coeﬃcients
(ic(000), . . . , ic(111)), where c(s) is the number of conﬂicts of assignment s in the formula
¯v1 ∧¯v2 ∧¯v3. For example, the assignment (v1 = true, v2 = false, v3 = true) makes two
clauses false, i. e. c(101) = 2 and i2 = −1.

Evolving Hogg’s Quantum Algorithm Using Linear-Tree GP
393
Quantum information processing is useless without readout (measurement).
When the state of a quantum computer is measured in the computational ba-
sis, result ‘k’ occurs with probability |αk|2. By measurement the superposition
collapses to |k⟩. A partial measurement of a single qubit is a projection into the
subspace, which corresponds to the measured qubit. The probability p of mea-
suring a single qubit q with result ‘0’ (‘1’) is the sum of the probabilities for all
basis states with qubit q = 0 (q = 1). The post-measurement state is just the su-
perposition of these basis states, re-normalized by the factor 1/√p. For example,
measuring the ﬁrst (right-most) qubit of |ψ⟩= α0|00⟩+α1|01⟩+α2|10⟩+α3|11⟩
gives ‘1’ with probability |α1|2 + |α3|2, leaving the post-measurement state
|ψ′⟩= 1/

|α1|2 + |α3|2(α1|01⟩+ α3|11⟩).
According to the quantum principle of deferred measurement, “measurements
can always be moved from an intermediate stage of a quantum circuit to the end
of the circuit” [12]. Of course, such a shift has to be compensated by some other
changes in the quantum circuit.
Note, that quantum measurements are irreversible operators, though it is
usual to call these operators measurement gates. To get a deeper insight into
quantum computing and quantum algorithms the following references might be
of interest to the reader: [12],[7],[8].
3
Previous Work in Automatic Quantum Circuit Design
Williams and Gray focus in [21] on demonstrating a GP-based search heuristic
more eﬃcient than the exhaustive enumeration strategy which ﬁnds a correct
decomposition of a given unitary matrix U into a sequence of simple quantum
gate operations. In contrast, however, to subsequent GP schemes for the evolu-
tion of quantum circuits, a unitary operator solving the given problem had to
be known in advance.
Extensive investigations concerning the evolution of quantum algorithms
were done by Spector et al. [15,18,17,1,2]. In [18] they presented three diﬀerent
GP schemes for quantum circuit evolution: the standard tree-based GP (TGP)
and both stack-based and stackless linear genome GP (SBLGP/SLLGP). These
were applied to evolve algorithms for Deutsch’s two-bit early promise problem,
using TGP, the scaling majority-on problem, using TGP as well, the quantum
four-item database search problem, using SBLGP, and the two-bit-AND-OR
problem, using SLLGP. Better-than-classical algorithms could be evolved for all
but the scaling majority-on problem.
Without doing a thorough comparison Spector et al. pointed out some pros
and cons of the three GP schemes: The tree structure of individuals in TGP sim-
pliﬁes the evolution of scalable quantum circuits, as it seems to be predestined
for “adaptive determination of program size and shape” [18]. A disadvantage
of the tree representation are its higher costs in time, space and complexity.
Furthermore, possible return-value/side-eﬀect interactions may make evolution
more complicated for TGP. The linear representation in SBLGP/SLLGP seems
to be better suited for evolution, because the quantum algorithms are itself se-

394
A. Leier and W. Banzhaf
quential (in accordance with the principle of deferred measurement). Moreover,
the genetic operators in linear GP are simpler to implement and memory re-
quirements are clearly reduced compared to TGP. The return-value/side-eﬀect
interaction is eliminated in SBGL, since the algorithm-building functions do not
return any values. Overall, Spector et al. stated that, applied to their prob-
lems, results appeared to emerge more quickly with SBLGP than with TGP. If
scalability of the quantum algorithms would be not so important, the SLLGP
approach should be preferred.
In [17] and [2] a modiﬁed SLLGP system was applied to the 2-bit-AND-OR
problem, evolving an improved quantum algorithm. The new system is steady-
state rather than generational as its predecessor system, supports true variable-
length genomes and enables distributed evolution on a workstation cluster. Ex-
pensive genetic operators allow for “local hill-climbing search [...] integrated into
the genetic search process”. For ﬁtness evaluation the GP system uses a stan-
dardized lexicographic ﬁtness function consisting of four ﬁtness components:
the number of ﬁtness cases on which the quantum program “failed” (MISSES),
the number of expected oracle-gates in the quantum circuit (EXPECTED-
QUERIES), the maximum probability over all ﬁtness cases of getting the wrong
result (MAX-ERROR) and the number of gates (NUM-GATES).
Another interesting GP scheme is presented in [14] and its function is demon-
strated by generating quantum circuits for the production of two to ﬁve maxi-
mally entangled qubits. In this scheme gates are represented by a gate type and
by bit-strings coding the qubit operands and gate parameters. Qubit operands
and parameters have to be interpreted corresponding to the gate type. Assign-
ing a further binary key to each gate type the gate representation is completely
based on bit strings, where appropriate genetic operators can be applied to.
4
The Linear-Tree GP Scheme
The steady-state GP system described here is a linear-tree GP scheme, intro-
duced ﬁrst in [11]. The structure of the individuals consists of linear program
segments, which are sequences of unitary quantum gates, and branchings, caused
by single qubit measurement gates. Depending on the measurement result (‘0’
or ‘1’), the corresponding (linear) program branch, the ‘0’- or ‘1’-branch, is exce-
cuted. Since measurement results occur with certain probabilities, usually both
branches have to be evaluated. Therefore, the quantum gates in the ‘0’- and
‘1’-branch have to be applied to their respective post-measurement states. From
the branching probabilities the probabilities for each ﬁnal quantum state can be
calculated.
In this way linear-tree GP naturally supports the use of measurements as an
intermediate step in quantum circuits. Measurement gates can be employed to
conditionally control subsequent quantum gates, like an “if-then-else”-construct
in a programming language. Although the principle of deferred measurement
suggests the use of purely sequential individual structures, the linear-tree struc-
ture may simplify legibility and interpretation of quantum algorithms.

Evolving Hogg’s Quantum Algorithm Using Linear-Tree GP
395
The maximum number of possible branches is set by a global system param-
eter; without using any measurement gates the GP system becomes very similar
to the modiﬁed SLLGP version in [17]. From there, we adopted the idea of using
ﬁtness components with certain weights: MISSES, MAX-ERROR and TOTAL-
ERROR (the summed error over all ﬁtness cases) are used in this way. A penalty
function based on NUM-GATES and a global system parameter is used to in-
crease slightly the ﬁtness value for any existing gate in the quantum circuit. In
order to restrict the evolution, in particular at the beginning of a GP run, ﬁtness
evaluation of an individual is aborted if the number of MISSES exceeds a certain
value, set by another global system parameter. The bitlength of gate parame-
ters (interpreted as a fraction of 2π) was ﬁxed to 12 bits which restricts angle
resolution. This corresponds to current precisions for NMR experiments. The ge-
netic operators used here are RANDOM-INSERTION, RANDOM-DELETION
and RANDOM-ALTERATION, each referred to a single quantum gate, plus
LINEAR-XOVER and TREE-XOVER. A GP run terminates when the number
of tournaments exceeds a given value (in our experiments, 500000 tournaments)
or the ﬁtness of a new best individual under-runs a given threshold.
It should be emphasized that the GP system is not designed to directly evolve
scalable quantum circuits. Rather, by scalability we mean that the algorithm
does not only work on n but also on n+1 qubits. At least for the 1-SAT problem,
scalability of the solutions became “visible”, as is shown below.
5
Evolving Quantum Circuits for 1-SAT
The 1-SAT problem for n variables, solved by classical heuristics in O(n) steps,
can be solved even faster on a quantum computer. Hogg’s quantum algorithm,
presented in [9,10], ﬁnds a solution in a single search step, using a clever input
matrix (see Sect. 2 and Fig. 2). Let R denote this input matrix, with Rss = ic(s)
where c(s) is the number of conﬂicts in the assignment s of a given logical 1-SAT
formula in n variables. Thus, the problem description is entirely encoded in this
input matrix. Furthermore, let be U the matrix deﬁned by Urs = 2−n/2(−1)d(r,s),
where d(r, s) is the Hamming distance between r and s. Then the entire algo-
rithm is the sequential application of Hadamard gates applied to n qubits (H⊗n)
initially in state |0⟩, R and U. It can be proven, that the ﬁnal quantum state
is the (equally weighted) superposition of all assignments s with c(s) = 0 con-
ﬂicts.1 A ﬁnal measurement will lead, with equal probability, to one of the 2n−m
solutions, where m denotes the number of clauses in the 1-SAT formula.
We applied our GP system on problem instances of n = 2..4 variables. The
number of ﬁtness cases (the number of formulas) is n
k=1
n
k

2k in total. Each
ﬁtness case consists of an input state (always |0⟩⊗n), an input matrix for the
formula and the desired output. For example,
1 For all 1-SAT (and also maximally constrained 2-SAT) problems Hogg’s algorithm
ﬁnds a solution with probability one. Thus, an incorrect result deﬁnitely indicates
the problem is not soluble [9].

396
A. Leier and W. Banzhaf
(|00⟩,




1 0 0 0
0 i 0 0
0 0 1 0
0 0 0 i



, | −0⟩)
is the ﬁtness case for the 1-SAT formula ¯v2 in two variables v1, v2. Here, the
‘−’ in | −0⟩denotes a “don’t care”, since only the rightmost qubit is essential
to the solutions {v1 = true/false, v2 = false}. That means, an equally weighted
superposition of all solutions is not required.
Table 1 gives some parameter settings for GP runs applied to the 1-SAT
problem.
Table 1. Parameter settings for the 1-SAT problem with n = 4. ∗)After evolving
solutions for n = 2 and n = 3, intermediate measurements seemed to be irrelevant for
searching 1-SAT quantum algorithms, since at least the evolved solutions did not use
them. Without intermediate measurements (gate type M), which constitute the tree
structure of quantum circuits, tree crossover is not applicable. In GP runs for n = 2, 3
the maximum number of measurements was limited by the number of qubits.
Population Size
5000
Tournament Size
16
Basic Gate Types
H,Rx,Ry,Rz,CkNOT,M
Max. Number of Gates
15
Max. Number of Measurments
0∗)
Number of Input Gates
1
Mutation Rate
1
Crossover (XO) Rate
0.1
Linear XO Probability
1∗)
Deletion Probability
0.3
Insertion Probability
0.3
Alteration Probability
0.4
For the two-, three- and four-variable 1-SAT problem 100 GP runs were done
recording the best evolved quantum algorithm of each run. Finally the over-all
best quantum algorithm was determined. For each problem instance our GP
system evolved solutions (Figs. 3 and 4) that are essentially identical to Hogg’s
algorithm. This can be seen at a glance, when noting that U = Rx[3/4π]⊗n.2
The diﬀerences in ﬁtness values of the best algorithms of each GP run, were
negligible, though they diﬀered in length and structure, i. e. in the arrangement
of gate-types. Most quantum algorithms did not make use of intermediate mea-
surements. Details of the performance and convergence of averaged ﬁtness values
over all GP runs can be seen in the three graphs of Fig. 5.
2 Note, that U is equal to Rx[3/4π]⊗n up to a global phase factor, which of course
has no inﬂuence on the ﬁnal measurement results.

Evolving Hogg’s Quantum Algorithm Using Linear-Tree GP
397
Misses:
0
Max. Error:
8.7062e-05
Total Error:
0.0015671
Oracle Number:
1
Gate Number:
10
Fitness Value:
0.00025009
Individual:
H 0
H 1
H 2
INP
RX 6.1083 0
RX 2.6001 0
RX 3.0818 0
RX 2.3577 1
RX 2.3562 2
RZ 0.4019 1
Fig. 3. Extract from the GP system output: After 100 runs this individual was the
best evolved solution to 1-SAT with three variables. Here, INP denotes the speciﬁc
input matrix R.
H 0
H 0
H 1
H 0
H 1
H 2
H 1
H 2
H 3
INP
INP
INP
Rx[3/4 Pi] 0
Rx[3/4 Pi] 0
Rx[3/4 Pi] 0
Rx[3/4 Pi] 1
Rx[3/4 Pi] 1
Rx[3/4 Pi] 1
Rx[3/4 Pi] 2
Rx[3/4 Pi] 2
Rx[3/4 Pi] 3
Fig. 4. The three best, slightly hand-tuned quantum algorithms to 1-SAT with n =
2, 3, 4 (from left to right) after 100 evolutionary runs each. Postprocessing was used to
eliminate introns, i. e. gates which have no inﬂuence on the quantum algorithm or the
ﬁnal measurement results respectively, and to combine two or more rotation gates of the
same sort into one single gate. Here, the angle parameters are stated more precisely in
fractions of π. INP denotes the input gate R as speciﬁed in the text. Without knowledge
of Hogg’s quantum algorithm, there would be strong evidence for the scalability of this
evolved algorithm.
Further GP runs with diﬀerent parameter settings hinted at strong parame-
ter dependencies. For example, an adequate limitation of the maximum number
of gates leads rapidly to good quantum algorithms. In contrast, stronger limita-
tions (somewhat above the length of the best evolved quantum algorithm) made
convergence of the evolutionary process more diﬃcult. We experimented also

398
A. Leier and W. Banzhaf
 0
 0.1
 0.2
 0.3
 0.4
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
Fitness
Tournaments
 0
 0.1
 0.2
 0.3
 0
 2500
 5000
 7500
 10000
 12500
 15000
Fitness
Tournaments
 0
 0.1
 0.2
 0
 5000
 10000  15000  20000  25000  30000  35000  40000
Fitness
Tournaments
Fig. 5. Three graphs illustrating the course of 100 evolutionary runs for quantum
algorithms for the two-, three- and four-variable 1-SAT problem. Errorbars show the
standard deviation for the averaged ﬁtness values of the 100 best evolved quantum
algorithms after a certain number of tournaments. The dotted line marks averaged
ﬁtness values. Convergence of the evolution is obvious.
with diﬀerent gate sets. Unfortunately, for larger gate sets “visible” scalability
was not detectable. GP runs on input gates implementing a logical 1-SAT for-
mula as a permutation matrix, which is a usual problem representation in other
quantum algorithms, did not lead to acceptable results, i. e. quantum circuits
with zero error probability. This may be explained with the additional problem-
speciﬁc information (the number of conﬂicts for each assignment) encoded in
the matrix R. The construction of Hogg’s input representation from some other
representation matrices does not need to be hard for GP at all, but it may re-
quire some more ancillary qubits to work. Note, however, that due to the small
number of runs with these parameter settings the results do not have statistical
evidence.

Evolving Hogg’s Quantum Algorithm Using Linear-Tree GP
399
6
Conclusions
The problems of evolving novel quantum algorithms are evident. Quantum al-
gorithms can be simulated in acceptable time only for very few qubits without
excessive computer power. Moreover, the number of evaluations per individual
to calculate its ﬁtness are given by the number of ﬁtness-cases usually increases
exponentially or even super-exponentially. As a direct consequence, automatic
quantum circuit design seems to be feasible only for problems with suﬃciently
small instances (in the number of required qubits). Thus the examination of
scalability becomes a very important topic and has to be considered with special
emphasis in the future.
Furthermore, as Hogg’s k-SAT quantum algorithm shows, a cleverly designed
input matrix is crucial for the outcome of a GP-based evolution. For the 1-SAT
problem, the additional tree structure in the linear-tree GP scheme did not take
noticeable eﬀect, probably because of the simplicity of the problem solutions.
Perhaps, genetic programming and quantum computing will have a brighter
common future, as soon as quantum programs do not have to be simulated on
classical computers, but can be tested on true quantum computers.
Acknowledgement. This work is supported by a grant from the Deutsche
Forschungsgemeinschaft (DFG). We thank C. Richter and R. Stadelhofer for
numerous discussions and helpful comments.
References
[1] H. Barnum, H. Bernstein, and L. Spector, Better-than-classical circuits for OR
and AND/OR found using genetic programming, 1999, LANL e-preprint quant-
ph/9907056.
[2] H. Barnum, H. Bernstein, and L. Spector, Quantum circuits for OR and AND of
ORs, J. Phys. A: Math. Gen., 33 (2000), pp. 8047–8057.
[3] D. Deutsch, Quantum theory, the Church-Turing principle and the universal quan-
tum computer, Proc. R. Soc. London A, 400 (1985), pp. 97–117.
[4] D. Deutsch and R. Jozsa, Rapid solution of problems by quantum computation,
Proc. R. Soc. London A, 439 (1992), pp. 553–558.
[5] Y. Ge, L. Watson, and E. Collins, Genetic algorithms for optimization on a quan-
tum computer, in Proceedings of the 1st International Conference on Unconven-
tional Models of Computation (UMC), C. Calude, J. Casti, and M. Dinneen, eds.,
DMTCS, Auckland, New Zealand, Jan. 1998, Springer, Singapur, pp. 218–227.
[6] L. Grover, A fast quantum mechanical algorithm for database search, in Proceed-
ings of the 28th Annual ACM Symposium on Theory of Computing (STOC),
ACM, ed., Philadelphia, Penn., USA, May 1996, ACM Press, New York, pp. 212–
219, LANL e-preprint quant-ph/9605043.
[7] J. Gruska, Quantum Computing, McGraw-Hill, London, 1999.
[8] M. Hirvensalo, Quantum Computing, Natural Computing Series, Springer-Verlag,
2001.
[9] T. Hogg, Highly structured searches with quantum computers, Phys. Rev. Lett.,
80 (1998), pp. 2473–2476.

400
A. Leier and W. Banzhaf
[10] T. Hogg, Solving highly constrained search problems with quantum computers, J.
Artiﬁcial Intelligence Res., 10 (1999), pp. 39–66.
[11] W. Kantschik and W. Banzhaf, Linear-tree GP and its comparison with other GP
structures, in Proceedings of the 4th European Conference on Genetic Program-
ming (EUROGP), J. Miller, M. Tomassini, P. Lanzi, C. Ryan, A. Tettamanzi,
and W. Langdon, eds., vol. 2038 of LNCS, Lake Como, Italy, Apr. 2001, Springer,
Berlin, pp. 302–312.
[12] M. Nielsen and I. Chuang, Quantum Computation and Quantum Information,
Cambridge University Press, 2000.
[13] X. Peng, X. Zhu, X. Fang, M. Feng, M. Liu, and K. Gao, Experimental imple-
mentation of Hogg’s algorithm on a three-quantum-bit NMR quantum computer,
Phys. Rev. A, 65 (2002).
[14] B. Rubinstein, Evolving quantum circuits using genetic programming, in Proceed-
ings of the 2001 Congress on Evolutionary Computation, IEEE, ed., Seoul, Korea,
May 2001, IEEE Computer Society Press, Silver Spring, MD, USA, pp. 114–151.
The ﬁrst version of this paper already appeared in 1999.
[15] L. Spector, Quantum computation - a tutorial, in GECCO-99: Proceedings of
the Genetic and Evolutionary Computation Conference, W. Banzhaf, J. Daida,
A. Eiben, M. H. Garzon, V. Honavar, M. Jakiela, and R. Smith, eds., Orlando,
Florida, USA, Jul. 1999, Morgan Kaufmann Publishers, San Francisco, pp. 170–
197.
[16] L. Spector, The evolution of arbitrary computational processes, IEEE Intelligent
Systems, (2000), pp. 80–83.
[17] L. Spector, H. Barnum, H. Bernstein, and N. Swamy, Finding a better-than-
classical quantum AND/OR algorithm using genetic programming, in Proceedings
of the 1999 Congress on Evolutionary Computation, P. Angeline, Z. Michalewicz,
M. Schoenauer, X. Yao, and A. Zalzala, eds., Washington DC, USA, Jul. 1999,
IEEE Computer Society Press, Silver Spring, MD, USA, pp. 2239–2246.
[18] L. Spector, H. Barnum, H. Bernstein, and N. Swamy, Quantum Computing Appli-
cations of Genetic Programming, in Advances in Genetic Programming, L. Spec-
tor, U.-M. O’Reilly, W. Langdon, and P. Angeline, eds., vol. 3, MIT Press, Cam-
bridge, MA, USA, 1999, pp. 135–160.
[19] A. Steane, Quantum computation, Reports on Progress in Physics, 61 (1998),
pp. 117–173, LANL e-preprint quant-ph/9708022.
[20] A. Surkan and A. Khuskivadze, Evolution of quantum algorithms for computer
of reversible operators, in Proceedings of the 2002 NASA/DoD Conference on
Evolvable Hardware (EH), IEEE, ed., Alexandria, Virginia, USA, Jul. 2002, IEEE
Computer Society Press, Silver Spring, MD, USA, pp. 186–187.
[21] C. Williams and A. Gray, Automated Design of Quantum Circuits, in Explorations
in Quantum Computing, C. Williams and S. Clearwater, eds., Springer, New York,
1997, pp. 113–125.

Hybrid Networks of Evolutionary Processors
Carlos Mart´ın-Vide1, Victor Mitrana2⋆, Mario J. P´erez-Jim´enez3⋆⋆, and
Fernando Sancho-Caparrini3
1 Rovira i Virgili University,
Research Group in Mathematical Linguistics,
P¸ca. Imperial T`arraco 1, 43005 Tarragona, Spain,
cmv@correu.urv.es
2 University of Bucharest, Faculty of Mathematics and Computer Science,
Str. Academiei 14, 70109 Bucharest, Romania,
mitrana@funinf.math.unibuc.ro
3 University of Seville,
Department of Computer Science and Artiﬁcial Intelligence,
{Mario.Perez,Fernando.Sancho}@cs.us.es
Abstract. A hybrid network of evolutionary processors consists of sev-
eral processors which are placed in nodes of a virtual graph and can
perform one simple operation only on the words existing in that node
in accordance with some strategies. Then the words which can pass the
output ﬁlter of each node navigate simultaneously through the network
and enter those nodes whose input ﬁlter was passed. We prove that these
networks with ﬁlters deﬁned by simple random-context conditions, used
as language generating devices, are able to generate all linear languages
in a very eﬃcient way, as well as non-context-free languages. Then, when
using them as computing devices, we present two linear solutions of the
Common Algorithmic Problem.
1
Introduction
This work is a continuation of the investigation started in [1] and [2] where one
has considered a mechanism inspired from cell biology, namely networks of evo-
lutionary processors, that is networks whose nodes are very simple processors
able to perform just one type of point mutation (insertion, deletion or substi-
tution of a symbol). These nodes are endowed with ﬁlters which are deﬁned by
some membership or random context condition.
Another source of inspiration is a basic architecture for parallel and dis-
tributed symbolic processing, related to the Connection Machine [13] as well as
⋆Corresponding author. This work, done when this author was visiting the Depart-
ment of Computer Science and Artiﬁcial Intelligence of the University of Seville, was
supported by the Generalitat de Catalunya, Direcci´o General de Recerca (PIV2001-
50)
⋆⋆Work supported by the project TIC2002-04220-C03-01 of the Ministerio de Ciencia
y Tecnolog´ıa of Spain, coﬁnanced by FEDER funds
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 401–412, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

402
C. Mart´ın-Vide et al.
the Logic Flow paradigm [6]. This consists of several processors, each of them
being placed in a node of a virtual complete graph, which are able to handle
data associated with the respective node. Each node processor acts on the local
data in accordance with some predeﬁned rules, and, then local data becomes
a mobile agent which can navigate in the network following a given protocol.
Only such data can be communicated which can pass a ﬁltering process. This
ﬁltering process may require to satisfy some conditions imposed by the sending
processor, by the receiving processor or by both of them. All the nodes send
simultaneously their data and the receiving nodes handle also simultaneously all
the arriving messages, according to some strategies, see, e.g., [7,13].
Starting from the premise that data can be given in the form of strings, [4]
introduces a concept called network of parallel language processors in the aim of
investigating this concept in terms of formal grammars and languages. Networks
of language processors are closely related to grammar systems, more speciﬁcally
to parallel communicating grammar systems [3]. The main idea is that one can
place a language generating device (grammar, Lindenmayer system, etc.) in any
node of an underlying graph which rewrite the strings existing in the node, then
the strings are communicated to the other nodes. Strings can be successfully
communicated if they pass some output and input ﬁlter.
Mechanisms introduced in [1] and [2] simplify as much as possible the net-
works of parallel language processors deﬁned in [4]. Thus, in each node is placed
a very simple processor, called evolutionary processor, which is able to perform
a simple rewriting operation only, namely either insertion of a symbol or substi-
tution of a symbol by another, or deletion of a symbol. Furthermore, ﬁlters used
in [4] are simpliﬁed in some versions deﬁned in [1,2].
In spite of these simpliﬁcations, these mechanisms are still powerful. In [2]
networks with at most six nodes having ﬁlters deﬁned by the membership to a
regular language condition are able to generate all recursively enumerable lan-
guages no matter the underlying structure. This result does not surprise since
similar characterizations have been reported in the literature, see, e.g., [5,11,10,
12,14]. Then one considers networks with nodes having ﬁlters deﬁned by random
context conditions which seem to be closer to the biological possibilities of im-
plementation. Even in this case, rather complex languages like non-context-free
ones, can be generated.
However, these very simple mechanisms are able to solve hard problems in
polynomial time. In [1] it is presented a linear solution for an NP-complete
problem, namely the Bounded Post Correspondence Problem, based on networks
of evolutionary processors able to substitute a letter at any position in the string
but insert or delete a letter in the right end only. This restriction was discarded
in [2], but the new variants were still able to solve in linear time another NP-
complete problem, namely the “3-colorability problem”.
In the present paper, we consider hybrid networks of evolutionary processors
in which each deletion or insertion node has its own working mode (at any
position, in the left end, or in the right end) and its own way of deﬁning the
input and output ﬁlter. Thus, in the same network one may co-exist nodes in

Hybrid Networks of Evolutionary Processors
403
which deletion is done at any position and nodes in which deletion is done in
the right end only. Also the deﬁnition of the ﬁlters of two nodes, though both
are random context ones, may diﬀer.
This model may be viewed as a biological computing model in the following
way: each node is a cell having a genetic information encoded in DNA sequences
which may evolve by local evolutionary events, that is point mutations (inser-
tion, deletion or substitution of a pair of nucleotides). Each node is specialized
just for one of these evolutionary operations. Furthermore, the biological data in
each node is organized in the form of arbitrarily large multisets of strings (each
string appears in an arbitrarily large number of copies), each copy being pro-
cessed in parallel such that all the possible evolutions events that can take place
do actually take place. Deﬁnitely, the computational process described here is
not exactly an evolutionary process in the Darwinian sense. But the rewriting
operations we have considered might be interpreted as mutations and the ﬁlter-
ing process might be viewed as a selection process. Recombination is missing but
it was asserted that evolutionary and functional relationships between genes can
be captured by taking into consideration local mutations only [17]. Furthermore,
we were not concerned here with a possible biological implementation, though a
matter of great importance.
The paper is organized as follows: in the next section we recall the some
basic notions from formal language theory and deﬁne the hybrid networks of
evolutionary processors. Then, we brieﬂy investigate the computational power
of these networks as language generating devices. We prove that all regular lan-
guages over an n-letter alphabet can be generated in an eﬃcient way by networks
having the same underlying structure and show that this result can be extended
to linear languages. Furthermore, we provide a non-context-free language which
can be generated by such networks. The last section is dedicated to hybrid net-
works of evolutionary processors viewed as computing (problem solving) devices;
we present two linear solutions of the so-called Common Algorithmic Problem.
The latter one needs linearly bounded resources (symbols and rules) as well.
2
Preliminaries
We start by summarizing the notions used throughout the paper. An alphabet is
a ﬁnite and nonempty set of symbols. The cardinality of a ﬁnite set A is written
card(A). Any sequence of symbols from an alphabet V is called string (word)
over V . The set of all strings over V is denoted by V ∗and the empty string is
denoted by ε. The length of a string x is denoted by |x| while the number of
occurrences of a letter a in a string x is denoted by |x|a. Furthermore, for each
nonempty string x we denote by alph(x) the minimal alphabet W such that
x ∈W ∗.
We say that a rule a →b, with a, b ∈V ∪{ε} is a substitution rule if both
a and b are not ε; it is a deletion rule if a ̸= ε and b = ε; it is an insertion rule
if a = ε and b ̸= ε. The set of all substitution, deletion, and insertion rules over
an alphabet V are denoted by SubV , DelV , and InsV , respectively.

404
C. Mart´ın-Vide et al.
Given a rule as above σ and a string w ∈V ∗, we deﬁne the following actions
of σ on w:
– If σ ≡a →b ∈SubV , then
σ∗(w) = σr(w) = σl(w) =

{ubv : ∃u, v ∈V ∗(w = uav)},
{w}, otherwise
– If σ ≡a →ε ∈DelV , then
σ∗(w) =

{uv : ∃u, v ∈V ∗(w = uav)},
{w}, otherwise
σr(w) =

{u : w = ua},
{w}, otherwise
σl(w) =

{v : w = av},
{w}, otherwise
– If σ ≡ε →a ∈InsV , then
σ∗(w) = {uav : ∃u, v ∈V ∗(w = uv)}, σr(w) = {wa}, σl(w) = {aw}.
α ∈{∗, l, r} expresses the way of applying an evolution rule to a word, namely
at any position (α = ∗), in the left (α = l), or in the right (α = r) end of the
word, respectively. For every rule σ, action α ∈{∗, l, r}, and L ⊆V ∗, we deﬁne
the α-action of σ on L by σα(L) = 
w∈L σα(w). Given a ﬁnite set of rules M,
we deﬁne the α-action of M on the word w and the language L by:
M α(w) =

σ∈M
σα(w) and M α(L) =

w∈L
M α(w),
respectively.
In what follows, we shall refer to the rewriting operations deﬁned above as
evolutionary operations since they may be viewed as linguistical formulations of
local gene mutations. For two disjoint subsets P and F of an alphabet V and a
word over V , we deﬁne the predicates
ϕ(1)(w; P, F) ≡P ⊆alph(w) ∧F ∩alph(w) = ∅
ϕ(2)(w; P, F) ≡alph(w) ⊆P
ϕ(3)(w; P, F) ≡P ⊆alph(w) ∧F ̸⊆alph(w)
The construction of these predicates is based on random-context conditions
deﬁned by the two sets P (permitting contexts) and F (forbidding contexts).
For every language L ⊆V ∗and β ∈{(1), (2), (3)}, we deﬁne:
ϕβ(L, P, F) = {w ∈L | ϕβ(w; P, F)}.
An evolutionary processor over V is a tuple (M, PI, FI, PO, FO), where:
– Either (M ⊆SubV ) or (M ⊆DelV ) or (M ⊆InsV ). The set M represents
the set of evolutionary rules of the processor. As one can see, a processor is
“specialized” in one evolutionary operation, only.
– PI, FI ⊆V are the input permitting/forbidding contexts of the processor,
while PO, FO ⊆V are the output permitting/forbidding contexts of the
processor.

Hybrid Networks of Evolutionary Processors
405
We denote the set of evolutionary processors over V by EPV .
A hybrid network of evolutionary processors (HNEP for short) is a 7-tuple
Γ = (V, G, N, C0, α, β, i0), where:
– V is an alphabet.
– G = (XG, EG) is an undirected graph with the set of vertices XG and the
set of edges EG. G is called the underlying graph of the network.
– N : XG −→EPV is a mapping which associates with each node x ∈XG the
evolutionary processor N(x) = (Mx, PIx, FIx, POx, FOx).
– C0 : XG −→2V ∗is a mapping which identiﬁes the initial conﬁguration of
the network. It associates a ﬁnite set of words with each node of the graph
G.
– α : XG −→{∗, l, r}; α(x) gives the action mode of the rules of node x on
the words existing in that node.
– β : XG −→{(1), (2), (3)} deﬁnes the type of the input/output ﬁlters of
a node. More precisely, for every node, x ∈XG, the following ﬁlters are
deﬁned:
input ﬁlter:
ρx(·) = ϕβ(x)(·; PIx, FIx),
output ﬁlter:
τx(·) = ϕβ(x)(·; POx, FOx).
That is, ρx(w) (resp. τx) indicates whether or not the string w can pass the
input (resp. output) ﬁlter of x. More generally, ρx(L) (resp. τx(L)) is the set
of strings of L that can pass the input (resp. output) ﬁlter of x.
– i0 ∈XG is the output node of the HNEP.
We say that card(XG) is the size of Γ. If α(x) = α(y) and β(x) = β(y) for
any pair of nodes x, y ∈XG, then the network is said to be homogeneous. In
the theory of networks some types of underlying graphs are common, e.g., rings,
stars, grids, etc. We shall investigate here networks of evolutionary processors
with their underlying graphs having these special forms. Thus a HNEP is said to
be a star, ring, or complete HNEP if its underlying graph is a star, ring, grid, or
complete graph, respectively. The star, ring, and complete graph with n vertices
is denoted by Sn, Rn, and Kn, respectively.
A conﬁguration of a HNEP Γ as above is a mapping C : XG −→2V ∗which
associates a set of strings with every node of the graph. A conﬁguration may
be understood as the sets of strings which are present in any node at a given
moment. A conﬁguration can change either by an evolutionary step or by a
communication step. When changing by an evolutionary step, each component
C(x) of the conﬁguration C is changed in accordance with the set of evolutionary
rules Mx associated with the node x and the way of applying these rules α(x).
Formally, we say that the conﬁguration C′ is obtained in one evolutionary step
from the conﬁguration C, written as C =⇒C′, iﬀ
C′(x) = M α(x)
x
(C(x)) for all x ∈XG.
When changing by a communication step, each node processor x ∈XG sends
one copy of each string it has, which is able to pass the output ﬁlter of x, to all
the node processors connected to x and receives all the strings sent by any node
processor connected with x providing that they can pass its input ﬁlter.

406
C. Mart´ın-Vide et al.
Formally, we say that the conﬁguration C′ is obtained in one communication
step from conﬁguration C, written as C ⊢C′, iﬀ
C′(x) = (C(x) −τx(C(x))) ∪

{x,y}∈EG
(τy(C(y)) ∩ρx(C(y))) for all x ∈XG.
Let Γ an HNEP, the computation in Γ is a sequence of conﬁgurations
C0, C1, C2, . . ., where C0 is the initial conﬁguration of Γ, C2i =⇒C2i+1 and
C2i+1 ⊢C2i+2, for all i ≥0. By the previous deﬁnitions, each conﬁguration Ci is
uniquely determined by the conﬁguration Ci−1. If the sequence is ﬁnite, we have
a ﬁnite computation. If one uses HNEPs as language generating devices, then
the result of any ﬁnite or inﬁnite computation is a language which is collected
in the output node of the network.
For any computation C0, C1, . . ., all strings existing in the output node at
some step belong to the language generated by the network. Formally, the lan-
guage generated by Γ is L(Γ) = 
s≥0 Cs(i0).
The time complexity of computing a ﬁnite set of strings Z is the minimal
number s such that Z ⊆s
t=0 Ct(i0).
3
Computational Power of HNEP as Language
Generating Devices
First, we compare these devices with the simplest generative grammars in the
Chomsky hierarchy. In [2], one proves that the families of regular and context-free
languages are incomparable with the family of languages generated by homoge-
neous networks of evolutionary processors. HNEPs are more powerful, namely
Theorem 1. Any regular language can be generated by any type (star, ring,
complete) of HNEP.
Proof. Let A = (Q, V, δ, q0, F) be a deterministic ﬁnite automaton; without loss
of generality we may assume that δ(q, a) ̸= q0 holds for each q ∈Q and each
a ∈V . Furthermore, we assume that card(V ) = n. We construct the following
complete HNEP (the proof for the other underlying structures is left to the
reader):
Γ = (U, K2n+3, N, C0, α, β, f).
The alphabet U is deﬁned by U = V ∪V ′ ∪Q ∪{sa | s ∈Q, a ∈V }, where
V ′ = {a′ | a ∈V }. The set of nodes of the complete underlying graph is
{x0, x1, xf} ∪V ∪V ′, and the other parameters are given in Table 1, where
s and b are generic states from Q and symbols from V , respectively. One can
easily prove by induction that
1. δ(q, x) ∈F for some q ∈Q \ {q0} if and only if xq ∈C8|x|(0).
2. x is accepted by A (x ∈L(A)) if and only if x ∈Cp(f) for any p ≥8|x| + 1.
Therefore, L(A) is exactly the language generated by Γ.

Hybrid Networks of Evolutionary Processors
407
Table 1.
Node
M
PI
FI
PO FO C0 α β
x0
{q →sb}δ(s,b)=q
∅
{sb}s,b ∪{b′}b
∅
∅
F ∗(1)
a ∈V
ε →a′
{sa}s ∪V
Q
U
∅
∅
l (2)
a′ ∈V ′
{sa →s}s
{a′}
Q
∅
∅
∅∗(1)
x1
{b′ →b}b
∅
{sb}s,b
∅
∅
∅∗(1)
xf
q0 →ε
{q0}
V ′
∅
V
∅r (1)
Surprisingly enough, the size of the above HNEP, hence its underlying struc-
ture, does not depend on the number of states of the given automaton. In other
words, this structure is common to all regular languages over the same alphabet,
no matter the state complexity of the automata recognizing them. Furthermore,
all strings of the same length are generated simultaneously.
Since each linear grammar can be transformed into an equivalent linear gram-
mar with rules of the form A →aB, A →Ba, A →ε only, the proof of the above
theorem can be adapted for proving the next result.
Theorem 2. Any linear language can be generated by any type of HNEP.
We do not know whether these networks are able to generate all context-free
languages, but they can generate non-context-free languages as shown below.
Theorem 3. There are non-context-free languages that can be generated by any
type of HNEP.
Proof. We construct the following complete HNEP which generates the non-
context-free language L = {wcx | x ∈{a, b}∗, w is a permutation of x}:
Γ = (V, K9, N, C0, α, β, y2),
where V = {a, b, a′, b′, Xa, Xb, X}, XK9 = {y0, y1, y2, ya, yb, ¯ya, ¯yb, ˜ya, ˜yb}, and
the other parameters are given in Table 2, where u is a generic symbol in {a, b}.
The working mode of this network is rather simple. In the node y0 there are
generated strings of the form Xn for any n ≥1. They can leave this node
as soon as they receive a D at their right end, the only node able to receive
them being y1. In y1, either Xa or Xb is added to their right end. Thus, for
a given n, the strings XnDXa and XnDXb are produced in y1. Let us follow
what happens with the strings XnDXa, a similar analysis applies to the strings
XnDXb as well. So, XnDXa goes to ya where any occurrence of X is replaced
by a′ in diﬀerent identical copies of XnDXa. In other words, ya produces each
string Xka′Xn−k−1DXa, 0 ≤k ≤n −1. All these strings are sent out but no
node, except ¯ya, can receive them. Here, Xa is replaced by a and the obtained
strings are sent to ˜ya where a is substituted to a′. As long as the strings contains
occurrences of X, they follow the same itinerary, namely y1, yu, ¯yu, ˜yu, u ∈{a, b},
depending on what symbol Xa or Xb is added in y1.
After a ﬁnite number of such cycles, when no occurrence of X is present in
the strings, they are received by y2 where D is replaced by c in all of them, and

408
C. Mart´ın-Vide et al.
Table 2.
Node
M
PI
FI
PO
FO
C0 α β
y0
{ε →X, ε →D}
∅
{a′, b′, a, b, Xa, Xb} {D}
∅
{ε} r (1)
y1
{ε →Xa, ε →Xb}
∅
{Xa, Xb, a′, b′}
∅
∅
∅
r (1)
yu
{X →u′}
{Xu}
{a′, b′}
∅
∅
∅
∗(1)
¯yu
{Xu →u}
{u′}
∅
∅
∅
∅
∗(1)
˜yu
{u′ →u}
{u′}
{Xa, Xb}
∅
∅
∅
∗(1)
y2
{D →c}
∅
{X, a′, b′, Xa, Xb}
∅
{a, b}
∅
∗(1)
they remain in this node for ever. By these explanations, the node y2 collects all
strings of L and any string which arrives in this node belongs to L.
A more precise characterization of the family of languages generated by
HNEPs remains to be done.
4
Solving Problems with HNEPs
HNEPs may be used for solving problems in the following way. For any instance
of the problem the computation in the associated HNEP must be ﬁnite. In
particular, this means that there is no node processor specialized in insertions.
If the problem is a decision problem, then at the end of the computation, the
output node provides all solutions of the problem encoded by strings, if any,
otherwise this node will never contain any word. If the problem requires a ﬁnite
set of words, this set will be in the output node at the end of the computation.
In other cases, the result is collected by speciﬁc methods which will be indicated
for each problem.
In [2] one provides a complete homogeneous NEP of size 7m+2 which solves
in O(m + n) time an (n, m)–instance of the “3-colorability problem” with n
vertices and m edges. In the sequel, following the descriptive format for three
NP-complete problems presented in [9] we present a solution to the Common
Algorithmic Problem. The three problems are:
1. The maximum independent set: Given an undirected graph G = (X, E),
where X is the ﬁnite set of vertices and E is the set of edges given as a
family of sets of two vertices, ﬁnd the cardinality of a maximal subset (with
respect to inclusion) of X which does not contain both vertices connected
by any edge in E.
2. The vertex cover problem: Given an undirected graph ﬁnd the cardinality of
a minimal set of vertices such that each edge has at least one of its extremes
in this set.
3. The satisﬁability problem: For a given set P of Boolean variables and a ﬁnite
set U of clauses over P, does a truth assignment for the variables of P exist
satisfying all the clauses of U?
For detailed formulations and discussions about their solutions, the reader is
referred to [8].

Hybrid Networks of Evolutionary Processors
409
These problems can be viewed as special cases of the following algorithmic
problem, called the Common Algorithmic Problem (CAP) in [9]: let S be a
ﬁnite set and F be a non-empty family of subsets of S. Find the cardinality of a
maximal subset of S which does not include any set belonging to F. The sets in
F are called forbidden sets. We say that (F, S) is an (card(S), card(F))–instance
of CAP
Let us show how the three problems mentioned above can be obtained as
special cases of CAP. For the ﬁrst problem, we just take S = X and F = E.
The second problem is obtained by letting S = X and F contains all sets o(x) =
{x}∪{y ∈X | {x, y} ∈E}. The cardinality one looks for is the diﬀerence between
the cardinality of S and the solution of the CAP. The third problem is obtained
by letting S = P ∪P ′, where P ′ = {p′ | p ∈P}, and F = {F(C) | C ∈U},
where each set F(C) associated with the clause C is deﬁned by
F(C) = {p′ | p appears in C} ∪{p | ¬p appears in C}.
From this it follows that the given instance of the satisﬁability problem has a
solution if and only if the solution of the constructed instance of the CAP is
exactly the cardinality of P.
First, we present a solution of the CAP based on homogeneous HNEPs.
Theorem 4. Let (S = {a1, a2, . . . , an}, F = {F1, F2, . . . , Fm}), be an (n, m)–
instance of the CAP. It can be solved by a complete homogeneous HNEP of size
m + 2n + 2 in O(m+n) time.
Proof. We construct the complete homogeneous HNEP
Γ = (U, Km+2n+2, N, C0, α, β).
Since the result will be collected in a way which will be speciﬁed later, the
output node is missing. The alphabet of the network is
U = S ∪¯S ∪S′ ∪{Y, Y1, Y2, . . . , Ym+1} ∪{b} ∪{Z0, Z1, . . . , Zn} ∪
{Y ′
1, Y ′
2, . . . , Y ′
m+1} ∪{X1, X2, . . . , Xn},
where ¯S and S′ are copies of S obtained by taking the barred and primed copies
of all letters from S, respectively. The nodes of the underlying graph are:
x0, xF1, xF2, . . . , xFm, xa1, xa2, . . . , xan, y0, y1, . . . , yn.
The mapping N is deﬁned by:
N(x0) = ({Xi →ai, Xi →¯ai | 1 ≤i ≤n} ∪{Y →Y1} ∪{Y ′
i →Yi+1 |
1 ≤i ≤m}, {Y ′
i | 1 ≤i ≤m}, ∅, ∅, {Xi | 1 ≤i ≤n} ∪{Y }),
N(xFi) = ({¯a →a′ | a ∈Fi}, {Yi}, ∅, ∅, ∅), for all 1 ≤i ≤m,
N(xaj) = ({a′
j →¯aj} ∪{Yi →Y ′
i | 1 ≤i ≤m}, {a′
j}, ∅, ∅, {a′
j} ∪{Yi |
1 ≤i ≤m}), for all 1 ≤j ≤n,
N(yn) = ({¯ai →b | 1 ≤i ≤n} ∪{Ym+1 →Z0}, {Ym+1}, ∅, {Z0, b}, ¯S),
N(yn−i) = ({b →Zi}, {Zi−1}, ∅, {b, Zi}, ∅), for all 1 ≤i ≤n.

410
C. Mart´ın-Vide et al.
The initial conﬁguration C0 is deﬁned by
C0(x) =

{X1X2 . . . XnY } if x = x0
∅, otherwise
Finally, α(x) = ∗and β(x) = (1), for any node x.
A few words on how the HNEP above works: in the ﬁrst 2n steps, in the ﬁrst
node one obtains 2n diﬀerent words w = x1x2 . . . xnY , where each xi is either
ai or ¯ai. Each such string w can be viewed as encoding a subset of S, namely
the set containing all symbols of S which appear in w. After replacing Y by Y1
in all these strings they are sent out and xF1 is the only node which can receive
them. After one rewriting step, only those strings encoding subsets of S which
do not include F1 will remain in the network, the others being lost. The strings
which remain are easily recognized since they have been obtained by replacing a
barred copy of symbol with a primed copy of the same symbol. This means that
this symbol is not in the subset encoded by the string but in F1. In the nodes
xai the modiﬁed barred symbols are restored and the symbol Y ′
1 is substituted
for Y1. Now, the strings go to the node x0 where Y2 is substituted for Y ′
1 and
the whole process above resumes for F2. This process lasts for 8m steps.
The last phase of the computation makes use of the nodes yj, 0 ≤j ≤n.
The number we are looking for is given by the largest number of symbols from
S in the strings from yn. It is easy to note that the strings which cannot leave
yn−i have exactly n −i such symbols, 0 ≤i ≤n. Indeed, only the strings which
contains at least one occurrence of b can leave yn and reach yn−1. Those strings
which do not contain any occurrence of b have exactly n symbols from S. In yn−1,
Z1 is substituted for an occurrence of b and those strings which still contain b
leave this node for yn−2 and so forth. The strings which remain here contain
n −1 symbols from S. Therefore, when the computation is over, the solution of
the given instance of the CAP is the largest j such that yj is nonempty. The last
phase is over after at most 4n + 1 steps. By the aforementioned considerations,
the total number of steps is at most 8m + 4n + 3, hence the time complexity of
solving each instance of the CAP of size (n, m) is O(m + n).
As far as the time and memory resources the HNEP above uses, the total
number of symbols is 2m + 5n + 4 and the total number of rules is
mn + m + 5n + 2 +
m

i=1
card(Fi) ∈Θ(mn)
The same problem can be solved in a more economic way, regarding especially
the number of rules, with HNEPs, namely
Theorem 5. Any instance of the CAP can be solved by a complete HNEP of
size m + n + 1 in O(m+n) time.
Proof. For the same instance of the CAP as in the previous proof, we construct
the complete HNEP Γ = (U, Km+n+1, N, C0, α, β).
The alphabet of the network is U = S ∪S′ ∪{Y1, Y2, . . . , Ym+1} ∪{b} ∪
{Z0, Z1, . . . , Zn}. The other parameters of the network are given in Table 3.

Hybrid Networks of Evolutionary Processors
411
Table 3.
Node
M
PI
FI PO FO
C0
α β
x0
{a′
i →ai}i
{a′
1}
∅
∅
{a′
i}i {a′
1 . . . a′
nY1} ∗(1)
{a′
i →T}i
xFj
{Yj →Yj+1}
{Yj}
Fj
∅
U
∅
∗(3)
yn
{T →Z0}
{Ym+1} ∅{T}
∅
∅
∗(1)
yn−i
{T →Zi}
{Zi−1}
∅{T}
∅
∅
∗(1)
In the table above, i ranges from 1 to n and j ranges from 1 to m. The
reasoning is rather similar to that from the previous proof. The only notable
diﬀerence concerns the phase of selecting all strings which do not contain any
symbol from any set Fj. This selection is simply accomplished by the way of
deﬁning the ﬁlters of the nodes xFj. The time complexity is now 2m + 4n + 1 ∈
O(m + n), while the needed resources are: m + 3n + 3 symbols and m + 3n + 1
rules.
5
Concluding Remarks and Future Work
We have considered a mechanism inspired from cell biology, namely hybrid net-
works of evolutionary processors, that is networks whose nodes are very simple
processors able to perform just one type of point mutation (insertion, deletion or
substitution of a symbol). These nodes are endowed with a ﬁlter which is deﬁned
by some random context conditions which seem to be close to the possibilities
of biological implementation. A rather suggestive view of these networks is that
of a group of connected cells that are similar to each other and have the same
purpose, that is a tissue. It is worth mentioning some similarities with the mem-
brane systems deﬁned in [16]. In that work, the underlying structure is a tree
and the (biological) data is transferred from one region to another by means of
some rules. A more closely related protocol of transferring data among regions
in a membrane system was considered in [15].
We ﬁnish with a natural question: We are conscious that our mechanisms
have likely no biological relevance. Then why to study them? We believe that by
combining our knowledge about behavior of cell populations with advanced for-
mal theories from computer science, we could try to deﬁne computational models
based on the interacting molecular entities. To this aim we need to accomplish
the followings:
(1) Understanding which features of the behavior of molecular entities form-
ing a biological system can be used for designing computing networks with an
underlying structure inspired from that of the biological system.
(2) Understanding how to control the data navigating in the networks via
precise protocols
(3) Understanding how to eﬀectively design the networks.
The results obtained in this paper suggest that these mechanisms might be a
reasonable example of global computing due to the real and massively parallelism

412
C. Mart´ın-Vide et al.
involved in molecular interactions. Therefore, they deserve a deep theoretical
investigation as well as an investigation of biological limits of implementation in
our opinion.
References
1. Castellanos, J., Mart´ın-Vide, C., Mitrana, V., Sempere, J.: Solving NP-complete
problems with networks of evolutionary processors. IWANN 2001 (J. Mira, A.
Prieto, eds.), LNCS 2084, Springer-Verlag (2001) 621–628.
2. Castellanos, J., Mart´ın-Vide, C., Mitrana, V., Sempere, J.: Networks of evolution-
ary processors. Submitted (2002).
3. Csuhaj-Varj´u, E., Dassow, J., Kelemen, J., P˘aun, G.: Grammar Systems, Gordon
and Breach, 1993.
4. Csuhaj-Varj´u, E., Salomaa, A.: Networks of parallel language processors. New
Trends in Formal Languages (Gh. P˘aun, A. Salomaa, eds.), LNCS 1218, Springer
Verlag (1997) 299–318.
5. Csuhaj-Varj´u, E., Mitrana, V.: Evolutionary systems: a language generating device
inspired by evolving communities of cells. Acta Informatica 36 (2000) 913–926.
6. Errico, L., Jesshope, C.: Towards a new architecture for symbolic processing. Arti-
ﬁcial Intelligence and Information-Control Systems of Robots ’94 (I. Plander, ed.),
World Sci. Publ., Singapore (1994) 31–40.
7. Fahlman, S.E., Hinton, G.E., Seijnowski, T.J.: Massively parallel architectures for
AI: NETL, THISTLE and Boltzmann machines. Proc. AAAI National Conf. on
AI, William Kaufman, Los Altos (1983) 109–113.
8. Garey, M., Johnson, D.: Computers and Intractability. A Guide to the Theory of
NP-completeness, Freeman, San Francisco, CA, 1979.
9. Head, T., Yamamura, M., Gal, S.: Aqueous computing: writing on molecules. Proc.
of the Congress on Evolutionary Computation 1999, IEEE Service Center, Piscat-
away, NJ (1999) 1006–1010.
10. Kari, L.: On Insertion and Deletion in Formal Languages, Ph.D. Thesis, University
of Turku, 1991.
11. Kari, L., P˘aun, G., Thierrin, G., Yu, S.: At the crossroads of DNA computing and
formal languages: Characterizing RE using insertion-deletion systems. Proc. 3rd
DIMACS Workshop on DNA Based Computing, Philadelphia (1997) 318–333.
12. Kari, L., Thierrin, G.: Contextual insertion/deletion and computability. Informa-
tion and Computation 131 (1996) 47–61.
13. Hillis, W.D.: The Connection Machine, MIT Press, Cambridge, 1985.
14. Mart´ın-Vide, C., P˘aun, G., Salomaa, A.: Characterizations of recursively enumer-
able languages by means of insertion grammars. Theoretical Computer Science 205
(1998) 195–205.
15. Mart´ın-Vide, Mitrana, V., P˘aun, G.: On the power of valuations in P systems.
Computacion y Sistemas 5 (2001) 120–128.
16. P˘aun, G.: Computing with membranes. J. Comput. Syst. Sci. 61(2000) 108–143.
17. Sankoﬀ, D. et al.: Gene order comparisons for phylogenetic inference: Evolution of
the mitochondrial genome. Proc. Natl. Acad. Sci. USA 89 (1992) 6575–6579.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 413–424, 2003.
© Springer-Verlag Berlin Heidelberg 2003
DNA-Like Genomes for Evolution in silico
Michael West, Max H. Garzon, and Derrel Blain
Computer Science, University of  Memphis
373 Dunn Hall, Memphis, TN 38152
{mrwest1, mgarzon}@memphis.edu, derrelb@earthlink.net
Abstract. We explore the advantages of DNA-like genomes for evolutionary
computation in silico. Coupled with simulations of chemical reactions, these ge-
nomes offer greater efficiency, reliability, scalability, new computationally fea-
sible fitness functions, and more dynamic evolutionary algorithms. The proto-
type application is the decision problem of HPP (the Hamiltonian Path Prob-
lem.) Other applications include pre-processing of protocols for biomolecular
computing and novel fitness functions for evolution in silico.
1   Introduction
The advantages of using DNA molecules for advances in computing, known as bio-
molecular computing (BMC), have been widely discussed [1], [3]. They range from
increasing speed by using massively parallel computations to the potential storage of
huge amounts of data fitting into minuscule spaces. Evolutionary algorithms have been
used to find word designs to implement computational protocols [4]. More recently,
driven by efficiency and reliability considerations, the ideas of BMC have been ex-
plored for computation in silico by using computational analogs of DNA and RNA
molecules [5]. In this paper, a further step with this idea is taken by exploring the use
of DNA-like genomes and online fitness for evolutionary computation.
The idea of using sexually split genomes (based on pair attraction) has hardly been
explored in evolutionary computation and genetic algorithms. Overwhelming evidence
from biology shows that “the [evolutionary] essence of sex is Mendelian recombina-
tion” [11]. DNA is the basic genomic representation of virtually all life forms on
earth. The closest approach of this type is the DNA-based computing approach of
Adleman [1]. We show that an interesting and intriguing interplay can exist between
the ideas of biomolecular-based and silicon-based computation. By enriching Adle-
man’s solution to the Hamiltonian Path Problem (HPP) with fitness-based selection in
a population of potential solutions, we show how these algorithms can exploit bio-
molecular and traditional computing techniques for improving solutions to HPP on
conventional computers. Furthermore, it is conceivable that these fitness functions
may be implemented in vitro in the future, and so improve the efficiency and reliabil-
ity of solutions to HPP with biomolecules as well.

414
M. West, M.H. Garzon, and D. Blain
In Section 2, we describe the experiments performed for this purpose, including the
programming environment and the genetic algorithms based on DNA-like genomes. In
Section 3, we discuss the results of the experiments. A preliminary analysis of some of
these results has been presented in [5], but here we present further results and a more
complete analysis. Finally, we summarize the results, discuss the implications of ge-
netic computation, and envision further work.
2   Experimental Design
As our prototype we took the problem that was used by Adleman [1], the Hamiltonian
Path Problem (HPP), for a proof-of-concept to establish the feasibility of DNA-based
computation. An instance of the problem is a digraph and a given source and destina-
tion; the problem is to determine whether there exists a path from the source to the
destination that passes through each vertex in the digraph exactly once. Solutions to
this problem have a wide-ranging impact in combinatorial optimization areas such as
route planning and network efficiency.
In Adleman’s solution [1], the problem is solved by encoding vertices of the graph
with unique strands of DNA and encoding edges so that their halves will hybridize
with the end vertex molecules. Once massive numbers of these molecules are put in a
test tube, they will hybridize in multiple ways and form longer molecules ultimately
representing all possible paths in the digraph. To find a Hamiltonian path, various
extraction steps are taken to filter out irrelevant paths, such as those not starting at the
source vertex or ending at the destination. Good paths must also have exactly as many
vertices as there are in the graph, and each vertex has to be unique within the final
path. Any paths remaining represent desirable solution Hamiltonian paths.
There have been several improvements on this technique. In [10], the authors at-
tempt to automate Adleman’s solution so that the protocols more intelligently con-
struct promising paths. Another improvement [2] uses reflective PCR to restrict or
eliminate duplicated vertices in paths. In [8], the authors extend Adleman’s solution,
by adding weights associated with melting temperatures to solve another NP-complete
problem, the Traveling Salesman Problem (TSP).
We further these genetic techniques by adding several on-line fitness functions for
an implementation in silico. By rewriting these biomolecular techniques within the
framework of traditional computing, we hope to begin the exploration of algorithms
based on concepts inspired by BMC. In this case, a large population of possible solu-
tions is evolved in a process that is also akin to a developmental process. Specifically,
a population of partially formed solutions is maintained that could react (hybridize), in
a pre-specified manner, with other partial solutions within the population to form a
more complete (fitter) solution. Several fitness functions ensure that the new solution
inherits the good traits of the mates in the hybridization. For potential future imple-
mentation in vitro, the fitness functions are kept consistent with biomolecular com-
puting by placing the genomes within a simulation of a test tube to allow for random
movement and interaction. Fitness evaluation is thus more attuned to developmental

DNA-Like Genomes for Evolution in silico
415
and environmental conditions than customary fitness functions solely dependent on
genome composition.
2.1   Virtual Test Tubes
The experimental runs were implemented using an electronic simulation of a test tube,
the virtual test tube Edna of Garzon et al. [5], [7] which simulates BMC protocols in
silico. As compared to a real test tube, Edna provides an environment where DNA
analogs can be manipulated much more efficiently, can be programmed and controlled
much more easily, cost much less, and produce results comparable to real test tubes
[5]. Users simply need to create object-oriented programming classes (in C++) speci-
fying the objects to be used and their interactions. The basic design of the entities that
are put in Edna represents each nucleotide within DNA strands as a single character
and the entire strand of DNA as a string, which may contain single- or double-stranded
sections, bulges, and other secondary structures. An unhybridized strand represents a
strand of DNA from the 5’-end to the 3’-end. In addition to the actual DNA strand
composition, other statistics were also saved such as the vertices making up the strand
and the number of encounters since extension.
The interactions among objects in Edna are chemical reactions through hybridiza-
tions and ligations resulting in longer paths. They can result in one or both reactants
being destroyed and a new entity possibly being created. In our case, we wanted to
allow the entities that matched to hybridize to each other’s ends so that an edge could
hybridize to its adjacent vertex. We called this reaction extension since the path, ver-
tex, or edge represented by one entity is extended by the path, vertex, or edge repre-
sented by the other entity, in analogy with the PCR reaction used with DNA. Edna
simulates the reactions in successive iterations. One iteration moves the objects ran-
domly in the tube’s container (the RAM really) and updates their status according to
the specified interactions based on proximity parameters that can be varied within the
interactions. The hybridization reactions between strands were controlled by the h-
distance [6] of hybridization affinity. Roughly speaking, the h-distance between two
strands provides the number of Watson-Crick mismatching pairs in a best alignment of
the two strands; strands at distance 0 are complementary, while the hybridization af-
finity decreases as the h-distance increases. Extension was allowed if the h-distance
was zero (which would happen any time the origin or destination of a path hybridized
with one of its adjacent edges); or half the length of a single vertex or edge (such as
when any vertex encountered an adjacent edge); or, more generally, when two paths,
both already partially hybridized, encountered each other, and each had an unhy-
bridized segment (of length equal to half the length of a vertex or edge) representing a
matching vertex and edge. These requirements essentially ensured perfect matches
along the sections of the DNA that were supposed to hybridize. Well-chosen DNA
encodings make this perfectly possible in real test tubes [4].
The complexity of the test tube protocols can be measured by counting the number
of iterations necessary to complete the reactions or achieve the desired objective. Al-
ternatively, one can measure the wall clock time. The number of iterations taken be-

416
M. West, M.H. Garzon, and D. Blain
fore a correct path is found has the advantage of being indifferent to the speed of the
machine(s) running the experiment. However, it cannot be a complete picture because
each iteration will last longer as more entities are put in the test tube. For this reason,
processor time (wall clock) was also measured.
2.2   Fitness Functions
Our genetic approach to solving HPP used fitness functions to be enforced online as
the reactions proceeded. The first stage, which was used as a benchmark, included
checks that vertices did not repeat themselves, called promise fitness. This original
stage also enforced a constant number of the initial vertices and edges in the test tube
in order to ensure an adequate supply of vertices and edges to form paths as needed.
Successive refinements improve on the original by using three types of fitnesses:
extension fitness, demand fitness, and repetition fitness, as described below. The goal
in adding these fitnesses was to improve the efficiency of path formation. The purpose
of the fitnesses implemented here was to bring down the number of iterations it took to
find a solution since Edna’s speed, although parallel, decreases with more DNA. To-
ward this goal, we aimed at increasing the opportunity for an object to encounter an-
other object that is likely to lead to a correct path. This entailed increasing the quantity
of entities that seemed to lead to a good path (were more fit) and decreasing the con-
centration of those entities that were less fit. By removing the unlikely paths, we
moved to improve the processor time by lowering the overall concentration in the test
tube. At this point, the only method to regulate which of its adjacent neighbors an
entity encounters is by adjusting the concentration and hence adjusting the probability
that its neighbors are of a particular type.
Promise Fitness. As part of the initial design, we limited the type of extensions that
were allowed to occur beyond the typical requirement of having matching nucleotides
and an h-distance as described above. Any two entities that encountered each other
could only hybridize if they did not contain any repeated vertices. It was checked
during the encounter by comparing a list of vertices that were represented by each
strand of DNA. A method similar to this was proposed in [2] to work in vitro. As a
consequence, much of the final screening otherwise needed to find the correct path
was eliminated. Searching for a path can stop once one is found that contains as many
vertices as are in the graph. Since all of the vertices are guaranteed to be unique, this
path is guaranteed to pass through all of the vertices in the graph. Because the origin
and destination are encoded as half the length of any other vertex, the final path’s
strand can only have them on the two opposite ends and hence the path travels from
the origin to the destination.

DNA-Like Genomes for Evolution in silico
417
Constant Concentration Enhancement. The initial design also kept the concen-
tration of the initial vertices and edges constant. Simply put, whenever vertices and
edges encountered each other and were extended, neither of the entities was removed
although the new entity was still put into the test tube. It is as if the two original
entities were copied before they hybridized and all three were returned to the mixture.
The same mechanism was used when the encountering objects were not single vertices
or edges but instead were paths. This, however, did not guarantee that the
concentration of any type of path remained constant since new paths could still be
created. The motivation behind this enhancement was to allow all possible paths to be
created without worrying about running out of some critical vertex or edge. It also
removed some of the complications about different initial concentrations of certain
vertices or edges and what paths may be more likely to be formed. However, this
fitness, while desirable and enforceable in silico (although not easily in vitro just yet)
creates a huge number of molecules that made the simulation slow and inefficient.
Extension Fitness. The most obvious paths to be removed are lazy paths that are not
being extended. These paths could be stuck in dead-ends where no extension to a
Hamiltonian path is possible. To make finding them easier, all paths were allowed to
have the same, limited number of encounters without being extended (an initial
lifespan) which, when met, would result in their being removed from the tube. If,
however, a path was extended before meeting its lifespan then the lifespan of both
reacting objects was increased by 50%. The new entity created during an extension
received the larger lifespan of its two parents.
Demand Fitness. The concentration of vertices and edges in the tube can be tweaked
based on the demand for each entity to participate in reactions. The edges that are used
most often (e.g., bridge edges) have a high probability of being in a correct
Hamiltonian path since they are likely to be a single or critical connection between
sections of the graph. Hence we increase the concentration of edges that are used the
most often. Since all vertices must be in a correct solution, those vertices that are not
extended often have a disadvantage in that they are less likely to be put into the final
solution. In order to remedy this, vertices that are not used often have their
concentration increased. The number of encounters and the number of extensions for
each entity was stored so a ratio of extensions to encounters was used to implement
demand fitness. To prevent the population of vertices and edges from getting out of
control, we set a maximum number of any individual vertex or edge to eight unless
otherwise noted.
Repetition Fitness. To prevent the tube from getting too full with identical strands,
repetition fitness was implemented. It filtered out low performing entities that were
repeated often throughout the tube. Whenever an entity encountered another entity, the
program checked to see if they encoded the same information. If they did, then they
did not extend, and they increased their count of encounters with the same path. Once
a path encountered a duplicate of itself too many times, it was removed if it was a low
enough performer in terms of its ratio of extensions to encounters.

418
M. West, M.H. Garzon, and D. Blain
2.3   Test Graphs and Experimental Conditions
Graphs for the experiments were made using Model A of random graphs [12]. Given a
number of vertices, an edge existed between two vertices with probability given by a
parameter p= (0.2, 0.4, or 0.6) of including an edge (more precisely, an arc) from the
set of all possibilities. For positive instances, one witness Hamiltonian path was placed
randomly connecting source to destination. For negative instances, the vertices were
divided into two random sets, one containing the origin and one containing the desti-
nation; no path was allowed to connect the origin set to the set containing the destina-
tion, although the reverse was allowed so that the graph may be connected.
The input to Edna was a set of non-crosshybridizing strands of size 64 consisting of
20-oligomers designed by a genetic algorithm using the h-distance as fitness criterion.
One copy of each vertex and edge was placed initially in the tube. The quality of the
encoding set is such that even under a mildly stringent hybridization criterion, two
sticky ends will not hybridize unless they’re perfect Watson-Crick complements. In
the first set of experiments, the retrieval time was measured in a variety of conditions
including variable library concentration, variable probe concentrations, and joint vari-
able concentration. At first, we permitted only paths that were promising to become
Hamiltonian. Later, other fitness constraints were added to make the path assembly
process smarter as discussed below with the results.
Each experiment was broken down into many different runs of the application all
with related configurations. All of the experiments went through several repetitions
where one or two parameters were slightly changed so that we could evaluate the
differences over these parameters (number of vertices and edge density), although we
sometimes changed other parameters such as maximum concentration allowed, maxi-
mum number of repeated paths, or tube size. Unless otherwise noted, all repetitions
were run 30 times with the same parameters, although a different randomly generated
graph was used for each run. We report below the averages of the various performance
measures. A run was considered unsuccessful if it went through 3000 iterations with-
out finding a correct solution, in which case the run was not included within the aver-
ages. We began with the initial implementation as discussed above and added each
fitness so that each could be studied without the other fitnesses interfering. Finally we
investigated the scalability of our algorithms by adding a population control parameter
and running the program on graphs with more vertices.
3   Analysis of Results
The initial implementation provided us with a benchmark from which to judge the
fitness efficiency. In terms of iterations (Fig. 1, left) and processor time (Fig. 1, right),
the results of this first experiment are not at all surprising. Both measures increase as
the number of vertices increases. There is also a noticeable trend where the 40% edge
densities take the most time. Edge density of 20% is faster because the graph contains
fewer possible paths to search through whereas 60% edge density shows a decrease in
time of search because the additional edges provide significantly more correct solu-

DNA-Like Genomes for Evolution in silico
419
tions. It should be noted that altogether there were only two unsuccessful attempts,
both with 9 vertices, one at 20% edge density and the other at 40% edge density. This
places the probability of success with these randomized graphs above 99%.
Fig. 1. Successful completion time for the baseline runs (only unique vertices and constant
concentration restrictions in force) in number of iterations (left) and processor time (right)
The first comparison made was with extension fitness. The test was done with the
initial lifespan set to 150 and the maximum lifespan also set to 150. As seen in Fig. 2,
the result cut the number of iterations 54% for 514 fewer iterations on average.
Fig. 2. Successful completion times with extension fitness
From what data is available at this time, demand fitness did not show as impressive an
improvement as extension fitness although it still seemed to help. The greatest gain
from this fitness is expected to be for graphs with larger numbers of vertices where
small changes in the number of vertices and edges will have more time to have a large
effect. The number of iterations recorded, on average, can be seen in Fig. 3. The
minimum ratio of extensions to encounters before an edge was copied, the edge ratio,
was set to .17. The maximum ratio of extensions to encounters below which a vertex
0
500
1000
1500
2000
Time 
(Iterations)
5
6
7
8
9
20%
40%
60%
Vertices
Edges
0
500
1000
1500
Real
Time
 (s)
5
6
7
8
9
20%
40%
60%
Vertices
Edges
0
500
1000
1500
2000
Time 
(Iterations)
5
6
7
8
9
20%
40%
60%
Vertices
Edges

420
M. West, M.H. Garzon, and D. Blain
was copied, the vertex ratio, was set to .07. Although it was not measured, the proces-
sor time for this fitness seemed to be considerably greater then that of the other fit-
nesses.
Fig. 3. Successful completion times with demand fitness
The last fitness to be implemented, repetition fitness, provided a 49% decrease in
iterations resulting in 465 less iterations on average (Fig. 4). The effect seems to be-
come especially pronounced as the number of vertices increases.
Fig. 4. Successful completion times with the addition of repetition fitness
Finally, we combined all of the fitnesses together. The results can be seen in Fig. 5 in
terms of iterations (left) and in terms of processor time (right). Note that the scale for
both graphs changed from the comparable ones above. We also increased the radius of
each entity from one to two. The initial lifespan of entities was 140, and it was al-
lowed to reach a maximum lifespan of 180. The edge ratio was set to .16, and the
vertex ratio was set to .07. For demand fitness, the number of paths allowed was 20,
and the removal ratio was .04. All of the fitnesses running together resulted in de-
creasing the number of iterations by 93% for 880 iterations less, on average. The proc-
essor time was cut by 69% saving, on average, 219.90 seconds per run.
0
500
1000
1500
2000
Time 
(Iterations)
5
6
7
8
9
20%
40%
60%
Vertices
Edges
0
1000
2000
Time 
(Iterations)
5
6
7
8
9
20%
40%
60%
Vertices
Edges

DNA-Like Genomes for Evolution in silico
421
Fig. 5. Successful completion time with all fitnesses running in terms of number of iterations
(left) and running time (right)
An important objective of these experiments is to explore the limits of Adleman’s
approach, at least in silico. What is the largest problem that could be solved? In order
to allow the program to run on graphs with large numbers of vertices, we put an upper
limit on the number of entities present in the tube at any time. Each entity, of course,
takes up a certain amount of memory and processing time so this limitation would
help keep the program’s memory usage in check. Unfortunately, when the limit on the
number of entities is reached, the fitnesses, if they are configured with reasonable
settings, will not remove very many paths during each iteration meaning that many
new paths cannot be added. The dark red line in Fig. 6 shows the results; as the num-
ber of entities in the tube reaches the maximum, only a small number of entities are
removed, thus not allowing room for many new entities to be created and preventing
new, possibly good paths, from forming.
It is necessary to not only limit the population but also to control it. The desired ef-
fect would be for the fitnesses to be aggressive as the entity count nears the maximum
and reasonable as it falls back down to some minimum. Additionally it would be ad-
vantageous for the more aggressive settings to be applied to shorter paths and not
longer ones since the shorter paths can be remade much faster then the longer ones.
Longer paths have more “memory” of what may constitute a good solution. In order to
achieve this, once the maximum number of vertices was reached a population control
parameter was multiplied by the values of the extension and repetition fitnesses. The
population control parameter is made up of two parts: the vertex effect, used on paths
with less vertices so that they are more likely to be effected by the population control
parameter, and the entities effect, used to change the population control parameter as
the number of entities in the tube changes. The vertex effect is calculated by:
 number of vertices in path / largest number of vertices in any path) .
(1)
	

 	
	
(max entities – actual entities in the tube) / (max entities – min entities) .
(2)
The population control parameter is then calculated using the vertex effect and entities
effect with:

422
M. West, M.H. Garzon, and D. Blain
Entities Effect + ( 1 – Entities Effect ) * Vertex Effect .
(3)
	
  !"#	!!
and minimum vertices of 6000, the dark blue line (population control parameter) in
Fig. 6 shows the number of entities added over time. In order to show that the popula-
tion control parameter also has the effect of improving the quality of the search, Fig. 6
also shows the length of the longest path, in terms of number of vertices times 100, for
both the use of just a simple maximum (in light red) and when using the population
control parameter (in light blue).
Comparison of Simple Maximum versus use of a 
Population Control Parameter
0
500
1000
1500
2000
2500
3000
0
1000
2000
3000
Iterations
Number of Entities
100 * Number of 
Vertices
number of entities
added with
population control
number of entities
added with simple
maximum
length of longest
path with
population control
length of longest
path with simple
maximum
Fig. 6. Comparison of use of a simple maximum versus a population control parameter in terms
of both the number of entities added over time and the length of the longest path
Under these conditions, random graphs under 10 vertices can be run with high reli-
ability on a single processor in a matter of hours. The nature of the approach in this
paper is instantly scalable to a cluster of processors. Experiments under way may test
whether running on a cluster of p processors, Edna is really able to handle random
graphs of about 10*p vertices, the theoretical maximum.
4   Summary and Conclusions
The results of this paper provide a preliminary estimation of the improved effective-
ness and reliability of evolutionary computations in vitro that DNA-like genomic
representations and environmentally dependent online fitness functions may bring to
evolutionary computation. DNA-like computation brings in advantages that biological
molecules (DNA, RNA and the like) have gained in the course of millions of years of
evolution [11], [7]. First, their operation is inherently parallel and distributable to any
number of processors, with the consequent computational advantages. Further, their
computational mode is asynchronous and includes massive communications over

DNA-Like Genomes for Evolution in silico
423
noisy media, load balancing, and decentralized control. Second, it is equally clear that
the savings in cost and perhaps even time, at least in the range of feasibility of small
clusters of conventional sequential computers, is enormous. The equivalent biochemi-
cal protocols in silico can solve the same problems with a few hundred virtual mole-
cules while requiring trillions of molecules in wet test tubes. Virtual DNA thus inher-
its the customary efficiency, reliability, and control now standard in electronic com-
puting, hitherto only dreamed of in wet tube computations.
On the other hand, it is also interesting to contemplate the potential to scale these
algorithms up to very large graphs when conducting these experiments, either in a real
or in virtual test tubes. Biomolecules seem unbeatable by electronics in their ability to
pack enormous amounts of information in tiny regions of space and to perform their
computations with very high thermodynamical efficiency [13]. This paper also sug-
gests that this efficiency can be brought to evolutionary algorithms in silico as well
using the DNA-inspired architecture Edna used herein.
References
1.
Adleman, L.M.: Molecular Computation of Solutions to Combinatorial Problems. In:
Science, Vol. 266. (1994) 1021-1024. http://citeseer.nj.nec.com/adleman94molecular.html
2.
Arita, M., Suyama, A., Hagiya, M.: A heuristic approach for Hamiltonian Path Problem
with molecules. In: Proceedings of the Second Annual Genetic Programming Conference
(GP-97), Morgan Kaufmann Publishers (1997) 457–461
3.
Condon, A., Rozenburg, G. (eds.): DNA Computing (Revised Papers). In: Proc. of the 6
th
International Workshop on DNA-based Computers. Leiden University, The Netherlands
(2000). Springer-Verlag Lecture Notes in Computer Science 2054

Deaton, R., Murphy, R., Rose, J., Garzon, M., Franceschetti, D., Stevens Jr., S.E.: Good
Encodings for DNA Solution to Combinatorial Problems. In Proc. IEEE Conference on
Evolutionary Computation, IEEE/Computer Society Press. (1997) 267–271
5.
Garzon, M., Blain, D., Bobba, K., Neel, A., West, M.: Self-Assembly of DNA-like struc-
tures In Silico. In Journal of Genetic Programming and Evolvable Machines 4:2 (2003), in
press
6.
M. Garzon, P. Neathery, R. Deaton, R.C. Murphy, D.R. Franceschetti,S.E. Stevens, Jr.. A
New Metric for DNA Computing. In: J.R. Koza, K. Deb, M. Dorigo, D.B. Fogel, M. Gar-
zon, H. Iba,  R.L. Riolo (eds.): Proc. 2nd Annual Genetic Programming Conference, San
Mateo, CA: Morgan Kaufmann (1997) 472–478
7.
Garzon, M., Oehmen, C.: Biomolecular Computation on Virtual Test Tubes, In: Proc. 7th
Int. Meeting on DNA Based Computers, Springer-Verlag Lecture Notes in Computer Sci-
ence 2340 (2001) 117–128
8.
Lee, J., Shin, S., Augh, S.J., Park, T.H., Zhang, B.: Temperature Gradient-Based DNA
Computing for Graph Problems with Weighted Edges. In: Hagiya, M. and Ohuchi, A.
(eds): Proceedings of the 8
th Int. Meeting on DNA Based Computers (DNA8), Hokkaido
University, Springer-Verlag Lecture Notes in Computer Science 2568 (2002) 73–84
9.
Lipton, R.: DNA Solutions of Hard Computational Problems. Science 268 (1995) 542-544.

424
M. West, M.H. Garzon, and D. Blain
10. Morimoto, N., Masanori, A., Suyama, A.: Solid Phase Solution to the Hamiltonian Path
Problem. In: DNA Based Computers III, DIMACS Series in Discrete Mathematics and
Theoretical Computer Science, Vol. 48 (1999) 193–206
11. Sigmund, K: Games of Life. Oxford University Press (1993) 145
12. Spencer, J.: Ten Lectures on the Probabilistic Method. In: CMBS 52, Society for Industrial
and Applied Mathematics, Philadelphia (1987) 17–28
13. Wetmur, J.G.: Physical Chemistry of Nucleic Acid Hybridization. In: Rubin, H. and Wood,
D.H. (eds.): Proc. DNA-Based Computers III, University of Pennsylvania, June 1997.
DIMACS series in Discrete Mathematics and Theoretical Computer Science 48 (1999) 1–
23
14. Wood, D.H., Chen, J., Lemieux, B., Cedeno, W.: A design for DNA computation of the
OneMax problem. In: Garzon, M., Conrad, M. (eds.): Soft Computing in Biomolecules.
Vol. 5:1. Springer-Verlag, Berlin Heidelberg New York (2001) 19–24

String Binding-Blocking Automata⋆
M. Sakthi Balan
Department of Computer Science and Engineering,
Indian Institute of Technology, Madras
Chennai – 600036, India
sakthi@cs.iitm.ernet.in
In a similar way to DNA hybridization, antibodies which speciﬁcally recognize
peptide sequences can be used for calculation [3,4]. In [4] the concept of peptide
computing via peptide-antibody interaction is introduced and an algorithm to
solve the satisﬁability problem is given. In [3], (1) it is proved that peptide com-
puting is computationally complete and (2) a method to solve two well-known
NP-complete problems namely Hamiltonian path problem and exact cover by
3-set problem (a variation of set cover problem) using the interactions between
peptides and antibodies is given.
In our earlier paper [1], we proposed a theoretical model called as binding-
blocking automata (BBA) for computing with peptide-antibody interactions. In
[1] we deﬁne two types of transitions - leftmost(l) and locally leftmost(ll) of BBA
and prove that the acceptance power of multihead ﬁnite automata is sandwiched
between the acceptance power of BBA in l and ll transitions. In this work we
deﬁne a variant of binding-blocking automata called as string binding-blocking
automata and analyze the acceptance power of the new model.
The model of binding-blocking automaton can be informally said as a ﬁnite
state automaton (reading a string of symbols at a time) with (1) blocking and
unblocking functions and (2) priority relation in reading of symbols. Blocking
and unblocking facilitates skipping 1 some symbols at some instant and reading
it when it is necessary.
In the sequel we state some results from [1,2] - (1) for every BBA there
exists an equivalent BBA without priority, (2) for every language accepted by
BBA with l transition, there exists BBA with ll transitions accepting the same
language, (3) for every language accepted by BBA with l transition there is
an equivalent multi-head ﬁnite automata which accepts the same language and
(4) for every language L accepted by a multi-head ﬁnite automaton there is a
language L′ accepted by BBA such that L can be written in the form h−1(L′)
where h is a homomorphism from L to L′.
The basic model of the string binding-blocking automaton is very similar to
a BBA but for the blocking and unblocking. Some string of symbols (starting
form the head’s position) can be blocked from being read by the head. So only
those symbols which are not already read and not blocked can be read by the
head. The ﬁnite control of the automaton is divided into three sets of states
namely blocking states, unblocking states and general reading states. A read
symbol can not be read gain, but a blocked symbol can be unblocked and read.
⋆Financial support from Infosys Technologies Limited, India is acknowledged
1 running through the symbols without reading
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 425–426, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

426
M.S. Balan
Let us suppose the input string is y. At any time the system can be in any
one of the three states - reading state, blocking state or unblocking state. In
reading state the system can read a string of symbols (say l symbols) at a time
and move its head l positions to the right. In the blocking state q, the system
blocks a string of symbols as speciﬁed by the blocking function (say x ∈L where
L ∈βb(q), x ∈Sub(y) 2) starting from the position of the head. The string x
satisﬁes the maximal property i.e., there exists no z ∈L such that x ∈Pre(z)
3 and z ∈Sub(y). When the system is in the unblocking state q the recently
blocked string x ∈Sub(y) and x ∈L where L ∈βub(q) is unblocked. We note
that the head can only read symbols which are neither read nor blocked. The
symbols which are read by the head are called marked symbols, which are blocked
are called as blocked symbols.
A string binding-blocking automaton with D-transition is denoted by strbbaD
and the language accepted by the above automaton is denoted by StrBBAD. If
the blocking languages are ﬁnite languages then the above system is represented
by strbba(Fin).
We show that strbbal system is more powerful than bba system working in l
transition by showing that L = {anban | n ≥1} is accepted by strbbal but not
by any bba working in l transition.
The above language is accepted by strbball.
The language L = {a2n+1(aca)2n+1 | n ≥1} shows that strbball system is
more powerful than bba system working in ll transition.
We also prove the following results,
1. For any bball we can construct an equivalent strbball.
2. For every L ∈StrBBAl there exists a random-context grammar RC with
Context-free rules such that L(RC) = L.
3. For every strbbaD, P there is an equivalent strbbaD, Q such that there is
only one accepting state and there is no transition from the accepting state.
Hence by above examples and results we have L(bball) ⊂L(strbball) and
L(bbal) ̸= L(strbbal)
References
1. M.Sakthi Balan and Kamala Krithivasan. Blocking-binding automata. poster pre-
sentation in Eigth International Confernce on DNA based Computers, 2002.
2. M.Sakthi Balan and Kamala Krithivasan. Normal-forms of binding-blocking au-
tomata. poster presentation in Unconventional Models of Computing, 2002.
3. M.Sakthi Balan, Kamala Krithivasan, and Y.Sivasubramanyam. Peptide comput-
ing – universality and complexity. In Natasha Jonoska and Nadrian Seeman, edi-
tors, Proceedings of Seventh International Conference on DNA Based Computers –
DNA7, LNCS, volume 2340, pages 290–299, 2002.
4. Hubert Hug and Rainer Schuler. Strategies for the developement of a peptide com-
puter. Bioinformatics, 17:364–368, 2001.
2 Sub(y) is the set of all sub-strings of y
3 Pre(z) is the set of all preﬁxes of z

On Setting the Parameters of QEA for Practical
Applications: Some Guidelines Based on
Empirical Evidence
Kuk-Hyun Han and Jong-Hwan Kim
Department of Electrical Engineering and Computer Science,
Korea Advanced Institute of Science and Technology (KAIST),
Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea
{khhan, johkim}@rit.kaist.ac.kr
Abstract. In this paper, some guidelines for setting the parameters
of quantum-inspired evolutionary algorithm (QEA) are presented. Al-
though the performance of QEA is excellent, there is relatively little or
no research on the eﬀects of diﬀerent settings for its parameters. The
guidelines are drawn up based on extensive experiments.
1
Introduction
Quantum-inspired evolutionary algorithm (QEA) recently proposed in [1] can
treat the balance between exploration and exploitation more easily when com-
pared to conventional GAs (CGAs). Also, QEA can explore the search space
with a small number of individuals and exploit the global solution in the search
space within a short span of time. QEA is based on the concept and princi-
ples of quantum computing, such as a quantum bit and superposition of states.
However, QEA is not a quantum algorithm, but a novel evolutionary algorithm.
In [1], the structure of QEA and its characteristics were formulated and
analyzed, respectively. According to [1], the results (on the knapsack problem)
of QEA with population size of 1 were better than those of CGA with population
size of 50. In [2], a QEA-based disk allocation method (QDM) was proposed.
According to [2], the average query response times of QDM are equal to or less
than those of DAGA (disk allocation methods using GA), and the convergence
of QDM is 3.2-11.3 times faster than that of DAGA. In [3], a QEA-based face
veriﬁcation was proposed. In this paper, some guidelines for setting the related
parameters are presented to maximize the performance of QEA.
2
Some Guidelines for Setting the Parameters of QEA
In this section, some guidelines for setting the parameters of QEA are investi-
gated. These guidelines are drawn up based on empirical results.
The initial values of Q-bit are set to

1
√
2,
1
√
2

for the uniform distribution of
0 or 1. To improve the performance, we can think of the two-phase mechanism
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 427–428, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

428
K.-H. Han and J.-H. Kim
0
10
20
30
40
50
60
70
80
90
100
2650
2700
2750
2800
2850
2900
2950
3000
3050
3100
QEA
CGA
Population size
Profit
0
10
20
30
40
50
60
70
80
90
100
2650
2700
2750
2800
2850
2900
2950
3000
3050
3100
0
10
20
30
40
50
60
70
80
90
100
2650
2700
2750
2800
2850
2900
2950
3000
3050
3100
QEA
CGA
Population size
Profit
(a) Mean best proﬁts
0
10
20
30
40
50
60
70
80
90
100
2
4
6
8
10
12
14
16
18
20
QEA
CGA
Population size
Standard dev.
0
10
20
30
40
50
60
70
80
90
100
2
4
6
8
10
12
14
16
18
20
0
10
20
30
40
50
60
70
80
90
100
2
4
6
8
10
12
14
16
18
20
QEA
CGA
Population size
Standard dev.
(b) Standard deviation of proﬁts
Fig. 1. Eﬀects of changing the population sizes of QEA and CGA for the knapsack
problem with 500 items. The global migration period and the local migration period
were 100 and 1, respectively. The results were averaged from 30 runs.
for initial conditions. In the ﬁrst phase, some promising initial values can be
searched. If they are used in the second phase, the performance of QEA will
increase.
From the empirical results, Table I in [1] for the rotation gate can be sim-
pliﬁed as [0 ∗p ∗n ∗0 ∗]T , where p is a positive number and n is a negative
number, for various optimization problems. The magnitude of p or n has an ef-
fect on the speed of convergence, but if it is too big, the solutions may diverge or
converge prematurely to a local optimum. The values from 0.001π to 0.05π are
recommended for the magnitude, although they depend on the problems. The
sign determines the direction of convergence.
From the results of Figure 1, the values ranging from 10 to 30 are recom-
mended to be used as the population size. However, if more robustness is needed,
the population size should be increased (see Figure 1-(b)). The global migration
period is recommended to be set to the values ranging from 100 to 150, and the
local migration period to 1.
These guidelines can help researchers and engineers who want to use QEA
for their application problems.
References
1. Han, K.-H., Kim, J.-H.: Quantum-inspired Evolutionary Algorithm for a Class of
Combinatorial Optimization. IEEE Trans. Evol. Comput. 6 (2002) 580–593
2. Kim, K.-H., Hwang, J.-Y., Han, K.-H., Kim, J.-H., Park, K.-H.: A Quantum-
inspired Evolutionary Computing Algorithm for Disk Allocation Method. IEICE
Trans. Inf. & Syst., E86-D (2003) 645–649
3. Jang, J.-S., Han, K.-H., Kim, J.-H.: Quantum-inspired Evolutionary Algorithm-
based Face Veriﬁcation. Proc. Genet. & Evol. Comput. Conf. (2003)

Evolutionary Two-Dimensional DNA Sequence
Alignment
Edgar E. Vallejo1 and Fernando Ramos2
1 Computer Science Dept., Tecnol´ogico de Monterrey, Campus Estado de M´exico
Carretera Lago de Guadalupe Km 3.5 Col. Margarita Maza de Ju´arez, 52926
Atizap´an de Zaragoza, Eestado de M´exico, M´exico
vallejo@itesm.mx
2 Computer Science Dept., Tecnol´ogico de Monterrey, Campus Cuernavaca
Ave. Paseo de la Reforma 182 Col. Lomas de Cuernavaca, 62589
Cuernavaca, Morelos, M´exico
fernando.ramos@itesm.mx
Abstract. This article presents a model for DNA sequence alignment.
In our model, a ﬁnite state automaton writes two-dimensional maps of
nucleotide sequences. An evolutionary method for sequence alignment
from this representation is proposed. We use HIV as the working exam-
ple. Experimental results indicate that structural similarities produced
by two-dimensional representation of sequences allow us to perform pair-
wise and multiple sequence alignment eﬃciently using genetic algorithms.
1
Introduction
The area of bioinformatics is concerned with the analysis of molecular sequences
to determine the structure and function of biological molecules [2]. Fundamental
questions about functional, structural and evolutionary properties of molecular
sequence can be answered using sequence alignment.
Research in sequence alignment has focused for many years on the design and
analysis of eﬃcient algorithms that operate on linear character representation of
nucleotide and protein sequences. The intractability of multiple sequence align-
ment algorithms evidences limitations for the analysis of molecular sequences
from this representation. Similarly, due to the extension of typical genomes, this
representation is also inconvenient from the human perception perspective.
2
The Model
In our model, a ﬁnite state automaton writes a two-dimensional map of DNA
sequences [3]. The proposed alignment method is based on the overlapping of
a collection of these maps. We overlap a two-dimensional map over another
to discover coincidences in character patterns. Sequence aligment consists of
the sliding of maps over a reference plane in order to search for the optimum
overlapping. We use genetic algorithms to evolve the cartesian positions of a
collection of maps that maximize coincidences in character patterns.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 429–430, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

430
E.E. Vallejo and F. Ramos
3
Experiments and Results
We performed several runs using HIV nucleotide sequences. Figure 1 shows the
results of a typical run. We performed comparisons using conventional sequence
aligment methods that operate on linear sequences. We found that our method
yields similar results to those produced by the SIM local alignment algorithm.
-700
-600
-500
-400
-300
-200
-100
0
100
-200
-150
-100
-50
0
50
Y
X
Experiment 1
HIV2ROD
HIV2ST
Fig. 1. Results. Pairwise DNA sequence alignment
4
Conclusions and Future Work
We present a sequence alignment method based on two-dimensional representa-
tion of DNA sequences and genetic algorithms. An immediate extension of this
work is the consideration of protein sequences and the construction of phyloge-
nies from two-dimensional alignment scores. Finally, a more detailed compara-
tive analysis using evolutionary [1] and conventional [2] alignment methods could
elucidate the signiﬁcance of evolutionary two-dimensional sequence alignment.
References
1. Fogel, G. E., Corne, D. W. (eds.) 2003. Evolutionary Computation in Bioinformatics.
Morgan Kaufmann Publishers.
2. Mount, D. 2000. Bioinformatics. Sequence and Genome Analysis. Cold Spring Har-
bor, NY: Cold Spring Harbor Laboratory Press.
3. Vallejo, E. E., Ramos, F. 2002. Evolving Finite Automata with Two-dimensional
Output for Biosequence Recognition and Visualization In W. B. Langton, E. Cant´u-
Paz, K. Mathias, R. Roy, R. Poli, K. Balakrishnan, V. Honovar, G. Rudolph, J. We-
gener, L. Bull, M. A. Potter, A. C. Schultz, J. E. Miller, E. Burke, N. Jonoska (eds.)
Proceedings of the Genetic and Evolutionary Computation Conference GECCO
2002. Morgan Kaufmann Publishers.

Active Control of Thermoacoustic Instability in
a Model Combustor with Neuromorphic
Evolvable Hardware
John C. Gallagher and Saranyan Vigraham
Department of Computer Science and Engineering
Wright State University, Dayton, OH, 45435-0001
{jgallagh,svigraha}@cs.wright.edu
Abstract. Continuous Time Recurrent Neural Networks (CTRNNs)
have previously been proposed as an enabling paradigm for evolving
analog electrical circuits to serve as controllers for physical devices [6].
Currently underway is the design of a CTRNN-EH VLSI chips that com-
bines an evolutionary algorithm and a reconﬁgurable analog CTRNN
into a single hardware device capable of learning control laws of physical
devices. One potential application of this proposed device is the control
and suppression of potentially damaging thermoacoustic instability in
gas turbine engines. In this paper, we will present experimental evidence
demonstrating the feasibility of CTRNN-EH chips for this application.
We will compare our controller eﬃcacy with that of a more traditional
Linear Quadratic Regulator (LQR), showing that our evolved controllers
consistently perform better and possess better generalization abilities.
We will conclude with a discussion of the implications of our ﬁndings
and plans for future work.
1
Introduction
An area of particular interest in modern combustion research is the study of lean
premixed (LP) fuel combustors that operate at low fuel-to-air ratios. LP fuels
have the advantage of allowing for more complete combustion of fuel products,
which decreases harmful combustor emissions that contribute to the formation
of acid rain and smog. Use of LP fuels however, contributes to ﬂame instabil-
ity, which causes potentially damaging acoustic oscillations that can shorten the
operational life of the engine. In severe cases, ﬂame-outs or major engine com-
ponent failure are also possible. One potential solution to the thermoacoustic
instability problem is to introduce active control devices capable of sensing and
suppressing dangerous oscillations by introducing appropriate control eﬀorts.
Because combustion systems can be so diﬃcult to model and analyze, self-
conﬁguring evolvable hardware (EH) control devices are likely to be of enormous
value in controlling real engines that might defy more traditional techniques.
Further, an EH controller would be able to adapt and change online, continuously
optimizing its control over the service life of a particular combustor. This paper
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 431–441, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

432
J.C. Gallagher and S. Vigraham
Fig. 1. Schematic of a Test Combustor
will discuss our eﬀorts to control the model combustor presented in [10] [11] with
a simulated evolvable hardware device. We will begin with brief summaries of
the simulated combustor and our CTRNN-EH device. Following, we will discuss
our evolved CTRNN-EH control devices and how their performance compares
to a traditional LQR controller. Finally, we will discuss the implications of our
results and discuss future work in which we will apply CTRNN-EH to the control
of real engines.
2
The Model Combustor
Figure 1 shows a schematic of a simple combustor. Premixed fuel and air is intro-
duced at the closed end and the ﬂame is anchored on a perforated disk mounted
inside the chamber a short distance from the closed end (the ﬂameholder). Com-
bustion products are forced out the open end. Thermoacoustic instability can
occur due to positive feedback between combustion dynamics of the ﬂame and
acoustic properties of the combustion chamber. Qualitatively speaking, ﬂame
dynamics are aﬀected by mechanical vibration of the combustion chamber and
mechanical vibration of the combustion chamber is aﬀected by heat release/ﬂame
dynamics. When these two phenomena reinforce one another, it is possible for the
vibrations of the combustion chamber to grow to unsafe levels. Figure 2 shows
the engine pressure with respect to time for the ﬁrst 0.04 seconds of uncontrolled
operation of an unstable engine. Note that maximum pressure amplitude is grow-
ing exponentially and would quickly grow to unsafe levels. In the model engine,
a microphone is mounted on the chamber to monitor the frequency and magni-
tude of pressure oscillations. A loudspeaker eﬀector used to introduce additional
vibrations is mounted either at the closed end of the chamber or along its side.
Figure 1 shows both speaker mounting options, though for any experiment we
discuss here, only one would be used at a time.

Active Control of Thermoacoustic Instability in a Model Combustor
433
Fig. 2. Time Series Response of the Uncontrolled EM1 Combustor
A full development of the simulation state equations, which have been veri-
ﬁed against a real propane burning combustor, is given in [10]. Using these state
equations, we implemented C language simulations of four combustor conﬁgura-
tions. All four simulations assumed a speciﬁc heat ratio of 1.4, an atmospheric
pressure of 1 atmosphere, an ambient temperature of 350K, a fuel/air mixture
of 0.8, a speed of sound of 350 m/s, and a burn rate of 0.4 m/s. The four engine
conﬁgurations, designated SM1, SM2, EM1, and EM2, were drawn from [10] and
represent speaker side-mount conﬁgurations resonant at 542 Hz and 708 Hz and
end-mount conﬁgurations resonant at 357 Hz and 714 Hz respectively.
3
CTRNN-EH
CTRNN-EH devices combine a reconﬁgurable analog continuous time recurrent
neural network (CTRNN) and Star Compact Genetic Algorithm (*CGA) into a
single hardware device.
CTRNNs are networks of Hopﬁeld continuous model neurons [2][5][12] with
unconstrained connection weight matrices. Each neuron’s activity can be ex-
pressed by an equation of the following form:
τi
dyi
dt = −yi +
N

j=1
wjiσ (yj + θj) + siIi (t)
(1)
where yi is the state of neuron i, τi is the time constant of neuron i, wji is
the connection weight from neuron j to neuron i, σ (x) is the standard logistic
function, θj is the bias of neuron j, si is the sensor input weight of neuron i, and
Ii (t) is the sensory input to neuron i at time t.
CTRNNs diﬀer from Hopﬁeld networks in that they have no restrictions on
their interneuron weights and are universal dynamics approximators [5]. Due to
their status as universal dynamics approximators, we can be reasonably assured

434
J.C. Gallagher and S. Vigraham
that any control law of interest is achievable using collections of CTRNN neu-
rons. Further, a number of analog and mixed analog-digital implementations are
known [13] [14] [15] and available for use.
*CGAs are any of a family of tournament-based modiﬁed Compact Genetic
Algorithms [9] [7] selected for this application because of the ease in which they
may be implemented using common VLSI techniques [1] [8]. The *CGAs require
far less memory than other EAs because they represent populations as compact
probability vectors rather than as sets of actual bit strings. In this work, we
employed the mCGA variation similar to that documented in [9]. The algorithm
can be stated as shown in ﬁgure 3.
Figure 4 shows a schematic representation of our CTRNN-EH device used in
intrinsic mode to learn the control law of an attached device. In this case, the
user would provide a hardware or software system that produces a scalar measure
(performance score) of the controlled devices eﬀectiveness based upon inputs
from some associated instrumentation. This is represented in the rightmost block
of Figure 4. The CTRNN-EH device, represented by the leftmost block in the
ﬁgure, would receive ﬁtness scores from the evaluator and sensory inputs from
the controlled device. The CGA engine would evolve CTRNN conﬁgurations that
monitor device sensors and supply eﬀector eﬀorts that maximized the controlled
devices performance.
4
CTRNN-EH Control Experiments
In the experiments reported in this paper, we employed a simulated CTRNN-
EH device that contained a ﬁve neuron, fully-connected CTRNN as the analog
neuromorphic component and a mCGA [8] as the EA component. The CTRNN
was interfaced to the combustor as shown in Figure 5. Each neuron received
the raw microphone value as input. The outputs of two CTRNN neurons con-
trolled the amplitude and frequency of a voltage controlled oscillator that itself
drove the loudspeaker (I.E. The CTRNN had control over the amplitude and
frequency of the loudspeaker eﬀector). Speaker excitations could range from 0
to 10 mA in amplitude and 0 to 150 Hz in frequency. The error function (per-
formance evaluator) was the sum of amplitudes of all pressure peaks observed in
a period of one second. This error function roughly approximates and produces
the same relative rankings that would be produced by using simple hardware
to integrate the area under the microphone signal in the time domain. mCGA
parameters were chosen as follows: simulated population size of 1023, a maxi-
mum tournament count of 100,000, and a bitwise mutation rate of 0.05. Forty
CTRNN parameters (ﬁve time constants, ﬁve biases, ﬁve sensor weights, and
twenty-ﬁve intra-network weights) were encoded as eight bit values resulting in
a 320 bit genome. All experiments were run on a 16 node SGI Beowulf cluster.
We ran 100 evolutionary trials for each of the four engine conﬁgurations. On
average, 589, 564, 529, and 501 tournaments were required to evolve eﬀective os-
cillation suppression for SM1, SM2, EM1, and EM2 respectively. Each of the the
resulting four hundred evolved champions was tested for control eﬃcacy across all

Active Control of Thermoacoustic Instability in a Model Combustor
435
1. Initialize probability vector
for i := 1 to L do p[i] := 0.5
2. Generate two individuals from the vector
a := generate(p);
b := generate(p);
3. Let them compete
winner, loser := evaluate(a, b)
4. Update the probability vector toward the winner
for i := 1 to L do
if winner[i] <> loser[i] then
if winner[i] = 1 then
p[i] := p[i] + (1 / N)
else
p[i] := p[i] - (1 / N)
5. Mutate champ and evaluate
if winner = a then
c := mutate(a);
evaluate(c);
if fitness(c) > fitness(a) then
a := c;
else
c := mutate(b);
evaluate(c);
if fitness(c) > fitness(b) then
b := c;
6. Generate one individual from the vector
if winner = a then
b := generate(p);
else
a := generate(p);
7. Check if probability vector has converged
for i := 1 to L do
if p[i] > 0 and p[i] < 1 then goto step 3
8. P represents the final solution
Fig. 3. Pseudo-code for mCGA
four modeled engine conﬁgurations (SM1, SM2, EM1, and EM2). All were eﬀec-
tive in suppressing vibrations under the conditions for which they were evolved.
In addition, all were capable of eﬀectively suppressing vibrations in the engine
conﬁgurations for which they were not evolved. Typical engine noise suppression

436
J.C. Gallagher and S. Vigraham
Fig. 4. Schematic of CTRNN-EH Controller
results for both a side mounted CTRNN-EH controller and a Linear Quadratic
Regulator (LQR) are shown in Figure 6. Tables 1, 2 3, and 4 summarize the
average settling times (the time the controller requires to stabilize the engine)
across all experiments. Note that in Figure 6, our evolved controller settles to
stability signiﬁcantly faster than the LQR. The LQR controllers presented in
[10] and [11] had settling times of about 40 mS and 20 mS for the end-mounted
and side-mounted conﬁgurations respectively. Note that our evolved CTRNNs
compare very well to LQR devices. On average, they evolved to produce set-
tling times of better than 20 ms. The very best CTRNN controllers settle in
as few as 8 ms. Further, the presented LQR controllers failed to function prop-
erly when used in a mounting conﬁguration for which they were not designed,
while all of our evolved controllers appear capable of controlling oscillations ir-
regardless of where the eﬀector is mounted. Both of these results suggest that
our evolved controllers may be both faster (in terms of settling time) and more
ﬂexible (in terms of eﬀector placement) than the given LQR devices. Presuming
that we implemented only the analog CTRNN portion of the CTRNN-EH de-
vice, this improved capability would be achieved without a signiﬁcant increase
in the amount of analog hardware required.
In other, related work, we have observed that mCGA seems better able to
evolve CTRNN controllers than the population based Simple Genetic Algorithm
(sGA) that it emulates [7]. This eﬀect was observed in experiments reported here
as well. We evolved 100 CTRNN controllers for the each engine conﬁguration
using a tournament based simple GA with uniform crossover, a bitwise mutation
rate of 0.05, and a population size of 1023. On average, the sGA required 5000
tournaments to evolve eﬀective control. The diﬀerence between the number of
generations required for sGA and mCGA is statistically signiﬁcant. Table 5 shows

Active Control of Thermoacoustic Instability in a Model Combustor
437
Fig. 5. CTRNN to Combustor Interface
the average settling times of sGA and mCGA controllers evolved in the SM1
conﬁguration. These results are representative of those observed under other
evolutionary conditions.
5
Conclusions and Discussion
In this paper, we demonstrated that, against an experimentally veriﬁed combus-
tor model, CTRNN-EH evolvable hardware controllers are consistently capable
of evolving highly eﬀective active oscillation suppression abilities that general-
ized to control diﬀerent engine conﬁgurations as well. Further, we demonstrated
that we could surpass the performance of a benchmark LQR device reported
in the literature as a means of solving the same problem. These results are in
themselves signiﬁcant. More signiﬁcant, however, are the implications of those
results.
First, the LQR devices referenced were developed based upon detailed knowl-
edge of the system to be controlled. A model needed to be constructed and val-
idated before controllers could be constructed. Even in the case of the relatively
simple combustion device that was modeled and simulated, this was a signiﬁcant
eﬀort. Though it may be the case that improved control can be had by using
other model-based methods, any such improvements would be purchased at the
cost of signiﬁcant additional work. Further, it is not clear that one would be
able to construct appropriately detailed mathematical models of more realistic
combustor systems with more realistic engine actuation methods. Thus, it is not
clear if model-based control methods could be applied to more realistic engines.
Our CTRNN-EH controllers were developed without speciﬁc knowledge of the
plant to be controlled. A *CGA evolved a very general dynamics approximator

438
J.C. Gallagher and S. Vigraham
Table 1. Controllers Evolved in SM1 Conﬁguration
Statistic Tested in EM1 Tested in EM2 Tested in SM1 Tested in SM2
Average
12.51 ms
11.80 ms
11.141 ms
11.78 ms
Stdev
5.38 ms
5.22 ms
5.21 ms
1.08 ms
Table 2. Controllers Evolved in EM1 Conﬁguration
Statistic Tested in EM1 Tested in EM2 Tested in SM1 Tested in SM2
Average
14.68 ms
13.84 ms
13.05 ms
12.20 ms
Stdev
6.37 ms
6.23 ms
5.97 ms
1.14 ms
Table 3. Controllers Evolved in SM2 Conﬁguration
Statistic Tested in EM1 Tested in EM2 Tested in SM1 Tested in SM2
Average
21.93 ms
21.41 ms
20.06 ms
13.03 ms
Stdev
3.74 ms
3.80 ms
3.92 ms
0.67 ms
Table 4. Controllers Evolved in EM2 Conﬁguration
Statistic Tested in EM1 Tested in EM2 Tested in SM1 Tested in SM2
Average
13.22 ms
12.53 ms
11.85 ms
11.91 ms
Stdev
5.79 ms
5.58 ms
5.58 ms
1.07 ms
Table 5. Controllers Evolved with sGA in SM1 Conﬁguration
Statistic Tested in EM1 Tested in EM2 Tested in SM1 Tested in SM2
Average
14.72 ms
17.31 ms
13.65 ms
14.03 ms
Stdev
4.92 ms
5.61 ms
5.16 ms
3.23 ms

Active Control of Thermoacoustic Instability in a Model Combustor
439
Fig. 6. Typical LQR Response vs. CTRNN-EH Response
to stabilize the engine. Such a technique could be applied without modiﬁcation
to any engine and/or combustor system – with any sort of engine eﬀectors. Nat-
urally, one might argue that the evolved control devices would be too diﬃcult
to understand and verify, rendering them less attractive for use in important
control applications. However, especially in cases where there are few sensor
inputs, we have already developed analysis techniques that should be able to
construct detailed explanations of CTRNN operation with respect to speciﬁc
control problems [3] [4]. The engine controllers we presented in this paper are
currently undergoing analysis using these dynamical systems methods and we
expect to construct explainations of their operation in the near future.
Second, although our initial studies have been of necessity in simulation, we
have made large strides in constructing hardware prototypes on our way to a
complete, self-contained VLSI implementation. We have already constructed and
veriﬁed a reconﬁgurable analog CTRNN engine using oﬀ-the-shelf components
[6] and have implemented the mCGA completely in hardware with FPGAs [7].
Our early experiments suggest that our hardware behaves as predicted in sim-
ulation. We are currently integrating these prototypes to create the ﬁrst, fully
hardware CTRNN-EH device. This ﬁrst integrated prototype will be used to
evolve oscillation suppression on a physical test combustor patterned after that

440
J.C. Gallagher and S. Vigraham
modeled in [10]. Our positive results in simulation make moving to this next
phase possible.
Third, earlier in this paper, we reported that mCGA evolves better solutions
than does a similar simple GA. This phenomenon is not unique to the engine
control problem, in fact, we have observed it in evolving CTRNN based con-
trollers for other physical processes [7]. Understanding why this is the case will
likely lead to important information about the nature of CTRNN search spaces,
the mechanics of the *CGAs, or both. This study is also currently underway.
Evolvable hardware has the potential to produce computational and control
devices with unprecedented abilities to automatically conﬁgure to speciﬁc re-
quirements, to automatically heal in the face of damage, and even to exploit
methods beyond what is currently considered state of the art. The results in
this paper argue strongly for the feasibility of EH methods to address a diﬃcult
problem of practical import. They also point the way toward further study and
development of general techniques of potential use to the EH community.
Acknowledgements. This work was supported by Wright State University and
The Ohio Board of Regents through the Research Challenge Grant Program.
References
1. Aporntewan, C. and Chongstitvatana. (2001). A hardware implementation of the
compact genetic algorithm. in The Proceedings of the 2001 IEEE Congress on
Evolutionary Computation
2. Beer, R.D. (1995). On the dynamics of small continuous-time recurrent neural
networks. in Adaptive Behavior3(4):469–509.
3. Beer, R.D., Chiel, H.J. and Gallagher, J.C. (1999). Evolution and analysis of model
CPGs for walking II. general principles and individual variability. in J. Computa-
tional Neuroscience 7(2):119–147.
4. Chiel, H.J., Beer, R.D. and Gallagher, J.C. (1999). Evolution and analysis of
model CPGs for walking I. dynamical modules. in J. Computational Neuroscience
7:(2):99–118.
5. Funahashi, K & Nakamura, Y. (1993), Approximation of dynamical systems by
continuous time recurrent neural networks, in Neural Networks 6:801–806
6. Gallagher, J.C. & Fiore, J.M., (2000). Continuous time recurrent neural networks:
a paradigm for evolvable analog controller circuits, in The Proceedings of the 51st
National Aerospace and Electronics Conference
7. Gallagher, J.C., Vigraham, S., Kramer, G. (2002). A family of compact genetic
algorithms for intrinsic evolvable hardware. Submitted to IEEE Transactions on
Evolutionary Computation
8. Gallagher, J.C. & Vigraham, S. (2002). A modiﬁed compact genetic algorithm for
the intrinsic evolution of continuous time recurrent neural networks. in The Pro-
ceedings of the 2002 Genetic and Evolutionary Computation Conference. Morgan-
Kaufmann.
9. Harik, G., Lobo, F., & Goldberg, D.E. (1999). The compact genetic algorithm. In
IEEE Transactions on Evolutionary Computation. Vol 3, No. 4. pp. 287–297

Active Control of Thermoacoustic Instability in a Model Combustor
441
10. Hathout, J.P., Annaswamy, A.M., Fleiﬁl, M. and Ghoniem, A.F. (1998). Model-
based active control design for thermoacoustic instability. in Combustion Science
and Technology, 132: 99–138
11. Hathout, J.P., Fleiﬁl, M., Rumsey, J.W., Annaswamy, A.M., and Ghoniem, A.F.
(1997). Model-based analysis and design of active control of thermoacoustic insta-
bility. in IEEE Conference on Control Applications, Hartford, CT, October 1997.
12. Hopﬁeld, J.J. (1984). Neurons with graded response properties have collective com-
putational properties like those of two-state neurons, in Proceedings of the National
Academy of Sciences 81:3088–3092
13. Maass, W. and Bishop, C. (1999). Pulsed Neural Networks. MIT Press.
14. Mead, C.A., (1989). Analog VLSI and Neural Systems, Addison-Wesley, New York
15. Murray, A. and Tarassenko, L. (1994). Analogue Neural VLSI : A Pulse Stream
Approach. Chapman and Hall, London.

Hardware Evolution of Analog Speed Controllers
for a DC Motor
David A. Gwaltney1 and Michael I. Ferguson2
1 NASA Marshall Space Flight Center,Huntsville, AL 35812, USA
David.A.Gwaltney@nasa.gov
2 Jet Propulsion Laboratory, California Institute of Technology
Pasadena, CA 91109, USA
Michael.I.Ferguson@jpl.nasa.gov
Abstract. Evolvable hardware provides the capability to evolve analog
circuits to produce ampliﬁer and ﬁlter functions. Conventional analog
controller designs employ these same functions. Analog controllers for
the control of the shaft speed of a DC motor are evolved on an evolv-
able hardware plaform utilizing a Field Programmable Transistor Array
(FPTA). The performance of these evolved controllers is compared to
that of a conventional proportional-integral (PI) controller. It is shown
that hardware evolution is able to create a compact design that pro-
vides good performance, while using considerably less functional elec-
tronic components than the conventional design.
1
Introduction
Research on the application of hardware evolution to the design of analog cir-
cuits has been conducted extensively by many researchers. Many of these eﬀorts
utilize a SPICE simulation of the circuitry, which is acted on by the evolutionary
algorithm chosen to evolve the desired functionality. An example of this is the
work done by Lohn and Columbano at NASA Ames Research Center to develop
a circuit representation technique that can be used to evolve analog circuitry in
software simulation[1]. This was used to conduct experiments in evolving ﬁlter
circuits and ampliﬁers. A smaller, but rapidly increasing number of researchers
have pursued the use of physical circuitry to study evolution of analog circuit
designs.
The availability of reconﬁgurable analog devices via commercial or
research-oriented sources is enabling this approach to be more widely studied.
Custom Field Programmable Transistor Array (FPTA) chips have been used for
the evolution of logic and analog circuits. Eﬀorts at the Jet Propulsion Labora-
tory (JPL) using their FPTA2 chip are documented in [2,3,4]. Another FPTA
development eﬀort at Heidelberg University is described in [5]. Some researchers
have conducted experiments using commercially available analog programmable
devices to evolve ampliﬁer designs, among other functions[6,7].
At the same time, eﬀorts to use evolutionary algorithms to design controllers
have also been widely reported. Most of the work is on the evolution of controller
designs suitable only for implementation in software.
Koza, et al., presented
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 442–453, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Hardware Evolution of Analog Speed Controllers for a DC Motor
443
automatic synthesis of control laws and tuning for a plant with time delay using
Genetic programming. This was done in simulation [8]. However, Zebulum, et.
al., have evolved analog controllers for a variety of industrially representative
dynamic system models[10]. In this work, the evolution was also conducted in
a simulated environment.
Hardware evolution can enable the deployment of a self-conﬁgurable con-
troller in hardware.
Such a controller will be able to adapt to environmen-
tal conditions that would otherwise degrade performance, such as temperature
varying to extremes or ionizing radiation. Hardware evolution can provide fault-
tolerance capability by re-routing internal connections around damaged compo-
nents or by reuse of degraded components in novel designs. These features, along
with the capability to accommodate unanticipated or changing mission require-
ments, make an evolvable controller attractive for use in a remotely located
platform, such as a spacecraft. Hence, this eﬀort focuses on the application of
hardware evolution to the in situ design of a shaft speed controller for a DC mo-
tor. To this end, the Stand-Alone Board-Level Evolvable (SABLE) System[3],
developed by researchers at the Jet Propulsion Laboratory, is used as the plat-
form to evolve analog speed controllers for a DC motor.
Motor driven actuators are ubiquitous in the commercial, industrial, military
and aerospace environments. A recent trend in aviation and aerospace is the use
of power-by-wire technologies. This refers to the use of motor driven actuators,
rather than hydraulic actuators for aero-control surfaces[11][12]. Motor driven
actuators have been considered for upgrading the thrust vector control of the
Space Shuttle main engines [13].
In spacecraft applications, servo-motors can
be used for positioning sun-sensors, Attitude and Orbit Control Subsystems
(AOCSs), antennas, as well as valves, linear actuators and other closed-loop
controllers.
In this age of digital processor-based control, analog controllers are still fre-
quently used at the actuator level in a variety of systems.
In the harsh envi-
ronment of space, electronic components must be rated to survive temperature
extremes and exposure to radiation. Very few microcontrollers and digital signal
processors are available that are rated for operation in a radiation environment.
However, operational ampliﬁers and discrete components are readily available
and are frequently applied. Reconﬁgurable analog devices provide a small form
factor platform on which multiple analog controllers can be implemented. The
FPTA2, as part of the SABLE System, is a perfect platform for implementa-
tion of multiple controllers, because its sixty-four cells can theoretically provide
sixty-four operational ampliﬁers, or evolved variations of ampliﬁer topologies.
Further, its relatively small size and low power requirements provide savings in
space and power consumption over the uses of individual operational ampliﬁers
and discrete components[2].
The round-trip communication time between the Earth and a spacecraft at
Mars ranges from 10 to 40 minutes. For spacecraft exploring the outer planets
the time increases signiﬁcantly.
A spacecraft with self-conﬁguring controllers
could work out interim solutions to control system failures in the time it takes

444
D.A. Gwaltney and M.I. Ferguson
Fig. 1. Conﬁguration of the SABLE System and motor to be controlled
for the spacecraft to alert its handlers on the Earth of a problem. The evolvable
nature of the hardware allows a new controller to be created from compromised
electronics, or the use of remaining undamaged resources to achieve required
system performance.
Because the capabilities of a self-conﬁguring controller
could greatly increase the probability of mission success in a remote spacecraft,
and motor driven actuators are frequently used,
the application of hardware
evolution to motor controller design is considered a good starting point for the
development of a general self-conﬁguring controller architecture.
2
Approach
The JPL developed Stand-Alone Board Level Evolvable (SABLE) System[3] is
used for evolving the analog control electronics. This system employs the JPL
designed Second Generation, Field Programmable Transistor Array (FPTA2).
The FPTA2 contains 64 programmable cells on which an electronic design can
be implemented by closing internal switches. The schematic diagram of one cell
is given in the Appendix. Each cell has inputs and outputs connected to external
pins or the outputs of neighboring cells. More detail on the FPTA2 architecture
is found in [2]. A diagram of the experimental setup is shown in Figure 1. The
main components of the system are a TI-6701 Digital Signal Processor (DSP), a
100kSa/sec 16-channel DAC and ADC and the FPTA2. There is a 32-bit digital
I/O interface connecting the DSP to the FPTA2. The genetic algorithm running
on the DSP follows a simple algorithm of download, stimulate the circuit with a
control signal, record the response, evaluate the response against the expected.
This is repeated for each individual in the population and then crossover, and
mutation operators are performed on all but the elite percentage of individuals.
The motor used is a DC servo-motor with a tachometer mounted to the
shaft of the motor.
The motor driver is conﬁgured to accept motor current
commands and requires a 17.5 volt power supply with the capability to produce
6 amps of current. A negative 17.5 volt supply with considerably lower current
requirements is needed for the circuitry that translates FPTA2 output signals

Hardware Evolution of Analog Speed Controllers for a DC Motor
445
to the proper range for input to the driver. The tachometer feedback range is
roughly [-4, +4] volts which corresponds to a motor shaft speed range of [-1300,
+1300] RPM. Therefore, the tachometer feedback is biased to create a unipolar
signal,
then reduced in magnitude to the [0, 1.8] volt range the FPTA2 can
accept.
3
Conventional Analog Controller
3.1
Design
All closed-loop control systems require the calculation of an error measure, which
is manipulated by the controller to produce a control input to the dynamic
system being controlled, commonly referred to as the plant. The most widely
used form of analog controller is a proportional-integral (PI ) controller.
This
controller is frequently used to provide current control and speed control for a
motor. The PI control law is given in Equation 1,
u(t) = KP e(t) +

1
KI
e(t)dt .
(1)
where e(t) is the diﬀerence between the desired plant response and the actual
plant response, KP is called the proportional gain, and KI is called the integral
gain. In this control law, the proportional and integral terms are separate and
added together to form the control input to the plant. The proportional gain
is set to provide quick response to changes in the error, and the integral term is
set to null out steady state error.
The FPTA2 is a unipolar device using voltages in the range of 0 to 1.8 volts.
In order to directly compare a conventional analog controller design with evolved
designs, the PI controller must be implemented as shown in Figure 2. This ﬁgure
includes the circuitry needed to produce the error signal.
Equation 2 gives the
error voltage, Ve, given the desired response VSP , or setpoint, and the measured
motor speed VT ACH.
The frequency domain transfer function for the voltage
output,Vu, of the controller, given Ve, is shown in Equation 3,
Ve = VSP
2
−VT ACH
2
+ 0.9V .
(2)
Vu = (Ve −Vbias2)(R2
R1
+
1
sR1C ) + Ve .
(3)
where s is complex frequency in rad/sec, R2
R1 corresponds to the proportional gain
and
1
R1C corresponds to the integral gain. This conventional design requires four
op-amps. Two are used to isolate voltage references Vbias1 and Vbias2 from the
rest of the circuitry, thereby maintaining a steady bias voltage in each case.
Vbias2 must be adjusted to provide a plant response without a constant error
bias.
The values for R1, R2, and C are chosen to obtain the desired motor
speed response.

446
D.A. Gwaltney and M.I. Ferguson
Fig. 2. Unipolar analog PI controller with associated error signal calculation and volt-
age biasing
3.2
Performance
The controller circuitry in Figure 2 is used to provide a baseline control response
to compare with the responses obtained via evolution. The motor is run with no
external torque load on the shaft. The controller is conﬁgured with R1 = 10K
ohms, R2 = 200K ohms, and C = 0.47uF. Vbias2 is set to 0.854 volts. Figure
3 illustrates the response obtained for VSP consisting of a 2 Hz sinusoid with
amplitude in the range of approximately 500 millivolts to 1.5 Volts, as well as
for VSP consisting of a 2 Hz square wave with the same magnitude.
Statistical
analysis of the error for sinusoidal VSP is presented in Table 1 for comparison
with the evolved controller responses.
Table 2 gives the rise time and error
statistics at steady state for the ﬁrst full positive going transition in the square
wave response. This is the equivalent of analyzing a step response. Note that
in both cases VT ACH tracks VSP very well.
In the sinusoid case, there is no
visible error between the two. For the square wave case, the only visible error
is at the instant VSP changes value.
This is expected,
because no practical
servo-motor can follow instantaneous changes in speed. There is always some
lag between the setpoint and response. After the transition, the PI controller
does not overshoot the steady state setpoint value, and provides good regulation
of motor shaft speed at the steady state values.
4
Evolved Controllers
Two cells within the FPTA2 are used in the evolution of the motor speed con-
trollers. The ﬁrst cell is provided with the motor speed setpoint, VSP , and the
motor shaft feedback , VT ACH, as inputs, and it produces the controller output,
Vu. An adjacent cell is used to provide support electronics for the ﬁrst cell. The
evolution uses a ﬁtness function based on the error between VSP and VT ACH.

Hardware Evolution of Analog Speed Controllers for a DC Motor
447
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0.5
1
1.5
PI Controller Vsp and Vtach, Sine
volts
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0.5
1
1.5
PI Controller Vsp and Vtach, Square
volts
seconds
Fig. 3. Response obtained using PI controller. Vsp is gray, Vtach is black
Lower ﬁtness is better, because the goal is to minimize the error. The population
is randomly generated, and then modiﬁed to ensure that, initially, the switches
are closed that connect VSP and VT ACH to the internal reconﬁgurable circuitry.
This is done because the evolution will, in many cases, attempt to control the
motor speed by using the setpoint signal only, resulting in an undesirable ”con-
troller” with poor response characteristics. Many evolutions were run, and the
frequency of the sinusoidal signal was varied, along with the population size
and the ﬁtness function. There were some experiments that failed to produce a
desirable controller and some that produced very desirable responses, with the
expected distribution of mediocre controllers in between . Two of the evolved
controllers are presented along with the response data for comparison to the PI
controller. The ﬁrst is the best evolved controller obtained, so far, and the sec-
ond provides a reasonable control response with an interesting circuit design. In
each case, the data presented in the plots was obtained by loading the previously
evolved design on the FPTA2, and then providing VSP via a function generator.
The system response was recorded using a digital storage oscilloscope.
4.1
Case 1
For this case the population size is 100 and a roughly 2 Hz sinusoidal signal was
used for the setpoint. For a population of 100, the evaluation of each generation
takes 45 seconds. The target ﬁtness is 400,000 and the ﬁtness function used is,

448
D.A. Gwaltney and M.I. Ferguson
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0.5
1
1.5
CASE1 Evolved Controller, Vsp and Vtach, Sine 
volts
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0.5
1
1.5
CASE1 Evolved Controller, Vsp and Vtach, Square
Fig. 4. Response obtained using CASE1 evolved controller. Vsp is gray, Vtach is black
F = 0.04 ∗
n

i=1
e2
i + 100
n
n

i=1
|ei| + 100000 ∗not(S57 ∨S53) .
(4)
where ei is the error between VSP and VT ACH at each voltage signal sample, n
is the number of samples over one complete cycle of the sinusoidal input, and
S57, S53 represent the state of the switches connecting VSP and VT ACH to the
reconﬁgurable circuitry . This ﬁtness function punishes individuals that do not
have switches S57 and S53 closed. The location of these switches can be seen
in the cell diagram in the Appendix. VSP is connected to Cell in6 and VT ACH
is connected to Cell in2.
The evolution converged to a ﬁtness of 356,518 at
generation 97.
The ﬁtness values are large due to the small values of error
that are always present in a physical system. Figure 4 illustrates the response
obtained for VSP consisting of a 2 Hz sinusoid with amplitude in the range of
approximately 500 millivolts to 1.5 Volts, as well as for VSP consisting of a 2 Hz
square wave with the same magnitude. This is the same input used to obtain
controlled motor speed responses for the PI controller.
In the sinusoidal case, the evolved controller is able to provide good peak
to peak magnitude response, but is not able to track VSP as it passes through
0.9. The evolved controller provides a response to the square wave VSP , which
has a slightly longer rise time but provides similar regulation of the speed at
steady state. The statistical analysis of the CASE 1 evolved controller response
to the sinusoidal VSP is presented in Table 1.
Note the increase in all the
measures, with the mean error indicating a larger constant oﬀset in the error
response. Despite these increases, the controller response is reasonable and could

Hardware Evolution of Analog Speed Controllers for a DC Motor
449
Table 1. Error metrics for sinusoidal response
Controller Max Error Mean Error Std Dev Error RMS Error
PI
0.16 V
0.0028 V
0.0430 V
0.0431 V
CASE1
0.28 V
0.0469 V
0.0661 V
0.0810 V
Table 2. Response and error metrics for square wave. First full positive transition only
Controller Rise Time Mean Error Std Dev Error RMS Error
PI
0.0358 sec 0.0626 V
0.1816 V
0.1920 V
CASE1
0.0394 sec 0.1217 V
0.2026 V
0.2362 V
be considered good enough. The rise time and steady state error analysis for the
ﬁrst full positive going transition in the square wave response is given in Table
2. While there is an increase in rise time and in the error measures at steady
state, when compared to those of the PI controller, the evolved controller can
be considered to perform very well. Note again that the increase in the mean
error indicates a larger constant oﬀset in the error response. In the PI controller,
this error can be manually trimmed out via adjustment of Vbias2. The evolved
controller has been given no such bias input, so some increase in steady state
error should be expected. However, the evolved controller is trimming this error,
because other designs have a more signiﬁcant error oﬀset. Experiments with the
evolved controller show that the ”support” cell is providing the error trimming
circuitry.
It is notable that the evolved controller is providing a good response using
a considerably diﬀerent set of components than the PI controller. The evolved
controller is using two adjacent cells in the FPTA to perform a similar function to
four op-amps, a collection of 12 resistors and one capacitor. The FPTA switches
have inherent resistance on the order of kilo-ohms, which can be exploited by
evolution during the design. But the two cells can only be used to implement
op-amp circuits similar to those in Figure 2 with the use of external resistors,
capacitors and bias voltages. These external components are not provided. The
analysis of the evolved circuit is complicated and will not be covered in more
detail here.
4.2
Case 2
This evolved controller is included, not because it represents a better controller,
but because it has an interesting characteristic.
In this case, the population
size is 200 and a roughly 3 Hz sinusoidal signal was used for the setpoint during
evolution. For a population of 200, the evaluation of each generation takes 90
seconds. The ﬁtness function is the same as used for Case 1, with one exception,
as shown in Equation 5.
F = 0.04 ∗
n

i=1
e2
i + 100
n
n

i=1
|ei| .
(5)

450
D.A. Gwaltney and M.I. Ferguson
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0.5
1
1.5
CASE2 Evolved Controller, Vsp and Vtach, Sine
volts
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
CASE2 Evolved Controller, Vsp and Vtach, Square
seconds
volts
Fig. 5. Response obtained using CASE2 evolved controller. Vsp is gray, Vtach is black
In this case, the switches S57, S53 are forced to be closed (refer to the cell dia-
gram in the appendix), and so no penalty based on the state of these switches is
included in the ﬁtness function. The evolution converged to a ﬁtness of approx-
imately 1,000,000, and was stopped at generation 320. The interesting feature
of this design is that switches S54, S61, S62, S63 are all open.
This indicates
that the VT ACH signal is not directly connected to the internal circuitry of the
cell. However, the controller is using the feedback, because opening S53 caused
the controller to no longer work. The motor speed response obtained using this
controller can be seen in Figure 5.
The response to sinusoidal VSP is good,
but exhibits noticeable transport delay on the negative slope. The response to
the square wave VSP exhibits oﬀset for the voltage that represents a ”negative”
speed.
Overall the response is reasonably good.
The analysis of this evolved
controller is continuing in an eﬀort to understand precisely how the controller is
using the VT ACH signal internally.
5
Summary
The results presented show the FPTA2 can be used to evolve simple analog
closed-loop controllers. The use of two cells to produce a controller that pro-
vides good response in comparison with a conventional controller shows that
hardware evolution is able to create a compact design that still performs as re-

Hardware Evolution of Analog Speed Controllers for a DC Motor
451
quired, while using less transistors than the conventional design, and no external
components. Recall that one cell is can be used to implement an op-amp design
on the FPTA2. While a programmable device has programming overhead that
ﬁxed discrete electronic and integrated circuit components do not, this overhead
is typically neglected when comparing the design on the programmable device
to a design using ﬁxed components. The programming overhead is indirect, and
is not a functional component of the design. As such, the cell diagram in the
Appendix shows that each cell contains 15 transistors available for use as func-
tional components in the design. Switches have a ﬁnite resistance, and therefore
functionally appear as passive components in a cell. The simpliﬁed diagram in
the data sheets for many op-amps indicate that 30, or more, transistors are uti-
lized in their design, and op-amp circuit designs require multiple external passive
components.
In order to produce self-conﬁguring controllers that can rapidly converge to
provide desired performance, more work is needed to speed up the evolution and
guide it to the best response. The per generation evaluation time of 45 or more
seconds is a bottleneck to achieving this goal. Further, the time constants of a
real servo-motor may make it impossible to achieve more rapid evaluation times.
Most servo-motor driven actuators cannot respond to inputs with frequency
content of more than a few tens of Hertz, without attenuation in the response.
Alternative methods of guiding the evolution or novel controller structures are
required.
A key to improving upon this work and evolving more complex controllers
is a good understanding of the circuits that have been evolved. Evolution has
been shown to make use of parasitic eﬀects and to use standard components in
novel, and often diﬃcult to understand, ways.
Case 2 illustrates this notion.
Gaining this understanding may prove to be useful in developing techniques for
guiding the evolution towards rapid convergence.
Acknowledgements. The authors would like to thank Jim Steincamp and
Adrian Stoica for establishing the initial contact between Marshall Space Flight
Center and the Jet Propulsion Laboratory leading to the collaboration for this
work. The Marshall team appreciates JPL making available their FPTA2 chips
and SABLE system design for conducting the experiments.
Jim Steincamps
continued support and helpful insights into the application of genetic algorithms
have been a signiﬁcant contribution to this eﬀort.
References
[1]
Lohn, J. D. and Columbano, S. P., A Circuit Representation Technique for Auto-
mated Circuit Design, IEEE Transactions on Evolutionary Computation, Vol. 3,
No. 3, September 1999.
[2]
Stoica, A., Zebulum, R., Keymeulen, D., Progress and Challenges in Building
Evolvable Devices, Evolvable Hardware, Proceedings of the third NASA/DoD
Workshop on, July 2001, pp 33–35.

452
D.A. Gwaltney and M.I. Ferguson
[3]
Ferguson, M. I., Zebulum, R., Keymeulen, D. and Stoica, A., An Evolvable Hard-
ware Platform Based on DSP and FPTA, Late Breaking Papers at the Genetic and
Evolutionary Computation Conference (GECCO-2002), July 2002, pp. 145–152.
[4]
Stoica, A., Zebulum, R., Ferguson, M. I., Keymeulen, D. and Duong V., Evolving
Circuits in Seconds: Experiments with a Stand-Alone Board Level Evolvable Sys-
tem, 2002 NASA/DoD Conference on Evolvable Hardware, July 2002, pp. 67–74.
[5]
Langeheine, J., Meier, K., Schemmel, J., Intrinsic Evolution of Quasi DC solutions
for Transistor Level Analog Electronic Circuits Using a CMOS FTPA Chip, 2002
NASA/DoD Conference on Evolvable Hardware, July 2002, pp. 75–84.
[6]
Flockton, S. J. and Sheehan, K., “Evolvable Hardware Systems Using Pro-
grammable
Analogue
Devices”,
Evolvable
Hardware
Systems
(Digest
No.
1998/233), IEE Half-day Colloquium on , 1998 ,Page(s): 5/1–5/6.
[7]
Ozsvald, Ian, “Short-Circuit the Design Process: Evolutionary Algorithms for Cir-
cuit Design using Reconﬁgurable Analogue Hardware”, Master’s Thesis, Univer-
sity of Sussex, September, 1998.
[8]
Koza,J. R., Keane, M. A., Yu, J., Mydlowec, W. and Bennet, F., Automatic Syn-
thesis of Both the Control Law and Parameters for a Controller for a Three-lag
plant with Five-Second delay using Genetic Programming and Simulation Tech-
niques, American Control Conference, June 2000.
[9]
Keane, M. A., Koza, J. R., and Streeter, M.J., Automatic Synthesis Using Genetic
Programming of an Improved General-Purpose Controller for Industrially Repre-
sentative Plants, 2002 NASA/DoD Conference on Evolvable Hardware, July 2002,
pp. 67–74.
[10] Zebulum, R. S., Pacheco, M. A., Vellasco, M., Sinohara, H. T., Evolvable
Hardware: On the Automatic Synthesis of Analog Control Systems, 2000 IEEE
Aerospace Conference Proceedings, March 2000, pp 451–463.
[11] Raimondi, G. M., et. al., Large Electromechanical Actuation Systems for Flight
Control Surfaces, IEE Colloquium on All Electronic Aircraft, 1998.
[12] Jensen, S.C., Jenney, G. D., Raymond, B., Dawson, D., Flight Test Experience with
an Electromechanicl Actuator on the F-18 Systems Research Aircraft, Proceedings
of the 19th Digital Avionics System Conference, Volume 1, 2000.
[13] Byrd, V. T., Parker, J. K, Further Consideration of an Electromechanical
Thrust Vector Control Actuator Experiencing Large Magnitude Collinear Tran-
sient Forces, Proceedings of the 29th Southeastern Symposium on System Theory,
March 1997, pp 338–342.

Hardware Evolution of Analog Speed Controllers for a DC Motor
453
Appendix: FPTA2 Cell Diagram

An Examination of Hypermutation and Random
Immigrant Variants of mrCGA for Dynamic
Environments
Gregory R. Kramer and John C. Gallagher
Department of Computer Science and Engineering
Wright State University, Dayton, OH, 45435-0001
{gkramer, johng}@cs.wright.edu
1
Introduction
The mrCGA is a GA that represents its population as a vector of probabilities,
where each vector component contains the probability that the cooresponding
bit in an individual’s bitstring is a one [2]. This approach oﬀers signiﬁcant ad-
vantages during hardware implementation for problems where power and space
are severely constrained. However, the mrCGA does not currently address the
problem of continuous optimization in a dynamic environment. While, many dy-
namic optimization techniques for population-based GAs exist in the literature,
we are unaware of any attempt to examine the eﬀects of these techniques on
probability-based GAs. In this paper we examine the eﬀects of two such tech-
niques, hypermutation and random immigrants, which can be easily added to
the existing mrCGA without signiﬁcantly increasing the complexity of its hard-
ware implementation. The hypermutation and random immigrant variants will
be compared to the performance of the original mrCGA on a dynamic version
of the single-leg locomotion benchmark.
2
Dynamic Optimization Variants of mrCGA
The hypermutation strategy, proposed in [1], increases the mutation rate follow-
ing an environmental change and then slowly decreases it back to its original
level. For this problem the hypermutation variant was set to increase the muta-
tion rate from 0.05 to 0.1. Random immigrants is another strategy that diversiﬁes
the population by inserting random individuals [4]. Simulating the insertion of
random individuals is accomplished in the probability vector by shifting each
bit probability toward its original value of 50%. For this problem the random
immigrants variant was set to shift each bit probability by 0.12. To ensure fair
comparisons between the two variants, the hypermutation rate, and the bit prob-
ability shift were empirically determined to produce roughly the same divergence
in the GA’s population.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 454–455, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

An Examination of Hypermutation and Random Immigrant Variants
455
3
Testing and Results
The mrCGA and its variants were tested on the single-leg robot locomotion
problem. The goal for this problem is to evolve a ﬁve neuron CTRNN (Contin-
uous Time Recurrent Neural Network) controller that allows the robot to walk
forward at optimal speed. Each benchmark run consisted of 50,000 evaluation
cycles with the leg’s length and angular inertia changed every 5,000 evaluation
cycles. The algorithms were each run 100 times on this problem. Performance
was evaluated by examining the quality of the ﬁnal solution achieved prior to
each leg model change. A more formal examination of the single-leg locomotion
problem can be found in [3].
Comparisons between the mrCGA, hypermutation, and random immigrant
results show that the best solutions are achieved by the hypermutation variant.
The average pre-shift error for the mrCGA is 18.12%, whereas the average pre-
shift error for the hypermutation variant shows a 2.27% decrease to 15.85%. In
contrast, the random immigrant variant performed worse than mrCGA, with a
4.18% increase in error to 22.30%.
4
Conclusions
Our results show that for the single-leg locomotion problem, hypermutation in-
creases the quality of the mrCGA’s solution in a dynamic environment, whereas
the random immigrant variant produces slightly lower scores. Both of these vari-
ants can be easily added to the existing mrCGA hardware implementation with-
out signiﬁcantly increasing its complexity. In the future we plan to categorize the
eﬀects of the hypermutation and random immigrant strategies on the mrCGA
for a variety of generalized benchmarks. This categorization will be useful to
help determine which dynamic optimization strategy should be employed for a
given problem.
References
1. Cobb, H.G. (1990) An investigation into the use of hypermutation as an adaptive
operator in genetic algorithms having continuous, time-dependent nonstationary en-
vironments. Technical Report AIC-90-001, Naval Research Laboratory, Washington,
USA.
2. Gallagher, J.C. & Vigraham, S. (2002) A Modiﬁed Compact Genetic Algorithm for
the Intrinsic Evolution of Continuous Time Recurrent Neural Networks. The Pro-
ceedings of the 2002 Genetic and Evolutionary Computation Conference. Morgan-
Kaufmann.
3. Gallagher, J.C., Vigraham, S., & Kramer, G.R. (2002) A Family of Compact Genetic
Algorithms for Intrinsic Evolvable Hardware.
4. Grefenstette, J.J. (1992) Genetic algorithms for changing environments. In R. Maen-
ner and B. Manderick, editors, Parallel Problem Solving from Nature 2, pages 137–
144. North Holland.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 456–457, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Inherent Fault Tolerance in Evolved Sorting Networks
Rob Shepherd and James Foster*
Department of Computer Science, University of Idaho, Moscow, ID 83844
rob_shepherd@bigfoot.com
foster@cs.uidaho.edu
Abstract.  This poster paper summarizes our research on fault tolerance arising
as a by-product of the evolutionary computation process. Past research has
shown evidence of robustness emerging directly from the evolutionary process,
but none has examined the large number of diverse networks we used. Despite a
thorough study, the linkage between evolution and increased robustness is
unclear.
Discussion
Previous research has suggested that evolutionary search techniques may produce
some fault tolerance characteristics as a by-product of the process. Masner et al. [1, 2]
found evidence of this while evolving sorting networks, as their evolved circuits were
more tolerant of low-level logic faults than hand-designed networks. They also
introduced a new metric, bitwise stability (BS), to measure the degree of robustness in
sorting networks.
We evaluated the hypothesis that evolved sorting networks were more robust than
those designed by hand, as measured by BS. We looked at sorting networks with
larger numbers of inputs to see if the results reported by Masner et al. would still be
apparent.  We selected our subject circuits from three primary sources: hand-
designed, evolved and “reduced” networks. The last category included circuits
manipulated using Knuth’s technique in which we created a sorter for a certain
number of inputs by eliminating inputs and comparators from an existing network [3].
Masner et al. found that evolution produced more robust 6-bit sorting networks than
hand-designed ones reported in the literature. We expanded our set of comparative
networks, comprising 157 circuits sorting between 4 and 16 inputs. Our 16 bit
networks were only used as the basis for other reduced circuits.
Table 1 shows the results for our entire set of circuits.  We listed the 3 best networks
for each width to give some sense of the inconsistency between design methods. As
with the 4-bit sorters, evolution produced the best 5-, 7- and 10-bit circuits, but
reduction was more effective for 6, 9, 12 and 13 inputs. Juillé’s evolved 13-bit
____________________________________________________________
* Foster was partially funded for this research by NIH NCRR 1P20 RR16448.

Inherent Fault Tolerance in Evolved Sorting Networks         457
network (J13b_E) was inferior to the reduced circuits and Knuth’s 12-bit sorter
(Kn12b_H) was the only hand-designed network to make this list.
Table 1. Top 3 results for all sorting networks in Shepherd [4]. K represents the number of
inputs to the network and BS indicates the bitwise stability, as defined in [1]. The last character
of the index inidicates the design method: E for evolved, H for hand-designed, R for reduced
Best circuit
2nd best circuit
3rd best circuit
K
Index
BS
Index
BS
Index
BS
4
M4A_E
0.943359
M4Rc_E
0.942057 Kn4Rd_R
0.941840
5
M5A_E
0.954282
M5Rd_R
0.954028 M5Rc_R
0.953935
6
M6Ra_R
0.962836
Kn6Ra_R
0.962565
M6A_E
0.962544
7
M7_E
0.968276
M7Rc_R
0.968206 M7Ra_R
0.967892
9
M9R_R
0.976066
G9R_R
0.975509 Kn9Rb_R
0.975450
10
M10A_E
0.978257
H10R_R
0.978201 G10R_R
0.978189
12
H12R_R
0.981970
G12R_R
0.981932 Kn12b_H
0.981832
13
H13R_R
0.983494
G13R_R
0.983461
J13b_E
0.983305
Our data do not support our hypothesis that evolved sorting networks are more robust,
in terms of bitwise stability, than those designed by hand. Masner’s early work
showed evolution’s strength in generating robust networks, but support for the
hypothesis evaporated as we added more circuits to our comparison set, to the point
that there is no clear evidence that one design method inherently produces more
robust sorting networks. Our data do not necessarily disconfirm our hypothesis, but
leave it open for further examination. One area for future study is the linkage between
faults and the evolutionary operators. Thompson [5] used a representation method in
which faults and genetic mutation had the same effect, but these operators affected
different levels of abstraction in our model.
References
1. Masner, J., Cavalieri, J., Frenzel, J., & Foster, J. (1999). Representation and Robustness for
Evolved Sorting Networks. In Stoica, A., Keymeulen, D., & Lohn, J., (Eds.), The First
NASA/DoD Workshop on Evolvable Hardware, California: IEEE Computer Society, 255–
261.
2. Masner, J. (2000). Impact of Size, Representation and Robustness in Evolved Sorting
Networks. M.S. thesis, University of Idaho.
3. Knuth, D. (1998). The Art of Computer Programming, Volume 3: Sorting and Searching,
Second Edition, Massachusetts: Addison-Wesley, 219–229.
4. Shepherd, R. (2002). Fault Tolerance in Evolved Sorting Networks: The Search for Inherent
Robustness.  M.S. thesis, University of Idaho.
5. Thompson, A. (1995). Evolving fault tolerant systems. In Proceedings of the 1st IEE/IEEE
International Conference on Genetic Algorithms in Systems: Innovations and Applications
(GALESIA ’95). IEE Conference Publication No. 414, 524–529.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 458–469, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Co-evolving Task-Dependent Visual Morphologies in
Predator-Prey Experiments
Gunnar Buason and Tom Ziemke
Department of Computer Science, University of Skövde
Box 408, 541 28 Skövde, Sweden
{gunnar.buason,tom}@ida.his.se
Abstract. This article presents experiments that integrate competitive co-
evolution of neural robot controllers with ‘co-evolution’ of robot morphologies
and control systems. More specifically, the experiments investigate the
influence of constraints on the evolved behavior of predator-prey robots,
especially how task-dependent morphologies emerge as a result of competitive
co-evolution. This is achieved by allowing the evolutionary process to evolve,
in addition to the neural controllers, the view angle and range of the robot’s
camera, and introducing dependencies between different parameters.
1   Introduction
The possibilities of evolving both behavior and structure of autonomous robots has
been explored by a number of researchers [5, 7, 10, 15]. The artificial evolutionary
approach is based upon the principles of natural evolution and the survival of the
fittest. That means, robots are not pre-programmed to perform certain tasks, but
instead they are able to ‘evolve’ their behavior. This, to a certain level, decreases
human involvement in the design process as the task of designing the behavior of the
robot is moved from the distal level of the human designer down to the more proximal
level of the robot itself [13, 16]. As a result, the evolved robots are, at least in some
cases, able to discover solutions that might not be obvious beforehand to human
designers.
A further step in minimizing human involvement is adopting the principles of
competitive co-evolution (CCE) from nature, where in many cases two or more
species live, adapt and co-evolve together in a delicate balance. The adaptation of this
approach in Evolutionary Robotics allows for simpler fitness function and that the
evolved behavior of both robot species emerges in incremental stages [13]. The use of
this approach has been extended, not only co-evolving the neural control system of
two competing robotic species, but also ‘co-evolving’ the neural control system of a
robot together with its morphology.
The experiments performed by Cliff and Miller [5, 6] can be mentioned as
examples of demonstrations of CCE in evolutionary robotics, both concerning
evolution of morphological parameters (such as ‘eye’ positions) and behavioral
strategies between two robotic species. More recent experiments are the ones
performed by Nolfi and Floreano [7, 8, 9, 12]. In a series of experiments they studied
different aspects of CCE of neural robot controllers in a predator-prey scenario. In

Co-evolving Task-Dependent Visual Morphologies in Predator-Prey Experiments
459
one of their experiments [12] Nolfi and Floreano demonstrated that the robots’
sensory-motor structure had a large impact on the evolution of behavioral (and
learning) strategies, resulting in a more natural ‘arms race’ between the robotic
species. Different authors have further pointed out in [14, 15] that an evolutionary
process that allows the integrated evolution of morphology and control might lead to
completely different solutions that are to a certain extent less biased by the human
designer.
The aim of our overall work has been to further systematically investigate the
tradeoffs and interdependencies between morphological parameters and behavioral
strategies through a series of predator-prey experiments in which increasingly many
aspects are subject to self-organization through CCE [1, 3]. In this article we only
present experiments that extend the experiments of Nolfi and Floreano [12]
considering two robots, both equipped with cameras, taking inspiration mostly from
Cliff and Miller’s [6] work on the evolution of “eye” positions. However, the focus
will not be on evolving the positions of the sensors on the robot alone but instead on
investigating the trade-offs the evolutionary process makes in the robot morphology
as a result of different constraints and dependencies, both implicit and explicit. The
latter is in line with the research of Lee et al. [10] and Lund et al. [11].
2   Experiments
The experiments described in this paper focus on evolving the weights of the neural
network, i.e. the control system, and the view angle of the camera (0 to 360 degrees)
as well as its range (5 to 500 mm) of two predator-prey robots. That means, only a
limited number of morphological parameters were evolved. The size of the robot was
kept constant, assuming a Khepera-like robot, using all the infrared sensors, for the
sake of simplicity. In addition constraints and dependencies were introduced, e.g. by
letting the view angle constrain the maximum speed, i.e. the larger the view angle, the
lower the maximum speed the robot was allowed to accelerate to. This is in contrast
to the experiments in [7, 8, 9, 12], where the predator’s maximum speed was always
set to half the prey’s. All experiments were replicated three times.
2.1   Experimental Setup
For finding and testing the appropriate experimental settings a number of pilot
experiments were performed [1]. The simulator used in this work is called YAKS [4],
which is similar to the one used in [7, 8, 9, 12]. YAKS simulates the popular Khepera
robot in a virtual environment defined by the experimenter (cf. Fig. 1). The simulation
of the sensors is based on pre-recorded measurements of a real Khepera robot’s
infrared sensors and motor commands at different angles and distances [1].
    The experimental framework that was implemented in the YAKS simulator was in
many ways similar to the framework used in [7, 8, 9, 12]. What differed was that in
our work we used a real-valued encoding to represent the genotype instead of direct

460
G. Buason and T. Ziemke
 
Input from  
infrared sensors 
Left motor output
Right motor output 
Input from  
vision module 
470 mm 
470 mm 
View angle 
Infrared  
sensors 
Camera 
View range 
Khepera robot 
Fig. 1. Left: Neural network control architecture (adapted from [7]). Center: Environment and
starting positions. The thicker circle represents the starting position of the predator while the
thinner circle represents the starting position of the prey. The triangles indicate the starting
orientation of the robots, which is random for each generation. Right: Khepera robot equipped
with eight short-range infrared sensors and a vision module (a camera).
encoding, and the number of generations was extended from 100 to 250 generations
to allow us to observe the morphological parameters over longer period of time.
Beside that, most of the evolutionary parameters were ‘inherited’ such as the use of
elitism as a selection method, choosing the 20 best individuals from a population of
100 for reproduction. In addition, a similar fitness function was used. Maximum
fitness was one point while minimum fitness was zero points. The fitness was a
simple time-to-contact measurement, giving the selection process finer granularity,
where the prey achieved the highest fitness by avoiding the predator for as long as
possible while the predator received the highest fitness by capturing the prey as soon
as possible. The competition ended if the prey survived for 500 time steps or when the
predator made contact with the prey before that.
For each generation the individuals were tested for ten epochs. During each epoch,
the current individual was tested against one of the best competitors of the ten
previous generations. At generation zero, competitors were randomly chosen within
the same generation, whereas in the other nine initial generations they were randomly
chosen from the pool of available best individuals of previous generations. This is in
line with the work of [7, 8, 9, 12]. In addition, the same environment as in [7, 8, 9, 12]
was used (cf. Fig. 1).
A simple recurrent neural network architecture was used, similar to the one used in
[7, 8, 9, 12] (cf. Fig. 1). The experiments involved both robots using the camera so
each control network had eight input neurons for receiving input from the infrared
sensors and five input neurons for the camera. The neural network had one sigmoid
output neuron for each motor of the robot. The vision module, which was only one-
dimensional, was implemented with flexible view range and angle while the number
of corresponding input neurons was kept constant.
For each experiment, the weights of the neural network were initially randomized
and evolved using a Gaussian distribution with a standard deviation of 2.0. The
starting values of angle and range were randomized using a uniform distribution
function, and during evolution the values were mutated using Gaussian distribution
with a standard deviation of 5.0. The view angle could evolve up to 360 degrees; if
the random function generated a value of over 360 degrees then the view angle was

Co-evolving Task-Dependent Visual Morphologies in Predator-Prey Experiments
461
set to 360 degrees. The same was valid for the lower bounds of the view angle and
also for the lower and upper bounds of the view range.
Constraints, such as those used in [7, 8, 9, 12], where the maximum speed of the
predator was only half the prey’s, were adapted here where speed was dependent on
the view angle. For this, the view angle was divided into ten intervals covering 36
degrees each1. The maximum speed of the robot was then reduced by 10% for each
interval, e.g. if the view angle was between 0 and 36 degrees there were no
constraints on the speed, and if it was a value between 36 and 72 degrees, the
maximum speed of the robot was limited to 90% of its original maximum speed.
2.2   Results
The experiments were analyzed using fitness measurements, Master Tournament [7]
and collection of CIAO data [5]. A Master Tournament shows the performance of the
best individuals of each generation tested against all best competitors from that
replication. CIAO data are fitness measurements collected by arranging a tournament
where the current individual of each generation competes against all the best
competing ancestors [5]. In addition some statistical calculations and behavioral
observations were performed. Concerning analysis of the robots’ behavior,
trajectories from different tournaments will be presented together with qualitative
descriptions. Here a summary of the most interesting results will be given (for further
details see [1]).
Experiment A: Evolving the Vision Module
This experiment (cf. experiment 9 in [1]) extends Nolfi and Floreano’s experiment in
[12]. What differs is that here the view angle and range are evolved instead of being
constant. In addition, the speed constraints were altered by setting the maximum
speed to the same value for both robots, i.e. 1.0, and instead the maximum speed of
the predator was constrained by its view angle.
Nolfi and Floreano [12] performed their experiments in order to investigate if more
interesting arms races would emerge if the richness of the sensory mechanisms of the
prey was increased by giving it a camera. The results showed that “by changing the
initial conditions ‘arms races’ can continue to produce better and better solutions in
both populations without falling into cycles” [12]. That is, the prey is able to refine its
strategy to escape the predator instead of radically changing it. In our experiments the
results varied between replications when considering this aspect, i.e. the prey was not
always able to evolve a suitable evasion strategy.
Fig. 2 presents the results of the Master Tournament. The graph presents the
average results of ten runs, i.e. each best individual was tested for ten epochs against
its opponent. Maximum fitness achievable was 250 points as there were 250
opponents. As Fig. 2 illustrates, both predator and prey make evolutionary progress
initially, but in later generations only the prey exhibits steady improvement.
The text on the right in Fig. 2 summarizes the Master Tournament. The two upper
columns describe in what generation it is possible to find the predator respectively the
                                                          
1 Alternatively, a linear relation between view angle and speed could be used.

462
G. Buason and T. Ziemke
50
100
150
200
250
50
100
150
200
250
Generation
Fitness
Master Tournament
Best Predator
1. FIT: 131, GEN: 8
2. FIT: 130, GEN: 18
3. FIT: 120, GEN: 157
4. FIT: 120, GEN: 42
5. FIT: 118, GEN: 25
Entertaining robots
1. FIT.DIFF: 6, GEN: 28
2. FIT.DIFF: 6, GEN: 26
3. FIT.DIFF: 8, GEN: 35
4. FIT.DIFF: 8, GEN: 31
5. FIT.DIFF: 11, GEN: 27
Best Prey
1. FIT: 233, GEN: 245
2. FIT: 232, GEN: 244
3. FIT: 232, GEN: 137
4. FIT: 231, GEN: 216
5. FIT: 229, GEN: 247
Optimized robots
1. PR: 110, PY: 232, GEN: 137
2. PR: 103, PY: 223, GEN: 115
3. PR: 102, PY: 221, GEN: 144
4. PR: 95, PY: 225, GEN: 143
5. PR: 94, PY: 222, GEN: 111
Average fitness for prey    
Average fitness for predator
Fig. 2. Master Tournament (cf. Experiment 9 in [1, 2]). The data was smoothed using rolling
average over three data points. The same is valid for all following Master Tournament graphs.
Observe that the values in the text to the right have not been smoothed, and therefore do not
necessarily fit the graph exactly.
best prey with the highest fitness score. The lower left column demonstrates where it
is possible to find the most entertaining tournaments, i.e. robots that report similar
fitness have a similar chance of winning. The lower right column demonstrates where
in the graph the most optimized robots can be found, i.e. generations of robots where
both robots have high fitness values.
The left graphs of Fig. 3 display the evolution of view angle and range for the
predator and prey, i.e. the evolved values from the best individual from each
generation. For the predator the average view range evolved was 344 mm and the
average view angle evolved was 111°. It does not seem that the evolutionary process
found a balance while evolving the view range as the standard deviation is 105 mm,
but the view angle is more balanced with a standard deviation of 48°. The prey
evolved an average view range of 247 mm (with a standard deviation of 125 mm) and
an average view angle of 200° (with a standard deviation of 86°). These results
indicate that the predator prefers a rather narrow view angle with a rather long view
range (in the presence of explicit constraints), while the prey evolves a rather wide
view angle with a rather short view range (in the absence of explicit constraints) (cf.
Fig. 3).
    Fig. 3, right graph, presents a histogram over the number of different angle
intervals evolved by the predator. The number above each interval represents the
maximum speed interval, e.g. in this case most of the predator individuals evolved a
view angle between 108 and 144 degrees and therefore the speed were constrained to
be within the interval of 0.0 to 0.7. The distribution seems to be rather normalized
over the different view angle intervals (between 0 and 252 degrees) (cf. Fig. 3 right).
In other replications of this experiment, the evolutionary process found a different
balance between view angle and speed, where a smaller view angle was evolved with
high speed. Unlike the distribution in the right graph in Fig. 3 where a large number
of predator individuals prefer to evolve a view angle between 108 and 144 degrees, in
other replications the distribution was mostly between 0 and 72 degrees, implying

Co-evolving Task-Dependent Visual Morphologies in Predator-Prey Experiments
463
Fig. 3. Left: Morphological description of predator and prey (cf. Experiment 9 in [1, 2]). The
graphs present the morphological description of view angle (left y-axis, thin line) and view
range (right y-axis, thick line). The values in the upper left corner of the graphs are the mean
and standard deviation for the view range over generations, calculated from the best individual
from each generation. Corresponding values for the view angle are in the lower left corner. The
data was smoothed using rolling average over ten data points. The same is valid for all
following morphological description graphs. Right: Histogram over view angle of predator (cf.
Experiment 9 in [1, 2]). The graph presents a histogram over view angle, i.e. the number of
individuals that preferred a certain view angle. The values above each bin indicate the
maximum speed interval.
small, focused view range and high speed. These results, however, depend on the
behavior that the prey evolves. If the prey is not successful in evolving its evasion
strategy, perhaps crashing into walls, then the predator could evolve a very focused
view angle with a high speed. On the other hand, if the prey evolves a successful
evasion strategy, moving fast in the environment, then the predator needs a larger
view angle in order to be able to follow the prey.
In Fig. 4 a number of trajectories are presented. The first trajectory snapshot is
taken from generation 43. This trajectory shows a predator with a view angle of 57°
and a view range of 444 mm chasing a prey with a view angle of 136° and a view
range of 226 mm. The snapshot is taken after 386 time steps. The prey starts by
spinning in place until it notices the predator in its field of vision. Then it starts
moving fast in the environment in an elliptical trajectory.
Moving this way the prey (cf. Fig. 4, left part) is able to escape the predator. This is
an interesting behavior from the prey as it can only sense the walls with its infrared
sensors while the predator needs only to follow the prey in its field of vision in a
circular trajectory. However, after few generations the predator looses the ability to
follow the prey and never really recovers in later generations. An example of this is
the snapshot of a trajectory taken in generation 157 after 458 time steps (Fig 4., right).
Here the predator has a 111° view angle and a 437 mm view range while the prey has
an 86° view angle and a 251 mm view range. As previously, the prey starts by
spinning until it notices the predator in the field of vision. Then it starts moving
around in the environment, this time following walls. The predator does not
demonstrate any good abilities in capturing the prey. Instead, it spins around in the
center of the environment, trying to locate the prey.

464
G. Buason and T. Ziemke
Fig. 4. Trajectories from generation 43 (left) (predator: 57°, 444 mm; prey: 136°, 226 mm) and
157 (right) (predator: 111°, 437 mm; prey: 86°, 251 mm), after 386 and 458 time steps
respectively (cf. Experiment 9 in [1]). The predator is marked with a thick black circle and the
trajectory with a thick black line. The prey is marked with a thin black circle and the trajectory
with a thin black line. Starting positions of both robots are marked with small circles. The view
field of the predator is marked with two thick black lines. The angle between the lines
represents the current view angle and the length of the lines represents the current view range.
Another interesting observation is that the prey mainly demonstrates the behavior
described above, i.e. staying in the same place, spinning, until it sees the predator, and
then starts its ‘moving around’ strategy.
Experiment B: Adding Constraints
This experiment (cf. experiment 10 in [1]) extends the previous experiment by adding
a dependency between the view angle and the speed of the prey. As previously, the
predator is implemented with this dependency. View angle and range of both species
are then evolved. The result of this experiment was that the predator became the
dominant species (cf. Fig. 5), despite the fact that the prey had certain advantages
over the predator considering the starting distance and the fitness function being
based on time-to-contact.
    A Master Tournament (cf. Fig. 5) illustrates that evolutionary progress only occurs
during the first generations and then the species come to a balance where minor
changes in the strategy result in a valley in the fitness landscape. To investigate if the
species cycle between behaviors, CIAO data was collected. Each competition was run
ten times and the results were then averaged, i.e. zero in fitness score is the worst and
one in fitness score is the best. The ‘Scottish tartan’ patterns in the graphs (Fig. 6)
indicate periods of relative stasis interrupted by short and radical changes of behavior
[7]. The CIAO data also shows that the predator is the dominating species. Stripes on
the vertical axis in the graph for the prey indicate a good predator where the stripe is
black respectively a bad predator where the stripe is white. This is more noticeable for
the predator than for the prey, i.e. either the predator is overall good or overall bad
while the prey is more balanced.
An interesting aspect is the evolution of the morphology (cf. Fig. 7). The predator, as
in the previous experiment, evolves a rather small view angle with a rather long
range. The prey also evolves a rather small view angle, in fact a smaller view angle
than the predator, and a relative short view range with a relatively high standard
deviation.

Co-evolving Task-Dependent Visual Morphologies in Predator-Prey Experiments
465
50
100
150
200
250
50
100
150
200
250
Generation
Fitness
Master Tournament
Best Predator
1. FIT: 218, GEN: 132
2. FIT: 217, GEN: 70
3. FIT: 215, GEN: 174
4. FIT: 214, GEN: 135
5. FIT: 212, GEN: 115
Entertaining robots
1. FIT.DIFF: 0, GEN: 29
2. FIT.DIFF: 3, GEN: 100
3. FIT.DIFF: 4, GEN: 3
4. FIT.DIFF: 4, GEN: 148
5. FIT.DIFF: 5, GEN: 22
Best Prey
1. FIT: 175, GEN: 25
2. FIT: 161, GEN: 23
3. FIT: 155, GEN: 19
4. FIT: 154, GEN: 29
5. FIT: 153, GEN: 22
Optimized robots
1. PR: 191, PY: 155, GEN: 19
2. PR: 201, PY: 144, GEN: 180
3. PR: 202, PY: 141, GEN: 181
4. PR: 205, PY: 132, GEN: 176
5. PR: 185, PY: 151, GEN: 20
Average fitness for prey    
Average fitness for predator
Fig. 5. Master Tournament.
Fig. 6. CIAO data (cf. Experiment 10 in [1, 2]). The colors in the graph represent fitness values
of individuals from different tournaments. Higher fitness corresponds to darker colors.
When looking at the relation between view angle and view range in the morphological
space then certain clusters can be observed (cf. Fig. 8). The predator descriptions
form a cluster in the upper left corner of the area where the view angle is rather
focused while the view range is rather long. The interesting part is that the prey also
forms clusters with an even smaller view angle, i.e. it ‘chooses’ speed over vision.
The clustering of the range varies from small range to very long range, indicating that
for the prey the range is not so important.
The evolution of the view angle is further illustrated in Fig. 9. While the predator
seems to prefer to evolve a view angle between 36 and 72 degrees, the prey prefers to
evolve a view angle between 0 and 36 degrees. This indicates that, in this case the
prey prefers speed to vision. The reason behind this lies in the morphology of the
robots. The robots have eight infrared sensors, two of them on the rear side of the
robots and six of them on the front side. The camera on the robots is placed in a
frontal direction, i.e. in the same direction as the six infrared sensors. The robots then
mainly use the front infrared sensors for obstacle avoidance. Therefore when the prey
evolves a strategy to move fast in the environment because the predator follows it, it

466
G. Buason and T. Ziemke
has more use of moving fast than being able to see. Therefore, it more or less
‘ignores’ the camera and evolves the ability to move fast, relying on its infrared
sensors.
50
100
150
200
250
0
36
72
108
144
180
216
252
288
324
360
Mean: 88; Std: 38
Mean: 392; Std: 74
Generation
Angle
50
100
150
200
250
5
50
100
150
200
250
300
350
400
450
500
Range
Predator description
Range
Angle
50
100
150
200
250
0
36
72
108
144
180
216
252
288
324
360
Mean: 55; Std: 58
Mean: 279; Std: 136
Generation
Angle
50
100
150
200
250
5
50
100
150
200
250
300
350
400
450
500
Range
Prey description
Range
Angle
Fig. 7. Morphological descriptions (cf. Experiment 10 in [1, 2]).
0
50
100
150
200
250
0
36 72 108144 180 216252288 324360
100
200
300
400
500
Angle
Range
Predator description (Generation 0 - 250)
0
50
100
150
200
250
0
36 72 108144 180216252 288 324360
100
200
300
400
500
Angle
Range
Prey description (Generation 0 - 250)
Fig. 8. Morphological space (cf. Experiment 10 in [1, 2]). The graphs present relations between
view angle and view range in the morphological space. Each diamond represents an individual
from a certain generation. The gray level of the diamond indicates the fitness achieved during a
Master Tournament, wit. darker diamonds indicating higher fitness.
A number of trajectories in Fig. 10 display the basic behavior observed during the
tournaments. On the left is a trajectory snapshot taken in generation 23 after 377 time
steps. The predator has evolved a 99° view angle and a 261 mm view range, while the
prey has evolved a 35° view angle and a 484 mm view range. The prey tries to avoid
the predator by moving fast in the environment following the walls. The predator tries
to chase the prey but the prey is faster than the predator so no capture occurs. In this
tournament, the predator also has the strategy of waiting for the prey until it appears
in its view field, and then attack (which in this case fails). Although this strategy was
successful in a number of tournaments, this strategy was rarely seen in the overall
evolutionary process.

Co-evolving Task-Dependent Visual Morphologies in Predator-Prey Experiments
467
0
36
72 108 144 180 216 252 288 324 360
0
100
200
300
400
500
Histogram of Predator angle
Angle interval
Count
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0
36
72 108 144 180 216 252 288 324 360
0
100
200
300
400
500
Histogram of Prey angle
Angle interval
Count
1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
Fig. 9. Histogram over view angle of predator and prey (cf. Experiment 10 in [1, 2]).
Fig. 10. Trajectories from generations 23 (predator: 99°, 261 mm; prey: 35°, 484 mm), 134
(predator: 34°, 432 mm; prey: 11°, 331 mm) and 166 (predator: 80°, 412 mm; prey: 79°, 190
mm), after 377, 54 and 64 time steps respectively (cf. Experiment 10 in [1]).
In the middle snapshot (cf. Fig. 10), both predator and prey have evolved narrow view
angle (less than 36°), which implies maximum speed. As soon as the predator
localizes the prey, it moves straight ahead trying to capture it.
The snapshot on the right demonstrates that for a few generations (snapshot taken
in generation 166) the prey tried to change strategy by starting to spin in the same
place and as soon as it had seen the predator in its field of vision it started moving
around. The prey has a view angle of 80° and a view range of 190 mm. This,
however, implies constraints on the speed and therefore the predator soon captures the
prey. The strategy was only observed for a few generations.
3   Summary and Conclusions
The experiments described in this article involved evolving camera angle and range of
both predator and prey robots. Different constraints were added into the behaviors of
both robots, manipulating the maximum speed of the robots.
In experiment A the prey ‘prefers’ a camera with a wide view angle and a short
view range. This can be considered as a result of coping with the lack of depth
perception, i.e. not being able to know how far away the predator is. In the presence
of constraints in experiment B, the prey made a trade-off between speed and vision,

468
G. Buason and T. Ziemke
preferring the former. The predator, on the other hand, in both experiments preferred
a rather narrow view angle with a relative long view range. Unlike the prey it did not
make the same trade-off between speed and vision, i.e. although speed was needed to
chase the prey, vision was also needed for that task. Therefore, the predator evolved a
balance between view angle and speed.
In sum, this paper has demonstrated the possibilities of allowing the evolutionary
process to evolve appropriate morphologies suited for the robots’ specific tasks. It has
also demonstrated how different constraints can affect both the morphology and the
behavior of the robots, and how the evolutionary process was able to make trade-offs,
finding appropriate balance. Although these experiments definitely have limitations,
e.g. concerning the possibilities of transfer to real robots, and only reflect on certain
parts of evolving robot morphology, we still consider this work as a further step
towards removing the human designer from the loop, suggesting a mixture of CCE
and ‘co-evolution’ of brain and body.
References
1. 
Buason, G. (2002a). Competitive co-evolution of sensory-motor systems. Masters
Dissertation HS-IDA-MD-02-004. Department of Computer Science, University of
Skövde, Sweden.
2. 
Buason, G. (2002b). Competitive co-evolution of sensory-motor systems - Appendix.
Technical Report HS-IDA-TR-02-004. Department of Computer Science, University of
Skövde, Sweden.
3. 
Buason, G. & Ziemke, T. (in press). Competitive Co-Evolution of Predator and Prey
Sensory-Motor Systems. In: Second European Workshop on Evolutionary Robotics.
Springer Verlag, to appear.
4. 
Carlsson, J. & Ziemke, T. (2001). YAKS - Yet Another Khepera Simulator. In: Rückert,
Sitte & Witkowski (eds.), Autonomous minirobots for research and entertainment -
Proceedings of the fifth international Heinz Nixdorf Symposium (pp. 235–241). Paderborn,
Germany: HNI-Verlagsschriftenreihe.
5. 
Cliff, D. & Miller, G. F. (1995). Tracking the Red Queen: Measurements of adaptive
progress in co-evolutionary simulations. In: F. Moran, A. Moreano, J. J. Merelo, & P.
Chacon, (eds.), Advances in Artificial Life: Proceedings of the third european conference
on Artificial Life. Berlin: Springer-Verlag.
6. 
Cliff, D. & Miller, G. F. (1996). Co-evolution of pursuit and evasion II: Simulation
methods and results. In: P. Maes, M. Mataric, J.-A. Meyer, J. Pollack & , S. W. Wilson
(eds.), From animals to animats IV: Proceedings of the fourth international conference on
simulation of adaptive behavior (SAB96) (pp. 506-515). Cambridge, MA: MIT Press.
7. 
Floreano, D. & Nolfi, S. (1997a). God save the Red Queen! Competition in co-
evolutionary robotics. In: J. R. Koza, D. Kalyanmoy, M. Dorigo, D. B. Fogel, M. Garzon,
H. Iba, & R. L. Riolo (eds.), Genetic programming 1997: Proceedings of the second
annual conference. San Francisco, CA: Morgan Kaufmann.
8. 
Floreano, D. & Nolfi, S. (1997b). Adaptive behavior in competing co-evolving species. In
P. Husbands, & I. Harvey (eds.), Proceedings of the fourth European Conference on
Artificial Life. Cambridge, MA: MIT Press.
9. 
Floreano, D., Nolfi, S. & Mondada, F. (1998). Competitive co-evolutionary robotics: From
theory to practice. In: R. Pfeifer, B. Blumberg, J-A. Meyer, & S. W. Wilson (eds.), From
animals to animats V: Proceedings of the fifth international conference on simulation of
adaptive behavior. Cambridge, MA: MIT Press.

Co-evolving Task-Dependent Visual Morphologies in Predator-Prey Experiments
469
10. Lee, W-P, Hallam, J. & Lund, H.H. (1996). A hybrid GP/GA Approach for co-evolving
controllers and robot bodies to achieve fitness-specified tasks. In: Proceedings of IEEE
third international conference on evolutionary computation (pp. 384–389). New York:
IEEE Press.
11. Lund, H., Hallam, J. & Lee, W. (1997). Evolving robot morphology. In: IEEE
International Conference on Evolutionary Computation (ed.), Proceedings of IEEE fourth
international conference on evolutionary computation (pp. 197–202). New York: IEEE
Press.
12. Nolfi, S. & Floreano, D. (1998). Co-evolving predator and prey robots: Do ‘arms races’
arise in artificial evolution? Artificial Life, 4, 311–335.
13. Nolfi, S. & Floreano, D. (2000). Evolutionary robotics: The biology, intelligence, and
technology of self-organizing machines. Cambridge, MA: MIT Press.
14. Nolfi, S. & Floreano, D. (2002). Synthesis of autonomous robots through artificial
evolution. Trends in Cognitive Sciences, 6, 31–37.
15. Pollack, J. B., Lipson, H., Hornby, G. & Funes, P. (2001). Three generations of
automatically designed robots. Artificial Life, 7, 215–223.
16. Sharkey, N. E. & Heemskerk, J. N. H. (1997). The neural mind and the robot. In: Browne,
A. (ed.), Neural network perspectives on cognition and adaptive robotics (pp. 169–194).
Institute of Physics Publishing, Bristol, UK.

Integration of Genetic Programming and
Reinforcement Learning for Real Robots
Shotaro Kamio, Hideyuki Mitsuhashi, and Hitoshi Iba
Graduate School of Frontier Science, The University of Tokyo,
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan.
{kamio,mituhasi,iba}@miv.t.u-tokyo.ac.jp
Abstract. We propose an integrated technique of genetic programming
(GP) and reinforcement learning (RL) that allows a real robot to execute
real-time learning. Our technique does not need a precise simulator be-
cause learning is done with a real robot. Moreover, our technique makes
it possible to learn optimal actions in real robots. We show the result of
an experiment with a real robot AIBO and represents the result which
proves proposed technique performs better than traditional Q-learning
method.
1
Introduction
When executing tasks by autonomous robots, we can make the robot learn what
to do so as to complete the task from interactions with its environment but not
manually pre-program for all situations. We know that such learning techniques
as genetic programming (GP)[1] and reinforcement learning (RL)[2] work as
means for automatically generating robot programs.
When applying GP, we should repeatedly evaluate many individuals over
several generations. Therefore, it is diﬃcult to apply GP to problems that re-
quires too much time for evaluations of individuals. That is why we ﬁnd very
few previous studies on learning with a real robot.
To obtain optimal actions using RL, it is necessary to repeat learning trials
time after time. The huge amount of learning time required presents a great
problem when using a real robot. Accordingly, most studies deal with the prob-
lems of receiving an immediate reward from an action as shown in [3], or loading
the results learned with a simulator into a real robot as shown in [4,5].
Although it is generally accepted to learn with a simulator and apply the
result to a real robot, there are many tasks that are diﬃcult to make a precise
simulator. Applying these methods with an imprecise simulator could result in
creating programs which may function optimally on the simulator but cannot
provide optimal actions with a real robot. Furthermore, the operating charac-
teristics of a real robot show certain variations due to minor errors in the manu-
facturing process or to changes with time. We cannot cope with such diﬀerences
of robots only using a simulator.
Learning process with a real robot is surely necessary, therefore, for it to
acquire optimal actions. Moreover, learning with a real robot sometimes makes
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 470–482, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Integration of Genetic Programming and Reinforcement Learning
471
Fig. 1. The robot AIBO, the box and the goal area.
possible to learn even hardware and environmental characteristics, thus allowing
the robot to acquire unexpected actions.
To solve the above diﬃculties, we propose a technique that allows a real
robot to execute real-time learning in which GP and RL are integrated. Our
proposed technique does not need a precise simulator because learning is done
with a real robot. As a result of this idea, we can greatly reduce the cost to make
the simulator much precise and acquire the program which acts optimally in the
real robot.
The main contributions of this paper are summarized as follows:
1. We propose an integrated method of GP and RL.
2. We give empirical results to show how our approach works well for real-robot
learning.
3. We conduct comparative experiments with traditional Q-learning to show
the superiority of our method.
The next section gives the deﬁnition of a task in this study. After that,
Section 3 explains our proposed technique and Section 4 presents experimental
results with a real robot. Section 5 provides the result of comparison and future
researches. Finally, a conclusion is given.
2
Task Deﬁnition
We used an “AIBO ERS-220” (Fig. 1) robot sold by SONY as the real robot
in this experiment. AIBO’s development environment is freely available for non-
commercial use and we can program with C++ language on it [6]. An AIBO
has a CCD camera on its head, and moreover, an AIBO is equipped with image
processor. It is able to easily recognize objects of speciﬁed colors on a CCD
image at high speed.
The task in this experiment was to carry a box to a goal area. One of the
diﬃculties of this task is that the robot has four legs. As a result, when the robot
moves ahead, we see cases where the box sometimes is moved ahead or deviates
from side to side, depending on the physical relationship between the box and
AIBO legs. It is extremely diﬃcult, in fact, to create a precise simulator that
accurately expresses this box movements.

472
S. Kamio, H. Mitsuhashi, and H. Iba
3
Proposed Technique
In this paper, we propose a technique that integrates GP and RL. As can be seen
in Fig. 2(a), RL as individual learning is outside of the GP loop in the proposed
technique. This technique enables us (1) to speed up learning in real robot and
(2) to cope with the diﬀerences between a simulator and a real robot.
(a) Proposed technique of the inte-
gration of GP and RL.
(b) Traditional method combined
GP and RL [7,8].
Fig. 2. The ﬂow of the algorithm.
The proposed technique consists of two stages (GP part and RL part).
1. Carry out GP on a simpliﬁed simulator, and formulate programs that have
the standards for robot actions required for executing a task.
2. Conduct individual learning (= RL) after loading the programs obtained in
Step 1 above.
In the ﬁrst step above, the programs that have the standards for the actions
required of a real robot to execute a task are created through the GP process.
The learning process of RL can be speeded up in the second step because the
state space is divided into partial spaces under the judgment standards obtained
in the ﬁrst step. Moreover, preliminary learning with a simulator allows us to
anticipate that a robot performs target-oriented actions from the beginning of
the second stage. We used Q-learning as RL method in this study.
Although the process expressed by the external dotted line in Fig.2(a) was
not realized in this study, it is a feedback loop. We consider that the parameters
in a real environment that have been acquired via individual learning should
ideally be fed back through this loop route.
The comparison with the traditional method (Fig. 2(b)) is discussed later in
Sect. 5.2.

Integration of Genetic Programming and Reinforcement Learning
473
3.1
RL Part Conducted on the Real Robot
Action set. We prepared six selectable robot actions (move forward, retreat,
turn left, turn right, retreat + turn left, and retreat + turn right). These actions
are far from ideal ones: e.g. “move forward” action is not only to move the robot
straightly forward but also has some deviations from side to side and “turn left”
action is not only to turn left but also move the robot a little bit forward. The
robot has to learn these characteristics of actions.
Every action takes approximately four seconds and eight seconds including
the swinging of the head described below. It is, therefore, advisable that the
learning time is as short as possible.
State Space. The state space was structured based on positions from where
the box and the goal area can be seen in the CCD image, as described in [4].
The viewing angle of AIBO CCD is so narrow that the box or the goal area
cannot be seen well with only one-directional images, in most cases. To avoid
this diﬃculty, we added a mechanism to compensate for the surrounding images
by swinging AIBO’s head so that state recognition can be conducted by the head
swinging after each action. This head swinging operation was always uniformly
given throughout the experiment as it was not an element to be learned in this
study.
Figure 3 is the projection of the box state on the ground surface. The “near
center” position is where the box ﬁts into the two front legs. The box can be
moved if the robot pushes it forward in this state. The box remains same position
“near center” after the robot turns left or right in this state because the robot
holds the box between two front legs. The state with the box not being in view
was deﬁned as “lost”; the state with the box not being in view and one preceding
step at the left was deﬁned as “lost into left” and, similarly, “lost into right” was
deﬁned.
Fig. 3. States in real robot for the box. The front of the robot is upside of this ﬁgure.
We should pay special attention to the position of legs. Depending on the
physical relationship between the box and AIBO legs, the movement of the box
varies from moving forward to deviating from side to side. If an appropriate state

474
S. Kamio, H. Mitsuhashi, and H. Iba
space is not deﬁned, the Markov property of the environment, which is a premise
of RL, cannot be met, thereby optimal actions cannot be found. Therefore,
we deﬁned “near straight left” and “near straight right” states at the frontal
positions of the front legs.
We thus deﬁned 14 states for the box. We similarly deﬁned states of the goal
area except that “near straight left” and “near straight right” states do not exist
in them. There are 14 states for the box and 12 for the goal area; hence, this
environment has states of their product, i.e., 168 states totally.
3.2
GP Part Conducted on the Simulated Robot
Simulator. The simulator in our experiment uses a robot expressed in circle
on a two-dimensional plane, a box, and a goal area ﬁxed on a plane. The task
is completed when the robot pushes the box forward and overlaps the goal area
on this plane.
We deﬁned three actions (move forward, turn left, turn right) as action set
and deﬁned the state space in the simulator which is the simpliﬁed state space
used for a real robot as Fig. 4. While actions of the real robot are not ideal ones,
these actions in the simulator are ideal ones.
Fig. 4. States for box and goal area in the simulator. The area box ahead is not the
state but the place where if box ahead executes ﬁrst argument.
Such actions and a state division is similar to that of a real robot, but is
not exactly the same. In addition, physical parameters such as box weight and
friction were not measured nor was the shape of the robot taken into account.
Therefore, this simulator is very simple and it is possible to build it in low cost.
The two transfer characteristics of the box expressed by the simulator are
the following.
1. The box moves forward if the box comes in contact with the front of the
robot when the robot goes ahead1.
2. After rotation, the box is near the center of the robot if the box is near the
center of the robot when the robot turns2.
1 This corresponds to the situation that real robot pushes the box forward.
2 This corresponds to the situation in which the box is placed between the front legs
of a real robot when it is turning.

Integration of Genetic Programming and Reinforcement Learning
475
Settings of GP. The terminals and functions used in GP were as follows:
Terminal set: move forward, turn left, turn right
Function set: if box ahead, box where, goal where, prog2
The terminal nodes above respectively correspond to the “move forward”, “turn
left”, and “turn right” actions in the simulator. The functional nodes box where
and goal where are the functions of six arguments, and they execute one of the
six arguments, depending on the states (Fig. 4) of the box and the goal area as
seen by the robot’s eyes. The function if box ahead which has two arguments
executes the ﬁrst argument if the box is positioned at “box ahead” position in
Fig. 4. We arranged conditions so that only the box where or the goal where
node becomes the head node of a gene of GP. The gene of GP is set to start
executing from the head node and the execution is repeated again from the head
node if the execution runs over the last leaf node until reaches maximum steps.
A trial starts with the state in which the robot and the box are randomly
placed at the initial positions, and ends when the box is placed in the goal area
or after a predetermined number of actions are performed by the robot. The
following ﬁtness values are allocated to the actions performed in a trial.
– If the task is completed:
fgoal = 100
fremaining moves = 10 ×

0.5 −
( Number of moves )
( Maximum limit of number of moves )

fremaining turns = 10 ×

0.5 −
( Number of turns )
( Maximum limit of number of turns )

– If the box is moved at least once: fmove = 10
– If the robot faces the box at least once: fsee box = 1
– If the robot faces the goal at least once: fsee goal = 1
– flost = −( Number of times having lost sight of the box )
( Number of steps )
The sum of the above ﬁgures indicates a ﬁtness value for the i-th trial in an
evaluation, or fitnessi.
To make robot acquire robust actions that do not depend on the initial
position, the average values of 100 trials in which the initial position is randomly
changed was taken when calculating the ﬁtness of individuals. The ﬁtness of
individuals is calculated by the following equation.
fitness =
1
100
99

i=0
fitnessi + 2.0 · (Maximum gene length) −(Gene length)
(Maximum gene length)
(1)
The second term of the right side of this equation has the meaning that a penalty
is given to a longer gene length.
Using the ﬁtness function determined above, learning was executed for 1,000
individuals of 50 generations with maximum gene length = 150. Learning costs
about 10 minutes on the Linux system equipped with an Athlon XP 1800+.
We ﬁnally applied the individuals that had proven to have the best perfor-
mance to learning with a real robot.

476
S. Kamio, H. Mitsuhashi, and H. Iba
Table 1. Action nodes and their selectable real actions.
action node
real actions which Q-table can select.
move forward “move forward”∗, “retreat + turn left”, “retreat + turn right”
turn left
“turn left”∗, “retreat + turn left”, “retreat”
turn right
“turn right”∗, “retreat + turn right”, “retreat”
∗The action which Q-table prefers to select with a biased initial value.
3.3
Integration of GP and RL
Q-learning is executed to adapt actions acquired via GP to the operating charac-
teristics of a real robot. This is aimed at revising the move forward, turn left
and turn right actions with the simulator to their optimal actions in a real
world.
We allocated a Q-table, on which Q-values were listed, to each of the
move forward, turn left and turn right action nodes. The states on the Q-
tables are regarded as those for a real robot. Therefore, actual actions selected
with Q-tables can vary depending on the state, even if the same action nodes
are executed by a real robot. Figure 5 illustrates the above situation. The states
“near straight left” and “near straight right”, which exist only in a real robot,
are translated into a “center” state in function nodes of GP.
Each Q-table is arranged to set the limits of selectable actions. This refers to
the idea that, for example, “turn right” actions are not necessary to learn in the
turn left node. In this study, we deﬁned three selectable robot actions for each
action node as Table 1. With this technique, each Q-table was initialized with a
biased initial value3. The initial value of 0.0001 was entered into the respective
Q-tables so that preferred actions were selected for each Q-table, while 0.0 was
entered for other actions. The actions which are preferred to select on each action
node are described in Table 1.
turn_left
move_forward
turn_left
turn_left
move_forward
turn_right
s
a
turn_right
turn_right
s
s
box_where
Fig. 5. Action nodes pick up a real action according to the Q-value of a real robot’s
state.
The total size of the three Q-tables is 1.5 times that of ordinary Q-learning.
Theoretically, convergence with the optimal solution is considered to require
3 According to the theory, we can initialize Q-values with arbitrary values, and Q-
values converge with the optimum solution regardless of the initial value [2].

Integration of Genetic Programming and Reinforcement Learning
477
more time than ordinary Q-learning. However, the performance of this technique
while programs are executed is relatively good. This is because all the states on
the Q-table are not necessarily used as the robot performs actions according to
the programs obtained via GP and the task-based actions are available after the
Q-learning starts.
The “state-action deviation” problem should be taken into account when
executing Q-learning with the state constructed from a visual image [4]. This
is the problem that optimal actions cannot be achieved due to the dispersion
of state transitions because the state composed only of the images remains the
same without clearly distinguishing diﬀerences in image values. To avoid this
problem, we redeﬁned “changes” in states. The redeﬁnition is that the current
state is unchanged if the terminal node executed in the program remains the
same and so does the executing state of a real robot4. Until the current state
changes, the Q-value is not updated and the same action is repeated.
As for parameters for Q-learning, the reward was set at 1.0 when the goal
is achieved and 0.0 for other states. We set the parameters as the learning rate
α = 0.3 and the discount factor γ = 0.9 .
4
Experimental Results with AIBO
Just after starting learning: The robot succeeded in completing the task when
Q-learning with a real robot started using this technique. This was because the
robot could perform actions by taking advantage of the results learned via GP.
At the situation in which the box was placed near the center of the robot along
with robot movements, the robot always achieved the task with regard to all the
states tried. Whereas, if the box was not placed near the center of the robot
after its displacement (e.g. if the box was slightly outside the legs), the robot
sometimes failed to move the box properly. The robot repeatedly turned right
to face the box, but continued vain movements going around the box because it
did not have a small turning circle, unlike the actions in the simulator. Figure
6(a) shows typical series of actions. In some situation, the robot turned right
but could not face the box and lost it in view (at the last of Fig. 6(a)).
This typical example proves that optimal actions with the simulator are not
always optimal in a real environment. This is because of diﬀerences between the
simulator and the real robot.
After ten hours (after about 4000 steps): We observed optimal actions as Fig.
6(b). The robot selected “retreat” or “retreat + turn” action in the situations in
which it could not complete the task at the beginning of Q-learning. As a result,
the robot could face the box and pushed the box forward to the goal, and ﬁnally
completed the task.
Learning eﬀects were found in other point, too. As the robot approached the
box smoothly, the number of occurrence of “lost” was reduced. This means the
robot acts more eﬃciently than the beginning of learning.
4 We modiﬁed Asada et al.’s deﬁnition [4] in order to deal with several Q-tables.

478
S. Kamio, H. Mitsuhashi, and H. Iba
(a) Failed actions losing the box at the
beginning of learning.
(b) Successful actions after 10-hour
learning.
Fig. 6. Typical series of actions.
5
Discussion
5.1
Comparison with Q-Learning in Both Simulator and Real
Robot
We compared our proposed technique with the method of Q-learning which
learns in a simulator and re-learns in a real world (we call this method as RL+RL
in this section). For Q-learning in the simulator, we introduced the qualitative
distance (“far”, “middle”, and “near”) so that the state space could be similar
to the one for the real robot5.
For this comparison, we selected ten situations which are diﬃcult to complete
at the beginning of Q-learning because of the gap between the simulation and
5 This simulator has 12 states for each of the box and the goal area; hence, this
environment has 144 states.

Integration of Genetic Programming and Reinforcement Learning
479
Table 2. Comparison of proposed technique (GP+RL) with Q-learning (RL+RL).
#. situation
GP+RL
RL+RL
avg. steps lost box lost goal avg. steps lost box lost goal
1
19.6
0
1
20.0
0
1
2
14.7
0
0
53.0
2
2
3
24.0
0
1
26.7
0
1
4
10.3
0
0
11.0
0
0
5
21.6
0
0
88.0
3
3
6
13.5
0
0
10.5
0
0
7
26.7
0
1
26.0
0
1
8
23.0
0
1
13.0
0
0
9
21.5
0
0
10.5
0
0
10
13.5
0
0
29.0
0
1
the real robot. We measured action eﬃciency after ten-hour Q-learning for these
ten situations. These tests are executed in a greedy policy in order that the robot
always selects the best action in each state.
Table 2 shows the result of both methods, i.e., proposed technique (GP+RL)
and Q-learning method (RL+RL). This table represents the average number of
steps to complete the task and the number of occurrences when the robot has
lost the box or the goal area in completing the task.
While RL+RL performed better than the proposed technique in four situa-
tions on the average of the steps, the proposed technique performed much better
than RL+RL in other six situations (bold font in Table 2). Moreover, the robot
evolved by the proposed technique less often lost the box and the goal area than
that by RL+RL. This result proves that our proposed technique learned more
eﬃcient actions than RL+RL method.
Figure 7 shows the changes in Q-values when they are updated in Q-learning
with the real robot. The absolute value of the Q-value change represents how
far the Q-value is from the optimal one. According to Fig. 7, large changes
occurred to RL+RL method more frequently than to our technique. This may
be because RL+RL has to re-learn optimal Q-values starting from the ones which
have already been learned with the simulator. Therefore, we can conclude that
RL+RL requires more time to converge to optimal Q-values.
5.2
Related Works
There are many studies combined evolutionary algorithms and RL [9,10]. Al-
though the approaches diﬀer from our proposed technique, we see several stud-
ies in which GP and RL are combined [7,8]. With these traditional techniques,
Q-learning is adopted as a RL, and individuals of GP represents the structure of
the state space to be searched. It is reported that searching eﬃciency is improved
in QGP, compared to traditional Q-learning [7].
However, the techniques used in these studies are also a kind of population
learning using numerous individuals. RL must be executed for numerous individ-
uals in the population because RL is inside the GP loop, as shown in Fig. 2(b).
A huge amount of time would become necessary for learning if all the processes

480
S. Kamio, H. Mitsuhashi, and H. Iba
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
3000
3200
3400
3600
3800
4000
The changes in Q-values
Steps
GP+RL
(a) Proposed technique (GP+RL).
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
3000
3200
3400
3600
3800
4000
The changes in Q-value
Steps
RL+RL
(b) Q-learning (RL+RL).
Fig. 7. Comparison of changes in Q-values after about 8-hour to 10-hour Q-learning
with a real robot.
are directly applied to a real robot. As a result, no studies using any of these
techniques with a real robot have been reported.
Several studies on RL pursue the use of hierarchical state space to enable
us to deal with complicated tasks [11,12]. The hierarchical state spaces in such
studies is structured manually in advance. It is generally considered diﬃcult to
automatically build the hierarchical structure only through RL. We can consider
the programs automatically generated in GP of proposed technique represents
the hierarchical structure of state space which is manually structured in [12].
Noises in simulators are often eﬀective to overcome the diﬀerences between a
simulator and real environment [13]. However, the robot learned with our tech-
nique showed suﬃcient performance in noisy real environment, while it learned
in ideal simulator. One of the reasons is that the coarse state division absorbs
the image processing noise. We plan to perform a comparison the robustness
produced by our technique with that by noisy simulators.
5.3
Future Researches
We used only several discrete actions in this study. Although this is simple, con-
tinuous actions are more realistic in applications. In that situation, for example,
“turn left in 30.0 degrees” in the beginning of RL can be changed to “turn left
in 31.5 degrees” after learning, depending on the operating characteristics of the
robot. We plan to conduct an experiment with such continuous actions.
We intend to apply the technique to more complicated tasks such as the
multi-agent problem and other real-robot learning. Based on our method, it can
be possible to use almost the same simulator and settings of RL as described in
this paper. Experiments will be conducted with various robots, e.g., a humanoid
robot “HOAP-1” (manufactured by Fujitsu Automation Limited) or “Khepera”.
The preliminary results were reported in [14]. We are in pursuit of the applica-
bility of the proposed approach to this wide research area.

Integration of Genetic Programming and Reinforcement Learning
481
6
Conclusion
In this paper, we proposed a technique for executing real-time learning with a
real robot based on an integration of GP and RL techniques, and veriﬁed its
eﬀectiveness experimentally.
At the initial stage of Q-learning, we sometimes observed unsuccessful dis-
placements of the box due to a lack of data concerning real robot characteristics,
which had not been reproduced by a simulator. The technique, however, was
adapted to the operating characteristics of the real robot through the ten hour
learning period. This proves that the step of individual learning in this technique
performed eﬀectively in our experiment.
This technique, however, still has several points to be improved. One is feed-
ing back data from learning in a real environment to GP and the simulator, which
corresponds to the loop represented by the dotted line in Fig.2(a). This may en-
able us to improve simulator precision automatically in learning. Its realization
is one of the future issues.
References
1. John R. Koza:
Genetic Programming, On the Programming of Computers by
means of Natural Selection. MIT Press (1992)
2. Richard S. Sutton and Andrew G. Barto: Reinforcement Learning: An introduc-
tion. MIT Press in Cambridge, MA (1998)
3. Hajime Kimura, Toru Yamashita and Shigenobu Kobayashi: Reinforcement Learn-
ing of Walking Behavior for a Four-Legged Robot. In: 40th IEEE Conference on
Decision and Control. (2001)
4. Minoru Asada, Shoichi Noda, Sukoya Tawaratsumida and Koh Hosoda: Purposive
Behavior Acquisition for a Real Robot by Vision-Based Reinforcement Learning.
Machine Learning 23 (1996) 279–303
5. Yasutake Takahashi, Minoru Asada, Shoichi Noda and Koh Hosoda: Sensor Space
Segmentation for Mobile Robot Learning. In: Proceedings of ICMAS’96 Workshop
on Learning, Interaction and Organizations in Multiagent Environment. (1996)
6. OPEN-R Programming Special Interest Group: Introduction to OPEN-R program-
ming (in Japanese). Impress corporation (2002)
7. Hitoshi Iba: Multi-Agent Reinforcement Learning with Genetic Programming. In:
Proc. of the Third Annual Genetic Programming Conference. (1998)
8. Keith L. Downing: Adaptive genetic programs via reinforcement learning. In: Proc.
of the Third Annual Genetic Programming Conference. (1998)
9. Moriarty, D.E., Schultz, A.C., Grefenstette, J.J.: Evolutionary algorithms for rein-
forcement learning. Journal of Artiﬁcial Intelligence Research 11 (1999) 199–229
10. Dorigo, M., Colombetti, M.: Robot Shaping: An Experiment in Behavior Engi-
neering. MIT Press (1998)
11. L.P. Kaelbling: Hierarchical Learning in Stochastic Domains: preliminary Results.
In: Proc. 10th Int. Conf. on Machine Learning. (1993) 167–173
12. T.G. Dietterich: Hierarchical reinforcement learning with the MAXQ value func-
tion decomposition. Journal of Artiﬁcial Intelligence Research 13:227 303 (2000)

482
S. Kamio, H. Mitsuhashi, and H. Iba
13. Schultz, A.C., Ramsey, C.L., Grefenstette, J.J.: Simulation-assisted learning by
competition: Eﬀects of noise diﬀerences between training model and target envi-
ronment. In: Proc. of Seventh International Conference on Machine Learning, San
Mateo, Morgan Kaufmann (1990) 211–215
14. Kohsuke Yanai and Hitoshi Iba: Multi-agent Robot Learning by Means of Ge-
netic Programming: Solving an Escape Problem. In Liu, Y., et al., eds.: Evolvable
Systems: From Biology to Hardware. Proceedings of the 4th International Confer-
ence on Evolvable Systems, ICES’2001, Tokyo, October 3-5, 2001, Springer-Verlag,
Berlin, Heidelberg (2001) 192–203

Multi-objectivity as a Tool for Constructing
Hierarchical Complexity
Jason Teo, Minh Ha Nguyen, and Hussein A. Abbass
Artiﬁcial Life and Adaptive Robotics (A.L.A.R.) Lab,
School of Computer Science, University of New South Wales,
Australian Defence Force Academy Campus, Canberra, Australia.
{j.teo,m.nguyen,h.abbass}@adfa.edu.au
Abstract. This paper presents a novel perspective to the use of multi-
objective optimization and in particular evolutionary multi-objective op-
timization (EMO) as a measure of complexity. We show that the partial
order feature that is being inherited in the Pareto concept exhibits char-
acteristics which are suitable for studying and measuring the complexi-
ties of embodied organisms. We also show that multi-objectivity provides
a suitable methodology for investigating complexity in artiﬁcially evolved
creatures. Moreover, we present a ﬁrst attempt at quantifying the mor-
phological complexity of quadruped and hexapod robots as well as their
locomotion behaviors.
1
Introduction
The study of complex systems has attracted much interest over the last decade
and a half. However, the deﬁnition of what makes a system complex is still the
subject of much debate among researchers [7,19]. There are numerous methods
available in the literature for measuring complexity. However, it has been ar-
gued that complexity measures are typically too diﬃcult to compute to be of
use for any practical purpose or intent [16]. What we are proposing in this paper
is a simple and highly accessible methodology for characterizing the complex-
ity of artiﬁcially evolved creatures using a multi-objective methodology. This
work poses evolutionary multi-objective optimization (EMO) [5] as a convenient
platform which researchers can utilize practically in attempting to deﬁne, mea-
sure or simply characterize the complexity of everyday problems in a useful and
purposeful manner.
2
Embodied Cognition and Organisms
The view of intelligence in traditional AI and cognitive science has been that of
an agent undertaking some form of information processing within an abstracted
representation of the world. This form of understanding intelligence was found to
be ﬂawed in that the agent’s cognitive abilities were derived purely from a pro-
cessing unit that manipulates symbols and representations far abstracted from
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 483–494, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

484
J. Teo, M.H. Nguyen, and H.A. Abbass
the agent’s real environment [3]. Conversely, the embodied cognitive view consid-
ers intelligence as a phenomenon that emerges independently from the parallel
and dynamical interactions between an embodied organism and its environment
[14]. Such artiﬁcial creatures possess two important qualities: embodiment and
situatedness.
A subﬁeld of research into embodied cognition involves the use of artiﬁcial
evolution for automatically generating the morphology and mind of embodied
creatures [18]. The term mind as used in this context of research is synonymous
with brain and controller - it merely reﬂects the processing unit that acts to
transform the sensory inputs into the motor outputs of the artiﬁcial creature. The
automatic synthesis of such embodied and situated creatures through artiﬁcial
evolution has become a key area of research not only in the cognitive sciences
but also in robotics [15], artiﬁcial life [14], and evolutionary computation [2,10].
Consequently, there has been much research interest in evolving both physi-
cally-simulated virtual organisms [2,10,14] and real physical robots [15,8,12].
The main objective of these studies is to evolve increasingly complex behaviors
and/or morphologies either through evolutionary or lifetime learning. Needless
to say, the term “complex” is generally used very loosely since there is currently
no general method for comparing between the complexities of these evolved ar-
tiﬁcial creatures’ behaviors and morphologies. As such, without a quantitative
measure for behavioral or morphological complexity, an objective evaluation be-
tween these artiﬁcial evolutionary systems becomes very hard and typically ends
up being some sort of subjective argument.
There are generally two widely-accepted views of measuring complexity. The
ﬁrst is an information-theoretic approach based on Shannon’s entropy [17] and is
commonly referred to as statistical complexity. The entropy H(X) of a random
variable X, where the outcomes xi occur with probability pi, is given by
H(X) = −C
N

i
pi log pi
(1)
where C is the constant related to the base chosen to express the logarithm.
Entropy is a measure of disorder present in a system and thus gives us an in-
dication of how much we do not know about a particular system’s structure.
Shannon’s entropy measures the amount of information content present within
a given message or more generally any system of interest. Thus a more complex
system would be expected to give a much higher information content than a less
complex system. In other words, a more complex system would require more bits
to describe compared to a less complex system. In this context, a sequence of
random numbers will lead to the highest entropy and consequently to the lowest
information content. In this sense, complexity is somehow a measure of order or
disorder.
A computation-theoretic approach to measuring complexity is based on Kol-
mogorov’s application of universal Turing machines [11] and is commonly known
as Kolmogorov complexity. It is concerned with ﬁnding the shortest possible com-
puter program or any abstract automaton that is capable of reproducing a given
string. The Kolmogorov complexity K(s) of a string s is given by

Multi-objectivity as a Tool for Constructing Hierarchical Complexity
485
K(s) = min{|p| : s = CT (p)}
(2)
where |p| represents the length of program p and CT (p) represents the result of
running program p on Turing machine T. A more complex string would thus
require a longer program while a simpler string would require a much shorter
program. In essence, the complexity of a particular system is measured by the
amount of computation required to recreate the system in question.
3
Complexity in the Eyes of the Beholder
None of the previous measures are suﬃcient to measure the complexity of em-
bodied systems. As such, we need ﬁrst to provide a critical view of these measures
and why they stand shorthanded in terms of embodied systems.
Take for example a simple behavior such as walking. Let us assume that we
are interested in measuring the complexity of walking in diﬀerent environments
and the walking itself is undertaken by an artiﬁcial neural network. From Shan-
non’s perspective, the complexity can be measured using the entropy of the data
structure holding the neural network. Obviously a drawback for this view is its
ignorance of the context and the concepts of embodiment and situatedness. The
complexity of walking on a ﬂat landscape is entirely diﬀerent from walking on
a rough landscape. Two neural networks may be represented using the same
number of bits but exhibit entirely diﬀerent behaviors.
Now, let us take another example which will show the limitations of Kol-
mogorov complexity. Assume we have a sequence of random numbers. Obviously
the shortest program which is able to reproduce this sequence is the sequence
itself. In other words, a known drawback for Kolmogorov complexity is that it
has the highest level of complexity when the system is random. In addition, let
us re-visit the neural network example. Assume that the robot is not using a
ﬁxed neural network but some form of evolvable hardware (which may be an
evolutionary neural network). If the ﬁtness landscape for the problem at hand
is monotonically increasing, a hill climber will simply be the shortest program
which guarantees to re-produce the behavior. However, if the landscape is rugged,
reproducing the behavior is only achievable if we know the seed; otherwise, the
problem will require complete enumeration to recreate the behavior.
In this paper, we propose a generic deﬁnition for complexity using the multi-
objective paradigm. However, before we proceed with our deﬁnition, we ﬁrst
remind the reader of the concept of partial order.
Deﬁnition 1: Partial and Lexicographic Order Assume the two sets A and
B. Assume the l-subsets over A and B such that A = {a1 < . . . < al} and
B = {b1 < . . . < bl}.
A partial order is deﬁned as A ≤j B if aj ≤bj, ∀j ∈{1, . . . , l}
A lexicographic order is deﬁned as A <j B if ∃ak < bk and aj = bj, j < k,
∀j, k ∈{1, . . . , l}
In other words, a lexicographic order is a total order. In multi-objective opti-
mization, the concept of Pareto optimality is normally used. A solution x belongs

486
J. Teo, M.H. Nguyen, and H.A. Abbass
to the Pareto set if there is not a solution y in the feasible solution set such that
y dominates x (ie. x has to be at least as good as y when measured on all ob-
jectives and better than y on at least one objective). The Pareto concept thus
forms partial orders in the objective space.
Let us recall the embodied cognition problem. The problem is to study the
relationship between the behavior, controller, environment, learning algorithm,
and morphology. A typical question that one may ask is what is the optimal
behavior for a given morphology, controller, learning algorithm and environment.
We can formally represent the problem of embodied cognition as the ﬁve sets B,
C, E, L, and M for the ﬁve spaces of behavior, controller, environment, learning
algorithm, and morphology respectively. Here, we need to diﬀerentiate between
the robot behavior B and the desired behavior ˆB. The former can be seen as
the actual value of the ﬁtness function and the latter can be seen as the real
maximum of the ﬁtness function. For example, if the desired behavior (task) is to
maximize the locomotion distance, then the global maximum of this function is
the desired behavior, whereas the distance achieved by the robot (what the robot
is actually doing) is the actual behavior. In traditional robotics, the problem can
be seen as Given the desired behavior ˆB, ﬁnd L which optimizes C subject to
E  M. In psychology, the problem can be formulated as Given C, E, L and M,
study the characteristics of the set B. In co-evolving morphology and mind, the
problem is Given the desired behavior ˆB and L, optimize C and M subject to
E. A general observation is that the learning algorithm is usually ﬁxed during
the experiments.
In asking a question such as “Is a human more complex than a Monkey?”,
a natural question that follows would be “in what sense?”. Complexity is not
a unique concept. It is usually deﬁned or measured within some context. For
example, a human can be seen as more complex than a Monkey if we are looking
at the complexity of intelligence, whereas a Monkey can be seen as more complex
than the human if we are looking at the number of diﬀerent gaits the monkey has
for locomotion. Therefore, what is important from an artiﬁcial life perspective
is to establish the complexity hierarchy on diﬀerent scales. Consequently, we
introduce the following deﬁnition for complexity.
Deﬁnition 2: Complexity is a strict partial order relation.
According to this deﬁnition, we can establish an order of complexity between
the system’s components/species. We can then compare the complexities of two
species S1 = (B1, C1, E1, L1, M1) and S2 = (B2, C2, E2, L2, M2) as:
S1 is at least as complex as S2 with respect to concept Ψ iﬀ
SΨ
2 = (B2, C2, E2, L2, M2) ≤j SΨ
1 = (B1, C1, E1, L1, M1), ∀j ∈{1, . . . , l}, Given
Bi = {Bi1 < . . . < Bil}, Ci = {Ci1 < . . . < Cil}, Ei = {Ei1 < . . . < Eil},
Li = {Li1 < . . . < Lil}, Mi = {Mi1 < . . . < Mil}, i ∈{1, 2}
(3)
where Ψ partitions the sets into l non-overlapping subsets.

Multi-objectivity as a Tool for Constructing Hierarchical Complexity
487
We can even establish a complete order of complexity by using the lexico-
graphic order as:
S1 is more complex thanS2 with respect to concept Ψ iﬀ
SΨ
2 = (B2, C2, E2, L2, M2) <j SΨ
1 = (B1, C1, E1, L1, M1), ∀j ∈{1, . . . , l}, Given
Bi = {Bi1 < . . . < Bil}, Ci = {Ci1 < . . . < Cil}, Ei = {Ei1 < . . . < Eil},
Li = {Li1 < . . . < Lil}, Mi = {Mi1 < . . . < Mil}, i ∈{1, 2}
(4)
The lexicographic order is not as ﬂexible as partial order since the former re-
quires a monotonic increase in complexity. The latter however, allows individuals
to have similar levels of complexity; therefore, it is more suitable for deﬁning hier-
archies of complexity. Some of the characteristics in our deﬁnition of complexity
here include
Irreﬂexive: The complexity deﬁnition satisﬁes irreﬂexivity; that is, x cannot
be more complex than itself.
Asymmetric: The complexity deﬁnition satisﬁes asymmetry; that is, if x is
more complex than y, then y cannot be more complex than x.
Transitive: The complexity deﬁnition satisﬁes transitivity; that is, if x is more
complex than y and y is more complex than z, then x is more complex than
z.
The concept of Pareto optimality is similar to the concept of partial order
except that Pareto optimality is more strict in the sense that it does not satisfy
reﬂexivity; that is, a solution cannot dominate itself; therefore it cannot exist
as a Pareto optimal if there is a copy of it in the solution set. Usually, when we
have copies of one solution, we take one of them; therefore this problem does
not arise. As a result, we can assume here that Pareto optimality imposes a
complexity hierarchy on the solution set.
The previous deﬁnition will simply order the sets based on their complexities
according to some concept Ψ. However, they do not provide an exact quantitative
measure for complexity. In the simple case, given the ﬁve sets B, C, E, L, and M;
assume the function f, which maps each element in each set to some value called
the ﬁtness, and assuming that C, E and L do not change, a simple measure of
morphological change of complexity can be
∂f(b)
∂m , b ∈B, m ∈M
(5)
In other words, assuming that the environment, controller, and the learning
algorithm are ﬁxed, the change in morphological complexity can be measured in
the eyes of the change in the ﬁtness of the robot (actual behavior). The ﬁtness
will be deﬁned later in the paper. Therefore, we introduce the following deﬁnition
Deﬁnition 3: Change of Complexity Value for the morphology is the rate
of change in behavioral ﬁtness when the morphology changes, given that
both the environment, learning algorithm and controller are ﬁxed.

488
J. Teo, M.H. Nguyen, and H.A. Abbass
The previous deﬁnition can be generalized to cover the controller and envi-
ronment quite easily by simply replacing “morphology” by either “environment”,
“learning algorithm”, or “controller”. Based on this deﬁnition, if we can come
up with a good measure for behavioral complexity, we can use this measure to
quantify the change in complexity for morphology, controller, learning algorithm,
or environment. In the same manner, if we have a complexity measure for the
controller, we can use it to quantify the change of complexity in the other four
parameters. Therefore, we propose the notion of deﬁning the complexity of one
object as viewed from the perspective of another object. This is not unlike Em-
meche’s idea of complexity as put in the eyes of the beholder [6]. However, we
formalize and solidify this idea by putting it into practical and quantitative us-
age through the multi-objective approach. We will demonstrate that results from
an EMO run of two conﬂicting objectives results in a Pareto-front that allows a
comparison of the diﬀerent aspects of an artiﬁcial creature’s complexity.
In the literature, there are a number of related topics which can help here.
For example, the VC-dimension can be used as a complexity measure for the
controller. A feed-forward neural network using a threshold activation function
has a VC dimension of O(WlogW) while a similar network with a sigmoid acti-
vation has a VC dimension of O(W 2), where W is the number of free parameters
in the network [9]. It is apparent from here that one can control the complexity
of a network by minimizing the number of free parameters which can be done
either by the minimization of the number of synapses or the number of hidden
units. It is important to separate between the learning algorithm and the model
itself. For example, two identical neural networks with ﬁxed architectures may
perform diﬀerently if one of them is trained using back-propagation while the
other is trained using an evolutionary algorithm. In this case, the separation
between the model and the algorithm helps us to isolate their individual eﬀects
and gain an understanding of their individual roles.
In this paper, we are essentially posing two questions, what is the change
of (1) behavioral complexity and (2) morphological complexity of the artiﬁcial
creature in the eyes of its controller. In other words, how complex is the behavior
and morphology in terms of evolving a successful controller?
3.1
Assumptions
Two assumptions need to be made. First, the Pareto set obtained from evolution
is considered to be the actual Pareto set. This means that for the creature
on the Pareto set, the maximum amount of locomotion is achieved with the
minimum number of hidden units in the ANN. We do note however that the
evolved Pareto set in the experiments may not have converged to the optimal
set. Nevertheless, it is not the objective of this paper to provide a method which
guarantees convergence of EMO but rather to introduce and demonstrate the
application of measuring complexity in the eyes of the beholder. It is important
to mention that although this assumption may not hold, the results can still be
valid. This will be the case when creatures are not on the actual Pareto-front

Multi-objectivity as a Tool for Constructing Hierarchical Complexity
489
but the distances between them on the intermediate Pareto-front are similar to
that of creatures on the actual Pareto-front.
The second assumption is there are no redundancies present in the ANN
architectures of the evolved Pareto set. This simply means that all the input
and output units as well as the synaptic connections between layers of the net-
work are actually involved in and required for achieving the observed locomotion
competency. We have investigated the amount of redundancy present in evolved
ANN controllers and found that the self-adaptive Pareto EMO approach pro-
duces networks with practically zero redundancy.
4
Methods
4.1
The Virtual Robots and Simulation Environment
The Vortex physics simulation toolkit [4] was utilized to accurately simulate
the physical properties, such as forces, torques, inertia, friction, restitution and
damping, of and interactions between the robot and its environment. Two arti-
ﬁcial creatures (Figure 1) were used in this study.
Fig. 1. The four-legged (quadruped) and six-legged (hexapod) creatures.
The ﬁrst artiﬁcial creature is a quadruped with 4 short legs. Each leg consists
of an upper limb connected to a lower limb via a hinge (1 degree-of-freedom
(DOF)) joint and is in turn connected to the torso via another hinge joint. Each
of the hinge joints is actuated by a motor that generates a torque producing
rotation of the connected body parts about that hinge joint.
The second artiﬁcial creature is a hexapod with 6 long legs, which are con-
nected to the torso by insect hip joints. Each insect hip joint consists of two
hinges, making it a 2 DOF joint: one to control the back-and-forth swinging and
another for the lifting of the leg. Each leg has an upper limb connected to a
lower limb by a hinge (1 DOF) joint. The hinges are actuated by motors in the
same fashion as in the ﬁrst artiﬁcial creature.
The Pareto-frontier of our evolutionary runs are obtained from optimizing
two conﬂicting objectives: (1) minimizing the number of hidden units used in

490
J. Teo, M.H. Nguyen, and H.A. Abbass
the ANN that act as the creature’s controller and (2) maximizing horizontal
locomotion distance of the artiﬁcial creature. What we obtain at the end of the
runs are Pareto sets of ANNs that trade-oﬀbetween number of hidden units and
locomotion distance. The locomotion distances achieved by the diﬀerent Pareto
solutions will provide a common ground where locomotion competency can be
used to compare diﬀerent behaviors and morphologies. It will provide a set of
ANNs with the smallest hidden layer capable of achieving a variety of locomotion
competencies. The structural deﬁnition of the evolved ANNs can now be used as
a measure of complexity for the diﬀerent creature behaviors and morphologies.
The ANN architecture used in this study is a fully-connected feed-forward
network with recurrent connections on the hidden units as well as direct input-
output connections. Recurrent connections were included to allow the creature’s
controller to learn time-dependent dynamics of the system. Direct input-output
connections were also included in the controller’s architecture to allow for direct
sensor-motor mappings to evolve that do not require hidden layer transforma-
tions. Bias is incorporated in the calculation of the activation of the hidden as
well as output layers.
The Self-adaptive Pareto-frontier Diﬀerential Evolution algorithm (SPDE)
[1] was used to drive the evolutionary optimization process. SPDE is an elitist
approach to EMO where both crossover and mutation rates are self-adapted.
Our chromosome is a class that contains one matrix Ωand one vector ρ. The
matrix Ωis of dimension (I + H) × (H + O). Each element ωij ∈Ω, is the
weight connecting unit i with unit j, where i = 0, . . . , (I −1) is the input unit i,
i = I, . . . , (I + H −1) is the hidden unit (i −I), j = 0, . . . , (H −1) is the hidden
unit j, and j = H, . . . , (H + O −1) is the output unit (j −H).
The vector ρ is of dimension H, where ρh ∈ρ is a binary value used to indicate
if hidden unit h exists in the network or not; that is, it works as a switch to turn a
hidden unit on or oﬀ. Thus, the architecture of the ANN is variable in the hidden
layer: any number of hidden units from 0 to H is permitted. The sum, H
h=0 ρh,
represents the actual number of hidden units in a network, where H is the
maximum number of hidden units. The last two elements in the chromosome are
the crossover rate δ and mutation rate η. This representation allows simultaneous
training of the weights in the network and selecting a subset of hidden units as
well as allowing for the self-adaptation of crossover and mutation rates during
optimization.
4.2
Experimental Setup
Two series of experiments were conducted. Behavioral complexity was investi-
gated in the ﬁrst series of experiments and morphological complexity was in-
vestigated in the second. For both series of experiments, each evolutionary run
was allowed to evolve over 1000 generations with a randomly initialized popula-
tion size of 30. The maximum number of hidden units was ﬁxed at 15 based on
preliminary experimentation. The number of hidden units used and maximum
locomotion achieved for each genotype evaluated as well as the Pareto set of

Multi-objectivity as a Tool for Constructing Hierarchical Complexity
491
solutions obtained in every generation were recorded. The Pareto solutions ob-
tained at the completion of the evolutionary process were compared to obtain a
characterization of the behavioral and morphological complexity.
To investigate behavioral complexity in the eyes of the controller, the mor-
phology was ﬁxed by using only the quadruped creature but the desired behavior
was varied by having two diﬀerent ﬁtness functions. The ﬁrst ﬁtness function
measured only the maximum horizontal locomotion achieved but the second ﬁt-
ness function measured both maximum horizontal locomotion and static stability
achieved. By static stability, we mean that the creature achieves a statically sta-
ble locomotion gait with at least three of its supporting legs touching the ground
during each step of its movement. The two problems we have are:
(P1)
f1 = d
(6)
f2 =
H

h=0
ρh
(7)
(P2)
f1 = d/20 + s/500
(8)
f2 =
H

h=0
ρh
(9)
where P1 and P2 are the two sets of objectives used. d refers to the locomotion
distance achieved and s is the number of times the creature is statically stable
as controlled by the ANN at the end of the evaluation period of 500 timesteps.
P1 is using the locomotion distance as the ﬁrst objective while P2 is using a
linear combination of the locomotion distance and static stability. Minimizing
the number of hidden units is the second objective in both problems.
To investigate morphological complexity, another set of 10 independent runs
was carried out but this time using the hexapod creature. This is to enable
a comparison with the quadruped creature which has a signiﬁcantly diﬀerent
morphology in terms of its basic design. The P1 set of objectives was used to keep
the behavior ﬁxed. The results obtained in this second series of experiments were
then compared against the results obtained from the ﬁrst series of experiments
where the quadruped creature was used with the P1 set of objective functions.
5
Results and Discussion
5.1
Morphological Complexity
We ﬁrst present the results for the quadruped and hexapod evolved under P1.
Figure 2 compares the Pareto optimal solutions obtained for the two diﬀerent
morphologies over 10 runs. Here we are ﬁxing E and L; therefore, we can either
measure the change of morphological complexity in the eyes of the behavior or
the controller; that is, δf(B)
δM
or δf(C)
δM
respectively. If we ﬁx the actual behav-
ior B as the locomotion competency of achieving a movement of 13 < d < 15,

492
J. Teo, M.H. Nguyen, and H.A. Abbass
0
5
10
15
0
2
4
6
8
10
12
14
16
18
20
No. of hidden units
Locomotion distance
Pareto−front for Quadruped
0
5
10
15
0
2
4
6
8
10
12
14
16
18
20
No. of hidden units
Locomotion distance
Pareto−front for Hexapod
Fig. 2. Pareto-frontier of controllers obtained from 10 runs using the quadruped and
hexapod with the P1 set of objectives.
then the change in the controller δf(C) is measured according to the number
of hidden units used in the ANN. At this point of comparison, we ﬁnd that the
quadruped is able to achieve the desired behavior with 0 hidden units whereas
the hexapod required 3 hidden units. In terms of the ANN architecture, the
quadruped achieved the required level of locomotion competency without using
the hidden layer at all, that it relied solely on direct input-output connections
as in a perceptron. This phenomenon has been previously observed to occur in
wheeled robots as well [13]. Therefore, this is an indication that from the con-
troller’s point of view, given the change in morphology δM from the quadruped
to the hexapod, there was an increase in complexity for the controller δC from 0
hidden units to 3 hidden units. Hence, the hexapod morphology can be seen as
being placed at a higher level of the complexity hierarchy than the quadruped
morphology in the eyes of the controller.
If we would like to measure the complexity of the morphology using the be-
havioral scale, we can notice from the graph that the maximum distance achieved
by the quadruped creature is around 17.8 compared to around 13.8 for the hexa-
pod creature. In this case, the quadruped can be seen as being able to achieve a
more complex behavior than the hexapod.
5.2
Behavioral Complexity
A comparison of the results obtained using the two diﬀerent sets of ﬁtness func-
tions P1 and P2 is presented in Table 1. Here we are ﬁxing M, L and E and
looking for the change in behavioral complexity. The morphology M is ﬁxed by
using the quadruped creature only. For P1, we can see that the Pareto-frontier
oﬀers a number of diﬀerent behaviors. For example, a network with no hidden
units can achieve up to 14.7 units of distance while the creature driven by a
network with 5 hidden units can achieve 17.7 units of distance within the 500

Multi-objectivity as a Tool for Constructing Hierarchical Complexity
493
Table 1. Comparison of global Pareto optimal controllers evolved for the quadruped
using the P1 and P2 objective functions.
Type of
Pareto
No. of
Locomotion
Static
Behavior
Controller
Hidden Units
Distance
Stability
P1
1
0
14.7
19
2
1
15.8
24
3
2
16.2
30
4
3
17.1
26
5
4
17.7
14
P2
1
0
5.2
304
2
1
3.3
408
3
2
3.6
420
4
3
3.7
419
timesteps. This is an indication that to achieve a higher speed gait entails a more
complex behavior than a lower speed gait.
We can also see the eﬀect of static stability, which requires a walking behavior.
By comparing a running behavior using a dynamic gait in P1 with no hidden
units against a walking behavior using a static gait in P2 with no hidden units,
we can see that using the same number of hidden units, the creature achieves
both a dynamic as well as a quasi-static gait. If more static stability is required,
this will necessitate an increase in controller complexity.
At this point of comparison, we ﬁnd that the behavior achieved with the P1
ﬁtness functions consistently produced a higher locomotion distance than the
behavior achieved with the P2 ﬁtness functions. This meant that it was much
harder for the P2 behavior to achieve the same level of locomotion competency
in terms of distance moved as the P1 behavior due to the added sub-objective
of having to achieve static stability during locomotion. Thus, the complexity of
achieving the P2 behavior can be seen as being at a higher level of the complexity
hierarchy than the P1 ﬁtness function in the eyes of the controller.
6
Conclusion and Future Work
We have shown how EMO can be applied for studying the behavioral and mor-
phological complexities of artiﬁcially evolved embodied creatures. The morpho-
logical complexity of a quadruped creature was found to be lower than the mor-
phological complexity of a hexapod creature as seen from the perspective of
an evolving locomotion controller. At the same time, the quadruped was found
to be more complex than the hexapod in terms of behavioral complexity. For
future work, we intend to provide an empirical proof of measuring not only be-
havioral complexity but also environmental complexity by evolving controllers
for artiﬁcial creatures in varied environments. We also plan to apply these mea-
sures for characterizing the complexities of artiﬁcial creatures evolved through
co-evolution of both morphology and mind.

494
J. Teo, M.H. Nguyen, and H.A. Abbass
References
1. Hussein A. Abbass. The self-adaptive pareto diﬀerential evolution algorithm. In
Proceedings of the 2002 Congress on Evolutionary Computation (CEC2002), vol-
ume 1, pages 831–836. IEEE Press, Piscataway, NJ, 2002.
2. Josh C. Bongard. Evolving modular genetic regulatory networks. In Proceedings
of the 2002 Congress on Evolutionary Computation (CEC2002), pages 1872–1877.
IEEE Press, Piscataway, NJ, 2002.
3. Rodney A. Brooks. Intelligence without reason. In L. Steels and R. Brooks (Eds),
The Artiﬁcial Life Route to Artiﬁcial Intelligence: Building Embodied, Situated
Agents, pages 25–81. Lawrence Erlbaum Assoc. Publishers, Hillsdale, NJ, 1995.
4. Critical Mass Labs. Vortex [online]. http://www.cm-labs.com [cited – 25/1/2002].
5. Kalyanmoy Deb.
Multi-objective Optimization using Evolutionary Algorithms.
John Wiley & Sons, Chicester, UK, 2001.
6. Claus Emmeche. Garden in the Machine. Princeton University Press, Princeton,
NJ, 1994.
7. David P. Feldman and James P. Crutchﬁeld. Measures of statistical complexity:
Why? Physics Letters A, 238:244–252, 1998.
8. Dario Floreano and Joseba Urzelai. Evolutionary robotics: The next generation.
In T. Gomi, editor, Proceedings of Evolutionary Robotics III, pages 231–266. AAI
Books, Ontario, 2000.
9. Simon Haykin. Neural networks – a comprehensive foundation. Prentice Hall, USA,
2 edition, 1999.
10. Gregory S. Hornby and Jordan B. Pollack. Body-brain coevolution using L-systems
as a generative encoding. In L. Spector et al. (Eds), Proceedings of the Genetic and
Evolutionary Computation Conference (GECCO-2001), pages 868–875. Morgan
Kaufmann, San Francisco, 2001.
11. Andrei N. Kolmogorov. Three approaches to the quantitative deﬁnition of infor-
mation. Problems of Information Transmission, 1:1–7, 1965.
12. Hod Lipson and Jordan B. Pollack. Automatic design and manufacture of robotic
lifeforms. Nature, 406:974–978, 2000.
13. Henrik H. Lund and John Hallam. Evolving suﬃcient robot controllers. In Pro-
ceedings of the 4th IEEE International Conference on Evolutionary Computation,
pages 495–499. IEEE Press, Piscataway, NJ, 1997.
14. Rolf Pfeifer and Christian Scheier. Understanding Intelligence. MIT Press, Cam-
bridge, MA, 1999.
15. Jordan B. Pollack, Hod Lipson, Sevan G. Ficici, Pablo Funes, and Gregory S.
Hornby. Evolutionary techniques in physical robotics. In Peter J. Bentley and
David W. Corne (Eds), Creative Evolutionary Systems, chapter 21, pages 511–523.
Morgan Kaufmann Publishers, San Francisco, 2002.
16. Cosma R. Shalizi. Causal Architecture, Complexity and Self-Organization in Time
Series and Cellular Automata. Unpublished PhD thesis, University of Wisconsin
at Madison, Wisconsin, 2001.
17. Claude E. Shannon. A mathematical theory of communication. The Bell System
Technical Journal, 27(3):379–423, 1948.
18. Karl Sims. Evolving 3D morphology and behavior by competition. In R. Brooks
and P. Maes (Eds), Artiﬁcial Life IV: Proceedings of the Fourth International
Workshop on the Synthesis and Simulation of Living Systems, pages 28–39. MIT
Press, Cambridge, MA, 1994.
19. Russell K. Standish. On complexity and emergence [online]. Complexity Interna-
tional, 9, 2001.

Learning Biped Locomotion from First
Principles on a Simulated Humanoid Robot
Using Linear Genetic Programming
Krister Wolﬀand Peter Nordin
Dept. of Physical Resource Theory, Complex Systems Group,
Chalmers University of Technology,
S-412 96 G¨oteborg, Sweden
{wolff, nordin}@fy.chalmers.se
http://www.frt.fy.chalmers.se/cs/index.html
Abstract. We describe the ﬁrst instance of an approach for control
programming of humanoid robots, based on evolution as the main adap-
tation mechanism. In an attempt to overcome some of the diﬃculties
with evolution on real hardware, we use a physically realistic simulation
of the robot. The essential idea in this concept is to evolve control pro-
grams from ﬁrst principles on a simulated robot, transfer the resulting
programs to the real robot and continue to evolve on the robot. The
Genetic Programming system is implemented as a Virtual Register Ma-
chine, with 12 internal work registers and 12 external registers for I/O
operations. The individual representation scheme is a linear genome, and
the selection method is a steady state tournament algorithm. Evolution
created controller programs that made the simulated robot produce for-
ward locomotion behavior. An application of this system with two phases
of evolution could be for robots working in hazardous environments, or
in applications with remote presence robots.
1
Introduction
Dealing with humanoid robots requires supply of expertise in many diﬀerent ar-
eas, such as vision systems, sensor fusion, planning and navigation, mechanical
and electrical hardware design, and software design only to mention a few. The
objective of this paper, however, is focused on the synthesizing of biped gait.
The traditional way of robotics locomotion control is based on derivation of an
internal geometric model of the locomotion mechanism, and requires intensive
calculations by the controlling computer, to be performed in real time. Robots,
designed in such a way that a model can be derived and used for controlling,
shows large aﬃnity with complex, highly specialized industrial robots, and thus
they are as expensive as conventional industrial robots. Our belief is that for
humanoids to become an everyday product in our homes and society, aﬀordable
for everyone, there is needed to develop low cost, relatively simple robots. Such
robots can hardly be controlled the traditional way; hence this is not our pri-
mary design principle.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 495–506, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

496
K. Wolﬀand P. Nordin
A basic condition for humanoids to successfully operate in human living environ-
ment is that they must be able to deal with unpredictable situations and gather
knowledge and information, and adapt to their actual circumstances. For these
reasons, among others, we propose an alternative way for control programming
of humanoid robots. Our approach is based on evolution as the main adaptation
mechanism, utilizing computing techniques from the ﬁeld of Evolutionary Algo-
rithms.
The ﬁrst attempt in using a real, physical robot to evolve gait patterns was
made at the University of Southern California. Neural networks were evolved
as controllers to produce a tripod gait for a hexapod robot with two degrees
of freedom for each leg [6]. Researchers at Sony Corporation have worked with
evolving locomotion controllers for dynamic gait of their quadruped robot dog
AIBO. These results show that evolutionary algorithms can be used on complex,
physical robots to evolve non-trivial behaviors on these robots [3] and [4].
However, evolving eﬃcient gaits with real physical hardware is a challenge, and
evolving biped gait from ﬁrst principles is an even more challenging task. It
is extremely stressing for the hardware and it is very time consuming [17]. To
overcome the diﬃculties with evolving on real hardware, we introduce a method
based on simulation of the actual humanoid robot.
Karl Sims was one of the ﬁrst to evolve locomotion in a simulated physics en-
vironment [13] and [14]. Parker use Cyclic Genetic Algorithms to evolve gait
actuation lists for a simulated six legged robot [11], and Jakobi et al has devel-
oped a methodology for evolution of robot controllers in simulator, and shown
it to be successful when transferred to a real, physical octopod robot [7] and
[9]. This method, however, has not been validated on a biped robot. Recently, a
research group in Germany reported an experiment relevant to our ideas, where
they evolved robot controllers in a physics simulator, and successfully executed
them onboard a real biped robot. They were not able to fully realize biped
locomotion behavior, but their results were deﬁnitely promising [18].
2
Background and Motivation
In this section we summarize an on-line learning experiment performed with
a humanoid robot. However this experiment was fairly successful in evolving
locomotion controller parameters that optimized the robot’s gait, it pointed out
some diﬃculties with on-line learning. We summarize the experiment here in
order to exemplify the diﬃculties of evolving gaits on-line, and let it serve as an
illustrative motivation for the work presented in the remainder of this paper.
2.1
Robot Platform
The robot used in the experiments is a simpliﬁed, scaled model of a full-size
humanoid with body dimensions that mirrors the dimensions of a human. It
was originally developed as an alternative, low-cost humanoid robot platform,

Learning Biped Locomotion from First Principles
497
intended for research [17]. It is a fully autonomous robot with onboard power
supply and computer, and it has 14 degrees of freedom.
The robot has the 32-bit micro-controller EyeBot MK3 [2] onboard, carrying
it as a backpack. All signal processing, including control, vision, and evolutionary
algorithm, is carried out on the EyeBot controller itself. In its present status,
the robot is capable of static walking.
Fig. 1. Image of the real humanoid robot ‘elvina’.
2.2
Gait Control Method
The gait control method for this robot involves repetition of a sequence of in-
tegrated steps. Considering fully realistic bipedal walk, two diﬀerent situations
arise in sequence: the statically stable double-support phase in which the robot
is supported on both feet simultaneously, and statically unstable single-support
phase when only one foot of the robot is in contact with the ground, the other
foot being transferred from the back to front position. When this sequence of
transitions has been repeated twice, one can consider a single gait cycle to be
completed. That is, the locomotion mechanism’s posture and limb’s positions
are the same after the completion as it was before it started to move, and hence
it’s internal state is the same.
If we now study only static walk, i.e. the projection of the center of mass of the
robot on the ground always lie within the support polygon formed by feet on the
ground, there is obviously a number of statically stable postures in between the
internal state of the robot and it’s ﬁnal state, during completion of a single gait
cycle. By interpolation between numbers of such, statically stable, consecutive
states it is possible to make the robot to complete a single gait cycle. Then, by
continually looping, biped gait is produced.

498
K. Wolﬀand P. Nordin
2.3
Evolutionary Gait Optimization Experiment
This experiment was performed in order to optimize a by hand developed set of
state vectors, deﬁning a static robot gait. The evolutionary algorithm used was
a tournament selection, steady state evolutionary strategy [1] and [16] running
on the robot’s onboard computer. Individuals were evaluated and ﬁtness scores
automatically determined using the robots onboard digital camera and proxim-
ity sensor.
A population of 30 individuals, stemming from a manually developed individual
was created with a uniform distribution over a given search range. The best-
evolved individual and the manually developed individual were independently
tested, and their performances were compared to each other’s. The former one
received a ﬁtness score, averaged over three trials, of 0.1707, and the latter one,
tested under equal conditions, got a ﬁtness of 0.1051. Within this context, a
higher ﬁtness value means a better individual, and thus the best-evolved indi-
vidual outperformed the manually developed individual both in its ability to
maintain the robot in a straight course and in robustness, i.e. with a lesser
tendency to fall over [17].
2.4
Observations
To run such an evolutionary experiment as described above span over several
days, and requires manual supervision all this time. Between each generation of
four individuals evaluated, the experiment was paused for about 15 minutes in
order to spare the hardware and especially the actuators. The main reason for
this is that the actuators accumulate heat when they are running continuously
under heavy stress. They then run the risk of getting overheated and gradually
destroyed. One way to handle this problem was, as mentioned above, to run
the robot intermittent so that the servos maintain an approximately constant
temperature.
Evolving eﬃcient gaits with real physical hardware is a challenging task. During
the experiments, the torso and both the ankle actuators were exchanged once as
well as the two hip servos. The most vulnerable parts of the robot were proved
to be the knee servos. Both these servos were replaced tree times.
Obviously there are a number of diﬃculties related with evolving biped walking
behavior on a real, physical robot. In an attempt to overcome some of the prob-
lems, we want to use a physically realistic simulation of the robot. The central
idea in this concept is to evolve control programs from ﬁrst principles on a sim-
ulated robot, transfer the resulting programs to the real robot and continue to
evolve eﬃcient gait on the real robot. Of course, there will arise other problems
applying this method, as simulation systems always imply some simpliﬁcations
of the real world.
3
Evolution of Control Programs
Our primary goal is to utilize Genetic Programming [5] and [8] for evolving
locomotion control programs from ﬁrst principles for our simulated biped robot,

Learning Biped Locomotion from First Principles
499
i.e. with no a priori knowledge for the robot on how to walk, information of
morphology etc. The evolved programs take the robot’s current internal state
parameter values as input vector and return a vector predicting it’s next internal
state parameter values, in order to produce robust biped gait.
3.1
Dynamic Physics Simulation
The Open Dynamics Engine (ODE) is a free library for simulating articulated
rigid body dynamics, developed by Russell Smith [15]. An articulated structure
is created when rigid bodies of various shapes are connected together with joints
of various kinds.
The robot model is qualitatively consistent with the real robot in the aspect of
geometry, mass distribution, and morphology. See [17] for details of the robot.
It consists of 12 actuated joints and 13 body elements. It is constructed with
its mass concentrated to the main body elements, which in the real robot cor-
respond to the servo actuators, batteries and computer. The plastic body parts,
interconnecting the servos to each other, are not rendered in the simulation,
since their mass is very low compared to the total mass.
Fig. 2. Snapshot of the simulated humanoid robot. The body elements are directly
connected to each other, although this is not visualized here.
3.2
Virtual Register Machine
The Genetic Programming representation used for this problem of robot control
program induction is an instance of a Virtual Register Machine, VRM(k, l) [10].

500
K. Wolﬀand P. Nordin
It has k I/O registers and l internal work registers. In the current implementation
of our system, l equals k. The function set consists in the present of arithmetic
functions ADD, SUB, MUL, DIV, where DIV is protected division, and SINE.
We now deﬁne a register state vector Reg ≡[Reg1, ..., Regk] of k integers, each
of the elements corresponding to one of the actuated joints of the simulated
robot. All program input/output is communicated through the states of the
I/O registers. That is, program inputs are supplied in the initial state Reg,
and output is taken from the ﬁnal register state Reg′. Further, the I/O register
state vector is initially copied into the internal work registers. We can do this
in a straight forward manner, since we have imposed that the number of I/O
registers, k, equals the number of work registers, l. The Virtual Register Machine
is allowed writing only to the internal work registers when looping the program
instructions. The I/O registers are write-protected in this phase, and their ﬁnal
state is updated after the end of the program execution cycle, before they are
passed to the robot and then updating it’s internal state.
3.3
Linear Genome Representation
Each individual is composed of simple instructions between input and output
parameters. Each instruction consists of four elements, encoded as integers, and
the whole individual is a linear list of such instructions:
8, 22, 3, 12,
19, 11, 2, 16,
15, 12, 3, 12,
8,
3, 4, 19,
12, 12, 4, 21,
1,
6, 5, 12,
20,
3, 1, 19,
9, 12, 2, 21,
23,
5, 3, 19,
16,
9, 3, 14,
13, 21, 5, 19,
6, 13, 5, 14,
16, 22, 3, 16,
16,
3, 4, 18,
8, 19, 2, 13,
20,
5, 3, 20,
13,
6, 1, 14,
The encoding scheme is as follows; the ﬁrst and second elements of an instruc-
tion refers to the registers to be used as arguments, the third element corresponds
to the operator, i.e. ADD=1, SUB=2, MUL=3, DIV=4, and SINE=5, and the
last element is a register reference for where to put the result of the operation.
The meaning of the ﬁrst line (instruction) here is: multiply register 8 with reg-
ister 22 and put the result in register 12. The operators take two arguments,

Learning Biped Locomotion from First Principles
501
except when the operator is SINE, which of course only take one argument. In
this case, the SINE operator is applied to the ﬁrst element in the instruction,
and the second element is simply discarded. A mutation on that element will
thus have no eﬀect on that individual’s genotype. The register references 1-11
are assigned to I/O-registers, and register references 12-23 are assigned for the
internal work registers. Parsing the individual above, and print out the ﬁrst three
instructions in ‘C-style’ looks like this:
Reg12 = Reg8 * Reg22;
Reg16 = Reg19 - Reg11;
Reg12 = Reg15 * Reg12;
3.4
Evolutionary Algorithm
At the beginning of the evolutionary process, the population is ﬁlled with ran-
domly created individuals. The length, or number of instructions, of an individual
is chosen randomly with Gaussian distribution, with expectation value 20. The
maximum length is restricted to 256 instructions. The genes are created with
a uniform distribution over their respective search range; 1-23 for the two ﬁrst
genes of an instruction, 12-23 for the last gene, and 1-5 for the third gene, which
corresponds to the function set.
Our GP-system is a steady state tournament selection algorithm, with the fol-
lowing execution cycle:
1. Select four members of the population for tournament.
2. For all members in tournament do:
a. Create an instance of the simulated robot.
b. Record the position in 3d-space of all the robot’s limbs.
c. Execute the individual for 2500 simulation time steps.
d. Record the final position of all the robot’s limbs.
e. Compute the fitness value (see below).
f. Destroy the simulated robot.
3. Perform tournament selection.
4. Apply genetic operators on the winners to produce two children.
5. Replace the two losers in the population with the offspring.
6. Go to step 1.
The individuals are evaluated (evaluation cycle starting with point 2a. above)
under identical conditions, since the simulation is entirely deterministic. They
all start from the same standing upright pose, with the same orientation. The
execution time for individuals are 2500 simulation time steps (corresponding
to approx. 20 seconds of real time simulation), and if an individual cause the
robot to fall before this time is completed, the evaluation is terminated. In
the beginning of an experiment, a great majority of individuals are terminated
before the intended time. Looping an individual once does not correspond to
a single simulation time step, but to moving the robot’s limbs between two
consecutive internal states (‘states’ being referred to as in the subsection Gait
Control Method).

502
K. Wolﬀand P. Nordin
Table 1. Koza style tableau, showing parameter settings for the evolution of locomo-
tion control programs for the simulated humanoid robot.
Parameter
Value
Objective
Approximate a function that produce robust biped gait
Terminal Set
24 integer registers,
Function Set
ADD, SUB, MUL, DIV, SINE
Raw Fitness
According to eq. (2), scalar value
Standardized Fitness
Same as Raw Fitness
Population Size
800
Initialization Method
Random
Simulation Time
2500 simulation time steps
Crossover Probability
100%
Mutation Probability
80%
Initial Program Length
Gaussian distribution, expectation value 20.
Maximum Program Length
256 instructions
Maximum Tournament Number None
Selection Scheme
Tournament, size 4
Termination Criteria
None (determined by the experimenter)
Fitness Calculation.
As in all GP-applications, ﬁnding a proper ﬁtness
function that guides the artiﬁcial evolution in the desired direction is of
great importance. The primary goal for the experiment was to produce a
”human-like”, bipedal gait without the robot falling. To accomplish this task,
the individual controlling the robot should; (i) locomote the robot as straight
forward as possible, and (ii) keep the robot in an upright pose during the
movement. Hence, the proper measurements to feed the ﬁtness function with
are related to the height maintained by the robot, and the covered distance
during simulation. Explicitly formulated in mathematical terms, the proper
ﬁtness function was found to be:
f = W

1.0 −hstart
hstop

+ (dleft + dright)
(1)
where hstart is the height of the robot at the starting position, hstop is the
height when evaluation terminates (either the simulation is fully completed, or
it is terminated before the intended time, caused by the robot falling). The
height measure is applied to the position of the robot’s head, however one could
take the height of any body part. The second term is a measure of the distance
covered by the robot during evaluation, applied to its feet. The robot is always
starting with its feet in origo (in xy-plane). The ﬁrst term will give a positive
contribution to ﬁtness if hstop > hstart, negative contribution in the case when
hstop < hstart, and zero contribution if hstop = hstart. Thus we have a ﬁtness
function rewarding forward locomotion and keeping the upright pose, and pun-
ishing backward movements and falling. The W in the ﬁrst term is a weight,

Learning Biped Locomotion from First Principles
503
scaling the mutual relation of rewarding and punishing. After some tweaking, it
was found to work best when set to a value in the order of 10.
Genetic Operators. We use only two-point string crossover, with 100% prob-
ability for crossover, divided mutually on the rate 4:1 on homologous and non-
homologous crossover.
When an individual is chosen for mutation, the mutation operator works by ran-
domly selecting one single instruction from the individual, and make a change
in the selected instruction. It makes that change either by changing any of the
register references to another randomly chosen register reference from the regis-
ter set, or the operator in the instruction may be changed. The probability for
an individual to undergo mutation is 80%.
4
Results
When observing the experiments in run-time, it is compelling how quickly the
simulated robot learns. In the ﬁrst couple of hundred tournaments, a great major-
ity of the individuals cause the robot to fall almost immediately in the beginning
of the evaluation cycle, and the greater part of them tip over backwards. Maybe
one out of ten individuals fall to the fore, which is a good starting point of taking
a step ahead. Rather soon, however, one can observe the opposite situation, one
out of ten individuals’ overturn backwards and the rest fall ahead. This was not
the desired goal for the evolution, but we regard this as being the ﬁrst reﬁned
behavior that emerged.
The next observable stage of development in the evolution is when a large frac-
tion of individuals is keeping the robot at a standstill, almost motionless, on its
feet. In the beginning of our experiments, we faced some problems with evolu-
tion converged to this state. By increasing the population size and making some
adjustments to the ﬁtness function (mainly by decreasing the weight w, giving
lesser punishment for tipping over), we could guide the evolution towards the
desired goal. The mix of individuals showing this behavior, and individuals with
a more ‘energetic’ behavior guarantee suﬃcient diversity of the population for
evolution to proceed.
The ﬁnal results of these experiment was indeed consistent with our initial ob-
jectives. That is, evolution created controller programs that made the simulated
robot produce forward locomotion behavior. Some of the resulting programs
made the robot walking forward in a spiral manner, with small movements, and
others produced gait patterns with more lively movements. When tested, some
of the individuals managed to keep the robot on its feet for the whole evalua-
tion time (2500 simulation time steps), but when executed for a longer time, the
robot usually ended up overturned. Nevertheless, a division of evolved programs
could accomplish the task during the test run, without ever tipping over the
robot.
Figures 3 and 4 displays some statistics from a representative run. In these exper-
iments we did more than thirty independent runs, ranging from a few thousand

504
K. Wolﬀand P. Nordin
0
1000
2000
3000
4000
5000
6000
−4
−2
0
2
4
6
Over All Best of Population
Number of Tournaments
Fitness
0
1000
2000
3000
4000
5000
6000
−3
−2
−1
0
1
2
3
4
5
6
Best of Tournament
Number of Tournaments
Fitness
Fig. 3. a,Fitness value of over all best individual in the population (left) and b,ﬁtness
value of the best individual in every tournament (right).
tournaments, up to more than 80000 tournaments. The way ﬁtness was deﬁned
(eq. 2), a ﬁtness value < 0 correspond to the robot falling backward, and a
small positive value (typically ranging from ∼0.3 to ∼0.6) correspond to the
robot immediately falling ahead, while a value around 1.5 indicate a standstill.
In ﬁgure 3a, one can observe how the best individual performed those behaviors;
falling backward in the ﬁrst few hundred tournaments, falling ahead in the ﬁrst
thousand tournaments, and standing still up to the 3000 tournaments. Fitness
values in the range of ∼1.5 to ∼2.5 indicate some good locomotion, but usually
ended up with the robot overturned, and ﬁtness > 2.5 was successful locomotion
behavior.
As depicted in ﬁgure 3a, the currently best individuals in the population showed
progress from the beginning of the evolution and continued to develop over time.
The program length typically decrease below the initialization length in the be-
ginning of a run, but after a short while it starts to increase above that threshold,
and ﬁnally it stabilize around some value. See ﬁgure 4. In all experiments we
used the same initialization program length, with gaussian distribution and ex-
pectation value 20. It was observed that the program length, averaged over the
whole population, did never go below the value 13, and never above 50, and it
usually stabilized somewhere around 30.
5
Summary and Conclusions
We describe the ﬁrst instance of an approach for control programming of hu-
manoid robots. It is based on evolution as the main adaptation mechanism,
utilizing computing techniques from the ﬁeld of Evolutionary Algorithms. The
central idea in this concept is to evolve control programs from ﬁrst principles
on a simulated robot, transfer the resulting programs to the real robot and con-
tinue to evolve eﬃcient gait on the real robot. As the key motivation for using

Learning Biped Locomotion from First Principles
505
0
1000
2000
3000
4000
5000
6000
16
18
20
22
24
26
28
30
32
34
Average Genome Lenght
Number of Tournaments
Genome Lenght
Fig. 4. Average genome length of all individuals in the population, length being deﬁned
as the number of instructions in an individual.
simulators, we brieﬂy describe an on-line learning experiment performed with a
biped humanoid robot.
The Evolutionary Algorithm is an instance of Genetic Programming, imple-
mented as a Virtual Register Machine with 12 internal work registers and 12
external registers for I/O operations. The individual representation scheme is a
linear genome, encoded as an array of integers. The selection method is a steady
state tournament algorithm, with size four. The ﬁnal results of these experiment
was consistent with our initial objectives. That is, evolution created controller
programs that made the simulated robot produce forward locomotion behavior.
Current versions of the simulation system and the robot, however, do not allow
the evolved programs to be directly downloaded to the robot. Further inves-
tigations and improvements are needed. To begin with, we must implement a
subsystem of the simulated robot’s control system and program interpreter on
the real robots micro controller. Further, the real robot has an active feedback
system, consisting of a color camera and a distance sensor, which will be imple-
mented on the simulated robot as well. The development of the robot platform
is an ongoing process, hence other sensors will be implemented on the robot.
Then, the simulated robot should of course reﬂect all aspects, morphological
and perceptual, of the real robot.
With this system of two phases of evolution, it will be possible to have a ﬂexible
adaptation mechanism that can react to hardware failures in the robot, e.g. if
an actuator or sensor break down. By extracting information about malfunc-
tioning parts and do oﬀ-line evolution with a modiﬁed model of the robot, it
will become possible to react to the changes in the robot morphology. Another
approach in this spirit, called Punctuated Anytime Learning, has been proposed
by Parker [12]. For robots working in hazardous environments, or in applications
with remote presence robots, this feature would be very useful.

506
K. Wolﬀand P. Nordin
References
1. Banzhaf, W., Nordin, P., Keller, R.E., and Francone F. D.: Genetic Programming
An Introduction: On the Automatic Evolution of Computer Programs and Its Ap-
plications. San Francisco: Morgan Kaufmann Publishers, Inc. Heidelberg: dpunkt
verlag. (1998)
2. Br¨aunl, T. 2002: EyeBot Online Documentation. Last visited: 01/21/2003.
http://www.ee.uwa.edu.au/∼braunl/eyebot/
3. Hornby, G.S., Fujita, M. Takamura, S., Yamamoto, T., and Hanagata, O.: Au-
tonomous evolution of gaits with the Sony quadruped robot. Proceedings of the
Genetic and Evolutionary Computation Conference. San Francisco: Morgan Kauf-
mann Publishers, Inc. (1999)
4. Hornby, G.S., Takamura, S., Yokono, J., Hanagata, O., Yamamoto, T., and Fujita,
M.: Evolving robust gaits with AIBO. IEEE International Conference on Robotics
and Automation, New York: IEEE Press, pages 3040–3045. (2000)
5. Koza, J.: Genetic Programming: On the Programming of Computers by Means of
Natural Selection. Cambridge, MA, USA: MIT Press. (1992)
6. Lewis, M. A., Fagg, A. H., and Solidum, A.: Genetic programming approach to
the construction of a neural network for control of a walking robot. Proceedings of
the IEEE International Conference on Robotics and Automation. New York: IEEE
Press. (1992)
7. Lund, H., and Miglino, O.: From Simulated to Real Robots. Proceedings of IEEE
3rd International Conference on Evolutionary Computation. New York: IEEE
Press. (1996)
8. Langdon, W. B., and Poli, R.: Foundations of Genetic Programming. New York:
Springer-Verlag. ISBN 3-540-42451-2, 274 pages. (2002)
9. Miglino, O., Lund, H., and NolﬁS.: Evolving Mobile Robots in Simulated and Real
Environments. Technical Report, Institute of Psychology, C.N.R., Rome. (1995)
10. Nordin, P.: Evolutionary Program Induction of Binary Machine Code and its Ap-
plications. Ph.D. Thesis, der Universit¨at Dortmund am Fachbereich Informatik,
Germany. (1997)
11. Parker, G. and Rawlins, G.: Cyclic Genetic Algorithms for the Locomotion of Hexa-
pod Robots. Proceedings of the World Automation Congress, Volume 3, Robotic
and Manufacturing Systems. (1996)
12. Parker, G.: Punctuated Anytime Learning for Hexapod Gait Generation. Proceed-
ings of the 2002 IEEE/RSJ International Conference on Intelligent Robots and
Systems. (2002)
13. Sims, K.: Evolving Virtual Creatures. Proceedings of Siggraph, pp.15–22. (1994)
14. Sims, K.: Evolving 3D Morphology and Behavior by Competition. Proceedings of
Artiﬁcial Life IV, Brooks and Maes, editors, MIT Press, pp.28–39. (1994)
15. Smith, R.: Open Dynamics Engine v0.030 User Guide. Last Visited: 03/27/2003.
http://opende.sourceforge.net/ode-0.03-userguide.html
16. Schwefel, H. P.: Evolution and Optimum Seeking. New York, USA: Wiley. (1995)
17. Wolﬀ, K., and Nordin, P.: Evolution of Eﬃcient Gait with an Autonomous Biped
Robot using Visual Feedback. Proceedings of the Mechatronics Conference. Uni-
versity of Twente, Enschede, the Netherlands. (2002)
18. Ziegler, J., Barnholt, J., Busch, J., and Banzhaf W.: Automatic Evolution of Con-
trol Programs for a Small Humanoid Walking Robot. 5th International Conference
on Climbing and Walking Robots. (2002)

An Evolutionary Approach to Automatic
Construction of the Structure in Hierarchical
Reinforcement Learning
Stefan Elfwing1,2,3, Eiji Uchibe2,3, and Kenji Doya2,3
1 KTH, Numerical Analysis and Computer Science department,
KTH, Nada, 100 44 Stockholm, Sweden
2 ATR, Human Information Science Laboratories, Department 3
3 CREST, Japan Science and Technology Corporation
2-2-2 Hikaridai, “Keihanna Science City” Kyoto 619-0288, Japan
1
Introduction
Hierarchical reinforcement learning (RL) methods have been developed to cope
with large scale problems. However, in most hierarchical RL methods, an ap-
propriate structure of hierarchy has to be hand-coded. This paper presents an
evolutionary approach for automatic construction of hierarchical structures in
RL.
2
Proposed Method
Our method combines the MAXQ method [1] and Genetic Programming (GP).
The MAXQ method learns the policy based on the hierarchy obtained by the
GP, while GP explores the appropriate hierarchies using the result of the MAXQ
method. Leaf nodes and inner nodes of MAXQ representation are regarded as
terminals and functions for GP. We use strongly-typed GP [2] that allows the
designer to assign speciﬁc types to the arguments and the return value of each
function.
3
Task and Experimental Results
We have performed simulation experiments with a rodent like robot, Cyber Ro-
dent. The task is to ﬁnd, approach and capture a battery pack, and then return
the battery pack to the nest. We have prepared three diﬀerent environments
shown in Fig. 1. Cyber Rodent has the distance sensors and the vision sys-
tem. For all sensor readings in the simulated environment, noise (10 % of in-
put range) is added. In this experiment, we prepared seven composite subtasks
(root, capture, deliver, find battery, visible battery, find nest, and
visible nest), and ﬁve primitive subtasks (avoid, wander, approach battery,
approach nest, and turn). Each individual in the population performs a ﬁxed
number of trials in each generation. The ﬁtness is calculated as the number of
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 507–509, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

508
S. Elfwing, E. Uchibe, and K. Doya


		



	
	


	

	

		

	
		


		





	

	



	


	
	

(a) E1: Simple environment
(b) E2: There are two battery packs, and the nest
is placed in the center of the environment.
	

		

		

	





	




	
 	






(c) E3: There are many obstacles, and the battery packs can not be observed when
the agent is in the nest.
Fig. 1. Tested environments and examples of obtained hierarchies. The small ﬁlled
light gray circles represent battery packs and the big darker gray circles represent the
nests, respectively.
time steps to complete the task. The parent hierarchies for crossover are chosen
by tournament selection.
Experimental results showed that GP found suitable hierarchies for all three
environments. A remarkable ﬁnding was that the complexity of the obtained
hierarchical structures was strongly constrained by the complexity of the envi-
ronment, GP for a simple environment obtained a simple and specialized hier-
archy and GP for a complex environment obtained a complex and general task
hierarchy. The main diﬀerence between E1 and E2 was that the nest in E2 was
surrounded by the wall although the battery packs were placed in an open ﬁeld
in both environments. Accordingly, the part of deliver in the obtained struc-
ture was diﬀerent, as shown in Fig. 1(a) and (b). In E3, since there were many
obstacles and the environment was crowded, the obtained structure was the most
complicated.
4
Conclusion
We plan to implement our method to the real hardware. A foreseeable exten-
sion of this study is to generalize the method as a model of cooperative and
competitive mechanisms of the learning modules in the brain.
Acknowledgments. This research was supported in part by the Telecommu-
nicatios Advancement Organization of Japan.

An Evolutionary Approach to Automatic Construction of the Structure
509
References
1. T. G. Dietterich. Hierarchical Reinforcement Learning with the MAXQ Value Func-
tion Decomposition. Journal of Artiﬁcial Intelligence Research, 13:227–303, 2000.
2. D. J. Montana. Strongly Typed Genetic Programming. Evolutionary Computation,
3(2):199–230, 1995.

Fractional Order Dynamical Phenomena in a GA
E.J. Solteiro Pires1, J.A. Tenreiro Machado2, and P.B. de Moura Oliveira1
1 Universidade de Tr´as-os-Montes e Alto Douro, Dep. de Engenharia Electrot´ecnica,
Quinta de Prados, 5000–911 Vila Real, Portugal,
{epires,oliveira}@utad.pt, http://www.utad.pt/˜epires
2 Instituto Superior de Engenharia do Porto, Dep. de Engenharia Electrot´ecnica,
Rua Dr. Ant´onio Bernadino de Almeida, 4200-072 Porto, Portugal
jtm@dee.isep.ipp.pt, http://www.dee.isep.ipp.pt/˜jtm
Abstract. This work addresses the fractional-order dynamics during
the evolution of a GA, which generates a robot manipulator trajectory.
In order to investigate the phenomena involved in the GA population
evolution, the crossover is exposed to excitation perturbations and the
corresponding ﬁtness variations are evaluated. The input/output signals
are studied revealing a fractional-order dynamic evolution, characteristic
of a long-term system memory.
1
The GA Trajectory Planning Scheme
This section presents a GA that calculates the trajectory of a two-link manipu-
lator that is required to move between two points. The path is encoded directly,
using real codiﬁcation, as strings in the joint space to be used by the GA as:
[∆t, (q11, q21), . . . , (q1j, q2j), . . . , (q1m, q2m)]. The ith joint variable for a robot
intermediate jth position is qij, at time j∆t. The ﬁtness function f adopted for
evaluating the trajectories is deﬁned as:
f = β1fτ + β2
m

j=2
2

i=1
˙q2
ij + β3
m−1

j=2
2

i=1
¨q2
ij + β4
m

j=2
˙p2
j + β5
m−1

j=2
¨p2
j
(1)
The fτ index represents the excessive torque that is demanded for the joints
motors, pj is the j cartesian arm position. This simple experiment consists on
moving a robotic arm between two points. In the GA are adopted pc = 0.8,
pm = 0.05, a 200 population size, a string size of m = 7 and a 3-tournament
selection. The robot parameters are li = 1m, mi = 1kg and τi,max = {16, 5}Nm
(i = 1, 2). Figure 1ab show the simulation results. The trajectory presents a
smooth behavior, both in the space and time evolution and the required joint
torques do not exceed the imposed limitations.
2
Fractional-Order Dynamics
The GA system is stimulated by perturbing the crossover probability pc through
a white noise signal, with a small amplitude (1%) during a time period Texc, and
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 510–511, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Fractional Order Dynamical Phenomena in a GA
511
the corresponding modiﬁcation of the population ﬁtness is evaluated. Therefore,
the variation of the crossover probability and the resulting ﬁtness modiﬁcation
of the GA population, during the evolution, can be viewed as the system input
and output signals versus time, respectively.
The transfer function Hn(jω), between the input and output signals, and the
fractional order analytical approximation Gn(jω) are depicted in ﬁgure 1c.
The numerical data of the transfer functions are approximated by analytical
expressions with gain k ∈ℜ, one zero and one pole (a,b) ∈ℜof fractional orders
(α,β) ∈ℜ, respectively, given by Gn (s) = k[
 s
a
α + 1]/[
 s
b
β + 1] for the nth
ﬁtness percentile Pn. For evaluating the inﬂuence of the excitation period Texc
several simulations are developed. The relation between the transfer function
parameters {k, α, β} and (Texc, Pn) are showed in ﬁgure 1def.
3
Conclusions
Fractional-order models capture phenomena and properties that classical integer-
order simply neglect. For the case under study the signal evolution have similar-
ities to those revealed by chaotic systems. This conclusion conﬁrms the require-
ment for mathematical tools well adapted to the phenomena under investigation.
In this line of thought, this article is a step towards the signal and system analysis
based on the theory of Fractional Calculus.
0
1
2
3
4
5
6
7
8
9
10
−1.5
−1
−0.5
0
0.5
1
1.5
2
t [s]
Rotacional Joint Positions 
q1
q2
0
50
100
150
200
250
10
0
10
1
10
2
10
3
Fitness f
T [s]
P0
P30
P70
P100
10
−3
10
−2
10
−1
10
0
10
1
10
2
30
40
50
60
70
80
90
100
w [rad/s]
H40(jw) [DB]
a)
b)
c)
2
3
4
5
6
7
0
20
40
60
80
100
4
5
6
7
8
9
10
11
ln(Texc)
Pn
ln(k)
2
3
4
5
6
7
0
20
40
60
80
100
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
ln(Texc)
Pn
ln(alfa)
2
3
4
5
6
7
0
20
40
60
80
100
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
ln(Texc)
Pn
ln(beta)
d)
e)
f)
Fig. 1. a) Robot joint positions vs. time. b) Percentiles of the population ﬁtness vs.
T. c) H40(jω) = F{δP40(T)}/F{δpc(T)} and G40(jω) for the percentile n = 40%.
d) Estimated gain ln(k) vs. (Texc, Pn). e) Estimated zero fractional-order ln(α) vs.
(Texc, Pn). f) Estimated pole fractional-order ln(β) vs. (Texc, Pn).

Dimension-Independent Convergence Rate for
Non-isotropic (1, λ) −ES
Anne Auger1,2, Claude Le Bris1,3, and Marc Schoenauer2
1 CERMICS – ENPC
Cit´e Descartes, 77455 Marne-La-Vall´ee, France
{auger,lebris}@cermics.enpc.fr
2 INRIA Rocquencourt, Projet Fractales
BP 105, 78153 LE CHESNAY Cedex, France
marc.schoenauer@inria.fr
3 INRIA Rocquencourt, Projet MIC MAC
BP 105, 78153 LE CHESNAY Cedex, France
Abstract. Based on the theory of non-negative super martingales, con-
vergence results are proven for adaptive (1, λ) −ES (i.e. with Gaus-
sian mutations), and geometrical convergence rates are derived. In the
d-dimensional case (d > 1), the algorithm studied here uses a diﬀer-
ent step-size update in each direction. However, the critical value for
the step-size, and the resulting convergence rate do not depend on the
dimension. Those results are discussed with respect to previous works.
Rigorous numerical investigations on some 1-dimensional functions vali-
date the theoretical results. Trends for future research are indicated.
1
Introduction
Since their invention in the mid-sixties (see the seminal books by Rechenberg [7]
and Schwefel [10]), Evolution Strategies have been thoroughly studied from the
theoretical point of view.
Early studies on two very particular functions (the sphere and the corridor)
have concerned the progress rate of the (1 + 1) −ES, and have lead, by extrap-
olation to any function, to the famous one-ﬁfth rule. The huge body of work
by Beyer, including many articles, and somehow summarized in his book [3],
has pursued along similar lines, studying more general algorithm, from the full
(µ +, λ)−ES to the (µ/µ, λ)−ES with recombination and the (1, λ)−σ−SA−ES
with self-adaptation. However, though giving important insights about the way
ES actually work, the study of local progress measures, such as the progress rate,
does not lead to global convergence results of the algorithm.
Some global convergence results, together with the associated (geometrical)
convergence rates have been obtained for convex functions [8,13], and for a
class of function slightly more general than quadratic functions, the so-called
(Q −K) −strongly convex functions [9]. These latter results deal with the so-
called adaptive version of evolution strategies, in which the step-size is computed
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 512–524, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Dimension-Independent Convergence Rate for Non-isotropic (1, λ) −ES
513
at each iteration according to some measures on the current population (the ter-
minology used here is taken from [6]) – namely the norm of the gradient of the
ﬁtness function.
Note that the results in [13] have been criticized in [4], in which an analytical
approach is provided in the case of the sphere function when the step-size is the
norm of the parent itself. In that case, the strong law of large number gives an
almost sure convergence.
The state-of-the-art in practical ES, however, recommends using self-adaptive
ES, in which the step-size is adjusted by the evolution itself at the individual
level. Whereas of course the results by Beyer on the (1, λ) −σ −SA −ES
do address self-adaptive ES [3], only recently some global convergence results
regarding self-adaptive ES-like algorithms were published [5,11]. However,
the algorithms studied in those works do not consider the standard normal
mutation, but rather use a simpliﬁed mutation operator: only a ﬁnite number
of variation of the step-size are allowed in [5], while [11] considers a uniform
mutation. Moreover, these papers only consider the simple and symmetrical
function f(x) = |x|. Finally, [5] does not give any estimation of the convergence
rate, and the proof in [11] relies on a numerical estimate of some inequality -
though this might probably be improved in the near future. An important point
about these latter two results is that they use the theory of super-martingales
[12], a somewhat more sophisticated technique than all previously cited works
(with the remarkable exception of [8]).
The same super martingale technique will be used in this paper, to analyze
some adaptive ES with Gaussian mutation, in which the step-size is adapted
either using the distance to the global optimum or using gradient information
about the ﬁtness function, but in a diﬀerent way than in [13,9]. Moreover, the
speed of convergence will also be studied: as in previous relevant work, some
geometrical upper-bounds will be derived, and their sharpness will be tested
through numerical experiments.
The paper is organized as follows. Next section formally describes the adap-
tive ES under study. We conﬁgure ES with an adaptivity that evolves more
deterministically than in standard self adaptive ES (see formula (1) below). Sec-
tion 3 gives the convergence results and the main ideas of the proofs (due to
size limitation, the complete proofs cannot be given here, see [2] for all the de-
tails). First, the one-dimensional case is thoroughly studied: in the case of the
sphere function analytical results are obtained for the sphere function, before
two diﬀerent ways of adapting the step-size are studied in turn for a more gen-
eral class of functions. It is indeed to be noted that our proofs and techniques
are not restricted to the speciﬁc cases we deal with here. Next, the optimality of
the critical value of the step size and convergence rate obtained is proved for the
sphere function. The case of larger dimension is ﬁnally presented. The originality
is that we derive estimates of the convergence rate that do not depend on the

514
A. Auger, C. Le Bris, and M. Schoenauer
dimension. This is done on a speciﬁc algorithm where the step-size is adapted
independently in each dimension.
In section 4, our results are thoroughly discussed, in the light of previous
works on adaptive algorithm (already cited in the Introduction). Section 5 next
gives experimental evidences (in one dimension only) that demonstrate the va-
lidity of the critical value of the step size and of the convergence rate, for more
general functions (such as functions that are neither symmetric (w.r.t. their min-
imum) nor convex). The article closes with some discussion and trends for future
work.
2
Notations and Algorithm
For the sake of simplicity, the results will ﬁrst be presented in dimension 1. The
case of higher dimensions will be introduced in section 3.4. Let f be a real-valued
function deﬁned on R to be minimized. The general adaptive (1, λ)-Evolution
Strategy algorithm we will consider henceforth is of the form:

X0 ∈R,
Xn+1 = arg min{f(Xn + σH(Xn)N n
i ), i ∈[1, λ]},
(1)
where Xn is the random variable modeling the parent at the generation n,
(N n
i )i=1,... ,λ are independent standard normal random variables, H(x) is real-
valued function (for conciseness, only two cases will be considered in the follow-
ing: H(x) = |x| or H(x) = |f ′(x)|, but other cases, such as H(x) = |f(x) −f ∗|
can be treated by the same technique, see [1])), and σ is a positive real param-
eter, often referred to as the step-size (or normalized step-size e.g. in [10,3], in
the case where H(x) = |x|).
This paper is concerned with studying the behavior of algorithm (1), or, more
precisely, with addressing the issue of the range of values for σ for which the
algorithm converges1. Moreover, whenever convergence takes place, bounds for
the convergence rate will also be sought.
Section 3 gives answers to both questions, ﬁrst for the sphere function (sec-
tion 3.1), as exact convergence rates can be easily computed, and then for
twice continuously diﬀerentiable functions with particular properties in the case
H(x) = |x| (section 3.2) and H(x) = |f ′(x)| (section 3.3).
3
Convergence Results the (1, λ)-ES
3.1
The Sphere Function – Again
The sphere function (f(X) = |X|2) has always been the preferred test function
of authors studying the theory of Evolution Strategies [7,10,8,3,4,5,11]. Indeed,
when f is the sphere function, many things get simpler, and most quantities of
interest can be computed analytically.
1 Both almost sure convergence and convergence in Lp (w.r.t. the norm E(|X|p)
1
p ) will
be looked at.

Dimension-Independent Convergence Rate for Non-isotropic (1, λ) −ES
515
For instance, it is clear that both cases H(x) = |x| and H(x) = |f ′(x)| behave
identically (up to a factor 2). But another important simpliﬁcation concerns the
algorithm itself:
Lemma 1. For the sphere function, the random variable Xn deﬁned by (1) with
H(x) = |x| satisﬁes,
Xn+1 = Xn(1 + σY (λ))
(2)
where Y (λ), the random variable deﬁned by
1 + σY (λ) = arg min{(1 + σN n
1 )2, ..., (1 + σN n
λ )2}
(3)
does not depend on σ.
A detailed proof, with the exact distribution of Y (λ) can be found in [4].
Convergence in Lp. The following theorem is an immediate consequence of
Lemma 1:
Theorem 1. For the sphere function, the random variable Xn deﬁned by (1)
with H(x) = |x| satisﬁes,
E(|Xn|p) = E(|X0|p) (E(|1 + σY (λ)|p))n .
(4)
Hence, the algorithm converges or diverges in Lp norm geometrically. More-
over, there exists a value σc(λ, p) such that Xn converges in Lp norm iﬀ
σ ∈]0, σc(λ, p)]. This value is deﬁned by
σc(λ, p) = inf{σ such that E(|1 + σY (λ)|p) ≥1}.
(5)
Remark 1. It can be proved that E(|1 + σY (λ)|p has a unique minimum w.r.t.
σ, which gives the best convergence rate. This minimum σs(λ, p) is thus deﬁned
by,
σs(λ, p) = argmin{E(|1 + σY (λ)|p), σ ∈]0, σc(λ, p)[}.
(6)
An alternative view on the progress rate. Interestingly, this result meets
early studies of ES [7,10,3] that did look at the progress rate ϕp, deﬁned by:
ϕp(Xn, σ, λ) = E
|Xn+1|p −|Xn|p
|Xn|p
|Xn

,
(7)
The progress rate measures the expectation of change from one iteration of
the algorithm to the next one, conditionally to the current parent Xn: Note that
this conditional dependency is often left implicit in the cited works. Those early
works determine, for a given λ, the optimal step size σ which minimizes the

516
A. Auger, C. Le Bris, and M. Schoenauer
progress rate. In general, this quantity depends on the current point Xn and
will not be very useful to study the dynamics of the algorithm.
However, in the case of the sphere function, things are diﬀerent. A direct
consequence of Lemma 1 is that for the sphere function with H(x) = |x|, the
progress rate does not depend on the value of Xn and is hence for instance equal
to the value for Xn = 1: (∀n > 0), ϕp(Xn, σ, λ) = E(|1 + σY (λ)|p −1).
Hence, minimizing the progress rate as in [10,3] thus amounts to ﬁnding the
value of σ such that E(|1 + σY (λ)|p) is minimal – and this is exactly the value
given by equation (6).
Convergence almost surely. For the almost sure convergence, Lemma 1 and
the strong law of large numbers gives the following result (see [4] for more de-
tails),
Theorem 2. Assume that E(ln(|1 + σY (λ)|)) < ∞. Then, for the sphere func-
tion, the random variable Xn deﬁned by (1) with H(x) = |x| satisﬁes,
1
n ln(|Xn|) −−−→
n→∞E(ln(|1 + σY (λ)|))
almost
surely.
Thus the critical value σc(λ, as) is here deﬁned as sup{σ\E(ln(|1+σY (λ)|) <
1}.
The following two sections will prove similar results for more general func-
tions, for each of the cases H(x) = |x| and H(x) = |f ′(x)|.
3.2
Convergence of the (1, λ)-ES with H(x) = |x|
The case where H(x) = |x| (or H(x) = |x −x∗| for some minimizer x∗of f)
is the case with constant (normalized) step-size, as deﬁned for instance in [3].
Though this algorithm has not a practical interest because it supposes that a
minimum is already known, it will allow us to develop the technique of analysis
to be later applied to the more interesting case H(x) = |f ′(x)|.
The ﬁrst step of this analysis consists in ﬁnding a value σc such that f(Xn) is
a super martingale for σ ∈]0, σc[. The convergence of the processes f(Xn) and Xn
will immediately follow (see [12]). For this purpose, we state some assumptions
on f:
Assumptions (H1).
(i)
The function f has a unique global minimizer x∗. Without loss of generality,
we assume that x∗= 0 and f(0) = 0, and therefore ∀x ∈R, f(x) > 0.
(ii)
The function f is twice continuously diﬀerentiable.
(iii) There exists M ﬁnite such that, for all x ∈R, |f ′′(x)| ≤M.
(iv) There exists α > 0 such that, for all x ̸= 0, | f ′(x)
x
| ≥α > 0
Remark 2. All our proofs (see [2]) still go through when the process Xn is re-
placed by inf(sup(Xn, −A), A) in equation (1) for some large A. Such a modiﬁ-
cation is an easy trick to render Assumptions (H1) easier to fulﬁll.

Dimension-Independent Convergence Rate for Non-isotropic (1, λ) −ES
517
Remark 3. Assumption (H1) above implies that f is monotonously decreasing
on R−and increasing on R+.
In the sequel, Fn denotes the ﬁltration adapted to the process f(Xn).
Lemma 2. Assume λ ≤2. Let g be deﬁned by
g(σ, λ, α, M) = E

min
1≤i≤λ

αN i + σ M
2 (N i)2

,
(8)
and let σc(λ, α, M) be the solution of
g(σc(λ, α, M), λ, α, M) = 0
(9)
Assume f satisﬁes Assumption (H1), f(Xn) is a Fn-super martingale2 for 0 ≤
σ ≤σc(λ, α, M).
Remark 4. The value σc(λ, α, M) deﬁned by equation (9) always exists and
is unique for λ ≥2, α and M given, because g(σ, λ, α, M) is a strictly in-
creasing and continuous function w.r.t. σ, and satisﬁes g(0, λ, α, M) < 0 and
limσ→+∞g(σ, λ, α, M) = +∞.
Key point of the proof. The demonstration of this result relies on the fol-
lowing inequality, based on Taylor formula:
E(f(Xn+1)|Fn) ≤f(Xn) + σ|Xn|2g(σ, λ, α, M) a.s.
(10)
Convergence result. From this Lemma, the theory of non-negative super mar-
tingale [12] gives the following theorem.
Theorem 3. Assume λ ≥2, assume f satisﬁes Assumption (H1), and σ ∈
]0, σc(λ, α, M)[ with σc(λ, α, M) deﬁned by equation (9), then, when n goes to
+∞, f(Xn) converges to 0, both almost surely and in L1, and Xn converges to
0 both almost surely and in L2.
Convergence Speed
Theorem 4. Assume λ ≥2, assume f satisﬁes Assumptions (H1), and that
σ ∈]0, σc(λ, α, M)[, with σc(λ, α, M) deﬁned by (9), then f(Xn) converges geo-
metrically to 0 in the following senses:
(i)
(Convergence a.s.):
f(Xn)
(1 + σCg(σ, λ, α, M))n converges to some random
variable Y ,
(ii) (Convergence in L1): E(f(Xn)) ≤(1 + σCg(σ, λ, α, M))nE(f(X0)),
where C =
2
M and M is deﬁned by (H1)(iii). In addition, the best convergence
rate is reached for σ = σs(λ, α, M) where σs(λ, α, M) is the unique value of σ
that minimizes 1 + σCg(σ, λ, α, M).
2 Zn is a super martingale if it satisﬁes E(Zn+1|Fn) ≤Zn

518
A. Auger, C. Le Bris, and M. Schoenauer
3.3
Convergence of the (1, λ)-ES with H(x) = |f ′(x)|
The general outline of the demonstration in this case is the same as in the
previous section: First, ﬁnd a value σc such that f(Xn) is a supermartingale for
σ ∈]0, σc[. Then, derive the convergence and the speed of convergence of f(Xn).
Contrary to the previous section, unimodality is not mandatory in the present
section to obtain the convergence result per se. But, some local convexity is
needed to derive the convergence rate. We consider the following assumptions,
Assumption (H2).
(i) The function f is bounded from below (say by zero) and is twice continuously
diﬀerentiable.
(ii) There exists M ﬁnite such that, for all x, |f ′′(x)| ≤M.
Remark 5. Once again, using the truncation trick mentioned in Remark 2 weak-
ens this assumption which is then satisﬁed for every C2 function.
Lemma 3. Assume λ ≥2. Let h be deﬁned by
h(σ, λ, M) = E

min
1≤i≤λ

N i + σ M
2 (N i)2

(11)
and let σ
′
c(λ, M) be the solution of
h(σ
′
c(λ, M), λ, M) = 0
(12)
Then, if f satisﬁes Assumption (H2), f(Xn) is a Fn-super martingale for 0 ≤
σ ≤σ
′
c(λ, M).
Remark 6. The proof of the existence of σ
′
c(λ, M) is exactly the same as in
Remark 4.
Key point of the proof. Once again, the demonstration of the above result
relies on the following inequality.
E(f(Xn+1)|Fn) ≤f(Xn) + σ|f ′(Xn)|2h(σ, λ, M) a.s.
(13)
Convergence result. A straightforward corollary of this Lemma is that f(Xn)
converges almost surely. The following theorem then gives the convergence of
f ′(Xn).
Theorem 5. Assume f satisﬁes Assumption (H2). Assume λ ≥2 and σ ∈
]0, σ
′
c(λ, M)[. Then f ′(Xn) converges to 0 in L2. If we moreover assume that
f(Xn) is bounded then f ′(Xn) converges almost surely.
Remark 7. If we moreover suppose that f is unimodal and that the only mini-
mum is 0, then the algorithm converges globally: f(Xn) converges to 0 a.s.

Dimension-Independent Convergence Rate for Non-isotropic (1, λ) −ES
519
Convergence speed. An additional hypothesis, somewhat connected to con-
vexity, is now needed to estimate the convergence speed. Before we state it,
we set by convention that infR f = 0, otherwise f(x) should be replaced by
f(x) −infR f in the assumption below.
Assumption (H3). There exists C > 0 such that infR
|f ′(x)|2
f(x)
≥C.
Remark 8. Example of non-trivial functions satisfying both Assumptions (H2)
and (H3) will be given in the numerical experiments (see section 5).
Theorem 6. Assume λ ≥2. Assume f satisﬁes Assumptions (H2)(H3) and
that σ ∈]0, σ
′
c(λ, M)[. Then f(Xn) converges geometrically to 0 at the rate (1 +
σCh(σ, λ, M)) both almost surely and L1 (in the sense of Theorem 4, and with
the constant C deﬁned in (H3)). The best convergence rate is reached for σ =
σ
′
s(λ, M) where σ
′
s(λ, M) minimizes 1 + σCh(σ, λ, M).
On the optimality of the general estimates when applied to the
sphere function. Going back to the sphere function, the values in Assump-
tion (H1),(H2) and (H3) are M = 2, α = 2 and C = 4, and straightforward
calculus gives E(|1 + σY (λ)|2) = E(min1≤i≤λ(1 + σNi)2) = 1 + σg(σ, λ, 2, 2) =
1 + 2σh( σ
2 , λ, 2). It is thus easy to show that the critical values given in Theo-
rems 3, 4, 5 and 6 are the optimal values given by equations (5) and (6).
3.4
Results in Higher Dimensions
The algorithm deﬁned in equation 1 must be slightly modiﬁed when going to
dimension d > 1. The general form of the non-isotropic ES algorithm considered
here is:
X0 ∈Rd,
Xn+1 = arg min{f(Xn + σ(Hk(Xn)N n,i
k )k∈[1,d]), i ∈[1, λ]},
(14)
where Xn is the random variable modeling the parent at the generation n,
(N n,i
k ), k ∈[1, d], i ∈[1, λ] are independent standard normal random variables,
and Hk(x), k ∈[1, d] are d real-valued functions. Diﬀerent step-sizes are here
applied to the diﬀerent directions, similarly with what can be done as far as
self-adaptation is concerned [10].
Only the case of practical interest where Hk(x) = ∂f(x)
∂xk
will be considered
here. The situation is then similar to that studied in section 3.3. Assump-
tion (H2)(ii) then becomes, ||D2f||d = supx∈Rd ∥D2fx∥d
∥x∥d
≤M. Similar deriva-
tions allow one to prove the following equation which is the equivalent of equa-
tion (13),
f(Xn+1) ≤f(Xn) + σ
d

k=1
(∂f(Xn)
∂xk
)2 
N n,i
k
+ M
2 σ(N n,i
k )2
a.s.

520
A. Auger, C. Le Bris, and M. Schoenauer
from which derives exactly the same result than that of Lemma 3. In particular,
the critical value σc, below which convergence takes place, is again deﬁned by
equations (11) and (12). The more remarkable fact here is that this critical
value (and hence the convergence rate that comes with it) does not depend on
the dimension!
4
Discussion
This section will discuss the results of previous section in the light of past related
work from the literature.
First, it should be clear that only works proposing global convergence re-
sults are relevant for comparison here, as opposed to all work studying local
convergence (see section 3.1 for a link with those works).
The work whose results are most similar to the ones presented here are by far
Rudolph’s work, either using also super martingale [8], or somehow simpliﬁed
and based on order statistics [9]. There are however quite a few diﬀerences.
First, Rudolph’s results are based on some strong convexity of function f –
but it is fair to say that on the other hand, he only needs f to be diﬀerentiable
once – whereas convexity is not required here for the convergence result, and, as
expected, only weak convexity is necessary to obtain the geometrical converge
rate. 3
Second, whereas Rudolph chooses all oﬀspring uniformly on some hyper-
sphere (or radius σ), the algorithm considered here uses the “true” Gaussian
mutation. A common argument is that both mutations behave similarly in high
dimension. However, when it comes to theoretical results, such a consideration is
of no help. Indeed, the method used by Rudolph based on order statistics [9] can
also be applied with Gaussian mutation, and gives the same kind of convergence
result: there exists a critical value σc such that whenever σ lies in ]0, σc[ the algo-
rithm converges. Unfortunately, this constant σc is then deﬁned as 2
E(N λ:λ)
ME((N λ:λ)2),
where N λ:λ is the λth order statistics for standard normal random variables.
The problem is that this quantity is a very poor upper bound: for instance, it
decreases for large values of λ, making the result almost useless.
A noticeable diﬀerence with Rudolph’s algorithm in [9] lies in the case where
the dimension is greater than 1: the oﬀspring of parent Xn in Rudolph’s al-
gorithms are chosen using H(x) = σ||∇f(x)||N (notation of equation (1)), for
some vector of standard normal random variables N. The approach proposed
here is diﬀerent (see section 3.4), and the results are indeed far more appealing:
the upper-bound geometrical rate obtained by Rudolph goes to 1 when the di-
mension goes to ∞(despite the fact that he does not use Gaussian mutation),
while the one proposed here does not depend on the dimension. However, the
3 In this line, we would like to mention that there seems to be a lot of room for
improvement in the proofs we present here (see [1]). Assumptions of regularity and
convexity are likely to be relaxed. We are currently working on such extensions.
Deﬁnite conclusions are however yet to be obtained

Dimension-Independent Convergence Rate for Non-isotropic (1, λ) −ES
521
gap between the two approaches remains open, as it has not been possible up to
now to analyze the algorithm 1 with Rudolph’s H function.
5
Numerical Experiments
All numerical experiments presented in the sequel are based on the Monte Carlo
approximation of the expectation of a random variable. The expectation E(Z)
of a random variable Z is approximated by
1
K
K
k=1 Zk, where Zk are K inde-
pendent random variables with the same law than Z. Then, for instance, from
the central limit theorem, for large values of K (K = 1500 in all numerical
experiments presented here), with probability 0.95,
E(Z) ∈[ 1
K
K

k=1
Zk −
√
V arZ 1.96
√
K
, 1
K
K

k=1
Zk +
√
V arZ 1.96
√
K
]
(15)
5.1
Computation of the Constants
The Monte-Carlo method described above has been used to compute approxi-
mate values of the constants σc and σs from section 3.
0
1
2
3
σ
−0.5
0
0.5
1
1.5
E(|1+σ Y(λ)|
2)
0
500
1000
σ
−500
−400
−300
−200
−100
0
E(ln(f(X
n)))
σ=0.5
σ=1.5
σ=2
σ=2.5
σ=3
(a)
(b)
Fig. 1. (a) E(|1 + σY (4)|2) vs σ – see equation (3). (b) E(ln(f(Xn))) with respect to
the number of generations n.
A ﬁrst example is given by the plot of E(|1 + σY (λ)|2) against σ for the
sphere function on Figure 1 (a), for λ = 4. The limit value of σ for which
E(|1 + σY (λ)|2) ≤1 is σc(λ, 2) = 2.7, and the corresponding minimal value for
E(|1 + σY (λ)|2) is σs(λ, 2) ≈0.8. Note that this method allows us to plot the
progress rate (3.1) for any dimension d, as in [10,3], without any assumption
regarding d →+∞.
5.2
Optimality of the Constants
The idea here is to compare the constants σc, σs, σ
′
c and σ
′
s for some functions
that are not quadratic, in order to test their optimality (whereas these constants

522
A. Auger, C. Le Bris, and M. Schoenauer
are known to be optimal in the case of quadratic functions, see section 3.3,
where optimal means here that theses constants are the limit values between
convergence and divergence.)
First, we need to circumvent a diﬃculty. Indeed, when evaluating E(f(Xn))
with the Monte Carlo method, the relative error given by the Central Limit
Theorem (

V ar(f(Xn))
1.96
√
KE(f(Xn))) grows geometrically with the number of
generations n (the exact computation can be made easily on the sphere func-
tion). On the other hand, that of evaluating E(ln(f(Xn))) decreases in
1
√n.
Hence, all numerical tests have been performed on the process ln(f(Xn)). This
fact in turn requires to come back to the convergence analysis. Indeed, it turns
out that the arguments used to treat the minimization of f also hold for the min-
imization of ln(f). Of course, since the a.s. convergence of f(Xn) implies that of
ln(f(Xn)), we know suﬃcient conditions for such a convergence. But, more than
that, ln(f(Xn) converges in the same fashion and under the same conditions as
f(Xn) with an arithmetic rate replacing the geometric rate of Theorems 4 and
6.
Only numerical results concerning the case H(x) = |f ′(x)| will be shown here.
The functions fM, deﬁned by equation (16) below, are examples among the
class of non symetrical functions satisfying both Assumptions (H2) and (H3)
that will be used for all experiments (where M > 0 is the value used in Assump-
tion (H2)-(ii)).
fM(x) = M
2

x2
ifx < 0
x arctan(x) ifx > 0
(16)
Figure 1 (b) plots 1
K
K

k=1
ln(f2(Xn
k )) against the number of generations for
diﬀerent values of σ. The relative error E(ln(f2(Xn)))−1
K
K
k=1 ln(f2(Xn
k ))
1
K
K
k=1 ln(f2(Xn
k ))
given by
equation (15), is here bounded by 0.01. This corroborates the linear rate of
convergence predicted by our theoretical study.
0
1
2
3
σ
−3
−1
1
3
5
M=2 ln(1 + σ  C h(σ, λ))
M=2 numerical speed
M=8 ln(1 + σ  C h(σ, λ))
M=8 numerical speed
0
5
10
15
λ
0
2
4
6
8
10
numerical σc(λ)
theoretical σc(λ)
(a)
(b)
Fig. 2. (a) Theoretical and numerical speeds of convergence for functions f2 and f8.
(b) Numerical σ
′
cnum(λ) and theoretical σ
′
c(λ).

Dimension-Independent Convergence Rate for Non-isotropic (1, λ) −ES
523
Figure 2 (a) plots the slopes of those linear functions (determined using
linear regressions), and the theoretical values σCh(σ, λ, α, M), for λ = 4 and
for both functions f2 and f8. Both curves have the same shapes. Moreover, on
these functions, the theoretical bounds indeed underestimate the threshold, as
expected.
Studying only fonction f2, the intersection between the theoretical curve and
the x-axis gives a numerical approximation σ
′
c(4) ≈1.4 of the theoretical value
σc(4) – and in the sequel, σ
′
cnum(4) will denote the intersection between the
experimental curve and the x-axis. From Figure 2 (a), it comes that σ
′
cnum(4) ≈
3.1. Deﬁning similarly σ
′
snum(4) as the critical point of the the numerical curve,
it may also be noted on the same Figure that σ
′
s(4) ≤σ
′
snum(4).
It may be observed from the same Figure 2 (a) that both theoretical and
numerical curves present the same scaling transformation when M is increased
– even though the theoretical bound still seems pessimistic. Last, Figure 2 (b)
shows, for function f2, the numerical σ
′
cnum(λ) and theoretical σ
′
c(λ) for λ =
2, ..., 13. Both are linear increasing functions in λ.
6
Conclusions and Perspectives
Convergence results and geometrical convergence rates for adaptive (1, λ) −ES
have been proved for a sub-class of C2 functions. The optimality of the critical
value for the step size and the resulting convergence rate have been proved for
the sphere function and numerical experiments have demonstrated their validity
for more general functions. The extension of the results to the d-dimensional
case with a non-isotropic ES algorithm (14) leads to a critical value of the step-
size and a convergence rate that are independent of the dimension, improving
over previous work. On-going work is concerned with relaxing the regularity
and convexity assumptions: it should be possible to nevertheless obtain. similar
results for convergence and convergence rates. In addition on can envision the
extension to a more practically useful algorithm, where the step-size is adapted
proportionally to |f(x) −f ∗| (where f ∗is the value at the global optimum).
However, the d-dimension case of this latter algorithm will probably lead to
dimension-dependent convergence rate. Finally, similar analysis should be possi-
ble for self-adaptive (1, λ) −ES, but probably requiring regularity assumptions
on the objective function.
References
1. A.Auger. ES, th´eorie et applications au contrˆole en chimie. PhD thesis, Universit´e
Paris 6, in preparation.
2. A. Auger, C. Le Bris, and M. Schoenauer. Rigorous analysis of some simple adap-
tative es. Technical Report INRIA, http://cermics.enpc.fr/∼auger/.
3. H.-G. Beyer. The Theory of Evolution Strategies. Springer, Heidelberg, 2001.
4. A. Bienven¨ue and O. Fran¸cois.
Global convergence for evolution strategies in
spherical problems: Some simple proofs and pitfalls. Submitted, 2001. http://www-
lmc.imag.fr/lmc-sms/Alexis.Bienvenue/.

524
A. Auger, C. Le Bris, and M. Schoenauer
5. J.M DeLaurentis, L. A. Ferguson, and W.E. Hart. On the convergence properties
of a simple self-adaptive evolutionary algorithm. In W.B. Langdon & al., editor,
Proceedings of the Genetic and Evolutionary Conference, pages 229–237. Morgan
Kaufmann, 2002.
6. A. E. Eiben, R. Hinterding, and Z. Michalewicz. Parameter control in Evolutionary
Algorithms. IEEE Transactions on Evolutionary Computation, 3(2):124, 1999.
7. I. Rechenberg. Evolutionstrategie: Optimierung Technisher Systeme nach Prinzip-
ien des Biologischen Evolution. Fromman-Hozlboog Verlag, Stuttgart, 1973.
8. G. Rudolph. Convergence of non-elitist strategies. In Z. Michalewicz, J. D. Schaﬀer,
H.-P. Schwefel, D. B. Fogel, and H. Kitano, editors, Proceedings of the First IEEE
International Conference on Evolutionary Computation, pages 63–66. IEEE Press,
1994.
9. G. Rudolph. Convergence rates of evolutionary algorithms for a class of convex
objective functions. Control and Cybernetics, 26(3):375–390, 1997.
10. H.-P. Schwefel. Numerical Optimization of Computer Models. John Wiley & Sons,
New-York, 1981. 1995 – 2nd edition.
11. M.A. Semenov.
Convergence velocity of an evolutionary algorithm with self-
adaptation. In W.B. Langdon & al., editor, Proceedings of the Genetic and Evolu-
tionary Conference, pages 210–213. Morgan Kaufmann, 2002.
12. D. Williams.
Probability with Martingales.
Cambridge University Press, Cam-
bridge, 2000.
13. G. Yin, G. Rudolph, and H.-P Schwefel. Analysing (1, λ) evolution strategy via
stochastic approximation methods. Evolutionary Computation, 3(4):473–489, 1996.

The Steady State Behavior of (µ/µI, λ)-ES on
Ellipsoidal Fitness Models Disturbed by Noise⋆
Hans-Georg Beyer and Dirk V. Arnold
Department of Computer Science XI,
University of Dortmund, D-44221 Dortmund, Germany
{hans-georg.beyer, dirk.arnold}@cs.uni-dortmund.de
Abstract. The method of diﬀerential-geometry is applied for deriving
steady state conditions for the (µ/µI, λ)-ES on the general quadratic test
function disturbed by ﬁtness noise of constant strength. A new approach
for estimating the expected ﬁnal ﬁtness deviation observed under such
conditions is presented. The theoretical results obtained are compared
with real ES runs showing a surprisingly excellent agreement.
1
Introduction
Understanding the impact of noise on the optimization behavior of evolutionary
algorithms (EAs) is of great interest: There is a certain beliefe that EAs are
especially good at coping with noisy information due to the use of a population
of candidate solutions. There is empirical evidence as well as some theoretical
support for this beliefe [3]. Furthermore, noise models on the level of the control
parameters to be optimized, also called actuator noise models in [11], are of
interest in the context of robust optimization [17,18,12].
While there is a need for a deeper understanding of the behavior of EAs on
such noisy problems, a theoretical analysis is still at its beginning. Up to now,
only the behavior of evolution strategies (ES) on the sphere model has been
analyzed [1]. Performing similar analyses on other test functions still remain to
be done. However, such analyses starting from scratch are expensive. Therefore,
it would be desirable to use results obtained from the sphere theory as a starting
point for deriving statements on the behavior of ES on other test functions.
This article is exactly in that spirit by taking up the thread from [8] where
(1, λ)-ES has been considered. First, it applies the diﬀerential-geometrical model
[7] in order to derive the condition for the zero progress rate in recombinant ES on
general quadratic models disturbed by ﬁtness noise of constant strength. Second,
it provides a new and simple but surprisingly accurate method for estimating
the expected ﬁnal ﬁtness deviation observed under such conditions.
The paper is organized as follows. After introducing the general quadratic test
function disturbed by ﬁtness noise we will determine the steady state condition
⋆This work was supported by the Deutsche Forschungsgemeinschaft (DFG), grant
Be1578/6-3, and by the Collaborative Research Center (SFB) 531.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 525–536, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

526
H.-G. Beyer and D.V. Arnold
starting from the standard noisy sphere model. Then we will provide the new
approach for determining the expected ﬁnal ﬁtness deviation. The predictions of
this model will be compared with (µ/µI, λ)-ES runs. In the concluding section
an outlook will be given emphasizing the potential of the methods presented.
2
The Steady State Condition of (µ/µI, λ)-ES on Noisy
Quadratic Functions
2.1
The General Quadratic Fitness Noise Model
We consider the general quadratic ﬁtness model based on the quality function
Qg(y) := b
Ty −y
TQy
(1)
where b and y are N-dimensional real-valued vectors and Q is a symmetric,
(w.l.o.g.) positive deﬁnite matrix. Given an object vector y the actually observed
objective value, i.e. ﬁtness Fng, is disturbed by Gaussian noise of strength σδ
Fng(y) := Qg(y) + N(0, σ2
δ).
(2)
It is assumed that σδ is constant for each single generation. That is, all oﬀspring
within the same generation experience the same noise strength.
2.2
Determining the Steady State Condition
It is a common phenomenon that EA optimizing ﬁtness functions disturbed by
noise of constant strength exhibit some kind of steady state behavior (after a
certain transient time of approaching the optimum) which is – on average – away
from the optimal solution [6]. If this steady state regime has been reached, the
expected ﬁtness improvement will be zero. In order to determine the steady state
condition of this behavior, we will reconsider the standard noisy sphere model
and apply the diﬀerential-geometrical model [7] to it.
Results from the Sphere Model Theory. The qualitative properties of an
ES can be characterized by evolution criteria [7, p. 90] which describe the ap-
poach toward the optimum in terms of inequalities in the space of the endogenous
strategy parameters such as the mutation strength and the noise strength. This
concept has been developed for the (1, λ)-ES on the noisy sphere model
Fnsp(y) := f(∥y∥) + N(0, σ2
δ),
f = f(r) monotonic function,
(3)
in [5] and recently extended for the (µ/µI, λ)-ES in [2]. The asymptotically
correct (N →∞) evolution criterion reads
σ∗2
δ + σ∗2 ≤(2µcµ/µ,λ)2
(4)
where

The Steady State Behavior of (µ/µI, λ)-ES on Ellipsoidal Fitness Models
527
σ∗= σ N
R
and
σ∗
δ = σδ
N
R|f ′|
with
f ′ = df
dr

r=R
(5)
are the normalized mutation strength (isotropic mutations assumed) and the
normalized noise strength, respectively. R := ∥yp∥is the distance of the parental
centroid (center of mass of the µ parents ym) to the optimum, and cµ/µ,λ is the
progress coeﬃcient (see e.g. [7, p. 247]).
Criterion (4) characterizes those endogenous strategy parameter states which
guarantee convergence toward the optimum. That is, when the “<” relation is
fulﬁlled in (4), the expected R value decreases from one generation to the next.
Criterion (4) necessarily implies
σ∗
δ ≤2µcµ/µ,λ.
(6)
Using (6) and the σ∗
δ deﬁnition in (5), the evolution criterion becomes
R|f ′| ≥
σδN
2µcµ/µ,λ
.
(7)
If the equal sign holds in (4) and (6, 7), respectively, the expected R value
remains constant (from one generation to the next) and the evolution stagnates
(on average). The latter case is of particular interest because it appears as the
steady state of the ES working in a ﬁtness environment with constant noise
strength σδ = const. Considering the special case
f(r) = βrα,
(8)
(7) can be solved for R yielding for the steady state
R ≥
α

σδN
2α|β|µcµ/µ,λ
=: R∞.
(9)
It appears that R∞in (9) provides a good approximation for the expected steady
state value of R in ES runs. This is so because the standard mutation strength
adaptation techniques yield comparatively small normalized mutation strengths
σ∗. This holds for the mutative σ self-adaptation ES (σSA-ES) as well as for the
cumulative step-size adaptation ES (CSA-ES).
Distorting the Sphere – the General Quadratic Model. In order to trans-
fer the idea behind the evolution criterion (7) to the general quadratic case (2),
we have to introduce the generalized quantities for R and f ′. This step is by
analogy. A mathematically rigorous proof for the correctness of this step is still
pending, however, there is experimental evidence for its accuaracy (see Sec-
tion 2.4).
The analogue of the absolute value of the ﬁrst derivative f ′ in the RN space
is the length of the gradient. Similarly, the radius R must be replaced by the
diﬀerential-geometrical mean radius R. Calculating the gradient in (1) yields
∇Qg = b −2Qy.
(10)

528
H.-G. Beyer and D.V. Arnold
Since the optimal state ˆy is given by ∇Qg = 0, we obtain from (10) b = 2Qˆy
and therefore we get
∇Qg = 2Q(ˆy −y) =: a.
(11)
Using the mean radius formula from [7, p.46], one gets
R = N −1
2
∥a∥
Tr[Q] −aTQ a
∥a∥2

.
(12)
In order to obtain the necessary evolution criterion we now substitute ∥∇Qg∥
for |f ′| and R for R in (7). After rearranging terms and considering N →∞,
the evolution criterion reads
∥a∥2
Tr[Q] −aTQ a
∥a∥2

≥
σδ
µcµ/µ,λ
.
(13)
Provided that the Rayleigh quotient aTQa/∥a∥2 can be neglected, this expres-
sion can be further simpliﬁed using (11). One obtains
∥Q(ˆy −y)∥2 ≥σδTr[Q]
4µcµ/µ,λ
.
(14)
Unlike the sphere model where the steady state is characterized by a constant
(expected) residual distance to the optimal state, the general case is character-
ized by y-states located on an ellipsoidal hypersurface (equal sign in (14)). It
is important to realize that these ellipsoidal hypersurfaces are not geometrically
similar to the ellipsoid deﬁned by Qg(y) = const. (in contrast to the sphere).
2.3
Estimating the Expected Stationary Fitness Error
As we have seen in Section 2.2, noisy ﬁtness information implies a localization
error of the optimizer in the y (object) parameter space. Therefore, choosing a
parental state yp produced by the ES after reaching the vicinity of the steady
state regime results in a yp ̸= ˆy. Thus, the actually obtained undisturbed ob-
jective function value Q = Qg(yp) will also deviate from the optimum value
ˆQ = Qg(ˆy). Since ∆Q := ˆQ −Qg(yp) is a random variate one can ask for its
expected value and its variance.
A ﬁrst attempt for estimating ∆Q by neglecting its random character has
been presented in [8]. The idea was to use the respective stationarity condition
(similar to Eq. (14) with µ = 1) as a constraint on the optimization of Qg
given by (1). While this approach yielded a ﬁrst rough lower bound on ∆Q, the
predicted strong dependency of the Q deviation on the largest eigenvalue of Q
and Tr[Q] was not observed in experiments. Astonishingly, one observed ∆Q
values the expected values of which were almost independent of the Q matrix.

The Steady State Behavior of (µ/µI, λ)-ES on Ellipsoidal Fitness Models
529
We will now present an approach for estimating the expected value of ∆Q
in accordance with the experimental observations mentioned above. In order to
simplify the formulae, we will ﬁrst switch to an appropriate coordinate system
by performing a principal axes transformation. Let ei (i = 1, . . . , N) be the
normalized eigenvectors and qi the corresponding eigenvalues of Q. Eq. (1) can
be transformed using the completeness condition 1 = N
i=1 eieT
i into
Qg(y) :=
N

i=1
(biyi −qiy2
i )
(15)
with bi = eT
i b and yi = eT
i y. Performing quadratic completion in (15) yields
Qg(y) :=
N

i=1
b2
i
4qi
−
N

i=1
qi

yi −bi
2qi
2
.
(16)
One can easily prove that the optimum state of (15) is given by
ˆyi = bi
2qi
and
ˆQ = max[Qg] =
N

i=1
b2
i
4qi
.
(17)
Thus, using Eq. (16) ∆Q can be expressed in terms of
∆Q = ˆQ −Qg(y) =
N

i=1
qi(yi −ˆyi)2.
(18)
As one can see, the principal axes transformation decomposes ∆Q in a sum of
N independent ﬁtness contributions fi
∆Q =
N

i=1
fi
where
fi = qi(yi −ˆyi)2.
(19)
Using the same transformation, the evolution criterion (14) can be expressed
similarly. Since ∥Q(ˆy −y)∥2 = (ˆy −y)TQ2(ˆy −y) we immediately obtain
∥Q(ˆy −y)∥2 =
N

i=1
q2
i (yi −ˆyi)2
(20)
and therefore with (14)
N

i=1
q2
i (yi −ˆyi)2 ≥σδTr[Q]
4µcµ/µ,λ
.
(21)
We are now at that position where we can formulate the core ideas for the
derivation of the expected Q deviation. At ﬁrst we note that condition (21) does

530
H.-G. Beyer and D.V. Arnold
hold for all states yi. Since yi are random variates, we can take the expected
value in (21) leading to
N

i=1
q2
i E[(yi −ˆyi)2] ≥σδTr[Q]
4µcµ/µ,λ
.
(22)
Provided that ˆyi = E[yi], the E[(yi −ˆyi)2] expressions can be interpreted as the
variances of the respective yi variates. This is admissible for the steady state
because of symmetry of the yi states in (21). In other words, after reaching the
vicinity of the steady state, the yi ﬂuctuate around the optimizer state ˆyi.
Consider the expected value of ∆Q. Using (19) we have
E[∆Q] =
N

i=1
E[fi]
where
E[fi] = qiE[(yi −ˆyi)2].
(23)
Now comes the crucial assumption which is quite similar to the equipartition the-
orem in statistical thermodynamics: Each degree of freedom in (23) contributes
on average the same eﬀect to the whole system. That is,
Equipartition Assumption:
∀i, j : E[fi] = E[fj].
(24)
It is quite clear that this assumption can only hold under equilibrium conditions,
i.e. after reaching the steady state. However, considering the steady state, it
should also be clear that the E[fi] = E[fj] assumption is a natural one: First,
the mutations generating new yi states are not directed. Second, selection only
“sees” the whole ﬁtness. Therefore it cannot prefer a speciﬁc fi degree and the fi
degrees ﬂuctuate independently of each other. If a speciﬁc fi degree dominated
the others (i.e. having had a much larger E[fi]) then this would mean that its fi
contributions to the actual Q values were much higher. Such states, however, are
likely to go extinct because selection prefers y realizations with lower Q values.
Third, since the yi states ﬂuctuate independently around the optimum state ˆyi,
selection does not prefer any speciﬁc y direction (if this were not the case we
would not be at the steady state but – on average – still move through the search
space in a speciﬁc direction).
Accepting the validity of (24) we obtain with (23) E[fi] = E[∆Q]/N and
therefore
E[(yi −ˆyi)2] = E[∆Q]
Nqi
.
(25)
Taking into account that at the steady state E[yi] = ˆyi does hold (recall the
discussion above), (25) is also the variance of yi
Var[yi] = E[∆Q]
Nqi
.
(26)
After insertion of (25) into the evolution criterion (22) we end up with a sur-
prisingly simple condition (recall that 
i qi = Tr[Q])
E[∆Q] ≥
σδN
4µcµ/µ,λ
.
(27)

The Steady State Behavior of (µ/µI, λ)-ES on Ellipsoidal Fitness Models
531
As can be checked by experiments (see Section 2.4), the equal sign in (27) pre-
dicts the average steady state ∆Q well as long as the mutation strength of the ES
is controlled appropriately. The most astonishing message from (27) is the inde-
pendence of E[∆Q] on the Q matrix. This was already observed in (1, λ)-ES runs
in [8]. However, the reason for this interesting behavior remained obscure. Now
we are able to explain this behavior by the equipartition eﬀect which decomposes
the (arbitrarily oriented) ellipsoid into its principal ﬁtness components.
Inserting (27) into (26) yields an estimate for the object parameter ﬂuctu-
ations at the steady state. Since the steady state is characterized by the equal
sign in (27), we obtain under steady state conditions
Var[yi] =
σδ
4µcµ/µ,λqi
.
(28)
This result is also in accordance with experiments. The main conclusion that can
be drawn from (28) is that yi parameter ﬂuctuations decrease with the increase
of the corresponding eigenvalue qi. This is reasonable: A large eigenvalue qi
(compared to a smaller qj) results in a higher sensitivity of the ﬁtness on the
particular parameter space direction ei. That is, it produces (on average) a
larger deviation from the optimal ﬁtness value. Such deviations, however, are
singled out by the (µ, λ) selection, only small deviations will survive. On the
other hand, small eigenvalues reduce the inﬂuence of the yi ﬂuctuations on the
ﬁtness. Therefore, y ﬂuctuations in such e-directions will be larger.
2.4
ES-Dynamics and Comparison with Experiments
The dynamical behavior of the ES maximizing the noisy function class (1), (2)
has been investigated on three ellipsoidal test functions
Qg1(y) = −
N

i=1
iy2
i ,
(Q)ij = iδij,
(qi = i),
(1.29a)
Qg2(y) = −
N

i=1
i2y2
i ,
(Q)ij = i2δij,
(qi = i2),
(1.29b)
Qg3(y) = −
N

j=1
 j

i=1
yi
2
,
(Q)ij = min[N −i + 1, N −j + 1],
(1.29c)
for dimensionalities N = 30 and 100. While Qg1(y) and Qg2(y) deﬁne axis-
parallel ellipsoids, the third function, also known as “Schwefel’s” function, has a
certain non-parallel orientation. Since we are using isotropic mutations (sphere-
symmetrical mutations) the orientation of the ellipsoids does not aﬀect the per-
formance of the ES on these test functions. However, Qg3(y) has the peculiarity
that the eigenvalue spectrum of Q possesses a dominating eigenvalue. Thus the
shape of this ellipsoid resembles a distorted discus. This might inﬂuence the dy-
namical behavior, however, in the expriments performed using the noise model
(2) no peculiarities concerning the steady-state behavior have been observed.

532
H.-G. Beyer and D.V. Arnold
Diﬀerences are observed, however, concerning the dynamic behavior of the
diﬀerent mutation strength σ control rules used. We have tested the standard
mutative σ self-adaptation (σSA, see e.g. [4]) and the cumulative step-length
adaptation (CSA) proposed by Gawelczyk, Hansen, and Ostermeier [14,15,16]
without covariance matrix adaptation, i.e. using isotropic mutations. Figures 1a–
d show the typical behaviors. Both ES versions end up with a steady state be-
havior where the ﬁtness values are in expectation away from the global optimum
∆Q = 0, i.e. E[∆Q] > 0. This is the typical behavior when noise is involved in
the ﬁtness evaluations. Clearly, the main aim is to have E[∆Q] as small as pos-
sible. When comparing the resulting steady-state E[∆Q] in Fig. 1c,d one notices
that the CSA-ES yields a much larger E[∆Q] than the σSA-ES. From this point
of view, the σSA-ES should be preferred. However, as one can see this is brought
at the expense of a slower approach to the steady state. Comparing the steady
state behavior of the two ES types on the two test functions (1.29a) and (1.29b)
one also sees that, using CSA-ES, the eﬀect of larger E[∆Q] gets larger with
 0.0001
 0.001
 0.01
 0.1
 1
 10
 100
 1000
 0
 1000
 2000
 3000
 4000
 5000
 6000
∆Q
σ
g
 0.0001
 0.001
 0.01
 0.1
 1
 10
 100
 1000
 0
 1000
 2000
 3000
 4000
 5000
 6000
∆Q
σ
g
a) (20/20I, 60)-σSA-ES on Qg1(y)
b) (20/20I, 60)-CSA-ES on Qg1(y)
 0.0001
 0.001
 0.01
 0.1
 1
 10
 100
 1000
 10000
 100000
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
∆Q
σ
g
 0.0001
 0.001
 0.01
 0.1
 1
 10
 100
 1000
 10000
 100000
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
∆Q
σ
g
c) (20/20I, 60)-σSA-ES on Qg2(y)
d) (20/20I, 60)-CSA-ES on Qg2(y)
Fig. 1. Evolution dynamics by (20/20I, 60)-ES on (1.29a, b) (N = 30) using mutative
self-adaptation (σSA, Figs. a and c) and cumulative step length adaptation (CSA, Figs.
b and d). The CSA exhibits premature convergence on test function Qg2(y) (Fig. d).
As noise strength σδ = 1 has been chosen.

The Steady State Behavior of (µ/µI, λ)-ES on Ellipsoidal Fitness Models
533
increasing non-sphericity.1 The reason for this undesirable behavior can be ex-
plained when considering the mutation strengths σ actually realized during the
evolution. While the σSA-ES produces a quasi-constant steady state mutation
strength, the CSA-ES produces an almost random walk like σ behavior on the
logarithmic scale with very small σ values. That is, the CSA σ control rule pro-
duces a nearly premature convergence behavior and the ES is not able to further
evolve towards the optimizer state. The reason for this – at ﬁrst glance – aston-
ishing behavior can be traced back to the optimality condition the CSA control
rule is based upon [13]: Consecutive changes of the parental centroids should be
– on average - perpendicular to each other in order to have maximal progress
on the sphere model. The analysis in [9] shows, however, that this assumption
leads to a wrong adaptation behavior when ﬁtness information is disturbed by
noise. As a result, σ is decreased even though it should be kept nearly constant
(for an in-depth discussion on the sphere model, see [9]).
There is a remedy for the undesired σ decrease in CSA-ES: Simply keep the
mutation strength σ above a certain (but small) limit σ0. Figure 2 shows the
eﬀect of this remedy. The CSA-ES is prevented from premature convergence.
 0.0001
 0.001
 0.01
 0.1
 1
 10
 100
 1000
 0
 1000
 2000
 3000
 4000
 5000
 6000
∆Q
σ
g
 0.0001
 0.001
 0.01
 0.1
 1
 10
 100
 1000
 10000
 100000
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
∆Q
σ
g
Fig. 2. Evolution dynamics of the (20/20I, 60)-CSA-ES keeping σ explicitly above
σ0 = 0.01. Left: test function (1.29a), N = 30; right: test function (1.29b), N = 30. As
noise strength σδ = 1 has been chosen.
The problem is, however, that ﬁxing σ0 is a diﬃcult task and also that the
approach to the steady state is slowed down. Therefore, this method cannot be
recommended as a clever strategy. In the following we will not consider the CSA-
ES further because in this article we are mainly interested in the expected steady
state ∆Q. Therefore, our simulations will be performed using the old σSA-ES.
Figure 3 compares the predictive quality of the equal sign in (27) as an esti-
mate for the expected steady state ∆Q. The (µ/µI, 60)-σSA-ES has been used for
the simulations. ∆Q was recorded at each generation after a number of transient
generations g0 by evaluating the (noisy) ﬁtness (1), (2) of the parental centroid
1 This might be an argument for using the covariance matrix adaptation (CMA) [15],
however, this is not the focus of this paper.

534
H.-G. Beyer and D.V. Arnold
using the test functions (1.29a,b,c). The number of generations used for averag-
ing ∆Q is 200,000. The noise strength used is σδ = 1. There is a good agreement
between experiments and the lower bound of E[∆Q] given by the curve obtained
from (27). Recall that the lower bound corresponds to vanishing normalized mu-
tation strength in the original evolution criterion (7). Considering the actually
realized σ values (see, e.g., the ﬁgures on the left-hand sides of Figs. 1 and 2)
one realizes that the σSA-ES exhibits a behavior where σ is obviously that small
at the steady state such that the equal sign in (27) is roughly fulﬁlled. That is
10
20
30
40
50
60µ
1
2
3
4
E[∆Q]
10
20
30
40
50
60µ
2
4
6
8
10
E[∆Q]
a) Qg1(y), N = 30, g0 = 100, 000
b) Qg1(y), N = 100, g0 = 200, 000
10
20
30
40
50
60µ
1
2
3
E[∆Q]
10
20
30
40
50
60µ
2
4
6
8
10
E[∆Q]
c) Qg2(y), N = 30, g0 = 200, 000
d) Qg2(y), N = 100, g0 = 1, 200, 000
10
20
30
40
50
60µ
1
2
3
E[∆Q]
10
20
30
40
50
60µ
2
4
6
8
10
E[∆Q]
e) Qg3(y), N = 30, g0 = 200, 000
f) Qg3(y), N = 100, g0 = 700, 000
Fig. 3. Dependence of the expected steady state ﬁtness error E[∆Q] on the parent
numbers µ = 1, 2, 4, 6, 10, 15, 20, 25, 30, 35, 40, 45, 50, 54, 56, 58, 59 given ﬁxed oﬀspring
number λ = 60. The vertical bars indicate the measured ± standard deviation of ∆Q.
Note, some data points are missing, see explanation in the text.

The Steady State Behavior of (µ/µI, λ)-ES on Ellipsoidal Fitness Models
535
why we observe such a good agreement between theory and experiments. On the
other hand the mutation strength is large enough to ensure convergence to the
vicinity of the steady state described by (27). This is in contrast to the CSA-ES
where the mutation strength goes down very rapidly when reaching the vicinity
of the steady state. Violating the smallness assumption of σ, however, will result
in a similar behavior: The ES cannot approach states which are described by
the equal sign in (27). This can be observed in σSA-ES with µ/λ near 1, i.e.
in strategies with low selective pressure, and can also be seen in the plots: De-
pending on the test function and the dimensionality there are some data points
missing (usually µ = 59 and µ = 58, sometimes even for smaller µ) due to di-
vergence. The behavior of the σSA-ES is diametrically opposite to the CSA-ES
under this condition. Having a very small selection pressure results in an almost
random selection behavior. As has been shown in [10], random selection results
in an exponential increase of the mutation strength of the σSA-ES. Therefore,
one observes a continuously increasing mutation strength if λ −µ is chosen too
small. This eﬀect starts gradually with increasing µ (keeping λ constant) and
can be observed in the experiments presented.
3
Conclusions and Outlook
Using the equipartition assumption we were able to derive a simple formula
which predicts the ﬁnal expected ﬁtness deviation surprisingly well. While the
σSA-ES reaches the predicted ﬁtness deviation, the CSA-ES exhibits premature
convergence on ellipsoidal test function with a high degree of non-sphericity.
Formula (27) can be used for population sizing. In order to get to the opti-
mizer as closely as possible µ/λ = 0.5 should be chosen. Getting to the steady
state as fast as possible, however, requires µ/λ ≈0.27 (sphere model assump-
tion and N →∞, not considered in this paper). Considering the plots in Fig. 3,
µ/λ = 0.3 seems to be a good compromise.
Since both CSA-ES and σSA-ES use isotropic mutations, in a next step
ES with nonisotropic mutations should be investigated. One might expect an
improved ES behavior using covariance matrix adaptation (CMA) [15]. While
the CMA-ES may yield better results than the CSA-ES, theoretically, CMA
can not signiﬁcantly improve the steady state results of the σSA-ES (basically,
CMA-ES transforms Q into another ˜Q, but (27) does not depend on Q or ˜Q at
all). However, we can expect an improved transient behavior (decreasing g0) of
the CMA-ES compared to the ES with isotropic mutations. This remains to be
investigated in the future.
References
1. D. V. Arnold. Noisy Optimization with Evolution Strategies. Kluwer Academic
Publishers, Dordrecht, 2002.
2. D. V. Arnold and H.-G. Beyer. Performance Analysis of Evolution Strategies with
Multi-Recombination in High-Dimensional RN-Search Spaces Disturbed by Noise.
Theoretical Computer Science, 289:629–647, 2002.

536
H.-G. Beyer and D.V. Arnold
3. D. V. Arnold and H.-G. Beyer. A Comparison of Evolution Strategies with Other
Direct Search Methods in the Presence of Noise. Computational Optimization and
Applications, 24:135–159, 2003.
4. T. B¨ack, U. Hammel, and H.-P Schwefel. Evolutionary computation: comments on
the history and current state. IEEE Transactions on Evolutionary Computation,
1(1):3–17, 1997.
5. H.-G. Beyer. Toward a Theory of Evolution Strategies: Some Asymptotical Results
from the (1,+ λ)-Theory. Evolutionary Computation, 1(2):165–188, 1993.
6. H.-G. Beyer. Evolutionary Algorithms in Noisy Environments: Theoretical Issues
and Guidelines for Practice. Computer Methods in Applied Mechanics and Engi-
neering, 186(2–4):239–267, 2000.
7. H.-G. Beyer.
The Theory of Evolution Strategies.
Natural Computing Series.
Springer, Heidelberg, 2001.
8. H.-G. Beyer and D. V. Arnold. Fitness Noise and Localization Errors of the Opti-
mum in General Quadratic Fitness Models. In W. Banzhaf, J. Daida, A.E. Eiben,
M.H. Garzon, V. Honavar, M. Jakiela, and R.E. Smith, editors, GECCO-99: Pro-
ceedings of the Genetic and Evolutionary Computation Conference, pages 817–824,
San Francisco, CA, 1999. Morgan Kaufmann.
9. H.-G. Beyer and D.V Arnold. Qualms Regarding the Optimality of Cumulative
Path Length Control in CSA/CMA-Evolution Strategies. Evolutionary Computa-
tion, 11(1):19–28, 2003.
10. H.-G. Beyer and K. Deb. On Self-Adaptive Features in Real-Parameter Evolution-
ary Algorithms. IEEE Transactions on Evolutionary Computation, 5(3):250–270,
2001.
11. H.-G. Beyer, M. Olhofer, and B. Sendhoﬀ. On the Behavior of (µ/µI, λ)-ES Op-
timizing Functions Disturbed by Generalized Noise. In K. De Jong, R. Poli, and
J. Rowe, editors, Foundations of Genetic Algorithms, 7, San Francisco, CA, 2003.
Morgan Kaufmann. in print.
12. J. Branke. Evolutionary Optimization in Dynamic Environments. Kluwer Aca-
demic Publishers, Dordrecht, 2001.
13. N. Hansen and A. Ostermeier. Adapting Arbitrary Normal Mutation Distributions
in Evolution Strategies: The Covariance Matrix Adaptation. In Proceedings of 1996
IEEE Int’l Conf. on Evolutionary Computation (ICEC ’96), pages 312–317. IEEE
Press, NY, 1996.
14. N. Hansen and A. Ostermeier.
Convergence Properties of Evolution Strategies
with the Derandomized Covariance Matrix Adaptation: The (µ/µI, λ)-CMA-ES.
In H.-J. Zimmermann, editor, 5th European Congress on Intelligent Techniques
and Soft Computing (EUFIT’97), pages 650–654, Aachen, Germany, 1997. Verlag
Mainz.
15. N. Hansen and A. Ostermeier. Completely Derandomized Self-Adaptation in Evo-
lution Strategies. Evolutionary Computation, 9(2):159–195, 2001.
16. A. Ostermeier, A. Gawelczyk, and N. Hansen.
A Derandomized Approach to
Self-Adaptation of Evolution Strategies. Evolutionary Computation, 2(4):369–380,
1995.
17. S. Tsutsui and A. Ghosh. Genetic Algorithms with a Robust Solution Searching
Scheme. IEEE Transactions on Evolutionary Computation, 1(3):201–208, 1997.
18. D. Wiesmann, U. Hammel, and T. B¨ack. Robust Design of Multilayer Optical
Coatings by Means of Evolutionary Algorithms. IEEE Transactions on Evolution-
ary Computation, 2(4):162–167, 1998.

Theoretical Analysis of Simple Evolution
Strategies in Quickly Changing Environments
J¨urgen Branke1 and Wei Wang2
1 Institute AIFB, University of Karlsruhe, 76128 Karlsruhe, Germany
branke@aifb.uni-karlsruhe.de
2 Department of Education Technologies
Nanjing University of Posts and Telecommunications
P.O.Box 73, 38 GuangDong Road, 210003 Nanjing, China
wwang@jlonline.com
Abstract. Evolutionary algorithms applied to dynamic optimization
problems has become a promising research area. So far, all papers in
the area have assumed that the environment changes only between gen-
erations. In this paper, we take a ﬁrst look at possibilities to handle
a change during a generation. For that purpose, we derive an analytical
model for a (1, 2) evolution strategy and show that sometimes it is better
to ignore the environmental change until the end of the generation, than
to evaluate each individual with the most up-to-date ﬁtness function.
1
Introduction
Many optimization problems are dynamic and change over time. A suitable opti-
mization algorithm has to reﬂect these changes by repeatedly adapting the solu-
tion to the changed environment. Evolutionary algorithms (EAs) are inspired by
natural evolution, which can be regarded as adaptation in an inherently dynamic
and stochastic environment. Given this background, EAs seem to be naturally
suited to be applied to dynamic optimization problems, and have already shown
great promise (for an overview on the area, see e.g. [2]).
EAs are iterative algorithms. In each “generation”, a number of new solu-
tions (individuals) are generated, evaluated, and inserted into the population.
So far, at least to the authors’ knowledge, all publications on EAs for dynamic
optimization problems assume that the environment (ﬁtness function) changes
between generations. Although this assumption is convenient, we consider it an
oversimpliﬁcation, because generally the environment is independent of the EA,
and thus can change at any time, i.e. also within a generation.
In this paper, we speciﬁcally address the issue of how to handle an environ-
mental change during a generation. For that purpose, we develop an analytical
model for a (1, 2) evolution strategy applied to the dynamic bit matching prob-
lem. The dynamic bit-matching problem is the dynamic variant of the well known
onemax problem. The goal is to reproduce a binary target template, with the
template changing over time. The ﬁtness is just the number of bits identical
with the template. We will analytically compare two ways to deal with a change
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 537–548, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

538
J. Branke and W. Wang
of the target within a generation (i.e. after the ﬁrst child has been evaluated):
One possibility is to use the new ﬁtness function for the second individual, while
the other possibility would be to ignore the change and use the same old ﬁtness
function also for the second individual.
The ﬁrst approach uses up-to-date ﬁtness information, but potentially suf-
fers from the fact that the selection is based on ﬁtness from two diﬀerent ﬁtness
functions. The second approach deliberately ignores new information about the
enviroonment, but on the other hand, selection chooses between individuals eval-
uated with the same ﬁtness function. Note that the second approach assumes
that the old ﬁtness function can still be used after the environment has changed.
This seems justiﬁed, since in most practical applications, the ﬁtness is evaluated
by simulating the environment on a computer. Continuing to use the old ﬁtness
function then simply means to delay the update of the computer model.
The paper is structured as follows: in the next section, we will brieﬂy mention
a number of papers related to our work. In Section 3 we derive the theoretical
model for the (1, 2) reproduction scheme and compare its performance to the (1+
1) reproduction scheme, assuming the environment changes between generations.
Then, we turn to the issue of changes within a generation and compare the ideas
of always using the up-to-date ﬁtness function or using an old ﬁtness function
for the whole generation for the (1, 2) reproduction scheme. The paper concludes
with a summary and an outlook for future work.
2
Related Work
Our paper is largely based on the work by Stanhope and Daida published in [5,6]
but extends it signiﬁcantly. Stanhope and Daida derive transition probabilities of
the individual’s ﬁtness for a (1 + 1) EA on the dynamic bit-matching problem.
Let us brieﬂy summarize their results, which will serve as a baseline for our
extensions.
We will be using the following notation:
L
:
number of bits in the bit string. In our results reported below,
we generally assume L = 100.
r
:
mutation rate in terms of the number of bits changed by muta-
tion (ﬁxed number, not probability per bit)
d
:
number of bits the target changes
a
:
parent individual
ft(b)
:
ﬁtness of individual b against the t-th target. The ﬁtness after a
change, i.e. against the next target, is usually denoted as ft+1(b).
For reasons of readability, the subscript t is often omitted
mr(b)
:
result of mutating individual b by r bits
The probability that an oﬀspring has ﬁtness x, given that it was generated by
mutating an individual a with ﬁtness f(a), is the probability that the number
of wrong bits mutated minus the number of correct bits mutated is equal to
x −f(a), which can be calculated as

Theoretical Analysis of Simple Evolution Strategies
539
P(f(mr(a)) = x) =

L−f(a)
(x−f(a)+r)/2

f(a)
r−(x−f(a)+r)/2

L
r

(1)
In a (1 + 1) EA, the better of the two individuals (parent and oﬀspring) is
kept, and the probability that the surviving individual has ﬁtness x can thus be
calculated as
P(
max
b∈{a,mr(a)} f(b) = x) =



0
:
x < f(a)
x
i=0 P(f(mr(a)) = i)
:
x = f(a)
P(f(mr(a)) = x)
:
x > f(a)
(2)
To account for changes of the environment, ﬁrst note that a change of the
target of d bits is equivalent (in terms of the individual’s ﬁtness distribution)
to a d bit mutation of the individual. Thus we can calculate the distribution
function of the ﬁtness of a selected individual after an environmental change as:
P(ft+1( arg max
b∈{a,mr(a)}
f(b))= x)=
L

i=0
P(
max
b∈{a,mr(a)} f(b)= i)P(f(md(ci)) = x) (3)
with ft+1 denoting the ﬁtness against the new target (i.e. a target with d bits
changed), and ci denoting any individual with ﬁtness i.
Further related work includes the paper by Droste [4]. There, the expected
time to encounter the optimal solution for the ﬁrst time is derived for a (1+1) EA
on the dynamic bit matching problem. The model is diﬀerent in that it assumes
a mutation probability for each bit instead of a ﬁxed number of bits inverted
as we do. A brief survey on EA approaches to dynamic optimization problems
has been presented e.g. in [1], a thorough treatment of diﬀerent aspects of this
subject can be found in [2].
3
Comparing (1, 2) and (1 + 1) on an Environment
Changing between Generations
Let us now derive similar equations for the (1, 2) reproduction scheme. Note
that although now two new individuals are generated in every iteration, the total
number of evaluations per generation is equal to the (1+1) reproduction scheme
when a change occurs after every generation, because the implicit assumption
above was that the old individual is re-evaluated in every iteration in order to
allow for correct selections.
Ignoring a change of the ﬁtness function for now, the ﬁtness distribution of
the better of the two children can be described as:
P

max
b∈{mr1(a),mr2(a)} f(b) = x
	
= P 2(f(mr(a)) ≤x) −P 2(f(mr(a)) < x) (4)
which can be calculated using Equation 1.

540
J. Branke and W. Wang
The following equation takes a change after each generation into account:
P

ft+1

arg max
b∈{mr1(a),mr2(a)}
f(b)

= x

=
L

i=0
P

max
b∈{mr1(a),mr2(a)} f(b) = i
	
P(f(md(ci)) = x)
(5)
3.1
Optimal Mutation Rate
Figure 1 compares the optimal mutation rate (yielding the highest expected
ﬁtness of the selected individual) for (1 + 1) and (1, 2) depending on the ﬁtness
of the current parent individual and assuming a string length of 100. The results
are identical for a change severity of d = 0, 1, 2, 3. Obviously, for both approaches,
with decreasing current ﬁtness the optimal mutation rate increases quickly, as
there is a higher chance for improvement and a lower chance for destroying
valuable bits.
Since (1 + 1) will never accept an individual worse than the current parent
individual, it is safe to allow some mutation even when the parent has a high
ﬁtness. This is not the case for (1, 2), which risks a signiﬁcant loss in ﬁtness
when mutation is introduced. For the examined case of a string length of 100, it
is optimal to have a mutation rate of 0 as long as the parent’s ﬁtness is greater
or equal to 73, and have a lower mutation rate than (1+1) over the whole range.
It is also interesting to note that at least for (1+1), mutating an even number
of bits is never optimal. The reason is probably that an improvement can only
occur when the child’s ﬁtness is actually greater than the parent’s ﬁtness. With
an even number of bit-ﬂips there is a relatively high probability that the eﬀects of
the diﬀerent bit ﬂips cancel out and the ﬁtness of the individual is not changed at
all (i.e. there can be no improvement). Using an odd number of bit ﬂips “forces”
mutation to change the individual’s ﬁtness. This leads to a larger number of
actually better individuals, while the also larger number of worse individuals
doesn’t matter (since only better individuals are accepted).
3.2
Convergence Plots
The expected ﬁtness distribution of next generation’s parent individual corre-
sponds to a transition matrix of a Markov chain. Assuming e.g. an initial random
individual with ﬁtness 50 and a total string length of 100, we can then compute
the expected ﬁtness of the parent individual over the generations. For the com-
parisons in this section, we used the optimal mutation rate for both approaches.
Figure 2 shows the derived convergence plots for d = 0, 1, 3.
As would be expected, in a stationary environment, (1 + 1) clearly outper-
forms (1, 2). The onemax ﬁtness function has no local optima and thus a local
hill-climber such as (1 + 1) works great. Our analysis here is restricted to the
simple dynamic bit matching benchmark, but it would be interesting to compare

Theoretical Analysis of Simple Evolution Strategies
541
0
10
20
30
40
50
50
55
60
65
70
75
80
85
90
95 100
optimal mutation rate
parent’s fitness
(1+1)
(1,2)
Fig. 1. Optimal mutation rate for (1, 2) and (1 + 1) reproduction depending on the
ﬁtness of the current parent individual. Total string length is assumed to be 100.
50
60
70
80
90
100
0
20
40
60
80 100 120 140
expected fitness
generation
(1+1)
(1,2)
(a) d = 0
50
52
54
56
58
60
62
64
66
68
0
20
40
60
80 100 120 140
expected fitness
generation
(1+1)
(1,2)
(b) d = 1
50
52
54
56
58
60
0
10
20
30
40
50
60
70
expected fitness
generation
(1+1)
(1,2)
(c) d = 3
Fig. 2. Comparison of the convergence curves for (1+1) and (1, 2) for diﬀerent change
severities d. Total string length is assumed to be 100.
the two reproduction schemes also on a more rugged ﬁtness landscape, where
(1 + 1) will get stuck in a local optimum.
As the environment starts to change, it is interesting to see that with in-
creasing dynamism, the exploratory (1, 2) reproduction scheme comes closer and
closer to the exploitatory (1+1) reproduction scheme. In any case, (1, 2) outper-
forms (1+1) while the parental ﬁtness is still low, therefore it would be beneﬁcial
to use (1, 2) in the beginning of a run and then switch to (1 + 1) later on.
4
Change within a Generation
In this section, we will consider the case where the ﬁtness function changes
during a generation of a (1, 2) reproduction scheme, i.e. after the ﬁrst child has
been evaluated. Given this simple framework, we will analytically compare the
two strategies already mentioned in the introduction, namely to evaluate the
two individuals with the respective (diﬀerent) current ﬁtness functions, or to
artiﬁcially delay the change and use the old ﬁtness function also for the second
child.

542
J. Branke and W. Wang
As has been explained in the introduction, each of the above approaches has
its advantages and its drawbacks, and we would like to know which approach is
preferable given speciﬁc circumstances. Let us ﬁrst consider the following illustra-
tive example: Let x1 and x2 be the two children generated, and let f(x1) = 10
and f(x2) = 12 be the respective ﬁtnesses before the environmental change,
ft+1(x1) = 11 and ft+1(x2) = 9 be the respective ﬁtnesses after the environ-
mental change. The approach using the up-to-date ﬁtness will select between
f(x1) = 10 and ft+1(x2) = 9, i.e. correctly select x1 assuming a maximiza-
tion problem. The approach delaying the change will compare f(x1) = 10 and
f(x2) = 12 and select x2, which is actually worse than x1. On the other hand, in
a situation where ft+1(x1) = 8 and ft+1(x2) = 9, the approach using up-to-date
ﬁtness is mistaken, while the approach which delays the change correctly selects
x2.
More generally, delaying the change will make the correct decision as long as
the change does not aﬀect the relative order of the two children. For the dynamic
bit matching problem this is probably the case for very large bit strings and
small mutation rates, because then x1 and x2 are identical for the vast majority
of bits, and a change of the template will most likely aﬀect them in the same
way. In particular, if the environmental change is severe compared to mutation,
the (undesirable) eﬀect of comparing individuals with diﬀerent ﬁtness functions
will “override” the ﬁtness diﬀerence due to mutation, and mistakes are likely. In
the following, we will compare the two strategies analytically.
For the strategy which delays the change, the ﬁtness distribution will be
identical to the case considered in Section 3 with a change at the end of a
generation. The situation for the other approach is depicted in Figure 3: First, a
child is generated and evaluated, then the environment changes, a second child
is generated and evaluated, and the child with the higher ﬁtness is selected.
For analytical comparison, we need the actual ﬁtness of the selected child,
i.e. its ﬁtness in the new environment. If the second child is selected, that is
no problem, since it has already been evaluated against the new environment.
However, if the ﬁrst child is selected, the assigned ﬁtness is outdated, and it has
to be re-evaluated in the new environment (note that this re-evaluation is only
necessary for the theoretical investigation and is not part of the implemented
EA). The two cases are compared in Figure 3.
The diﬃculty is that the change before re-evaluating the ﬁrst child has to be
identical to the one which occurred before the evaluation of the second child. We
have to be able to replicate that change, while at the same time we would like
to continue using the high-level approach from the previous sections, avoiding
to enumerate all possible changes on a bit-level. To solve this diﬃculty, let us
ﬁrst introduce the concept of two-step mutation.
4.1
Two-Step Mutation
Basically, a mutation of r bits can be regarded as ﬁrst mutating s < r bits, and
then mutating the remaining r −s bits on a shorter string, not containing the s
bits mutated ﬁrst (to avoid ﬂipping the same bits twice).

Theoretical Analysis of Simple Evolution Strategies
543
parent
m
m r
d
m r
child 2
child 1
m d
selection
should be equivalent
child 1
reevaluated
x
(a) The ﬁrst child is selected
parent
m
m r
d
m r
selection
child 2
child 1
x
(b) The second child is selected
Fig. 3. Illustration of the process when the ﬁtness function changes after the ﬁrst child
is evaluated, depending on whether (a) the ﬁrst child is selected or (b) the second child
is selected. We would like to derive the probability distribution for the ﬁtness of the
individual marked “x”. Note that there is only one change of the environment, thus
the two changes in (a) need to be identical.
1 1
1
0
0 0 0
1 1
1
0
0 0 0
1 0
1 0
1 0
1 0
1 1
1
0
0 0 0
1 0 0 0
0 0 1
1 0 0 0
0 0 1
...
...
...
s−bit mutation
(r−s) bit mutation on shorter string
split off
rejoin
r−bit mutation
fitness (i+f(a)−s)/2
fitness i
fitness f(a)
fitness j
fitness j−(i−f(a)+s)/2
s bits
r bits
...
...
1
0
r−s bits
0
Fig. 4. Illustration of the concept of two-step mutation. Note that the order of the bits
is irrelevant, thus w.l.o.g. we assume in this ﬁgure that the ﬁrst bits are mutated.
The concept is illustrated in Figure 4: First, we do an ordinary s-bit mutation
on the whole string. Let us assume that the resulting individual has ﬁtness i.
Then, the r −s-bit mutation can be captured by the basic mutation operation

544
J. Branke and W. Wang
described by Equation 1, with the following parameters: The length of the sub-
string is L −s, the mutation rate is r −s, the initial ﬁtness of the substring is
(i + f(a) −s)/2, and the ﬁtness after mutation should be j −(i −f(a) + s)/2
(assuming the ﬁtness of the individual after the whole two-step mutation is equal
to j). We will denote such a mutation on a string of ﬁtness (i + f(a) −s)/2 and
length L−s as mr−s(cL−s
(i+f(a)−s)/2). The probability distribution of the complete
two-step mutation can then be expressed as
P (f(mr(a)) = j) =
min{L,f(a)+s}

i=max{0,f(a)−s}
P (f(ms(a)) = i)
· P

f(mr−s(cL−s
(i+f(a)−s)/2)) = j −i −f(a) + s
2
	
(6)
4.2
Using Two-Step Mutation to Model Change within a
Generation
The above proposed two-step mutation can now be used to recover the change
that has happened before the second child was evaluated, and apply it to the
ﬁrst child as well. The basic idea is to split the mutation into two steps: ﬁrst, the
s bits are mutated that are common to the r-bit mutation and the d-bit change
of the target. Then, the remaining r −s bits for mutation and the remaining
d −s bits to account for the environmental change are applied (see Figure 5 for
illustration). Then, if we would like to apply the environmental change to the
ﬁrst child, we can do so by reversing the eﬀect of the ﬁrst s-bit mutation, and
then applying a d −s bit mutation on a string of length L −r (since this last
mutation has no common bits with the r-bit mutation).
f(a)
i
j
k
n
x
child 1
child2
reevaluated
child
selection
m
m
m
m
m
m
m
r
d
r
d
r−s
d−s
s
Fig. 5. Illustration of the use of two-step mutation for a (1,2) EA with a change of
the environment after the ﬁrst child has been evaluated, and assuming that the ﬁrst
child is selected. The letters inside the circles denote the ﬁtnesses of the corresponding
individuals.

Theoretical Analysis of Simple Evolution Strategies
545
The s-bit mutation and the r −s bit mutation have already been discussed
above. For the d −s bit mutation, the string length is L −r (since it has no
common bits with either the s-bit mutation nor the r −s bit mutation), the
initial ﬁtness is (j +f(a)−r)/2, and the ﬁtness after mutation should be k −i+
(j + f(a) −r)/2 (assuming the ﬁtness of the individual after the whole two-step
mutation is equal to k). Overall,
P((f(md(a)) = k)|(f(mr(a)) = j) ∧(f(ms(a)) = i))
= P

f(md−s(cL−r
(j+f(a)−r)/2)) = k −i + j + f(a) −r
2
	
(7)
Let us ﬁrst consider the case when the ﬁrst child has an observed ﬁtness
equal or better than the second child, and let us assume that in this case, the
ﬁrst child is selected (selecting either child with equal probability in the case of
equal ﬁtness could also be handled, but is omitted here for clarity). As usual, we
would like to calculate the probability that the child (in this case the ﬁrst one)
has ﬁtness x.
The probability that the ﬁrst child is selected can be calculated as
min{L,f(a)+r}

j=max{0,f(a)−r}
P(f(mr(a)) = j)P(f(mr(md(a))) ≤j)
=
min{L,f(a)+r}

j=max{0,f(a)−r}
P(f(mr(a) = j)
·
min{L,f(a)+d}

k=max{0,f(a)−d}
P(f(md(a)) = k)P(f(mr(ck)) ≤j)
(8)
Using two-step mutation, this can be re-formulated as
min{r,d}

s=0
P(v(r, d) = s)
min{L,f(a)+s}

i=max{0,f(a)−s}
P(f(ms(a)) = i)
·
min{L,i+r−s}

j=max{0,i−r+s}
P

f(mr−s(cL−s
(i+f(a)−s)/2)) = j −i −f(a) + s
2
	
·
min{L,i+d−s}

k=max{0,i−d+s}
P

f(md−s(cL−r
(j+f(a)−r)/2)) = k −i + j + f(a) −r
2
	
· P(f(mr(ck)) ≤j)
(9)
where P(v(r, d) = s) denotes the probability that a d-bit mutation and a r-bit
mutation have exactly s common bits and can be calculated as
P(v(r, d) = s) =
L
s

·
L−s
r−s

·
L−r
d−s

L
r

·
L
d

(10)

546
J. Branke and W. Wang
The ﬁtness of the ﬁrst child after re-evaluation is determined by x = f(a)+j+
k−2i. Since x is assumed to be known, k can be replaced by k = x−f(a)+2i−j.
The probability that the ﬁrst child has been selected and has ﬁtness x after
re-evaluation can thus be calculated as
min{r,d}

s=0
P(v(r, d) = s)
min{L,f(a)+s}

i=max{0,f(a)−s}
P(f(ms(a)) = i)
·
min{L,i+r−s}

j=max{0,i−r+s}
P

f(mr−s(cL−s
(i+f(a)−s)/2)) = j −i −f(a) + s
2
	
· P

f(md−s(cL−r
(j+f(a)−r)/2)) = x + i −j + f(a) + r
2
	
· P(f(mr(cx−f(a)+2i−j)) ≤j)
(11)
The second case, namely that the second child has a better observed ﬁtness
and is selected is much easier to handle. Since we don’t have to re-evaluate the
second individual, we just need to calculate the probability that it has ﬁtness x
while the ﬁrst individual has ﬁtness smaller than x. In terms of equations, this
can be expressed as
min{L,f(a)+d}

k=max{0,f(a)−d}
P(f(md(a) = k)P(f(mr(ck)) = x)P(f(mr(a)) < x)
(12)
The total probability of the new parent having ﬁtness x is then just the
probability that the ﬁrst child is selected and has actual ﬁtness x (Equation 11)
plus the probability that the second child is selected and has ﬁtness x (Equation
12). As has been shown in [3], the presented framework can also be adapted to
the general case of (1, λ) evolution strategies with λ > 2.
4.3
Comparisons
The above equations allow us to compare the two strategies, namely to use
the old ﬁtness function for both individuals or to always use the latest ﬁtness
function, in terms of the expected ﬁtness of the next generation’s parent. Again,
we assume a bit string of length 100 for the results reported below.
Figure 6 compares the diﬀerence in expected ﬁtness of the next generation’s
parent individual depending on the current parent individual’s ﬁtness and the
change severity d of the environment. As can be seen, for d = 1 it is always
somewhat better to use the up-to-date information when evaluating individuals.
This can also be derived diﬀerently: The ﬁtness diﬀerence of the two children
before the change can never be equal to 1, since in both mutations, an equal
number of bits are ﬂipped. If the ﬁtness diﬀerence is 0, it is important to know
the eﬀect of the environment to correctly select the better individual. If the
ﬁtness diﬀerence ≥2, a change of a single bit can not reverse the ordering of the
children and thus both ﬁtness evaluation schemes will make the same decision.

Theoretical Analysis of Simple Evolution Strategies
547
However, for d > 1 such a simple analysis is not possible. Clearly according
to Figure 6, with d = 2 or d = 3, using the old environment for the second child
yields better results unless the parent’s ﬁtness is very low. As long as the parent’s
ﬁtness is very low, the optimal mutation rate is very high, and the two children
are likely to diﬀer signiﬁcantly in their true ﬁtness. But with growing parental
ﬁtness and smaller optimal mutation rate, the children’s true ﬁtnesses may be
quite similar, and when a severe change (d > 1) is only taken into account for
one individual, there is the danger that it will “hide” the true ﬁtness diﬀerence,
misleading selection. The “steps” in the lines in Figure 6 correspond to the points
where the optimal mutation probability changes. The eﬀect of diﬀerent ﬁtness
evaluation schemes on the convergence curves can be seen in Figure 7.
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
50
55
60
65
70
75
delta expected fitness
parent’s fitness
d=1
d=2
d=3
Fig. 6. Diﬀerence of expected ﬁtness for
next generation’s parent individual de-
pending on whether new or old ﬁtness
function is used for second child. If value
is greater 0, it is better to use new ﬁtness
function and vice versa.
50
52
54
56
58
60
62
64
0
10
20
30
40
50
60
70
80
expected fitness
generation
d=1, new fitness
d=1, old fitness
d=2, old fitness
d=2, new fitness
d=3, old fitness
d=3, new fitness
Fig. 7. Convergence curves of (1, 2) repro-
duction strategy with either new or old ﬁt-
ness function used for the second child, for
diﬀerent change severities d. Lines are la-
beled in the order they appear in the plot.
5
Conclusion and Future Work
In this paper, we have approached the problem of quickly changing environ-
ments, and in particular the issue of how to handle a change of the ﬁtness
function that occurs within a generation of an evolutionary algorithm. Besides a
general description of the issues and some ideas of how to handle such changes,
we have derived analytic expressions which allow to calculate the probability
distribution for ﬁtnesses of the next generation’s parent individual for the (1,2)
EA on the dynamic bit matching problem. Using this framework, we compared
two strategies to handle changes within a generation, namely to always use the
up-to-date ﬁtness information, or to keep the old ﬁtness function despite the
change of the environment. As has been shown, depending on the current ﬁtness
and the severity of the environmental change, one or the other strategy may be
beneﬁcial.

548
J. Branke and W. Wang
We are currently examining the issue of changes within a generation also from
an empirical point of view, allowing us to look at much more complex problems
and EA variants as suggested in the introduction.
Acknowledgments. We would like to thank Christopher Ronnewinkel for help-
ful discussions in the early phases of this research.
References
1. J. Branke. Evolutionary approaches to dynamic optimization problems - updated
survey. In GECCO Workshop on Evolutionary Algorithms for Dynamic Optimiza-
tion Problems, pages 27–30, 2001.
2. J. Branke. Evolutionary Optimization in Dynamic Environments. Kluwer, 2001.
3. J. Branke and W. Wang.
Theoretical analysis of simple evolution strategies in
quickly changing environments. Technical Report 423, Institut AIFB, University of
Karlsruhe, 76128 Karlsruhe, Germany, 2002.
4. S. Droste. Analysis of the (1 + 1) EA for a dynamically changing onemax-variant.
In Congress on Evolutionary Computation, pages 55–60, 2002.
5. S. A. Stanhope and J. M. Daida. Optimal mutation and crossover rates for a genetic
algorithm operating in a dynamic environment. In Evolutionary Programming VII,
volume 1447 of LNCS, pages 693–702. Springer, 1998.
6. S. A. Stanhope and J. M. Daida. Genetic algorithm ﬁtness dynamics in a changing
environment. In Congress on Evolutionary Computation, volume 3, pages 1851–
1858. IEEE, 1999.

Evolutionary Computing as a Tool for Grammar
Development
Guy De Pauw
CNTS – Language Technology Group
UIA – University of Antwerp
Antwerp – Belgium
guy.depauw@ua.ac.be
Abstract. In this paper, an agent-based evolutionary computing technique is in-
troduced, that is geared towards the automatic induction and optimization of
grammars for natural language (grael). We outline three instantiations of the
grael-environment: the grael-1 system uses large annotated corpora to bootstrap
grammatical structure in a society of autonomous agents, that tries to optimally
redistribute grammatical information to reﬂect accurate probabilistic values for
the task of parsing. In grael-2, agents are allowed to mutate grammatical infor-
mation, effectively implementing grammar rule discovery in a practical context.
Finally, by employing a separate grammar induction module at the onset of the
society, grael-3 can be used as an unsupervised grammar induction technique.
1
Introduction
An important trend in the ﬁeld of Machine Learning sees researchers employing com-
binatory methods to improve the classiﬁcation accuracies of their algorithms. Natural
language problems in particular beneﬁt from combining classiﬁers to deal with the large
datasets and expansive arrays of features that are paramount in describing this difﬁcult
and disparate domain that typically features a considerable amount of sub-regularities
and exceptions [1]. Not only system combination and cascaded classiﬁers are well-
established methods in the ﬁeld of Machine Learning for natural language [2,3], also
the techniques of bagging and boosting [4] have been used successfully on a number of
natural language classiﬁcation tasks [5,6]. These techniques hold in common that in no
way do they alter the actual content of the information source of the predictor. Simply
by re-distributing the data, different resamplings of the same classiﬁer are generated to
create a combination of classiﬁers.
The ﬁeld of evolutionary computing has been applying problem-solving techniques
that are similar in intent to the aforementioned Machine Learning recombination meth-
ods. Most evolutionary computing approaches hold in common that they try and ﬁnd
a solution to a particular problem, by recombining and mutating individuals in a soci-
ety of possible solutions. This provides an attractive technique for problems involving
large, complicated and non-linearly divisible search spaces. The evolutionary computing
paradigm has however always seemed reluctant to deal with issues of natural language
syntax. The fact that syntax is in essence a recursive, non-propositional system, dealing
with complex issues such as long-distance dependencies and constraints, has made it
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 549–560, 2003.
c⃝Springer-Verlag Berlin Heidelberg 2003

550
G. De Pauw
difﬁcult to incorporate it in typically propositional evolutionary systems such as genetic
algorithms. Most GA syntactic research so far has focused on non-linguistic data, with
some notable exceptions [7,8,9,10]. Yet none of these systems are suited to a generic
grammar optimization task, mainly because the grammatical formalism and evolutionary
processes underlying these systems are designed to ﬁt a particular task, such as infor-
mation retrieval [11]. Yet so far, little or no progress has been achieved in evaluating
evolutionary computing as a tool for the induction or optimization of data-driven parsing
techniques.
The grael (GRAmmar EvoLution) framework [12] attempts to combine the sensi-
bilities of the recombination machine learning methods and the attractive evolutionary
properties of the concepts of genetic programming. It provides a suitable framework
for the induction and optimization of any type of grammar for natural language in an
evolutionary setting. In this paper we want to provide a general overview of grael
as a natural language grammar development technique. We will ﬁrst identify the basic
problem in Section 2, after which we outline the general architecture of the grael envi-
ronment in Section 3. Next, we will introduce three different instantiations of the grael
environment: in GRAEL-1 (Section 4) large annotated corpora are used to bootstrap
grammatical structure in a society of agents, who engage in a series of communicative
attempts, during which they redistribute grammatical information to reﬂect optimized
probabilistic values for the task of parsing. In GRAEL-2 (Section 5), agents are allowed
to mutate grammatical information, effectively implementing grammar rule discovery
in a practical context. Finally, we look at grael-3 in Section 6, which provides a method
for unsupervised grammar induction.
2
Natural Language Grammar Development
Syntactic processing has always been deemed to be paramount to a wide range of ap-
plications, such as machine translation, information retrieval, speech recognition and
the like. It is therefore not surprising that natural language syntax has always been one
of the most active research areas in the ﬁeld of language technology. All of the typi-
cal pitfalls in language like ambiguity, recursion and long-distance dependencies, are
prominent problems in describing syntax in a computational context. Historically, most
computational systems for syntactic parsing, employ hand-written grammars, consisting
of a laboriously crafted set of grammar rules to apply syntactic structure to a sentence1.
But in recent years, a lot of research efforts are trying to automatically induce workable
grammars from annotated corpora, i.e. large collections of pre-parsed sentences [13].
Since the tree-structures in these annotated corpus already implicitly contain a grammar,
it is a relatively trivial task to induce a large-scale grammar and parser that is able to
acquire reasonably high parsing accuracies on a held-out set of data [14,15,16].
Yet, data-analysis of the output generated by these parsers still brings to light fun-
damental limitations to these corpus-based methods. Even though they generally pro-
vide a much broader coverage as well as higher accuracy than hand-built grammars,
corpus-induced grammars will still not hold enough grammatical information to provide
1 Syntactic structure is typically presented as a parse tree, such as the ones in Figure 1.

Evolutionary Computing as a Tool for Grammar Development
551
structures for a large number of sentences in language, as some rules that are needed
to generate the correct tree-structures are not induced from the original corpus. But
even if there were such a thing as a full-coverage corpus-induced grammar, performance
would still be limited by the probabilistic weights attributed to its rules. The grael
system described in this paper tries to alleviate the problems inherent to corpus-induced
grammars, by establishing a distributed evolutionary computing method for grammar
induction and optimization. Generally, grael can be considered as a system that allows
for the simultaneous development of a range of alternative solutions to a grammatical
problem, optimized in a series of practical interactions in a society of agents controlled
by evolutionary parameters.
3
Grammar Evolution
A typical grael society consists of a population of agents in a virtual environment, each
of which holds a number of structures that allow them to generate sentences as well as an-
alyze other agents’sentences. These grammars are updated through an extended series of
inter-agent interactions, using a form of error-driven learning. The evolutionary param-
eters are able to deﬁne the content and quality of the grammars that are being developed
over time, by imposing ﬁtness functions on the society. By embedding the grammars in
autonomous agents, grael ensures that the grammar development is grounded in the
practical task of parsing itself.
The grammatical knowledge of the agents is typically bootstrapped by using an anno-
tated natural language corpus [13].At the onset of such a corpus-based grael society, the
syntactic structures of the corpus are randomly distributed over the agents, so that each
agent holds a number of tree-structures in memory. The actual communication between
agents is implemented in language games [17]: an agent (ag1) presents a sentence to an-
other agent (ag2). If ag2 is able to correctly analyze ag1’s sentence, the communication
is successful. If on the other hand, ag2 is lacking the proper grammatical information
to parse the sentence correctly, ag1 shares the necessary information for ag2 to arrive at
the proper solution.
A Toy Example
We take a look at an example of a very basic language game: Figure 1 shows a typical
interaction between two agents. In this example, an annotated corpus of two sentences
has been distributed over two agents. The two agents engage in a language game, in
which ag1 provides an assignment to ag2: ag1 presents the sentence “I offered some
bear hugs” to ag2 for parsing. ag2’s knowledge does not contain the proper grammatical
information to interpret this sentence the way ag1 intended and so ag2 will return an
incorrect parse, albeit consistent with its own grammar.
ag1 will consequently try and help ag2 out by revealing the minimal correct sub-
structure of the correct parse that should enable ag2 to arrive at the correct solution. ag2
will incorporate this information in its grammar and try to parse the sentence again with
the updated knowledge. Once ag2 is able to provide the correct analysis (or is not able
to after a certain number of attempts) either ag1’s next sentence will be parsed, or two
other agents in the grael society will be randomly selected to play a language game.

552
G. De Pauw
ag1’s Initial Treebank
ag2’s Initial Treebank
S
NP
I
VP
offered
NP
some bear hugs
S
NP
I
VP
offered
NP
some bear
NP
hugs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ag1’s assignment to ag2
parse “I offered some bear hugs”
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ag2’s solution to ag1’s assignment
S
NP
I
VP
offered
NP
some bear
NP
hugs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ag1’s suggestion to ag2
VP
offered
NP
some bear hugs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ag2’s Updated Treebank
S
NP
I
VP
offered
NP
some bear
NP
hugs
VP
offered
NP
some bear hugs
Fig. 1. A grael language game

Evolutionary Computing as a Tool for Grammar Development
553
Generations
This type of interaction is based on a high amount of knowledge sharing between agents.
This extends the agents’grammars very fast, so that their datasets can grow very large in
a short period of time. It is therefore beneﬁcial to introduce new generations in the grael
society from time to time. This not only allows for tractable computational processing
times, but also allows the society to purge itself of bad agents and build new generations
of good parser agents, who contain a fortuitous distribution of grammatical knowledge.
This introduces a neo-darwinist aspect in the system and involves the use of ﬁtness
functions that can distinguish good agents from bad ones. Typically, we deﬁne the ﬁtness
of an agent in terms of its parsing accuracy (i.e. the number of correct analyses), but we
can also require the agents to have fast and efﬁcient grammars and the like. The use of
ﬁtness functions and generations ideally makes sure that the required type of grammat-
ical knowledge is retained throughout different generations, while useless grammatical
knowledge can be marginalized over time.
Unfortunately, it is not feasible to provide a detailed description of the evolutionary
parameters in the context of this overview paper. We would like to refer to [12] for
speciﬁc details on the architecture and parameters of the grael environment.
4
GRAEL-1: Probabilistic Grammar Optimization
grael-1 is the most straightforward instantiation of grael and deals with probabilistic
grammar optimization. Developing a corpus-based parser requires inducing a grammar
from an annotated corpus and using it to parse new sentences. Typically, these grammars
areverylarge,sothatforanygivensentenceahugeamountofpossibleparsesisgenerated
(a parse forest), only one of which provides the correct analysis for the sentence. Parsing
can therefore be considered as a two-step process: ﬁrst a parser generates all possible
parses for a sentence, after which a disambiguation step makes sure the correct analysis is
retrieved from the parse forest. Fortunately, we can also induce probabilistic information
from the annotated corpus2 that provides a way to rank the analyses in the parse forests in
order of probabilistic preference. Even though these statistics go a long way in providing
well ordered parse forests, it can be observed in many cases that the ranking of the parse
forest is sometimes counter-intuitive in that correct constructs are often overtaken by
obviously erroneous, but highly frequent structures.
With grael-1, we propose an agent-based evolutionary computing method to resolve
the issue of suboptimal probability mass distribution: by distributing the knowledge
over a group of agents and having them interact with each other, we basically create a
multiple-route model for probabilistic grammar optimization. Grammatical structures
extracted from the training corpus, will be present in different quantities and variations
throughout the grael society (similarly to the aforementioned machine learning method
of bagging). While the agents interact with each other and in effect practice the task
on each other’s grammar, a varied range of probabilistic grammars are optimized in
a situation that directly relates to the task at hand (similarly to the machine learning
method of boosting).
2 This is achieved by observing the relative frequency of grammatical constructs in the annotated
corpus.

554
G. De Pauw
4.1
Experimental Setup
In the grael-1 experiments, we measure two types of accuracy: the baseline accuracy
is measured by directly inducing a grammar from the training set to power a parser,
which disambiguates the test set. The same training set is then randomly distributed over
a number of agents in the grael society, who will consequently engage in a number
of language games. At some point, the society is halted and the ﬁttest agent is selected
from the society. This agent effectively constitutes a redistributed and probabilistically
optimized grammar, which can be used to power another parser. grael-1 accuracy is
achieved by having this parser disambiguate the same test set as the baseline parser.
Data and tools. Two data sets from the Penn Treebank [13] were used. The main
batch of experiments was conducted on an edited version of the small, homogeneous
atis-corpus, which consists of a collection of annotated sentences recorded by a spoken-
dialogue system. The larger Wall Street Journal Corpus (henceforth wsj), a collection
of annotated newspaper articles, was used to test the system on a larger scale corpus.
We used the parsing system pmpg [16], which combines a CKY parser [18] and a post-
parsing parse forest reranking scheme that employs probabilistic information as well
as a memory-based operator that ensures that larger syntactic contexts are considered
during parsing.
Apart from different corpora, we also experimented on different society sizes (5, 10,
20, 50, 100 agents), generation methods, ﬁtness functions and methods to determine
when to halt a grael society. An exhaustive overview of all experimental parameters
can be found in [12], but we will brieﬂy outline some key notions. New generations are
created as follows: if an agent is observed not to acquire any more rules over the course
of n communicative attempts3, it is considered to be an end-of-life agent.As soon as two
end-of-life agents are available that belong to the 50% ﬁttest agents in the society, they
are allowed to procreate by crossing over the grammars they have acquired during their
lifespan. This operation yields three new agents, two of which will take their ancestors’
slots in the society, while the other one takes the slot of the oldest agent among the
50% unﬁt agents at that point in time. The ﬁtness of an agent is deﬁned by recording a
weighted average of the F-score (see below) during inter-agent communication and the
F-score of the agent’s parser on a held-out validation set. This information was also used
to try and halt the society at a global maximum and to select the ﬁttest agent from the
society. For computational reasons, the experiments on the wsj-corpus were limited to
two different population sizes (50 and 100) and used an approximation of grael that
can deal with large datasets in a reasonable amount of time.
Note that grael-1 includes two different notions of crossover, although either one
is a far stretch from the classic GA-type deﬁnition of the concept. The ﬁrst type of
crossover occurs during the language game when parts of syntactic tree-structures are
being shared between agents. This operation relates to the recombination of knowledge
aspect of crossover. The second type of crossover occurs when new agents are created by
crossing over the grammars of two end-of-life agents.Again, the aspect of recombination
is apparent in this operation. Note however the distinction with crossover in the context
3 n is by default set to the number of agents in a society.

Evolutionary Computing as a Tool for Grammar Development
555
of genetic algorithms in that neither crossover operation in the grael system occurs on
the level of the genotype and that it is in fact the phenotype that is being adapted and
which evolves over time4.
Table 1. Baseline vs. grael-1 results
atis
wsj
Exact Match Fβ=1-score Exact Match Fβ=1-score
Baseline
70.7
89.3
16.0
80.5
grael (5)
72.4
90.9
—
—
grael (10)
77.6
92.1
—
—
grael (20)
77.6
92.1
—
—
grael (50)
75.9
92.2
22.2
80.7
grael (100)
75.9
92.0
22.8
81.1
4.2
Results
Table 1 displays the results of these experiments. Exact Match Accuracy expresses the
percentage of sentences that were parsed completely correct, while the F-score is a
measure of how well the parsers work on a constituent level. The baseline model is a
standard pmpg parser using a grammar directly induced from the training set. Table 1 also
displays scores of the grael system for different population sizes.We notice a signiﬁcant
gain for all grael models over the baseline model on the atis corpus. The small society
of 5 agents achieves only a very limited improvement over the baseline method. Data
analysis showed that the best moment to halt the society and select the ﬁttest agent from
the society, is a relatively brief period right before actual convergence sets and grammars
throughout the society are starting to resemble each other more closely. The size of the
the society seems to be the determining factor controlling the duration of this period. In
smaller societies, it may occur that convergence sets in too fast, since there is a narrower
spread of data throughout the society. This causes convergence to set in prematurely,
before the halting procedures even had a chance to register a proper halting point for the
society. Hence, the low accuracy for the 5 agent society on the atis corpus.
Some preliminary experiments on a subset of the wsj corpus had shown that society
sizes of 20 agents and less to be unsuitable for a large-scale corpus, again ending up in a
harmful premature stagnation. The gain achieved by the grael society is less spectacular
than on the atis corpus, but it is still statistically signiﬁcant. Larger society sizes and full
grael processing on the wsj corpus should achieve a larger gain, but is not currently
feasible due to computational constraints.
The results show that grael-1 is indeed an interesting method for probabilistic
grammar redistribution and optimization. Data analysis shows that many of the counter-
intuitive parse forest orderings that were apparent in the baseline model, are being
4 Theoretically, this method relates to the empiricist point of view of language acquisition, rather
than the nativist point of view.

556
G. De Pauw
resolved after grael-1 processing. It is also interesting to point out that we are achieving
an error reduction rate of more than 26% over the baseline method, without introducing
any new grammatical information in the society, but solely by redistributing what is
already there.
5
GRAEL-2: Grammar Rule Discovery
Any type of grammar, be it corpus-induced or hand-written will not be able to cover all
sentences of language. Some sentences will indeed require a rule that is not available in
the grammar. Even for a large corpus such as the wsj, missing grammar rules provide
a serious accuracy bottleneck. We therefore set out to ﬁnd a method that can take a
grammar and improve its coverage by generating new rules. But doing so in an unguided
manner, would yield huge, over-generating grammars, containing many nonsensical
rules. The grael-2 system described in this section provides a guidance mechanism
to grammar rule discovery. In grael-2, the original grammar is distributed among a
group of agents, who can randomly mutate the grammatical structures they hold. The
new grammatical information they create is tried and tested by interacting with each
other. The neo-darwinist aspect of this evolutionary system tries to retain any useful
mutated grammatical information throughout the population, while noise is ﬁltered out
over time. This method provides a way to create new grammatical structures previously
unavailable in the corpus, while at the same time evaluating them in a practical context,
without the need for an external information source.
Some minor alterations need to be made to the initial grael-1 system to accomplish
this, most notably the addition of an element of mutation. This occurs in the context
of a language game (cf. Figure 1) at the point where ag1 suggests the minimal correct
substructure to ag2. In grael-1 this step introduced a form of error-driven learning,
making sure that the probabilistic value of this grammatical structure is increased. The
functionality of grael-2 however is different: we assume that there is a virtual noisy
channel between ag1 and ag2 which may cause ag2 to misunderstand ag1’s structure.
Small mutations on different levels of the substructure may occur, such as the deletion,
addition and replacement of nodes in the tree-structure. This mutation introduces pre-
viously unseen grammatical data in the grael society, some of which will be useless
(and will hopefully disappear over time), some of which will actually constitute good
grammar rules.
Note again that the concept of mutation in the grael system, stretches the classic
GA-notion. Mutation in grael-2 does not occur on the level of the genotype at all.
It is the actual grammatical information that is being mutated and consequently com-
municated throughout the society. This provides a signiﬁcant speed-up of grammatical
evolution over time, as well as enable a transparent insight into the grammar rule dis-
covery mechanism itself.
Experimental Setup and Results. The grael-2 experiments have a similar setup to the
grael-1 experiments (Section 4). For the experiments on the atis corpus, we compiled
a special worst-case scenario test set to speciﬁcally test the grammar-rule discovery
capabilities of grael-2. This test set consists of 97 sentences that require a grammar

Evolutionary Computing as a Tool for Grammar Development
557
Table 2. Baseline vs grael-1 vs grael-2 vs grael2+1 Results
atis
wsj
Fβ=1 Ex. Match Fβ=1 Ex. Match
Baseline
69.8
0
80.5
16
grael-1
73.8
0
81.4
22.8
grael-2
83.0
7.2
76.5
19.3
grael2+1 85.7
11.3
81.6
23.4
rule that cannot be induced from the training set. For the wsj-experiments the standard
test set was used. A 20-agent and a 100 agent society were respectively used for the atis
and wsj experiments.
Table 2 compares the grael-2 results to the baseline and grael-1 systems. The
latter systems trivially achieve an exact match accuracy of 0% on the atis test set, which
also has a negative effect on the F-score (Table 2). grael-2 is indeed able to improve
on this signiﬁcantly. The results on the wsj corpus show however that grael-2 has lost
the beneﬁcial probabilistic optimization effect that was paramount to grael-1. Another
experiment was therefore conducted in which we turned the grael-2 society into a
grael-1 society after the former’s halting point. In other words: we take a society of
agents using mutated information and consequently apply grael-1 probabilistic redis-
tribution on these grammars. This achieves a signiﬁcant improvement on all data sets
and establishes an interesting grammar development technique that is able to extend and
optimize any given grammar without the need for an external information source.
6
GRAEL-3: Unsupervised Grammar Induction
In grael-2 we started off with an initial grammar that is induced from an annotated
corpus, so that we could consider it to be a form of supervised grammar induction, since
we still require annotated data. Yet, annotated data is hard to come by and resources
are necessarily limited. Recent research efforts however try to implement methods that
can apply structure to raw data, simply on the basis of distributional properties and
grammatical principles [19,20,21]. Further alterations to the grael-2 system can extend
its functionality to include this type of task.
The grael-3 system requires us to develop a basic, but workable grammar induc-
tion module, that can build tree-structures on the basis of mutual information content
calculated on bigrams. This module should not be considered a part of grael-3 proper:
any kind of grammar induction method can in principle be used to bootstrap structure in
the society. Next, we performed three types of experiment for each data set: grael-3a
takes the whole training set and applies structure to those sentences based on information
content values calculated on the entire data set. grael-3b ﬁrst distributes the sentences
over the agents, after which the grammar induction module applies structure to these sen-
tences based on information content values calculated for each agent individually. Since
this grammar induction module effectively constitutes a parser, we also conducted some
experiments in which parsing was only performed using this method (grael-3ab-2).

558
G. De Pauw
The same training set/test set divisions were used as in the grael-1 experiments,
while the experiments were performed on a 20-agent society for the atis-corpus and
a 100-agent society for the wsj-corpus. Due to reasons of time, we did not perform
the baseline experiment using the pmpg for the wsj experiments, nor the grael-3a-1
experiment. We measure the F-score and the zero-crossing brackets measure which is
typically used to evaluate unsupervised grammar induction methods.
Table 3. grael-3 Results
atis
wsj
Fβ=1 0CB Fβ=1 0CB
Baseline (pmpg) 22.4 22.9
–
–
grael-3a-1
25.6 24.9
–
–
grael-3b-1
22.7 22.9 31.8 32.8
Baseline (gim)
28.4 30.8 32.2 32.5
grael-3ab-2
31.0 31.1 33.8 34.0
The ﬁrst line of Table 3 shows the baseline accuracy using a pmpg on the training
set annotated by the grammar induction module. These ﬁgures are very low, which
is mainly due to the problematic labeling properties the grammar induction method
imposes. A parser using ps-type rules such as pmpg does indeed need accurate node
labels to be able to process grammatical structure in an accurate manner. The grael-
3a-1 system however is able to improve on this grammar signiﬁcantly. Unsupervised
grammar induction methods typically need a lot of data to achieve reasonable accuracy.
It is therefore not surprising that grael-3b does not achieve any improvement over the
baseline, since the grammar induction method only has a very limited amount of data
to extract useful information content values from. Using the grammar induction method
as a parser itself circumnavigates the problem of labeling and this has a positive effect
on parsing accuracy. More importantly, grael-3 seems again able to improve parsing
accuracy signiﬁcantly, both on the atis and the wsj corpus.
7
Concluding Remarks
This paper presented a broad overview of the grael system, which can be used for
different grammar optimization and induction tasks. We believe this to be one of the ﬁrst
research efforts that employs agent-based evolutionary computing as a machine learning
method for data-driven grammar development. Using the same architecture and only
applying minor alterations, we were able to implement three different tasks: grael-1
provides a beneﬁcial re-distribution of the probability mass of a probabilistic grammar by
using a form of error-driven learning in the context of interactions between autonomous
agents. By introducing an element of mutation, we extended grael-1’s functionality
and projected grael-2 as a workable grammar rule discovery method, signiﬁcantly
improving grammatical coverage on corpus-induced grammars. Following up grael-
2’s grammar rule discovery method with grael-1’s probabilistic grammar optimization

Evolutionary Computing as a Tool for Grammar Development
559
proved to be an interesting optimization toolkit for corpus-induced grammars. Finally,
we described grael-3 as a ﬁrst attempt to provide an unsupervised grammar induction
technique. Even though the scores achieved by grael-3 are rather modest compared to
supervised approaches, the experiments show that the grael environment is again able
to take a collection of deﬁcient grammars and turn them into better grammars through
an extended process of inter-agent interaction.
The grael framework provides an agent-based evolutionary computing approach to
natural language grammar optimization and induction. It integrates the sensibilities of
combinatory machine learning methods such as bagging and boosting, with the dynamics
of evolutionary computing and agent-based processing. We have shown that grael-1
and grael-2 are able to take a collection of annotated data, providing an already well-
balanced grammar, and squeeze more performance out of them without using an external
information source. The experiments with grael-3 showed however that is equally able
to improve on a collection of poor initial grammars, proving that the grael framework
is indeed able to provide an optimization for any type of grammar, regardless of its initial
quality. This projects grael as an interesting workbench for natural language grammar
development, both for supervised, as well as unsupervised grammar optimization and
induction tasks.
Acknowledgments. The research described in this paper was ﬁnanced by the FWO
(Fund for Scientiﬁc Research). The author would like to acknowledge Frederic Chap-
pelier, for kindly making his parser available [18].
References
1. Daelemans, W., van den Bosch, A., Zavrel, J.: Forgetting exceptions is harmful in language
learning. Machine Learning, Special issue on Natural Language Learning 34 (1999) 11–41
2. van Halteren, Hans, J.Z., Daelemans, W.: Improving accuracy in word class tagging through
combination of machine learning systems. Computational Linguistics 27 (2) (2001) 199–230
3. TjongKimSang,E.,Daelemans,W.,D’ejean,W.,Koeling,H.,Krymolowski,R.,Punyakanok,
Y., Roth,V.: Applying system combination to base noun phrase identiﬁcation. In: Proceedings
of COLING 2000, Saarbruecken, Germany (2000) 857–863
4. Breiman, L.: Bagging predictors. Machine Learning 24 (1996) 123–140
5. Abney, S., Schapire, R., Singer, Y.: Boosting applied to tagging and pp attachment. In: Pro-
ceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora. (1999) 38–45
6. Henderson, J., Brill, E.: Bagging and boosting a treebank parser. In: Proceedings of the 1st
Meeting of the North American Chapter of the Association for Computational Linguistics
(NAACL-2000). (2000) 34–41
7. Smith, T.C., Witten, I.H.: Learning language using genetic algorithms. In Wermter, S.,
Riloff, E., Scheler, G., eds.: Connectionist, Statistical, and Symbolic Approaches to Learning
for Natural Language Processing. Volume 1040 of LNAI. Springer Verlag, Berlin (1996)
132–145
8. Wyard, P.: Context-free grammar induction using genetic algorithms. In Belew, R., Booker,
L., eds.: Proceedings of the Fourth International Conference on Genetic Algorithms, San
Mateo, ICGA, Morgan Kaufmann (1991) 514–518

560
G. De Pauw
9. Antonisse, H.J.: A grammar-based genetic algorithm. In Rawlings, G.J.E., ed.: Foundations
of genetic algorithms. Morgan Kaufmann, San Mateo (1991) 193–204
10. Araujo, L.: A parallel evolutionary algorithm for stochastic natural language parsing. In:
Proceedings of The Seventh International Conference on Parallel Problem Solving From
Nature, Granada, Spain (2002) 700–709
11. Losee, R.: Learning syntactic rules and tags with genetic algorithms for information re-
trieval and ﬁltering: An empirical basis for grammatical rules. Information Processing and
Management 32 (1995) 185–197
12. De Pauw, G.: An Agent-Based Evolutionary Computing Approach to Memory-Based Syn-
tactic Parsing of Natural Language. PhD thesis, University of Antwerp, Antwerp, Belgium
(2002)
13. Marcus, M.P., Santorini, B., Marcinkiewicz, M.: Building a large annotated corpus of en-
glish: the penn treebank. Computational linguistics 19 (1993) 313–330 Reprinted in Susan
Armstrong, ed. 1994, Using large corpora, Cambridge, MA: MIT Press, 273–290.
14. Bod, R.: Beyond Grammar—An Experience-Based Theory of Language. Cambridge Uni-
versity Press, Cambridge, England (1998)
15. Collins, M.: Head-driven Statistical Models for Natural Language Parsing. PhD thesis,
University of Pennsylvania, Pennsylvania, USA (1999)
16. De Pauw, G.: Aspects of pattern-matching in dop. In: Proceedings of the 18th International
Conference on Computational Linguistics. (2000) 236–242
17. Steels, L.: The origins of syntax in visually grounded robotic agents. Artiﬁcial Intelligence
103 (1998) 133–156
18. Chappelier, J.C., Rajman, M.: A generalized cyk algorithm for parsing stochastic cfg. In:
Proceedings of Tabulation in Parsing and Deduction (TAPD’98), Paris (FRANCE) (1998)
133–137
19. van Zaanen, M., Adriaans, P.: Alignment-Based Learning versus EMILE: A comparison. In:
Proceedings of the Belgian-Dutch Conference on Artiﬁcial Intelligence (BNAIC); Amster-
dam, the Netherlands. (2001) 315–322
20. Clark, A.: Unsupervised induction of stochastic context-free grammars using distributional
clustering. In Daelemans, W., Zajac, R., eds.: Proceedings of CoNLL-2001, Toulouse, France
(2001) 105–112
21. Yuret, D.: Discovery of Linguistic Relations Using Lexical Attraction. PhD thesis, MIT,
Cambridge, MA (1998)

Solving Distributed Asymmetric Constraint
Satisfaction Problems Using an Evolutionary
Society of Hill-Climbers
Gerry Dozier
Department of Computer Science and Software Engineering
Auburn University, Auburn AL 36849-5347, USA
gvdozier@eng.auburn.edu
Abstract. The distributed constraint satisfaction problem (DisCSP)
can be viewed as a 4-tuple (X, D, C, A), where X is a set of n vari-
ables, D is a set of n domains (one domain for each of the n variables),
C is a set of constraints that constrain the values that can be assigned
to the n variables, and A is a set of agents for which the variables and
constraints are distributed. The objective in solving a DisCSP is to allow
the agents in A to develop a consistent distributed solution by means of
message passing. In this paper, we present an evolutionary society of hill-
climbers (ESoHC) that outperforms a previously developed algorithm for
solving randomly generated DisCSPs that are composed of asymmetric
constraints on a test suite of 2,800 distributed asymmetric constraint
satisfaction problems.
1
Introduction
A DisCSP [17] can be viewed as a 4-tuple (X, D, C, A), where X is a set of
n variables, D is a set of n domains (one domain for each of the n variables),
C is a set of constraints that constrain the values that can be assigned to the
n variables, and A is a set of agents for which the variables and constraints
are distributed. Constraints between variables belonging to the same agent are
referred to as intra-agent constraints while constraints between the variables of
more than one agent are referred to as inter-agent constraints. The objective in
solving a DisCSP is to allow the agents in A to develop a consistent distributed
solution by means of message passing. The constraints are considered private and
are not allowed to be communicated to fellow agents due to privacy, security,
or representational reasons [17]. When comparing the eﬀectiveness of DisCSP-
solvers the number of communication cycles (through the distributed algorithm)
needed to solve the DisCSP at hand is more important than the number of
constraint checks [17].
Many real world problems have been modeled and solved using DisCSPs [1,
2,3,5,6,12]; however, many of these models use mirrored (symmetric) inter-agent
constraints. Since these inter-agent constraints are known by the agents involved
in the constraint, they cannot be regarded as private. If these constraints were
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 561–572, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

562
G. Dozier
truly private then the inter-agent constraints of one agent would be unknown
to the other agents involved in those constraints. In this case the DisCSP would
be composed of asymmetric constraints. To date, with the exception of [4,5,12],
little research has been done on distributed asymmetric CSPs (DisACSPs).
In this paper, we demonstrate how a distributed restricted form of uniform
mutation can be used to improve the eﬀectiveness of a previously developed
evolutionary computation (EC) for solving DisACSPs known as a society of hill-
climbers (SoHC) [4]. We refer to this new algorithm as an evolutionary SoHC
(ESoHC). Our results show that ESoHC outperforms SoHC on a test suite of
2,800 DisACSPs.
The remainder of this paper is organized as follows. In Section 2, we present
an overview of constraint processing which includes an introduction to the con-
cept of asymmetric constraints and presents a formula for predicting where the
most diﬃcult randomly generated asymmetric CSPs are located, known as the
phase transition [4,7,13]. In Section 3, we introduce the SoHC concept and ex-
plain how our ESoHC operates. In Section 4, we present the results of applying
SoHC and ESoHC to 800 randomly generated distributed DisACSPs. In this
section, we also compare SoHC and ESoHC on an additional 2,000 randomly
generated DisACSPs in order to better visualize their performance across the
phase transition. In Section 5, we present our conclusions and future work.
2
CSPs, Asymmetric Constraints, and the Phase
Transition
A CSP [15] can be viewed as triple ⟨X, D, C⟩where X is set of variables, D
is set of domains where each xi ∈X takes its value from the correspond-
ing domain di ∈D, and where C is a set of r constraints. Consider a binary
constraint network (one where each constraint constrains the values of exactly
two variables)1 ⟨X, D, C⟩where X = {E, F, G}, D = {dE = {e1, e2, e3}, dF
= {f1, f2, f3}, dG = {g1, g2, g3}}, and C = {cEF , cEG, cF G}. Suppose that the
constraints cEF , cEG, cF G are as follows:
cEF = {⟨e1,f2 ⟩, ⟨e1,f3 ⟩, ⟨e2,f2 ⟩, ⟨e3,f2 ⟩}
cEG = {⟨e2,g3 ⟩, ⟨e3,g1 ⟩}
cF G = {⟨f2,g1 ⟩, ⟨f2,g3 ⟩}.
Constraint networks possess two additional attributes: tightness and density.
The tightness of a constraint is the ratio of the number of tuples disallowed by
the constraint to the total number of tuples in di × dj. The average constraint
tightness of a binary constraint network is the sum of the tightness of each
constraint divided by the number of constraints in the network. The density of
a constraint network is the ratio of the number of constraints in the network to
the total number of constraints possible.
1 In this paper, we only consider binary constraint networks because any constraint
that involves more than two variables can be transformed into a set of binary con-
straints [15].

Solving Distributed Asymmetric Constraint Satisfaction Problems
563
2.1
Asymmetric Constraints
Constraints in a binary constraint network may also be represented as two di-
rectional constraints referred to as arcs [8,15]. For example, the symmetric con-
straint cEF can be represented as cEF = {c ⇀
EF , c ↼
EF }, where c ⇀
EF = c ↼
EF = {⟨
e1,f2 ⟩, ⟨e1,f3 ⟩, ⟨e2,f2 ⟩, ⟨e3,f2 ⟩}, where c ⇀
EF represents the direc-
tional constraint imposed on variable F by variable E, and where c ↼
EF represents
the directional constraint imposed on variable E by variable F. This view of
a symmetric binary constraint admits the possibility of an asymmetric binary
constraint between variables E and F as one where c ⇀
EF ̸= c ↼
EF .
2.2
Predicting the Phase Transition
Classes of randomly generated CSPs can be represent as a 4-tuple (n,m,p1,p2)
[13] where n is the number of variables in X, m is the number of values in each
domain, di ∈D, p1 represents the constraint density, the probability that a
constraint exists between any two variables, and p2 represents the tightness of
each constraint.
Smith [13] developed a formula for determining where the most diﬃcult sym-
metric randomly generated CSPs can be found. This equation is as follows, where
ˆp2critS is the critical tightness at the phase transition for n, m, p1.
ˆp2critS = 1 −m
−2
p1(n−1)
(1)
Randomly generated symmetric CSPs of the form (n,m,p1, ˆp2critS) have been
shown to be the most diﬃcult because they have on average only one solution.
Problems of this type are at the border (phase transition) between those classes
of CSPs that have solutions and those that have no solution. Classes of randomly
generated symmetric CSPs for which p2 is relatively small compared to ˆp2critS
are easy to solve because they contain a large number of solutions. Similarly,
classes of CSPs where p2 is relatively large compared to ˆp2critS, are easy to
solve because the constraints are so tight that simple backtrack-based CSP-
solvers [13] can quickly determine that no solution exists. Thus, for randomly
generated CSPs, one will observe an easy-hard-easy transition as p2 is increased
from 0 to 1.
Smith’s equation can be modiﬁed [4] to predict the phase transition in ran-
domly generated asymmetric CSPs as well. This equation is as follows where
p1α represents the probability that an arc exits between two variables and where
ˆp2critA is the critical tightness at the phase transition for n, m, and p1α.
ˆp2critA = 1 −m
−1
p1α(n−1) .
(2)
3
Society of Hill-Climbers
A society of hill-climbers (SoHC) [4,11] is a collection of hill-climbers that search
in parallel and communicate promising (or futile) directions of search to one

564
G. Dozier
another through some type of external collective structure. In the society of hill-
climbers that we present in this paper, the external collective structure which
records futile directions of search comes in the form of a distributed list of
breakout elements, where each breakout element corresponds to a previously
discovered nogood2 of a local minimum [10]. Before presenting the society of hill-
climbers concept, we must ﬁrst discuss the distributed hill-climber that makes
up the algorithm. In this section, we ﬁrst introduce a modiﬁed version of Yokoo’s
distributed breakout algorithm with broadcasting [17] (mDBA) which is based
on Morris’ Breakout Algorithm [10]. After introducing mDBA we will describe
the framework of a SoHC.
For the mDBA, each agent ai ∈A is responsible for the value assignment of
exactly one variable. Therefore agent ai is responsible for variable xi ∈X, can
assign variable xi one value from domain di ∈D, and has as constraints C
⇀
xixj
where i ̸= j. The objective of agent ai is to satisfy all of its constraints C
⇀
xixj.
Each agent also maintains a breakout management mechanism (BMM) that
records and updates the weights of all of the breakout elements corresponding
to the nogoods of discovered local minima. This distributed hill-climber seeks
to minimize the number of conﬂicts plus the sum of all of the weights of the
violated breakout elements.
3.1
The mDBA
The mDBA used in our SoHCs is very similar to Yokoo’s DBA+BC with the
major exceptions being that each agent broadcasts to every other agent the
number of conﬂicts that its current value assignment is involved in. This allows
the agents to calculate the total number of conﬂicts (ﬁtness) of the current best
distributed candidate solution (dCS) and to know when a solution has been
found (when the ﬁtness is equal to zero). The mDBA, as outlined in Figure 1,
is as follows.
Initially, each agent, ai, randomly generates a value vi ∈di and assigns it to
variable xi. Next, each agent broadcasts its assignment, xi = vi, to its neighbors
ak ∈Neighbori where Neighbori3 is the set of agents that ai is connected with
via some constraint. Each agent then receives the value assignments of every
neighbor. This collection of value assignments is known as the agent view of
an agent ai [17]. Given the agent view, agent ai computes the number of conﬂicts
that the assignment (xi = vi) is involved in. This value is denoted as γi.
Once the number of conﬂicts, γi, has been calculated, each agent ai randomly
searches through its domain, di, for a value bi ∈di that resolves the greatest
number of conﬂicts (ties broken randomly). The number of conﬂicts that an
agent can resolve by assigning xi = bi is denoted as ri. Once γi and ri have been
computed, agent ai broadcasts these values to each of its neighbors.
When an agent receives the γj and rj values from each of its neighbors, it
sums up all γj including γi and assigns this sum to fi where fi represents the
2 A nogood is a tuple that causes a conﬂict.
3 In this paper, Neighbori = A −{ai}.

Solving Distributed Asymmetric Constraint Satisfaction Problems
565
ﬁtness of the current dCS. If agent ai has the highest ri value of its neighborhood
then agent ai sets vi = bi, otherwise agent ai leaves vi unchanged. Ties are broken
randomly using a commonly seeded tie-breaker4 that works as follows: if t(i) >
t(j) then ai is allowed to change otherwise aj is allowed to change where t(k)
= (k+rnd()) mod |A|, and where rnd() is a commonly seeded random number
generator used exclusively for breaking ties.
If ri for each agent is equal to zero, i.e. if none of the agents can resolve
any of their conﬂicts, then the current best solution is a local minimum and all
agents ai send the nogoods that violate their constraints to their BMMi. An
agent’s BMM will create a breakout element for all nogoods that are sent to it.
If a nogood has been encountered before in a previous local minimum then the
weight of its corresponding breakout element is incremented by one. All weights
of newly created breakout elements are assigned an initial value of one. Therefore
the task for mDBA is to reduce the total number of conﬂicts plus the sum of all
breakout elements violated.
After the agents have decided who will be allowed to change their value and
invoked their BMMs (if necessary), the agents check their fi value. If fi > 0 the
agents begin a new cycle by broadcasting their value assignments to each other.
If fi = 0 the algorithm terminates with a distributed solution.
3.2
The Simple and Evolutionary SoHCs
The SoHCs reported in this paper are based on mDBA. Each SoHC runs ρ
mDBA hill-climbers in parallel, where ρ represents the society size. Each of the
ρ hill-climbers communicate with each other indirectly through a distributed
BMM. Figure 2 provides a simpliﬁed view of a simple SoHC. Notice in Figure 2
that each agent, ai assigns values variables xi1, xi2, · · · , xiρ where each variable
xij represents the ith variable for the jth dCS. Each agent, ai, has a local BMM
(BMMi) which manages the breakout elements that correspond to the nogoods
of its constraints.
The ESoHC works exactly like the SoHC mentioned earlier except for on each
cycle a distributed restricted uniform mutation operator is applied as follows.
Each distributed candidate solution, dCSj, that has an above average number
of conﬂicts is replaced with an oﬀspring that is a mutated version of the best
individual, dCSq, as follows. Given a distributed individual, dCSk, that is in-
volved in an above average number of conﬂicts, with probability µ, agent ai will
randomly assign vik a value from di and with probability 1 −µ agent ai will set
vik = viq. Of course, µ is referred to as the mutation rate. We refer to this form
of mutation as distributed restricted uniform mutation (dRUM-µ).
4 In case of a tie between two agents ai and aj, Yokoo’s DBA+BC will allow the agent
with the lower agent address to change its current value assignment. We refer to this
as the deterministic tie-breaker (DTB) method

566
G. Dozier
procedure mDBA(Agent ai){
Step 0: randomly assign vi ∈di to xi;
do {
Step 1: broadcast (xi = vi) to other agents;
Step 2: receive assignments from other agents, agent viewi;
Step 3: assign γi the number conflicts that (xi = vi)
is involved in;
Step 4: randomly search for value bi ∈di that minimizes the
number of conflicts of xi (ties broken randomly),
Step 5: let ri be the number of conflicts resolved
by (xi = bi);
Step 6: broadcast γi and ri to other agents;
Step 7: receive γj and rj from other agents, let fi = 
γk;
Step 8: if (max(rk) == 0)
for each conflict, (xi = v, xj = w)
update breakout elements(BMMi,(xi, v,xj, w));
Step 9: if (ri == max(rk))†
vi = bi;
} while (fi > 0)
}
† Ties are broken with randomly with a synchronized tie-breaker.
Fig. 1. The mDBA Protocol
Fig. 2. An Simpliﬁed View of a SoHC with a Society Size of 3 dCSs

Solving Distributed Asymmetric Constraint Satisfaction Problems
567
4
Results
4.1
Experiment I
In our ﬁrst experiment, our test suite consisted of 800 instances of randomly
generated DisACSPs of the form <30,6,1.0,p2> and <30,6,p1α,0.098>. In this
experiment, p2 took on values from the set, {0.03, 0.04, 0.05, 0.06}, for the 400
instances of <30,6,1.0,p2>, where ˆp2critA ≈0.06 and p1α took on values from
the set, {0.3, 0.4, 0.5, 0.6}, for the 400 instances of <30,6,p1α,0.098>, where
instances of <30, 6, p1α = 0.6, 0.098> were at the phase transition. Each of the
30 agents randomly generated 29 arcs where each arc contained approximately
1.08, 1.44, 1.88, 2.16, and 3.53 nogoods respectively for p2 values of 0.03, 0.04,
0.05, 0.06, and 0.098. The arcs were generated according to a hybrid between
Models A & B in [7]. This method of constraint generation is as follows. If each
arc was to have 1.08 nogoods (which is the case when p2 = 0.03) then every
arc received at least 1 nogood and was randomly assigned an additional nogood
with probability 0.08. Similarly, if the average number of nogoods needed for each
constraint was 2.16, (which is the case when p2 = 0.06) then every constraint
received at least 2 nogoods and was randomly assigned and additional nogood
with probability 0.16. The probability that an arc existed was determined with
probability p1α.
Tables 1a-1d show the performance results of applying eight algorithms on
each of the 100 instances of the <30,6,1.0,p2> classes of DisACSPs. The eight
algorithms compared are mDBA-DTB, (the mDBA that uses Yokoo’s deter-
ministic tie-breaker algorithm to break ties between agents with the highest ri
value), six SoHCs with values of ρ taken from the set {1, 2, 4, 8, 16, 32}, and
an ESoHC with ρ = 32 (ESoHC-32) that used dRUM-0.12.
In each table, the ﬁrst column represents the algorithm. The second column
represents the success rate (SR) that an algorithm had in ﬁnding a solution on
the 100 problems when allowed a maximum of 2000 cycles. In the third column,
the average number of cycles per run is recorded, and in the fourth column, the
average number of constraint checks made by the algorithm is recorded.
When comparing the SoHCs, one can see that the larger the society size
the better the performance is with respect to SR and average number of cycles.
However using larger society sizes also results in an increased number of con-
straint checks and larger message sizes. In Table 1d, one can see that the classes
of DisACSPs where p2 = 0.06 contains the most diﬃcult problems.
It is important to realize that when comparing DisCSP-solvers that the most
important criterion is success rate followed by communication cycles followed
by the total number of constraint checks. For this reason, SoHC-32 has been
selected as the overall best SoHC for the fully connected DisACSPs.
To make this point clearer, consider the communication medium used by the
agents to be a network. With this being the case, we can compute the utilization
of a link between any two agents given a normal 20 byte internet protocol (IP)
packet header [16]. SoHC-01 will utilize
1
20+1 of the bandwidth while SoHC-32

568
G. Dozier
Table 1. Performances on the <30,6,1.0,0.03>, <30,6,1.0,0.04> <30,6,1.0,0.05> and
<30,6,1.0,0.06> DisACSPs
Alg.
SR Cycles Checks
mDBA-DTB 0.74 563.35 623849
SoHC-01
0.83 384.49 450684
SoHC-02
0.95 139.29 379128
SoHC-04
1.00 28.20
238510
SoHC-08
1.00 24.34
441775
SoHC-16
1.00 20.47
790013
SoHC-32
1.00 17.93 1448802
ESoHC-32
1.00 12.78
916539
Alg.
SR Cycles Checks
mDBA-DTB 0.60 989.30 1433954
SoHC-01
0.71 928.94 1422377
SoHC-02
0.93 444.35 1423055
SoHC-04
0.94 313.52 2122349
SoHC-08
1.00 124.22 1915596
SoHC-16
1.00 72.62 2456001
SoHC-32
1.00 54.28 3909942
ESoHC-32
1.00 26.09 1830178
(a) Performances on <30,6,1.0,0.03>
(b) Performances on <30,6,1.0,0.04>
Alg.
SR
Cycles
Checks
mDBA-DTB 0.07 1917.75
4218533
SoHC-01
0.10 1845.13
4149324
SoHC-02
0.11 1856.94
8466398
SoHC-04
0.17 1829.86 16838690
SoHC-08
0.22 1694.59 31591364
SoHC-16
0.36 1548.81 58666291
SoHC-32
0.52 1323.80 101607214
ESoHC-32
0.98 263.20
16101363
Alg.
SR
Cycles
Checks
mDBA-DTB 0.00 2000.00
5529200
SoHC-01
0.00 2000.00
5573943
SoHC-02
0.00 2000.00 11252052
SoHC-04
0.00 2000.00 22665737
SoHC-08
0.00 2000.00 45861472
SoHC-16
0.01 1982.02 91767502
SoHC-32
0.02 1981.06 184858509
ESoHC-32
0.11 1849.02 134172949
(c) Performances on <30,6,1.0,0.05>
(d) Performances on <30,6,1.0,0.06>
will utilize
32
20+32 of the bandwidth. For this reason alone it is a welcomed result
to see that larger society sizes lead to better performance results.
In Tables 1a-1d, the results also suggest that breaking ties randomly is more
eﬀective than breaking ties using Yokoo’s deterministic tie-breaking method.
This can be seen by comparing the success rates (SR) of mDBA-DTB and SoHC-
01. Notice that in Tables 1a-1c that SoHC-01 has a slightly higher success rate.
When comparing SoHC-32 and ESoHC-32 in Tables 1a-1d, one can see that
ESoHC-32 has the better performance on each of the four class of DisACSPs. As
the tightness is increased from 0.03 to the critical value of 0.06, the diﬀerence
in the performance as compared with SoHC-32 becomes more pronounced. At
the phase transition, the SR of ESoHC-32 is 5 1
2 times better than the SR for
SoHC-32.
Tables 2a-d show the performances of the eight algorithms on 100 instances
of the <30,6,p1α,0.098> classes of DisACSPs. Notice that as p1α is increased,
in Tables 2a-2d, that the success rates of the algorithms decreases. Notice once
again that the hardest DisACSPs seem to be located at the predicted phase
transition, <30,6,0.6,0.098>. Also in Table 2, one can see that mDBA-DTB
outperforms SoHC-01 on the <30,6,0.3,0.098> class of DisACSPs but loses to
SoHC-01 on the <30,6,0.4,0.098> and <30,6,0.5,0.098> classes. When compar-

Solving Distributed Asymmetric Constraint Satisfaction Problems
569
Table 2. Performances on the <30,6,p1α.3,0.098> DisACSPs
Alg.
SR Cycles Checks
mDBA-DTB 0.64 758.98 896922
SoHC-01
0.63 790.50 904392
SoHC-02
0.97 307.26 735397
SoHC-04
1.00 36.37
280431
SoHC-08
1.00 30.57
490479
SoHC-16
1.00 21.53
791964
SoHC-32
1.00 18.20 1422854
ESoHC-32
1.00 12.77
894213
Alg.
SR
Cycles Checks
mDBA-DTB 0.47 1197.23 1784095
SoHC-01
0.51 1163.29 1757059
SoHC-02
0.67 841.67 2534178
SoHC-04
0.89 450.68 2954802
SoHC-08
0.97 172.90 2461325
SoHC-16
1.00 129.86 4031357
SoHC-32
1.00
60.12
4184293
ESoHC-32
1.00
29.83
2009218
(a) Performances on <30,6,0.3,0.098> (b) Performances on <30,6,0.4,0.098>
Alg.
SR
Cycles
Checks
mDBA-DTB 0.05 1960.59 4177278
SoHC-01
0.10 1864.31 3991908
SoHC-02
0.12 1859.26 8124593
SoHC-04
0.24 1700.12 14848626
SoHC-08
0.34 1600.72 28663041
SoHC-16
0.50 1432.35 52936951
SoHC-32
0.56 1154.75 85608964
ESoHC-32
0.96 279.36 16493167
Alg.
SR
Cycles
Checks
mDBA-DTB 0.00 2000.00
5329815
SoHC-01
0.00 2000.00
5459918
SoHC-02
0.01 1987.72 10844644
SoHC-04
0.00 2000.00 22344655
SoHC-08
0.02 1977.10 44391268
SoHC-16
0.01 1983.21 89557109
SoHC-32
0.03 1974.31 180966004
ESoHC-32
0.17 1755.67 126561756
(c) Performances on <30,6,0.5,0.098>
(d) Performances on <30,6,0.6,0.098>
ing SoHC-32 and ESoHC-32, the results are similar to what has been observed
earlier. ESoHC-32 outperforms SoHC-32 on all classes of DisACSPs and has an
SR that is 5 2
3 times greater than SoHC on the <30,6,0.6,0.098> class.
4.2
Experiment II
In the previous section, we presented the results of applying eight SoHCs to 800
randomly generated DisACSPs. In that presentation, we were only able to show
the side of the phase transition where classes were likely to contain at least one
solution [7,13]. This was done because the distributed hill-climbers presented in
this paper are not complete; they cannot determine if the problem at hand has
no solution at all. In order to visualize the phase transition in DisACSPs we
used a technique introduced by Solnon in [14]. This technique is simple. When
randomly generating a CSP make sure that it has at least one solution. Using
the above approach will allow incomplete search algorithms to experience the
easy-hard-easy behavior across the phase transition as tightness and/or density
is increased from 0.0 to 1.0.
In order to visualize the phase transition for the <30,6,1.0,p2> classes of
DisACSPs we randomly generated an additional 1,100 DisACSPs where p2 took
values from the set, {0.03, 0.04, 0.05, 0.06, 0.065, 0.07, 0.08, 0.09, 0.1, 0.17, 0.25}.

570
G. Dozier
For each value of p2, 100 DisACSPs were generated and guaranteed to have at
least 1 solution. SoHC-32 and ESoHC-32 were then run on each of the problems
with a maximum number of 2000 cycles in which to ﬁnd a solution.
Figure 3 shows the performance results of SoHC-32 and ESoHC-32 on the
1,100 DisACSPs of the class <30,6,1.0,p2>. The range of values of p2 (from 0.03
to 0.25) can be seen along the x-axis. Figure 3a shows the results in terms of the
average number of cycles needed solve a DisACSP (shown on the y-axis) and
Figure 3b shows the performance results in terms of failure rate (as shown on
the y-axis).
In Figure 3a, one can see that the average number of cycles increases rapidly
as p2 is increased from 0.03 to about 0.065. For these problems, the actual phase
transition seems to occur at p2 = 0.065. As p2 is increased beyond 0.065 one can
see a rapid reduction in average number of cycles needed to ﬁnd a solution. As
the constraints become increasingly tighter, it becomes easier for SoHC-32 and
ESoHC-32 to ﬁnd the one and only solution. The shape of the curve in Figure 3b
is very similar to the one shown in Figure 3a. Notice also that the performance
of ESoHC-32 is superior to SoHC-32 over the total range of values for p2.
In order to visualize the phase transition as p1α is increased, we created
900 randomly generated DisACSPs of the form <30,6,p1α,0.098> where the arc
density, p1α, took on values from the set {0.3, 0.4, 0.5, 0.6, 0.62, 0.7, 0.8, 0.9,
1.0 }. Once again for each value of p1α 100 DisACSPs were randomly generated,
and the SoHCs were run on each problem with an allowed a maximum of 2000
cycles to ﬁnd a solution.
Figure 4 shows the search behavior of SoHC-32 and ESoHC-32 as p1α was
increased from 0.3 to 1.0 in terms of the average number of cycles needed to ﬁnd
a solution as well as the failure rate. The results are similar to those shown in
Figure 3; ESoHC-32 dramatically outperforms SoHC-32. However, Figures 3 and
4 diﬀer in that the easy-hard-easy transition is less abrupt. The reason for this
is that constraint tightness is a more sensitive predictor of the relative hardness
of a CSP.
4.3
Discussion
The increased performance of ESoHC over SoHC is primarily due to the way in
which the dRUM-µ operator intensiﬁes search around the current best individual
in the population. The basic assumption made by anyone applying an EC to a
problem is that optimal (or near optimal) solutions are surrounded by good
solutions. However, this assumption is not true for constrained problems. Even
for problems where this is the case ECs typically employ local search in an eﬀort
to exploit promising regions. Thus, the EC will intensify search periodically
in some region. Actually, the search behavior of ESoHC is no diﬀerent. The
individuals that are involved in a below average number of conﬂicts are allowed
to continue to be reﬁned by mDBA while individuals that are involved in an
above average number of conﬂicts are replaced by oﬀspring that more closely
resemble the current best individual in the population. Upon closer inspection
of the results in Tables 1 and 2 one can see that as ρ is increased in the SoHCs the

Solving Distributed Asymmetric Constraint Satisfaction Problems
571
0
200
400
600
800
1000
1200
1400
1600
1800
2000
0
0.05
0.1
0.15
0.2
0.25
Average Number of Cycles
p2
The Phase Transition for the (30,6,1.0,p2) Classes of Asymmetric DisACSPs
SoHC
ESoHC
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
0.25
Failure Rate
p2
The Phase Transition for the (30,6,1.0,p2) Classes of Asymmetric DisACSPs
SoHC
ESoHC
(a) Avg. Number of Cycles
(b) Failure Rate
Fig. 3. Phase Transition Based on Avg. Number of Cycles and Failure Rate
0
200
400
600
800
1000
1200
1400
1600
1800
2000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Average Number of Cycles
p1_alpha
The Phase Transition for the (30,6,p1_alpha,0.098) Classes of Asymmetric DisACSPs
SoHC
ESoHC
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Failure Rate
p1_alpha
The Phase Transition for the (30,6,p1_alpha,0.098) Classes of Asymmetric DisACSPs
SoHC
ESoHC
(a) Avg. Number of Cycles
(b) Failure Rate
Fig. 4. Phase Transition Based on Avg. Number of Cycles and Failure Rate
performance gain diminishes. Therefore it seems reasonable, given a suﬃciently
large ρ, that half of the individuals can be used to intensify search without
adversely aﬀecting the convergence rate.
5
Conclusions and Future Work
In this paper, we have introduced the concept of DisACSPs and have demon-
strated how distributed restricted uniform mutation can be used to improve the
search of a society of hill-climbers on easy and diﬃcult DisACSPs. We also pro-
vided a brief discussion of some of the reasons why the performance of ESoHC-32
is superior to SoHC-32. Our future work will include the development of other

572
G. Dozier
distributed forms procreation that may increase the performance of ESoHC as
well as the study of eﬀect that diﬀerent reallocation strategies have on the per-
formance of ESoHC.
Acknowledgement. The author would like to thank the National Science
Foundation for the support of this research under grant #IIS-9907377.
References
1. Bejar, R., Krishnamachari, B., Gomes, C., and Selman, B. (2001). “Distributed
Constraint Satisfaction in a Wireless Sensor Tracking System”, Proc. of the IJCAI
Workshop on Distributed Constraint Reasoning, pp. 81–90.
2. Calisti, M., and Faltings, B. (2000). “Agent-Based Negotiations for Multi-Provider
Interactions”, Proc. of the Intl. Sym. on Agent Sys. and Applications, pp. 235–248.
3. Calisti, M., and Faltings, B. (2000). “Distributed Constrained Agents for Allocating
Service Demands in Multi-Provider Networks”, Journal of the Italian Operational
Society, Special Issue on Constraint Problem Solving, vol. XXIX, no. 91.
4. Dozier, G. and Rupela, V. (2002). “Solving Distributed Asymmetric CSPs via a
Society of Hill-Climbers”, Proc. of IC-AI’02, pp. 949–953, CSREA Press.
5. Freuder, E. C., Minca, M., and Wallace, R. J. (2001). “Privacy/Eﬃciency Trade-
oﬀs in Distributed Meeting Scheduling by Constraint-Based Agents”, Proc. of the
IJCAI Workshop on Distributed Constraint Reasoning, pp. 63–71.
6. Krishnamachari, B., Bejar, R., and Wicker, S. (2002). “Distributed Problem Solv-
ing and the Boundaries of Self-Conﬁguration in Multi-hop Wireless Networks”,
Proc. of the Hawaii Intl. Conference on System Sciences, HICSS-35.
7. MacIntyre, E., Prosser, P., Smith, B., and Walsh, T. (1998). “Random Constraint
Satisfaction: Theory Meets Practice,” The Proc. of CP-98, pp. 325–339.
8. Mackworth, A. K. (1977). “Consistency in networks of relations”. Artiﬁcial Intel-
ligence, 8 (1), pp. 99–118.
9. Modi, P. J., Jung, H., Tambe, M., Shen, W.-M., and Kulkarni, S. (2001). “Dynamic
Distributed Resource Allocation: A Distributed Constraint Satisfaction Approach”
Proc. of the IJCAI Workshop on Distributed Constraint Reasoning, pp. 73–79.
10. Morris, P. (1993). “The Breakout Method for Escaping From Local Minima,” Proc.
of AAAI’93, pp. 40–45.
11. Sebag, M. and Shoenauer, M. (1997). “A Society of Hill-Climbers,” The Proc. of
ICEC-97, pp. 319–324, IEEE Press.
12. Silaghi, M.-C., Sam-Haroud, D., Calisti, M., and Faltings, B. (2001). “Generalized
English Auctions by Relaxation in Dynamic Distributed CSPs with Private Con-
straints”, Proc. of the IJCAI Workshop on Distributed Constraint Reasoning, pp.
45–54.
13. Smith, B. (1994). “Phase Transition and the Mushy Region in Constraint Satis-
faction Problems,” Proc. of ECAI-94, pp. 100–104, John Wiley & Sons, Ltd.
14. Solnon, C. (2002). “Ants can solve constraint satisfaction problems”, to appear in:
IEEE Transactions on Evolutionary Computation, IEEE Press.
15. Tsang, E. (1993). Foundations of Constraint Satisfaction, Academic Press, Ltd.
16. Walrand, J. (1998). Communication Networks: A First Course, 2nd Edition,
WCB/McGraw-Hill.
17. Yokoo, M. (2001). Distributed Constraint Satisfaction, Springer-Verlag.

573
Use of Multiobjective Optimization Concepts to Handle
Constraints in Single-Objective Optimization
Arturo Hern´andez Aguirre1, Salvador Botello Rionda1, Carlos A. Coello Coello2, and
Giovanni Liz´arraga Liz´arraga1
1 Center for Research in Mathematics (CIMAT)
Department of Computer Science
Guanajuato, Gto. 36240, M´exico
{artha,botello,giovanni}@cimat.mx
2 CINVESTAV-IPN
Evolutionary Computation Group
Depto. de Ingenier´ıa El´ectrica
Secci´on de Computaci´on
Av. Instituto Polit´ecnico Nacional No. 2508
Col. San Pedro Zacatenco
M´exico, D. F. 07300
ccoello@cs.cinvestav.mx
Abstract. In this paper, we propose a new constraint-handling technique for evo-
lutionary algorithms which is based on multiobjective optimization concepts. The
approach uses Pareto dominance as its selection criterion, and it incorporates a
secondary population. The new technique is compared with respect to an approach
representative of the state-of-the-art in the area using a well-known benchmark
for evolutionary constrained optimization. Results indicate that the proposed ap-
proach is able to match and even outperform the technique with respect to which
it was compared at a lower computational cost.
1
Introduction
The success of Evolutionary Algorithms (EAs) in global optimization has triggered
a considerable amount of research regarding the development of mechanisms able to
incorporate information about the constraints of a problem into the ﬁtness function
of the EA used to optimize it [7]. So far, the most common approach adopted in the
evolutionary optimization literature to deal with constrained search spaces is the use of
penalty functions [10]. Despite the popularity of penalty functions, they have several
drawbacks from which the main one is that they require a careful ﬁne tuning of the
penalty factors that indicates the degree of penalization to be applied [12].
Recently, some researchers have suggested the use of multiobjective optimization
concepts to handle constraints in EAs.This paper introduces a new approach that is based
on an evolution strategy that was originally proposed for multiobjective optimization:
the ParetoArchived Evolution Strategy (PAES) [5]. Our approach (which is an extension
of PAES) can be used to handle constraints in single-objective optimization problems
and does not present the scalability problems of the original PAES. Besides using Pareto-
based selection, our approach uses a secondary population (one of the most common
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 573–584, 2003.
c⃝Springer-Verlag Berlin Heidelberg 2003

574
A. Hern´andez Aguirre et al.
notions of elitism in evolutionary multiobjective optimization), and a mechanism that
reduces the constrained search space so that our technique can approach the optimum
more efﬁciently.
2
Problem Statement
We are interested in the general nonlinear programming problem in which we want to:
Find x which optimizes f(x)
(1)
subject to:
gi(x) ≤0, i = 1, . . . , n
(2)
hj(x) = 0, j = 1, . . . , p
(3)
where x is the vector of solutions x = [x1, x2, . . . , xr]T , n is the number of inequality
constraints and p is the number of equality constraints (in both cases, constraints could
be linear or non-linear). For an inequality constraint that satisﬁes gi(x) = 0, then we
will say that is active at x. All equality constraints hj (regardless of the value of x used)
are considered active at all points of the feasible region F.
3
Basic Concepts
A multiobjective optimization problem (MOP) has the following the form:
Minimize [f1(x), f2(x), . . . , fk(x)]
(4)
subject to the m inequality constraints:
gi(x) ≥0
i = 1, 2, . . . , m
(5)
and the p equality constraints:
hi(x) = 0
i = 1, 2, . . . , p
(6)
where k is the number of objective functions fi
:
Rn
→
R. We call x
=
[x1, x2, . . . , xn]T the vector of decision variables.We wish to determine from among the
set F of all vectors which satisfy (5) and (6) the particular set of values x∗
1, x∗
2, . . . , x∗
n
which yield the optimum values of all the objective functions.
3.1
Pareto Optimality
A vector u = (u1, . . . , uk) is said to dominate v = (v1, . . . , vk) (denoted by u ⪯v) if
and only if u is partially less than v, i.e., ∀i ∈{1, . . . , k}, ui ≤vi ∧∃i ∈{1, . . . , k} :
ui < vi. For a given multiobjective optimization problem, f(x), the Pareto optimal set
(P∗) is deﬁned as:
P∗:= {x ∈F | ¬∃x′ ∈F f(x′) ⪯f(x)}.
(7)

Use of Multiobjective Optimization Concepts to Handle Constraints
575
Thus, we say that a vector of decision variables x∗∈F is Pareto optimal if there does not
exist another x ∈F such that fi(x) ≤fi(x∗) for all i = 1, . . . , k and fj(x) < fj(x∗)
for at least one j. In words, this deﬁnition says that x∗is Pareto optimal if there exists
no feasible vector of decision variables x ∈F which would decrease some criterion
without causing a simultaneous increase in at least one other criterion. Unfortunately,
this concept almost always gives not a single solution, but rather a set of solutions called
the Pareto optimal set. The vectors x∗correspoding to the solutions included in the
Pareto optimal set are called nondominated. The image of the Pareto optimal set under
the objective functions is called Pareto front.
4
Related Work
The main idea of adopting multiobjective optimization concepts to handle constraints
is to redeﬁne the single-objective optimization of f(x) as a multiobjective optimization
problem in which we will have m + 1 objectives, where m is the total number of
constraints. Then, we can apply any multiobjective optimization technique [3] to the
new vector ¯v = (f(x), f1(x), . . . , fm(x)), where f1(x), . . . , fm(x) are the original
constraints of the problem. An ideal solution x would thus have fi(x)=0 for 1 ≤i ≤m
and f(x) ≤f(y) for all feasible y (assuming minimization).
These are the mechanisms taken from evolutionary multiobjective optimization that
are more frequently incorporated into constraint-handling techniques:
1. Use of Pareto dominance as a selection criterion.
2. Use of Pareto ranking [4] to assign ﬁtness in such a way that nondominated indi-
viduals (i.e., feasible individuals in this case) are assigned a higher ﬁtness value.
3. Split the population in subpopulations that are evaluated either with respect to the
objective function or with respect to a single constraint of the problem.
In order to sample the feasible region of the search space widely enough to reach
the global optima it is necessary to maintain a balance between feasible and infeasible
solutions. If this diversity is not reached, the search will focus only on one area of the
feasible region. Thus, it will lead to a local optimum.
A multiobjective optimization technique aims to ﬁnd a set of trade-off solutions
which are considered good in all the objectives to be optimized. In global nonlinear
optimization, the main goal is to ﬁnd the global optimum. Therefore, some changes
must be done to those approaches in order to adapt them to the new goal. Our main
concern is that feasibility takes precedence, in this case, over nondominance. Therefore,
good “trade-off” solutions that are not feasible cannot be considered as good as bad
“trade-off” solutions that are feasible. Furthermore, a mechanism to maintain diversity
must normally be added to any evolutionary multiobjective optimization technique. In
our proposal, diversity is kept by using an adaptive grid, and by a selection process
applied to the external ﬁle that maintains a mixture of both good “trade-off” and feasible
individuals.
There are several approaches that have been developed using multiobjective opti-
mization concepts to handle constraints, but due to space limitations we will not discuss
them here (see for example [2,13,8,9]).

576
A. Hern´andez Aguirre et al.
5
Description of IS-PAES
Our approach (called Inverted-Shrinkable Pareto Archived Evolution Strategy, or IS-
PAES) has been implemented as an extension of the Pareto Archived Evolution Strategy
(PAES) proposed by Knowles and Corne [5] for multiobjective optimization. PAES’s
main feature is the use of an adaptive grid on which objective function space is located
using a coordinate system. Such a grid is the diversity maintenance mechanism of PAES
and it constitutes the main feature of this algorithm. The grid is created by bisecting k
times the function space of dimension d = g +1. The control of 2kd grid cells means the
allocation of a large amount of physical memory for even small problems. For instance,
10 functions and 5 bisections of the space produce 250 cells. Thus, the ﬁrst feature
introduced in IS-PAES is the “inverted” part of the algorithm that deals with this space
usage problem. IS-PAES’s ﬁtness function is mainly driven by a feasibility criterion.
Global information carried by the individuals surrounding the feasible region is used to
concentrate the search effort on smaller areas as the evolutionary process takes place. In
consequence, the search space being explored is “shrunk” over time. Eventually, upon
termination, the size of the search space being inspected will be very small and will
contain the solution desired. The main algorithm of IS-PAES is shown in Figure 1.
maxsize: max size of ﬁle
c: current parent ∈X (decision variable space)
h:child of c ∈X, ah: individual in ﬁle that dominates h
ad: individual in ﬁle dominated by h
current: current number of individuals in ﬁle
cnew: number of individuals generated thus far
current = 1; cnew=0;
c = newindividual();
add(c);
While cnew≤MaxNew do
h = mutate(c); cnew+ =1;
if (c⪯h) then Label A
else if (h⪯c) then { remove(c); add(g); c=h; }
else if (∃ah ∈ﬁle | ah ⪯h) then Label A
else if (∃ad ∈ﬁle | h ⪯ad) then {
add( h ); ∀ad { remove(ad); current−=1 }
else test(h,c,ﬁle)
Label A
if (cnew % g==0) then c = individual in less densely populated region
if (cnew % r==0) then shrinkspace(ﬁle)
End While
Fig. 1. Main algorithm of IS-PAES
The function test(h,c,ﬁle) determines if an individual can be added to the external
memory or not. Here we introduce the following notation: x12x2 means x1 is located in

Use of Multiobjective Optimization Concepts to Handle Constraints
577
a less populated region of the grid than x2. The pseudo-code of this function is depicted
in Figure 2.
if (current < maxsize) then {
add(h);
if (h 2 c) then c=h }
else if (∃ap∈ﬁle | h 2 ap) then {
remove(ap); add(h)
if (h 2 c) then c = h; }
Fig. 2. Pseudo-code of test(h,c,ﬁle)
5.1
Inverted “Ownership”
PAES keeps a list of individuals on every grid location, but in IS-PAES each individ-
ual knows its position on the grid. Therefore, building a sorted list of the most dense
populated areas of the grid only requires to sort the k elements of the external memory.
In PAES, this procedure needs to inspect all 2kd locations in order to generate a list of
the individuals sorted by the density of their current location in the grid. The advantage
of the inverted relationship is clear when the optimization problem has many functions
(more than 10), and/or the granularity of the grid is ﬁne, for in this case only IS-PAES
is able to deal with any number of functions and granularity level.
5.2
Shrinking the Objective Space
Shrinkspace(ﬁle) is the most important function of IS-PAES since its task is the reduc-
tion of the search space. The pseudo-code of Shrinkspace(ﬁle) is shown in Figure 3.
xpob: vector containing the smallest value of either xi ∈X
xpob: vector containing the largest value of either xi ∈X
select(ﬁle);
getMinMax( ﬁle, xpob, xpob);
trim(xpob, xpob );
adjustparameters(ﬁle);
Fig. 3. Pseudo-code of Shrinkspace(ﬁle)
The function select(ﬁle) returns a list whose elements are the best individuals found
in ﬁle. The size of the list is set to 15% of maxsize. Thus, the goal of select(ﬁle) is

578
A. Hern´andez Aguirre et al.
m: number of constraints
i: constraint index
maxsize: max size of ﬁle
listsize: 15% of maxsize
constraintvalue(x,i): value of individual at constraint i
sortﬁle(ﬁle): sort ﬁle by objective function
worst(ﬁle,i): worst individual in ﬁle for constraint i
validconstraints={1,2,3,...,m};
i=ﬁrstin(validconstraints);
While (size(ﬁle) > listsize and size(validconstraints) > 0) {
x=worst(ﬁle,i)
if (x violates constraint i)
ﬁle=delete(ﬁle,x)
else validconstraints=removeindex(validconstraints,i)
if (size(validconstraints) > 0) i=nextin(validconstraints)
}
if (size(ﬁle) == listsize))
list=ﬁle
else
ﬁle=sort(ﬁle)
list=copy(ﬁle,listsize) *pick the best listsize elements*
Fig. 4. Pseudo-code of select(ﬁle)
to create a list with: 1) only the best feasible individuals, 2) a combination of feasible
and partially feasible individuals, or 3) the “best” infeasible individuals. The selection
algorithm is shown in Figure 4. Note that validconstraints (a list of indexes to the
problem constraints) indicates the order in which constraints are tested. One individual
(the worst) is removed at a time in this loop of constraint testing till there is none to
delete (all feasible), or 15% of the ﬁle is reached. The function getMinMax(ﬁle) ﬁnds
the extreme values of the decision variables represented by the individuals of the list.
Thus, the vectors xpob and xpob are found. Function trim(xpob, xpob) shrinks the feasible
space around the potential solutions enclosed in the hypervolume deﬁned by the vectors
xpob and xpob. Thus, the function trim (xpob, xpob) (see Figure 5) determines the new
boundaries for the decision variables.
The value of β is the percentage by which the boundary values of either xi ∈X
must be reduced such that the resulting hypervolume H is a fraction α of its previous
value. In IS-PAES all objective variables are reduced at the same rate β. Therefore, β
can be deduced from α as discussed next. Since we need the new hypervolume to be a
fraction α of the previous one, then
Hnew ≥αHold
n

i=1
(xt+1
i
−xt+1
i
) = α
n

i=1
(xt
i −xt
i)

Use of Multiobjective Optimization Concepts to Handle Constraints
579
n: size of decision vector;
xi: actual upper bound of the ith decision variable
xi: actual lower bound of the ith decision variable
xpob,i: upper bound of ith decision variable in population
xpob,i: lower bound of ith decision variable in population
∀i : i ∈{ 1, . . . , n }
slacki = 0.05 × (xpob,i −xpob,i)
width pobi = xpob,i −xpob,i; widtht
i = xt
i −xt
i
deltaMini =
β∗widtht
i−width pobi
2
deltai = max(slacki, deltaMini);
xt+1
i
= xpob,i + deltai; xt+1
i
= xpob,i −deltai;
if (xt+1
i
> xoriginal,i) then
xt+1
i
−= xt+1
i
−xoriginal,i; xt+1
i
= xoriginal,i;
if (xt+1
i
< xoriginal,i) then xt+1
i
+ = xoriginal,i −xt+1
i
;
xt+1
i
= xoriginal,i;
if (xt+1 > xoriginal,i) then xt+1
i
= xoriginal,i;
Fig. 5. Pseudo-code of trim
Either xi is reduced at the same rate β, thus
n

i=1
β(xt
i −xt
i) = α
n

i=1
(xt
i −xt
i)
βn
n

i=1
(xt
i −xt
i) = α
n

i
(xt
i −xt
i=1)
βn = α
β = α
1
n
Summarizing, the search interval of each decision variable xi is adjusted as follows
(the complete algorithm is shown in Figure 3):
widthnew ≥β × widthold
In our experiments, α = 0.90 worked well in all cases. Since α controls the shrinking
speed, it can prevent the algorithm from ﬁnding the optimum if small values are chosen.
In our experiments, values in the range [85%,95%] were tested with no effect on the
performance. The last step of shrinkspace() is a call to adjustparameters(ﬁle). The
goal is to re-start the control variable σ using: σi = (xi −xi)/√n i ∈(1, . . . , n). This
expression is also used during the initial generation of the EA. In that case, the upper
and lower bounds take the initial values of the search space indicated by the problem.
The variation of the mutation probability follows the exponential behavior suggested by
B¨ack [1].

580
A. Hern´andez Aguirre et al.
6
Comparison of Results
We have validated our approach with the benchmark for constrained optimization pro-
posed in [7]. Our results are compared against the homomorphous maps [6], which
is representative of the state-of-the-art in constrained evolutionary optimization. The
following parameters were adopted for IS-PAES in all the experiments reported next:
maxsize = 200, bestindividuals = 15%, slack = 0.05, r = 400. The maximum
number of ﬁtness function evaluations was set to 350,000, whereas the results of Koziel
and Michalewicz [6] were obtained with 1,400,000 ﬁtness function evaluations.
1. g01: Min: f(x) = 5 4
i=1 xi −5 4
i=1 x2
i −13
i=5 xi subject to: g1(x) = 2x1 +
2x2 + x10 + x11 −10 ≤0, g2(x) = 2x1 + 2x3 + x10 + x12 −10 ≤0, g3(x) =
2x2 + 2x3 + x11 + x12 −10 ≤0, g4(x) = −8x1 + x10 ≤0, g5(x) = −8x2 +
x11 ≤0, g6(x) = −8x3 + x12 ≤0, g7(x) = −2x4 −x5 + x10 ≤0, g8(x) =
−2x6 −x7 + x11 ≤0, g9(x) = −2x8 −x9 + x12 ≤0 where the bounds are
0 ≤xi ≤1 (i = 1, . . . , 9), 0 ≤xi ≤100 (i = 10, 11, 12) and 0 ≤x13 ≤1. The
global optimum is at x∗= (1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1) where f(x∗) = −15.
Constraints g1, g2, g3, g4, g5 and g6 are active.
2. g02: Max: f(x) =

n
i=1 cos4(xi)−2 n
i=1 cos2(xi)
√n
i=1 ix2
i
 subject to:
g1(x) = 0.75 −
n

i=1
xi ≤0
g2(x) =
n

i=1
xi −7.5n ≤0
(8)
where n = 20 and 0 ≤xi ≤10 (i = 1, . . . , n). The global maximum is unknown;
the best reported solution is [11] f(x∗) = 0.803619. Constraint g1 is close to being
active (g1 = −10−8).
3. g03: Max: f(x) = (√n)n n
i=1 xi subject to: h(x) = n
i=1 x2
i −1 = 0 where
n = 10 and 0 ≤xi ≤1 (i = 1, . . . , n). The global maximum is at x∗
i = 1/√n
(i = 1, . . . , n) where f(x∗) = 1.
4. g04: Min: f(x) = 5.3578547x2
3 + 0.8356891x1x5 + 37.293239x1 −40792.141
subject to:
g1(x) = 85.334407 + 0.0056858x2x5 + 0.0006262x1x4 −0.0022053x3x5 −92 ≤0
g2(x) = −85.334407 −0.0056858x2x5 −0.0006262x1x4 + 0.0022053x3x5 ≤0
g3(x) = 80.51249 + 0.0071317x2x5 + 0.0029955x1x2 + 0.0021813x2
3 −110 ≤0
g4(x) = −80.51249 −0.0071317x2x5 −0.0029955x1x2 −0.0021813x2
3 + 90 ≤0
g5(x) = 9.300961 + 0.0047026x3x5 + 0.0012547x1x3 + 0.0019085x3x4 −25 ≤0
g6(x) = −9.300961 −0.0047026x3x5 −0.0012547x1x3 −0.0019085x3x4 + 20 ≤0
(9)
where: 78 ≤x1 ≤102, 33 ≤x2 ≤45, 27 ≤xi ≤45 (i = 3, 4, 5). The
optimumsolutionisx∗= (78, 33, 29.995256025682, 45, 36.775812905788)where
f(x∗) = −30665.539. Constraints g1 y g6 are active.

Use of Multiobjective Optimization Concepts to Handle Constraints
581
5. g05 Min: f(x) = 3x1 + 0.000001x3
1 + 2x2 + (0.000002/3)x3
2 subject to:
g1(x) = −x4 + x3 −0.55 ≤0
g2(x) = −x3 + x4 −0.55 ≤0
h3(x) = 1000 sin(−x3 −0.25) + 1000 sin(−x4 −0.25) + 894.8 −x1 = 0
h4(x) = 1000 sin(−x3 −0.25) + 1000 sin(x3 −x4 −0.25) + 894.8 −x2 = 0
h5(x) = 1000 sin(−x4 −0.25) + 1000 sin(x4 −x3 −0.25) + 1294.8 = 0
(10)
where 0 ≤x1 ≤1200, 0 ≤x2 ≤1200, −0.55 ≤x3 ≤0.55, and −0.55 ≤
x4 ≤0.55. The best known solution is x∗= (679.9453, 1026.067, 0.1188764,
−0.3962336) where f(x∗) = 5126.4981.
6. g06 Min: f(x) = (x1 −10)3 + (x2 −20)3 subject to:
g1(x) = −(x1 −5)2 −(x2 −5)2 + 100 ≤0
g2(x) = (x1 −6)2 + (x2 −5)2 −82.81 ≤0
(11)
where 13 ≤x1 ≤100 and 0 ≤x2 ≤100. The optimum solution is x∗=
(14.095, 0.84296) where f(x∗) = −6961.81388. Both constraints are active.
7. g07 Min:
f(x) =
x2
1 + x2
2 + x1x2 −14x1 −16x2 + (x3 −10)2 + 4(x4 −5)2
+(x5 −3)2 + 2(x6 −1)2 + 5x2
7 + 7(x8 −11)2 + 2(x9 −10)2
+(x10 −7)2 + 45
(12)
subject to:
g1(x) = −105 + 4x1 + 5x2 −3x7 + 9x8 ≤0
g2(x) = 10x1 −8x2 −17x7 + 2x8 ≤0
g3(x) = −8x1 + 2x2 + 5x9 −2x10 −12 ≤0
g4(x) = 3(x1 −2)2 + 4(x2 −3)2 + 2x2
3 −7x4 −120 ≤0
g5(x) = 5x2
1 + 8x2 + (x3 −6)2 −2x4 −40 ≤0
g6(x) = x2
1 + 2(x2 −2)2 −2x1x2 + 14x5 −6x6 ≤0
g7(x) = 0.5(x1 −8)2 + 2(x2 −4)2 + 3x2
5 −x6 −30 ≤0
g8(x) = −3x1 + 6x2 + 12(x9 −8)2 −7x10 ≤0
(13)
where −10 ≤xi ≤10 (i = 1, . . . , 10). The global optimum is x∗= (2.171996,
2.363683, 8.773926, 5.095984, 0.9906548, 1.430574, 1.321644, 9.828726,
8.280092, 8.375927) where f(x∗) = 24.3062091. Constraints g1, g2, g3, g4,
g5 and g6 are active.
8. g08 Max: f(x) =
sin3(2πx1) sin(2πx2)
x3
1(x1+x2)
subject to: g1(x) = x2
1 −x2 + 1 ≤0,
g2(x) = 1−x1+(x2−4)2 ≤0 where 0 ≤x1 ≤10 and 0 ≤x2 ≤10.The optimum
solution is located at x∗= (1.2279713, 4.2453733) where f(x∗) = 0.095825. The
solutions is located within the feasible region.

582
A. Hern´andez Aguirre et al.
9. g09 Min:
f(x) = (x1 −10)2 + 5(x2 −12)2 + x4
3 + 3(x4 −11)2
+10x6
5 + 7x2
6 + x4
7 −4x6x7 −10x6 −8x7
(14)
subject to:
g1(x) = −127 + 2x2
1 + 3x4
2 + x3 + 4x2
4 + 5x5 ≤0
g2(x) = −282 + 7x1 + 3x2 + 10x2
3 + x4 −x5 ≤0
g3(x) = −196 + 23x1 + x2
2 + 6x2
6 −8x7 ≤0
g4(x) = 4x2
1 + x2
2 −3x1x2 + 2x2
3 + 5x6 −11x7 ≤0
(15)
10. g10 Min: f(x) = x1 + x2 + x3 subject to:
g1(x) = −1 + 0.0025(x4 + x6) ≤0
g2(x) = −1 + 0.0025(x5 + x7 −x4) ≤0
g3(x) = −1 + 0.01(x8 −x5) ≤0
g4(x) = −x1x6 + 833.33252x4 + 100x1 −83333.333 ≤0
g5(x) = −x2x7 + 1250x5 + x2x4 −1250x4 ≤0
g6(x) = −x3x8 + 1250000 + x3x5 −2500x5 ≤0
(16)
where 100 ≤x1 ≤10000, 1000 ≤xi ≤10000, (i = 2, 3), 10 ≤xi ≤1000,
(i = 4, . . . , 8). The global optimum is: x∗= (579.3167, 1359.943, 5110.071,
182.0174, 295.5985, 217.9799, 286.4162, 395.5979), where f(x∗) = 7049.3307.
g1, g2 and g3 are active.
11. g11 Min: f(x) = x2
1 + (x2 −1)2 subject to: h(x) = x2 −x2
1 = 0 where: −1 ≤
x1 ≤1, −1 ≤x2 ≤1. The optimum solution is x∗= (±1/
√
2, 1/2) where
f(x∗) = 0.75.
The comparison of results is summarized in Table 1. It is worth indicating that IS-
PAES converged to a feasible solution in all of the 30 independent runs performed. The
discussion of results for each test function is provided next (HM stands for homomor-
phous maps):
For g01 both the best and the mean results found by IS-PAES are better than the
results found by HM, although the difference between the worst and the best result is
higher for IS-PAES. For g02, again both the best and mean results found by IS-PAES
are better than the results found by HM, but IS-PAES has a (slightly) higher difference
between its worst and best results. In the case of g03, IS-PAES obtained slightly better
results than HM, but in this case it also has a lower variability. It can be clearly seen that
for g04, IS-PAES had a better performance with respect to all the statistical measures
evaluated. The same applies to g05 in which HM was not able to ﬁnd any feasible
solutions (the best result found by IS-PAES was very close to the global optimum). For
g06, again IS-PAES found better values than HM (IS-PAES practically converges to the
optimum in all cases). For g07 both the best and the mean values produced by IS-PAES
were better than those produced by HM, but the difference between the worst and best

Use of Multiobjective Optimization Concepts to Handle Constraints
583
Table 1. Comparison of the results for the test functions from [7]. Our approach is called IS-PAES
and the homomorphous maps approach [6] is denoted by HM. N.A. = Not Available.
BEST RESULT
MEAN RESULT
WORST RESULT
TF OPTIMAL
IS-PAES
HM
IS-PAES
HM
IS-PAES
HM
g01
-15.0
-14.995
-14.7864
-14.909
-14.7082
-12.4476
-14.6154
g02 -0.803619
-0.8035376
-0.79953
-0.798789
-0.79671
-0.7855539
-0.79199
g03
-1.0
-1.00050019
-0.9997
-1.00049986
-0.9989
-1.00049952
-0.9978
g04 -30665.539 -30665.539
-30664.5
-30665.539
-30655.3
-30665.539
-30645.9
g05
5126.498
5126.99795
N.A.
5210.22628
N.A.
5497.40441
N.A.
g06 -6961.814 -6961.81388
-6952.1
-6961.81387
-6342.6
-6961.81385
-5473.9
g07
24.306
24.3410221
24.620
24.7051034
24.826
25.9449662
25.069
g08 -0.095825 -0.09582504 -0.0958250 -0.09582504 -0.0891568 -0.09582504 -0.0291438
g09
680.630
680.638363
680.91
680.675002
681.16
680.727904
683.18
g10
7049.331
7055.11415
7147.9
7681.59187
8163.6
9264.35787
9659.3
g11
0.750
0.75002984
0.75
0.74992803
0.75
0.74990001
0.75
result is slightly lower for HM. For g08 the best result found by the two approaches
is the optimum of the problem, but IS-PAES found this same solution in all the runs
performed, whereas HM presented a much higher variability of results. In g09, IS-PAES
had a better performance than HM with respect to all the statistical measures adopted.
For g10 none of the two approaches converged to the optimum, but IS-PAES was much
closer to the optimum and presented better statistical measures than HM. Finally, for
g11, HM presented slighly better results than IS-PAES, but the difference is practically
negligible.
Summarizing, we can see that IS-PAES either outperformed or was very close to
the results produced by HM even when it only performed 25% of the ﬁtness functions
evaluations of HM. IS-PAES was also able to approach the global optimum of g05, for
which HM did not ﬁnd any feasible solutions.
7
Conclusions and Future Work
We have presented a new constraint-handling approach that combines multiobjective
optimization concepts with an efﬁcient reduction mechanism of the search space and
a secondary population. We have shown how our approach overcomes the scalability
problem of the original PAES (which was proposed exclusively for multiobjective op-
timization) from which it was derived, and we also showed that the approach is highly
competitive with respect to a constraint-handling approach that is representative of the
state-of-the-art in the area.
The proposed approach illustrates the usefulness of multiobjective optimization con-
cepts to handle constraints in evolutionary algorithms used for single-objective optimiza-
tion. Note however, that this mechanism can also be used for multiobjective optimization
and that is in fact part of our future work.
Another aspect that we want to explore in the future is the elimination of all of the
parameters of our approach using online or self-adaptation. This task, however, requires

584
A. Hern´andez Aguirre et al.
a careful analysis of the algorithm because any online or self-adaptation mechanism may
interefere with the mechanism used by the approach to reduce the search space.
Acknowledgments. The ﬁrst and second authors acknowledge support from CONACyT
project No. P-40721-Y. The third author acknowledges support from NSF-CONACyT
project No. 32999-A.
References
1. Thomas B¨ack. Evolutionary Algorithms in Theory and Practice. Oxford University Press,
New York, 1996.
2. Eduardo Camponogara and Sarosh N. Talukdar. A Genetic Algorithm for Constrained and
Multiobjective Optimization. In Jarmo T. Alander, editor, 3rd Nordic Workshop on Genetic
Algorithms and Their Applications (3NWGA), pages 49–62, Vaasa, Finland, August 1997.
University of Vaasa.
3. Carlos A. Coello Coello, David A. Van Veldhuizen, and Gary B. Lamont.
Evolutionary
Algorithms for Solving Multi-Objective Problems. Kluwer Academic Publishers, New York,
May 2002. ISBN 0-3064-6762-3.
4. David E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning.
Addison-Wesley Publishing Company, Reading, Massachusetts, 1989.
5. Joshua D. Knowles and David W. Corne. Approximating the Nondominated Front Using the
Pareto Archived Evolution Strategy. Evolutionary Computation, 8(2):149–172, 2000.
6. Slawomir Koziel and Zbigniew Michalewicz.
Evolutionary Algorithms, Homomorphous
Mappings, and Constrained Parameter Optimization. Evolutionary Computation, 7(1):19–
44, 1999.
7. Zbigniew Michalewicz and Marc Schoenauer.
Evolutionary Algorithms for Constrained
Parameter Optimization Problems. Evolutionary Computation, 4(1):1–32, 1996.
8. I. C. Parmee and G. Purchase. The development of a directed genetic search technique for
heavily constrained design spaces. In I. C. Parmee, editor, Adaptive Computing in Engineer-
ing Design and Control-’94, pages 97–102, Plymouth, UK, 1994. University of Plymouth,
University of Plymouth.
9. Tapabrata Ray, Tai Kang, and Seow Kian Chye. An Evolutionary Algorithm for Constrained
Optimization. In Darrell Whitley et al., editor, Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO’2000), pages 771–777, San Francisco, California, 2000.
Morgan Kaufmann.
10. Jon T. Richardson, Mark R. Palmer, Gunar Liepins, and Mike Hilliard. Some Guidelines
for Genetic Algorithms with Penalty Functions. In J. David Schaffer, editor, Proceedings of
the Third International Conference on Genetic Algorithms (ICGA-89), pages 191–197, San
Mateo, California, June 1989. George Mason University, Morgan Kaufmann Publishers.
11. T.P. Runarsson and X. Yao. Stochastic Ranking for Constrained Evolutionary Optimization.
IEEE Transactions on Evolutionary Computation, 4(3):284–294, September 2000.
12. Alice E. Smith and David W. Coit. Constraint Handling Techniques—Penalty Functions. In
ThomasB¨ack,DavidB.Fogel,andZbigniewMichalewicz,editors,HandbookofEvolutionary
Computation, chapter C 5.2. Oxford University Press and Institute of Physics Publishing,
1997.
13. Patrick D. Surry and Nicholas J. Radcliffe. The COMOGA Method: Constrained Optimisation
by Multiobjective Genetic Algorithms. Control and Cybernetics, 26(3):391–412, 1997.

Evolution Strategies with Exclusion-Based
Selection Operators and a Fourier Series
Auxiliary Function
Kwong-Sak Leung and Yong Liang
Department of Computer Science & Engineering,
The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
{ksleung, yliang}@cse.cuhk.edu.hk
Abstract. To improve the eﬃciency of the currently known evolution-
ary algorithms, we have proposed two complementary eﬃciency speed-up
strategies in our previous research work respectively: the exclusion-based
selection operators and the Fourier series auxiliary function. In this pa-
per, we combine these two strategies together to search the global optima
in parallel, one for optima in large attraction basins and the other for op-
tima in very narrow attraction basins respectively. They can compliment
each other to improve evolutionary algorithms (EAs) on eﬃciency and
safety. In a case study, the two strategies have been incorporated into
evolution strategies (ES), yielding a new type of accelerated exclusion
and Fourier series auxiliary function ES: the EFES. The EFES is ex-
perimentally tested with a test suite containing 10 complex multimodal
function optimization problems and compared against the standard ES
(SES). The experiments all demonstrate that the EFES consistently and
signiﬁcantly outperforms the SES in eﬃciency and solution quality.
1
Introduction
Evolutionary algorithms (EAs) are global search procedures based on the evolu-
tion of a set of solutions viewed as a population of interacting individuals. They
have been successfully used for optimization problems. But for solving large scale
and complex optimization problems, EAs have not demonstrated themselves to
be very eﬃcient [4] [5]. We believe the main factor which causes low eﬃciency
of the current EAs is the convergence towards undesired attractors. This phe-
nomenon occurs when the objective function has some local optima with large
attraction basins or its global optimum is located in a small attraction basin in
a minimization case. The relationship between the convergence to a global min-
imum and the geometry (landscape) of the diﬃcult function problems is very
important. If the population of EAs gets trapped into suboptimal states, which
locate in comparative large attraction basins, then it is diﬃcult for the variation
operators to produce an oﬀspring which outperforms its parents. In the second
case, if global optima are located in relatively small attraction basins, and the
individuals of EAs have not found these basins yet, the probability of the varia-
tion operators to produce oﬀspring which locate in these small attraction basins
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 585–597, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

586
K.-S. Leung and Y. Liang
is quite low. In both cases, the stochastic mechanism of EAs yields unavoid-
able resampling, which increases the algorithm’s complexity and decelerates the
search eﬃciency.
To overcome these two limitations that cause low eﬃciency of the currently
known EAs, we have proposed two complementary eﬃciency speed-up strategies
in our previous research work respectively [6]: the exclusion-based selection op-
erators and the Fourier series auxiliary function. The exclusion-based selection
operators could eﬃciently prevent the individuals of EAs from getting into the at-
traction basins of local optima. While the Fourier series auxiliary function could
guide an algorithm to search for optima with small attraction basins eﬃciently.
Moreover, this strategy can compensate the deﬁciency of the exclusion-based
selection operators on the algorithm’s safety, i.e. the avoidance of excluding a
global optimum contained in a narrow attraction basin. In this paper, we devel-
oped a new algorithm, the EFES, which incorporate the exclusion and Fourier
series auxiliary function into the evolution strategies (ES) [2]. We expect that
EFES will have the advantages of both two strategies — eﬃciency and safety.
This paper is organized as follows: we will explain the two novel eﬃciency
speed-up strategies for EAs implementation in Sections 2 and 3 respectively.
Particularly, a set of “exclusion-based” selection operators is proposed in Sec-
tion 2 to simulate the “survival of the ﬁttest” principle more powerfully and
a Fourier series auxiliary function is introduced in Section 3. In Section 4, we
will demonstrate how to embed the exclusion-based selection operators and the
Fourier series auxiliary function into ES to generate EFES. In Section 5, the
EFES is experimentally examined, analyzed and compared with a set of typical,
multi-modal function optimization problems. The last section is the conclusion.
2
Exclusion-Based Selection Operators
This section deﬁnes and explores a somewhat diﬀerent selection mechanism —
the exclusion-based selection operators.
Any EAs solve an optimization problem, say,
(P)
min{f(x) : x ∈Ω},
where f : Ω⊂Rn →R is a function. For simplicity, consider the problem
(P) with the domain Ωspeciﬁed by Ω= [u1, v1] ∗[u2, v2] ∗· · · [un, vn]. The new
scheme is based on the cellular partition methodology. Given an integer d, let
hi = vi−ui
d
, we deﬁne
σ(j1, j2, · · · , jn) =
{x = (x1, x2, · · · , xn) ∈Ω: (ji −1) × hi ≤xi −ui ≤ji × hi, 1 ≤ji ≤d}
and let the subregion σ(j1, j2, · · · , jn), be a cell, and the cell collection Γ(d) is
called a cellular partition of Ω. The vector representing the center of the cell is
deﬁned by
m(σ(j1, · · · , jn)) = ((u1 + (j1 + 1
2) × h1), · · · , (un + (jn + 1
2) × hn)).

Evolution Strategies with Exclusion-Based Selection Operators
587
Let us ﬁrst formalize the exclusion operators with reference to the cellular
partition Γ(d) of Ω.
Deﬁnition 1: An exclusion operator is a computationally veriﬁable test for
nonexistence of solution to problem (P) which can be implemented on every cell
of Γ(d).
For example, assume that f satisﬁes the Lipschitz condition: |f(x) −f(y)| ≤
α||x −y||. Then, for any σ ∈Γ(d), there is a global minimizer x∗in σ only if
| f(m(σ)) −f(x∗) |≤α||m(σ) −x∗|| ≤α
2 ω(σ), where ω(σ) is the mesh size of
Γ(d), deﬁned by
ω(σ) = max1≤i≤n{vi −ui
d
}
This implies that f(m(σ)) −α
2 ω(σ) ≤f(x∗) is a necessary condition for the
existence of the global optimum of (P). Thus f(m(σ)) −α
2 ω(σ) > f(x∗) gives a
nonexistence test for the solution. This test is an exclusion operator because it
can be computationally veriﬁed in each cells of Γ(d).
By Deﬁnition 1, any exclusion operator can serve to computationally test
the nonexistence of the solution of (P) in any given cells of Γ(d). It therefore
can be used to check if a given cell in Γ(d) is prospective or not as a global
optimum (or, as a portion of a attraction basin of a global optimum) of (P).
Accordingly, the non-global optimum, or, more loosely, less prospective cells can
be identiﬁed and can be deleted from further consideration. This is the key
mechanism that is adopted in the present work to accelerate EAs. Hereafter
any selection mechanism based on such exclusion principle will be called an
exclusion-based selection operator.
Let σ be an arbitrary cell in Γ(d) with its center m(σ) and the mesh size ω(σ).
The following provides us with a series of exclusion operators for minimization
problems. Please note that these operators (tests) can be equally applied to
maximization problems by reversing the inequality signs.
Example 1: Concavity Test Operator
Suppose f is continuous, then a necessary condition for a point x to be a
global minimizer is that f is convex at the neighborhood of x. Therefore, when
d is suﬃciently large, the following tests (E1) and (E2) are exclusion operators.
(E1): There is a direction e ∈Rn such that
f(m(σ)) > 1
2[f(x+) + f(x−)]
where x−= x −ηe, x+ = x + ηe and 0 < η ≤1
2ω(σ)
(E2): There is a direction e ∈Rn such that max{f(x+), f(x−)} > fbestand
[f(x+) −f(m(σ))][f(x−) −f(m(σ))] < 0
where x+ and x−are the same as in (E1), and fbest
is the current known
best ﬁtness value.
The tests (E1) and (E2) can immediately follow from the observations that
every m(σ) is in the interior of Ω, the (E1) features the concave property of f

588
K.-S. Leung and Y. Liang
on cell σ, and (E2) characterizes the convex and concave overlapped property in
which no global optimum of f exists.
Example 2: Lipschitz Test Operator
Let £(f, α) denote the family of all continuous functions that satisﬁes the
Lipschitz conditions: | f(x) −f(y) |≤α(Ω)||x −y||, x, y ∈Ω∈Γ(d). Then the
following tests (E3) and (E4) are exclusion operators.
(E3): fbest < f(m(σ)) −α(σ)
2 ω(σ), if f ∈£(f, α)
(E4): fbest < f(m(σ)) −α(σ)
8 ω2(σ), if f ′ ∈£(f, α)
where fbest is again the “best-so-far” ﬁtness value, and f ′ is the derivative of f.
Example 3: Formal Series Test Operator
Let ℑbe the class of all functions that can be expressed as a ﬁnite number
of superpositions of formal series and their absolute values A(f) of f is deﬁned
by
A(f) = A(f (1)) +
k

j=2
A(f (j))(x).
For any f ∈ℑand g ≻A(f), it is known [8] that the following basic inequality
holds:
|f(x) −f(y)| ≤A(g)(|y| + |x −y|) −A(g)(|y|), ∀x, y ∈Rn
This implies, similar to test (E3), that the following test (E5) is an exclusion
operator.
(E5): There is a formal series g ≻A(f)such that
fbest ≤f(m(σ)) −[A(g)(|m(σ)| + 1
2ω(σ)) −A(g)(|m(σ)|)]
Various other exclusion operators may also be constructed by virtue of other
delicate mathematical tools such as interval arithmetic [1] and the cell mapping
methods [7] [8].
Remark 1:
(i) The exclusion-based selection is the main component and contributor to
the accelerated evolutionary algorithms (the fast-EAs). Aiming at suppressing
the resampling eﬀect of EAs, such type of selection provides a smart guidance to
EAs search towards promising areas through eliminating non-promising areas.
Diﬀerent in principle from the conventional selection operators, the construction
of which is based on suﬃcient condition for the existence of solution to (P), the
exclusion-based selection operators can be constructed based on necessary con-
dition. This presents a general methodology of constructing exclusion operators.
The above listed tests (E1)-(E5) show such examples of the construction.
(ii) The application of exclusion-based operator has another advantage: It can
very naturally incorporate some useful properties of the objective function into
the EAs search, providing other acceleration means whenever possible. Indeed,
Examples 1-3 all have taken advantage of properties of f in certain ways (say,
continuity in Examples 1, Lipschitz conditions in Example 2 and analyticity in

Evolution Strategies with Exclusion-Based Selection Operators
589
Example 3). As a general rule, the more exclusive properties of f are utilized, the
more accurate a test could be deduced (For example, (E3) is more accurate than
(E4) when the Lipschitz condition was applied for the derivative f ′ instead of
for f ). These diﬀerent tests deduced from diﬀerent properties by no means have
to be applied for a problem in the same time. They can, for instance, be applied
either independently, or with several others together, or totally simultaneously,
depending on the available information on f that can actually be made use of.
3
A Fourier Series Auxiliary Function
The exclusion-based selection operators are the eﬃcient accelerating operators
based on the interval arithmetic and the cell mapping methods. However, if the
function optimization problem is very complex, say there are many sharp basins,
the optima may be excluded by mistake using the above operators. Meanwhile,
searching an optimum with a small attraction basin is diﬃcult for standard
evolutionary algorithms. To solve the above problem, a Fourier series auxiliary
function is introduced in this section.
As we know, if f(x) is continuous or merely piecewise continuous (continuous
except for ﬁnitely many ﬁnite jumps in the interval of integration), then the
Fourier series of f(x) is convergent. Its sum is f(x), except at a point x0 at
which f(x) is discontinuous and the sum of the series is the average of the left-
and right-hand limits of f(x) at x0 [3]. We deﬁne the ﬁnite partial sum of the
Fourier series called F k
ℓ(x)(e.g. for one dimension),
F k
ℓ(x) =
k

n=ℓ
(an cos 2nπ
u −v x + bn sin 2nπ
u −v x), x ∈[u, v].
The inﬁnite Fourier series F ∞
1 (x) converges to f(x) at any point, but the
convergent speed of the ﬁnite partial sum F ℓ
1(x)(ℓ< ∞) is diﬀerent at each
point. For numerical function optimization, the ﬁnite partial sum F ℓ
1(x∗)(ℓ< ∞)
converges to f(x∗) much slower for optimum x∗with small attraction basin in
f(x) than that of optimum x∗with a large attraction basin. This indicates the
partial sum |F ∞
ℓ(x∗)| = |f(x∗) −F ℓ
1(x∗)| at x∗with a small attraction basin is
larger than at x∗with a large attraction basin. Because when an integer k →∞,
the coeﬃcients of the kth Fourier series term ak and bk →0, the inﬁnite partial
sum F ∞
k (x) →0. So we consider the ﬁnite partial sum F k
ℓ(x)(ℓ< k) instead of
F ∞
ℓ(x). The proposition |F ∞
ℓ(x∗)| > |F ∞
ℓ(x)| equals to |F k
ℓ(x∗)| > |F k
ℓ(x)|(ℓ<
k) when x∗locates in a small attraction basin.
The features of the ﬁnite partial sum F k
ℓ(x)(ℓ< k) include enlarging small
attraction basins, and smoothing large attraction basins of f(x) as shown in
Fig.1. We have designed three strategies: the region partition strategy, the one-
element strategy and the double integral strategy to construct auxiliary function
g(x). The ﬁrst strategy is designed for representing all optima with small attrac-
tion basins by the F k
ℓ(x)(ℓ< k) with a small number of terms. The second
one is for signiﬁcantly reducing the computational complexity. The last one is

590
K.-S. Leung and Y. Liang
0
0.5
1
1.5
2
2.5
3
3.5
4
−3
−2
−1
0
1
2
3
4
5
6
7
       F(f)
(100,...,1000) 
f(x) 
Fig. 1. A schematic illustration for the feature of the Fourier ﬁnite partial sum F k
ℓ(ℓ=
100, k = 1000).
for expanding the dummy optima out of the original feasible region, and keep-
ing the original position of the optimum unchanged (Fig.2). Consequently, we
could construct the auxiliary function g(x) using the ﬁnite partial sum of the
one element of the Fourier trigonometric system
g(x) =
k

m=ℓ
am
n

i=1
cos mxi,
where n is dimensions, ℓ= 100 and k = 200;
am =
1
u −2v
 2v
u
f(x) cos 2mπ
2v −uxdx;
to locate the optima of the origin function f(x).
Since the auxiliary function g(x) can enlarge the small attraction basins of the
optima and ﬂatten the large attraction basins, the g(x) can guide an algorithm to
search the optima with small attraction basins more eﬃcient, and these optima
are diﬃcult to ﬁnd in the original objective function by EAs. Furthermore, this
strategy runs in parallel with ﬁrst strategy and compensates the deﬁciency of
the exclusion-based selection operators on the algorithm’s risk of missing optima
with many sharp attraction basins.
4
EFES: The Evolution Strategies with Exclusion-Based
Selection Operators and a Fourier Series Auxiliary
Function
In this section, we demonstrate how all the strategies developed in the previous
sections could be embedded into the known evolutionary algorithms, to yield new
versions of the algorithms. We will particularly take evolution strategies (ES)
as an example. Consequently, a new type of ES the EFES will be developed.
The incorporation of the developed strategies with other known evolutionary
algorithms might be straightforward, and could be done similarly as that in the
example presented below.

Evolution Strategies with Exclusion-Based Selection Operators
591
−5
0
5
−5
0
5
−0.2
0
0.2
0.4
(a)
−5
0
5
−5
0
5
0
0.05
0.1
(b)
−5
0
5
10
15
−5
0
5
10
15
−0.01
0
0.01
0.02
(c)
Fig. 2. A graphical illustration of the g(x). (a) shows the complete Fourier system rep-
resentation; (b) shows one element Fourier representation with parameters determined
by [u, v]; (c) shows one element Fourier representation with parameters are determined
by [u, 2v].
The EFES Algorithm Is Given as Follows:
I. Initialization Step
I.1. Set k = 0, Ω(0) = Ω, n(0) = 1 and f 0
best = 108; set the pre-determined
number of cellular partition d, and stopping criteria ε1 and ε2, where ε1 is
the solution precision requirement and ε2 is the space discretization precision
tolerance.
I.2. Initialize all the ES parameters including:
N−the population size;
M−the maximum number of ES evolution.
II. Iteration Step (Epoch)
II.1.ES Search in the auxiliary function g(x):
(a) randomly select N individuals from Ω(0) to form an initial population;
(b) perform M steps of ES search, yielding the currently best minimum g(x∗)
in the region Ω(0);
(c) if generation = 1, we put these points into population of ES for f(x); if
generation > 1, we will compare the ﬁtness of f(x) at these points with
the current optimum of f(x), and only put the points which values of f(x)
smaller than the current optimum of f(x).
The two steps II.2 and II.3 below will use the cellular partition method,
assuming Ω(k) consists of n(k) subregions, say, Ω(k) = Ω(k)
1
∪Ω(k)
2
∪...∪Ω(k)
n(k).
For each subregion Ω(k)
i
, do the following:
II.2.ES Search in the objective function f(x):
(a) use the ﬁxed points by step II.1 and some random selected points from
Ω(k)
i
to form initial population;
(b) perform M steps of ES search, yielding the currently best minimum f (k)
i
in the subregion Ω(k)
i
;
(c) let f (k)
best := min{f (k)
i
, f (k−1)
best
}.
II.3. Exclusion-based Selection:
With the “best-so-far” ﬁtness value f (k)
best guided, eliminate the “less prospec-
tive” individuals (cells) from Ω(k)
i
by employing appropriate exclusion oper-
ator(s) and a speciﬁc exclusion scheme. The remaining cells are denoted by
Π(k)
i
.

592
K.-S. Leung and Y. Liang
II.4. Space Shrinking:
(a) generate a cell Ω(k+1)
i
such that Π(k)
i
⊂Ω(k+1)
i
and ω(Ω(k+1)
i
) < ω(Ω(k)
i
)
whenever possible; in this case, set Ω(k+1)
i1
= Ω(k+1)
i
and Ω(k+1)
i2
= ∅;
(b) bisect Π(k)
i
and construct two large cells Ω(k+1)
i1
and Ω(k+1)
i2
such that
Π(k)
i
⊂Ω(k+1)
i1
∪Ω(k+1)
i2
and max{ω(Ω(k+1)
i1
), ω(Ω(k+1)
i2
)} < ω(Π(k)
i
).
III. Termination Test Step
If ω(Ω(k)) ≤ε2 and |f (k) −f (k−1)| ≤ε1 hold for three consecutive iteration steps,
then stop; Otherwise, go to step II with k := k + 1, and
Ω(k+1) = Ω(k+1)
i1
∪Ω(k+1)
i2
· · · ∪Ω(k)
n(k).
Detailed remarks on each step of the above algorithm are given in [6].
The iteration step (step II) is the core of the algorithm. Step II.1 performs
the ES for M generations to search in the auxiliary function g(x), the best indi-
viduals are potential global optima, which are diﬃcult to ﬁnd in f(x). Adopting
the comparison criterion could eﬃciently eliminate interference caused by other
kinds of points (like discontinuous point). In order to ensure the safety of the
algorithm, we do not apply the exclusion-based selection operators in the search
process of g(x).
Table 1. The test suite used in our experiments.
f1 = 4x2
1 −2.1x4
1 + 1
3 x6
1 + x1x2 −4x2
2 + 4x4
2 , n = 2, x ∈[−5, 5];
f2 = x2
2 + 2x2
1 −0.3cos(3πx2) −0.4cos(3πx1) + 0.7, n = 2, x ∈[−5.12, 5.12] ;
f3 =
n/4

i=1
(x4i−3 +10x4i−2)2 +5(x4i−1 −x4i)2 +(x4i−2 −2x4i−1)2 +10(x4i−3 −x4i)4,n = 40, x ∈[−100, 100];
f4 =
n/4

i=1
3[exp(x4i−3 −x4i−2)2 + 100(x4i−2 −x4i−1)6 + [tan(x4i−1 −x4i)4 + x8
4i−3],n = 40, x ∈[−4, 4];
f5 =
n/4

i=1
{100[x2
4i−3 −x4i−2]2 +(x4i−3 −1)2 +90(x2
4i−1 −x4i)2 +10.1[(x4i−2 −1)2 +(x4i −1)2]+19.8(x4i−2 −
1)(x4i −1)}, n = 40, x ∈[−50, 50];
f6 =
1
4000
n
i=1
x2
i −
n
i=1
cos( xi
√
i
) + 1, n = 40, x ∈[−600, 600];
5
Simulations and Comparisons
We will experimentally evaluate the performance of the EFES, and compare it
with the standard evolution strategies (SES). The EFES was implemented with
k = 10, N = 1000 and ε1 = ε2 = 10−8. The maximal number M of ES evolution
was taken uniformly to be 500 on f(x) and g(x) respectively, called a epoch. All
these parameters and schemes were kept invariant unless noted otherwise. For
fairness of comparison, we also implemented the SES with the same parameter
settings and the same initial population. The maximum number of SES evolution

Evolution Strategies with Exclusion-Based Selection Operators
593
−5
0
5
−5
0
5
(b)
−5
0
5
−5
0
5
(c)
−1
0
1
−1
−0.5
0
0.5
1
(d)
−1
0
1
−1
−0.5
0
0.5
1
(e)
−1
0
1
−1
−0.5
0
0.5
1
(f)
−1
0
1
−1
0
1
0
5
(a)
Fig. 3. The search space (the white cells) shrinking process when the EFES applied
to f1. (b) shows the locations of the global optima of f1 (denoted by “•”); (c) and
(d) show the cells excluded by the algorithm in epochs 1 and 2 (current best search
point is denoted by “*”. In epoch 3, the search space is forked into two subregions in
(e) because the two global optima all lie in the husk layer of the search space. After
forking, one more epoch yields the two global optima with precision 10−8.
is 105. All experiments were run for ten times with random initial populations,
and the averages of the ten runs were taken as the ﬁnal result.
The test suite used in our experiments include those minimization problems
listed in Table 1. The suite mainly contains some representative, complex, mul-
timodal functions with many local optima and being highly nonseparable in
features. Our experiments were divided into two groups with diﬀerent purposes.
We report the results of each group below.
Explanatory Experiments: This group of experiments aims to exhibit the
evolution processes of the EFES in detail. To clearly demonstrate the running
process of the exclusion-based selection operators, this simulation ﬁrst studies
the ES with exclusion-based selection operators and without the Fourier series
auxiliary function to solve the problem f1. Fig.3-(a) shows the function f1 that
has only two global minimizers. The evolution details (particularly, the search
space shrinking details) of the EFES when applied to minimization of this func-
tion is presented in Figs.3 (b)-(f), which demonstrate clearly how the remaining
cells are accumulated around the currently acceptable search point (denoted by
“*”) for the global minimum, and how local minima are successively excluded.
This demonstrates the common features of the EFES. The experiments also
demonstrate that the number of subregions, n(k), contained in search space
Ω(k) in each step are uniformly bounded. These bounds are seen to be very
small in each case, but vary with the problems under consideration. Particularly
we observed in the experiments that the bounds n(k) are generally related (ac-

594
K.-S. Leung and Y. Liang
tually, proportional) to the number of the global optima of the function to be
optimized.
−1
0
1
−1
0
1
0
2
4
(a)
−1
0
1
−1
−0.5
0
0.5
1
(b)
−1
0
1
−1
−0.5
0
0.5
1
(c)
−1
0
1
−1
−0.5
0
0.5
1
(d)
(2) 
(3) 
(1) 
Fig. 4. A schematic illustration that the EFES to search on f2. (a) shows random
version of f2. (b) and (c) show the results after a ﬁrst and second epoches respectively.
The points (1), (2) and (3) show the optima (−1, 10−5), (−0.5, 10−3) and (−0.5, 10−3),
respectively.
We designed the test function based on benchmark multimodal functions
f2 to demonstrate the feature of the auxiliary function g(x). Three optima
with small attraction basins are generated randomly in the feasible solution
space of f(x) and they have the following properties respectively: (−0.5, 10−3),
(−0.5, 10−3) and (−1, 10−5), representing the value of the optima and the width
of its attraction basin in the bracket, respectively. The locations of these optima
are decided in random.
Fig.4-(a) shows random version of f2. Figs.4-(b) and (c) show the results
of the EFES within the ﬁrst and second epoches respectively, the points ∗are
obtained from the EFES in the search in g(x). Fig.4-(b) demonstrates that the
EFES can ﬁnd the two optima (−0.5, 10−3) in the ﬁrst epoch(1000 generations),
however the optimum (−1, 10−5) is not represented in g(x) at this time. In
the second epoch, after bisecting the whole space between these two optima
(−0.5, 10−3), the optimum (−1, 10−5) is represented by g(x) and identiﬁed by the
EFES(c.f. Fig.4-(c)). These results conﬁrm that the EFES can ﬁnd these three
optima with small attraction basins in g(x). Fig.4-(d) shows SES is converged to
the optimum f1(0, 0) = 0 after 10000 generations, which is the global optimum of
the original version of function f2. However, this point is not the global optimum
of random version of f2.
Comparisons: To assess the eﬀectiveness and eﬃciency of the EFES, its per-
formance is compared with the standard ES (SES). We deﬁne that f7 −f10 are
the random versions of f3 −f6. Functions f7 −f10 have one optimum with a
small attraction basin (−1, 10−5), representing the value of the optimum and
the width of its attraction basin in the bracket, respectively. The location of
the optimum are decided in random. The comparisons are made in terms of the
solution quality and computational eﬃciency and on the basis of applications of
the algorithms to the test functions f3 −f10 in the test suite. As each algorithm

Evolution Strategies with Exclusion-Based Selection Operators
595
has its associated overhead, a time measurement was taken as a fair indication of
how eﬀectively and eﬃciently each algorithm could solve the problems. The so-
lution quality and computational eﬃciency are therefore respectively measured
by the solution precision and the ﬁtness attained by each algorithm within an
equal period of ﬁxed time. Unless mentioned otherwise, the time is measured in
minutes as measured on the computer.
Table 2 and Fig.5 present the solution quality comparison results in terms of
f (t)
best when the EFES and SES are applied to the test functions f3 −f10. We can
observe that while the EFES consistently converges to the global optima for all
test functions, SES is unable to ﬁnd the global solutions for f3 −f6 and their
random versions, f7 −f10. On the other hand, Table 2 and Fig.5 also show that
the EFES can always locate the global optimum with higher solution precision.
That is, the EFES outperforms SES in solution eﬀectiveness.
The computational eﬃciency comparison results are shown in Fig.5. It is
clear from these ﬁgures that the EFES signiﬁcantly outperform the SES for all
test functions. In addition, we could see from Figs.5 (e)-(h) that the eﬃciency in-
creases because the auxiliary function g(x) eﬃciently guide the EFES to ﬁnd the
global optimum with a small attraction basin. Even with such eﬃciency speed-
up, the guaranteed monotonic convergence of the EFES is still clearly observed
in all these experiments. All these comparisons show the superior performance
of the EFES in eﬃcacy and eﬃciency.
Table 2. The results of the EFES and SES when applied to test functions
Function
Epoches
The running time (minute)
The solution precision attained
SES
EFES
SES
EFES
f3
5
17.2
17.2
10−4
10−8
f4
5
19.0
19.0
10−3
10−8
f5
5
21.6
21.6
10−3
10−8
f6
5
16.8
16.8
10−3
10−8
f7
3
17.2
7.3
10−4
10−8
f8
2
19.0
5.8
10−3
10−8
f9
3
21.6
15.8
10−3
10−8
f10
3
16.8
9.1
10−3
10−8
6
Conclusion
In this paper, we have developed a new evolutionary algorithm— EFES, which
incorporates two strategies, exclusion-based selection operators and the Fourier
series auxiliary function into ES, to solve global optimization problems.
The EFES has been experimentally tested with a diﬃcult test suite consisted
of two groups of complex multimodal function optimization examples. The per-
formance of the EFES is compared against the standard evolution strategies

596
K.-S. Leung and Y. Liang
0
10
10
0
(a)
0
10
20
10
0
(b)
0
10
20
10
0
(c)
0
10
10
0
(d)
0
10
10
−5
10
0
(e)
0
10
20
10
−5
10
0
(g)
0
10
10
−5
10
0
(h)
0
10
10
−5
10
0
(f)
Fig. 5. The solution quality and computational eﬃciency comparisons of the EFES and
SES when applied to problems f3(a), f4(b), f5(c), f6(d), f7(e), f8(f), f9(g), f10(h),
where the abscissa is the time (minutes), and the ordinate is the solution precision
|f (t)
best −f ∗|(absolute error)(Keys: v – EFES, . – SES).
(SES). All experiments have demonstrated that the EFES consistently and sig-
niﬁcantly outperforms the SES in eﬃciency and solution quality, particularly
towards problems whose global optima are located in small attraction basins.
Since the Fourier series auxiliary function could be used on discontinuous func-
tion optimization problems and prevent accidental deletion of the global optima
with very narrow attraction basins by the exclusion-based selection operators,
EFES has wider application, both for continuous and discontinuous problems.
Acknowledgment. This research was partially supported by RGC Earmarked
Grant 4212/01E of Hong Kong SAR and RGC Research Grant Direct Allocation
of the Chinese University of Hong Kong.
References
1. E.Hansen, Global optimization Using Interval Analysis, Marcel Dekker, INC, 1992
2. H.P.Schwefel, Evolution and Optimum Seeking, Chichester, UK: John Wiley, 1995
3. R.E.Edwards, Fourier Series, Second Edition, Springer-Verlag New York Heidelberg
Berlin, 1999
4. Thomas B¨ack, Evolutionary algorithms in theory and practice: evolution strate-
gies, evolutionary programming, genetic algorithms, Oxford, New York: Oxford
University Press, 1996.
5. Y.Xin, Evolutionary computation: theory and applications, Singapore; New Jersey:
World Scientiﬁc, 1999
6. Y.Liang & K.S.Leung, Fast-GA: A Genetic Algorithm with Exclusion-based Se-
lections, Proceedings of 2001 WSES International Conference on: Evolutionary
Computations (EC’01), pp. 638(1)–638(6), Spain.

Evolution Strategies with Exclusion-Based Selection Operators
597
7. Z.B.Xu, J.S.Zhang & W.Wang, A cell exclusion algorithm for ﬁnding all the so-
lutions of a nonlinear system of equation, Applied Mathematics and Computation,
80, 1996, pp. 181–208
8. Z.B.Xu, J.S.Zhang, & Y.W.Leung, A general CDC formulation for specializing the
cell exclusion algorithms of ﬁnding all zeros of vector functions, Appled Mathemat-
ics and Computation, 86(2 & 3), 1997, pp. 235–259

 
E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 598–609, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Ruin and Recreate Principle Based Approach for the
Quadratic Assignment Problem
Alfonsas Misevicius
Kaunas University of Technology, Department of Practical Informatics,
Studentu St. 50−400a, LT−3031 Kaunas, Lithuania
misevi@soften.ktu.lt
Abstract. In this paper, we propose an algorithm based on so-called ruin and
recreate (R&R) principle. The R&R approach is conceptual simple but at the
same time powerful meta-heuristic for combinatorial optimization problems.
The main components of this method are a ruin (mutation) procedure and a
recreate (improvement) procedure. We have applied the R&R principle based
algorithm for a well-known combinatorial optimization problem, the quadratic
assignment problem (QAP). We tested this algorithm on a number of instances
from the library of the QAP instances − QAPLIB. The results obtained from the
experiments show that the proposed approach appears to be significantly
superior to a "pure" tabu search on real-life and real-life like QAP instances.
1   Introduction
The quadratic assignment problem (QAP) is formulated as follows. Let two matrices
A = (aij)n×n and B = (bkl)n×n and the set Π of permutations of the integers from 1 to n be
given. Find a permutation π = (π(1), π(2), ..., π(n)) ∈ Π that minimizes
∑∑
=
=
=
n
i
n
j
j
i
ijb
a
z
1
1
)
(
)
(
)
(
π
π
π
.
(1)
The context in which Koopmans and Beckmann [1] first formulated this problem
was the facility location problem. In this problem, one is concerned with locating n
units on n sites with some physical products flowing between the facilities, and with
distances between the sites. The element aij is the "flow" from unit i to unit j, and the
element bkl represents the distance between the sites k and l. The permutation
π = (π(1), π(2), ..., π(n)) can be interpreted as an assignment of units to sites (π(i) de-
notes the site what unit i is assigned to). Solving the QAP means searching for an as-
signment π that minimizes the "transportation cost" z.
An important area of the application of the QAP is computer-aided design (CAD),
more precisely, the placement of electronic components into locations (positions) on a
board (chip) [2,3,4]. Other applications of the QAP are: campus planning [5], hospital
layout [6], image processing (design of grey patterns) [7], testing of electronic devices
[8], typewriter keyboard design [9], etc (see [10,11] for a more detailed description of
the QAP applications).

Ruin and Recreate Principle Based Approach for the Quadratic Assignment Problem         599
The quadratic assignment problem is one of the most complex combinatorial opti-
mization problems. It has been proved that the QAP is NP-hard [12]. To this date
problems of size n, where n>30, are not practically solvable in terms of obtaining ex-
act solutions. Therefore, heuristic approaches have to be used for solving medium-
and large-scale QAPs: ant algorithms [13,14], genetic algorithms [15,16,17], greedy
randomized search (GRASP) [18], iterated local search [19], simulated annealing
[20,21], tabu search [22,23,24]. An extended survey of heuristics for the QAP one can
find in [11,25].
This paper is organized as follows. In Section 2, basic definitions are given. Sec-
tion 3 outlines the ruin and recreate approach for combinatorial optimization prob-
lems. Section 4 describes the R&R principle based algorithm for the quadratic as-
signment problem. The results of the computational experiments on the QAP
instances are presented in Section 5. Section 6 completes the paper with conclusions.
2   Preliminaries
Let S be a set of solutions (solution space) of a combinatorial optimization problem
with objective function f: S → R. Furthermore, let Ν: S → 2
S be a neighbourhood
function which defines for each s ∈ S a set Ν(s) ⊆ S − a set of neighbouring solutions
of s. Each solution s′ ∈ Ν(s) can be reached from s by an operation called a move.
Suppose, S = {s | s = (s(1), s(2), ..., s(n))}, where n is the cardinality of the set, i.e.
the problem size. Given a solution s from S, a λ-exchange neighbourhood function
Νλ(s) is defined as follows:
}
)
,
(
 ,
 | 
{
)
(
λ
λ
≤
′
∈
′
′
=
s
s
d
S
s
s
s
 (2≤ λ ≤n),
(2)
where d(s,s′) is the "distance" between solutions s and s′:
∑
=
′
−
=
′
n
i
i
s
i
s
s
s
d
1
|)
(
)
(
|
sgn
)
,
(
.
For the QAP, the commonly used case is λ = 2, i.e. the 2-exchange function Ν2. In
this case, a transformation from the current permutation (solution) π to the neigh-
bouring permutation π′ can formally be defined by using a special move (operator) −
2-way perturbation pij: Π → Π (i,j = 1, 2, ..., n), which exchanges ith and jth elements
in the current permutation. The notation 
ij
p
⊕
=
′
π
π
 means that π′ is obtained from π
by applying pij. The difference in the objective function values (when moving from
the current permutation to the neighbouring one) can be calculated in O(n) operations:
[
]
∑
≠
=
−
−
+
−
−
+
−
−
=
n
j
i
k
k
i
k
j
k
kj
ki
k
i
k
j
jk
ik
j
i
i
j
ji
ij
z
b
b
a
a
b
b
a
a
b
b
a
a
j
i
,
,1
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
)(
(
)
)(
(
)
)(
(
)
,
,
(
π
π
π
π
π
π
π
π
π
π
π
π
π
,
(3)
where aii(or bii) = const, ∀i ∈ {1, 2, ..., n}.
If the matrix A and/or matrix B are symmetric formula (3) becomes much more
simpler. For example, if the matrix B is symmetric, one can transform the matrix A to
the symmetric one A′ by adding up corresponding entries of A.

600          A. Misevicius
Formula (3) then reduces to the following formula
∑
≠
=
−
′
−
′
=
n
j
i
k
k
k
i
k
j
jk
ik
z
b
b
a
a
j
i
,
,1
)
(
)
(
)
(
)
(
)
)(
(
)
,
,
(
π
π
π
π
π
,
(4)
where 
ki
ik
ik
a
a
a
+
=
′
, ∀i,k ∈ {1, 2, ..., n}, i ≠ k. Moreover, for two consecutive solu-
tions π and 
uv
p
⊕
=
′
π
π
, if all values ∆z(π,i,j) have been stored, the values ∆z(π′,i,j)
(i ≠ u,v and j ≠ u,v) can be computed in time O(1) [24]:
).
)(
(
  
          
          
)
)(
(
)
,
,
(
)
,
,
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
j
u
j
v
i
v
i
u
uj
vj
vi
ui
u
j
v
j
v
i
u
i
ju
jv
iv
iu
z
b
b
b
b
a
a
a
a
b
b
b
b
a
a
a
a
j
i
z
j
i
π
π
π
π
π
π
π
π
π
π
π
π
π
π
π
π
π
π
−
+
−
−
+
−
+
−
+
−
−
+
−
+
=
′
(5)
(If i = u or i = v or j = u or j = v, then formula (3) is applied.)
3   Ruin and Recreate Principle
The ruin and recreate (R&R) principle was formulated by Schrimpf et al. [26]. Note
also that this principle has some similarities with other approaches, among them:
combined local search heuristic (chained local optimization) [27], iterated local search
[19], large step Markov chains [28], variable neighbourhood search [29]. As men-
tioned in [26], the basic element of the R&R principle is to obtain better optimization
results by a reconstruction (destruction) of an existing solution and a following im-
provement (rebuilding) procedure. By applying this type of process frequently, i.e. in
an iterative way, one seeks for high quality solutions. In the first phase of the process,
one reconstructs a significant part of the existing solution, roughly speaking, one "ru-
ins" the current solution. (That is a relatively easy part of the method.) In the second
phase, one tries to rebuild the solution just "ruined" as best as one can. Hopefully, the
new solution is better than the solution(s) obtained in the previous phase(s) of the im-
provement. (Naturally, the improvement is the harder part of the method.) There are a
lot of different ways to reconstruct (ruin) and, especially, to improve the solutions. So,
we think of the ruin and recreate approach as a meta-heuristic − not a pure heuristic.
It should be noted that the approach we have outlined differs a little from the origi-
nal R&R version. The main distinguishing feature is that, in our version of R&R,
there is considered rather an iterative process of some kind of mutations (as recon-
structions) and local searches (as improvements) applied to solutions. In the original
R&R, in contrast, one speaks rather of a sequence of disintegrations and constructions
of solutions (see Schrimpf et al. [26]).
The advantage of the R&R approach − which is very similar to the iterated local
search [19] − over the well-known random multistart method (that is based on multi-
ple starts of local improvements applied to randomly generated solutions) is that, in-
stead of generating (constructing) new solutions from scratch, a better idea is to re-
construct (a part of) the current solution: continuing search from this reconstructed
solution may allow to escape from a local optimum and to find better solutions. (So,
improvements of the reconstructions of local optima are considered rather than im-
provements of the purely random solutions.) For the same computation time, many
more reconstruction improvements can be done than when starting from randomly

Ruin and Recreate Principle Based Approach for the Quadratic Assignment Problem         601
generated solutions, because the reconstruction improvement procedure requires only
a few steps to reach the next local optimum.
The R&R principle can be thought of as a method with so-called "large" moves
(moves consisting of a large number of perturbations) [26], instead of "small" moves
(e.g. pair-wise interchanges) that typically take place in "classical" algorithms. For
simple problems, there is no need to use "large" moves − "small" moves are enough.
The "large" moves − that are also referred to as "kick" moves [27] − are very impor-
tant when dealing with complex problem (QAP among them). These problems can
often be seen as "discontinuous": if one walks in the solution space, the qualities of
the solutions can be significantly different, i.e. the "landscapes" of these problems can
be very rugged.
Two aspects regarding to the "kick" moves are of high importance (see also [19]).
Firstly, the "kick" move should be large (strong) enough to allow to leave the current
locally optimal solution and to enable the improvement procedure to find, probably,
better solution. However, if the "kick" move is too large, the resulting algorithm
might be quite similar to random multistart, which is known to be not a very efficient
algorithm. Secondly, the "kick" move should be small (weak) enough to keep features
(characteristics) of the current local minimum since parts of the solution may be close
to the ones of the globally optimal solution. However, if the "kick" move is too small,
the improvement procedure may return to the solution to which the "kick" move has
been applied, i.e. a cycling may occur.
In the simplest case, it is sufficient to use for the "kick" move a certain number of
random perturbations (further, we shall refer those perturbations to as mutation). Do-
ing so can be interpreted as a random search of higher order neighbourhoods Νλ
(λ>2), i.e. random variable neighbourhood search [29].
Additional component of the R&R method is an acceptance criterion that is used to
decide which solution is to be chosen for the "ruin". Here, two main alternatives are
so-called intensification (exploitation) and diversification (exploration). Intensifica-
tion is achieved by choosing only the best local optimum as a candidate for the "ruin".
On the other hand, diversification takes place if every new local optimum is accepted
for the reconstruction.
The paradigm of the ruin and recreate approach, which is conceptually surprisingly
simple, is presented in Fig. 1.
4   Ruin and Recreate Principle Based Algorithm for the QAP
All we need by creating the ruin and recreate principle based algorithm for a specific
problem is to design four components: 1) an initial solution generation (construction)
procedure, 2) a solution improvement procedure (we shall also refer this procedure to
as local search procedure), 3) a solution reconstruction (mutation) procedure, and 4) a
candidate acceptance (choosing) rule.
Now we present details of the ruin and recreate principle based algorithm for the
QAP, which is entitled R&R-QAP.

602          A. Misevicius
Fig. 1. The paradigm of the ruin and recreate (R&R) principle based procedure
4.1   Initial Solution Generation
We use randomly generated permutations as initial permutations for the algorithm
R&R-QAP. These permutations can be generated by a very simple procedure.
4.2   Local Search
In principle, any local search concept based algorithm can be applied at the improve-
ment phase of R&R method. In the simplest case, a greedy descent ("first improve-
ment") algorithm or steepest descent ("best improvement") algorithm (also known as
a "hill climbing") can be used. Yet, it is possible to apply more sophisticated algo-
rithms, like limited simulated annealing or tabu search. In our algorithm, we use the
tabu search algorithm, more precisely, a modified version of the robust tabu search
algorithm due to Taillard [24]. The framework of the algorithm can briefly be de-
scribed as follows. Initialize tabu list, T = (tij)n×n, and start from an initial solution π.
Continue the following process until a termination criterion is satisfied (a predeter-
mined number of trials is executed): a) find a neighbour π′′ of the current solution π
in such a way that 
)
(
min
arg
)
(
2
π
π
π
π
′
=
′′
′
∈
′
z
N
, where
} )
(
)
(
or 
 
not tabu)
 
is
 
 
and
 
 ( ,)
(
 | 
{
)
(
2
2
π
π
π
π
π
π
π
π
′′′
<
′
⊕
=
′
∈
′
′
=
′
z
z
p
p
ij
ij
 (π′′′ is
the best so far solution); b) replace the current solution π by the neighbour π′′ (even if
z(π′′) − z(π)>0), and use as a starting point for the next trials; c) update the tabu list T.
The last found solution, which is locally optimal, is declared as a solution of the tabu
search algorithm. The detailed template of the tabu search based local search algo-
rithm for the QAP is presented in Fig. 2.
procedure R&R { ruin and recreate procedure }
generate (or construct) initial solution s°
s• := recreate(s°) { recreate (improve) the initial solution }
s∗ := s•, s := s•
repeat { main loop }
s  := choose_candidate_for_ruin(s,s•)
s~ := ruin(s)
{ ruin (reconstruct) the current solution }
s• := recreate(~) { recreate (improve) the ruined solution }
if s• is better than s∗ then s∗ := s•
until termination criterion is satisfied
return s∗
end { R&R }

Ruin and Recreate Principle Based Approach for the Quadratic Assignment Problem         603
function local_search(π,n,τ) { local search (based on tabu search) for the QAP }
{ π − the current permutation, n − problem size, τ − number of iterations }
set lower and higher tabu sizes hmin and hmax
π• := π { π• denotes the best permutation found }
calculate the objective function differences δij = ∆z(π,i,j): 
1
,1
−
=
n
i
;
n
i
j
,1
+
=
T := 0
choose h randomly between hmin and hmax
i := 1, j := 1, k := 1
improved := FALSE
while (k≤τ) or improved do begin { main loop }
δmin := ∞ { δmin − minimum difference in the objective function values }
for l := 1 to |Ν2| do begin
))
1,1
,1
if(
,
,
if(
:
+
−
<
<
=
i
n
i
i
n
j
i
,
)1
,1
,
if(
:
+
+
<
=
i
j
n
j
j
tabu := if(tij≥k,TRUE,FALSE), aspired := if(z(π) + δij<z(π•),TRUE,FALSE)
if ((δij<δmin) and not tabu) or aspired then begin
δmin := δij, u := i, v := j
end { if }
end { for l }
if δmin<∞ then begin
uv
p
⊕
= π
π :
 { replace the current permutation by the new one }
if z(π)<z(π•) then π• := π
improved := if(δuv<0,TRUE,FALSE)
update the differences δlm: 
1
,1
−
=
n
l
;
n
l
m
,1
+
=
tuv := k + h
if k mod 2hmax = 0 then choose new h randomly between hmin and hmax
end { if }
end { while }
return π = π•
end { local_search }
Fig. 2. Template of the tabu search based local search algorithm for the QAP.
Notes. 1. The objective function difference δij is calculated according to (3) (or 4) formula.
2. The difference δlm is updated according to (5) formula.
3. The function if(x,y1,y2) returns y1 if x=TRUE, otherwise it returns y2
4.3   Mutation
As mentioned in Section 3, a mutation of the current solution is achieved by per-
forming a complex move, i.e. a move in the neighbourhood Νλ, where λ>2. This can
be modelled by generating µ=(λ+1)/2 sequential elementary perturbations, like pij;
here, the parameter µ (µ≥2) is referred to as a mutation level.

604          A. Misevicius
Fig. 3. Template of the mutation procedure for the QAP.
Note. The function randint(x,y) returns integer random, uniformly distributed number between
x and y
Fig. 4. Template of the ruin and recreate (R&R) principle based algorithm for the QAP
procedure R&R-QAP { ruin and recreate principle based algorithm for the QAP }
{ input:
A,B − flow and distance matrices, n − problem size }
{ Q − the total number of iterations }
{ α, βmin, βmax − the control parameters (α>0, 0<βmin≤βmax≤1) }
{ output: π∗ − the best permutation found }
τ := max(1,αn), µmin := max(2,βminn), µmax := max(2,βmaxn)
generate random initial permutation π°
π• := local_search(π°,n,τ)
π∗ := π•, π := π•
µ := µmin − 1 { µ is the current value of the mutation level }
for q := 1 to Q do begin { main loop of R&R-QAP }
)
(
π
π
π
π
π
,
),
(
)
(
if
:
•
• <
=
z
z
{ π is the candidate for the mutation }
)
(
min
max
µ
µ
µ
µ
µ
,1
,
if
:
+
<
=
π
~ := mutation(π,n,µ)
{ mutate (reconstruct) the permutation π }
π• := local_search(π
~,n,τ)
{ try to improve the mutated permutation }
if z(π•)<z(π∗) then begin
π∗ := π•, µ := µmin − 1 { save the best permutation, reset the mutation level }
end { if }
end { for q }
return π∗
end { R&R-QAP }
function mutation(π,n,µ) { mutation procedure for the QAP }
{ π − the current permutation, n − problem size, µ − mutation level (2≤µ≤n) }
π• := π
for k := 1 to µ do begin { main loop }
i := randint(1,n), j := randint(1,n−1)
if i≤j then j := j + 1
ij
p
⊕
= π
π :
 { replace the current permutation by the new one }
if z(π)<z(π•) then break
end { for k }
return π
end { mutation }

Ruin and Recreate Principle Based Approach for the Quadratic Assignment Problem         605
We can add more robustness to the R&R method if we let vary the parameter µ in
some interval, say [µmin,µmax] ⊆ [2,n] (n is the problem size), during the execution of
the algorithm. Two ways are possible when varying µ: random and deterministic. In
the first case, "a coin is tossed"; in the second one, µ is varied sequentially within the
interval [µmin,µmax] starting from µmin. Once maximum value µmax has been reached (or a
better locally optimum solution has been found), the value of µ is dropped to the
minimum value µmin, and so on. The last case is used in our implementation. The de-
tailed template of the mutation procedure is presented in Fig. 3.
4.4   Candidate Acceptance
In our algorithm, we accept only the best locally optimal solution as a candidate for
the mutation. The candidate solution at qth iteration is defined according to the fol-
lowing formula 
(
)
)
1
(
)
1
(
)
(
,
),
(
)
(
if
−
•
−
• <
=
q
q
q
z
z
π
π
π
π
π
, where π
(q) is the candidate so-
lution at the current iteration, π
(q−1) is the solution before the last local search execu-
tion (the best so far solution), and π• is the solution after the current local search
execution.
The template of the resulting R&R approach based algorithm is presented in Fig. 4.
Note that, for convenience, the parameters α>0, βmin, βmax ∈ (0,1] are used instead of τ,
µmin, µmax, respectively.
5   Computational Experiments and Results
We have carried out a number of computational experiments in order to test the
performance of the algorithm R&R-QAP. The following types of the QAP instances
taken from the quadratic assignment problem library QAPLIB [30] were used:
a) real-life instances (instances of this class are real world instances from practical
applications of the QAP, among them: chr25a, els19, esc32a, esc64a, kra30a, ste36a,
tai64c (also known as grey_8_8_13 [7]));
b) real-life like instances (they are generated in such a way that the entries of the
matrices A and B resemble a distribution from real-life problems; the instances are
denoted by tai20b, tai25b, tai30b, tai35b, tai40b, tai50b, tai60b, tai80b, tai100b (tai∗b)).
For the comparison, we used the tabu search algorithm due to Taillard [24] entitled
as the robust tabu search algorithm − RTS-QAP, because it is among the best algo-
rithms for the quadratic assignment problem. As a performance measure, the average
deviation from the best known solution is chosen. The average deviation, θavg, is de-
fined according to the formula 
%]
[ 
)
(
100
avg
avg
b
b
z
z
z
−
=
θ
, where zavg is the average
objective function value over W restarts of the algorithm, and zb is the best known
value (BKV) of the objective function. (Note: BKVs are from [30].)
All the experiments were carried out on x86 Family 6 processor. The computations
were organized in such a way that both algorithms use identical initial assignments,
and require the similar CPU times. The execution of the algorithms is controlled by a
fixed a priori number of iterations. Some differences of the run time for RTS-QAP

606          A. Misevicius
and R&R-QAP are due to non-deterministic behaviour of the local search procedure
used in R&R-QAP. The results of the comparison, i.e. the average deviations from
BKV for both RTS-QAP and R&R-QAP, as well as approximated CPU times per re-
start (in seconds) are presented in Table 1. The deviations are averaged over 100 re-
starts. The best values are printed in bold face.
Table 1. Comparison of the algorithms on real-life and real-life like instances (Q =50, α = 0.1)
Looking at Table 1, it turns out that the efficiency of the algorithms depends on the
type of the instances (problems) being solved. For some real-life instances, R&R-
QAP produces only slightly better results than RTS-QAP. For the real-life like prob-
lems, the situation is different: for these problems, R&R-QAP is much more better
than RTS-QAP. The results for the particular instances differ dramatically (see, for
example, the results for the instances tai20b, tai25b, tai35b).
Two directions that may lead to the improvement of the results of R&R-QAP are
as follows: the parameter-improvement-based direction, and the algorithm’s-idea-
improvement-based direction. First of all, the results can be improved by a more accu-
rate tuning the control parameters α, βmin and βmax. This can be seen from Table 1: by
the proper choosing the values βmin, βmax the average deviation can be lowered noticea-
bly (see, for example, the results for chr25a, els19, tai25b, tai35b). The results can also
be improved by increasing the value of the parameter Q, but at the cost of a longer
computation time. Some results for the instances tai∗b are shown in Table 2.

Ruin and Recreate Principle Based Approach for the Quadratic Assignment Problem         607
Table 2. Additional computational results of R&R-QAP (α=0.01, βmin=0.45, βmax=0.55). The
following notations are used: W − the total number of restarts, W
1% − the number of restarts at
which the solution that is within 1% optimality was found, W∗ − the number of restarts at which
best known solution was found, tavg − the average CPU time per restart (in seconds)
Instance
name
R&R-QAP(Q=1000)
θavg
W/W
1%/W∗tavg
R&R-QAP(Q=10000)
θavg
W/W
1%/W∗tavg
R&R-QAP(Q=100000)
θavg
W/W
1%/W∗tavg
tai20b
0.000
30/30/300.17
−−−−−−−
−−−−−−−
tai25b
0.000
30/30/300.36
−−−−−−−
−−−−−−−
tai30b
0.000
30/30/300.67
−−−−−−−
−−−−−−−
tai35b
0.083
30/30/19 1.1
0.037
30/30/24 10.7
0.000
30/30/30
107
tai40b
0.134
30/28/27 1.7
0.000
30/30/30 17.0
−−−−−−−
tai50b
0.224
30/30/8 3.5
0.169
30/30/6
34.9
0.027
30/30/28
349
tai60b
0.587
30/20/5 6.2
0.000
30/30/30 62.4
−−−−−−−
tai80b
0.598
30/25/0 16.4
0.432
30/29/5
164
0.198
30/30/19
1640
tai100b
0.195
30/30/0 35.6
0.121
30/30/5
356
0.086
30/30/10
3560
It should be stressed that R&R-QAP was successful in finding a large number of
new best solutions for so-called problems of grey densities. Problems of this type are
described in [7,31] under the name grey_n1_n2_m, where m is the density of the grey
(0≤m≤n=n1⋅n2), and n1⋅n2 is the size of a frame. New solutions for the instance family
grey_16_16_∗ are presented in Table 3.
Table 3. New best known solutions for grey density problems
Instance
name
Previous
best known
value
a
New best
known
value
Instance
name
Previous
best known
value
a
New best
known
value
grey_16_16_25
2216160
2215714
grey_16_16_57
14798834
14793682
grey_16_16_43
7795062
7794422
grey_16_16_58
15395948
15363628
grey_16_16_44
8219428
8217264
grey_16_16_61
17202312
17194812
grey_16_16_45
8677670
8676802
grey_16_16_62
17824828
17822806
grey_16_16_46
9134172
9130426
grey_16_16_65
19859448
19848790
grey_16_16_49
10523016
10518838
grey_16_16_66
20655396
20648754
grey_16_16_51
11517060
11516840
grey_16_16_67
21461130
21439396
grey_16_16_52
12019082
12018388
grey_16_16_68
22271676
22234020
grey_16_16_54
13098850
13096646
grey_16_16_69
23086332
23049732
grey_16_16_55
13668486
13661614
grey_16_16_70
23898390
23852796
grey_16_16_56
14238840
14229492
a comes from [31].

608          A. Misevicius
6   Conclusions
The quadratic assignment problem is a very difficult combinatorial optimization
problem. In order to obtain satisfactory results in a reasonable time, heuristic algo-
rithms are to be applied. One of them, a ruin and recreate principle based algorithm
(R&R-QAP) is described in this paper. The proposed algorithm applies a reconstruc-
tion (mutation) to the best solution found so far and subsequently applies an im-
provement (local search) procedure. This scheme − despite its conceptual simplicity,
easy programming and practical realization − allowed to achieve very good results
with small amount of the computation time. The results from the comparison with the
robust tabu search algorithm ("pure" tabu search algorithm) show that R&R-QAP
produces much more better results than this algorithm (one of the most efficient heu-
ristics for the QAP), as far as real-life and real-life like problems are concerned. The
power of R&R-QAP is also corroborated by the fact that many new best known solu-
tions were found for the largest available QAP instances, so-called grey density
problems.
There are several possible directions to try to improve the performance of the pro-
posed algorithm: a) implementing a faster local search procedure; b) investigating
new reconstruction (mutation) operators; c) introducing a memory based mechanism
when selecting the solutions (candidates) for the reconstruction; d) incorporating
R&R-QAP into hybrid genetic algorithms as an initial population generation and/or
local improvement procedure.
References
1.
Koopmans, T., Beckmann, M.: Assignment Problems and the Location of Economic Ac-
tivities. Econometrica 25 (1957) 53–76
2.
Hanan, M., Kurtzberg, J.M.: Placement Techniques. In: Breuer, M.A. (ed.): Design Auto-
mation of Digital Systems: Theory and Techniques, Vol.1. Prentice-Hall (1972) 213–282
3.
Hu, T.C., Kuh, E.S. (ed.): VLSI Circuit Layout: Theory and Design. IEEE Press, New
York (1985)
4.
Steinberg, L.: The Backboard Wiring Problem: A Placement Algorithm. SIAM Review 3
(1961) 37–50
5.
Dickey, J.W., Hopkins, J.W.: Campus Building Arrangement Using TOPAZ. Transporta-
tion Research 6 (1972) 59–68
6.
Elshafei, A.N.: Hospital Layout as a Quadratic Assignment Problem. Operations Research
Quarterly 28 (1977) 167–179
7.
Taillard, E.: Comparison of Iterative Searches for the Quadratic Assignment Problem. Lo-
cation Science 3 (1995) 87–105
8.
Eschermann, B., Wunderlich, H.J.: Optimized Synthesis of Self-testable Finite State Ma-
chines. 20th International Symposium on Fault-Tolerant Computing (FFTCS 20) (1990)
9.
Burkard, R.E., Offermann, J.: Entwurf von Schreibmaschinentastaturen mittels Quadra-
tischer Zuordnungsprobleme. Zeitschrift fuer Operations Research 21 (1977) 121–132
10. Burkard, R.E.: Locations with Spatial Interactions: The Quadratic Assignment Problem.
In: Mirchandani, P.B., Francis, R.L. (eds.): Discrete Location Theory. Wiley, New York
(1991) 387–437

Ruin and Recreate Principle Based Approach for the Quadratic Assignment Problem         609
11. Burkard, R.E., Çela, E., Pardalos, P.M., Pitsoulis, L.: The Quadratic Assignment Problem.
In: Du, D.Z., Pardalos, P.M. (eds.): Handbook of Combinatorial Optimization, Vol.3.
Kluwer (1998) 241–337
12. Sahni, S., Gonzalez, T.: P-complete Approximation Problems. J. of ACM 23 (1976) 555–
565
13. Gambardella, L.M., Taillard, E., Dorigo, M.: Ant Colonies for the Quadratic Assignment
Problems. J. of the Operational Research Society 50 (1999) 167–176
14. Stuetzle, T., Dorigo, M.: ACO Algorithms for the Quadratic Assignment Problem. In:
Corne, D., Dorigo, M., Glover, F. (eds.): New Ideas in Optimization. McGraw-Hill (1999)
33–50
15. Ahuja, R.K., Orlin, J.B., Tiwari, A.: A Greedy Genetic Algorithm for the Quadratic As-
signment Problem. Computers & Operations Research 27 (2000) 917–934
16. Fleurent, C., Ferland, J.A.: Genetic Hybrids for the Quadratic Assignment Problem. In:
Pardalos, P.M., Wolkowicz, H. (eds.): Quadratic Assignment and Related Problems.
DIMACS Series in Discrete Mathematics and Theoretical Computer Science, Vol.16.
AMS, Providence (1994) 173–188
17. Merz, P., Freisleben, B.: A Genetic Local Search Approach to the Quadratic Assignment
Problem. Proceedings of the Seventh International Conference on Genetic Algorithms
(ICGA'97) (1997)
18. Li, Y., Pardalos, P.M., Resende, M.G.C.: A Greedy Randomized Adaptive Search Proce-
dure for the Quadratic Assignment Problem. In: Pardalos, P.M., Wolkowicz, H. (eds.):
Quadratic Assignment and Related Problems. DIMACS Series in Discrete Mathematics
and Theoretical Computer Science, Vol.16. AMS, Providence (1994) 237–261
19. Stuetzle, T.: Iterated Local Search for the Quadratic Assignment Problem. Tech. Report
AIDA-99-03, Darmstadt, Germany (1999)
20. Boelte, A., Thonemann, U.W.: Optimizing Simulated Annealing Schedules with Genetic
Programming. European J. of Operational Research 92 (1996) 402–416
21. Connolly, D.T.: An Improved Annealing Scheme for the QAP. European J. of Operational
Research 46 (1990) 93–100
22. Battiti, R., Tecchiolli, G.: The Reactive Tabu Search. ORSA J. on Computing 6 (1994)
126–140
23. Skorin-Kapov, J.: Tabu Search Applied to the Quadratic Assignment Problem. ORSA J. on
Computing 2 (1990) 33–45
24. Taillard, E.: Robust Taboo Search for the QAP. Parallel Computing 17 (1991) 443-455
25. Çela, E.: The Quadratic Assignment Problem: Theory and Algorithms. Kluwer, Dordrecht
(1998)
26. Schrimpf, G., Schneider, K., Stamm-Wilbrandt, H., Dueck, V.: Record Breaking Optimi-
zation Results Using the Ruin and Recreate Principle. J. of Computational Physics 159
(2000) 139–171
27. Martin, O., Otto, S.W.: Combining Simulated Annealing with Local Search Heuristics.
Annals of Operations Research 63 (1996) 57–75
28. Martin, O., Otto, S.W., Felten, E.W.: Large-step Markov Chains for the Traveling Sales-
man Problem. Complex Systems 5 (1991) 299–326
29. 	
, N., Hansen, P.: Variable Neighbourhood Search. Computers & Operations
Research 24 (1997) 1097–1100
30. Burkard, R.E., Karisch, S., Rendl, F.: QAPLIB – A Quadratic Assignment Problem Li-
brary. J. of Global Optimization 10 (1997) 391–403
31. Taillard, E., Gambardella, L.M.: Adaptive Memories for the Quadratic Assignment Prob-
lem. Tech. Report IDSIA-87-97, Lugano, Switzerland (1997)

 
	

	
		

	

		
 

	








 


	















 



!	



  	
	

	
 	
 	


		 	
 	


 
"

#

$

$
"
$	%

$


$&
'	
("


	
	
)





'
$




'

	$
*
+$$'
	
'






"

'
)



,
$
$'


*
"
$
	


"

$$

"

""'
$



"
,

"
(

"
	
'

$


("


	
(
)


'
*
-
(








'&
"	
"

'



 &"
)


'
*




	
"
$'	
(




&




("
"
$


"
	
'
		&

'




"
)


'
*
.

$


(
	
'



&

/
'


/
("
"

0

'&
&1


(
#



	
'*


$$

'&

$

"

&


'&
"	

	

'

"	


$$'*
"




"


"
(

'"	

2
	
("

	
'
'
	
2

	
'




'
""
	
'

*



"



'
"




/

&

/

	
'


/*
 


















 
!







"
#$%
#$$%&
' 

(






)



"













&
 



 
*



 






"
+
"



&
,

 







(













)


+






 








&
'"








"




&
-















"




.

*/
'







"




+










(








"


+
"
&
'




"











+
"
&
'

"





*




 






.





#0%
#1%
#2%
#3%&


 





+



"


#4%&
E. Cant’u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 610−621, 2003.
 Springer-Verlag Berlin Heidelberg 2003

 
	

	







	

		


	


	


	








		

	
	







	
	

	
	


	

	
	
	



	

	 








	
!
"
#
 
	







	
	


	$




	



%
&

	


	


	

	
&



	




 



	
	

	


	







	


	
	


 
'
$(
)
	
*
+'($)*,
-


	








	
	
&


	


	


	

		
(	






	
		

	
&






	
-

	

	
#

	


	
	







	
	


.





	

	
&


	



	

)*
&

	

$


 
	





	


$

/
0


&



	

	 

1$2$3
		
+123,
	

4




	










	


	

		4
	
	
"
*	
#
	



	







	
		
&
)*
0




	
'

(
*$
*
)*
+'(**$)*,

	
	


	
	
-




	


	
	



		


		

	

	
	
5
 





&
		
	


	




 

	


6	

 $		



&

	

		

+ ,4




&

	
(

	

	

	4




	



	
7

+ ,

	
	





		
	

	


$
	



	

		

+ ,
0



	


1$2$3
		
+123,
	

4



	
	

&

	
	






		


(
$
	

	

123$	




	
	
8
 
123
	




&
9


&
 
	
	


$

		
	


7

+ ,
 
	


	

	
	




 	
+	




	
		,4

	
	





			

	
	
	
	





	




 +

8,
 

7



123
	




	


	

&		





&

		/
7

+,
:

 ;
  




+  

  ,
+8,
611
Model-Assisted Steady-State Evolution Strategies

 

 


	











 













−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
−5
0
5
10
15
20
25
data
model
target
 





	








	



 !
 

    

  
	
	

 

		





	

	








 





 


  

	

		





	

	

	


    

  


 
  

   
 

 

	

 





	

	
	


 


	
	





		


	


	








	




	




	




	
  
 
	


!
		
		
 


 


	
!
		


	


"

#
	




"
		

















	
	"	
$

	

%&





	

 
!


%&








	






	



#'



	





	

	


	
()
 
	
	




	
	
	
 
	


	

	
	


	



*+











 



	





(
*+

	








	
	
 
	






	

	





	

,	


	

612
H. Ulmer, F. Streichert, and A. Zell

 
	
 






 







	


	







 



 




 
 



















 

	
	








 


	



!
"





 




	







!






 
 





#!


$


#$





%


&'(





	




	


	


 	
	


		
	

	













	
 

	


  
 

 !
"##
  
"##
$
"##


 "##
	 
% &



 

 
 

	



 
 
 
$'(
	


  
 
	$ 

 !
   
 


  $
  )	$

"##
  	
"##


	$"##

 "##
	 
% &



 




	



 
 
 

		
	



	






 


	









	







		



&)(






 
)
#	




 

 





 


!


	

	
 




 





 


*

 


	

+



 



!
















 


,


 









	







 





















	

	 !



 













613
Model-Assisted Steady-State Evolution Strategies

 



	

		



	
	
 
	




	

	








	



	

		



	
	

	
 	

	









	
 
	


	

		

	


	



		

	

	
	





	

		

	



	







	


 





	
	

	


 


	
	

	

	



	

	

	

	



!


 

"

	

	



#


	
$ 
%
&'
(


 

	



	

	

	



	








	
	

	



 
	
	

	

	

	
	
	
(	





		
	




	







	


	

	









	

	
)
	


	





	
	





	

	


	
*	

	

		


	
)
	


	



+,&'





	
	
	






(	


	




	





	

	

*	

	







* 






	
	


	









	

	
 
	



	
	






	




	


	




-	
	

	


	






	

	






		







			
		
.
,
	
			
&'


)


	
	

$ /
%
&'
0
1








	


	
	


	

	


	


		


	
	

	

2
	
	



	
	

	




	









		

	


	
	
	

	



-	
	
			
		










	





	


	


	






	
	


	
	

	


&'
,

	

	

	

	


	
			
&'


	





3



	









	


	



	


 #
4





	

 
	
	
	






	
$ 
%
&'





*	


	


*	



	
	

 
	

5



	


	

	

  


,


	




	
		
	

	





	

	




	



		
*	
	
	

 

 
	






	






	
			
&'







*
6







	
	
			
&'


	





+,&'



	


	

+

,
	
'	'		
&'
$+,''&'%
$




*
7%



	















	





		
	







	
	


*	


	

614
H. Ulmer, F. Streichert, and A. Zell

 
	



  
 	

 !"
 #
$
	

 

 $
%#


	
 

  

	
	
 
   



	


 
$&	



  
 	

$ 

 !"
     


  $
  	'
$

  

#
	


$
  $

%#


	
 

	


  

	

	
 
   



	


 
	


	

 


	




	
























	


	


	






	

 
 
 !
 "
#$

"
% $
 &'







!
 "
#$

"
%$

 

"
( $
 
!



	

 "
$

 "
) $
 &'








!



	

 "
$

 "
)$

 

"
 
*



	

	



+,* 










	


	



-


$
-

.



	

$

	



/

,! 



-
	

.




$
.



	




.







-
	


	
"


 
  
 
 

0

 


( 
 
,!



/



&'












	
 


















1








.


 





.


	
.


&'






$
615
Model-Assisted Steady-State Evolution Strategies

 
 

	 


	  

	



	
 




	


		


 

	
 
	



	
 







    
 

	


 

!

" 
	
	  



 	




		

 





 



 	
 

	


	  
 

	



	
 
#$%&



 

	
 
#$&
	

 

'


('


 '
 

	
		 

	


 

 

)*
 

 

	








 









 

 

	











 









616
H. Ulmer, F. Streichert, and A. Zell

 

 

	

 









 









 


	

	

			

	
	

	


	


	


 

		
			

 

	

	






		



		
	

	

	

	


	
	


	
 


!

	

"	!



	






!


	




	
	




	
	

 




	


	

			

	

	

	
 


	


	



	



!

	


!


	

	


			

#	
	

	!



	

	
	







	

			
		






	


	

	


$%



	

		
	
	


	

		

&


'
 

(
	



)


	





$

	




	

			


	
	
	



&
*+
,+

	
	







	
-

	

	
		
		

 

$








+



	


	
	


	



	



 

.


/
	
%0

+

	


	
	
 	


!



		
	




!



	
&

	







	


	


	

$%
	
	

	

		
 	



	


	
&	
!





/





!



	
	




	

	(	


%
	

$

	

			
		


	

	

	







	


		



$%
 

	




$	

	
!
/




!








	
%

	(	
	

/






	
	
/









	

	


	
(

 
,

	
		
617
Model-Assisted Steady-State Evolution Strategies

 

 
	

	
	
	



	
	

 
	

	

	 



 	

	 
	


 
	

 


 
	
 
	
 	

 


	
 

	!
	 

!

 	


	

	!
"


 !
	
	
#

 
 

 

	








 









$


  	
	
	
 


 


 	
	

	




	
 



!
  
 

%
 

 	
	

	  & 	

	'

 

()
 

!
 
	

'

 
! 


*+
	


 

(
$	




!
	
	
 

	


 
	

	 



+

 


 
!

 



*'+
,
  -
%


.-
/





!


 

 	

.-
 	


	!
	 

 




	
	




 

	


&

 	
	
*+
 


	



 
	
*'
+
 

	
*'+
 

(0
 
	!

 	
	
!

1 
!





 
	1 

	

.-
 	
$	




*'+




!

 


 

(%

(2
	
 


	


	







!
  




+


	

.-
 	
 
	!'


 	
 
 
	


	


	
3
4)5
	
	
 

	


 
  	



	 



	
	



 
6

 
	
 
 
,78


	
.
	



 	
!
+


'
+
 
 
 	




 




 
1 

 
 	
	
/
 '  	

 	
*	
 
'




	

 
!
	


	

# 


 	
	
	




	  
   
!

 



  	
$	





 
	



	
'
 



!


	



618
H. Ulmer, F. Streichert, and A. Zell

 

 

	

 







 









 

 

	










 









 
	
























	















	







 




















 
 



 	























	
 



























!






	
 



"





















 	


#











!









	






$










	
619
Model-Assisted Steady-State Evolution Strategies

 
	
 

	

  

  


  



 
 










   



  

 




 




	

 
 

  


  





 
 










  
 



 
 

 




 






	

 	

  


  
 

 

 



	
 

 


 

 










  	



 	

 




 

	

  
  


  



 
 










   


  
 




 


	

 



  




	

  
 


 
 




 
 












  





 



 




 
 
 
	

 

  

	 

 
 










 





 





 

 



	
 
 












  



 

 




620
H. Ulmer, F. Streichert, and A. Zell

 
 

	
 

 	













 






 

!"
#

$
%&


&
	#'

$#	&
	


	(#)
	#

#
*
















	
 +,-.
 
.

!


!
/
"

&
	#'

$#	&
	


	(#
&
&
	
$
*
01!

2#
!(








 



!"#
	
3-4,3 5
-- 
5

##$

!
6
&&
7

'#
8
7
$
9

"
69
&
#


&

*




$
%


 

&
	
.+,.3-
--

 
2

!

#


$
		



&

:

#'
$

$
#
	)
8
;   
<$#
*
'(










)

(*

%

	
+-,+5
 3
+
=
/





0>



&
	#
:
		

#
?
&$
*
















	
34+,3.
---
3
=
/





0>
!
#:
9


&
	#'
:
		
#
?
&$

)





!

+
(

,



-
,%% .
#/0!11/2-
--
4
=
/


0>
@
		
#


&
$#	&


&


*
















	
  -,    
--

!
1
!$$

$

$


&
#
(

?
$	

		
#
*
!
(








$
%


 

&
	
43,+
 4
 -
*
1$

(






"03
##'(


0&
 5
  
2

0$

:

 
(


!+




!





97
&

 33
 
2

0$

:

 &

&

$$

	#
&#,9
*

!
0
:


%




.
)


)



	
, 5
 
 .
/
A
9&

!
B
!
:
$
$
#



&


*
C
A

C

6(

$
9


D
&
'

0	
$
*

#

26










(


!




4


	
35,4- 

E

 

/&
 - 
---

"&#
&(
0
@
$$

621
Model-Assisted Steady-State Evolution Strategies

On the Optimization of Monotone Polynomials
by the (1+1) EA and Randomized Local Search
Ingo Wegener⋆and Carsten Witt⋆
FB Informatik, LS 2
Univ. Dortmund
44221 Dortmund, Germany
{wegener, witt}@ls2.cs.uni-dortmund.de
Abstract. Randomized search heuristics like evolutionary algorithms
and simulated annealing ﬁnd many applications, especially in situations
where no full information on the problem instance is available. In order
to understand how these heuristics work, it is necessary to analyze their
behavior on classes of functions. Such an analysis is performed here for
the class of monotone pseudo-boolean polynomials. Results depending on
the degree and the number of terms of the polynomial are obtained. The
class of monotone polynomials is of special interest since simple functions
of this kind can have an image set of exponential size, improvements can
increase the Hamming distance to the optimum and, in order to ﬁnd a
better search point, it can be necessary to search within a large plateau
of search points with the same ﬁtness value.
1
Introduction
Randomized search heuristics like random local search, simulated annealing, and
all variants of evolutionary algorithms have many applications and practitioners
report surprisingly good results. However, there are few theoretical papers on
the design and analysis of randomized search heuristics.
In this paper, we investigate general randomized search heuristics, namely a
random local search algorithm and a mutation-based evolutionary algorithm. It
should be obvious that they do not improve heuristics with well-chosen problem-
speciﬁc modules. Our motivation to investigate these algorithms is that such
algorithms are used in many applications and that only an analysis will provide
us with some knowledge to understand these algorithms better. This will give us
the chance to improve these heuristics, to decide when to apply them, and also to
teach them. The idea is to analyze randomized search heuristics for complexity-
theoretical easy scenarios. One may hope that the heuristics behave similarly
also on those functions which are “close” to the considered ones.
Each pseudo-boolean function f : {0, 1}n →R can be written uniquely as a
polynomial
⋆Supported in part by the Deutsche Forschungsgemeinschaft as a part of the Colla-
borative Research Center “Computational Intelligence” (SFB 531).
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 622–633, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

On the Optimization of Monotone Polynomials by the (1+1) EA
623
f(x) =

A⊆{1,...,n}
wA ·

i∈A
xi .
The degree d := max{|A| | wA ̸= 0} and the number N of non-vanishing terms
wA ̸= 0 are parameters describing properties of f. Note that the value of N
can vary if we exchange the meanings of ones and zeros for some variables, i. e.,
replace some xi by their negations, 1−xi. For instance, the product of all (1−xi)
has the maximal number of 2n non-vanishing terms but only one non-vanishing
term if we replace xi by yi := 1 −xi. The parameter N will be relevant in some
upper bounds presented in this paper. However, all search heuristics that we will
consider treat zeros and ones in the same way. Therefore, we may silently assume
that in the polynomial representation of some monotone polynomial f, variables
xi have possibly been replaced by their negations 1−xi in such a way that N takes
its minimum value. Droste, Jansen and Wegener (2) have analyzed evolutionary
algorithms on polynomials of degree d = 1 and Wegener and Witt (14) have
investigated polynomials of degree d = 2. The last case is known to be NP-hard
in general. A simpler subcase is the case of monotone polynomials where f
can be written as a polynomial with non-negative weights on some variable set
z1, . . . , zn, where zi = xi or zi = 1−xi. In the ﬁrst case, the function is monotone
increasing with respect to xi and, in the second case, monotone decreasing. In
this paper, we investigate randomized search heuristics for the maximization
of monotone polynomials of degree bounded by some parameter d. Since all
considered heuristics treat zeros and ones in the same way, we can restrict our
analysis to monotone increasing polynomials where zi = xi for all i. The results
hold for all monotone polynomials.
The investigation of polynomials of small degree is well motivated since many
problems lead to polynomials of bounded degree. Monotonicity is a restric-
tion that simpliﬁes the problem. However, in the general setting it is unknown
whether the function is monotone increasing or decreasing with respect to xi.
Evolutionary algorithms are general problem solvers that eventually optimize
each f : {0, 1}n →R. We conjecture that our arguments would not lead to better
upper bounds when we allow large populations and/or crossover. Indeed, it seems
to be the case that they increase the optimization time moderately. Therefore, we
investigate a simple standard evolutionary algorithm (EA), which is mutation-
based and works with population size 1. This so-called (1+1) EA consists of an
initialization step and an inﬁnite loop.
(1+1) EA
Initialization: Choose a ∈{0, 1}n randomly.
Loop:
The loop consists of a mutation and a selection step.
Mutation:
For each position i, decide independently whether ai should be
ﬂipped (replaced by 1 −ai). The ﬂipping probability equals 1/n.
Selection:
Replace a by a′ iﬀf(a′) ≥f(a).
The advantage of the (1+1) EA is that each point can be created from each
point with positive probability but steps ﬂipping only a few bits are preferred.
Therefore, it is not necessary (as in simulated annealing) to accept worsenings.

624
I. Wegener and C. Witt
Random local search (RLS) ﬂips only one bit per step. The algorithm is also
called random mutation hill-climbing (Mitchell, Holland and Forrest 10).
RLS works like the (1+1) EA with a diﬀerent mutation operator.
Mutation: Choose i ∈{1, . . . , n} randomly and ﬂip ai.
RLS cannot escape from local optima, where the local neighborhood is the
Hamming ball with distance 1. However, it can optimize monotone polynomials
(by our assumption the optimum is 1n) since, for each a, there exists a sequence
a0 = a, a1, . . . , am = 1n such that m ≤n, the Hamming distance of ai and
ai+1 equals 1, and f(a0) ≤f(a1) ≤· · · ≤f(am). The analysis of RLS will be
much easier than the analysis of the (1+1) EA; however, only the (1+1) EA is
a general problem solver. The diﬃculty is that accepted steps can increase the
number of zeros and the Hamming distance to the optimum. The problem for
all heuristics is that it can be necessary to change many bits (up to d) until one
ﬁnds a search point with larger ﬁtness.
We have to discuss how we analyze RLS and the (1+1) EA, which are deﬁned
as inﬁnite loops. In applications, we need a stopping criterion; however, this is
not the essential problem. Hence, we are interested in the random optimization
time Xf, deﬁned as the minimum time step t where an optimal search point
is created. Its mean value E(Xf) is called the expected optimization time and
Prob(Xf ≤t) describes the success probability within t steps.
We present monotone polynomials of degree d where the expected optimiza-
tion time equals Θ((n/d) · log(n/d + 1) · 2d) for RLS and the (1+1) EA, and we
believe that the upper bound holds for all monotone polynomials. This can be
proved for RLS, but our best bound for the (1+1) EA is worse and depends on
N. For this reason, we also investigate a class of algorithms that bridge the dif-
ference between RLS and the (1+1) EA. The ﬁrst idea is to reduce the mutation
probability 1/n of the (1+1) EA. However, then we increase the probability of
useless steps ﬂipping no bit. Hence, we guarantee that at least one bit is ﬂipped.
We call the new algorithm RLSp since it is a modiﬁcation of RLS.
RLSp works like the (1+1) EA and RLS with a diﬀerent mutation operator.
Mutation: Choose i ∈{1, . . . , n} randomly and ﬂip ai. For each j ̸= i, ﬂip aj
independently of the other positions with probability p.
Obviously, RLSp equals RLS for p = 0. For p = 1/n, RLSp is close to the
(1+1) EA, but omits steps without ﬂipped bit. Hence, we investigate RLSp only
for 0 ≤p ≤1/n and try to maximize p such that we can prove the upper
bound O((n/d) · log(n/d + 1) · 2d) on the expected optimization time of RLSp
on monotone polynomials.
The paper is structured as follows. Search heuristics with population size 1
lead to a Markov chain on {0, 1}n. Therefore, we have developed some results on
such Markov chains. The results are presented without proof in an appendix. In
Sect. 2, we investigate the very special case of monomials, i. e., monotone polyno-
mials where N = 1. These results are crucial since we later consider how long it
takes to maximize a special monomial in the presence of many other monomials
in the polynomial. In the Sections 3, 5, and 6, we prove upper bounds on the

On the Optimization of Monotone Polynomials by the (1+1) EA
625
expected optimization time of the algorithms RLS, RLSp and the (1+1) EA on
monotone polynomials. In Sect. 4, we present a worst-case monotone polynomial
for RLS, which is conjectured to also be a worst-case monotone polynomial for
RLSp and the (1+1) EA. We ﬁnish with some conclusions. Some preliminary
ideas of this paper have been given in a survey (Wegener 13).
2
The Optimization of Monomials
Because of the symmetry of all considered algorithms with respect to the bit
positions and the role of zeros and ones, we can investigate w.l.o.g. the monomial
m(x) = x1 · · · xd. The following result has been proved by Garnier, Kallel and
Schoenauer (4) for the special case d = n and for the algorithms RLS0 and
(1+1) EA. We omit the proof of our generalizations.
Theorem 1. The algorithms RLSp, p ≤1/n, and (1+1) EA in their pure form
and under the condition of omitting all steps ﬂipping more than two bits optimize
monomials of degree d in an expected time of Θ((n/d)·2d). The upper bounds also
hold if the initialization is replaced by the deterministic choice of any a ∈{0, 1}n.
3
On the Analysis of Random Local Search
The random local search algorithm, RLS, is easy to analyze since it ﬂips one bit
per step. This implies that activated monomials, i. e., monomials where all bits
are 1, never get passive again.
Theorem 2. The expected optimization time of RLS on a monotone polynomial
of degree d is bounded by O((n/d) · log(n/d + 1) · 2d).
Sketch of proof. First, we investigate the case of polynomials with pairwise non-
overlapping monomials, i. e., monomials that do not share variables. For each
monomial of degree i, the probability of activating it in O(2i) steps is at least
1/2 (see Theorem 1) if we count only steps ﬂipping bits of the monomial. Now
the arguments for proving the famous Coupon Collector’s Theorem (see Motwani
and Raghavan 11) can be applied to obtain the result.
In the general case, we choose a maximal set M1 of pairwise non-overlapping
monomials and consider the time T1 until all monomials of M1 are activated. The
existence of further monotone monomials can only decrease the time for activat-
ing the monomials of M1. Here the property of monotonicity is essential. Hence,
by the considerations above, E(T1) is bounded by O((n/d) · log(n/d + 1) · 2d).
The key observation is that afterwards each passive monomial contains at least
one variable that is shared by an active monomial and, therefore, is ﬁxed to 1.
Hence, we are essentially in the situation of monomials whose degree is bounded
by d −1. This argument can be iterated and we obtain an upper bound on the
expected optimization time, which is the sum of all O((n/i) · log(n/i + 1) · 2i),
1 ≤i ≤d. Simple calculations show that this sum is only by a constant factor
larger than the term for i = d. This proves the theorem.
⊓⊔

626
I. Wegener and C. Witt
4
Royal Roads as a Worst-Case Example
It is interesting that the royal road functions RRd introduced by Mitchell, For-
rest and Holland (9) are the most diﬃcult monotone polynomials for RLS and
presumably also for RLSp and the (1+1) EA. The function RRd is deﬁned for
n = kd by
RRd(x) =
k−1

i=0
xid+1 · · · xid+d ,
or the number of blocks of length d containing ones only. Theorem 2 contains an
upper bound of O((n/d) · log(n/d + 1) · 2d) for the expected optimization time
of RLS on RRd, and this can be easily generalized to RLSp, where p ≤1/n,
and the (1+1) EA. The result for RLS was also shown by Mitchell, Holland
and Forrest (10). The mentioned upper bound disproved the conjecture that
RRd are royal roads for the crossover operator. Real royal roads have been pre-
sented only recently by Jansen and Wegener (7,8). Here we prove matching lower
bounds on the expected optimization time for RRd. First, we investigate RLS
and, afterwards, we transfer the results to RLSp and the (1+1) EA.
Theorem 3. The probability that RLS has optimized the function RRd within
(n/d) · log(n/d) · 2d−5 steps is o(1) (convergent to 0) if d = o(n). The expected
optimization time of RLS on RRd equals Θ((n/d) · log(n/d + 1) · 2d).
Sketch of proof. We only have to prove the lower bounds. For d = Θ(n), the
result has been proved by Droste, Jansen, Tinnefeld and Wegener (1) for all
considered algorithms. For d = O(1), the bounds follow easily by considering the
time until each bit initialized as 0 has ﬂipped once (again the Coupon Collector’s
Theorem).
In the following, we assume that d = ω(1) and d = o(n). First, we inves-
tigate the essential steps for a single monomial m, i. e., those steps ﬂipping a
bit of m. Let τ be the random number of essential steps until m is activated.
Garnier, Kallel and Schoenauer (4) have proved that this process is essentially
memoryless. More precisely
|Prob(τ ≥t) −Prob(τ ≥t + t′ | τ ≥t′)| = O(1/d) ,
and Prob(τ ≥t) is approximately 1 −e−t. Hence, since d = ω(1), we have
Prob(τ ≤2d−1 + t′ | τ ≥t′) ≤1/2 for all t′.
The next idea is that all monomials are aﬀected by essentially the same
number of steps. However, many steps for one monomial imply less steps for the
other monomials. We partition the k · (log k) · 2d−5 steps into (log k)/4 phases
of length k · 2d−3 each. Let pi be the random number of passive monomials
after phase i. We claim that the following events all have an exponentially small
probability with respect to k1/4: the event p0 < k/2 and the events pi < pi−1/8.
Hence, the probability that none of these events happens is still 1 −o(1). This
implies the existence of at least p0 · (1/8)(log k)/4 ≥k1/4/2 passive monomials at
the end of the last phase implying that RRd is not optimized.

On the Optimization of Monotone Polynomials by the (1+1) EA
627
The expected value of p0 equals k·(1−2−d), and, therefore, the probability of
the event p0 < k/2 can be estimated by Chernoﬀbounds. If pj ≥pj−1/8 for all
j < i, there are at least k1/4/2 passive monomials at the end of phase i −1. The
expected number of steps essential for one of the passive monomials in phase i
equals pi−1 ·2d−3, and the probability that this number is less than pi−1 ·2d−2 is
exponentially close to 1. By the pigeon-hole principle, there are at most pi−1/2
monomials with at least 2d−1 essential steps each. Pessimistically, we assume all
these monomials to become active in phase i. We have proved before that each
other monomial activates with probability at most 1/2. By Chernoﬀbounds, the
probability of activating at least 3/4 of these and altogether at least 7/8 of the
passive monomials is exponentially small. This proves the theorem.
⊓⊔
Theorem 4. For each ε > 0, the probability that the (1+1) EA is on RRd by a
factor of 1 + ε faster than RLS is O(1/n). The same holds for RLSp, p ≤1/n,
and the factor 2 + ε.
Sketch of proof. We prove the result on the (1+1) EA by replacing the (1+1) EA
by a faster algorithm (1+1)* EA and comparing the faster algorithm with RLS.
A step of the (1+1)* EA works as follows. First, the number k of ﬂipped bits
is chosen according to the same distribution as for the (1+1) EA. Then the
(1+1)* EA ﬂips a random subset of k bits. This can be realized as follows. In each
step, one random bit is ﬂipped until one obtains a point of Hamming distance
k to the given one. Now the new search point of the (1+1)* EA is obtained as
follows. The selection procedure of RLS is applied after each step. This implies
by the properties of the royal road functions that we obtain a search point a∗
compared to the search point a of the (1+1) EA such that a ≤a∗according to
the componentwise partial order. This implies that the (1+1)* EA reaches the
optimal string 1n no later than the (1+1) EA.
However, the (1+1)* EA chooses ﬂipped bits as RLS, and it uses the same
selection procedure. The diﬀerence is that the (1+1)* EA sometimes simulates
many steps of RLS in one step, while the (1+1)* EA ﬂips on average one bit per
step. It is easy to see that we have to consider t = Ω(n) steps. Then it is for each
γ > 0 very likely that the (1+1)* EA ﬂips not more than (1 + γ)t bits within t
steps. Moreover, with high probability the number of ﬂipped bits is bounded by
δn in each step, δ > 0 a constant. Let a be the starting point of the simulation of
one step. The probability of increasing the Hamming distance to a with the next
ﬂipped bit is at least 1 −δ. Hence, with large probability we have among t steps
an overhead of (1 −3δ)t distance-increasing steps. Hence, the probability that
(1+γ)·t/(1−3δ) steps of RLS do not suﬃce to simulate t steps of the (1+1)* EA
is exponentially small. Choosing γ and δ such that (1 + γ)/(1 −3δ) = 1 + ε, we
are done. The statement on RLSp follows in the same way taking into account
that RLSp, p ≤1/n, ﬂips on average not more than two bits per step.
⊓⊔
5
On the Analysis of RLSp
In contrast to RLS, RLSp with p > 0 can deactivate monomials by simultane-
ously activating other monomials. Even the analysis of the time until a single

628
I. Wegener and C. Witt
monomial is activated becomes much more diﬃcult. Steps where two bits of the
monomial ﬂip from 0 to 1 and only one bit ﬂips from 1 to 0 may decrease the
ﬁtness and be rejected. Hence, we do not obtain simple Markov chains as in the
case of RLS or in the case of single monomials.
We can rule out the event of three or more ﬂipped bits contained in the
same monomial if its degree is not too large, more precisely d = O(log n). This
make sense since Theorems 3 and 4 have shown that we cannot obtain polyno-
mial upper bounds otherwise. To analyze the optimization process of RLSp on a
monotone polynomial, we ﬁrst consider some ﬁxed, passive monomial and esti-
mate the time until it becomes active for the ﬁrst time. The best possible bound
O((n/d)·2d) can be proved if p is small enough. Afterwards, we apply this result
in order to bound the expected optimization time on the monotone polynomial.
The bound we obtain here is close to the lower bound from Theorem 4.
Lemma 1. Let f be a monotone polynomial of degree d ≤c log n and let m
be one if its monomials. There is a constant α > 0 such that RLSp with p =
min{1/n, α/(nc/2 log n)} activates m in an expected time of O((n/d) · 2d) steps.
Sketch of proof. The idea is to prove that RLSp activates m with a constant
probability ε > 0 within a phase of c′ · (n/d) · 2d steps, for some constant c′.
Since our analysis does not depend on the starting point, this implies an upper
bound c′ · (n/d) · 2d/ε on the expected time to activate m. We assume w.l.o.g.
that m = x1 · · · xd and call it the preﬁx of the search point.
We bound the probability of three events we consider as a failure. The ﬁrst
one is that we have a step ﬂipping at least three preﬁx bits in the phase. The
second one is that, under the condition that the ﬁrst type of failure does not
happen, we do not create a search point where m is active in the phase. The
third one occurs if the ﬁrst search point in which m is active is not accepted. If
none of the failures occurs, m is obviously activated.
The ﬁrst and third type of failure can be handled by standard techniques.
A simple calculation shows that the ﬁrst type of failure occurs with probability
at most d3p2/n in one step. Multiplying by the length of the phase, we obtain
a failure probability bounded by a constant if α is small enough. For the third
failure type it is necessary that at least one of the suﬃx bits xd+1, . . . , xn ﬂips.
Since we assume m to be activated in the considered step, the related conditional
probability of not ﬂipping a suﬃx bit can be bounded below by the constant
1/(2e). All this holds also under the condition that the ﬁrst two types of failure
do not happen.
For the second type of failure, we apply the techniques developed in the
appendix by comparing the Markov chains Y0 and Y1. Y0 equals RLS∗
p, namely
RLSp on the monomial m, where the condition holds that no step ﬂips more
than two bits of the preﬁx. Y1 equals RLS∗
p on the monotone polynomial f,
which again equals RLSp under the condition that no step ﬂips more than two
preﬁx bits. Both Markov chains are investigated on the compressed state space
D = {0, . . . , d} representing the number of 1-bits in the preﬁx. We can ignore the
fact that the Markov chain Y1 is not time-homogeneous by deriving bounds on its
transition probabilities that hold for all search points. We denote these bounds

On the Optimization of Monotone Polynomials by the (1+1) EA
629
still by P1(i, j). Then the following conditions for Lemma 7 imply the bound
O((n/d) · 2d) of the lemma. (See also Deﬁnitions 1, 2 and 3 in the appendix.)
1. Y1 has a relative advantage to Y0 for c-values such that cmin ≥1/(2e),
2. Y0 has a (2e −1)-advantage, and
3. E(τ i
0) = O((n/d) · 2d) for all i.
The third claim is shown in Theorem 1. The second one follows from Lemma 4
since d ≤(n −1)/(4e + 1) if n is large enough. For the ﬁrst claim, recall that
at most two preﬁx bits ﬂip. Now Deﬁnition 3 implies that we have to consider
c(i, j) = P1(i, j)/P0(i, j) for j ∈{i −2, i −1, i + 1, i + 2} and to prove that
1. 1/(2e) ≤c(i, i + 1) ≤1
2. c(i, i + 2) ≥c(i, i + 1),
3. c(i, i −1) ≤c(i, i + 1), and
4. c(i, i −2) ≤c(i, i + 1) (or even c(i, i −2) ≤c(i, i −1)).
The inequality c(i, i + 1) ≤1 holds since RLS∗
p on m accepts each new string
as long as the optimum is not found. The bound c(i, i + 1) ≥1/(2e) follows
from the fact that RLS∗
p on the monotone polynomial f accepts a step where
one preﬁx bit ﬂips from 0 to 1 and no suﬃx bit ﬂips.
For the remaining inequalities, observe that c(i, j) is the conditional proba-
bility of RLS∗
p accepting (for f) a search point x given that x contains j preﬁx
ones and has been created from a string with i preﬁx ones. The idea is to condi-
tion these probabilities even further by considering a ﬁxed change of the suﬃx
bits. Let the suﬃx change from c to c′, and let b be a preﬁx containing i ones.
If RLS∗
p accepts the string (b′, c′), where b′ is obtained from b by ﬂipping a
zero to one, then RLS∗
p also accepts (b′′, c′), where b′′ is obtained from b′ by
ﬂipping another zero. Estimating the number of such strings (b′′, c′) leads to
c(i, i + 2) ≥c(i, i + 1). By a dual argument, we prove c(i, i −2) ≤c(i, i −1).
Finally, the inequality c(i, i + 1) ≥c(i, i −1) follows from the following observa-
tion. If there is at least one string (b′, c′) that is not accepted and where b′ has
been obtained by ﬂipping a zero of b, then all strings (b′′, c′), where b′′ has been
obtained by ﬂipping a one of b, are also rejected. This completes the proof.
⊓⊔
Theorem 5. The expected optimization time of RLSp on a monotone polyno-
mial f of degree d ≤c log n is bounded above by O((n2/d) · 2d) if 0 < p ≤
min{(1 −γ)/(2dn), α/(nc/2 · log n)} for the constant α from Lemma 1 and each
constant γ > 0.
Sketch of proof. The optimization process is not reﬂected by the f-value of the
current search point. An f-value of v can be due to a single monomial of degree 1
or to many monomials of large degree. Instead, we count the number of essential
ones (with respect to f). A 1-entry of a search point is called essential if it is
contained in an activated monomial of f. All other 1-entries may ﬂip to 0 without
decreasing the f-value and are therefore called inessential. 0-entries are always
called inessential. An essential one can only become inessential if simultaneously
some monomial is activated. A step where a monomial is activated is called

630
I. Wegener and C. Witt
essential. By Lemma 1, it suﬃces to prove an O(n) bound on the expected
number of essential steps.
To prove this bound, we apply Lemma 8, an approach sometimes called drift
analysis (see Hajek 5; Sasaki and Hajek 12; He and Yao 6). Let Xi be the number
of essential ones after the i-th essential step, i. e., X0 is the number of essential
ones after initialization. Let D0 = X0 and Di = Xi −Xi−1 for i ≥1. Then we
are interested in τ, the minimal i where D0 +D1 +· · ·+Di = n. Some conditions
of Lemma 8 are veriﬁed easily. We have |Di| ≤n and E(τ) < ∞since there is
always a probability of at least pn to create the optimal string. If we can prove
that E(Di | τ ≥i) ≥ε for some ε > 0, Lemma 8 implies E(τ) = O(n).
At least one monomial is activated in an essential step, i. e., at least one bit
turns from inessential into essential. We have to bound the expected number of
bits turning from essential into inessential. Since the assumption that the new
search point is accepted only decreases this number, we consider the number of
ﬂipped ones under the condition that a 0-bit is ﬂipped.
Let Y be the random number of additional bits ﬂipped by RLSp under the
assumption that a speciﬁed bit (activating a monomial) ﬂips. A lengthy calcu-
lation shows that E(Y ) ≤(1 −ε)/d for some ε > 0 since p ≤(1 −γ)/(2dn). The
problem is that given Y = i, more than i bits may become inessential. There-
fore, we upper bound the expected number of bits turning from essential into
inessential if Y = i. In the worst case, these i ﬂipped bits contain essential ones.
Since we do not take into account whether the new search point is accepted,
each subset of size i of the essential ones has the same probability of being the
ﬂipped ones. We apply the accounting method on the random number L of es-
sential ones becoming inessential if a random essential one ﬂips. The idea is as
follows. In order to make the essential one in bit j inessential, some essential
one contained in all monomials that contain xj ﬂips. This leads to E(L) ≤d.
Then we can show that by ﬂipping i essential ones, we lose on average at most
id essential ones. Since E(Y ) ≤(1 −ε)/d, the expected number of essential ones
becoming inessential is at most 1 −ε. Since at least one bit gets essential, this
implies E(Di | τ ≥i) ≥ε and the theorem.
⊓⊔
6
On the Analysis of the (1+1) EA
Since the (1+1) EA ﬂips too many bits in a step, the bound of Theorem 5 cannot
be transferred to the (1+1) EA, and we only obtain a bound depending on the
parameter N here. However, a result corresponding to Lemma 1 can be proved.
Lemma 2. Let f be a monotone polynomial of degree d, and let m be one of
its monomials. There is a constant α such that the (1+1) EA activates m in an
expected number of O((n/d) · 2d) steps if d ≤2 log n −2 log log n −α.
Sketch of proof. We follow the same structure as in the proof of Lemma 1 and
need only few diﬀerent arguments.
First, the probability of at least three ﬂipped preﬁx bits in one step is bounded
above by d3n−3/6 for the (1+1) EA. Therefore, the probability that such a step

On the Optimization of Monotone Polynomials by the (1+1) EA
631
happens in a phase of length c·(n/d)·2d for some constant c is still smaller than
1 by choosing α large enough. Second, the probability that no suﬃx bit ﬂips is at
least (1 −1/n)n−1 ≥1/e. Also Lemma 7 can be applied with a value cmin ≥1/e
and an (e−1)-advantage of Y0. It is again possible to apply Theorem 1. Instead of
Lemma 4, Lemma 3 is applied. Here it is suﬃcient that d ≤(n−1)/e. Finally, the
argument that Y1 has a relative advantage to Y0 for c-values such that cmin ≥1/e
can be used in the same way here.
⊓⊔
Theorem 6. The expected optimization time of the (1+1) EA on a monotone
polynomial with N monomials and degree d ≤2 log n −2 log log n −α for the
constant α from Lemma 2 is bounded above by O(N · (n/d) · 2d).
Sketch of proof. Here we use the method of measuring the progress by ﬁtness
layers. Let the positive weights of the N monomials be sorted, i. e., w1 ≥· · · ≥
wN > 0. We partition the search space {0, 1}n into N + 1 layers L0, . . . , LN,
where
Li = {a | w1 + · · · + wi ≤f(a) < w1 + · · · + wi+1}
for i < N, and LN contains all optimal search points. Each layer Li, i < N, is
left at most once. Hence, it is suﬃcient to prove a bound of O((n/d) · 2d) on the
expected time to leave Li. Let a ∈Li. Then there exists some j ≤i+1 such that
the monomial mj corresponding to wj is passive. By Lemma 2, the expected time
until mj is activated is bounded by O((n/d) · 2d). We can bound the probability
of not leaving Li in the step activating mj by 1 −e−1. The expected number of
such phases is therefore bounded by e.
⊓⊔
Conclusions
We have analyzed randomized search heuristics like random local search and a
simple evolutionary algorithm on monotone polynomials. The conjecture is that
all these algorithms optimize monotone polynomials of degree d in an expected
number of O((n/d)·log(n/d+1)·2d) steps. It has been shown that some functions
need that amount of time. Moreover, for random local search the bound has
been veriﬁed. If the expected number of ﬂipped bits per step is limited, a little
weaker bound is proved. However, for the evolutionary algorithm only a bound
depending on the number of monomials with non-zero weights has been obtained.
Although there is room for improvement, the bounds and methods are a step to
understand how randomized search heuristics work on simple problems.
References
Droste, S., Jansen, T., Tinnefeld, K., Wegener, I.: A new framework for the valuation
of algorithms for black-box optimization. In: Proc. of FOGA 7. (2002) 197–214.
Final version of the proceedings to appear in 2003.
Droste, S., Jansen, T., Wegener, I.: On the analysis of the (1+1) evolutionary algorithm.
Theoretical Computer Science 276 (2002) 51–81

632
I. Wegener and C. Witt
Feller, W.: An Introduction to Probability Theory and its Applications. Wiley, New
York (1971)
Garnier, J., Kallel, L., Schoenauer, M.: Rigorous hitting times for binary mutations.
Evolutionary Computation 7 (1999) 173–203
Hajek, B.: Hitting-time and occupation-time bounds implied by drift analysis with
applications. Advances in Applied Probability 14 (1982) 502–525
He, J., Yao, X.: Drift analysis and average time complexity of evolutionary algorithms.
Artiﬁcial Intelligence 127 (2001) 57–85
Jansen, T., Wegener, I.: Real royal road functions – where crossover provably is essen-
tial. In: Proc. of GECCO 2001. (2001) 375–382
Jansen, T., Wegener, I.: The analysis of evolutionary algorithms – a proof that crossover
really can help. Algorithmica 34 (2002) 47–66
Mitchell, M., Forrest, S., Holland, J.H.: The royal road for genetic algorithms: Fitness
landscapes and GA performance. In Varela, F.J., Bourgine, P., eds.: Proc. of the
First European Conference on Artiﬁcial Life, Paris, MIT Press (1992) 245–254
Mitchell, M., Holland, J.H., Forrest, S.: When will a genetic algorithm outperform
hill climbing. In Cowan, J.D., Tesauro, G., Alspector, J., eds.: Advances in Neural
Information Processing Systems. Volume 6., Morgan Kaufmann (1994) 51–58
Motwani, R., Raghavan, P.:
Randomized Algorithms.
Cambridge University Press
(1995)
Sasaki, G.H., Hajek, B.: The time complexity of maximum matching by simulated
annealing. Journal of the ACM 35 (1988) 387–403
Wegener, I.: Theoretical aspects of evolutionary algorithms (invited paper). In: Proc.
of ICALP 2001. Number 2076 in LNCS (2001) 64–78
Wegener, I., Witt, C.: On the analysis of a simple evolutionary algorithm on quadratic
pseudo-boolean functions. To appear in Journal of Discrete Algorithms (2003).
A
Some Results on Markov Chains
The behavior of randomized search heuristics on single monomials is of special
interest. For a monomial of degree d, the current state can be identiﬁed with the
number of ones among the variables of the monomial. This leads to the state
space D = {0, . . . , d}. In order to obtain an ergodic Markov chain, we replace the
selection operator by the selection operator that accepts each a′, i. e., a′ always
replaces a. Then we are interested in the minimal t such that in time step t
the state d is reached. The transition probabilities for the (1+1) EA under the
condition that each step changes the state number by at most 2 are denoted by
Q(i, j) and the corresponding transition probabilities for RLSp by R(i, j).
We prove that these Markov chains have the property that it is more likely to
reach from i “higher” states than from i −1. This intuitive notion is formalized
as follows.
Deﬁnition 1. Let P(i, j) be the transition probabilities of a time-homogeneous
Markov chain on D = {0, . . . , d}. The Markov chain has an ε-advantage, ε ≥0,
if for all i ∈{0, . . . , d −2} the following properties hold.
1. P(i, j) ≥(1 + ε) · P(i + 1, j) for j ≤i,
2. P(i + 1, j) ≥(1 + ε) · P(i, j) for j > i.

On the Optimization of Monotone Polynomials by the (1+1) EA
633
Lemma 3. Let ε ≥0 and d ≤(n −1)/(1 + ε). Then the Markov chain with
transition probabilities Q(i, j) has an ε-advantage.
Lemma 4. Let ε ≥0 and d ≤(n −1)/(3 + 2ε). Then the Markov chain with
transition probabilities R(i, j) has an ε-advantage.
We are interested in the random variable τ k describing for a time-homoge-
neous Markov chain Y on D with transition probabilities P(i, j) the ﬁrst point
of time when it reaches state d if it starts in state k. If Y has a 0-advantage, it
should be advantageous to start in a “higher state.” This is made precise in the
following lemma.
Lemma 5. Let P(i, j) be the transition probabilities of a time-homogeneous
Markov chain with 0-advantage on D = {0, . . . , d}. Then Prob(τ i ≥t) ≥
Prob(τ i+1 ≥t) for 0 ≤i ≤d −1 and each t. Moreover, E(τ i) ≥E(τ i+1).
We compare diﬀerent Markov chains. The complicated Markov chain Y1,
describing a randomized search heuristic on a monotone polynomial with many
terms, is compared with the simple Markov chain Y0, describing a randomized
search heuristic on a single monomial. The idea is to use results for Y0 to obtain
results for Y1. We denote by τ i
0 and τ i
1 the random time to reach state d from
state i with respect to Y0 and Y1, respectively.
Deﬁnition 2. Let P0(i, j) and P1(i, j) be the transition probabilities of the time-
homogeneous Markov chains Y0 and Y1 on D = {0, . . . , d}. The Markov chain
Y1 has an advantage compared to Y0 if P1(i, j) ≥P0(i, j) for j ≥i + 1 and
P1(i, j) ≤P0(i, j) for j ≤i −1.
Lemma 6. If Y1 has an advantage compared to Y0 and Y0 has a 0-advantage,
then Prob(τ i
1 ≥t) ≤Prob(τ i
0 ≥t) and E(τ i
1) ≤E(τ i
0).
Finally, we apply Lemma 6 to compare two Markov chains Y0 and Y1 where
weaker conditions hold than in Lemma 6. We compare Y0 and Y1 by parameters
c(i, j) such that P1(i, j) = c(i, j) · P0(i, j). This includes an arbitrary choice of
c(i, j) if P0(i, j) = P1(i, j) = 0.
Deﬁnition 3. Let P0(i, j) and P1(i, j) be the transition probabilities of Y0 and
Y1 such that P1(i, j) = c(i, j) · P0(i, j) for some c(i, j). Then Y1 has a relative
advantage compared to Y0 if c(i, j) ≥c(i, i + 1) for j ≥i + 1, c(i, j) ≤c(i, i + 1)
for j ≤i −1, and 0 < c(i, i + 1) ≤1 for all i ≤d −1.
Lemma 7. If Y1 has a relative advantage compared to Y0 and Y0 has a (c−1
min−1)-
advantage, then E(τ i
1) ≤c−1
min · E(τ i
0) for cmin := min{c(i, i + 1) | 0 ≤i ≤d −1}.
The last result in this technical section is a generalization of Wald’s identity
(see Feller 3). We do not claim to be the ﬁrst to prove this result, but we have
not found it in the literature.
Lemma 8. Let Di, i ∈N, be a sequence of random variables such that |Di| ≤c
for a constant c. For s > 0, let τs be the minimal i where D1 + · · · + Di = s. If
E(τs) < ∞and E(Di | τs ≥i) is bounded below by a positive constant ℓfor all
i where Prob(τs ≥i) > 0, then E(τs) ≤s/ℓ.

A Forest Representation for Evolutionary
Algorithms Applied to Network Design
A.C.B. Delbem1 and Andre de Carvalho1
University of Sao Paulo – ICMC – USP, Sao Carlos – SP, Brazil,
{acbd,andre}@icmc.usp.br
Abstract. Network design involves several areas of engineering and sci-
ence. Computer networks, electrical circuits, transportation problems,
and phylogenetic trees are some examples. In general, these problems are
NP-Hard. In order to deal with the complexity of these problems, several
strategies have been proposed. Among them, approaches using evolution-
ary algorithms have achieved relevant results. However, the graph encod-
ing is critical for the performance of such approaches in network design
problems. Aiming to overcome this drawback, alternative representations
of spanning trees have been developed. This article proposes an encoding
for generation of spanning forests by evolutionary algorithms.
1
The Proposed Representation
The proposed forest representation basically consists of linear lists (which may
be an array T) containing the tree nodes and their depths. The order the pairs
(node,depth) are disposed in the list is important and it must follow a preorder
traversal. The forest representation is composed by the union of the encodings
of all trees of a forest.
Two operators are proposed (named operator 1 and operator 2) to gener-
ate new spanning forests using the node-depth encoding. Both operators generate
a spanning forest F ′ of a graph G when they are applied to another spanning
forest F of G. The results produced by the application of the operators are sim-
ilar. The application of the operator 1 (or 2) to a forest is equivalent to transfer
a subtree from a tree Tfrom to another tree Tto of the same forest. Applying
operator 1, the root of the pruned subtree will be also the root of this subtree
in its new tree (Tto). On the other hand, the transferred subtree will have a new
root when applying operator 2.
In the description of the operator 1, we consider that two nodes were previ-
ously chosen: the prune node p, which indicates the root of the subtree of Tfrom
to be transferred; and the adjacent node a, which is a node of a tree diﬀerent
from Tfrom. This node is also adjacent to p in G. An eﬃcient procedure to de-
termine such nodes are proposed in [1]. Besides, we assume that the node-depth
representation was implemented using arrays and that the indices of p (ip) and
a (ia), respectively, in the arrays Tfrom and Tto are also known. The operator 1
can be described by the following steps:
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 634–635, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

A Forest Representation for Evolutionary Algorithms
635
1. Determine the range (ip-il) of indices in Tfrom corresponding to the subtree
rooted at node p. Since we know ip, we only need to ﬁnd il;
2. Copy the data in the range ip-il from Tfrom into a temporary array Ttmp
(corresponding to the subtree being transferred) and update the node depths
using the depth of a.
3. Create an array T ′
to (new tree) copying Tto and inserting Ttmp (pruned sub-
tree) after the node a in Tto.
4. Construct an array T ′
from copying Tfrom without the nodes of Ttmp.
5. Copy the forest F to F ′ exchanging the pointers to Tfrom and Tto for pointers
to T ′
from and T ′
to, respectively.
The operator 2 requires a new root node r, besides the nodes p and a. The
copy of the pruned subtree for the operator 2 can be divided in two steps: The
ﬁrst step corresponds to the step 2 for the operator 1 exchanging the range ip-
il by ir-il. The array returned by this procedure is named Ttmp1. The second
step considers the nodes in the path from r to p (i.e. r0, r1, r2, . . ., rn, where
r0 = r and rn = p) as roots of subtrees. The subtree rooted at r1 contains
the subtree rooted at r0. The subtree rooted at r2 contains the subtree rooted
at r1, and so on. The algorithm for the second step copies the subtrees rooted
at rj (j = 1, . . . , n) without the subtree rooted at rj−1, updates the depths 1,
and store the resultant subtrees in a temporary array Ttmp2. The step 3 of the
operator 2 is equivalent to the same step of the operator 1, exchanging Ttmp for
the concatenation of Ttmp1 and Ttmp2 (Ttmp = [Ttmp1|Ttmp2]). The steps 4 and
5 are equal in both operators.
2
Final Considerations
This proposal focuses on the production of spanning forests instead of trees (usu-
ally found in the literature). As consequence, the operator complexity depends
on, for example, the size of the modiﬁed trees from F to F ′, while the complex-
ity of the operators found in the literature are usually functions of the number
of nodes and/or edges in the underlying graph. The proposed operators do not
require a graph G to be complete in order to produce only feasible spanning
forests of G. Many practical problems do not involve complete graphs (in fact,
several networks correspond to sparse graphs).
References
[1] A. C. B. Delbem and Andre de Carvalho. New data structure for spanning forest
operators for evolutionary algorithms.
Centro LatinoAmericano de Estudios en
Informatica – CLEI 2002, CD-ROM, 2002.
1 The updated depth of node x is given by Tfrom[ix].depth −Tfrom[iri].depth +
Tfrom[ir].depth −Tfrom[irj].depth + depth of a + 1 .

Solving Three-Objective Optimization Problems
Using Evolutionary Dynamic Weighted
Aggregation: Results and Analysis
Yaochu Jin, Tatsuya Okabe, and Bernhard Sendhoﬀ
Honda Research Institute Europe
Carl-Legien-Str. 30, 63073 Oﬀenbach/Main, Germany
yaochu.jin@honda-ri.de
The main purposes of this paper is twofold. First, the evolutionary dynamic
weighted aggregation (EDWA) [1] approaches are extended to the optimization of
three-objective problems. Fig. 1 shows two example patterns for weight change.
Through two three-objective test problems [2], the methods have shown to be
eﬀective. Theoretical analyses reveal that the success of the weighted aggregation
based methods can largely be attributed to the following facts:
– The change of the weights is equivalent to the rotation of the Pareto front
about the origin. All Pareto-optimal solutions, no matter whether they are
located in the convex or concave region, are dynamically capturable. In con-
trast, classical analyses of the weighted aggregation method only consider
the static stability of the Pareto-optimal solutions. Note that a dynamically
capturable Pareto-optimal solution is not necessarily statically stable.
– Many multiobjective optimization problems exhibit the characteristics
known as global convexity, which means that most Pareto-optimal solutions
are concentrated in a small fraction of the parameter space. Furthermore,
the solutions in the neighborhood in the ﬁtness space are also in the neigh-
borhood in the parameter space, and vice versa. This property is also known
as the connectedness.
– The evolution strategies are able to carry out locally causal search. Once
the population has reached any point on the Pareto front, the local search
ability is very important for the algorithms to “scan” the Pareto front point
by point smoothly. The resolution of the scanning is determined by the speed
of the weight change.
In the second part of the paper, we show some additional nice properties of
the Pareto-optimal solutions beyond the global convexity. It is empirically shown
that the Pareto-optimal set exhibits surprising regularity and simplicity in the
parameter space, which is very interesting and helpful. By taking advantage of
such regularities, it is possible to build simple models from the obtained Pareto-
optimal solutions for approximating the deﬁnition function. Such an approximate
model can be of great signiﬁcance in the following aspects.
– It allows to get more accurate, more complete Pareto solutions from the
approximate solutions obtained by an optimizer. Fig. 2(a) shows the Pareto
front obtained by the EDWA. The Pareto front reconstructed from the ap-
proximate deﬁnition function is presented in Fig. 2(b).
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 636–637, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Solving Three-Objective Optimization Problems
637
1.0
1.0
1.0
t
0
0
0
w2
w1
T
3T
6T
T
2T
4T
5T
2T
3T
5T
6T
4T
w3
t
t
0
50
100
150
200
250
300
350
400
450
500
0
0.2
0.4
0.6
0.8
1
Generations
Change of Weights
w 1 
w 2 
w 
3 
(a)
(b)
Fig. 1. An example of changing weights for (a) BWA and (b) DWA for solving three-
objective optimization problems.
– It alleviates many diﬃculties in multiobjective optimization. If the whole
Pareto front can be reconstructed from a few Pareto solutions, then many
requirements on the optimizer can be alleviated, e.g., a uniform distribution
is no more critical in approximating Pareto-optimal solutions.
0
2
4
6
8
10
15
15.5
16
16.5
17
17.5
−0.1
−0.05
0
0.05
0.1
0.15
f1
f2
f3
0
2
4
6
8
15.5
16
16.5
17
−0.1
−0.05
0
0.05
0.1
f1
f2
f3
1 
2 
3 
4 
S 
(a)
(b)
Fig. 2. (a) Obtained by the optimizer, (b) Reconstructed.
References
1. Y. Jin, M. Olhofer, and B. Sendhoﬀ. Evolutionary dynamic weighted aggregation
for multiobjective optimization: Why does it work and how? In Genetic and Evo-
lutionary Computation Conference, pages 1042–1049, San Francisco, CA, 2001.
2. R. Viennet, C Fonteix, and I. Marc. Multicriteria optimization using genetic al-
gorithms for determining a pareto set. International Journal of Systems Science,
27(2):255–260, 1996.

The Principle of Maximum Entropy-Based
Two-Phase Optimization of Fuzzy Controller by
Evolutionary Programming
Chi-Ho Lee, Ming Yuchi, Hyun Myung, and Jong-Hwan Kim
Dept. of Electrical Engineering and Computer Science,
Korea Advanced Institute of Science and Technology (KAIST),
373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea
{chiho,ycm,johkim}@vivaldi.kaist.ac.kr
Abstract. In this paper, a two-phase evolutionary optimization scheme
is proposed for obtaining optimal structure of fuzzy control rules and
their associated weights, using evolutionary programming (EP) and the
principle of maximum entropy (PME) based on the previous research [1].
1
Two-Phase Evolutionary Optimization
A fuzzy logic controller (FLC) with weighted rules, which is equivalent to a
conventional fuzzy controller with a weighting factor of each rule, is adopted [2]
and a two-phase evolutionary optimization scheme is applied to the FLCs.
In the ﬁrst phase, initial population for rule structures are given as a stable
fuzzy rule. Rule structures and scale factors of the error, change of error and
input to the FLC are optimized by EP. The variation of the rule structures is
done by the adjacent mutation operator and the scale factors are mutated by the
Gaussian random variables. The objective function is constituted by the sum of
error, sum of input and the number of used rules.
Scale
factors
Fuzzy rule
generation
Linguistic
variables
EP with
adjacent mutation
EP
based on PME
Weight
determination
Fuzzy rules
with weights
  First phase
  Second phase
Fig. 1. Overall structure of the two-phase evolutionary optimization
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 638–639, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

The Principle of Maximum Entropy-Based Two-Phase Optimization
639
In the second phase, the resultant rules and scale factors of the ﬁrst step are
used. Then PME is applied to determine the weight of each fuzzy rule eﬃciently.
The application of the PME in ﬁnding the weights is based on the assumption
that all the rules should be utilized to the greatest extent.
The optimization of the second phase can be regarded as ﬁne tunning for the
desired output response of the controlled system. Since only several decades of
generation is needed for determining the weights in the second phase, the pro-
posed scheme can be used for the on-line control of the time-varying plant. The
eﬀectiveness of the proposed scheme is demonstrated by computer simulations.
2
Simulation Results
Consider the following plant:
H(z−1) = 1
2π · 0.02940z−1 + 0.01532z−2 + 4.643 × 10−5z−3
1 −1.039z−1 + 0.03870z−2 −8.993 × 10−8z−3
(1)
In Figure 2(a), solid line is the step response of the second phase, while dotted
line is the response of the ﬁrst phase. The ﬁgure shows that the performance can
be considerably improved by employing the second phase. The control input is
also compared in Figure 2(b).
0
20
40
60
80
100
120
140
160
180
200
0
0.2
0.4
0.6
0.8
1
1.2
y(k)
(a) Step response of the sys-
tem
0
20
40
60
80
100
120
140
160
180
200
−2
0
2
4
6
8
10
12
14
u(k)
(b) Control input
Fig. 2. Step response and control input using fuzzy rule obtained in the second phase
References
1. J.-H. Kim and H. Myung, “Fuzzy Logic Control Using Evolutionary Programming
and Principle of Maximum Entropy”, Proc. First International ICSC Symposium
on Fuzzy Logic, Zurich, Switzerland, pp. C122–C127, 1995.
2. M. Mizumoto, “Fuzzy controls by fuzzy singleton-type reasoning method,” Proc. of
the Fifth IFSA world congress, Seoul, Korea, pp. 945–948, 1993.

A Simple Evolution Strategy to Solve Constrained
Optimization Problems
Efr´en Mezura-Montes and Carlos A. Coello Coello
CINVESTAV-IPN
Evolutionary Computation Group (EVOCINV)
Departamento de Ingenier´ıa El´ectrica
Secci´on de Computaci´on
Av. Instituto Polit´ecnico Nacional No. 2508
Col. San Pedro Zacatenco
M´exico D.F. 07300, M´EXICO
emezura@computacion.cs.cinvestav.mx
ccoello@cs.cinvestav.mx
1
Our Approach
In this paper, we argue that the self-adaptation mechanism of a conventional evolu-
tion strategy combined with some (very simple) tournament rules based on feasibility
similar to some previous proposals (e.g., [1]) can provide us with a highly competitive
evolutionary algorithm for constrained optimization. In our proposal, however, no extra
mechanisms are provided to maintain diversity. In order to verify our hypothesis, we
performed a small comparative study among ﬁve different types of ES: (µ +, λ)-ES
with and without correlated mutation and a (µ + 1)-ES using the “1/5-success rule”.
The tournament rules adopted in the ﬁve types of ES implemented are the following:
Between 2 feasible solutions, the one with the highest ﬁtness value wins, if one solution
is feasible and the other one is infeasible, the feasible solution wins and if both solutions
are infeasible, the one with the lowest sum of constraint violation is preferred. To evalu-
ate the performance of the ﬁve types of ES under study, we decided to use ten (out of 13)
of the test functions described in [2]. The (µ+1)−ES had the best overall performance
(both in terms of the best solution found and in terms of its statiscal measures). The
algorithm of the type of ES adopted (due to its simplicity, we decided to call it Simple
Evolution Strategy, or SES) is presented in Figure 1.
Compared with respect to other state-of-the-art techniques (due to space limitations
we only compare with respect to [2]), our algorithm produced very competitive results
(See Table 1). Besides being a very simple approach, it is worth reminding that SES does
not require any extra parameters (besides those used with an evolution strategy) and the
number of ﬁtness function evaluations performed (350,000) is the same used in [2].
Acknowledgments. The ﬁrst author acknowledges support from the mexican Consejo
Nacional de Ciencia y Tecnolog´ıa (CONACyT) through a scholarship to pursue graduate
studies at CINVESTAV-IPN’s Electrical Engineering Department. The second author
acknowledges support from (CONACyT) through project number 32999-A.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 640–641, 2003.
c⃝Springer-Verlag Berlin Heidelberg 2003

A Simple Evolution Strategy to Solve Constrained Optimization Problems
641
Begin
t=0
Create a random initial solution x0
Evaluate f(x0)
For t=1 to MAX GENERATIONS Do
Produce µ mutations of x(t−1) using:
xj
i = xt−1
i
+ σ[t] · Ni(0, 1) ∀i ∈n, j = 1, 2, . . . , µ
Generate one child xc by the combination of the µ mutations using
m=randint(1, µ)
xc
i = xm
i , ∀i ∈n
Evaluate f(xc)
Apply comparison criteria to select the best individual xt between x(t−1) and xc
t = t + 1
If (t mod n = 0)Then
σ[t] =



σ[t −n]/c if ps > 1/5
σ[t −n] · c if ps < 1/5
σ[t −n]
if ps = 1/5
End If
End For
End
Fig. 1. SES algorithm (n is the number of decision variables of the problem)
Table 1. Comparison of results between our approach (SES) and Stochastic Ranking (SR) [2].
Best Result
Mean Result
Worst Result
Problem
Optimal
SES
SR
SES
SR
SES
SR
g01
−15.000000
−15.000000
−15.000
−14.848614
−15.000
−12.999997
−15.000
g02
0.803619
0.793083
0.803515
0.698932
0.781975
0.576079
0.726288
g03
1.000000
1.000497
1.000
1.000486
1.000
1.000424
1.000
g04
−30665.539000 −30665.539062 −30665.539 −30665.441732 −30665.539 −30663.496094 −30665.539
g06
−6961.814000
−6961.813965
−6961.814
−6961.813965
−6875.940
−6961.813965
−6350.262
g07
24.306000
24.368050
24.307
24.702525
24.374
25.516653
24.642
g08
0.095825
0.095825
0.095825
0.095825
0.095825
0.095825
0.095825
g09
680.630000
680.631653
680.630
680.673645
680.656
680.915100
680.763
g11
0.750000
0.749900
0.750
0.784395
0.750
0.879522
0.750
g12
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
1.000000
References
1. Kalyanmoy Deb. An Efﬁcient Constraint Handling Method for GeneticAlgorithms. Computer
Methods in Applied Mechanics and Engineering, 186(2/4):311–338, 2000.
2. Thomas P. Runarsson and Xin Yao. Stochastic Ranking for Constrained Evolutionary Opti-
mization. IEEE Transactions on Evolutionary Computation, 4(3):284–294, September 2000.

Eﬀective Search of the Energy Landscape for
Protein Folding
Eugene Santos Jr.1, Keum Joo Kim1, and Eunice E. Santos2
1 University of Connecticut, Storrs, CT 06269
{eugene,keumjoo}@engr.uconn.edu,
2 Virginia Polytechnic Institute and State University, Blacksburg, VA 24061
santos@cs.vt.edu
Abstract. We propose a new algorithmic approach for global optimiza-
tion in protein folding. We use the information found in various local
minima to direct the search for the global minimum. In this way, we
explore the energy landscape eﬃciently by considering only the space of
local minima instead of the whole feasible space of conformations.
Our fundamental approach is to sample only the space of local minima and
guide the sampling process by exploring protein structure building blocks found
in sampled local minima. These building blocks form the basis of information
in searching for the global minimum. In particular we employ an iterative algo-
rithm that begins with an initial pool of local minima; construct a new pool of
solutions by combining the various building blocks found in the original pool;
take each solution and map them to their representative local minima; and, re-
peat the process. Our procedure seems to share a great deal of commonality with
evolutionary computing techniques. Indeed, we even employ genetic operators
in our algorithm. However, unlike existing hybrid evolutionary computing algo-
rithms where local minimization algorithms are simply used to “ﬁne-tune” the
solutions, we focus primarily on constructing local minima from previously ex-
plored minima and only use genetic operators to assist in diversiﬁcation. Hence,
our total number of iterations/generations were demonstrated (empirically) to
be quite low (≈50) whereas standard genetic algorithms and Monte Carlo are
very high ranging from 150,000 to nearly 20,000,000 generations in order to pro-
vide suﬃcient opportunity for these methods to converge and achieve their best
solution. We applied our idea to several proteins from the Protein Data Bank
(PDB) using the UNRES model[1]. We compared against Standard Genetic Al-
gorithms(SGA) and Metropolis Monte Carlo(MMC) approaches. In all cases, our
new approach computed the lowest energy conformation.
Procedure LMBE
begin
t = 0;
initialize P(t) with local minima;
while termination condition not satisﬁed do
begin
select individuals P sub
new(t) from current pool P(t);
recombine structures with selected individuals P sub
new(t);
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 642–643, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Eﬀective Search of the Energy Landscape for Protein Folding
643
determine local minima corresponding to Pnew(t)
replace local minima in Pnew(t);
evaluate structures Pnew(t);
end
end.
Although LMBE is clearly derived from standard genetic algorithm ap-
proaches, our emphasis is on exploring the local minima space and exploits the
genetic operators for diversiﬁcation of the population. Furthermore, this is po-
tentially more systematic in local minimization than memetic algorithms.
Given the prohibitive amount of time to conduct multiple runs of each
method over all 100 proteins, each method was run exactly once using the param-
eter settings determined from pre-trial runs. Hence, the weaknesses and strengths
of each method is averaged over the testbed.
For each protein, we initially constructed 100 random conformations. Next,
we found the local minimum for each conformation with the gradient descent
algorithm [2]. The initial pool consists of these 100 random minimized conforma-
tions. The same initial pool was used for LMBE, SGA and MMC for algorithm
comparison. The computation time of LMBE varied from 10 mins to 13 hrs, de-
pending on the protein length, amino acid sequence and the genetic parameters
(i.e. crossover rate, mutation rate). For MMC, the time was between 21 mins
and 16 hours. For SGA, the time varied between 13 mins to 14 hours. Table 1
shows the average energy improvement of LMBE compared with SGA, MMC,
and the baseline from PDB. For all 100 proteins, LMBE computed the best
energy conformation. Finally, it is interesting to observe that the improvement
using LMBE seems to improve signiﬁcantly for longer proteins on comparison
to the existing baseline.
Table 1. Percentage improvement of LMBE over SGA, MMC, and the baseline
Protein Group
SGA(%) MMC(%) baseline(%)
Group A (11–20 res.)
8.75
8.82
25.81
Group B (21–30 res.)
11.94
12.50
40.45
Group C (31–40 res.)
13.67
14.05
44.95
Group D (41–50 res.)
13.93
14.30
56.47
References
1. Liwo, A., Kazmierkiewicz, R., Oldziej, S., Pincus, M. R., Wawak, R. J., Rackovsky,
S., and Scheraga, H. A.: A United-Residue Force Field for Oﬀ-Lattice Protein-
Structure Simulations: III. Origin of Backbone Hydrogen-Bonding Cooperativity in
United-Residue Potentials. J. Com. Chem. (1998) 19, 259–276
2. Gay, David M.: Algorithm 611: Subroutines for Unconstrained Minimization Using
a Model/Trust-Region Approach. ACM ToMS(1983) 9, 503–524

A Clustering Based Niching Method for
Evolutionary Algorithms
Felix Streichert1, Gunnar Stein2, Holger Ulmer1, and Andreas Zell1
1 Center for Bioinformatics T¨ubingen (ZBIT), University of T¨ubingen,
Sand 1, 72074 T¨ubingen, Germany,
streiche@informatik.uni-tuebingen.de,
http://www-ra.informatik.uni-tuebingen.de
2 Institute of Formal Methods in Computer Science (FMI), University of Stuttgart,
Breitwiesenstr. 20/22, D-70565 Stuttgart, Germany,
http://www.informatik.uni-stuttgart.de/ifi/fk/index e.html
1
Clustering Based Niching
We propose the Clustering Based Niching (CBN) method for Evolutionary Algo-
rithms (EA) to identify multiple global and local optima in a multimodal search
space. The basic idea is to apply the biological concept of species in separate
ecological niches to EA to preserve diversity. We model species using a multi-
population approach, one population for each species. To identify species in a
EA population we apply a clustering algorithm based on the most suitable indi-
vidual geno-/phenotype representation. One of our goals is to make the niching
method as independent of the underlying EA method as possible in such a way
that it can be applied to multiple EA methods and that the impact of the nich-
ing method on the EA mechanism is as small as possible.
CBN starts with a single primordial unclustered population P0. Then the CBN-
EA generational cycle is entered. First for each population Pi one complete
EA generation of evaluation, selection and reproduction is simulated. Now CBN
starts with the diﬀerentiation of the populations by calling the clustering algo-
rithm on each Pi. If multiple clusters are found in Pi, it splits into multiple new
populations. All individuals of Pi not included in the clusters found are moved to
P0 as straying loners. To prevent multiple populations to explore the same niche
CBN uses representatives (e.g. a centroid) of all populations Pi>0 to determine if
populations are to be merged. To stabilize the results of the clustering algorithm
we currently reduce the mutation step size within all clustered populations Pi>0.
A detailed description of the CBN model can be found in [2].
Of course the performance of CBN depends on the clustering algorithm used,
since this algorithm speciﬁes the number and kind of niches that can be distin-
guished. We decided to use the density-based clustering [1] which can identify
an a priori unknown number of niches of arbitrary size, shape and spacing.
This multi-population approach of CBN replaces the global selection of a stan-
dard EA with localized niche based selection and mating. This ensures the sur-
vival of each identiﬁed niche if necessary. Also each converged population Pi>0
directly designates a local/global optimum.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 644–645, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

A Clustering Based Niching Method for Evolutionary Algorithms
645
Table 1. Mean of found optima, in parentheses the number of evaluations needed.
M0
M1
M2
M3
No. of optima
5
5
6
10
MS-HC
4.80
(6.000) 4.90
(6.000) 4.52
(6.000) 8.70
(6.000)
Sharing
4.66
(6.000) 4.54
(6.000) 1.98
(6.000) 8.40
(6.000)
MN-GA(W)
4.83
(355.300) 5.00
(355.300) 5.60
(812.300) 8.98 (1.221.600)
MN-GA(N)
4.94
(355.300) 4.99
(355.300) 3.91
(812.300) 9.80 (1.221.600)
CBN-ES
5.00
(6.000) 4.64
(6.000) 3.94
(6.000) 8.10
(6.000)
2
Results and Conclusions
We examined a CBN Evolution Strategy (ES), a standard ES with ﬁtness sharing
with an additional hill-climbing post-processing step and a µ-multi-start hill-
climber (MS-HC). We used a (µ + 2 · µ)-ES, µ = 100 and T = 60 generations
as default settings. We compared these algorithms the Multinational GA (MN-
GA) on four real-valued two-dimensional test functions [3]. The performance is
measured by the number of optima each algorithm has found, averaged over ﬁfty
runs. An optimum oj is considered as found if ∃xi ∈Pt=T | ∥xi, oj∥≤ϵ = 0.005,
with the ﬁnal population Pt=T = 
i Pi,t=T in the case of CBN.
Tab. 1 shows that the MN-GA needs much more ﬁtness evaluation than the ES
based methods. It shows also that the MS-HC performs well on these simple test
functions, so does Sharing in combination with the HC post-processing. Although
the parameters for MS-HC and Sharing where optimized for each problem, the
CBN-ES proves to be competitive with default parameters.
The advantages of CBN are that is does not alter the search space, that it is
able to ﬁnd niches of arbitrary size, shape and spacing and that it inherits all
properties of the applied EA method, since it does not signiﬁcantly interfere with
the EA procedure. There are a number of extensions that can further enhance
the CBN. First applying a population size balancing in the case of unevenly
sized areas of attraction. Second using a greedy strategy of convergence state
management to save function evaluations if a population Pi>0 is converged.
References
1. M. Ester, H.-P. Kriegel, J. Sander, and X. Xiaowei. A density-based algorithm for
discovering clusters in large spatial databases with noise. In E. Simoudis, J. Han,
and U. Fayyad, editors, 2nd International Conference on Knowledge Discovery and
Data Mining, pages 226–231, Portland, Oregon, 1996. AAAI Press.
2. G. Stein.
Verteiltes dynamisches Nischenmodell fuer JavaEvA.
(in German),
Diploma Thesis at the Institute of Formal Methods in Computer Science (FMI),
University of Stuttgart, Germany, 2002.
3. R. K. Ursem. Multinational evolutionary algorithms. In P.J. Angeline, Z. Michale-
wicz, M. Schoenauer, X. Yao, and A. Zalzala, editors, Proceedings of the Congress
on Evolutionary Computation, volume 3, pages 1633–1640, Mayﬂower Hotel, Wash-
ington D.C., USA, 6–9 1999. IEEE Press.

 
E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 646–656, 2003.
© Springer-Verlag Berlin Heidelberg 2003
A Hybrid Genetic Algorithm for the Capacitated Vehicle
Routing Problem
Jean Berger and Mohamed Barkaoui
Defence Research and Development Canada - Valcartier, Decision Support Technology Section
2459 Pie-XI Blvd. North, Val-Bélair, PQ, Canada, G3J 1X5
jean.berger@drdc-rddc.gc.ca
Abstract. Recently proved successful for variants of the vehicle routing
problem (VRP) involving time windows, genetic algorithms have not yet shown
to compete or challenge current best search techniques in solving the classical
capacitated VRP. In this paper, a hybrid genetic algorithm to address the
capacitated vehicle routing problem is proposed. The basic scheme consists in
concurrently evolving two populations of solutions to minimize total traveled
distance using genetic operators combining variations of key concepts inspired
from routing techniques and search strategies used for a time-variant of the
problem to further provide search guidance while balancing intensification and
diversification. Results from a computational experiment over common
benchmark problems report the proposed approach to be very competitive with
the best-known methods.
1   Introduction
In the classical vehicle routing problem (VRP) [1], customers with known demands
and service time are visited by a homogeneous fleet of vehicles with limited capacity
and initially located at a central depot. Routes are assumed to start and end at the
depot. The objective is to minimize total traveled distance, such that each customer is
serviced exactly once (by a single vehicle), total load on any vehicle associated with a
given route does not exceed vehicle capacity, and route duration combining travel and
service time, is bounded to a preset limit.
A variety of algorithms including exact methods and efficient heuristics have
already been proposed for VRP. For a survey on the capacitated Vehicle Routing
Problem and variants see Toth and Vigo [1]. The authors present both exact and
heuristic methods developed for the VRP and its main variants, focusing on issues
common to VRP. Overview of classical heuristics and metaheuristics may also be
found in Laporte et al. [2], and Gendreau et al. [3,4] respectively.
Tabu search techniques [5,6] and (hybrid) genetic algorithms represent some of the
most efficient metaheuristics to address VRP and/or its variants. The basic idea in
tabu search is to allow selection of worse solutions once a local optimum has been
reached. Different memory structures are then used to prevent repeating the same
solutions (cycling), and to diversify and intensify the search. Genetic algorithms [7–9]

A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem         647
are adaptive heuristic search methods that mimic evolution through natural selection.
They work by combining selection, recombination and mutation operations. The
selection pressure drives the population toward better solutions while recombination
uses genes of selected parents to produce offspring that will form the next generation.
Mutation is used to escape from local minima. Hybrid genetic algorithms combine the
above scheme with heuristic methods to further improve solution quality. Tabu search
heuristics have proved so far the most successful technique for the capacitated VRP
[2], [3], [10], [11]. Alternatively, despite its relative success reported for the traveling
salesman problem (see Gendreau et al., [3]) and variants of the vehicle routing
problem (VRP) involving time windows [3], [12-21], genetic algorithms have not yet
shown to compete with tabu search techniques in solving the capacitated VRP.
Limited work using genetic-based techniques for the classical capacitated VRP
reports mitigated success so far. As recently proposed procedures match the
performance of well-known classical methods [22], others fail to report comparative
performance with the best well-known routing techniques, while sometime
demonstrating prohibitive run-time to obtain modest solution quality [15], [23]. It is
nonetheless believed that genetic-based methods targeted to the classical capacitated
VRP have not yet been fully exploited.
In this paper, a competitive hybrid genetic algorithm (HGA-VRP) to address the
classical capacitated vehicle routing problem is proposed for the first time. It consists
in concurrently evolving two populations of solutions subject to periodic migration in
order to minimize total traveled distance using genetic operators combining variations
of key concepts inspired from routing techniques and search strategies used for a
time-variant of the problem to further provide search guidance while balancing
intensification and diversification. A computational experiment conducted on
common benchmark problems shows the proposed hybrid genetic approach to be
competitive with the best-published methods.
The paper is outlined as follows. Section 2 introduces the main concepts of the
proposed hybrid genetic algorithm. Basic principles and features of the algorithm are
first introduced. Then, the selection scheme, recombination and mutation operators
are presented. Concepts derived from well-known heuristics such as large
neighborhood search [24], route neighborhood-based two-stage metaheuristic [25]
and λ-interchange mechanism [26] are briefly outlined. Section 3 presents the results
of a computational experiment to assess the value of the proposed approach and
reports a comparative performance analysis to alternate methods. Finally, some
conclusions and future research directions are presented in Section 4.
2   Hybrid Genetic Approach
2.1  General Description
The proposed HGA-VRP algorithm mainly relies on the basic principles of genetic
algorithms, disregarding explicit solution encoding issues for problem representation.
Genetic operators are simply applied to a population of solutions rather than a
population of encoded solutions (chromosomes). We refer to these solutions as
solution individuals.

648        J. Berger and M. Barkaoui
Emphasizing genetic diversity, our approach consists in concurrently evolving two
populations of solutions (Pop1, Pop2) while exchanging a certain number of
individuals (migration) at the end of a new generation. Exclusively formed of feasible
solution individuals, populations are evolved to minimize total traveled distance using
genetic operators based upon variations of known routing methods. Whenever a new
best solution emerges, a post-processing procedure (RC_M) aimed at reordering
customers is applied to further improve its solution quality. The RC_M mutation
operator is introduced in Section 2.3. The evolutionary process is repeated until a
predefined stopping condition is met. The proposed technique is significantly
different from the algorithm presented by Berger and Barkaoui [14] in many respects,
including the introduction of new and more efficient operators and its application to a
problem variant.
The proposed steady-state genetic algorithm resorts to overlapping populations to
ensure population replacement for Pop1 and Pop2. At first, new individuals are
generated and added to population Popp (
2
,1
=
p
). The process continues until the
overlapping population outnumbers the initial population by np. Then, the np worst
individuals are eliminated to maintain population size using the following individual
evaluation:
.)
,
(
max
/
i
m
i
i
d
d
d
Eval =
(1)
where
di  = total traveled distance related to individual i,
dm = average total traveled distance over the individuals forming the initial
populations.
The lower the evaluation value the better the individual score (minimization
problem). An elitist scheme is also assumed, meaning that the best solution ever
computed from a previous generation is automatically replicated and inserted as a
member of the next generation.
The general algorithm is specified as follows:
Initialization
Repeat
p=1
Repeat    {evolve population Popp - new generation}
For j =1..np do
Select two parents from Popp
Generate a new solution Sj using recombination and mutation operators
associated with Popp
Add Sj to Popp
end for
Remove from Popp the np worst individuals using the evaluation function (1)
p=p+1
Until (all populations Popp have been visited)
if (new best feasible solution) then apply RC_M on best solution {cust.
reordering}
Population migration {local best solutions exchange across populations}
Until (convergence criteria or max number of generations)

A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem         649
The initialization phase involves the generation of initial populations Pop1 and Pop2
using a random procedure to construct feasible solution individuals. Solutions are
generated using a sequential insertion heuristic in which customers are inserted in
random order at randomly chosen insertion positions within routes. This strategy is
fast and simple while ensuring unbiased solution generation. Migration consists in
exchanging local best individuals from one population to another. Convergence is
assumed to occur either when solution quality fails to significantly improve over a
consecutive number of generations or, after a maximum number of generations.
2.2   Selection
The selection process consists in choosing two individuals (parent solutions) within
the population for mating purposes. The selection procedure is stochastic and biased
toward the best solutions using a roulette-wheel scheme [9]. In this scheme, the
probability to select an individual is proportional to its fitness value. Individual fitness
for both populations Pop1 and Pop2 is computed as follows:
.
i
i
d
fitness =
(2)
The notation is the same as in Equation (1). Better individuals show a shorter total
traveled distance (minimization problem).
2.3   Genetic Operators
The proposed genetic operators incorporate and combine key feature variations of
efficient routing techniques such as Solomon’s insertions heuristic I1 [27], large
neighborhood search [24] and the route neighborhood-based two-stage metaheuristic
(RNETS) [25] successfully applied for the Vehicle Routing problem with Time
Windows [1]. Details on the recombination and mutation operators used are given in
the next sections.
Recombination. A single recombination operator is considered, namely IB_X(k). It
recombines two parent solutions by removing and reinserting customers exploiting a
variant of a well-known customer insertion heuristic in constructing a child solution.
The insertion-based IB_X crossover operator creates an offspring by combining, one
at a time, k routes (R1) of parent solution P
1 with a subset of customers, formed by
nearest-neighbor routes (R2) in parent solution P
2. The neighborhood R2 includes the
routes of P
2 whose centroid is located within a certain range of r1 ∈ R
1 (centroid). A
route centroid corresponds to a virtual site whose coordinates refer to the average
position of its specific routed customers. The related range corresponds to the average
distance separating r1 from the routes defining P
2. The routes of R
1 are selected either
randomly, with a probability proportional to the number of customers characterizing a
tour or based on average distance separating consecutive customers over a route.
A stochastic removal procedure is first carried out to remove from r1, customers
likely to be migrated to alternate routes. Targeted customers are either selected
according to waiting times, distance separating them from their immediate neighbors,

650       J. Berger and M. Barkaoui
or randomly. Then, using a modified insertion heuristic inspired from Solomon [27] a
feasible child tour is constructed, expanding the altered route r1 by inserting customer
visit candidates derived from the nearest-neighbor routes R2 defined earlier. The
proposed insertion technique consists in adding a stochastic feature to the standard
customer insertion heuristic I1 [27], by selecting randomly the next customer visit
over the three best candidates with a bias toward the best. Once the construction of
the child route is completed, and reinsertion is no longer possible, a new route
construction cycle is initiated. The overall process is repeated for the k routes of R
1.
Finally, the child inherits the remaining “diminished” routes (if any) of P
1. If
unvisited customers still remain, additional routes are built using a nearest-neighbor
procedure.
The whole process is then iterated once more to generate a second child by
interchanging the roles of P
1 and P
2. Further details of the operator may be found in
Berger and Barkaoui [14].
Mutation. A suite of four mutation operators is proposed, namely LNSB_M(d),
EE_M, IEE_M and RC_M(I). Each mutator is briefly described next.
The LNSB_M (d) (large neighborhood search -based) mutation operator relies on
the concept of the Large Neighborhood Search (LNS) method proposed by Shaw [24].
The LNS consists in exploring the search space by repeatedly removing related
customers and reinserting them using constraint-based tree search (constraint
programming). Customer relatedness defines a relationship linking two customers
based upon specific properties (e.g. proximity and/or identical route membership),
such that when both customers are considered simultaneously for a visit, they can
compete with each other for reinsertion creating new opportunities for solution
improvement. Therefore, customers close to one another naturally offer interchange
opportunities to improve solution quality. Similarly, solution number of tours is more
likely to decrease when customers sharing route membership are removed all
together. As stated in Shaw [24], a set of related customers is first removed. The
reinsertion phase is then initiated.
The proposed customer reinsertion technique differs from the procedure introduced
by Shaw [24] resorting to alternate insertion cost functions and, customer visit
ordering schemes (variable ordering scheme) to carry out large neighborhood search.
Customer visit ordering determines the effective sequence of customers to be
consecutively visited while exploring the solution space (search tree expansion). For
diversification purposes, two customer reinsertion methods are proposed, one of them
being randomly selected (50% probability) on mutator invocation.
The first reinsertion method relies on the insertion cost function prescribed by
Solomon’s procedure I1 [27] for the VRP with time windows and, a rank-based
customer visit ordering scheme. Customer insertion cost is defined by the sum of key
contributions referring respectively to traveled distance increase, and delayed service
time. As for customer ordering, customers ({c}) are sorted (CustOrd) according to a
composite ranking, departing from the myopic scheme originally proposed by Shaw.
The ranking is defined as an additive combination of two separate rankings,
previously achieved over best insertion  costs (RankCost(c)) on the one hand, and
number of feasible insertion positions (Rank|Pos|(c)) on the other hand:

A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem         651
.)
)
(
)
(
(
c
Rank
c
Rank
Sort
CustOrd
Pos
Cost
+
←
(3)
The smaller the insertion cost (short total distance, traveled time) and the number
of positions (opportunities), the better (smaller) the ranking. The next customer to be
visited within the search process is selected according to the following expression:
.
)]
(
[
D
rand
L
INTEGER
CustOrd
customer
×
←
(4)
where
L
= current number of customers to be inserted,
rand = real number over the interval [0,1] (uniform random number generator),
D
= parameter controlling determinism. If D=1 then selection is purely random
(default: D=15).
Customer position selection (value ordering) is then based on insertion cost
minimization.
The second reinsertion method involves features of the successful insertion
heuristic proposed by Liu and Shen [25], for the VRP with time windows, exploiting
the maximization of a regret insertion cost function which concurrently takes into
account multiple insertion opportunities (regret cost), to determine customer visit
ordering. The regret cost -based customer visit ordering scheme is specified as
follows. In the insertion procedure proposed by Liu and Shen [25], route
neighborhoods associated to unvisited customers are repeatedly examined for
customer insertion. This new route-neighborhood structure relates one or multiple
routes to individual customers. In our approach the route neighborhood which differs
from the one reported by Liu and Shen [25], is strictly bounded to two tours,
comprising routes whose distance separating their centroid from the customer location
is minimal. Each feasible customer insertion opportunity is explored over its entire
route neighborhood. The next customer visit is selected by maximizing a so-called
regret cost function that accounts for multiple route insertion opportunities:
∑
∈
−
=
)
(
*)}
(
)
(
{
Cost
Regret 
c
RN
r
c
c
r
C
r
C
(5)
where
)
(c
RN
=
route neighborhood of customer c,
)
(r
Cc
=
minimum insertion cost of customer c within route r (see [25]),
*)
(r
Cc
=
minimum insertion cost of customer c over its route neighborhood.
For both reinsertion methods, once a customer is selected, search is carried out
over its different insertion positions (value ordering) based on insertion cost
minimization, exploiting limited discrepancy search [28] as specified in Shaw [24].
However, search tree expansion is achieved using a non-constant discrepancy factor
d, selected randomly (uniform probability distribution) over the set {1,2}. Remaining
unvisited customers (if any) are then inserted in additional routes.
The EE_M (edge exchange) mutator focuses on inter-route improvement. EE_M
attempts to shift customers to alternate routes as well as to exchange sets of customers
between two routes. It is inspired from the λ-interchange mechanism of Osman [26],
performing reinsertions of customer sets over two neighboring routes. In the proposed

652        J. Berger and M. Barkaoui
mutation procedure, each customer is explored for reinsertion in its surrounding route
neighborhood made up of two tours. Tours are being selected such that the distance
separating their centroid from customer location is minimal. Customer exchanges
occur as soon as the solution improves, i.e., we use a "first admissible" improving
solution strategy. Assuming the notation (x, y) to describe the different sizes of
customer sets to be exchanged over two routes, the current operator explores values
running over the range (x=1, y=0,1,2). The IEE_M (intra-route edge exchange)
mutation operator is similar to EE_M except that customer migration is restricted to
the same route.
The RC_M (I) (reorder customers) mutation operator is an intensification
procedure intended to reduce total traveled distance of feasible solutions by
reordering customers within a route. The procedure consists in repeatedly
reconstructing a new tour using the sequential insertion heuristic I1 over I different
sets (e.g. I=20) of randomly generated parameter values, returning the best solution
generated shall an improved one emerge.
3   Computational Results
A computational experiment has been conducted to compare the performance of the
proposed algorithm with some of the best techniques designed for VRP. The
algorithm has been tested on the well-known VRP benchmark proposed by
Christofides et al. [29]. For these instances, travel time separating two customers
corresponds to their relative Euclidean distance. Based on the study reported in
Cordeau et al. [10], the experiment consisted in performing a single simulation run for
each problem instance and reporting on average performance. HGA-VRP has been
implemented in C++, using the GAlib genetic algorithm library of Wall [30] and the
experiment carried out on a 400 MHz Pentium processor. Solution convergence is
assumed to occur when its quality fails to improve by at least 1% over 20 consecutive
generations. The parameter values for the investigated algorithm are described below.
In the LNSB_M(d) mutation operator the number of customers considered for
elimination runs in the range [15, 21]. The discrepancy factor d is randomly chosen
over {1,2}.
Parameter values for the proposed genetic operators are defined as follows:
Population size: 15
Migration: 5
Population replacement:
Elitism
Population overlap per generation: n1= n2=2
Recombination:
IB_X(k=2) (20%)
Mutation:
LNSB_M(d) (80%)
EE_M (50%), IEE_M (50%)
RC_M(I=20) - whenever a new best feasible solution is found.
The migration parameter, a feature provided by GAlib, refers to the number of
(best) chromosomes exchanged between populations after each generation. Because
of limited computational resources, parameter values were determined empirically

A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem         653
over a few intuitively selected combinations, choosing the one that yielded the best
average output.
Comparative performance is reported for some of the best-known VRP methods,
namely referred to as OS [26], GHL [31], CGL [32], TV [33], WH [34], RR [35], RT
[36], TA [37] and BB for HGA-VRP. The results are expressed in terms of total
traveled distance. Published competing methods with an average performance gap
exceeding about 1% (over all instances) of the best-known result, and/or failing to
specify run-time and computational resource characteristics, or reporting prohibitive
run-time have been deliberately omitted for comparison purposes. Additional results
involving other techniques including classical heuristics may nonetheless be found in
Cordeau et al. [10].
Table 1. Comparison of selected heuristics for VRP
Probl.
Inst (n)
Perf.
Time
OS
GHL
CGL
TV
WH
RR
BB
Best
1 (50)
Dist
(min)
524.61
1.90
524.61
6.0
524.61
4.57
524.61
0.81
524.61
20.0
524.61
1.05
524.61
2.00
524.61
2 (75)
Dist
(min)
844
0.84
835.77
53.8
835.45
7.27
838.60
2.21
835.8
50.0
835.32
43.38
835.26
14.33
835.26
3 (100)
Dist
(min)
838
25.72
829.45
18.4
829.44
11.23
828.56
2.39
830.7
145.0
827.53
36.72
827.39
27.90
826.14
4 (150)
Dist
(min)
1044.35
59.33
1036.16
58.8
1038.44
18.72
1033.21
4.51
1038.5
285.0
1044.35
48.47
1036.16
48.98
1028.42
5 (199)
Dist
(min)
1334.55
54.10
1322.65
90.9
1305.87
28.10
1318.25
7.50
1321.3
480.0
1334.55
77.07
1324.06
55.41
1291.45
6 (50)
Dist
(min)
555.43
2.88
555.43
13.5
555.43
4.61
555.43
0.86
555.4
30.0
555.43
2.38
555.43
2.33
555.43
7 (75)
Dist
(min)
911.00
17.61
913.23
54.6
909.68
7.55
920.72
2.75
911.8
45.0
909.68
82.95
909.68
10.5
909.68
8 (100)
Dist
(min)
878.00
49.99
865.94
25.6
866.38
11.17
869.48
2.90
878.0
165.0
866.75
18.93
868.32
5.05
865.94
9 (150)
Dist
(min)
1184.00
76.26
1177.76
71.0
1171.81
19.17
1173.12
5.67
1176.5
345.0
1164.12
29.85
1169.15
17.88
1162.55
10(199)
Dist
(min)
1441.00
76.02
1418.51
99.8
1415.40
29.74
1435.74
9.11
1418.3
535.0
1420.84
42.72
1418.79
43.86
1395.85
11(120)
Dist
(min)
1043.00
24.07
1073.47
22.2
1074.13
14.15
1042.87
3.18
1043.4
275.0
1042.11
11.23
1043.11
22.43
1042.11
12(100)
Dist
(min)
819.59
14.87
819.56
16.0
819.56
10.99
819.56
1.10
819.6
95.0
819.56
1.57
819.56
7.21
819.56
13(120)
Dist
(min)
1547.00
47.23
1573.81
59.2
1568.91
14.53
1545.51
9.34
1548.3
510.0
1550.17
1.95
1553.12
34.91
1541.14
14(100)
Dist
(min)
866.37
19.60
866.37
65.7
866.53
10.65
866.37
1.41
866.4
140.0
866.37
24.65
866.37
4.73
866.37
Average
Deviation from
Best
Average Time
(min)
1.03%
33.60
0.86%
46.8
0.69%
13.75
0.64%
3.84
0.63%
222.85
0.55%
24.65
0.48%
21.25
Computational results for all problem data sets are summarized in Table 1. The
first column describes the various instances and their related size whereas the second
specifies total traveled distance and run-time (in minutes). The following columns
refer to particular problem-solving methods. Best-known results are depicted in the
last column (Taillard [37] and, Rochat and Taillard [36] for instances 5 and 10). The

654        J. Berger and M. Barkaoui
last row refers to average run-time and performance deviation from the best-known
solutions over all problem instances. Related computer platforms include VAX 8600
for OS, Silicon Graphics 36 MHz for GHL, Sun Ultrasparc 10 (440 MHz) for CGL,
Pentium PC 200 MHz for TV, Sun 4/630 MP for WH, Sun Sparc4 IPC for RR,
Silicon Graphics 100 MHz for RT, Silicon Graphics 4D/35 for TA and Pentium 400
MHz for BB respectively. Explicit results for RT and TA have been omitted because
no run-time was provided. It is worth noticing that reported results for WH includes
the best computed solution over five execution runs as well as cumulative run-time.
The results of the experiment do not show any conclusive evidence to support a
dominating heuristic over the others. But, solution quality and run-time reported for
BB proves the HGA-VRP method to be competitive in comparison to alternate
techniques as it mostly matches the performance of best-known heuristic routing
procedures. Accordingly, the average solution quality deviation (0.48%) and
reasonable run-time obtained certainly shows that hybrid genetic algorithms can be
comparable to tabu search techniques.
4   Conclusion
A hybrid genetic algorithm (HGA-VRP) to address the classical capacitated vehicle
routing problem was presented. Focusing on total traveled distance minimization,
HGA-VRP concurrently evolves two populations of solutions in which respective best
individuals are mutually exchanged through migration over each generation. Genetic
operators were designed to incorporate and combine variations of key concepts
emerging from recent promising techniques for a time-variant of the problem, to
further emphasize search diversification and intensification. Results from a limited
computational experiment showed that HGA-VRP is cost-effective and very
competitive in comparison to the best-known VRP metaheuristics.
Future work will be conducted to further improve the proposed algorithm. Existing
alternate metaheuristic features and insertion procedures including techniques
explicitly designed for the capacitated VRP will be examined to enhance genetic
operators while reducing computational cost. Other improvements lie in the
introduction of alternate population replacement schemes, fitness models, and an
adaptive scheme to dynamically adjust parameters simplifying the configuration
procedure in selecting suitable parameters. Application of the approach to other
related problems will be explored as well.
References
1. 
Toth, P. and D. Vigo (2002), "The Vehicle Routing Problem", SIAM Monographs Discrete
Mathematics and Applications, edited by P. Toth and D. Vigo, Philadelphia, USA.
2. 
Laporte, G., M. Gendreau, J.-Y. Potvin and F. Semet (1999), "Classical and Modern
Heuristics for the Vehicle Routing Problem", Les Cahiers du GERAD, G-99-21, Montreal,
Canada.
3. 
Gendreau, M., G. Laporte and J.-Y. Potvin (1998), "Metaheuristics for the Vehicle
Routing: Problem", Les Cahiers du GERAD, G-98-52, Montreal, Canada.

A Hybrid Genetic Algorithm for the Capacitated Vehicle Routing Problem         655
4. 
Gendreau, M., G. Laporte and J.-Y. Potvin (1997), "Vehicle routing: modern heuristics.
Local Search in Combinatorial Optimization", eds.: E. Aarts and J.K. Lenstra, 311–336,
Wiley:Chichester.
5. 
Glover, F. (1986), “Future Paths for Integer Programming and Links to Artificial
Intelligence”, Computers and Operations Research 13, 533−549.
6. 
Glover, F. and M. Laguna (1997), Tabu Search, Kluwer Academic Publishers, Boston.
7. 
Holland, J. H. (1975), Adaptation in Natural and Artificial Systems, University of
Michigan Press, Ann Arbor.
8. 
Jong De, K. A. (1975), An Analysis of the Behavior of a Class of Genetic Adaptive
Systems, Ph.D. Dissertation, University of Michigan, U.S.A.
9. 
Goldberg, D.E (1989), Genetic Algorithms in Search, Optimization, and Machine
Learning, Addison-Wesley, New York.
10. Cordeau, J.-F., M. Gendreau, G. Laporte, J.-Y. Potvin and F. Semet (2002), “A Guide to
Vehicle Routing Heuristics”, Journal of the Operational Research Society 53, 512-522.
11. Cordeau, J.-F. and G. Laporte (2002), "Tabu Search Heuristics for the Vehicle Routing
Problems", Les Cahiers du GERAD, G-2002-15, Montreal, Canada.
12. Bräysy, O. and M. Gendreau (2001), “Vehicle Routing Problem with Time Windows, Part
II: Metaheuristics”, Internal Report STF 42 A01025, SINTEF Applied Mathematics,
Department of Optimization, Norway.
13. Dalessandro, S.V., L.S. Ochi and L.M. de A. Drummond (1999), A Parallel Hybrid
Evolutionary Metaheuristic for the Period Vehicle Routing Problem. IPPS/SPDP 1999, 2
nd
Workshop on Biologically Inspired Solutions to Parallel Processing Problems, San Juan,
Puerto Rico, USA, 183–191.
14. Berger, J. and M. Barkaoui (2000), “An Improved Hybrid Genetic Algorithm for the
Vehicle Routing Problem with Time Windows”, International ICSC Symposium on
Computational Intelligence, part of the International ICSC Congress on Intelligent Systems
and Applications (ISA'2000), University of Wollongong, Wollongong, Australia.
15. Machado, P., J. Tavares, F. Pereira and E. Costa (2002), "Vehicle Routing Problem: Doing
it the Evolutionary Way", Proc. of the Genetic and Evolutionary Computation Conference,
New York, USA.
16. Gehring, H. and J. Homberger (2001), “Parallelization of a Two-Phase Metaheuristic for
Routing Problems with Time Windows”, Asia-Pacific Journal of Operational Research
18, 35−47.
17. Tan, K.C., L.H. Lee and K. Ou (2001), “Hybrid Genetic Algorithms in Solving Vehicle
Routing Problems with Time Window Constraints”, Asia-Pacific Journal of Operational
Research 18, 121−130.
18. Thangiah, S.R., I.H. Osman, R. Vinayagamoorthy and T. Sun (1995), “Algorithms for the
Vehicle Routing Problems with Time Deadlines”, American Journal of Mathematical and
Management Sciences 13, 323−355.
19. Thangiah, S.R. (1995), “Vehicle Routing with Time Windows Using Genetic Algorithms”,
In Application Handbook of Genetic Algorithms: New Frontiers, Volume II, 253−277, L.
Chambers (editor), CRC Press, Boca Raton.
20. Thangiah, S.R. (1995), “An Adaptive Clustering Method using a Geometric Shape for
Vehicle Routing Problems with Time Windows”, In Proceedings of the 6th International
Conference on Genetic Algorithms, L.J. Eshelman (editor), 536−543 Morgan Kaufmann,
San Francisco.
21. Blanton, J.L. and R.L. Wainwright (1993), “Multiple Vehicle Routing with Time and
Capacity Constraints using Genetic Algorithms”, In Proceedings of the 5th International
Conference on Genetic Algorithms, S. Forrest (editor), 452−459 Morgan Kaufmann, San
Francisco.
22. Sangheon, H. (2001), "A Genetic Algorithm Approach for the Vehicle Routing Problem",
Journal of Economics, Osaka University, Japan.

656        J. Berger and M. Barkaoui
23. Peiris, P. and S.H. Zak (2000), "Solving Vehicle Routing Problem Using Genetic
Algorithms", Annual Research Summary – Part I – Research, Section 1
24. Automatic Control, School of Electrical and Computer Engineering, Purdue University,
http://www.ece.purdue.edu/ECE/Research/ARS/ARS2000/PART_I/Section1/1_19.whtml.
25. Shaw, P. (1998), “Using Constraint Programming and Local Search Methods to Solve
Vehicle Routing Problems”, In Principles and Practice of Constraint Programming,
Lecture Notes in Computer Science, M. Maher and J.-F. Puget.(eds.), 417−431, Springer-
Verlag, New York.
26. Liu, F.-H. and S.-Y. Shen (1999), “A Route-Neighborhood-based Metaheuristic for
Vehicle Routing Problem with Time Windows”, European Journal of Operational
Research 118, 485−504.
27. Osman, I.H. (1993), “Metastrategy Simulated Annealing and Tabu Search Algorithms for
the Vehicle Routing Problem”, Annal of Operations Research 41, 421–451.
28. Solomon, M.M. (1987), “Algorithms for the Vehicle Routing and Scheduling Problems
with Time Window Constraints”, Operations Research 35, 254−265.
29. Harvey, W.D. and M.L. Ginsberg (1995), “Limited Discrepancy Search”, In Proceedings
of the 14th IJCAI, Montreal, Canada.
30. Christofides N., A. Mingozzi and P. Toth (1979), “The Vehicle Routing Problem”, in
Christofides N., Mingozzi A., Toth P. and Sandi C. (eds). Combinatorial Optimization,
Wiley, Chichester  315–338.
31. Wall, M. (1995), GAlib - A C++ Genetic Algorithms Library, version 2.4.
(http://lancet.mit.edu/galib-2.4/), MIT, Boston.
32. Gendreau, M., A. Hertz and G. Laporte (1994), "A Tabu Search Heuristic for the Vehicle
Routing: Problem", Management Science 40, 1276–1290.
33. Cordeau, J.-F., M. Gendreau and G. Laporte (1997), "A Tabu Search Heuristic for the
Periodic and Multi-depot Vehicle Routing Problems", Networks 30, 105–119.
34. Toth, P. and D. Vigo (1998), "The Granular Tabu Search and its Application to the
Vehicle Routing Problem", Technical Report OR/98/9, DEIS, University of Bologna,
Bologna, Italy.
35. Wark, P. and J. Holt (1994), "A Repeated Matching Heuristic for the Vehicle Routing
Problem", Journal of Operational Research Society 45, 1156–1167.
36. Rego, C. and C. Roucairol (1996), “A Parallel Tabu Search Algorithm Using Ejection
Chains for the Vehicle Routing Problem”, In: Osman IH and Kelly JP (eds). Meta-
Heuristics: Theory and Applications, Kluwer, Boston, 661–675.
37. Rochat, Y. and E.D. Taillard (1995), “Probabilistic Diversification and Intensification in
Local Search for Vehicle Routing”, Journal of Heuristics 1, 147–167.
38. Taillard E.D. (1993), “Parallel Iterative Search Methods for Vehicle Routing Problems”,
Networks 23, 661–673.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 657–668, 2003.
© Springer-Verlag Berlin Heidelberg 2003
An Evolutionary Approach to Capacitated Resource
Distribution by a Multiple-Agent Team
Mudassar Hussain
1, Bahram Kimiaghalam
1, Abdollah Homaifar
1, Albert Esterline
1,
and Bijan Sayyarodsari
2
1 NASA Autonomous Control and Information Technology Center, Department of Electrical
Engineering, North Carolina A&T State University, Greensboro, NC 27411
muddie27@yahoo.com, {bahram, homaifar, esterlin}@ncat.edu
2 Pavilion Technologies, 11100 Metric Blvd., #700
Austin, TX 78758
bijan@pav.com
Abstract. A hybrid implementation of an evolutionary metahueristic scheme
with local optimization has been applied to a constrained problem of routing
and scheduling a team of robotic agents to perform a resource distribution task
in a possibly dynamic environment.  In this paper a central planner is responsi-
ble for planning routes and schedules for the entire team of cooperating robots.
The potential computational complexity of such a centralized solution is ad-
dressed by an innovative genetic approach that transforms the task of multiple
route design into a special manifestation of the traveling salesperson problem.
The key advantage of this approach is that globally optimal or near optimal so-
lutions can be produced in a timeframe amenable for real-time implementation.
The algorithm was tested on a set of standard problems with encouraging re-
sults.
1   Introduction
In the era of digital technology, the demand for technological solutions to increasingly
complex problems is climbing rapidly. With this increase in demand, the tasks which
robots are required to execute also rapidly grow in variety and complexity. A single
robot is no longer the best solution for many of these new application domains; in-
stead, teams of robots are required to coordinate intelligently for successful task exe-
cution. For example, a single robot is not an efficient solution to automated construc-
tion [1], urban search and rescue, assembly-line automation [2], mapping and
investigation of unknown and hazardous environments [3], and many other similar
tasks.
In this work the problem of resource distribution to a set of distributed goal points
by a team of agents is addressed. The formulation is called the Multi-Source Multi-
Robot Scheduling (MSMRS) problem. In the MSMRS problem a number of robotic
vehicles are available to service a set of goal points with certain demands for a spe-
cific type of resource stored at a number of depots or source points in the environ-
ment. The capacitated multi-source multi-robot scheduling problem (MSMRS) is an
extension to the traditional vehicle routing problem (VRP) in the sense that it in-

658
M. Hussain et al.
corporates additional features and constraints, e.g., multiple depots or resource distri-
bution points for serving the demands at the distributed goal points. The vehicles can
use the nearest or optimally located depot for reloading in case the need arises while
serving the assigned customers or goal points. The problem has an apparent analogy
to the VRP. The difficulty in finding a solution lies in the added complexity and gen-
erality of the MSMRS problem. The VRP has itself been proven to be NP complete in
[4] and hence cannot be solved to optimality in polynomial time. Optimal solutions
for small instances of the VRP have been reported in the literature using exact meth-
ods like branch and bound, branch and cut, column generation and dynamic pro-
gramming techniques [5].
2   Problem Formulation
In the capacitated MSMRS problem a number of possibly heterogeneous robotic ve-
hicles with capacity ci are available to service a set of goal points. Each goal point has
a demand for a specific type of resource stored at different depots or source points in
the environment. The objective is to minimize a measure of time and/or distance re-
quired to distribute the desired resources to the goal points using optimum number of
vehicles. We have treated the MSMRS problem as a form of multiple traveling sales-
person problem (MTSP), and the core component of our algorithm is the transforma-
tion of this MTSP into a single Traveling salesperson representation that can be
solved efficiently.
2.1 
Multi-vehicle Resource Distribution with One Source/Depot
We define a multiple robot-scheduling (MRS) problem without capacity constraints
as one in which n goal points have to be visited by m robotic vehicles, represented by
)
,....
,
,
(
1
2
1
0
−
m
R
R
R
R
, after first going to one source or depot point s. This means the n
goal points can be divided into at most m groups to be assigned to the m available ve-
hicles. If the n goal points are represented by an n element permutation vector, we
have to use at most m-1 delimiters or markers to indicate the separate subgroups of
goal points assigned to different vehicles. These delimiters will also be referred to as
virtual sources or as copies of the source point in the rest of this paper. One such de-
limiter is implicitly assumed to be present at the start and end of the permutation ar-
ray. To represent the delimiters, we append m-1 elements to the original n element ar-
ray to make it an array of length n+m-1. These delimiters can have any random
distribution within the permutation vector. If two or more of them appear adjacent in
the array, it means that only one of the whole group of vehicles represented by the
adjacent delimiter points will be used to serve the following group of goal points, and
the number of subgroups will be less than m. The tour or group assigned to the vehi-
cles contains all the following points until a new delimiter or a group of delimiters is
encountered. Hence, in case of any adjacent delimiters appearing within the array,
m
q <
 vehicles will serve the n goal points. We call the sequence of goal points as-
signed to a robotic vehicle a subtour. The different arrangements of the delimiters
within a solution array have different associated costs, and the algorithm looks for

An Evolutionary Approach to Capacitated Resource Distribution
659
improvement in this cost. Since the m-1 additional elements of the array are only hy-
pothetical markers, we can treat the whole solution array as a graph 
)
,
(
V
N
G
, where
N=n+m-1 is the set of nodes (goal points and additional virtual points appearing as
markers and representing vehicles serving their individual tours), and V is the set of
arcs connecting these goal points. The task is to construct a Hamiltonian cycle, as in
the case of the single TSP, that starts and ends at the source implicitly represented by
an invisible delimiter and is assumed to be assigned to vehicle 
0
R . The rest of the q
subtours are assigned to vehicles 
1
2
1
,....
,
−
q
R
R
R
. The cost of all the subtours are cal-
culated and summed up to give a measure of total cost of the overall tour scheme rep-
resented by each candidate solution array.
Let 
q
k
k
k
k
,.....,
,
,
3
2
1
 be the numbers of goal points in the subtours 1 to q and
( ), 1
i
i
g
j
j
k
≤
≤
 is a goal point in the subtour i then the subtour can be represented as
q
i
k
g
g
g
i
subtour
i
i
i
i
,....
2,1
where
))
(
..,
),........
2
(
),
1(
(
)
(
=
=
(1)
and the cost for each sub-tour i = 1,2, …, q can be calculated as:
∑
−
=
+
+
+
+
=
1
1
))
1
(
),
(
(
)
),
(
(
))
1(
,
(
)
(
))
(
(
cost
ik
j
i
i
i
j
g
j
g
dist
s
k
g
dist
g
s
dist
i
d
i
subtour
(2)
Where s is the source, d(i) is the initial distance of robot i from the source s, and
dist(a,b) is the distance measure between points a and b.  The overall cost C can be
calculated as follows, where the objective is to minimize the total distance traveled or
the total time for the trips for all the vehicles:
∑
=
+
=
q
j
j
j
subtour
j
subtour
a
C
1
)]
(
cost[
)]}
(
cost[
{
max
*
(3)
where q  m, and a is a scaling factor whose value determines whether more weight is
given to the use of less or more vehicles to do the entire tour. The overall objective is
to minimize C, subject to the constraint that no goal point can be visited more than
once. This constraint is enforced in the permutation array, where each goal point can
be assigned to only one vehicle and can never be visited more than once.  In the more
complex capacitated multiple source multiple vehicle scenario, we have additional
constraints such as capacities and demands.
2.2 
Multi-vehicle Capacitated Resource Distribution with Multiple Sources
To maintain its advantageous computational properties, the solution representation for
the multi-source, multi-vehicle, capacitated resource distribution problem is kept
identical to the one for the single-source, un-capacitated problem described in Section
2.1. Hence the effect of multiple depots (S(1),S(2),….S(s)), vehicle capacities, and
goal point demands are all accounted for by a revised cost function. The revised cost
function calculates the individual subtour costs, accounts for the reload trips required
by the individual vehicles, and checks the availability of the resources at each

660
M. Hussain et al.
source/depot prior to the vehicle’s trip to the source. These trips become necessary
when, in the middle of an assigned subtour, a vehicle runs out of resources and has to
visit a source point (depot) for reload. If within a subtour the reload trip happens be-
tween the edges g(k) and g(k+1) and the optimum source point for the reload is S(m)
then the distance for the edge g(k) and g(k+1) i.e. dist(g(k), g(k+1)) will be replaced
by  dist(g(k), S(m))+ dist(S(m), g(k+1)) in the cost function. This adjustment is in turn
done for all the reloading trips in the all the subtours assigned to different vehicles.
We have adopted a common-sense strategy for the selection of the resource to
which the vehicle must travel for reload: choose the resource that minimizes the cost
of vehicles trip to the next goal point. While such strategy does not, in general, guar-
antee an optimal overall solution for an assigned tour to a vehicle (for example, a re-
ordering of the cities to be visited by a vehicle may result in a lower overall cost for
that tour), the computational burden of seeking an optimal reloading strategy con-
vinced us to adopt the above-mentioned heuristic to preserve the real-time plausibility
of the proposed algorithm. An alternate heuristic based reload optimization strategy
was also developed to seek local improvement through adjustment of reload points
and is discussed in some detail in section 3.4.
The data structure for tour representation is still the n + m – 1 length permutation
array and the reload trips are not explicitly represented in the candidate solutions.
3   The Evolutionary Algorithm: Genetic Structure
A permutation of integer values, representing the labels of n goal points to be visited,
has been used along with m-1 points treated as delimiters (virtual sources) to divide
the array into at most m sub tours. The use of a vehicle at the start of the permutation
is implicitly assumed. This makes the total length of each permutation array to be
1
−
+ m
n
 as shown in figure 1.
1
2
3
. .
..
..
n = 8
n  + 1
n  +  2
…
..
n + m - 1
Fig. 1. Representation of a Tour plan for 8 Goals and m robots
The m-1 extra points representing virtual sources are used as markers (de-
limiters) that can divide the n goal points into at most m tours. All the virtual source
points are represented by an integer of some value greater than n. So, every time this
integer appears in the permutation, the sequence of goal points following this number
up to the next virtual source point is a tour associated with one robot. If two or more
of the virtual sources happen to appear side by side or if one appears at the beginning
or the end of the permutation then the arrangement represents the use of only one of
the agents, and the rest of the robots represented by the adjacent virtual sources will
not be used. Figure 2 shows a sample chromosome with eight goal points and five ro-
bots. Here only one of the two robots represented by virtual sources at positions 7 and
8 will be used. Hence, one robot at the beginning of the tour has to go to goal points
2, 1 and 3, the second robot goes to 4 and 6, the third robot goes to 5 and 8, while the
fourth robot at position 11 goes to only point 7. All the robots can be made to go back
to the source point where they started the tour and the cost function will account for
the cost of this additional journey. Therefore, four robots, out of a total of five, will be

An Evolutionary Approach to Capacitated Resource Distribution
661
used to accomplish the combined task. Each subtour will be assigned a robot for
which the distance to the first goal point in the tour, accounting for the necessary trip
to a resource closest to the robot, is the smallest.
2
1
3
n + 1
4
6
n  + 2
n + 4
5
8
n + 3
7
Fig. 2. Tour with virtual sources distributed throughout
Note that different arrangements of these virtual sources within the candidate solu-
tion, and hence different numbers of sub-tours, are possible. The fitness value of the
chromosome will of course vary with each arrangement. Thus, by preserving a per-
mutation representation for the multi-vehicle multi-source capacitated distribution
problem, we can also determine the optimal number of vehicles needed.
3.1   Recombination and Mutation Operators
The representation allows for the use of the standard genetic operators applied to the
TSP-like sequencing problems based on the permutation representation of the candi-
date solutions. The crossover operators include partially mapped crossover (PMX)
[6], cycle crossover and modified cycle crossover (CX) [7], and the edge recombina-
tion crossover (ER) [8] and many others. Different versions of these operators can be
found in the literature and all have been coded and used in different combinations
with other genetic operators to assess their impact on the quality of the off-spring
produced.
The edge recombination operator has proved to work best on the problems where
the edge information is of critical importance and not the position of the goal points,
e.g., for all variants of the TSP problems and this conclusion proved true during the
test performed for the MSMRS evolutionary algorithm.
The swap mutation operator has been used with a low probability in this work. In
this procedure, two goal points or nodes are randomly picked from the parent and the
positions are swapped. This operation is meant to introduce diversity in the population
to prevent premature convergence.
This is a “steady state” Evolutionary Algorithm (EA), where the population
changes incrementally, one by one, rather than with the replacement of the entire gen-
eration. In each iteration, one new child is produced by breeding and replaces the
worst population member. The replacement scheme allows new individuals to be in-
serted into the population only if they differ from existing best by a certain percentage
there by preserving the diversity in the population.
3.2
2-Opt Edge Exchange Local Improvement Heuristic
To speed up the convergence of the algorithm to good solutions, a local improvement
heuristic has also been tested in the algorithm run to yield a hybridized version of the
EA. The hybrid EA incorporates the local search techniques at various stages of the
genetic process. The k-Opt like procedure [9] is used to locally optimize the sub-tours
assigned to each robotic vehicle by eliminating the crossing edges. The k-Opt ex-

662
M. Hussain et al.
change process basically comprises deletion of k edges in the tour and their replace-
ment by k new edges. If the change results in tour cost improvement, then the modi-
fied tour is kept, otherwise it is discarded. Either the whole random population gener-
ated initially (preprocessing) or the offspring, produced after the recombination and
mutation operations (post processing), can be improved. The implications of applying
the heuristic at different stages are discussed in  Section 4.
3.3
Back Stepping Heuristic for Improved Reload Point Assignment
One very important aspect affecting the cost of distribution resources is the cost of
making reload trips during the execution of the tour plans. The reload trips have to be
planned in such a way that they add minimum possible cost to the overall tour. A lo-
cal optimization process was developed with the intention of making minor adjust-
ments to the reload points along vehicle subtours to obtain an improvement in overall
tour costs. The process is referred to as Reload Back Stepping (RBS). To begin with,
all the subtours, assigned to different vehicles, within the complete tour are extracted.
The reload points, based on the full exhaustion of vehicle capacity as discussed earlier
for the cost evaluation process, are then sorted out. The parts of the subtour separated
by the reload operation will be referred to as sub-subtours here. In almost all the
cases, the vehicles have unused capacity based on this kind of reload scheme, i.e., the
points serviced after the last reload trip of the vehicle (referred to as “tail” here) do
not use all of the vehicle capacity. This unused capacity provides an opportunity for
adding more goal points to the tail, i.e., the points that were a part of the sub-subtour
before the last reload can now be added to the tail. More options are hence available
to shift (back step) the last reload point in the actual subtour to a point that minimizes
the reload trip cost. This minimization is possible because of the flexibility in the
choice of the reload points instead of having to make the trip at a fixed prescribed
point as in previous case. The available new choices for the reload point are then
evaluated by calculating the cost of a reload trip to the closest source point in each
case, and the position with the best result is picked. The last reload point is then
shifted back if needed, hence adding new points to the tail if the shift is profitable.
The points after this new last reload point are curtailed from the subtour vector and
stored in a separate array called the newtour array. The whole process is repeated for
the reduced tour and eventually the newtour array becomes the new possibly im-
proved subtour. The adjustment is propagated back toward the beginning of the sub-
tour, where the tail is always the sequence of points after the last reload point that has
not been considered for readjustment. This procedure is done to the subtours assigned
to all the vehicles, and new subtours for each are then put back together to obtain a
new overall tour with possibly lower cost. Figure 3 presents the example single source
scenario with Figure 4 presenting one with multiple sources. Here 9 point each with
demand one have to be serviced by a robotic vehicle with capacity 6.
The reload trips based on initial scheme are shown with solid lines and the new
improved reload point assignments are represented by dotted lines. The back stepping
moves yield a decrease in cost associated to the reload trip and hence to the tour as-
signed to the vehicle.

An Evolutionary Approach to Capacitated Resource Distribution
663


 







Fig. 3. Sample single source tour for reload back stepping



  







Fig. 4. Sample multiple source tour for reload back stepping
4   Discussion of the Results
The datasets that we have used for testing and comparison include the three datasets
by Augerat [10] and one by Eilon [11]. All these datasets have been used extensively
and best solutions have been reported. The datasets include problems of varying di-
mensions. The coordinates of goal points and the respective demands have been pro-
vided as well as the coordinates of the single source point or the depot. To make com-
parison to the available results in the literature possible, we have reduced the number
of sources to one, and assumed that all the vehicles are stationed at that one source
point. This effectively means that one vehicle is making the entire tour and that the
subtours indicated by the reload trips can be treated as independent trips by different
vehicles in order to make the comparison to the VRP benchmark problems feasible.
This can be done without loss of generality since the algorithm is flexible in the num-
ber and location of source points and the vehicle starting positions.
Several exploratory runs were made to find effective values for the population size
and operator probabilities. The final parameters chosen are a population size of 100
for the small sized problems, 150 for the eighty goal point problem, and 250 for the
two larger ones. This choice of population size depicts the values that performed best
during rigorous testing. Bigger initial population size is required for the bigger prob-
lems due to the requirement of the representation of more diverse regions of the
search space in the initial candidate solution pool. Recombination probability was set
to 1 and the mutation process has a low probability of 0.001. The algorithm was al-
lowed to run for more than 10000 iterations for all the sample problems tested. The
results obtained for the seven test problems have been tabulated in Table 1.
As shown in Table 1, the results obtained with the pure EA with no local im-
provement for the 32, 33, 44, 64 goal points show near optimal outputs whereas, for

664
M. Hussain et al.
larger problems, the algorithm was not able to find good solutions within the 12000
iterations and needs more run-time to converge. The same problems were tested with
a 2-Opt like local improvement heuristic, described above, applied to seed the initial
population with some quality solutions. 30% of the candidates in the initial population
were pre-improved using the 2-Opt exchange process. All of the initial population
could be pre-optimized, but this was not done in the interest of maintaining diversity
in the genetic information processed by the genetic operators. Local improvement re-
sulted in better solutions for all the problem instances and reduced the convergence
time for the algorithm (columns 7 and 8).
Table 1. Simulation results and comparison to the reported best solutions
 Prob size
No of Iter-
ations x100
Pop
size
Best reported
Best with
pure EA
Sol. reached
at iteration.
(pure EA)
Best with
2-opt Improve-
ment
Sol. reached
at iteration (2-
opt Imp)
Number of
vehicles used
% Deviation
from best
(pure EA)
% Deviation
from best
(2-opt Imp)
32
200
100
784
798.35
13000
786.5
8000
5
1.83
.32
33
200
100
742
751.23
7000
742
7000
6
1.24
0
44
200
100
944
974.46
8000
973
8470
6
3.23
3.07
64
200
100
1402
1463.76
9800
1421.8
7300
9
4.41
1.41
80
200
150
1764
2313.5
7500
1816
18000
10
31.15
2.95
10
0
1000
250
681
1002.78
10700
731.3
95000
4
47.25
6.87
13
5
1200
250
1165
1859
11300
1180
40500
7
59.5
1.29
Table 2. Comparison of results to the existing known best solutions applying
Prob
size
No of
Iteratio-
ns
Best
reported
Best with
Hybrid EA
(BS)
%De
-viation
from
best
(Hybrd
EA)
32
20000
784
786.5
.32
33
20000
742
742
0
44
20000
944
972.1
2.98
64
20000
1402
1421.8
1.41
80
20000
1764
1816
2.95
100
100000
681
706.8
3.79
135
120000
1165
1180
1.29
The same set of problems was solved using the EA augmented by both of the local
improvement heuristics, i.e. the 2-Opt local improvement and the reload back step-
ping process for reload tour improvement. The results for the test runs obtained with
the same set of parameters and stopping criteria are tabulated in Table 2. It can be

An Evolutionary Approach to Capacitated Resource Distribution
665
seen from the table that some improvement was achieved through the reassignment of
reload points for the 44 and 100 goal point problems. The reason for improvement in
only these two cases is that either enough extra capacity in the tail part of the solu-
tions was not available for the other problems or they were already close to optimal
and the initial assignment of reload points was good enough so that the RBS proce-
dure could not make any significant improvements.
The relationship between problem size and the time to reach the solution, for the
problem instances tested, has somewhere between a linear and quadratic rate of in-
crease. The time to reach the best solution is measured on a 600 MHz Intel Pentium
III based computer with 512 megabytes of physical memory and MS Windows 2000
operating system.
A sample route plot for the 64-city capacitated benchmark problem by Augerat is
provided in figure 5. It can be seen that all the robot tours are locally optimal and the
overall result is within 1.5% of the global optimum reported in the literature (Table 1)
Fig. 5.  Route plot of 64-goal point problem (Augerat, et. al.)
Since our literature search did not produce any benchmark resource distribu-
tion/vehicle routing problem with multiple resources and capacitated vehicles, we
created some hypothetical problem instances to test the utility of our proposed algo-
rithm. Sample results for a very simple and a relatively complex problem are shown
in Figure 6. Figure 6(a) shows the route distribution for a problem with nine goal
points, each having a demand of 1, two robots with capacity three, and two sources.
Figure 6(b) shows the route distribution of a ninety-six goal point problem with four
source points and five available vehicles each having a capacity of 195. The simple
problem yielded optimal solution where as the ninety-six point problem yielded a
good feasible solution, as can be seen from Figure 6(b). The exact route distributions
for the sub-tours depicted in Figure 6(b) are shown in Table 3. Column 2 shows the
breakdown of the assigned tours for each vehicle into sub-subtours depicting the
number of reloads that particular vehicle has to make to one of the source points.

666
M. Hussain et al.
Table 3. Route distribution of multi-source, multi-vehicle resource distribution problem using
the EA
Vehicle
Sub-
Subtour
Route
Demand
Cost
 1
1
84,59,41,43,63,13,8,68,38,93,92,74,55,
44,73,62,19,81
188
603
2
86,27,20,16,17,37,69,9,72,60,25
184
463
2
1
78,94,7,5,39,64,32,87,65,47,1,88,33
184
537
2
42
13
52
3
1
56,66,36,71,53,12,3,76,50,51,24,80,48,
10,2,18,14,67,96
183
444
2
6,22,85,15
87
247
4
1
21,91,23,30,83,40,49,34,4,77,31,35,82,
79,45
186
485
2
28,11,26,75,46,57,95,29,54,58,70,61,90
,52,89
195
465
Objective Function:
3296
 
        
(a)
(b)
Fig. 6. (a) 9 goal point problem  (b) 96 goal point problem
The EA hybridized with both the 2-Opt and RBS reload local optimization
schemes was also applied to the multiple source point problems to study the effect of
any adjustment of reload points for individual vehicle subtours. The effect of the re-
load back stepping local improvement to the simple 9-point example of figure 6(a) is
shown in Figure 7. In this case, the RBS applied to the original tour with a cost of
204.63 reduced the cost to 196.34 after adjustment.
The effect of application of The RBS process to the example of Figure 6(b) is
tabulated in Table 4. It can be seen from that the original overall tour cost of 3296
was reduced to 3228 due the back stepping adjustment of reload points in subtours for
the vehicles two and three. No improvement in other subtours was obtained due to the

An Evolutionary Approach to Capacitated Resource Distribution
667
unavailability of flexibility in the tail part of those subtours. Moreover, the change
due to the application of the RBS heuristic is not very significant because of the close
proximity of the source points to the vehicle subtour clusters. It can be much more
significant if the source points are located farther from the subtours assigned to re-
spective vehicles
     
                 
(a)
(b)
Fig. 7. Effect of the application of RBS heuristic to 9 goal point multiple source problem:
(a)After improvement, (b) original assignment
Table 4. Effect of cost improvement with RBS heuristic for 96 goalpoint multiple source pro-
blem
Vehicles
Tourcost
(without RBS)
Tourcost
(with RBS)
1
1066
1066
2
589
553
3
691
659
4
950
950
Total
3296
3228
5   Conclusions and Future Work
A permutation based steady state GA and a modified version of this algorithm with
local improvements have been used to efficiently solve a Multi-robot Multi-Source,
Capacitated Resource Distribution problem. A novel formulation of the problem is
used to translate the original problem into a variant of the well known TSP problem

668
M. Hussain et al.
for which efficient GA-based solver is developed.. The results verify the utility of the
approach for routing different sized robot teams for a resource delivery application.
The algorithm has been tested in a static environment.
The algorithm has achieved acceptable results with favorable numerical properties.
Ongoing research aims at introducing more realistic constraints encountered in real-
life logistics problems into the problem. The end product will be a good robust algo-
rithm for logistics problems with spatial and temporal as well as precedence con-
straints.
References
1. 
Bohringer, K., Brown, R., Donald, B., Jennings, J., and Rus, D., “Distributed Robotic Ma-
nipulation: Experiments in Minimalism”, Proceedings of the International Symposium on
Experimental Robotics (ISER), 1995.
2. 
Cicirello, V., and Smith, S., “Insect Societies and Manufacturing”, the IJCAI-01 Workshop
on Artificial Intelligence and Manufacturing: New AI Paradigms for Manufacturing, 2001.
3. 
Burgard, W., Moors, M., Fox, D., Simmons, R., and Thrun, S., “Collaborative Multi-Robot
Exploration”, Proceedings of the IEEE International Conference on Robotics and Auto-
mation, San Francisco CA, April 2000.
4. 
Parker, G. R. and R. L. Rardin. “An Overview of Complexity Theory in Discrete Optimi-
zation: Part II. Results and Implications,” IIE Transactions, 14(2): 83–89,1982.
5. 
Araque, J.R., Kudva, G., Morin, T.L., and J.F. Pekny, “A Brach-and-Cut Algorithm for
Vehicle Routing Problems”,Annals of Operations Research 50,1994.
6. 
Goldberg, D. “Genetic Algorithms in Search Optimization and Machine Learning”,
Addison Wesley 1989.
7. 
Oliver, I. Smith, D. and Holland, J. “A Study of Permutation Crossover Operators on the
Traveling Salesman Problem”. In the proceedings of Second International Conference on
Genetic Algorithms and their Applications 1987.
8. 
Whitley, D., Starkweather, T. and Fukuay, D. “Scheduling Problems and the travelling
Salesman: The genetic Edge Recombination Operator” in Proceedings of third Interna-
tional Conference on Genetic Algorithms and their Applications, pp 133–139,1989.
9. 
Johnson, D. S., “ The Traveling Salesman Problem: A case Study”. Local Search in Com-
binatorial Optimization, John Wiley and Sons, Chichester, UK. 215–310.
10. Augerat, P. Vrp-instances. http://www-apache.imag.fr/-paugerat/VRP/INSTANCES.
11. Eilon, S. and Christofides, N. (1969), "An Algorithm for Vehicle Dispatching Problem."
Operational Research Quarterly 20(3), 309–318.

A Hybrid Genetic Algorithm Based on
Complete Graph Representation for the
Sequential Ordering Problem
Dong-Il Seo and Byung-Ro Moon
School of Computer Science & Engineering, Seoul National University
Sillim-dong, Kwanak-gu, Seoul, 151-742 Korea
{diseo, moon}@soar.snu.ac.kr
http://soar.snu.ac.kr/˜{diseo, moon}/
Abstract. A hybrid genetic algorithm is proposed for the sequential or-
dering problem. It is known that the performance of a genetic algorithm
depends on the survival environment and the reproducibility of building
blocks. For decades, various chromosomal structures and crossover opera-
tors were proposed for the purpose. In this paper, we use Voronoi quan-
tized crossover that adopts complete graph representation. It showed
remarkable improvement in comparison with state-of-the-art genetic al-
gorithms.
1
Introduction
Given n nodes, sequential ordering problem (SOP) is the problem of ﬁnding
a Hamiltonian path of minimum cost satisfying given precedence constraints.
Formally, given a set of nodes V = {1, 2, . . . , n} and cost matrix C = (cij),
cij ∈N ∪{∞}, i, j ∈V , it is the problem of ﬁnding a Hamiltonian path π that
satisﬁes precedence constraints and minimizes the following:
Cost(π) =
n−1

i=1
cπ(i)π(i+1).
Here, the precedence constraints are marked by inﬁnity (∞) in the cost matrix,
i.e., if cji = ∞, node j cannot precede node i in the path. The relationship is
denoted by i ≺j; node i is called a predecessor of node j and node j is called
a successor of node i. It is assumed that the path starts at node 1 and ends
at node n, i.e., 1 ≺i and i ≺n for all i ∈V \ {1, n}. Generally, the cost ma-
trix C is asymmetric and the precedence constraints are transitive and acyclic.
The problem is also called ‘asymmetric Hamiltonian path problem with prece-
dence constraints’. The special case of SOP with empty precedence constraints
is reduced to asymmetric traveling salesman problem (ATSP). As ATSP is an
NP-hard problem, so is SOP.
The problem arises in various practical ﬁelds such as manufacturing, rout-
ing, and scheduling. However, not very much attention has been paid to the
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 669–680, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

670
D.-I. Seo and B.-R. Moon
problem, while TSP, which is a reduction of SOP, has been one of the most pop-
ular problems in the combinatorial optimization area. Cutting-plane approach
[1], Lagrangian relax-and-cut method [2], and branch-and-cut algorithm [3] are
mathematical model-based approaches. The genetic algorithm using a crossover
called maximum partial order/arbitrary insertion (MPO/AI) [4] and the hy-
brid ant colony system called HAS-SOP [5] are state-of-the-art metaheuristics
for SOP. Path preserving 3-Opt (pp-3-Opt) algorithm and its variants such as
SOP-3-exchange [5] are the most popular local improvement heuristics for hybrid
metaheuristics.
In this paper, we propose a new genetic algorithm for SOP. We adopt Voronoi
quantized crossover to exploit the topological linkages of genes in the genetic
search. The crossover is based on complete graph representation.
The rest of this paper is organized as follows. We mention the background
in Section 2 and describe the proposed genetic operators in Section 3. The ex-
perimental results are provided in Section 4. Finally, the conclusions are given
in Section 5.
2
Background
The building block hypothesis implies that the power of a genetic algorithm
lies in its ability to create and grow the building blocks eﬃciently. Building
blocks appear in interactive gene groups. The interaction between genes means
the dependence of a gene’s contribution to the ﬁtness upon the values of other
genes. The interaction is also called epistasis in GA, although it is wider than
the biological deﬁnition of epistasis [6,7,8].
A gene group is said to have strong linkage if the survival probability of the
corresponding schema is higher than normal, and it is said to have weak link-
age otherwise [6]. To make building blocks survive through recombinations, we
must let the strongly epistatic gene groups have stronger linkage than ordinary
gene groups [6,9]. The linkage of a gene group is aﬀected by various factors.
Particularly, the linkage determined by the relative positions of genes in the
chromosome is called topological linkage [10]. In the case, each gene is placed in
an Euclidean or non-Euclidean space, called chromosomal space, to represent the
linkages between genes. In order to make the topological linkages reﬂect well the
epistatic structure of a given problem, we need to choose an appropriate chromo-
somal structure. The chromosomal structure here means the conceptual struc-
ture of genes used for the crossover operator. A typical chromosomal structure
is one-dimensional array. In general, multi-dimensional representations are more
advantageous than simple one-dimensional representations for highly epistatic
problems [10]. For example, two-dimensional array, two-dimensional real space
(plane), and complete graph are available.
Recently, a large number of genetic algorithms that exploit the topological
linkages of genes have been proposed. They are classiﬁed into three models:
static linkage model, adaptive linkage model, and evolvable linkage model [10].
The linkages are ﬁxed during the genetic process in the static linkage model.

A Hybrid Genetic Algorithm Based on Complete Graph Representation
671
1.
VQX(n, k, dg, p1, p2)
2.
{
3.
I ←{1, 2, . . . , n}; K ←{1, 2, . . . , k};
4.
Select a subset R = {s1, s2, . . . , sk} ⊂I at random;
5.
for each i ∈I {
6.
r[i] ←arg min
j∈K{dg(sj, i)}, sj ∈R;
7.
}
8.
for each j ∈K { u[j] ←0 or 1 at random; }
9.
for each i ∈I {
10.
if (u[r[i]] = 0 and u[r[p1[i]]] = 0) then o[i] ←p1[i];
11.
else if (u[r[i]] = 1 and u[r[p2[i]]] = 1) then o[i] ←p2[i];
12.
else o[i] ←nil;
13.
}
14.
o ←GreedyRepair(o);
15.
return o;
16. }
Fig. 1. Voronoi quantized crossover for SOP.
They adaptively changes in the adaptive linkage model, and evolve in parallel
with the allele values in the evolvable linkage model. We adopt the Voronoi
quantized crossover [11] and apply the static linkage model in this paper.
3
Genetic Operators
3.1
Voronoi Quantized Crossover
In Voronoi quantized crossover (VQX), a chromosome is a complete graph of
genes where each edge weight, called genic distance, reﬂects the epistatic strength
between the two corresponding genes. The graph is directed if the genic distance
is asymmetric. In fact, the genes are assigned a position in a non-Euclidean space
deﬁned by the genic distances. By adopting such a non-Euclidean chromosomal
space, we aim to reﬂect the epistases with minimal distortion in the crossover.
The proposed heuristic for the genic distance assignment is described in Section
3.2. VQX was applied to the traveling salesman problem for the ﬁrst time [11].
Applying VQX to SOP needs considerable modiﬁcation. We describe the VQX
for SOP in the following.
For the problem, we use the locus-based encoding1 as in [12]; one gene is
allocated for every node and the gene value represents the index of its next
node in the path. VQX has a simple structure. Figure 1 shows the pseudo code
1 The term encoding here must be distinguished from the term representation because
we mean by encoding the actual scheme to store solutions not for crossover in this
paper.

672
D.-I. Seo and B.-R. Moon
1.
GreedyRepair(o)
2.
{
3.
S ←Extract path segments from o;
4.
S ←PrecCycleDecomposition(S);
5.
s0 ←the segment that contains node 1 in S;
6.
S ←S \ {s0};
7.
do {
8.
s ←the nearest segment from s0 among the segments, in S,
9.
all whose predecessors are already contained in the segment
10.
itself or in s0;
11.
Attach s to s0; S ←S \ {s};
12.
} while (|S| > 0);
13.
o′ ←the solution of the segment s0;
14.
return o′;
15. }
Fig. 2. Greedy repair.
of VQX where n is the number of genes and k is the crossover degree ranged
from 2 to n. The function dg : I2 →R represents the genic distance. The two
parents and the oﬀspring are denoted by p1, p2, and o, respectively. Following
the convention, the notation “arg min” takes the argument that minimizes the
value. Given a number of vectors, the Voronoi region of a vector is deﬁned to
be the nearest neighborhood of the vector [13]. In VQX, the chromosomal space
deﬁned by dg is quantized into k Voronoi regions determined by the k randomly
selected genes (lines 4–7), then a sort of block-uniform crossover [14] is performed
on the regions (lines 8–13). We use a random tie-breaking in the calculation of
“arg min” in the crossover (line 6).
The part of gene inheritance (lines 8–13) goes as follows. At ﬁrst, each region
is masked white or gray at random. The white and gray correspond to 0 and 1,
respectively, in line 8. Then the genes in the white regions are inherited from
parent 1 and the others are inherited from parent 2 (lines 9–13). At this time,
the gene values are not always copied but only when a gene (gene i) and the gene
pointed by it (gene p1[i] or gene p2[i]) belong to the same-colored region. That
is, an arc in a parent has a chance to survive in the oﬀspring when both end
points belong to the same-colored region(s). The word nil is used for the genes
whose values are not determined. As a result, a partial solution consisting of path
segments is generated. We use a greedy approach to repair it. Figure 2 shows the
pseudo code of the greedy repair. Beginning with the segment containing node
1 (lines 5–6), it repeatedly merge segments available (lines 7–12). An available
segment is a segment all whose predecessors are contained in the segment itself
or in the segments already merged.
Because the segments are inherited from the two parents, it may include
precedence cycles. Therefore, a precedence cycle decomposition algorithm is re-

A Hybrid Genetic Algorithm Based on Complete Graph Representation
673
1.
PrecCycleDecomposition(S)
2.
{
3.
START:
4.
D ←∅; T ←∅;
5.
do {
6.
Select a segment s from S \ D at random;
7.
D ←D ∪{s}
8.
for each node i in s {
9.
for each predecessor ip of i {
10.
sp ←the segment contains ip in S;
11.
if (sp ̸= s and (s, sp) /∈T) {
12.
if ((sp, s) ∈T) {
13.
Split s into s′ and s′′;
14.
S ←S \ {s} ∪{s′, s′′};
15.
goto START;
16.
} else {
17.
T ←T ∪{(s, sp)};
18.
T ←TransitiveClosure(T);
19.
}
20.
}
21.
}
22.
}
23.
} while (|D| < |S|);
24.
return S;
25. }
Fig. 3. Precedence cycle decomposition algorithm.
quired before merging the segments (line 4 in Figure 2). Figure 3 shows the
pseudo code of the algorithm. The algorithm inspects the precedence relation-
ships between the segments and if it ﬁnds a precedence cycle, it decomposes the
cycle by splitting a segment involved in the cycle into two sub-segments (lines
13–14). The splitting point is determined to be the position before the node i
or the position after the node i in the ﬁgure. The position with more balanced
sizes of the resulting segments is preferred. The splitting is repeated until no
cycle is found (lines 3–23). TransitiveClosure() returns the transitive closure of
a precedence relation T (line 18).
Figure 4 shows an example of VQX for SOP. In the ﬁgure, the nodes (genes)
and the non-trivial precedence constraints are drawn by small circles and dashed
arrows, respectively. For the convenience of illustration, we assumed the chro-
mosomal space to be a two-dimensional Euclidean space. The assumption is
merely for the visualization. At ﬁrst, the chromosomal space is quantized into
nine Voronoi regions as in (a). Then, the oﬀspring inherits path segments from
the parents. Figures 4(b)–(c) shows the two parents and Figure 4(d) shows the

674
D.-I. Seo and B.-R. Moon
node 21
node 1
(a)
(c)
s’’
s’
(e)
(b)
s
(d)
(f)
Fig. 4. An illustration of VQX for SOP. (a) A chromosomal space quantized into nine
Voronoi regions. (b) Parent 1. (c) Parent 2. (d) Inherited path segments. (e) After
precedence cycle decomposition. (f) Repaired path segments.
inherited path segments. By the precedence cycle decomposition, the segment s
in (d) is split into segments s′ and s′′ in (e). Finally, an oﬀspring is generated
by the greedy repair as in (f).
3.2
Genic Distance Assignment
We apply the static linkage model to the genetic algorithm, i.e., the genic dis-
tances are assigned statically before running the genetic algorithm. Intuitively,
an ideal value of a genic distance is a value inversely proportional to the strength
of the epistasis. However, no practical method to get the exact values of the epis-
tases is known yet. Therefore, we rely on heuristics. The genic distance from gene
i to gene j is deﬁned as
dg(i, j) = |{l ∈V : cil < cij}|
(1)

A Hybrid Genetic Algorithm Based on Complete Graph Representation
675
where V is the set of nodes and cpq is the (p, q) element of the cost matrix. It is
based on the fact that the epistasis reﬂects the topological locality of the nodes.
The genic distance is asymmetric as the cost matrix C is asymmetric.
3.3
Heterogeneous Mating
It is known that VQX shows faster convergence than other crossovers; this may
cause the premature convergence of genetic algorithms. To avoid it, we use a
special type of mating used in [11]. In the mating, each individual is mated with
one of its dissimilar individuals. Hollstien called this type of breeding a negative
assortive mating [15]. The heterogeneous mating is done similarly to a selection
method called crowding [16]. First, given an individual p1, m candidate indi-
viduals are selected from the population P by roulette-wheel selection. Among
them, the most diﬀerent one from p1 is selected as p2. Hamming distance2 is
used for the distance measure. The heterogeneous mating improved the perfor-
mance of VQX by slowing down the convergence of the genetic algorithm. It is
notable that we could not found any synergy eﬀect between the mating and other
crossovers such as k-point crossover and uniform crossover in our experiments.
3.4
Properties of VQX
VQX has two notable properties:
– Convexity — Voronoi regions are convex3 (see [13] p. 330).
– Diversity — It has
n
k

2k crossover operators.
In VQX, genes in the chromosome are quantized into several groups by randomly
selected Voronoi regions, and the gene values in the same group are inherited
from the same parent. Therefore, the ﬁrst property that Voronoi regions are
convex implies that the gene groups of relatively short genic distance have high
survival probabilities, i.e., strong linkages.
The other property means that VQX has a lot of crossover operators. The
number of crossover operators aﬀects the creativity of new schemata. The num-
ber of crossover operators of k-point crossover is
n−1
k

. For n = 10000 and
k = 12, for example, VQX has about 1043 crossover operators, while k-point
crossover has about 1039. However, we should mention that we do not pursue
the maximal number of crossover operators.
4
Experimental Results
The genetic algorithms used in this paper are steady-state hybrid genetic algo-
rithms. Figure 5 shows the template. In the template, n is the problem size, m
is the group size of heterogeneous mating, k is the crossover degree, and dg is
2 the number of diﬀerent edges between two paths.
3 A set S ∈Rk is convex if a, b ∈S implies that αa + (1 −α)b ∈S for all 0 < α < 1.

676
D.-I. Seo and B.-R. Moon
1.
VGA(n, m, k, dg)
2.
{
3.
Initialize population P;
4.
repeat {
5.
p1 ←Selection(P);
6.
p2 ←MateSelection(P, m, p1);
7.
o ←VQX(n, k, dg, p1, p2);
8.
o ←Mutation(o);
9.
o ←LocalImprovement(o);
10.
P ←Replacement(P, p1, p2, o);
11.
} until (stopping condition);
12.
return the best of P;
13. }
Fig. 5. The steady-state hybrid genetic algorithm for SOP.
a b
b’ c
c’ d
a c
c’ b
b’ d
Fig. 6. An illustration of the path-preserving 3-exchange.
the genic distance. The two selected parents and the oﬀspring are denoted by
p1, p2, and o, respectively. The genetic operators and their parameters used in
this paper are summarized in the following.
– Population Initialization — Initial solutions are generated at random, then
the local improvement algorithm is applied to each of them. All the solutions
in the population are feasible.
– Population Size — |P| = 50.
– Selection — Roulette-wheel selection, i.e., the ﬁtness value fi of the solution
i is calculated as
fi = (Cw −Ci) + (Cw −Cb)/4
(2)
where Ci, Cw, and Cb are the costs of the solution i, the worst solution, and
the best solution in the population, respectively. The ﬁtness value of the best
solution is ﬁve times as great as that of the worst solution in the population.
– Group Size of Heterogeneous Mating — m = 3.
– Crossover Degree — k = 6.
– Mutation — Five random feasible-path-preserving 3-exchanges are applied
to each oﬀspring with probability 0.1. Figure 6 shows a symbolic drawing of
the exchange.

A Hybrid Genetic Algorithm Based on Complete Graph Representation
677
Table 1. The experimental results for ESC78 and ft70.∗.
Graph
GA
BK#/t
Best (%)
Avg (%)
σ/
√
t Gen Time
(Bst-Kn)
(s)
DGA 1000/1000 18230 (0.000) 18230.00 (0.000)
0.00
223
2.68
ESC78 MGA 1000/1000 18230 (0.000) 18230.00 (0.000)
0.00
335
1.83
(18230) VGA 1000/1000 18230 (0.000) 18230.00 (0.000)
0.00
115
0.91
DGA
953/1000 39313 (0.000)
39315.75 (0.007)
0.39
268
4.40
ft70.1 MGA
548/1000 39313 (0.000)
39351.03 (0.097)
1.35 2256
8.48
(39313) VGA 1000/1000 39313 (0.000) 39313.00 (0.000)
0.00
629
6.27
DGA
718/1000 40419 (0.000)
40421.26 (0.006)
0.59
710
7.66
ft70.2 MGA
117/1000 40419 (0.000)
40424.45 (0.013)
0.68
601
3.48
(40419) VGA
930/1000 40419 (0.000) 40419.18 (0.000)
0.02 1190
7.66
DGA
526/1000 42535 (0.000)
42549.87 (0.035)
0.50
205
2.41
ft70.3 MGA
619/1000 42535 (0.000)
42546.86 (0.028)
0.48
177
1.45
(42535) VGA
909/1000 42535 (0.000) 42537.82 (0.007)
0.28
319
2.38
DGA
405/1000 53530 (0.000)
53560.35 (0.057)
0.88
594
4.59
ft70.4 MGA
12/1000 53530 (0.000)
53571.90 (0.078)
0.29
666
2.54
(53530) VGA
618/1000 53530 (0.000) 53543.97 (0.026)
0.58
559
3.83
– Local Improvement — A simple path-preserving 3-Opt (pp-3-Opt) algorithm
is used. In the algorithm, a path-preserving 3-exchanges of maximum gain
is selected and performed repeatedly. The gain of an exchange, with Figure
6 as an example, is computed by
gain = cab + cb′c + cc′d −cac −cc′b −cb′d
(3)
where cpq is the (p, q) element of the cost matrix. For eﬃcient feasibility
checking, a marking technique is used as the SOP labeling procedure in [5].
– Replacement — A variant of preselection [17] is used as in [12]. Each oﬀspring
is replaced with (i) its more similar parent if the oﬀspring is better, (ii)
the other parent if the oﬀspring is better, (iii) the worst solution in the
population, otherwise.
– Stopping Condition — Until 70 percent of the population converges with the
same cost as the best solution. This takes account of the cases that more
than one best solution of the same quality competes with each other.
The algorithms were implemented in C on Pentium III 1132 MHz running
Linux 2.2.14. We tested on eighteen SOP instances taken from [18]. They are all
instances that have more than seventy nodes.
Tables 1–3 compare the performance of VGA with DGA and MGA. VGA
represents the genetic algorithms using Voronoi quantized crossover (VQX) with
the genic distance assignment heuristic described in Section 3.2. DGA and MGA
represent the genetic algorithms using distance preserving crossover (DPX) and

678
D.-I. Seo and B.-R. Moon
Table 2. The experimental results for kro124p.∗and prob.100.
Graph
GA
BK#/t
Best (%)
Avg (%)
σ/
√
t
Gen
Time
(Bst-Kn)
(s)
DGA
357/1000 39420
(0.000)
39481.95 (0.157)
1.58
431 25.46
kro124p.1 MGA
565/1000 39420
(0.000)
39505.79 (0.218)
6.12
902 15.64
(39420) VGA
930/1000 39420
(0.000) 39426.45 (0.016)
0.95
518 12.92
DGA
876/1000 41336
(0.000) 41344.27 (0.020)
0.70
529 27.96
kro124p.2 MGA
543/1000 41336
(0.000)
41566.05 (0.557) 12.77
1079 14.91
(41336) VGA
789/1000 41336
(0.000)
41353.22 (0.042)
1.76
688 12.49
DGA
6/1000 49499
(0.000)
50035.24 (1.083)
9.16
3884 42.68
kro124p.3 MGA
78/1000 49499
(0.000)
50029.73 (1.072) 12.81
3051 17.05
(49449) VGA
705/1000 49499
(0.000) 49582.64 (0.169)
6.27
1146 12.71
DGA
999/1000 76103
(0.000)
76103.27 (0.000)
0.27
227 11.75
kro124p.4 MGA
841/1000 76103
(0.000)
76138.68 (0.047)
2.61
298
7.00
(76103) VGA 1000/1000 76103
(0.000) 76103.00 (0.000)
0.00
249
8.36
DGA
0/50
1197
(0.588)
1260.72 (5.943)
5.62
112869
5108
prob.100 MGA
1/50
1175 (−1.261)
1244.36 (4.568)
4.28 2165330 54166
(1190) VGA
2/50
1163 (−2.269)
1255.86 (5.534)
5.85
122586
1767
maximum partial order/arbitrary insertion (MPO/AI)4 [4], respectively. DPX
tries to generate an oﬀspring that has equal Hamming distance to both of its
parents, i.e., its aim is to achieve that the three Hamming distances between oﬀ-
spring and parent 1, oﬀspring and parent 2, and parent 1 and parent 2 are identi-
cal. It was proposed originally for traveling salesman problem [19]. In MPO/AI,
the longest common subsequence (maximum partial order) of the two parents is
inherited to the oﬀspring and the crossover is completed by repeatedly inserting
arbitrary nodes (arbitrary insertion) not yet included into a feasible position of
minimum cost. The same local improvement algorithm was used in all the ge-
netic algorithms. In the tables, the frequency of ﬁnding solutions better than or
equal to the best-known (BK#), the best cost (Best), average cost (Avg), group
standard deviation (σ/
√
t), average generation (Gen), and average running time
(Time) are presented. We got the results from 1000 (= t) runs on ESC78, ft70.∗,
kro124p.∗, rbg1∗, and 50 runs on prob.100, rbg2∗, and rbg3∗. The values (%) af-
ter the best and average costs represent the percentages above the best-known5.
VGA outperformed other genetic algorithms for twelve instances, while DGA
and MGA outperformed the others for four instances and one instance, respec-
tively. VGA broke the best-known for prob.100, rbg323a, and rbg341a. All three
genetic algorithms consumed comparable running time for all instances except
prob.100, rbg341a, rbg358a, and rbg378a. The overall results show that VGA is
the most eﬃcient and stable among them.
4 Available
at
http://www.cs.cmu.edu/afs/cs.cmu.edu/user/chens/WWW/MPOAI
SOP.tar.gz.
5 Available at
http://www.idsia.ch/˜luca/has-sop.html.

A Hybrid Genetic Algorithm Based on Complete Graph Representation
679
Table 3. The experimental results for rbg∗.
Graph
GA
BK#/t
Best (%)
Avg (%)
σ/
√
t Gen Time
(Bst-Kn)
(s)
DGA 956/1000 1038
(0.000) 1038.07 (0.007)
0.01
97 11.19
rbg109a MGA 177/1000 1038
(0.000)
1039.88 (0.181)
0.04
772 16.51
(1038) VGA 953/1000 1038
(0.000)
1038.12 (0.011)
0.02
209 11.88
DGA 987/1000 1750
(0.000) 1750.04 (0.002)
0.01
77 31.14
rbg150a MGA 108/1000 1750
(0.000)
1752.63 (0.150)
0.03
331 33.12
(1750) VGA 901/1000 1750
(0.000)
1750.30 (0.017)
0.03
216 34.20
DGA 994/1000 2033
(0.000) 2033.01 (0.001)
0.01
192 78.37
rbg174a MGA 623/1000 2033
(0.000)
2033.71 (0.035)
0.04
381 67.14
(2033) VGA 927/1000 2033
(0.000)
2033.15 (0.007)
0.02
433 85.85
DGA
36/50 2950
(0.000)
2950.32 (0.011)
0.08
199
346
rbg253a MGA
47/50 2950
(0.000)
2950.08 (0.003)
0.05
155
222
(2950) VGA
50/50 2950
(0.000) 2950.00 (0.000)
0.00
382
325
DGA
1/50 3141
(0.000)
3144.20 (0.102)
0.28
866
2559
rbg323a MGA
0/50 3142
(0.032)
3142.42 (0.045)
0.07
628
1281
(3141) VGA
16/50 3140 (−0.032) 3141.94 (0.030)
0.13
1358
2515
DGA
0/50 2572
(0.078)
2575.30 (0.206)
0.33
1281
4262
rbg341a MGA
0/50 2571
(0.039)
2578.32 (0.324)
0.55
1686
3174
(2570) VGA
12/50 2568 (−0.078) 2571.88 (0.073)
0.28
5620 10164
DGA
3/50 2545
(0.000)
2553.98 (0.353)
0.76
1890
7345
rbg358a MGA
0/50 2549
(0.157)
2555.24 (0.402)
0.54 17355 34675
(2545) VGA
9/50 2545
(0.000) 2548.56 (0.140)
0.41
8640 24340
DGA
0/50 2819
(0.107)
2819.86 (0.137)
0.31
1065
7785
rbg378a MGA
2/50 2816
(0.000)
2818.96 (0.105)
0.22
3873 11669
(2816) VGA
22/50 2816
(0.000) 2818.44 (0.087)
0.45
7814 33774
5
Conclusions
In this paper, we proposed a new hybrid genetic algorithm for the sequential or-
dering problem (SOP). It adopts a crossover, called Voronoi quantized crossover
(VQX), on a complete graph representation. The crossover was modiﬁed by em-
ploying several new features for SOP. In the experiments, the proposed genetic
algorithm outperformed state-of-the-art genetic algorithms for SOP. We suspect
that the power of VQX is based on two main properties, convexity and diversity.
The properties are believed to improve the performance of genetic algorithms by
encouraging the survival probability and reproducibility of high-quality building
blocks in the genetic process.
Acknowledgments. This work was partly supported by Optus Inc. and Brain
Korea 21 Project. The RIACT at Seoul National University provided research
facilities for this study.

680
D.-I. Seo and B.-R. Moon
References
1. N. Ascheuer, L. F. Escudero, M. Grotschel, and M. Stoer. A cutting plane ap-
proach to the sequential ordering problem (with applications to job scheduling in
manufacturing). SIAM Journal on Optimization, 3:25–42, 1993.
2. L. F. Escudero, M. Guignard, and K. Malik. A Lagrangian relax-and-cut approach
for the sequential ordering problem with precedence relationships. Annals of Op-
erations Research, 50:219–237, 1994.
3. N. Ascheuer, M. J¨unger, and G. Reinelt. A branch & cut algorithm for the asym-
metric traveling salesman problem with precedence constraints.
Computational
Optimization and Applications, 17(1):61–84, 2000.
4. S. Chen and S. Smith. Commonality and genetic algorithms. Technical Report
CMU-RI-TR-96-27, The Robotic Institute, Carnegie Mellon University, 1996.
5. L. M. Gambardella and M. Dorigo. An ant colony system hybridized with a new
local search for the sequential ordering problem. INFORMS Journal on Computing,
12(3):237–255, 2000.
6. J. Holland. Adaptation in Natural and Artiﬁcial Systems. The University of Michi-
gan Press, 1975.
7. Y. Davidor. Epistasis variance: Suitability of a representation to genetic algorithms.
Complex Systems, 4:369–383, 1990.
8. D. I. Seo, Y. H. Kim, and B. R. Moon.
New entropy-based measures of gene
signiﬁcance and epistasis. In Genetic and Evolutionary Computation Conference,
2003.
9. D. E. Goldberg. Genetic Algorithms in Search, Optimization, Machine Learning.
Addison-Wesley, 1989.
10. D. I. Seo and B. R. Moon. A survey on chromosomal structures and operators for
exploiting topological linkages of genes. In Genetic and Evolutionary Computation
Conference, 2003.
11. D. I. Seo and B. R. Moon.
Voronoi quantized crossover for traveling salesman
problem. In Genetic and Evolutionary Computation Conference, pages 544–552,
2002.
12. T. N. Bui and B. R. Moon. A new genetic approach for the traveling salesman
problem. In IEEE Conference on Evolutionary Computation, pages 7–12, 1994.
13. A. Gersho and R. M. Gray. Vector Quantization and Signal Compression. Kluwer
Academic Publishers, 1992.
14. C. Anderson, K. Jones, and J. Ryan. A two-dimensional genetic algorithm for the
Ising problem. Complex Systems, 5:327–333, 1991.
15. R. B. Hollstien. Artiﬁcial Genetic Adaptation in Computer Control Systems. PhD
thesis, University of Michigan, 1971.
16. K. De Jong. An Analysis of the Behavior of a Class of Genetic Adaptive Systems.
PhD thesis, University of Michigan, 1975.
17. D. Cavicchio. Adaptive Search Using Simulated Evolution. PhD thesis, University
of Michigan, 1970.
18. TSPLIB.
http://www.iwr.uni-heidelberg.de/ groups/comopt/software/TSPLIB95/.
19. B. Freisleben and P. Merz. New genetic local search operators for the traveling
salesman problem. In Parallel Problem Solving from Nature, pages 890–900. 1996.

An Optimization Solution for Packet Scheduling:
A Pipeline-Based Genetic Algorithm Accelerator
Shiann-Tsong Sheu, Yue-Ru Chuang, Yu-Hung Chen, and Eugene Lai
Department of Electrical Engineering, Tamkang University,
Tamsui, Taipei, Taiwan 25137, R.O.C.
stsheu@ee.tku.edu.tw, g8350021@tkgis.tku.edu.tw
Abstract. The dense wavelength division multiplexing (DWDM) tech-
nique has been developed to provide a tremendous number of wave-
lengths/channels in an optical ﬁber. In the multi-channel networks, it
has been a challenge to eﬀectively schedule a given number of wave-
lengths and variable-length packets into diﬀerent wavelengths in order
to achieve a maximal network throughput. This optimization process has
been considered as diﬃcult as the job scheduling in multiprocessor sce-
nario, which is well known as a NP-hard problem. In current research, a
heuristic method, genetic algorithms (GAs), is often employed to obtain
the near-optimal solution because of its convergent property. Unfortu-
nately, the convergent speed of conventional GAs cannot meet the speed
requirement in high-speed networks. In this paper, we propose a novel
hyper-generation GAs (HG-GA) concept to approach the fast conver-
gence. By the HG-GA, a pipelined mechanism can be adopted to speed
up the chromosome generating process. Due to the fast convergent prop-
erty of HG-GA, which becomes possible to provide an eﬃcient scheduler
for switching variable-length packets in high-speed and multi-channel
optical networks.
1
Introduction
The fast explosion of Internet traﬃc demands more and more network band-
width day by day. It is evident that the optical network has become the Internet
backbone because it oﬀers suﬃcient bandwidth and acceptable link quality for
delivering multimedia data. With the dense wavelength division multiplexing
(DWDM) technique, an optical ﬁber can easily provide a set of parallel channels,
each operating at diﬀerent wavelengths [1], [2]. In each channel, the statistical
multiplexing technique is used to transport data packets from diﬀerent sources
to enhance the bandwidth utilization. However, this technique incurs the com-
plicated packet scheduling and channel assignment problem in each switching
node underlying the tremendous wavelengths. Hence, it is desired to design a
faster and more eﬃcient scheduling algorithm for transporting variable-length
packets in high-speed and multi-channel optical networks.
So far, many scheduling algorithms for multi-channel networks have been
proposed and they are basically designed under two diﬀerent network topolo-
gies: the star-based WDM network and the optical interconnected network. The
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 681–692, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

682
S.-T. Sheu et al.
Passive Star
 Coupler
(PSC)
N1
N2
N3
N4
 W   W       WK 
P11
P12
P13
P21
P22
P31
P32
P41
P42
P43
P44
a collection window (C)
GAs
 W   W       WK 
 W   W       WK 
 W   W       WK 
Ni  : node i.
Fig. 1. An example illustrates the packet scheduling problem in a star-based network.
star-based network consists a passive start coupler (PSC), which is in charge
of coupling packets/messages from diﬀerent wavelengths and broadcasting all
wavelengths to every connected node. The star-based network is often built for
local area network due to its centralized control [3], [4], [5]. On the contrary, the
other switching component, optical cross connect (OXC), performs eﬃciently in
the space, timing and wavelength switching/converting, and thus, is often used
in the optical backbone network [6]. An example shown in Fig. 1 is given to illus-
trate the scheduling problem in a star-based network while a number of packets
with variable lengths from four nodes (N) arriving at PSC, in which there are
K parallel channels per ﬁber (in usual, the number of nodes is smaller than
wavelengths.). In this ﬁgure, notation Pij is denoted as the j-th packet from
node i. In order to minimize the total packet switching delay and maximize the
channel utilization, these packets should be well scheduled in K available chan-
nels. In the literatures, the scheduling of sequencing tasks for multiprocessor has
been addressed extensively and proved as an NP-hard problem [7]. Similarly,
the packet scheduling and wavelength assignment problem under constraint of
sequence maintenance is also well known as a diﬃcult-to-solve issue. We believe
that it is hard to design a real-time scheduling algorithm to resolve the NP-hard
problem by general heuristic schemes.
In the past few years, Genetic Algorithms (GAs) have received considerable
attention regarding their potential as an optimization technique for complex
problems and have been successfully applied in the areas of scheduling, match-
ing, routing, and so on [7], [8], [9], [10]. The GAs mimic the natural genetic
ideas to provide excellent evolutionary processes including crossover, mutation
and selection. Although GAs have been already applied in many scheduling and
sequencing problems, the slow convergence speed of typical GAs limits the pos-
sibility of applying them in real-time systems (e.g., network optimization prob-
lems often require short response time for each decision.). It has been a major
drawback for this mechanism. To overcome the drawback, in this paper, we pro-
posed a pipeline-based hyper-generation GA (HG-GA) mechanism for solving
this tough packet schedule problem. The proposed HG-GA mechanism adopts

An Optimization Solution for Packet Scheduling
683
a hyper-generation concept to break the thumb rule of performing crossover on
two chromosomes of the same generation. By generating more ’better’ chromo-
somes than the general GA (G-GA) mechanisms within a limited time interval,
the HG-GA mechanism can improve signiﬁcantly in convergence speed, as com-
pared with the G-GA mechanisms. Therefore, the proposed HG-GA mechanism
makes the possibility of employing GAs to solve the complicated optimization
problem in any real-time environment.
The rest of paper is organized as following. The general GAs packet sched-
uler (G-GAPS) is introduced in Section 2. In Section 3, we describe the proposed
hyper-generation GAs packet scheduler (HG-GAPS) and analyze the precisely
generating time of each oﬀspring chromosome. Section 4 provides the simulation
comparison to compare the performance diﬀerence between these two technolo-
gies. Finally, some conclusion remarks are given in Section 5.
2
General Genetic Algorithms Packet Scheduler
(G-GAPS)
The G-GA mechanisms applied in industrial engineering contain three main
components: the Crossover Component (XOC), Mutation Component (MTC)
and Selection Component (SLC), as show in Fig. 2. A process to apply the G-
GA mechanisms in solving the optimization problem of packet scheduling and
wavelength assignment in networks is named as the general GAs packet scheduler
(G-GAPS) [11], [12]. Basically, the G-GAPS needs a collection window (C) for
collecting packets. As soon as the scheduling process is executed, a new collection
window will be started. This workﬂow will smooth the traﬃc ﬂow if the window
size is properly selected.
2.1
Deﬁnition
In G-GAPS, packets destined to the same output port are collected and permu-
tated for all available wavelengths to form a chromosome (i.e., a chromosome
presents a kind of permutations), in which each packet is referred to a gene [11],
[12]. The example showed in Fig. 3 demonstrates that a set of collected pack-
ets (P) with diﬀerent lengths (l) and time stamps (T) are permutated for two
available wavelengths (W1 and W2) to form a chromosome. A number of chro-
mosomes, denoted as N, will be ﬁrst generated to form the base generation (also
called the ﬁrst generation). Following the sequencing, each arrival packet is asso-
ciated with a time stamp and all permutations will follow these time stamps of
packets to be executed as in the scheduling principle. Therefore, the problem be-
comes how to decide the switch timing and the associated wavelength of a packet
so that the precedence relations of the same connection can be maintained and
the total required switching time (TRST) of schedule can be minimized. More
deﬁnitely, the TRST presents the maximum scheduled queue length of packets
assigned into diﬀerent wavelengths. Therefore, we can deﬁne the TRST(j) by
the formula:

684
S.-T. Sheu et al.
N
2(x)
1(y)
1(y)
crossover
operation
mutation
operation
mutation
operation
N
offspring
candidate
generation
1:(sj    N)
N
base generation
(1st Generation)
selection
operation
(roulette
wheel)
offspring
generation
(z)
Fig. 2. The ﬂow block diagram of the G-GAPS.
l = 3
l = 2
l = 2
l = 4
l = 3
l = 3
l = 3
l = 2
l = 5
l = 2
l = 2
2
t  = 0
t  = 18
t
chromosome (TRST = 18)
W1
W2
P31
(T1)
P11
(T1)
P32
(T3)
P42
(T3)
P43
(T4)
P41
(T1)
P21
(T2)
P12
(T3)
P13
(T4)
P22
(T4)
P44
(T5)
Fig. 3. An example presents a permutation to form a chromosome.
TRST(j) = max

trst(Wj(1)), trst(Wj(2)), ..., trst(Wj(K))

.
(1)
where j is the j-th chromosome, and trst(Wj(k)) is the TRST for these packets
scheduled in the k-th wavelength of the j-th chromosome. Here we assume an
optical ﬁber carries K wavelengths.
2.2
The Fitness Function
The ﬁtness function, denoted as Ωin the G-GAPS is deﬁned as the objective
function that we want to optimize. It is used to evaluate chromosomes during
selection operation to determine which oﬀspring should be remained as the par-
ents for the next generation. The objective function in the scheduling is the
TRST and it is often converted into maximization form. Thus, the ﬁtness value
of the j-th chromosome, denoted as Ω(j), is calculated as following:
Ω(j) = Ψworst
1
−TRST(j).
(2)
where Ψworst
1
= 
u

v luv represents the worst TRST in the ﬁrst generation
(i.e., all packets are scheduled in one wavelength.). Therefore, the optimal sched-
ule will be the chromosome with the largest ﬁtness value denoted as Ωopt.

An Optimization Solution for Packet Scheduling
685
P : The accumulated number of offspring chromosomes generated from
      selection operation.
N
2(x)
1(y)
1(y)
crossover
operation
mutation
operation
mutation
operation
G(g)
offspring
candidate
pool
1:(sj    N)
P
base generation
(1st Generation)
selection
operation
(roulette
wheel)
ts(1,g)
tp(1,g)
tp(G(g),g)
tc(G(g),g)
tc(1,g)
B(g)
offspring
pool
(z)
ts(G(g),g)
 
Fig. 4. The ﬂow block diagram of the HG-GAPS.
2.3
Implementation of the Genetic Algorithms
In G-GAPS, each crossover operation selects two chromosomes from the same
generation and generates two new oﬀspring chromosomes as the candidates for
the next generation. These candidate oﬀspring will involve in mutation opera-
tion and selection operation according to their mutation probabilities (Pm) and
ﬁtness values, respectively [11], [12].
In the implementation, we simply assume the number of chromosomes in the
base generation, say N, is even. Let Pc and Pm denote as the crossover and
mutation probabilities, respectively. According to roulette wheel method, the
selected probability of the j-th chromosome is sj = Ω(j)/ r=N
r=1 Ω(r).
3
Hyper-generation GAs Packet Scheduler (HG-GAPS)
Basically, the G-GAPS is a generation-based scheme, which processes chromo-
somes generation by generation. In this scheme, the population size in each
generation is kept to N. It means that the selection operation is only triggered
when all crossovers and mutations on the chromosomes in a generation are com-
pleted, and, also all ﬁtness values of N chromosomes are calculated to support
the roulette wheel method. These restraints cause considerable waiting time for
propagating good chromosomes to the next generation. For general optimization
problems, they do not require quick response time, thus, such a batch behav-
ior will work well to provide an acceptable solution. However, the features of
long waiting time and slow convergence speed deﬁnitely block the G-GAPS to
be a suitable solution for real-time systems. In this section, we will introduce
a pipeline-based mechanism, named hyper-generation GAPS (HG-GAPS), to
overcome the potential drawbacks of G-GAPS.
As shown in Fig. 4, the key feature of the HG-GAPS is to adopt the pipeline
concept and to discard the generation restraint to accelerate convergence speed.
From Fig. 4, at the candidate state after mutation operation, the number of

686
S.-T. Sheu et al.
base generation
1th chromosome group
2nd chromosome group
time
1
1
1
3rd chromosome group
2
3
4
5
6
7
8
9
10
11
4th chromosome group
12
13
14
15
5th chromosome group
Fig. 5. An example demonstrates the concepts of the chromosome groups and the
hyper-generation crossovers in the HG-GAPS when there are N = 10 chromosomes in
the base generation.
oﬀspring chromosomes is a function of g, which presents the number of ’chro-
mosome group’, as shown in Fig. 5. HG-GAPS uses ’chromosome group’ concept
instead of ’generation’ concept to break the crossover limitation in the same
generation (i.e., batch operation). In other words, the member of a chromosome
group may be generated from parent mating with parent, parent mating with
oﬀspring, or oﬀspring mating with oﬀspring.
3.1
Hardware Block Diagram of the HG-GAPS
The detailed HG-GAPS hardware block diagram is designed and shown in Fig. 6.
As mentioned before, all arrival packets destined to the same outlet in a collection
window are gathered and queued in a Shared Memory. Each of them is tagged
with a global time stamp. In the Shared Memory, packets with the same time
stamp are linked together. At the end of collection window, packets of the same
link are concurrently assigned into K wavelengths through an M × K switch in
a random manner to form a chromosome (where the number of the inlets (I) is
M, and K presents the number of wavelengths (W) in a ﬁber.). This procedure
is repeated in the Chromosome Generator until a number of N chromosomes
are generated to form the base generation. To promote a more eﬃcient schedul-
ing process, the ﬁrst two newborn chromosomes in the base generation will be
immediately forwarded into the XOC once they are generated.

An Optimization Solution for Packet Scheduling
687
Chromosome
Generator
(base generation)
Latch
Crossover
(XOC)
Fitness
Calculation
Selection
(SLC)
Offspring Candidate  Pool
Packet
arrival
Output
chromosome
(schedule)
Mux
Demux
Random Number Generator
data path
control signal
Accumulator [G(g)]
bypass
Accumulator
Including
parents &
offsprings
   
  trigger
elitism
trigger
Pc
Pm
Pm
Mutation
(MTC)
Filter
Mutation
(MTC)
Fitness
Calculation
bypass
Fitness
Calculation
Fitness
Calculation
            Offspring
Pool
 
Shared
Memory
M    K
Switch
 
I1
I2
IM
W1
W2
WK
Counter[N]
         fitness pool
chromosome pool
Fig. 6. The hardware architecture of HG-GAPS.
Before the system generates the ﬁrst oﬀspring chromosome, two chromosomes
needed for the XOC are provided from the Chromosome Generator and this time
period is named as the start-up phase. Then the system enters the warm-up
phase as soon as the ﬁrst oﬀspring participates the crossover operation. And the
system will switch the chromosomes from the base generation and the Oﬀspring
Pool (which is included in the SLC.) in a round robin manner. How fast of the
timing for the system entering the warm-up phase. It depends on the processing
speeds of GA components and the number of remainders in the base generation.
Once the base generation runs out of its chromosomes, the system will enter the
saturation phase, where both participators for crossover operation are provided
from the Oﬀspring Pool. Afterward, the HG-GAPS becomes a closed system, in
which the cycle processing delay is constant. In the warm-up or the saturation
phase, the chromosome ﬁrst arriving at the XOC must be buﬀered in the Latch
in order to synchronize the crossover operation with another chromosome.
In the saturation phase, the HG-GAPS behaves more like the conventional
G-GAPS. Nevertheless, there are two signiﬁcant diﬀerences between them: (1)
The oﬀspring generating procedure in the HG-GAPS is still faster than G-GAPS
due to all components in the HG-GAPS system are executed in parallel. On the
contrary, in the G-GAPS, the SLC cannot work unless the mutation operation
has been completed. Afterward, when the SLC performs selection process, the
other two components are also stalled. The stop-and-go behavior is the well-
known drawback in most batch systems. (2) The number of the chromosomes
circulating in both systems may diﬀer from each other even when the population
sizes in their base generations are set to equal. In the G-GAPS, the oﬀspring
chromosome will be selected and collected to form a new generation, and the
population size of the new generation is the same as previous one. This feature
has no longer been maintained in the HG-GAPS. Due to the limited pages, the
analyses of the timings of generating chromosomes, the population size of each
group and the convergent speed are not included in this paper.
A Random Number Generator is required for the XOC and the MTC to
generate the desired crossover probability Pc and mutation probability Pm. In

688
S.-T. Sheu et al.
addition, it also provides a random number for some random processes in the
GA operation. After the crossover operation, two mated chromosomes are sepa-
rately forwarded into the MTCs. Meanwhile, they are also bypassed to Fitness
components, one for each, to calculate their ﬁtness values and then to queue in a
temp pool (i.e., the Filter). As the pairs of the original parents and the produced
oﬀsprings are all stored in the temp pool, two chromosomes with better ﬁtness
values will be selected and pushed into the Oﬀspring Candidate Pool for elitism,
which is similar to the concept of enlarged sampling space [8].
Finally, the SLC equips two Accumulators: one is used to accumulate the
ﬁtness values of the current chromosome group and the other is used to count
the number of chromosomes queued in this group. Both of them dedicate the
necessary information for the roulette wheel method adopted in SLC. When
the last chromosome queued in Oﬀspring Pool is forwarded to the XOC, the
Oﬀspring Candidate Pool will pass whole group of chromosomes into SLC by
selection and duplication. (That is why we use the ’chromosome group’ to replace
’generation’ as a set of chromosomes to calculate the selection probability of the
chromosome in the HG-GAPS.) Thus these two Accumulators will be reset for
the next group. As soon as the oﬀspring is produced in the Oﬀspring Pool, it
can be as a new parent for the next GA cycle.
4
Simulation Model and Results
4.1
Simulation Model
In the simulation, we construct the GAPS simulation model with several re-
alistic system parameters: the number of time units consumed by each XOC
(= x), MTC (= y) and SLC (= z). Besides, there are N chromosomes in both
base generations in the G-GAPS and the HG-GAPS. During simulating, we set
the time units to be N = 10, x = 2, y = 1 and z = 2. (Here, we assume the
crossover and the selection operations are more complicated than the mutation
operation.) The simulation probabilities of the crossover (Pc) and the mutation
(Pm) operations are 0.9 and 0.05, respectively. To simplify the model, we con-
sider the deterministic service rate in each wavelength is measured in the preset
time units. The traﬃc arrival rate of a wavelength in each input ﬁber is following
a Poisson distribution with a mean λ. The packet length is following an expo-
nential distribution with a mean L in the preset time units. The number of the
wavelengths in each input or output ﬁber is K. Thus, the total traﬃc load Λ is
equal to K × λ × L. Furthermore, in order to simulate a real-time system, we ﬁx
the scheduling time period to enforce the G-GAPS and the HG-GPAS to output
its current optimal schedule within the due time.
4.2
Simulation Results
Fig. 7 shows the average TRSTs derived from the G-GAPS and the HG-GAPS
under the variable collection windows (C) and ﬁxed the scheduling time interval

An Optimization Solution for Packet Scheduling
689
10
15
20
25
30
35
40
45
50
55
0
10
20
30
40
50
60
70
80
90
100
time units
average TRST
HG-GAPS (C=30)
HG-GAPS (C=20)
HG-GAPS (C=10)
G-GAPS (C=30)
G-GAPS (C=20)
G-GAPS (C=10)
Fig. 7. The average TRSTs are simulated under the diﬀerent collection window sizes
(C) from 10 to 30 time units, when K = 8, Λ = 8 and L = 5.
at K = 8, Λ = 8 and L = 5 time units. Here, we consider the collection window
varying from 10 to 30 time units and the scheduling time period is ﬁxed at 105
time units. In Fig. 7, we can see that the G-GAPS will generate a schedule with a
smaller TRST as soon as a generation is completed. That is, the improvements
on TRST in the G-GAPS will occur at the time units of 35, 70 and 105. On
the contrary, our HG-GAPS starts to minimize the TRST within a short period
and obtains the near-optimal TRST at approximate 35 time units. In addition,
we also note that under a larger collection window size, the more gain in the
decrease TRST will be obtained by the HG-GAPS comparing to the G-GAPS.
Fig. 8 presents the diﬀerence in the accumulated chromosome generating
rates between the G-GAPS and the HG-GAPS. During the same scheduling
time period of 105 time units, the G-GAPS evolves three generations (including
the ﬁrst generation) and only generates 30 chromosomes. On the contrary, the
HG-GAPS requires a shorter period to increase the generating rate than the
G-GAPS due to its chromosome group and the pipeline concepts. HG-GAPS
does not only have the advantage in continuously generating oﬀsprings during
a short period, but also keeps the advantage in having a large candidates space
for selection operation. Therefore, HG-GAPS can evolve 56 chromosomes during
105 time units.
Fig. 9 shows the consecutively snap shops during a period of 500 time units
with a randomly selection from the whole simulation run. We set both of the
scheduling time period and the collection windows to be 50 time units. The
other system parameters are set as following: K = 8, Λ = 6.4 and L = 5 time
units. Within a limited scheduling window, the HG-GAPS always provides a

690
S.-T. Sheu et al.
0
10
20
30
40
50
60
0
10
20
30
40
50
60
70
80
90
100
time units
accumulated chromosomes
G-GAPS (N=10, x=2, y=2, z=2)
G-GAPS (N=10, x=2, y=1, z=2)
HG-GAPS (N=10, x=2, y=2, z=2)
HG-GAPS (N=10, x=2, y=1, z=2)
Fig. 8. An illustration to present the accumulated chromosomes generated by the G-
GAPS and the HG-GAPS under the diﬀerent system parameters.
25
35
45
55
65
75
0
50
100
150
200
250
300
350
400
450
500
time units during 10 scheduling windows
TRST
G-GAPS (C=50)
HG-GAPS (C=50)
Fig. 9. A comparison between the G-GAPS and the HG-GAPS in the TRSTs of the
chromosomes during 10 consecutively scheduling windows.
smaller TRST than the one from the G-GAPS. In fact, if we further shorten
the scheduling window to conform a real-time situation, the performance diﬀer-
ence between these two GAPSs will become more obvious. During a very short
period, the HG-GAPS can generate a scheduling result to approach to a near-
optimal solution, but the G-GAPS cannot. In a real continuous transmission
environment, a larger TRST from the data transmission will defer the following

An Optimization Solution for Packet Scheduling
691
scheduling tasks. Thus, the diﬀerence between the accumulated TRSTs from the
G-GAPS and the HG-GAPS will become larger and larger as the time expired,
and the packet loss is also getting larger due to the buﬀer overﬂow. Therefore, we
conclude that the proposed HG-GAPS cannot only indeed provide a signiﬁcant
improvement in solving an optimization problem, but also further support more
complicated real-time systems.
5
Conclusions
In this paper, a novel and faster convergent GAPS mechanism, the hyper-
generation GAPS (HG-GAPS) mechanism, for scheduling variable-length pack-
ets in high-speed optical networks was proposed. It is a powerful mechanism to
provide a near-optimal solution for scheduling an optimization problem within
a limited response time. This proposed HG-GAPS utilizes the hyper-generation
and pipeline concepts to speed up the way of generating chromosomes and to
shorten the evolutional consuming time produced from traditional genetic al-
gorithms. From the simulation results, we proved that the HG-GAPS is indeed
more suitable for solving the complex optimization problems, such as the packets
scheduling and the wavelength assignment problem in a real-time environment.
References
1. Charles A. Brackett: Dense Wavelength Division Multiplexing Networks: Principles
and Applications. IEEE J. Select. Areas Communication, Vol. 8, No. 6, pp. 948–
964, August (1990).
2. Paul Green: Progress in Optical Networking. IEEE Communications magazine,
Vol. 39, No. 1, pp. 54–61, January (2001).
3. F. Jia, B. Mukherjee, J. Iness: Scheduling Variable-length Messages in A Single-
hop Multichannel Local Lightwave Network. IEEE/ACM Trans. Networking, Vol.
3, pp. 477–487, August (1995).
4. J. H. Lee, C. K. Un: Dynamic Scheduling Protocol for Variable-sized Messages in
A WDM-based Local Network. J. Lightwave Technol., pp. 1595–1600, July (1996).
5. Babak Hamidzadeh, Ma Maode, Mounir Hamdi: Eﬃcient Sequencing Techniques
for Variable-Length Messages in WDM Network. J. Lightwave Tech., Vol. 17, pp.
1309–1319, August (1999).
6. Sengupta S., Ramamurthy R.: From Network Design to Dynamic Provisioning
and Restoration in Optical Cross-connect Mesh Networks: an Architectural and
Algorithmic Overview. IEEE Network, Vol. 15, Issue 4, pp. 46–54, July-Aug (2001).
7. Edwin S.H. Hou, Nirwan Ansari, Hong Ren: A Genetic Algorithm for Multipro-
cessor Scheduling. IEEE Transaction on Parallel and Distributed Systems, Vol. 5,
No. 2, February (1994).
8. Mitsuo Gen, Runwei Cheng: Genetic Algorithms and Engineering Design. Wiley
Interscience Publication, (1997).
9. J. S.R. Jang, C. T. Sun, E. Mizutani: Neuro-Fuzzy Soft Computing. Prentice-Hall
International, Inc., Chapter 7.
10. D. E. Goldberg: Genetic Algorithms in Search, Optimization, and Machine Learn-
ing. Addison-Wesley, Reading, MA, (1989).

692
S.-T. Sheu et al.
11. Shiann-Tsong Sheu, Yue-Ru Chuang, Yu-Jie Cheng, Hsuen-Wen Tseng: A Novel
Optical IP Router Architecture for WDM Networks. In Proceedings of IEEE
ICOIN-15, pp. 335–340, (2001).
12. Shiann-Tsong Sheu, Yue-Ru Chuang: Fast Convergent Genetic Algorithm for
Scheduling Variable-Length Packets in High-speed Multi-Channel Optical Net-
works. Submitted to IEEE Transaction on Evolutionary Computation, (2002).

Generation and Optimization of Train
Timetables Using Coevolution
Paavan Mistry and Raymond S.K. Kwan
School of Computing, University of Leeds
Leeds LS2 9JT, United Kingdom
{paavan,rsk}@comp.leeds.ac.uk
Train timetabling is a process of assigning suitable arrival and departure times
to trains at the stations they visit and at key track junctions. It is desirable
that the timetable focusses on passenger preferences and is operationally viable
and proﬁtable for the Train Operating Companies (TOCs). Many hard and soft
constraints need to be considered relating to the track capacities, set of trains
to be run on the network, platform assignments at stations and passenger con-
venience. In the UK, train timetabling is mainly the responsibility of a single
rail infrastructure operator - Network Rail. The UK rail network has a struc-
ture that is complex to integrate, which makes it diﬃcult to achieve regularised
train timetables that are common in many European countries. With a large
number of independent TOCs bidding for slots to operate over limited capaci-
ties, the need for an eﬃcient and intelligent computer-aided tool is obvious. This
work proposes a Cooperative Coevolutionary Train Timetabling (CCTT) algo-
rithm concerned with the automatic generation of planning timetables, which
still demands a high degree of accuracy and optimization for them to be useful.
Determining the departure times of the train trips at their origins is the most
critical step in the timetabling process. Timings of the train trips en route can
be computed from the departure times. Pathing is the time added to or removed
from a train’s journey from one station to another. The amount of duration
a train stops at the station is the dwell-time. Along with the departure and
arrival times at every station, a train’s journey also needs to determine track
and platform/siding utilisation from origin to destination.
The idea of parallel evolution of problem subcomponents that interact in use-
ful ways to optimize complex higher level structures was introduced by [3]. The
advantages of such decomposition are independent representation and evolution
of interacting subcomponents that facilitate an eﬃcient concentrated exploration
of the search space. The decision variables of the train timetabling problem are
substructured into coevolving subpopulations - the departure times (Pd), sched-
uled runtime and dwell-time patterns (Pp) and capacity usage (Pc). Departure
time of the trains being key to timetable generation, is evolved by Evolution
Strategy [2]. An adaptive mutation strategy is used to control the trains’ de-
parture time evolution with a higher probability for ﬁner mutations. Scheduled
runtime of a train is the normal travel time of a train combined with varia-
tions to the travel time during a train’s journey. Switching between high and
low scheduled runtimes and dwell-times for trains is performed through a bi-
nary representation. Hence, Pp is evolved through a Genetic Algorithm [1]. The
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 693–694, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

694
P. Mistry and R.S.K. Kwan
rail network being considered assumes a single track system (one track in each
direction) between stations with two platforms available at each station. This
network set-up facilitates platform allocation and helps identify constraint viola-
tions. With either of the two platforms to be utilised by a train at each station,
Pc evolves using a simple GA framework using binary chromosomes. The indi-
vidual being evaluated and the representatives from collaborating populations
generate a complete timetable. A greedy collaborator selection method [4] is
undertaken. The individual being evaluated is assigned a ﬁtness proportional to
that of the complete timetable. The ﬁtness function identiﬁes and penalizes hard
and soft constraint violations at the conﬂict points.
We run the algorithm with diﬀerent random seeds for 5 times and the re-
sults achieved after 1000 iterations by CCTT are promising (shown in table 1).
Considering the use of the same cost function, the quality of results i.e. the explo-
ration of search space is better than those from a two-phase Simulated Annealing
(SA) algorithm similar to the Planning Timetable Generator (PTG), which is a
sophisticated train timetable planning tool developed by AEA Technology, Rail.
Table 1. Test Results from an average of 5 runs of the algorithm
SA
CCTT
Test
Case
Best
Fitness
Avg.
Fitness
Time
(sec)
Best
Fitness
Avg.
Fitness
Time
(sec)
T-50
4064
4368
3.84
3395
3857
4.79
T-80
6064
6965
5.73
5575
6276
7.13
This research is on-going. The next phase of research will further reﬁne the
collaborative coevolution approach with further experiments and testing using
real-world data sets.
References
1. D. E. Goldberg (1989). Genetic Algorithms in Search, Optimization and Machine
Learning. Addison-Wesley.
2. N. Hansen and A. Ostermeier (2001). Completely Derandomized Self-Adaptation in
Evolution Strategies. Evolutionary Computation, 9(2):159–195. MIT Press.
3. M. A. Potter and K. A. De Jong (1994). A Cooperative Coevolutionary Approach
to Function Optimization. In Proceedings of Third Conference on Parallel Problem
Solving from Nature, pages 249–257, Jerusalem, Israel. Springer-Verlag.
4. R. P. Weigand, W. C. Liles and K. A. De Jong (2001). An Empirical Analysis of
Collaboration Methods in Cooperative Coevolutionary Algorithms. In Proceedings
of the Genetic Evolutionary Computation Conference (GECCO-2001), pages 1235–
1242, San Fransisco, California, USA. Morgan Kaufmann.

Chromosome Reuse in Genetic Algorithms
Adnan Acan and Y¨uce Tekol
Computer Engineering Dept., Eastern Mediterranean University, Gazimagusa,
T.R.N.C. Mersin 10, TURKEY
adnan.acan@emu.edu.tr, ytekol@ieee.org
Abstract. This paper introduces a novel genetic algorithm strategy
based on the reuse of chromosomes from previous generations in the
creation of oﬀspring individuals. A number of chromosomes of above-
average quality, that are not utilized for recombination in the current
generation, are inserted into a library called the chromosome library.
The main motivation behind the chromosome reuse strategy is to trace
some of the untested search directions in the recombination of potentially
promising solutions. In the recombination process, chromosomes of cur-
rent population are combined with the ones in the chromosome library
to form a population from which oﬀspring individuals are to be created.
Chromosome library is partially updated at the end of each generation
and its size is limited by a maximum value. The proposed algorithm is
applied to the solution of hard numerical and combinatorial optimiza-
tion problems. It outperforms the conventional genetic algorithms in all
trials.
1
Introduction
Genetic algorithms (GA’s) are biologically inspired search procedures that have
been successfully used for the solution of hard numerical and combinatorial op-
timization problems. Since their introduction by John Holland in 1975, there
has been a great deal on the derivation of various algorithmic alternatives of
the standard implementation toward a faster and better localization of optimal
solutions. In all these eﬀorts, mechanisms of natural evolution developed over
millions of years have became the main source of inspiration. The power and
success of GA’s is mainly achieved by the diversity of individuals of a popula-
tion which evolve following the Darwinian principle of ”survival of the ﬁttest”.
In the standard implementation of GA’s, the diversity of individuals is achieved
using the genetic operators mutation and crossover which facilitate the search
for high quality solutions without being trapped into local optimal points [1],
[2], [3], [4].
In order to determine the most eﬃcient ways of using GA’s, many researchers
have carried out extensive studies to understand several aspects such as the role
and types of selection mechanism, types of chromosome representations, types
and application strategies of the genetic operators, memory-based approaches,
parallel implementations, and hybrid algorithms. In particular, several studies
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 695–705, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

696
A. Acan and Y. Tekol
were made concerning the development of problem-speciﬁc hybrids combining
genetic algorithms with other intelligent search methods, and it has been demon-
strated by thousands of applications that these approaches provide better results
than the conventional genetic algorithms on very diﬃcult problems [5], [6], [7],
[8].
Among many diﬀerent improvement eﬀorts, memory-based approaches have
also been studied and successfully applied for the solution diﬃcult problems.
Memory-based approaches aim to improve the learning performance of GAs by
reintroducing chromosomes of previous generations into the current population,
and their fundamental inspiration comes from redundancy within genetic mate-
rial in natural biology and intelligent search methods which use the experience-
based knowledge developed during the search to decide on the new search direc-
tions. In memory-based implementations, information stored within a memory
is used to adapt the GAs behavior either in problematic cases where the solution
quality is not improved over a number of iterations or to provide further direc-
tions of exploration and exploitation. Memory in GAs can be provided internally
(within the population) or externally (outside the population) [9].
The most common approaches using internal memory are polyploidy struc-
tures and polygenic inheritance. Polyploidy structures in combination with dom-
inance mechanisms use redundancy in genetic material by having more than one
copy of each gene. When a chromosome is decoded to determine the correspond-
ing phenotype, the dominant copy is chosen. By switching between copies of
genes, the GA can adapt faster to changing environments and recessive genes
are used to provide information about ﬁtness values from previous generations
[10], [11], [12]. Polygenic inheritance is based on the idea that a trait can depend
on more than one gene or gene pair. In this case, the more gene pairs involved
in the calculation of a trait, the more diﬃcult it is to distinguish between vari-
ous phenotypes. This is certainly a situation which smooths the evolution in a
variable environment [13], [14].
External memory implementations store speciﬁc information and reintroduce
that information into the population at a later moment. In most cases, this
means that individuals from memory are put into the initial population of a
new or restarted GA [15]. Case-based memory approaches, which is actually a
long term elitism, is the most typical form of external memory implemented in
practice. In general, there are two kinds of case-based memory implementations:
in one kind, case-based memory is used to re-seed the population with the best
individuals from previous generations when a change in the variable problem
domain takes place
[15]. A diﬀerent kind of case-based memory stores both
problems and solutions [16], [17]. When GA has to solve a problem similar to
problems in its case-based memory, it uses the stored solutions to seed the initial
population. Case-based memory aims to increase the diversity by reintroducing
individuals from previous generations and achieves exploitation by reintroducing
individuals from case-based memory when a restart from a good initial solution
is required.

Chromosome Reuse in Genetic Algorithms
697
This paper introduces a novel external memory-based genetic algorithms
strategy based on the reuse of chromosomes from previous generations in the
creation of oﬀspring individuals. At the end of each generation, a number of po-
tentially promising chromosomes, based on their ﬁtness values, are inserted into
a library, called the chromosome library. Basically, starting from any point in
the solution space, it is possible to form a path to an optimal solution over many
diﬀerent alternatives. Consequently, chromosome reuse aims to trace untested
possibilities in the recombination of potentially promising solutions. Those indi-
viduals having a ﬁtness value above a threshold, that are not used in the current
recombination process, are selected for insertion into the chromosome library.
During the recombination process, chromosomes of current population are com-
bined with the ones in the chromosome library to form a population from which
oﬀspring individuals are to be created. The size of the chromosome library is
limited by a maximum value and in case of excessive insertions, only the best
individuals within the limits are accepted. The proposed algorithm is applied to
the solution of hard numerical and combinatorial optimization problems. The
obtained results demonstrate the superiority of the proposed approach over the
conventional genetic algorithms.
The idea of reusing some chromosomes of previous generations, in the forma-
tion of oﬀspring individuals, arises from a well-known fact in intelligent search
algorithms: a search process has to make frequent backtracks or restarts to ﬁnd
a path to an optimal solution [18], [19]. This is because, an alternative search
direction that may not be seen attractive at some point, due to more promising
alternatives or due to many alternatives, may provide a link to an optimal solu-
tion with smaller number of computational steps. This idea is illustrated with a
simple example as follows:
Assume that we want to maximize the objective function f(x) = x2, x ∈
[0, 1], using 8-bit binary encoding. Certainly, f(x) takes its maximum value for
an individual p∗= 11111111. Now, consider the following individuals p1 =
00011111, p2 = 11100000, and p3 = 10000001. Due to ﬁtness-based selection
procedures, it is obvious that p2 and p3 will produce much more oﬀspring than
p1 for the next generation. In addition to that, the number of recombinations
between p2 and p3 will be greater than the ones between p1 and p2, and between
p1 and p3. However, as can be seen from the structures of p1 and p2, a one-point
crossover between the two from the position j = 4 will produce the optimal
solution. Hence, it is worth to store chromosomes like p1 for a while to give them a
chance for recombination with high quality individuals to provide a shorter path
to an optimal solution. It is also important to note that, the individuals like p1
which can be accessed from the chromosome library are computationally free
because their structure and ﬁtness values are known from previous generations.
As explained with the above particular examples, in the recombination of
two potential solutions, there are lots of possibilities and only a few of them are
randomly tried due to restrictions of ﬁtness-based selection procedures and the
population size. In fact, for a binary encoding of length l, two individuals can
be recombined in (l −1) diﬀerent ways using the 1-point crossover. The number

698
A. Acan and Y. Tekol
of oﬀspring that can be produced with 2-point crossover is l(l −1), whereas this
number using the uniform crossover is 2k, k ≤l, where k is the number of posi-
tions where the two parents are diﬀerent. Obviously, since the individuals of the
current generation are completely replaced by their oﬀspring, there is no way
to retry another recombination operation with these individuals unless they are
reproduced in future generations. In theoretical models of genetic algorithms,
the branching process in genetic evolutionary search is explained by the schema
theorem which is based on hyperplane sampling where the convergence process
is modelled by increasingly more frequent sampling from high ﬁtness individuals
by crossover and mutation acting as a background operator to prevent prema-
ture convergence. In this respect, the use of the chromosome library will help
the search process by providing additional intensiﬁcation and diversiﬁcation al-
ternatives, through potentially promising untried candidates, at all stages of the
search process. To clarify these points by experimental analysis, some statistical
results for ﬁtness-based selection behavior are given in section 2.
This paper is organized as follows. The statistical bases of chromosome reuse
idea are illustrated in section 2. Algorithmic description of GAs with chromo-
some reuse strategy is given in section 3. Section 4 covers the case studies for
numerical and combinatorial optimization problems. Finally, conclusions and
future research directions are speciﬁed in section 5.
2
Statistical Reasoning on the Chromosome Reuse
Strategy
The roulette-wheel and the tournament selection methods are the two most com-
monly used selection mechanisms in genetic algorithms. Both of these selection
methods are ﬁtness-based and they aim to produce more oﬀspring from those
high-ﬁtness individuals. However, these selection operators leave a signiﬁcant
number of individuals having close to average ﬁtness value unused in the sense
that these individuals don’t take part in any recombination operation. The idea
of chromosome reuse is based on the fact that a signiﬁcant percentage of these
unused individuals have above average ﬁtness values and they should not be just
wasted. On the one hand, their reuse will provide additional intensiﬁcation and
diversiﬁcation capabilities to the evolutionary search process. On the other hand,
the use of the individuals in the chromosome library brings no extra computa-
tional cost. This is because, the structure and ﬁtness values of these individuals
are already known. When these individuals are reused, it is possible to localize
an optimal solution over a shorter computational path as exempliﬁed in Section
1 and as demonstrated by experimental evaluations in Section 4.
In order to understand the above reasoning more clearly, let’s take the min-
imization problem for the Ackley’s function of 20 variables [20]. A genetic al-
gorithm with 200 individuals, uniform crossover with a crossover rate 0.7 and a
mutation rate 0.01 is considered. Since it is more commonly used, the tourna-
ment selection operator is selected for illustration. Statistical data are collected
over 1000 generations. First, the ratio of the unused individuals to population

Chromosome Reuse in Genetic Algorithms
699
size is shown in Figure 1. Obviously, on the average, 74% of the individuals in
every generation remain unused, they are simply discarded and replaced by the
newly produced oﬀspring individuals. This ratio of unused individuals is inde-
pendent of the encoding method used. That is, almost the same ratio is obtained
with binary-valued and real-valued encodings.
0
10
20
30
40
50
60
70
80
90
100
0.7
0.75
0.8
0.85
0.9
0.95
Generations
Ratio of Unused Individuals
Ratio of Individuals not Selected for Recombination: PopSize=200
Fig. 1. The ratio of individuals which are not selected in any recombination operation
for a population of 200 individuals.
The average ratio of individuals not selected for recombination changes with
the population size. For example, this average is 52% for 100 individuals and 85%
for 1000 individuals. In addition to this, these average ratios are approximately
the same for the roulette-wheel selection method also.
A more clear insight can be obtained from the ratio of unused individuals
having a ﬁtness value greater than the population’s average ﬁtness. As illus-
trated in Figure 2, on the average, 32% of the individuals having a ﬁtness value
above the population average are not used at all in any recombination opera-
tion. The main motivation behind the chromosome reuse strategy is to put these
close to average quality individuals into a chromosome library and make use of
them for a number of future generations. This way, possible alternative paths to
optimal solutions over these potentially promising solutions may be traced. In
these experimental evaluations, it is also seen that 24% of the individuals having
a ﬁtness value above 0.75 ∗Average Fitness are not selected for recombination
in all generations. Instead of totally wasting these potentially promising solu-
tions, we can reuse them for a while to speedup the convergence process and to
reduce the computational cost of constructing new individuals because chromo-
somes and ﬁtness values of the individuals in the chromosome library are already
determined.

700
A. Acan and Y. Tekol
0
10
20
30
40
50
60
70
80
90
100
0.28
0.3
0.32
0.34
0.36
0.38
0.4
Generations
Ratio of Unused Individuals
Ratio of Above−Average Individuals not Selected for Recombination: PopSize=200
Fig. 2. The ratio of individuals having above average ﬁtness and not selected in any
recombination operation for a population of 200 individuals.
3
GAs with Chromosome Reuse Strategy
GAs with chromosome reuse strategy diﬀers from the conventional GAs in the
formation and maintenance of a chromosome library and the union of its indi-
viduals with the current population during the recombination procedure. The
algorithmic description of the proposed approach is given in Figure 3.
In the proposed approach, the total memory space used to store individuals
does not increase compared to the memory space needed by conventional GAs,
because GAs with chromosome reuse strategy achieves better performance with
smaller size populations. In experimental studies, the total number of individuals
in the population and in the chromosome library is set equal to the number of
individuals in the population of conventional GAs implementation, with which
the proposed approach achieved better performance.
4
Two Case Studies
To study the performance of the described chromosome reuse strategy, it is com-
pared with the conventional GAs for the solution of some benchmark problems
from numerical and combinatorial optimization ﬁelds. Those benchmark numer-
ical optimization problems handled in evaluations are listed in Table 1. They
are taken from
[20] and
[21], which are claimed to provide reasonable test
cases for the necessary combination of path-oriented and volume-oriented char-
acteristics of a search strategy. For the combinatorial optimization problems, the
100-city symmetric traveling salesman problem, kroA100, taken from the web-
site http://www.iwr.uni-heidelberg.de/ groups/ comopt/ software/ TSPLIB95/
tsp/ is taken as a representative problem instance.
In all experiments, real-valued chromosomes are used for problem represen-
tations. The selection method used is the tournament selection with elitism.

Chromosome Reuse in Genetic Algorithms
701
1. Max Library Size = α ∗Population Size, 0 < α < 1.0;
2. Fitness Treshold = β, 0 < β < 1.0;
3. Life Time = K, where K is a predeﬁned integer constant;
4. Generate chromosome library with randomly generated individuals;
5. Set the life time of individuals in the chromosome library to Life Time;
6. Evaluate chromosome library;
7. Generate initial population;
8. Evaluate initial population;
9. While (NOT DONE)
10.
Combine the individuals in the current population and the
chromosome library;
11.
Reproduction;
12.
Crossover;
13.
Mutation;
14.
Evaluate new population;
15.
Decrease the life time of individuals in the chromosome library by 1;
16.
Update chromosome library with individuals having
Fitness V alues > β ∗Average Fitness and not used in any
recombination operation.
17. end
Fig. 3. Genetic algorithms with chromosome reuse strategy.
The elite size is 10% of the population size. The uniform crossover operator
is employed with a crossover rate equal to 0.7 and the mutation rate is 0.01.
Experiments are carried out using a population size of 200 individuals for con-
ventional GAs, also the total number of individuals in the population and the
chromosome library for the proposed approach is 200, i.e. 100 individuals in
each. This way, total number of individuals in conventional GAs and GAs with
chromosome reuse strategy are kept the same. Individuals in the chromosome
library have a predeﬁned life duration, taken as 5 iterations in the experiments,
and the removal of an individual from the chromosome library occurs either at
the end its life time or the chromosome library is full and an individual with a
better ﬁtness replaces it. Each experiment is performed 10 times. All the tests
were run over 1000 generations.
In the following worked examples, the results obtained with conventional
GA and GAs with the chromosome reuse strategy are compared for relative
performance evaluations.
4.1
Performance of Chromosome Reuse Strategy in Numerical
Optimization
Conventional GAs are compared with GAs with chromosome reuse strategy for
the minimization of functions listed in Table 1. Each function has 20 variables.
The best solution found using the conventional GAs and GAS with chromosome
reuse strategy are given in Table 2. Chromosome reuse strategy provided very

702
A. Acan and Y. Tekol
Table 1. Bechmark functions considered for numerical optimization.
Function Name
Expression
Michalewicz
f(x) = −n−1
i=1 sin(xi)sin(
sin(ix2
i )
π
)(2m)
−n−1
i=1 sin(xi+1)sin(
2x2
i+1
π
)(2m)
0 ≤xi ≤π
Griewangk
f(x) = 1 + n
i=1
x2
i
4000 −n
i=1 cos( xi
√xi )
−100 ≤xi ≤100
Rastrigin
f(x) = 10n + n
i=1(x2
i −10cos(2πxi))
−5.12 ≤xi ≤5.12
Schwefel
f(x) = n
i=1(−xisin(

|xi|))
−512 ≤xi ≤512
Ackley’s
f(x) = −ae−b√1
n
n
i=1 x2
i −e
1
n
n
i=1 cos(cxi) + a + e
a = 20, b = 0.2, c = 2π
−32.768 ≤xi ≤32.768
De Jong (Step)
f(x) = 6n + n
i=1⌊xi⌋
−5.12 ≤xi ≤5.12
close to optimal results in all trials. These results demonstrate the success of the
implemented GAs strategy for the numerical optimization problems.
Table 2. Performance evalution of conventional GAs and GAs with chromosome reuse
for numerical optimization.
Function
Global Opt.,
Best Found: Conv. GA Best Found: Proposed
n=Num. Vars.
Global Min.
ITER
Global Min.
ITER
Michalewicz
-9.66 ,n = m = 10
-8.55
100
-9.36
100
Griewangk
0, n = 20
0.0001
85
1.0e−8
35
Rastrigin
0, n = 20
0.1
100
0.001
100
Schwefel
−n ∗418.9829, n = 20
-8159
100
-8374
100
Ackley’s
0, n = 20
0.03
100
0.001
100
De Jong (Step)
0, n=20
3
100
0
77
4.2
Performance of Chromosome Reuse Strategy in Combinatorial
Optimization
To test the performance of the chromosome reuse strategy over a diﬃcult prob-
lem of combinatorial type, the 100-city TSP kroA100 is selected. The best found
solution for this problem is 21282 obtained using a branch-and-bound algorithm.
In the ten experiments performed, the best solution found for this problem us-
ing the conventional GAs is 21340 which is obtained in 1000 generations with

Chromosome Reuse in Genetic Algorithms
703
population size equal to 200. The best solution obtained with the chromosome
reuse strategy is 21282 which is obtained after 620 generations. Figure 4 shows
the relative performance of chromosome reuse approach compared to the con-
ventional GAs implementation, the straight line plot shows the results for the
chromosome reuse strategy.
0
100
200
300
400
500
600
700
800
900
1000
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9 x 104
Generations
Average Fitness
Conventional GAs vs. the proposed approach for TSP
Fig. 4. Performance comparison of conventional genetic algorithms and chromosome
reuse strategy in combinatorial
5
Conclusions and Future Work
In this paper a novel external memory-based genetic algorithms strategy based
on the reuse of some potentially promising solutions from previous generations
for the production of current oﬀspring individuals is introduced as an alternative
to the conventional implementation of GAs. The implemented strategy is used
to solve diﬃcult problems from numerical and combinatorial optimization areas
and its performance is compared with the conventional GAs for representative
problem instances. Each problem is solved exactly the same number of times
with the employed strategies and the best and the average ﬁtness results are
analyzed for performance comparisons. All GA parameters are kept the same in
the comparison of the two approaches.
From the results of case studies, for the same population size, it is concluded
that the chromosome reuse strategy outperforms the conventional implementa-
tion in all trials. The performance of the chromosome reuse approach is the same
for both numerical and combinatorial optimization problems. In fact, problems
from these classes are purposely chosen to examine this side of the proposed
strategy.

704
A. Acan and Y. Tekol
This work requires further investigation from following point of views: perfor-
mance comparisons with other memory-based methods, performance evaluations
for other problem classes, such as neural network design, speech processing, and
face recognition; problem representations involving variable size chromosomes,
particularly genetic programming; and mathematical analysis of chromosome
reuse strategy.
References
1. Holland, J.H.: Adaptation in Natural and Artiﬁcial Systems: An introductory Anal-
ysis with Applications to Biology, Control, and Artiﬁcial Intelligence, MIT Press,
(1992).
2. Goldberg, D.E.: Genetic Algorithms in Search, Optimization, and Machine Learn-
ing, Addison-Wesley Publishing Company, (1989).
3. Eshelman, L., Schaﬀer, J.: Foundations of Genetic Algorithms 2. In: L. Whitley
(editor): pp. 187–202, Morgan Kaufmann Publishers, San Mateo, CA, (1993).
4. Back, T.: Evolutionary Algorithms in Theory and Practice, Oxford University
Press, (1996).
5. Gen, M., Runwei, C.: Genetic Algorithms in Engineering Design, John Wiley &
Sons. Inc., (1997).
6. Miettinen, K., Neitaanmaki, P., Makela, M.M., Periaux, J.: Evolutionary Algo-
rithms in Engineering and Computer Science, John Wiley & Sons Ltd., (1999).
7. Cantu-Paz, E., Mejia-Olvera, M.: Designing eﬃcient master-slave parallel genetic
algorithms, IlliGAL Report No. 97004, Illinois Genetic Algorithm Laboratory, Ur-
bana, IL, (1997).
8. Whitley, D., Starkweather, T.: GenitorII: A distributed genetic algorithm, Journal
of Experimental and Theoretical Artiﬁcial Intelligence, (1990).
9. Eggermont, J., Lenaerts, T.: Non-stationary function optimization using evolution-
ary algorithms with a case-based memory,
url:http://citeseer.nj.nec.com/484021.html.
10. Goldberg, D. E., Smith, R. E.: Non-stationary function optimization using genetic
algorithms and with dominance and diploidy, Genetic Algorithms and their Applca-
tions: Proceedings of the Second International Conference on Genetic Algorithms,
p. 217–223, (1987).
11. Goldberg, D. E., Deb, K., Korb, B.: Messy Genetic Algorithms: Motivation, anal-
ysis, and the ﬁrst results, Complex Systems, Vol. 3, No. 5, p. 493–530, (1989).
12. Lewis, J., Hart, E., Ritchie, G.: A comparison of dominance mechanisms and simple
mutation on non-stationary problems, in Eiben, A. E., Back, T., Schoenauer, M.,
Schwefel, H. (Editors): Parallel Problem Solving from Nature- PPSN V, p. 139–148,
Berlin, (1998).
13. Ryan, C., Collins, J. J.: Polygenic inheritance- a haploid scheme that can outper-
form diploidy, in Eiben, A. E., Back, T., Schoenauer, M., Schwefel, H. (Editors):
Parallel Problem Solving from Nature- PPSN V, p. 178–187, Berlin, (1998)
14. Ryan, C.: The degree of oneness, Firts Online Workshop on Soft Computing, Aug.
19–30, (1996).
15. Ramsey, C.L., Grefenstette, J. J.: Case-based initialization of GAs, in Forest, S.,
(Editor): Proceedings of the Fifth International Conference on Genetic Algorithms,
p. 84–91, San Mateo, CA, (1993).

Chromosome Reuse in Genetic Algorithms
705
16. Louis, S., Li, G.: Augmenting genetic algorithms with memory to solve travelling
salesman problem, (1997).
17. Louis, S. J., Johnson, J.: Solving similar problems using genetic algorithms and
case-based memory, in Back, T., (Editor):Proceedings of the Seventh International
Conference on Genetic Algorithms, p. 84–91, San Fransisco, CA, (1997).
18. Luger, G.F.: Artiﬁcial Intelligence, 4th edition, Addison-Wesley, (2002).
19. S. Russel and P. Norvig, Artiﬁcial Intelligence: A Modern Approach, Prentice-Hall,
(1995).
20. http://www.f.utb.cz/people/zelinka/soma/func.html.
21. Kim, H.S., Cho, S.B: An Eﬃcient genetic algorithm with less ﬁtness valuations
by clustering, Proc. of the 2001 IEEE Congress on Evolutionary Computation,
p.887–894, Seoul, Korea, May 27-30, (2001).

An Adaptive Penalty Scheme for Steady-State
Genetic Algorithms
Helio J.C. Barbosa1 and Afonso C.C. Lemonge2
1 LNCC/MCT
Rua Getulio Vargas 333
25651 070 Petropolis RJ, BRAZIL
hcbm@lncc.br
2 Depto. de Estruturas, Faculdade de Engenharia
Universidade Federal de Juiz de Fora
36036 330 Juiz de Fora MG, BRAZIL
lemonge@numec.ufjf.br
Abstract. A parameter-less adaptive penalty scheme for steady-state
genetic algorithms applied to constrained optimization problems is pro-
posed. For each constraint, a penalty parameter is adaptively computed
along the run according to information extracted from the current pop-
ulation such as the existence of feasible individuals and the level of vi-
olation of each constraint. Using real coding, rank-based selection, and
operators available in the literature, very good results are obtained.
1
Introduction
Evolutionary algorithms (EAs) are weak search algorithms which can be directly
applied to unconstrained optimization problems where one seeks for an element
x belonging to the search space S, which minimizes (or maximizes) the real
function f. Such EAs usually employ a ﬁtness function closely related to f.
The straightforward application of EAs to constrained optimization problems
(COPs) is not possible due to the additional requirement that a set of constraints
must be satisﬁed. Several diﬃculties may arise: (i)the objective function may be
undeﬁned for some or all infeasible elements, (ii)the check for feasibility can be
more expensive than the computation of the objective function value, and (iii)an
informative measure of the degree of infeasibility of a given candidate solution
is not easily deﬁned. It is easy to see that even if both the objective function
f(x) and a measure of constraint violation v(x) are deﬁned for all x ∈S it is not
possible to know in general which of two given infeasible solutions is closer to the
optimum and thus should be operated upon or kept in the population. For min-
imization problems, for instance, one can have f(x1) > f(x2) and v(x1) = v(x2)
or f(x1) = f(x2) and v(x1) > v(x2) and still have x1 closer to the optimum.
It is also important to note that –for convenience and easier reproducibility–
most comparisons between EAs in the literature have been conducted in prob-
lems with constraints which can be written as gi(x) ≤0, where each gi(x) is
a given explicit function of the independent(design) variable x ∈IRn. Although
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 718–729, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

An Adaptive Penalty Scheme for Steady-State Genetic Algorithms
719
the available test problems attempt to represent diﬀerent types of diﬃculties
one is expected to encounter when dealing with practical situations, very often
the constraints cannot be put explicitly in the form gi(x) ≤0. For instance, in
structural engineering design most constraints (such as stress and deformation)
are only known as implicit functions of the design variables. In order to check
if a constraint has been violated, a whole computational simulation (carried out
by a speciﬁc code expending considerable computational resources) is required.
The techniques for handling constraints within EAs can be classiﬁed either
as direct (feasible or interior), when only feasible elements in S are considered or
as indirect (exterior), when both feasible and infeasible elements are used during
the search process. Direct techniques comprise the use of: a) closed genetic op-
erators (in the sense that when applied to feasible parents they produce feasible
oﬀspring) which can be designed provided enough domain knowledge is available
[1], b) special decoders [2] (which always generate feasible individuals from any
given genotype) although no applications considering implicit constraints have
been published, c) repair techniques [3,4] which use domain knowledge in order
to move an infeasible oﬀspring into the feasible set (a challenge when implicit
constraints are present), and d) “the death penalty”, when any infeasible element
is simply discarded irrespective of its potential information content.
Summarizing, direct techniques are problem dependent (with the exception
of the “death penalty”) and actually of extremely reduced practical applicability.
Indirect techniques comprise the use of: a) Lagrange multipliers [5], which
may also lead to a min-max problem deﬁned for the associated Lagrangean
L(x, λ) where the primal variables x and the multipliers λ are approximated
by two diﬀerent populations in a coevolutionary GA [6], b) ﬁtness as well as
constraint violation values in a multi-objective optimization setting [7], c) special
selection techniques [8], and d) “lethalization”: any infeasible oﬀspring is just
assigned a given, very low, ﬁtness value [9]. For other methods proposed in the
evolutionary computation literature see [1,10,11,12,13] and references therein.
Methods to tackle COPs which require the knowledge of constraints in ex-
plicit form have thus limited practical applicability. This fact, together with sim-
plicity of implementation are perhaps the main reasons why penalty techniques,
in spite of their shortcomings, are the most popular ones.
In a previous paper [14] a penalty scheme which does not require the knowl-
edge of the explicit form of the constraints as a function of the decision/design
variables and is free of parameters to be set by the user was developed. In con-
trast with previous approaches where a single penalty parameter is used for all
constraints, an adaptive scheme automatically sizes the penalty parameter corre-
sponding to each constraint along the evolutionary process. However, the method
was conceived for a generational genetic algorithm (GA), where the ﬁtness of the
whole population is computed at each generation.
In this paper, the procedure proposed in [14] is extended to the case of
a steady-state GA where, in each “generation”, usually only one or two (in
general just a few) new individuals are introduced in the population. Substantial

720
H.J.C. Barbosa and A.C.C. Lemonge
modiﬁcations were necessary in order to ﬁnally obtain a robust procedure capable
of reaching very good results in a standard test-problem suite.
In the next section the penalty method and some of its implementations
within EAs are presented. In Section 3 the proposed adaptive scheme for steady-
state GAs is discussed, Section 4 presents numerical experiments with several
test-problems from the literature and the paper closes with some conclusions.
2
Penalty Methods
A standard COP in Rn can be thought of as the minimization of a given objective
function f(x), where x ∈Rn is the vector of design/decision variables, subject
to inequality constraints gp(x) ≥0, p = 1, 2, . . . , ¯p as well as equality constraints
hq(x) = 0, q = 1, 2, . . . , ¯q. Additionally, the variables may be subject to bounds
xL
i ≤xi ≤xU
i but this type of constraint is trivially enforced in a GA and need
not be considered here.
Penalty techniques can be classiﬁed as multiplicative or additive. In the mul-
tiplicative case [15], a positive penalty factor p(v(x), T) is introduced in order to
amplify the value of the ﬁtness function of an infeasible individual in a minimiza-
tion problem. One would have p(v(x), T) = 1 for a feasible candidate solution x
and p(v(x), T) > 1 otherwise. Also, p(v(x), T) increases with the “temperature”
T and with constraint violation. An initial value for the temperature is required
as well as the deﬁnition of a function such that T grows with the generation
number. This type of penalty has received much less attention in the evolution-
ary computation (EC) community than the additive type. In the additive case,
a penalty functional is added to the objective function in order to deﬁne the ﬁt-
ness value of an infeasible element. They can be further divided into: (a)interior
techniques1 and (b)exterior techniques, where a penalty functional is introduced
F(x) = f(x) + kP(x)
(1)
such that P(x) = 0 if x is feasible and P(x) > 0 otherwise (for minimization
problems). In both cases, as k →∞, the sequence of minimizers of the uncon-
strained problem converges to the solution of the original constrained one.
Deﬁning the amount of violation of the j-th constraint by the candidate
solution x ∈Rn as
vj(x) =

|hj(x)|,
for an equality constraint,
max{0, −gj(x)} otherwise
it is common to design penalty functions that grow with the vector of violations
v(x) ∈Rm where m = ¯p + ¯q is the number of constraints to be penalized. The
most popular penalty function is given by
P(x) =
m

j=1
(vj(x))β
(2)
1 When a barrier functional, which grows rapidly as x approaches the boundary of the
feasible domain, is added to the objective function.

An Adaptive Penalty Scheme for Steady-State Genetic Algorithms
721
where β = 2. Although it is easy to obtain the unconstrained problem, the
deﬁnition of a good penalty parameter k is usually a time-consuming trial-and-
error process.
Powell & Skolnick[16] proposed a method enforcing the superiority of any
feasible solution over any infeasible one deﬁning the ﬁtness as
F(x) = f(x) + r
m

j=1
vj(x) + θ(t, x)
where θ(t, x) is conveniently deﬁned and r is a constant.
A variant, (see Deb[17]) uses the ﬁtness function:
F(x) =
f(x),
if x is feasible,
fmax + m
j=1 vj(x), otherwise
where fmax is the objective function value of the worst feasible solution.
Besides the widely used case of a single constant penalty parameter k, several
other proposals are available [18,10,19] and some of them, more closely related
to the work presented here, will be brieﬂy discussed in the following.
2.1
Related Methods in the Literature
Two-level Penalties. Le Riche et al.[20] present a GA where two ﬁxed penalty
parameters k1 and k2 are used independently in two diﬀerent populations. The
idea is to create two sets of candidate solutions where one of them is evaluated
with the parameter k1 and the other with the parameter k2. With k1 ≫k2 there
are two diﬀerent levels of penalization and there is a higher chance of maintaining
feasible as well as infeasible individuals in the population and to get oﬀspring
near the boundary between the feasible and infeasible regions.
Multiple Coeﬃcients. Homaifar et al.[21] proposed diﬀerent penalty coeﬃ-
cients for diﬀerent levels of violation of each constraint. The ﬁtness function is
written as
F(x) = f(x) +
m

j=1
kij(vj(x))2
where i denotes one of the l levels of violation deﬁned for the j−th constraint.
This is an attractive strategy because, at least in principle, it allows for a good
control of the penalization process. The weakness of this method is the large
number, m(2l +1), of parameters that must be set by the user for each problem.
Dynamic Coeﬃcients. Joines & Houck[22] proposed that the penalty pa-
rameters should vary dynamically along the search according to an exogenous
schedule. The ﬁtness function F(x) was written as in (1) and (2) with the penalty
parameter, given by k = (C × t)α, increasing with the generation number t.

722
H.J.C. Barbosa and A.C.C. Lemonge
Adaptive Penalties. A procedure where the penalty parameters change ac-
cording to information gathered during the evolution process was proposed by
Bean & Hadj-Alouane[23]. The ﬁtness function is again given by (1) and (2) but
with the penalty parameter k = λ(t) adapted at each generation by the rules:
λ(t + 1) =



( 1
β1 )λ(t), if bi ∈F for all t −g + 1 ≤i ≤t
β2λ(t),
if bi ̸∈F for all t −g + 1 ≤i ≤t
λ(t)
otherwise
where bi is the best element at generation i, F is the feasible set, β1 ̸= β2 and
β1, β2 > 1. In this method the penalty parameter of the next generation λ(t+1)
decreases when all best elements in the last g generations were feasible, increases
if all best elements were infeasible and otherwise remains without change.
The method proposed by Coit et al.[24], uses the ﬁtness function:
F(x) = f(x) + (Ffeas(t) −Fall(t))
m

j=1
(vj(x)/vj(t))α
where Fall(t) corresponds to the best solution, until the generation t (without
penalty), Ffeas corresponds to the best feasible solution and α is a constant.
Schoenauer & Xanthakis[25] presented a strategy that handles constrained
problems in stages: (i) initially, a randomly generated population is evolved
considering only the ﬁrst constraint until a certain percentage of the population is
feasible with respect to that constraint; (ii) the ﬁnal population of the ﬁrst stage
of the process is used in order to optimize with respect to the second constraint.
During this stage, the elements that had violated the previous constraint are
removed from the population, (iii) the process is repeated until all the constraints
are processed. This strategy becomes less attractive as the number of constraints
grows and is potentially dependent on the order in which the constraints are
processed.
Recently, Hamida & Schoenauer[26] proposed an adaptive scheme using a
niching technique with adaptive radius to handle multimodal functions.
Other Techniques. Runarsson & Yao[8] presented a novel approach where a
good balance between the objective and the penalty function values is sought by
means of a stochastic ranking scheme. However, there is a parameter, Pf, (the
probability of using only the objective function for ranking infeasible individuals)
that must be set by the user.
Later, Wright & Farmani[27] proposed a method that requires no parameters
and aggregates all constraint violations in a single infeasibility measure.
For constraint satisfaction problems, adaptive EAs have been developed suc-
cesfuly by Eiben and co-workers (see [28]).
3
The Proposed Method
In a previous paper[14] a penalty scheme was proposed which adaptively sizes
the penalty coeﬃcient of each constraint using information from the population

An Adaptive Penalty Scheme for Steady-State Genetic Algorithms
723
such as the average of the objective function and the level of violation of each
constraint. The ﬁtness function was written as:
F(x) =
f(x),
if x is feasible,
h(x) + m
j=1 kjvj(x) otherwise
(3)
where
h(x) =

f(x), if f(x) > ⟨f(x)⟩,
⟨f(x)⟩otherwise
(4)
and ⟨f(x)⟩is the average of the objective function values in the current popula-
tion. The penalty parameter was deﬁned at each generation by:
kj = |⟨f(x)⟩|
⟨vj(x)⟩
m
l=1[⟨vl(x)⟩]2
(5)
and ⟨vl(x)⟩is the violation of the l-th constraint averaged over the current
population. The idea is that the penalty coeﬃcients should be distributed in
such a way that those constraints which are more diﬃcult to be satisﬁed should
have a relatively higher penalty coeﬃcient. It is also clear that the notion of the
superiority of any feasible over any infeasible solution[16] is not enforced here.
It must be observed that in all procedures where a penalty coeﬃcient varies
along the run one must ensure that the ﬁtness value of all elements is computed
with the same penalty coeﬃcient(s) so that standard selection schemes remain
valid. For a generational GA, one can simply update the coeﬃcient(s) every, say
g, generations. As the concept of generation does not hold for a steady-state GA
extra care must be taken in order to ensure that selection (for reproduction as
well as for replacement) works properly.
A straightforward extension of that penalty procedure[14] to the steady-state
case would be to periodically update the penalty coeﬃcients and the ﬁtness
function values for the population. However, in spite of using a real-coding, the
results obtained were inferior to those of the binary-coded generational case[14].
Further modiﬁcations are then proposed here for the steady-state version of
that penalty scheme. The ﬁtness function is still computed according to (3).
However, h and the penalty coeﬃcients are redeﬁned respectively as
h =

f(xworst)
if there is no feasible element in the population,
f(xbestfeasible) otherwise
(6)
kj = h
⟨vj(x)⟩
m
l=1[⟨vl(x)⟩]2
(7)
Also, every time a better feasible element is found (or the number of new elements
inserted into the population reaches a certain level) h is redeﬁned and all ﬁtness
values are recomputed using the updated penalty coeﬃcients. The updating of
each penalty coeﬃcient is performed in such a way that no reduction in its value
is allowed. For convenience one should keep, for each individual in the population,
the objective function value and all constraint violations . The ﬁtness function
value is then computed using (6), (7), and (3).

724
H.J.C. Barbosa and A.C.C. Lemonge
It is clear from the deﬁnition of h in (6) that if no feasible element is present
in the population one is actually minimizing a measure of the distance of the
individuals to the feasible set since the actual value of the objective function
is not taken into account. However, when a feasible element is found then it
immediately enters the population since, after updating all ﬁtness values using
(6), (7), and (3), it becomes the element with the best ﬁtness value.
A pseudo-code for the proposed adaptive penalty scheme for a steady-state
GA can be written as shown in Figure 1. Numerical experiments are then pre-
sented in the following section.
Begin
Initialize population
Compute objective function and constraint violation values
if there is no feasible element then
h = worst objective function value
else
h = objective function value of best feasible individual
endif
Compute penalty coefficients
Compute fitness values
ninser = 0
repeat
Select operator
Select parent(s)
Generate offspring
Evaluate offspring
Keep best offspring
if offspring is the new best feasible element then
update penalty coefficients and fitness values
ninser = 0
endif
if offspring is better than the worst in the population then
worst is removed
offspring is inserted
ninser = ninser + 1
endif
if (ninser/popsize >= r) then
update penalty coefficients and fitness values
ninser = 0
endif
until maximum number of evaluations is reached
End
Fig. 1. Pseudo-code for the steady-state GA with adaptive penalty scheme. (ninser
is a counter for the number of oﬀspring inserted in the population, popsize is the
population size and r is a ﬁxed constant that was set to 3 in all cases)

An Adaptive Penalty Scheme for Steady-State Genetic Algorithms
725
4
Numerical Experiments
In order to investigate the performance of the proposed penalty procedure, the
11 well known G1-G11 test-functions presented by Koziel & Michalewicz[2] are
considered. The G-Suite is made up of diﬀerent kinds of functions and involves
constraints given by linear inequalities, nonlinear equalities, and nonlinear in-
equalities. An extended discussion involving each one of these problems and other
techniques from the evolutionary computation literature can be found in [29].
A simple real-coded steady-state GA with a linear ranking selection scheme
was implemented. The operators used were: (i) random mutation (which modiﬁes
a randomly chosen variable of the selected parent to a random value uniformly
distributed between the lower and upper bounds of the corresponding variable),
(ii) non-uniform mutation (as proposed by Michalewicz[30]), (iii) Muhlenbein’s
mutation (as described in [31]), (iv) multi-parent discrete crossover (which gen-
erates an oﬀspring by randomly taking each allele from one of the np selected
parents), and (v) Deb’s SBX crossover as described in [32].
No parameter tuning was attempted. The same probability of application
(namely 0.2) was assigned to all operators above, np was set to 4, and η was
set to 2 in SBX. This set of values was applied to all test-problems in order
to demonstrate the robustness of the procedure. Each equality constraint was
converted into one inequality constraint of the form |h(x)| ≤0.0001. Enlarging
the set of operators, changing the relative probabilities of application, population
size, or parameters associated with operators in each case could of course lead
to local performance gains.
The Tables 1, 2, 3, and 4 show the results obtained for the G1-G11 test-
functions, in 20 independent runs, using a population containing 800 individuals
and a maximum number of function evaluations neval set to 320000, 640000,
1120000, and 1440000, respectively. It is clear that good results were found for
all test-functions and at all levels of neval.
The Table 5 displays a comparison of results found in the Experiment 3
(Table 3) –where neval = 1120000– and the results found in the Experiment #2
of [14] where a generational binary-coded GA –with popsize = 70 and neval =
1400000– was used in 20 independent runs.
The Table 6 compares the results from Experiment 3 with those presented by
Hamida & Shoenauer[26] using a (100 + 300)–ES segregational selection scheme
with an adaptive penalty and a niching strategy. They performed 31 independent
runs comprising 5000 generations (neval = 1500000) each.
The Tables 5 and 6 show that better results are obtained with the proposed
adaptive steady-state GA using less function evaluations. The interested reader
can ﬁnd additional results in [2,33,27,29], and verify that they are not superior
to those presented here.
Finally, in Table 7 we compare the results obtained with the parameter-less
scheme proposed here, using popsize = 700, with those of Runarsson & Yao[8],
both with neval = 350000. It must be observed that the results in Table 7 are the
best in [8] (and probably the best in the evolutionary computation literature)
and correspond to the choice Pf = 0.45. However, one can see in [8] that slightly

726
H.J.C. Barbosa and A.C.C. Lemonge
Table 1. Exp. 1: neval = 320000.
f(x)
Experiment 1
worst
best
average
G1
−15.00
−15.00
−15.00
G2 0.7701039 0.7980134 0.7894922
G3 0.7468729 0.9970834 0.8733876
G4 −30665.54 −30665.54 −30665.54
G5
5667.431
5126.484
5829.603
G6 −6961.811 −6961.811 −6961.811
G7
27.05797
24.31103
24.86856
G8 0.0958250 0.0958250 0.0958250
G9
680.7184
680.6303
680.64824
G10 10864.27
7139.031 7679.41880
G11
0.749
0.749
0.74899
Table 2. Exp. 2: neval = 640000.
f(x)
Experiment 2
worst
best
average
G1
−13.00
−15.00
−14.90
G2 0.7624246 0.8036177 0.7904785
G3 0.9318285
1.000491
0.9890722
G4 −30665.54 −30665.54 −30665.54
G5
5632.585
5126.484
5257.531
G6 −6961.811 −6961.811 −6961.811
G7
25.77410
24.32803
24.70925
G8 0.0958250 0.0958250 0.0958250
G9
680.6932
680.6305
680.6385
G10 7786.534
7098.464
7413.0185
G11
0.749
0.749
0.74899
Table 3. Exp. 3: neval = 1120000.
f(x)
Experiment 3
worst
best
average
G1
−15.00
−15.00
−15.00
G2 0.7778333 0.8036125 0.7900538
G3 0.9593665
1.000498
0.9981693
G4 −30665.54 −30665.54 −30665.54
G5
5639.265
5126.484
5205.561
G6 −6961.811 −6961.811 −6961.811
G7
25.24219
24.31465
24.58272
G8 0.0958250 0.0958250 0.0958250
G9
680.6494
680.6301
680.6333
G10 8361.596
7049.360
7339.957
G11
0.749
0.749
0.74899
Table 4. Exp. 4: neval = 1440000.
f(x)
Experiment 4
worst
best
average
G1
−15.00
−15.00
−15.00
G2 0.7778334 0.8036024 0.7908203
G3
1.000340
1.000499
1.000460
G4 −30665.54 −30665.54 −30665.54
G5
5672.701
5126.484
5206.389
G6 −6961.811 −6961.811 −6961.811
G7
25.51170
24.30771
24.52875
G8 0.0958250 0.0958250 0.0958250
G9
680.7122
680.6301
680.6363
G10 7942.683
7072.100
7300.013
G11
0.749
0.749
0.74899
Table 5. Results from this study (SSGA) and the generational GA (GGA) of [14].
f(x)
optimum
best values
average values
worst values
SSGA
GGA
SSGA
GGA
SSGA
GGA
G1
−15.0
−15.00
−15.00
−15.00
−15.00
−15.00
−15.00
G2
0.803619
0.8036125 0.7918570
0.7900538 0.7514353
0.7778333 0.6499022
G3
1.0
1.000498
1.000307
0.9981693 0.9997680
0.9593665 0.9983935
G4 −30655.539 −30665.54 −30665.51 −30665.54 −30665.29 −30665.54 −30664.91
G5
5126.4981
5126.484
5126.571
5205.561
5389.347
5639.265
6040.595
G6
−6961.814 −6961.811 −6961.796 −6961.811 −6961.796 −6961.811 −6961.796
G7
24.306
24.31465
24.85224
24.58272
27.90973
25.24219
33.07581
G8
0.0958250
0.0958250 0.0958250
0.0958250 0.0942582
0.0958250 0.0795763
G9
680.630
680.6301
680.6678
680.6333
680.9640
680.6494
681.6396
G10
7049.33
7049.360
7080.107
7339.957
8018.938
8361.596
9977.767
G11
0.75
0.749
0.75
0.74899
0.75
0.749
0.75

An Adaptive Penalty Scheme for Steady-State Genetic Algorithms
727
changing that parameter to Pf = 0.475 produces changes in the second most
relevant digit of the best values found for functions G6 and G10, and severely
degrades the mean value for functions G1, G6 and G10. It is clear that our ﬁrst
results presented in this paper are very competitive.
Table 6. Comparison between this study (SSGA) and Hamida & Schoenauer[26].
Average values for this study were computed with feasible and infeasible ﬁnal solutions.
Those in [26] considered only feasible solutions. Worst values were not given in [26].
f(x)
optimum
best values
average values
SSGA
H&S
SSGA
H&S
G1
−15.0
−15.00
−15.00
−15.00
−14.84
G2
0.803619
0.8036125
0.785
0.7900538
0.59
G3
1.0
1.000498
1.0
0.9981693
0.99989
G4 −30655.539 −30665.54 −30665.5 −30665.54 −30665.5
G5
5126.4981
5126.484
5126.5
5205.561
5141.65
G6
−6961.814 −6961.811 −6961.81 −6961.811 −6961.81
G7
24.306
24.31465
24.3323
24.58272
24.6636
G8
0.0958250
0.0958250 0.095825
0.0958250 0.095825
G9
680.630
680.6301
680.630
680.6333
680.641
G10
7049.33
7049.360
7061.13
7339.957
7497.434
G11
0.75
0.749
0.75
0.74899
0.75
Table 7. Comparison of results between this study (SSGA) and Runarsson & Yao[8].
f(x)
optimum
best values
worst values
SSGA
R&Y
SSGA
R&Y
G1
−15.0
−15.00
−15.00
−15.00
−15.00
G2
0.803619
0.8035839
0.803515
0.7777818
0.726288
G3
1.0
0.9960645
1.0
0.6716288
1.00
G4 −30655.539 −30665.54 −30665.539 −30644.32 −30665.539
G5
5126.4981
5126.484
5126.497
5624.208
5142.472
G6
−6961.814 −6961.811 −6961.814 −6961.811 −6350.262
G7
24.306
24.32190
24.307
29.82257
24.642
G8
0.0958250
0.0958250
0.095825
0.0958250
0.095825
G9
680.630
680.6304
680.630
680.6886
680.763
G10
7049.33
7102.265
7054.316
7229.3908
8835.655
G11
0.75
0.749
0.75
0.749
0.75
5
Conclusions
A new adaptive parameter-less penalty scheme which is suitable for implementa-
tion within steady-state genetic algorithms has been proposed in order to tackle
constrained optimization problems. Its main feature, besides being adaptive and
not requiring any parameter, is to automatically deﬁne a diﬀerent penalty coeﬃ-
cient for each constraint. The scheme was introduced in a real-coded steady-state

728
H.J.C. Barbosa and A.C.C. Lemonge
GA and, using available operators from the literature, produced results compet-
itive with the best available in the EC literature, besides alleviating the user
from the delicate and time consuming task of setting penalty parameters.
Acknowledgements. The authors acknowledge the support received from
CNPq and FAPEMIG. The authors would also like to thank the reviewers for
the corrections and suggestions which helped improve the quality of the paper.
References
1. M. Schoenauer and Z. Michalewicz.
Evolutionary computation at the edge of
feasibility.
In Parallel Problem Solving from Nature - PPSN IV, volume 1141,
pages 245–254. Springer-Verlag, 1996. LNCS.
2. S. Koziel and Z. Michalewicz. Evolutionary algorithms, homomorphous mappings,
and constrained parameter optimization. Evolutionary Computation, 7(1):19–44,
1999.
3. G.E. Liepins and W.D. Potter. A genetic algorithm approach to multiple-fault
diagnosis. In Lawrence Davis, editor, Handbook of Genetic Algorithms, chapter 17,
pages 237–250. Van Nostrand Reinhold, New York, New York, 1991.
4. D. Orvosh and L. Davis. Using a genetic algorithm to optimize problems with fea-
sibility contraints. In Proc. of the First IEEE Conf. on Evolutionary Computation,
pages 548–553, 1994.
5. H. Adeli and N-T. Cheng. Augmented lagrangian genetic algorithm for structural
optimization. Journal of Aerospace Engineering, 7(1):104–118, January 1994.
6. H.J.C. Barbosa. A coevolutionary genetic algorithm for constrained optimization
problems. In Proc. of the Congress on Evolutionary Computation, pages 1605–1611,
Washington, DC, USA, 1999.
7. P.D. Surry and N.J. Radcliﬀe. The COMOGA method: Constrained optimisation
by multiobjective genetic algorithms. Control and Cybernetics, 26(3), 1997.
8. T.P. Runarsson and X. Yao. Stochastic ranking for constrained evolutionary opti-
mization. IEEE Trans. on Evolutionary Computation, 4(3):284–294, 2000.
9. A.H.C. van Kampen, C.S. Strom, and L.M.C. Buydens.
Lethalization, penalty
and repair functions for constraint handling in the genetic algorithm methodology.
Chemometrics and Intelligent Laboratory Systems, 34:55–68, 1996.
10. Z. Michalewicz and M. Schoenauer. Evolutionary algorithms for constrained pa-
rameter optimization problems. Evolutionary Computation, 4(1):1–32, 1996.
11. R. Hinterding and Z. Michalewicz. Your brains and my beauty: Parent matching
for constrained optimization.
In Proc. of the Fifty Int. Conf. on Evolutionary
Computation, pages 810–815, Alaska, May 4-9 1998.
12. S. Koziel and Z. Michalewicz. A decoder-based evolutionary algorithm for con-
strained optimization problems.
In Proc. of the Fifth Parallel Problem Solving
from Nature. Springer-Verlag, 1998. Lecture Notes in Computer Science.
13. J.-H. Kim and H. Myung. Evolutionary programming techniques for constrained
optimization problems. IEEE Trans. on Evolutionary Computation, 2(1):129–140,
1997.
14. H.J.C. Barbosa and A.C.C. Lemonge. An adaptive penalty scheme in genetic al-
gorithms for constrained optimization problems. In Proc. of the Genetic and Evo-
lutionary Computation Conference, pages 287–294. Morgan Kaufmann Publishers,
2002.

An Adaptive Penalty Scheme for Steady-State Genetic Algorithms
729
15. S.E. Carlson and R. Shonkwiler. Annealing a genetic algorithm over constraints. In
Proc. of the IEEE Int. Conf. on Systems, Man and Cybernetics, pages 3931–3936,
1998.
16. D. Powell and M.M. Skolnick.
Using genetic algorithms in engineering design
optimization with non-linear constraints. In Proc. of the Fifth Int. Conf. on Genetic
Algorithms, pages 424–430. Morgan Kaufmann, 1993.
17. K. Deb. An eﬃcient constraint handling method for genetic algorithms. Computer
Methods in Applied Mechanics and Engineering, 186(2-4):311–338, June 2000.
18. Z. Michalewicz. A survey of constraint handling techniques in evolutionary compu-
tation. In Proc. of the 4th Int. Conf. on Evolutionary Programming, pages 135–155,
Cambridge, MA, 1995. MIT Press.
19. Z. Michalewicz, D. Dasgupta, R.G. Le Riche, and M. Schoenauer. Evolutionary
algorithms for constrained engineering problems. Computers & Industrial Engi-
neering Journal, 30(2):851–870, 1996.
20. R.G. Le Riche, C. Knopf-Lenoir, and R.T. Haftka. A segregated genetic algorithm
for constrained structural optimization. In Proc. of the Sixth Int. Conf. on Genetic
Algorithms, pages 558–565, 1995.
21. H. Homaifar, S.H.-Y. Lai, and X. Qi. Constrained optimization via genetic algo-
rithms. Simulation, 62(4):242–254, 1994.
22. J.A Joines and C.R. Houck. On the use of non-stationary penalty functions to
solve nonlinear constrained optimization problems with GAs. In Proc. of the First
IEEE Int. Conf. on Evolutionary Computation, pages 579–584, June 19–23 1994.
23. J.C. Bean and A.B. Alouane. A dual genetic algorithm for bounded integer pro-
grams. Dept. of Industrial and Operations Engineering, The University of Michi-
gan, Tech. Rep. 92-53 1992.
24. D.W. Coit, A.E. Smith, and D.M. Tate. Adaptive penalty methods for genetic
optimization of constrained combinatorial problems. INFORMS Journal on Com-
puting, 6(2):173–182, 1996.
25. M. Schoenauer and S. Xanthakis.
Constrained GA optimization.
In Proc. of
the Fifth Int. Conf. on Genetic Algorithms, pages 573–580. Morgan Kaufmann
Publishers, 1993.
26. S. Ben Hamida and M. Schoenauer. ASCHEA: new results using adaptive seg-
regational constraint handling.
In Proc. of the 2002 Congress on Evolutionary
Computation, volume 1, pages 884–889, May 2002.
27. J.A. Wright and R. Farmani. Genetic algorithms: A ﬁtness formulation for con-
strained minimization. In GECCO 2001: Proc. of the Genetic and Evolutionary
Computation Conference, pages 725–732. Morgan Kaufmann, 2001.
28. A.E. Eiben and J. I. van Hemert. Saw-ing EAs: adapting the ﬁtness function for
solving constrained problems. In D. Corne, M. Dorigo, and F. Glover, editors, New
ideas in optimization, chapter 26, pages 389–402. McGraw-Hill, London, 1999.
29. Z. Michalewicz and D.B. Fogel. How to Solve It: Modern Heuristics. Springer-
Verlag, 1999.
30. Z. Michalewicz.
Genetic Algorithms + Data Structures = Evolution Programs.
Springer-Verlag, New York, 1992.
31. H. Muhlenbein, M. Schomisch, and J. Born.
The parallel genetic algorithm as
function optimizer. Parallel Computing, 17(6-7):619–632, Sep 1991.
32. K. Deb and H.G. Beyer. Self-adaptive genetic algorithms with simulated binary
crossover. Evolutionary Computation Journal, 9(2):197–221, 2001.
33. S.B. Hamida and M. Schoenauer. An adaptive algorithm for constrained optimiza-
tion problems. In PPSN VI – LNCS, volume 1917, pages 529–538. Springer-Verlag,
2000.

Asynchronous Genetic Algorithms for
Heterogeneous Networks Using Coarse-Grained
Dataﬂow
John W. Baugh Jr.1 and Sujay V. Kumar2
1 North Carolina State University, Raleigh, NC 27695 USA
john.baugh@ncsu.edu
2 NASA Goddard Space Flight Center, Greenbelt, MD 20771 USA
sujay@hsb.gsfc.nasa.gov
Abstract. Genetic algorithms (GAs) are an attractive class of tech-
niques for solving a variety of complex search and optimization prob-
lems. Their implementation on a distributed platform can provide the
necessary computing power to address large-scale problems of practical
importance. On heterogeneous networks, however, the performance of
a global parallel GA can be limited by synchronization points during
the computation, particularly those between generations. We present a
new approach for implementing asynchronous GAs based on the dataﬂow
model of computation — an approach that retains the functional prop-
erties of a global parallel GA. Experiments conducted with an air quality
optimization problem and others show that the performance of GAs can
be substantially improved through dataﬂow-based asynchrony.
1
Introduction
Numerous studies have sought to exploit the inherent parallelism in GAs to
achieve better performance. A recent report by Cantu-Paz [4] surveys the exten-
sive research in this area and categorizes techniques for parallelization. One of
the more straightforward techniques is global parallelization, in which the eval-
uation of individuals is performed in parallel [3]. Certain variations on global
parallel GAs, such as evolving independent subpopulations [8] and hierarchi-
cally evolving populations [7], have also been developed. These and other global
parallel GAs are synchronous in the sense that computations involving subse-
quent generations may not proceed until those of the current generation are
complete. The speedup lost as a result of these synchronization points can be
signiﬁcant, particularly in a heterogeneous, networked environment, since the
presence of a single slow processor can impede the overall progress of the GA.
The limitations of global parallel GAs due to end-of-generation synchroniza-
tion points have been studied by a number of researchers. Most of the reported
approaches use localized evolution strategies such as island-based approaches [5,
9] to achieve asynchrony. However, approaches other than global parallelization
introduce fundamental changes in the structure of a GA [3]. For example, island-
based GAs work with multiple interacting subpopulations whose parameters for
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 730–741, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Asynchronous Genetic Algorithms for Heterogeneous Networks
731
interaction require additional, problem-speciﬁc tuning. Poor settings can result
in either convergence to an inferior solution or suboptimal parallel performance.
Steady-state GAs [10], which work with a single evolving population, are another
means of eliminating end-of-generation synchronization points. Instead of plac-
ing oﬀspring in subsequent populations, such GAs return them to the original
population by an operator that selects individuals to be replaced. In addition
to suﬀering in some cases from problems of premature convergence, steady-state
GAs, like island-based approaches, introduce fundamental changes in the GA.
In this paper, we present a new approach for implementing asynchronous
GAs that is functionally equivalent to a global parallel GA, and hence to a se-
quential GA as well. By functionally equivalent we mean that the outputs are
determined by precisely the same numerical operations and are likewise iden-
tical. Equivalence is achieved by “unrolling” the main loop of a global parallel
GA, i.e., the loop responsible for advancing from one generation to the next.
Inter-generational data dependencies are then captured formally using dataﬂow
graphs, which enable the concurrent processing of multiple generations to the
extent allowed by those dependencies. The beneﬁts of functional equivalence be-
tween sequential and parallel implementations are substantial. Numerical results
obtained from either implementation can be compared one-to-one with assur-
ance that artifacts have not been introduced via parallelization. Further, the
additional parameter tuning required when moving from sequential to parallel
runs of a GA need not be repeated.
While applicable in other contexts, our approach targets GAs on heteroge-
neous workstation networks that may need hours, days, or even weeks to com-
plete. In such a scenario participating computers may vary over time in their
availability and in the resources that are committed to a given GA run. This
type of variability imposes severe performance penalties when extraneous syn-
chronization points are encountered. For all its beneﬁts with compute-intensive
runs, though, it is equally appealing that the approach adds very little compu-
tational overhead: it is lightweight enough to be imperceptible on runs taking
well under a minute to complete.
2
Dataﬂow Principles
Dataﬂow [6] is a term that refers to algorithms or machines whose order of exe-
cution is based on the availability and forwarding of data. A dataﬂow program
is a directed graph with nodes that represent operators and directed arcs that
represent data dependencies. Nodes are computational tasks, and may be primi-
tive machine-level instructions or arbitrarily complex functions. As a result, the
dataﬂow model is applicable to ﬁne- or coarse-grained parallelism. In addition
to supporting varying levels of parallelism, the dataﬂow model also supports
various types of parallelism. For instance, vectorizing and pipelining are simply
special cases of standard ﬂow graphs.
In the dataﬂow model, data values are carried on tokens, which travel along
arcs, which we model as one-place buﬀers. The status of nodes can be determined

732
J.W. Baugh and S.V. Kumar
by a simple ﬁring rule: A node is said to be ﬁrable when the data it needs are
available. When a node is ﬁred, its input tokens are absorbed. The computation is
performed and the result is sent to its output arcs for other nodes to use. There
is no communication between tasks — each task simply receives and outputs
data.
The dataﬂow model has the following properties [1]:
– parallelism: nodes may execute in parallel unless there is an explicit data
dependence between them;
– determinacy: results do not depend on the relative ordering in which nodes
execute.
The natural parallelism in the dataﬂow model occurs because it does not force
over-speciﬁcation of an algorithm. The ﬁring rule only says when a node can
ﬁre. It does not require that it be executed at any particular time.
3
Using Dataﬂow for Asynchrony
A synchronous distributed GA (SDGA) based on global parallelism begins with
an initial population from which subsequent ones are obtained through a se-
lection process. Here we assume the use of a binary tournament scheme, which
selects two individuals at random, evaluates their ﬁtnesses remotely, and pro-
duces a single winner. To generate a new population of size P this process is
performed P times. Processor loads are dynamically balanced by placing evalua-
tion requests in a task pool. Crossover and mutation operators are then applied
and the entire process is repeated until convergence.
The “repeat until convergence” part of the above algorithm forces synchro-
nization at the end of each generation since individuals in subsequent genera-
tions cannot be evaluated until all of their individuals are in place. An asyn-
chronous distributed GA (ADGA) is obtained by “unrolling” this loop and
building dataﬂow graphs that capture the algorithm’s inter-generational data
dependencies. Intuitively, once a suﬃcient number of individuals have been eval-
uated in one generation, some of their oﬀspring can be produced and undergo
evaluation, even before the prior generation is complete. The extent to which
generations are processed concurrently is limited only by the data dependen-
cies derived from the synchronous implementation. Typically a “band” of 2 to 4
generations is active at any one time as the computation unfolds.
Pseudo-code for an ADGA using dataﬂow is shown in Figure 1. Populations
are constructed and named using the new population procedure, which initiates
enough dataﬂow threads (or “lightweight” processes) to carry out the genetic
operations necessary for that generation. As each dataﬂow thread completes its
task, the resulting oﬀspring are placed in the subsequent generation. The succ
function ﬁnds and returns the subsequent (or “successor”) generation: if it does
not exist it is created via new population, which has the side eﬀect of forking
a new round of dataﬂow threads for the next generation unless a termination
condition is met.

Asynchronous Genetic Algorithms for Heterogeneous Networks
733
Pﬁnal = empty
main
new population (P0)
while Pfinal is empty do wait
return ﬁttest from Pfinal
procedure new population (Pt)
if termination condition met
then Pfinal = Pt
else start n/2 threads: dataﬂow (Pt)
thread dataﬂow (Pt)
place 4 random individuals from Pt in graph
(evaluate remotely, compete, mate)
write 2 oﬀspring into succ (Pt)
function succ (Pt)
if Pt+1 is empty
then new population (Pt+1)
return Pt+1
Fig. 1. Pseudo-code for an Asynchronous GA
An illustration of a running ADGA program is shown in Figure 2. The ﬁg-
ure depicts three active generations, each with a population of 10 individuals.
Unshaded circles in each population denote empty token positions — a place to
put an individual once it is produced. The initial population, G1, begins with
randomly generated individuals so all of its circles are ﬁlled with tokens. The
ﬁgure shows that some processing has already occurred. Dataﬂow graphs D11,
D12, D14, and D15 have completed, as indicated by their dashed outlines and
the fact that they have produced oﬀspring (shaded circles) in generation G2.
Dataﬂow graph D13, on the other hand, is still working: it has a solid outline
and has yet to produce its oﬀspring in generation G2. There is a mix of working
and completed dataﬂow graphs in generation G2 as well. In generation G3, how-
ever, no dataﬂow graphs have completed, and some are still waiting for input.
No space will be allocated for generation G4 until one of the graphs in G3 is
ready to produce its oﬀspring.
The inputs to each dataﬂow graph are the randomly selected individuals
that will be used in the genetic operations. For instance, dataﬂow graph D13
takes individuals 7, 2, 5, and 0 from generation G1 and produces its oﬀspring
in positions 4 and 5 of generation G2. This behavior is more clearly seen in
Figure 3, which provides a detailed view of dataﬂow graph D13. As shown in
the ﬁgure, individuals 7 and 2 compete for position 4, and individuals 5 and 0
compete for position 5. This processing is performed by nodes in the graph, each

734
J.W. Baugh and S.V. Kumar
Fig. 2. Dataﬂow Graphs Dynamically Unfolding
being implemented by concurrent threads that block until their requisite inputs
are available.
The Copy nodes ensure that an individual can be selected and processed
simultaneously by other dataﬂow graphs. The need to copy is a result of having
data ﬂow through the model via tokens instead of being referenced as variables
— a fundamental requirement of the dataﬂow model. Pointer copying is suﬃ-
cient here, ensuring implementation eﬃciency. Compare nodes are used to keep
track of an incumbent organism — the ﬁttest seen during the GA run. Other
nodes in the graph —Evaluate, Compete, and Mate— perform the usual genetic

Asynchronous Genetic Algorithms for Heterogeneous Networks
735
operations. True parallelism is obtained in the implementation of the Evaluate
nodes, which place in a task pool a request to evaluate the individual’s ﬁtness
on a remote processor; each blocks until the result becomes available.
Fig. 3. Details of Dataﬂow Graph D13
4
Analysis and Results
Realizations of the SDGA and ADGA approaches, as described above, have been
conveniently implemented in the Java programming language using its multi-
threading capabilities and socket libraries for network communication. The im-
plementations have been shown to be both eﬃcient and portable across multiple
platforms and operating systems — even within a single GA run. Experiments
have been conducted with homogeneous as well as heterogeneous systems of
processors, and simple empirical models have been developed to predict execu-
tion times. We begin by describing these models and then comparing predicted
results with those obtained on a simple 0/1 knapsack problem and on a more
complex air quality management problem.

736
J.W. Baugh and S.V. Kumar
4.1
Homogeneous System of Processors
Consider a homogeneous network of computers consisting of N identical proces-
sors. For a single generation of a GA to complete, P organisms must be evaluated.
It is assumed that all of the N processors start simultaneously, and that each
takes time tcomp to execute a ﬁtness evaluation and time tcomm for communi-
cation with the client. The tasks associated with the GA can then be laid out
in blocks, with each block representing the tasks performed by N processors in
time tcomp + tcomm, as shown in Figure 4.
Fig. 4. GA Tasks Executing on N Homogeneous Processors
The pattern of blocks repeats itself until the end of a generation, at which
point some number of evaluations n remain to be performed. Since N individuals
are evaluated in each block, the total number of blocks in a generation is equal
to ⌈P
N ⌉. From the ﬁgure, the time taken for a single generation (Tg) and the
total time taken by an SDGA (Tsync) can be estimated as:
Tg = ⌈P
N ⌉(tcomp + tcomm)
(1)

Asynchronous Genetic Algorithms for Heterogeneous Networks
737
Tsync = Tg G
= ⌈P
N ⌉(tcomp + tcomm) G
(2)
In the case of an ADGA, the processors are not constrained by the lack of
available tasks at the end of a generation since, in practice, a suﬃcient number
are available from subsequent generations to avoid idling. The total number of
tasks in an ADGA evaluation is PG. Since there are N processors the total time
taken by an ADGA (Tasync) can be estimated as:
Tasync = P G
N (tcomp + tcomm)
(3)
4.2
Heterogeneous System of Processors
To model a heterogeneous system, ns identically slow processors are introduced
into the system of N processors. Each of these slow processors is assumed to
require a factor of f more processing time to evaluate an individual. The quan-
tities t and tslow are deﬁned to be the sum of tcomp and tcomm for fast and slow
processors, respectively. As with the homogeneous case, the tasks on a heteroge-
neous system can be laid out in blocks, where in this case each block is of width
tslow. Figure 5 shows GA tasks on a heterogeneous system with a single slow
processor and f equal to 4. As depicted in the ﬁgure, for an SDGA, the presence
of a slow processor clearly leaves idle a large number of faster processors.
Fig. 5. GA Tasks Executing on Heterogeneous Processors

738
J.W. Baugh and S.V. Kumar
The number of blocks in a generation can be estimated as:
nb = ⌈
P
f(N −ns) + ns
⌉
(4)
Depending on the ordering of tasks, the number of tasks that remain at the end
of generation becomes important. The number present in the ﬁnal block of a
generation (δ1) can be estimated as:
δ1 = P −(nb −1) (f(N −n) + n)
(5)
If there are more tasks in the last block than fast processors, the slow processors
will receive tasks to evaluate. Taking these factors into account, the total time
taken by an SDGA can be estimated as:
Tsync =

(nb −1)f t G + t G
if δ1 ≤(N −ns)
nb t f G
otherwise
(6)
Since end-of-generation synchronizations are eliminated in an ADGA, the overall
GA execution can be thought of as an ordering of PG tasks among processors.
The number of blocks is estimated as:
nb = ⌈
PG
f(N −ns) + ns
⌉
(7)
At the end of the GA execution, if the last block contains more tasks than
the number of fast processors, the slow processors will be involved in the ﬁnal
computations. The number of tasks present in the ﬁnal block of GA execution
(δ2) can be estimated as:
δ2 = PG −(nb −1) (f(N −ns) + ns)
(8)
The estimated time taken by an ADGA is:
Tasync =

(nb −1)ft + t
if δ2 ≤(N −ns)
nb t f
otherwise
(9)
4.3
0/1 Knapsack Problem
The 0/1 knapsack problem is representative of the large class of problems known
as combinatorial optimization problems. Informally stated, the objective of the
knapsack problem is to select items that maximize proﬁt without exceeding
capacity. As such, the problem is ﬁne grained since ﬁtness evaluation is typically
inexpensive.
Both SDGA and ADGA implementations are applied to the 0/1 knapsack
problem with anywhere from 3 to 30 processors. To assess their scalability with
increased problem size, ﬁtness evaluation times are artiﬁcially varied to achieve
four diﬀerent levels of granularity based on the ratio of tcomp to tcomm. Since

Asynchronous Genetic Algorithms for Heterogeneous Networks
739
tcomm is approximately 250 milliseconds in our set up, tcomp times are artiﬁ-
cially set to 250, 500, 750 and 1000 milliseconds, resulting in granularity factors
of 1 through 4. To simulate a heterogeneous system, a slow processor is intro-
duced with f set to 5. GA runs conducted with a population size of 100 for
200 generations yield the results shown in Figure 6. Although tcomp and tcomm
are underpredicted in the model, the trends are as expected, with execution
times increasing with problem granularity, and the ADGA scaling better than
the SDGA.
Fig. 6. Execution Time vs. Granularity using 15 Processors: 0/1 Knapsack Problem
4.4
Air Quality Optimization
Tropospheric ozone formed from the emissions of vehicles and industrial sources
is considered a major pollutant. As a result, air quality management strategies
may be necessary for geographic regions containing hundreds of sources, with
each in turn having thousands of processes. Formal search strategies using GAs
can be applied to ﬁnd cost-eﬀective ways of reducing ozone formation. For in-
stance, an ambient least cost (ALC) model [2] is an optimization approach that
incorporates source marginal control costs and emission dispersion characteris-
tics to compute the source emissions at the least cost. A number of modeling
techniques can be used to determine dispersion characteristics, such as the Em-
pirical Kinetic Modeling Approach (EKMA), a Lagrangian box model that is
used in this study. Because of the execution times typically required for EKMA,
this GA formulation is somewhat coarse grained.

740
J.W. Baugh and S.V. Kumar
Experiments for an air quality management study around Charlotte, NC,
were conducted on a network of workstations with as many as 19 processors. To
simulate a heterogeneous system, a slow processor with an f factor of 5 is used.
In each case, the GA was run for 50 generations using a population size of 50.
The execution times are found to be in close agreement with the values predicted
by the empirical model, as shown in Figure 7. Better agreement here than in
the knapsack problem is likely due to increased problem granularity. Similar to
earlier trends, the SDGA is outperformed by the ADGA; the execution times of
the SDGA follow a step function pattern implying that, in between each step,
there is no marginal beneﬁt in using additional processors.
Fig. 7. Execution Time vs. Processors: Air Quality Optimization
5
Final Remarks
The growing acceptance of GAs has led to widespread use and attempts at solv-
ing larger and more challenging problems. A practical approach for doing so may
rest on the ability to use available computer resources eﬃciently. Motivating the
algorithmic developments in this paper is the expectation that a heterogeneous
collection of personal computers, workstations, and laptops should be able to
contribute their cycles to the solution of substantial problems without inad-
vertently detracting from overall performance. Removing the end-of-generation
synchronization points from global parallel GAs is necessary to meet this expec-
tation. The application of loop unrolling and dataﬂow modeling described herein

Asynchronous Genetic Algorithms for Heterogeneous Networks
741
has been shown to be eﬀective in keeping available processors from idling even
when substantial variations exist in the processors’ capabilities.
Although other asynchronous approaches might be used, one that is func-
tionally equivalent to a simple, sequential GA oﬀers real beneﬁts with respect to
parameter tuning. In a signiﬁcant study on air quality management [references
temporarily withheld for blind review process], our research team was able to
move with little eﬀort between atmospheric models that varied widely in their
computational demands — from simple ones that can be solved using sequential
GAs, to ones that require 20 minutes to evaluate a single individual on a high-
end workstation: the same basic algorithm and parameters could be (and were)
used in either case.
The GA implementations described in this paper are part of Vitri, an object-
oriented framework implemented in Java for high-performance distributed com-
puting [references temporarily withheld for blind review process]. Among its fea-
tures are basic support for distributed computing and communication, as well
as visual tools for evaluating run-time performance, and modules for heuristic
optimization. It balances loads dynamically using a client-side task pool, allows
the addition or removal of servers during a run, and provides fault tolerance
transparently for servers and networks.
References
1. Arvind and D. E. Culler. Dataﬂow architectures. Annual Reviews in Computer
Science, 1:225–253, 1986.
2. S. E. Atkinson and D. H. Lewis. A cost-eﬀective analysis of alternative air quality
control strategies. Journal of Environmental Economics, pages 237–250, 1974.
3. E. Cantu-Paz. Designing eﬃcient master-slave parallel genetic algorithms. Tech-
nical report, University of Illinois at Urbana-Champaign, Urbana, IL, 1997.
4. E. Cantu-Paz. A survey of parallel genetic algorithms. Technical Report 97003,
University of Illinois at Urbana Champaign, May 1997.
5. V. Coleman. The DEME mode: An asynchronous genetic algorithm. Technical
Report UM-CS-1989-033, University of Massachusetts, May 1989.
6. Computer. Special issue on data ﬂow systems. 15(2), 1982.
7. J. Kim and P. Zeigler. A framework for multiresolution optimization in a paral-
lel/distributed environment: Simulation of hierarchical GAs. Journal of Parallel
and Distributed Computing, 32:90–102, 1996.
8. Yu-Kwong Kwok and Ahmad Ishfaq. Eﬃcient scheduling of arbitrary task graphs
to multiprocessors using a parallel genetic algorithm.
Journal of Parallel and
Distributed Computing, 47:58–77, 1997.
9. M. G. Schleuter. Asparagas: An asynchronous parallel genetic optimization strat-
egy.
Proceedings of the Third International Conference on Genetic Algorithms,
pages 422–427, 1989.
10. J. E. Smith and T. C. Fogarty. Self adaptation of mutation rates in a steady state
genetic algorithm. In Proceedings of IEEE International Conference on Evolution-
ary Computing, volume 72, pages 318–323, 1999.

A Generalized Feedforward Neural Network
Architecture and Its Training Using Two
Stochastic Search Methods
Abdesselam Bouzerdoum1 and Rainer Mueller2⋆
1 School of Engineering and Mathematics
Edith Cowan University, Perth, WA, Australia
a.bouzerdoum@ieee.org
2 University of Ulm, Ulm, Germany
Abstract. Shunting Inhibitory Artiﬁcial Neural Networks (SIANNs)
are biologically inspired networks in which the synaptic interactions are
mediated via a nonlinear mechanism called shunting inhibition, which
allows neurons to operate as adaptive nonlinear ﬁlters. In this article,
The architecture of SIANNs is extended to form a generalized feedfor-
ward neural network (GFNN) classiﬁer. Two training algorithms are de-
veloped based on stochastic search methods, namely genetic algorithms
(GAs) and a randomized search method. The combination of stochastic
training with the GFNN is applied to four benchmark classiﬁcation prob-
lems: the XOR problem, the 3-bit even parity problem, a diabetes dataset
and a heart disease dataset. Experimental results prove the potential of
the proposed combination of GFNN and stochastic search training meth-
ods. The GFNN can learn diﬃcult classiﬁcation tasks with few hidden
neurons; it solves perfectly the 3-bit parity problem using only one neu-
ron.
1
Introduction
Computing has historically been dominated by the concept of programmed com-
puting, in which algorithms are designed and subsequently implemented using
the dominant architecture at the time. An alternative paradigm is intelligent
computing, in which the computation is distributed and massively parallel and
learning replaces a priori program development. This new, biologically inspired,
intelligent computing paradigm is called Artiﬁcial Neural Networks (ANNs) [1].
ANNs have been used in many applications where the conventional programmed
computing has immense diﬃculties, such as understanding speech and hand-
written text, recognizing objects, etc. However, an ANN needs to learn the task
at hand before it can be operated in practice to solve the real problem. Learn-
ing is accomplished by a training algorithm. To this end, a number of diﬀerent
training methods have been proposed and used in practice.
⋆R. Mueller was a visiting student at ECU for the period July 2001 to June 2002.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 742–753, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

A Generalized Feedforward Neural Network Architecture
743
Another biologically inspired computing paradigm is genetic and evolution-
ary algorithms [2],[3]. Evolutionary algorithms are stochastic search methods
that mimic the metaphor of natural biological evolution. They operate on popu-
lation of potential solutions applying the principle of survival of the ﬁttest. The
combination of these two biologically inspired computing paradigms is a pow-
erful instrument for solving problems in pattern recognition, signal and image
processing, machine vision, control, etc.. The aim in this article is to combine a
Generalized Feedforward Neural Network (GFNN) architecture with genetic al-
gorithms to design a new class of artiﬁcial neural networks that has the potential
to learn complex problems more eﬃciently.
In the next section, the generalized shunting neuron and the GFNN archi-
tecture are introduced. Two training methods for the GFNN architecture are
presented in section 3. First the randomized search method is presented in Sub-
section 3.1, then the GA technique in Subsection 3.2. The developed training
algorithms are tested with some common benchmark problems in Section 4,
followed by concluding remarks and future work in Section 5.
2
The Generalized Feedforward Neural Network
Architecture
In [4] Bouzerdoum introduced the class of shunting inhibitory artiﬁcial neural
networks (SIANNs) and used them for classiﬁcation and function approximation.
In this section, we extend SIANNs to form a generalized feedforward neural net-
work architecture. But before describing the generalized architecture, we ﬁrst
introduce the elementary building block of the architecture, namely the gener-
alized shunting inhibitory neuron.
2.1
Generalized Shunting Inhibitory Neuron
The output of a generalized shunting inhibitory neuron is given by
xj =
f(
i wjiIi + wj0)
aj + g(
i cjiIi + cj0) =
f(wj · I + wj0)
aj + g(cj · I + cj0)
(1)
where xj is the activity (output) of neuron j; Ii is the ith input; cji is the
“shunting inhibitory” connection weight from input i to neuron j; wji is the
connection weight from input i to neuron j; wj0 and cj0 are bias constants; aj is
a constant preventing the division by zero, by keeping the denominator always
positive; f and g are activation functions. The name shunting inhbition comes
from the fact that a high term in the denominator tends to supress (or inhibit
in a shunting fashion) the activity caused by the term in the numerator of (1).
2.2
The Network Architecture
The architecture of the generalized feedforward neural network is similar to that
of a Multilayer Perceptron Network [1], and is shown in Fig. 1. The network

744
A. Bouzerdoum and R. Mueller
S
S
S
S
S
S
P
P
P
P
S
S
S
S
S
P
Input layer
Output layer
Hidden layers
Shunting Inhibitory Neuron
Perceptron
Fig. 1. Generalized Feedforward Neural Network architecture (GFNN).
consists of many layers, each of which has a number of neurons. The input layer
only acts as a receptor that receives inputs from the environment and broadcasts
them to the next layer; therefore, no processing is done in the input layer. The
processing in the network is done by the hidden and output layers. Neurons in
each layer receive inputs from the previous layer, process them and then pass
their outputs to the next layer. Hidden layers are so named because they have no
direct connection with the environment. In the GFNN architecture, the hidden
layers consist of only generalized shunting inhibitory neurons. The role of the
shunting inhibitory layers is to perform a nonlinear transformation on the input
data so that the results can easily be combined by the output neurons to form the
correct decision. The output layer, which may be a linear or sigmoidal type (i.e.,
perceptron), is diﬀerent from the hidden layers; each output neuron basically
calculates the weighted sum of its inputs followed by an appropriate activation
function. The response, y, of an output neuron is given by
y = h(wo · x + b)
(2)
where x is the input vector wo is the weight vector, b is the bias constant, and
h is the activation function, which may be a linear or a sigmoid function.
3
Training Methods
An artiﬁcial neural network needs to be trained instead of being a priori pro-
grammed. Supervised learning is a form of learning in which the target values are

A Generalized Feedforward Neural Network Architecture
745
included part of the training data. During the training phase, the set of training
data is repeatedly applied to the network and the weights of the network are
adjusted until the diﬀerence between the target values and the network output
values is within the desired tolerance.
Neural
Network
Input
data
training
(included in 
training data)
Target value
Output
Fig. 2. Supervised learning: the weights are adjusted until the target values are reached.
In this section, two diﬀerent methods for training the GFNN are described:
the Random Optimization Method (ROM) and the GA based method. Since GAs
are known for being able to ﬁnd good solutions for many complex optimization
problems, this training method is of particular interest to us.
3.1
Random Optimization Method (ROM)
The ROM is employed because it is a simple method to implement and intuitively
appealing. It is used to test the network structure before the GA is applied, and
serves as a benchmark for comparing the GA based training method. The ROM
searches the weight space by generating randomized vectors in the weight space
and testing them. The basic ROM procedure is as follows [1]:
1. Randomly choose a weight vector W and a small vector R.
2. If the output of the net Y (W +R) is better than Y (W ) then W = W +R.
3. Check for termination criteria, end the algorithm when one of the termina-
tion criteria is achieved.
4. Randomly choose a new R and go to step (2)
There are some obvious extensions to the above algorithm which we have
implemented. The ﬁrst one implements reverse side checking. This means instead
of checking only W + R we check W −R as well. Furthermore, an orthogonal
vector R∗is also checked in both directions. That alone wouldn’t improve the
algorithm much, but there is another extension. If there is an improvement in any
of the four previous directions, simply extend the search in the same direction,
instead of just generating another value of R. The idea is that if W + R gives
an improved output Y , then another scaled step k · R in the same direction
might be in a “downhill” direction, and hence a successful direction. All these
extensions have been implemented to train the GFNN.

746
A. Bouzerdoum and R. Mueller
3.2
Genetic Algorithms (GAs)
The GAs are used as a training method because they are known for their ability
to perform well on complex optimization problems. Furthermore, they are less
likely to get trapped in local minima, a problem suﬀered by traditional gradi-
ent based training algorithms. GAs are stochastic search methods that mimic
the metaphor of natural biological evolution. They operate on a population of
potential solutions applying the principle of survival of the ﬁttest to produce
an improved approximation to a solution. At each generation, a new set of ap-
proximations is created by the process of selecting individuals according to their
level of ﬁtness in the problem domain and breeding them together using oper-
ators borrowed from natural evolution. This process leads to the evolution of
populations of individuals that are better suited to their environment than the
individuals they were created from, just as in natural adaptation.
GAs model natural evolutionary processes, such as selection, recombination,
mutation, migration, locality and neighborhood. They work on populations of
individuals instead of single solutions. Furthermore, simple GAs can be extended
to multipopulation GAs. In multipopulation GAs several subpopulations are in-
troduced, which evolve independently over few generations before one or more
individuals are exchanged between the subpopulations. Figure 3 shows the struc-
ture of an extended multipopulation genetic algorithm.
generate
new
population
fitness assignment
selection
evaluation of
offspring
competition
recombination
migration
mutation
reinsertion
Are termination
criteria met?
best
individuals
− creation of initial
population
− evaluation of
individuals
initialization
result
yes
no
Fig. 3. Structure of an extended multipopulation genetic algorithm (adapted from [5]).

A Generalized Feedforward Neural Network Architecture
747
The genetic operators that can be applied to evolve the population depend
on the variable representation in the GA: binary, integer, ﬂoating point (real), or
symbolic. In this research, we employed the real variable representation because
it is the most natural representation for weights and biases of neural networks.
Furthermore, it has been shown that the real-valued GA is more eﬃcient than
the binary GA [3]. Some of the most common GA operators are described below.
Selection. Selection determines the individuals which are chosen for mating (re-
combination) and how many oﬀsprings each selected individual produces. Each
individual in the selection pool receives a reproduction probability depending
on its own objective value and the objective values of all other individuals in
the population. There are two ﬁtness-based assignment methods: proportional
ﬁtness assignment and rank-based ﬁtness assignment. The proportional ﬁtness
assignment assigns a ﬁtness value proportional to the objective value, whereas
the ﬁtness value in a rank-based assignment depends only on the rank of the
individual in a list sorted according to the objective values.
Roulette-wheel selection, also called “stochastic sampling with replacement”
[6], maps the individuals to contiguous segments of a line, such that each indi-
vidual’s segment is equal in size to its ﬁtness [5]. The individual whose segment
spans a generated random number, is selected.
In stochastic universal sampling, the individuals are mapped to N contiguous
segments of a line (N being the number of individuals), each segment having
a length proportional to its ﬁtness. Then N equally spaced Pointers are placed
above the line and the position of the ﬁrst pointer is given by a randomly gener-
ated number in the range [0, 1/N]. Every pointer indicates a selected individual.
In local selection every individual interacts only with individual residing in
its local neighborhood [5].
In truncation selection individuals are sorted according to their ﬁtness and
only the best individuals are selected as parents.
The tournament selection chooses randomly a number of individuals from
the population and the best individual from this group is selected as parent.
The process is repeated until enough mating individuals are found.
Recombination. The process of recombination produces new individuals by
combining the information contained in the parents. There are diﬀerent recom-
bination methods depending on the variable representation. Discrete recombi-
nation can be used with all representations. In addition, there are two speciﬁc
methods for real valued recombination, the intermediate recombination and the
line recombination. In intermediate recombination the variables of the oﬀspring
are chosen somewhere around and between the variable values of the parents.
Line recombination, on the other hand, generates the oﬀspring on a line deﬁned
by the variable values of the parents.
Mutation. After recombination, every oﬀspring undergoes a mutation, like in
nature. Small perturbations mutate the oﬀspring variables with low probability.
Mutation of real variables means that randomly generated values are added to

748
A. Bouzerdoum and R. Mueller
the oﬀspring variables with low probability. Thus, the probability of mutating
a variable (mutation rate) and the size of change for each mutated variable
(mutation step) must be deﬁned. In our simulations, the mutaion rate is inversely
proportional to the number of variables; the more variables an individual has,
the smaller is the mutation rate.
Reinsertion. After an oﬀspring is produced it must be inserted into the popula-
tion. There are two diﬀerent situations. First, the size of the oﬀspring population
produced is less than the size of the original population. In this case, the whole
oﬀspring population has to be inserted to maintain the size of the original pop-
ulation. Second more oﬀsprings are generated than there are individuals in the
original population. In this case, the reinsertion scheme determines which in-
dividuals should be reinserted into the new population and which individuals
should be replaced by the oﬀsprings. There are diﬀerent schemes for reinsertion.
Pure reinsertion produces as many oﬀsprings as parents and replaces all parents
by the oﬀspings. Uniform reinsertion produces fewer oﬀsprings than parents
and replaces parents uniformly at random. Elitist reinsertion produces fewer
oﬀsprings than parents and replaces the worst parents. Fitness based reinsertion
produces more oﬀsprings than needed and reinserts only the best oﬀsprings.
After reinsertion, one needs to verify if a termination criteria is met. If a cri-
teria is met, then the cycle can be stopped; otherwise, the cycle will be repeated
until a termination criteria is met. The GA parameters used in the simulations
are presented in Table 1 below.
Table 1. Evolutionary algorithm parameters used in the simulations.
subpopulations individuals 50 30 20 20 10
variable format real values
selection
function
selsus (stochastic universal sampling)
pressure
1.7
gen. gap
0.9
reinsertion
rate
1
recombination name
discrete and line recombination
rate
1
mutation
name
mutreal (real-valued mutation)
rate
0.00826
range
0.1 0.03 0.01 0.003 0.001
precision
12
regional model
migration
rate
0.1
competition
rate
0.1
The objective function to be minimized here is the mean squared error.
MSE = 1
Np
Np

j=1
(yj −dj)2
(3)

A Generalized Feedforward Neural Network Architecture
749
where yj is the output of the GFNN, dj the desired output for input pattern xj,
and Np is the number of training patterns.
4
Experimental Results
Experiments were conducted to assess the ability of the proposed NN architec-
ture to learn some diﬃcult classiﬁcation tasks. Four benchmark problems were
selected to test the network architecture: two Boolean functions, the Exclusive-
OR (XOR) and the 3-bit parity, and two medical diagnosis problems, the heart
disease and diabetes. The heart disease and diabetes data sets were obtained
from UCI Machine Learning Repository [7].
4.1
The XOR and 3-Bit Parity Problems
A two-layer network architecture consisting of two inputs, one or two hidden
units, and an output unit is trained with XOR problem. For every network con-
ﬁguration, ten training runs, with diﬀerent intializations, were performed using
both the GA- and the ROM-based training algorithms. If during the training
a network reaches an error of zero, training is halted. Table 2 summarizes the
results: the ﬁrst column indicates the f/g combination of activation functions
(see Eq. (1)), along with the training algorithm. In all the simulations f was
hyperbolic tangent sigmoid activation function, tansig, and g was either the ex-
ponential function, exp, or the logarithmic sigmoid activation function, logsig.
The GA uses a search space ranging from -128 to 128, and hence is labeled
GA128. The second column shows the number of training runs that achieved
zero error. The “Best case error” column shows the lowest test error of trained
networks. Note that even when an error of zero is not reached during training,
the network can still learn the desired function after thresholding its output.
Table 2. Training with the XOR problem.
Runs w. Aver. generation
Aver. time
Best case Mean Std
E=0
to reach E=0
to reach E=0
error
error
No. of neurons: 1 (hidden layer), 9 weights
tansig/logsig GA128
1
620
15.89
0.00
25.50
790
tansig/logsig ROM
4
4423
4.56
0.00
15.00 1290
tansig/exp GA128
10
21
0.51
0.00
0.00
000
tansig/exp ROM
6
488
0.47
0.00
10.00 1290
No. of neurons: 2 (hidden layer), 17 weights
tansig/logsig GA128
8
68
2.02
0.00
5.00 1054
tansig/logsig ROM
10
393
0.52
0.00
0.00
000
tansig/exp GA128
10
13
0.37
0.00
0.00
000
tansig/exp ROM
10
845
1.05
0.00
0.00
000
The best results were obtained using two neurons in the hidden layer with
the exponential activation function, exp, in the denominator. Note that both
training algorithms, GA and ROM, reached an error of zero at least once during

750
A. Bouzerdoum and R. Mueller
training. The GA was slightly faster with 0.37 minutes average time to reach an
error of zero than the ROM, which needed 1.05 minutes. Figure 4 displays the
percentage mean error vs. training time for the best combination of activation
functions (tansig/exp). More importantly, however, is the fact that even with
one hidden neuron and tansig/exp combination, ten out of ten runs reached an
error of zero, with the GA as training algorithm. However, the time to reach
an error of 0 was 0.51 minutes slightly longer than the time of the two neuron
network. Also, we can observe that both the ROM and GA perform well in the
sense of reaching runs with error zero. Furthermore, all trained were able to
classify the XOR problem correctly.
0
1
2
3
4
5
6
7
8
0
10
20
30
40
50
60
minutes
percentage mean error
(a)
0
1
2
3
4
5
6
7
8
0
10
20
30
40
50
60
minutes
percentage mean error
(b)
Fig. 4. Percentage mean error over time with tansig/exp as activation functions: (a) 1
hidden unit, (b) 2 hidden units. The dotted line is the result of the ROM and the solid
line is the result of the GA.
For the 3-bit partiy problem, the network architecture consists of three in-
puts, one hidden layer and one ouptput unit of the perceptron type; the hid-
den layer comprises one, two or three shunting neurons. The same experiments
as with the XOR problem were conducted with 3-bit parity; that is, ten runs
for each architecture are performed with tansig/logsig or tansig/exp activation
functions. Table 3 presents the result of the ten runs. None of the networks with
logsig activation function in the denominator reach an error of zero during
training. However, using the exponential activation function in the denomina-
tor, some networks with one hidden unit reach zero error during training and
most networks, even those that do not reach zero error during training, learn to
classify the even-parity correctly.
4.2
Diabetes Problem
The diabetes dataset has 768 samples with 8 input parameters and two output
classes: presence (1) or absence (0) of diabetes. The dataset was partitioned into
two sets: 50% of the data points were used for training and the other 50% for
testing. The network architecture consisted of 8 input units, one hidden layer

A Generalized Feedforward Neural Network Architecture
751
Table 3. Training with the 3-bit even parity.
Runs w. Aver. generation
Aver. time Best case Mean
Std
E=0
to reach E=0
to reach E=0
error error
No. of neurons: 1 (hidden layer), 11 weights
tansig/logsig GA128
0
NaN
NaN
12.50 20.00
6.45
tansig/logsig ROM
0
NaN
NaN
12.50 28.75 11.86
tansig/exp GA128
2
629
7.13
0.00 17.50 12.08
tansig/exp ROM
0
2720
1.36
0.00 20.00 10.54
No. of neurons: 2 (hidden layer), 21 weights
tansig/logsig GA128
0
NaN
NaN
12.50 22.50
5.27
tansig/logsig ROM
0
7320
4.99
0.00 18.75
8.84
tansig/exp GA128
6
243
3.33
0.00
6.25
8.84
tansig/exp ROM
4
11180
6.56
0.00
7.50
6.45
No. of neurons: 3 (hidden layer), 31 weights
tansig/logsig GA128
3
753
12.58
0.00 12.50 10.21
tansig/logsig ROM
3
4770
6.59
0.00 13.75 10.94
tansig/exp GA128
8
57
0.92
0.00
2.50
5.27
tansig/exp ROM
7
9083
12.04
0.00
3.75
6.04
of shunting neurons, and one output unit. The number of hidden units varied
from one to eight. The size of the search space is also varied: [−64, 64] (GA64),
[−128, 128] (GA128), [−512, 512] (GA512). Again ten training runs for each ar-
chitecture and each algorithm, GA and ROM, were performed. The network
GA128 was also trained on a reduced data set (a quarter of the total data);
this network is denoted GA128q. After training is completed, the generalization
ability of each network is tested by evaluating its performance on the test set.
Figure 5 presents the percentage mean error of the training dataset. It can be
observed that the tansig/exp activation function combination performs slightly
better than the tansig/logsig. The ROM gets worse with increasing number of
neurons, what we expected. The reason is that the one hidden-neuron conﬁgu-
ration has 21 weights/biases whereas the 8 hidden-neuron conﬁguration has 161
Mean error training
0.0
5.00
10.00
15.00
20.00
25.00
30.00
35.00
1
2
3
4
5
6
7
8
No. of neurons
percentage mean error
tansig exp GA512
tansig exp GA128
tansig exp GA128q
tansig exp GA64
tansig exp ROM
(a)
Mean error training
0.00
5.00
10.00
15.00
20.00
25.00
30.00
35.00
1
2
3
4
5
6
7
8
No. of neurons
percentage mean error
tansig logsig GA512
tansig logsig GA128
tansig logsig GA128q
tansig logsig GA64
tansig logsig ROM
(b)
Fig. 5. Percentage mean error (train dataset) of the 10 runs: (a) tansig/exp, (b) tan-
sig/logsig conﬁguration.

752
A. Bouzerdoum and R. Mueller
Percentage mean error GA128
0.00
5.00
10.00
15.00
20.00
25.00
30.00
35.00
1
2
3
4
5
6
7
8
No. of neurons
Percentage mean error
training
test
(a)
Mean error test
23.00
24.00
25,00
26.00
27.00
28.00
29.00
30.00
1
2
3
4
5
6
7
8
No. of neurons
percentage mean error
tansig logsig GA128
tansig logsig GA128q
(b)
Fig. 6. (a) Percentage mean error of the GA128 on the training and test sets. (b)
Generalization performance of GA128 and GA128q on the test set.
weights/biases. With increasing number of weights/biases the dimension of the
search space increases, which leads to worse performance by the ROM.
In Fig. 6 the percentage mean error of the training dataset is compared
with the percentage mean error of the test set; both are almost equal for all the
diﬀerent number of neurons. This shows that overﬁtting is not a serious problem.
4.3
Heart Disease Problem
The experimental procedure was the same as for the diabetes diagnoses problem,
except that the data set has only 270 samples with 13 input parameters. This
increases the number of parameters of the network and slows down the training
process. To avoid being bogged down by the training process, only GA128 was
trained on the Heart dataset. Figure 7(a) presents the mean error rates on the
training set. Not surprising, the mean error rate of the ROM increases with
increasing number of neurons. Figure 7(b) compares the performances of the
GA on the training and test sets. The results of the heart disease problem are
Percentage mean error (training)
0.00
5.00
10,00
15.00
20.00
25.00
30.00
1
2
3
4
5
6
7
8
No. of neurons
percentage mean error
tansig logsig GA128
tansig logsig ROM
tansig exp GA128
tansig exp ROM
(a)
Percentage mean error of GA128
0.00
5.00
10.00
15.00
20.00
25.00
30.00
1
2
3
4
5
6
7
8
No. of neurons
percentage mean error
training
test
(b)
Fig. 7. Percentage mean error: (a) training set, (b) training set compared totest set.

A Generalized Feedforward Neural Network Architecture
753
similar to those of the diabetes diagnoses problem, except the errors are much
lower; it is well known that the Diabetes problem is harder to learn that the
Heart Disease problem.
5
Conclusions and Future Work
In this article we presented a new class of neural networks and two training
methods: the ROM and the GA algorithms. As expected, the ROM works well
for a small number of weights/biases but becomes worse as the number of pa-
rameters increases. The experimental results show that the presented network
architecture, with the proposed learning schemes, can be a powerful tool for
solving problems in prediction, forecasting and classiﬁcation. It was shown that
the proposed architecture can learn a Boolean function perfectly with a small
number of hidden units. The tests on the two medical diagnosis problems, dia-
betes and heart disease, proved that the proposed architecture can learn complex
tasks with good generalization ability and hardly any overﬁtting.
Some further work needs to be considered to improve the learning perfor-
mance of the proposed architecture. Firstly, a suitable termination criteria must
be found to stop the algorithm, which could be the classiﬁcation error on a vali-
dation set. Secondly, the settings of the GA should be optimized. In this project
only diﬀerent sizes of the search space were used. To get better results other
settings, e.g. size of population, mutation methods, should be optimized. Finally
a combination of the GA and, e.g., gradient descent method can improve the
results further. GAs are known for their global search and gradient methods for
their local search; by combining the two, we should expect better results.
References
1. Schalkoﬀ, R. J.: Artiﬁcial Neural Networks. McGraw-Hill 1997.
2. Goldberg, D. E.: Genetic Algorithms in search, Optimization and Machine Learning.
Addison-Wesley, 1989.
3. Michalewicz, Z.: Genetic Algorithms + Data Structures = Evolution Programs, (2nd
edition). Berlin, Heidelberg, New York: Springer-Verlag, 1994.
4. Bouzerdoum, A.: “Classiﬁcation and function approximation using feed-forward
shunting inhibitory artiﬁcial neural networks,” Proc. IEEE/INNS Int. Joint Conf.
Neural networks (IJCNN-2000), Vol. VI, pp. 613–618, 24–27 July 2000, Como, Italy.
5. Pohlheim, H.: Genetic and Evolutionary Algorithms: Principles, Methods and Algo-
rithms, 1999. http://www.geatbx.com.
6. Baker, J. E.: “Reducing bias and ineﬃciency in the selection algorithms,” Proc.
Second Int. Conf. on Genetic Algorithms, pp. 14–21, 1987.
7. Blake, C. L., Merz, C. J.: “UCI Repository of Machine Learning Databases,” Dept.
Information and Computer Science, University of California, Irvine, 1998.
8. Rooij, Jain and Johnson: Neural Network Training using Genetic Algorithms. World
Scientiﬁc, 1996.

Ant-Based Crossover for Permutation Problems
J¨urgen Branke, Christiane Barz, and Ivesa Behrens
Institute AIFB, University of Karlsruhe, 76128 Karlsruhe, Germany
branke@aifb.uni-karlsruhe.de
Abstract. Crossover for evolutionary algorithms applied to permuta-
tion problems is a diﬃcult and widely discussed topic. In this paper
we use ideas from ant colony optimization to design a new permutation
crossover operator. One of the advantages of the new crossover operator
is the ease to introduce problem speciﬁc heuristic knowledge. Empirical
tests on a travelling salesperson problem show that the new crossover op-
erator yields excellent results and signiﬁcantly outperforms evolutionary
algorithms with edge recombination operator as well as pure ant colony
optimization.
1
Introduction
Crossover for evolutionary algorithms (EAs) applied to permutation problems is
notoriously diﬃcult, and many diﬀerent crossover operators have been suggested
in the literature. Ant colony optimization (ACO), however, seems particularly
well suited for permutation problems. In this paper, we propose to hybridize
these two approaches in a way that performs better than either of the original
approaches. In particular, we design a new crossover operator, called ant-based
crossover (ABX), which uses ideas from ACO within an EA framework.
In ACO, new solutions are constructed step by step based on a pheromone
matrix which contains information about which decisions have been successful
in the past. Furthermore, problem speciﬁc heuristic knowledge is usually used
to inﬂuence decisions. In ABX, a temporary pheromone matrix is constructed
based on the parents selected for mating. This temporary pheromone matrix is
then used to create one or several children in the standard way employed by
ACO. This has several interesting implications: First of all, it is now as easy as
in ACO to incorporate problem-speciﬁc heuristic knowledge. Furthermore, we
gain additional ﬂexibility. For example, it is natural to extend ABX to construct
children from more than two parents, or to integrate ACO as local optimizer.
Finally, the use of a population allows us to explicitly maintain several diﬀerent
good solutions, which is not possible in pure ACO approaches.
While we do not see any reason why the proposed approach should not be
successful on a wide range of permutation problems, in this paper we concen-
trate on the travelling salesperson problem (TSP). We empirically compare our
approach with an evolutionary algorithm with edge recombination as well as a
pure ACO algorithm.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 754–765, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Ant-Based Crossover for Permutation Problems
755
The paper is structured as follows: the next section surveys related work and
provides a brief overview on recombination operators for permutation problems
as well as on ant colony optimization. In Section 3 we introduce the new ant-
based crossover operator. The approach is evaluated empirically in Section 4.
The paper concludes in Section 5 with a summary and ideas for future work.
2
Related Work
2.1
Permutation Crossover
Crossover for permutation problems is diﬃcult, and has been discussed in the
literature for a long time. Generally, a crossover operator should create feasible
oﬀspring by combining parental information in a sensible way. What is to be
considered sensible also depends on the application at hand. For example, with
regard to a TSP, it seems more important to preserve edges from the parents
(i.e. direct adjacencies in the permutation), while for a scheduling problem, it is
more important to preserve the general precedence relations (cf. [2]).
Standard one-point or multi-point crossover does not work for permutations,
as it would generate infeasible oﬀspring. The crossover operators suggested in the
literature are numerous and range from simple approaches such as order crossover
[5] or partially mapped crossover [8] to more complicated ones such as distance
preserving crossover [7], edge assembly crossover [15], inner-over crossover [18],
natural crossover [12], or edge recombination crossover [20]. The diﬃculty of
designing a proper permutation crossover even led some researchers to abandon
a permutation representation, and to use e.g. random keys encoding [1] instead.
For TSPs, edge recombination crossover (ERX) seems to be a very eﬀective
crossover operator as it is able to preserve more than 95% of the parental edges
[20]. We will use it later for comparison with ABX and therefore discuss it here
in slightly more detail: Starting from a random city, ERX iteratively constructs
a tour. In each step, it ﬁrst considers the (up to 4) cities that are neighbors (i.e.
connected) to the current location in either of the two parents. If at least one
of those has not been visited so far, it selects the city which has the fewest yet
unvisited other cities as neighbors in the parents. Otherwise, a random successor
is selected. For details, see [20].
There have also been attempts to incorporate problem-speciﬁc knowlege into
the crossover operator. For example, Grefenstette [9] and Tang and Leung [17],
propose variants of ERX, which, when they have to choose between parental
edges, prefer the short ones. Julstrom and Raidl [11] compare several ways for
prefering short edges within an ERX framework, for decisions between parental
edges as well as for decisions when all parental edges are unadmissible. In eﬀect,
the latter approach comes quite close to the the simplest form of ABX proposed
here. Despite of this similarity, it still diﬀers in the way the parental information
and the heuristic information are combined. Furthermore, it lacks the whole
general ACO framework, which allows us to e.g. additionally use ACO as local
optimizer.

756
J. Branke, C. Barz, and I. Behrens
2.2
Ant Colony Optimization
Standard ACO: ACO is an iterative probabilistic optimization heuristic in-
spired by the way real ants ﬁnd short paths between their nest and a food source.
The fundamental principle used by ants for communication is stigmergy, i.e. ants
use pheromones to mark their trails. A higher pheromone intensity suggests a
better path and consequently inclines more ants to take a similar path.
Transferring these ideas to the artiﬁcial scenario of a TSP with n cities, an
ACO approach works as follows (cf. [3,6]): In every iteration, a number of m (ar-
tiﬁcial) ants construct one solution each through all the given n cities. Starting
at a random city, an ant iteratively selects the next city based on heuristic in-
formation as well as pheromone information. The heuristic information, denoted
by ηij, represents a priori heuristic knowledge w.r.t. how good it is to go from
city i to city j. For TSPs, ηij = 1/dij where dij is the distance between city i
and j. The pheromone values, denoted by τij, are dynamically changed by the
ACO algorithm and serve as a kind of memory, indicating which choices were
good in the past.
When having inserted city i in the previous step, the next city j is chosen
probabilistically according to the following probabilities:
pij =
τ α
ij · ηβ
ij

h∈S τ α
ihηβ
ih
,
(1)
where S is the set of cities that have not been visited yet, and α and β are con-
stants that determine the relative inﬂuence of the heuristic and the pheromone
values on the ant’s decision.
After each of the m ants have constructed a solution, the pheromone infor-
mation is updated. First, some of the old pheromone is evaporated on all edges
according to τij →(1 −ρ) · τij, where parameter ρ ∈(0, 1) speciﬁes the evapora-
tion rate. Afterwards, a ﬁxed amount ∆of additional pheromone is ‘deposited’
along all tour edges of the best ant in the iteration. Often, the elitist ant (rep-
resenting the best solution found so far) is also allowed to deposit pheromone
along its path. Each of these positive updates has the form τij →τij + ∆for all
cities i and j connected by an edge of the respective tour. Initially τij = τ0 for
each edge eij.
Population-Based Ant Colony Optimization (PACO): The population-
based ACO (PACO), which has been proposed by Guntsch [10], is a modiﬁca-
tion of the standard ACO. The main diﬀerence is that the pheromone matrix no
longer accumulates the information from all the updates over time, but instead
only contains information about a small number k of solutions explicitly main-
tained in a population. Solution construction is performed probabilistically as in
the standard ACO described above. The main change is the pheromone update,
which is described in more detail in the next paragraph.
In the beginning, the pheromone matrix is initialized with a constant value
τ0, the solution population with a maximal size of k is empty. Then, in each

Ant-Based Crossover for Permutation Problems
757
of the k ﬁrst iterations, the iteration’s best ant is allowed to lay pheromone
(τij →τij + ∆) on all edges of its tour in the pheromone matrix. Futhermore,
the tour is added to the solution population. No pheromone evaporates during
the ﬁrst k iterations. In all subsequent iterations (k + 1), (k + 2), . . ., the best
ant updates as before and is added to the solution population. To keep the
population size constant, another solution of the population (usually the worst
or the oldest) is deleted, and the respective amount of pheromone is subtracted
from the elements of the pheromone matrix corresponding to the deleted solution
(τij →τij −∆). The information of the deleted ant completely disappears in
one iteration. Consequently, the pheromone matrix only preserves information
about the k ants currently in the solution population. Observe that in PACO,
pheromone values never fall below the initial amount of pheromone τ0 and never
exceed τ0 + k∆.
The fact that the pheromone matrix used in PACO represents only a small
number of solutions inspired us to design ABX which shall be described in Sec-
tion 3.
2.3
Hybrids
A couple of authors have suggested to combine the ideas of ACO and EAs in
several ways. Bonabeau et al. [4], for example, propose to optimize ACO param-
eters using an EA, and Miaghikh and Punch [13,14] design a hybrid which uses
a pheromone matrix as well as a complete solution as part of each individual’s
representation. To the authors’ knowledge, no one has ever proposed to use an
ACO algorithm to replace crossover in the way presented in this paper.
Many approaches combine metaheuristics with local search for best results
[7]. But here we are interested in the workings of the speciﬁc crossover opera-
tor proposed. Since we were afraid that local search might blur the eﬀects of
crossover, we decided to concentrate on crossover alone.
3
Ant-Based Crossover
The fundamental idea of ABX is as follows: In each generation of the EA the
parents are regarded as a solution population in the sense of a PACO. Their tour
information is used to generate temporary pheromone matrices. These tempo-
rary pheromone matrices are then used by ants to generate new solutions. The
generated set of solutions is the candidate set for the children returned to the
EA. This creates a number of design options which are discussed in the following:
Number of parents: In principle, the temporary pheromone matrix can be
created from an arbitrary number of parents, ranging from 1 to the population
size p. We denote this parameter ♯parents.

758
J. Branke, C. Barz, and I. Behrens
Pheromone matrix initialization: It is important how much inﬂuence is
given to the parents relative to the basic initialization value τ0 = 1/n. We tested
two basic possibilities:
– Uniform update: each parent deposits a pheromone value of 1/♯parents on
each of the edges along its tour.
– Rank-based update: The amount of pheromone a parent is allowed to deposit
depends on its rank within the set of parent individuals. The individual with
rank i(i = 1 . . . ♯parents) is allowed to deposit
∆i =
b
♯parents −
 2b −2
♯parents
 
i −1
♯parents −1

with b = 1.5, which results in a linear weighting from best to worst.
In both cases, the total amount of pheromone in each row of the pheromone
matrix is equal to 2. Half of it results from the initialization τ0 and half of it
from the parents’ updates.
ACO run: Given a temporary pheromone matrix, we have to decide on the
number of iterations ♯iter we would like to run the ACO, and the number of
solutions m that are constructed in each iteration. In case we decide to run the
ACO for more than one iteration, a pheromone update strategy has to be chosen
as well. We used the standard evaporation strategy in combination with an elite
ant for pheromone update, the update value was set to ∆= 1/♯parents.
Number of children: The general scheme allows us to create any number of
children from a single crossover operation, ranging from one to m · ♯iter. The
number of children is henceforth denoted ♯children, and the best ♯children from
the m · ♯iter generated solutions are returned as children.
4
Empirical Evaluation
For empirical evaluation, we proceed as follows: ﬁrst, we try to ﬁnd a reasonable
set of the basic EA parameter settings. Parameters are tuned independently for
an EA with ERX and an EA with ABX. Then, in a second step we will examine
the eﬀect of the parameters and design choices speciﬁc to the ant-based crossover.
Finally, we will compare our approach to the standard algorithms ACO and EA
with ERX on diﬀerent TSP test instances.
4.1
Test Setup
For the initial parameter tuning, we use the eil101 TSP instance from TSPLIB
[16] which has an optimal tour length of 629. Our basic EA uses a (µ + λ)-
reproduction scheme1 with tournament selection and tournament size of 2. To
keep the number of free parameters small, we ﬁx µ to 50 and only vary λ.
1 λ children are created in every generation, and then compete with the µ individuals
from the last generation’s population for survival into the next generation

Ant-Based Crossover for Permutation Problems
759
Mutation swaps the subtour between two randomly selected cities. The ﬁrst
city is selected at random, and the second city is selected in its neighborhood.
More speciﬁcally, if c1 is the position of the ﬁrst city in the current tour, the
second city’s position is determined using a gaussian distribution with expected
value of c1 and standard deviation σ (result modulo n). The mutation operator is
called with probability mutprob. If an individual is mutated, at least one swap is
performed. Additional swaps are performed with probability repeatSwap, which
results in a geometric distribution of the number of swaps with mean 1/(1 −
repeatSwap). All children are created by crossover, i.e. crossover probability is
equal to 1.0. Speciﬁcally for ABX, parameters α and β are ﬁxed to standard
values 1 and 5 respectively.
Each algorithm terminates after a ﬁxed number of 50, 000 evaluations. Note
that the EA with ERX always generates one child per crossover and performs λ
evaluations per generation of the EA, i.e. the EA runs for 50, 000/λ generations.
With ABX, each solution generated by an ant counts as one evaluation, i.e.
there are (λ/♯children)(m · ♯iter) evaluations per generation of the EA, which
can be signiﬁcantly larger than λ. The number of EA generations is reduced
accordingly. Recalculating the ﬁtness after mutation is not counted towards the
number of evaluations, since this can be done very eﬃciently in constant time
for the given mutation operator. A comparison based on a ﬁxed number of
evaluations implicitly assumes that evaluation is much more time consuming
than the crossover operation. This is true for many problems but not for a TSP.
On the other hand, ﬁxing the runtime makes the result very much dependent
on implementation issues. In our experiments with up to 198 cities, the actual
runtime diﬀerences between the diﬀerent examined approaches were negligible.
We therefore decided to use a ﬁxed number of evaluations as stopping criterion.
In the results reported below, the performance of each parameter set is av-
eraged over 20 runs with diﬀerent random seeds. T-tests with signiﬁcance level
of 0.99 are used to analyze signiﬁcance.
4.2
Basic EA Parameters
The basic EA parameters tuned ﬁrst are the number of oﬀspring per generation
λ, the mutation probability mutprob, the expected length of the swapped tour σ,
and the mutation frequency repeatSwap. With regard to ABX, for the test re-
ported here, we use rank-based update of the parents, two parents per crossover,
and a single ant producing a single child based on the temporary pheromone
matrix (♯children = 1, m = 1, ♯iter = 1).
We test all possible combinations of the parameter settings listed in Table 1.
The settings that perform best for ERX are λ = 50, mutprob = 0.8, σ = 15 and
repeatSwap = 0.1 which yield a solution quality of 691.8.
For the EA with ABX, λ = 1 performs slightly (but not signiﬁcantly) better
than λ = 24. Nevertheless, we chose λ = 24 for further testing, since λ = 1
restricts the testing of child-parent combinations too much. The eﬀect of the
mutation parameters seem to be relatively small. We select the following pa-
rameters for future tests: mutprob = 0.25, σ = 1 and repeatSwap = 0.1. It is

760
J. Branke, C. Barz, and I. Behrens
Table 1. Tested parameter values for reproduction and mutation, settings chosen for
future tests are bold.
ERX
ABX
λ
1, 25, 50
1, 24, 50
mutprob
0.25, 0.6, 0.8, 1.0
0.0, 0.25, 0.5, 0.75
σ
3, 10, 15
1, 3, 10
repeatSwap
0.1, 0.4, 0.5, 0.6
0.0, 0.1, 0.5
interesting to note that the results without mutation (mutprob = 0) are almost
as good. The fact that mutation plays a minor role in ant-based crossover is not
really surprising, because variation is introduced implicitly as part of crossover
by the way ants construct their tours probabilistically.
4.3
Parameters for Ant-Based Crossover
In this section, we analyze the inﬂuence of the parameters and design choices
speciﬁc to ABX. For that purpose, we test all feasible combinations of the param-
eters speciﬁed in Table 2. Evaporation rate ρ is set to 0.1 where needed. Addition-
ally, we test a large number of combinations with ♯children = 8, ♯parents = 1,
♯parents = 50 as well as ♯iter = 15.
Table 2. Tested parameter values for ABX
parameter
values tested
♯parents
2, 4, 8
parentalUpdate
constant, rank-based
♯children
1, 2, 24
m
1, 2, 12, 24
♯iter
1, 2 or 5
Overall, the approach seems to be rather robust with respect to the parameter
settings chosen. The following paragraphs outline the main results for the ﬁve
examined parameters. Results with respect to a speciﬁc parameter are averaged
over all settings of the other parameters (as long as they existed for all settings
of the examined parameter).
Number of parents: Table 3 shows the best tour length over all performed
test runs classiﬁed according to the number of parents and the parental update
strategy. As can be seen, using two or four parents for crossover is better than
only one or more than eight. The diﬀerences are statistically signiﬁcant. Looking
at the convergence graphs (not shown), it becomes apparent that increasing the
number of parents slows down convergence.

Ant-Based Crossover for Permutation Problems
761
Table 3. Test results depending on the number of parents and the parental update
parental update
all
constant
rank-based
♯parents
mean
std. error
mean
mean
1
639.61
0.2234
639.61
639.61
2
636.38
0.2095
636.36
636.72
4
636.38
0.1820
636.89
636.33
8
637.68
0.2596
637.91
637.50
50
641.27
0.4559
642.60
639.94
all combinations
637.93
0.1736
638.30
637.56
Parental update: Unsurprisingly, rank-based parental update leads to faster
convergence than uniform parental update, due to the additional inﬂuence of
good parents (convergence curves are not shown due to space limitations). As
can be seen in Table 3, the diﬀerence of the two update strategies w.r.t. the ob-
tained tour length is rather small, but becomes more pronounced in combination
with a large number of parents. As has been noted in the previous paragraph,
increasing the number of parents slows down convergence. This eﬀect should be
counterbalanced to some degree e.g. by using the rank-based parental update.
Number of children per crossover: The 24 children generated per genera-
tion of the EA can be produced by calling the ABX once with ♯iter · m > 24.
Alternatively, one may call the ABX several times, thereby splitting the total of
24 children to be generated evenly among the ABXs. Our test results suggest
that it is signiﬁcantly better to generate only a few children per crossover and
rather call the ABX more than once with a smaller number of children each. In
other words, it seems to be important that the children are generated based on
the information from diﬀerent sets of parents. The reason may be that if all 24
children are based on one temporary pheromone matrix, they might be so similar
that they lead to early convergence of the EA. Overall, test runs converge slower
with decreasing ♯children, but to a better solution (cf. Figure 1). This eﬀect is
strengthened with increasing ♯iter (see below).
Number of ants per iteration: Increasing the number of ants m per ACO
iteration implicitly leads to better children. On the other hand, the number of
ﬁtness evaluations required per generated child is increased, meaning that the
EA can only run for fewer generations. Our tests show that the parameter has
little inﬂuence on the ﬁnal results, although convergence is slowed down a bit
with increasing m. Apparently, the eﬀect of improved children is not able to
outweigh the reduction of EA generations, at least not given the limit of 50,000
evaluations (cf. Table 4). For our test environment, between two and twelve ants
per iteration seem to perform best.

762
J. Branke, C. Barz, and I. Behrens
640
645
650
655
660
5000
20000
35000
50000
mean tour length
number of evaluations
1 child
2 children
8 children
24 children
Fig. 1. Convergence behavior of runs with diﬀerent numbers of children per crossover.
Number of ACO iterations: Similar to increasing the number of ants per
iteration, increasing the number of iterations per ACO improves the quality of
the generated children at the expense of requiring a larger number of ﬁtness
evaluations. Although the additional search should be more structured, when
comparing Tables 4 and 5, little diﬀerence can be observed regarding the eﬀect
of these two parameters. According to our test results, two or ﬁve iterations of
ants yield the shortest tours. These two settings are signiﬁcantly better than
only a single iteration (cf. Table 5).
Note that the standard error of the results for 15 iterations is relatively
high. As can be seen in Figure 2, this high variance can be traced back to two
diﬀerent eﬀects. First of all, in case all children of one generation are generated
from a single ACO run, 15 generations lead to premature convergence after only
15, 000 −20, 000 evaluations and very poor results. The eﬀect of many children
generated from a single temporary pheromone matrix, as has been described
above, is emphasized by running many ACO iterations, since the pheromone
matrix converges and thus the children become even more similar. If few children
are generated, two cases can be distinguished: If m is large, the number of
evaluations per child becomes so high that the runs are far from convergence
given the maximum of 50,000 evaluations, and consequently the results are rather
poor. On the opposite , the algorithm converges and the results are very good if m
Table 4. Test results depending on the
number of ants per ACO iteration
m
mean
std. error
1
636.85
0.3200
2
636.33
0.2619
12
636.14
0.2816
24
637.79
0.4289
Table 5. Test results depending on the
number of ACO iterations
♯iter
mean
std. error
1
637.27
0.2458
2
636.54
0.2292
5
636.62
0.3122
15
637.53
0.6761

Ant-Based Crossover for Permutation Problems
763
635
640
645
650
655
660
665
670
675
680
5000
20000
35000
50000
mean tour length
number of evaluations
24 children
set A
set B
Fig. 2. Convergence behavior of runs with 15 generations of ants. The ﬁrst line has
24 children per ABX. Sets A and B are averages over runs with ≤12 children per
operator, set A over those with less than 4000 evaluations per crossover, set B over
those with more than 4000 evaluations per crossover.
is suﬃciently small. On the whole, increasing the number of ACO iterations leads
to promising solutions given that the algorithm has suﬃcient time to converge
and the number of children per population is small.
Summary: To sum up, the EA with ABX is quite robust with respect to the
examined parameter settings. As is often the case, the ideal parameter settings
probably depend on the time available for computation. We have demonstrated
that the number of evaluations per crossover operator (m · #iter) plays an im-
portant role. If this number is too large, the algorithm will not converge in the
given time frame. Apparently, in most cases the eﬀect of local optimization due
to the larger number of tours evaluated cannot outweigh the reduction of gen-
erations performed by the EA. This stresses the importance of the EA heuristic
and clariﬁes that ABX avails itself of both algorithms and is more than a splitted
ACO. For the tests reported in the next section, we use two parents per ABX
with uniform update and allow 12 ants to run for 5 iterations to produce one
child.
4.4
Comparison of ABX with ERX, and ACO
To compare the performance of our ABX with the other heuristics, we carry
out test runs on the following three benchmark problems from the TSPlib [19]:
eil101 with 50, 000 evaluations, kroA150 with 75, 000 evaluations and d198 with
100, 000 evaluations (linearly increasing the maximum allowed number of eval-
uations with the number of cities in the problem). Since in practice, it is not
possible to perform extensive parameter tuning when solving a new problem in-
stance, for all heuristics we use the same parameter settings that have proven
successful for eil101 respectively. The results are summarized in Table 6.

764
J. Branke, C. Barz, and I. Behrens
Table 6. Comparison of the ant-based crossover with other approaches
Problem Instance
Heuristic
eil101
kroA150
d198
ERX
691.8
32985.85
18671.8
Standard ACO
638.5
27090.76
16123.36
Ant-Based Crossover
632.5
26807.8
16080.8
Optimum
629
26524
15780
As can be seen, our EA with ABX clearly outperforms the EA with ERX in all
tested problem instances. It also performs signiﬁcantly better than pure ACO2.
In addition, we can compare ABX to the relatively similar weight-biased edge-
crossover reported in [11]. For the tested kroA150 problem, Julstrom and Raidl
report an average result of 27081 for their best strategy after 150,000 evaluations,
which is clearly inferior to our result of 26807.8 after 75,000 evaluations (at least
when ignoring other factors inﬂuencing computational complexity).
5
Conclusion and Future Work
In this paper we introduced a new crossover operator for permutation problems
which draws on ideas from ant colony optimization (ACO). With the suggested
ant-based crossover (ABX), it is straightforward to integrate problem-speciﬁc
heuristic knowledge and local ﬁne-tuning into the crossover operation. First em-
pirical tests on the TSP have shown that the approach is rather robust with
respect to parameter settings, and that it signiﬁcantly outperforms an EA with
edge recombination crossover, as well as pure ACO.
Given these excellent results, the performance of the ABX should also be
tested on other permutation problems such as scheduling or the quadratic as-
signment problem. A more thorough comparison of the computational complex-
ities of the diﬀerent approaches would also be desirable. Finally, for best results,
a hybridization of our approach with local optimizers like Lin-Kernighan should
be tested.
References
1. J. C. Bean. Genetic algorithms and random keys for sequencing and optimization.
ORSA Journal on Computing, 6(2):154–160, 1994.
2. C. Bierwirth, D.C. Mattfeld, and H. Kopfer. On permutation representations for
scheduling problems. In H.-M. Voigt, editor, Parallel Problem Solving from Nature,
volume 1141 of LNCS, pages 310–318. Springer, Berlin, 1996.
3. E. Bonabeau, M. Dorigo, and G. Theraulaz. Swarm intelligence: from natural to
artiﬁcial systems. Oxford University Press, 1999.
2 m = 15, α = 1, β = 5, ρ = 0.01, τ0 = 0.5, ﬁx update of ∆= 0.05 for best ant of
iteration and elite ant, and minimal pheromone value of τmin = 0.001.

Ant-Based Crossover for Permutation Problems
765
4. H. M. Botee and E. Bonabeau. Evolving ant colonies. Advanced Complex Systems,
1:149–159, 1998.
5. L. Davis. Applying adaptive algorithms to epistatic domains. In International
Joint Conference on Artiﬁcial Intelligence, pages 162–164, 1985.
6. M. Dorigo and G. Di Caro.
The ant colony optimization meta-heuristic.
In
D. Corne, M. Dorigo, and F. Glover, editors, New Ideas in Optimization, pages
11–32. McGraw-Hill, 1999.
7. B. Freisleben and P. Merz. New genetic local search operators for the traveling
salesman problem. In Hans-Michael Voigt, Werner Ebeling, Ingo Rechenberg, and
Hans-Paul Schwefel, editors, Parallel Problem Solving from Nature, volume 1141,
pages 890–899, Berlin, 1996. Springer.
8. D. E. Goldberg and R. Lingle.
Alleles, loci, and the TSP.
In J. J. Grefen-
stette, editor, First International Conference on Genetic Algorithms, pages 154–
159. Lawrence Erlbaum Associates, 1985.
9. J. J. Grefenstette.
Incorporating problem speciﬁc knowledge into genetic algo-
rithms.
In Genetic Algorithms and Simulated Annealing, pages 42–60. Morgan
Kaufmann, 1987.
10. M. Guntsch and M. Middendorf.
A population based approach for ACO.
In
European Workshop on Evolutionary Computation in Combinatorial Optimization,
volume 2279 of LNCS, pages 72–81. Springer, 2002.
11. B. A. Julstrom and G. R. Raidl. Weight-biased edge-crossover in evolutionary al-
gorithms for two graph problems. In G. Lamont, J. Carroll, H. Haddad, D. Morton,
G. Papadopoulos, R. Sincovec, and A. Yfantis, editors, 16th ACM Symposium on
Applied Computing, pages 321–326. ACM Press, 2001.
12. S. Jung and B.-R. Moon.
Toward minimal restriction of genetic encoding and
crossovers for the two-dimensional Euclidean TSP. IEEE Transactions on Evolu-
tionary Computation, 6(6):557–565, 2002.
13. V. V. Miagkikh and W. F. Punch. An approach to solving combinatorial optimiza-
tion problems using a population of reinforcement learning agents. In Genetic and
Evolutionary Computation Conference, pages 1358–1365, 1999.
14. V. V. Miagkikh and W. F. Punch. A generalized approach to handling parameter
interdependencies in probabilistic modeling and reinforcement learning optimiza-
tion algorithms. In Workshop on Frontiers in Evolutionary Algorithms, 2000.
15. Y. Nagata and S. Kobayashi.
Edge assembly crossover: A high-power genetic
algorithm for the traveling salesman problem. In T. B¨ack, editor, International
Conference on Genetic Algorithms, pages 450–457. Morgan Kaufmann, 1997.
16. G. Reinelt. TSPLIB - a travelling salesman problem library. ORSA Journal on
Computing, 3:376–384, 1991.
17. A.Y.-C. Tang and K.-S. Leung. A modiﬁed edge recombination operator for the
travelling salesman problem. In Parallel Problem Solving from Nature II, volume
866 of LNCS, pages 180–188, Berlin, 1994. Springer.
18. G. Tao and Z. Michalewicz. Evolutionary algorithms for the TSP. In A. E. Eiben,
T. B¨ack, M. Schoenauer, and H.-P. Schwefel, editors, Parallel Problem Solving from
Nature, volume 1498 of LNCS, pages 803–812. Springer, 1998.
19. http://www.iwr.uni-heidelberg.de/groups/comopt/software/TSPLIB95/
index.html.
20. D. Whitley, T. Starkweather, and D’A. Fuquay. Scheduling problems and traveling
salesman: The genetic edge recombination operator. In J. Schaﬀer, editor, Inter-
national Conference on Genetic Algorithms, pages 133–140. Morgan Kaufmann,
1989.

Selection in the Presence of Noise
J¨urgen Branke and Christian Schmidt
Institute AIFB, University of Karlsruhe, 76128 Karlsruhe, Germany
{branke|csc}@aifb.uni-karlsruhe.de
Abstract. For noisy optimization problems, there is generally a
trade-oﬀbetween the eﬀort spent to reduce the noise (in order to
allow the optimization algorithm to run properly), and the number of
solutions evaluated during optimization. However, for stochastic search
algorithms like evolutionary optimization, noise is not always a bad
thing. On the contrary, in many cases, noise has a very similar eﬀect to
the randomness which is purposefully and deliberately introduced e.g.
during selection. Using the example of stochastic tournament selection,
we show that the noise inherent in the optimization problem should be
taken into account by the selection operator, and that one should not
reduce noise further than necessary.
Keywords: Noise, tournament selection, stochastic ﬁtness
1
Introduction
Many real-world optimization problems are noisy, i.e. a solution’s quality (and
thus the ﬁtness function) is a random variable. Examples include all applications
where the ﬁtness is determined by a stochastic computer simulation, or where ﬁt-
ness is measured physically and prone to measuring error. Researchers have long
argued that evolutionary algorithms (EAs) should be relatively robust against
noise (see e.g. [FG88]), and recently a number of publications have appeared
which support that claim at least partially [MG96,AB00a,AB00b,AB03].
For most noisy optimization problems, the uncertainty in ﬁtness evaluation
can be reduced by sampling an individual’s ﬁtness several times and using the
average as estimate for the true mean ﬁtness. Sampling n times reduces a random
variable’s standard deviation by a factor of √n, but on the other hand increases
the computation time by a factor of n. Thus, there is a generally perceived trade-
oﬀ: either one can use relatively exact estimations but only evaluate a small
number of individuals (because a single estimation requires many evaluations),
or one can let the algorithm work with relatively crude ﬁtness estimations, but
allow for more evaluations (as each estimation requires less eﬀort). Generally,
noise is considered harmful, as it may mislead the optimization algorithm. The
main issue is probably the selection step: If due to the noise, a bad individual
is evaluated better than it actually is, and/or a good individual is evaluated
worse than its true ﬁtness, the EA may wrongly select the worse individual
although (according to the algorithmic design) it should have selected the better
individual. Clearly, if such errors happen too frequently, optimization stagnates.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 766–777, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Selection in the Presence of Noise
767
However, noise is not always a bad thing, on the contrary. EAs are ran-
domized search algorithms, which use deliberate randomness to purposefully
introduce errors into the selection process, primarily in order to get out of local
minima. Therefore, in this paper we argue that it should be possible to accept
the noise inherent in the optimization problem and to use it to (at least par-
tially) replace the randomness in the optimization algorithm. As a result, it is
possible to get the optimization algorithm to behave closer to its behavior on
deterministic problems, even without excessive sampling. Furthermore, we will
demonstrate that, depending on the ﬁtness values and variances, noise aﬀects
some tournaments much stronger than others. As a consequence, we suggest a
simple but eﬀective resampling strategy to adapt the sample size to the speciﬁc
tournament, allowing us to again get closer to the algorithm’s behavior in a
deterministic setting, while drastically reducing the number of samples required.
The paper is structured as follows: In Section 2, we survey some related work
on EAs applied to noisy optimization problems, followed by a brief description of
stochastic tournament selection. Section 4 demonstrates the eﬀect noise has on
tournament selection, and describes two ways to integrate a possible sampling
error into the selection procedure. The idea of adapting not only the selection
probability but also the sample size is discussed in Section 5. The paper concludes
with a summary and some ideas for future work.
2
Related Work
The application of EAs in noisy environments has been the focus of many re-
search papers. There are several papers that have looked at the trade-oﬀbetween
population size and sample size to estimate an individual’s ﬁtness, with some-
times conﬂicting results. Fitzpatrick and Grefenstette [FG88] conclude that for
the genetic algorithm studied, it is better to increase the population size than
the sample size. On the other hand, Beyer [Bey93] shows that for a (1, λ) evo-
lution strategy on a simple sphere, one should increase the sample size rather
than λ. Hammel and B¨ack [HB94] conﬁrm these results and empirically show
that it also doesn’t help to increase the parent population size µ. Finally, Arnold
and Beyer [AB00a,AB00b] show analytically that for the simple sphere, increas-
ing the parent population size µ is helpful in combination with intermediate
multirecombination. Miller [Mil97,MG96] has developed some simpliﬁed theo-
retical models which allow to simultaneously optimize the population size and
the sample size. A good overview of theoretical work on EAs applied to noisy
optimization problems can be found in [Bey00] or [Arn02].
All papers mentioned so far assume that the sample size is ﬁxed for all in-
dividuals. Aizawa and Wah [AW94] were probably the ﬁrst to suggest that the
sample size could be adapted during the run, and suggested two adaptation
schemes: increasing with the generation number, and higher sample size for in-
dividuals with higher estimated variance. Albert and Goldberg [AG01] look at a
slightly diﬀerent problem, but also conclude that the sample size should increase
over the run. For (µ, λ) or (µ + λ) selection, Stagge [Sta98] has suggested basing

768
J. Branke and C. Schmidt
the sample size on an individual’s probability to be among the µ best (and thus
to survive to the next generation).
Branke et al. [Bra98,BSS01] and Sano and Kita [SK00,SKKY00] propose
taking the ﬁtness estimations of neighboring individuals into account when es-
timating an individual’s ﬁtness. This improves the estimation without requiring
additional samples.
Finally, another related subject is that of searching for robust solutions, where
instead of a noisy ﬁtness function the decision variables are perturbed (cf. [TG97,
Bra98,Bra01]).
3
Stochastic Tournament Selection
Stochastic tournament selection (STS) [GD91] is a rather simple selection scheme
where two individuals are randomly chosen from the population, and then the
better is selected with probability (1 −γ). If individuals are sorted from rank 1
(best) to rank m (worst), this results in a linearly decreasing selection probability
for an individual on rank i, with the slope of the line being determined by the
selection probability (1 −γ).
4
Selection Based on a Fixed Sample Size
Selecting the better of two individuals with probability (1 −γ) in a noisy envi-
ronment can be achieved in two fundamental ways: The standard way would be
to eliminate the noise as much as possible by using a large number of samples,
and then selecting the better individual with probability (1 −γ). The noise-
adapted selection proposed here has a diﬀerent philosophy: instead of eliminat-
ing the noise and then artiﬁcially introducing randomness, we propose accepting
a higher level of noise, and only add a little bit of randomness to achieve the
desired behavior.
In the following, we will start with the standard STS, demonstrate the conse-
quences in a noisy environment, and then develop a simple and a more complex
model to get closer to the ideal noise-adapted selection.
4.1
Basic Notations
Let us denote the two individuals to be compared as x and y. If the ﬁtness is
noisy, the ﬁtness of individual x (y) is a random variable Fx (Fy) with Fx ∼
N(µx, σ2
x) (Fy ∼N(µy, σ2
y))1. If µx > µy, we would like to select individual x
with probability (1−γ) and vice versa. However, µx and µy are unknown, we can
only estimate them by sampling each individual’s ﬁtness a number of n times
1 Note that it will be suﬃcient to assume that the average diﬀerence obtained from
sampling the individuals’ ﬁtnesses n times is normally distributed. This is certainly
valid if each individual’s ﬁtness is normally distributed, but also independent of the
actual ﬁtness distributions for large enough n (central limit theorem).

Selection in the Presence of Noise
769
and using the averages ¯fx and ¯fy as estimators for the ﬁtnesses, and the sample
variances s2
x and s2
y as estimators for the true variances.
If the actual ﬁtness diﬀerence between the two individuals is denoted as
δ = µx −µy, the observed ﬁtness diﬀerence D = ¯fx −¯fy is again a random
variable D ∼N(δ, σ2
d). The variance of D depends on the number of samples
drawn from each individual, n, and can be calculated as σ2
d = (σ2
x + σ2
y)/n. A
speciﬁc realization of the observed ﬁtness diﬀerence is named d. Furthermore, we
will need a standardized observed ﬁtness which we deﬁne as d∗= d/

s2
d where
s2
d = (s2
x + s2
y)/n is the unbiased estimated standard deviation of the ﬁtness
diﬀerence. The corresponding true counterpart is δ∗= δ/σd. Note that non-
linear transformations of unbiased estimators are no longer unbiased, therefore
d∗is a biased estimator for δ∗.
While γ is the desired selection probability for the truely worse individual,
we denote with β the implemented probability for choosing the worse individual
based on the estimated standardized ﬁtness diﬀerence d∗, and ξ(δ∗, β) the actual
selection probability for the better individual given a true standardized ﬁtness
diﬀerence of δ∗.
4.2
Standard Stochastic Tournament Selection
The simplest (and standard) way to apply STS would be to ignore the uncer-
tainty in evaluation by making the following assumption:
Assumption: The observed ﬁtness diﬀerence is equal to the actual ﬁtness
diﬀerence, i.e. d = δ.
As a consequence, individual x is selected with probability (1 −β) = (1 −γ)
if d ≥0 and with probability β = γ if d < 0.
However, there can be two sources of error: Either we observe a ﬁtness dif-
ference d > 0 when actually δ < 0, or vice versa. The corresponding error
probability α can be calculated as
α =



P(D > 0) = 1 −Φ

−δ
σd

= Φ

δ
σd

:
δ ≤0
P(D < 0) = Φ

−δ
σd

:
δ > 0
= Φ
−|δ|
σd

= Φ (−|δ∗|)
(1)
with Φ being the cumulative distribution function for a standard gaussian.
The overall selection probability for individual x can then be calculated as
ξ = P(D > 0)(1 −β) + P(D < 0)β
= (1 −α)(1 −β) + αβ
(2)
Example: To visualize the eﬀect of the error probability on the actual se-
lection probability ξ, let us consider an example with σ2
x = σ2
y = 10, n = 20 and
γ = 0.2. The actual selection probability for individual x depending on δ∗can be
determined by a Monte Carlo simulation. We did this in the following way: For

770
J. Branke and C. Schmidt
a given δ∗, we generated 100,000 realizations of d∗according to d∗=
¯
fx−¯
fy
√
(s2
x+s2
y)/n
based on Fx ∼N(0, σ2
x), Fy ∼N(−δ∗σd, σ2
y). For each observed d∗, we select x
with probability (1 −β) if d∗> 0 and with probability β otherwise. The actual
selection probability ξ(δ∗, β) is then the fraction of times x has been selected.
0.5
0.6
0.7
0.8
0.9
0
1
2
3
4
5
6
7
8
standard
δ∗
ξ
Fig. 1. True selection probability of individual x depending on the actual standardized
ﬁtness diﬀerence δ∗. The dotted line represents the desired selection probability (1−γ).
Figure 1 depicts the resulting true selection probability of individual x de-
pending on the actual standardized ﬁtness diﬀerence δ∗. The dotted line corre-
sponds to the desired behavior in the deterministic case, the bold line labeled
“standard” is the actual selection probability due to the noise. As can be seen,
the actual selection probability for the better individual largely depends on the
ratio δ∗of the ﬁtness diﬀerence δ and the amount of noise measured as σd.
While it corresponds to the desired selection probability of (1 −γ) for δ∗> 3,
it approaches 0.5 for δ∗→0. The latter fact is unavoidable, since for δ∗→0
it becomes basically impossible to determine the better of the two individuals.
The interesting question is how quickly ξ approaches 1 −γ, and whether this
behavior can be improved. Note that we only show the curves for δ∗≥0 (as-
suming without loss of generality that µx > µy). For δ∗< 0 the curve would be
symmetric to (0, 0.5).
In previous papers, it has been noted that the eﬀect of noise on EAs is
similar to a smaller selection pressure (e.g. [Mil97]). Figure 1 demonstrates that
this is not entirely true for STS. A lower selection pressure in form of a higher
γ would change the level of the dotted line, but it would still be horizontal, i.e.
the selection probability for the better individual would be independent of the
actual ﬁtness diﬀerence. With noise, only the tournaments between individuals

Selection in the Presence of Noise
771
of similar ﬁtness are aﬀected. Hence, a dependence on the actual ﬁtness values
is introduced which somehow contradicts the idea of rank-based selection.
4.3
A Simple Correction
If we know that our conclusion about which of the two individuals has a better
ﬁtness is prone to some error, it seems straightforward to take this error prob-
ability into account when deciding which individual to select. Instead of always
selecting the better individual with probability (1−γ), we could try to replace γ
by a function β(d∗) which depends on the standardized observed diﬀerence d∗.
Let us make the following assumption:
Assumption: It is possible to accurately estimate the error probability α.
Then, since we would like to have an overall true selection probability of
(1 −γ), an appropriate β-function could be derived as
(1 −α)(1 −β) + αβ
!= (1 −γ)
(3)
1 −β −α + αβ + αβ = (1 −γ)
β(−1 + 2α) = (1 −γ) −1 + α
β = γ −α
1 −2α.
(4)
β is a probability and can not be smaller than 0, i.e. the above equation assumes
α ≤γ < 0.5. For α > γ we set β = 0.
Unfortunately, α can not be calculated using Equation 1, because we don’t
know either δ nor σd.
0.5
0.6
0.7
0.8
0.9
0
1
2
3
4
5
6
7
8
standard
corr
δ∗
ξ
Fig. 2. True selection probability of individual x depending on the actual standardized
ﬁtness diﬀerence δ∗. The dotted line represents the desired selection probability (1−γ).

772
J. Branke and C. Schmidt
It seems straightforward then to estimate δ by the observed diﬀerence d,
and σ2
d by the observed variance s2
d. Then, α is estimated as ˆα = Φ(−|d|/sd) =
Φ(−|d∗|), which is only a biased estimator due to the non-linear transformations.
Nevertheless, this may serve as a reasonable ﬁrst approximation of an optimal β-
function. Figure 3 visualizes this β-function (labeled as “corr”). As can be seen,
the probability to select the worse individuals decreases when the standardized
diﬀerence d∗becomes small, and is 0 for |d∗| < −Φ−1(γ) (i.e. the observed better
individual is always selected if the observed standardized ﬁtness diﬀerence d∗is
small). Assuming the same parameters as in the example above, the resulting
true selection probabilities ξ(δ∗, β(.)) are depicted in Figure 2 (labeled as “corr”).
The true selection probability approaches the desired selection probability faster
than with the standard approach, but then it overshoots before it converges
towards (1 −γ). Nevertheless, the approximation is already much better than
the standard approach (assuming a uniform distribution of δ∗).
4.4
Bootstrapping
The β-function proposed above can be further improved by bootstrapping
[Efr90]. This method compares the observed selection probabilities p given the
current β function with the desired selection probabilities, and then reduces β
where the selection probability is too low, and increases β where the selection
probability is too high. The observed selection probabilities ξ(δ∗, β(.)) have to
be estimated by Monte Carlo simulation, generating realisations of d∗and then
selecting according to β(d∗). Unfortunately, the distribution of d∗depends on
the variance σ2
d of the observed ﬁtness diﬀerence which is unknown. Therefore,
in this approach we make the following simplifying assumption:
Assumption: The estimated variance of the diﬀerence corresponds to the
true variance of the diﬀerence, i.e. s2
d = σ2
d.
From that it follows that d∗is normally distributed according to N(δ∗, 1).
More speciﬁcally, our bootstrapping approach starts with an initial β0(z)
which corresponds to the β function deﬁned in the section above. Then, it iter-
atively adapts beta according to
βt+1(z) = βt(z) + ξ(z, βt(.)) −(1 −γ).
(5)
This procedure can be iterated until one is satisﬁed with the outcome.
The resulting β-function is depicted in Figure 3. At ﬁrst sight, the strong
ﬂuctuations seems surprising. However, a steeper ascent of the true selection
probability can only be achieved by keeping β(d∗) = 0 for as long as possible. The
resulting overshoot then has to be compensated by a very high β etc. such that in
the end, an oscillating acceptance pattern emerges as optimal. The corresponding
true selection probabilities ξ(δ∗) are shown in Figure 4. As can be seen, despite
the oscillating β-function, this curve is very smooth, and much closer to the
actually desired selection probability of γ resp. (1 −γ) than either the standard
approach of ignoring the noise, or the ﬁrst approximation of an appropriate
β-function presented in the previous section.

Selection in the Presence of Noise
773
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
standard
corr
bootstrap
d∗
β
Fig. 3. The probability to select the worse individual (β-function), depending on the
observed standardized ﬁtness diﬀerence d∗. Results of the diﬀerent approaches.
0.5
0.6
0.7
0.8
0.9
0
1
2
3
4
5
6
7
8
standard
corr
bootstrap
bound
δ∗
ξ
Fig. 4. True selection probability of individual x depending on the actual standardized
ﬁtness diﬀerence δ∗. The line denoted by “bound” is an idealized curve which depicts
a limit to how close one can get to the desired selection probability. The dotted line
represents the desired selection probability (1 −γ).
Even though the bootstrapping method yields a much better approximation
to the desired selection probability than the other two approaches, it could per-
haps be further improved by basing it not only on d∗but on all three observed
variables, namely d, σ2
x, and σ2
y. However, we expect that the additional im-
provement would be rather small. Furthermore, there is a bound to how close
one can get to the desired selection probability: the steepest possible ascent of
the true selection probability is clearly obtained if the individual with the higher

774
J. Branke and C. Schmidt
observed ﬁtness is always selected. However, as long as α exceeds γ, the resulting
true selection probability would still be below the desired selection probability.
The corresponding steepest ascent curve is also shown in Figure 4 and denoted
as “bound”.
Instead of trying to further improve the estimation, we will now turn to the
idea of drawing additional samples if the probability for a selection error is high.
5
Resampling
From the above discussion, it is clear that the deviation from actual selection
probability to desired selection probability is only severe for small values of δ/σd,
i.e. if the individuals have similar ﬁtness and/or the noise is large.
Therefore, we now attempt to counteract that problem by adapting the num-
ber of samples to the expected error probability, i.e. by drawing a large number
of samples whenever we assume that the selection error would be high and vice
versa. We propose to do that in the following way: Starting with a reduced num-
ber of 10 samples for every individual, we calculate d∗. If |d∗| ≥ϵ where ϵ is
a constant, we stop and use d∗to decide which individual to select. Otherwise,
we repeatedly draw another sample for each of the two individuals until either
|d∗| ≥ϵ or the total number of samples exceeds a maximum number N. For
our experiments, we set N = 100 and ϵ = 1.33, which approximately yields an
error probability of 1% if δ∗= 1 assuming that d∗is normally distributed as
d∗∼N(δ∗, 1), i.e. if δ∗= 1, there is only a 1% chance that we will observe a
distance d < 0.
For our standard example with σ2
x = σ2
y = 10 and γ = 0.2, the above sampling
scheme results in an average number of samples depending on δ∗as depicted in
Figure 5. For small standardized distances d∗, the average number of samples is
quite high, but it drops quickly and approaches the lower limit of 20 for δ∗> 3.
Depending on the distribution of δ∗in a real EA, this sampling scheme is thus
able to achive tremendous savings compared to the ﬁxed sampling rate of 20
samples per individual (40 samples in total). Furthermore, the actual selection
probabilities using this sampling scheme are much closer to the desired selection
probability than if a ﬁxed number of samples is used. The two sampling schemes
in combination with standard STS are compared in Figure 6.
Just as for the ﬁxed sample size, we can apply bootstrapping also to the
adaptive sampling scheme. The resulting β-function and selection probabilities
are depicted in Figures 7 and 8. The resulting beta-function is much smoother
than the one obtained for the ﬁxed sampling scheme. Also, although there is
still a clear beneﬁt of bootstrapping with respect to the deviation of ξ from the
desired (1−γ), the improvement over standard STS is signiﬁcantly smaller than
with a ﬁxed sample size. This is probably because due to the smaller initial
sample size in combination with the resampling scheme used, our assumption
that D∗is normally distributed may be less appropriate.

Selection in the Presence of Noise
775
0
20
40
60
80
100
0
1
2
3
4
5
6
7
8
adaptive sample size
fixed sample size
n
δ∗
Fig. 5. Average sample size depending
on the actual standardized ﬁtness diﬀer-
ence δ∗, with the ﬁxed sampling scheme
(dashed line) and the adaptive sampling
scheme (solid line).
0.5
0.6
0.7
0.8
0.9
0
1
2
3
4
5
6
7
8
standard fixed
standard adaptive
δ∗
ξ
Fig. 6. Actual sampling probability de-
pending
on
the
actual
standardized
ﬁtness
diﬀerence
δ∗,
for
the
stan-
dard stochastic tournament selection
with ﬁxed and with adaptive sampling
scheme.
0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
10
standard
bootstrap
d∗
β
Fig. 7. β-function derived by bootstrap-
ping for the case of an adaptive sample
size.
0.5
0.6
0.7
0.8
0.9
0
1
2
3
4
5
6
7
8
standard adaptive
bootstrap
δ∗
ξ
Fig. 8. Comparison of the actual sam-
pling probability depending on the ac-
tual standardized ﬁtness diﬀerence δ∗
for the standard STS and the bootstrap-
ping approach, when using the adaptive
sampling scheme.
6
Conclusion
In this paper, we have argued that the error probability due to a noisy ﬁtness
function should be taken into account in the selection step. At the example
of stochastic tournament selection, we have demonstrated that it is possible to
obtain a much better match between actual and desired selection probability for
an individual.
In a ﬁrst step, we have derived two models which determine the selection
probability for the better individual depending on the observed ﬁtness diﬀer-
ence. The simple model was based on some simplifying assumptions regarding

776
J. Branke and C. Schmidt
the distribution of the error probability; the second model was based on boot-
strapping.
In a second step, we looked at a diﬀerent sampling scheme, namely adapting
the number of samples to the expected error probability. That way, a pair of
similar individuals is sampled much more often than a pair of individuals with
very diﬀerent ﬁtness values. This approach also greatly improves the accuracy
of the actual selection probability. Additionally, depending on the distribution
of ﬁtness diﬀerences in an actual EA run, it will signiﬁcantly reduce the number
of samples required.
We are currently exploring a number of diﬀerent extensions. For one, it should
be relatively straightforward to extend our framework to other selection schemes
and even to other heuristics like simulated annealing. Furthermore, we intend to
improve the adaptive sampling scheme by using statistical test theory.
Acknowledgements. We would like to thank David Jones for pointing us to
the bootstrapping methodology, and the anonymous reviewers for their helpful
comments.
References
[AB00a]
D. V. Arnold and H.-G. Beyer. Eﬃciency and mutation strength adap-
tation of the (µ/µi, λ)-es in a noisy environment.
In Schoenauer et al.
[SDR+00], pages 39–48.
[AB00b]
D. V. Arnold and H.-G. Beyer. Local performance of the (µ/µi, λ)-es in a
noisy environment. In W. Martin and W. Spears, editors, Foundations of
Genetic Algorithms, pages 127–142. Morgan Kaufmann, 2000.
[AB03]
D. V. Arnold and H.-G. Beyer. A comparison of evolution strategies with
other direct search methods in the presence of noise. Computational Opti-
mization and Applications, 24:135–159, 2003.
[AG01]
L. A. Albert and D. E. Goldberg. Eﬃcient evaluation genetic algorithms
under integrated ﬁtness functions. Technical Report 2001024, Illinois Ge-
netic Algorithms Laboratory, Urbana-Champaign, USA, 2001.
[Arn02]
D. V. Arnold. Noisy Optimization with Evolution Strategies. Kluwer, 2002.
[AW94]
A. N. Aizawa and B. W. Wah. Scheduling of genetic algorithms in a noisy
environment. Evolutionary Computation, pages 97–122, 1994.
[Bey93]
H.-G. Beyer. Toward a theory of evolution strategies: Some asymptotical
results from the (1 +, λ)-theory. Evolutionary Computation, 1(2):165–188,
1993.
[Bey00]
H.-G. Beyer. Evolutionary algorithms in noisy environments: Theoretical
issues and guidelines for practice. Computer methods in applied mechanics
and engineering, 186:239–267, 2000.
[Bra98]
J. Branke. Creating robust solutions by means of an evolutionary algo-
rithm. In A. E. Eiben, T. B¨ack, M. Schoenauer, and H.-P. Schwefel, edi-
tors, Parallel Problem Solving from Nature, volume 1498 of LNCS, pages
119–128. Springer, 1998.
[Bra01]
J. Branke. Evolutionary Optimization in Dynamic Environments. Kluwer,
2001.

Selection in the Presence of Noise
777
[BSS01]
J. Branke, C. Schmidt, and H. Schmeck. Eﬃcient ﬁtness estimation in noisy
environments. In L. Spector, E. D. Goodman, A. Wu, W. B. Langdon,
H.-M. Voigt, M. Gen, S. Sen, M. Dorigo, S. Pezeshk, M. H . Garzon,
and E. Burke, editors, Genetic and Evolutionary Computation Conference,
pages 243–250. Morgan Kaufmann, 2001.
[Efr90]
B. Efron.
The Jackknife, the Bootstrap and Other Resampling Plans.
SIAM, 1990.
[FG88]
J. M. Fitzpatrick and J. J. Grefenstette. Genetic algorithms in noisy en-
vironments. Machine Learning, 3:101–120, 1988.
[GD91]
D. E. Goldberg and K. Deb. A comparative analysis of selection schemes
used in genetic algorithms. In G. Rawlins, editor, Foundations of Genetic
Algorithms, San Mateo, CA, USA, 1991. Morgan Kaufmann.
[HB94]
U. Hammel and T. B¨ack.
Evolution strategies on noisy functions, how
to improve convergence properties.
In Y. Davidor, H. P. Schwefel, and
R. M¨anner, editors, Parallel Problem Solving from Nature, volume 866 of
LNCS. Springer, 1994.
[MG96]
B. L. Miller and D. E. Goldberg. Genetic algorithms, selection schemes,
and the varying eﬀects of noise. Evolutionary Computation, 4(2):113–131,
1996.
[Mil97]
Brad L. Miller.
Noise, Sampling, and Eﬃcient Genetic Algorithms.
PhD thesis, Dept. of Computer Science, University of Illinois at Urbana-
Champaign, 1997. available as TR 97001.
[SDR+00]
M. Schoenauer, K. Deb, G. Rudolph, X. Yao, E. Lutton, J. J. Merelo, and
H.-P. Schwefel, editors. Parallel Problem Solving from Nature, volume 1917
of LNCS. Springer, 2000.
[SK00]
Y. Sano and H. Kita. Optimization of noisy ﬁtness functions by means of
genetic algorithms using history of search. In Schoenauer et al. [SDR+00],
pages 571–580.
[SKKY00]
Y. Sano, H. Kita, I. Kamihira, and M. Yamaguchi. Online optimization
of an engine controller by means of a genetic algorithm using history of
search. In Asia-Paciﬁc Conference on Simulated Evolution and Learning.
Springer, 2000.
[Sta98]
P. Stagge. Averaging eﬃciently in the presence of noise. In A. E. Eiben,
T. B¨ack, M. Schoenauer, and H.-P. Schwefel, editors, Parallel Problem
Solving from Nature V, volume 1498 of LNCS, pages 188–197. Springer,
1998.
[TG97]
S. Tsutsui and A. Ghosh. Genetic algorithms with a robust solution search-
ing scheme. IEEE Transactions on Evolutionary Computation, 1(3):201–
208, 1997.

778
E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 778–789, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Effective Use of Directional Information in
Multi-objective Evolutionary Computation
Martin Brown1 and Robert E. Smith
2
1Department of Computing and Mathematics
Manchester Metropolitan University, Manchester, UK,
m.brown@mmu.ac.uk
2The Intelligent Computer Systems Centre
The University of The West of England, Bristol, UK.
robert.smith@uwe.ac.uk
Abstract. While genetically inspired approaches to multi-objective optimization
have many advantages over conventional approaches, they do not explicitly ex-
ploit directional/gradient information. This paper describes how steepest-
descent, multi-objective optimization theory can be combined with EC concepts
to produce improved algorithms. It shows how approximate directional infor-
mation can be efficiently extracted from parent individuals, and how a multi-
objective gradient can be calculated, such that children individuals can be placed
in appropriate, dominating search directions. The paper describes and intro-
duces the basic theoretical concepts as well as demonstrating some of the con-
cepts on a simple test problem.
1 
Introduction
Multi-objective optimization is a challenging problem in many disciplines, from prod-
uct design to planning [2][3][7][9][10]. Evolutionary computation (EC) approaches to
multi-objective problems have had many successes in recent years. In the realm of
real-valued, single-objective optimization, recent results with EC algorithms that more
explicitly exploit gradient information have shown distinct performance advantages
[4]. However, as will be shown in this paper, the rationale employed in these EC algo-
rithms must be adjusted for multi-objective EC. This paper provides a theoretical
framework and some empirical evidence for these adjustments.
This paper describes how evolutionary multi-objective optimization can efficiently
utilize approximate, local directional (gradient) information. The local gradients asso-
ciated with each point in the population can be combined to produce a multi-objective
gradient (MOG). The MOG indicates whether the design is locally Pareto optimal, or
if the design can be improved further by altering the parameters along the direction
defined by the negative MOG.
The main problem associated with the conventional approach to steepest-descent
optimization is the need to estimate the local gradient for each design at each iteration.
Therefore, viewing the problem from an EC perspective (where a population of de-

Effective Use of Directional Information           779
signs is maintained at every iteration) allows the directional information to be obtained
from neighboring samples (mates), thus lowering the number of design evaluations
that must be performed. This paper presents theory on how this information should be
used. In describing the theory, insight is gained into the structure of the multi-
objective problem by analyzing the geometry of the directional cones at different
stages of learning. Reasons for the apparently rapid rate of initial convergence (but
poor rate of final convergence) in typical multi-objective EC algorithms are also de-
scribed.
2 
Directional Multi-objective Optimization
In recent years, there have been a number of advances in steepest descent-type algo-
rithms applied to differentiable, multi-objective optimization problems [1][5]. While
they suffer from the same disadvantages as their single objective counterparts (slow
final convergence, convergence to local minima), they possess both an explicit test for
convergence and rapid initial convergence both of which are desirable properties in
many practical design problems. This section reviews the basic concepts of these gra-
dient-based, multi-objective algorithms, describing how to calculate a multi-objective
gradient, how it can be used to test for optimality and how it can be used to produce a
dominating search direction. In addition, insights are given into the structure of the
multi-objective EC optimization problem during initial and final convergence, and
reasons for the change in the convergence rate are provided. It should be acknowl-
edged that the concepts described in this paper are only directly applicable to differen-
tiable multi-objective design problems. However, a large number of complex shape
and formulation optimization problems [7][8] are differentiable. Moreover, the theory
presented may aid the reasoning used in EC algorithm design on a broader class of
problems.
2.1 Single-Objective Half-Spaces
One way to generalize the conventional, single-objective steepest descent algorithms
to a multi-objective setting is by considering which search directions simultaneously
minimize each objective. For any single objective (for instance, the j
th objective, fj,) a
search direction will reduce the objective’s value if it lies in the corresponding nega-
tive half-space, H
-, whose normal vector is the negative gradient vector, as illustrated
in Fig. 1. This fact is exploited when second-order, single-objective optimization algo-
rithms are derived, because (as long as the Hessian is positive definite) the search
direction will lie in the negative half-space. Therefore, the objective function will
decrease in value when points are moved into this half space. It is interesting to note
that this concept of a half-space is independent of the objective function’s form and
does not depend on whether the point is close to the local minima or not. It simply
states that a small step in any direction will either increase or decrease the objective.

780         M. Brown and R.E. Smith
 
x1 
X2 
 
H+
H-
∆x 
fj 
∇f 
x 
Fig. 1. The half-spaces defined for single-objective fj. The objective’s contours are illustrated as
well as the gradient for the current design x.
Also, although one must appeal to probabilistic notions to do so, this idea can be
related to modern, real-valued EC, typified by [4]. In such algorithms one can consider
those population members selected to survive and recombine to be on one side of an
approximate half-space division in the search space, and those deleted from the popu-
lation without recombination to be on the other side of this division. Note that in the
high-performance real-valued EC algorithm introduced in [4], the new individuals
generated by GA operators are biased to lie near the selected “parents”, thus enforcing
the idea of exploiting the preferred side of this approximate half-space.
2.2 Directional Cones and Multi-objective Search
For the multi-objective space, any search direction that lies in the negative half-space
of all the objectives will simultaneously minimize them, and the search direction will
be “aligned” with the negative gradients associated with each objective. This is illus-
trated in Fig. 2.
 
x1 
x2 
H1
+
H1
-
H2
+
H2
-
f1 
f2
Fig. 2. Directional cones for a 2 variable, 2 objective optimization problem. The Pareto set is
the curve between dotted centers, and the directional cone that simultaneously minimizes both
objectives is shaded gray.

Effective Use of Directional Information           781
This region is known as a “directional cone” and in fact, the m half-spaces partition
the n-dimensional variable space into 2
m directional cones, within which each objec-
tive either increases or decreases in value. This is illustrated in Fig. 3. This interpreta-
tion is useful to define search directions that span the Pareto set, rather than converg-
ing to it, where some objectives will increase in value and others decrease.
It is also useful to consider the size of the directional cone during initial and final
stages of optimization process. When a point is far from the local optima, typically the
objective gradients are aligned and the directional cone is almost equal to the half-
spaces associated with each objective. Therefore, if the search directions are randomly
chosen, there is a 50% chance that a search direction will simultaneously reduce all
the objectives. However, when a point is close to the Pareto set/front, the individual
objective gradients are contradictory and in almost opposite directions. This follows
directly from the definition of Pareto optimality, where, if one objective is decreased,
another objective must increase. The size of the directional cone is small. Therefore, if
a search direction is selected at random, there is only a small probability that it will lie
in this directional cone and thus simultaneously reduce all the objectives. The likeli-
hood is that it will lie in a cone such that some of the objectives will increase and the
others decrease, thus spanning the Pareto front, rather than converging to it. This is
one of the main differences between single and multi-objective design problems.
Appealing once again to probabilistic notions, this reasoning suggests that the chil-
dren individuals in a multi-objective EC algorithm should be created to lie within the
directional cone. Early in the search process, this is likely to be the same as any given
single-objective half space, suggesting that children individuals should be placed near
the parents, as in [4]. Later in the search process, this is not the case, and one could
expect that locating children near parents will not lead to efficient convergence to-
wards the Pareto front.
 
x1 
x2 
H1 
H2 {+,+} 
{-,+} 
{+,-} 
{-,-} 
x1
x2 
H1 
H2 
{+,+} 
{-,+} 
{+,-} 
{-,-} 
 
(a) 
(b) 
Fig. 3. The directional cones for a 2 parameter, 2 objective design problems during initial (a)
and final (b) stages of convergence. The cones are labeled with the sign of the corresponding
change in objectives and as can be seen, the descent cone {-,-} shrinks to zero during the final
stages of convergence.

782         M. Brown and R.E. Smith
2.3 Test for Local Pareto Optimality
The interpretation of multi-objective optimization described in the last section is ap-
propriate, as long as the design is not Pareto optimal (i.e., as long as there exists a
descent cone that will simultaneously reduce all the objectives). To test whether a
design is locally Pareto optimal [5] is an important part of any design process and this
can be formulated as:
))
(
(
N
x
J
∈
for any non-zero vector 
0
≥
, where N(J) is the null space of the Jacobian matrix, J,
and R(J
T) is the range of J
T. The Jacobian is the matrix of derivative of each variable
with respect to each objective. The equation above is equivalent to:
0
x
J
=
)
(
The geometric interpretation of this test in objective space (shown in Fig. 4) is that
there exists a non-negative combination of the individual gradients that produces an
identically zero vector. When this occurs, any changes to the design parameters will
affect only R(J
T), which is orthogonal to . Therefore, no changes to the design pa-
rameters will produce a descent direction that simultaneously reduces all the objec-
tives. This is the limiting case of the situation described in Section 2.2, during the final
stages of convergence, when the gradients become aligned, but in the opposite direc-
tion. When the alignment is perfect (local Pareto optimality), any change to the design
parameters will increase at least one of the objectives, so the movement will be along
the Pareto front, rather than minimizing all the objectives. In fact, for an optimal de-
sign, R(J
T) defines the local tangent to the Pareto front and thus defines the space that
must be locally sampled in order to generate the complete local Pareto set/front.
 
f1 
f2 
-λ ∈ N(J) 
R(JT) 
Fig. 4. Geometrical interpretation of the Null space and Range of the Jacobian matrix when a
design is Pareto optimal. The vector  specifies the local normal to the Pareto front.
Once again appealing by analogy to [4], note that concentration of “children” indi-
viduals in an EC algorithm near “parents” is not likely to result in individuals within
the appropriate directional cone, in a way that is analogous to points being biased to
the appropriate half-space in single-objective search. Therefore, to exploit analogous
advantages offered by modern, real-valued EC in multi-objective settings, it is appro-

Effective Use of Directional Information           783
priate to consider further operations as a part of the search process. These are outlined
in the following section.
2.4 Multi-objective Steepest Descent
A multi-objective steepest descent search direction must lie in the directional cone that
simultaneously reduces all the objectives. This specification can be made unique
[1][5] by requiring that the reduction is maximal which can be formulated as:
α
α
α
1
s
J
s
s
≤
+
=
T
st
min
arg
)
,
(
2
2
2
1
*
*
(1)
where J is the local Jacobian matrix, s is the calculated search direction and  repre-
sents the smallest reduction in the objectives’ values. This is the primal form of the
Quadratic Programming (QP) problem in (n+1) dimensions. It requires as large a
reduction in the objectives as possible for a fixed sized variable update. When all the
constraints are active, the primal form of the multi-objective optimization problem
reduces each objective by the same amount and thus the current point is locally pro-
jected towards the Pareto front at 45
o in objective space (assuming that the objectives
have been scaled to a common range).
It can be shown that when the current point is not Pareto optimal, this problem has a
solution such that * is negative, and the calculated search direction s* therefore lies
in the appropriate directional cone. However, it may be easier to solve this problem in
the dual form [1][5]:
1
min
arg
2
2
2
1
*
=
≥
=
∑j
j
st
λ
0
J
(2)
This is now a QP problem in m variables. Once it has been solved, the corresponding
search direction is given by:
*
*
J
s
−
=
This search direction will simultaneously reduce all objectives and do so in a maximal
fashion, as described by the primal problem. Hence, it is known as the multi-objective
steepest descent algorithm. The Multi-Objective Gradient (MOG) that is given by:
*
J
g =
(3)
is calculated from a non-negative linear combination of the individual gradients.
Therefore, the multi-objective search direction will be “aligned” with the individual
gradients, although it should be noted that the degree of alignment, 
*, will dynami-
cally change as the point moves closer to the Pareto set. The link with weighted opti-
mization should also be noted, but it should be stressed that this procedure is valid for
both convex and concave Pareto fronts.

784         M. Brown and R.E. Smith
In order to implement this calculation, it is necessary to obtain the Jacobian J. This
can be an expensive operation, especially for practical design problems where it is
necessary to perform some form of local experimental design. This is considered fur-
ther in the next section.
2.5 Dimensionality Analysis
This theory also provides some relevant results about the problem’s dimensionality.
Firstly, the dimension of both the Pareto set and front has the upper bound min(n,m-1).
This can be derived by simply considering the rank of the Jacobian when a point is
Pareto optimal. The dimension of the parameter-objective space mapping is locally
rank(J) = min(n,m). When a point is Pareto optimal, this reduces the dimension of the
objective space by one. In fact, rank(J) is the actual local dimension which is bounded
above by min(n,m-1). This is an important result, as it specifies the dimension of the
sub-space that a population-based EC algorithm must sample. An EC population must
be large enough to adequately sample a space of this size. It is also important as it
provides an approximate bound of how the number of objectives and variables should
be balanced. The dimension of the actual Pareto set and front is bounded by min(n,m-
1), so it may be unnecessary to have either n >> m or n << m. Secondly, it is not al-
ways necessary to consider all the design parameters in order to whether a descent
direction can be calculated. Suppose that only a single variable is considered for ad-
aptation. As long as this change, either positive or negative, simultaneously reduces all
the objectives, it is possible to state that the current point is not Pareto optimal and
there exists a descent direction in the current sub-space. This is true for any sub-set of
the design parameters considered, and is important as it may allow sparse Jacobian
estimates to be used in certain situations.
3 
Directional Evolutionary Computation
The work described in this section addresses the question of how to estimate a domi-
nating search direction, based on the information contained in a local neighborhood of
designs. In the language of EC, this essentially examines how to perform an “intelli-
gent” crossover in order to produce new designs that dominate the old ones in the
population (by placing children in the directional cone). The aim is to minimize the
number of actual objective evaluations, by maximally re-using information contained
in the population. For each point in the population, a set of local neighbors is used to
approximate the local Jacobian and thus calculate the MOG. By searching along this
path, a new point will be found that dominates the starting point. In the worst case,
each recombination of this sort involves estimating the Jacobian, which is an order
n*m computation, and the solution of a quadratic program, which is approximately an
O(min{m,n}
3) calculation. However, while this is computationally expensive com-
pared to many conventional crossover operations, this computational expensive is
likely to be vastly outweighed by the expense of evaluating fitness values for the
population (as is usually the case in EC).

Effective Use of Directional Information           785
3.1 Population-Based Estimation of the Multi-objective Gradient
To begin to understand how the local population can help to determine an “intelligent”
crossover operation, consider the trivial case where the local population consists of a
point that is dominated by the current point, see Figure 6. By moving along the path s,
where s = x-x1, a new design will be found such that it dominates the current design
f(x).
 
f1 
x1  
x1 
x2 
f1 
f2 
x   
s   
f 
Fig. 5. Calculating a descent direction when the local neighborhood about a design, x, consists
of a single dominated design x1.
In general, the situation is rarely this simple. During the latter stages of conver-
gence, designs rarely dominate each other as they are spread out along the local esti-
mate of the Pareto set/front. In this situation, it is possible to use the primal/dual
steepest descent theory described in Section 2.4, as long as the Jacobian information
can be generated efficiently. Suppose that a set of r, = min{m,n}, linearly independent
designs in the local neighborhood have been identified. Typically, these would be
closest set of designs to the current design of interest. As illustrated in Fig. 6, the dif-
ferences in the design parameters and the objectives along the search directions si can
be measured. Two matrices then represent this difference information:
j
i
s
j
i
s
s
f
s
x
∆
∆
=
∆
∆
=
F
X
(4)
and the local Jacobian can be estimated by
s
T
s
s
T
x
F
X
X
X
J
1
)
(
−
=
(5)
Therefore, the differences between the point of interest and each member of the local
neighborhood is used to estimate gradient information, which is then utilized (2) to

786         M. Brown and R.E. Smith
calculate the MOG. This relies on the local neighborhood being sufficiently small so
that the gradients can be estimated sufficiently accurately.
 
f1 
x1  
x1 
x2 
f1 
f2 
x   
s1  
f 
x2  
s2  
f2 
Fig. 6. Estimating the Jacobian matrix by using difference information along the indicated
search directions si.
3.2 Population-Based Pareto Optimality
When the local Jacobian is calculated using the members of the local population to
estimate difference information, it is assumed that the designs are sufficiently well
distributed such that they span the necessary r, dimensions (note that this may be a
subset of the complete design space) and that they are sufficiently close to the central
point so that any estimation error is small. When this is true, the test for Pareto opti-
mality is described in Section 2.3 and can be (naively) implemented by testing
whether the MOG is sufficiently small.
It should be noted that while this paper has concentrated on describing techniques
for testing for optimality and calculating descent directions, it is possible to use the
geometric insights about the shape of the Pareto set/front in order to specify search
directions that span the Pareto set/front as specified by R(J
T). This will be described in
a later paper that analyses the complete algorithm.
4 
Example
The use of directional information for multi-objective optimization will now be dem-
onstrated on a simple test problem with 2 variables and 2 objectives. Specifically, we
consider two quadratic objective functions, centered on [0.25, 0.75] and [0.75, 0.25]
and with Hessian matrices are [80 40; 40 40] and [40 40; 40 80], respectively. It
should be noted that the aim is not to provide a full and rigorous comparison with
other approaches, as a complete algorithm has not been described in this paper. Rather
the aim is to demonstrate how the theory provides the basis for such an approach, and
to give an indication of the power of using directional information.

Effective Use of Directional Information           787
This concepts described above were implemented as a simple multi-objective opti-
mization procedure where the initial population of 50 points approximately lay along a
line with a small amount of random noise added, as illustrated in Fig. 7.
Fig. 7. Using the directional MOG to drive a population towards the Pareto set (left) and front
(right). Each progressive population is shown with points in a different color, progressing to-
wards the indicated front.
In Fig. 7, it can be clearly seen that successive iterations produce new “child” values
that dominate their parents. Convergence to the Pareto set/front occurred after around
6 iterations. It should be noted that a fixed step size was used here and it would be
expected that a more intelligent line search method would produce faster convergence.
It should also be noted that the spanning of the Pareto set/front occurs because of the
diversity in the initial population. While there is no diversity component in the method
described in this paper, it can be clearly seen that the approximate MOG descent cal-
culation projects the points directly towards the Pareto set/front in a rapid and efficient
manner. In addition, the development of the diversity component [6] is the subject of
current research.
As a final, simple comparison, a naïve genetic multi-objective procedure was de-
veloped. A simplex-type scheme was used to adapt the points, where each point was
tested against neighboring values, and if the point was dominated or it was too close to
a neighboring point (within a threshold distance), it was replaced by a combination of
its n neighboring values (thus using averaging of parents as a form of recombination),
else it was subject to a small random mutation (addition of Gaussian noise), with a
standard deviation = 0.01. The results of this procedure are shown in Fig. 8. The same
initial population is used as in Fig. 7. It should be noted that the final population now
occurs after 60 iterations and the convergence in variable space is poor. While it is
readily acknowledged that more sophisticated combination/selection processes could
be used, the aim was to demonstrate that an “intelligent” use of directional information
can dramatically improve the rate of convergence for suitably smooth, differentiable
design problems. Moreover, we feel that this reasoning can be extended for the for-
mulation of recombination operators in a much broader class of problems.

788         M. Brown and R.E. Smith
Fig. 8. Using a genetic multi-objective optimization process to solve the simulated problem.
5 
Conclusions and Further Work
One must consider how to efficiently estimate a dominating search direction for a
multi-objective problem, based on the information contained in a local neighborhood
of designs, to properly apply EC to such problems. The theory developed in this paper
clarifies how to logically use such information in multi-objective EC. The theory can
be both used to explain the performance of current techniques, and gain insights into
the problem’s dimensionality. Also, the concepts developed (of convergence, direc-
tional cones and MOG) are directly applicable to differentiable multi-objective prob-
lems, and may be useful in EC algorithm design for a broader class of problems. In
particular, a key aspect of EC algorithm design is the specification of recombination
and mutation operators. When points are close to the optimal front/set children should
be placed in the descent cone, or convergence should be expected to be poor in many
cases. This paper shows that appropriate, informed use of direction information in
MOEAs will improve performance. Moreover, the authors believe that the theory can
both be extended to more logically design operators in non-continuous problems and
also to understanding how diversity [6] can be integrated into such schemes, which is
a subject of their current research.
References
[1] Brown, M. 3(2002) Steepest Descent Vector Optimization and Product Design, submitted
for publication.in preparation
[2] Coello, Carlos A., Van Veldhuizen, David A., and Lamont, Gary B (2002) Evolutionary
Algorithms for Solving Multi-Objective Problems. Kluwer Academic Publishers.
[3] Deb, K. (2000). Multi-Objective Optimization Using Evolutionary Algorithms. Wiley.

Effective Use of Directional Information           789
[4] Deb, K , Anand, A., and Joshi, D (2002). A Computationally Efficient Evolutionary
Algorithm for Real-Parameter Optimization. KanGAL Report No. 2002003. (to appear in
the journal Evolutionary Computation).
[5] Fliege, J. (2000) Steepest Descent Methods for Multicriteria Optimization. Mathematical
Methods of Operations Research, 51(3), pp. 479–494.
[6] Laumanns, M., Thiele, L., Deb, K., and Zitzler, E. (2002) Combining Convergence and
Diversity in Evolutionary Multi-objective Optimization. Evolutionary Computation 10(3)
pp. 263–282.
[7] Parmee, I. C., Watson, A., Cvetkovic, D., Bonham, C. (2000). Multi-objective Satisfaction
within an Interactive Evolutionary Design Environment. Evolutionary Computation. 8(2).
pp 197–222.
[8] Product Formulation using Intelligent Software, 
http://www.brad.ac.uk/acad/profits/website/
[9] Sobieszczanski-Sobieski, J.; and Haftka, R.T. (1997). Multidisciplinary aerospace design
optimization: survey of recent developments. Structural Optimization 14, pp. 1-23.
[10] Vicini, A., and Quagliarella, D. (1997) Inverse and direct airfoil design by means of a
multi-objective genetic algorithm. AIAA Journal. 35(9).

Pruning Neural Networks with Distribution
Estimation Algorithms
Erick Cant´u-Paz
Center for Applied Scientiﬁc Computing
Lawrence Livermore National Laboratory
Livermore, CA 94551
cantupaz@llnl.gov
Abstract. This paper describes the application of four evolutionary al-
gorithms to the pruning of neural networks used in classiﬁcation prob-
lems. Besides of a simple genetic algorithm (GA), the paper considers
three distribution estimation algorithms (DEAs): a compact GA, an ex-
tended compact GA, and the Bayesian Optimization Algorithm. The
objective is to determine if the DEAs present advantages over the simple
GA in terms of accuracy or speed in this problem. The experiments con-
sidered a feedforward neural network trained with standard backpropa-
gation and 15 public-domain and artiﬁcial data sets. In most cases, the
pruned networks seemed to have better or equal accuracy than the orig-
inal fully-connected networks. We found few diﬀerences in the accuracy
of the networks pruned by the four EAs, but found large diﬀerences in
the execution time. The results suggest that a simple GA with a small
population might be the best algorithm for pruning networks on the data
sets we tested.
1
Introduction
The success of neural networks (NNs) largely depends on their architecture,
which is usually determined by a trial-and-error process. Evolutionary algorithms
(EAs) have been used in numerous ways to optimize the network architecture
to reach the highest possible classiﬁcation accuracy [1,2]. In the present paper,
we examine neural network pruning by four evolutionary algorithms to improve
the generalization accuracy in classiﬁcation problems.
We experimented with a simple genetic algorithm (sGA) and three distribu-
tion estimation algorithms (DEAs): a compact GA (cGA), an extended compact
GA (ecGA), and the Bayesian Optimization Algorithm (BOA). Instead of the
mutation and crossover operations of conventional GAs, DEAs use a statisti-
cal model of the individuals that survive selection to generate new individuals.
Numerous experimental and theoretical results show that DEAs can solve hard
problems reliably and eﬃciently [3,4,5].
The objective of this study is to determine if DEAs present advantages over
simple GAs in terms of accuracy or speed when applied to neural network prun-
ing. The experiments used conventional feedforward perceptrons with one hidden
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 790–800, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Pruning Neural Networks with Distribution Estimation Algorithms
791
layer and were trained with the backpropagation algorithm. The experiments
used 13 public-domain and two artiﬁcial data sets.
Our target was to maximize the accuracy of classiﬁcation. The experiments
demonstrate that, in most cases, the accuracy of the pruned networks is at least
as good as that of fully-connected networks. We found few signiﬁcant diﬀerences
in the accuracy of networks pruned by the four EAs, but found large diﬀerences
in the execution time.
The next section presents background on neural network pruning, including
some previous applications of EAs to this task. Section 3 describes the algo-
rithms, data sets, and the method used to compare the algorithms. The exper-
imental results are presented in section 4. Section 5 concludes this paper with
a summary, the conclusions of this study, and a discussion of future research
directions.
2
Neural Network Pruning
It is well known that a network that is too big for a particular classiﬁcation
task is more likely to overﬁt the training data and have poor performance on
unseen examples (i.e., poor generalization) than a small network. Therefore, a
heuristic to obtain good generalization is to use the smallest network that will
learn to classify correctly the training data. However, the optimal network size
is usually unknown and tedious experimentation becomes necessary to ﬁnd it.
An alternative to improve generalization is to train a network that is believed
to be larger than necessary and prune the excess parts.
Numerous algorithms have been used to prune neural networks [6]. Prun-
ing begins by training a fully-connected neural network. Most pruning methods
delete a single weight at a time in a greedy fashion, which may result in sub-
optimal pruning. Additionally, many pruning methods fail to account for the
interactions among multiple weights. This may be problematic if deleting one
weight makes it appear as if another weight that should be pruned is important
for the operation of the network. An algorithm that considers weight interactions
and more than one weight at a time may have a better chance of reducing the
size of the network signiﬁcantly without aﬀecting the classiﬁcation accuracy. For
these reasons, GAs and DEAs seem promising for NN pruning.
Genetic algorithms have been used to prune networks with good results [7,8,
9]. Applying GAs to prune networks is straightforward: The chromosomes con-
tain one bit for each weight of the original network, and the value of the bit
determines whether the weight will be used in the ﬁnal network. This simple bi-
nary encoding is used in the experiments in the present paper. More sophisticated
methods of simultaneously training and pruning the networks were introduced
by Schmidt and Stidsen [10].
Whitley [11] suggests to retrain the network for a few epochs after pruning
the weights. We performed experiments to test this idea, but our experiments
show only limited advantages of retraining.

792
E. Cant´u-Paz
It is also possible to prune entire (input and hidden) nodes, but in the present
paper we experiment only with the more common approach of pruning individual
weights. We leave pruning units to future work.
3
Methods
This section describes the algorithms and the data sets used in this paper as
well as the statistical method used to compare the algorithms.
3.1
Algorithms
The simple genetic algorithm in this study uses binary strings, pairwise tourna-
ment selection without replacement, uniform crossover, and bitwise point muta-
tion. Simple GAs such as this have been used successfully in many applications.
However, it has long been recognized that the problem-independent crossover
operators used in simple GAs can disrupt groups of related variables and pre-
vent the algorithm from reaching the global optimum, unless exponentially-sized
populations are used. (Thierens [12] gives a good description of this problem).
One approach to identify and exploit the relationships among variables is
to estimate the joint distribution of the individuals that survive selection and
use this model to generate new individuals. The complexity of the models has
increased over time as more sophisticated methods of building models from data
and more powerful computers become available. Interested readers can consult
the reviews by Pelikan et al. [13] and Larra˜naga et al. [14].
The simplest model-building EA used in the experiments reported here is the
compact GA [15]. This algorithm assumes that the variables (bits) that represent
the problem are independent, and therefore the cGA models the population as a
product of Bernoulli distributions. The compact GA receives its name from its
small memory requirements: Instead of using an explicit population, the cGA
uses a vector p of length equal to the problem’s length, l. Each element of p
contains the probability that a sample will take the value 1. If the Bernoulli trial
is not successful the sample will be 0. All positions of p are initialized to 0.5 to
simulate the usual uniform random initialization of simple GAs. New individuals
are obtained by sampling consecutively from each position of p and concatenating
the values obtained. The probabilities vector is updated by comparing the ﬁtness
of two individuals obtained from it. For each pk, k = 1, .., l, if the ﬁttest individual
has a 1 in the k-th position, pk is increased by 1/n, where n is the size of
the virtual population that the user wants to simulate. Likewise, if the ﬁttest
individual has a 0 in the k-th position, pk is decreased by 1/n. The cGA iterates
until all positions in pk contain either zero or one. PBIL [16] and the UMDA [17]
are other algorithms that use univariate models and operate on binary alphabets.
They diﬀer from the cGA in the method to update the probabilities vector.
The extended compact GA [18] uses a product of marginal distributions on a
partition of the variables. In this model, subsets of variables are modeled jointly,
and the subsets are considered independent of other subsets. Formally, the model

Pruning Neural Networks with Distribution Estimation Algorithms
793
(a) ecGA
(b) BOA
Fig. 1. Representation of the models used in the ecGA and the BOA. Variables are
represented as circles. The ecGA groups related variables into subsets, but cannot
represent individual relationships among variables in the same subset.
is P = m
i=0 Pi, where m is the number of subsets in a partition of the variables
and Pi represents the distribution of the i-th subset. The distribution of a subset
with k members is stored in a table with 2k −1 entries. The challenge is to ﬁnd
a partition that models the population correctly. Harik [18] proposed a greedy
search that initially supposes that all variables are independent. The model
search tries to merge all pairs of subsets and chooses the merger that minimizes
a complexity measure based on information theory. The search continues until no
further subsets can be merged. In contrast to the cGA, the ecGA has an explicit
population that is evaluated and is subject to selection at each iteration of the
algorithm. The algorithm builds the model considering only those solutions that
survive selection. The population is initialized randomly, and new individuals
are generated by sampling consecutively from the m subset distributions.
The Bayesian Optimization Algorithm [3] models the selected individuals
using a Bayesian network, which can represent dependence relations among an
arbitrary number of variables. Independently, Etxeberria and Larra˜naga [4] and
M¨uhlenbein and Mahnig [5] introduced similar algorithms. The BOA uses a
greedy search to optimize the Bayesian Dirichlet metric, a measure of how well
the network represents the data (the BOA could use other metrics). The user
speciﬁes the maximum number of incoming edges to any node of the network.
This number corresponds to the highest degree of interaction assumed among the
variables of the problem. As the ecGA, the BOA builds the model considering
only the solutions that survived selection. New individuals are generated by
sampling from the Bayesian network.
The main diﬀerence between the ecGA and the BOA is the model that they
use to represent the survivors. Figure 1 illustrates the diﬀerent models used by
the ecGA and the BOA. The ecGA cannot represent individual relationships
among the variables in a subset.

794
E. Cant´u-Paz
The experiments used the C++ implementations of the ecGA [19] and the
BOA version 1.0 [20] that are distributed by their authors on the web (at
http://www-illigal.ge.uiuc.edu). The ecGA code has a non-learning mode
that emulates the cGA. The sGA and the neural network were developed in C++.
All programs were compiled with g++ version 2.96 using -O2 optimizations. The
experiments were executed on a single processor of a Linux (Red Had 7.2) work-
station with dual 2.4 GHz Intel Xeon processors and 512 Mb of memory. The
ecGA and the BOA codes were modiﬁed to use a Mersenne Twister random
number generator, which was also used in the GA and the data partitioning.
The algorithms used populations with 1024 individuals and were initialized
uniformly at random. The GA used uniform crossover with probability 1.0, and
mutation with probability 1/l, where l was the length of the chromosomes and
corresponds to the total number of weights in the network. Promising solutions
were selected with pairwise binary tournaments without replacement. The cGA,
ecGA, and the BOA used the default parameters provided in their distributions:
The cGA and ecGA used tournaments among 16 individuals, and the BOA used
truncation selection with a threshold of 50%. All algorithms were terminated
after observing no improvement in the best individual over ﬁve consecutive gen-
erations, or until a limit of 50 generations was reached.
The network used in the experiments was a fully-connected perceptron with
a single hidden layer. The hidden and output units compute their output as
f(net) = tanh(net), where net = d
i=1 xiwi + w0 is the net activation, the xi
are inputs to the unit, wi are the connection weights and w0 is a bias term. The
weights were initialized uniformly at random in the interval [-1,1]. Before each
EA run, a fully-connected network was trained with simple backpropagation
using a learning rate of 0.15 and a momentum term of 0.9. In each epoch, the
examples were presented to the network in a diﬀerent random order. The sizes
of the network and the number of training epochs varied for each data set and
are speciﬁed in table 1.
For all the algorithms, the classiﬁcation accuracy of the pruned network on
the training data served as the ﬁtness function. In cases where the pruned net-
work was retrained with backpropagation, the algorithms exploited the Baldwin
eﬀect: The retrained pruned network was used to evaluate the ﬁtness, but the
retrained weights were not inherited. Note that the ﬁtness measure does not
bias the search explicitly toward networks with few weights. Adding this bias is
a future extension of the work presented here.
3.2
Data Sets
The data sets used in the experiments are described in table 1. The data sets are
available in the UCI machine learning repository [21], except for Random21 and
Redundant21, which are artiﬁcial data sets with 21 features each. The target
concept of these two data sets is whether the ﬁrst nine features are closer to
(0,0,...,0) or (9,9,...,9) in Euclidean distance. The features were generated uni-
formly at random in the range [3,6]. All the features in Random21 are random,

Pruning Neural Networks with Distribution Estimation Algorithms
795
Table 1. Description of the data sets used in the experiments. For each data set, the
table shows the number of instances; the number of classes; the number of continuous
and discrete features; the number of input, hidden, and output units; and the number
of epochs of backpropagation used to train the networks.
Features
Neural Network
Domain
Cases Class Cont. Disc. Input Output Hidden Epochs
Breast Cancer
699
2
9
–
9
1
5
20
Credit-Australian
653
2
6
9
46
1
10
35
Credit-German
1000
2
7
13
62
1
10
30
Heart-Cleveland
303
2
6
7
26
1
5
40
Housing
506
3
12
1
13
3
2
70
Ionosphere
351
2
34
–
34
1
10
40
Iris
150
3
4
–
4
3
5
80
Kr-vs-kp
3196
2
–
36
74
1
15
20
Pima-Diabetes
768
2
8
–
8
1
5
30
Segmentation
2310
7
19
–
19
7
15
20
Sonar
208
2
60
–
60
1
10
60
Vehicle
846
4
18
–
18
4
10
40
Wine
178
3
13
–
13
3
5
15
Random21
2500
2
21
–
21
1
1
100
Redundant21
2500
2
21
–
21
1
1
100
and the ﬁrst, ﬁfth, and ninth features are repeated four times each in Redun-
dant21. We took the deﬁnition of Redundant21 from the paper by Inza et al. [22].
Each numeric feature in the data was linearly normalized to the interval
[−1, 1]. The discrete features and the class labels were encoded with the usual
1-in-C coding if there are C > 2 values (one of the C outputs is set to 1 and the
rest to -1). Binary values were encoded as a single -1 or 1 value.
The instances with missing values in Credit-Australian were deleted. Fol-
lowing the usual practice, the missing values in Pima-Diabetes (denoted with
zeroes) were not removed and were treated as if their values were meaningful.
Following Lim et al. [23], the classes in Housing were obtained by discretizing
the attribute “mean value of owner-occupied homes” as follows: class = 1 if
log(median value) ≤9.84, class = 2 if 9.84 < log(median value) ≤10.075, and
class = 3 otherwise.
3.3
Evaluation Method
To evaluate the generalization accuracy of the pruning methods, we used 5 itera-
tions of 2-fold crossvalidation (5x2cv). In each iteration, the data were randomly
divided in halves. One half was input to the EAs. The best pruned network found
by the EA was tested on the other half of the data. The accuracy results pre-
sented in table 2 are the averages of the ten tests.
To determine if the diﬀerences among the algorithms were statistically sig-
niﬁcant, we used a combined F test proposed by Alpaydin [24]. Let p(j)
i
denote

796
E. Cant´u-Paz
Table 2. Mean accuracies found in the 5x2cv experiments. Bold typeface indicates
the best result and those not signiﬁcantly diﬀerent from the best according to the
combined F test at a 0.05 level of signiﬁcance.
Domain
Unpruned
sGA
cGA
ecGA
BOA
Breast Cancer
96.39
96.54
96.13
95.84
96.42
Cr-Australian
82.53
85.78
85.75
86.18
85.84
Cr-German
70.12
70.68
70.92
70.30
70.14
Heart-Cleveland
58.17
89.70
88.05
88.78
89.37
Housing
64.62
75.36
67.11
64.18
66.24
Ionosphere
84.77
84.61
82.95
82.22
84.22
Iris
94.53
92.93
70.13
67.73
93.60
Kr-vs-kp
74.30
92.56
93.53
93.81
93.85
Pima-Diabetes
73.30
74.84
75.91
76.04
75.88
Segmentation
44.16
64.02
62.45
64.32
63.66
Sonar
73.17
83.46
86.15
84.90
83.55
Vehicle
69.71
78.20
76.73
76.64
78.62
Wine
95.16
94.15
89.88
87.41
93.48
Random21
91.70
94.04
94.08
94.03
94.09
Redundant21
91.75
95.77
95.82
95.82
95.72
the diﬀerence in the accuracy rates of two classiﬁers in fold j of the i-th iteration
of 5x2cv, ¯p = (p(1)
i
+ p(2)
i )/2 denote the mean, and s2
i = (p(1)
i
−¯p)2 + (p(2)
i
−¯p)2
the variance, then
f =
5
i=1
2
j=1

p(j)
i
2
2 5
i=1 s2
i
is approximately F distributed with 10 and 5 degrees of freedom, and we rejected
the null hypothesis that the two algorithms have the same error rate with a 0.05
level of signiﬁcance if f > 4.74 [24]. The algorithms used the same data partitions
and started from identical initial populations.
4
Experiments
Table 2 has the average accuracies obtained with each method. For each data
set, the best observed result and those that according to the combined F test
are not signiﬁcantly diﬀerent from the best are highlighted in bold type.
These results suggest that, in most cases, the accuracy of the pruned networks
is at least as good as the original fully-connected networks. In these experiments
the networks were not retrained after pruning. Unexpectedly, pruning does not
seem to have harmful eﬀects on the accuracy, except in two cases (Iris and Wine)
where the networks pruned with cGA and ecGA perform signiﬁcantly worse than
the fully-connected networks. The simple GA and the BOA performed equally
well, and their results were not signiﬁcantly diﬀerent than the best result for all
the data sets we tried.

Pruning Neural Networks with Distribution Estimation Algorithms
797
Pruning results in only minor accuracy gains over the fully-connected net-
works, except when the fully-connected nets performed poorly. In those cases,
pruning resulted in dramatic improvements. For example, the pruned networks
on Heart-Cleveland show improvements of ≈30% in accuracy, while in Kr-vs-kp
and Segmentation the improvements are ≈20%, and in Vehicle the improvements
are ≈10%.
One reason why pruning might improve the accuracy is because pruning
may eliminate the eﬀect of irrelevant or redundant inputs. The experiments
with Random21 and Redundant21 were intended to explore this hypothesis. In
Random21, the pruning methods always selected weights corresponding to the
nine true inputs, but the algorithms always selected two or three additional
weights corresponding to random inputs. However, the performance does not
seem to degrade much. It is possible that backpropagation had assigned low
values to those irrelevant weights or it may be that the hypothesis that pruning
improves the accuracy by removing irrelevant weights is wrong. Further work is
required to clarify these results. In Redundant21, the pruning methods did not
eliminate the redundant features. In fact, the pruned networks retained more
than 20 of their 24 weights. Again, it is not clear why the performance did not
degrade with the redundant weights and additional work is needed to address
this issue.
With respect to the number of weights of the ﬁnal networks, all algorithms
had similar results, successfully pruning between 30 and 50% of the total weights
(with the exception of Redundant 21 discussed above).
Table 3 shows that the sGA and the BOA ﬁnished in similar number of
generations (except for Credit-Australian and Heart-Cleveland), and were the
slowest algorithms in most cases. On most data sets, the ecGA ﬁnishes faster
than the other algorithms.1 However, the ecGA produced networks with inferior
accuracy than the other methods or the fully-connected networks in three cases
(Housing, Iris, and Wine). Despite the occasional inferior accuracies, it seems
that the ecGA is a good pruning method with a good compromise of accuracy
and execution time. However, further experiments described below suggest that
simple GAs might be the best option.
We performed additional experiments retraining the networks after pruning
for one, two, and ﬁve epochs of backpropagation (results not shown). In most
cases, retraining the networks improves the classiﬁcation accuracy only slightly
over pruning without retraining (1–2%), and there does not appear to be a
signiﬁcant advantage to retrain for more than one epoch. Among the data sets
we tested, the largest impact of retraining (using one epoch) was in Housing
with an increase of approximately 7% over pruning without retraining.
Retraining, however, had a large impact on the number of generations until
the algorithms terminated. In most cases, retraining for one epoch reduced the
generations by approximately 40%. Only in one case (sGA on Random21) the
1 The time needed by the DEAs to build a model of the selected individuals and gen-
erate new ones was short compared to the time consumed evaluating the individuals,
so one generation took roughly the same time in all algorithms.

798
E. Cant´u-Paz
Table 3. Mean generations until termination. Bold typeface indicates the best result
and those not signiﬁcantly diﬀerent from the best according to the combined F test at
a 0.05 level of signiﬁcance.
Domain
sGA
cGA
ecGA
BOA
Breast Cancer
9.2
6.7
7
10.9
Credit-Australian
10
14
14.9
14.4
Credit-German
17.1
22.8
21.3
14.3
Heart-Cleveland
9.8
10.4
10.2
15.8
Housing
19.4
7.4
7.1
18.6
Ionosphere
16.8
15.7
15.1
17.8
Iris
10.1
5.9
5.9
10.1
Kr-vs-kp
37.7
28.8
26
35.7
Pima-Diabetes
12.8
14.7
11.5
14.2
Segmentation
26
18.1
17.4
24.9
Sonar
14.5
20.5
19.3
16.9
Vehicle
26.1
16.5
14.8
30.2
Wine
12.5
9.9
9.4
11.7
Random21
13.6
9
9.1
14.8
Redundant21
13.7
8.5
8.5
16.1
number of generations increased (from 13.6 to 20). Retraining for more than one
epoch did not have a noticeable eﬀect on the number of generations. Of course,
in all cases, retraining increased the total execution time considerably.
The population size of 1024 individuals was chosen because the DEAs require
a large population to estimate correctly the parameters of the models of selected
individuals. However, for the simple GAs, it is likely that such a large population
is unnecessary. In additional experiments, we set the sGA population size to the
largest of 20 or 3
√
l, where l is the size of the chromosomes (number of weights
in the network). The only signiﬁcant diﬀerence in accuracy between the sGA
with 1024 individuals and the smaller population was in Iris (87.73% with 20
individuals vs. 92.93% with 1024). There were no other signiﬁcant diﬀerences
with the sGA with the large population or the best pruning method for each data
set. Naturally, the execution time was much shorter with the smaller populations.
Therefore, for pruning neural networks, it seems that the best alternative among
the algorithms we examined is a simple GA with small populations.
5
Conclusions
This paper presented experiments with four evolutionary algorithms applied to
neural network pruning. The experiments considered public-domain and artiﬁcial
data sets. With these data sets we found that there are few diﬀerences in the
accuracy of networks pruned by the four EAs, but that the extended compact
GA needs fewer generations to ﬁnish. However, we also found that, in a few
cases, the ecGA results in networks with lower accuracy than those obtained by
the other EAs or a fully-connected network.

Pruning Neural Networks with Distribution Estimation Algorithms
799
We also found that in most cases retraining the pruned networks improves
the classiﬁcation accuracy only very slightly but incurs in a much higher com-
putational cost. Therefore, it appears that retraining is only recommended in
applications where time is not critical.
Additional experiments revealed that a simple GA with a small population
can reach results that are not signiﬁcantly diﬀerent from the best pruning meth-
ods. Since the smaller populations result in much shorter execution times, the
simple GA seems to have an advantage over the other methods.
The experiments with redundant and irrelevant attributes presented here are
not conclusive and additional work is needed to clarify those results. Future work
is also necessary to explore methods to improve the computational eﬃciency of
the algorithms to deal with much larger data sets. In particular, subsampling
the training sets and parallelizing the ﬁtness evaluations seem like promising
alternatives. Another possible extensions of this work are to prune entire units
and attempt to reduce the size of the pruned networks by including a bias toward
small networks in the ﬁtness function.
Acknowledgments. I thank Martin Pelikan for providing the graphs in ﬁgure
1 and the anonymous reviewers for their detailed and constructive comments.
UCRL-JC-151521. This work was performed under the auspices of the U.S. De-
partment of Energy by University of California Lawrence Livermore National
Laboratory under contract No. W-7405-Eng-48.
References
1. Yao, X.: Evolving artiﬁcial neural networks. Proceedings of the IEEE 87 (1999)
1423–1447
2. Castillo, P.A., Arenas, M.G., Castillo-Valdivieso, J.J., Merelo, J.J., Prieto, A.,
Romero, G.: Artiﬁcial neural networks design using evolutionary algorithms. In:
Proceedings of the Seventh World Conference on Soft Computing. (2002)
3. Pelikan, M., Goldberg, D.E., Cant´u-Paz, E.: BOA: The Bayesian optimization algo-
rithm. In Banzhaf, W., Daida, J., Eiben, A.E., Garzon, M.H., Honavar, V., Jakiela,
M., Smith, R.E., eds.: Proceedings of the Genetic and Evolutionary Computation
Conference 1999: Volume 1, San Francisco, CA, Morgan Kaufmann Publishers
(1999) 525–532
4. Etxeberria, R., Larra˜naga, P.: Global optimization with Bayesian networks. In: II
Symposium on Artiﬁcial Intelligence (CIMAF99). (1999) 332–339
5. M¨uhlenbein, H., Mahnig, T.: FDA-A scalable evolutionary algorithm for the opti-
mization of additively decomposed functions. Evolutionary Computation 7 (1999)
353–376
6. Reed, R.: Pruning algorithms—a survey. IEEE Transactions on Neural Networks
4 (1993) 740–747
7. Whitley, D., Starkweather, T., Bogart, C.: Genetic algorithms and neural networks:
Optimizing connections and connectivity. Parallel Computing 14 (1990) 347–361
8. Hancock, P.J.B.: Pruning neural networks by genetic algorithm. In Aleksander,
I., Taylor, J., eds.: Proceedings of the 1992 International Conference on Artiﬁcial
Neural Networks. Volume 2., Amsterdam, Netherlands, Elsevier Science (1992)
991–994

800
E. Cant´u-Paz
9. LeBaron, B.: An evolutionary bootstrap approach to neural network pruning and
generalization. unpublished working paper (1997)
10. Schmidt, M., Stidsen, T.:
Using GA to train NN using weight sharing, weight
pruning and unit pruning. Technical report, Aarhus University, Computer Science
Department, Aarhus, Denmark (1995)
11. Whitley, D., Bogart, C.: The evolution of connectivity: Pruning neural networks
using genetic algorithms. Technical Report CS-89-113, Colorado State University,
Department of Computer Science, Fort Collins (1989)
12. Thierens, D.:
Scalability problems of simple genetic algorithms.
Evolutionary
Computation 7 (1999) 331–352
13. Pelikan, M., Goldberg, D.E., Lobo, F.: A survey of optimization by building and
using probabilistic models.
IlliGAL Report No. 99018, University of Illinois at
Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL (1999)
14. Larra˜naga, P., Etxeberria, R., Lozano, J.A., Pe˜na, J.M.: Optimization by learning
and simulation of Bayesian and Gaussian networks. Tech Report No. EHU-KZAA-
IK-4/99, University of the Basque Country, Conostia-San Sebastian, Spain (1999)
15. Harik, G.R., Lobo, F.G., Goldberg, D.E.: The compact genetic algorithm. In: Pro-
ceedings of the 1998 IEEE International Conference on Evolutionary Computation,
Piscataway, NJ, IEEE Service Center (1998) 523–528
16. Baluja, S.: Population-based incremental learning: A method for integrating ge-
netic search based function optimization and competitive learning. Tech. Rep. No.
CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA (1994)
17. M¨uhlenbein, H.: The equation for the response to selection and its use for predic-
tion. Evolutionary Computation 5 (1998) 303–346
18. Harik, G.:
Linkage learning via probabilistic modeling in the ECGA.
IlliGAL
Report No. 99010, University of Illinois at Urbana-Champaign, Illinois Genetic
Algorithms Laboratory, Urbana, IL (1999)
19. Lobo, F.G., Harik, G.R.: Extended compact genetic algorithm in C++. IlliGAL
Report No. 99016, University of Illinois at Urbana-Champaign, Illinois Genetic
Algorithms Laboratory, Urbana, IL (1999)
20. Pelikan, M.:
A simple implementation of the Bayesian optimization algorithm
(BOA) in C++ (version 1.0). IlliGAL Report No. 99011, University of Illinois at
Urbana-Champaign, Illinois Genetic Algorithms Laboratory, Urbana, IL (1999)
21. Blake, C., Merz, C.: UCI repository of machine learning databases (1998)
22. Inza, I., Larra˜naga, P., Etxeberria, R., Sierra, B.:
Feature subset selection by
Bayesian networks based on optimization. Artiﬁcial Intelligence 123 (1999) 157–
184
23. Lim, T.J., Loh, W.Y., Shih, Y.S.: A comparison of prediction accuracy, complexity,
and training time of thirty-three old and new classiﬁcation algorithms. Machine
Learning 40 (2000) 203–228
24. Alpaydin, E.:
Combined 5 × 2cv F test for comparing supervised classiﬁcation
algorithms. Neural Computation 11 (1999) 1885–1892

Are Multiple Runs of Genetic Algorithms Better
than One?
Erick Cant´u-Paz1 and David E. Goldberg2
1 Center for Applied Scientiﬁc Computing
Lawrence Livermore National Laboratory
7000 East Avenue, Livermore, CA 94550
cantupaz@llnl.gov
2 Department of General Engineering
University of Illinois at Urbana-Champaign
104 S. Mathews Avenue Urbana, IL 61801
deg@uiuc.edu
Abstract. There are conﬂicting reports over whether multiple indepen-
dent runs of genetic algorithms (GAs) with small populations can reach
solutions of higher quality or can ﬁnd acceptable solutions faster than
a single run with a large population. This paper investigates this ques-
tion analytically using two approaches. First, the analysis assumes that
there is a certain ﬁxed amount of computational resources available, and
identiﬁes the conditions under which it is advantageous to use multiple
small runs. The second approach does not constrain the total cost and
examines whether multiple properly-sized independent runs can reach
the optimal solution faster than a single run. Although this paper is lim-
ited to additively-separable functions, it may be applicable to the larger
class of nearly decomposable functions of interest to many GA users. The
results suggest that, in most cases under the constant cost constraint, a
single run with the largest population possible reaches a better solution
than multiple independent runs. Similarly, a single large run reaches the
global faster than multiple small runs. The ﬁndings are validated with
experiments on functions of varying diﬃculty.
1
Introduction
Suppose that we are given a ﬁxed number of function evaluations to solve a
particular problem with a genetic algorithm (GA). How should we use these
evaluations to maximize the expected quality of the solution? One possibility
would be to use all the evaluations in a single run of the GA with the largest
population possible. This approach seems plausible, because it is well known
that, in general, the solution quality improves with larger populations. Alterna-
tively, we could use a smaller population and run the GA multiple times, keeping
the best solution found by the diﬀerent runs. Although the quality per run is
expected to decrease, we would have more chances of reaching a good solution.
This paper examines the tradeoﬀbetween increasing the likelihood of success
of a single run vs. using more trials to reach the goal. The ﬁrst objective is to
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 801–812, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

802
E. Cant´u-Paz and D.E. Goldberg
determine what conﬁguration reaches solutions with the highest quality. The pa-
per also examines the question of single vs. multiple runs removing the constant
cost constraint. The objective in this case is to determine what conﬁguration
reaches the solution faster.
It would be desirable to ﬁnd that multiple runs are advantageous, because
they could be executed concurrently on diﬀerent processors. Multiple indepen-
dent runs are a special case of island-model parallel GAs, and have been studied
in that context before with conﬂicting and controversial results [1,2,3,4,5]. Some
results suggest that multiple runs can reach solutions of similar or better quality
than a single run in a shorter time, which implies that superlinear speedups are
possible.
Most of the previous work on this topic has been experimental, which makes
it diﬃcult to identify the problem characteristics that give an advantage to
multiple runs. Instead of trying to analyze experimental results from a set of
arbitrarily-chosen problems, we use simple mathematical models and consider
only additively separable functions. The paper clearly shows when one approach
can be superior, and reveals that, for the functions considered, multiple runs are
preferable only in conditions of limited practical value.
The paper also considers the extreme case when multiple runs with a single
individual—which are equivalent to random search—are better in terms of ex-
pected solution quality than a single GA. Although it is known than in some
problems random search must be better than GAs [6], it is not clear on what
problems this occurs. This paper sheds some light on this topic.
The next section summarizes related work on this area. The gambler’s ruin
(GR) model [7] is summarized in section 3 and extended to multiple independent
runs in section 4. Section 5 presents experiments that validate the accuracy of
the models. Section 6 lifts the total cost constraint and discusses multiple short
runs. Finally, section 7 presents a summary and the conclusions.
2
Related Work
Since multiple runs can be executed in parallel, they have been considered by
researchers working with parallel GAs. Tanese [1] found that, in some problems,
the best overall solution found in any generation by multiple isolated populations
was at least as good as the solution found by a single run. Similarly, multiple
populations showed an advantage when she compared the best individual in the
ﬁnal generation. However, when she compared the average population quality at
the end of the experiments, the single runs seemed beneﬁcial.
Other studies also suggest that multiple isolated runs can be advantageous.
For example, Shonkwiler [2] used a Markov chain model to argue that multiple
small independent GAs can reach the global solution using fewer function eval-
uations than a single GA. He suggested that superlinear parallel speedups are
possible if the populations are executed concurrently on a parallel computer.
Nakano, Davidor, and Yamada [8] proved that, under the ﬁxed cost con-
straint, there is an optimal population size and corresponding run count that

Are Multiple Runs of Genetic Algorithms Better than One?
803
maximizes the chances of reaching a solution of certain quality, if the single-run
success probability increases with larger populations until it reaches a saturation
point (less than 1). The method used in the current paper can be used to ﬁnd
this optimum, but a numerical optimization would be required, because eﬀorts
to characterize the optimal conﬁguration in closed form have been unsuccessful.
Cant´u-Paz and Goldberg [3] compared multiple isolated runs against a single
run that reaches a solution of the same expected quality. They determined that—
even without a ﬁxed time constraint—the savings on execution time seemed
marginal when compared against a single GA, and recommended against using
isolated runs. The ﬁndings in the present paper, however, show that with the
cost constraint there are some cases where multiple runs are advantageous.
Recently, Fuchs [4] and Fern´andez et al. [5] studied empirically multiple iso-
lated runs of genetic programming. They found that in some cases it is advanta-
geous to use multiple small runs. Luke [9] studied the tradeoﬀbetween executing
a single run for many generations or using multiple shorter runs to ﬁnd solutions
of higher quality given a ﬁxed amount of time. In two out of three problems, his
experiments showed that multiple short runs were preferable.
There have been several attempts to characterize the problems in which GAs
perform better than other methods [10,11]. However, without relating the per-
formance of the algorithms to properties of the problems it is diﬃcult to make
predictions and recommendations for unseen problems, even if they belong to
the same class. This paper identiﬁes cases where random search reaches better
solutions based on properties that describe the diﬃculty of the problems.
3
The Gambler’s Ruin Model
It is common in GAs to encode the variables of the problem using a ﬁnite alpha-
bet Σ. A schema is a string over Σ ∪{∗} that represents the set of individuals
that have a ﬁxed symbol F ∈Σ in exactly the same positions as the schema.
The ∗is a “don’t care” symbol that matches anything. For example, in a domain
that uses 10-bit binary strings, the individuals that start with 1 and have a 0 in
the second position are represented by the schema 10 ∗∗∗∗∗∗∗∗.
The number k of ﬁxed positions in a schema is its order. Low-order highly-ﬁt
schemata are sometimes called building blocks (BBs) [12]. Following Harik et
al. [7], we refer to the lowest-order schema that consistently leads to the global
optimum as the correct BB. In this view, the correct BB must (1) match the
global optimum and (2) have the highest average ﬁtness of all the schemata in
the same partition. All other schemata in the partition are labeled as incorrect.
Harik, Cant´u-Paz, Goldberg, and Miller [7] modeled selection in GAs as a
biased random walk. The number of copies of the correct BB in a population
of size n is represented by the position, x, of a particle on a one-dimensional
space. Absorbing barriers at x = 0 and x = n bound the space, and represent
ultimate convergence to the wrong and to the right solutions, respectively. The
initial position of the particle, x0, is the number of copies of the correct BB in
the initial population.

804
E. Cant´u-Paz and D.E. Goldberg
At each step of the random walk there is a probability, p, of obtaining one
additional copy of the correct BB. This probability depends on the problem that
the GA is facing, and Goldberg et al. [13] showed how to calculate it for functions
composed of m uniformly-scaled subfunctions. The probability that a particle
will eventually be captured by the absorbing barrier at x = n is [14]
Pbb(x0, n) =
1 −

q
p
x0
1 −

q
p
n
(1)
where q = 1 −p. Therefore, the expected probability of success is
Ps(n) =
n

x0=0
P0(x0) · Pbb(x0, n),
(2)
where P0(x0) =
 n
x0
 
1
χk
x0 
1 −
1
χk
n−x0
is the probability of having exactly
x0 correct BBs in the initial population, and χ = |Σ| is the cardinality of Σ.
The GR model makes several assumptions, but it has been shown that it
accurately predicts the solution quality of artiﬁcial and real-world problems [7,
15]. For details, the reader is referred to the paper by Harik et al. [7], but one as-
sumption aﬀects the experiments in this paper: Having absorbing walls bounding
the random walk implicitly assumes that mutation and crossover do not create
or destroy BBs. The only source of BBs is the random initialization of the pop-
ulation. This is why the experiments described below do not use mutation.
4
Multiple Small Runs
We measure the quality, Q, of the solution as the number of partitions that
converge to the correct BBs. The probability that one partition converges cor-
rectly is given by the GR model, Ps(n) (Equation 2). For convenience, we use
P1 = Ps(n1) to denote the probability that a partition converges correctly in one
run with population size n1 and Pr = Ps(nr) for the probability that a partition
converges correctly in one of the multiple runs with a population size nr.
4.1
Solution Quality
Under the assumption that the m partitions are independent, the quality has
a binomial distribution with parameters m and Ps(n). Therefore, the expected
solution quality of a single run is E(Q) = mPs(n). Of course, some runs will
reach better solutions than others, and when we use multiple runs we consider
that the problem is solved when one of them ﬁnds a solution of the desired
quality. Let Qr:r denote the quality of the best solution found by r runs of size
nr. We are interested in its expected value, which can be calculated as [16]
E(Qr:r) =
m−1

x=0
1 −F r(x),
(3)

Are Multiple Runs of Genetic Algorithms Better than One?
805
where F(x) = P(Q ≤x) = x
j=0
m
j

P j
r (1 −Pr)m−j is the cumulative distri-
bution function of the solution quality. Unfortunately, there is no closed-form
expression for the means of maximal order statistics of binomial distributions.
However, there are approximations for the extreme order statistics of the Gaus-
sian distribution, and we can use them to make some progress in our analysis.
We can approximate the binomial distribution of the quality with a Gaussian,
and normalize the number of correct partitions by subtracting the mean and di-
viding by the standard deviation: Zr:r =
Qr:r−mPr
√
mPr(1−Pr). Let µr:r = E(Zr:r) denote
the expected value of Zr:r. We can approximate the expected value of the best
quality in r runs as
E(Qr:r) ≈mPr + µr:r

mPr(1 −Pr).
(4)
If there are no restrictions on the total cost, adding more runs to an experiment
results in a higher quality. The problem is that µr:r increases very slowly as more
runs are used: µr:r ≈
√
2 ln r. Therefore, the increase in quality is marginal,
and multiple isolated runs seem unappealing [20].
However, the situation may be diﬀerent if the total cost is constrained. Equa-
tion 4 shows an interesting tradeoﬀ: µr:r grows as r increases, but Pr decreases
because the population size per run must decrease to keep the cost constant. Mul-
tiple runs would perform better than a single one if the quality degradation is not
too pronounced. In fact, the tradeoﬀsuggests that there is an optimal number
of runs and population size that maximize the expected quality. Unfortunately,
we cannot obtain a closed-form expression for these optimal parameters.
The quality reached by multiple runs is better than one run if
mPr + µr:rσr > mP1,
(5)
where σr =

mPr(1 −Pr). We can bound the standard deviation as σr =
0.5√m to obtain an upper bound on the quality of the multiple runs. Substituting
this bound into the inequality above, dividing by m, and rearranging we obtain
µr:r
2√m > P1 −Pr.
(6)
This equation shows that multiple runs are more likely to be beneﬁcial on short
problems (small m), everything else being equal. This is bad news for the case
of multiple runs, because interesting problems in practice may be very long.
The equation above also shows that multiple runs can be advantageous if the
diﬀerence between the solution qualities is small. This may happen at very small
population sizes where the quality is very poor, even for a single run. This case
is not very interesting, because normally we want to ﬁnd high-quality solutions.
However, the diﬀerence is also small when the quality does not improve much
after a critical population size. This is the case that Nakano et al. [8] examined,
and represents an interesting possibility where multiple runs can be beneﬁcial.
The optimum population size is probably near the point where there is no further
improvement: Using a larger population would be a waste of resources, which
would be better used in multiple runs to increase the chance of success.

806
E. Cant´u-Paz and D.E. Goldberg
4.2
Models of Convergence Time
We can write the ﬁxed number of function evaluations that are available as
T = rgnr,
(7)
where g is the domain-dependent number of generations until the population
converges to a unique value, r is the number of independent runs, and nr is the
population size of each run. GAs are often stopped after a ﬁxed number of gener-
ations, with the assumption that they have converged by then. In the remainder
we assume that the generations until convergence are constant. Therefore, to
maintain a ﬁxed total cost, the population size of each of the multiple runs must
be nr = n1/r, where n1 denotes the population size that a single run would use.
Assuming that g is constant may be an oversimpliﬁcation, since it has been
shown that the convergence time depends on factors such as the population size
and the selection intensity, I. For example, under some conditions, the genera-
tions until convergence are given by g ≈π
2
√n
I
[17]. In general, if the generations
until convergence are given by the power-law model g = κnθ, the population size
of each of the multiple runs would have to be nr = n1/r1/(θ+1) to keep the total
cost constant (e.g., in the previous equation, θ = 1/2 and nr would be n1/r2/3).
This form of nr would give an advantage to the multiple runs, because their
sizes (and the quality of their solutions) would not decrease as much as with the
constant g assumption, so this assumption is a conservative one.
4.3
Random Search
Using all the available computation time in one run with a large population
is clearly one extreme. The other extreme are multiple runs with the smallest
population, which is one individual. The latter case is equivalent to random
search, because there is no evolution possible (we are assuming no mutation).
The models above account for the two extreme cases. When the population size
is one, Pr =
1
χk , because only one term in equation 2 is diﬀerent from zero. The
quality of the best solution found by r runs of size one can be calculated with
equation 3.1
To identify when random search can outperform a GA, we calculated the
expected solution quality using equation 3 varying the order of the BBs, k, and
the number of runs. The next section will deﬁne the functions used in these
calculations; for now we only need to know that k varied. Figure 1 shows the
ratio of the quality obtained by random search over the quality found by a
simple GA with a population size of n1 = r. Values over 1 indicate that multiple
runs perform better. The ﬁgure shows that random search has an advantage as
the problems become harder (with longer BBs). However, this peculiar behavior
occurs only at extremely low population sizes, where the solution quality is so low
1 Taking Qr:r = m[1−(1−
1
χk )r] may seem tempting, but it greatly overestimates the
true quality. This calculation implicitly assumes that the ﬁnal solution is formed by
correct BBs that may have been obtained in diﬀerent runs.

Are Multiple Runs of Genetic Algorithms Better than One?
807
5
10
15
runs
3
4
5
6
7
8
k
0
0.5
1
1.5
2
Qr/Q1
5
10
15
runs
(a) Theory
5
10
15
runs
3
4
5
6
7
8
k
1
1.5
2
Qr/Q1
5
10
15
runs
(b) Experiments
Fig. 1. Ratio of the quality of multiple runs of size 1 (random search) vs. a single run
varying the order of the BBs and the number of runs.
that it is of no practical importance. When we increase the population size (and
the number of random search trials), the GA moves ahead of random search.
These results suggest that superlinear speedups can be obtained if random
trials are executed in parallel and the simple GA is used as the base case. Inter-
estingly, Shonkwiler [2] used very small population sizes (≈2 individuals) and
at least two of his functions are easily solvable by random search.
5
Experiments
The GA in the experiments used pairwise tournament selection without replace-
ment, one-point crossover with probability 1, and no mutation. All the results
presented in this section are the average of 200 trials.
The ﬁrst function is the one-max function with a length of m = 25 bits.
We varied the population size nr from 2 to 50 individuals. For each population
size, we varied the number of runs from 1 to 8 and recorded the quality of the
best solution found in any of the runs, Qr:r. Figure 2 shows the ratio of Qr:r
over the quality Q1 that a GA with a population size n1 = rnr reached. The
experiments match the predictions well, and in all cases the larger single runs
reached solutions of better quality than the multiple smaller runs.
To illustrate that multiple runs are more beneﬁcial when m is small, we
conducted experiments varying the length of the problem to m = 100 and m =
400 bits. The population size per run was ﬁxed at nr = 10, and the number of
runs varied from 1 to 8. The results in ﬁgure 3 clearly show that as the problems
become longer, the single large runs ﬁnd better solutions than the multiple runs.

808
E. Cant´u-Paz and D.E. Goldberg
2
4
6
8
Runs
10
20
30
40
50
Pop size
0
0.25
0.5
0.75
1
Qr/Q1
2
4
6
Runs
(a) Theory
2
4
6
8
Runs
10
20
30
40
50
Pop size
0
0.25
0.5
0.75
1
Qr/Q1
2
4
6
Runs
(b) Experiments
Fig. 2. Ratio of the quality of multiple runs vs. a single run for the one-max with
m = 25 bits.
0
2
4
6
8
runs
0.6
0.7
0.8
0.9
1
Qr/Q1
m=400
m=100
m=25
Fig. 3. Ratio of the quality of multiple runs vs. a single run varying the problem size.
The next two test functions are formed by adding fully-deceptive trap func-
tions [18]. The order-k traps are deﬁned as
f (k)
dec(u) =

k −u −1
if u < k,
k
if u = k.
(8)
Two deceptive test function were formed by concatenating m = 25 copies of
f (3)
dec and f (4)
dec. Figures 4 and 5 show the ratio Qr:r/Q1, varying the run size from
2 to 100 individuals and the number of runs from one to eight. The experimental
results are very close to the predictions, except with very small population sizes,
where the GR model is inaccurate. In most cases, the ratio is less than one,
indicating that a single large run reaches a solution with better quality than
multiple small runs. The exceptions occur at very small population sizes, where
even random search performs better.

Are Multiple Runs of Genetic Algorithms Better than One?
809
2
4
6
8
Runs
10
20
30
40
50
Pop size
0
0.25
0.5
0.75
1
Qr/Q1
2
4
6
Runs
(a) Theory
2
4
6
8
Runs
10
20
30
40
50
Pop size
0
0.25
0.5
0.75
1
Qr/Q1
2
4
6
Runs
(b) Experiments
Fig. 4. Ratio of the quality of multiple runs vs. a single run for the order-3 trap.
We performed experiments to validate the results about random search. Fig-
ure 1b shows the ratio of the quality of the solutions found by the best of r
random trials and the solution obtained by a GA with a population size of r.
For each value of k from 3 to 8, the test functions were formed by concatenating
m = 25 order-k trap functions. The experiments show the same general tendency
as the predictions (ﬁgure 1a).
6
Multiple Short Runs
Until now we have examined the solution quality under the constant cost con-
straint and after the population converges to a unique solution. However, in
practice it is common to stop a GA run as soon as it ﬁnds a solution that meets
some quality criterion. The framework introduced in this paper could be applied
to this type of experiment, if we had a model that predicted the solution quality
as a function of time: Ps(n, t). In any generation (or any other suitable time
step), the expected solution quality in one run would be mPs(n, t), but again
we would be interested in the expected value of the best solution in the r runs,
which can be found by substituting the appropriate distribution in equation 3.
There are existing models of quality as a function of time, but they assume
that the population is sized such that the GA will reach the global solution and
that recombination of BBs is perfect [17]. If we adopt these assumptions, we could
use the existing models, but we would not be able to reduce the population size
to respect the constraint of ﬁxed cost. M¨uhlenbein and Schlierkamp-Voosen [17]
derived the following expression for the one-max function:
Ps(n, t) = 1
2
	
1 + sin( I
√nt)

,
(9)

810
E. Cant´u-Paz and D.E. Goldberg
2
4
6
8
Runs
10
20
30
40
50
Pop size
0
0.25
0.5
0.75
1
Qr/Q1
2
4
6
Runs
(a) Theory
2
4
6
8
Runs
10
20
30
40
50
Pop size
0
0.25
0.5
0.75
1
Qr/Q1
2
4
6
Runs
(b) Experiments
Fig. 5. Ratio of the quality of multiple runs vs. a single run for the order-4 trap.
0
10
20
30
40
50
runs
0.8
0.85
0.9
0.95
1
Gr/G1
m=25
m=50
m=100
m=200
Fig. 6. Ratio of the generations until convergence of multiple over single runs. The
total cost is not constant.
and Miller and Goldberg [19] used it successfully to predict the quality of de-
ceptive functions. If we abandon the cost constraint, we can show that the best
of multiple runs of the same size (that is at least large enough to reach the
global optimum) reaches the solution in fewer generations than a single run of
the same size. This argument has been used in the past to support the use of
multiple parallel runs [2].
Figure 6 shows the ratio of the number of generations until convergence (to
the global) of multiple runs over the number of generations of convergence of
a single run. The ﬁgure shows that the time decreases as more runs are used,
and the advantage is more pronounced for shorter problems. If each run was
executed concurrently on a diﬀerent processor of a parallel machine, the elapsed
time to reach the solution would be reduced (assuming that the cost to determine
convergence by any run is negligible, which may not be the case). However, this

Are Multiple Runs of Genetic Algorithms Better than One?
811
scheme oﬀers a relatively small advantage, and it is probably not the best use of
multiple processors since we can obtain almost linear speedups in other ways [20].
7
Summary and Conclusions
There are conﬂicting reports of the advantage of using one or multiple indepen-
dent runs. This problem has consequences on parallel GAs with isolated popu-
lations and also to determine when random search can outperform a GA. This
paper presented an analytical study that considered additively-separable func-
tions. Under a constraint of ﬁxed cost and assuming no mutation, the analysis
showed that the expected quality of the solution reached by multiple indepen-
dent small runs is higher than the quality reached by a single large run only in
very limited conditions. In particular, multiple runs seem advantageous at very
small population sizes, which result in solutions of poor quality, and close to a
saturation point where the solution quality does not improve with increasingly
larger populations. In addition, the greatest advantage of multiple independent
runs is on short problems, and the advantage tends to decrease with higher BB
order. The results suggest that for diﬃcult problems (long and with high-order
BBs), the best alternative is to use a single run with the largest population
possible. Small independent runs should be avoided.
Acknowledgments. We would like to thank Hillol Kargupta, Jeﬀrey Horn,
and Georges Harik for many interesting discussions on this topic. UCRL-JC-
142172. This work was performed under the auspices of the U.S. Department
of Energy by University of California Lawrence Livermore National Laboratory
under contract no. W-7405-Eng-48. Portions of this work were sponsored by the
Air Force Oﬃce of Scientiﬁc Research, Air Force Materiel Command, USAF,
under grant F49620-00-0163. Research funding for this work was also provided
by the National Science Foundation under grant DMI-9908252.
References
1. Tanese, R.:
Distributed genetic algorithms.
In Schaﬀer, J.D., ed.: Proceedings
of the Third International Conference on Genetic Algorithms, Morgan Kaufmann
(1989) 434–439
2. Shonkwiler, R.: Parallel genetic algorithms. In Forrest, S., ed.: Proceedings of the
Fifth International Conference on Genetic Algorithms, Morgan Kaufmann (1993)
199–205
3. Cant´u-Paz, E., Goldberg, D.E.:
Modeling idealized bounding cases of parallel
genetic algorithms. In Koza, J., et al., eds.: Proceedings of the Second Annual
Genetic Programming Conference, Morgan Kaufmann (1997) 353–361
4. Fuchs, M.: Large populations are not always the best choice in genetic program-
ming. In Banzhaf, W., et al., eds.: Proceedings of the Genetic and Evolutionary
Computation Conference, Morgan Kaufmann (1999) 1033–1038

812
E. Cant´u-Paz and D.E. Goldberg
5. Fern´andez, F., Tomassini, M., Punch, W., S´anchez, J.M.:
Experimental study
of isolated multipopulation genetic programming.
In Whitley, D., et al., eds.:
Proceedings of the Genetic and Evolutionary Computation Conference, Morgan
Kaufmann (2000) 536
6. Wolpert, D., Macready, W.: No-free-lunch theorems for optimization. IEEE Trans-
actions on Evolutionary Computation 1 (1997) 67–82
7. Harik, G., Cant´u-Paz, E., Goldberg, D., Miller, B.L.: The gambler’s ruin problem,
genetic algorithms, and the sizing of populations. Evolutionary Computation 7
(1999) 231–253
8. Nakano, R., Davidor, Y., Yamada, T.: Optimal population size under constant
computation cost. In Davidor, Y., Schwefel, H.P., M¨anner, R., eds.: Parallel Prob-
lem Solving fron Nature, PPSN III, Berlin, Springer-Verlag (1994) 130–138
9. Luke, S.: When short runs beat long runs. In Spector, L. et al., eds.: Proceedings of
the Genetic and Evolutionary Computation Conference, Morgan Kaufmann (2001)
74–80
10. Mitchell, M., Holland, J.H., Forrest, S.: When will a genetic algorithm outperform
hill climbing? In Advances in Neural Information Processing Systems 6 (1994)
51–58
11. Baum, E., Boneh, D., Garrett, C.: Where genetic algorithms excel. Evolutionary
Computation 9 (2001) 93–124
12. Goldberg, D.E.: Genetic algorithms in search, optimization, and machine learning.
Addison-Wesley, Reading, MA (1989)
13. Goldberg, D.E., Deb, K., Clark, J.H.: Genetic algorithms, noise, and the sizing of
populations. Complex Systems 6 (1992) 333–362
14. Feller, W.: An Introduction to probability theory and its applications. 2nd edn.
Volume 1. John Wiley and Sons, New York, NY (1966)
15. van Dijk, S., Thierens, D., de Berg, M.: Scalability and eﬃciency of genetic al-
gorithms for geometrical applications.
In Schoenauer, M., et al., eds.: Parallel
Problem Solving from Nature—PPSN VI, Berlin, Springer-Verlag (2000) 683–692
16. Arnold, B., Balakrishnan, N., Nagaraja, H.N.: A ﬁrst course in order statistics.
John Wiley and Sons, New York, NY (1992)
17. M¨uhlenbein, H., Schlierkamp-Voosen, D.: Predictive models for the breeder genetic
algorithm: I. Continuous parameter optimization. Evolutionary Computation 1
(1993) 25–49
18. Deb, K., Goldberg, D.E.: Analyzing deception in trap functions. In Whitley, L.D.,
ed.: Foundations of Genetic Algorithms 2, Morgan Kaufmann (1993) 93–108
19. Miller, B.L., Goldberg, D.E.: Genetic algorithms, selection schemes, and the vary-
ing eﬀects of noise. Evolutionary Computation 4 (1996) 113–131
20. Cant´u-Paz, E.: Eﬃcient and Accurate Parallel Genetic Algorithms. Kluwer Aca-
demic Publishers, Boston, MA (2000)

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 813–824, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Constrained Multi-objective Optimization Using Steady
State Genetic Algorithms
Deepti Chafekar, Jiang Xuan, and Khaled Rasheed
Computer Science Department
University of Georgia
Athens, GA 30602 USA
{chafekar, xuan, khaled}@cs.uga.edu
Abstract. In this paper we propose two novel approaches for solving
constrained multi-objective optimization problems using steady state GAs.
These methods are intended for solving real-world application problems that
have many constraints and very small feasible regions. One method called
Objective Exchange Genetic Algorithm for Design Optimization (OEGADO)
runs several GAs concurrently with each GA optimizing one objective and
exchanging information about its objective with the others. The other method
called Objective Switching Genetic Algorithm for Design Optimization
(OSGADO) runs each objective sequentially with a common population for all
objectives. Empirical results in benchmark and engineering design domains are
presented. A comparison between our methods and Non-Dominated Sorting
Genetic Algorithm-II (NSGA-II) shows that our methods performed better than
NSGA-II for difficult problems and found Pareto-optimal solutions in fewer
objective evaluations. The results suggest that our methods are better applicable
for solving real-world application problems wherein the objective computation
time is large.
1
Introduction
This paper concerns the application of steady state Genetic Algorithms (GAs) in
realistic engineering design domains which usually involve simultaneous optimization
of multiple and conflicting objectives with many constraints. In these problems
instead of a single optimum there usually exists a set of trade-off solutions called the
non-dominated solutions or Pareto-optimal solutions. For such solutions no
improvement in any objective is possible without sacrificing at least one of the other
objectives. No other solutions in the search space are superior to these Pareto-optimal
solutions when all objectives are considered. The user is then responsible for choosing
a particular solution from the Pareto-optimal set later.
Some of the challenges faced in the application of GAs to engineering design domains
are:

814         D. Chafekar, J. Xuan, and K. Rasheed
 The search space can be very complex with many constraints and the feasible
(physically realizable) region in the search space can be very small.
 Determining the quality (fitness) of each point may involve the use of a simulator
or an analysis code which takes a non-negligible amount of time. This simulation
time can range from a fraction of a second to several days in some cases. Therefore
it is impossible to be cavalier with the number of objective evaluations in an
optimization.
For such problems steady state GAs may perform better than generational GAs
because they better retain the feasible points found in their populations and may have
higher selection pressure which is desirable when evaluations are very expensive.
With good diversity maintenance, steady state GAs have done very well in several
realistic domains [1]. Significant research has yet to be done in the area of steady state
multi-objective GAs. We therefore decided to focus our research on this area.
The area of multi-objective optimization using Evolutionary Algorithms (EAs) has
been explored for a long time. The first multi-objective GA implementation called the
Vector Evaluated Genetic Algorithm (VEGA) was proposed by Schaffer in 1985 [9].
Since then, many Evolutionary algorithms for solving multi-objective optimization
problems have been developed. The most recent ones are the Non-Dominated Sorting
Genetic Algorithm-II (NSGA-II) [3], Strength Pareto Evolutionary Algorithm-II
(SPEA-II) [16], Pareto Envelope based selection-II (PESA-II) [17]. Most of these
approaches propose the use of a generational GA. Deb proposed an Elitist Steady
State Multi-objective Evolutionary Algorithm (MOEA) [18] which attempts to
maintain spread [15] while attempting to converge to the true Pareto-optimal front.
This algorithm requires sorting of the population for every new solution formed
thereby increasing its time complexity. Very high time complexity makes the Elitist
steady state MOEA impractical for some problems. To the best of our knowledge,
apart from Elitist Steady State MOEA, the area of steady state multi-objective GAs
has not been widely explored. Also constrained multi-objective optimization which is
very important for real-world application problems has not received the deserved
exposure. In this paper we propose two methods for solving constrained multi-
objective optimization using steady state GAs. These methods are relatively fast and
practical. It is also easy to transform a single-objective GA to a multi-objective GA by
using these methods.
In the first method called the Objective Exchange Genetic Algorithm for Design
Optimization (OEGADO) several single objective GAs run concurrently. Each GA
optimizes one of the objectives. At certain intervals these GAs exchange information
about their respective objectives with each other. In the second method called the
Objective Switching Genetic Algorithm for Design Optimization (OSGADO) a single
GA runs multiple objectives in a sequence switching at certain intervals between
objectives.
Our methods can be viewed as multi-objective transformations of GADO (Genetic
Algorithm for Design Optimization) [1, 2]. GADO is a GA that was designed with the
goal of being suitable for the use in engineering design. It uses new operators and

Constrained Multi-objective Optimization Using Steady State Genetic Algorithms         815
search control strategies that target engineering domains. GADO has been applied in a
variety of optimization tasks which span many fields. It has demonstrated a great deal
of robustness and efficiency relative to competing methods.
In GADO, each individual in the GA population represents a parametric description
of an artifact. All parameters have continuous intervals. The fitness of each individual
is based on the sum of a proper measure of merit computed by a simulator or some
analysis code, and a penalty function if relevant. A steady state model is used, in
which several crossover and mutation operators including specific and innovative
operators like guided crossover are applied to two parents selected by linear rank
based selection. The replacement strategy used is a crowding technique, which takes
into consideration both the fitness and the proximity of the points in the GA
population. GADO monitors the degree of diversity of the GA population. If at any
stage it is discovered that the individuals in the population became very similar to one
another, the diversity maintenance module rebuilds the population using previously
evaluated points in a way that restores diversity. The diversity maintenance module in
GADO also rejects proposed points that are extremely similar to previously evaluated
points. The GA stops when either the maximum number of evaluations has been
exhausted or the population loses diversity and practically converges to a single point
in the search space. Floating point representation is used. GADO also uses some
search control strategies [2] such as a screening module which saves time by avoiding
the full evaluation of points that are unlikely to correspond to good designs.
We compared the results of our two methods with the state-of-the-art Elitist Non-
Dominated Sorting Algorithm-II (NSGA-II) [3]. NSGA-II is a non-dominated sorting
based multi-objective evolutionary algorithm with a computational complexity of
O(MN2) (where M is the number of objectives and N is the population size). NSGA-II
incorporates an elitist approach, a parameter-less niching approach and a simple
constraint handling strategy. Due to NSGA-II’s low computational requirements,
elitist features and constraint handling capacity, it has been successfully used in many
applications. It proved to be better than many other multi-objective optimization GAs
[3, 18].
In the remainder of the paper, we provide a brief description of our two proposed
methods. We then present results of the comparison of our methods with NSGA-II.
Finally, we conclude the paper with a discussion of the results and future work.
2
Methods for Multi-objective Optimization Using Steady State
GAs
We propose two methods for solving constrained multi-objective optimization
problems using steady state GAs. One is the Objective Exchange Genetic Algorithm
for Design Optimization (OEGADO), and other is the Objective Switching Genetic
Algorithm for Design Optimization (OSGADO). It should be noted that for multi-
objective GAs, maintaining diversity is a key issue. However we did not need to take

816         D. Chafekar, J. Xuan, and K. Rasheed
any extra measures for diversity maintenance as the diversity maintenance module
already present in GADO [1, 2] seemed to handle this issue effectively. We focused
on the case of two objectives in our experiments for simplicity of implementation and
readability of the results, but the methods are applicable for multi-objective
optimization problems with more than two objectives.
2.1
Objective Exchange Genetic Algorithm for Design Optimization
(OEGADO)
The main idea of OEGADO is to run several single objective GAs concurrently. Each
of the GAs optimizes one of the objectives. All the GAs share the same representation
and constraints, but have independent populations. They exchange information about
their respective objectives every certain number of iterations.
In our implementation, we have used the idea of informed operators (IOs) [4]. The
main idea of the IOs is to replace pure randomness in traditional GA operators with
decisions that are guided by reduced models formed using the methods presented in
[5, 6, 7]. The reduced models are approximations of the fitness function, formed using
some approximation techniques, such as least squares approximation [5, 7, 8]. These
functional approximations are then used to make the GA operators such as crossover
and mutation more informed. These IOs generate multiple children [4], rank them
using the approximate fitness obtained from the reduced model and select the best.
Every single objective GA in OEGADO uses least squares to form a reduced model
of its own objective. Every GA exchanges its own reduced model with those of the
other GAs. In effect, every GA, instead of using its own reduced model, uses other
GAs’ reduced models to compute the approximate fitness of potential individuals.
Therefore each GA is informed about other GAs’ objectives. As a result each GA not
only focuses on its own objective, but also gets biased towards the objectives which
the other GAs are optimizing.
The OEGADO algorithm for two objectives looks as follows:
1. Both the GAs are run concurrently for the same number of iterations, each GA
optimizes one of the two objectives while also forming a reduced model of it.
2. At intervals equal to twice the population size, each GA exchanges its reduced
model with the other GA.
3. The conventional GA operators such as initialization (only applied in the
beginning), mutation and crossover are replaced by informed operators. The IOs
generate multiple children and use the reduced model to compute the approximate
fitness of these children. The best individual based on this approximate fitness is
selected to be the newborn. It should be noted that the approximate fitness function
used is of the other objective.
4. The true fitness function is then called to evaluate the actual fitness of the newborn
corresponding to the current objective.
5. The individual is then added to the population using the replacement strategy.

Constrained Multi-objective Optimization Using Steady State Genetic Algorithms         817
6. Steps 2 through 5 are repeated till the maximum number of evaluations is
exhausted.
If all objectives have similar computational complexity, the concurrent GAs can be
synchronized, so that they exchange the current approximations at the right time. On
the other hand, when objectives vary considerably in their time complexity, the GAs
can be run asynchronously.
It should be noted that OEGADO is not really a multi-objective GA, but several
single objective GAs working concurrently to get the Pareto-optimal region. Each GA
finds its own feasible region, by evaluating its own objective. For the feasible points
found by a single GA, we need to run the simulator to evaluate the remaining
objectives. Thus for OEGADO with two objectives:
Total number of objective evaluations = Sum of objective evaluations of each GA +
Sum of the number of feasible points found by each GA
A potential advantage of this method is speed, as the concurrent GAs can run in
parallel. Therefore multiple objectives can be evaluated at the same time on different
CPUs. Also the asynchronous OEGADO works better for objectives having different
time complexities. If some objectives are fast, they are not slowed down by the slower
objectives. It should be noted that because of the exchange of reduced models, each
GA optimizes its own objective and also gives credit to the other objectives.
2.2
Objective Switching Genetic Algorithm for Design Optimization
(OSGADO)
The main idea of OSGADO is to use a single GA that optimizes multiple objectives in
a sequential order. Every objective is optimized for a certain number of evaluations,
then a switch occurs and the next objective is optimized. The population is not
changed when objectives are switched. This continues till the maximum number of
evaluations is complete.
We modified GADO [1, 2] to create multi-objective OSGADO. OSGADO is inspired
from the Vector Evaluated GA (VEGA) [9]. Schaffer (1985) proposed VEGA for
generational GAs. In VEGA the population is divided into m different parts for m diff
objectives; part i is filled with individuals that are chosen at random from current
population according to objective i. Afterwards the mating pool is shuffled and
crossover and mutation are performed as usual. Though VEGA gave encouraging
results, it suffered from bias towards the extreme regions of the Pareto-optimal curve.
The OSGADO algorithm looks as follows:
1. The GA is run initially with the first objective as the measure of merit for a certain
number of evaluations. The fitness of an individual is calculated based on its
measure of merit and the constraint violations. Selection, crossover and mutation
take place in the regular manner.

818         D. Chafekar, J. Xuan, and K. Rasheed
2. After a certain numbers of evaluations, the GA is run for the next objective. When
the evaluations for the last objective are complete, the GA switches back to the
first objective.
3. Step 2 is repeated till the maximum number of evaluations is reached.
In order to fairly compare the methods, in the experiments we first ran OEGADO and
obtained the number of feasible points found by each of the two GAs. We then ran
OSGADO for the number of evaluations calculated as follows,
Total number of objective evaluations = Sum of evaluations of each objective in
OEGADO + Sum of the number of feasible points found by each objective in
OEGADO
OSGADO has certain advantages over VEGA. In VEGA every solution is evaluated
for only one of the objectives each time and therefore it can converge to individual
objective optima (the extremes of the Pareto-optimal curve) without adequately
sampling the middle section of the Pareto-optimal curve. However OSGADO
evaluates every solution using each of the objectives at different times. So OSGADO
is at less risk of converging at individual objective optima.
3
Experimental Results
In this section, we first describe the test problems used to compare the performance of
OEGADO, OSGADO and NSGA-II. We then briefly discuss the parameter settings
used. Finally, we discuss the results obtained for various test cases by these three
methods.
3.1
Test Problems
The test problems for evaluating the performance of our methods were chosen based
on significant past studies. We chose four problems from the benchmark domains
commonly used in past multi-objective GA research, and two problems from the
engineering domains. The degree of difficulty of these problems varies from fairly
simple to difficult.
The problems chosen from the benchmark domains are BNH used by Binh and Korn
[10], SRN used by Srinivas, Deb [11], TNK suggested by Tanaka [12] and OSY used
by Osyczka, Kundu [13]. The problems chosen from the engineering domains are
Two-Bar Truss Design used by Deb [14] and Welded Beam design used by Deb [14].
All these problems are constrained multi-objective problems. Table 1 shows the
variable bounds, objective functions and constraints for all these problems.

Constrained Multi-objective Optimization Using Steady State Genetic Algorithms         819
Table 1. Test problems used in this study, all objective functions are to be minimized
Problem
Variable
bounds
Objectives functions f (x) and
Constraints C(x)
BNH
]3,0
[
]
5,0
[
2
1
∈
∈
x
x
7.7
)3
(
)
8
(
)
x
(
25
)
5
(
)
x
(
)
5
(
)
5
(
)
x
(
4
4
)
x
(
2
2
2
1
2
2
2
2
1
1
2
2
2
1
2
2
2
2
1
1
≥
+
+
−
≡
≤
+
−
≡
−
+
−
=
+
=
x
x
C
x
x
C
x
x
f
x
x
f
SRN
]
20
,
20
[
]
20
,
20
[
2
1
−
∈
−
∈
x
x
0
10
3
)
x
(
225
)
x
(
)1
(
9
)
x
(
)
2
(
)
2
(
2
)
x
(
2
1
2
2
2
2
1
1
2
2
1
2
2
2
2
1
1
≤
+
−
≡
≤
+
≡
−
−
=
−
+
−
+
=
x
x
C
x
x
C
x
x
f
x
x
f
TNK
]
,0
[
]
,0
[
2
1
π
π
∈
∈
x
x
5.0
)
5.0
(
)
5.0
(
)
x
(
0
)
arctan
16
cos(
1.0
1
)
x
(
)
x
(
)
x
(
2
2
2
1
2
2
1
2
2
2
1
1
2
2
1
1
≤
−
+
−
≡
≥
−
−
+
≡
=
=
x
x
C
x
x
x
x
C
x
f
x
f
OSY
]
10
,0
[
]
5,1[
]
6,0
[
]
5,1[
]
10
,0
[
]
10
,0
[
6
5
4
3
2
1
∈
∈
∈
∈
∈
∈
x
x
x
x
x
x
0
4
)3
(
)
x
(
0
)3
(
4
)
x
(
0
3
2
)
x
(
0
2
)
x
(
0
6
)
x
(
0
2
)
x
(
)
x
(
]
)1
(
)
4
(
  
          
)1
(
)
2
(
)
2
(
25
[
)
x
(
6
2
5
6
4
2
3
5
2
1
4
1
2
3
2
1
2
2
1
1
2
6
2
5
2
4
2
3
2
2
2
1
2
2
5
2
4
2
3
2
2
2
1
1
≥
−
+
−
≡
≥
−
−
−
≡
≥
+
−
≡
≥
+
−
≡
≥
−
−
≡
≥
−
+
≡
+
+
+
+
+
=
−
+
−
+
−
+
−
+
−
−
=
x
x
C
x
x
C
x
x
C
x
x
C
x
x
C
x
x
C
x
x
x
x
x
x
f
x
x
x
x
x
f
Two-bar
Truss
Design
]
3,1[
]
01
.0,0
[
]
01
.0,0
[
3
2
1
∈
∈
∈
x
x
x
3
2
2
3
2
3
1
2
3
1
5
2
1
1
2
1
2
2
3
2
2
3
1
1
/
1
80
/
16
20
10
)
,
max(
)
x
(
)
,
max(
)
x
(
1
16
)
x
(
x
x
x
x
x
x
C
f
x
x
x
x
f
+
=
+
=
≤
≡
=
+
+
+
=
σ
σ
σ
σ
σ
σ

820         D. Chafekar, J. Xuan, and K. Rasheed
Welded
Beam
Design
]
10
,1.0
[
]
10
,1.0
[
]
5,
125
.0
[
]
5,
125
.0
[
∈
∈
∈
∈
t
l
b
h
3
2
2
2
2
2
"
’
2
2
"
’
2
"
2
’
4
3
2
1
3
2
2
1
)
0282346
.0
1(
022
.
64746
/
504000
)
)
(
25
.0
12
/
(
2
2
)
)
(
(
25
.0
)
5.0
14
(
6000
2
/
6000
)
)
(
(
25
.0
/
)
(
)
(
0
6000
)
x
(
)
x
(
0
)
x
(
0
)
x
(
30000
)
x
(
0
)
x
(
13600
)
x
(
/
1952
.2
)
x
(
)
14
(
04811
.0
10471
.1
)
x
(
tb
t
P
b
t
t
h
l
hl
t
h
l
l
hl
t
h
l
l
P
C
h
b
C
C
C
b
t
f
l
tb
l
h
f
c
c
−
=
=
+
+
+
+
+
=
=
+
+
+
+
=
≥
−
≡
≥
−
≡
≥
−
≡
≥
−
≡
=
+
+
=
σ
τ
τ
τ
τ
τ
τ
τ
σ
τ
3.2
Parameter Settings
Each optimization run was carried out with similar parameter settings for all the
methods. The following are the parameters for the three GAs.
Let ndim be equal to the number of dimensions of the problems.
1. Population size: For OEGADO and OSGADO the population size was set to
10*ndim. For NSGA-II the population size was fixed to 100 as recommended in
[19].
2. Number of objective evaluations: Since the three methods work differently the
number of objective evaluations is computed differently. The number of objective
evaluations for OEGADO and OSGADO according to Section 2.1 and 2.2 is given
as Objective evaluations for OEGADO and OSGADO = 2*500*ndim + sum of
feasible points found by each GA in OEGADO model
NSGA-II is a generational GA, therefore for a two-objective NSGA-II:
Total number of objective evaluations =2*population size * number of generations
Since we did not know exactly how many evaluations would be required by
OEGADO before hand, to give fair treatment to NSGA-II, we set the number of
generations of NSGA-II to be 10*ndim. In effect NSGA-II ended up doing
significantly more evaluations than OEGADO and OSGADO for some problems.
We however did not decrease the number of generations for NSGA-II and repeat
the experiments as our methods outperformed it in most domains anyway.
3.3
Results
In the following section, Figures 1-4 present the graphical results of all three methods
in the order of OEGADO, OSGADO and NSGA-II for all problems. The outcomes of
five runs using different seeds were unified and then the non-dominated solutions
were selected and plotted from the union set for each method. We are using graphical

Constrained Multi-objective Optimization Using Steady State Genetic Algorithms         821
representations of the Pareto-optimal curve found by the three methods to compare
their performance.
It is worth mentioning that the number of Pareto-optimal solutions obtained by
NSGA-II is limited by its population size. Our methods keep track of all the feasible
solutions found during the optimization and therefore do not have any restrictions on
the number of Pareto-optimal solutions found.
The BNH and the SRN (figures not shown) problems are fairly simple in that the
constraints may not introduce additional difficulty in finding the Pareto-optimal
solutions. It was observed that all three methods performed equally well within
comparable number of objective evaluations (mentioned in Section 3.2), and gave a
dense sampling of solutions along the true Pareto-optimal curve.
Fig. 1. Results for the benchmark problem TNK
Fig. 2. Results for the benchmark problem OSY
The TNK problem (Fig. 1) and the OSY problem (Fig. 2) are relatively difficult. The
constraints in the TNK problem make the Pareto-optimal set discontinuous. The
constraints in the OSY problem divide the Pareto-optimal set into five regions that
can demand a GA to maintain its population at different intersections of the constraint
boundaries. As it can be seen from the above graphs for the TNK problem, within
comparable number of fitness evaluations, the OEGADO model and the NSGA-II
model performed equally well. They both displayed a better distribution of the Pareto-

822         D. Chafekar, J. Xuan, and K. Rasheed
optimal points than the OSGADO model. OSGADO performed well at the extreme
ends, but found very few Pareto points at the mid-section of the curve. For the OSY
problem, it can be seen that OEGADO gave a good sampling of points at the mid-
section of the curve and also found points at the extreme ends of the curve. OSGADO
also performed well, giving better sampling at one of the extreme ends of the curve.
NSGA-II however did not give a good sampling of points at the extreme ends of the
Pareto-optimal curve and gave a poor distribution of the Pareto-optimal solutions. In
this problem OEGADO and OSGADO outperformed NSGA-II while running for
fewer objective evaluations.
Fig. 3. Results for the Two-bar Truss design problem
For the Two-bar Truss design problem (Fig. 3), within comparable fitness
evaluations, NSGA-II performed slightly better than our methods in the first
objective. OEGADO showed a uniform distribution of the Pareto-optimal curve.
OSGADO however gave a poor distribution at one end of the curve, but it achieved
very good solutions at the other end and converged to points that the other two
methods failed to reach.
Fig. 4. Results for the Welded Beam design problem
In the Welded Beam design problem (Fig. 4), the non-linear constraints can cause
difficulties in finding the Pareto solutions. As shown in Fig. 4, within comparable
fitness evaluations, OEGADO outperformed OSGADO and NSGA-II in both
distribution and spread [15]. OEGADO found the best minimum solution for f1 with a
value of 2.727 units. OSGADO was able to find points at the other end that the other

Constrained Multi-objective Optimization Using Steady State Genetic Algorithms         823
two methods failed to reach. NSGA-II did not achieve a good distribution of the
Pareto solutions at the extreme regions of the curve.
4
Conclusion and Future Work
In this paper we presented two methods for multi-objective optimization using steady
state GAs, and compared our methods with a reliable and efficient generational multi-
objective GA called NSGA-II. The results show that a steady state GA can be used
efficiently for constrained multi-objective optimization. For the simpler problems our
methods performed equally well as NSGA-II. For the difficult problems, our methods
outperformed NSGA-II in most respects. In general, our methods demonstrated
robustness and efficiency in their performance. OEGADO in particular performed
consistently well and outperformed the other two methods in most of the domains.
Moreover, our methods were able to find the Pareto-optimal solutions for all the
problems in fewer objective evaluations than NSGA-II. For real-world problems, the
number of objective evaluations performed can be critical as each objective
evaluation takes a long time. Based on this study we believe that our methods can
outperform multi-objective generational GAs for such problems. However, we need
to experiment more and find out whether there are other factors that contribute to the
success of our methods other than their steady state nature.
In the future, we would like to experiment with several steady state GAs as the base
method. We would also like to improve both of our methods. Currently they do not
have any explicit bias towards non-dominated solutions. We therefore intend to
enhance them by giving credit to non-dominated solutions. OEGADO has shown
promising results and we would like to further improve it, extend its implementation
to handle more than two objectives and further explore its capabilities. The current
OSGADO implementation can already handle more than two objectives. We would
also like to use our methods for more complex real-world applications.
Acknowledgement. This research is sponsored by the US National Science
Foundation under grant CTS-0121058. The program managers are Drs. Frederica
Darema, C. F. Chen and Michael Plesniak.
References
1. 
Khaled Rasheed. GADO: A genetic algorithm for continuous design optimization.
Technical Report DCS-TR-352, Department of Computer Science, Rutgers, The State
University of New Jersey, New Brunswick, NJ, January 1998. Ph.D. Thesis, 
http://webster.cs.uga.edu/~khaled/thesis.ps.
2. 
Khaled Rasheed and Haym Hirsh. Learning to be selective in genetic-algorithm-based
design optimization. Artificial Intelligence in Engineering, Design, Analysis and
Manufacturing, 13:157–169, 1999.
3. 
Deb, K., S. Agrawal, A. Pratap, and T. Meyarivan (2000). A fast elitist non-dominated
sorting genetic algorithm for multi-objective optimization: NSGA-II. In Proceedings of
the Parallel Problem Solving from Nature VI, pp.849–858.

824         D. Chafekar, J. Xuan, and K. Rasheed
4. 
Khaled Rasheed and Haym Hirsh. Informed operators: Speeding up genetic-algorithm-
based design optimization using reduced models. In Proceedings of the Genetic and
Evolutionary Computation Conference (GECCO’2000), pp. 628–635, 2000.
5. 
K. Rasheed., S. Vattam, X. Ni. Comparison of Methods for Using Reduced Models to
Speed up Design Optimization. In Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO’2002), pp. 1180–1187, 2002.
6. 
Khaled Rasheed. An incremental-approximate-clustering approach for developing
dynamic reduced models for design optimization. In Proceedings of the Congress on
Evolutionary Computation (CEC’2002), pp. 986–993, 2002.
7. 
K. Rasheed., S. Vattam, X. Ni. Comparison of methods for developing dynamic reduced
models for design optimization. In Proceedings of the Congress on Evolutionary
Computation (CEC’2002), pp. 390–395, 2002.
8. 
William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery.
Numerical Recipes in C: the Art of Scientific Computing. Cambridge University Press,
Cambridge [England]; New York, 2nd edition, 1992.
9. 
J.D.Schaffer .Multi-objective optimization with vector evaluated genetic algorithms. In
Proceedings of an International Conference on Genetic Algorithms and Their
Applications, J.J. Grefenstette, Ed., Pittsburg, PA, July 24–26 1985, pp. 93–100,
sponsored by Texas Instruments and U.S. Navy Center for Applied Research in Artificial
Intelligence (NCARAI).
10. Binh and Korn. MOBES: A multi-objective Evolution Strategy for constrained
optimization Problems. In Proceedings of the 3rd International Conference on Genetic
Algorithm MENDEL 1997, Brno, Czech Republic, pp.176–182.
11. Srinivas, N. and Deb, K. (1995). Multi-Objective function optimization using non-
dominated sorting genetic algorithms. Evolutionary Computation (2), 221–248.
12. Tanaka, M. (1995). GA-based decision support system for multi-criteria, optimization. In
Proceedings of the International Conference on Systems, Man and Cybernetics-2, pp.
1556–1561.
13. Osycza, A. and Kundu, S. (1995). A new method to solve generalized multicriteria
optimization problems using the simple genetic algorithm. Structural Optimization (10).
94–99.
14. Deb, K. Pratap, A. and Moitra, S. (2000). Mechanical Component Design for Multiple
Objectives Using Elitist Non-Dominated Sorting GA. KanGAL Report No. 200002.
15. Ranjithan, S.R., S.K. Chetan, and H.K. Dakshina (2001). Constraint method-based
evolutionary algorithm (CMEA) for multi-objective optimization. In E.Z. et al. (Ed.),
Evolutionary Multi-Criteria Optimization 2001, Lecture Notes in Computer Science 1993,
pp. 299–313. Springer-Verlag.
16. Zitzler, E., Laumanns, M., and Thiele, L. (2001). SPEA2: Improving the Strength Pareto
Evolutionary Algorithm. Technical Report 103, Computer Engineering and Networks
Laboratory (TIK), Swiss Federal Institute of Technology (ETH) Zurich, Gloriastrasse 35,
CH-8092 Zurich, Switzerland.
17. Crone, D. W., Knowles, J.D., and Oates, M.J. (2000). The Pareto Envelope-based
Selection Algorithm for Multi-objective Optimization. In Schoenauer, M., Deb, K.,
Rudolph, g., Yao, X., Luton, E., Merelo, J.J., and Schewfel, H.-P., editors, Proceedings of
the Parallel Problem Solving from Nature VI Conference, pp.839–848, Paris, France.
Springer. Lecture Notes in Computer Science No. 1917.
18. K. Deb. Multi-objective optimization using evolutionary algorithms. Chichester, UK: John
Wiley, 2001.
19. K. Deb. S. Gulati (2001). Design of truss-structures for minimum weight using genetic
algorithms, In Journal of Finite Elements in Analysis and Design, pp.447–465, 2001.

An Analysis of a Reordering Operator with
Tournament Selection on a GA-Hard Problem
Ying-ping Chen1 and David E. Goldberg2
1 Department of Computer Science and Department of General Engineering
University of Illinois, Urbana, IL 61801, USA
ypchen@illigal.ge.uiuc.edu
2 Department of General Engineering
University of Illinois, Urbana, IL 61801, USA
deg@uiuc.edu
Abstract. This paper analyzes the performance of a genetic algorithm
that utilizes tournament selection, one-point crossover, and a reordering
operator. A model is proposed to describe the combined eﬀect of the
reordering operator and tournament selection, and the numerical solu-
tions are presented as well. Pairwise, s-ary, and probabilistic tournament
selection are all included in the proposed model. It is also demonstrated
that the upper bound of the probability to apply the reordering opera-
tor, previously derived with proportionate selection, does not aﬀect the
performance. Therefore, tournament selection is a necessity when using
a reordering operator in a genetic algorithm to handle the conditions
studied in the present work.
1
Introduction
In order to ensure a genetic algorithm (GA) works well, the building blocks
represented in the chromosome of the underlying problem have to be tightly
linked. Otherwise, studies [1,2] have shown that a GA may fail to solve prob-
lems without such prior knowledge. Because it is diﬃcult to guarantee that the
chosen chromosome representation can provide tightly linked building blocks for
processing, linkage learning operators should be adopted to overcome the diﬃ-
culty, which is called the coding trap [3]. Currently, one way to conduct linkage
learning is to use the (gene number, allele)-style coding scheme and reordering
operators in a genetic algorithm. Reordering operators, including inversion [4,
5,6,7,8], order-based crossover operators [9,10,11,12,13,14,15], and so on, have
already been studied for quite some time. The eﬀectiveness of using an idealized
reordering operator (IRO) has been demonstrated [3], but an upper bound on
the probability to apply the IRO was also pointed out in the same work.
Since the introduction of the minimal deceptive problem (MDP) as a tool
for genetic algorithm modeling and performance analysis [16], the MDP has
been widely used and discussed. Some studies [3,17,18] tested the GA perfor-
mance with their theoretical frameworks on the MDP, while others [19,20,21]
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 825–836, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

826
Y.-p. Chen and D.E. Goldberg
were interested in the nature and property of the MDP and tried to understand
the relationship among the epistasis, deception, and diﬃculty for genetic algo-
rithms. In the present work, we use the MDP with diﬀerent initial conditions as
our test problems in the theoretical model because of its simplicity for analysis.
Previous analysis on reordering [3] was based on a genetic algorithm including
proportionate selection, one-point crossover, and idealized reordering operator.
Because genetic algorithms nowadays usually do not use proportionate selection,
this paper seeks the answer to whether the eﬀectiveness of using a reordering
operator with selection other than proportionate selection changes or not. In
particular, we ﬁrst modularize the previous model so that diﬀerent selection
operators can be easily plugged into the framework. Then tournament selection,
including its variants, is put into the model with the idealized reordering operator
on the minimal deceptive problem, and the performance of the model is displayed
and analyzed.
The organization of this paper is in the following. The next section gives a
brief review of the framework, which includes the test problems, our assumptions,
and the previous results. Section 3 describes the modularization and extension
of the theoretical model in detail and presents the numerical solutions. Finally,
the conclusions and future work of this paper are presented in Sect. 4.
2
The Framework
In this section, we introduce the problem we use in this paper for research
and analysis, the assumptions we make to build the theoretical model, and the
previous results based on the model.
2.1
Minimal Deceptive Problem
In order to understand how a reordering operator can help a GA to solve prob-
lems, we have to use a test problem which is hard enough so that a GA cannot
solve it by itself. On the other hand, the test problem should be not so compli-
cated that we can easily have it theoretically analyzed. In this study, we employ
a problem of known and controllable diﬃculty as our study subject.
In particular, the minimal deceptive problem (MDP) [16] is adopted as the
test problem. The MDP is a two-bit problem and designed to mislead a GA away
from the optimal solution and toward sub-optimal ones. There are two types of
MDP [16] depending on whether f0,1 is greater or less than f0,0, where f0,1
and f0,0 are the ﬁtness for point (0, 1) and (0, 0), respectively. Further analysis
shows that the MDP Type II is more diﬃcult than Type I because the GA cannot
converge to the optimal solution if the initial population is biased toward the
sub-optimal solution.
By utilizing the MDP Type II and setting the initial condition which makes
a GA diverge, we conduct our analysis on the combined eﬀect of a reordering
operator and tournament selection. Figure 1 shows the MDP Type II, and in
this paper, we have the following ﬁtness values for each point:

An Analysis of a Reordering Operator with Tournament Selection
827
(0,0)
(1,0)
(1,1)
(0,1)
Fitness
Fig. 1. The Minimal Deceptive Problem (MDP) Type II. f0,0 > f0,1.
f1,1 = 1.1;
f0,0 = 1.0;
f0,1 = 0.9;
f1,0 = 0.5.
2.2
Assumptions
In the present paper, we study a generational genetic algorithm that combines
tournament selection, one-point crossover, and a reordering operator on the MDP
Type II. The following assumptions are made for simplifying the theoretical
study and analysis.
First, instead of analyzing any particular reordering operator, an idealized
reordering operator (IRO) [3] is analyzed. The IRO transfers a building block
from short to long or from long to short with a reordering probability pr. Here
we consider the net eﬀect produced by the IRO. The diﬀerence of a building
block being short or long reﬂects on the eﬀective crossover probability pc. The
longer the building block is, the more likely it will be disrupted, and vice versa.
Second, crossover events can only occur between individuals containing the
building block of the identical deﬁning length. This assumption might be untrue
for actual implementations and ﬁnite populations. However, it further simpliﬁes
our analysis, makes the model more capable of displaying the transition between
shorts and longs, and gives us more insights about linkage learning process.
Finally, because population portions of diﬀerent schemata are considered, an
inﬁnite population is assumed implicitly as well.
2.3
Reordering and Linkage Learning
Conducting linkage learning in a GA can overcome the diﬃculty of the chromo-
some representation design when no prior knowledge about the problem struc-
ture exists. One of the straightforward methods for linkage learning is to employ
the (gene number, allele)-style coding scheme and reordering operators. For an
example of a ﬁve-bit problem, an individual 01101 might be represented as
((2, 1) (4, 0) (1, 0) (5, 1) (3, 1))
or
((5, 1) (4, 0) (3, 1) (2, 1) (1, 0)).

828
Y.-p. Chen and D.E. Goldberg
If we consider an order-two schema composed of gene 2 and gene 3, for the
ﬁrst case, the schema is 1∗∗∗1, while it is ∗∗11∗for the second case. The ordering
of the (gene number, allele)’s does not aﬀect the ﬁtness value of the individual
but aﬀects the deﬁning length of the schema and therefore the probability to
disrupt the schema when processing. Thus, reordering operators can eﬀectively
change the linkage among genes during the evolutionary process in this manner,
and it is the reason to study reordering operators as linkage learning operators
in our present work.
2.4
Previous Results
A genetic algorithm with IRO on the MDP Type II was analyzed and compared
to one without IRO [3]. The results showed that a GA without IRO might
diverge under certain initial conditions, and IRO can help a GA to overcome
such a diﬃculty. However, they also derived an upper bound on the probability
pr to apply the reordering operator
0 < pr ≤(r −1)(1 −Pf)
r
,
(1)
where proportionate selection is used, r is the ratio of the ﬁtness value of the
optimal schema to that of the sub-optimal schema, and the converged population
contains proportion of at least Pf optimal individuals.
Calculating the upper bound of pr on the MDP Type II used in the paper is
straightforward:
r = f1,1
f0,0
= 1.1
1.0 = 1.1.
If at least 50% optimal solutions are desired in the converged population, the
upper bound of pr will be
pr ≤(r −1)(1 −Pf)
r
= 0.1
1.1(1 −0.5) = 0.0455.
It was showed that if pr is greater than the upper bound, the GA still diverges
even with the help of IRO. Therefore, although IRO was demonstrated to be
useful for helping a GA to overcome the coding trap, the upper bound of the
reordering probability quite limits its applicability.
3
IRO with Tournament Selection
Now, we propose our theoretical model and analyze the combined eﬀect of IRO
and tournament selection. We start from the model developed based on using
proportionate selection [3]. By separating the parts of selection and crossover and
making the model modularized, we then develop the corresponding selection
part of pairwise tournament selection. After adding IRO into the model, we
generalize tournament selection of our model to s-ary tournament selection and
probabilistic tournament selection.

An Analysis of a Reordering Operator with Tournament Selection
829
3.1
Separating Selection and Crossover
Start from the model for proportionate selection [16]:
P t+1
0,0 = P t
0,0
f0,0
f

1 −pc
f1,1
f P t
1,1

+ pc
f0,1f1,0
f
2
P t
0,1P t
1,0;
P t+1
0,1 = P t
0,1
f0,1
f

1 −pc
f1,0
f P t
1,0

+ pc
f1,1f0,0
f
2
P t
1,1P t
0,0;
P t+1
1,0 = P t
1,0
f1,0
f

1 −pc
f0,1
f P t
0,1

+ pc
f0,0f1,1
f
2
P t
0,0P t
1,1;
P t+1
1,1 = P t
1,1
f1,1
f

1 −pc
f0,0
f P t
0,0

+ pc
f1,0f0,1
f
2
P t
1,0P t
0,1,
where P t
i,j i, j ∈{0, 1} is the portion of population of schema (i, j) at gen-
eration t, pc is the eﬀective crossover probability which combines the actual
crossover probability with the disrupting probability introduced by the linkage
of the schema, and f is the average ﬁtness value. We can separate the selec-
tion and crossover parts of the model by deﬁning the population portion after
proportionate selection as
Qt
i,j = fi,j
f P t
i,j
i, j ∈{0, 1}.
By writing the model, we obtain
P t+1
i,j
=P t
i,j
fi,j
f

1 −pc
f(1−i),(1−j)
f
P t
(1−i),(1−j)

+ pc
fi,(1−j)f(1−i),j
f
2
P t
i,(1−j)P t
(1−i),j
=P t
i,j
fi,j
f
−pc
fi,jf(1−i),(1−j)
f
2
P t
i,jP t
(1−i),(1−j)
+ pc
fi,(1−j)f(1−i),j
f
2
P t
i,(1−j)P t
(1−i),j
=Qt
i,j −pcQt
i,jQt
(1−i),(1−j) + pcQt
i,(1−j)Qt
(1−i),j
where i, j ∈{0, 1}.
Hence, the model can be described as two separate modules:
1. Proportionate selection:
Qt
i,j = fi,j
f P t
i,j
i, j ∈{0, 1}.
(2)
2. One-point crossover:
P t+1
i,j
=Qt
i,j −pcQt
i,jQt
(1−i),(1−j) + pcQt
i,(1−j)Qt
(1−i),j
i, j ∈{0, 1}.

830
Y.-p. Chen and D.E. Goldberg
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0)
P(0,1)
P(1,0)
P(1,1)
Fig. 2. Numerical solution of the MDP
Type II showing convergence to the op-
timal solution when the initial condition
is P 0
i,j = 0.25 i, j ∈{0, 1}.
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0)
P(0,1)
P(1,0)
P(1,1)
Fig. 3. Numerical solution of the MDP
Type II showing divergence away from the
optimal solution when the initial condi-
tion is P 0
0,0 = 0.7; P 0
0,1 = P 0
1,0 = P 0
1,1 =
0.1.
3.2
Pairwise Tournament Selection
After getting separate parts of the model, replacing the selection part with pair-
wise tournament selection is straightforward. Because the ﬁtness values of the
test function follow
f1,1 > f0,0 > f0,1 > f1,0,
we can easily write down the equations representing the portion of population
after pairwise tournament selection:
Qt
1,1 = 1 −(1 −P t
1,1)2;
Qt
0,0 = (1 −P t
1,1)2 −(1 −(P t
1,1 + P t
0,0))2;
Qt
0,1 = (1 −(P t
1,1 + P t
0,0))2 −(P t
1,0)2;
Qt
1,0 = (P t
1,0)2.
(3)
Substituting the proportionate selection module with the pairwise tourna-
ment selection module, we get the model combining IRO and tournament se-
lection. Figures 2 and 3 show the numerical results of the pairwise tournament
selection model for two diﬀerent initial conditions. In the ﬁrst initial condition,
portions of all schemata are equal, i.e., P 0
i,j = 0.25 i, j ∈{0, 1}. In the second
initial condition, the initial population is biased toward the sub-optimal solution
that P 0
0,0 = 0.7; P 0
0,1 = P 0
1,0 = P 0
1,1 = 0.1.
The two initial conditions used here are identical to that used elsewhere [3]
for comparison purpose. The results show that replacing proportionate selection
with pairwise tournament selection alone does not make the GA capable of
overcoming the diﬃculty. It still diverges under the second initial condition. The
diﬀerence of using tournament selection is that the convergence or divergence
comes much faster. Since it is well-known that the takeover time of tournament

An Analysis of a Reordering Operator with Tournament Selection
831
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0)
P(0,1)
P(1,0)
P(1,1)
Fig. 4. Numerical solution of the MDP Type II showing convergence to the optimal
solution when pr = 0.01. Combined results for both short building blocks and long
building blocks.
selection is much shorter than that of proportionate selection [22], the time
diﬀerence is expected.
3.3
Using IRO
Apparently, replacing proportionate selection does not change the basic behavior
of a GA. We now insert the idealized reordering operator (IRO) into our model
to verify its performance. IRO is assumed to transfer a build block between its
long version (loose linkage) and short version (tight linkage). For simplicity, we
add another index k to the model equation terms for distinguishing short (k = 0)
and long (k = 1). The diﬀerence of being long or short reﬂects on the eﬀective
crossover probability. If a building block is tightly linked (short), we assume that
the eﬀective crossover probability pc,0 = 0, which means the building block will
not be disrupted. Otherwise, we assume pc,1 = 1, meaning the schema is very
likely to be destroyed.
Because crossover events only occur between individuals of the same deﬁning
length of building blocks, we can write the crossover parts with the extra index
by introducing a new intermediate portion Rt
i,j,k as
Rt+1
i,j,k =Qt
i,j,k −pc,kQt
i,j,kQt
(1−i),(1−j),k
+ pc,kQt
i,(1−j),kQt
(1−i),j,k
i, j, k ∈{0, 1},
(4)
where Rt
i,j,k is the population portion of schema (i, j, k) at generation t after
crossover.
After crossover, IRO is responsible for transferring a building block between
its long and short version with reordering probability pr as
P t+1
i,j,k = (1 −pr)Rt
i,j,k + prRt
i,j,(1−k)
i, j, k =∈{0, 1},
(5)
where on the right hand side, the ﬁrst term indicates the building blocks remain-
ing to be the same version, and the second term speciﬁes the building blocks
transferred from the other version.

832
Y.-p. Chen and D.E. Goldberg
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0,0)
P(0,1,0)
P(1,0,0)
P(1,1,0)
Fig. 5. Numerical solution of the MDP
Type II showing convergence to the opti-
mal solution when pr = 0.01. Short build-
ing blocks.
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0,1)
P(0,1,1)
P(1,0,1)
P(1,1,1)
Fig. 6. Numerical solution of the MDP
Type II showing convergence to the opti-
mal solution when pr = 0.01. Long build-
ing blocks.
Thus, the model with IRO consists of the following three modules:
1. Pairwise tournament selection (Equation (3));
2. One-point crossover (Equation (4));
3. Idealized reordering operator (Equation (5)).
To make the problem harder, we adopt the third initial condition that P 0
0,0 =
0.8; P 0
0,1 = P 0
1,0 = 0.1; P 0
1,1 = 0 [3]. This initial condition speciﬁes that the way to
have schema (1, 1) is to create it via crossover and make it stay in the population
without being disrupted. We ﬁrst try a low reordering probability pr = 0.01 to
see if the reordering operator also helps a GA to converge with tournament
selection.
Figures 4, 5, and 6 show the numerical results after inserting IRO into the
model. Apparently, IRO works as we expected to help the GA to converge to the
optimal solutions. The process can be roughly divided into three stages. First,
the short version of (1, 1) is created by the crossover. Only the short version can
survive at this stage because it cannot be disrupted even both short and long
versions are equally favored by the selection. Then, the optimal schema starts to
takeover the population. The period of this stage is determined by the takeover
time. After the optimal schema takeover the population, there is no need to
maintain linkage. Therefore, the portion of long starts to grow, and the portion
of short starts to decrease until reaching the balance.
Until now, there seems no fundamental diﬀerence between using proportion-
ate selection and using tournament selection. Except for the time scale, the
behavior does not seem to be diﬀerent. However, if we use a higher reordering
probability pr = 0.10. We can get the numerical results in Figure 7.
Unexpectedly, the GA also converged to the optimal solution. Using the same
reordering probability, the GA diverges instead of converges. Because the upper
bound for the reordering probability was developed based on using proportionate
selection, it might be diﬀerent if tournament selection is used. Therefore, we

An Analysis of a Reordering Operator with Tournament Selection
833
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0)
P(0,1)
P(1,0)
P(1,1)
Fig. 7. Numerical solution of the MDP
Type II showing convergence to the op-
timal solution even when pr
=
0.10.
Combined results for both short building
blocks and long building blocks.
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0)
P(0,1)
P(1,0)
P(1,1)
Fig. 8. Numerical solution of the MDP
Type II showing convergence to the op-
timal solution even when pr
=
0.25.
Combined results for both short building
blocks and long building blocks.
conduct simulations with even higher reordering probabilities pr = 0.25, 0.75,
and 0.99. The results are shown in Figures 8, 9, and 10.
Surprisingly, the GA still converged to the optimal solution even with a very
high reordering probability. It indicates that there might not be a upper bound
for reordering probability except that 0 < pr < 1.
3.4
S-ary Tournament Selection
In addition to pairwise tournament selection, we also generalize the model to
include the commonly used s-ary tournament selection as follows. First, we deﬁne
an order function o(·) for each schema based on their ﬁtness values:
o(0) = (−1, −1);
o(1) = (1, 1);
o(2) = (0, 0);
o(3) = (0, 1);
o(4) = (1, 0),
where (−1, −1) is a boundary condition for convenience, and P t
−1,−1 = 0 ∀t ≥0.
Second, we deﬁne the accumulated population portion with the order given by
o(·) as
At
o(n) =
n

m=0
P t
o(m)
0 ≤n ≤4.
With the help of the ordering function and accumulated portion, we can rewrite
(3) as follows:
Qt
o(n) =
0
n = 0

1 −At
o(n−1)
2
−

1 −At
o(n)
2
0 < n ≤4
(6)

834
Y.-p. Chen and D.E. Goldberg
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0)
P(0,1)
P(1,0)
P(1,1)
Fig. 9. Numerical solution of the MDP
Type II showing convergence to the op-
timal solution even when pr
=
0.75.
Combined results for both short building
blocks and long building blocks.
0
0.2
0.4
0.6
0.8
1
0
5
10
15
20
25
30
35
40
45
50
Proportion
Time (Number of Generation)
P(0,0)
P(0,1)
P(1,0)
P(1,1)
Fig. 10. Numerical solution of the MDP
Type II showing convergence to the op-
timal solution even when pr
=
0.99.
Combined results for both short building
blocks and long building blocks.
Then we can generalize the equation to s-wise tournament selection by replacing
the square with the sth power:
Qt
o(n) =
0
n = 0

1 −At
o(n−1)
s
−

1 −At
o(n)
s
0 < n ≤4
(7)
3.5
Probabilistic Tournament Selection
Probabilistic tournament selection can also be modeled with our framework.
Considering pairwise tournament selection, after tournament, the winner gets
into the next generation with a ﬁxed probability p, 0.5 < p ≤1. Pairwise tour-
nament selection can be considered as a special case with p = 1.
To include probabilistic tournament selection, we start from (6). Since when
some schema wins, it actually gets into the next generation with p, and it also
gets selected with 1−p when losing a tournament, we can modiﬁed (6) to model
probabilistic tournament selection as
Qt
o(n) =











0
n = 0
p

1 −At
o(n−1)
2
−

1 −At
o(n)
2
+(1 −p)

At
o(n)
2
−

At
o(n−1)
2
0 < n ≤4
(8)
Numerical results for both s-ary tournament selection and probabilistic tour-
nament selection are omitted because of the length limitation. Those results are
also consistent with our previous observation on pairwise tournament selection.
The fundamental behavior of the algorithm remains the same when diﬀerent
kinds of tournament selection are employed.

An Analysis of a Reordering Operator with Tournament Selection
835
4
Conclusions
It has been demonstrated that an idealized reordering operator can help a ge-
netic algorithm to overcome certain diﬃculty on the MDP Type II [3]. However,
an upper bound of the reordering probability was also derived to explain why
the reordering operator did not help the genetic algorithm when the probability
was set to be a little bit high. In this paper, we extend the model to include the
commonly used tournament selection. The proposed model can describe pairwise
tournament selection, s-ary tournament selection, and probabilistic tournament
selection. By analyzing the performance of a GA with the proposed model, we
can ﬁnd that there seems no upper bound on reordering probability. The genetic
algorithm still converges even if the probability approaches to 1. Therefore, us-
ing the reordering operator with tournament selection can give us much better
results than with proportionate selection.
Tournament selection has been widely utilized because of its excellence, in-
cluding independence of ﬁtness scaling, ease for implementation, and so on. How-
ever, based on the study, when conducting linkage learning, tournament selection
becomes a necessity rather than a choice. If proportionate selection is used, the
reordering probability has to be limited to ensure success. But in practice, the
reordering probability might not be known or even controllable. Hence, we have
to use tournament selection under this condition so that the algorithm is able
to achieve its goal.
Future work along this line includes extending the model so that more opera-
tors and parameters can be described, using the model to observe and explain the
linkage learning process, and deriving characteristic properties, such as tightness
time, of linkage learning.
Acknowledgments. The work was sponsored by the Air Force Oﬃce of Sci-
entiﬁc Research, Air Force Materiel Command, USAF, under grant F49620-00-
0163. Research funding for this work was also provided by a grant from the
National Science Foundation under grant DMI-9908252. The US Government
is authorized to reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation thereon.
The views and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the oﬃcial policies or en-
dorsements, either expressed or implied, of the Air Force Oﬃce of Scientiﬁc
Research, the National Science Foundation, or the U.S. Government.
References
1. Goldberg, D.E., Deb, K., Clark, J.H.: Genetic algorithms, noise, and the sizing of
populations. Complex Systems 6 (1992) 333–362
2. Goldberg, D.E., Deb, K., Thierens, D.: Toward a better understanding of mixing
in genetic algorithms. Journal of the Society of Instrument and Control Engineers
32 (1993) 10–16

836
Y.-p. Chen and D.E. Goldberg
3. Goldberg, D.E., Bridges, C.L.: An analysis of a reordering operator on a GA-hard
problem. Biological Cybernetics 62 (1990) 397–405
4. Bagley, J.D.: The Behavior of Adaptive Systems Which Employ Genetic and Cor-
relation Algorithms. PhD thesis, University of Michigan (1967) (University Micro-
ﬁlms No. 68-7556).
5. Cavicchio, Jr., D.J.:
Adaptive Search Using Simulated Evolution.
Unpublished
doctoral dissertation, University of Michigan, Ann Arbor, MI (1970) (University
Microﬁlms No. 25-0199).
6. Frantz, D.R.: Non-linearities in genetic adaptive search. PhD thesis, University of
Michigan (1972) (University Microﬁlms No. 73-11,116).
7. Holland, J.H.: Adaptation in natural and artiﬁcial systems. University of Michigan
Press, Ann Arbor, MI (1975)
8. Smith, S.F.: A learning system based on genetic adaptive algorithms. Dissertation
Abstracts International 41 (1980) 4582B (University Microﬁlms No. 81-12638).
9. Davis, L., Smith, D.: Adaptive design for layout synthesis. Internal report, Texas
Instruments, Dallas (1985)
10. Davis, L.: Applying adaptive algorithms to epistatic domains. Proceedings of the
9th International Joint Conference on Artiﬁcial Intelligence 1 (1985) 162–164
11. Goldberg, D.E., Lingle, Jr., R.: Alleles, loci, and the traveling salesman problem.
Proceedings of an International Conference on Conference on Genetic Algorithms
(1985) 154–159
12. Oliver, I.M., Smith, D.J., Holland, J.R.C.: A study of permutation crossover oper-
ators on the traveling salesman problem. Proceedings of the Second International
Conference on Genetic Algorithms (1987) 224–230
13. Whitley, D., Starkweather, T., Fuquay, D.:
Scheduling problems and traveling
salesmen: The genetic edge recombination operator. Proceedings of the Third In-
ternational Conference on Genetic Algorithms (1989) 133–140
14. Banzhaf, W.: The ’molecular’ traveling salesman. Biological Cybernetics 64 (1990)
7–14
15. Starkweather, T., McDaniel, S., Mathias, K., Whitley, D., Whitley, C.: A com-
parison of genetic sequencing operators. Proceedings of the Fourth International
Conference on Genetic Algorithms (1991) 69–76
16. Goldberg, D.E.: Simple genetic algorithms and the minimal, deceptive problem.
In Davis, L., ed.: Genetic algorithms and simulated annealing. Morgan Kaufmann
Publishers, Inc., Los Altos, CA (1987) 74–88
17. Yamamura, M., Satoh, H., Kobayashi, S.: An Markov analysis of generation alter-
nation models on minimal deceptive problems. In Wang, P.P., ed.: International
Conference of Information Sciences: Fuzzy Logic, Intelligent Control & Genetic
Algorithm. Volume 1., Durham, NC, Duke University (1997) 47–50
18. Novkovic, S., ˇSverko, D.: The minimal deceptive problem revisited: the role of
‘genetic waste’. Computers and Operations Research 25 (1998) 895–911
19. Grefenstette, J.J.: Deception considered harmful. Foundations of Genetic Algo-
rithms 2 (1993) 75–91
20. Forrest, S., Mitchell, M.: What makes a problem hard for a genetic algorithm?
Some anomalous results and their explanation. Machine Learning 13 (1993) 285–
319
21. Naudts, B., Verschoren, A.: Epistasis and deceptivity. Simon Stevin - Bulletin of
the Belgian Mathematical Society 6 (1999) 147–154
22. Goldberg, D.E., Deb, K.: A comparative analysis of selection schemes used in ge-
netic algorithms. Foundations of Genetic Algorithms 1 (1991) 69–93 (Also TCGA
Report 90007).

Tightness Time for the Linkage Learning
Genetic Algorithm
Ying-ping Chen1 and David E. Goldberg2
1 Department of Computer Science and Department of General Engineering
University of Illinois, Urbana, IL 61801, USA
ypchen@illigal.ge.uiuc.edu
2 Department of General Engineering
University of Illinois, Urbana, IL 61801, USA
deg@uiuc.edu
Abstract. This paper develops a model for tightness time, linkage learn-
ing time for a single building block, in the linkage learning genetic al-
gorithm (LLGA). First, the existing models for both linkage learning
mechanisms, linkage skew and linkage shift, are extended and investi-
gated. Then, the tightness time model is derived and proposed based on
the extended linkage learning mechanism models. Experimental results
are also presented in this study to verify the extended models for linkage
learning mechanisms and the proposed model for tightness time.
1
Introduction
Linkage learning, one of the fundamental challenges of the research and develop-
ment of genetic algorithms (GAs), has often been either ignored or overlooked in
the ﬁeld of evolutionary computation. In spite of Holland’s call for the evolution
of tight linkage in his historical publication of Adaptation in Natural and Arti-
ﬁcial Systems [1], there have been relatively few eﬀorts made on the subject of
linkage learning and evolution until the past decade. Thierens and Goldberg [2,3]
showed that simple genetic algorithms fail to solve hard problems without tight
linkage and analyzed several possible but unsuccessful techniques to overcome
the linkage learning diﬃculty. Recognizing the importance of linkage evolution
in powerful and general evolutionary processes, linkage-related operators and
mechanisms were therefore developed.
Among the ways to achieve tight linkage or its equivalent such as pertur-
bation techniques, model builders, and linkage learners is the linkage learning
genetic algorithm (LLGA), which uses (gene number, allele) style coding scheme
with non-coding segments [4,5,6] to create an evolvable genotypic structure that
makes GAs capable of learning tight linkage of building blocks through the spe-
cial expression mechanism, probabilistic expression (PE) [7,8]. Compared to the
perturbation-based algorithms, the LLGA does not employ extra linkage detec-
tion procedures to gain knowledge of linkage. Compared to the model builders,
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 837–849, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

838
Y.-p. Chen and D.E. Goldberg
the LLGA operates in a local, distributed manner without building a global
model across all individuals. Therefore, the linkage learning process is a very
special and essential part of the LLGA, and this paper seeks to better under-
stand the LLGA’s linkage learning mechanisms.
In particular, this paper examines the current theoretical analysis of the
linkage learning mechanisms [9,7]. Models for both linkage learning mechanisms,
linkage skew and linkage shift, are reﬁned and extended. The theoretical results
are conﬁrmed with experiments. Moreover, a model for tightness time, linkage
learning time for a single building block, is therefore proposed and empirically
veriﬁed. Artiﬁcial evolutionary systems usually create ﬁtness associated with a
greedy choice of the best alleles before linkage has evolved. Understanding the
race between allelic convergence and linkage convergence is critical to designing
LLGAs that work well. In particular, understanding tightness time is critical to
getting time scale right, especially in systems like the LLGA.
This paper is organized as follows. The following section gives a short survey
of competent GAs and a brief review of the LLGA. Section 3 extends the existing
theoretical analysis of both linkage learning mechanisms, and Sect. 4 veriﬁes the
models with experimental results. Section 5 proposes the tightness time model
based on the extended mechanism models. Finally, the conclusions of this paper
are drawn in Sect. 6.
2
Brief Review of Competent GAs and the LLGA
Current linkage detection, adaptation, or learning techniques can be roughly
classiﬁed into three categories. The ﬁrst category is based on perturbation tech-
niques. Algorithms in this category, such as messy GA (mGA) [10], fast messy
GA (fmGA) [11], gene expression messy GA (GEMGA) [12], and linkage iden-
tiﬁcation by nonlinearity check/non-monotonicity detection (LINC/LIMD) [13,
14], perturb chromosomes in some particular way and detect linkage among
genes by observing the diﬀerence of ﬁtness. After determining the relationship
among genes, building-block (BB) preserving recombination operators are then
employed to create promising solutions.
The second category uses model building techniques. Population-based in-
cremental learning (PBIL) [15], univariate marginal distribution algorithm
(UMDA) [16], compact GA (cGA) [17], extended compact GA (ECGA) [18],
iterated distribution estimation algorithm (IDEA) [19], probabilistic incremen-
tal program evolution (PIPE) [20], and BOA [21] belong to this category. Model
builders usually grasp the linkage or relationship among genes by building a
global probabilistic model with the current population. New individuals are then
derived according to the model, and the model building-utilizing process is re-
peated until the termination criterion is satisﬁed.
The ﬁnal category adapts linkage through the chromosome representation
and genetic operators. The linkage learning genetic algorithm (LLGA) [9,7,8]
falls into this category. By making the chromosome itself capable of representing

Tightness Time for the Linkage Learning Genetic Algorithm
839
(3,1)
(3,0)
0 or 1
50% : 50%
(a) Gene 3 is expressed as 0
with probability 0.5 and 1
with probability 0.5.
δ
(3,1)
(3,0)
δ
l−δ
0 or 1
δ/l   : 1−/l
(b) Gene 3 is expressed as
0 with probability δ/l and
1 with probability 1 −δ/l.
Fig. 1. Probability distributions of gene 3’s alleles represented by PE chromosomes.
linkage, the LLGA integrates the linkage learning and utilizing into a uniﬁed
operation and creates oﬀspring with conceptually well-known genetic operators.
The LLGA is brieﬂy reviewed in the remainder of this section. Readers who
are interested in more detail should refer to other materials [9,7,8].
2.1
Chromosome Representation
The LLGA’s chromosome representation is mainly composed of moveable genes,
non-coding segments, and probabilistic expression. Moveable genes are encoded
as (gene number, allele) pairs in the LLGA chromosome, and a LLGA chromo-
some is considered as a circle. These genes are allowed to move around and reside
anywhere in any order in the chromosome. Non-coding segments are inserted into
the chromosome to create an evolvable genotype capable of expressing linkage.
Non-coding segments act as non-functional genes, which have no eﬀect on ﬁtness,
residing between adjacent genes to generate gaps for expressing linkage precisely.
Probability expression (PE) was proposed to preserve building-block level di-
versity. For each gene, all possible alleles coexist in a PE chromosome at the same
time. For the purpose of evaluation, a chromosome is interpreted with a point
of interpretation (POI). The allele for each gene is determined by the order in
which the chromosome is traversed clock-wisely from the point of interpretation.
A complete string is then expressed and evaluated.
Consequently, each PE chromosome represents not just a single solution but
a probability distribution over the range of possible solutions. Figure 1 shows the
probability distribution over alleles of gene 3 of the chromosome. Therefore, if
diﬀerent points of interpretation are selected, a PE chromosome might be inter-
preted as diﬀerent solutions. Furthermore, the probability of a PE chromosome
to be expressed as a particular solution depends on the length of the non-coding
segment between genes critical to that solution. It is the essential technique of
the LLGA to capture the knowledge and to prompt the evolution of linkage.
2.2
Exchange Crossover
The exchange crossover operator is another key to make the LLGA able to learn
linkage. It is deﬁned on a pair of chromosomes. One of the two chromosomes is

840
Y.-p. Chen and D.E. Goldberg
gap y1
gap y2
gap y3
y1 + y2 + y3 = 1
y1^2 + y2^2 + y3^2
linkage =
gene 2
gene 1
gene 3
Fig. 2. Calculation for the linkage of a three-gene building block.
the donor, and the other is the recipient. The exchange crossover cuts a random
segment of the donor, selects a grafting point on the recipient, and grafts the
segment onto the recipient. The grafting point is the point of interpretation of the
oﬀspring. Starting from the point of interpretation, redundant genetic materials
caused by injection are removed right after crossover to ensure the validity.
2.3
Mechanisms Making the LLGA Work
With the integration of PE and the exchange crossover operator, the LLGA is
capable of solving diﬃcult problems without prior knowledge of good linkage.
Traditional GAs have been shown to perform poorly on diﬃcult problems [2]
without such knowledge. To understand the working of the LLGA, two mecha-
nisms of linkage learning: linkage skew and linkage shift has been identiﬁed and
analyzed [9]. Linkage skew occurs when an optimal building block is transferred
from the donor to the recipient. Linkage shift occurs when an optimal build-
ing block resides in the recipient and survives an injection. Both linkage skew
and linkage shift make the building block’s linkage tighter. Thus, the linkage of
building blocks can evolve during the linkage learning process, and tightly linked
building blocks are formed.
3
Linkage Learning Mechanisms
In this section, we extend the existing theoretical models for both linkage learning
mechanisms. First, the deﬁnition of linkage of a building block is introduced.
Then, the existing models for linkage skew and linkage shift [9] are extended.
3.1
Quantifying Linkage
In order to show the linkage learning and evolutionary process of the LLGA,
linkage of building blocks has to be quantiﬁed. In this paper, we employ a pro-
posed deﬁnition [9], which is the sum of the square of its inter-gene distances,
considering the chromosome to be a circle of circumference 1. Figure 2 shows an
example for calculating the linkage of a three-gene building block. The deﬁni-
tion is appropriate in that linkage in such deﬁnition speciﬁes a measure directly

Tightness Time for the Linkage Learning Genetic Algorithm
841
proportional to the probability for a building block to be preserved under the
exchange crossover operator. Additionally, it was also theoretically justiﬁed that
for any linkage learning operator working on the same form of representation,
the expected linkage of a randomly spaced order-k building block is
2
k+1 [9].
3.2
Linkage Skew
Linkage skew, the ﬁrst linkage learning mechanism, occurs when an optimal
building block is successfully transferred from the donor onto the recipient. The
conditions for an optimal building block to be transferred are (1) the optimal
building block resides in the cut segment, and (2) the optimal building block gets
expressed before the deceptive one does. The eﬀect of linkage skew was found
to make linkage distributions move toward higher linkages by eliminating less ﬁt
individuals. Linkage skew does not make the linkage of a building block of any
particular individual tighter. Instead, it drives the whole linkage distribution to
a higher state.
Let Λt(λ) be the probability density function of the random variable λ repre-
senting the linkage of the optimal building block at generation t. The following
model to describe the evolution of linkage under linkage skew only has been
proposed [9]:
Λt+1(λ) = λΛt(λ)
Λt
.
(1)
Based on (1), the linkage average at generation t + 1 was calculated as [7]:
Λt+1 =
 1
0
λΛt+1(λ) dλ = Λ2
t
Λt
,
(2)
and thus
Λt+1 = Λt + σ2 (Λt)
Λt
,
(3)
where σ2 (Λt) is the variance in the linkage distribution at generation t.
In addition to the average (i.e. the ﬁrst moment) of Λt+1(λ), we can actually
calculate all other moments of Λt+1(λ) in the same way:
Λn
t+1 =
 1
0
λnΛt+1(λ) dλ = Λn+1
t
Λt
.
(4)
According to the property of a probability distribution, the moments about the
origin completely characterize a probability distribution [22]. From (4), we can
construct all the moments about the origin of Λt+1(λ) as follows:
Λn
t+1 = Λn+1
t
Λt
n = 1, 2, 3, · · ·
(5)

842
Y.-p. Chen and D.E. Goldberg
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
80
100
Average of Linkage
Time (Number of Generation)
Experimental Results
Theoretical Prediction
Initial Maximum Linkage
(a) The Average of Linkage.
0
0.005
0.01
0.015
0.02
0.025
0
20
40
60
80
100
Variance of Linkage
Time (Number of Generation)
Experimental Results
Theoretical Prediction
(b) The Variance of Linkage.
Fig. 3. Linkage Skew on an Order-4 Trap Building Block.
Hence, the relation between Λt(λ) and Λt+1(λ) is established.
After knowing the relation between Λt(λ) and Λt+1(λ), the moment genera-
tion function (mgf) deﬁned as follows can help us a lot:
mΛt(s) = E

esΛt
=
 ∞
−∞
esλΛt(λ) dλ.
(6)
Assume that the mgf of Λt(λ) exists, it can be written as
mΛt(s) = E

esΛt
= E

1 + Λts + (Λts)2
2!
+ (Λts)3
3!
+ · · ·

= 1 + Λts + Λ2
t
s2
2! + Λ3
t
s3
3! + · · ·
The rth moment of Λt(λ) can be obtained with
Λr
t = m(r)
Λt (0) = drmΛt(s)
dsr

s=0
.
Given the relation between Λt(λ) and Λt+1(λ) and the property of the moment
generating function, we can now get the mgf of Λt+1(λ):
mΛt+1(s) = m
′
Λt

s
m
′
Λt(0)

.
(7)
Therefore, we have a model to calculate Λt+1(λ) from Λt(λ) when the mgf is
available.

Tightness Time for the Linkage Learning Genetic Algorithm
843
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
80
100
Average of Linkage
Time (Number of Generation)
Experimental Results
Theoretical Prediction
Initial Maximum Linkage
(a) The Average of Linkage.
0
0.005
0.01
0.015
0.02
0.025
0
20
40
60
80
100
Variance of Linkage
Time (Number of Generation)
Experimental Results
Theoretical Prediction
(b) The Variance of Linkage.
Fig. 4. Linkage Skew on an Order-6 Trap Building Block.
Moreover, also based on (5), we can obtain the following result:
Λn
t = Λn+1
t−1
Λt−1
= Λn+2
t−2 /Λt−2
Λ2
t−2/Λt−2
= Λn+2
t−2
Λt−2
= · · ·
= Λt+n
0
Λ0
,
(8)
which clearly indicates that under linkage skew, any moment of the linkage
distribution at any given generation can be predicted with the information of
the initial linkage distribution. The linkage learning process is solely determined
by the initial distribution if there is only linkage skew working.
Finally, based on its property, linkage skew does not really tighten building
blocks in any individual. It drives the linkage distribution to a higher place by
propagating tight building blocks among individuals. Apparently, the linkage
cannot exceed the maximum linkage in the initial population. The evolution of
linkage described by (7) is therefore bounded by the initial maximum linkage.
3.3
Linkage Shift
Linkage shift is the second linkage learning mechanism [9]. It occurs when an
optimal building block resides in the recipient and survives a crossover event. For
the optimal building block to survive, there cannot be any gene contributing to a
deceptive building block transferred. Linkage shift gets the linkage of a building
block in an individual higher with deletion of duplicate genetic material caused
by injection of the exchange crossover. Compared to linkage skew, linkage shift
gets linkage of building blocks in each individual higher.

844
Y.-p. Chen and D.E. Goldberg
For linkage shift, the following recurrence equation was used [9] to depict the
eﬀect of the second mechanism on building blocks that survive crossover:
λ0(t + 1) = λ0(t) + (1 −λ0(t))
2
(k + 2)(k + 3),
(9)
for an order-k building block. Tracking only the average of linkage, we can ap-
proximately rewrite (9) as
Λt+1 = Λt + (1 −Λt)
2
(k + 2)(k + 3).
(10)
Given a ﬁxed k, let c =
2
(k+2)(k+3), we can get the following recurrence relation:
Λt+1 = Λt + c(1 −Λt)
= Λt (1 −c) + c.
(11)
By solving the recurrence relation, the new linkage shift model is obtained as
Λt = 1 −(1 −Λ0)(1 −c)t.
(12)
Therefore, the rate of linkage learning is mainly determined by the linkage av-
erage of the initial linkage distribution. Besides, the higher order the building
block is, the longer it takes to evolve to some speciﬁc level of linkage.
4
Experimental Results
The experimental results to verify the extended linkage learning mechanism mod-
els are presented in this section. First, the parameter settings of the experiments
are described. Then, experimental results for both models are shown.
4.1
Parameter Settings
In this paper, we use trap functions [23] to verify the theoretical model. We
basically use similar experiments presented elsewhere [9], but the experiments
in this study were done for both order-4 and order-6 traps. An order-k trap
function used in this study can be described by
trapk(u) =
	u
u = k
k −1 −u
otherwise ,
where u is the number of ones in the bitstring. In order to simulate the inﬁnite-
length chromosome, we let the functional genes occupy only one percent of the
chromosome. That is, for the order-4 building block, the 4 genes are embed-
ded in a 400-gene chromosome with 396 nonfunctional genes; for the order-6
building block, the 6 genes are embedded in a 600-gene chromosome with 594
nonfunctional genes. The population size are 5000 in both cases. All results in
this section are averaged over 50 independent runs.

Tightness Time for the Linkage Learning Genetic Algorithm
845
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
80
100
Average of Linkage
Time (Number of Generation)
Experimental Results
Theoretical Prediction
Adjusted Theoretical Prediction
(a) Order-4 Trap Building Block.
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
80
100
Average of Linkage
Time (Number of Generation)
Experimental Results
Theoretical Prediction
Adjusted Theoretical Prediction
(b) Order-6 Trap Building Block.
Fig. 5. The Average of Linkage under Linkage Shift.
4.2
Linkage Skew
The extended linkage skew model can predict all moments at any given gener-
ation. For illustration purpose, we show only the prediction of the average (the
ﬁrst moment) and the variance (the second moment minus the square of the
average) in ﬁgures.
Figures 3 and 4 show the experimental results compared to the theoretical
prediction. The theoretical prediction was made based on (8). As shown in the
ﬁgures, the experimental results agree with the prediction, and the extended
linkage skew model is conﬁrmed experimentally.
4.3
Linkage Shift
For linkage shift, we predict the average of linkage on both order-4 and order-6
traps. Figure 5 shows the experimental results. The theoretical prediction was
made according to (12). We also employ the adjustment scheme [9] to reﬂect the
diﬀerence between the inﬁnite-length chromosome model and the real chromo-
somes in experiments. In this study, the maximum possible linkage in both cases
is (0.99)2 = 0.9801, and (12) is adjusted accordingly. As indicated in Fig. 5, the
experimental results agree with the adjusted theoretical prediction, and we are
now given good reason to believe that the extended model is accurate.
5
Tightness Time
5.1
The Model
Equipped with the extended models for linkage learning, we are now ready to
develop the tightness time model. Based on the observation and intuition, the
working relationship between linkage skew and linkage shift is as follows. Linkage
shift is responsible for making the linkage of a building block in each individual

846
Y.-p. Chen and D.E. Goldberg
0
20
40
60
80
100
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (Number of Generation)
Average of Linkage
Expreimental Results
Theoretical Prediction
Adjusted Theoretical Prediction
(a) Order-4 Trap Building Block.
0
20
40
60
80
100
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time (Number of Generation)
Average of Linkage
Expreimental Results
Theoretical Prediction
Adjusted Theoretical Prediction
(b) Order-6 Trap Building Block.
Fig. 6. Tightness Time for a Single Building Block.
tighter; linkage skew is responsible for driving the whole distribution toward a
higher place. Considering linkage shift as a reﬁner, linkage skew as a propagator,
and that the eﬀect of linkage skew comes pretty fast based on the experimental
result, the linkage learning bottleneck is in fact linkage shift. Hence, we now start
to develop the model for tightness time based on the most critical component in
the framework ﬁrst.
Start from (12), we can obtain
t = log(1 −Λt) −log(1 −Λ0)
log(1 −c)
.
Then, it can be rewritten as a function of linkage λ:
t(λ) = log(1 −λ) −log(1 −Λ0)
log(1 −c)
.
By taking the propagation eﬀect of linkage skew into account, a constant cs
standing for the linkage learning speed-up caused by linkage skew is added into
the model. Thus, we obtain the tightness time model as follows:
tℓ(λ) = log(1 −λ) −log(1 −Λ0)
cs log(1 −c)
,
(13)
where tℓ(λ) is the tightness time for a given linkage λ, and cs ≈2 is determined
empirically.
Furthermore, given the initial linkage distribution, Λ0 remains constant dur-
ing the whole process. For simplicity, we can deﬁne
ϵ = 1 −λ,
ϵ0 = 1 −Λ0.

Tightness Time for the Linkage Learning Genetic Algorithm
847
Also, c =
2
(k+2)(k+3) ≈
2
k2 when k →∞. Therefore, (13) can be rewritten as a
function of ϵ as
t′
ℓ(ϵ) = k2
2cs
log ϵ
ϵ0
.
(14)
Equation (14) shows that tightness time is proportional to the square of the
order of building blocks. The longer the building block, the much longer the
tightness time. Besides, tightness time is proportional to the logarithm of the
desired linkage.
5.2
Veriﬁcation
Experiments were also performed to verify the model for tightness time. Us-
ing the same parameter settings described in section 4.1, both linkage learning
mechanisms work together in the experiments.
The experimental results are shown in Figure 6. The theoretical prediction
made based on (13) is also adjusted with the maximum possible linkage 0.9801.
The obtained numerical data agree with our tightness time model quite well.
Our hypothesis and model are therefore experimentally veriﬁed.
6
Conclusions
One of the most important issues of the design of genetic algorithms is linkage
learning. Harik took Holland’s call for the evolution of tight linkage seriously and
developed the linkage learning genetic algorithm, which learns linkage among
genes with specially designed chromosome representation and conceptually well-
known genetic operators. While the LLGA performs remarkably well on badly
scaled building blocks, it does not do so on uniformly scaled building blocks.
This paper seeks to gain better understanding of linkage learning mechanisms
of the LLGA in order to improve the capability of the LLGA.
In this paper, the current theoretical analysis of the linkage learning mech-
anisms are extended in several aspects and veriﬁed with experiments. Based on
the extended models, a model for tightness time is proposed. Under the two link-
age learning mechanisms, the evolution of linkage is basically determined by the
initial linkage distribution. If the linkage learning process needs to be modiﬁed or
improved, we have to seek help from other mechanisms outside the framework.
Additionally, the cooperation and interaction between linkage skew and linkage
shift is also theorized. It helps us to better understand the overall eﬀect of the
LLGA’s linkage learning mechanisms. Finally, some important insights into the
whole linkage learning process are obtained from the tightness time model.
Among existing techniques, perturbation-based algorithms and model
builders get linkage information and evolve the population in two separate steps,
while the LLGA gets linkage tight and alleles right at the same time. Hence,
tightness time for the LLGA provides an important key to correctly handling
the competition of linkage and allele on time scale. Understanding tightness time

848
Y.-p. Chen and D.E. Goldberg
should enable us to improve the LLGA and design better genetic algorithms as
well.
More work along this line still needs to be done to understand the capabilities
and limits of linkage adaptation techniques. The results shown in this paper give
us more theoretical insight of the LLGA’s linkage learning mechanisms and take
us one step further toward scalable linkage learning.
Acknowledgments. The author would like to thank Kumara Sastry for many
useful discussions and valuable comments.
The work was sponsored by the Air Force Oﬃce of Scientiﬁc Research, Air
Force Materiel Command, USAF, under grant F49620-00-0163. Research funding
for this work was also provided by a grant from the National Science Founda-
tion under grant DMI-9908252. The US Government is authorized to reproduce
and distribute reprints for Government purposes notwithstanding any copyright
notation thereon.
The views and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the oﬃcial policies or en-
dorsements, either expressed or implied, of the Air Force Oﬃce of Scientiﬁc
Research, the National Science Foundation, or the U.S. Government.
References
1. Holland, J.H.: Adaptation in natural and artiﬁcial systems. University of Michigan
Press, Ann Arbor, MI (1975)
2. Thierens, D., Goldberg, D.E.: Mixing in genetic algorithms. Proceedings of the
Fifth International Conference on Genetic Algorithms (1993) 38–45
3. Thierens, D.: Analysis and Design of Genetic Algorithms. doctoral dissertation,
Katholieke Universiteit Leuven, Leuven, Belgium (1995)
4. Lindsay, R.K., Wu, A.S.: Testing the robustness of the genetic algorithm on the
ﬂoating building block representation. Proceedings of the twelfth National Confer-
ence on Artiﬁcial Intelligence/eighth Innovative Applications of Artiﬁcial Intelli-
gence Conference (1996) 793–798
5. Wineberg, M., Oppacher, F.: The beneﬁts of computing with introns. Proceedings
of the First Annual Conference on Genetic Programming (1996) 410–415
6. Levenick, J.R.: Swappers: Introns promote ﬂexibility, diversity and invention. Pro-
ceedings of the Genetic and Evolutionary Computation Conference 1999: Volume
1 (1999) 361–368
7. Harik, G.R.: Learning gene linkage to eﬃciently solve problems of bounded dif-
ﬁculty using genetic algorithms. Unpublished doctoral dissertation, University of
Michigan, Ann Arbor (1997) (Also IlliGAL Report No. 97005).
8. Harik, G.R., Goldberg, D.E.: Learning linkage through probabilistic expression.
Computer Methods in Applied Mechanics and Engineering 186 (2000) 295–310
9. Harik, G.R., Goldberg, D.E.: Learning linkage. Foundations of Genetic Algorithms
4 (1996) 247–262
10. Goldberg, D.E., Korb, B., Deb, K.: Messy genetic algorithms: Motivation, analysis,
and ﬁrst results. Complex Systems 3 (1989) 493–530

Tightness Time for the Linkage Learning Genetic Algorithm
849
11. Goldberg, D.E., Deb, K., Kargupta, H., Harik, G.: Rapid, accurate optimization
of diﬃcult problems using fast messy genetic algorithms. Proceedings of the Fifth
International Conference on Genetic Algorithms (1993) 56–64
12. Kargupta, H.: The gene expression messy genetic algorithm. Proceedings of 1996
IEEE International Conference on Evolutionary Computation (1996) 814–819
13. Munetomo, M., Goldberg, D.E.: Identifying linkage groups by nonlinearity/non-
monotonicity detection. Proceedings of the Genetic and Evolutionary Computation
Conference 1999: Volume 1 (1999) 433–440
14. Munetomo, M., Goldberg, D.E.: Linkage identiﬁcation by non-monotonicity de-
tection for overlapping functions. Evolutionary Computation 7 (1999) 377–398
15. Baluja, S.: Population-based incremental learning: A method for integrating ge-
netic search based function optimization and competitive learning. Tech. Rep. No.
CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, PA (1994)
16. M¨uhlenbein, H.: The equation for response to selection and its use for prediction.
Evolutionary Computation 5 (1997) 303–346
17. Harik, G.R., Lobo, F.G., Goldberg, D.E.: The compact genetic algorithm. Proceed-
ings of 1998 IEEE International Conference on Evolutionary Computation (1998)
523–528
18. Harik, G.:
Linkage learning via probabilistic modeling in the ECGA.
IlliGAL
Report No. 99010, University of Illinois at Urbana-Champaign, Illinois Genetic
Algorithms Laboratory, Urbana, IL (1999)
19. Bosman, P.A.N., Thierens, D.: Linkage information processing in distribution es-
timation algorithms. Proceedings of the Genetic and Evolutionary Computation
Conference 1999: Volume 1 (1999) 60–67
20. Salustowicz, R.P., Schmidhuber, J.: Probabilistic incremental program evolution.
Evolutionary Computation 5 (1997) 123–141
21. Pelikan, M., Goldberg, D.E., Cant´u-Paz, E.:
BOA: The bayesian optimization
algorithm. Proceedings of the Genetic and Evolutionary Computation Conference
1999: Volume 1 (1999) 525–532
22. Zwillinger, D., Kokoska, S.:
CRC standard probability and statistics tables and
formulae. CRC Press, Boca Raton, Florida (2000)
23. Deb, K., Goldberg, D.E.: Analyzing deception in trap functions. Foundations of
Genetic Algorithms 2 (1993) 93–108

A Hybrid Genetic Algorithm for the Hexagonal
Tortoise Problem
Heemahn Choe, Sung-Soon Choi, and Byung-Ro Moon
School of Computer Science and Engineering,
Seoul National University,
Seoul, 141-742 Korea
{hchoe,irranum,moon}@soar.snu.ac.kr
Abstract. We propose a hybrid genetic algorithm for the hexagonal
tortoise problem. We combined the genetic algorithm with an eﬃcient
local heuristic and aging mechanism. Another search heuristic which fo-
cuses on the space around existing solutions is also incorporated into
the genetic algorithm. With the proposed algorithm, we could ﬁnd the
optimal solutions of up to a fairly large problem.
1
Introduction
Hexagonal tortoise problem (HTP), also known as Jisuguimundo, is a numerical
puzzle which was invented by medieval Korean scholar and minister Suk-Jung
Choi (1646-1715) [1]. The problem is to assign the consecutive numbers 1 through
n to the vertices in a graph, which is composed of hexagons like beehives, to make
the sum of the numbers of each hexagon the same. Figure 1 shows two example
hexagonal tortoises. Choi showed a 30-node hexagonal tortoise in his book along
with other various kinds of magic squares.
The HTP is somewhat similar to the magic squares. As for the magic squares,
there is no known method that generates solutions for an arbitrary HTP, either.
A special kind of HTPs, diamond HTPs, have a particular solution using their
symmetry [2]. Figure 1(a) shows the particular solution of a 30-node HTP.
We consider a generalized problem to minimize the variance of hexagonal
sums. In this paper, we propose an eﬃcient local optimization heuristic for the
HTP using problem-speciﬁc knowledge. We also combined a hybrid genetic al-
gorithm with aging mechanism and another search heuristic which focuses on
the space around existing solutions.
The rest of this paper is organized as follows. In Section 2, we describe some
properties of the hexagonal tortoise problem. In Section 3, we describe our local
optimization heuristic. In Section 4, we describe the proposed hybrid genetic
algorithm in detail. In Section 5, we show the experimental results. Finally, the
conclusion is given in Section 6.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 850–861, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem
851
30
1
4
29
26
2
5
8
28
25
22
3
6
9
12
27
24
21
18
7
10
13
23
20
17
11
14
19
16
15
95
95
95
95
95
95
95
95
95
30
11
5
29
99
17
3
20
6
99
14
27
19
16
7
24
99
4
8
99
28
26
99
25
22
2
18
99
13
23
99
21
9
99
10
1
31
15
99
12
32
99
(a) A 30-node 3 × 3 diamond hexagonal
tortoise using the known particular solution.
Dotted lines show numbering sequence.
(b) A 32-node hexagonal tortoise
Fig. 1. Example hexagonal tortoises
2
Hexagonal Tortoise Problem
Hexagonal tortoise problem is to assign a sequence of numbers to vertices in a
graph with a ﬁxed topology. The graph is composed of a number of overlapping
hexagons. The numbers are typically consecutive integers from 1 to n, where n
is the number of vertices in the graph. The term “tortoise” comes from the fact
that the overall shape of the graph resembles the theca (or shell) of a turtle.
Each number has to be assigned to a vertex exactly once. A hexagonal sum is
deﬁned to be the sum of the six numbers of a hexagon. The objective is to ﬁnd
an assignment of the numbers to the vertices so that the standard deviation (or
variation) of the hexagonal sums is zero. In this paper, we consider a special
type of hexagonal tortoise, a k × k diamond hexagonal tortoise problem, which
arrangement of hexagons is like diamond shape [3]. Figure 1(a) shows a solution
of 3 × 3 diamond hexagonal tortoise.
A typical HTP has many solutions with diﬀerent hexagonal sums, which
means that the ﬁtness landscape of the problem is multimodal. For example, in
the 16-node HTP, we found 140,915 diﬀerent optimum solutions among 232 ran-
dom permutations; from this, we presume there are more than 6 × 108 optimum
solutions among total 16! possible assignments for the 16-node HTP. The diﬃ-
culty of multimodal space search with respect to genetic algorithms was pointed
out in the literature [4] [5] [6].
To get more insight into the ﬁtness landscape of the problem, we examined
the ﬁtness-distance correlation (FDC) [7], which is a popular measure for pre-
dicting the diﬃculty of a problem. We compared it with that of the traveling

852
H. Choe, S.-S. Choi, and B.-R. Moon
Table 1. FDC values of the HTP and the TSP instances
(a) HTP instances
Instance FDC value
HTP30
0.1197
HTP48
0.1748
HTP70
0.0877
HTP96
0.0816
HTP126
0.0777
(b) TSP instances
Instance FDC value
lin318
0.6853
pcb442
0.4999
att532
0.5617
rat783
0.6692
dsj1000
0.2187
do {
changed ←False;
for (l0 ←0; l0 < n −1; l0++)
for (l1 ←l0 + 1; l1 < n; l1++)
if (gain(l0, l1) > 0) {
exchange the gene at l0 with the gene at l1;
changed ←True;
}
} while (changed);
Fig. 2. A simple 2-Opt exchange
salesman problem (TSP). Table 1 compares the FDC values of the HTP and
TSP for a number of instances. In the table, “HTPn” represents the n-node
HTP. The table shows that the HTPs with just dozens of nodes have far more
rugged landscapes than the TSPs with hundreds of nodes. For example, the
HTP with 30 nodes shows a lower FDC value than that of the TSP with 1,000
nodes. Although FDC is not a deﬁnite measure for the problem diﬃculty, this
observation says at least the extreme ruggedness of HTP.
3
Local Optimization Heuristic
3.1
Consecutive Exchange
A simple 2-Opt heuristic can be used as a local optimization method for most
combinatorial optimization problems [8]. Figure 2 shows the outline of the 2-
Opt heuristic for the HTP. It examines all possible
n
2

pairs of genes. When it
ﬁnds a pair with positive gain, it exchanges them greedily. The gain is deﬁned
as gain(i, j) = (ﬁtness after exchanging the genes at i and j) - (ﬁtness before
the exchange).
We used a variant of 2-Opt in the following reasons. When we investigated
the gene pairs whose exchanges led to ﬁtness improvements, we found that the
gene pairs with small gene-value diﬀerences were more probable. Figure 3 shows
the frequencies of the gene-value diﬀerences for the pairs that led to ﬁtness

A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem
853













	






 !"#
Fig. 3. Frequencies of the gene value of the pairs diﬀerences that led to ﬁtness im-
provements (HTP160)
do {
changed ←False;
for (v ←1; v < n; v++) {
l0 ←locationOf(v);
l1 ←locationOf(v + 1);
if (gain(l0, l1) > 0) {
exchange the gene at l0 with the gene at l1;
changed ←True;
}
}
} while (changed);
Fig. 4. The outline of consecutive exchange
improvement in a typical run of our genetic algorithm for the 160-node HTP.
For initial chromosomes, the number of the pairs whose diﬀerence in value is
only one was more than 30% of the total number of pairs. And the number grew
further to more than 50% at the 100th generation. This means that the 2-Opt
heuristic would waste most time in vainly examining changes of big diﬀerences.
This is more serious when high-quality solutions are locally optimized.
We designed a local optimization heuristic which examines only the gene pairs
whose values are diﬀered by one. We call it consecutive exchange. Figure 4 shows
the outline of the consecutive exchange. For the problem of size n, while the time
complexity of the inner loop of the original 2-Opt exchange in Figure 2 is Θ(n2),
that of the consecutive exchange is Θ(n). Comparisons of performances of the
2-Opt and consecutive exchanges are presented in Table 2 which are described
in the next section.

854
H. Choe, S.-S. Choi, and B.-R. Moon
T = { original solution };
do {
changed ←False;
for each pair l0, l1 by the exchanging policy
if (gain(l0, l1) > 0) {
exchange the gene at l0 with the gene at l1;
T ←{ current solution };
changed ←True;
} else if (gain(l0, l1) = 0 ∧current solution ̸∈T) {
exchange the gene at l0 with the gene at l1;
T ←T ∪{ current solution };
changed ←True;
}
} while (changed);
Fig. 5. Local optimization with tabu search
3.2
Further Optimization Using Tabu Search
In the process of local optimization, there are gene pairs whose exchanges do not
aﬀect ﬁtness. Exchanging them may help improvement in other gene positions
since it changes the distribution of the hexagonal sums. Usually, there are a
great number of equi-ﬁtness solutions near a solution. We adopted tabu search
to search these solutions eﬃciently; we kept a list of the visited equi-ﬁtness
solutions and prevented revisiting them.
Tabu search is a meta-heuristic for optimization problems invented by Glover
[9]. The tabu search keeps one or more lists of recently visited solutions to prevent
the search from revisiting the visited solutions before.
Figure 5 shows the outline of our local optimization incorporated with tabu
search. The performances of the 2-Opt heuristic and the consecutive exchange
with and without tabu search are summarized at Table 2. We tested them with
a traditional genetic algorithm of population size 1,000. We used the number of
ﬁtness calculation as the index of processor time. Table 2 shows the numbers of
ﬁtness calculations of the heuristics for the initial chromosomes and the chromo-
somes in 100th generation, respectively. In both cases, the 2-Opt heuristic had
slightly lower standard deviation of hexagonal sums (higher ﬁtness) than that of
the consecutive exchange. But, the 2-Opt was much slower than the consecutive
exchange, especially with tabu search. Table 2 indicates that the consecutive
exchanges is more eﬀective in later generations.
4
The Hybrid Genetic Algorithm for the Hexagonal
Tortoise Problem
A notable feature of our hybrid genetic algorithm is that it is combined with
another search heuristic to search the space around the solutions in the current

A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem
855
Table 2. Performances of the local heuristics. Problem size is 160. SD means standard
deviation
(a) The initial chromosomes in a typical GA run
Avg.
Avg.
Avg. #
Avg. gain
Version
SD of
ﬁtness
of ﬁtness per average
hex. sums
gain
calculation calculation
2-Opt without tabu search
1.1648 12364.90
143127.4
0.086391
2-Opt with tabu search
0.7343 12365.75
831483.0
0.014872
Consecutive without tabu search
1.1748 12364.87
8018.8
1.541989
Consecutive with tabu search
0.7516 12365.72
8939.1
1.383337
(b) The chromosomes in the 100th generation in a typical GA run
Avg.
Avg.
Avg. #
Avg. gain
Version
SD of
ﬁtness of ﬁtness per average
hex. sums gain calculation calculation
2-Opt without tabu search
0.9425 401.68
104929.3
0.003828
2-Opt with tabu search
0.6452 402.19
844419.2
0.000476
Consecutive without tabu search
0.9866 401.59
3073.6
0.130660
Consecutive with tabu search
0.6840 402.13
3871.9
0.103858
population. We call it nearby search. This scheme can be regarded as a three
level optimization; the GA is used for large-scale search, the nearby search is
used for medium-scale search, and the local optimization is used for small-scale
search.
The overall structure of the proposed hybrid genetic algorithm is described
in Figure 6. In the main loop, oﬀsprings are generated and local-optimized as in
a typical hybrid GA. Then, the nearby search is applied to all the chromosomes
in the population.
The details of the algorithm are as follows.
– Problem Encoding: Each locus is numbered top to bottom in zigzag order.
Figure 7 shows indices of vertices in the 30-node HTP. The zigzag order was
chosen to reﬂect more geographic localities of genes in the chromosome.
– Fitness and Selection: The ﬁtness of a chromosome is deﬁned as
Fitness = −σ2
where σ2 is the variance of hexagonal sums. Note that the ﬁtness values are
negative. Selection is done based on the rank of the eﬀective ﬁtness that is
slightly modiﬁed from the above ﬁtness. The eﬀective ﬁtness is described in
Section 4.2.
– Population: We set the population size to be 512.
– Local optimization: Local optimization used in our algorithm was explained
in Section 3.

856
H. Choe, S.-S. Choi, and B.-R. Moon
create initial population {c0, c1, . . . , cN−1};
for (i ←0; i < N; i++)
ci ←localOptimize(ci);
while (stop condition is not satisﬁed) {
for (i ←0; i < N
2 ; i++) {
choose two parents p1, p2;
oi ←crossover(p1, p2);
oi ←mutation(oi);
oi ←localOptimize(oi);
}
replace N
2 chromosomes in population with o0, o1, . . . , o N
2 −1;
for (i ←0; i < N; i++) {
ni ←modiﬁcation(ci);
ni ←localOptimize(ni);
if (ﬁtness(ni) ≥ﬁtness(ci)) ci ←ni;
}
}
return the best solution;
Fig. 6. Overall structure of proposed hybrid GA
– Replacement: At each generation, the worst half of the population is re-
placed.
– Crossover: Two-point crossover is used.
– Mutation: Each gene value is increased by one with probability
1
3 or de-
creased by one with probability 1
3.
– Modiﬁcation: This is used in the nearby search and described in Section 4.1.
– Repair: Since crossover or mutation can generate infeasible solutions (i.e.,
one number is assigned to more than one node), repair is needed. In repair
process, gene values are replaced with their sorted orders. If there are genes
with the same value, they are randomly reordered. Figure 8 shows an example
of the process.
– Stop Condition: The algorithm stops when an optimum is found or the num-
ber of generations reaches a threshold.
4.1
Nearby Search
Since the HTPs have signiﬁcantly rugged landscapes as mentioned before, tra-
ditional GAs do not solve well the HTPs with dozens or more nodes, even if
they are combined with some local optimization heuristics. We adopted another
search heuristic which focuses on the space around the solutions in the current
population. This heuristic is similar to the idea of iterated local search [10]. But
our approach is based on population and the search is applied to each chromo-
some once at each generation. From the viewpoint of high-quality solutions that

A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem
857
0
1
2
7
6
5
4
3
8
9
10
11
12
13
14
21
20
19
18
17
16
15
22
23
24
25
26
29
28
27
Fig. 7. Problem encoding
−→
3
1
1
4
5
3
3
2
1
5
6
4
Fig. 8. An example repair process
survive over a number of generations, the heuristic is similar to iterated local
search or large-step Markov chain [11].
Nearby search is applied to every chromosome in the population at the end
of each generation. Each chromosome is modiﬁed and the local optimization
heuristic is applied to it. If the ﬁtness of the newly generated chromosome is not
worse than the original one, the original chromosome is replaced by the new one.
We devised a guided modiﬁcation which uses problem-speciﬁc knowledge. We
deﬁne the error for a gene k as follows:
ε(k) =

h∈Hk
[s(h) −µ],
where Hk is the set of hexagons that include k, s(h) is the hexagonal sum of
hexagon h, and µ is the mean of hexagonal sums.
The modiﬁcation decreases the value of the gene whose error is the largest
to positive and increases the value of the gene whose error is the largest to
negative. It compulsorily alleviates the worst defect caused by those genes and
lets the local optimization heuristic to cope with the change in hexagonal sums
afterward.
Since the perturbation of the applied modiﬁcation is weak, many of the mod-
iﬁed solutions return to the original ones by the local optimization heuristic. In

858
H. Choe, S.-S. Choi, and B.-R. Moon
our algorithm, we look up the tabu list of the original solutions as well as that of
the new ones so that the new chromosomes are diﬀerent from the original ones.
Experimental results for the nearby search are presented in Section 4.3.
4.2
Aging
Since the HTPs have very rugged landscapes, most of the population is often
occupied by local optima that contribute little to ﬁnd the global optima. We
adopted a kind of aging mechanism [12] [13] in which the ﬁtness of a chromosome
decreases over the generations of the genetic algorithm. The age of a newly
generated solution is set to zero. At each generation, the age increases by one.
The eﬀective ﬁtness considering the aging is deﬁned as follows:
Eﬀective Fitness = Fitness −
1
10H · Age
where H is the number of hexagons in a graph. The coeﬃcient
1
10H is chosen by
the following reasons. Since low-quality solutions are easily replaced by better
solutions even without aging, we focused on high-quality solutions. High-quality
solutions that can be found in a mature population usually have the property
that most of its hexagonal sums have the same value and only a few of the sums
are larger or smaller by one. For a chromosome having i hexagons with hexagonal
sum S + 1, j hexagons with hexagonal sum S −1 and H −i −j hexagons with
hexagonal sum S, the variance of its hexagonal sums is
σ2 = i + j
H
−(i −j)2
H2
.
For small i and j, σ2 ≈i+j
H . So the variances of hexagonal sums of high-quality
solutions are discrete and the interval between them is about
1
H . (The property
holds even for the hexagons with hexagonal sum S±2.) Since the ﬁtness is deﬁned
to be −σ2, we chose the coeﬃcient
1
10H so that the ﬁtness of a solution decreases
at intervals of 1
H every ten generations. Thus, if the ﬁtness of a chromosome does
not improve by the nearby search, it is eventually replaced by a new chromosome.
4.3
Performance Improvement by Nearby Search and Aging
Table 3 shows the eﬀects of nearby search and aging. The 96-node HTP was
used for the test. We set four versions of GA depending on the uses of nearby
search and aging. All the versions except version I, which is a standard hybrid
GA, found optimal solutions on every run.
When we compare version I and Version II, we can see the usefulness of nearby
search. Version II was signiﬁcantly better and faster than Version I. Version III
was also similarly better and faster than Version I. Version IV, which combines
nearby search and aging, showed further improvement over version II and III.

A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem
859
Table 3. The eﬀect of the nearby search and aging for the 96-node HTP. The maximal
number of generation was set to be 10,000. SD and CV means standard deviation and
coeﬃcient of variation, respectively.
Version Nearby Aging SD of hex. sums CPU time (seconds)
Search
Best
Avg.
Avg.
CV(%)
I
N
N
0.0000
0.1524 2143.69
18.76
II
Y
N
0.0000
0.0000
170.73
138.53
III
N
Y
0.0000
0.0000
257.96
55.16
IV
Y
Y
0.0000
0.0000
73.67
50.48
Table 4. Experimental results for various instances. Generation 0 means that the
optimum is found in initial population. SD and CV mean standard deviation and
coeﬃcient of variation, respectively.
Size
SD of hex. sums
Generations
CPU time (seconds)
Time per
Best
Avg.
SD
Avg.
CV(%)
Avg.
CV(%)
generation
30
0.0000 0.0000 0.0000
0.00 (SD=0.00)
0.12
17.70
‡
48
0.0000 0.0000 0.0000
0.59
107.99
0.35
24.98
‡
70
0.0000 0.0000 0.0000
14.90
80.82
3.61
64.82
0.28990
96
0.0000 0.0000 0.0000
253.41
57.31
79.40
55.01
0.32395
126 0.0000 0.0000 0.0000
1424.19
49.19
679.13
47.71
0.48207
160 0.0000 0.0047 0.0239 17480.10
68.89 13782.66
67.35
0.75851
‡ We could not compute reliable values for these problem sizes since the numbers of
generations are too small for these problems.
5
Experimental Results
Our oﬃcial version is the GA with the nearby search and aging mechanism. The
algorithm was implemented in C and run on Pentium4 1.5 GHz. Table 4 shows
the experimental results of our algorithm for various instances. The result of
the 160-node HTP is from 50 runs and those of the rest are from 100 runs. We
found the optimal solutions for up to the 160-node HTP in reasonable time. Also,
we could ﬁnd some optimal solutions of the 198-node HTP in a few days. The
average time to ﬁnd the optimal solutions increases exponentially with respect to
the problem size. For the problem of sizes 30, 48, 70, 96, and 126, the algorithm
found optimal solutions on every run. Figure 9 shows one of the solutions that
we found for the 198-node HTP.
6
Conclusion
We proposed an eﬃcient local optimization heuristic for the hexagonal tortoise
problem using some problem-speciﬁc knowledge. The hybrid GA was further
improved by the addition of nearby search. The proposed GA can be regarded
as a three level optimization. Another notable aspect of our algorithm is the

860
H. Choe, S.-S. Choi, and B.-R. Moon
22
157
161
35
622
186
12
134
113
622
51
46
622
118
72
622
91
87
622
99
103
622
23
76
122
622
173
17
181
622
29
622
73
142
622
152
71
622
189
88
622
36
127
65
622
93
54
622
197
67
622
180
622
162
138
133
24
62
107
622
105
126
622
622
83
89
622
80
149
622
114
148
622
187
131
42
622
61
124
622
40
57
622
158
622
147
48
622
47
143
622
90
86
622
188
112
55
622
2
194
622
191
64
622
98
622
18
140
92
154
60
622
111
185
622
135
82
622
75
120
622
110
193
622
19
59
5
622
178
97
31
622
622
198
58
622
63
49
622
94
195
622
108
117
622
53
121
622
168
16
26
179
174
172
622
85
137
622
33
164
1
30
622
139
66
622
192
70
622
68
56
622
3
146
622
102
167
159
38
622
183
69
622
116
79
622
182
156
622
25
175
155
41
622
125
50
622
128
109
622
132
160
27
177
622
96
153
622
37
6
8
166
622
184
11
14
9
170
34
622
74
95
622
150
104
622
163
169
13
44
622
123
45
622
145
10
622
81
77
622
28
21
7
171
622
151
100
622
119
144
622
141
136
622
84
101
622
176
15
622
165
78
622
196
39
622
130
52
622
115
129
622
106
190
622
43
20
4
32
Fig. 9. An optimal solution of the 198-node hexagonal tortoise
aging mechanism. It helps the GA to weed out local optima that contribute
little to ﬁnd global optima.
The proposed GA found optimal solutions for up to the 198-node hexagonal
tortoise problem. This seems to be a fairly good achievement considering the
extreme ruggedness of the problem. Future studies may include investigating
larger size of HTPs, probably with parallel computing, applying the developed
ideas to other combinatorial optimization problems, and further reﬁning the GA.
Acknowledgements. This work was partly supported by Optus Inc. and Brain
Korea 21 Project. The RIACT at Seoul National University provided research
facilities for this study.

A Hybrid Genetic Algorithm for the Hexagonal Tortoise Problem
861
References
1. S. J. Choi.
Gusuryak (a reprint).
Shungshin Womens University Press, Seoul,
Korea, 1983.
2. Y. H. Jun. Mysteries of mathematics: The order of the universe hidden in numbers.
Dong-A Science, 14(7):68–77, 1999.
3. S. K. Lee, D. I. Seo, and B. R. Moon. A hybrid genetic algorithm for optimal
hexagonal tortoise problem. In Genetic and Evolutionary Computation Conference,
page 689, 2002.
4. D. E. Goldberg. Genetic Algorithms in Search, Optimization, and Machine Learn-
ing. Addison Wesley, 1989.
5. S. Forrest and M. Mitchell. Relative building-block ﬁtness and the building-block
hypothesis. In Foundations of Genetic Algorithms, volume 2, pages 109–126. Mor-
gan Kaufmann, 1993.
6. J. Horn and D. E. Goldberg.
Genetic algorithm diﬃculty and the modality of
ﬁtness landscapes. In Foundations of Genetic Algorithms, volume 3, pages 243–
270. Morgan Kaufmann, 1995.
7. T. Jones and S. Forrest.
Fitness distance correlation as a measure of problem
diﬃculty for genetic algorithms.
In Sixth International Conference on Genetic
Algorithms, pages 184–192. Morgan Kaufmann, 1995.
8. E. Aarts and J. K. Lenstra, editors. Local Search in Combinatorial Optimization.
John Wiley & Sons, 1997.
9. F. Glover. Tabu search: Part I,. ORSA Journal of Computing, 3(1):190–206, 1977.
10. H. R. Louren¸co, O. C. Martin, and T. St¨utzle. Iterated local search. In F. W. Glover
and G. Kochenberger, editors, Handbook of Metaheuristic, chapter 11. Kluwer Aca-
demic Publisher, 2002.
11. O. Martin, S. W. Otto, and E. W. Felten. Large-step markov chains for the trav-
eling salesman problem. Complex Systems, 5:299–326, 1991.
12. Z. Michalewicz. Genetic Algorithms + Data Structures = Evolutionary Programs.
Springer, 1992.
13. A. Ghosh, S. Tsutsui, and T. Tanaka. Function optimization in nonstationary en-
vironment using steady state genetic algorithms with aging of individuals. In IEEE
International Conference on Evolutionary Computation, pages 666–671, 1998.

Normalization in Genetic Algorithms
Sung-Soon Choi and Byung-Ro Moon
School of Computer Science and Engineering
Seoul National University
Seoul, 151-742 Korea
{sschoi,moon}@soar.snu.ac.kr
Abstract. Normalization is an approach that transforms the genotype
of one parent to be consistent with that of the other parent. It is a method
for alleviating diﬃculties caused by redundant encodings in genetic algo-
rithms. We show that normalization plays a role of reducing the search
space to another one of less size. We provide insight into normalization
through theoretical arguments, performance tests, and examination of
ﬁtness-distance correlations.
1
Introduction
In a genetic algorithm, solutions are encoded into chromosomes by an encoding
scheme, and the chromosomes are handled by the genetic operators. A ﬁtness
function plays a role as a bridge that connects chromosomes and solutions in that
it evaluates the qualities of solutions by decoding chromosomes. By the analogy
with biology, chromosomes and solutions are called genotypes and phenotypes,
respectively.
An encoding scheme determines the relation between genotype space and
phenotype space. According to the relations between the two spaces, encoding
schemes are classiﬁed into three classes. First, one phenotype is represented by
one genotype. In this case, the genotype space is one of the minimal spaces that
represent the phenotype space without any loss of information. This is in general
considered as the most desirable [1] [2]. Representative examples are real-valued
encodings in function optimization, function approximation, and so on [3] [4].
Next, several phenotypes are represented by one genotype. Such an encoding
scheme can be used to reduce the size of the search space by considering only the
important parameters related to a problem. It seems to be suitable for hybrid ge-
netic algorithms in which more important parameters are treated by the genetic
process and less important ones are ﬁne-tuned by local optimization methods.
In most practical problems, however, various parameters are so intertwined that
it is another diﬃcult problem to select more important parameters [5].
Finally, one phenotype is represented by several genotypes. Such encoding
schemes are called redundant encodings. There are quite a few problems in which
it is diﬃcult to represent one phenotype by one genotype using traditional en-
coding schemes due to the characteristics of the problems. Redundant encodings
are still being used in most of those problems.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 862–873, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Normalization in Genetic Algorithms
863
There are two representative groups of problems belonging to the third class.
One is the group of grouping problems [2]. These are commonly concerned with
partitioning a given item set into mutually disjoint subsets. Examples belonging
to this group include k-way graph partitioning [6] [7] [8], graph coloring, bin
packing, and workshop layouting [2]. The other is the group of the structural
optimization problems. These are concerned with ﬁnding the optimal structures
satisfying given constraints. In the problems, there are often several other so-
lutions structurally equivalent to a solution. Example problems include sorting
network optimization [9], neural network optimization [10], and RNA structure
prediction [11].
Recently, there were discussions about the beneﬁts of redundancy in encod-
ing in evolutionary algorithms [12] [13] [14]. In particular, Shipman, Shackleton,
and Ebner [12] [13] showed that some redundant encodings are useful to help
mutation-based hill-climbing search by constructing many diﬀerent neutral paths
in the genotype space that enable moves at the same ﬁtness level. However, the
tackled problems were of small sizes and only mutation operators were consid-
ered. As mentioned in the next section, in the cases that the problems are of
large sizes and highly epistatic, redundant encodings may lead to severe losses
in the search powers of traditional crossovers (such as 1-point, multi-point, and
uniform) and accordingly of genetic algorithms with them as well.
In order to alleviate the problems caused by redundant encodings, a number
of approaches were proposed. In some problems, adaptive crossovers were devel-
oped that recombine parents in terms of phenotypes [6] [15] [16]. Van Hoyweghen
et al. [17] pointed out problems in a certain class of redundant encodings and
presented some solutions to them. Normalization transforms the genotype of a
parent to another genotype to be consistent with the other parent. There were
a few successful studies that used normalization [7] [8] [9].
Among those approaches, we focus on normalization. We review the normal-
ization methods used in a few problems and state their merits associated with
search space reduction. Finally, we support its validity with experimental results.
The rest of this paper is organized as follows. In Section 2, we argue that
redundant encodings lead to severe losses of search power in genetic algorithms,
in particular, with respect to traditional crossovers. In Section 3, we describe
normalization as a method of alleviating the problems by redundant encodings
and show that the normalization plays a role of reducing the search space to
another space of less size. In Section 4, we review the redundant encodings
and normalization methods used in some problems, and examine the eﬀects
of normalization on the search spaces by experiments. Finally, we make our
conclusions in Section 5.
2
Drawbacks of Redundant Encoding
In general, respectfulness and combination are considered to be two major fea-
tures that crossovers have to possess. Respectfulness means the property that
the alleles common to the parents are transmitted to oﬀsprings. The concept

864
S.-S. Choi and B.-R. Moon
of respectfulness stands on the basis of the commonality hypothesis: “schemata
common to above-average solutions are above average” [18]. Many papers re-
ferred to the importance of respectfulness in the crossover design [1] [18] [19]
[20]. Combination means the property of producing good schemata of higher
orders by recombining good schemata of low orders in parents. The concept
of combination is based on the building block hypothesis [21] and has received
heavy and repeated emphasis in the literature [1] [19] [22] [23].
A redundant encoding represents an optimal phenotype by multiple geno-
types and thus the genotype space becomes multimodal [21]. The diﬃculty of
multimodal problems with respect to genetic algorithms has been pointed out
in the literature [21] [24] [25]. Similarly to the optimal phenotypes, high-quality
schemata are redundantly represented by the encoding: a phenotype schema in
an above-average phenotype is represented by multiple genotype schemata that
are equivalent in terms of phenotype.1 On the other hand, traditional crossovers
such as 1-point crossover and uniform crossover operate on the genotype space
regardless of phenotypes. The redundant encoding is harmful to those crossovers
in both aspects of respectfulness and combination, as described below.
– Respectfulness: Most traditional crossovers are basically respectful in the
genotype space (e.g., 1-point, multi-point, and uniform crossovers). However,
they are not respectful in terms of phenotypes. Those crossovers do not
preserve “equivalent” genotype schemata of high-quality in parents if they
have diﬀerent representations. The probability that a common phenotype
schema in parents is transmitted to the oﬀspring decreases as the schema
has more diﬀerent representations in the genotype space.
– Combination: In highly epistatic problems,2 a low-order schema in a geno-
type may have a positive or negative eﬀect on the quality of the genotype,
depending on the alleles on the other loci of the genotype, i.e., the genotype
context [26]. Traditional crossovers recombine schemata in parents regard-
less of the genotype context. The change in the context may diminish the
longevity of high-quality schemata with respect to crossover. For this reason,
it is diﬃcult to build desirable schemata of higher orders by juxtaposition.
As a phenotype schema has more diﬀerent genotypes, the probability that
the schema emerges decreases.
Thus, in the problems of large sizes and high epistases, it is hard for ge-
netic algorithms with traditional crossovers to cope with the genotype spaces
expanded by redundant encodings. Traditional crossovers cannot handle pheno-
type schemata eﬀectively. Normalization methods transform a given parent to
another genotype so that the genotype contexts of the parents are as similar
as possible in crossover. As mentioned in the next section, these methods help
1 A phenotype schema in a phenotype is implicitly deﬁned to be a partial property
contained in the phenotype.
2 Most of the problems mentioned in Section 1, in which redundant encodings are
used, belong to this class.

Normalization in Genetic Algorithms
865
Gx
Gy
x
G
S
φ(x)
φ(y)
y
Fig. 1. Genotype-phenotype space relationship
traditional crossovers overcome the problems related to respectfulness and com-
bination. They play a role of reducing the search space by considering multiple
genotypes corresponding to a phenotype as a conceptual group.
3
Normalization and Space Reduction
Given the phenotype space S for a problem, a redundant encoding determines
the genotype space G and a function φ : G −→S. For a genotype x ∈G, consider
the subset Gx of G as follows:
Gx = {u ∈G|φ(u) = φ(x)}.
(1)
Gx corresponds to the set of genotypes that share the phenotype of x. We call Gx
the coset (neutral set) of x. The above deﬁnition of cosets gives an equivalence
relation among the genotypes and G is partitioned by these cosets. We denote the
set of cosets by ¯G, and then ¯G and S are in one-to-one correspondence (Figure
1).
As mentioned before, it is necessary to recombine parents in a phenotype level
in order to alleviate the problems of redundant encodings. Then, given parents
x, y ∈G, how do we recombine the corresponding phenotypes, φ(x) and φ(y) ?
The essence of normalization is to view the genotypes in a coset as a phenotype.
Such a view allows that x is substituted by an element in Gx that is the most
consistent to y. That is, given φ(x) and φ(y), normalization operators play a
role of selecting the most appropriate x′ ∈Gx and returning it to the crossover
operator so that high-quality schemata are well preserved and combined.
More formally, suppose a distance function d : G ×G −→R and a consistency
measure s : G × G −→R deﬁned on genotype pairs (R denotes the set of real
numbers). Given parents x, y ∈G, the normalization operator transforms x to
x′ ∈Gx so that s(·, y) is maximized. Then the crossover recombine the two geno-
types x′ and y. In other words, the distances between genotypes are redeﬁned
by normalization and consequently the landscape of search space alters. Since ¯G
and S are in one-to-one correspondence as mentioned, the genotype space is re-
duced to the phenotype space by normalization from the viewpoint of crossover
(Figure 2).

866
S.-S. Choi and B.-R. Moon
φ
Gx
Gy
φ(x)
φ(y)
G
S
x′
d(x′, y)
y
x
d(x′, y)
Fig. 2. Space reduction
Space reduction by normalization modiﬁes the structure of the landscape of
search space. Thus, it changes the diﬃculty of problem with respect to genetic
algorithms and so aﬀects the performances of the genetic algorithms. The change
in the diﬃculty has much to do with the consistency measure s, and selecting
a suitable measure s is crucial in the normalization operator design. It suggests
that normalization is more than simply decreasing the genotype distance (e.g.,
Hamming distance) between parents.
To get insight into the eﬀect of normalization on search space, we consider
ﬁtness distance correlation (FDC) [27], which is a popular measure for predicting
problem diﬃculty [28] [29], in the next section. As seen in the section, normal-
ization leads to considerable increases of FDC values.
Normalization gives rise to the moves at the same level of ﬁtness in the
genotype space, so called, the neutral walks [30] by replacing parents with the
genotypes in their cosets. It is related to the previous studies in the aspect of
using the neutral paths created by redundancy [12] [13]. However, normalization
attempts to alleviate the negative eﬀects of neutral paths.
4
Normalization Examples
In this section, we review the normalization methods used in two problems. In
addition, we consider the eﬀects of normalization on space reduction and analyze
the landscapes of the search spaces before and after normalization. Experimental
results with respect to normalization are also provided. The problems that we
consider in this section are the graph partitioning problem and the sorting net-
work problem, which belong to the groups of grouping problems and structural
optimization problems, respectively.
4.1
Graph Partitioning Problem
Given a graph G = (V, E), where V represents the set of vertices and E rep-
resents the set of edges, k-way partitioning is grouping the vertex set V into k

Normalization in Genetic Algorithms
867
1
4
0
6
9
11
15
12
14
10
5
8
3
2
7
13
2
2
2
3
3
2
1
1
3
0
0
3
1
1
0
0
(a) Parent 1
1
4
0
6
9
11
15
12
14
10
5
8
3
2
7
13
1
3
0
0
0
1
1
3
3
2
2
0
1
3
2
2
(b) Parent 2
1
4
0
6
9
11
15
12
14
10
5
8
3
2
7
13
2
1
3
3
3
2
2
1
1
0
0
3
2
1
0
0
(c) Normalized parent 2
Fig. 3. A normalization example in 4-way graph partitioning
disjoint subsets. In particular, two-way partition is called bisection or biparti-
tion. In a k-way partition, the total number of edges whose endpoints belong to
diﬀerent subsets is called cut size. A k-way partition is said to be balanced if
the diﬀerence of cardinalities between the largest and the smallest subsets is at
most one. The k-way partitioning problem in this section is to ﬁnd a balanced
k-way partition with minimal cut size.
In the k-way partitioning problem, the k-ary encoding scheme, in which k
subsets are represented by the integers from 0 to k −1, has been generally used
[6] [7] [8] [15] [31]. In this case, a phenotype (k-way partition) is represented by
k! diﬀerent genotypes according to the numbering of the subsets.
In the k-way partitioning problem, a normalization method was used in [8],
which is based on the adaptive crossovers proposed in [6] and [15]. The method
can be described as follows. First, it selects a partition from each parents so that
they contain the most vertices in common, and then it assigns the partitions a
common number. It repeats this process with the other partitions and remaining
partition numbers. Figure 3 shows an example of normalization. The parent 2
was transformed to be as consistent as possible to the parent 1. Figure 4 shows an
example crossover operator with and without normalization. The same parents
as in Figure 3 were used. We can see that the oﬀspring was too much perturbed
without normalization; good characteristics of the parents were inherited little
to the oﬀspring.
Tables 1 and 2 show the experimental results of the cases without normaliza-
tion (Ordinary) and with normalization (Normalization) for the bipartitioning
problem and the 32-way partitioning problem, respectively. The experiments
were performed for the instances from Johnson’s benchmark [7]. In the tables,
Ed and ρ indicate the average hamming distance and the ﬁtness distance corre-
lation coeﬃcient (FDC) value [27], respectively, of 10, 000 local optima. To get
the local optima, the Kernighan-Lin heuristic [7] and the heuristic in [31] were
used for bipartitioning and 32-way partitioning, respectively. Best, Avg., and
CPU represent the best cut size, average cut size, and average running time over
100 runs of a hybrid genetic algorithm.
In both problems, the average distances among local optima considerably
decreased by normalization. In the 32-way partitioning problem, the distances
before normalization were close to the chromosome lengths, which shows that

868
S.-S. Choi and B.-R. Moon
w/o norm
Offspring
with norm
Offspring
Parent 2
Parent 1
distance = 5
distance = 13
3 crossover points
15
14
13
12
11
9
8
7
6
5
4
3
2
1
0
Normalized
10
parent 2
1
1
0
0
3
2
3
1
0
2
2
1
3
3
2
1
0
2
2
1 3
3
0
3
0 0
0
2
0
2 3 3 3
1
2
1
1 3
3
2
3
2
1 0 0 0 1
1
3
3
2
2
3
0
1
0
1
0
2 3 3 3 2
2
1
1
0
0
1
3
2
0 0 1
1
3
2
2
2
0
1
4
0
6
9
15
12
14
11
10
5
3
2
7
13
8
1
2
0
0
3
2
1
3
3
0
2
1
1
2
3
0
1
4
0
6
9
11
15
12
14
10
5
8
3
2
7
13
2
2
3
3
3
2
2
1
3
0
0
1
1
0
0
3
Fig. 4. A normalization example in 4-way graph partitioning
common schemata are rare in genotypes. The distances were dramatically re-
duced after normalization. In another experiment, we observed that the average
distances among local optima were almost the same as those among random
solutions in the case without normalization. However, in the case with normal-
ization, we observed that the average distances among local optima were much
less than those among random solutions.3 This implies that the local optima got
closer to one another in the space after normalization.
The considerable increase of FDC values by normalization are observed from
the tables. This indicates that the landscapes of the search spaces altered by
normalization, so that the diﬃculty of search with respect to genetic algorithms
diminished. As expected, normalization led to considerable improvement in per-
formance. Figure 5 shows the change of landscape of an instance G500.005 in
the 32-way partitioning problem. The x-axis represents the distance from the
best out of 10, 000 local optima. Before normalization, the solution quality had
little to do with the distance from the best local optimum. After normalization,
they showed a strong correlation.
3 Experiments showed that, for bipartitioning problem, the average distances among
100, 000 random solutions were 250.00 and 241.02 without and with normalization
for 500-node instances and 500.00 and 487.32 for 1000-node instances. For 32-way
partitioning problem, they were 484.37 and 432.58 for 500-node instances and 968.75
and 899.54 for 1000-node instances.

Normalization in Genetic Algorithms
869
Table 1. Comparison of experiments in bipartitioning
Ordinary
Normalization
Graph
Ed
ρ
Best
Avg.
CPU
Ed
ρ
Best
Avg.
CPU
G500.005
249.99 0.003
51
52.58
0.82 213.22 0.343
50
52.03
0.89
G500.04
250.05 0.012 1744 1745.65 4.51 215.52 0.292 1744 1745.77 4.35
G1000.0025 499.96 -0.018
97
100.57
2.39 445.01 0.398
95
98.18
2.78
G1000.02
500.00 0.005 3382 3385.65 23.54 447.83 0.487 3382 3385.18 22.60
U500.05
249.94 0.009
2
4.80
1.09 215.82 0.312
2
3.48
1.24
U500.40
250.23 -0.004 412
412.00
1.68
33.38 0.867 412
412.00
1.24
U1000.05
499.90 -0.005
1
5.23
2.77 447.85 0.290
1
2.49
3.24
U1000.40
498.24 0.030 737
737.00
9.88 249.13 0.754 737
737.00
7.85
Table 2. Comparison of experiments in 32-way partitioning
Ordinary
Normalization
Graph
Ed
ρ
Best
Avg.
CPU
Ed
ρ
Best
Avg.
CPU
G500.005
483.59 0.019 181
184.92
163.05 300.61 0.621 179
181.63
166.97
G500.04
484.36 0.013 4039 4049.56 1410.56 358.82 0.412 4037 4045.03 1197.93
G1000.0025 968.15 0.009 316
328.55
666.85 723.75 0.472 312
321.32
630.88
G1000.02
968.78 0.030 7824 7839.46 4500.29 813.99 0.303 7818 7832.49 4350.69
U500.05
484.00 0.005 113
120.86
278.33 170.76 0.602 113
116.59
290.75
U500.40
484.04 0.016 5363 5391.38 1197.35 173.56 0.338 5359 5380.15 1161.56
U1000.05
968.47 0.021 127
143.74
702.39 525.48 0.610 117
126.04 1199.85
U1000.40
969.04 0.006 7406 7425.16 3405.49 225.09 0.672 7398 7416.85 3218.18
4.2
Sorting Network Problem
A sorting network is a hardware sorting logic in which the comparisons and
exchanges of data are carried out in a prescribed order. A sorting network is
composed of buses and a number of homogeneous comparators, where each com-
parator c(a,b) performs the elementary operation that compares the ath and bth
buses; if the values are in order, ignore them, otherwise exchange them. We call
a sorting network for n inputs an n-bus sorting network. The sorting network
problem is to ﬁnd an n-bus sorting network with minimal comparators for a
given input size n.
In the sorting network problem, a sorting network is generally represented by
a sequence of comparators [9] [32] [33] [34]. Choi and Moon [9] deﬁned sorting
network isomorphism in terms of construction scheme and showed that the se-
quence enumeration is a redundant encoding. Contrary to the case of the graph
partitioning problem, the number of genotypes for each phenotype may be dif-
ferent from phenotype to phenotype.
Figure 6 shows an example of two isomorphic networks. Between the two
networks, there is a permutation

0 1 2 3 4 5
5 4 3 2 1 0

of bus indices. Although they
are quite diﬀerent in appearance, they are the same in function.

870
S.-S. Choi and B.-R. Moon
0
5
10
15
20
25
30
35
40
45
0
50
100
150
200
250
300
350
400
450
500
Cut size difference
Distance
(a) Ordinary
0
5
10
15
20
25
30
35
40
45
0
50
100
150
200
250
300
350
400
450
500
Cut size difference
Distance
(b) Normalization
Fig. 5. Normalization eﬀect in 32-way partitioning
0
1
2
3
4
5
0
1
2
3
4
5
Fig. 6. Two isomorphic networks
In sorting networks, it is known that the comparators in the front part (left
side) strongly aﬀect the subsequent comparators [34] [35]. This indicates that the
comparators in the front part have stronger eﬀects on the qualities of networks
than others. In this context, a normalization method was proposed in [9] that
transforms a parent so that the front part is as consistent as possible with the
other parent.
For example, consider the parents 1 and 2 in Figure 7. Although the subnet-
works with the ﬁrst eight comparators in the two networks are totally diﬀerent,
they are isomorphic. Figure 7(c) shows the network after the parent 2 is nor-
malized with respect to the parent 1. The actual crossover is conducted between
the parent 1 and the normalized parent 2.
For 16-bus sorting network problem, we performed similar experiments to
those in the graph partitioning problem. In this case, the local heuristic and
hybrid genetic algorithm in [9] were used. Figure 8 shows the change of the
landscapes obtained from 10, 000 local optima generated by the local heuristic.
(a) Parent 1
(b) Parent 2
(c) Normalized parent 2
Fig. 7. A normalization example in the 8-bus sorting network problem

Normalization in Genetic Algorithms
871
0
2
4
6
8
10
12
14
16
0
10
20
30
40
50
60
70
Length difference
Distance
(a) Ordinary
0
2
4
6
8
10
12
14
16
0
10
20
30
40
50
60
70
Length difference
Distance
(b) Normalization
Fig. 8. Normalization eﬀect in 16-bus sorting network problem
Like in the graph partitioning problem, the FDC value considerably increased by
normalization (from 0.079 to 0.847) and signiﬁcant improvement in performance
was observed. (Average running time to ﬁnd 60-comparator networks known as
the best decreased from 1, 316 to 717 minutes.)
5
Conclusion
We argued that redundant encodings, in which a phenotype is represented by
multiple genotypes, lead to severe loss of search power in genetic algorithms,
in particular, with respect to traditional crossovers. As a method of alleviating
the problem of redundant encodings, normalization considers the genotypes cor-
responding to a phenotype as a group. We also showed that the normalization
alters the search space.
We reviewed the redundant encodings and normalization methods used in
the graph partitioning problem and sorting network problem. Through some
experiments, we examined the change of search space, the change in problem
diﬃculty, and the performance improvement with respect to genetic algorithms.
In both problems, the normalization methods decreased the problem diﬃculties
and led to remarkable improvement in performance.
We are currently working on the eﬀective design of normalization methods
for other problems such as neural network optimization and magic tortoise op-
timization.
Acknowledgements. The authors thank Jong-Pil Kim and Yong-Hyuk Kim
for insightful discussions about graph partitioning problem. This work was partly
supported by KOSEF through the Statistical Research Center for Complex Sys-
tems at Seoul National University (SNU) and Brain Korea 21 Project. The
RIACT at Seoul National University provided research facilities for this study.

872
S.-S. Choi and B.-R. Moon
References
1. N. J. Radcliﬀe. Forma analysis and random respectful recombination. In Fourth
International Conference on Genetic Algorithms, pages 222–230, 1991.
2. E. Falkenauer. Genetic Algorithms and Grouping Problems. Wiley, 1998.
3. J. Yang, J. Horng, and C. Kao. A continuous genetic algorithm for global optimiza-
tion. In Seventh International Conference on Genetic Algorithms, pages 230–237.
Morgan Kaufmann, 1997.
4. Y. K. Kwon and B. R. Moon. A genetic hybrid for CHF function approximation.
In Genetic and Evolutionary Computation Conference, pages 1119–1125, 2002.
5. M. J. Martin-Bautista and M. A. Vila. A survey of genetic feature selection in
mining issues. In Congress on Evolutionary Computation, pages 1314–1321, 1999.
6. G. Laszewski.
Intelligent structural operators for the k-way graph partitioning
problem. In Fourth International Conference on Genetic Algorithms, pages 45–52,
1991.
7. T. N. Bui and B. R. Moon. Genetic algorithm and graph partitioning. IEEE Trans.
on Computers, 45(7):841–855, 1996.
8. S. J. Kang and B. R. Moon.
A hybrid genetic algorithm for multiway graph
paritioning.
In Genetic and Evolutionary Computation Conference, pages 159–
166, 2000.
9. S. S. Choi and B. R. Moon. Isomorphism, normalization, and a genetic algorithm
for sorting network optimization. In Genetic and Evolutionary Computation Con-
ference, pages 327–334, 2002.
10. C. Igel and P. Stagge. Eﬀects of phenotypic redundancy in structure optimization.
IEEE Trans. on Evolutionary Computation, 6(1):74–85, 2002.
11. P. Schuster. Molecular insights into evolution of phenotypes. In J. P. Crutchﬁeld
and P. Schuster, editors, Evolutionary Dynamics — Exploring the Interplay of
Accident, Selection, Neutrality and Function. Oxford Univ. Press, 2002.
12. R. Shipman. Genetic redundancy: Desirable or problematic for evolutionary adap-
tation. In Fourth International Conference on Artiﬁcial Neural Networks and Ge-
netic Algorithms, pages 337–344. Springer-Verlag, 1999.
13. M. A. Shackleton, R. Shipman, and M. Ebner.
An investigation of redundant
genotype-phenotype mappings and their role in evolutionary search. In Congress
on Evolutionary Computation, pages 493–500, 2000.
14. K. Weicker and N. Weicker. Burden and beneﬁts of redundancy. In Foundations
of Genetic Algorithms, volume 6, pages 313–333. Morgan Kaufmann, 2001.
15. H. M¨uhlenbein. Parallel genetic algorithms in combinatorial optimization. In Com-
puter Science and Operations Research: New Developments in Their Interfaces,
pages 441–453, 1992.
16. R. Dorne and J. K. Hao. A new genetic local search algorithm for graph coloring.
In Parallel Problem Solving from Nature, pages 745–754. Springer-Verlag, 1998.
17. C. Van Hoyweghen, B. Naudts, and D. E. Goldberg.
Spin-ﬂip symmetry and
synchronization. Evolutionary Computation, 10(4):317–344, 2002.
18. S. Chen. Is the Common Good ? A New Perspective Developed in Genetic Algo-
rithms. PhD thesis, Robotics Institute, Carnegie Mellon University, 1999.
19. G. Syswerda.
Uniform crossover in genetic algorithms.
In Third International
Conference on Genetic Algorithms, pages 2–9, 1989.
20. S. Chen and S. Smith. Commonality and genetic algorithms. Technical Report
CMU-RI-TR-96-27, Robotics Institute, Carnegie Mellon University, 1996.

Normalization in Genetic Algorithms
873
21. D. E. Goldberg. Genetic Algorithms in Search, Optimization, and Machine Learn-
ing. Addison Wesley, 1989.
22. L. Davis. Handbook of Genetic Algorithms. Van Nostrand Reinhold, 1991.
23. R. A. Watson and J. B. Pollack. Recombination without respect: Schema combi-
nation and disruption in genetic algorithm crossover. In Genetic and Evolutionary
Computation Conference, 2000.
24. S. Forrest and M. Mitchell. Relative building-block ﬁtness and the building-block
hypothesis. In Foundations of Genetic Algorithms, volume 2, pages 109–126. Mor-
gan Kaufmann, 1993.
25. J. Horn and D. E. Goldberg.
Genetic algorithm diﬃculty and the modality of
ﬁtness landscapes. In Foundations of Genetic Algorithms, volume 3, pages 243–
270. Morgan Kaufmann, 1995.
26. S. A. Kauﬀman. Adaptation on rugged ﬁtness landscapes. In D. Stein, editor,
Lectures in the Sciences of Complexity, pages 527–618. Addison Wesley, 1989.
27. T. Jones and S. Forrest.
Fitness distance correlation as a measure of problem
diﬃculty for genetic algorithms.
In Sixth International Conference on Genetic
Algorithms, pages 184–192. Morgan Kaufmann, 1995.
28. P. Merz and B. Freisleben. Fitness landscapes, memetic algorithms and greedy
operators for graph bi-partitioning. Evolutionary Computation, 8(1):61–91, 2000.
29. P. Merz and B. Freisleben. Fitness landscape analysis and memetic algorithms for
the quadratic assignment problem. IEEE Trans. on Evolutionary Computation,
4(4):337–352, 2000.
30. M. Huynen.
Exploring phenotype space through neutral evolution.
Journal of
Molecular Evolution, 43:165–169, 1996.
31. J. P. Kim and B. R. Moon. A hybrid genetic search for multi-way graph parti-
tioning based on direct partitioning. In Genetic and Evolutionary Computation
Conference, pages 408–415, 2001.
32. W. D. Hillis. Co-evolving parasites improve simulated evolution as an optimization
procedure. In C. Langton, C. Taylor, J. D. Farmer, and S. Rasmussen, editors,
Artiﬁcial Life II. Addison Wesley, 1992.
33. G. L. Drescher. Evolution of 16-number sorting networks revisited. Unpublished
manuscript, 1994.
34. S. S. Choi and B. R. Moon.
A hybrid genetic search for the sorting network
problem with evolving parallel layers. In Genetic and Evolutionary Computation
Conference, pages 258–265, 2001.
35. S. S. Choi and B. R. Moon.
A graph-based approach to the sorting network
problem. In Congress on Evolutionary Computation, pages 457–464, 2001.

874
Coarse-Graining in Genetic Algorithms: Some Issues
and Examples
Andr´es Aguilar Contreras1, Jonathan E. Rowe2, and Christopher R. Stephens3
1 Instituto de Investigaci´on en Matem´aticas Aplicadas y Sistemas, UNAM, Circuito Escolar,
Ciudad Universitaria, M´exico D.F. 04510
andresac@uxmcc2.iimas.unam.mx
2 School of Computer Science, University of Birmingham, Birmingham B15 2TT, Great Britain
J.E.Rowe@cs.bham.ac.uk
3 Instituto de Ciencias Nucleares, UNAM, Circuito Exterior, A. Postal 70-543, M´exico D.F.
04510.
stephens@nuclecu.unam.mx
Abstract. Following the work of Stephens and coworkers on the coarse-grained
dynamics of genetic systems, we work towards a possible generalisation in the
context of genetic algorithms, giving as examples schemata, genotype-phenotype
mappings, and error classes in the Eigen model. We discuss how the dynamics
transforms under a coarse-graining, comparing and contrasting different notions
of invariance. We work out some examples in the two-bit case, to illustrate the
ideas and issues. We then ﬁnd a bound for the Selection Weighted Linkage Dise-
quilibrium Coefﬁcient for the two-bit onemax problem.
1
Introduction
To model the exact evolution of a genetic algorithm requires us, in general, to track
what happens to each possible individual. For example, if the search space is binary
strings of length ℓ, we have evolution equations for each of the 2ℓpossible strings. It
may also be of interest to investigate what happens to certain subsets of individuals.
There are three reasons for doing this. Firstly, it may be possible to reduce the number of
degrees of freedom in the evolution equations and so make a more tractable model. This
is particularly true when modelling the appropriate effective degrees of freedom for the
dynamics. Secondly, one may be interested in the evolutionary history of one particular
individual, and there may be only a limited number of subsets to which its ancestors
could have belonged. Thirdly, of course, one may have some intrinsic interest in a certain
subset (for example, it may be a subset of high quality individuals or represent some
“kinship” or genetically related group such as the individuals associated with a “niche”
or a species).
The idea of tracking subsets is the basis of Holland’s schemata [Holland, 1975],
Radcliffe’s forma [Radcliffe, 1992] and Vose’s predicates [Vose, 1991]. More re-
cently, Stephens [Stephens and Waelbroek, 1997] has formally studied evolution equa-
tions under this kind of coarse-graining using schemata and extended the anal-
ysis to other contexts than evolutionary computation in
[Stephens, 2003] and
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 874–885, 2003.
c⃝Springer-Verlag Berlin Heidelberg 2003

Coarse-Graining in Genetic Algorithms: Some Issues and Examples
875
[Stadler and Stephens, 2003]. Van Nimwegen [van Nimwegen et al., 1997] has mod-
elled the dynamics of GAs on royal-road, and other functions, using approximate coarse-
grained models. Rowe [Rowe, 1998] has considered the use of unitation classes as a basis
for a coarse-grained model of selection-mutation algorithms. We intend to extend this
work by looking at possible generalisations and limitations in the context of genetic
algorithms.
In [Stephens and Waelbroek, 1997] it was shown that the dynamical equations gov-
erning the evolution of a GA with proportional selection, mutation and one-point
crossover was form invariant under a coarse graining to schemata. This was later ex-
tended [Stephens, 2001] to any selection, mutation and homologous crossover opera-
tors. This form invariance was later studied byVose and Wright [Vose and Wright, 2001]
who discussed a more restrictive form of the invariance using the notion of compatibil-
ity [Vose, 1999] between the coarse graining and the genetic operators.
In the following section, we formally deﬁne the idea of coarse-graining. We then
consider how the dynamics looks after a coarse graining. The ideas and issues are then
illustrated by a series of examples. Finally, we apply coarse-graining to help us estimate
what happens to the linkage disequilibrium coefﬁcient in the two-bit onemax problem.
2
Coarse-Grained Dynamics
Let Ω= {0, 1, 2, . . . , n−1} be the search space. In the case of binary strings of length ℓ,
we identify each string with an integer under standard binary encoding, and n = 2ℓ. We
represent a population by a vector p = (p0, p1, . . . , pn−1) in which pk is the proportion
of individual k in the population. Population vectors are elements of the simplex
Λ =

p ∈Rn :

k
pk = 1, and pk ≥0 for all k

A coarse-graining of Ωwill be a collection of subsets of Ω. Given a ﬁtness function
f : Ω→R we wish to deﬁne the ﬁtness of a given subset. Notice, that this will, in
general, depend on the details of the population. One can think of this situation as being
analogous to a co-evolutionary model, in which the ﬁtness of an individual depends on
the current population. Let P(Ω) denote the power set of Ω(that is, the set of all subsets
of Ω). Then, formally, we have a function
F : Λ −→(P(Ω) −→R)
deﬁned as
F(p)(A) =

i∈A pif(i)

i∈A pi
That is, given a population p ∈Λ, F(p) is a “ﬁtness function” which assigns ﬁtnesses
to subsets of Ω. The ﬁtness of a subset A is the average ﬁtness of elements of A in
population p.
Deﬁnition 1. Let Γ = {γi} ⊆P(Ω) be a collection of subsets of the search space that
covers the search space. That is,

γi = Ω

876
A. Aguilar Contreras, J.E. Rowe, and C.R. Stephens
We call such a collection a coarse-graining of Ω. For any population p ∈Λ we can
assign a ﬁtness to each element of Γ using the function F(p).
A coarse-graining is non-degenerate if Γ is a partition of the search space
i ̸= j =⇒γi ∩γj = ∅
Notice that the ﬁtness of a subset in a coarse-graining depends on the current population
and therefore on time, even if the underlying ﬁtness function f is static.
Examples
1) Schemata. We can associate a schema with the set of all strings which match it. The
set of all schemata forms a highly degenerate cover of the search space, however
given an arbitrary choice of string then all the schemata that contain the string
forms a new basis of the same dimensionality as the original - the Building Block
Basis [Stephens, 2003]. However, unless the ﬁtness function is a constant for all
strings matching a given schema, then the ﬁtness of the schema itself will be a
dynamic quantity (that is, it will depend on the details of the current population).
2) Genotype-phenotype mappings. Suppose we have a map ϕ : Ω→Φ which maps
genotypestophenotypes,whereΦisthespaceofphenotypes.Fitnessisthenassessed
via an individual’s phenotype. That is, there is a function g : Φ →R. The ﬁtness of
a genotype is then f = g ◦ϕ. We can create a non-degenerate coarse-graining by
considering subsets of Ωwhich map to the same phenotype. That is, for each i ∈Φ,
set
γi = {a ∈Ω: ϕ(a) = i}
The ﬁtness of such a subset is constant:
F(p)(γi) =

j∈γi pjf(j)

j∈γi pj
=

ϕ(j)=i pjg ◦ϕ(j)

j∈γi pj
=
g(i) 
ϕ(j)=i pj

j∈γi pj
= g(i)
Thiscoarse-grainingisnaturalwithrespecttoselection,asweonlyneedtokeeptrack
of what happens to the subsets, without worrying about their detailed composition.
2.1 Unitation. A particular example of a genotype-phenotype mapping is when we have
a function of unitation. That is, the search space is binary strings of length ℓand
ﬁtness only depends on the number of ones in a string. The phenotype set is
Φ = {0, 1, 2, . . . , ℓ}

Coarse-Graining in Genetic Algorithms: Some Issues and Examples
877
2.2 The Eigen Model (Needle-in-a-haystack). A second example is that of the Eigen
model [Eigen, 1971]. In this landscape all the strings have the same ﬁtness except for
a special string (the optimum) that has a relatively high ﬁtness. - the so called “master
sequence”. In this landscape, the genotype-phenotype coarse-graining creates only
two equivalence classes, hence there is a reduction in degrees of freedom from N
to one.
3
Exact and Approximate Invariance under a Coarse-Graining
Having motivated the idea of coarse graining and given some simple examples one needs
to understand how the evolution equations for the GA look under the coarse graining.As
mentioned, it was shown in [Stephens and Waelbroek, 1997] that the canonical GA is
form invariant under a coarse graining to schemata, i.e. that the equations have exactly the
same functional form after such a coarse graining. This is a highly non-trivial result, as
a coarse graining in general will not preserve the functional form, as can be simply seen,
for example, in the case of coarse graining from genotype to phenotype in the presence of
mutation or crossover.Vose later showed [Vose, 1999] that schemata are the only coarse-
grained variable that leave the dynamical equations for homologous crossover invariant
and hence form a priviliged set. However, Vose also introduced a more restrictive form
of invariance under coarse graining - compatibility - wherein it was not sufﬁcient that
the equations be form invariant. Formally, if an operator M : Λ →Λ gives the effect of
applying an operator to a population, then a coarse-graining (Γ) is compatible with M
if and only if, for any two populations x, y ∈Λ

j∈γi
xj =

j∈γi
yj =⇒

j∈γi
M(x)j =

j∈γi
M(y)j
for all γi ∈Γ (see chapter 16-17 of [Vose, 1999]). It is known, for example, that schemata
are compatible with crossover (by masks) and that unitation classes are compatible with
mutation.
A simple example illustrates the difference between the two different notions of
invariance and coarse graining. Consider selection only in a two-bit one-max model:
The equation of motion for proportional selection is
P(h1h2, t + 1) = (f(h1h2)/ ¯f(t))P(h1h2, t).
(1)
We pass to the schema h1∗by coarse graining h2 to ﬁnd
P(h1∗, t + 1) = (f(h1∗, t)/ ¯f(t))P(h1∗, t)
(2)
where f(h1∗, t)
=
(f(h1h2)P(h1h2, t) + f(h1¯h2)P(h1¯h2, t)/(P(h1h2, t) +
P(h1¯h2, t)) and ¯h2 is the bit complement of h2. Clearly (1) and (2) have the same
functional form. However, to satisfy compatibility f(h1∗, t) would have to satisfy
f(h1∗, t) = f(h1), where f(h1 = 1) = 1 and f(h1 = 0) = 0. Compatibility will
only be valid when the problem exhibits an exact equivalence relation (“symmetry”)
and the genetic operators respect this symmetry, such as is the case for the genotype-
phenotype map and selection only, or with schemata and crossover only. The existence

878
A. Aguilar Contreras, J.E. Rowe, and C.R. Stephens
of an exact symmetry usually allows for a reduction in the number of degrees of freedom
by going to those effective degrees of freedom that are invariant under the symmetry.
However, the utility of coarse graining is not restricted to when it is compatible with
the dynamics. For instance, in [Stephens, 2001] the form invariance of the equations of
motion was used to prove a generalization of Geiringer’s theorem to the case of non-
ﬂat landscapes. Further, in the physical sciences, where coarse graining has played an
essential role, its utility is precisely for those cases where symmetries are not present, but
rather where the coarse-grained dynamics can provide an approximate description of the
system. In this case the closer are the coarse-grained variables to the true effective degrees
of freedom then the better the approximation. For instance, in a strong selection regime
one would expect phenotypes to approximate well the true dynamics with mutation
and/or crossover inducing a small “interaction” between different phenotypes. Similarly,
in the case of strong crossover and weak selection one would expect one-schemata to
approximate well the dynamics with selection inducing a small interaction between the
different one-schemata.
4
Crossover and Schemata Coarse Graining
We can track the evolutionary history of the production of a string via crossover, by
looking at its constituent schemata, i.e. by using the Building Block basis. Suppose that
we only have one-point crossover and no selection or mutation. We adopt the following
notation, given that γ is a schema:
– D(γ) is the set of indices for which γ has deﬁned bit values. For example
D(1 ∗∗1 1 ∗) = {1, 4, 5}
– Lj(γ) is the schema which has the same deﬁning bits as γ for all indices ≤j, and
stars elsewhere. For example, L4(1 ∗∗1 1 ∗) = 1 ∗∗1 ∗∗.
– Rj(γ) is the schema which has the same deﬁning bits as γ for all indices > j, and
stars elsewhere. For example, R4(1 ∗∗1 1 ∗) = ∗∗∗∗1 ∗.
– If the population at time t is p ∈Λ, we write P(γ, t) = 
i∈γ pi.
Then, following [Stephens and Waelbroek, 1997] we have
P(γ, t + 1) =

j∈D(γ)
P(Lj(γ), t)P(Rj(γ), t)
Notice that this equation also applies to strings, by associating a string with the schema
having all the corresponding bits deﬁned.
We can use this equation to see how a given string can be created by crossover over
several generations. For example, the string 1 1 1 can be created from the pairs 1 ∗∗,
∗1 1 and 1 1 ∗, ∗∗1. In the previous generation the schema ∗1 1 may have been created
from the pair ∗1 ∗, ∗∗1. We see that there are several different possible “family trees”
that can be constructed, the leaves of which are the order-one schemata which match the
string at the root. All of the elements of the trees however are elements of the Building
Block basis associated with the string of interest.

Coarse-Graining in Genetic Algorithms: Some Issues and Examples
879
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
P
p_m
P(0,t)
P(1,t)
 P(2,t)
P(1,t)/2
Fig. 1. Asymptotic string proportions in a onemax landscape with mutation rate pm and no
crossover. P(2) decreases as we increase the probability of mutation, while P(1) and P(0)
increase with pm, and when pm reaches 1/2, all the strings are equally represented.
5
Mutation and the Genotype-Phenotype Coarse-Graining
The unitation coarse graining is a particular case of the genotype-phenotype coarse-
graining in which the ﬁtness of a string is its Hamming weight (or the Hamming distance
to the 0 string, f(i) = w(i) = d(i, 0)). Now we can write the equivalence classes like
γj = {i ∈Ω|w(i) = j}. In this particular case the reduction of the search space is huge,
going from 2ℓdegrees of freedom to only ℓ+ 1 effective degrees of freedom.
As an example of the coarse-graining technique in this scenario, consider a GA with
probability of crossover zero, probability of mutation µ and ℓ= 2, with the onemax
ﬁtness function.The set of equations describing the evolution of the system are as follows




p00(t + 1)
p01(t + 1)
p10(t + 1)
p11(t + 1)



=




(1 −µ)2 (1 −µ)µ (1 −µ)µ
µ2
(1 −µ)µ (1 −µ)2
µ2
(1 −µ)µ
(1 −µ)µ
µ2
(1 −µ)2 (1 −µ)µ
µ2
(1 −µ)µ (1 −µ)µ (1 −µ)2




×
1
p01(t) + p10(t) + 2p11(t)




0 0 0 0
0 1 0 0
0 0 1 0
0 0 0 2








p00(t)
p01(t)
p10(t)
p11(t)




(3)
While in the coarse-grained basis the equations are:

880
A. Aguilar Contreras, J.E. Rowe, and C.R. Stephens
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
P
p_m
P(0,t)
P(1,t)
 P(2,t)
P(1,t)/2
Fig. 2. Asymptotic string proportions in a Eigen model landscape with mutation rate pm and no
crossover.


p(γ0, t + 1)
p(γ1, t + 1)
p(γ2, t + 1)

=


(1 −µ)2
(1 −µ)µ
µ2
2(1 −µ)µ (1 −µ)2 + µ2 2(1 −µ)µ
µ2
(1 −µ)µ
(1 −µ)2


×
1
p(γ1, t) + 2p(γ2, t)


0 0 0
0 1 0
0 0 2




p(γ0, t)
p(γ1, t)
p(γ2, t)


(4)
We can solve this system using a similarity transformation for the state-transition
matrix, ﬁnding the eigenvectors and replacing the original matrix with a diagonal similar
matrix (see, for example, chapter 6 of [Reeves and Rowe, 2001]). The results are shown
in ﬁgure 1, where we can see the ﬁxed points as a function of the probability of mutation
irrespective of the initial conditions of the population.
The unitation coarse-graining allow us to eliminate a redundant variable. Of course, it
proves to be more useful as we increase the dimension of the search space [Rowe, 1998].
In contrast, for the Eigen model the genotype-phenotype coarse-graining is not com-
patible with mutation. Instead, we divide the space into Hamming distance classes from
the master sequence. That is γj = {i ∈Ω|d(i, cms) = j}, where cms is the master
sequence. In the case where cms = 0, this gives us the unitation coarse-graining.
Now we can solve the system using the same method as before, considering f(cms) ≫
f(j), ∀j ̸= cms. The results in the long-time limit for the two-bit problem are shown
in ﬁgure 2. Note that qualitatively we obtain a very similar behaviour to the onemax
landscape. This means that in spite of the fact that all the strings different to the master
sequence have the same low ﬁtness, evolution favours those close to the master sequence
as we can see in ﬁgure 2. This phenomenon in known in the literature as the formation
of a quasi-species.

Coarse-Graining in Genetic Algorithms: Some Issues and Examples
881
6
Linkage-Disequilibrium
Let ij be any string of 2 bits, i, j ∈{0, 1}, then the dynamics of the system under
proportional selection (with the onemax landscape) and one-point crossover is given by
the equations:
P(ij, t + 1) = (1 −pc)P ′(ij, t) + pcP ′(ij, t) [P ′(ij, t) + P ′(i ̸ j, t) + P ′(̸ ij, t)]
+ pcP ′(i ̸ j, t)P ′(̸ ij, t)
= (1 −pc)P ′(ij) + pcP ′(ij, t) [1 −P ′(̸ i ̸ j, t)] + pcP ′(i ̸ j, t)P ′(̸ ij, t)
= P ′(ij) + pc [P ′(i ̸ j, t)P ′(̸ ij, t) −P ′(ij, t)P ′(̸ i ̸ j, t)]
where P ′ is the proportion after selection, pc is the probability of crossover and ̸ i is the
complement base 2 of i.
∆′(t) = pc [P ′(i ̸ j, t)P ′(̸ ij, t) −P ′(ij, t)P ′(̸ i ̸ j, t)] is the Selection Weighted
Linkage Disequilibrium Coefﬁcient (SWLDC) explicitly introduced in [Stephens, 2001]
(and implicit in earlier work) in analogy with the original Linkage Disequilibrium
Coefﬁcient (LDC), well known in population biology, that measures how far is the
current population from Robbins proportions (in which the bits are distributed inde-
pendently — Geiringer’s Theorem tells us that this is the limit of repeatedly applying
crossover [Geiringer, 1944]). We can write
w(t + 1) = pcx(t)y(t)
¯f 2(t)
(5)
x(t + 1) = x(t)
¯f(t) −pcx(t)y(t)
¯f 2(t)
(6)
y(t + 1) = y(t)
¯f(t) −pcx(t)y(t)
¯f 2(t)
(7)
z(t + 1) = 2z(t)
¯f(t) + pcx(t)y(t)
¯f 2(t)
(8)
where w(t) = P(00, t), x(t) = P(01, t), y(t) = P(10, t), z(t) = P(11, t), notice that
P(00, t) = ∆′(t)∀t > 0
Substituting the value of x(t) from (6) and y(t) from (7) in (5)
w(t + 1=
pc
¯f 2(t)
x(t −1)
¯f(t −1) −pcx(t −1)y(t −1)
¯f 2(t −1)
 y(t −1)
¯f(t −1) −pcx(t −1)y(t −1)
¯f 2(t −1)

(9)
which we can rewrite as:
w(t + 1) =
1
¯f 2(t)

w(t) −pcw(t)
x(t −1) + y(t −1)
¯f(t −1)

+ pcw2(t)

(10)
As there are four genotypes and three phenotypes we now coarse-grain using the
genotype-phenotypemap,consideringthephenotypicvariableb(t) = x(t)+y(t),adding
(6) and (7) at time t to ﬁnd
b(t −1)
¯f(t −1) = b(t) + 2w(t)
(11)
which can then be substituted into (10) to obtain

882
A. Aguilar Contreras, J.E. Rowe, and C.R. Stephens
0
0.01
0.02
0.03
0.04
0.05
0.06
1
2
3
4
5
6
Fig. 3. Linkage Disequilibrium Coefﬁcient w(t) (continuous line) and the bound w∗(t) (dotted
line) in a typical run with a random initial population. The initial values correspond to different
values of pc = j+2
5 , j ∈{0, 1, 2, 3}
w(t + 1) = w(t)
¯f 2(t) [1 −pc(w(t) + b(t))]
(12)
with ¯f(t) = b(t) + 2z(t). Now if we assume pc = 1 we can simplify (12) to
w(t + 1) =
w(t)
2z(t) + b(t)

z(t)
2z(t) + b(t)

(13)
Note that this equation, because of the substitution (11), is only valid for w(t) = 0, as
the latter is a ﬁxed point of (13) but not of (5) (unless x(t) = y(t) = 0 aswell), and also
for pc > 0.
Let g(t) =

z(t)
2z(t)+b(t)

, iterating we ﬁnd the following solution
w(t) =
w(0) t−1
i=0 g(i)
2tz(0) + b(0) + δt≥3w(0) t−2
i=1 2i t−2−i
j=0
g(j)
(14)
where δC = 1 if C is true and 0 otherwise. It is easy to see that g(t) ≤1
2∀t, so we can
write
w(t) ≤
 1
2
t w(0)
2tz(0) + b(0) + δt≥3w(0)
t−2
i=1 2i  1
2
t−i−1
=
 1
2
t w(0)
2tz(0) + b(0) + δt≥3
w(0)
3
(2t−1 −23−t)
≤
( 1
2)tw(0)
2tz(0) + b(0) = w∗(t)
(15)

Coarse-Graining in Genetic Algorithms: Some Issues and Examples
883
0
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
b
Fig. 4. The dotted lines represent zones with similar SLDC in the simplex as a function of x(t) +
y(t). The continuous line is the limit of the simplex.
Fig. 5. The dotted lines represent the SCSSSS and the continuous SCSCSCSC
A comparison of the bound and the actual results are shown in ﬁg. 3. Note that we
would expect the bound to be better when b(t) ≪z(t), however, for 3 b(0) = 2z(0) so it
is interesting to see that the bound gives reasonable results. Of course, strictly speaking
the derived bound is for pc = 1. The weaker the selective difference between 11 and
10 or 01 then the worse the bound. On the contrary, when the selective advantage of 11
over the other strings is large we expect the bound to become better and better. A further
simpliﬁcation lead us to
w∗(t) =
 1
2
t w(0)
2tz(0) + b(0) ≤
1
4
t w(0)
z(0)
(16)
where we can appreciate more clearly the exponential decay of the SWLDC.

884
A. Aguilar Contreras, J.E. Rowe, and C.R. Stephens
By the deﬁnition of w(t) in (5) and its value in (12) when pc = 1 we can conclude
that in every generation before selection we have w(t)z(t) −x(t)y(t) = 0 i.e. the
population is in linkage (in the usual sense). This means that the population follows
a path (in the simplex) always in linkage equilibrium, i.e. on the Geiringer manifold,
towards a selective linkage equilibrium. Clearly, the latter is of more relevance than the
former for the dynamical evolution.
In ﬁgure (4) we can see the different “contours” with the same SWLDC as a function
of the coarse-grained variable b(t) Notice b(t) = P(1, t) plays an important role in the
solution of the equations suggesting again a possible effective degree of freedom.
As the effects of crossover decay in time, as b(t) gets smaller, we might think about
approximating the usual iteration sequence SCSCSC... with the sequence SCSSSS....
Where S denotes selection and C crossover. The effects can be seen in Figure 5. Note
how the approximation to SCSCSC... uniformly improves in a “perturbative” fashion as
we include more Cs in the sequence, four Cs giving a very good approximation.
7
Conclusion
We have discussed the notion of coarse graining in GAs, giving a formal deﬁntion and
some representative examples such as genotype-phenotype mappings, schemata, error
classes. We discussed how the dynamical equations for a GA transform under a coarse
graining comparing and contrasting the notions of form invariance and compatibility
and discussed some practical issues and potential problems that arise when applying
arbitrary coarse-grainings. However, the evolution equations of certain GAs can be sim-
pliﬁed with an appropriate choice of subsets. Schemata are natural subsets to consider
as the dynamics is form invariant under a schema coarse graining for selection, mutation
and crossover, whereas only mutation and crossover are compatible with the dynamics.
Unitation classes were also seen to be natural in the case of selection and bitwise mu-
tation. Genotype-phenotype coarse-grainings are natural in the case of selection, since
they give rise to constant (that is, static) ﬁtness values for the subsets. We have illus-
trated these ideas and problems with some simple examples. Finally, we have shown how
a genotype-phenotype coarse-graining can help calculate an estimate for the selective
linkage disequilibrium coefﬁcient in the two-bit case.
References
[Eigen, 1971] Eigen, M. (1971). Self-organization of matter and the evolution of biological
macromolecules. Naturwissenschften, 58:465.
[Geiringer, 1944] Geiringer, H. (1944). On the probability theory of linkage in Mendelian hered-
ity. Annals of Mathematical Statistics, 15(1):25–57.
[Holland, 1975] Holland, J. (1975). Adaptation in Natural and Artiﬁcial Systems. University of
Michigan Press, Ann Arbor, Michigan.
[Radcliffe, 1992] Radcliffe, N. J. (1992). The algebra of genetic algorithms. Anals of Mathematics
and Artiﬁcial Intelligence, 10:339–384.
[Reeves and Rowe, 2001] Reeves, C. R. and Rowe, J. E. (2001). GeneticAlgorithms — Principles
and Perspectives, chapter 6, The Dynamical Systems Model. Kluwer Academic Publishers.

Coarse-Graining in Genetic Algorithms: Some Issues and Examples
885
[Rowe, 1998] Rowe, J. (1998). Population ﬁxed-points for functions of unitation. In Banzhaf,
W. and Reeves, C., editors, Foundations of Genetic Algorithms, pages 69–84. Morgan Kauf-
mann.
[Stadler and Stephens, 2003] Stadler, P. F. and Stephens, C. R. (2003). Landscapes and effective
ﬁtness. Comm. Theor. Biol. Accepted for publication.
[Stephens, 2001] Stephens, C. R. (2001). Some exact results from a coarse grained formulation
of genetic dynamics. In Spector, L., Goodman, E. D., Wu, A., Langdon, W. B., Voigt,
H.-M., Gen, M., Sen, S., Dorigo, M., Pezeshk, S., Garzon, M. H., and Burke, E., editors,
Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2001),
pages 631–638, San Francisco, California, USA. Morgan Kaufmann.
[Stephens, 2003] Stephens, C. R. (2003). The renormalization group and the dynamics of genetic
systems. Acta Phys. Slov., 52:515–524.
[Stephens and Waelbroek, 1997] Stephens, C. R. and Waelbroek, H. (1997). Effective degrees
of freedom in genetic algotithms and the block hypothesis. In B¨ack, T., editor, Proceed-
ings of the Seventh International Conference on Genetic Algotithms, pages 34–41. Morgan
Kauffman.
[van Nimwegen et al., 1997] van Nimwegen, E., Crutchﬁeld, J. P., and Mitchell, M. (1997). Finite
populations induce metastability in evolutionary search. Physics Letters A, 229:144–150.
[Vose, 1991] Vose, M. D. (1991). Generalizing the notion of a schema in genetic algorithms.
Artiﬁcial Intelligence, 50:385–396.
[Vose, 1999] Vose, M. D. (1999). The Simple Genetic Algorithm: Foundations and Theory. MIT
Press, Cambridge, MA.
[Vose and Wright, 2001] Vose, M. D. and Wright, A. H. (2001). Form invariance and implicit
parallelism. Evolutionary Computation, 9(3):355–370.

Building a GA from Design Principles for
Learning Bayesian Networks
Steven van Dijk, Dirk Thierens, and Linda C. van der Gaag
Universiteit Utrecht, Institute of Information and Computing Sciences, Decision
Support Systems, PO Box 80.089, 3508 TB Utrecht, The Netherlands
{steven, dirk, linda}@cs.uu.nl
Abstract. Recent developments in GA theory have given rise to a num-
ber of design principles that serve to guide the construction of selecto-
recombinative GAs from which good performance can be expected. In
this paper, we demonstrate their application to the design of a GA for
a well-known hard problem in machine learning: the construction of a
Bayesian network from data. We show that the resulting GA is able to
eﬃciently and reliably ﬁnd good solutions. Comparisons against state-
of-the-art learning algorithms, moreover, are favorable.
1
Introduction
Recent developments in GA theory [5,18,12,6,20] have yielded in-depth insight
in the search behavior of selecto-recombinative GAs. This insight stresses the
importance of concepts such as linkage, mixing, and disruption. The use of a
selecto-recombinative GA for solving a search problem is appropriate if the link-
age of the problem can be assessed with reasonable conﬁdence. The linkage then
determines the location of the building blocks for good solutions for the prob-
lem. It further allows for the design of a crossover operator that mixes well and
indicates where disruption can occur, thereby making counter measures easier
to take. We use these recent insights to deﬁne various design principles to guide
the construction of GAs from which we can expect good performance.
To demonstrate the eﬀectiveness of our GA design principles, we apply them
to the problem of learning Bayesian networks (BNs) from data. BNs [13] are
probabilistic graphical models that capture a joint probability distribution by
explicitly modeling the independences between the statistical variables involved
in a directed acyclic graph (DAG). The strengths of the relationships between
the variables are quantiﬁed by probability tables. Figure 1 depicts a hypothetical
example of a BN. While Bayesian networks have proven their value as a robust
framework for reasoning with uncertainty in a range of applications, their con-
struction can be a daunting task. Especially when having to rely upon extensive
collaboration of domain experts to build the graphical structure and to elicit the
required probabilities, handcrafting a network is hard and very time consuming.
If databases are available that store information about the statistical variables
of importance, however, these data can be exploited to construct a Bayesian
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 886–897, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Building a GA from Design Principles for Learning Bayesian Networks
887
Fig. 1. Bayesian network with three variables (X = {x, ¯x}, Y = {y, ¯y} and Z = {z, ¯z}).
network automatically. The learned network can then be further improved with
the help of domain experts.
The BN learning problem essentially is an optimization problem, where a
Bayesian network has to be found that best represents the probability distri-
bution that has generated the data in a given database. The use of maximum-
likelihood estimates allows us to take the frequencies in the data for a net-
work’s probability tables. The learning problem thereby reduces to the problem
of searching the optimum in the space of all DAGs. A trade-oﬀhas to be made
between the structural complexity of a network and the accuracy with which
it describes the data, since complex networks tend to suﬀer from over-ﬁtting
and make the running time of inference algorithms prohibitively large. An oft-
used measure that serves to balance accuracy and complexity is the MDL mea-
sure, based on the principle of minimal description length from information
theory [15,9]. In this paper we solve the BN learning problem for databases with
complete cases by searching for a DAG that, after completion to a network with
maximum-likelihood estimates, minimizes the MDL score.
The learning problem is a suitable problem to exemplify our GA design prin-
ciples with since the graphical nature of a BN allows for an easy assessment of
the structure of the problem. A learned network should connect two nodes if
the corresponding variables are dependent. Even though the reverse need not
be true, strong dependences found in the data indicate which pairs of nodes
are important for the search. This observation translates into an assessment of
the linkage between the genes in an appropriately-chosen representation, and
therefore of the location of the building blocks.
Our contributions are the following. We show how to solve an important
search problem from GA design principles. We further describe a carefully de-
signed GA for the learning problem that compares favorably against current
state-of-the-art learning algorithms. We present results on two real networks in
addition to the commonly used Alarm network. The remainder of the paper is
organized as follows. In Section 2, we discuss previous work on the BN learning
problem. We continue in Section 3 with a discussion of recent developments in
GA theory and with a statement of our design principles. In Section 4, we de-
scribe our GA for the learning problem. We present results and comparisons in
Section 5. We conclude and discuss future work in Section 6.

888
S. van Dijk, D. Thierens, and L.C. van der Gaag
2
Related Work
Most state-of-the-art algorithms for learning BNs from complete data can be
classiﬁed as taking one of two approaches: the use of an (in)dependence test such
as Pearson χ2 or mutual information [16,4], and the use of a quality measure such
as MDL [2,7,9]. With both approaches, encouraging results have been reported.
Both approaches, however, also have their disadvantages.
In the ﬁrst approach, a statistical test is employed for examining whether or
not two variables are (in)dependent given some conditioning set of variables. The
order of the test is the size of the conditioning set used. By starting with zero-
order tests and selectively growing the conditioning set, in theory, all relevant
(in)dependences can be extracted from the data and the network can be exactly
recovered. In practice, however, the test quickly becomes unreliable for higher
orders. For these higher orders, the number of data available for the test decreases
exponentially with the order. For example, for binary variables and a database
of 1000 cases (a realistic size), a sixth-order independence test would be based,
on average, on approximately 1000/26 ≈15 cases from the database. As a result,
the test can be inaccurate, which can aﬀect the quality of the learned network.
In the second approach, an information measure is used for assessing the
quality of candidate DAGs. This approach suﬀers from the size of the search
space of DAGs. To eﬃciently traverse this huge space, often a greedy search
algorithm is used to focus attention on the most promising regions. Other algo-
rithms explicitly constrain the search space by assuming a topological ordering
on the nodes of candidate DAGs. With both types of algorithm, the optimal
DAG may be pruned from the space that is eﬀectively searched.
Larra˜naga et al. [11] have proposed a genetic algorithm based upon the latter
approach. In their GA, a DAG is represented by a connectivity matrix that is
stored as a string (the concatenation of its rows). Recombination is implemented
as one-point crossover on these strings; mutation is implemented as random bit-
ﬂipping. After children have been generated, the resulting graphs are rendered
acyclic by randomly deleting arcs that are part of a cycle. In addition, for nodes
with a number of (immediate) predecessors that exceeds a predeﬁned constant,
the best subset is chosen that is small enough. In related work, Larra˜naga et
al. [10] experimented with a GA that searches for an ordering that is passed on to
K2, a greedy search algorithm. They concluded that the results were comparable
to those of their previous GA. The use of diﬀerent crossover operators in a GA
for the learning problem has recently been explored by Cotta and Muruz`abal [3],
who concluded that adaptive operators gave the best results.
Recently, Wong et al. [22] proposed the “hybrid evolutionary programming”
(HEP) algorithm that combines the use of independence tests with a quality-
based search. The search space of DAGs is constrained in the sense that each
possible DAG only connects two nodes if they show a strong dependence in the
available data. The algorithm evolves a population of DAGs to ﬁnd a solution
that minimizes the MDL score. A new population Pop(t + 1) is constructed
as follows. First, half the members from Pop(t) are copied and put in an in-
termediate population. The members from the other half are “merged” with

Building a GA from Design Principles for Learning Bayesian Networks
889
the members from the previous population Pop(t −) that were not selected.
Merging is similar to crossover, except that it selectively uses those parts of the
parents that give the largest improvement of the MDL score of the child. The
child is then put in the intermediate population. To each individual in this inter-
mediate population, four diﬀerent mutation operators are applied. The resulting
population is then added to Pop(t). Selection consists of ranking the individuals
and putting the top half in Pop(t + 1).
As mentioned before, the HEP algorithm constrains the search space of
DAGs by only allowing edges that connect nodes with a strong dependence be-
tween them. More speciﬁcally, the algorithm allows an edge between two nodes
if there exists a zero-order dependence between them, and no ﬁrst-order order
independences. The (in)dependence test employed makes use of a threshold that
indicates the required level of signiﬁcance. Wong et al. argue that it is better to
avoid manually setting the threshold. Instead, they allow the search algorithm
to evolve the threshold and constrain the solutions to match their individual
threshold. Initially, a threshold is chosen at random for each individual in the
population. A new individual receives the mutated threshold of its parent.
Wong et al. compared an earlier version of their algorithm against the GA
of Larra˜naga et al. [11], and found that it was faster and produced networks of
better MDL scores. We will compare our GA against HEP in Section 5.
3
Design Considerations
During the previous decade, GA theory has progressed to the point where GAs
giving predictable results can be designed. A descriptive model of the selecto-
recombinative GA has emerged from the literature [5,18,12,6,20] in which the
representation of solutions is divided into several partitions1. A schema of highest
ﬁtness in a partition is called a building block. The proportion of building blocks
of a certain partition in the population should converge to 1.0. The propagation
of building blocks to this end is modeled by competitions between strings and
statistical decision making. By mixing on the boundaries of the partitions, the
building blocks are assembled into a close-to-optimal solution. In other words,
a good solution matches the building block of each partition. The model is ap-
plicable especially to problems that involve an additively decomposable ﬁtness
function. In the work by Harik et al. [6], more speciﬁcally, partitions are separa-
ble, that is, any gene is part of a single partition. The partitions themselves then
correspond to the subfunctions into which the ﬁtness function can be decom-
posed. The model was tested on deceptive subfunctions, and proved to be quite
accurate. Later it was shown that the model could also predict results for more
realistic problems that involve overlapping partitions, such as the map-labeling
1 A chromosome is a string over an alphabet A. A schema is a string over A ∪{#},
which is matched by a chromosome that has the same symbols in the same positions,
except for the #’s. A partition is a string over {#, f} that is matched by schemata
that have an element from A at positions where the partition has an f.

890
S. van Dijk, D. Thierens, and L.C. van der Gaag
problem [20], provided that the deviations from the underlying assumptions are
not too severe.
The insights yielded by the model of the selecto-recombinative GA can be
formulated into the following design principles [21]:
– Use an additively decomposable ﬁtness function.
– Ensure a good building-block supply, either in the initial population or dur-
ing the run.
– Ensure that the proportions of building blocks increase.
– Ensure good mixing of building blocks.
– Minimize disruption of building blocks.
The ﬁrst principle allows for a representation of solutions with a direct mapping
of the ﬁtness subfunctions to genes. Ensuring a good building-block supply is
vital since it is impossible to construct a good solution without building blocks.
Building blocks will be scattered throughout the initial population, or will be
injected during the run by the genetic operators. Proper selection of building
blocks is necessary to ensure their proportions within the population grow. Dur-
ing the run of the algorithm, all building blocks have to be assembled on the
same individual, which makes good mixing of building blocks vital. Since mix-
ing tends to be disruptive, however, care needs to be taken that not too many
building blocks are lost. Central to most of these principles is the assessment of
the linkage, which is the ﬁrst design issue to be addressed. We next exemplify
the various principles by designing a GA for the BN learning problem.
4
The GA
We recall that the learning problem involves searching for a DAG. For the rep-
resentation of DAGs, we use a list of genes (corresponding with connections
between nodes) of ﬁxed length. Each gene can be set to three diﬀerent alleles
corresponding with the two possible arc directions and the absence of an arc.
Within this representation, we need to group genes into partitions in such a
way that the building blocks of the partitions match a close-to-optimal solu-
tion. Recall that each node corresponds with a statistical variable. The arcs to
and from that node correspond with dependences that are reﬂected in the data.
Therefore, genes are linked (part of the same partition) if they involve the same
node. The linkage of an eﬃcient GA involves partitions whose size is bounded
by a small constant. Therefore, we use only connections that correspond with
plausible dependences. This eﬀectively constrains the search to plausible DAGs.
The whole algorithm now consists of two phases. First, we construct from
the database an undirected graph, called the skeleton graph, that represents the
search space for the GA. The graph contains a node for each statistical variable
present in the database. The edges of the skeleton graph are found by performing
conditional dependence tests on the data: nodes are connected by edges if their
corresponding variables are found to be dependent. In the next phase, the GA
will turn the skeleton into a DAG by evolving a population of DAGs that use

Building a GA from Design Principles for Learning Bayesian Networks
891
the skeleton as a template. We observe that during the run of the GA, the
graphical structures yielded need to be acyclic. Our GA ensures this property by
applying a “repair” operator as soon as cycles are introduced during initialization
and crossover. This repair operator breaks cycles in a manner that is not too
disruptive. In addition, no node can have more than a predeﬁned number of
direct predecessors. This constraint is enforced because Bayesian networks with
a large number of predecessors are impractical, since performing probabilistic
inference on such networks is computationally prohibitive.
4.1
Generation of the Skeleton Graph
We construct from the data a skeleton graph to reduce the search space for
the GA. The skeleton graph is constructed from the “important” edges for each
node. An edge is deemed important if zero- and ﬁrst-order tests show that the
corresponding variables are dependent. We use just zero- and ﬁrst-order condi-
tional dependence tests, since higher-order tests quickly become unreliable. If
we now consider the Bayesian network with the optimal (lowest) MDL score
given the data, then, ideally, the skeleton graph is the underlying undirected
graph of the DAG of the network. The skeleton graph, however, can include
spurious edges when a higher-order independence test would have been required
to remove a connection between two nodes. In our experiments, we found that
skeleton graphs were produced that could contain approximately twice as many
edges as the original network from which the data was sampled.
To construct the skeleton graph, we start with the empty graph and ﬁnd for
each node the nodes that should be connected to it. An edge is added between
two nodes if their corresponding variables are dependent. The neighbors of a
node X are found by building a candidate list of neighbors. This list contains
all nodes Y that show a dependence on X through a zero-order dependence
test. It is possible that two nodes (variables) are dependent without having
to be connected, such as the nodes X and Y in Figure 2. These nodes become
independent given Z. We therefore try to remove each node Y from the candidate
list of neighbors for X by testing for a dependence on X given any other node
Z in the list. If such a test shows that X and Y become independent given Z,
then Y is removed from X’s candidate list. For each node Y in the ﬁnal list of
neighbors of X we then add an edge between X and Y in the skeleton graph.
The total number of dependence tests to be performed when constructing the
skeleton graph is bounded by O(n2), where n denotes the number of statistical
variables (and therefore nodes). In our experiments, up to 45 percent of the total
running time was spent on calculating the skeleton graph.
We used the χ2-test for determining whether or not two statistical variables
are dependent given some conditioning set of variables. This test uses a threshold
indicating the largest p-value that would cause it to reject the null-hypothesis
that the variables are independent. A large threshold allows weak dependences
to be found, but may also result in reporting incorrect dependences. A small
threshold makes the test more selective. In practice, we found that the zero-
order dependences from the original network could be recovered with a threshold

892
S. van Dijk, D. Thierens, and L.C. van der Gaag
Fig. 2. Variables X and Y are dependent, yet independent given Z.
very close to zero. In fact, the p-values of the conditional tests rise signiﬁcantly
for nodes that are not true neighbors. In our experiments, therefore, we set the
threshold as low as 0.005.
4.2
The GA
In the design of our GA for the BN learning problem, we applied the design prin-
ciples mentioned before. The requirement of using an additively decomposable
ﬁtness function is satisﬁed by using the MDL measure. This measure is derived
from information theory and states that the description length of a BN and a
database is the sum of the size of the network, and the size of the database after
it has been compressed using the network. The best network given a database
then is the one yielding the smallest description length. We note that a network
that is more complex can represent the underlying distribution of the data better
and compress it to a smaller size. Complex networks, however, require a larger
encoding to specify their arcs and tables. The MDL measure balances these two
issues. It is to be minimized and has the following decomposed form [9]:
MDL(D, B) =
n

i=1
MDLi(D, π(i)) ,
where D denotes the database, B is the candidate Bayesian network, n is the
number of nodes or variables, and π(i) equals the set of predecessors of node i.
The measure thus decomposes into local terms per node.
Having chosen an additively decomposable ﬁtness function, we now outline
the GA, indicating how the other design principles are satisﬁed:
Encoding: a string of genes, each of which corresponds with an edge in the
skeleton graph, and can be set in one of the following states (the alleles):
directedFrom, directedTo, none.
Initialization: an assignment of one of the three alleles is made to each gene
at random. In other words, an edge is either removed or turned into an arc.
All incoming arcs of nodes that thus end up with too many predecessors
(recall that no node can have more than a given number of predecessors) are
traversed in random order. An arc is reversed and if it then still causes an
infeasible solution, it is removed. Finally, the solution is made acyclic with
the operator MakeAcyclic.

Building a GA from Design Principles for Learning Bayesian Networks
893
Selection scheme: for selection and replacement, the incremental elitist re-
combination scheme [17] is used.
Mutation: no mutation in the traditional sense is used. Traditionally, random
mutation is used to (re)generate building blocks during the run. In our GA,
we ensure a good building-block supply by choosing an appropriately sized
population and as a side-eﬀect of the genetic operators (recombination, Re-
pair and MakeAcyclic).
Recombination: to guarantee good mixing of building blocks, recombination
is performed by crossover on the level of partitions. Through the crossover
operator, we try to transfer schemata from the relevant partitions intact
from parent to child. Since partitions overlap, this may still cause possible
building blocks to be disrupted. Disruption is dealt with after crossover. For
the crossover operator, a node is picked at random. The genes corresponding
with the edges to its neighbors in the skeleton graph are added to the selec-
tion. The operator continues picking nodes until half the number of genes are
selected. The ﬁrst child now inherits the selection of genes from the ﬁrst par-
ent and its complement from the second one. The other child is constructed
by taking the selection of genes from the second parent and its complement
from the ﬁrst one. The genetic operator Repair is applied to the edges in
both children that are part of a possibly disrupted building block. Finally,
MakeAcyclic is applied to both children.
Fitness function: the MDL measure is used to compute the ﬁtness of solutions.
It is minimized to ﬁnd the best solution.
Stop criterion: the GA is halted when the diﬀerence between the average ﬁt-
ness in the population and the ﬁtness of the best individual is smaller than a
certain threshold (we used 0.001), or when the last 10% of the recombinations
performed did not improve the average score by 1.
We describe the Repair and MakeAcyclic operators in more detail.
The Repair operator is used to handle the disruption of possible building
blocks after crossover. We consider a node in one of the parents before recom-
bination. The genes that correspond with the edges in the skeleton graph con-
nected to that node can store a building block, that is, a combination of alleles
for those genes that matches a close-to-optimal solution. If the node is picked
by the crossover operator, these genes are transfered together to a child and
no disruption occurs. If, on the other hand, the child inherits alleles for these
genes from both parents, a building block may be disrupted. We can repair the
building block by returning the genes inherited from one of the parents to their
former setting. Since we don’t know which settings are building blocks, we rely
on the MDL score to ﬁnd the best change. For our implementation, we make the
simplifying assumption, for reasons of eﬃciency, that we can retrieve the original
building block by changing genes one at a time. Moreover, to avoid changing a
child too much, we try to ﬁnd a minimal set of edges to be changed. The results
obtained with the simplifying assumption are quite satisfactory.
The Repair operator now proceeds as follows. For every node, it investi-
gates the genes that correspond with the edges connected to that node in the

894
S. van Dijk, D. Thierens, and L.C. van der Gaag
skeleton graph. Suppose that the ﬁrst parent donates x alleles and the second
parent donates y alleles to a child. If x < y, the operator puts the x (indices to
the) edges in a set S, otherwise the y edges. It then continues with subsequent
nodes, disregarding edges already in S. For example, if the second node that is
investigated has seven incident edges, of which two are already in S and three
are inherited from the ﬁrst parent, the remaining two edges are put in S. After
all nodes have been visited, each edge in S is investigated for each child. If the
allele for the gene corresponding with the edge can be changed to another allele
with an improvement of the MDL score, the best change is chosen. In a typical
run of the GA, for a skeleton of 95 edges, on average eight edges are changed in
a child in the early iterations of the run. This number gradually drops to zero
as the population converges.
The function MakeAcyclic operator is used to render a graph acyclic. It
ﬁrst ﬁnds all cycles in the graph by performing a recursive descent from each
node of a spanning tree. Next, it ﬁnds the change for any edge that is part of
a cycle that deteriorates the MDL score the least. Note that any change will
break at least one cycle. This is repeated until all cycles have been resolved.
If a change caused a new cycle or causes an arc to point to a node with the
maximum number of predecessors, the operator reverts the change and marks
it as forbidden. The process is then repeated (but forbidden states are avoided)
until all cycles have been broken. Note that termination is guaranteed since a
change to a deleted edge will break a cycle but not cause new conﬂicts.
It is worthwhile to point out that the running time of the GA can be dramat-
ically optimized by caching the local score for a node with a certain predecessor
set. Since the calculation of the MDL score is computationally intensive and has
to be done by any MDL-based learning algorithm, the use of a population-based
search method gives a relatively minor overhead. For example, a typical run with
a population size of 150 for a database of 10000 cases took 14 minutes; a run
with a population size of 1000—a more than six-fold increase—took only 35.
In our experiments, the GA (using an incremental selection scheme) performed
on average 2385 recombinations until convergence, corresponding with about 32
generations in a generational scheme.
5
Results
To study the performance of the resulting GA, we conducted experiments on vari-
ous databases that were sampled from three real-life Bayesian networks by means
of logic sampling [8]. The Alarm network [1] (37 nodes, 46 arcs) was originally
built to help anesthetists monitor their patients, and is widely used for evalu-
ating the performance of BN learning algorithms. The Oesoca network [19] (42
nodes, 59 arcs) was developed at Utrecht University, in collaboration with The
Netherlands Cancer Institute, to aid gastroenterologists in staging oesophageal
cancer and predicting the outcome of diﬀerent treatment alternatives. The VSD
network [14] (38 nodes, 52 arcs) also comes from the medical domain and was
developed for treatment planning for a congenital heart disease.

Building a GA from Design Principles for Learning Bayesian Networks
895
We compared our GA against a rather straightforward hillclimber (HC), for a
baseline performance. The hillclimber starts with the empty graph and considers
pairs of nodes that are connected in the skeleton graph. In each step, it ﬁnds
the change (remove, insert, or ﬂip an arc) that improves the MDL score the
most. This is continued until no progress is made, upon which MakeAcyclic
is called to produce the result. We also compared our GA against the original
implementation of the HEP algorithm by Wong et al. In our experiments, we
used databases of 1000 and 10000 cases. The GA and the HEP algorithm were
run ten times. All algorithms allowed a maximum of ﬁve predecessors per node.
The GA used a population size of 150. The HEP algorithm used the default
settings as suggested by Wong et al. The results are presented in Table 1.
Table 1. Results of the experiments. Below the name of the database, the MDL score
of the original network is shown. In each table, the ﬁrst column denotes the algorithm,
the second gives the average MDL score with standard deviation, and the third column
shows the best MDL score found in the ten runs.
algo avg±sd
best
algo avg±sd
best
Alarm-
1000
(17834.3)
GA
HEP
HC
16757.4±12.7
16591.9±40.4
17387.3
16747.1
16567.2
Alarm-
10000
(139266.9)
GA
HEP
HC
139240.4±89.7
140196.6±590.6
140946.4
139097.5
139441.7
Oesoca-
1000
(26070.0)
GA
HEP
HC
24065.0±0.0
23800.3±72.8
24676.6
24065.0
23748.5
Oesoca-
10000
(213765.7)
GA
HEP
HC
213089.8±0.0
213478.8±1096.1
215192.8
213089.8
212715.1
VSD-
1000
(25790.6)
GA
HEP
HC
23186.3±0.0
24641.7±52.8
23318.8
23186.3
24551.5
VSD-
10000
(207647.4)
GA
HEP
HC
206296.1±15.56
220160.1±958.3
209101.0
206251.2
219423.3
From the table we observe that both the GA and HEP yield networks with
MDL scores that are close to the score of the original network. In fact, for all net-
works and associated databases, the GA found an average score that was better
than the score of the original network. For all but two databases (Alarm-10000
and VSD-10000), the same observation holds for HEP. It may seem surprising
that the original network is not the network of the best MDL score. The dif-
ference originates from the fact that the database is a ﬁnite sample, subject to
sampling error, that does not reﬂect all the dependences from the original net-
work accurately. The probability distribution observed in the data, therefore, is
likely to diﬀer from the distribution captured by the original network.
A striking diﬀerence in the results yielded by the two algorithms is that the
GA shows a much smaller standard deviation than HEP. The GA can thus be
seen as the more reliable of the two algorithms as it is more likely to always
give results of similar quality. We observe, however, that for half the number
of databases (Alarm-1000, Oesoca-1000 and Oesoca-10000) HEP’s best result
is slightly better than the GA’s. We further observe that the hillclimber also

896
S. van Dijk, D. Thierens, and L.C. van der Gaag
performs quite well, thereby giving an indication of the beneﬁt from the skeleton
graph. The average MDL score yielded by the GA is always better than that by
the HC, whereas HEP’s best solution can be worse than the result from the HC.
The GA and the HEP algorithm yielded their results fairly quickly, within
a maximum of 20 minutes per run. For the databases of 1000 cases, the longest
running time was four minutes. HEP was generally faster than the GA, with a
maximal diﬀerence in running time by a factor eight. It is important to realize,
however, that for the BN learning problem, the time spent is not a critical factor.
Once a network is learned, it is either used directly for daily problem solving,
or inspected and improved manually. The time spent on learning the network,
therefore, is well within reasonable bounds for both algorithms.
6
Conclusions
Building upon recent developments in GA theory we discussed various principles
for GA design. We demonstrated how to apply these design principles, rather
straightforwardly, to derive a GA for learning Bayesian networks from data that
performs at least comparably to the state-of-the-art algorithms. We feel that the
robustness of the resulting GA is a conﬁrmation of the feasibility of our approach.
For future work, we would like to put the model from which our design principles
were derived on a more formal footing. Speciﬁcally, it is worth investigating how
overlap between partitions inﬂuences the predictions of the model.
Acknowledgments. This research was supported by the Netherlands Organ-
isation for Scientiﬁc Research (NWO). We would like to thank Wong, Lee and
Leung for graciously providing us with an implementation of their HEP algo-
rithm.
References
1. I. A. Beinlich, H. J. Suermondt, R. M. Chavez, and G. F. Cooper. The Alarm
monitoring system: a case study with two probabilistic inference techniques for
belief networks. In J. Hunter et al., editors, Proceedings of the Second Conference
on Artiﬁcial Intelligence in Medicine, pages 247–256. Springer, 1989.
2. G. F. Cooper and E. Herskovits. A Bayesian method for the induction of proba-
bilistic networks from data. Machine Learning, 9:309–347, 1992.
3. C. Cotta and J. Muruz´abal.
Towards more eﬃcient evolutionary induction of
Bayesian networks. In J.-J. M. Guerv´os et al., editors, Lecture Notes in Computer
Science, Volume 2439: Proceedings of the Parallel Problem Solving from Nature
VII Conference, pages 730–739. Springer-Verlag, 2002.
4. L. M. de Campos and J. F. Huete. A new approach for learning belief networks
using independence criteria.
International Journal of Approximate Reasoning,
24(1):11–37, 2000.
5. D. E. Goldberg, K. Deb, and J. H. Clark. Genetic algorithms, noise, and the sizing
of populations. Complex Systems, 6(4):333–362, 1992.

Building a GA from Design Principles for Learning Bayesian Networks
897
6. G. Harik, E. Cant´u-Paz, D. E. Goldberg, and B. L. Miller. The gambler’s ruin prob-
lem, genetic algorithms, and the sizing of populations. Evolutionary Computation,
7(3):231–253, 1999.
7. D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The
combination of knowledge and statistical data. Machine Learning, 20(3):197–243,
1995.
8. M. Henrion. Propagating uncertainty in Bayesian networks by probabilistic logic
sampling. In J. F. Lemmer and L. N. Kanal, editors, Proceedings of the Second
Conference on Uncertainty in Artiﬁcial Intelligence, pages 149–163. Elsevier, 1988.
9. W. Lam and F. Bacchus. Using causal information and local measures to learn
Bayesian networks. In D. Heckerman and A. Mamdani, editors, Proceedings of the
Ninth Conference on Uncertainty in Artiﬁcial Intelligence, pages 243–250. Morgan-
Kaufmann, 1993.
10. P. Larra˜naga, C. M. H. Kuijpers, R. H. Murga, and Y. Yurramendi.
Learning
Bayesian network structures by searching for best ordering with genetic algorithm.
IEEE Transactions on Systems, Man, and Cybernetics, 26(4):487–493, 1996.
11. P. Larra˜naga, M. Poza, Y. Yurramendi, R. Murga, and C. Kuijpers. Structure
learning of Bayesian networks by genetic algorithms: A performance analysis of
control parameters. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 18(9):912–926, 1996.
12. B. L. Miller and D. E. Goldberg. Genetic algorithms, selection schemes, and the
varying eﬀects of noise. Evolutionary Computation, 4(2):113–131, 1996.
13. J. Pearl.
Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann, 1988.
14. N. Peek and J. Ottenkamp. Developing a decision-theoretic network for a congeni-
tal heart disease. In E. Keravnou et al., editors, Proceedings of the Sixth European
Conference on Artiﬁcial Intelligence in Medicine, pages 157–168. Springer, 1997.
15. J. J. Rissanen. Modelling by shortest data description. Automatica, 14:465–471,
1978.
16. P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs.
Social Science Computer Review, 9(1):62–73, 1991.
17. D. Thierens. Selection schemes, elitist recombination, and selection intensity. In
T. B¨ack, editor, Proceedings of the Seventh International Conference on Genetic
Algorithms and their Applications, pages 152–159. Morgan-Kaufmann, 1997.
18. D. Thierens and D. E. Goldberg. Mixing in genetic algorithms. In S. Forrest,
editor, Proceedings of the Fifth International Conference on Genetic Algorithms
and their Applications, pages 38–45. Morgan-Kaufmann, 1993.
19. L. C. van der Gaag, S. Renooij, C. Witteman, B. Aleman, and B. Taal. Proba-
bilities for a probabilistic network: A case-study in oesophageal cancer. Artiﬁcial
Intelligence in Medicine, 25(2):123–148, 2002.
20. S. van Dijk, D. Thierens, and M. de Berg. Scalability and eﬃciency of genetic
algorithms for geometrical applications. In M. Schoenauer et al., editors, Lecture
Notes in Computer Science, Volume 1917: Proceedings of the Parallel Problem
Solving from Nature VI Conference, pages 683–692. Springer-Verlag, 2000.
21. S. van Dijk, D. Thierens, and M. de Berg. On the design and analysis of competent
GAs. Technical Report TR-2002-15, Utrecht University, 2002.
22. M. L. Wong, S. Y. Lee, and K. S. Leung.
A hybrid data mining approach to
discover Bayesian networks using evolutionary programming. In W. B. Langdon
et al., editors, Proceedings of the Genetic and Evolutionary Computation Confer-
ence. Morgan-Kaufmann, 2002.

A Method for Handling Numerical Attributes in
GA-Based Inductive Concept Learners
Federico Divina, Maarten Keijzer, and Elena Marchiori
Department of Computer Science
Vrije Universiteit
De Boelelaan 1081a, 1081 HV Amsterdam
The Netherlands
{divina,mkeijzer,elena}@cs.vu.nl
Abstract. This paper proposes a method for dealing with numerical
attributes in inductive concept learning systems based on genetic algo-
rithms. The method uses constraints for restricting the range of values
of the attributes and novel stochastic operators for modifying the con-
straints. These operators exploit information on the distribution of the
values of an attribute. The method is embedded into a GA based system
for inductive logic programming. Results of experiments on various data
sets indicate that the method provides an eﬀective local discretization
tool for GA based inductive concept learners.
1
Introduction
Inductive Concept Learning (ICL) [14] constitutes a central topic in Machine
Learning. The problem can be formulated in the following manner: given a de-
scription language used to express possible hypotheses, a background knowledge,
a set of positive examples, and a set of negative examples, one has to ﬁnd a hy-
pothesis which covers all positive examples and none of the negative ones (cf. [12,
15]). The so learned concept can be used to classify previously unseen examples.
Concepts are induced because obtained from the observation of a limited set of
training examples. When hypotheses are expressed in (a fragment of) ﬁrst order
logic, ICL is called Inductive Logic Programming (ILP).
Many learning problems use data containing numerical attributes. Numerical
attributes aﬀect the eﬃciency of learning and the accuracy of the learned the-
ory. The standard approach for dealing with numerical attributes in inductive
concept learning is to discretize them into intervals that will be used instead of
the continuous values. The discretization can be done during the learning pro-
cess (local discretization), or beforehand (global discretization). Discretization
methods that employ the class information of the instances are called supervised
methods, while if they do not use this information they are called unsupervised
methods. The simplest way is to use an equal interval width method. In this
way, the continuous values are simply divided into n equal sized bins, where n
is a parameter. A better way for discretizing numerical attributes was proposed
by Fayyad and Irani [9]. This method uses a recursive entropy minimization
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 898–908, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

A Method for Handling Numerical Attributes
899
algorithm and employs the Minimum Description Length principle in the stop-
ping criterion. In [16] a variant of the Fayyad and Irani’s method is used for
discretizing numerical attributes in an Inductive Logic Programming system. In
[1,2] methods using adaptive discrete intervals are used within a GA based sys-
tem for classiﬁcation. Another approach for local discretization is proposed by
Kwedlo and Kretowski. In their work, information on a subset of all thresholds
of a numerical attribute is used as to determine thresholds in evolved decision
rules [13].
The aim of this paper is to introduce an alternative method for dealing
with numerical attributes in evolving classiﬁers where the actual discretization
is determined at run time (i.e. local discretization). An unsupervised, global
method is used to determine the density of data. Due to the use of unsupervised
methods, unlabeled data or a priori knowledge about the distribution of data
can be used to ﬁne-tune the density estimation. The information gathered in the
global pre-processing step is used to guide the genetic operators to make density
controlled operations in the search process.
The method introduced here is general, in that guiding the genetic opera-
tors using estimated, unlabeled or a priori information about the distribution
of data can be used in any evolutionary classifying system. In order to assess
the beneﬁts of the method we use one particular system: the evolutionary ILP
system ECL [7,6]. We run experiments on diﬀerent data sets and compare the
results obtained using the original ECL system (where numerical attributes are
treated as nominal), ECL with numerical attributes discretized using Fayyad
and Irani’s method, and ECL with numerical attributes treated using the novel
method based on constraints. The results of the experiments indicate that the
proposed method allows ECL to ﬁnd better solutions which are comparable or
better than those found using Fayyad and Irani’s method.
The paper it structured in the following way. In section 2 we describe the
method for dealing with numerical attributes. In section 3 we introduce the main
features of the ECL system. In section 4 we perform experiments and discuss
the results and ﬁnally in section 5 some conclusions and future work are given.
2
Handling Numerical Attributes Using Constraints
We propose to handle numerical attributes by using constraints of the form
a ≤X ≤b, where X is a variable relative to a numerical attribute, and a, b
are attribute values. During the execution of a GA based inductive concept
learner, a constraint for a numerical attribute is generated when that attribute is
selected. Constraints are then modiﬁed during the evolutionary process by using
the novel operators deﬁned in the following sections. These novel operators use
information on the distribution of the values of attributes in order to update the
interval boundaries of the constraints. Information about the distribution of the
values of attributes is obtained by clustering the values of each attribute using
a mixture of Gaussian distribution.

900
F. Divina, M. Keijzer, and E. Marchiori
We will ﬁrst brieﬂy introduce the clustering algorithm that is used, and then
we will describe how constraints are modiﬁed. In the sequel we will use Prolog
syntax, where a variable starts with uppercase character while a constant starts
with a lowercase character.
2.1
Clustering Attribute Values
We cluster the values of each attribute in order to get information about the
distribution of the data, and use this information in the operators for modi-
fying constraints. Clustering is performed using the Expectation-Maximization
(EM) algorithm [5] (in the experiments we use WEKA implementation [17]). For
each attribute the EM algorithm returns n clusters described by means µi and
standard deviations σi, 1 ≤i ≤n of Gaussian distributions.
A begin (bcli) and end (ecli) of a cluster clusteri are generated by inter-
secting the distributions of clusteri with the one of clusteri−1 and clusteri+1,
respectively. Special cases are bcl1 = −∞and ecln = +∞.
The boundaries a, b of each constraint a ≤X ≤b are contained in one cluster.
2.2
Operators
Within each cluster, we use constraints for restricting the range of values of an
attribute variable.
A constraint can be modiﬁed either by enlarging its boundaries, or by shrink-
ing, or by shifting the boundaries, or by changing the cluster of its boundaries,
or by grounding the constraint (i.e., restricting its range to a single value).
Formally, consider the constraint C : a ≤X ≤b, and let bcl and ecl be the
begin and the end of the cluster cl containing C.
Enlarge. This operator applied to C returns a constraint C′ = a′ ≤X ≤b′
where a′ ≤a and b ≤b′. The new bounds a′, b′ are computed in the following
way:
1. let min = minimum {P(bcl ≤X ≤a), P(b ≤X ≤ecl)} the minimum of the
probability that X is between bcl and a and the probability that X is between
b and ecl.
2. generate randomly p with 0 ≤p ≤min;
3. ﬁnd two points a′, b′ such that p = P(a′ ≤X ≤a) and p = P(b ≤X ≤b′).
Bounds are enlarged by generating probabilities instead of random points
inside the cluster because in this way we can exploit the information about the
distribution of the data values in an interval.
Shrink. This operator applied to C returns C′ = a′ ≤X ≤b′ where a′ ≥a
and b′ ≤b. a′ and b′ are computed by randomly choosing p ≤P(a, b) such that
p = P(a ≤X ≤a′) = P(b′ ≤X ≤b), and a′ ≤b′.

A Method for Handling Numerical Attributes
901
Ground.
This operator, applied to C returns C′ = a′ ≤X ≤a′, with a′ in the
cluster containing a, b.
Shift. This operator, applied to C returns C′ = a′ ≤X ≤b′ where a′, b′ are
points in the cluster containing a, b such that P(a′ ≤X ≤b′) = P(a ≤X ≤b).
Change Cluster. This operator, applied to C = a ≤X ≤b returns C′ =
a′ ≤X ≤b′ where a′, b′ belong to a diﬀerent cluster. The new cluster is chosen
at random. Once the new cluster has been chosen, a pair a′, b′ with a′ ≤b′ is
randomly generated. In general, P(a′ ≤X ≤b′) is not equal to P(a ≤X ≤b).
3
ECL: A GA Based Inductive Concept Learner
In order to test the eﬀectiveness of our discretization method, we embed it in
the ILP system ECL. Like many ILP systems, ECL treats numerical attributes
as if they were nominal, therefore it can be used as a platform for testing and
comparing our local discretization method with the global discretization method
by Fayyad and Irani.
In ﬁgure 1 a scheme of ECL is given.
ALGORITHM ECL
Sel = positive examples
repeat
Select partial Background Knowledge
Population = ∅
while (not terminate) do
Adjust examples weights
Select n chromosomes using Sel
for each selected chromosome chrm
Mutate chrm
Optimize chrm
Insert chrm in Population
end for
end while
Store Population in Final Population
Sel = Sel - { positive examples
covered by clauses in Population }
until max iter is reached
Extract final theory from Final Population
Fig. 1. The overall learning algorithm ECL

902
F. Divina, M. Keijzer, and E. Marchiori
The system takes as input a background knowledge (BK), and a set of positive
and negative examples, and outputs a set of Horn clauses that covers many
positive examples and few negative ones.
Recall that a Horn clause is of the form p(X, Y ) : −r(X, Z), q(Y, a). with head
p(X, Y ) and body r(X, Z), q(Y, a). A clause has a declarative interpretation:
∀X, Y, Z(r(X, Z), q(X, a) →p(X, Y )) and a procedural one: in order to solve
p(X, Y ) solve r(X, Z) and q(Y, a). Thus a set of clauses forms a logic program,
which can directly (in a slightly diﬀerent syntax) be executed in the programming
language Prolog. The background knowledge used by ECL contains ground facts
(i.e. clauses of the form r(a, b) ←. with a, b constants). The training set contains
facts which are true (positive examples) and false (negative examples) for the
target predicate. A clause is said to cover an example if the theory formed by
the clause and the background knowledge logically entails the example.
In the repeat statement of the algorithm a Final population is iteratively
built from the empty one. Each iteration performs the following actions: part of
the background knowledge is randomly selected, an evolutionary algorithm that
uses that part of BK is run and the resulting set of Horn clauses is joined to the
actual Final population.
The evolutionary algorithm evolves a Population of Horn clauses starting
from an empty population, where an individual represents a clause, by the re-
peated application of selection, mutation (the system does not use any crossover
operator) and optimization in the following way.
At each generation n individuals are selected using a variant of the US se-
lection operator [10]. Roughly, the selection operator selects a positive example
and performs a roulette wheel on the set of individuals in the Population that
cover that example. If that example is not covered by any individual then a new
clause is created using that example as seed.
Each selected individual undergoes mutation and optimization.
Mutation consists of the application of one of the following four generaliza-
tion/specialization operators. A clause is generalized either by deleting an atom
from its body or by turning a constant into a variable, and it is specialized
by either adding an atom or turning a variable into a constant. Each operator
has a degree of greediness. In order to make a mutation, a number of mutation
possibilities is considered, and the one yielding the best improvement, in terms
of ﬁtness, is applied.
Optimization consists of the repeated application of one of the mutation
operators, until the ﬁtness of the individual increases, or a maximum number of
optimization steps has been reached.
Individuals are then inserted in the population. If the population is not full
then the individuals are simply inserted. If the population has reached its maxi-
mum size, then n tournaments are made among the individuals in the population
and the resulting n worst individuals are substituted by the new individuals.
The ﬁtness of an individual x is given by the inverse of its accuracy:
fitness(x) =
1
Acc(x) =
P +N
px+(N−nx)

A Method for Handling Numerical Attributes
903
In the above formula P and N are respectively the total number of positive and
negative examples, while px and nx are the number of positive and negative ex-
amples covered by the individual x. We take the inverse of the accuracy, because
ECL was originally designed to minimize a ﬁtness function.
When the Final population is large enough (after max iter iterations) a
subset of its clauses is extracted in such a way that it covers as many positive ex-
amples as possible, and as few negative ones as possible. To this aim an heuristic
algorithm for the weighted set covering is used.
3.1
Clu-Con: ECL Plus Local Discretization
We consider the following variant of ECL, called Clu-Con (Clustering and Con-
strain), which incorporates our method for handling numerical values.
When a new clause is built using a positive example as a seed, or when a
clause is specialized, atoms of the background knowledge are added to its body.
Each time an atom describing the value of a numerical attribute is introduced
in a clause, a constraint relative to that attribute is added to the clause as well.
For example, consider the following clause for example c23:
Cl = p(c23) : −q(c23, a), t(c23, y).
Suppose now that we would like to add the atom r(c23, 8) stating that in example
c23 attribute r has value 8. Then we obtain the clause
p(c23) : −q(c23, a), t(c23, y), r(c23, X), 8 ≤X ≤8.
The operators for handling constraints, introduced in section 2.2, are used as
mutation operators. When an operator is chosen then it is applied to a constraint
a number n choices of times, where n choices is a user supplied parameter. In
this way n choices new constraints are generated and the one yielding the best
ﬁtness improvement is selected.
Shrink and Ground. These two operators are applied when specializing a
clause. More precisely, when the system is specializing a clause by turning a
variable into a constant, if the selected variable occurs in a constraint then
either Shrink or Ground are applied to that constraint.
Enlarge. This operator is applied when the system decides to generalize a
clause. ECL has two generalization operators: delete an atom and constant
into variable operators. When delete an atom is selected and the atom
chosen for deletion describes the value of a numerical attribute, then both
the atom and the constraint relative to the described attribute are deleted.
If delete an atom is not selected and there are constraints in the body of
the clause chosen for mutation, then the system randomly selects between
the constant into variable and enlarge operators.

904
F. Divina, M. Keijzer, and E. Marchiori
Change Cluster and Shift. The standard operators and the above described
operators are applied inside a mutate procedure. Before calling this proce-
dure, a test is performed to check if the selected individual has got constraints
in the body of its clause. If this is the case, then either the change cluster
or the shift operator is applied with a probability pc (typical value 0.2),
otherwise the mutate procedure is called.
3.2
Ent MDL: ECL Plus Global Discretization
The other variant of ECL we consider, called Ent MDL (Entropy minimization
plus Minimum Description Length principle), incorporates the popular Fayyad
and Irani’s method for discretizing numerical values [9]. In [8,11] a study of some
discretization methods is conducted, and it emerged that Fayyad and Irani’s
method represents a good way for globally discretizing numerical attributes.
This supervised recursive algorithm uses the class information entropy of
candidate intervals to select the boundaries of the bins for discretization. Given
a set S of instances, an attribute p, and a partition bound t, the class information
entropy of the partition induced by t is given by
E(p, t, S) = Entropy(S1)|S1|
|S| + Entropy(S2)|S2|
|S|
where S1 is the set of instances whose values of p are in the ﬁrst half of the
partition and S2 the set of instances whose values of p are in the second half of the
partition. Moreover, |S| denotes the number of elements of S and Entropy(S) =
−p+log2(p+)−p−log2(p−) with p+ the proportion of positive examples in S and
p−is the proportion of negative examples in S.
For a given attribute p the boundary t∗which minimizes E(p, t, S) is selected
as a binary discretization boundary. The method is then applied recursively to
both the partitions induced by t∗until a stopping criterion is satisﬁed. The
Minimum Description Length principle is used to deﬁne the stopping criterion.
Recursive partitions within a set of instances stops if Entropy(S) −E(p, t, S) is
smaller than log2(N −1)/N + ∆(p, t, S)/N, where ∆(p, t, S) = log2(3k −2) −
[k · Entropy(S) −k1 · Entropy(S1) −k2 · Entropy(S2)], and ki is the number of
class labels represented in Si.
The method is used as a pre-processing step for ECL, where the domains
of numerical attributes are split into a number of intervals, and each interval is
considered as one value of a nominal attribute.
4
Experiments
In order to asses the goodness of the proposed method for handling numerical
attributes, we conduct experiments on benchmark datasets using ECL and the
two variants Clu Con and Ent MDL described previously. The characteristics
of the datasets are shown in table 1. Datasets 1 to 6 are taken from the UCI
repository [3], while dataset 7 originates from [4]. These datasets are chosen

A Method for Handling Numerical Attributes
905
Table 1. Features of the datasets. The ﬁrst column shows the total number of training
examples with, between brackets, the number of positive and negative examples. The
second and third columns show the number of numerical and nominal attributes. The
fourth column shows the number of elements of the background knowledge.
Dataset
Instances (+,-) Numerical Nominal
BK
1
Australian
690 (307,383)
6
8
9660
2
German
1000 (700,300)
24
0
24000
3
Glass2
163 (87,76)
9
0
1467
4
Heart
270 (120,150)
13
0
3510
5
Ionosphere
351 (225,126)
34
0
11934
6 Pima-Indians 768 (500,268)
8
0
6144
7 Mutagenesis
188 (125,63)
6
4
13125
Table 2. Parameter settings: pop size = maximum size of population, mut rate =
mutation rate, n = number of selected clauses, max gen = maximum number of GA
generations, max iter = maximum number of iterations, N(1,2,3,4) = parameters of
the genetic operators, p= probability of selecting a fact in the background knowledge,
l = maximum length of a clause.
Australian German Glass2
Heart
Ionosphere Pima-Indians Mutagenesis
pop size
50
200
150
50
50
60
50
mut rate
1
1
1
1
1
1
1
n
15
30
20
15
15
7
15
max gen
1
2
3
1
6
5
2
max iter
10
30
15
10
10
10
10
N(1,2,3,4)
(4,4,4,4)
(3,3,3,3) (2,8,2,9) (4,4,4,4)
(4,8,4,8)
(2,5,3,5)
(4,8,2,8)
nb
∞
200
8
20
20
8
8
p
0.4
0.3
0.8
1
0.2
0.2
0.8
l
6
9
5
6
5
4
3
because they contain mainly numerical attributes. The parameters used in the
experiments are shown in table 2 and are obtained after a number of preliminary
experiments. We use ten-fold cross validation. Each dataset is divided in ten
disjoint sets of similar size; one of these sets is used as test set, and the union of
the remaining nine forms the training set. Then ECL is run on the training set
and it outputs a logic program, whose performance on new examples is assessed
using the test set. Three runs with diﬀerent random seed are performed on each
dataset.
Table 3 contains the results of the experiments. The performance of the
GA learner improves when numerical attributes are discretized. In particular,
Clu-Con
seems to perform best on these datasets, being outperformed by
Ent-MDL only in two cases, namely on the Ionosphere and the Pima-Indians
datasets.

906
F. Divina, M. Keijzer, and E. Marchiori
Table 3. Results of experiments. Average accuracy, with standard deviation between
brackets. In the ﬁrst column the method adopting clusters and constraints described
in the paper is employed. In the second column numerical attributes are treated as
nominal. In the third column the Fayyad and Irani’s discretization algorithm was used
for globally discretizing numerical attributes.
Dataset
Clu-Con
ECL
Ent-MDL
1 Australian
0.79 (0.03)
0.71 (0.05)
0.62 (0.06)
2 German
0.73 (0.02)
0.63 (0.07)
0.59 (0.08)
3 Glass2
0.78 (0.07)
0.51 (0.06)
0.69 (0.11)
4 Heart
0.74 (0.09)
0.67 (0.11)
0.68 (0.10)
5 Ionosphere
0.81 (0.07)
0.41 (0.03)
0.85 (0.06)
6 Pima-Indians
0.65 (0.09)
0.61 (0.05)
0.69 (0.07)
7 Mutagenesis
0.90 (0.04)
0.83 (0.05)
0.85 (0.07)
5
Conclusions and Future Work
In this paper we have proposed an alternative method for dealing with numerical
attributes. The method uses standard density estimation to obtain a view of the
distribution of data, and this information is used to guide the genetic operators
to ﬁnd local discretizations that are optimal for classiﬁcation. By using an un-
supervised method for density estimation, the method can make use of either
unlabeled data or a priori knowledge on the density of the numeric attribute.
In section 4 we have tested the method in the system ECL on some datasets
containing numerical attributes. In a previous version of the system, no particu-
lar method for dealing with numerical attributes was implemented, so numerical
attributes were treated as if they were nominal. We have shown that not only
the proposed method improves the performance of the system, but also that
the proposed method is in general more eﬀective than a global discretization by
means of Fayyad and Irani’s algorithm. We believe that the proposed method
could be proﬁtably applied to other learning systems for dealing with numerical
attributes.
Currently, the density estimation procedure only works with single attributes.
However, when there is reason to believe that numeric attributes are covariant
(for instance weight and height of a person), the entire method can be converted
to use multiple attributes. For this, a multivariate variant of the expectation
maximization algorithm can be used which will ﬁnd the covariance matrix Σ in-
stead of a single standard deviation σ. Because our genetic operators are deﬁned
to use only density information derived from the clusters, the operators can be
easily adapted to mutate two or more numerical attributes at the same time in
the context of the globally estimated covariance matrices. This is however left
for future work.
We are currently investigating the possibility of changing the operators that
act on constraints. In particular we would like to generate the probabilities pi
in the enlarge and shrink operator not at random, but in a way that could take

A Method for Handling Numerical Attributes
907
into consideration the actual coverage of the constraint. We are also considering
the possibility of enlarging or shrinking a constraint asymmetrically.
References
1. J. Bacardit and J. M. Garrel, Evolution of adaptive discretization intervals for
a rule-based genetic learning system, in GECCO 2002: Proceedings of the Genetic
and Evolutionary Computation Conference, New York, 9–13 July 2002, Morgan
Kaufmann Publishers, p. 677.
2. —, Evolution of multi-adaptive discretization intervals for a rule-based genetic
learning system, in Proceedings of the 7th Iberoamerican Conference on Artiﬁ-
cial Intelligence (IBERAMIA2002), 2002, p. to appear.
3. C. Blake and C. Merz, UCI repository of machine learning databases, 1998.
4. A. Debnath, R. L. de Compadre, G. Debnath, A. Schusterman, and C.
Hansch, Structure-Activity Relationship of Mutagenic Aromatic and Heteroaro-
matic Nitro Compounds. Correlation with molecular orbital energies and hydropho-
bicity, Journal of Medical Chemistry, 34(2) (1991), pp. 786–797.
5. A. P. Dempster, N. M. Laird, and D. B. Rubin, Maximum likelihood from
incomplete data via the EM algorithm, Journal of the the Royal Statistical Society,
39 (1977), pp. 1–38.
6. F. Divina, M. Keijzer, and E. Marchiori, Non-universal suﬀrage selection
operators favor population diversity in genetic algorithms, in Benelearn 2002: Pro-
ceedings of the 12th Belgian-Dutch Conference on Machine Learning (Technical
report UU-CS-2002-046), Utrecht, The Netherlands, 4 December 2002, pp. 23–30.
7. F. Divina and E. Marchiori, Evolutionary concept learning, in GECCO 2002:
Proceedings of the Genetic and Evolutionary Computation Conference, New York,
9–13 July 2002, Morgan Kaufmann Publishers, pp. 343–350.
8. J. Dougherty, R. Kohavi, and M. Sahami, Supervised and unsupervised dis-
cretization of continuous features, in International Conference on Machine Learn-
ing, 1995, pp. 194–202.
9. U. Fayyad and K. Irani, Multi-interval discretization of continuos attributes as
pre-processing for classiﬁcation learning, in Proceedings of the 13th International
Join Conference on Artiﬁcial Intelligence, Morgan Kaufmann Publishers, 1993, pp.
1022–1027.
10. A. Giordana and F. Neri, Search-intensive concept induction, Evolutionary
Computation, 3 (1996), pp. 375–416.
11. R. Kohavi and M. Sahami, Error-based and entropy-based discretization of con-
tinuous features, in Proceedings of the Second International Conference on Knowl-
edge Discovery and Data Mining, 1996, pp. 114–119.
12. M. Kubat, I. Bratko, and R. Michalski, A review of Machine Learning Meth-
ods, in Machine Learning and Data Mining, R. Michalski, I. Bratko, and M. Kubat,
eds., John Wiley and Sons Ltd., Chichester, 1998.
13. W. Kwedlo and M. Kretowski, An evolutionary algorithm using multivariate
discretization for decision rule induction, in Principles of Data Mining and Knowl-
edge Discovery, 1999, pp. 392–397.
14. T. Mitchell, Generalization as search, Artiﬁcial Intelligence, 18 (1982), pp. 203–
226.
15. —, Machine Learning, Series in Computer Science, McGraw-Hill, 1997.

908
F. Divina, M. Keijzer, and E. Marchiori
16. W. Van Laer, From Propositional to First Order Logic in Machine Learning and
Data Mining – Induction of ﬁrst order rules with ICL, PhD thesis, Department of
Computer Science, K.U.Leuven, Leuven, Belgium, June 2002. 239+xviii pages.
17. I. H. Witten and E. Frank, Weka machine learning algorithms in java, in Data
Mining: Practical Machine Learning Tools and Techniques with Java Implementa-
tions, Morgan Kaufmann Publishers, 2000, pp. 265–320.

Analysis of the (1+1) EA for a Dynamically
Bitwise Changing OneMax
Stefan Droste⋆
LS Informatik 2, Universit¨at Dortmund, 44221 Dortmund, Germany
stefan.droste@udo.edu
Abstract. Although evolutionary algorithms (EAs) are often success-
fully used for the optimization of dynamically changing objective func-
tion, there are only very few theoretical results for EAs in this sce-
nario. In this paper we analyze the (1+1) EA for a dynamically chang-
ing OneMax, whose target bit string changes bitwise, i. e. possibly by
more than one bit in a step. We compute the movement rate of the
target bit string resulting in a polynomial expected ﬁrst hitting time of
the (1+1) EA asymptotically exactly. This strengthens a previous result,
where the dynamically changing OneMax changed only at most one bit
at a time.
1
Introduction
Evolutionary Algorithms (EAs) optimize objective functions heuristically by fol-
lowing principles of natural evolution (see [BFM97] for an in-depth coverage).
A great variety of EAs is successfully used in practice and the experimental
knowledge about EAs is immense. In comparison, theory of EAs is still in its
beginnings. Many results are based on empirical observations, precluding gen-
eralizations. Other results, like the basic forms of the schema theorem ([Hol75])
are limited to the short-time behaviour of EAs. Such results cannot be carried
over to other dimensions of the search space, even if the objective function is
ﬁxed. This can only be achieved by a rigorous mathematical analysis showing
how the ﬁrst hitting time of the EA (or any other measure of interest) depends
on the dimension of the search space.
As there are no theoretical foundations, the EAs and objective functions rig-
orously analyzed so far are simple in comparison to those used in practice. One
of the most thoroughly examined EAs is the (1+1) EA which was analyzed by
many diﬀerent researchers (e. g., see [M¨uh92], [Rud97], or [DJW02]). The tech-
niques developed while analyzing this simple-structured EA have led to analyses
of more complex EAs using crossover and populations (see [JW99] or [JW01]),
making it a good starting point for the analysis of dynamic EAs.
So far theoretical analyses of EAs almost completely focus on static optimiza-
tion problems, i. e. objective functions that do not change over time. Although
⋆This research was partly supported by the Deutsche Forschungsgemeinschaft as part
of the Collaborative Research Center “Computational Intelligence” (531).
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 909–921, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

910
S. Droste
diﬀerent kinds of EAs are successfully used for a broad range of dynamical op-
timization problems (see [Bra01] for an overview), there are only very few theo-
retical results. Most investigations are limited to experimental work or heuristic
approximations. A ﬁrst more theoretical approach is [SD99], where the transi-
tion probabilities of a (1+1) EA for a dynamic variant of the OneMax-problem
are numerically evaluated and compared to empirical data, but not analyzed
mathematically.
In this paper, we strengthen the ﬁrst rigorous mathematical analysis of an EA
for a dynamic optimization problem presented in [Dro02]. There the standard
(1+1) EA with a mutation probability of 1/n is analyzed on a dynamic variant of
OneMax, where in every step with probability p′ exactly one uniformly chosen
bit of the target bit string changes. Hence, in contrast to the static case, the
Hamming distance of the current search point to the target bit string can increase
by one. It is shown in [Dro02] that the expected ﬁrst hitting time of the (1+1) EA
is polynomial if and only if p′ is asymptotically of growth rate at most log(n)/n,
i. e. O(log(n)/n) (see [MR95] for the notation of growth rates).
Here, we look at the more interesting case of changing the target bit string by
the same type of random process as the current search point of the (1+1) EA, i. e.
each bit is ﬂipped with probability p′. We prove that the expected ﬁrst hitting
time of the (1+1) EA is with high probability super-polynomial if p′ grows faster
than log(n)/n2, i. e. p′ = ω(log(n)/n2). In the light of [Dro02] the result may not
be surprising, but its analysis is signiﬁcantly more complicated, because we have
to consider a process where the Hamming distance between the current search
point and the target bit string can increase by more than one in one step. By
showing how such a process can be upper bounded by a random process where
the distance increases by at most one in one step we prove the desired result.
This bounding technique (although it should not be new, we have not found it
in the literature) may be applicable in other settings and therefore be of more
general interest.
In the next section we deﬁne the (1+1) EA for dynamic objective functions,
the dynamic variant of OneMax, and the ﬁrst hitting time, our measure of
eﬃciency. In Sect. 3 we show that for p′ = ω(log(n)/n2) the ﬁrst hitting time
of the (1+1) EA is polynomial only with super-polynomially small probability,
implying a super-polynomial expected ﬁrst hitting time. In Sect. 4 we show how
stochastic processes that can move away from the target in one step by more
than one can be related to processes where this diﬀerence can only increase by
one in one step. Using this result we show that the expected ﬁrst hitting time for
p′ = O(log(n)/n2) is polynomial in n. The paper ends with some conclusions.
2
The (1+1) EA and a Dynamic OneMax
The (1+1) EA is probably the most simple EA for maximizing an objective func-
tion f : {0, 1}n →R as it uses only one individual being eﬀected by mutation
only. The commonly used mutation operator for bit strings is the bitwise mu-
tation, i. e. each bit is ﬂipped independently with probability p ∈[0, 1]. In most

Analysis of the (1+1) EA for a Dynamically Bitwise Changing OneMax
911
cases p is 1/n. Selection chooses between the old bit string and the mutant, where
the mutant is only chosen if its f-value is at least as high as that of the old bit
string. This mutation-selection scheme is repeated until a stopping criterion is
fulﬁlled. Since we are interested in the number of steps until an optimum of f is
evaluated for the ﬁrst time, we omit this stopping criterion.
If f is not static but changes over time, we model it by a family (ft : {0, 1}n →
R)t∈N0 of functions, where ft is the current objective function after t steps of
the (1+1) EA. In this situation the (1+1) EA should evaluate even the old bit
string after each step, as its ﬁtness might have changed since the last evaluation.
So, the (1+1) EA for dynamic optimization problems is deﬁned as follows:
Deﬁnition 1 ((1+1) EA for dynamic optimization problems).
1. Set t := 0 and choose xt ∈{0, 1}n randomly uniformly.
2. Set x′ := xt and flip each bit of x′ with probability 1/n.
3. If ft(x′) ≥ft(xt), set xt+1 := x′, else xt+1 := xt.
4. Set t := t + 1 and go to step 2.
There are a number of diﬀerent measures for the performance of an algorithm
for dynamic optimization, e. g. Hamming distance to the nearest optimum. Here
we concentrate on the number Tf of steps until the (1+1) EA for the ﬁrst time
evaluates a point x′ where this distance is zero:
Deﬁnition 2 (First hitting time of the (1+1) EA).
Let f = (ft : {0, 1}n →R)t∈N0 be a dynamically changing objective function. The
ﬁrst hitting time Tf of the (1+1) EA on f is deﬁned as
Tf := min{t ∈N0 | ft(xt) = max{ft(x) | x ∈{0, 1}n}}.
One of the ﬁrst functions the (1+1) EA has been analyzed for in the static
case is OneMax. Here the expected ﬁrst hitting time is O(n log(n)) (see [M¨uh92]
for an approximate analysis and [DJW98] for a rigorous one). OneMax simply
counts the number of ones in its argument x ∈{0, 1}n. This can also be in-
terpreted as the number of matching bits with the target bit string (1, . . . , 1).
Hence, maximizing OneMax is equivalent to minimizing the Hamming distance
to this target bit string. Using this interpretation, a dynamic OneMax-variant
where the target bit string is changed by a random process is obvious. Let
M : {0, 1}n →{0, 1}n represent this random process (formally it is a function
M : {0, 1}n × Ω→{0, 1}n, where Ωtogether with a function P : P(Ω) →[0, 1]
forms a probability space, but we omit these technicalities) and H(x, y) denote
the Hamming distance between bit strings x and y:
Deﬁnition 3 (OneMaxt,M).
The family (OneMaxt,M : {0, 1}n →N0)t∈N0 of functions is deﬁned as
OneMaxt,M(x) := n −H(x, yt),
where y0 := (1, . . . , 1) and yt+1 := M(yt) for t ≥0.

912
S. Droste
(As the initial individual x0 is chosen uniformly at random, initializing y0
with any other bit string makes no diﬀerence.) In [Dro02] the random operator
M1 ﬂips exactly one uniformly chosen bit with probability p′ and it is shown
that the expected ﬁrst hitting time of the (1+1) EA is polynomial if and only
if p′ = O(log(n)/n). Here we look at a more natural random operator Mn by
allowing Mn to ﬂip each bit independently with probability p′:
Deﬁnition 4 (Bitwise operator Mn).
Let p′ ∈[0, 1]. Then the random operator Mn : {0, 1}n →{0, 1}n is deﬁned for
all y, y′ ∈{0, 1}n by:
P(Mn(y) = y′) := (p′)i · (1 −p′)n−i if H(y, y′) = i.
The parameter p′ is called the movement rate of the target bit string as it
inﬂuences the number of steps in which the target moves and also the number
of bits that ﬂip. In the following we will denote (OneMaxt,Mn)t∈N0 shortly
by OneMaxD to indicate the dynamic variant of OneMax. To separate more
easily between the changes made by the mutation of the (1+1) EA and those
of the random operator Mn, we call bits ﬂipped by mutation mutating bits and
bits changed by Mn moving bits. In the next section we show that for p′ =
ω(log(n)/n2) the (1+1) EA has a super-polynomial expected ﬁrst hitting time.
3
A Super-Polynomial Lower Bound for Large Movement
Rates
In this section we assume that p′ grows asymptotically faster than log(n)/n2,
i. e. p′ = ω(log(n)/n2). We show that the probability of the (1+1) EA reaching
the optimum of OneMaxD in polynomially many steps is super-polynomially
small, i. e. at most 1/β(n), where β(n) grows faster than any polynomial in n.
First, we consider p′ = Ω(1/n1−ε) for a constant ε > 0. Then the probability
of any speciﬁc movement of the target bit string is super-polynomially small,
implying that in the last step before reaching the optimum of OneMaxD a super-
polynomially unlikely movement of the target bit string has to happen (here we
assume that p′ ≤1/2 as otherwise more than half of the bits of the target would
move on average in each step, making eﬃcient optimization impossible):
Theorem 1. Let p′ = Ω(1/n1−ε) for a constant ε ∈]0, 1[. Then the ﬁrst hitting
time of the (1+1) EA on OneMaxD is exp(o(nε)) only with exponentially small
probability.
Proof. We look at an arbitrary run of the (1+1) EA that reaches the optimum
after exactly T > 0 steps. Then it is necessary that in the last step the movement
of the target bit string from yT −1 to yT leads to yT = x′. For H(yT −1, x′) = i,
the probability of such a movement equals (p′)i · (1 −p′)n−i.
If i grows linearly in n, the factor (p′)i is exp(−O(n)). If n −i grows linearly
in n, the second factor is asymptotically at most (where n −i = β(n) = Θ(n))

1 −
1
n1−ε
β(n)
=

1 −nε
n
(n/nε)·(nε/n)·β(n)
,

Analysis of the (1+1) EA for a Dynamically Bitwise Changing OneMax
913
which is upper bounded by exp(−c · nε) for a constant c > 0. Since i or n −i
grows linearly and the probability of initializing with x0 ̸= y0 is exponentially
close to 1, we know that in order to reach the optimum an event with probability
at most exp(−c · nε) has to happen.
⊓⊔
We have to argue more carefully if p′ is not that large but still ω(log(n)/n2).
Although the proof follows the lines of the super-polynomial lower bound in
[Dro02], it must be adjusted to take into account that the target bit string can
move by more than one bit at a time step. First, we show the following lemma:
Lemma 1. Let a Bernoulli experiment with success probability p ∈]0, 1] be re-
peated independently a + b-times (a, b ∈N). The probability that in the ﬁrst a
experiments the number of successes is larger than the number of successes in
the last b experiments is at most a/b.
Proof. Let the random variables A resp. B denote the number of successes in
the ﬁrst a resp. last b experiments and deﬁne Y := A/(B + 1). Then E(Y ) =
pa/(pb+1) ≤a/b. If and only if the number of successes in the ﬁrst a experiments
is larger than in the last b experiments Y is at least 1. Using the Markov bound
([MR95]) we get P(Y ≥1) ≤E(Y ) ≤a/b.
⊓⊔
Theorem 2. Let p′ = ω(log(n)/n2). Then the ﬁrst hitting time of the (1+1) EA
on OneMaxD is polynomial only with super-polynomially small probability.
Proof. Because of Theorem 1 we concentrate on values of p′ with p′ ≤1/n6/7.
Let α(n) = min

n/log(n), p′ · n2/log(n)

be the minimum of n/log(n) and the
factor by which p′ grows faster than log(n)/n2. Then limn→∞α(n) = ∞.
Our ﬁrst assumption on the (1+1) EA is that the initial bit string contains
at most n −2G matching bits, where G grows slightly faster than log(n), i. e.
G := log(n) · α(n)4/7. As G = o(n), Chernoﬀbounds ([MR95]) guarantee that
initializing with a bit string with at most n−2G matching bits has a probability
super-polynomially close to one, i. e. this event is almost sure. We show that the
(1+1) EA reaches the optimum in polynomially many steps only via a search
point whose number of matching bits is between n−2G and n−G almost surely.
Since the mutation rate of the (1+1) EA is 1/n, almost surely at most log(n)
matching bits are mutated in one step. If the movement rate p′ of the target
bit string is O(1/n), the number of moving bits of the target bit string in one
step is also at most log(n) almost surely. Hence, in one step the OneMaxD-
value of the current search point changes by at most 2 log(n) almost surely. As
G is growing asymptotically faster than log(n), it is at least 2 log(n) for n large
enough. Hence, any run of the (1+1) EA with polynomial many steps starting
with at most n −2G matching bits reaches the optimum only via a search point
with at least n −2G and at most n −G matching bits almost surely.
If p′ = ω(1/n), we have α(n) = n/log(n). As p′ is O(1/n6/7) by assumption,
Chernoﬀbounds yield that the number of moving bits in one step is at most
cn1/7 for a constant c > 1 almost surely. Since G is n4/7 · log(n)3/7 in this case,
i. e. asymptotically larger than the number of moving or mutating bits in one

914
S. Droste
step, we again know that the (1+1) EA reaches the optimum in polynomial time
only via a search point with at least n−2G and at most n−G bits almost surely.
Let It denote the current state of the (1+1) EA, i. e. the number of matching
bits of xt. The preceding argumentation shows that if state n is reached in
polynomially many steps, almost surely two points in time t1 < t2 exist, where
It1 ∈{n−2G, . . . , n−G}, It ∈{n−G+1, . . . , n−1} for all t ∈{t1+1, . . . , t2−1},
and It2 = n. In the following we show that for every t = t2 −t1 ∈N it is super-
polynomially unlikely that such a sequence It1, . . . , It2 exists (a similar technique
is used in [RRS95]).
We exclude the possible, but very unlikely event that the (1+1) EA bridges
the gap, i. e. comes from n −G to n ones, using only few but far reaching steps.
Therefore, we show that in one step at most L := α(n)1/7 non-matching bits
ﬂip almost surely. The probability of ﬂipping at least L + 1 of the at most 2G
non-matching bits during one step by mutation and/or movement of the target
bit string is at most
 2G
L + 1

·
 1
n + p′
L+1
≤
 4G
n6/7
L+1
,
which is super-polynomially small as G ≤n4/7 log(n)3/7 and L grows with n to
inﬁnity. Hence, we can rule out mutations increasing the number of ones by more
than L. The error introduced by this assumption has only super-polynomially
small probability if we concentrate on polynomially many steps.
Now, we upper resp. lower bound the probability p+
i resp. p−
i , that the num-
ber of matching bits increases resp. decreases during one step when the current
number of matching bits is i. The number of matching bits decreases if one
matching bit moves, but no non-matching bit moves and no bit mutates. As-
suming that p′ ≤1/n we can lower bound p−
i by (where c > 0 is a constant):

1 −(1 −p′)i
· (1 −p′)n−i ·

1 −1
n
n
≥i · p′ · c ≥c
2 · α(n) · log(n)
n
.
If p′ is larger than 1/n, there is a constant probability that at least one of the
i matching bits, but none of the n −i non-matching bits moves, while no bit
mutates. Hence, the inequality p−
i ≥(c/2) · α(n) · (log(n)/n) is still valid for an
appropriate constant c > 0, because α(n) = n/ log(n) for all p′ ≥1/n.
For the upper bound p+
i
we notice that a step increasing the number of
matching bits implies that the mutation resp. the movement of the target bit
string ﬂips more non-matching bits than matching bits after the movement resp.
mutation has occurred. According to Lemma 1 both events have probability at
most (n−i+L)/(i−L) because at most L matching bits change during movement
resp. mutation by assumption. Consequently, p+
i ≤2(n −i + L)/(i −L). Since
i is at least n −2G = n −2 log(n)α(n)4/7 and L = o(G), we know that for all
n ≥n0 for a constant n0 (i. e. n is large enough):
p+
i ≤2 ·
2G + L
n −2G −L ≤2 ·
3G
n −3G ≤12 · α(n)4/7 log(n)
n
.

Analysis of the (1+1) EA for a Dynamically Bitwise Changing OneMax
915
Since it is suﬃcient for us that decreasing the number of matching bits is by
a factor α(n)2/7 more likely than increasing it, we lower resp. upper bound p−
i
resp. p+
i for n large enough by
˜p−
i := c
2 · α(n) log(n)
n
resp. ˜p+
i := c
2 · α(n)5/7 log(n)
n
.
Hence, the assumption that the probabilities of steps decreasing resp. in-
creasing the number of matching bits are exactly ˜p−
i
resp. ˜p+
i
and that every
improvement leads us L bits closer to the target while every deterioration only
changes one bit, results in a “faster process” (a proof would follow the line of
Lemma 2 in the next section). Then the probability that the next mutation
changing the number of matching bits (called eﬀective mutation) increases the
number of matching bits equals
c
2 · α(n)5/7 log(n)
n

α(n) + α(n)5/7
· c
2 · log(n)
n
=
α(n)5/7
α(n) + α(n)5/7 ≤α(n)−2/7.
Consequently, the expected number of increasing mutations during t eﬀective
mutations is at most t/α(n)2/7 and we can follow the argumentation presented in
[Dro02]. Again, by assuming that the expected number of increasing mutations
is exactly t/α(n)2/7 the process can only become faster. To obtain a superpoly-
nomially small upper bound by Chernoﬀbounds we show that
1. the expected number of increasing mutations grows signiﬁcantly faster than
log(n), i. e. t/α(n)2/7 = ω(log(n)) and
2. the number of increasing mutations necessary to reach the optimum is by a
constant factor larger than the expected number of increasing mutations.
To prove these claims we argue that t, the length of the sequence It1, . . . , It2,
is at least G/L = log(n)α(n)3/7, because every increasing mutation raises the
number of matching bits by at most L = α(n)1/7. Hence, t/α(n)2/7 = ω(log(n)),
i. e. only sequences of some minimum length are candidates for It1, . . . , It2. The
necessary number of increasing mutations can be lower bounded as follows: if
t+ denotes the number of increasing and t−the number of decreasing mutations
of t = t+ + t−eﬀective mutations, the number of matching bits increases by
t+ · L −t−. In order to reach the global optimum this value has to be at least
G. This is equivalent to
t+ · L −t−≥G ⇐⇒t+ ≥G + t −t+
L
⇐⇒t+ ≥G + t
L + 1.
For n large enough we have
G + t
L + 1 = log(n)α(n)4/7 + t
α(n)1/7 + 1
≥2 ·
t
α(n)2/7 .
In other words, to reach the global optimum in t steps the number of increasing
mutations necessary is at least by a factor 2 larger than the expected number

916
S. Droste
t/α(n)2/7 of increasing mutations. Hence, using Chernoﬀbounds the probability
of a sequence It1, . . . , It2 of length t with the desired properties is bounded above
by exp

−
t
α(n)2/7 · 4
3

. Because t is at least α(n)3/7 log(n), this is upper bounded
by exp

−Ω(α(n)1/7 log(n))

= n−Ω(α(n)1/7).
All in all, we have shown that the (1+1) EA ﬁnds the optimum of OneMaxD
with p′ = ω(log(n)/n2) in polynomially many steps only, if a super-polynomially
unlikely event happens.
⊓⊔
4
A Polynomial Upper Bound for Small Movement Rates
We now show that the expected ﬁrst hitting time of the (1+1) EA for OneMax
is polynomial for p′ = O(log(n)/n2). While doing this we have to analyze a
process where the distance from the target (the number of non-matching bits)
can increase in one step by more than one. The crucial idea is to show how this
process can be replaced by a slower one (i. e. how to upper bound the process),
where the distance from the target in one step increases by at most one.
In the following we describe the (1+1) EA and other processes by their tran-
sition probabilities pi,j (i, j ∈{0, . . . , n}) of changing from a point with i match-
ing bits in one step to a point with j matching bits. As all considered processes
are Markovian, i. e. time-independent, and operating on {0, . . . , n} the transi-
tion probabilities describe them completely. Hence, we denote the process with
transition probabilities pi,j shortly by (p·,·). The random variable Ti,j denotes
the ﬁrst hitting time to come from state i to state j. The names are sometimes
adapted suitably, e. g. ˜Ti,j denotes the ﬁrst hitting time of the process (˜p·,·).
The outline of the proof is as follows: ﬁrstly, we “cut oﬀ” all transitions from
states i to states j ≥i + 2 (we will call processes with pi,j = 0 for all i and
j ≥i + 2 brieﬂy ≤1-processes in the following). This ≤1-process is replaced
by one with a stronger tendency towards lower states. Finally, we replace this
process by one changing its state from i only to i −1, i, or i + 1 and analyze it
by standard methods.
To show that all these steps lead to processes having at least the same ex-
pected ﬁrst hitting time, we need a number of lemmata:
Lemma 2. Let (p·,·) be a process. For the ≤1-process (˜p·,·) deﬁned by
˜pi,j :=
	
pi,j
if j < i or j = i + 1,
pi,i + 
n
j=i+2 pi,j if j = i,
we have for all i ∈{0, . . . , n−1}, k ≥i and all t ∈N0: P( ˜Ti,n ≥t) ≥P(Tk,n ≥t).
Proof. By induction over t ∈N0: for t = 0 the statement is trivial. For the step
from t to t + 1, we lower bound P( ˜Ti,n ≥t + 1) according to two cases:
1.
k > i :
i+1

j=0
˜pi,j · P( ˜Tj,n ≥t) ≥
i+1

j=0
˜pi,j · P(Tk,n ≥t) ≥P(Tk,n ≥t + 1).

Analysis of the (1+1) EA for a Dynamically Bitwise Changing OneMax
917
2.
k = i :
i−1

j=0
˜pi,j · P( ˜Tj,n ≥t) + ˜pi,i · P( ˜Ti,n ≥t) + ˜pi,i+1 · P( ˜Ti+1,n ≥t)
≥
i−1

j=0
pi,j · P(Tj,n ≥t) + pi,i · P(Ti,n ≥t) +
n

j=i+2
pi,j · P(Tj,n ≥t)
+pi,i+1 · P(Ti+1,n ≥t) = P(Ti,n ≥t + 1).
2
Since E(Ti,n) = 
∞
t=1 t · P(Ti,n = t) = 
∞
t=1 P(Ti,n ≥t), Lemma 2 tells
us that by “cutting oﬀ” all improvements by more than one, the expected ﬁrst
hitting time does not decrease. A ≤1-process has two useful properties (for the
sake of brevity we omit the easy proofs by induction):
Lemma 3. Let (p·,·) be a ≤1-process. Then we have for all i ∈{0, . . . , n} and
t ∈N0: P(Ti,n ≥t) ≥P(Ti+1,n ≥t) (implying E(Ti,n) ≥E(Ti+1,n)).
Lemma 4. Let (p·,·) and (˜p·,·) be ≤1-processes. If for all i ∈{0, . . . , n −1} and
all j < i we have ˜pi,j ≥pi,j and ˜pi,i+1 ≤pi,i+1 then P( ˜Ti,n ≥t) ≥P(Ti,n ≥t)
(implying E( ˜Ti,n) ≥E(Ti,n)) for all i ∈{0, . . . , n} and t ∈N0.
In general the expected ﬁrst hitting time of a ≤1-process is diﬃcult to upper
bound as it might jump to lower states with high probability. But if this is not
too likely we can ﬁnd a Markov process jumping only from i to i −1, i, or i + 1
(called a {−1, 0, 1}-process) having at least the same expected ﬁrst hitting time.
The key idea is to interpret a number of steps of a given {−1, 0, 1}-process as a
single step of a ≤1-process. If we do not “forget” any transitions of the {−1, 0, 1}-
process we get a ≤1-process that lower bounds the original {−1, 0, 1}-process:
Lemma 5. Let (p·,·) resp. (˜p·,·) be a ≤1- resp. a {−1, 0, 1}-process where
1. ∀i ∈{1, . . . , n −1} : pi,0 = i
k=1 ˜pk,k−1,
2. ∀i ∈{1, . . . , n −1} : ∀j ∈{1, . . . , i −1} : pi,j = ˜pj,j
i
k=j+1 ˜pk,k−1,
3. ∀i ∈{0, . . . , n −1} : pi,i+1 = ˜pi,i+1.
Then for all i ∈{0, . . . , n −1}: E(Ti,i+1) ≤E( ˜Ti,i+1).
Proof. Assume that the current state of the process (˜p·,·) is i. Consider the next
at most i transitions of (˜p·,·). According to the following cases we split a preﬁx
from this sequence and consider it as a one step of the process (p·,·):
– All i transitions decrease the number of the current state by exactly one. This
event has probability i
k=1 ˜pk,k−1 and afterwards the process is in state 0.
– The ﬁrst i −j ∈{1, . . . , i −1} transitions decrease the number of the cur-
rent state by exactly one, while the (i −j + 1)-th transition stays in the
current state. This event has probability ˜pj,j
i
k=j+1 ˜pk,k−1 and afterwards
the process is in state j.
– The ﬁrst transition leads from state i to i + 1. This has probability ˜pi,i+1
and afterwards the process is in state i + 1.

918
S. Droste
– In all other cases, i. e. when the considered i transitions start with at least
one but at most i−1 decreasing transitions directly followed by an increasing
transition, the process is in state at most i. By assuming that the process is
exactly in state i, the expected ﬁrst hitting time does not increase according
to Lemma 3, because the described process is a ≤1-process.
These cases show how the {−1, 0, 1}-process (˜p·,·) can be considered as a ≤1-
process (p·,·) with exactly the transition probabilities given above. By interpret-
ing the considered transition(s) as a single transition (observe that their number
varies according to the respective case), we get the claimed lower bound.
⊓⊔
Lemma 5 can hardly be applied directly as it only correlates processes where
the transition probabilities fulﬁll some equalities. Using Lemma 4 we can weaken
the conditions of Lemma 5 to inequalities:
Corollary 1. Let (p·,·) resp. (˜p·,·) be a ≤1-process resp. a {−1, 0, 1}-process on
{0, . . . , n}. If the following conditions hold
1. ∀i ∈{1, . . . , n −1} : pi,0 ≤i
k=1 ˜pk,k−1,
2. ∀i ∈{1, . . . , n −1} : ∀j ∈{1, . . . , i −1} : pi,j ≤˜pj,j
i
k=j+1 ˜pk,k−1,
3. ∀i ∈{0, . . . , n −1} : pi,i+1 ≥˜pi,i+1,
then E(Ti,n) ≤E( ˜Ti,n) for all i ∈{0, . . . , n}.
Now we can upper bound the expected ﬁrst hitting time of the (1+1) EA by
repeatedly replacing the original process by simpler ones and using the preceding
results to show that the new process is not faster than the former one. As we are
interested in asymptotical results, the processes may be deﬁned in a way that
only makes sense for n large enough (e. g. some probabilities may be negative
for small n). At the end we come up with a {−1, 0, 1}-process we can analyze
by standard methods.
Theorem 3. The expected ﬁrst hitting time of the (1+1) EA on OneMaxD
with p′ = O(log(n)/n2) is polynomial.
Proof.
1. Let (p1
·,·) denote the random process exactly describing the (1+1) EA
on OneMaxD. According to Lemma 2, for the ≤1-process deﬁned by
p2
i,j :=

p1
i,j
if j < i or j = i + 1,
p1
i,i + 
n
j=i+2 p1
i,j if j = i,
E(T 2
i,n) ≥E(T 1
i,n) for all i ∈{0, . . . , n}. Hence, from now on we consider the
simpler process (p2
·,·) instead of (p1
·,·).
2. The ≤1-process (p2
·,·) is replaced by the ≤1-process (p3
·,·), where for all i ∈
{0, . . . , n −1} and k ∈{1, . . . , i} (remember that p′ = O(log(n)/n2)):
p3
i,i−k :=
i
k

· (p′)k and p3
i,i+1 := n −i
n
·
2
3 exp(1)

Analysis of the (1+1) EA for a Dynamically Bitwise Changing OneMax
919
(p3
i,i is implicitly deﬁned as the “remaining probability” to stay in state i).
According to Lemma 4, this process (p3
·,·) does not have smaller expected
ﬁrst hitting times than the process (p2
·,·), because for all i ∈{0, . . . , n −1}
and k ∈{1, . . . , i}
p2
i,i−k = p1
i,i−k ≤
i
k

· (p′)k = p3
i,i−k and for n large enough
p2
i,i+1 = p1
i,i+1 ≥
n −i
1

· 1
n ·

1 −1
n
n−1
· (1 −p′)n ≥p3
i,i+1
as p′ = O(log(n)/n2). Hence, from now on we consider the process (p3
·,·).
3. The {−1, 0, 1}-process (p4
·,·) is deﬁned by (for all i ∈{0, . . . , n −1})
p4
i,i−1 := 2 · i · p′ and p4
i,i+1 := n −i
n
·
1
2 exp(1).
(Note that p4
i,i is only implicitly deﬁned as 1 −p4
i,i−1 −p4
i,i+1, which is at
least 1/2 for n large enough, since p′ = O(log(n)/n2).) Corollary 1 yields
that (p4
·,·) upper bounds (p3
·,·): Condition 1 of Corollary 1 is fulﬁlled, since
p3
i,0 = (p′)i ≤i
k=1 2kp′ = i
k=1 p4
k,k−1 for i ∈{1, . . . , n −1}.
Condition 2 holds because for all i ∈{1, . . . , n −1} and j ∈{1, . . . , i −1}:
p3
i,j ≤p4
j,j
i
k=j+1
p4
k,k−1 ⇐⇒

i
i −j

(p′)i−j ≤p4
j,j
i
k=j+1
2kp′,
which is valid, since p4
j,j ≥1/2 for n large enough. Condition 3 follows
directly by p3
i,i+1 > p4
i,i+1 for all i < n. Now the {−1, 0, 1}-process (p4
·,·) can
be analyzed with standard methods. It is well-known (see e. g. [DJW00])
that
E(T 4
i,i+1) =
i

k=0
1
p4
k,k+1
·
i
l=k+1
p4
l,l−1
p4
l,l+1
.
Filling in our values for p4
i,j, we get the following upper bound on E(T 4
i,n)
for p′ ≤c · log(n)/n2 (see [Dro02] for a similar calculation):
i

k=0
2 exp(1) · n
n −k
·
i
l=k+1
2 · l · c · log(n)
n2
· 2 exp(1) · n
n −l
=
i

k=0
2 exp(1) · n
n −k
·
4c exp(1) log(n)
n
i−k
· i!
k! · (n −i −1)!
(n −k −1)!
≤2 exp(1) · n
n −i
·
i

k=0
4c exp(1) log(n)
n
i−k
·
i
k
 n −k −1
n −i −1

≤2 exp(1) ·
n
n −i ·

1 + 4c exp(1) log(n)
n
i

920
S. Droste
Using the estimation (1 + 1/x)x ≤exp(1), E(T 4
i,i+1) is bounded above by:
2 exp(1) ·
n
n −i · exp
4c log(n) exp(1)
n
· i

≤2 exp(1) ·
n
n −i · n
4c exp(1)
ln(2) .
Hence, by linearity of expectation and pessimistically assuming that we ini-
tialize in (0, . . . , 0),
E(TOneMaxD) ≤2 exp(1) · n
4c exp(1)
ln(2)
+1 ·
n

i=1
1
i = O

n4c· exp(1)
ln(2) +1 · log(n)

.
2
5
Conclusions
We have analyzed the (1+1) EA on a dynamic variant of OneMax where every
bit of the target ﬂips with probability p′. The critical growth rate of p′ where the
expected ﬁrst hitting time of the (1+1) EA changes from polynomial to super-
polynomial was shown to be Θ(log(n)/n2). Besides this result, the techniques
developed may be of more general interest. They show how a Markov process
which can move away from the target by more than one in one step can be
replaced by a slower Markov process that can move away from the target by at
most one in one step (Lemma 5 resp. Corollary 1). This method might help in
the analysis of other EAs or randomized processes in general.
Acknowledgements. I thank Jens J¨agersk¨upper, Ingo Wegener, and Carsten
Witt for their valuable advice and help while preparing this paper.
References
[BFM97]
Th. B¨ack, D.B. Fogel, and Z. Michalewicz, editors. Handbook of Evolutionary
Computation. Institute of Physics Publishing, 1997.
[Bra01]
J. Branke. Evolutionary Optimization in Dynamic Environments. Kluwer
Academic, 2001.
[DJW98]
S. Droste, Th. Jansen, and I. Wegener. A rigorous complexity analysis of
the (1 + 1) EA for linear functions with Boolean inputs. In Proceedings of
ICEC 1998, pages 499–504, 1998.
[DJW00]
S. Droste, Th. Jansen, and I. Wegener. Dynamic parameter control in simple
evolutionary algorithms.
In Proceedings of FOGA 2000, pages 275–294,
2000.
[DJW02]
S. Droste, Th. Jansen, and I. Wegener. On the analysis of the (1 + 1) EA.
Theoretical Computer Science, (276):51–81, 2002.
[Dro02]
S. Droste. Analysis of the (1+1) EA for a dynamically changing OneMax-
variant. In Proceedings of CEC 2002, pages 55–60, 2002.
[Hol75]
J.H. Holland. Adaptation in Natural and Artiﬁcial Systems. University of
Michigan Press, 1975.

Analysis of the (1+1) EA for a Dynamically Bitwise Changing OneMax
921
[JW99]
Th. Jansen and I. Wegener. On the analysis of evolutionary algorithms –
a proof that crossover really can help. In Proceedings of ESA 1999, pages
184–193, 1999. LNCS 1643.
[JW01]
Th. Jansen and I. Wegener. Real royal road functions – where crossover is
provably essential. In Proceedings of GECCO 2001, pages 1034–1041, 2001.
[MR95]
R. Motwani and P. Raghavan. Randomized Algorithms. Cambridge Univer-
sity Press, 1995.
[M¨uh92]
H. M¨uhlenbein. How genetic algorithms really work: I. mutation and hill-
climbing. In Proceedings of PPSN 1992, pages 15–26, 1992.
[RRS95]
Y. Rabani, Y. Rabinovich, and A. Sinclair. A computational view of popu-
lation genetics. In Proceedings of STOC 1995, pages 83–92, 1995.
[Rud97]
G. Rudolph. Convergence Properties of Evolutionary Algorithms. Verlag
Dr. Kovaˇc, 1997.
[SD99]
S.A. Stanhope and J.M. Daida. (1+1) GA ﬁtness dynamics in a changing
environment. In Proceedings of the CEC 1999, pages 1851–1858, 1999.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 922–933, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Performance Evaluation and Population Reduction for a
Self Adaptive Hybrid Genetic Algorithm (SAHGA)
Felipe P. Espinoza1, Barbara S. Minsker
1, and David E. Goldberg
2
1
 University of Illinois, Department of Civil and Environmental Engineering,
205 N. Matthews Ave, Urbana IL 61801
(fespinoz, minsker)@uiuc.edu
2
 University of Illinois, Department of General Engineering,
104 N. Matthews Ave, Urbana IL 61801
deg@uiuc.edu
Abstract. This paper examines the effects of local search on hybrid genetic
algorithm performance and population sizing. It compares the performance of a
self-adaptive hybrid genetic algorithm (SAHGA) to a non-adaptive hybrid
genetic algorithm (NAHGA) and the simple genetic algorithm (SGA) on eight
different test functions, including unimodal, multimodal and constrained
optimization problems. The results show that the hybrid genetic algorithm
substantially reduces required population sizes because of the reduction in
population variance. The adaptive nature of the SAHGA algorithm together
with the reduction in population size allow for faster solution of the test
problems without sacrificing solution quality.
1 Introduction
Hybrid genetic algorithms (HGAs) are a natural extension to genetic algorithms (GA),
combining the genetic algorithm’s global search capabilities with the strengths of
local search algorithms. These algorithms have been used in a number of different
fields, including transportation engineering [8], water resources modeling [4],
operations research [12], and groundwater management [11] to name a few. For
example in the so-called “Bicriteria Transportation Problem” presented in [8], the GA
was combined with the traditional simplex method to solve linear problems to create
the HGA. Another example is the groundwater management problem. The HGA
presented in [11] was created by combining the GA with constrained differential
dynamic programming. In these applications and others, the HGA and the GA solved
the problem using the same population size, and the local search step is applied in
most of the cases to a small number of individuals in the population generation after
generation. In this study, we analyze the effect of the local search component of the
HGA on the search and on reduction of the search space. This reduction reduces the
population size necessary to solve the HGA problem relative to the SGA for the same
solution reliability, thereby decreasing the computational burden.
Section 2 gives an overview of the HGA algorithms analyzed in this work and
Section 3 presents the test functions that were used to evaluate performance of the

Performance Evaluation and Population Reduction         923
HGA algorithms in Section 4. Finally, Section 5 presents our conclusions and
recommendations.
2 SAHGA and NAHGA Algorithms
The two hybrid genetic algorithms (HGA) used in this work are NAGHA (non
adaptive hybrid genetic algorithm) and SAGHA (self adaptive hydrid genetic
algorithm) were originally proposed by [7]. A brief description of the algorithms is
presented next. The full details of these algorithms can be found in [7].
2.1
Non-adaptive Hybrid Genetic Algorithm (NAHGA)
The NAHGA algorithm combines a simple genetic algorithm (SGA) with local
search. The local search step is defined by three basic parameters: local search
frequency, probability of local search, and number of local search iterations. Local
search frequency determines how frequently local search is invoked (e.g., every 3
iterations); probability of local search represents the fraction of individuals in the
population that undergo local search at each invocation; and number of local search
iterations represents the number of local search steps performed at each invocation.
2.2
Self-Adaptive Hybrid Genetic Algorithm (SAHGA)
The SAHGA algorithm works with the same operators as the NAHGA algorithm:
local search frequency, probability of local search and number of local search
iterations. The major difference in the approaches is that the SAHGA adapts in
response to recent performance of the algorithm as it converges to the solution. In
other words, the operators are used only when they can provide new information to
the search. The adaptation process for the three parameters is different. The global-
local switch is adapted by evaluating the ratio of the coefficient of variation of the
fitness between two consecutives generations. Local search is performed when this
ratio is greater than a specified local search threshold. This approach ensures that
local search is only performed when the coefficient of variation is significantly
increasing, which indicates that a new area of decision space is being searched and
local search is needed. The second adaptive parameter is the probability of local
search, which is decreased from the initial value at the beginning of every local search
step. Finally, the number of local search steps is controlled by comparing the
improvement attained in the local search step with the improvement attained by
global search. When local search no longer improves average fitness more than the
most recent global search iteration, the search reverts to global search.

924
F.P. Espinoza, B.S. Minsker, and D.E. Goldberg
3 Test Functions
In order to evaluate the performance of the algorithm, the following 8 test functions
were selected from the GA literature:
• 
Unimodal problems: De Jong’s 1 and De Jong’s 2 [5]
• 
Multimodal problems with the same local mimimum at different locations:
Branin [3] and Six Hump [6]
• 
2 multimodal problems with different optima: Schwefel [17] and Griewank [1]
• 
2 constrained unimodal problems: Test04 [13] and Bracken & McCormick [2]
4 Evaluation of HGA Performance
This section is divided in 5 sub-sections: local search algorithms, population size
evaluation for the SGA, standard deviation reduction, population size for the HGA,
and analysis of results.
4.1
Local Search Algorithm
The local search operator attempts to find the best solution starting at a previoulsy
selected point, in this case a solution in the SGA population. For this analysis, 5 local
search algortithms were selected. These algorithms are:
• Random Walk with Uniform Distribution (LS1): In general, the random walk is
simply the movement from one point of the decision space to a new point
randomly selected using a uniform distribution from a neighborhood around the
starting point [18]. One iteration of this algorithm requires one fitness function
evaluation.
• Random Walk with Normal Distribution (LS2):  This algorithm is similar to the
uniform distribution discussed previously, but the change of location is evaluated
with a normal distribution instead of a uniform distribution. For this reason, the
points located near the starting point are more likely to be selected than those
located closer to the boundary of the search area. Again, one iteration requires one
function evaluation.
• (1+1)-Evolutionary Strategy (LS3): This algorithm, proposed by [14] and [16],
randomly selects a new location using a normal distribution with variable standard
deviation. The standard deviation changes following the so-called 
5
1  success rule
based on the evaluation of success of the search. This algorithm also requires one
fitness function evaluation per local search iteration.
• Random Derivative (LS4): This algorithm randomly selects a search direction, and
using this direction, the location of a new point is evaluated.  This algorithm needs
2 fitness function evaluation for every local search iteration (one for the evaluation
of the coordinates of the new point and one for its fitness).
• Steepest Descent (LS5):  The algorithm evaluates the new point following the
direction of the gradient of the function at the starting point. This algorithm
performs one function evaluation for every one of the decision variables of the

Performance Evaluation and Population Reduction         925
particular test problem (to numerically evaluate gradient) and one function
evaluation to evaluate the fitness of the new individual.
4.2 
Population Size for Genetic Algorithm
One of the critical elements for optimal performance of a GA is the population size.
For the population size evaluation, a relationship derived from the random walk
model proposed by [9] is used. In their approach the population size is given by:
(
)





σ
α
−
≥
−
d
2
N
f
1
K
ln
   
   (1)
In Eq. (1), K represents the building block (BB) order, α is the reliability (i.e., the
probability that the GA finds the optimal solution); σf is the standard deviation of the
fitness function, and d is the signal difference between the best and second best
solution. The parameters σf and d are estimated using a large, random initial
population. For these test problems, d was estimated from the differences in fitness of
the best members of the population. In this case the analysis was performed by means
of a probabilistic approach based on frequency analysis theory [10], in which the
histogram of the fitness function is evaluated. This histogram represents different
“classes“ of fitness that can be statistically identified. Using the histogram, “d“ is
evaluated as the size of the first class of the histogram. The building block order, K, is
unknown but can be assumed to vary between 1 and 5 [15].
Using this information, a range of initial population sizes for the SGA was
estimated from Eq. (1) for all 8 test problems for differrent values of parameter K, as
shown in Table 1.
 The final step in the analysis is the selection of the starting population from the
values given in Table 1. For this analysis, the convergence time must be less than the
drift time. The convergence time is evaluated with the relation t ≈ 2l (where l is the
chromosome length) [19]. The drift time is given by tdrift ≈ 1.4 N (where N is the
population size) [20]. Imposing the condition that convergence time must be less than
drift time to obtain the correct solution, the population size must satisfy the relation
presented in Eq. (2) (lower boundary for population size):
N > 1.42 l
    (2)
Table 1. Population size (SGA) for different values of K
Test Function
Building Block
Order (K)
DJ1
DJ2
Branin Six-Hump Schwefel
Griewank Test04
B&M
1
20
12
20
10
35
20
16
24
2
40
24
40
20
70
40
32
48
3
80
48
80
40
140
80
64
96
4
160
96
160
80
280
160
128
192
5
320
192
320
160
560
320
256
384
Table 2 shows the chromosome length for each test function and the corresponding
lower boundary for the population evaluated with Eq. (2). The table also includes the

926
F.P. Espinoza, B.S. Minsker, and D.E. Goldberg
minimum building block order that satisfies Eq. (2) (see Table 1) and the adopted
population for the SGA runs.
Table 2. Adopted SGA Population for all 8 problems
Function
Chromosome
Length
Lower
Boundary
Minimum Building
Block Order (K)
Adopted
Population
DJ1
150
213
5
320
DJ2
60
83
4
96
Branin
60
83
4
160
Six-Hump
60
83
5
160
Schwefel
150
213
4
280
Griewank
60
86
4
160
Test04
60
83
4
128
B&M
60
86
3
96
4.3 
Standard Deviation Reduction: Local Search Effect on Population Size
For the HGA case, Eq. (1) is modified to include the effect of local search. During
local search, the diversity of the initial population is decreased because multiple
members of the population usually have the same nearby local optima. The following
example contains the results of local search on the multimodal function Griewank for
a population of 20 individuals. Fig. 1 shows the fitness for a random population
before and after local search (uniform random and gradient). The line represents the
average population fitness. From Fig. 1, it is clear that the standard deviation of the
fitness for the population after local search is substantially reduced. Initially the
average is 30 and the standard deviation is 27.1. After 3 iterations (60 function
evaluations) uniform random search reduces the standard deviation to 24.6 and the
average to 25.2 (see open squares and dashed line). Steepest descent local search
performed 1 iteration (60 function evaluations) and changed the standard deviation to
7.3 and reduced the average to 12.1 (see open circles and solid line). Steepest descent
local search lowered the standard deviation and the average by an amount larger than
uniform random local search while using the same 60 function evaluations. Both
methods of local search performed the same number of function evaluations, however
the gradient method only did 1 iteration while the uniform random method did 3
iterations in order to evaluate the results for the same amount of effort. These results
show that gradient search decreases diversity faster than random search.
The local search reduction effect can be modeled as σfl = β σf, where 0  β < 1 is
the standard deviation reduction by local search. Eq. (1) can be rewritten as:
(
)






σ
β
α
−
≥
−
d
2
N
f
1
K
ln
    (3)
Eq. (3) shows that the population for the HGA algorithms should be directly
proportional to the SGA population.
The parameter β can be estimated by applying local search to the initial random
population for a predefined number of iterations. For the current application, local
search was applied to a population of 1,000 individuals for a total of 10 iterations.

Performance Evaluation and Population Reduction         927
Fig. 2 shows the change of standard deviation with respect to the standard deviation
of the initial population for the selected test problem (Griewank).  Since most of the
local search steps involved fewer than 6 iterations, the value used for posterior
evaluations was the average value of β across the first 5 iterations.
a)
    b)
-10
20
50
80
110
0
5
10
15
20
Individual
Fitness
-10
20
50
80
110
0
5
10
15
20
Individual
Fitness
Fig. 1. Local search effect on fitness: a) before local search, b) after local search
In Fig. 2, gradient (LS5) has the largest effect on standard deviation;1 iteration of
this algorithm is equivalent to 3 iterations of LS1, LS2 or LS3. The random derivative
(LS4) method is the second most effective local search method at decreasing the
standard deviation of the population. The first three local search methods also have
substantial effects on the standard deviation, especially if the amount of effort
involved is taken into consideration.
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
6
7
8
9
10
# Local Search Iterations
σ/σ0
LS1
LS2
LS3
LS4
LS5
Fig. 2.	

 Griewank Function

928
F.P. Espinoza, B.S. Minsker, and D.E. Goldberg
4.4 
Population Size for Hybrid Algorithms
Using Eq. (3) and the value of β estimated for every one of the 5 algorithms, Table 3
displays the HGA population results for every test problem and local search
algorithm. From the results, it is clear that the effect of the different local search
algorithms is the same for DJ1 and Six-Hump. On the other hand, for Griewank the
population size is different for every one of the 5 local search algorithms in
consideration because of the different reductions in standard deviation.
Table 3. HGA Population
Local Search Algorithm
Test Function
LS1
LS2
LS3
LS4
LS5
DJ1
288
288
288
288
288
DJ2
64
64
64
40
40
Branin
96
96
80
48
48
Six-Hump
96
96
96
96
96
Schwefel
240
224
216
160
160
Griewank
144
136
128
112
64
Test04
120
112
112
112
80
B&M
92
88
88
48
48
4.5 
HGA Performance
The HGA performance results consist of 2 types of data: results evaluated for one
population for a default set of parameters defined in [7], and results evaluated using
Monte Carlo simulation for different combinations of parameters also defined in [7].
The first set of results for Griewank can be presented in two graphs. The first graph
(Fig. 3) plots the best fitness in the population during the SGA and SAHGA search
processes, with SAHGA using the 5 local search techniques. The results are presented
in terms of number of function evaluations because the adaptive local search uses
different numbers of function evaluations in each generation. The results presented
also only include the results attained by the application of SAHGA because SAHGA
performance is consistently better for the 8 test problems.
Fig. 3 shows that the SGA is inferior to SAHGA with all five local search
algorithms. SGA required the most function evaluations with a total of 8,800. Steepest
descent (LS5) performed only 4,896 function evaluations with a population of 64, a
reduction of 44%. The steepest descent method’s speed and its ability to reduce the
required population offset the fact that it is the most expensive method. In fact, its
performance is better than the performance of normal random search (LS2) and
(1+1)-ES (LS3).
The second result, shown in Fig. 4, is the convergence ratio, which is defined as
the number of individuals with fitness equal to or near the fitness of the best
individual. Fig. 4 shows that the results attained by SAHGA are again better than the
SGA for all 5 local search approaches. The results from the other 7 test functions are
not shown because they present the same trend, with SAHGA performing better than
the SGA by 5% to 33%.

Performance Evaluation and Population Reduction         929
0.0
0.1
0.2
0.3
0.4
0
2000
4000
6000
8000
# Function Evaluation
Best Fitness
SGA
LS1
LS2
LS3
LS4
LS5
Fig. 3. Best Fitness for Griewank
0.0
0.2
0.4
0.6
0.8
1.0
0
2000
4000
6000
8000
10000
# Function Evaluation
Convergence Ratio
SGA
LS1
LS2
LS3
LS4
LS5
Fig. 4. Convergence Ratio for Griewank
The solution quality of the algorithms was also evaluated by performing 100
Monte Carlo simulations over the initial population for every one of the parameter
combinations described in [7]. Table 4 shows the results of these simulations. The
results are presented for the best, default and worst parameter combinations. For
SAHGA, the average number of function evaluations for the default and best set of
parameters are always better than the results attained with the SGA, and the
difference between the best set of results and the default varies between 2% and 8%.
On the other hand, for NAHGA sometimes the results associated with the default set

930
F.P. Espinoza, B.S. Minsker, and D.E. Goldberg
of parameters are worse than the results attained with the SGA, and the difference
between the best set of results and the default set of results varies between 10% and
25% for different local search algorithms. This shows that parameter estimation is
critical to attaining good performance for the NAHGA, but the adaptive capabilities
of SAHGA reduce the need for careful parameter selection. For both algorithms, the
worst set of results was attained when all the individuals are undergoing local search.
Table 4. Monte Carlo Simulation Results
Average Number of Function Evaluations
SAHGA
NAHGA
Local Search
SGA
Worst
Default
Best
Worst
Default
Best
LS1
4,326
6,851
4,020
3,906
9,462
4,373
3,936
LS2
4,326
6,470
3,800
3,720
8,936
4,131
3,717
LS3
4,326
6,402
3,775
3,660
8,723
4,036
3,478
LS4
4,326
13,776
3,964
3,666
14,337
4,645
3,508
LS5
4,326
7,905
3,503
3,292
8,961
4,153
3,071
For the other test problems in this study, the results are similar. The only difference
is that the best local search algorithm sometimes varies among gradient, (1+1)-ES,
uniform random walk, normal random walk, and random derivative. This
phenomenon is related to the differences in standard deviation reduction of each
algorithm for different test functions. For example, when the differences in standard
deviation reduction between steepest descent and random search is not significant (see
Table 3 for functions DJ1 and Six-Hump), random search is more likely to perform
better than steepest descent given the big difference in the effort necessary to apply
the algorithms. On the other hand, when the difference is significant (see Griewank
function in Table 3), steepest descent is better than random walk.
Fig. 5 shows the reliability of SGA, SAHGA and NAHGA for the default set of
parameters. SAGHA reliability was in the range of 98-100%, while solving the
problem over a much smaller population than the SGA, which means it is less
expensive. NAGHA is very efficient as well, with reliability values of 96% to 100%,
again with smaller population sizes than the SGA. SAHGA and NAHGA have
somewhat lower reliabilities for LS4 and LS5 because the extensive standard
deviation reduction associated with the gradient-based algorithms sometimes causes
premature convergence due to loss of diversity in the population.
5 Recommendations and Conclusions
The results presented in this paper indicate that the local search capabilities of the
HGA algorithm enabled robust solution of complex, unimodal, multi-modal, and
constrained problems with less effort than the SGA. Of the two hybrid algorithms,
SAHGA is always superior to NAHGA because near-optimal performance is attained
for a broad range of parameters. Another result is related to the importance of the

Performance Evaluation and Population Reduction         931
90
92
94
96
98
100
LS1
LS2
LS3
LS4
LS5
Local Search Algorithm
Reliability (%)
SGA
NAHGA
SAHGA
   Fig. 5. SGA and HGA Reliabilities for Default Set of Parameters
local search algorithm selected. The results show that the best performance is not
always attained for the same local search algorithm across all test functions, mostly
because of the difference in effort necessary to apply the different local search
algorithms. For any function, the most suitable local search algorithm can be pre-
selected using only the population sizing analysis shown in Table 3. When the
difference between populations (for different local search algorithms) is not
significant, it is more likely that a random search algorithm (which requires only 1
function evaluation per local search iteration) will perform better than steepest
descent (which requires n+1 function evaluations per iteration, with n the number of
decision variables).
Another important result derived from this research is related to the setting of
population sizing in Section 4.  This process allows us to evaluate a good estimate of
the population to achieve optimal performance, but there is a possibility that the same
level of reliability is achieved with a smaller population. With respect to the HGA, the
analysis presented shows clearly how local search reduces the standard deviation of
fitness in the population, which in turn reduces the required population size to achieve
the same level of reliability. This reduced population together with the HGA reduces
the total number of function evaluations by 44% on average. In this way, the HGA is
almost twice as fast as the original GA.
The next step of this research will be to analyze possible ways to improve the
performance of the algorithm with a more detailed study of the processes involved.
Finally, the algorithm will be applied to solve real world problems.

932
F.P. Espinoza, B.S. Minsker, and D.E. Goldberg
Acknowledgments. This material is based upon work supported by the National
Science Foundation under Grant No. BES 97-34076 CAR and the U. S. Army
Research Office under Grant No. DAAD 19-00-7-0389.
Disclaimer. Any opinions, findings, and conclusions or recommendations expressed
in this publication are those of the author(s) and do not necessarily reflect the views of
the National Science Foundation or the U. S. Army Research Office.
References
1.
Bäck , T., D. Fogel and Z. Michalewicz, (eds): Handbook of Evolutionary Computation,
Bristol and New York. Institute of Physics Publishing Ltd and Oxford University Press
(1997)
2.
Bracken, J. G. P. McCormick: Ausgewählte Anwendungen Nichtlinearer Program-
mierung. Berliner Union and Kohlhammer, Stuttgart (1970)
3.
Branin, F. K.: A Widely Convergent Method for Finding Multiple Solutions of
Simultaneous Nonlinear Equations. IBM J. Res. Develop., pp. 504-522. (1972)
4.
Cai, X., McKinney, D. and Lasdon. L.: Solving Nonlinear Water Management Models
Using a Combined Genetic Algorithm and Linear Programming Approach. Advances in
Water Resources, 24, 667-676. (2001)
5.
De Jong, K. A.: An Analysis of the Behavior of a Class of Genetic Adaptive Systems.
Ph.D. Dissertation, University of Michigan, Ann Arbor, MI. (1975)
6.
Dixon, L. C. W. and Szego, G. P.: The Optimization Problem: An Introduction. In Dixon,
L. C. W. and Szego, G. P. (Eds.): Towards Global Optimization II, New York: North
Holland. (1978)
7.
Espinoza, F., B. S. Minsker, and D. Goldberg. (2001). “A Self Adaptive  Hybrid Genetic
Algorithm”. L. Spector, E. Goodman, A. Wu, W.B. Langdon, H.-M. Voigt, M. Gen, S.
Sen, M. Dorigo, S. Pezeshk, M. Garzon, and E. Burke, editors. 2001. Proceedings of the
Genetic and Evolutionary Computation Conference, GECCO'2001. San Francisco,
Morgan Kaufmann Publishers.
8.
Gen, M., Ida, K., and Li, Y.: Bicriteria Transportation Problem by Hybrid Genetic
Algorithm. Computers & Industrial Engineering, 35(1-2), 363-366. (1998)
9.
Harik, G.R., Cantú-Paz, E., Goldberg, D. E., and Miller, B. L.: The Gambler's Ruin
Problem, Genetic Algorithms, and the Sizing of Populations.” In Proceedings of the 1997
IEEE Conference on Evolutionary Computation, pp. 7-12, IEEE Press, New York, NY.
(1997)
10. Hogg, R., and Craig: A. Introduction to Mathematical Statistics. Macmillan Publishing
Co., Inc., New York. (1978)
11. Hsiao, C. and Chang, L.: Dynamic Optical Groundwater Management With Inclusion Of
Fixed Costs. Journal of Water Resources Planning and Management, ASCE, 128(1), 57-
65. (2002)
12. Lin, W., Delgado-Frias, J, Gause, D., and Vassiliadis, S.: Hybrid Newton-Raphson
Genetic Algorithm for the Traveling Salesman Problem. Cybernetics & Systems, 26(4),
387-412. (1995)
13. Kim, J-H. and H. Myung: Evolutionary Programming Techniques for Constrained
Optimization Problems. Evolutionary Computation, 1(2), 129-140. (1997)
14. Rechenberg, I.: Ecolutionsstrategie: Optimierung Technischer Systeme Nach Prinzipien
Der Biologishen Evolution. Frommann-Iolzboog Verlag, Stuttgart. (1973)

Performance Evaluation and Population Reduction         933
15. Reed, P., Minsker, B. S., and Goldberg, D. E.: Designing a Competent Simple Genetic
Algorithm for Search and Optimization. Water Resources Research, 36(12), 3757-3761.
(2000)
16. Schwefel, H. P.: Evolutionsstrategie und Numerische Optimierung. PhD Disser-
tation, Department of Process Engineering, Technical University of Berlin, Berlin,
Germany. (1975)
17. Schwefel, H. P.: Numerical Optimization of Computer Models. John Wiley & Sons,
Chichester-New York-Brisbane-Toronto, (1981)
18. Spitzer, F.: Principles of random walk. D. Van Nostrand Company, Inc. (1964)
19. Thierens, D., Goldberg, D. E., and Guimaraes Pereira: A. Domino Convergence, Drift, and
the Temporal-Salience Structure of Problems. In The 1998 IEEE International Conference
on Evolutionary Computation Proceedings, pp. 535-540, IEEE Press, New York, NY,
(1998) 

Schema Analysis of Average Fitness in
Multiplicative Landscape
Hiroshi Furutani
Kyoto University of Education,
Fushimi-ku, Kyoto, 612-8522 Japan
furutani@kyokyo-u.ac.jp
Abstract. By applying the schema theorem, we study the eﬀects of
crossover in Genetic Algorithms with the multiplicative ﬁtness function.
On this landscape, the analytical expression of the exact schema theorem
can be obtained, and this makes it possible to carry out the mathemat-
ical investigation of genetic operators. We consider the average ﬁtness
under the action of selection, mutation and crossover. To do this, we
give the expressions for the average and variance of ﬁtness in terms of
schema frequencies. The theoretical results are compared with numerical
experiments.
1
Introduction
It is in general very diﬃcult to know the eﬀects of crossover and mutation in
Genetic Algorithms (GAs). Even now, we cannot predict the behavior of a pop-
ulation in a given GA calculation under crossover and mutation. In this paper,
we consider this problem by applying the method of schema analysis [1].
The schema theorem was proposed by Holland [2], and used in the theoretical
analysis of GAs. However, there have been also given many criticisms on its
eﬀectiveness. For example, it only takes into account the destructive nature of
crossover and mutation on schemata without considering their constructive roles
[1].
There are several attempts to make the schema theory more quantitative.
Stephens and Waelbroeck obtained exact schema evolution equations for muta-
tion and crossover [3,4]. Vose has pointed out a close relationship between the
schema frequency and the Walsh transform of genotype frequency [5]. Wright
derived a version of exact schema theorem by using the Walsh transformation
method of Vose [6].
Recently, we have also developed a method to derive a schema theorem from
a evolution equation of genotype frequency by applying Walsh transformation
[7]. Fortunately, it was found recently that evolution equations for mutation and
crossover can be expressed in very simple forms by Walsh transformation [5,
8]. Therefore, it is not diﬃcult to obtain the schema evolution equations for
mutation and crossover [1].
Selection is the most important operator in genetics and GA, and plays the
role of a driving force in evolution. In this paper, we consider the evolution of
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 934–947, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Schema Analysis of Average Fitness in Multiplicative Landscape
935
a population on the multiplicative landscape. Though there are many papers
on this landscape [9,10,11], we study this problem again from the viewpoint of
schema evolution. Previously, we have shown that the evolution equation for
that population can be solved exactly [12]. In this paper, we apply the schema
theorem to this problem, and study the roles of genetic operators microscopically.
We derive an exact evolution equation of schemata evolving under the action of
selection, mutation and crossover. The eﬀect of linkage on the average ﬁtness
is studied mathematically. The theoretical results are compared with numerical
experiments.
2
Mathematical Model
2.1
Model
In this paper, we study the action of selection, mutation and crossover in GAs.
We use the ﬁtness proportionate selection and uniform crossover. A population is
assumed to evolve in discrete and non-overlapping generations, and the process is
described by a set of diﬀerence equations. We consider GAs of inﬁnite population
size, and neglect the eﬀect of random sampling.
An individual is represented by a binary string of length ℓ, and thus there
are n = 2ℓpossible genotypes, or strings. The ith genotype Bi is deﬁned as
Bi =< i(ℓ), · · · , i(1) >. The integer i is sometimes identiﬁed with Bi. We use
the notation |i| for the number of bit ones in i
|i| =
ℓ

k=1
i(k).
(1)
We use the relative frequency xi(t) of the genotype Bi at generation t for the
analysis. The relative frequencies satisfy the normalization condition
n−1

i=0
xi(t) = 1.
(2)
2.2
Walsh Transformation
The evolution of a GA system can also be described by the Walsh transform of
xj(t)
˜xi(t) ≡
n−1

j=0
Wijxj(t),
(3)
where Walsh function Wij is
Wij ≡Wi(j) =
ℓ

k=1
(−1)i(k)·j(k),
(4)

936
H. Furutani
and we call ˜xi as Walsh coeﬃcient. When it is necessary to show the number of
bit ones and their positions in i
k = |i|,
1 ≤b1 < . . . < bk ≤ℓ,
we use the notation
˜xi(t) = ˜x(k)[b1, . . . , bk](t),
(5)
where k is the degree of Walsh coeﬃcient.
Substituting W0j = 1 into equation (3), and using the normalization condi-
tion (2), we have the zeroth order Walsh coeﬃcient ˜x0
˜x0(t) =
n−1

j=0
xj(t) = 1.
(6)
The following relation will be used later
˜x(k)[b1, b2 . . . bk] =
n−1

j=0
k

m=1
(−1)j(bm)xj,
(7)
where j(b1), . . . , j(bk) stand for the bit values of j at the positions of ones in i.
2.3
Evolution Equation
In proportionate selection, the frequency of genotype Bi at generation t + 1 is
given in terms of frequencies at generation t
xi(t + 1) =
fi
¯f(t)xi(t)
(i = 0, . . . , n −1),
(8)
where fi is a ﬁtness of Bi, and the average ﬁtness of the population ¯f(t)
¯f(t) =
n−1

i=0
fi xi(t).
(9)
There is a very important relation for the selection process. The change in
average ﬁtness per generation
∆¯f(t) = ¯f(t + 1) −¯f(t),
is given by
∆¯f(t) =
1
¯f(t)V (f),
(10)
V (f) =

i
f 2
i xi(t) −¯f(t)
2.

Schema Analysis of Average Fitness in Multiplicative Landscape
937
This is a version of Fisher’s fundamental theorem of natural selection [13].
Fisher’s theorem means that the increase in the average ﬁtness is proportional
to the ratio of the variance of ﬁtness V (f) to its average.
The Walsh transform of the evolution equation (8) becomes
S˜xi(t) =
1
n ¯f(t)
n−1

j=0
˜fj ˜xi⊕j(t),
(11)
where ⊕is the bitwise exclusive-or operation, and S shows the eﬀect of selection
symbolically. The ﬁtness ˜fi is the Walsh transform of fi
˜fi =
n−1

j=0
Wijfj.
(12)
The average ﬁtness is given by
¯f(t) = 1
n
n−1

j=0
˜fj ˜xj(t).
(13)
The change in the frequency under mutation is
xi(t + 1) =
n−1

j=0
Mijxj(t),
(14)
where the mutation matrix Mij stands for the probability of mutation from Bj
to Bi over one generation. An (i, j)th element of M is a function of the Hamming
distance d(i, j) between strings i and j.
Mij = (1 −p)ℓ−d(i,j)pd(i,j),
(15)
where p denotes the mutation rate at one bit position over one generation.
The matrix Mij can be diagonalized by Walsh functions
n−1

i′=0
n−1

j′=0
Wii′ Mi′j′ Wj′j = n (1 −2p)|i| δi,j.
Thus the Walsh transform of the evolution equation under mutation (14) is

M ˜xi(t) = (1 −2p)|i| ˜xi(t),
(16)
where 
M symbolically shows the eﬀect of mutation.
The evolution equation under crossover can be given in terms of crossover
tensor C
xk(t + 1) =
n−1

i=0
n−1

j=0
C(k|i, j) xi(t) xj(t).
(17)

938
H. Furutani
Using the Walsh transformation, we can express the eﬀect of crossover in a very
simple form [5,8]
C ˜xk(t) =
n−1

i=0
ci,i⊕k ˜xi(t) ˜xi⊕k(t),
(18)
where C denotes the eﬀect of crossover. The factor cij is given by
cij = (1 −χ)δ(i) + δ(j)
2
+ χ cij(χ = 1),
(19)
where χ is a crossover rate, and cij(χ = 1) is the value of cij with χ = 1. The
discrete δ function is deﬁned for integer m
δ(m) =

1
(m = 0)
0
(m ̸= 0).
For one-point crossover, we obtain [5]
– i = 0 or j = 0
cij(χ = 1) =
1
2(ℓ−1){lo(i) −hi(i) + lo(j) −hi(j)}.
(20)
The functions hi(i) and lo(i) are deﬁned as
hi(i) =

1
(i = 0)
imax
(otherwise),
lo(i) =

ℓ
(i = 0)
imin
(otherwise),
where imax and imin stand for the leftmost and rightmost ones in i, respec-
tively.
– i ̸= 0 and j ̸= 0
cij(χ = 1) =



(imin −jmax)/2(ℓ−1)
( imin > jmax)
(jmin −imax)/2(ℓ−1)
( jmin > imax)
0
(otherwise).
(21)
For uniform crossover, we have a very simple expression
cij = (1 −χ)δ(i) + δ(j)
2
+ χ
ℓ

m=1
δ{i(m)} + δ{j(m)}
2
.
(22)
3
Schema Theorem
After introducing schemata and Holland’s schema theorem [2], we describe the
new schema theorem obtained by Walsh transformation [7].

Schema Analysis of Average Fitness in Multiplicative Landscape
939
A schema H represents a set of genotypes. It is given by three types of
symbols, 0,1 and ∗. The bits 0 and 1 are deﬁning bits, and ∗is a wild card that
allows both 0 and 1. The order of schema O(H) is the number of deﬁning bits,
and the deﬁning length L(H) is the distance between the rightmost and leftmost
deﬁning bits.
The most famous example of the schema theorem is Holland’s one with one-
point crossover [2].
h(H, t + 1) ≥h(H, t)f(H)
¯f(t) {1 −χ L(H)
ℓ−1 −p O(H)},
(23)
where h(H, t) is the relative frequency of the schema H at generation t, and
f(H) is the average ﬁtness of genotypes included in H.
We also use the notation showing explicitly the order of schema, the positions
of deﬁning bits, and their bit values,
H = H(k)[i(b1), i(b2), . . . , i(bk)],
Here, k = O(H), and 1 ≤b1 < b2 < . . . < bk ≤ℓare positions of deﬁning bits.
In a similar manner, we use the notation for the relative frequency h(H),
h(H) = h(k)[i(b1), i(b2), . . . , i(bk)].
Then we derive the new schema theorem by using the Walsh transformation.
When i and j take binary values, 0 and 1, the condition of i = j is given by
δ(i −j) = 1
2{1 + (−1)i(−1)j}.
From this, we can obtain an expression for the frequency h(H) of the ﬁrst order
schema H(1)[i(b1)]. Using the normalization condition (2) and deﬁnition (7), we
have
h(1)[i(b1)] =
n−1

j=0
δ(i(b1) −j(b1))xj
=
n−1

j=0
1
2{xj + (−1)i(b1)(−1)j(b1)xj}
= 1
2{1 + (−1)i(b1)˜x(1)[b1]}.
For the Lth order schema, noting
h(L)[i(b1), i(b2), . . . , i(bL)] =
n−1

j=0
L

m=1
δ(i(bm) −j(bm))xj,
and expanding the products of the delta functions, we have a new expression for
the schema frequency in terms of the Walsh coeﬃcients. Giving the positions of
all deﬁning bits S = {b1, . . . , bL}, and its subset S′ = {b
′
1, . . . , b
′
k}, we have

940
H. Furutani
h(L)[i(b1), . . . , i(bL)]
= 1
2L

S′∈P(S)
(−1)i(b
′
1)+···+i(b
′
k)˜x(k)[b
′
1, . . . , b
′
k],
(24)
where P(S) is the power set of the set S. The power set is a set of all subsets of
a given set. Thus the summation is taken over all subsets S′ of S.
For example, the second order term is
h(L)[i(b1), i(b2)]
= 1
4
	
1 + (−1)i(b1)˜x(1)[b1] + (−1)i(b2)˜x(1)[b2]
+ (−1)i(b1)+i(b2)˜x(2)[b1, b2]

The inverse transformation is given by
(−1)i(b1)+···+i(bL) ˜x(L)[b1, . . . , bL]
=

S′∈P(S)
(−1)L−k 2k h(k)[i(b
′
1), . . . , i(b
′
k)].
(25)
It becomes possible to derive the schema evolution equation for genetic oper-
ators. From the evolution equation of the Walsh coeﬃcient under mutation (16),
we obtain

M h(L)[i(b1), . . . , i(bL)]
=

S′∈P(S)
(1 −2p)k pL−k h(k)[i(b
′
1), . . . , i(b
′
k)].
(26)
For the ﬁrst order Walsh coeﬃcients, we have

M h(1)[i(b1)] = (1 −2p) h(1)[i(b1)] + p.
(27)
For crossover, we give here the ﬁnal results of the schema theorem under
uniform crossover. The process of derivation is described in [7]. We use an integer
i(H) as another representation of the schema H
{0, 1} →1,
{∗} →0
(28)
For example, the new representation of H = ∗10∗is i(H) =< 0, 1, 1, 0 >. Though
this representation does not distinguish between 0 and 1, it may not bring any
confusion in the case of crossover process.
The schema theorem for crossover can be given by
C h(k) =
n−1

i=0
ci,i⊕k h(i) h(i ⊕k),
(29)
The integers i and k in h(k) and h(i) are the binary expression of schema (28),
and please consider that
i = i(H),
k = k(H).

Schema Analysis of Average Fitness in Multiplicative Landscape
941
The coeﬃcient ci,i⊕k is zero when i and i ⊕k both take the value of one at the
same bit position.
We will use a shorthand notation
h(1)[ik = 1] →h(1k),
h(1)[ik = 0] →h(0k).
In this analysis, the notion of linkage [14] is very important, and the linkage
disequilibrium coeﬃcient D is deﬁned as
D[k, m] = h(2)[1k, 1m] −h[1k]h[1m].
(30)
This quantity shows the correlation between alleles at diﬀerent loci. When each
gene evolves independently, a population is in linkage equilibrium, while if there
are any correlations among genes at diﬀerent loci, it is in linkage disequilib-
rium. When the population is in linkage equilibrium, all D coeﬃcients are zero,
D[k, m] = 0.
If we set the crossover rate χ = 1, the eﬀect of crossover is given by [8]. For
uniform crossover,
C D[k, m] = 1
2D[k, m].
(31)
We note that crossover has an eﬀect of reducing the magnitude of D. The action
of mutation on D is given by

M D[k, m] = (1 −2p)2 D[k, m],
(32)
and we see that mutation also reduces the magnitude of D.
4
Multiplicative Landscape
This section gives some mathematical results for GAs on the multiplicative land-
scape obtained by the schema theorem. The ﬁtness function of multiplicative
form is deﬁned as
fi = (1 −s)|i| =
ℓ

k=1
(1 −s)i(k),
(0 < s < 1),
(33)
where s is a parameter of the strength of selection. We consider a maximization
problem on this landscape. In [12], we have shown that the evolution equation
of genotypes can be solved exactly for GAs under the action of selection with
the multiplicative ﬁtness and mutation. We will obtain here an exact schema
evolution.
4.1
Schema Analysis
We assume that the population is in linkage equilibrium state at generation t,
and it will later become clear that this is an essential assumption in the following
analysis. Thus we have
xi =
ℓ

m=1
h{i(m)},
(34)
and the average ﬁtness is

942
H. Furutani
¯f(t) = ¯f (eq),
where the average ﬁtness in equilibrium is also given in the product form
¯f (eq) =
ℓ

m=1
{h(0m) + (1 −s)h(1m)} =
ℓ

m=1
{1 −s h(1m)}.
(35)
Then we may deﬁne the ﬁtness function at each bit as
gk = 1 −s h(1k).
The Walsh transform of the ﬁtness function is obtained as
˜fi =
ℓ

m=1
{1 + (−1)i(m) (1 −s)}
(36)
In the next subsection, without assuming linkage equilibrium, we will obtain the
exact expression for the average ﬁtness using this equation.
Under this assumption, we can obtain the schema equation of the ﬁrst order
schemata for selection. Substituting equation (36) into evolution equation for
selection (11), we have
S˜xi(t) =
ℓ

m=1
β{i(m)}
β(0m) .
(37)
Here, the function β{i(m)} is deﬁned as
β{i(m)} = h(0m) + (−1)i(m) (1 −s)h(1m).
Then we can derive the schema equation of the Lth order schemata. With
the set of deﬁning bits S = {b1, . . . , bL}, we have
Sh(L)[i(b1), . . . , i(bL)] =

m∈S
a{i(m)}h{i(m)}
h(0m) + (1 −s)h(1m),
(38)
where a(0) = 1 and a(1) = 1 −s. For h(1k), the eﬀect of selection is
S h(1k) = (1 −s)h(1k)
1 −sh(1k) .
It is important to note that the schemata after selection also satisfy the
condition of linkage equilibrium. From equations (26) and (29), we can also
verify that the schemata after mutation and crossover satisfy the condition of
equilibrium. Thus, if the population is in linkage equilibrium at t = 0, then it
always satisﬁes the condition of equilibrium.
The change in the ﬁtness function gk under the action of selection is
∆gk(t) ≡Sgk(t) −gk(t),
and we have
∆gk(t) = vk(t)
gk(t),
(39)
where the variance vk is
vk = s2 h(1k){1 −h(1k)}.

Schema Analysis of Average Fitness in Multiplicative Landscape
943
4.2
Schema Representation of Average Fitness
We will discuss here the direct eﬀects of crossover and mutation on the average
ﬁtness. By using equations (13) and (25), the average ﬁtness ¯f(t) can be given
in terms of schema frequencies. After some calculations, we have
¯f(t) = ¯f (eq) + s2∆(2) −s3∆(3) + . . . + (−s)ℓ∆(ℓ).
(40)
The contribution of the mth order term is given by
∆(m) =

k1

<k2
. . .

<km
D[k1, k2, . . . , km],
where the linkage equilibrium coeﬃcient of the order m is given by the schema
frequencies
D[k1, k2, . . . , km] = h(m)(1k1, 1k2, . . . , 1km) −h(1k1)h(1k2) · · · h(1km).
(41)
When we use a small value of s, the equation is approximately given by
¯f(t) ≈¯f (eq) + s2 
k1

<k2
D[k1, k2].
(42)
We have shown in the paper [15] that the variance V of Hamming distance
from the optimum solution can be given by the sum of additive variance Va and
epistatic variance Ve
V = Va + Ve,
(43)
where
Va =
ℓ

k=1
h(1k) {1 −h(1k)},
and
Ve = 2

k1

<k2
D[k1, k2].
Though we use the term “variance”, the epistatic variance Ve can take negative
values.
If we use a small value of s, we obtain the variance of the ﬁtness V (f)
V (f) = s2(Va + Ve) + s3(· · ·) + . . . ≈s2(Va + Ve).
(44)
The variance V (f) in the linkage equilibrium is given by
V (eq) = ¯f (eq)|s→2s−s2 −¯f (eq) · ¯f (eq).
(45)

944
H. Furutani
Fig. 1. Numerical experiments for ¯f(t). Thick lines for the experiments with crossover,
and thin lines without crossover. Solid lines for ¯f(t), and dotted lines for ¯f (eq). The
mutation rate p = 0.001, selection parameter s = 0.2, and N = 200.
Fig. 2. The sum of the second order linkage disequilibrium coeﬃcients, s2Ve, with
(χ = 1) and without (χ = 0) crossover.

Schema Analysis of Average Fitness in Multiplicative Landscape
945
Fig. 3. Variance of ﬁtness. V (f) and V (eq) with (χ = 1) and without (χ = 0) crossover.
Solid lines and dotted lines show V (f) and V (eq), respectively.
5
Experiments
We carried out numerical experiments of GA on the multiplicative landscape,
and their results were compared with the theory in the previous section. The
population size N was 200, and bit length was ℓ= 8. The selection parameter
was s = 0.2. We used the mutation rates p of 0.001, and uniform crossover. The
eﬀect of crossover was studied by comparing the experiments with the crossover
rates χ = 0 and 1. At t = 0, we used the initial value of h(1k) = 1/8. The
calculations were performed repeatedly with the same parameters, and their
results were averaged over 100 runs.
Figure 1 demonstrates the average ﬁtness obtained in four types of calcu-
lations. Thick solid line shows ¯f(t) with crossover χ = 1, and thin solid line
without crossover χ = 0. Thick dotted line is for ¯f (eq) with crossover, and thin
dotted line without crossover. We cannot observe the thick dotted line because
it is masked by the thick solid line. The eﬀect of crossover is very signiﬁcant in
these experiments. We see that crossover works as a beneﬁcial operator. The ef-
fect of linkage disequilibrium is very small, and ¯f(t) ≈¯f (eq) in experiments with
and without crossover. This result seems to suggest that the eﬀect of linkage
disequilibrium is negligibly small. However, this is not the case.
Figure 2 gives s2Ve with and without crossover. Ve shows the degree of linkage
disequilibrium of the second order. When crossover is active, Ve is almost 0. On
the other hand, if χ = 0, Ve takes negative values of large magnitude.
In Fig. 3, the variance of ﬁtness V (f) is shown. The variances in linkage
equilibrium V (eq) calculated by equation (45) are given in dotted lines. There is

946
H. Furutani
large diﬀerence in the results with (thick lines) and without (thin lines) crossover.
The GA with crossover demonstrates the large value of V (f), and the dotted
line for V (eq) is masked by the solid line for V (f). When crossover is absent,
the eﬀect of linkage disequilibrium becomes large, and as a result V (f) is small.
The contribution of linkage is negative, and we observe large diﬀerence between
V (f) and V (eq). Fisher’s theorem states that the large variance of ﬁtness can
give good performance [15], and this explains the fast increase of ¯f(t) in GA
with crossover in Fig.1.
6
Conclusion
In this paper, we have shown that the exact schema theorem can be obtained
for the GA on the multiplicative landscape. In this problem, if we ignore the
eﬀect of random sampling, the assumption of linkage equilibrium holds at all
generations when the initial state is in linkage equilibrium. Therefore, the system
is completely determined by the ﬁrst order schemata h(1k). However, in the
practical calculations, there appears the stochastic eﬀect, and we have to take
into account the eﬀect of linkage disequilibrium.
Crossover works as a beneﬁcial operator in this problem. It reduces the mag-
nitude of D coeﬃcients, and as a result accelerates the speed of evolution by
increasing the total variance V (f). This is an indirect eﬀect of crossover.
On the other hand, there is a direct eﬀect of linkage given by equation (40).
However, this eﬀect is very small as shown in Fig.1 in our experiments.
References
1. H. Furutani: Schema Analysis of OneMax Problem – Evolution Equation for First
Order Schemata –, Foundations of Genetic Algorithms 7, pp.19–36, Morgan Kauf-
mann (2003).
2. J.H. Holland: Adaptation in Natural and Artiﬁcial Systems, MIT press (1992).
3. C. R. Stephens and H. Waelbroeck: “Eﬀective Degrees of Freedom in Genetic
Algorithms,” Physical Review E, 57 (1998) 3251–3264.
4. C. R. Stephens and H. Waelbroeck: “Schemata Evolution and Building Blocks,”
Evolutionary Computation, 7 (1999) 109–124.
5. M.D. Vose: The Simple Genetic Algorithms, MIT Press (1999).
6. A. H. Wright: “The Exact Schema Theorem,” Technical report, University of Mon-
tana, Missoula, MT59812 (1999).
7. H. Furutani: “Derivation of Schema Theorem for Mutation and Crossover by Walsh
Transformation,” IPSJ Journal, 43 (2002) 1050–1060.
8. H. Furutani: “Walsh Analysis of Crossover in Genetic Algorithms,” IPSJ Journal,
42 (2001) 2270–2283.
9. G. Woodcock and P.G. Higgs: “Population Evolution on a Multiplicative Single-
Peak Fitness Landscape,” Journal of Theoretical Biology, 179 (1996) 61–73.
10. A. Pr¨ugel-Bennett: “Modelling Evolving Populations,” Journal of Theoretical Bi-
ology, 185 (1997) 81–95.
11. J. Rowe: “A Normal Space of Genetic Operators,” Evolutionary Computation, 9
(2001) 25–42.

Schema Analysis of Average Fitness in Multiplicative Landscape
947
12. H. Furutani: “Study of Evolution in Genetic Algorithms by Eigen’s Theory Includ-
ing Crossover Operator,”
Proceedings of the Simulated Evolution and Learning
Conference, SEAL’00, (2000) 2696–2703.
13. R.A. Fisher: The Genetical Theory of Natural Selection, Clarendon Press (1930).
Reprint: Dover Publications (1959).
14. J. Maynard Smith: Evolutionary Genetics, Oxford University Press (1998).
15. H. Furutani: “Study of Crossover in One Max Problem by Linkage Analysis,” L.
Spector et al. (Eds.) Proceedings of the Genetic and Evolutionary Computation
Conference, GECCO-2001 Morgan Kaufmann, (2001) 320–327.

On the Treewidth of NK Landscapes
Yong Gao and Joseph Culberson
Department of Computing Science, University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
{ygao, joe}@cs.ualberta.ca
Abstract. The concepts of treewidth and tree-decomposition on graphs
generalize those of the trees. It is well established that when restricted
to instances with a bounded treewidth, many NP hard problems can be
solved polynomially. In this paper, we study the treewidth of the NK
landscape models. We show that the NK landscape model with adja-
cent neighborhoods has a constant treewidth, and prove that for k ≥2,
the treewidth of the NK landscape model with random neighborhoods
asymptotically grows with the problem size n.
1
Introduction
NK landscapes have been widely used in the study of genetic algorithms and
computational biology [1]. There are basically two classes of NK landscape mod-
els: the NK landscape model with adjacent neighborhood and the NK landscape
model with random neighborhood. Both of the two models have been analyzed
and characterized from the perspectives of statistics and computational com-
plexity [2,3,4,5].
In [2], it was shown that even though the NK landscape model with adjacent
neighborhoods can be solved polynomially and the NK landscape model with
the random neighborhood is usually NP complete, the two classes of NK models
share almost identical statistical characteristics such as the average number of
local minima and the average height of the local minima. This has puzzled
researchers in this ﬁeld for a while. In [4], it was shown that the decision versions
of NK landscapes with random neighborhoods are easy to solve with probability
asymptotic to 1 under two commonly used probabilistic settings. This is more
or less in contrast to the common observation that NK landscapes with random
neighborhoods are usually hard for genetic algorithms.
In the study of constraint satisﬁability problems and algorithmic graphs [6,7],
it is well-known that problems with an underlying tree structure can be solved
linearly. The concepts of treewidth and tree-decomposition of graphs generalize
the concept of trees and measure the degree to which a graph behaves like a tree
[7]. It is well established that many NP complete problems, when restricted to
instances with a bounded treewidth structure, can be solved polynomially via
dynamic programming or some other deterministic algorithms [8].
In this paper, we study the treewidth of NK landscapes in an eﬀort to further
understand the diﬀerences between the two classes of NK models and the reasons
why they appear to be hard for genetic algorithms.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 948–954, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

On the Treewidth of NK Landscapes
949
In Section 2, we introduce the NK landscape model, its interaction graph,
and the concepts related to the treewidth and tree decomposition of graphs. In
Section 3, we study the treewidth of the NK landscape models and prove that
the adjacent neighborhood NK landscape model has a ﬁxed treewidth and the
treewidth of the random neighborhood NK landscape model grows linearly with
the problem size n. In Section 4, we discuss the implications of our results and
future work.
2
NK Landscape Models
An NK landscape
f(x) =
n

i=1
fi(xi, Π(xi)),
(1)
is a real-valued function deﬁned on binary strings of ﬁxed length, where n > 0 is
a positive integer and x = (x1, · · · , xn) ∈{0, 1}n. It is the sum of n local ﬁtness
functions fi, 1 ≤i ≤n. Each local ﬁtness function fi(xi, Π(xi)) depends on the
main variable xi and its neighborhood
Π(xi) ⊂Pk({x1, · · · , xn}\{xi})
(2)
where Pk(X) denotes the set of all subsets of size k from X. The most important
parameters of an NK landscape are the number of variables n, and the size of
the neighborhood k = |Π(xi)|.
In an NK landscape, the neighborhood Π(xi) can be chosen in two ways:
the random neighborhood, where the k variables are randomly chosen from the
set {x1, · · · , xn}\{xi}, and the adjacent neighborhood, where k variables with
indices nearest to i (modulo n) are chosen. To simplify the discussion, we assume
in this paper that the adjacent neighborhoods are deﬁned as follows: for each i,
Π(xi) = (xmax(0,i−⌈k
2 ⌉), · · · , xi−1, xi+1, · · · , xmin(n,i+⌈k
2 ⌉)).
(3)
We use A(n, k) to represent the NK landscape model with adjacent neighborhood
and N(n, k) to represent the NK landscape model with the random neighbor-
hood.
Deﬁnition 1. The interaction graph of an NK landscape model is a graph
G(V, E) where the vertex set V = {x1 · · · , xn} corresponds to the set of vari-
ables in the NK landscape and (xi, xj) ∈E if and only if xi and xj both appear
in a local ﬁtness function.
The interaction graph of an NK landscape model captures all the interactions
among the variables in the NK landscapes. A knowledge of these interactions is
critical in understanding the complexity and designing appropriate algorithms to
solve the problems. For example, if the interaction graph is a tree, then a linear
time algorithm readily exists to solve the problem. As yet another example, if the
underlying graph can be decomposed into several connected components, then

950
Y. Gao and J. Culberson
a viable approach to the problem is to ﬁrst solve the subproblems represented
by each connected component and then combine the partial solutions together.
The concept of treewidth and tree decomposition generalizes the above ideas
further. Let us start with the deﬁnition of the l-tree.
Deﬁnition 2. ([7]) l-Trees are deﬁned recursively as follows:
1. A clique with l+1 vertices is an l-tree;
2. Given an l-tree Tn with n vertices, an l-tree with n+1 vertices is constructed
by adding to Tn a new vertex which is made adjacent to an l-clique of Tn
and non-adjacent to the rest of the vertices.
Deﬁnition 3. ([7]) A graph is called a partial l-tree if it is a subgraph of an l-
tree. The treewidth of a graph G is the minimum value l for which G is a partial
l-tree.
The treewidth of a graph has an equivalent deﬁnition based on the concept
of tree decomposition.
Deﬁnition 4. ([7]) A tree decomposition of a graph G = (V, E) is a pair D =
(S, T) where S = {Xi, i ∈I} is a collection of subsets of vertices of G and
T = (I, F) is a tree with one node for each subset of S, such that
1. 
i∈I Xi = V ,
2. for all the edges (v, w) ∈E there exists a subset Xi ∈S such that both v and
w are in Xi, and
3. for each vertex v, the set of nodes {i, v ∈Xi} forms a subtree of T.
The width of the tree decomposition D = (S, T) is maxi∈I(|Xi| −1). And the
treewidth of a graph is the minimum width over all tree decompositions of the
graph.
3
The Treewidth of the NK Landscape Models
In this section the treewidth of the NK landscapes models is studied. We start
with the treewidth of the NK landscape model with adjacent neighborhoods.
In [2], it has been shown that the NK landscape model with adjacent neigh-
borhoods can be solved by dynamic programming in linear time. The following
theorem shows that the interaction graph of the NK landscape model with ad-
jacent neighborhoods has a treewidth independent of n.
Theorem 1. Let A(n, k) be the NK landscape model with adjacent neighbor-
hoods with the underlying graph G. Then, the treewidth of G is at most 2k.
Proof. By direct construction, we can get a tree decomposition with a width k if
the cyclic interactions at the boundaries are ignored. When taking into account
the cyclic interactions at the boundaries, we can get a tree decomposition with
a width 2k.
⊓⊔

On the Treewidth of NK Landscapes
951
We now turn to the NK landscape model with random neighborhoods. Since
the problem is in general NP hard, we do not expect its interaction graph to
have a bounded treewidth because that will mean the problem is polynomially
solvable. Instead, we are interested in how the treewidth changes as the prob-
lem size n and the interaction size k increase, and the probability with which
the treewidth remains small enough for algorithms making use of treewidth-
related information to work eﬃciently. Our result below, however, shows that
the treewidth asymptotically grows linearly with n.
Deﬁnition 5. ([7]) Let G(V, E) be a graph with |V | = n. A partition (S, A, B)
of V is a balanced l-partition if the following conditions are satisﬁed:
1. |S| = l + 1;
2.
1
3(n −l −1) ≤|A|, |B| ≤2
3(n −l −1); and
3. S separates A and B, i.e., there are no edges between vertices of A and
vertices of B.
Theorem 2. Let w(n, k) be the treewidth of the interaction graph of the NK
landscape model with random neighborhoods. Then, for k ≥2, there is a ﬁxed
constant δ > 0 such that
lim
n Pr{w(n, k) ≤δn} = 0.
(4)
Proof. Let l = w(n, k). It is well-known that if a graph has a treewidth l, then
the graph must have a balanced l-partition [7]. Consider the interaction graph
G = G(V, E) of the NK landscape with random neighborhoods. Let P be the
set of all the partitions of the vertex set V that satisﬁes the ﬁrst two conditions
in Deﬁnition 5. For a given P = (S, A, B) ∈P, deﬁne a random variable IP as
follows:
IP =

1, if P is a balanced partition;
0, otherwise.
(5)
and let O be the event that IP is 1, i.e., that there are no edges between vertices
of A and vertices of B. Recall that Π(xi) is the set of neighbors of the i-th local
ﬁtness function. For each 1 ≤i ≤n with xi ∈A (or xi ∈B), let Oi be the event
that Π(xi) ⊂A  S (Π(xi) ⊂B  S respectively). For xi ∈S, let Oi be the
event that Π(xi) ⊂A  S or Π(xi) ⊂B  S. Then, by the deﬁnition of the NK
landscape model with random neighborhoods and its interaction graph, we have
O =

1≤i≤n
Oi.
(6)
Since each local ﬁtness function selects its neighbors independently, O1, · · · , On
are mutually independent. We have
Pr{O} =
n

i=1
Pr{Oi}.
(7)

952
Y. Gao and J. Culberson
For xi ∈A (or xi ∈B), we have
P{Oi} ≤
 2
3 (n−l−1)+l
k

n−1
k

= (1
3)k(2 +
l
n −1)k.
(8)
Similarly, for xi ∈S, we have
Pr{Oi} ≤2(1
3)k(2 +
l
n −1)k.
(9)
Then,
Pr{O} ≤2l+1(1
3)kn(2 +
l
n −1)kn.
(10)
Let I = 
P ∈P
IP . By its deﬁnition, we have
|P| =
	 n
l + 1


1
3 (n−l−1)≤a≤2
3 (n−l−1)
	n −l −1
a

≤
	 n
l + 1

2n−l−1.
(11)
It follows that the expectation of I satisﬁes
E{I} =

P ∈P
E{IP }
≤
	 n
l + 1

2n−l−12l+1(1
3)kn(2 +
l
n −1)kn
≤
	 n
l + 1

2n(2
3 + 1
3
l
n −1)kn.
(12)
Let 0 < y = l+1
n < 1. We obtain from Stirling’s formula that
	 n
l + 1

∼
1

2πy(1 −y)n
	
1
yy(1 −y)1−y

n
.
(13)
And hence,
E{I} ≤
1

2πy(1 −y)n
	
2
yy(1 −y)1−y · (2
3 + 1
3y)k

n
.
(14)
Since for k ≥2,
lim
y→0
	
2
yy(1 −y)1−y · (2
3 + 1
3y)k

= 2(2
3)k < 1,
(15)
there exists a 0 < δ < 1 such that
lim
n
1

2πδ(1 −δ)n
	
2
δδ(1 −δ)1−δ · (2
3 + 1
3δ)k

n
= 0.
(16)

On the Treewidth of NK Landscapes
953
Therefore, we have
lim
n Pr{w(n, k) ≤δn} ≤lim
n Pr{I > 0}
≤lim
n E[I] = 0.
(17)
This concludes the proof.
⊓⊔
4
Conclusions and Future Work
As we have shown in the previous sections, the treewidth of the NK landscape
is bounded by the interaction index for the adjacent neighborhood model, but
grows linearly with the problem size for the random neighborhood model. In
addition to the NP complete study of the random neighborhood model, our
result is the ﬁrst one that depicts the diﬀerence between the two statistically
similar NK landscape models.
It is well-known that optimization problems with bounded treewidth can be
decomposed into independent sub-problems and solved polynomially using dy-
namic programming techniques. This is the case for the NK landscapes with
adjacent neighborhoods [2]. Other examples include the constraint satisfaction
problems and the inference problem for Bayesian networks [6,9] in which the
popular tree-clustering method run polynomially if the problems under consid-
eration have a bounded treewidth. For the random neighborhood model, our
result shows that algorithms that make use of the information about the struc-
tures of the interactions in the same way as the tree-clustering approach cannot
solve the problem eﬃciently.
An interesting question that deserves further investigation is “Do genetic al-
gorithms exploit the treewidth-related structural information? And if so, to what
extent do they rely on that information to work?” We suspect that the answer
to the ﬁrst question is aﬃrmative. In fact, this is best illustrated by the recent
work on sampling-based genetic algorithms. Instead of using genetic operators
to generate new solutions, these sampling-based algorithms generate candidate
solutions by sampling some probability distributions on the solution space and
update the distributions based on the information gathered as new solutions are
evaluated. The probability distributions may be modelled as the product of inde-
pendent distributions [10], decomposable distributions naturally obtained from
the knowledge about the interaction structures[11], or Bayesian networks that
are constructed from the existing candidate solutions[12]. All of these models
depend on the factorization of a multivariate probability distribution into the
form
p(x1, · · · , xn) = 1
Z

C∈C
ψC(xC),
where p(·) is the original distribution and C is a tree decomposition of a graph,
the structure of which is deﬁned (explicitly or implicitly) by the designers of the
sampling-based algorithms and is believed to be able to capture the interaction

954
Y. Gao and J. Culberson
structure of the original optimization problems. The eﬀectiveness and eﬃciency
of these sampling-based algorithms thus depend critically on how well the fac-
torization approximates the real tree-decomposition of the original problem, and
on the width of the tree decomposition which is lower bounded by the treewidth
of the original problem.
Another direction of future work is to study the treewidth of the NK land-
scape models by considering the number of local ﬁtness functions as a parameter
as well as the interaction index k. By relaxing the requirement that each variable
is associated with a local ﬁtness function in the current model, we can consider a
more generalized model in which the number of variables that have an associated
local ﬁtness function is another tunable parameter. It would be interesting to
study the treewidth of such a generalized NK landscape model.
References
1. Kauﬀman, S.: The Origins of Order: Self-organization and Selection in Evolution.
Oxford University Press, Inc. (1993)
2. Weinberger, E.D.: NP completeness of Kauﬀman’s NK model, a tunable rugged
ﬁtness landscape. Technical Report Working Papers 96-02-003, Santa Fe Institute,
Santa Fe (1996)
3. Wright, A.H., Thompson, R.K., Zhang, J.: The computational complexity of NK
ﬁtness functions. Technical report, Department of Computer Science, University
of Montana (1999)
4. Gao, Y., Culberson, J.: An analysis of phase transition in NK landscapes. Journal
of Artiﬁcial Intelligence Research 17 (2002) 309–332
5. Evans, S.N., Steinsaltz, D.: Estimating some features of NK ﬁtness landscapes.
Ann. Appl. Probab. 12 (2002) 1299–1321
6. Dechter, R., Fattah, Y.: Topological parameters for time-space tradeoﬀ. Artiﬁcial
Intelligence 125 (2001) 93–118
7. Kloks, T.: Treewidth: Computations and Approximations. Springer-Verlag (1994)
8. Downey, R.G., Fellows, M.R.: Parameterized Complexity. Springer (1997)
9. Gottlob, G., Leone, N., Scarcello, F.: A comparison of structural CSP decomposi-
tion methods. Articial Intelligence 124 (2000) 243–282
10. Leung, Y., Gao, Y., Zhang, W.: A genetic-based method for training fuzzy systems.
In: Proc. of the 10th IEEE International Conference on Fuzzy Systems. Volume 1.,
IEEE (2001) 123–126
11. M¨uhlenbein, H., Mahnig, T.: Convergence theory and applications of the factor-
ized distribution algorithm. Journal of Computing and Information Technology 7
(1999) 19–32
12. Pelikan, M., Goldberg, D.E., Lobo, F.: A survey of optimization by building and
using probabilistic models. Technical Report 99018, IlliGAL, University of Illinois
(1999)

Selection Intensity in Asynchronous Cellular
Evolutionary Algorithms
Mario Giacobini1, Enrique Alba2, and Marco Tomassini1
1 Computer Science Institute, University of Lausanne, Lausanne, Switzerland
2 Department of Computer Science, University of M´alaga, M´alaga, Spain
Abstract. This paper presents a theoretical study of the selection pres-
sure in asynchronous cellular evolutionary algorithms (cEAs). This work
is motivated by the search for a general model for asynchronous update
of the individuals in a cellular EA, and by the necessity of better ac-
curacy beyond what existing models of selection intensity can provide.
Therefore, we investigate the diﬀerences between the expected and actual
values of the selection pressure induced by several asynchronous update
policies, and formally characterize the update dynamics of each variant
of the algorithm. New models for these two issues are proposed, and are
shown to be more accurate (lower ﬁt error) than previous ones.
1
Introduction
Cellular evolutionary algorithms (cEAs), also called diﬀusion or ﬁne-grained
models, have been popularized, among others, by early work of Gorges-Schleuter
[4] and Manderick and Spiessen [6]. These models are based on a spatially dis-
tributed population in which genetic operations may only take place in a small
neighborhood of each individual. Usually, individuals are arranged on a regular
grid of dimensions d = 1, 2 or 3. Cellular EAs are a kind of decentralized EA
model. They are not just a parallel implementation of an EA; in fact, although
parallelism could be used to speed up the search, we do not address it in this
work.
Although fundamental theory is still an open research line for cEAs, they
have been empirically reported as being useful in maintaining diversity and pro-
moting slow diﬀusion of solutions through the grid (exploration). Part of their
behavior is due to a lower selection pressure compared to that of panmictic EAs
(here panmictic means that any one chromosome may mate with any other in
the population). The inﬂuence of the neighborhood, grid topology, and high ef-
ﬁciency in comparison to other EAs have all been investigated in detail in [2,
5,7,8], and tested on diﬀerent applications such as combinatorial and numerical
optimization.
Cellular EAs can be seen as stochastic cellular automata (CAs) [11,13] where
the cardinality of the set of states is equal to the number of points in the search
space. CAs, as well as cEAs, usually assume a synchronous or “parallel” update
policy (reproduction at a time), in which all the cells are formally updated
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 955–966, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

956
M. Giacobini, E. Alba, and M. Tomassini
simultaneously. However, this is not the only option available. Indeed, several
works on asynchronous CAs have shown that sequential update policies have a
marked eﬀect on their dynamics (see e.g. [9,10]). Thus, it would be interesting
to investigate asynchronous cEAs and their problem solving capabilities. A ﬁrst
step in that direction was made in [1], where a set of standard problems were
studied under several asynchronous update policies in a 2-d cGA environment.
The main observation was that, although asynchronous update is not always
the best choice in terms of solution quality, it is numerically faster, and the
speed of convergence can be varied by changing the updating scheme. Thus,
since convergence and diversity in EAs are related to selection, we would like to
get a better understanding of the behavior of selection in asynchronous cEAs as
compared to synchronous update and to the panmictic case. We present here an
extension of the works [2,5,7,8] on selection pressure to asynchronous cEAs. For
reasons of space, we limit ourselves to the two-dimensional grid case, the most
common in practice.
The paper is organized as follows. The next section contains some background
on asynchronous cEAs. Section 3 describes the results of our experiments on se-
lection pressure in asynchronous cEAs. Section 4 analyzes the current logistic
model of selection pressure and presents an improved characterization of asyn-
chronous algorithms, leading to a new model proposal. Finally, section 5 oﬀers
our conclusions, as well as some comments on future work.
2
Asynchronous cEAs
Updating a cell (individual) in a cellular EA means selecting two parents in
the individual’s neighborhood (including the individual itself), applying genetic
operators to them, and ﬁnally replacing the individual with the best oﬀspring.
In a conventional synchronous cEA, all the individuals in the grid are updated
simultaneously. This step makes up a generation, and the process is repeated
until a termination condition is reached.
There exist many ways for sequentially updating the cells of a 2-d cEA . Here
we employ step-driven updates and ignore the so-called time-driven methods, in
which (real) time is explicit. Time-driven methods are more realistic for physical
simulation but are not needed in the EA case (an excellent discussion of asyn-
chronous update in CAs is available in [9]). The most general update scheme
is independent random ordering of updates in time, which consists of randomly
choosing the cell to be updated next with replacement. This corresponds to a bi-
nomial distribution for the update probability. This update policy will be called
uniform choice (UC) in the following and it is similar to the time-driven Poisson
update in the limit of large n, n being the population size.
In our study we also consider three other update methods: ﬁxed line sweep,
ﬁxed random sweep, and new random sweep (we employ the same terminology
as in [9]).
– In ﬁxed line sweep (LS), the simplest method, the n grid cells are updated
sequentially (1, 2 . . . n), line by line of the 2-d grid.

Selection Intensity in Asynchronous Cellular Evolutionary Algorithms
957
– In the ﬁxed random sweep update (FRS), the next cell to be updated is
chosen with uniform probability without replacement; this will produce a
certain update sequence (cj
1, ck
2, . . . , cm
n ), where cp
q means that cell number p
is updated at time q and (j, k, . . . , m) is a permutation of the n cells. The
same permutation is then used for all update cycles.
– The new random sweep method (NRS) works like FRS, except that a new
random cell permutation is chosen anew for each sweep through the array.
A time step is deﬁned as updating n times sequentially, which corresponds
to updating all the n cells in the grid for LS, FRS and NRS, and possibly less
than n diﬀerent cells in the uniform choice method, since some cells might be
updated more than once. It should be noted that, with the exception of ﬁxed
line sweep, the other asynchronous updating policies are stochastic, representing
an additional source of non-determinism besides that of the genetic operators.
3
Takeover Times
In order to study the induced selection pressure by itself (without introducing the
perturbing eﬀect of recombination or mutation operators) a standard technique
is to let selection be the only active operator, and then monitor the growth rate
of the best individual in the initial population [3]. The takeover time is the time
it takes for the single best individual to conquer the whole population. A shorter
takeover time thus means a higher selection pressure. It has been shown that
when we move from a panmictic population to a spatially structured one of the
same size with synchronous updating of the cells, the global selection pressure
induced on the entire population is qualitatively similar but weaker (Sarma and
De Jong [7]).
Three standard selection algorithms were used in [7], namely ﬁtness propor-
tionate, linear ranking, and binary tournament. The cellular EA structure was
a two-dimensional toroidal grid of size 32 × 32 with three diﬀerent neighbor-
hood shapes with 5, 9 and 13 neighbors respectively, which are the most com-
mon in practice. In the spatially distributed case it was observed that, for all
three mentioned neighborhoods, the global selection pressure induced by ﬁtness-
proportionate selection was smaller than the pressures induced by linear rank-
ing and binary tournament, with binary tournament being roughly equivalent to
ranking as the neighborhood size increases, as it can be inferred from well-known
existing theoretical considerations on selection pressure.
Sarma and De Jong [8] performed a more detailed empirical analysis of the ef-
fects of the neighborhood’s size and shape on the local selection algorithms. They
were able to show that propagation times are closely related to the neighborhood
size, with larger neighborhoods giving rise to stronger selection pressures.
In the following, we report results (for three diﬀerent neighborhoods) on the
selection pressure for three selection methods in the case of asynchronous up-
date. The neighborhoods used are Von Neumann (5 neighbors along the NWSE
directions and the center cell, also called Linear 5), Moore (9 neighbors, includ-
ing the central cell and its eight nearest neighbors, also called Compact 9), and

958
M. Giacobini, E. Alba, and M. Tomassini
Compact 13 (like Moore, with four neighbors added along the N,W,S, and E
directions -like a diamond-).
For the sake of comparison, we also include the curves corresponding to the
panmictic case and to the synchronously updated grid. Since the results with
the asynchronous Fixed Random Sweep are very similar to those using the New
Random Sweep policy, only the latter curves are reported. Each of these results
is the average of 100 independent runs.
0
10
20
30
40
50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
0
10
20
30
40
50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
(a)
(b)
0
10
20
30
40
50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
(c)
Fig. 1. Takeover times with rank selection. Linear 5 neighborhood (a); Compact 9
neighborhood (b); Compact 13 neighborhood (c). Mean values over 100 runs. The
vertical axis represents the proportion of population consisting of best individual as a
function of the time step.
Figure 1 shows the mean growth curves of a cEA using rank selection in a
Linear 5, Compact 9 and Compact 13 neighborhood, respectively. Figure 2 de-
picts graphs of mean growth curves for a cEA using binary tournament selection
respectively in the Linear 5, Compact 9 and Compact 13 neighborhood. The
mean takeover times results for a cEA using a roulette wheel selection respec-
tively in a Linear 5, Compact 9, and Compact 13 neighborhood are reported in
Figure 3.

Selection Intensity in Asynchronous Cellular Evolutionary Algorithms
959
0
10
20
30
40
50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
0
10
20
30
40
50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
(a)
(b)
0
10
20
30
40
50
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
(c)
Fig. 2. Takeover times with binary tournament selection. Linear 5 neighborhood (a);
Compact 9 neighborhood (b); Compact 13 neighborhood (c). Mean values over 100
runs. The vertical axis represents the proportion of population consisting of copies of
the best individual as a function of the time step.
These results largely conﬁrm the ﬁndings of Sarma and De Jong as far as
synchronous and panmictic cEAs are concerned. Indeed, binary tournament and
ranking induce very similar global selection pressure, while proportional selection
exhibits less pressure. Moreover, for a given selection policy, larger neighborhoods
induce a stronger selection intensity.
What is new in this paper (our contribution) is the behavior of the asyn-
chronous models. Generally speaking, it can be observed that the asynchronous
models give an emergent selection pressure that is between the panmictic up-
per bound and the synchronous lower bound. All graphs show that the global
selection intensity grows going from uniform choice update to line sweep, with
new random sweep and ﬁxed random sweep in between, although an analysis
of the variances should be conducted to quantitatively conﬁrm the trend. This
suggests that, by choosing the appropriate asynchronous update policy, one is
able to control the selection pressure without using ad hoc numerical parameters.
This opens new possibilities for dynamical EAs in which the selection pressure
is under the control of the modeler even during the run (work in progress).

960
M. Giacobini, E. Alba, and M. Tomassini
0
10
20
30
40
50
60
70
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual$ Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
0
10
20
30
40
50
60
70
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual$ Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
(a)
(b)
0
10
20
30
40
50
60
70
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Time Steps
Best Individual$ Proportion
synchronous
uniform choice
new random sweep
line sweep
panmictic
(c)
Fig. 3. Takeover times with ﬁtness proportional selection. Linear 5 neighborhood (a);
Compact 9 neighborhood (b); Compact 13 neighborhood (c). Mean values over 100
runs. The vertical axis represents the proportion of population consisting of copies of
the best individual as a function of the time step. Note the change of scale on the
horizontal axis to avoid cutting oﬀthe curves in ﬁgure (a).
The impression is conﬁrmed by Table 1, where mean takeover times of all the
update methods for each selection mechanism and each of the three considered
neighborhoods are reported with their standard deviations.
4
Modelling Individual Growth
In this section, quantitative models for the individual growth (and thus for the
diﬀerent global selection pressures induced in the population) are presented using
asynchronous update. We ﬁrst give some statistical results valid for all ﬁnite 2-
d cEAs discrete lattices. Next, we oﬀer a quantitative analysis of the takeover
time, and ﬁnally we hint at some possible improvements in the existing logistic
model.

Selection Intensity in Asynchronous Cellular Evolutionary Algorithms
961
Table 1. Mean takeover time for the three selection mechanisms (vertically) and the
ﬁve update methods (horizontally). Upper part: Linear 5 neighborhood. Middle: Com-
pact 9 neighborhood. Lower part: Compact 13 neighborhood. The last column refers
to the classical panmictic case. Standard deviations in parentheses.
LINEAR 5
Synchro
LS
FRS
NRS
UC
Panmictic
Roulette
52 (3.7)
34 (2.8)
37 (3.0)
37 (3.0)
44 (3.7)
12 (1.0)
Tournament
42 (2.7)
21 (1.9)
26 (2.0)
28 (1.9)
33 (3.7)
10 (0.7)
Ranking
39 (2.1)
18 (1.6)
24 (1.5)
24 (1.7)
30 (3.2)
10 (0.8)
COMPACT 9
Synchro
LS
FRS
NRS
UC
Panmictic
Roulette
38 (2.6)
23 (2.5)
26 (1.8)
26 (2.2)
29 (4.2)
12 (1.0)
Tournament
31 (1.8)
15 (1.4)
19 (1.4)
19 (1.5)
23 (2.9)
10 (0.7)
Ranking
30 (1.7)
13 (1.4)
18 (1.3)
18 (1.4)
22 (2.7)
10 (0.8)
COMPACT 13
Synchro
LS
FRS
NRS
UC
Panmictic
Roulette
31 (1.7)
18 (1.9)
20 (1.9)
20 (1.8)
23 (3.3)
12 (1.0)
Tournament
25 (1.4)
12 (1.2)
15 (1.0)
15 (1.2)
18 (2.9)
10 (0.7)
Ranking
25 (1.2)
11 (1.1)
14 (1.1)
15 (1.0)
18 (2.5)
10 (0.8)
4.1
Statistical Results on Information Propagation
Sch¨onﬁsh and de Roos [9] derived the expected value E(Z) and the variance
V (Z) of the number of single steps between an update of a cell x and the next
update of a cell y ̸= x in the neighborhood of x, U(x), in order to compare
the diﬀerent asynchronous CA updating methods. These results can also be
applied to asynchronous cEAs, where the local transition function f0 does not
deterministically determine the next state of the cell, but describes a probabilistic
rule for such an update [11,13]. This rule is generally determined by the diﬀerent
selection, crossover and mutation mechanisms used in the EA. In our study of
the takeover times induced by diﬀerent update methods the local function f0
only depends on the selection mechanism used.
Table 2. Values of the expected value E(Z) and of the variance V (Z) of the number
of time steps between an update of a cell x and the next update of a cell y ̸= x in the
neighborhood U(x) for a cEA on a square grid. For Asynchronous Line Sweep, which
depends from the chosen neighborhood, the Linear 5 neighborhood result is shown.
Synchro
LS
FRS
NRS
UC
E(Z)
1
1
2
1
2
1
12

7 + 1
n

1
V(Z)
0
1
2
 1
n −√n + 1
2n
1
12 (n −2)
1
12
 23
12n −13
6 −
13
12n

n-1

962
M. Giacobini, E. Alba, and M. Tomassini
Sch¨onﬁsh and de Roos used the number of cell updates in their statistics. In
order to be able to extend their results to the synchronous cEA, their statistics
need to be translated using the number of time steps between an update of a
cell x and the next update of a cell y ̸= x in the neighborhood U(x) of x. In fact,
for synchronous cEAs it is not possible to count the single cells updating, but a
generational synchronous step can be compared to a time step of an asynchronous
cEA. Table 2 contains the values of E(Z) and V (Z) for the four asynchronous and
the synchronous updating policies. If in a cEA we keep the selection mechanism
ﬁxed and we vary the updating method, the results of Table 2 explain the ranking
of the observed takeover times; notice that some experimental values are slightly
diﬀerent while theoretically they should be identical on the average. This is the
case for Line Sweep and Fixed Random Sweep, as well as for Uniform Choice
and Synchronous. However, these results do not explain the actual shapes of the
selection pressure curves. We are currently working on the analytical study of
the curves and some preliminary results are reported in the next two sections.
4.2
Fitting the Selection Pressure Curves
Sarma and De Jong [7] proposed a simple quantitative model for the study of
the selection pressure curves for cEAs. They assumed that the diﬀusion of the
best individual in the artiﬁcial evolution of a structured population would follow
a logistic curve. Let us analyze their result, shown in Equation 1.
Pb(t) =
1
1 +

1
Pb(0) −1

e−αt
(1)
This equation, where Pb(t) represents the proportion of the best individual in
the population at time t, was proposed for synchronous cEAs, and therefore we
wondered whether it holds for asynchronous ones. Consequently, we proceeded
to analyze the error (mean squared error) between an actual average selection
pressure and the theoretically predicted values, for all the update methods con-
sidered in this work. The steps were (1) to compute the theoretical value of α,
(2) to generate the predicted curve by using one point of the average observed
performance curve, and (3) to compare it against the whole set of points of this
observed curve.
To derive the α parameter we selected a mid point (with Pb(t) around 0.5)
from the experimental curves. Then, we generated the corresponding curve, and
computed the squared error. Table 3 shows our measurements.
This table also shows that, although the ﬁtting is satisfactory, it is not that
good, since there exists a gap between the ﬁtted curves and the experimental
points. This claim is conﬁrmed by Figure 4, in which the panmictic case is
particularly good, while the other ﬁttings could clearly be improved. This lead
us to think that there could exist a better ﬁtting for cellular EAs than the logistic
one, whose main advantage is its similitude to the theoretical results existing for
panmictic algorithms [3].

Selection Intensity in Asynchronous Cellular Evolutionary Algorithms
963
Table 3. Mean squared error between predicted and actual logistic ﬁttings for Linear
5, Compact 9, and Compact 13 neighborhoods for all the update modes.
LINEAR 5
Synchro
LS
FRS
NRS
UC
Panmictic
Roulette
0.00406
0.00358
0.00372
0.00427
0.00358
0.00069
Tournament
0.00309
0.00270
0.00281
0.00274
0.00248
0.01053
Ranking
0.00366
0.00282
0.00257
0.00234
0.00290
0.00181
COMPACT 9
Synchro
LS
FRS
NRS
UC
Panmictic
Roulette
0.00349
0.00376
0.00284
0.00280
0.00332
0.00069
Tournament
0.00287
0.00194
0.00193
0.00178
0.00202
0.01053
Ranking
0.00311
0.00190
0.00197
0.00184
0.00209
0.00181
COMPACT 13
Synchro
LS
FRS
NRS
UC
Panmictic
Roulette
0.00339
0.00328
0.00199
0.00242
0.00281
0.00069
Tournament
0.00231
0.00148
0.00142
0.00149
0.00145
0.01053
Ranking
0.00225
0.00166
0.00153
0.00150
0.00122
0.00181
0
20
40
60
80
0
0.5
1
Synchronous
error = 0.00406
0
20
40
60
80
0
0.5
1
Line Sweep
error = 0.00358
0
20
40
60
80
0
0.5
1
Fixed Random Sweep
error = 0.00372
0
20
40
60
80
0
0.5
1
New Random Sweep
error = 0.00427
0
20
40
60
80
0
0.5
1
Uniform Choice
error = 0.00358
0
20
40
60
80
0
0.5
1
Panmictic
error = 0.00069
Fig. 4. Fitting of the experimental takeover time curves (full) with the logistic model
(dashed) for the various update modes. Results refer to ﬁtness proportional selection
with Linear 5 neighborhood.

964
M. Giacobini, E. Alba, and M. Tomassini
We have shown in this section that the logistic ﬁtting should be improved for
decentralized algorithms. In the next section some steps toward a more accurate
model are described.
4.3
An Improved Model
It is well known since the work of Verhulst [12], that the assumption of logistic
growth is true for biological populations within bounded resources. It is easy to
see that this behavior also holds for the best individual growth in the artiﬁcial
evolution of a ﬁnite panmictic population [3]. In fact, if we consider a population
of size n, the number N(t) of copies of the best individual in the population at
time step t is given by the following recurrence:

N(0) = 1
N(t) = N(t −1) + psN(t −1)(n −N(t −1))
(2)
where ps is the probability that the best individual is chosen. This recurrence
can be easily transformed into one that describes a logistic population growth
in discrete time:
N(0) = 1
N(t) = N(t −1) + (psn)N(t −1)

1 −1
nN(t −1)

(3)
Such a recurrence can be expressed in analytical form by the logistic equation:
N(t) =
n
1 +

n
N(0) −1

e−αt
(4)
where the growth coeﬃcient α depends on the probability ps and the population
size n. This happens to be the approach taken in [8] for synchronous cEAs.
As suggested by Gorges-Schleuter in [5], in the artiﬁcial evolution of locally
interacting, spatially structured populations, the assumption of a logistic growth
doesn’t hold anymore. In fact, in the case of a ring or a torus structure we have
respectively a linear and a quadratic growth. We complete here her analysis
which holds for unrestricted growth, extending it to bounded synchronously
updated spatial populations.
For a structured population, let us consider the limiting case, which repre-
sents an upper bound on growth rate, in which the selection mechanism is deter-
ministic, and a cell always chooses its best neighbor for updating. If we consider
a population of size n with a ring structure (like that of 1-d cellular automata
in which the two cells on the borders are linked) and a neighborhood radius of
k (i.e. a neighborhood of a cell contains 2k + 1 cells), the following recurrence
describes the growth of the number of copies of the best individual:
N(0) = 1
N(t) = N(t −1) + 2k
(5)
This recurrence can be described by the closed equation N(t) = N(0) + 2kt,
that clearly shows the linear character of the growth rate.

Selection Intensity in Asynchronous Cellular Evolutionary Algorithms
965
In the case of a population of size n disposed on a toroidal grid of size √n×√n
(assuming √n odd) and the Linear 5 neighborhood structure, the number of
copies of the best individual can be described by the following recurrence:





N(0) = 1
N(t) = N(t −1) + 4t
,
for 0 ≤t ≤
√n−1
2
N(t) = N(t −1) + 4(√n −t) ,
for t ≥
√n−1
2
(6)
This growth is described by a convex quadratic equation followed by a con-
cave one, as the two closed forms of the recurrence clearly show:

N(t) = 2t2 + 2t + 1
,
for 0 ≤t ≤
√n−1
2
N(t) = −2t2 + 2(2√n −1)t + 2√n −n ,
for t ≥
√n−1
2
(7)
Thus, a more accurate ﬁtting should take into account the non-exponential
growth followed by saturation (crowding eﬀect).
5
Conclusions
In this work we have presented diﬀerent update policies to deal with cellular
EAs in a search for a more eﬃcient algorithm with respect to its canonical form.
We have investigated the induced selection pressure of such policies with respect
to the widespread synchronous update and panmictic algorithms. Our results
indicate that these two algorithms represent the smaller and higher (respec-
tively) bounds to selection intensity, and that the asynchronous update methods
represent intermediate values of pressure (which can even be ranked for such
methods).
We have studied the existing proposals dealing with a logistic ﬁtting that,
although very similar to that existing for panmictic EAs and globally valid, are
susceptible to further improvement for cellular EAs. We provide such an im-
proved model of the best individual growth and additionally characterize the
expected time between the update of two individuals residing in the same neigh-
borhood for all the update methods considered in the paper.
Our aim in this paper has been to advance in the study of selection pressure
in cellular EAs. Future work will consider more suitable functions for ﬁtting the
experimental curves (maybe a quadratic one). We will consider as well further
extensions of the results to other aspects inﬂuencing the selection pressure of
cellular EAs, such as the relationship existing between the neighborhood and
the grid topology. Also, extensions to dimensions larger than two are being con-
sidered, as well as the application of cellular EAs to several hard problems. Our
global aim is to obtain a body of knowledge of these algorithms, especially with
regard to the numerical eﬃciency of the search.

966
M. Giacobini, E. Alba, and M. Tomassini
Acknowledgments. The authors would like to acknowledge useful discussions
on the models with A. Tettamanzi. The second author wishes to thank founding
from the Spanish “Ministerio de Ciencia y Tecnolog´ıa” and FEDER through
contract TIC2002-04498-C05-02 (the TRACER project).
References
1. E. Alba, M. Giacobini, and M. Tomassini.
Comparing synchronous and asyn-
chronous cellular genetic algorithms. In J. J. Merelo et al., editor, Parallel Prob-
lem Solving from Nature - PPSN VII, volume 2439 of Lecture Notes in Computer
Science, pages 601–610. Springer-Verlag, Heidelberg, 2002.
2. E. Alba and J. M. Troya.
Cellular evolutionary algorithms: Evaluating the in-
ﬂuence of ratio. In M. Schoenauer et al., editor, Parallel Problem Solving from
Nature, PPSN VI, volume 1917 of Lecture Notes in Computer Science, pages 29–
38. Springer-Verlag, 2000.
3. D. E. Goldberg and K. Deb. A comparative analysis of selection schemes used in
genetic algorithms. In G. J. E. Rawlins, editor, Foundations of Genetic Algorithms,
pages 69–93. Morgan Kaufmann, 1991.
4. M. Gorges-Schleuter. ASPARAGOS an asynchronous parallel genetic optimisation
strategy. In J. D. Schaﬀer, editor, Proceedings of the Third International Confer-
ence on Genetic Algorithms, pages 422–427. Morgan Kaufmann, 1989.
5. M. Gorges-Schleuter.
An analysis of local selection in evolution strategies.
In
Genetic and evolutionary conference, GECCO99, volume 1, pages 847–854. Morgan
Kaufmann, San Francisco, CA, 1999.
6. B. Manderick and P. Spiessens. Fine-grained parallel genetic algorithms. In J. D.
Schaﬀer, editor, Proceedings of the Third International Conference on Genetic Al-
gorithms, pages 428–433. Morgan Kaufmann, 1989.
7. J. Sarma and K. A. De Jong. An analysis of the eﬀect of the neighborhood size and
shape on local selection algorithms. In H. M. Voigt, W. Ebeling, I. Rechenberg,
and H. P. Schwefel, editors, Parallel Problem Solving from Nature (PPSN IV), vol-
ume 1141 of Lecture Notes in Computer Science, pages 236–244. Springer-Verlag,
Heidelberg, 1996.
8. J. Sarma and K. A. De Jong. An analysis of local selection algorithms in a spa-
tially structured evolutionary algorithm. In T. B¨ack, editor, Proceedings of the
Seventh International Conference on Genetic Algorithms, pages 181–186. Morgan
Kaufmann, 1997.
9. B. Sch¨onﬁsch and A. de Roos. Synchronous and asynchronous updating in cellular
automata. BioSystems, 51:123–143, 1999.
10. M. Sipper, M. Tomassini, and M. S. Capcarrere. Evolving asynchronous and scal-
able non-uniform cellular automata. In Proceedings of International Conference on
Artiﬁcial Neural Networks and Genetic Algorithms (ICANNGA97), pages 67–71.
Springer-Verlag KG, Vienna, 1998.
11. M. Tomassini. The parallel genetic cellular automata: Application to global func-
tion optimization.
In R. F. Albrecht, C. R. Reeves, and N. C. Steele, editors,
Proceedings of the International Conference on Artiﬁcial Neural Networks and Ge-
netic Algorithms, pages 385–391. Springer-Verlag, 1993.
12. P. F. Verhulst. Mem. Acad. Roy. Bruxelles, 28(1), 1844.
13. D. Whitley. Cellular genetic algorithms. In S. Forrest, editor, Proceedings of the
Fifth International Conference on Genetic Algorithms, page 658. Morgan Kauf-
mann Publishers, San Mateo, California, 1993.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 967–978, 2003.
© Springer-Verlag Berlin Heidelberg 2003
A Case for Codons in Evolutionary Algorithms
Joshua Gilbert and Maggie Eppstein
Dept. of Computer Science, University of Vermont,
Burlington VT, 05405 USA.
{jgilbert,eppstein}@emba.uvm.edu
Abstract. A new method is developed for representation and encoding in
population-based evolutionary algorithms. The method is inspired by the
biological genetic code and utilizes a many-to-one, codon-based, genotype-to-
phenotype translation scheme. A genetic algorithm was implemented with this
codon-based representation using three different codon translation tables, each
with different characteristics. A standard genetic algorithm is compared to the
codon-based genetic algorithms on two difficult search problems; a dynamic
knapsack problem and a static problem involving many suboptima. Results on
these two problems indicate that the codon-based representation may promote
rapid adaptation to changing environments and the ability to find global minima
in highly non-convex problems.
1   Introduction
The design and implementation of evolutionary algorithms as a means of
computational optimization have been inspired by biological evolution through
natural selection. It is generally recognized that there are several important aspects
that contribute to the performance of a genetic algorithm, including the underlying
data representation, method of selection, genetic operators (e.g., mutation and
recombination), fitness evaluation, population structure, and the nature of the problem
being solved [1]. While the choices of these aspects are not independent of each other,
the purpose of this work is to explore a new method for data representation, motivated
by the genetic code. Herein we use the term “genotype” to mean the data upon which
the genetic operators are applied, and the term “phenotype” to mean the data that is
evaluated in the fitness function.
1.1   Genotypic and Phenotypic Data Representation in Evolutionary Algorithms
Many optimization problems are of the form F(p)=fitness: PP np→ RR, where we attempt
to find the optimal phenotype vector p, of length np, that optimizes the fitness
function F, possibly subject to some additional constraints. In discrete search
problems, the phenotype alphabet  may be the set of integers II, or some subset
thereof.  In continuous optimization problems, the phenotype alphabet PP may be the
set of real numbers RR, or some subset thereof. In evolutionary algorithms, the
phenotype p∈ PP
 np may be represented by a genotype g∈ GG
 ng, of length ng, such that

968         J. Gilbert and M. Eppstein
there is an encoding γ(g)=p: GG
 ng→ PP
 np.  For example, in “simple” genetic algorithms
the genotype is represented as a binary string, where each ng/np successive bits are
translated to the phenotypic alphabet using a standard binary encoding or Gray
encoding function [1, 2], although recently there has been a trend towards use of
strings of integers or real numbers directly for both genotype and phenotype (i.e., γ is
trivially the identity function), as has been the case all along in evolution strategies
[1]. Conceptually, there is nothing preventing the implementation of evolution
strategies with a nontrivial encoding function γ between genotype and phenotype.
Regardless of whether there is a trivial or non-trivial encoding function, the key point
here is that the mapping from genotype to phenotype is almost always one-to-one.
There are some interesting exceptions to this generalization, however, and where
redundant encodings have been used they have often been shown to offer advantages.
In “messy GAs” [3] the adjacencies between alleles in the variable-length genotype
are explicitly stored with index numbers; overdetermined strings are disambiguated
using a conflict-resolution mechanism, and are thus an implementation of a
dynamically changing many-to-one mapping. However, the emphasis in messy GAs,
which have been shown to perform well on deceptive problems, has been on their
ability to overcome linkage problems via the explicit adjacency representation rather
than on the potential benefits offered by the redundancy. Diploidy and dominance [4]
offer another way to effect a dynamic many-to-one mapping between genome and
phenome, and have been shown to improve performance in some dynamic
optimization problems. Similarly, gene “regulation” can be used to turn various genes
on and off and thereby maintain hidden diversity, enabling rapid adaptation to
dynamic changes in the fitness landscape [5].
Given that evolutionary computation algorithms were inspired by biological
evolution, we find it somewhat surprising that we could find few references in the
literature to implementations that adopted a more biologically motivated encoding
between genotype and phenotype. Banzhaf and colleagues [6-8] are a rare exception
to this, and have shown that they can improve results in genetic programming by
adopting a partially redundant genotype-to-phenotype encoding, loosely modeled on
the genetic code, to map binary strings to symbols in a genetic programming
application. We briefly discuss biological encoding in the next section, and then
address how we  have adapted this for use in evolutionary algorithms.
1.2   Genotypic and Phenotypic Data Representation in Biological Systems.
In DNA, there is a 4-letter alphabet at the genotype level GG ={a, g, c, t}, representing
the four nucleotide bases (adenine, guanine, cytosine, and thymine).  A string of DNA
bases is translated into a protein, or string of amino acids, using a codon triplet
translation scheme (i.e. each sequence of 3 bases within the coding portions of genes
is translated into 1 amino acid, or terminates the amino acid sequence).  Mutation and
recombination occur at the genotype level on the DNA strings, whereas the translated
sequences of amino acids are the proteins that comprise the basis of the phenotype
upon which selection acts.  Since there are 4
3=64 possible codon values, but only 20
amino acids (plus the stop codon), this is a redundant coding scheme (64 to 21) with
some distinct codons mapping to the same amino acid, as shown in Table 1.

A Case for Codons in Evolutionary Algorithms         969
Table 1. The “universal” codon translation table from codon triplets to the 20 amino acids
(shown by their standard 3-letter abbreviations) and the stop codon [9].  Here, the three
dimensions of the table have been compressed to two
T
C
A
G
Phe
Ser
Tyr
Cys
T
Phe
Ser
Tyr
Cys
C
Leu
Ser
Stop
Stop
A
Leu
Ser
Stop
Trp
G
Leu
Pro
His
Arg
T
Leu
Pro
His
Arg
C
Leu
Pro
Gln
Arg
A
Leu
Pro
Gln
Arg
G
Ile
Thr
Asn
Ser
T
Ile
Thr
Asn
Ser
C
Ile
Thr
Lys
Arg
A
Met
Thr
Lys
Arg
G
Val
Ala
Asp
Gly
T
Val
Ala
Asp
Gly
C
Val
Ala
Glu
Gly
A
Val
Ala
Glu
Gly
G
T
C
A
G
1st codon position
2nd codon position
3rd codon position
3rd codon position
3rd codon position
3rd codon position
A quick examination of Table 1 reveals that the organization of the genetic code is
non-random, with most of the redundancy occurring in the 3
rd codon position.  Crick
[10] has proposed, in his “wobble hypothesis”, that this may be due to biochemical
limitations that cause decoding non-specificity by transfer RNA.  However, statistical
arguments have also been used to show that the code is “optimal” in that it minimizes
errors, since erroneous codons either code for the same amino acid or to ones with
similar hydrophobicity, therefore minimizing the phenotypic impact of single
nucleotide polymorphisms [11]. More recently, Freeland [12] proposed that the
redundant genetic code may actually act to maximize the rate of adaptive evolution to
changing environments. Doubtless there are biochemical constraints that have
strongly influenced why life as we know it has evolved with a base alphabet of 4, a
codon size of 3, and this particular redundant codon mapping to the 21 possible
phenotypic outcomes.  Nonetheless there is, biochemically speaking, room for at least
some variability.  For example, there is a growing body of information regarding the
existence of non-standard genetic codes in vertebrate mitochondria as well as in
nuclear DNA in a wide range of organisms [13]. Even the size of the basic alphabet is
potentially variable [14-15]. Given the highly conserved nature of the redundant
codon-based genetic code, despite its demonstrated evolvability, we are motivated to
explore the potential advantages such an encoding may have to offer in evolutionary
search.
Certainly, the implementation of a codon-based representation affects the possible
schemata [1] that can be manifested. Over the past decade, the potential information
theoretic benefits of a codon-based representation have begun to receive some
attention in the literature [6-8, 12, 16]. Using a Fourier analysis, Kargupta [16]
demonstrates that some, but not all, codon-like translation schemes can enable

970         J. Gilbert and M. Eppstein
polynomial time approximations of some exponentially hard problems.  However, his
analysis is so far restricted to binary alphabets, and does not address the more general
problem of how to optimally construct a codon translation scheme for arbitrary
phenotypic alphabets and arbitrary problems.  Freeland [12] argues that the structure
of the redundant genetic code maximizes the probability that mutations will be
favorable in dynamic fitness landscapes.  The empirical studies by Banzhaf’s group
[7-8] found that a binary-to-symbolic codon mapping scheme in a genetic program
improved search performance over a traditional genetic program in which genotype
and phenotype were the same.  These studies are intriguing and indicate the need for
more comprehensive research in this area.  In this manuscript we report on
preliminary studies in codon-based representations in genetic algorithms.
2   Methods
We restricted our test problems to those with phenotype alphabet size of 21 distinct
values, as in the biological phenotype. In what we will refer to as the “non-codon
GA”, the alleles in each chromosome consisted of 21 distinct integers.  In the “codon
GA” we continued with the biological metaphor and restricted the alleles in each
genotypic chromosome to an alphabet size of 4 and a codon size of 3 (each three
adjacent alleles were logically associated into a codon triplet).  Thus, the codon-based
chromosomes had three times as many entries as the non-codon based chromosomes,
although the alphabet size was smaller (i.e., with packing, memory requirements
could be made similar).  In both cases, mutation and recombination occurred on these
chromosomes of genotypic alleles.   Mutation step size for each allele in the genotype,
for the experiments reported here, followed a negative exponential distribution, with a
range of mutation step size of 0..1 for the codon-GA genotype alphabet {1..4} and a
range of 0..2 for the non-codon GA genotype alphabet {0..20} (we determined these
values through experimentation with a variety of mutation step sizes and selected the
values that generated the best performance for each type of GA on our test problems).
Single-point crossover was employed for recombination, and elitism was
implemented such that the best individual from each generation was included in the
population for the next generation.
In the non-codon GA, the genotype doubled as the phenotype; i.e., the modified
chromosomes were directly input into the objective function for fitness calculation
(Figure 1a).  In the codon GA, each codon (sequence of three alleles) was first
mapped to one of the 21 possible phenotype values via a redundant codon translation
table to yield the phenotype (sequence of values each in range 0..20); the phenotype
was then input to the objective function for fitness calculation (Figure 1b).
Selection was performed using stochastic universal sampling [1].  For each
problem, we ran 200 replicates (i.e., we used 200 distinct starting seeds for the
random number generator) of the non-codon GA and the codon GA with three distinct
codon translation tables (see Section 2.2), where each replicate had a population size
of 300 individuals (chromosomes).  For the results shown here, each replicate was
allowed to run for 100 generations.  Monte Carlo replicates of the non-codon GA and
each codon GA were paired, in that the same set of 200 starting seeds for the random
number generator was used for the non-codon and codon versions, and the paired
individuals started with the same fitness.  This latter was accomplished by first

A Case for Codons in Evolutionary Algorithms         971
generating random populations of individuals for the non-codon GA, then
stochastically decoding them.  I.e., using the codon table in reverse, we stochastically
selected one of the matching codon triplet values for each corresponding phenotypic
value, in order to ensure initial genotypes that yielded the same initial phenotypes as
in the non-codon GA.
Fig. 1. Schematic of our implementation of data representation in a) a traditional non-codon
genetic algorithm, and b) genetic algorithms using codon translation tables to separate
phenotype from genotype
2.2 Creation of Codon Translation Tables
We defined two indices to characterize codon tables. The “similarity index” is a
measure of how similar the codon triplets are within the various semantic “groups”,
where a semantic group is defined as the set of codon values that translate into the
same phenotypic value. In a codon triplet C, two distinct codon values (C
i and C
j )can
share allele values at 0, 1, or 2 positions; we use the number of shared allelic values,
at each of the 3 codon allele positions k, between codons in a group as a measure of
similarity within the group. The sum (Sg) of the maximum possible number of shared
alleles between any two pairs of codons in a group g with a given number of members
ng can be calculated based on the size of the genetic allele alphabet (e.g., 4), and the
size of a codon (e.g., 3), and is used to normalize the index. The similarity index
(Simt) of a codon mapping table t with m groups (e.g., 21) is then defined as follows:

972         J. Gilbert and M. Eppstein
1..
1:
1
1:
1:3
1..
1, if 
 
, where 
 means 
0, otherwise  
g
g
i
j
k
k
i
j
g
m
i
n
j i
n
k
i
j
k
k
t
k
k
g
g
m
C
C
C
C
Sim
C
C
S
=
=
−
= +
=
=


≡





=


=
≡


∑
∑∑∑
∑
(1)
Similarity indices thus range from a maximum of 1.0 (each codon group is as similar
as possible) to a minimum of 0.0 (all pairs of codons in each codon group share no
common alleles).
We also define an “adjacency index” for codon mapping tables. A phenotypic
value is considered adjacent to another phenotypic value if it can be reached by a
single base mutation in the genotype. The adjacency index (Adjt) of a codon table t
with m groups is defined as the sum, over all groups g, of the number of unique
phenotypic values (not including the phentoype for the group itself) that can be
reached by any single allele change in any codon value in the group. Higher
adjacency indices thus imply that more phenotypic values are adjacent (i.e. they
define the average number of modes in the multi-modal mutation step size
distribution, not counting the mode at zero). Unlike the similarity index, the adjacency
index is non-normalized.
Fig. 2. A graphical representation of the universal biological codon translation table shown in
Table 1. Each group of codon triplets that translate into the same phenotypic value are
connected by lines depicting the number of base values (out of the three positions) that are the
same between any two codon values in the group; solid lines mean they share two base in
common, dashed lines mean they share one base in common, and dotted lines mean they share
no bases
We can depict the biological codon table shown in Table 1 as a 3-D graph, with
potential allelic values for each of the three codon positions plotted on a different axis
(Figure 2).  Here, we have used lines to connect codon values (nodes in the graph) for
every two pairs of codons in each of the same semantic group; solid lines denote 2

A Case for Codons in Evolutionary Algorithms         973
shared alleles, dashed lines denote 1 shared allele, and dotted lines denote 0 shared
alleles.  The biological codon table has a high similarity index Simbiological=0.92 (most of
the codons in each group share the same first two allelic values, as reflected by all the
vertical lines), but a relatively low adjacency index Adjbiological=8.10, suggesting that
perhaps evolution has acted to maximize similarity but minimize adjacency.
In the biological codon table, semantic group sizes range from 1 to 6, reflecting the
unequal frequencies of the various amino acids and the stop codon.  However, if we
are to adapt the codon translation idea to unbiased evolutionary algorithms designed
to solve arbitrary black box optimization problems, then we desire equal a priori
probabilities of all possible phenotypic values (i.e., all semantic groups should be as
close to the same size as possible). For these initial studies, we selected problems with
the same alphabet sizes as in the biological problem. Thus, within the constraints of
mapping the 64 possible codon values onto 21 phenotypic values, we chose to create
20 codon group sizes of 3 and one codon group size of 4. We created 3 distinct
mapping tables to try with our codon GA, each with different characteristics of
similarity and adjacency.  The first table, HL, has High (maximum) similarity and
Low adjacency, similar to the characteristics of the biological codon table (Figure 3a).
The second table, HM, has High (maximum) similarity and Medium high adjacency
(Figure 3b). The third table, LH, has Low similarity (the minimum possible, given
that there is one codon group size of 4) and High adjacency (Figure 3c). The
similarity and adjacency indices of these three tables, and of the biological table, are
given in Table 2. In this study, we have not yet accounted for the phenotypic nearness
of adjacent phenotypes in the codon translation table; phenotypes were arbitrarily
assigned to codon groups.
Fig. 3.  Graphical depictions of the 3 codon translation tables used in this study. a) HL, b) HM,
and c) LH.  Meanings of linetypes are as described in Figure 2
2.3  Test Problems
In these preliminary studies, we tested the non-codon GA and the codon GA (using
each of the three codon translation tables) on two arbitrarily selected difficult test
problems: 1) Dynamic Knapsack, and 2) Indecisive.
2.3.1   Dynamic Knapsack
The objective in the “Dynamic Knapsack” problem is to fill a knapsack with items, so
as to maximize the sum of the values of the items in the knapsack, without violating a

974         J. Gilbert and M. Eppstein
weight constraint. (This was converted to a minimization problem by inverting the
sum of values; a penalty term was used to enforce the weight constraint.) We
arbitrarily specified that there were 5 distinct object types with randomly assigned
real values and weights. We allowed between 0 and 20 of each item type placed in the
knapsack. The phenotype for each individual was thus a sequence of 5 integers, each
in the range 0..20.  The problem was made dynamic by alternating the weight
constraint between 100 and 400 each 5 generations.  The true optimal solution was
determined for each random set of values and weights, and for each weight constraint,
by exhaustive search, for comparison to evolved solutions.
Table 2. Similarity and adjacency indices of the biological codon table and the three artificial
codon tables used in these experiments
2.3.2  Indecisive
The phenotype for the “Indecisive” problem is again a sequence of integer values,
each in the range 0..20.  The results shown here are for sequences of length 10.  The
objective function is computed as follows: 1) count the frequency of occurrence of
each value, 0 through 20.  2) Subtract 1 from all frequency counts except for the count
for one arbitrarily chosen (but fixed) number.  3) Take the maximum of the resulting
modified counts. 4) Invert this value to turn this into a minimization problem.  Note
that Indecisive is a difficult search problem in that it has a multi-modal search space
with many attractive local suboptima, but only one global optimum, all located at
maximum Hamming distances from one another.
3   Results
3.1 Dynamic Knapsack
The results for Dynamic Knapsack, on one set of representative random object
weights and values, are plotted in Figures 4 and 5. In all cases the GAs exhibited
rapid adaptation to each of the individual weight constraints over each series of 5
generations for which the same weight constraint applied.  Since all solutions found
that satisfied the weight constraint of 100 were also feasible solutions at the weight
constraint of 400, the transition from weight constraint of 100 to 400 was relatively
easily accommodated. However, since the converse is not true, the transition from
weight constraint of 400 to 100 caused a large increase in objective function value
and this “overshoot” was notably worse for the non-codon GA than for the codon GA,
and continued to get worse with each cycle (Figure 4). On the other hand, overshoot
for all three codon GAs was much lower than for the non-codon GA (note the log
scale in Figure 4), and tended to stay constant or even reduce (e.g., when using the

A Case for Codons in Evolutionary Algorithms         975
codon translation table LH) over successive cycles. Because the scale differences in
these results make comparisons difficult to see in Figure 4, we present a different
view of the data in Figure 5. In Figure 5a, we show the objective function values at
the end of each period with weight constraint of 100; here, it is apparent that the non-
codon GA is not able to recover quickly enough from the increasing overshoot in
order to adapt well to the periodic weight constraint. A close-up view of this same
data is plotted in Figure 5b, to enable discrimination between the codon GA using the
three tables.  All three codon tables gave good results and in contrast to the non-codon
GA they all continued to improve their adaptation to the weight constraint of 100 over
the periodic cycles.  It is of interest that codon table HL had the highest overshoot
(i.e., greatest variability in solution quality) (Figure 4) but evolved the lowest
objective function values at the ends of each 5 successive generations at weight
constraint 100. In Figure 5c we show the objective function values at the end of each
period with weight constraint of 400; in this case, all the GAs did well and the
performance over successive periodic cycles was fairly constant, with the non-codon
GA showing slightly better adaptation to the higher weight constraint than the codon
GAs.
Fig. 4. a) Average over 200 replicates of the objective function values for the best individual in
a population of 300 individuals for both codon and non-codon GAs on the Dynamic Knapsack
problem. The symbols indicated the step function representing the true global optima under the
two different weight constraints. Note the log scale. b) Close up of the last cycle with weight
constraint 100.

976         J. Gilbert and M. Eppstein
3.2   Indecisive
The results on Indecisive are shown in Figure 6, where there is a dramatic difference
in performance between the non-codon GA, which never found the global optimum
within 100 generations, and the codon GAs, which always found the global optimum
Fig. 5. Three views of the data plotted in Figure 4.  a) The objective function value at the end of
each interval with a weight constraint of 100. b) Close-up of data plotted in a. c) The objective
function value at the end of each interval with a weight constraint of 400
rapidly. In this problem, the codon GA using codon table HL consistently found the
global optimum the most rapidly.
4   Conclusions
Our preliminary results imply that, at least for some problems, a redundant codon-
based representation positively affects the ability of a genetic algorithm to a) rapidly
adapt to dynamic changes in the fitness function and b) find a global optimum in a
difficult multi-modal search space. The optimal choice of how to organize the codon
translation table for a cyclical fitness landscape appears to depend on whether it is
more important to minimize the variability of solution quality (low similarity, high
adjacency) or the evolution of the best solution quality over time (high similarity, low
adjacency).  In the static problem we tested, the codon translation table with high

A Case for Codons in Evolutionary Algorithms         977
Fig. 6. Average, over 200 replicates, of the objective function values for the best individual in a
population of 300 individuals, for both codon and non-codon GAs on the Indecisive problem.
Values in the legend indicate the number of times the global optimum was found out of the 200
replicates, and the number of generations required to find it is shown as mean ± standard
deviation
similarity and low adjacency yielded both lowest average values and variances of
number of generations to find the single global minimum. Interestingly, high
similarity and low adjacency are the characteristics that have evoloved in the
biological codon table. Much more work in this area is warranted to further
understand the mechanisms by which redudant codon-based representations enhance
search, which evolutionary algorithms and application problems can benefit from this
approach, and how to best select the fundamental parameters such as the size of the
codon, the size of the genotypic alphabet at each codon position, and the internal
organization of the codon translation table. In addition, it will be important to explore
whether the effects of redudant codon translation schemes are complementary,
synergystic, or alternatives to the benefits offered by other types of redundant data
reprentations in evolutionary algorithms.
Acknowledgement. This work was supported in part by DE-FG02-00ER45828
awarded by the US Department of Energy through its EPSCoR Program.
References
1.
Bäck T. Fogel, D.B., and Michalewicz, T. (eds): Evolutionary Computation 1: Basic
Algorithms and Operators, Inst. of Physics Publ., Bristol, UK (2001)

978         J. Gilbert and M. Eppstein
2.
Rothlauf, 
F.: 
Binary 
representations 
of 
integers 
and 
the 
performance 
of
selectorcombinative genetic algorithms. In: Proceedings of the Genetic and Evolutionary
Computation Conference.  Morgan Kaufmann, San Francisco, California (2002) 695
3.
Goldberg, D.E., Deb, K., and Korb, B.: Don’t worry, be messy. In: R.K. Belew and L.B
Booker, (eds.): Proc. 4th Intl. Conf. On Genetic Algorithms. Morgan Kaufman, San Mateo,
CA (1991) 24–30
4.
Goldberg, D.E., and Smith, R.E.: Nonstationary function optimization using genetic
algorithms with dominance and diploidy. In J.J. Grefenstette, (ed.): 2nd Intl. Conf. On
Genetic Algorithms. Lawrence Erlbaum Assoc. (1987) 59–68
5.
Dasgupta, D.: Incorporating redundancy and gene activation mechanisms in genetic search
for adapting to non-stationary environments. In: L. Chambers, (ed.), Practical Handbook of
Genetic Algorithms: New Frontiers (Vol II).  CRC Press, (1995) 303–316
6.
Banzhaf, W.: Genotype-Phenotype-Mapping and Neutral Variation – A case study in
Genetic Programming. In: Y. Davidor, H.-P. Schwefel and R. Männer, (eds.) Proc. Parallel
Problem Solving from Nature III, Lecture Notes in Computer Science (1994) 322–332
7.
Keller, R., and Banzhaf, W.: Genetic Programming using Genotype-Phenotype Mapping
from Linear Genomes into Linear Phenotypes. In: J. Koza, D. Goldberg, D. Fogel, R.
Riolo, (eds.) Proc. 1st annual Conf. on Genetic Programming (GP-96).  MIT Press,
Cambridge, MA (1996) 116–122
8.
Banzhaf, W.: The Evolution of Genetic Code in Genetic Programming. In: W. Banzhaf, J.
Daida, A. Eiben, M. Garzon, V. Honavar, M. Jakiela and R. Smith (eds.) Proc. of first
GECCO conference, Morgan Kaufmann, San Francisco. (1999) 1077–1082
9.
Creighton, T.E.: Proteins: Structures and Molecular Properties, 2
nd Ed., W.H. Freeman and
Co., New York (1993)
10. Crick, F.H.C.: Codon-anticodon pairing: the wobble hypothesis. J. Mol. Biol., 19 (1966)
548–555
11. Freeland, S.J. and Hurst, L.D.: The genetic code is one in a million. J. Mol. Evol., 47
(1998) 238–248
12. Freeland, S.J.: The Darwinian genetic code: an adaptation for adapting? Genetic
Programming and Evolvable Machines, 3 (2002) 113–127
13. Knight, R.D., Freeland, S.J., and Landweber, L.F.: Rewiring the keyboard: evolability of
the genetic code. Nature Reviews: Genetics, 2 (2001) 49–58
14. Szathmáry, E.: What is the optimum size for the genetic alphabet? Proc. Natl. Acad. Sci.,
USA, 89 (1992) 2614–2618
15. Piccirilli, J.A., Karuch, T., Moroney, S.E., and Benner, S.A.: Enzymatic incorporation of a
new base pair into DNA and RNA extends the genetic alphabet. Nature 343 (1990) 33–37
16. Kargupta, H.: A striking property of genetic code-like transformations. Complex Systems,
13 (2001) 1–32

Natural Coding: A More Eﬃcient
Representation for Evolutionary Learning⋆
Ra´ul Gir´aldez, Jes´us S. Aguilar-Ruiz, and Jos´e C. Riquelme
Department of Computer Science, University of Seville
Avenida Reina Mercedes s/n, 41012 Sevilla, Spain
{giraldez,aguilar,riquelme}@lsi.us.es
Abstract. To select an adequate coding is one of the main problems
in applications based on Evolutionary Algorithms. Many codings have
been proposed to represent the search space for obtaining decision rules.
A suitable representation of the individuals of the genetic population
can reduce the search space, so that the learning process is accelerated
by decreasing the number of necessary generations to complete the task.
In this sense, natural coding achieves such reduction and improves the
results obtained by other codings. This paper justiﬁes the use of natural
coding by comparing it with hybrid coding that joins well-known
binary and real representations. We have tested both codings on a
heterogeneous subset of databases from the UCI Machine Learning
Repository. The experiments’ results show that natural coding improves
the quality of the obtained knowledge-model using only one third of
the generations that hybrid coding needs as well as a smaller population.
Keywords: Evolutionary Algorithms, Coding, Supervised Learning
1
Introduction
Machine Learning is used when we want to build a knowledge model from a
training dataset and predict the outcome of a new unseen instance. Normally,
the instances of the dataset are named examples and each example is formed by
attributes. These attributes can be classiﬁed by diﬀerent criteria according to the
kind of values that they take. The most common classiﬁcation is in continuous
and discrete attributes. If an attribute takes values in a real and inﬁnite domain,
we say that it is continuous. On the contrary, if the domain is a ﬁnite set of values,
we name it as a discrete attribute. Usually, one of these attributes is selected as
a decision attribute or class. Thus, the knowledge model is built to predict the
value that the class takes in new examples depending on the other attributes
values. When the class of the training data is known (labeled dataset), we work
in the Supervised Learning ﬁeld.
⋆This research was supported by the Spanish Research Agency CICYT under grant
TIC2001-1143-C03-02.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 979–990, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

980
R. Gir´aldez, J.S. Aguilar-Ruiz, and J.C. Riquelme
The knowledge model can be represented by diﬀerent structures: decision
rules, association rules, decision trees, etc. In Supervised Learning, the struc-
tures generated are usually decision rules or trees. The databases have a set of
attributes or characteristics that deﬁne each example. A decision rule establishes
conditions which the examples must fulﬁl in order to be classiﬁed by this rule.
These conditions aﬀect the values that the attributes can take in that if the
attributes of an example meet all the conditions that a rule establishes then we
say that the rule covers the example, independent of whether it classiﬁes such
example correctly or not. The usual representation of rules generated by learning
systems is shown in Figure 1.
If Cond1 and Cond2 and. . . and CondM then Class = C
Fig. 1. Decision Rule.
where Condi is the condition that the ith attribute of an example has to satisfy
in order to be classiﬁed with class C, and M is the number of attributes in the
database. Logically, the case can arise where an attribute does not appear in the
rule, from which we assume that the condition concerning the attribute is always
evaluated as true. When an attribute (ai) is continuous, the condition (Condi)
takes the form ai ∈[li, ui], restricting the range of values of the attribute to the
interval deﬁned by the lower (li) and the upper (ui) bound. On the other hand,
when the attribute is discrete, the condition takes the form ai ∈{v1, v2, . . . , vk},
where the values {v1, v2, . . . , vk} are not necessarily all those the attribute can
take.
In literature, there are many methods that acquire the inherent knowledge
from a labeled dataset and generate a structure that represents it: CN2 [6,7],
AQ-based systems [13],OC1 [15], C4.5 [16], SEE5.0 [17], SIA [20], among oth-
ers. This work is focused on those methods that apply Evolutionary Algorithms
(henceforth EA) in the training phase, and speciﬁcally in the tool Hider (Hi-
erarchical Decision Rules) [2]. This tool generates a set of hierarchical decision
rules by applying an EA whose population is a set of encoded rules. Thus, each
individual is a decision rule where each attribute is coded by one or several
genes. Diﬀerent versions of Hider have used diﬀerent codings for the individu-
als of population: binary, real and hybrid. This work is focused on a new version
of this tool, called Hider*, which applies the natural coding [1] to represent the
individuals. Thus, we compare the two last versions, Hider and Hider*, that
use the hybrid and natural coding respectively. Both codings can treat contin-
uous and discrete attributes. Hybrid coding applies binary coding for discrete
attributes and real coding for continuous. Natural is an original coding that rep-
resents both kinds of attributes by means of a simple natural number. As it is
explained later, the main advantage of natural coding is the reduction of the
search space.
Our goal in this paper is to present the advantages of natural coding against
hybrid coding. Hider and Hider* were run and their performances measured.
The results show that Hider* ﬁnds solutions before Hider, since natural coding

Natural Coding: A More Eﬃcient Representation for Evolutionary Learning
981
reduces the number of candidate solutions. Thus, Hider* can be run with fewer
generations and with a smaller population size than Hider.
The rest of this paper is organized into the following: Section 2 describes the
EA used by Hider and Hider*. The characteristics of hybrid coding and natural
coding are presented in Sections 3 and 4 respectively. The experimental results
of our research are presented in Section 5. Finally, in Section 6, the conclusions
of this work are summarized.
2
Algorithm
Hider is a tool that produces a hierarchical set of rules. When a new example
needs to be classiﬁed, the set of rules is sequentially evaluated according to its
hierarchy, so if the example does not fulﬁl a rule, the next one in the hierarchy
order is evaluated. This process is repeated until the example matches every
condition of a rule and is classiﬁed with the class that such rule establishes.
Hider uses an EA to search for the best solutions. Since the aim is to obtain
a set of decision rules, the population of the EA is formed by some possible
solutions. Each genetic individual is a rule that evolves by applying the muta-
tion and crossover operators. In each generation, some individuals are selected
according to their goodness of ﬁt and they are included in the next population
along with their oﬀspring. Thus, the performance of the tool is inﬂuenced by
two factors: the ﬁtness function and the coding. The ﬁtness function used in this
research is
f(r) = N −CE(r) + G(r) + coverage(r)
(1)
where r is an individual; N is the number of examples being processed; CE(r)
is the class error, i.e. the number of examples belonging to the region deﬁned by
the rule r but they do not have the same class; G(r) is the number of examples
correctly classiﬁed by r; and coverage(r) gives the proportion of the search space
covered by the rule. This ﬁtness function is described in detail in [2]. The goal of
our work is focused on the coding, so the another factor is not analyzed in detail.
In fact, Hider and Hider* use the same EA and the same ﬁtness function. The
diﬀerence between both is in the coding. Hider applies hybrid coding, whereas
Hider* uses natural coding.
The pseudocode of Hider is shown in Figure 2. The main algorithm is a
typical sequential covering method [14], where the algorithmic function that
produces the rules is an EA. Each call for this function (line 8) generates only
one rule that is inserted into the ﬁnal set of rules (line 9) and is used to eliminate
examples from the training data (line 10). The evolutionary function is started
again with the reduced training data. This loop is repeated until the set of
training data is empty.
The function EvoAlg has a set of examples as an input parameter. It returns
a rule which is the best individual of the last generation. The initial population
is built randomly by the function InitializePopulation. Some examples are ran-
domly selected and individuals that cover such examples are generated. After

982
R. Gir´aldez, J.S. Aguilar-Ruiz, and J.C. Riquelme
1 Procedure Hider
2
Input: T: File of examples (Training file)
3
Output: R: Set of rules (Sorted set)
4
begin
5
R := ∅
6
initialSize := |T |
7
while |T | > initialSize
8
r:=EvoAlg(T)
9
R:=R ⊕r
10
DeleteCoveredExamples(T,r)
11
end while
12 end Hider
13 Function EvoAlg (T: File of encoded-examples) ret(r: Rule)
14
begin
15
InitializePopulation(P)
16
For i:=1 to num generations
17
Evaluate(P)
18
next P:=SelectTheBestOf(P)
19
next P:=next P+Replicate(P)
20
next P:=next P+Recombine(P)
21
P:=next P
22
end for
23
Evaluate(P)
24
return SelectTheBestOf(P)
25 end EvoAlg
Fig. 2. Pseudocode of Hider.
initializing the population, the for-loop repeats the evolutionary process a num-
ber of times which is determined by the parameter num generations. In each
iteration, the individuals of the population are evaluated according to a deﬁned
ﬁtness function, thus each individual acquires a “goodness” (function Evaluate).
The best individual of every generation is replicated to the next one (elitism).
Later, a set of individuals are selected through the roulette wheel method and
replicated to the next generation. Finally, another set of individuals are recom-
bined and the oﬀspring is included in the next generation. The selection of these
individuals is also carried out by means of the roulette wheel. Once the loop
ﬁnishes, the best individual of the last generation is returned.
3
Hybrid Coding
After studying other EA-based approaches proposed in the bibliography, we
deduced that a combination of binary coding and real coding would be suitable
for our problem. First, we analyzed two well-known systems: GABIL [8] and
GIL [11]. These concept learners use binary coding, assigning a bit to each value
of an attribute. A value of one, in a bit, implicates that the value is present,
so that several bits could be active for the same attribute. This implies that
binary coding is a good election for encoding symbolic attributes. Nevertheless,
it is not suitable for continuous attributes, because the size of their alphabet
is very large (theoretically inﬁnite) and this aspect does not allow a complete
search. Non-binary codings have been widely researched in literature [3,4,12,

Natural Coding: A More Eﬃcient Representation for Evolutionary Learning
983
li
ui
Continuous
Attribute
Discrete
Attribute
Class
. . .
b1
b2
bk
cj
Binary Values
Real Values
Fig. 3. An hybrid individual with a continuous attribute and a discrete one.
21,22]. These jobs show that real coding is more appropriate than binary in
continuous domains.
Hybrid coding tries to proﬁt from the advantages of binary and real coding.
Thus, it mixes both codings by using the real to represent the boundaries of the
intervals in continuous domains, and the binary to encode the discrete attributes.
Figure 3 explains the representation for continuous and discrete attributes in
hybrid coding. The continuous attribute is encoded by two real values being the
lower (li) and the upper bound (ui) of the interval that the rule establishes for
such attribute. The discrete attribute is represented by a number k of bits, where
each of them denotes the presence or absence of a discrete value in the condition
of the rule. The last value is the class, which is not usually encoded. Each class
is represented as a integer, so that for a number n of classes, the value cj will
belong to the set {0, 1, 2, ..., n-1}.
With regard to the genetic operators, crossover and mutation for hybrid
coding are described in detail in [2]. The crossover for continuous attributes is
an extension of Radcliﬀe’s [18] to parents coded as intervals. If two parents, a
and b, establish the intervals [la
i , ua
i ] and [lb
i, ub
i] respectively for the attribute ai,
then the interval of the oﬀspring ([lo
i , uo
i ]) fulﬁls expressions in Equation 2. In
case of discrete attributes, the uniform crossover is carried out [19].
lo
i ∈[min(la
i , lb
i), max(la
i , lb
i)]
uo
i ∈[min(ua
i , ub
i), max(ua
i , ub
i)]
(2)
In order to apply the mutation to a continuous attribute, a gene (gi) is
randomly selected. Such gene can be the lower bound (li) or the upper (ui)
bound of the interval. The mutation consist in replacing the gene gi with gi ± δ,
where δ is the smaller HOEM (Heterogeneous Overlap-Euclidean Metric [23])
between two examples of the training dataset. Thus, an interval is expanded
(li −δ or ui + δ) or contracted (li + δ or ui −δ). When the attribute is discrete,
the mutation changes the gene (a bit) depending on the discrete-value mutation
probability, so that some values are included or excluded from the condition.
4
Natural Coding
The ideal coding must fulﬁl the following properties: completeness, coherence,
uniformity (uniqueness), simplicity, locality, consistency and minimality. To ﬁnd

984
R. Gir´aldez, J.S. Aguilar-Ruiz, and J.C. Riquelme
Table 1. Coding for a continuous attribute.
Cutpoints
2.5
3.9
4.7
5.0
6.2
1.4 1 ≡[1.4, 2.5] 2 ≡[1.4, 3.9] 3 ≡[1.4, 4.7]
4 ≡[1.4, 5.0]
5 ≡[1.4, 6.2]
2.5
−
7 ≡[2.5, 3.9] 8 ≡[2.5, 4.7]
9 ≡[2.5, 5.0] 10 ≡[2.5, 6.2]
3.9
−
−
13 ≡[3.9, 4.7] 14 ≡[3.9, 5.0] 15 ≡[3.9, 6.2]
4.7
−
−
−
19 ≡[4.7, 5.0] 20 ≡[4.7, 6.2]
5.0
−
−
−
−
25 ≡[5.0, 6.2]
a suitable representation is very diﬃcult but to ﬁnd one that fulﬁls every afore-
mentioned property is practically impossible. Thus, we try to design a coding
that keeps at least two well-known principles proposed by Goldberg in [10]: the
principle of meaningful building blocks and the principle of minimal alphabets.
After analyzing the problem, we developed a coding, named “natural” [1],
which only uses natural numbers to represent the set of values that can take
part of the decision rules, independently of the kind of attributes (continuous or
discrete). In particular, natural coding encode each condition of a rule with only
one gene that is a natural number. Thus, this coding fulﬁls two aforementioned
properties: uniqueness (every individual has an unique representation) and min-
imality (the length of coding must be as short as possible). Although continuous
and discrete values are encoded as natural numbers, the meaning of this number
is diﬀerent in both cases.
As in regards to continuous attributes, a method that calculates the cutpoints
in range of values for each continuous attribute is applied. The cutpoints are the
possible bounds of intervals that a condition can set. In principle, these cutpoints
can all be of values the attribute takes in the training dataset. However, this
choice generates too many cutpoints. In order to reduce the number of cutpoints,
we apply the method named USD [9]. This method produces a set of bounds that
maximize the goodness of the possible intervals. Such an aspect has a favorable
inﬂuence on the rules that Hider* is going to obtain later. Once the cutpoints
are calculated, a natural number is assigned to each possible combination of
bounds, so every possible interval is represented by a natural number. Table 1
shows an example of coding for a continuous attribute where the set of cutpoints
generated by USD is {1.4, 2.5, 3.9, 4.7, 5.0, 6.2}.
In general, if k is the number of cutpoints, the rows are labeled from the ﬁrst
one to (k −1)th and the column are labeled from the second one to kth cutpoint.
Thus, each element of the table is a natural number that encodes the interval
deﬁned by its row and column.
In case the attributes are discrete, the natural coding is obtained from a
binary coding similar to that used in GABIL and GIL. In decision rules, a
condition can establish a set of discrete values that the attribute must take
to classify an example. Each gene that represents a condition for a discrete
attribute is also a natural number. The binary representation of this gene is a
number where there is a bit for each diﬀerent discrete value that an attribute
can take. Thus, if Ωis the set of possible values for a discrete attribute, then
the natural number that encodes the corresponding condition of the rule is in
interval [0, 2|Ω| −1]. If a value is included in the condition, its corresponding bit

Natural Coding: A More Eﬃcient Representation for Evolutionary Learning
985
Table 2. Coding for a discrete attribute.
Discrete values
Natural
white red green blue black
Coding
0
0
0
0
0
−
0
0
0
0
1
1
0
0
0
1
0
2
0
0
0
1
1
3
...
...
...
...
...
...
0
1
0
1
1
11
...
...
...
...
...
...
1
1
1
1
0
30
1
1
1
1
1
31
is equal to 1, otherwise it is 0. The natural coding for this gene is the conversion
of the binary number into a natural one. Table 2 shows an example for a discrete
attribute with ﬁve diﬀerent values: white, red, green, blue and black.
The mutation and crossover operators are described in detail in [1]. For con-
tinuous attributes, mutation is a shift (up, down, left or right) in the coding table
(see Table 1). Such shift mutates the gene changing the bounds of the interval.
The crossover consists in calculating the intersection of the rows and columns of
both parents, so that the obtained set of natural numbers is the oﬀspring. For
discrete attributes, the mutation is a transformation of the value that encodes
the gene, by changing some bits of the binary code associated with the natural
number. The crossover is based on the mutation. Each gene that take part in
the crossover provides a set of candidates obtained from its possible mutations
joined to itself. The oﬀspring of two genes is a random selection of numbers from
the intersection of both sets of candidates. Although the natural operators seem
to be very complex processes, in fact, they are very simple algebraic operations.
They do not imply any conversion between binary and natural numbers and do
not need any operation with arrays. As shown in [1], the natural operators are
algebraic operations with a low computational cost.
Figure 4 illustrates the diﬀerences between natural and hybrid coding. It
shows two individuals that encode the same rule. The attribute AT1 is continuous
whereas AT2 is discrete. Both attributes are encoded according to Tables 1 and
2.
We can see that natural coding is simpler, since the hybrid needs eight genes
to code the rule whereas the natural encodes it with only three genes. Natural
coding minimizes the size of individuals, assigning only one gene for each at-
tribute. Hybrid uses two genes for continuous attributes and —Ω— for discrete,
where —Ω— is the number of diﬀerent values that an attribute can take.
5
Results
In order to show the quality of natural coding, we applied Hider and Hider* to
a set of databases from UCI Repository [5]. As we mentioned in Section 2, both
tools use the same EA, although Hider uses hybrid coding, to the contrary of

986
R. Gir´aldez, J.S. Aguilar-Ruiz, and J.C. Riquelme
Rule:  If  AT1   [3.9, 5.0]    and   AT 2    {red, blue, black}  Then Class 0
3.9
5.0
1
0
1
1
0
0
AT1
AT2
Class
14
11
0
AT1
AT2 Class
AT1(Continuous): [1.4, 6.2]
AT2(Discrete): {white, red, green, blue, black}
Domains
Hybrid Coding
Natural Coding
Fig. 4. Hybrid individual vs. Natural individual.
Table 3. Parameter of Hider.
Parameter
Hider (hybrid) Hider* (natural)
Population Size
100
70
Number of Generations
300
100
Replication
20%
20%
Recombination
80%
80%
Individual Mutation Probability
0.5
0.5
Gen Mutation Probability
1
∥attributes∥
1
∥attributes∥
Discrete-value Mutation Probability
1
∥values∥
1
∥values∥
Hider* which uses natural coding. Both were run with the same crossover and
mutation parameters, but with a diﬀerent number of individuals and generations.
Table 3 shows the parameters used in each case. The lower values that Hider*
needs for the population size and number of generations are due to the reduction
of the search space by natural coding. Thus, Hider* used only 100 generations
and 70 individuals to obtain similar numbers to those obtained by Hider, which
needed 300 and 100 respectively (see Table 4).
The databases used in the experiments were 16, some of which contain only
continuous attributes, others contain only discrete attributes and the remainder
include both types of attributes. Thus, we can compare the behaviour of natural
and hybrid coding with both types of attributes. To measure the performance
of each method, a 10-fold cross-validation was achieved with each database. The
values that represent the performance are the error rate (ER) and the number
of rules (NR) obtained. The ER is the average number of misclassiﬁed examples
expressed as a percentage and the NR is the average number of rules for the
10-fold cross-validation. The algorithms were run on the same training sets and
the knowledge structures tested using the same test sets, so the results were
comparable. With these conditions and the aforementioned parameters for the
EAs, the results obtained by applying both codings are given in Table 4.
In Table 4, the ﬁrst column shows the databases used in the experiments;
the next two columns give the ER and NR obtained respectively by Hider with
hybrid coding for each database. Likewise, the fourth and ﬁfth columns give
the ER and NR for Hider* with natural coding. The last two columns show a

Natural Coding: A More Eﬃcient Representation for Evolutionary Learning
987
Table 4. Comparing Hider and Hider*.
Hider (hybrid)
Hider* (natural)
Improvement
Database
ER
NR
ER
NR
ϵer
ϵnr
breast-c
4.3
2.6
4.06
2
1.06
1.3
bupa
35.7
11.3
37.35
4.2
0.96
2.69
cleve
20.5
7.9
25.33
5.9
0.81
1.34
german
29.1
13.3
27.4
8
1.06
1.66
glass
29.4
19
35.24
11.7
0.83
1.62
heart
22.3
9.2
21.85
4.3
1.02
2.14
hepatiti
19.4
4.5
16.67
3.7
1.16
1.22
horse-co
17.6
6
20
11.1
0.88
0.54
Iris
3.3
4.8
3.33
3.2
0.99
1.5
Lenses
25
6.5
25
4.5
1
1.44
mushroom
0.8
3.1
1.18
3.5
0.68
0.89
pima
25.9
16.6
25.66
5.1
1.01
3.25
vehicle
30.6
36.2
33.81
19.7
0.91
1.84
vote
6.4
4
4.42
2.2
1.45
1.82
wine
3.9
3.3
8.82
5.6
0.44
0.59
zoo
8
7.2
4
7.9
2
0.91
Average
17,64
9,72
18,38
6,41
1,02
1,55
measure of improvement for the error rate (ϵer) and the number of rules (ϵnr).
The ϵer coeﬃcient was calculated by dividing the error rate for Hider by the
corresponding error rate for Hider*. The same operation was carried out to
obtain ϵnr, but by using the number of rules for both tools. Finally, the last row
shows the average results for each column. As we can observe, Hider* does not
attain a reduction in ER for 8 out of 16 datasets. Nevertheless, on average, it
improves on Hider, although this improvement is very small (2%). As regards to
number of rules, the results are more signiﬁcant, since Hider* obtains a smaller
number of rules in 12 out of 16 cases, with an average improvement of 55%.
Although the results show that Hider* has a better performance, we must
not forget that those numbers were obtained using a smaller number of genera-
tions and individuals of a genetic population. In particular, Hider* needed one
third of the generations and fewer than three quarters of the population size
invested by Hider. The reduction of the search space that natural coding gets,
allows Hider* to obtain these results with such parameters.
In Table 5, the length of individuals of the genetic population for the hybrid
and natural coding are shown. The ﬁrst column contains the databases. The
second shows the number of continuous attributes (NC), if any, for each database.
Likewise, the next column gives the number of discrete attributes (ND) along
with the total number of diﬀerent values in brackets (NV). The column labeled as
“Hybrid” gives the length of individuals (number of genes) with hybrid coding.
Finally, the last one shows the length of individuals encoded with natural coding.
These lengths were calculated easily from the second and third column. Hybrid

988
R. Gir´aldez, J.S. Aguilar-Ruiz, and J.C. Riquelme
Table 5. Comparing length of Hybrid and Natural Individuals.
Database
NC
ND (NV)
Hybrid
Natural
breast-c
9
-
18
9
bupa
6
-
12
6
cleve
6
7 (19)
31
13
german
7
13 (54)
68
20
glass
9
-
18
9
heart
13
-
26
13
hepatiti
6
13 (26)
38
19
horse-co
7
15 (55)
69
22
Iris
4
-
8
4
Lenses
-
4 (9)
9
4
Mushroom
-
22 (117)
117
22
Pma
8
-
16
8
tic-tac-
9
-
18
9
vehicle
18
-
36
18
vote
-
16 (48)
48
16
wine
13
-
26
13
zoo
-
16 (36)
36
16
Average
34,94
13
coding uses two genes to represent continuous attributes and a number k of genes
for discrete ones, k being the number of diﬀerent values of the attribute. On the
other hand, natural coding uses only one gene for each attribute, regardless
of its type (continuous or discrete). Thus, the length for hybrid individuals is
2 × NC + NV , whereas for natural individuals is NC + NV . As we can see,
natural coding noticeably decreases the length of individuals. On average, it
obtains a reduction greater than 63% regarding the hybrid individuals.
The most important contribution of natural coding is the decrease in size
of the search space. This aspect is considered only when the dataset includes
continuous attributes. For discrete attributes, natural coding does not reduce
the space, since it is based on binary coding. Thus, if Ωd is the set of values for
a discrete attribute, then the size of the search space is 2|Ωd| for both codings.
However, hybrid coding represents continuous attributes with real coding, i.e
the genes can take values in an inﬁnite domain. Thus, when there are continu-
ous attributes, the search space with hybrid individuals is also inﬁnite. On the
contrary, genes that encode continuous attributes with natural coding can only
take values belonging to a ﬁnite set of natural numbers. Let N be the number
of cutpoints that USD algorithm obtains for an attribute ai. Let Ωc be the set
of natural numbers that the gene gi can take according to the cutpoints. Then,
the size of the search space for the attribute ai is the number of elements of Ωc,
that it is given by Equation 3.
|Ωc| =
N
2

= N(N −1)
2
(3)

Natural Coding: A More Eﬃcient Representation for Evolutionary Learning
989
Thus, the total size of the search space for a dataset with a number M of
attributes using hybrid or natural coding is
S =
M

i=1
|Ωi|
(4)
where |Ωi| is the number of possible values of the attribute ai. If there are any
continuous ai, such size is inﬁnite for hybrid coding but ﬁnite for natural coding.
6
Conclusions
In this paper, natural coding for EA-based decision rules generation is described
and tested. This coding transforms the attributes domain (continuous and dis-
crete) in a ﬁnite set of natural numbers. Such conversion allows a decrease in
size of search space in such a way that the algorithm converges more quickly.
Furthermore, natural coding encodes each attribute with only one gene, reduc-
ing considerably the length of individuals. The quality of this coding has been
tested by applying the same evolutionary learning tool (Hider) with natural and
hybrid coding and by comparing the obtained sets of rules for a set of databases
from the UCI Repository. The algorithms with hybrid coding needed 300 gener-
ations and a population with 100 individuals in order to obtain a suitable model.
Nevertheless, the use of natural coding obtained models with the same accuracy,
but a lesser number of rules. Although this is a noteworthy aspect, the most
important fact is that such results were obtained with only 70 individuals per
population and a number of generations as little as 100. The faster convergence
of the algorithm is due to the decreasing in size of search space, since natural
coding converts the inﬁnite domain of the continuous attributes to a ﬁnite set
of natural numbers.
References
1. Jesus S. Aguilar-Ruiz and J.C. Riquelme. Improving the Evolutionary Coding for
Machine Learning Tasks. European Conference on Artiﬁcial Intelligence, ECAI’02,
pp. 173–177, IOS Press, Lyon, France, 2002.
2. Jesus S. Aguilar-Ruiz, J.C. Riquelme and M. Toro. Evolutionary Learning of Hi-
erarchical Decision Rules. IEEE Transactions on Systems, Man, and Cybernetics,
Part B: Cybernetics, ISSN: 1083-4419, vol. 33, no. 2, pp. 324–331, 2003.
3. J. Antonisse. A new interpretation of schema notation that overturns the binary
encoding constraint. In: Third International Conference on Genetic Algorithms,
pp. 86–97, 1989.
4. S. Bhattacharyya and G. Koehler. An analysis of non–binary genetic algorithms
with cardinality 2v. Complex Systems 8, pp. 227–256, 1994.
5. C. Blake and E. K. Merz. UCI repository of machine learning databases, 1998.
6. P. Clark and R. Boswell. The cn2 induction algorithm. Machine Learning, vol. 3,
no. 4, pp. 261–283, 1989.

990
R. Gir´aldez, J.S. Aguilar-Ruiz, and J.C. Riquelme
7. P. Clark and R. Boswell. Rule induction with cn2: Some recents improvements.
Machine Learning: Proceedings of EWSL-91, pp. 51–163, 1991.
8. K. A. DeJong, W. M. Spears, and D. F. Gordon. Using genetic algorithms for
concept learning. Machine Learning, 1(13):161–188, 1993.
9. R. Gir´aldez, Jesus S. Aguilar-Ruiz and J.C. Riquelme. Discretization Oriented to
Decision Rule Generation International Conference on Knowledge-Based Intelli-
gent Information & Engineering Systems, KES’02 IOS Press, pp. 275–279 Septem-
ber 16–18, Crema, Italy, 2002.
10. D. E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learn-
ing. Addison-Wesley, 1989.
11. C. Z. Janikow. A knowledge-intensive genetic algorithm for supervised learning.
Machine Learning, 1(13):169–228, 1993.
12. G. Koehler, S. Bhattacharyya and M. Vose. General cardinality genetic algorithms.
Evolutionary Computation 5(4), 439–459, 1998.
13. R. S. Michalski, I. Mozetic, J. Hong, and N. Lavrac. The aq15 inductive learning
system: An overview and experiments. In Proceedings of the American Association
for Artiﬁcial intelligence Conference (AAAI), 1986.
14. T. Mitchell. Machine Learning. McGraw Hill, 1997.
15. S. K. Murthy, S. Kasif, and S. Salzberg. A system for induction of oblique decision
trees. Journal of Artiﬁcial Intelligence Research, 1994.
16. J. R. Quinlan. C4.5: Programs for Machine Learning., Morgan Kaufmann, San
Mateo, California, 1993.
17. J. R. Quinlan. See5.0 (http://www.rulequest.com), 1998–2001.
18. N. J. Radcliﬀe. Genetic Neural Networks on MIMD Computers. Ph. d., University
of Edinburgh, 1990.
19. G. Syswerda. Uniform crossover in genetic algorithms. In: Proceedings of the Third
International Conference on Genetic Algorithms. pp. 2–9, 1989.
20. G. Venturini. SIA: a supervised inductive algorithm with genetic search for learning
attributes based concepts.
In Proceedings of European Conference on Machine
Learning, pp. 281–296, 1993.
21. M. Vose and A. Wright. The simple genetic algorithm and the Walsh transform:
Part I, theory. Evolutionary Computation 6(3), pp. 253–273, 1998.
22. M. Vose and A. Wright. The simple genetic algorithm and the Walsh transform:
Part II, the inverse. Evolutionary Computation 6(3), pp. 275–289, 1998.
23. D. R. Wilson and T. R. Martinez. Improved Heterogeneous Distance Functions.
Journal of Artiﬁciall Intelligence Research 6(1), pp. 1–34, 1997.

Hybridization of Estimation of Distribution
Algorithms with a Repair Method for Solving
Constraint Satisfaction Problems
Hisashi Handa
Okayama University, Tsushima-Naka 3-1-1,
Okayama 700-8530, JAPAN,
handa@sdc.it.okayama-u.ac.jp,
http://www.sdc.it.okayama-u.ac.jp/˜handa/index-e.html
Abstract. Estimation of Distribution Algorithms (EDAs) are new
promising methods in the ﬁeld of genetic and evolutionary algorithms.
In the case of conventional Genetic and Evolutionary Algorithm studies
to apply Constraint Satisfaction Problems (CSPs), it is well-known that
the incorporation of the domain knowledge in the CSPs is quite eﬀective.
In this paper, we propose a hybridization method (memetic algorithm)
of Estimation of Distribution Algorithms with a repair method. Experi-
mental results on general CSPs tell us the eﬀectiveness of the proposed
method.
1
Introduction
As the scale and the complexity of engineering problems increases, the dis-
tributed processing approaches inspired by the constraints-oriented problem
solving methods are now receiving attentions. The notion of Constraint Sat-
isfaction Problems (CSPs) provides us general framework adaptable to a wide
variety of problems in various ﬁelds [1,2]. The CSPs are a problem class which
consists of variables and constraints on the variables. Solving problems by using
Constraint-oriented approach is that, ﬁrst, we have to describe constraints which
should satisfy between elements in the target environments. Then, we employ
“constraint satisfaction problem solver (CSP solver),” to ﬁnd satisﬁed solutions
of the described problem instance such that all constraints in the problem are
satisﬁed. Recently, CSP solvers from genetic and evolutionary algorithms have
been broadly studied by many researchers [2]-[9]. These studies showed that the
uses of the domain knowledge in the CSPs, that is, the hybridization with lo-
cal search method (repair method) based upon the notion of Min-Conﬂict Hill
Climbing (MCHC) [10], and the utilization of constraint networks [6], are quite
eﬀective.
Estimation of Distribution Algorithms (EDAs) are new promising methods
in the ﬁeld of genetic and evolutionary algorithms [11,12]. The EDAs employ the
probabilistic model, which is constituted by a database containing the genetic
information of the selected individuals in the previous generation, to yield a
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 991–1002, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

992
H. Handa
   Set of Units:  U = {w, x, y, z}
 
 
   Set of Labels:  L = {r, g, b}
   Unit Constraint Relations: T = {t1, t2, t3, t4, t5}
                 t1=(x,y), t2=(y,z), t3=(z,w), t4=(x,w), t5=(y,w)
   Unit-Label Constraint Relations:
:  R = {R1, R2, R3, R4, R5}
                 R1 = {(r,g), (r,b), (g,r), (g,b), (b,r), (b,g)}
                 R2 = {(r,g), (r,b), (g,r), (g,b), (b,r), (b,g)}
                 R3 = {(r,g), (r,b), (g,r), (g,b), (b,r), (b,g)}
                 R4 = {(r,g), (r,b), (g,r), (g,b), (b,r), (b,g)}
                 R5 = {(r,g), (r,b), (g,r), (g,b), (b,r), (b,g)}
y
z
w
x
x
y
w
z
{r, g, b}
{r, g, b}
{r, g, b}
{r, g, b}
(b) Graph representation of the CSP(a)
(a) M
1
4
5
3
2
ap representation
Fig. 1. An example of CSP: graph coloring problem
new population. Hence, in the EDAs, Genetic operations such like crossover and
mutation are not adopted.
In this paper, we propose a new evolutionary constraints satisfaction problem
solver incorporating a repair method into the EDA. Moreover, we also introduce
a manner which incorporates the knowledge of the constraint network to the
Bayesian Network structure search.
Related works are described as follows: Tsang wrote comprehensive text book
about CSPs which is also written about genetic approach for solving CSPs [1].
Eiben summarized how to solve several classes of CSPs by using GAs in [3,4].
Riﬀproposed a ﬁtness function and genetic operators for solving CSPs eﬀec-
tively, which are utilizing the knowledge with regard to the constraint network
[5,6]. Coevolutionary Evolutionary Computations were often adopted to solve
CSPs [2,9]. With respect to the EDAs, Larra˜naga and Lozano edited a compre-
hensive book of the EDAs[12]. EDAs in section 3 are introduced by referring
to this textbook. Genetic algorithms with local search methods are often called
“memetic algorithms,” and have been studied by many researcher for the last
decade [13].
In the next section, the basics of the CSPS will be described. Then we will
brieﬂy introduce three kinds of EDAs in section 3, which are employed for our
experiments. Section 4 will introduce the proposed method. Then, experiments
on general Constraint Satisfaction Problems will be carried out for conventional
GEA approach, EDAs introduced in section 3 and the proposed method. Section
6 will conclude this paper.
2
Constraint Satisfaction Problems
2.1
Formulation
Constraint Satisfaction Problems (CSPs) are a class of problems which consists
of variables and constraints on the variables [1]. In addition, the class of CSPs in
which each constraint in the problems is related only to two variables is called
binary CSPs. In this paper, we treat a class of discrete binary CSPs, where the

Hybridization of Estimation of Distribution Algorithms
993
Procedure Min-Conﬂict Hill Climbing
begin
e(i) ←evaluate each variable i in the current solution
until Stopping criteria is hold
i∗←Select a variable with the worst evaluation
Re-evaluate e(i∗) for all labels in the variable i∗
Select a label in the variable i∗with the best evaluation
Modify the current solution to the selected label
Re-evaluate the current solution with respect to the modiﬁcation
end
end
Fig. 2. Pseudo code of Min-Conﬂict Hill Climbing
word ‘discrete’ means that each variable is associated with a ﬁnite set of discrete
values (labels) that are candidate values of the variable. An example of the
graph coloring problem [10], a binary CSP which is one of the benchmark CSP
is delineated in Fig. 1. As depicted in the ﬁgure, CSPs are deﬁned by (U, L, T, R).
U, L, T and R denote a set of units (variables), a set of labels (values), unit
constraint relations and unit-label constraint relations, respectively.
In this paper, we use two indices, tightness and density to analyse the diﬃ-
culty of CSPs [1]. The tightness of an edge ij is given as the ratio of the number
of satisfying 2-compound labels (in unit-label constraint relations) on the edge
ij over the number of all 2-compound labels on the edge ij. Furthermore, the
tightness of a problem is given by the averaged value of tightness of the edges
in the problem. The density of a problem indicates the proportion of constraint
relations that actually exist between any pair of nodes. Furthermore, the number
of 2-compound labels on the edge XY is the same as the product of the number
of labels on each nodes, that is, 3 × 3.
2.2
Min-Conﬂict Hill Climbing
This local search method, often called heuristic repair method, is adopted not
only from genetic and evolutionary algorithms but also from approximation al-
gorithms for solving CSPs. The procedure of the MCHC is described in Fig. 2:
The MCHC begins with a given solution. First, the solution is evaluated. In that
time, the number of constraint violations for each variable is memorized. Then,
the variable which have the most constraint violations, i.e., the least evaluation,
is chosen. If some variables tie with each other, the variable is randomly chosen
among them. For the selected variable, all the labels are examined and evalu-
ated. In similar to the variable selection, one of labels with the least constraint
violations is chosen (randomly in the case of tie). Re-evaluation for above mod-
iﬁcation is carried out, and the process returns to the variable selection phase.

994
H. Handa
Procedure Estimation of Distribution Algorithm
begin
initialize D0
evaluate D0
until Stopping criteria is hold
DSe
l−1 ←Select N individuals from Dl−1
pl(x) ←Estimate the probabilistic model from DSe
l−1
DSe
l
←Sampling M individuals from pl(x)
end
end
Fig. 3. Pseudo code of Estimation of Distribution Algorithms
3
Brief Introduction of the Estimation of Distribution
Algorithms
3.1
General Framework of EDAs
The Estimation of Distribution Algorithms are a class of evolutionary algorithms
which adopt probabilistic models to reproduce the genetic information of the
next generation, instead of conventional crossover and mutation operations. The
probabilistic model is represented by conditional probability distributions for
each variable (locus). This probabilistic model is estimated from the genetic
information of selected individuals in the current generation. Hence, the pseudo-
code of EDAs can be written as Fig. 3, where Dl, DSe
l−1, and pl(x) indicate the
set of individuals at lth generation, the set of selected individuals at l −1th
generation, and estimated probabilistic model at lth generation, respectively
[12]. The representation and estimation methods of the probabilistic model are
devised by each algorithm. The following subsections will overview some EDAs.
For a more thorough overview, see [12,14].
3.2
UMDA
UMDA (Univariate Marginal Distribution Algorithm) was introduced by M¨uhle-
nbein [12]. As indicated by its name, the variables of the probabilistic model in
this algorithm is assumed to be independent from other variables. That is, the
probability distribution pl(x) is denoted by a product of univariate marginal
distributions, i.e.,
pl(x) =
n

i=1
pl(xi),
where pl(xi) denotes the univariate marginal distribution Xi = xi at a vari-
able Xi at generation l. This univariate marginal distribution is estimated from
marginal frequencies:
pl(xi) = the number of solutions, where Xi = xi in the selected individuals
N
,
where N denotes the number of selected individuals, which is ﬁxed in advance.

Hybridization of Estimation of Distribution Algorithms
995
3.3
MIMIC
De Bonet et al. proposed MIMIC [12,17], a kind of EDAs whose probabilistic
model is constructed with bivariate dependency such like COMIT [18]. While
the COMIT generates a tree as dependency graph, the probabilistic model of
the MIMIC is based upon a permutation π.
pl(x) =
n−1

j=1
pl(xin−j|xin−j+1) · pl(xin),
where the permutation π is represented by (i1, i2, . . . , in). The permutation π is
decided in each generation such that the following Kullback-Leibler divergence
Hπ
l (x) is minimized:
Hπ
l (x) = hl(Xin) +
n−1

j=1
hl(Xij|Yij+1),
where hl(X) = −
x p(X = x) log p(X = x), and hl(X|Y ) = −
x

y p(Y =
y)p(X = x|Y = y) log p(X = x|Y = y). However, such minimization is NP so
that this minimization is carried out by greedy search.
3.4
EBNA
Like BOA and LFDA [15,16], The EBNA (Estimation of Bayesian Networks
Algorithms) adopts Bayesian Network (BN) as the probabilistic model, which
is proposed by Larra˜naga et al. [12,19]. They proposed several kinds of EBNAs,
such as EBNAP C, EBNAK2+pen, EBNABIC, and so on. Here, we introduce only
EBNABIC used in our experiments. EBNABIC searches for the better structure
of BN by using search+score method. In the case of the EBNABIC, scoring is
achieved by penalized maximum likelihood BIC(S, D) for a given structure S
and a dataset D, called Bayesian Information Criteria, denoted by the following
equation:
BIC(S, D) =
n

i=i
qi

j=1
ri

k=1
Nijk log Nijk
Nij
−1
2 log N
n

i=1
qi(ri −1),
where the structure S is represented by Direct Acyclic Graphs, n is the number
of variables of the Bayesian Network, ri is the number of diﬀerent values that
variable Xi can take, qi is the number of diﬀerent values that the parent variables
of Xi in the structure S can take, Nij is the number of individuals in D in which
the parent variables of variable Xi take their jth value, and Nijk is the number
of individuals in D in which variable Xi takes its kth value and the parent
variables of the variable i take their jth value [12]. As the search method in
the EBNABIC, an arc-based local search is adopted due to the NP property of
searching the best structure for BNs.

996
H. Handa
Procedure EDA for CSP
begin
initialize D0
evaluate D0
until Stopping criteria is hold
DSe
l−1 ←Select N individuals from Dl−1
pl(x) ←Estimate the probabilistic model from DSe
l−1
D′Se
l
←Sample M individuals from pl(x)
DSe
l
←carry out a Repair Algorithm to D′Se
l
end
end
Fig. 4. Pseudo code of EDA for CSP
4
The Proposed Method
As mentioned in the introduction of this paper, applying conventional GEA to
solve CSPs have been studied by many researchers. According to the conclu-
sions of their studies, the utilization of the CSP-speciﬁc knowledge, such as the
topology of constraint networks, local constraint evaluation (sub-evaluation of
individuals), and so on, yields the great improvement of the search ability of
GEAs [3]-[9]. Hence, we propose a hybrid method of EDAs with repair method.
Moreover, we also introduce a manner which incorporates the knowledge of the
constraint network to the BN structure search.
As a repair method for the proposed method, we employ asexual heuristic
operator in H-GA proposed by Eiben et al. [3]. The asexual heuristic operator
uses the notion of the MCHC described in Fig. 2 to operate the genetic infor-
mation of individuals. The number of applying the MCHC is pre-deﬁned, that
is, 1/4 of the string length. The timing to apply this operation to the EDA pop-
ulation is after sampling new individuals from the estimated (joint) probability
distribution as denoted in the Fig. 4.
Furthermore, in order to reduce the computational eﬀort, the variable candi-
date selection guided by constraint networks is introduced to bivariate or mul-
tivariate dependent algorithms. That mechanism is quite simple: arcs which do
not have constraint-relations, which can be referred by the constraint network,
do not calculate its indices to construct the probabilistic model, such like Hπ
l (x)
and BIC(S, D). However, it saves a large amount of computational time since
the calculation of conditional probabilities takes much time and appears for cor-
responding candidate structures.
5
Experimental Results
In this paper, we carry out several experiments based on various general CSPs
that are generated randomly for a wide variety of “density” and “tightness” of
constraint conditions in the CSPs that are the basic measures of characterizing
CSPs and are described in section 2. The general CSPs are randomly generated

Hybridization of Estimation of Distribution Algorithms
997
The success Ratio: 1
0
10
50
90
90
50
10
restart
tightness
density
10
50
90
90
50
10
SSGA
tightness
density
10
50
90
90
50
10
H-GA
tightness
density
10
50
90
90
50
10
UMDA
tightness
density
10
50
90
90
50
10
MIMIC
tightness
density
10
50
90
90
50
10
EBNA
tightness
density
10
50
90
90
50
10
UMDACSP
tightness
density
10
50
90
90
50
10
MIMICCSP
tightness
density
10
50
90
90
50
10
EBNACSP
tightness
density
Fig. 5. Experimental results on general CSP, the ratio of success examination: results
with higher success ratio tend to be brighter, and results with lower success ratio
tend to be darker; The upper row is for conventional methods, restart method with
MCHC, Steady-State Genetic Algorithms, and H-GA; The middle row is for EDAs,
UMDA, MIMIC, and EBNA; The lower row is for the proposed methods, UMDACSP,
MIMICCSP, and EBNACSP.
as follows: First, specify the tightness and density in the sense in section 2.
Next, for all combination of two indices, decide whether unit constraint relation
is set to each of the pairs of variables by taking account of the value of density.
Finally, for all unit constraint relations, the number of the unit-label constraint
relationships is set to be directly proportional to the tightness.
In order to solve the general CSPs, i.e., to ﬁnd a solution such that it has no
constraint violation, we introduce following ﬁtness function which is generally
adopted:
1 −NCV /NMC,

998
H. Handa
No. Constraint Chk:1
2x1e9
10
50
90
90
50
10
restart
tightness
density
10
50
90
90
50
10
SSGA
tightness
density
10
50
90
90
50
10
H-GA
tightness
density
10
50
90
90
50
10
UMDA
tightness
density
10
50
90
90
50
10
MIMIC
tightness
density
10
50
90
90
50
10
EBNA
tightness
density
10
50
90
90
50
10
UMDACSP
tightness
density
10
50
90
90
50
10
MIMICCSP
tightness
density
10
50
90
90
50
10
EBNACSP
tightness
density
Fig. 6. Experimental results on general CSP, the number of constraint checks until
ﬁnding satisﬁable solutions when satisﬁable solutions are found within 2 billion con-
straint checks: results with faster discovers tend to be brighter, and results with later
discovers tend to be darker; The allocation of each graph is the same as the previous
ﬁgure.
where NCV and NMC indicate the number of constraint violations in a certain
individual, and the number of possible constraint relations in a problem with n
variables, i.e., n(n −1)/2 in the case of binary CSPs, respectively. The coding
method adopted in this paper is a naive one such that each variable in a problem
instance is corresponding to each gene. That is, each label associated with each
variable is directly represented as each allele at corresponding locus.
First, we compare the proposed methods for UMDA, MIMIC, and EBNA,
i.e., UMDACSP, MIMICCSP, and EBNACSP, respectively, with various kinds
of conventional method: restart method, steady-state GA, H-GA proposed by
Eiben et al., UMDA, MIMIC, and EBNA [21]. Parameters for each algorithm
are described as follows: The restart method is a (non-evolutionary) conventional

Hybridization of Estimation of Distribution Algorithms
999
The success Ratio: 1
0
10
50
90
90
50
10
H-GA
tightness
density
10
50
90
90
50
10
EBNA
tightness
density
10
50
90
90
50
10
EBNACSP
tightness
density
10
50
90
90
50
10
H-GA
tightness
density
10
50
90
90
50
10
EBNA
tightness
density
10
50
90
90
50
10
EBNACSP
tightness
density
10
50
90
90
50
10
H-GA
tightness
density
10
50
90
90
50
10
EBNA
tightness
density
10
50
90
90
50
10
EBNACSP
tightness
density
Fig. 7. The scalability of each algorithm, the ratio of success examination: the depic-
tion manner of each graph is the same as Fig. 5; 40 variables with 10(UPPER ROW),
20(MIDDLE ROW), and 30(LOWER ROW) labels in each variable; H-GA(LEFT
COL.), EBNA(MIDDLE COL.), and EBNACSP(RIGHT COL.)
CSP solver and employs the MCHC, described in Fig. 2, as Hill-Climber. If the
MCHC fails the evaluation improvement of the solution 100 times succeedingly,
a new initial solution is randomly generated again. The population size of SSGA
and H-GA are set to be 200, which is a resultant of parameter tuning. The
mutation probability of SSGA is set to be 0.025 which is identical to the 1 /
(string length). We use the H-GA version 1 which shows the best performance
in [3] to our experiment. The population size M of EDAs including the proposed
methods is set to be 3000. The size N of selected individual used to estimate
the probabilistic model is set to be 1000.
The experimental results for solving general CSPs are plotted in Fig. 5 and
6. The x and y axes of all graphs in these ﬁgures denote the values of density
and tightness, respectively. All graphs in this paper are averaged results over

1000
H. Handa
No. Constraint Chk:1
2x1e9
10
50
90
90
50
10
H-GA
tightness
density
10
50
90
90
50
10
EBNA
tightness
density
10
50
90
90
50
10
EBNACSP
tightness
density
10
50
90
90
50
10
H-GA
tightness
density
10
50
90
90
50
10
EBNA
tightness
density
10
50
90
90
50
10
EBNACSP
tightness
density
10
50
90
90
50
10
H-GA
tightness
density
10
50
90
90
50
10
EBNA
tightness
density
10
50
90
90
50
10
EBNACSP
tightness
density
Fig. 8. The scalability of each algorithm, the number of constraint checks until ﬁnd-
ing satisﬁable solutions when satisﬁable solutions are found within 2 billion constraint
checks: the depiction manner of each graph is the same as Fig. 6; 40 variables with
10(UPPER ROW), 20(MIDDLE ROW), and 30(LOWER ROW) labels in each vari-
able; H-GA(LEFT COL.), EBNA(MIDDLE COL.), and EBNACSP(RIGHT COL.)
25 experiments for each couple of (density, and tightness), where density and
tightness separately range from 10 to 90 with step size 10. We deﬁne “a suc-
cess experiment” whether algorithms can ﬁnd satisﬁable solution until 2 billion
constraint checks. That is, the ratio of success (Fig. 5) indicates how many suc-
cess experiments exist over 25 experiments. If the success ratio is equal to 0,
the number of constraint checks until ﬁnding satisﬁable solution in the case of
success experiments (Fig. 6) is set to be 2 billion. The colors in Fig. 5 and 6,
such that favourable results tend to be brighter, and unfortunate results tend to
be darker, indicate the success ratio and the number of constraint checks until
ﬁnding satisﬁable solutions. As shaded in all graphs, darker areas lie across the

Hybridization of Estimation of Distribution Algorithms
1001
couples of density and tightness. Such an area is called the “phase transition”,
where it is diﬃcult to solve the problem for any algorithms.
As depicted in Fig. 5 and Fig. 6, the restart method and the SSGA were
not found satisﬁable solution eﬀectively. Almost all experiments of them were
failed. We can conﬁrm the role of GEAs and EDAs in these hybrid approaches
since naive iteration cannot help us to ﬁnd satisﬁable solutions. The conventional
EDAs outperform conventional GA (SSGA). By comparing H-GA with SSGA,
we can conﬁrm the eﬀectiveness of the repair algorithm for CSPs since H-GA also
adopts the steady-state selection. The proposed method outperform any other
algorithms in the sense of the success ratio. However they have a drawback that it
takes a large number of constraint checks even if it is easier problem instances for
H-GA and the proposed methods, for problem instances in the phase transition
area, the proposed method outperform the H-GA.
Next, we investigate the scalability of the H-GA, EBNA and EBNACSP in
Fig. 7 and Fig. 8. In these ﬁgures, the domain size (the number of labels in each
variables) of general CSPs with 40 variables change from 10 to 30. EBNACSP
exhibit the best scalability in the sense of the success ratio (Fig. 7) among three
algorithms. H-GA can quickly solve problem instances, but its success ratio is
low.
6
Conclusions
In this paper, we proposed the hybrid method of EDAs with repair method
for solving CSPs. As the same as the conventional GEAs, the repair method im-
prove the search methods of EDAs dramatically. Moreover, the proposed method
outperforms the restart method, GEAs with or w/o the repair method and con-
ventional EDAs. Future work is (1) to reduce the number of constraint checks
in the case of easier problems. That is, actually, the proposed method apply the
repair method to all individuals. It yields great number of constraint checks. Fur-
thermore, (2) another one is further incorporation of the domain knowledge in
CSPS to estimation of the probabilistic model. The proposed method employed
BIC as the score metric. However, the BIC does not take account into the local
consistency of labels. We consider the local consistency of labels makes the score
metric more eﬀective.
References
1. E. Tsang: Foundations of Constraint Satisfaction, Academic Press, 1993
2. J. Paredis: Genetic State-Space Search for Constraint Optimization Problems, Pro-
ceedings of the 13th International Joint Conference on Artiﬁcial Intelligence II
(1993) 967–972
3. A. E. Eiben et al.: Solving Constraint Satisfaction Problems Using Genetic Algo-
rithms, Proceedings of the 1st International Conference on Evolutionary Compu-
tation II (1994) 542–547

1002
H. Handa
4. A. E. Eiben et al.: How to Apply Genetic Algorithms to Constrained Problems,
Practical Handbook of Genetic Algorithms, Lance Chambers Editors, CRC Press,
Chapter 10 (1995) 307–365
5. M.-C. Riﬀ: Using the knowledge of the Constraint Network to design an evolution-
ary algorithm that solves CSP, Proceedings of the 3rd International Conference on
Evolutionary Computation (1996) 279–284
6. M.-C. Riﬀ: Evolutionary Search guided by the Constraint Network to solve CSP,
Proceedings of the 4th International Conference on Evolutionary Computation
(1997) 337–342
7. E. Marchiori: Combining Constraint Processing and Genetic Algorithms for Con-
straint Satisfaction Problems, Proceedings of the Seventh International Conference
of Genetic Algorithm (1997) 330–337
8. G. Dozier et al.: Solving constraint satisfaction problems using hybrid evolutionary
search, IEEE Transactions on Evolutionary Computation 2(1) (1998) 23–33
9. H.Handa et al.: Coevolutionary Genetic Algorithm for Constraint Satisfaction with
a Genetic Repair Operator for Eﬀective Schemata Formation, Proceedings of the
1999 IEEE System Man and Cybernetics Conference III (1999) 617–621
10. S. Minton et al.: Minimizing conﬂicts: a heuristic repair method for constraint
satisfaction and scheduling problems, Constraint-Based Reasoning, E. C. Freuder
and A. K. Mackworth Editors, The MIT Press (1994) 161–206,
11. H. M¨uhlenbein and G. Paaß:From Recombination of genes to the estimation of
distributions I. Binary parameters. Parallel Problem Solving from Nature - PPSN
IV (1996) 178–187
12. P. Larra˜naga and J. A. Lozano Editors: Estimation of Distribution Algorithms,
Kluwer Academic Publishers (2002)
13. Memetic Algorithms’ Home Page,
http://www.ing.unlp.edu.ar/cetad/mos/memetic home.html
14. M. Pelikan: Bayesian optimization algorithm: From single level to hierarchy,
Ph.D. thesis, University of Illinois at Urbana-Champaign, Urbana, IL. Also Illi-
GAL Report No. 2002023 (2002)
15. M. Pelikan et al.: BOA: The Bayesian optimization algorithm, Proceedings of the
Genetic and Evolutionary Computation Conference 1 (1999) 525–532
16. H. M¨uhlenbein and T. Mahnig: FDA - a scalable evolutionary algorithms for the
optimization of additively decomposed functions, Evolutionary Computation 7(4)
(1999) 353–376
17. J. S. De Bonet et al.: MIMIC: Finding optima by estimating probability densities,
Advances in Neural Information Processing Systems 9 (1996)
18. S. Baluja: Using a priori knowledge to create probabilistic models for optimization
International J. of Approximate Reasoning, 31(3) (2002) 193–220
19. P. Larra˜naga et al.: Combinatorial Optimization by Learning and Simulation of
Bayesian, Uncertainty in Artiﬁcial Intelligence, Proceedings of the Sixteenth Con-
ference (2000) 343–352
20. E. Bengoetxea et al.: Learning and simulation of Bayesian networks applied to
inexact graph matching, Pattern Recognition, 35(12) (2002) 2867–2880
21. G. Syswerda: A Study of Reproduction in Generational and Steady-State Genetic
Algorithms, Foundations of Genetic Algorithms, G. J. E. Rawlins Editors, Morgan
Kaufman (1990) 94–101

Efﬁcient Linkage Discovery by Limited Probing
Robert B. Heckendorn1 and Alden H. Wright2
1 University of Idaho, Moscow, ID 83844-1010 USA, heckendo@cs.uidaho.edu,
2 Computer Science, University of Montana, Missoula, MT USA, wright@cs.umt.edu
Abstract. Thispaperaddressestheproblemofdeterminingtheepistaticlinkageof
a function from binary strings to the reals. There is a close relationship between the
Walsh coefﬁcients of the function and “probes” (or perturbations) of the function.
This relationship leads to two linkage detection algorithms that generalize earlier
algorithms of the same type. A rigorous complexity analysis is given of the ﬁrst
algorithm. The second algorithm not only detects the epistatic linkage, but also
computes all of the Walsh coefﬁcients. This algorithm is much more efﬁcient than
previous algorithms for the same purpose.
1
Introduction
In very simple ﬁtness functions each bit in the domain independently contributes to
the total value of the function. In optimizing these simple ﬁtness functions, each bit
can be tested independently against a ﬁxed background of other bits to determine the
contribution of that bit. Proceeding through all the bits the optimum can be found in
linear time with respect to the number of bits.
Most practical functions, however, are not nearly as simple. For many, the contribu-
tion of a bit in the domain to the function value is non-linear in that it is dependent on
the state of one or more other bits in the domain. This linkage effect is called epistasis
and can be succinctly deﬁned:
“...if the effect of one unit is not predictable unless the value of another unit
is known, then the effects are epistatic...in other words, the effect of a unit is
context dependent” [Bro00].
Applied to the case of evolutionary computation the “units” in the quote above refer to the
positions in the problem representation whose values are selected from an alphabet. The
more units, or positions, that simultaneously interact (the higher the epistasis) the greater
the degree of freedom to “hide” the optimum anywhere in the subdomain formed by the
interacting units [HW99]. High epistasis, however, is no guarantee of a difﬁcult problem.
Nor is low epistasis a guarantee of an easy problem. In fact, MAX3SAT problems are
equivalent to problems of low epistasis in which all epistatic interactions are known
and they are provably NP-complete [Hec99]. Still, knowing the location of epistatically
interactingblocksofbitsmaybeusedtoguideasearchfortheoptimumortheformulation
of a representation [MG99b,MG99a,KP01]. If the function is separable, each component
can be solved separately. If the function is close to separable, this can guide the choice
of crossover operators. In this case, M¨uhlenbein and Mahnig [MM99] also suggest
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1003–1014, 2003.
c⃝Springer-Verlag Berlin Heidelberg 2003

1004
R.B. Heckendorn and A.H. Wright
applying the UMDA algorithm where each component makes up a string position with a
higher-order alphabet. M¨uhlenbein, Mahnig, and Rodriguez [MMR99] give a factorized
distribution algorithm (FDA) that applies to additively decomposed functions (that we
call embedded landscapes in this paper).
The general problem of discovering epistatic linkage has been addressed directly
and indirectly by many papers. Munetomo and Goldberg showed a simple direct pertur-
bational approach to generalized linkage discovery over a binary alphabet in [MG99b]
[MG99a]. These papers also summarize some other approaches to the problem. Kar-
gupta et al. [KP01] have shown that for epistatically bounded functions, f, where the
epistasis is known to be bounded by k bits, all the Walsh coefﬁcients, a direct measure
of the magnitude of epistasis, can be computed in time O(Lk), where L is the length of
the representation.
In this paper we present a theoretical framework for the detection of epistatic link-
age and the computation of Walsh coefﬁcients for epistatically bounded functions. The
Walsh coefﬁcients completely describe the function and so completely characterize the
epistatic linkage. The algorithms in this paper are blackbox algorithms in that they
assume minimal prior knowledge of the function being analyzed. This paper deals with
perturbation methods, or what we call probes. We give a randomized algorithm for
linkage detection which is based on our theoretical framework, and we give rigorous
complexity bounds for this algorithm. We extend this to another randomized algorithm
that both detects linkage and computes the Walsh coefﬁcients. This algorithm makes
much more efﬁcient use of function evaluations than previous algorithms.
2
Notation
The space of all bit strings of length L is denoted by B. The binary operators on B include
∧which denotes bitwise and, and ⊕which denotes bitwise exclusive-or. An overbar
(e. g., m) denotes 1’s complement. Since the L-bit binary representations of the integers
in the interval [0, 2L) coincide with the elements of B, a bit strings may be denoted by
the corresponding integer. For example, the integer 2k, 0 ≤k < L corresponds to the
bit string with a single one in position k, where bit positions are labeled from the right
starting at 0. Thus, 22 ≡0000100 for L = 7. It is convenient to think of a bit string i as
corresponding to the set of bit positions indicated by the 1 bits in i. Thus, we write i ⊆j
(i is contained in j) when the set corresponding to i is contained in the set corresponding
to j, i. e., when i ∧j = i. If i ⊆j and i ̸= j we write i ⊂j. The unitation or bit count
function bc(i) of string i is the number of ones in i. Given a mask m ∈B, let the set
Bm = {i ∈B : i ⊆m}. Note |Bm| = 2bc(m). Square brackets are used to denote an
indicator function: if expr is an expression that may be true or false, then
[expr] =

1
if expr is true
0
otherwise

Efﬁcient Linkage Discovery by Limited Probing
1005
3
Walsh Analysis and Embedded Landscapes
Any function f : B→R can be written as a linear combination of Walsh functions:
f(x) =

i∈B
wiψi(x)
where ith Walsh function is deﬁned:
ψi(x) = (−1)bc(i∧x)
and the wi are referred to as Walsh coefﬁcients. The Walsh transform is a linear
transform of the Walsh coefﬁcients represented as a vector w in R2L to the function
space f in R2L. This is a change of basis transformation corresponding to the matrix Ψ
with Ψi,j = ψi(j).
f = Ψw
and
w = 1
2L Ψf
(1)
It is not hard to show that Ψ is symmetric and ΨΨ = 2LI where I is the identity matrix.
f depends on a bit position k, 0 ≤k < L, if there exists a j ∈B such that
f(j) ̸= f(j ⊕2K). In other words, f depends on bit position k if ﬂipping bit k changes
the value assigned to some string j. The support of f is the set of loci that f depends on.
The support mask of f is a bitstring in B with 1 bits in exactly and only those positions
that support f. By the deﬁnition the support mask of ψi is i.
An embedded landscape is a function f : B→R which can be written in the form
f =  gj where each subfunction gj has a support mask mj. Normally, there will be
some restriction on the support set masks mj. The function f : B→R has k-bounded
epistasis if it can be written as the sum of subfunctions each of whose support is a set
of at most k bits. It has been shown, perhaps most recently in [Hec02]:
Theorem 1. (Embedded Landscape Theorem) A function f : B→R has k-bounded
epistasis if and only if wj = 0 ∀bc(j) > k
Thus, f has k-bounded epistasis if and only if all of its Walsh coefﬁcients of order
greater than k are zero.The function f is linear if it has 1-bounded epistasis.The function
f is additively separable if it can be written as a sum of at least two subfunctions where
the supports of all subfunctions are pairwise disjoint.
4
Probes
A probe is a way of determining epistatic properties of a function f : B→R by per-
forming a series of speciﬁc function evaluations. More speciﬁcally, a probe is:
P(f, m, c) =
1
2bc(m)

i∈Bm
(−1)bc(i)f(i ⊕c)
where m ∈B and c ∈Bm. c is called the background of the probe. The order of the
probe is number of ones in the mask, or bc(m). The direct computation of the value of
a probe requires 2bc(m) function evaluations.

1006
R.B. Heckendorn and A.H. Wright
Theorem 2. (Walsh Function Probing) For any j, m ∈B and c ∈Bm,
P(ψj, m, c) =

ψj(c)
if m ⊆j
0
otherwise
Proof.
P(ψj, m, c) =
1
2bc(m)

i∈Bm
(−1)bc(i)ψj(i ⊕c)
=
1
2bc(m)

i∈Bm
ψ1(i)ψj(i ⊕c)
=
1
2bc(m)

i∈Bm
ψ1(i)ψj(i)ψj(c)
=
1
2bc(m) ψj(c)

i∈Bm
ψj(i)
By the Balanced Sum Theorem for Hyperplanes [HW99] the sum is 2bc(m) if j ⊆m
which is the same as m ⊆j and is 0 otherwise.
⊓⊔
A probe is really probing for nonzero Walsh coefﬁcients by adding and subtracting
over a set of Walsh coefﬁcients. If the result is nonzero then one of the component Walsh
coefﬁcients is nonzero. If it is zero then without further information we can say very
little. The following theorem identiﬁes the set of Walsh coefﬁcients.
Theorem 3. (Probe Subset) For any m ∈B and c ∈Bm,
P(f, m, c) =

j∈B
[m ⊆j]wjψj(c)
Proof. Recall that f = 
j∈B wjψj. Thus,
P(f, m, c) =

j∈B
wjP(ψj, m, c)
=

j∈B
[m ⊆j]wjψj(c)
by the Walsh Function Probing theorem
⊓⊔
A maximal nonzero Walsh coefﬁcient is a Walsh coefﬁcient wm such that wm ̸= 0
and wj = 0 ∀j ⊃m.
Corollary 1. (Maximal Probe) If wm is a maximal nonzero Walsh coefﬁcient, then for
any c ∈Bm,
P(f, m, c) = wm

Efﬁcient Linkage Discovery by Limited Probing
1007
Proof. It follows from theorem 3 that
P(f, m, c) = wmψm(c)
And from the deﬁnition of a Walsh function: ψm(c) = (−1)bc(m∧c) = (−1)0 = 1.
⊓⊔
A probe can be written as a sum of lower-order probes.
Theorem 4. (Probe Recursion) For any function f : B →R, any masks m, n ∈B with
n ⊆m, and any c ∈Bm:
P(f, m, c) =
1
2bc(n)

i∈Bn
(−1)bc(i)P(f, m ⊕n, i ⊕c)
Proof. Any j ∈Bm can be written uniquely as j = i ⊕u where i ∈Bn and u ∈Bm⊕n.
Thus:
P(f, m, c) =
1
2bc(m)

j∈Bm
(−1)bc(j)f(j ⊕c)
=
1
2bc(n)

i∈Bn
(−1)bc(i)
1
2bc(m⊕n)

u∈Bm⊕n
(−1)bc(u)f(u ⊕i ⊕c)
=
1
2bc(n)

i∈Bn
(−1)bc(i)P(f, m ⊕n, i ⊕c)
⊓⊔
Theorem 5. (Nonzero Probe Existence) Given a maximal nonzeroWalsh coefﬁcient wm,
for all a: a ⊆m, there exists an i ∈Bm⊕a such that
P(f, a, i ⊕c) ̸= 0
∀c ∈Bm
Proof. By the Maximal Probe Corollary, P(f, m, c) = wm ̸= 0 for any c ∈Bm. By the
Probe Recursion Theorem applied with n = m ⊕a,
P(f, m, c) =
1
2bc(n)

i∈Bn
(−1)bc(i)P(f, m ⊕n, i ⊕c)
Thus, there must exist an i ∈Bn such that P(f, m ⊕n, i ⊕c) = P(f, a, i ⊕c) ̸= 0.
⊓⊔
5
The Linkage Graph and Hypergraph
A hypergraph is a collection of vertices V together with a family of subsets E of V
called hyperedges where each hyperedge is nonempty. The set of vertices of the linkage
hypergraph is the set of string positions j. A set of vertices corresponding to mask
m ̸= 0 is a hyperedge if there is a c ∈Bm such that P(f, m, c) ̸= 0. A hyperedge will
be identiﬁed with the corresponding mask. The order of a hyperedge is the number of
ones in the mask. In view of Theorem 5, the mask m is a hyperedge if and only if there
is a j ⊇m such that wj ̸= 0. Thus, we have the following corollary.

1008
R.B. Heckendorn and A.H. Wright
Corollary 2. If m is a hyperedge of the hypergraph, and if a ⊆m, then a is also a
hyperedge.
Detect-Linkage(j,N)
begin
V ←{0, 1, . . . , L −1}
E ←∅
for i ←1 to N do
for each mask m with bc(m) = j do
if m /∈E then
c ←a random string in Bm
if P(f, m, c) ̸= 0 then
E ←E ∪{m}
end if
end if
end for
end for
return E
end Detect-Linkage
The order-j linkage detection algorithm constructs the set of order-j hyperedges
of the linkage hypergraph. The order-2 version of this algorithm is similar to the LINC
algorithm of [MG99b]. However, they start with a population of strings. Then each probe
is done using one of the strings of the population to provide the background for the probe.
For an arbitrary function f it is impossible to conclude anything conclusively from
evaluating f at a subset of points. For example, if f would be k-epistatically bounded
except for its value at one point, then the above algorithm for j > k will return 0 for any
probe unless the probe happens to sample the one exceptional point. For a large string
length, the probability that this one exceptional point is sampled can be very small.
Thus, assumptions on f are needed in order to use the order-j linkage detection algo-
rithm to make conclusions. The natural assumption is that f is k-epistatically bounded.
The following theorems give a worst-case complexity analysis of the order-j linkage
detection algorithm in this case.
Theorem 6. (Nonzero Probe Probability) Let f be k-epistatically bounded and let m
be a mask corresponding to an order-j hyperedge of the linkage hypergraph of f. If c
is a randomly chosen string in Bm, then the probability that P(f, m, c) ̸= 0 is at least
2j−k.
Proof. Since m is a hyperedge by the Nonzero Probe Existence Theorem there is a u
such that m ⊆u and wu ̸= 0. Without loss of generality we can assume that u has the
property that u ⊂v ⇒wv = 0. By assumption, bc(u) ≤k. Theorem 5 shows that there
is at least one i ∈Bu⊕m such that P(f, m, i ⊕b) ̸= 0 for any b ∈Bu. The probability
that the randomly selected background c matches some such i on the positions masked
by u ⊕m is at least 2−bc(u⊕m) = 2bc(m)−bc(u) ≥2j−k.
⊓⊔

Efﬁcient Linkage Discovery by Limited Probing
1009
The lower bound of Theorem 6 cannot be improved under these assumptions. Start
with a (j −1)-epistatically bounded function whose support is m with bc(m) = k, and
then perturb the value of one point. Any probe that does not include the perturbed point
will return a value of zero. Since an order-j probe includes 2j points, and since there are
2k probes, the probability of including the perturbed point is 2j−k.
Theorem 7. Let f be k-epistatically bounded and let J be the number of order-j hy-
peredges in the linkage hypergraph of f. If the number of iterations N in the order-j
linkage detection algorithm is chosen so that either
N ≥ln(1 −δ1/J)
ln(1 −2j−k)
or
N ≥−2k−j ln(1 −δ1/J)
then the probability that all order-j hyperedges are detected is at least δ.
Proof. In the following, a “success” is the detection of a nonzero probe. Theorem 6
shows that the probability of failure for one probe on one trial is at most 1 −2j−k.
Thus, the probability of failure on N trials is at most (1 −2j−k)N, and the probability
of success on N trials is at least 1 −(1 −2j−k)N. The probability of success on all J
hyperedges is at least

1 −(1 −2j−k)NJ
Thus, we want to choose N so that

(1 −(1 −2j−k))NJ ≥δ
1 −δ1/J ≥(1 −2j−k)N
ln(1 −δ1/J) ≥N ln(1 −2j−k)
ln(1 −δ1/J)
ln(1 −2j−k) ≤N
To prove the second formula, note that −ln(1 −x) ≥x for any x > 0 from the Taylor
series of −ln(1 −x). Thus, 2k−j ≥1/ ln(1 −2j−k).
⊓⊔
Strictly speaking, these results do not apply to the LINC algorithm of [MG99b] since
the above analysis assumes that the backgrounds of probes are chosen independently,
and this is not the case for the LINC algorithm. However, our empirical results show that
these formulas are quite accurate when the backgrounds are chosen from a population.
(See Section 8.) In fact, it is much more accurate than the population sizing formula
given by [MG99b].
Thus, the overall worst-case complexity of the order-2 linkage detection algorithm
is O(2kL2 ln(1 −δ1/J)) if one wants to maintain the same probability of overall suc-
cess as the string length increases. Munetomo and Goldberg [MG99b] give the overall
complexity of their LINC algorithm as O(2kL2), but this assumes that the probability
of success per subfunction of the embedded landscape stays constant as the string length
increases, which would seem to be a less desirable assumption.

1010
R.B. Heckendorn and A.H. Wright
6
Computing the Walsh Coefﬁcients Using the Kargupta-Park
Top-Down Algorithm
Kargupta and Park [KP01] give a “deterministic” algorithm to ﬁnd theWalsh coefﬁcients
of a function f with k-bounded epistasis. It is “top-down” since it does high-order probes
before low-order probes. In this section we show how this algorithm can be expressed
in terms of probes.
Let wm be a maximal nonzero Walsh coefﬁcient. The Maximal Probe Corollary
shows that P(f, m, c) = wm for any c ∈Bm. Thus, if f has k-bounded epistasis, and
if we do the probe P(f, m, 0) where bc(m) = k, the result will be wm. Thus, all of the
order-k Walsh coefﬁcients can be computed by doing
L
k

probes, each of which uses
2k function evaluations.
Let j be a mask with bc(j) = k −1. Then Theorem 3 gives the equation
P(f, j, 0) = wj +

j⊂u
wuψu(0) = wj +

j⊂u
wu
(2)
(Note that ψu(0) = 1.) The potentially nonzero Walsh coefﬁcients in the summation are
all of order k and have been computed. Thus, wj can be computed from P(f, j, 0) plus
these order-k Walsh coefﬁcients. Let m be such that bc(m) = k and j ⊆m. If the Probe
Recursion Theorem is applied to P(f, m, 0) with n = m ⊕j, then the ﬁrst term in the
summation is P(f, j, 0). This shows that all function evaluations necessary to compute
P(f, j, 0) have already been done in the computation of P(f, m, 0). (This observation
is ours and is not included in [KP01].)
The same idea can be used to compute the lower order Walsh coefﬁcients. Thus, the
Walsh coefﬁcients are computed in of decreasing bit count, starting with bit count k.
7
Detecting Linkage and Computing the Walsh Coefﬁcients
Kargupta and Park [KP01] give a “bottom up” randomized algorithm that ﬁnds the
nonzero Walsh coefﬁcients. They suggest that they can ﬁnd the values of these nonzero
Walsh coefﬁcients, but the method to do this is not included in their algorithm, and so
presumably one applies the algorithm referred to in Section 6.
In this section, we give a well-speciﬁed algorithm that efﬁciently ﬁnds the nonzero
Walsh coefﬁcients and computes their values. The algorithm ﬁrst proceeds in a bottom-
up fashion to ﬁnd which Walsh coefﬁcients are nonzero, and then it proceeds top-down
to determine their values without doing any additional function evaluations. (We assume
that function evaluations are disproportionately expensive to compute.)
A key observation is that if probe backgrounds are determined using a population, as
in the Munetomo/Goldberg LINC algorithm, then higher order probes can be computed
relatively cheaply by using the function evaluations of previously computed lower order
probes. This is justiﬁed by Theorem 8 below. In other words, computing P(f, m, c) can
be done with only one additional function evaluation as long as the probes for all a,
a ⊂m, have been computed using the same background c.

Efﬁcient Linkage Discovery by Limited Probing
1011
Theorem 8. For any m ∈B, c ∈Bm,
f(m ⊕c) =

a∈Bm
(−2)bc(a)P(f, a, c)
This can be restated as:
P(f, m, c) = f(m ⊕c) −

a∈Bm\{m}
(−2)bc(a)P(f, a, c)
Proof. Using the deﬁnition of a probe, the conclusion can be rewritten as:
f(m ⊕c) =

a∈Bm
(−1)bc(a) 
i∈Ba
(−1)bc(i)f(i ⊕c)
We prove this by induction on bc(m). The base cases of bc(m) = 0 and bc(m) = 1 are
easy. Let m = u ⊕v where u ∧v = 0, u ̸= 0, v ̸= 0. Then

a∈Bm
(−1)bc(a) 
i∈Ba
(−1)bc(i)f(i ⊕c)
=

j∈Bu
(−1)bc(j) 
k∈Bv
(−1)bc(k)

i∈Bj⊕k
(−1)bc(i)f(i ⊕c)
=

j∈Bu
(−1)bc(j) 
r∈Bj
(−1)bc(r) 
k∈Bv
(−1)bc(k) 
s∈Bk
(−1)bc(s)f(s ⊕r ⊕c)
=

j∈Bu
(−1)bc(j) 
r∈Bj
(−1)bc(r)f(v ⊕r ⊕c)
= f(u ⊕v ⊕c) = f(m ⊕c)
⊓⊔
The algorithm takes advantage of previously computed function evaluations by
caching all function evaluations in a hash table. When the function f is applied to a
bit string, this hash table is checked before doing the actual function evaluation.
The basic idea of the bottom-up part of the algorithm (Traverse-Hypergraph) is
to do a breadth-ﬁrst traversal of the lattice of masks, starting with the empty mask, then
looking at the order-1 masks, etc. When a new mask m is considered for inclusion in
the linkage hypergraph, all submasks of order bc(m) −1 are checked for membership
in the hypergraph. If any of these submasks is not in the hypergraph, then m cannot be
in the hypergraph. If these tests succeed, then a sequence of probes is done to determine
if the mask is in the hypergraph.
The backgrounds of the probes can be determined either by using a population or
by randomly choosing background strings. The ﬁrst element of the population or the
ﬁrst background is the all-zeros string since this simpliﬁes the computation of the Walsh
coefﬁcients in the top-down part of the algorithm. If a population is used, the remainder
of the population is chosen randomly. The probe value using the all-zeros background
is saved in the hash-table hypergraph which is also used to determine whether a mask
has been added to the hypergraph.

1012
R.B. Heckendorn and A.H. Wright
In addition to the queue used for the breadth-ﬁrst traversal, the masks added to the
hypergraph are stored in a linked list hypergraphList which is traversed in the top-down
part of the algorithm.
The TestByProbes function does up to N probes using the mask a. If one of these
probes is nonzero (or greater than a tolerance in practice), then it returns the probe value
corresponding to the all-zeros string. If all probes are zero, then it returns null.
Traverse-Hypergraph()
population.initialize()
hypergraphList.initialize()
queue.initialize()
m ←{ }
// Empty mask
ProbeV alue ←TestByProbes(a)
if ProbeV alue ̸= null then
queue.add(m)
hypergraph[m] ←ProbeV alue
end if
while queue.notEmpty() do
m ←queue.remove()
probeV alue ←hypergraph[m]
for all supersets a of m of cardinality bc(m) + 1 do
if all subsets of a of cardinality bc(m) are in the hypergraphList then
ProbeV alue ←TestByProbes(a)
if ProbeV alue ̸= null then
queue.add(a)
hypergraph[a] ←ProbeV alue
hypergraphList.addFirst(a)
end if
end if
end for
end while
The top-down part of the algorithm (Compute-Walsh-Coefs) traverses the hyper-
edges of hypergraph using the list hypergraphList from higher order masks to lower
order, that is in the reverse order from which they were added to the hypergraph. The
Walsh coefﬁcients are computed using only the function evaluations done in the bottom-
up part of the algorithm.
The algorithm is based on Equation 2. This equation would suggest that to compute
wa, one would want to traverse those supersets of a that correspond to hyperedges.
However, in the top-down algorithm we are already traversing these superset hyperedges,
and it is more efﬁcient to add the Walsh coefﬁcient of each these superset hyperedges to
its subsets, and this is what the algorithm does. In other words, as the supersets of a are
traversed in the algorithm, their Walsh coefﬁcients are added to wCoef[a].
Compute-Walsh-Coefs(hypergraphList)
for m ∈hypergraphList do
//traverse in the reverse order from the order added

Efﬁcient Linkage Discovery by Limited Probing
1013
probeV alue ←hypergraph[m]
if wCoef[m] ̸= null then wCoef[m] ←wCoef[m] + probeV alue
else wCoef[m] ←probeV alue end if
for each a ⊂m do
if wCoef[a] ̸= null then wCoef[m] ←wCoef[a] −wCoef[m]
else wCoef[a] ←−wCoef[m] end if
end for
end for
8
Empirical Results
The test function used in this section is an embedded landscape with 50 5-bit subfunctions
and a string length of 50. Each subfunction is linear with a randomly placed “needle”.
The coefﬁcients of the linear function are chosen randomly from the interval [0, 1]. The
needle is a single point with a value of 0.1 greater than the maximum value given by
the linear function. The support of each subfunction is randomly chosen. A value is
considered to be zero if it is less than 10−7. This class of test function represents the
worst case for the algorithms given in this paper.
The algorithm used is that given in Section 7. The algorithm is considered to be suc-
cessful only if it correctly ﬁnds all hyperedges of the hypergraph. On smaller examples,
when the algorithm ﬁnds all hyperedges, it correctly computes all Walsh coefﬁcients.
When the algorithm fails (on this class of functions), it is most likely to fail when doing
the order-2 probes. Thus, the formula of Theorem 7 should be applied with j = 2 and
k = 5. The number of order-2 hyperedges is at most 50
5
2

= 500 since there are
5
2

order-2 hyperedges per subfunction. However, some of these overlap, and the actual
number is about 420.
The algorithm of Section 7 was run for 1000 trials for each of N = 40, 50, 60, 70,
80, 90, where N is the number of probes per potential hyperedge. (i. e., N is the popula-
tion size.) The algorithm was also run with the same parameters using randomly chosen
backgrounds instead of backgrounds from a population. In addition, the ﬁrst equation
of Theorem 7 was solved for the success rate for the same values of N and with j = 2,
k = 5, and J = 420. These are shown in the table on the left below. The table on the
right shows the average number of function evaluations for these experiments.
These results suggest that when f is an embedded landscape with the number of
subfunctions being O(L), then the complexity is given by the formula of Theorem 7.
Further theory and/or experiments are needed to conﬁrm this hypothesis.
Accuracy
N
Theory
Population Random
40
0.1331
0.222
0.168
50
0.5889
0.658
0.648
60
0.8700
0.891
0.888
70
0.9640
0.963
0.967
80
0.9904
0.992
0.992
90
0.9975
1.00
0.995
Function Evaluations
N
Population
Random
40
69008
279526
50
85889
345577
60
102547
411381
70
119241
490876
80
135907
540427
90
152501
604383

1014
R.B. Heckendorn and A.H. Wright
9
Conclusions
There are two contributions of this paper. First, the paper gives a rigorous mathematical
foundationforperturbationalmethodsfordeterminingtheepistaticstructureofafunction
from binary strings to the real numbers. These methods are closely related to the Walsh
basis representation of the function.
Second, the paper gives two new randomized algorithms. The ﬁrst generalizes the
LINC algorithm of [MG99b] and [MG99a] to ﬁnding epistasis of arbitrary order. The
primary parameter in the algorithm is the number of probes. If the function has k-
bounded epistasis (is k-delineable in the terminology of [MG99a]), then rigorous bounds
can be given for the number of probes that are needed, and this leads to a complexity
analysis of the algorithm. The second algorithm generalizes the algorithms of [KP01].
This algorithm both determines the epistatic structure and ﬁnds the Walsh coefﬁcients of
a k-epistatic function. It is more practical when most of the Walsh coefﬁcients of order
less than k are zero. This algorithm is more efﬁcient than the methods of [KP01].
More research is needed in applying this class of algorithms to functions where the
assumptions of k-bounded epistasis and sparseness of the Walsh basis representation
are only approximately satisﬁed. Further research is also needed in understanding how
genetic algorithms and estimation of distribution algorithms can work in tandem to take
advantage of the epistatic structure of functions that can be discovered by algorithms
such as the ones given in this paper.
References
[Bro00]
E. D. Brodie. Why Evolutionary Genetics Doesn’t Always Add Up, pages 3–19.
Oxford University Press, Oxford, England, 2000.
[Hec99]
Robert B. Heckendorn. Walsh Analysis, Epistasis, and Optimization Problem Difﬁ-
cultyforEvolutionaryAlgorithms. PhDthesis,ColoradoStateUniversity,Department
of Computer Science, Fort Collins, Colorado, 1999.
[Hec02]
Robert B. Heckendorn. Embedded landscapes. Evolutionary Computation, 10(4):
345–376, 2002.
[HW99]
R. B. Heckendorn and Darrell Whitley. Predicting epistasis from mathematical mod-
els. Evolutionary Computation, 7(1):69–101, 1999.
[KP01]
H. Kargupta and B. Park. Gene expression and fast construction of distributed evo-
lutionary representation. Evolutionary Computation, 9(1):43–69, 2001.
[MG99a]
Masaharu Munetomo and David E. Goldberg.
Identifying linkage groups by
nonlinearity/non-monotonicity detection.
In W. Banzhaff et. al., editor, Proc. of
the Genetic and Evolutionary Computation Conference, volume 1, pages 433–440,
Palo Alto, CA, 1999. Morgan Kaufmann Publishers, Inc.
[MG99b]
Masaharu Munetomo and David E. Goldberg.
Linkage identiﬁcation by non-
monotonicity detection for overlapping functions. Evolutionary Computation, 7(4):
377–398, 1999.
[MM99]
Heinz M¨uhlenbein and Thilo Mahnig. Convergence theory and application of the
factorized distribution algorithm. Journal of Computing and InformationTechnology,
7(1):19–32, 1999.
[MMR99]
Heinz M¨uhlenbein, Thilo Mahnig, andAberto O. Rodriguez. Schemata, distributions
and graphical models in evolutionary optimization. J. of Heuristics, 5:215–247, 1999.

Distributed Probabilistic Model-Building
Genetic Algorithm
Tomoyuki Hiroyasu1, Mitsunori Miki1, Masaki Sano1, Hisashi Shimosaka1,
Shigeyoshi Tsutsui2, and Jack Dongarra3
1 Doshisha University, Kyoto, Japan,
tomo@is.doshisha.ac.jp,
2 Hannan University, Osaka, Japan,
3 University of Tennessee, TN, USA
Abstract. In this paper, a new model of Probabilistic Model-Building
Genetic Algorithms (PMBGAs), Distributed PMBGA (DPMBGA), is
proposed. In the DPMBGA, the correlation among the design variables
is considered by Principal Component Analysis (PCA) when the oﬀ-
springs are generated. The island model is also applied in the DPMBGA
for maintaining the population diversity. Through the standard test func-
tions, some models of DPMBGA are examined. The DPMBGA where
PCA is executed in the half of the islands can ﬁnd the good solutions in
the problems whether or not the problems have the correlation among
the design variables. At the same time, the search capability and some
characteristics of the DPMBGA are also discussed.
1
Introduction
Genetic Algorithms (GAs) are stochastic search algorithms based on the me-
chanics of natural selection and natural genetics[1]. The GAs can be applied to
several types of optimization problems by encoding design variables to individ-
uals. Recently, a new type of GA called the Probabilistic Model-Building GA
(PMBGA)[2] or Estimation of Distribution Algorithm (EDA)[3] have been the
focus. In the canonical GA, children are generated from parents that are selected
randomly. However, in the PMBGA and EDA, the good characteristics of par-
ents are inherited by children using the statistical information. Since children
must have the characteristics of parents, eﬀective searching is expected. It is
reported that the PMBGA and EDA have a better search ability than that of
the canonical GA.
To make an eﬀective search in continuous problems, the correlation among
the design variables should be handled. Therefore, new searching points should
be generated so that the correlation exists in the new points. Many real coded
GAs where the real vectors are used as a genotype treat this correlation problem.
One of the typical real coded GAs is Unimodal Normal Distribution Crossover
(UNDX)[4]. The UNDX is good at ﬁnding the optimum in the functions where
there is strong correlation between the design variables. Takahashi et al. intro-
duced a new method[5]; they used the Principle Component Analysis (PCA) and
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1015–1028, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

1016
T. Hiroyasu et al.
the Individual Component Analysis (ICA). In the Takahashi et al. algorithm,
by using the PCA and the ICA, the individuals are transferred into the space.
Then, Blend Crossover (BLX-α) [6] is performed to the transferred individu-
als. The correlation between the design variables is considered by using both
ICA and PCA. At the same time, the diversity of the solutions is maintained
by using BLX-α. Besides GAs, some of the evolutionary strategies[7] have the
operation for the correlation relationship among the design variables. One of
them is the correlated mutation method that is proposed by Schwefel[8]. In this
method, there is a parameter that indicates the direction of the distribution of
the individuals. By using this parameter, the mutation operation is performed
by considering the correlation relationship among the design variables.
In this paper, a new PMBGA for continuous problems is proposed, which
is called Distributed PMBGA (DPMBGA). This is one of the real coded GAs
and the real vectors are treated as a genotype. In this algorithm, PCA is used
for transforming the set of the solutions. This operation handles the correlation
relationship among the design variables. The PMBGA sometimes lacks the di-
versity of the solutions during the search. To overcome this problem, a model of
the distributed GA is performed.
In this paper, the basic algorithm of the DPMBGA is explained. In this pa-
per, the DPMBGA is applied to solve test functions. Through these experiments,
the following four topics of the DPMBGA are discussed. Firstly, the DPMBGA
that is based on the distributed scheme is examined. Secondly, the search ca-
pability of the DPMBGA is compared with UNDX and Minimum Generation
Gap. From the results, it is found that the PCA prevents the eﬀective search
in some functions. The discussion of the results is the third topic. Finally, the
search capability of the DPMBGA for functions whose optimum is located near
the boundary is discussed.
2
Distributed Probabilistic Model-Building Genetic
Algorithm
2.1
Flow of DPMBGA
In this paper, a new PMBGA is proposed: that is Distributed Probabilistic
Model-Building Genetic Algorithm (DPMBGA). The DPMBGA uses the dis-
tributed GA schem[9,10]. Therefore, there are several subpopulations and the
migration operation is performed. In the DPMBGA, the following procedure is
performed for the migration operation. The topology of the migration is a ring.
This ring topology is formed randomly when the migration is performed. In the
ring, migrated individuals are moved from one subpopulation to the other in one
direction. The migrated individuals are chosen randomly and they are substi-
tuted for the individuals whose ﬁtness values are the worst in the subpopulation.
In the DPMBGA, the following procedures are performed at the generation
t.(Fig. 1).

Distributed Probabilistic Model-Building Genetic Algorithm
1017
  
   
   
  

  





x
y
y
v
v
 
Fig. 1. DPMBGA
1. The elite individual is reserved.
2. The individuals who have the good values of the ﬁtness are sampled.
3. The above individuals are transferred by the PCA into the new space.
4. The new individuals are generated in the new space.
5. The new individuals are transferred into the original space.
6. The new individuals are substituted for the old individuals.
7. The mutation is operated.
8. When the reserved elite individuals are eliminated, they are recovered.
9. The new individuals are evaluated.
In the following sections, each operation is explained precisely.
2.2
Sampling Individuals for Probabilistic Model
The following operation is performed in each island. The individuals who have
the good evaluation values are chosen from each island Psub(t). The number of
these individuals is determined with the sampling rate Rs. These individuals
become sample individuals S(t). The new individuals are generated from the
information of these sampling individuals. These sampled individuals are cho-
sen according to the higher ﬁtness values. However, the same individual is not
chosen repeatedly, hence the total number of S(t) is ﬁxed. When the number
of individuals is low, the individuals are generated randomly and added to S(t)
and S(t) exists in each subpopulation.
2.3
Sampling Individuals for PCA
This operation is also performed in each island. S(t) is transferred by the PCA
operation. The PCA is determined using the information of individual set T(t).

1018
T. Hiroyasu et al.
T(t) is diﬀerent from S(t) and T(t) is formed in the following way. T(t) consists
of the individuals who are the best in each generation. Even when the number
of T(t) is less than the particular size, the new individuals are not added. When
the size of T(t) is exceeded, the worst individual is eliminated one by one. By
this operation, the arbitrary number of the individuals can be used for the in-
formation of PCA. This is independent from the number of the subpopulation.
T(t) also exists in each island.
2.4
PCA Transformation
The average of T(t) is subtracted from T(t) and T(t) becomes matrix T
(nT(t)column × Dline). The average of T(t) is also subtracted from S(t) and
S(t) becomes X (nS(t)column × Dline). Then, the covariance matrix S of T
is derived and the eigen values and vectors are obtained. S is a real symmetric
matrix and derived as follows,
S =
1
nS(t) −1T T T .
(1)
The eigen vector indicates the axis of the new space.
Using the derived eigen vectors, the design variables X of the solution set
S(t) are transferred. After the transfer into the new space, there is no correla-
tion among the design variables. The coordinate transfer matrix consists of the
vectors V = [v1, v2, . . . , vD]. After multiplying V , the vector X becomes Y .
The coordinate of Y corresponds to the eigen vectors.
2.5
Generation of New Individuals
The new individuals are generated using the normal distribution of the informa-
tion of Y . Each value of the design variable in a new individual is also determined
one by one independently. Therefore, when there are n design variables in an
individual, there should be n diﬀerent normal distributions. The normal distri-
bution is formed as follows; the average is the same as the average value of the
target design value of Y . The distribution is derived by multiplying the distribu-
tion of Y by the parameter Amp. The values of design variables are determined
randomly, but the total distribution of the new individuals should be the same
as the formed normal distribution. The number of the created new individuals
is the same as the number of individuals in an island (nP(t)) and the generated
individuals are stored in Y offs.
2.6
Restoring Correlation and Substitution of Old Individuals with
New Individuals
Y is the transferred set of X into the new space. In this step, the derived Y offs
is then substituted into the original space. Y offs is multiplied by the inverse of
V . After this operation, the set of Y offs is in the original space.

Distributed Probabilistic Model-Building Genetic Algorithm
1019
Xoffs = Y offs · V −1
(2)
The average of Xoffs is added to the new individuals. These new individuals
are substitute for the old ones P(t) and those become P(t + 1).
2.7
Mutation
The values of the design variables are changed randomly within the constraints
using the mutation ratio Rmu.
2.8
Preservation and Recovering of Elite Individuals
The elite individuals are preserved as E(t). The number of the preserved indi-
viduals is nE(t). After the substitution of the new individuals generated from
the probabilistic model, the elite individuals are recovered in the total popula-
tion. In this case, the elite individuals E(t) are substituted with the individuals
P(t + 1) whose evaluation values are not good.
3
Test Functions and Used Parameters for Numerical
Experiments
In the following sections, the search capability and the characteristics of the
DPMBGA are discussed. These discussions are illustrated through numerical
experiments. In this section, the test functions and parameters for these experi-
ments are explained.
The DPMBGA is used to ﬁnd optimum solutions of the following ﬁve test
functions: the Rastrigin function, Schwefel function, Rosenbrock function, Ridge
function, and Griewank function. All of test functions are minimization prob-
lems. The global optimums are located at O. There are 10 dimensions of the
design variables in the Schwefel function and 20 dimensions of the design vari-
ables in the rest of the functions.
There is no correlation between the design variables in the Rastrigin function
and the Schwefel function. There are many sub-peaks in the landscape of these
functions. On the other hand, there is a correlation between the design variables
in the Rosenbrock function and the Ridge function. In these test functions, there
is only a peak in a landscape. In the Griewank function, there is a correlation
between the design variables and many peaks in the landscape.
FRastrigin = 10n +
n

i=1

x2
i −10 cos(2πxi)

(3)
(−5.12 ≤xi < 5.12)

1020
T. Hiroyasu et al.
FSchwefel =
n

i=1
−xi sin

|xi|

−C
(4)
(C : optimum.)
(−512 ≤xi < 512)
FRosenbrock =
n

i=2

100(x1 −x2
i )2 + (1 −xi)2
(5)
(−2.048 ≤xi < 2.048)
FRidge =
n

i=1

i

j=1
xj
2
(6)
(−64 ≤xi < 64)
FGriewank = 1 +
n

i=1
x2
i
4000 −
n

i=1

cos
 xi
√
i

(7)
(−512 ≤xi < 512)
The parameters used in these experiments are summarized in Table 1.
Table 1. Parameters
Population size
512
Number of elites
1
Number of islands
32
Migration rate
0.0625
Migration interval
5
Archive size for PCA
100
Sampling rate
0.25
Amp. of Variance
2
Mutation rate
0.1/ (Dim. of function)
4
Discussion on Eﬀectiveness of PCA and Distributed
Environment Scheme
In the DPMBGA, the new individuals are generated using the PCA. By this
operation, the information from the correlation among the design variables that
is found during the search is reﬂected to the new individuals. In the problems
where there is a correlation among the design variables, the distribution of the
value of each design variable is not independent from each other. Therefore,

Distributed Probabilistic Model-Building Genetic Algorithm
1021
the distribution of each design variable is aﬀected by the distribution of the
other design variables. In the new model, at ﬁrst, the set of the individuals
is transferred into the space where there is no correlation among the design
variables. After this transformation, it is easy to generate new individuals using
their information of the distribution. Therefore, it is expected that the DPMBGA
can perform an eﬀective search in the problems where there is a strong correlation
among the design variables. In the numerical experiments, the following three
models are discussed.
Model 1 : In every island, the PCA is performed.
Model 2 : In every island, the PCA is not performed.
Model 3 : In half of the islands, the PCA is performed.
In these models, the number of the islands where the PCA is performed is dif-
ferent. The model 1 is the same model explained in section 2. In model 2, the
PCA is not performed at all. In model 3, the distributed environment scheme is
applied for using the PCA.
The Distributed Environment GA (DEGA) is one of the distributed GA
schemes that Miki et al. proposed[11]. In the DEGA, the diﬀerent parameters or
the diﬀerent operations are performed in each island. It is well known that the
search capability of GA depends on the value of the parameters. The optimum
values of these parameters also depend on the targeted problems. Therefore,
preliminary experiments are necessary in order to derive the optimum values of
the parameters. In the DEGA, the values of the parameters and the operations
are diﬀerent in each island. These parameter values are not the best but can
derive adequate solutions. In this paper, the DEGA scheme is applied; the CPA
is performed in some islands and not in other islands.
In Table 2, the number of the trials where the optimum value is derived
is summarized in using 20 trials. The higher number means that the robust
search has been performed. In Figure 2, the average number of the evaluations
is illustrated when the simulation derives the optimum values. The model that
has the smaller number of this value may be the better model.
Table 2. Number of times that the threshold is reached
model 1 model 2 model 3
Rastrigin
0
20
20
Schwefel
20
20
20
Rosenbrock
20
0
20
Ridge
20
20
20
Griewank
19
17
20
In the Schwefel function, all the models derive the optimum solutions in
every trial with the small number of the evaluations. In the Rastrigin function,
the search ability of model 1 is worse than the other models. That means the

1022
T. Hiroyasu et al.
  	
  	   











   
 
 
 !
Fig. 2. Comparison of search capability between models
PCA does not help for achieving a good solution. In the Rosenbrock function
that has the correlation relationship among the design variables, model 2 (where
the PCA is not performed) cannot derive the optimum solutions. In the Ridge
function that also has the correlation, model 2 can derive the optimum solutions
but needs many evaluations compared to the other models. These results suggest
that the PCA should have a positive eﬀect on the search in these problems.
From these results, the following point is made clearer; the PCA is useful
for the problems that have the correlation among the design variables but not
useful for the problems that do not have the correlation. Therefore, the eﬀect
of the PCA depends on the type of the problems. On the other hand, model
3 where the PCA operations are performed in some islands but not in others
is good at ﬁnding optimum solutions in every function. The Griewank function
has the correlation among the design variables and has also many peaks in the
landscape. Therefore, it is a diﬃcult problem to ﬁnd the optimum. Model 1 and
2 did not ﬁnd the optimum solutions in some trials. On the other hand, model
3 derived the optimum in all the trials. From these results, there is a possibility
that model 3 can ﬁnd the optimum solutions not only in the problems whether
the correlation of the design variables exists or not but also in the problems where
there are many peaks in the landscape. Because of the result in this section, in
the following discussions, model 3 is used.
5
Comparison of DPMBGA with UNDX + MGG
In this section, the search capability of the DPMBGA is compared with the
conventional real-coded GA. The comparison real-coded GA is UNDX[4] with
Minimal Generation Gap (MGG)[12,13]. In the UNDX, two new individuals
are generated from three individuals. Two of the parent individuals form the
main axis with the normal distribution generated on this main axis. The third
parent determines the variance of the normal distribution. The child individuals
are generated in accordance with this normal distribution. Using the UNDX,
an eﬀective search can be performed with the consideration of the correlation
between the design variables.

Distributed Probabilistic Model-Building Genetic Algorithm
1023
MGG is one of generation alternation models. When the generation alter-
nation occurs in the MGG, the following procedure is performed. Two of the
parent individuals are chosen from the population randomly from which child
individuals are generated by the crossover with the parent one time or many
times. From the set of the child and parent individuals, the individuals that re-
main as the next generation are selected. These individuals are then substituted
for the parent individuals and are backed to the total population. The MGG has
the characteristics to maintain the diversity of the solutions during the search
since the selection is limited to the small number of solutions.
In Figure 3, the transitions of the DPMBGA search and the UNDX+MGG
model are shown. The horizontal axis shows the number of evaluations and the
vertical axis shows the average ﬁtness values in 20 trials. In the UNDX+MGG
model, some of 20 trials did not get the optimum solution. The average values
of the trials where the optimum solutions are derived are illustrated. These are
minimization problems with smaller ﬁtness values indicating the better solutions.
For the parameters of UNDX+MGG, there are 300 individuals for the func-
tions with many peaks and there are 50 individuals for the functions with only
one peak. The number of crossover is 100 and α = 0.5. β = 0.35 is used.
                
  	   	
  	
  	
  	
     
  

  
 
 !"#$
% &'"
                
  	   	
  	
  	
  	
     
  
()*
  
 
 !"#$
% &'"
                
  	   	
  	
  	
  	
     
  
+,
  
 
 !"#$
% &'"
                
  	   	
  	
  	
  	
     
  
+)
  
 
 !"#$
% &'"
                
  	   	
  	
  	
  	
     
  
+-,
  
 
 !"#$
% &'"
Fig. 3. History of average of evaluation values
Figure 3 indicates that the results of the DPMBGA are better than the other
models in the problems whether a correlation relationship between the design
variables exists or not. Therefore, it can be concluded that the DPMBGA is a
useful GA for these continuous functions.

1024
T. Hiroyasu et al.
6
Discussion on Case Where PCA Does Not Work
Eﬀectively
In the former section, it is found that the PCA operation sometimes prevents the
eﬀective ﬁnding of the optimum. In the Rastrigin function, the model without
the PCA operation derived better solutions than the model with the PCA. In this
section, the reasoning for why the PCA does not work eﬀectively is discussed.
One reason may be the early convergence of the solutions in the archive. The
PCA uses the information from the solutions in the archive and it is important for
the eﬀective search that this archived information should reﬂect the information
of the real landscape of the problem. If these solutions in the archive are not
renewed, the proper transfer by the PCA cannot be expected to occur. Since the
Rastrigin function has many sub-peaks on the landscape, it may be that many
solutions are stuck in the sub-peaks. Then, the solutions of the archive are not
renewed possibly preventing an eﬀective search.
This assumption is illustrated with numerical experiments. In these ex-
periments, the model where the PCA operation is performed in all the sub-
populations is used. The target test functions are the Rastrigin function and the
Rosenbrock function.
In Figure 4, the history for the renewal of the archive is shown. The horizontal
axis shows the number of the evaluations and the vertical axis shows the number
of the renewed individuals. From this ﬁgure, it is obvious that the number of the
renewed individuals becomes small, especially in the latter part of the search in
the Rastrigin function.
Fig. 4. History of number of updated individuals in archive of the best individuals
Conversely, in the Rosenbrock function, the number of the renewed individ-
uals does not decrease and most of the individuals of the archive are always
renewed.
In Figure 5, the history of the search is illustrated. The horizontal axis shows
the number of evaluations and the vertical axis shows the average value of the
evaluation in 20 trials. This is the minimizing problem; the smaller evaluation
value indicates the better solution. The term ”normal” in this ﬁgure indicates

Distributed Probabilistic Model-Building Genetic Algorithm
1025
the result of the normal model. ”erase/10” indicates the result of the model
where the archive of the individuals is eliminated every 10 generations.
Fig. 5. History of average of evaluation values in the model in which archive is erased
each 10 generation
From this ﬁgure, the result of the ”erase/10” model is better than the normal
model in the Rastrigin function. On the other hand, the result of the ”normal”
model is better than the ”erase/10” in the Rosenbrock function. These results
point out that the archive eﬀect is worse in the Rastrigin function.
In conclusion, one of the reasons that the PCA operation prevents an eﬀective
search in the Rastrigin function is stagnation of the renewal of the individuals
in the archive.
7
Discussion on Search Capability of DPMBGA for
Functions Whose Optimum Is Located Near the
Boundary
When a normal distribution is used in the crossover operation, it is often said
that the real-coded GA is good at ﬁnding the solution in the problem where the
optimum is located at the center of the search area but is not good at ﬁnding
the solution in the problem where the optimum is located at the boundaries [14].
One solution to this problem is Boundary Extension by Mirroring (BEM) [15].
In the BEM, the solutions that violate the constraints can exist when these
solutions are within certain distance. The distance is determined by the extension
rate re (0.0 < re < 1.0).
The DPMBGA is one of the real-coded GAs that may be weak at ﬁnding the
solutions in the problems where the optimum is located in the boundary. At the
same time, when the optimum is located on the boundary, the probabilistic model
may be diﬀerent from the real distribution of the individuals. This situation may
prevent an eﬀective search. The search capability of the DPMBGA for problems
where the optimum solutions is located near the boundary is discussed in this

1026
T. Hiroyasu et al.
section. The search capability of the DPMBGA is compared with the model
using the BEM.
The test functions are modiﬁed to have their optimum solutions near their
boundaries. The ranges of the functions are summarized in Table 3.
Table 3. Domain of objective functions
Function
Optimal solution
Domain
Rastrigin
0.0
[0 , 5.12]
Schwefel
420.968746
[-512, 421]
Rosenbrock 1.0
[-2.048, 1]
Ridge
0.0
[0, 64]
In Figure 6, the transition of the search is expressed. The horizontal axis
shows the number evaluations and the vertical axis shows the average of the
ﬁtness values for 20 trials.
These ﬁgures illustrate that the proposed model where BEM is not used
derives better solutions. In the proposed model, the individuals who are out of
the feasible region are pulled back on the closest boundary of the feasible region.
The search concentrates on the individual with a good evaluation value by the
DPMBGA. When the optimum solution is on or near the boundary, the search is
concentrated near the boundary. This may be a reason why the proposed model
is better than the model using the BEM. Thus, the DPMBGA derives the good
solution for the type of problems with constraints that are the upper and lower
boundaries of the design variables.
8
Conclusions
In the DPMBGA, the correlation among the design variables is analyzed by
PCA. The new individuals are generated from the probabilistic model that is
formed with the individuals that have good ﬁtness values. However, before gen-
eration, the individuals that are used for forming the probabilistic model are
transferred by PCA into the space where there is no correlation among the de-
sign variables. Then the new individuals are generated and these are placed into
the original space. From this operation, the generated new individuals may have
the correlation among the design variables. Therefore, an eﬀective search may
be expected. At the same time, the island model is utilized for maintaining the
diversity of the solutions during the search.
The DPMBGA is applied to ﬁnd the optimum solutions of the test func-
tions. Through these numerical experiments, the following four topics are made
clariﬁed.
Firstly, the DPMBGA with PCA operations is useful for ﬁnding the solutions
in the test functions where there is a correlation relationship among the design

Distributed Probabilistic Model-Building Genetic Algorithm
1027
   
  
 
  
     	
  
  
  
     
 
  


  !""#
   
  
 
  
     	
  
  
  
     
  
 
  


$% !""#
   
  
     	
  
  
  
     
  
 
  


&$'( !""#
   
  
     	
  
  
  
     
  
 
  


"  !""#
Fig. 6. History of the average of evaluation values on functions with an optimum at
the edge of search space
variables. On the other hand, the PMBGA without PCA is good at ﬁnding the
optimum in the functions where there is no correlation relationship among the
design variables. From these results, the new model of PMBGA that is based
on the distributed environment scheme where PCA is performed in only half
of the subpopulations is proposed. The DPMBGA is very useful for ﬁnding
the optimum in the functions whether or not there is a correlation relationship
among the design variables or not.
Secondly, the results of the DPMBGA are compared to those of UNDX with
MGG. This comparison shows that the DPMBGA has higher search capability.
In the DPMBGA, the Principle Component Analysis is used to analyze the
correlation between the design variables. However, the PCA does not work eﬀec-
tively for ﬁnding the optimum solutions in some test functions. The reason for
this problem is the third discussion. Numerical experiments conclude that one
of the reasons the PCA operation prevents the eﬀective search in the function
is stagnation of the renewal of the individuals.
Finally, the DPMBGA is used to ﬁnd the solutions in the functions where the
optimums are located at the edge of the feasible region. In the DPMBGA, when
the new individuals violate the constraints, the solutions are pulled back on the
boundary. This operation is compared with the BEM. Numerical experiments
illustrate that the operation in the proposed method is better than BEM in these
test functions.

1028
T. Hiroyasu et al.
References
1. Goldberg, D.E.: Genetic Algorithms in Search Optimization and Machine Learnig.
Addison-Wesley (1989)
2. Pelikan, M., Goldberg, D.E., Lobo, F.: A Survey of Optimization by Building and
Using Probabilistic Models. Technical Report 99018, IlliGAL (1999)
3. Larranaga, P., Lozano, J.A.: Estimation of Distribution Algorithms. A New Tool
for Evolutionary Computation. Kluwer Academic Publishers (2001)
4. Ono, I., Satoh, H., Kobayashi, S.: A Real-Coded Genetic Algorithm for Function
Optimization Using the Unimodal Normal Distribution Crossover. Transactions of
the Japanese Society for Artiﬁcial Intelligence 14 (1999) 1146–1155
5. Takahashi, M., Kita, H.: A Crossover Operator Using Independent Component
Analysis for Real-Coded Genetic Algorithms.
Proc. 13th SICE Symposium on
Decentralized Autonomous Systems (2001) 245–250
6. Eshelman, L. J. and Shaﬀer, J. D.: Real-coded Genetic Algorithms and Interval
Schemata. Foundation of Genetic Algorithms Morgan Kaufmann Publishers (1993)
182–202
7. B¨ack, T., Hoﬀmeister, F., Schwefel, H.P.: A Survey of Evolution Strategies. Proc.
4th International Conference on Genetic Algorithms (1991) 2–9
8. Hansen, N.: Invariance, Self-Adaptation and Correlated Mutations and Evolution
Strategies. Parallel Problem Solving from Nature – PPSN VI, 6th International
Conference, 2000, Proceedings (2000) 355–364
9. Tanese, R.: Distributed Genetic Algorithms. Proc. 3rd International Conference
on Genetic Algorithms (1989) 434–439
10. Cant´u-Paz, E.: Eﬃcient and accurate parallel Genetic Algorithms. Kluwer Aca-
demic publishers (2000)
11. Miki, M., Hiroyasu, T., Kaneko, M., Hatanaka, K.: A Parallel Genetic Algorithm
with Distributed Environment Scheme. IEEE Proceedings of Systems, Man and
Cybernetics Conference SMC’99 (1999)
12. Sato, H., Ono, I., Kobayashi, S.: A New Generation Alternation Model of Genetic
Algorithms and its Assessment. Transactions of the Japanese Society for Artiﬁcial
Intelligence 12 (1997) 734–744
13. Yamamura, M., Satoh, H., Kobayashi, S.: An Analysis on Generation Alternation
Models by using the Minimal Deceptive Problems. Transactions of the Japanese
Society for Artiﬁcial Intelligence 13 (1998) 746–756
14. Someya, H., Yamamura, M.: Toroidal Search Space Conversion for Robust Real-
Coded GA. Proc. 13th SICE Symposium on Decentralized Autonomous Systems
(2001) 239–244
15. Tsutsui, S.: Multi-parent Recombination in Genetic Algorithms with Search Space
Boundary Extension by Mirroring.
Proc. the 5th International Conference on
Parallel Problem Solving from Nature (PPSN V) (1998) 428–437

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1029–1040, 2003.
© Springer-Verlag Berlin Heidelberg 2003
 HEMO: A Sustainable Multi-objective Evolutionary
Optimization Framework
Jianjun Hu
1,2, Kisung Seo
1, Zhun Fan
1, Ronald C. Rosenberg
3, and Erik D. Goodman
1
1Genetic Algorithms Research and Applications Group (GARAGe)
2Department of Computer Science & Engineering
3Department of Mechanical Engineering
Michigan State University, East Lansing, MI, 48824
{hujianju, ksseo, fanzhun, rosenber, goodman}@egr.msu.edu
Abstract. The capability of multi-objective evolutionary algorithms (MOEAs)
to handle premature convergence is critically important when applied to real-
world problems. Their highly multi-modal and discrete search space often
makes the required performance out of reach to current MOEAs. Examining the
fundamental cause of premature convergence in evolutionary search has led to
proposing of a generic framework, named Hierarchical Fair Competition
(HFC)
[9], for robust and sustainable evolutionary search. Here an HFC-based Hi-
erarchical Evolutionary Multi-objective Optimization framework (HEMO) is
proposed, which is characterized by its simultaneous maintenance of individuals
of all degrees of evolution in hierarchically organized repositories, by its con-
tinuous inflow of random individuals at the base repository, by its intrinsic hier-
archical elitism and hyper-grid-based density estimation. Two experiments
demonstrate its search robustness and its capability to provide sustainable evo-
lutionary search for difficult multi-modal problems. HEMO makes it possible to
do reliable multi-objective search without risk of premature convergence. The
paradigmatic transition of HEMO to handle premature convergence is that in-
stead of trying to escape local optima from converged high fitness populations,
it tries to maintain the opportunity for new optima to emerge from the bottom up
as enabled by its hierarchical organization of individuals of different fitnesses.
1
Introduction
After a decade of intensive study on evolutionary multi-objective optimization (EMO),
extensive insight has been obtained regarding convergence and the diversity of the
Pareto front. Several successful multi-objective EAs have emerged, such as PESA [1],
NSGA-II [4], and SPEA2 [13]. However, the capability to handle premature conver-
gence for difficult multi-modal optimization problems has attracted insufficient atten-
tion. The performances of modern MOGAs are usually compared on some easy con-
tinuous test problems [5]. The scalability of MOEAs is focused on the scalability over
the objective dimension rather than over the problem difficulty [11]. Unfortunately,

1030         J. Hu et al.
many real-world problems are characterized as highly multi-modal in highly discrete
search spaces. Without careful attention to the premature convergence issues, modern
MOGAs will easily fail to find the true Pareto fronts for these problems [3] and the
performance comparison results will be misleading for MOGA practitioners.
Based on the research on dealing with premature convergence of single-objective
EA search [9,10], a sustainable multi-objective optimization framework called HEMO
(Hierarchical Evolutionary Multi-objective Optimization) is proposed in this paper. In
addition to the external Pareto archive commonly found in PESA and SPEA, HEMO
features  hierarchically organized archives of individuals with different fitness ranks, a
“workshop” subpopulation associated with each archive, and a random individual
generator that continually feeds raw genetic material into the lowest-level archive. By
incorporating favorable features from PESA [1], SPEA [12], and HFC (Hierarchical
Fair Competition) EA model [9, 10], and extending ideas from the improved NSGA-II
[3], this framework promises to have strong capability to avoid premature conver-
gence in EMO and thus to constitute a sustainable search procedure for solving diffi-
cult real-world problems.
2
Convergence, Diversity, and Premature Convergence in EMO
From the first generation of modern MOGAs such as NSGA, SPEA, and PAES to the
improved versions like NSGA-II, SPEA2 and PESA, much attention is allocated to
diversity maintenance of the Pareto front by estimating the density of individuals
along the Pareto front (SPEA2, NSGA-II, PESA), ensuring sufficient selection pres-
sure in special cases (SPEA2), utilization of elitism (NSGA-II), and other efforts to
obtain computational efficiency. However, the diversity along the Pareto front is dif-
ferent from the diversity required for avoiding premature convergence, which is la-
beled as lateral diversity in [3]. The capability to maintain lateral diversity varies
widely among MOGAs, which contributes much to the performance differences for
different test problems.
2.1   Performance Comparison of Modern MOGAs
In terms of lateral diversity maintenance, PESA, NSGA-II, and SPEA2 have different
strategies, which largely determine their advantages and disadvantages. Among the
three, PESA is the greediest algorithm. By selecting the mating pool only from the
currently discovered Pareto front, it is on one extreme of elitism and depends strongly
on the mutation operator for exploration. As a result, PESA has the fastest conver-
gence speed, but is only good for continuous, relatively simple problems. It is shown
to be inferior on the T4 test function, for example, which is a continuous multi-modal
problem [1]. It can be expected that the uncontrolled, extreme elitism of PESA will
make PESA unusable for highly discrete multi-modal problems.
By maintaining a constant size of the archive (parent) population, SPEA2 and
NSGA-II allow the persistence of dominated individuals in cases in which the non-
dominated individuals do not fill the archive population. So for some continuous mul-
timodal test functions such as QV and KUR [14], SPEA2 and NSGA-II are shown to

HEMO: A Sustainable Multi-objective Evolutionary Optimization Framework         1031
be able to achieve good performance. However, for other multimodal problems in
which there are too many non-dominated individuals, SPEA2 and NSGA-II will al-
ways select mating individuals from the current Pareto front, in effect degrading to the
extreme elitism case of PESA. This uncontrolled elitism makes NSGA-II without
mutation perform poorly on difficult multimodal problems such as ZDT4, ZDT6, and
Griewank [3]. As high mutation rate is not the solution to premature convergence,
even with mutation, NSGA-II will fail on other difficult multimodal problems. To
explicitly maintain the dominated individuals to promote lateral diversity, Deb and
Goel [3] proposed the controlled elitism NSGA-II, which turns out to be very success-
ful. The basic idea is to allocate a predefined distribution of individuals to each current
Pareto front in NSGA-II. However, as the fronts in NSGA-II usually move in clusters
to better regions of the objective space based on limited evaluations (for minimization
problems), there is increasing risk that all fronts get trapped in local Pareto fronts, and
gradually, the exploratory capability will be lost. This is attributable to the fitness
assignment scheme of NSGA-II, which is based on the relative fronts, and on the
convergent nature of conventional GAs.
2.2   Premature Convergence and the Issue of Exploitation vs. Exploration
To a large extent, the premature convergence problem in EMO is similar to of the
situation in single-objective EAs. Most previous studies attributed the cause of pre-
mature convergence to the loss of diversity of the population and proposed various
diversity-oriented approaches to increase the population diversity by “brute force.”
Representative methods include increasing the mutation rate; introducing random
individuals into highly converged populations, and using diversity-detection and in-
creasing techniques. All these methods are shown to ameliorate only partially the
premature convergence problem. For example, in genetic programming, a high muta-
tion rate usually destroys the good solutions evolved and, despite the diversity of the
population, no progress can be made with this “brute-force” diversity maintenance.
Actually, the loss of diversity is only a symptom of premature convergence. The
more fundamental reason is, instead, the loss of exploratory capability. In single-
objective EAs, the absolute average fitness of the whole population is constantly in-
creasing as the result of fitness-biased selection. The consequence is that “new ex-
plorer” individuals (i.e., early individuals in a new region of the search space),
whether the offspring of mutation or crossover or randomly generated, find it increas-
ingly hard to survive, since these explorer individuals usually have low fitness until
sufficient exploration in the new search region is conducted. Rare high-fitness “ex-
plorer” individuals, due to their sparseness, will also have high risk of getting lost as
the result of sampling bias in parent selection toward more crowded areas, similar to
the analysis in [2]. To fight against this “unfair competition” among highly evolved
individuals and new “explorers”, there must be some mechanism to protect new ex-
plorers. This is achieved to some extent by widely used approaches such as fitness
sharing and crowding. However, using horizontal expansion in the genetic space, these
techniques usually suffer from the problem of balancing a limited population size
against a huge number of local optima in difficult multi-modal problems.

1032         J. Hu et al.
Another perspective on premature convergence can be obtained by examining
building block concepts. The evolution process is widely seen as a process in which
different building blocks become co-adapted to achieve higher and higher fitness by
mixing and mutation. The higher the fitness of an individual, the stronger the coupling
of its subcomponents, and the more difficult to make large modification of the highly
evolved individual without destroying the co-adaptation relationship. So the explora-
tory capability decreases with increasing fitness of the population. It is similar to the
Cambrian explosion in the evolution of living organisms, during which most existing
species (body plan innovations) were created. However, by allocating all the search
effort to highly evolved individuals, without control, conventional EAs essentially
discard the low-fitness evolution stages after limited mixing experiments, and thus are
essentially convergent algorithms.
NSGA-II, with its controlled elitism [3], is one of the first algorithms that pays spe-
cial attention to dominated inferior individuals. However, while derived from the
conventional EA framework, the improved NSGA-II still suffers from the tendency
that all individuals in the fronts are moving toward the best yet-discovered regions of
the objective space, based on limited mixing experiments, and the components are
increasingly co-adapted to each other (Fig. 1). As the result, the exploratory capability
of the population is gradually lost and premature convergence occurs. The distribution
of individuals to the relatively diverse fronts is insufficient to avoid this kind of pre-
mature convergence.
Based on the analysis above, it turns out to be important to maintain intermediate
individuals and to make the building block mixing process occur at all fitness levels.
This naturally provides a mechanism to ensure fair competition and protects “ex-
plorer” individuals.  At the same time, to reduce the large population size requirement
Fig. 1. The population of NSGA-II moves in clusters leaving the initial low objective value
space and converging to the promising space. Even the maintenance of a predefined proportion
of population into all fronts, but in the whole these fronts are converging to local areas, thus
making it incapable to maintain the explorative capability in the long run.
Converging of NSGA-II
0.0
.2
.4
.6
.8
1.0
1.2
Y 
0
50
100
150
200
250
Generation 1
Generation 10
Generation 100
Generation 500

HEMO: A Sustainable Multi-objective Evolutionary Optimization Framework         1033
[7] for difficult problems, it is desirable to continuously introduce random individuals
into the lowest fitness levels to provide the required building blocks, rather than de-
pending on a large initial population to identify them, as is done in messy GA [8]. This
suggests the assembly line structure of the subpopulations in the HFC framework
proposed in [9]. The HEMO framework is thus an extension of HFC to multi-objective
optimization, incorporating ideas from SPEA, PESA and the improved NSGA-II.
2.3   Combining Ideas in SPEA, PESA, and the Improved NSGA-II
The different performances of SPEA, PESA and NSGA-II over different test functions
reflect the unique, positive features of each approach.  HEMO will attempt to capture
some features of each of these. Specifically, the maintenance of an external Pareto
archive and the breeding population first proposed in SPEA [12] is employed in the
HEMO framework, but extended so that both the Pareto archive and archives of in-
termediate individuals are maintained. The elitism in the Pareto front update is sup-
ported by low-level HFC archives, as explained in the next section. For density esti-
mation, the grid-based methods in PESA [1] are used, which are naturally suited for
the absolute division of the objective space as required by the HEMO framework. This
grid-based method is also demonstrated to have excellent performance in maintaining
Pareto front diversity [11]. The distribution of individuals into all fronts in the im-
proved NSGA-II is extended to all fitness levels.
Fig. 2. The assembly line structure of the HEMO Framework. In HEMO, repositories are or-
ganized in a hierarchy with ascending fitness level (or rank level in the objective space as em-
ployed in this paper). Each level accommodates individuals within a certain fitness range (or
belonging to a given rank level) as determined by the admission criteria.
Rank
R5
work deme 0
work deme 1
work deme 2
work deme 3
work deme 4
work deme 5
R0
HFC Archives
R1
R2
R3
R4
Pareto work
deme
Pareto
Archive
Repositories
Workshops
random individual
generator

1034         J. Hu et al.
3
HEMO: Hierarchical Evolutionary Multiobjective
Optimization
Based on the analysis of the fundamental cause of premature convergence and draw-
ing ingenious ideas from previous successful MOGAs, we propose the HEMO frame-
work for difficult multi-objective problems in which the avoidance of premature con-
vergence is of great concern. Essentially, it is an extension of PESA enhanced with the
continuing search capability of HFC. In addition to the Pareto archive and the Pareto
workshop population, a succession of archives for maintaining individuals of different
fitness levels is added to allow mixing of lower- and intermediate-level building
blocks. A random individual generator is located at the bottom to feed raw genetic
material into this building block mixing machine continually. The structure of HEMO
is illustrated in Fig. 2:
The HEMO algorithm proceeds as follows:
1) Initialization
 
Determine the number of levels (nLevel) into which to divide the objective space
for each objective dimension. Determine the grid divisions (nGrid) as in PESA.
Note that nLevel is different from nGrid. The first one is used to organize inter-
mediate individuals into the hierarchical archives, while the latter is used to esti-
mate the density of individuals.
 
Determine the population sizes of the Pareto archive, HFC archive and corre-
sponding workshop demes. The distribution of population sizes among archives
(workshop demes) can be determined separately or using some special distribu-
tion scheme like the geometric distribution in (Deb and Goel, 2000).
 
Initialize the workshop demes with random individuals. The archives are empty at
the beginning.
 
Evaluate all individuals and calculate the crowding factor of each individual ac-
cording to the hyper-grid approach in PESA.
 
Calculate the fitness range of each objective dimension for all individuals in the
whole population:
min
max
[
,
]
i
i
f
f
 where  
0,...,
1
i
ObjDim
=
−
 
Divide the fitness range into nLevel Levels. For all individuals, calculate the ob-
jective 
ranks 
for 
each 
objective 
dimension, 
,
0,...,
1
r
i
ObjDim
i
=
−,
[0,
1]
r
nLevel
i∈
−
;
 
For each individual, calculate its fitness rank = the average rank over all objective
dimensions of each individual.  
1
1
ObjDim
f
i
i
r
r
ObjDim
−
=
∑
 
Migrate (move out) individuals in the workshop demes to the corresponding HFC
archives according to their fitness ranks 
fr
. Then add all non-dominated indi-
viduals of each workshop deme to the Pareto archive. There are two cases possi-
ble during these migrations. If the target archive is full, we will replace a selected
individual according to the Pareto archive and HFC archive update procedures de-

HEMO: A Sustainable Multi-objective Evolutionary Optimization Framework         1035
scribed below; else, we simply add the migrating individual into the target re-
pository.
2) Loop until meeting stopping criterion
A steady state evolutionary model is used in the HEMO framework. First,
 
Compute the breeding probability of each workshop deme of the HFC rank levels.
This is calculated as follows:
1
1
l
nLevel
k
Popsizeof workshopdemeof levell
pBreed
Popsizeof workshopdemeof level k
−
=
=
∑
These probabilities can instead be dynamically adjusted  irrespective of the work-
shop deme sizes. These probabilities determine the allocation of search effort to
each level, thus determining the greediness of the algorithm.
 
Decide whether to do Pareto workshop breeding or HFC workshop deme breeding
by probability pParetoBreed . If setting pParetoBreed =1, then HEMO reduces
to an algorithm similar to PESA. This parameter is used to control the greediness
of the Pareto search.
If Pareto workshop breeding is to be done:
 Decide whether or not to do crossover according to its probability. Mutate each
gene of the offspring with probability pGeneMutate.
 Select parents from the Pareto archive using tournament selection based on the
crowding factors of individuals. The less crowded, the more chance an individ-
ual will get selected. When selecting parents for crossover or mutation, the prob-
ability to select only from the Pareto archive is pSelectFromPareto. The prob-
ability to select a second parent from the rank 0 HFC Archive is 1- pSelect-
FromPareto. When there is only one individual in the Pareto archive, the second
parent for crossover is selected from the highest HFC archive.
 Create an offspring (two in crossover) and add it to the Pareto workshop deme.
If the Pareto workshop deme is not full, simply add the new candidate to it; else,
trigger the Pareto Archive Update Procedure. Then a migration process will
move individuals of each HFC archive to their new qualified HFC archives be-
cause of the update of the objective ranges.
If  HFC workshop deme breeding is to be done:
 Decide at which level (L) breeding will occur according to the probability
l
pBreed
 Decide whether or not to do crossover according to its probability. Mutate each
gene of the offspring with probability pGeneMutate.
 Select parents from the HFC archive of level L by tournament selection based on
the crowding factors. The lower the crowding factor, the higher the probability
to be selected. If there is only one parent in the current HFC archive, then the
second parent will be selected from the next lower archive.
 Create an offspring (two in crossover) and add it to the workshop deme. If the
workshop deme is not full, simply add to the end; else, trigger the HFC Archive
Update Procedure and the Pareto Archive Update Procedure.

1036         J. Hu et al.
 
With low probability pRandomImport , update perRandomIn percent of the indi-
viduals of the lowest HFC archive with random individuals.
Pareto Archive Update Procedure ( )
 Screen out the non-dominated individuals in the workshop deme.
 Update the objective ranges of the whole population with the non-dominated indi-
viduals.
 Recalculate the crowding factors of all individuals of the selected non-dominated
individuals and the individuals in the Pareto archive.
 Update the Pareto archive with the selected non-dominated individuals. If the Pa-
reto archive is full, truncate it by removing individuals with higher crowding fac-
tors.
 Empty the Pareto workshop deme.
HFC Archive Update Procedure ( )
 Update the objective ranges of the whole population and recalculate the fitness
ranks of all individuals in the workshop demes.
 Migrate individuals in the current HFC archives into their corresponding new lev-
els. If the target HFC archive is full, replace an individual selected by tournament
selection. The more offspring an individual produces, the higher the probability it
will be replaced.
 Update the HFC archives with the individuals in the workshop deme. If the target
HFC archive is full, replace an individual selected by tournament selection. The
bigger the crowding factor is, the higher probability it will have to be replaced.
Note that only higher archives are updated with the current workshop deme (uni-
directional migration policy)
4
Experiments and Results
In this section, two multi-modal test functions are selected to demonstrate the ex-
ploratory capability of HEMO to avoid premature convergence. Here, HEMO is only
compared to PESA, since HEMO is most closely derived from PESA.
1) Multi-objective Rastrigin’s problem (ZDT4)
1
1
2
1
10
2
1
( )
( )
( )[1
/
( )]
4 :
2
( )
91
[
10cos(4
)]
[0,1],
[ 5,5],
2,...,10.
i
i
i
i
Minimize f
x
x
Minimize f
x
g x
x
g x
ZDT
g x
x
x
x
x
i
π
=
=
=
−
=
+
−
∑
∈
∈−
=




2) Multiobjective Griewank Problem (GWK)
GWK problem is constructed by replacing g (x) in 1) with Griewank’s function, where
10
2
10
2
1
2
2
4000
cos(
[0,1]
[-512,511]
2,...,10
( )
/
/
),
,
i
i
i
i
i
g x
x
x
i
x
where
x
i
=
=
= +
−∏
∈
∑
∈
=

HEMO: A Sustainable Multi-objective Evolutionary Optimization Framework         1037
Fig. 3. Distribution of individuals over the objective space of GWK in HEMO after 1000
evaluations. Compared with Fig. 1. of  NSGA-II, the difference is that the archive population of
NSGA-II is drifting and has the risk of converging to a local area.  HEMO has less tendency to
converge, since it maintains representative individuals at many levels in the objective function
space and continuously introduces new genetic material, thus providing the fitness gradient for
new optima emerge in a bottom-up way.
Fig. 3 illustrates the distribution of individuals of HEMO during the evolutionary
process. It is clear that HEMO works by trying to expand the individuals in its re-
positories evenly across the objective space, rather than by converging to the early-
discovered high-fitness areas. This provides the necessary fitness gradient for new
optima to emerge in a bottom-up way, from the bottom level HFC archive and work-
shop subpopulations.
The robustness of PESA and that of HEMO are compared by examining the rela-
tionship of performance and the mutation rates applied to each type double gene after
crossover. We use the statistical comparison method of [1] to compare the Pareto
fronts obtained with different mutation rates by PESA and HEMO (Table 1). Cells in
the upper right triangle of the table hold the comparison results of different mutation
rates for HEMO, while the lower left (shaded cells) are for PESA. The first entry in
each cell represents the percentage of Pareto front solutions obtained with the row’s
mutation rate that are non-dominated, with 95% confidence, by the solutions obtained
with the column’s mutation rate. The second entry in each cell, similarly, shows the
percentage of Pareto front solutions obtained with the column’s mutation rate that are
non-dominated, with 95% confidence, by the solutions obtained with the row’s muta-
tion rate.  From [3], we know that for test function ZDT4, NSGA-II fails to find the
true Pareto front. This is also the case for PESA, as illustrated in the first column.
PESA without mutation is worse than any PESA configuration with mutation. It is
also suggestive that for PESA, the performance varies greatly with different mutation
rates, achieving best performance here with a mutation rate of 0.12. In contrast,
HEMO is more robust over mutation rates. The performance difference with no muta-
Distribution of Individuals of repositories of HEMO after 1000 evaluations 
f1
0.0
.2
.4
.6
.8
1.0
1.2
f2
0
50
100
150
200
250
300
350
HFC archive 3
HFC Archive 2
HFC Archive 1
HFC Archive 0
Pareto Archive

1038         J. Hu et al.
tion is not much different from that with mutation rate 0.16.It is clear that results with
mutation rate 0.16 is much better than 0.04, 0.08, 0.12. However, experiments with
more evaluations show that they all achieve similar results.
Table 1. Comparison of the robustness of PESA (in shaded cells) and HEMO with test function
ZDT4. First entry in each cell is percentage of solutions obtained with row’s mutation rate that
are not dominated by those obtained with the column’s mutation rate, and vice versa for the
second entry.  PESA can be seen to depend strongly on mutation to maintain its exploratory
capability. It is very sensitive to the mutation rate, for which the optimal value is hard to know
in advance. HEMO is much less sensitive to the mutation rate, since it doesn’t depend on muta-
tion to maintain the explorative capability.
We also compared the best performance of PESA (mutation rate 0.12) with that of
HEMO (mutation rate 0.16) for the same number (10,000) of evaluations (Table 2) .
For ZDT4, the Pareto front found by HEMO was much better than PESA found. In the
case of GWK, HEMO had limited advantage over PESA. The reason is that the statis-
tical comparison procedure used here [6,1] compares the merged Pareto fronts found
during 20 runs. PESA with different random seeds may converge to different points in
the objective space, which on the whole comprise a good Pareto front. However,
PESA is a poor opportunist in the sense that for both the ZDT4 and GWK functions,
PESA converges to only one or two Pareto solutions in 6 or 7 runs of a total of 20. In
contrast, HEMO always obtains diversified solutions in the Pareto archive.
5
Conclusions and Future Work
Current MOEAs still suffer from their convergent nature inherited from the conven-
tional EA framework. The loss of population diversity turns out to be only a symptom
of the phenomenon of premature convergence. Maintenance of exploratory capability
is central to ensuring sustainable evolutionary search. A new evolutionary multi-
objective framework named HEMO is introduced, featuring:  a hierarchical organiza
tion of repositories of individuals of different fitness levels (defined as the composite
objective ranks in the divided objective space), the continual introduction of raw ge-
netic material at the bottom evolutionary level, and hyper-grid-based density estima-
tion. Two experiments are reported to show the sustainable search capability of
HEMO, demonstrated along with its robustness over a variety of mutation rates, as
compared to PESA. The paradigmatic transition in handling premature convergence
Mutation
Rate
0.00
0.04
0.08
0.12
0.16
0.00
99  100
99.5 100
99.7 100
97.5 100
0.04
100  50
99.8 100
99.9 100
0.5   100
0.08
100  7.3
100  99.7
100  100
2.4   99.7
0.12
100  3.2
100  50.0
100  70.6
2.4   99.9
0.16
100  7.3
100  50.1
98.7  100
100  100

HEMO: A Sustainable Multi-objective Evolutionary Optimization Framework         1039
Table 2. Opportunistic PESA and robust HEMO. HEMO obtains a much better Pareto front for
ZDT4 and a small advantage for GWK. However, for each independent run, the frequency (in
last row) with which PESA converges to one or two Pareto solutions is around 33%, while
HEMO seldom converges to local Pareto fronts.
from HEMO is:  instead of trying to escape local optima from within converged, high-
fitness populations, the continuing EA framework (as represented, for example, by
HFC and HEMO here) ensure the opportunity for new optima to emerge from the
bottom up, enabled by the hierarchical organization of individuals by fitness.
By combining features from PESA and SPEA and extending the ideas in the
NSGA-II with controlled elitism, and including the HFC organization, HEMO is ex-
pected to be well suited for difficult multi-modal real-world problems in which pre-
mature convergence is of great concern. We also expect that HEMO will be especially
advantageous in multi-objective genetic programming, where the highly multi-modal
and discrete fitness landscape often makes modern MOEAs such as PESA fail by
converging prematurely to a local Pareto front. It is interesting to sort the MOEAs by
their capabilities to handle premature convergence. From the lowest to highest, we
have PESASPEA2  NSGA-II  NSGA-II with controlled elitism  HEMO,
each improving the previous one by paying more attention to the non-inferior domi-
nated individuals. However, HEMO differs from all the others in its continuing search
nature without premature convergence, while the others are all based on the tradi-
tional, convergent EA framework.
As a generic framework, HEMO is easily applicable to other modern MOEAs such
as SPEA-2 and NSGA-II. To improve running efficiency, a better density estimation
method is needed. The scheme for organizing individuals by rank levels can also be
improved. In addition, to distribute the individuals of the repositories more evenly in
the objective space, the HFC archive update scheme needs further refinement. Espe-
cially, extensive comparative experiments with NSGA-II with controlled elitism and
other MOEAs are required to fully demonstrate the potential of HEMO.
Acknowledgements. We are grateful to Dr. D. W. Corne and J. D. Knowles for their
helpful PESA code and the statistical Pareto front comparison code. This work is
supported by the National Science Foundation through grant DMII 0084934.
Test Function
ZDT4
GWK
PESA
HEMO
PESA
HEMO
% Non-Dominated Pareto
Solutions in 20-Run Ensemble
0.3%
by HEMO
   100%
by PESA
    47.3%
by HEMO
    53.7%
by PESA
Premature Convergence
Frequency
6/20
   0/20
   7/20
    0/20

1040         J. Hu et al.
References
1. 
Corne, D. W., Knowles, J.D. and Oates, M. J. The Pareto Envelope-based Selection Algo-
rithm for Multiobjective Optimization, In Marc Schoenauer, Kalyanmoy Deb, Guter
Rudolph, Xin Yao, Evelyne Lutton, J. J. Merelo and Hans-Paul Schwefel (editors), Pro-
ceedings of the Parallel Problem Solving from Nature VI Conference. Springer. (2000):
839–848
2. 
Corne, D.W., Jerram, N.R., Knowles, J.D., and Oates, M.J. PESA-II: Region-Based Se-
lection in Evolutionary Multiobjective Optimization. In Proceedings of the Genetic and
Evolutionary Computation Conference (GECCO-2001), , Morgan Kaufmann Publishers.
(2001): 283–290
3. 
Deb K., Goel, T. Controlled elitist non-dominated sorting genetic algorithms for better
convergence, in: E. Zitzler, K. Deb, L. Thiele, C.A. Coello Coello, D. Corne (eds.) Pro-
ceedings of the First International Conference on Evolutionary Multi-Criterion Optimiza-
tion, Springer, Berlin (2001) : 67–81.
4. 
Deb, K., Pratap, A., Agarwal, S., and Meyarivan, T. A Fast and Elitist Multiobjective
Genetic Algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, 6(2)
(2002): 182–197.
5. 
Deb, K., Thiele, L., Laumanns, M. and Zitzler, E. Scalable Multi-Objective Optimization
Test Problems. In: Proceedings of the 2002 IEEE Congress on Evolutionary Computation
(CEC 2002), IEEE Press, Piscataway (NJ). (2002)
6. 
Fonseca, C.M. and Fleming, P.J. On the Performance Assessment and Comparison of
Stochastic Multiobjective Optimizers. In Voigt, H-M., Ebeling, W., Rechenberg, I. And
Schwefel, H-P, editors. Parallel Problem Solving From Nature-PPSN IV:, Springer.
(1995): 584–593.
7. 
Goldberg, D. Sizing Populations for Serial and Parallel Genetic Algorithms. In Proc.
Third International Conf. on Genetic Algorithms. Morgan Kaufmann. (1989).
8. 
Goldberg, D. E. The Design of Innovation: Lessons from and for Competent Genetic
Algorithms, Boston, MA: Kluwer Academic Publishers, (2002).
9. 
Hu, J., Goodman, E.D. Hierarchical Fair Competition Model for Parallel Evolutionary
Algorithms. In Proceedings, Congress on Evolutionary Computation, CEC 2002, IEEE
World Congress on Computational Intelligence, Honolulu, Hawaii. (2002)
10. Hu, J., Goodman, E. D., Seo, K., Pei, M. 2002. Adaptive Hierar-chical Fair Competition
(AHFC) Model for Parallel Evolutionary Algorithms. In Proceedings of the Genetic and
Evolutionary Computation Conference: 772–779, GECCO-2002, New York.
11. Khare, V., Yao, X. and Deb, K. Performance Scaling of Multi-objective Evolutionary
Algorithms. Kangal Report No. 2002009. (2000)
12. Zitzler, E. and Thiele, L. Multiobjective evolutionary algorithms: A comparative case
study and the strength Pareto approach. IEEE Transactions on Evolutionary Computation,
3(4), (1999):257–271.
13. Zitzler, E., Laumanns, M. and Thiele, L. SPEA2: Improving the Strength Pareto Evolu-
tionary Algorithm. Technical Report 103, Computer Engineering and Communication
Networks Lab (TIK), Swiss Federal Institute of Technology (ETH) Zurich, Gloriastrasse
35, CH-8092 Zurich. (2001)
14. Zitzler, E., Deb, K., and Thiele, L. Comparison of Multiobjective Evolutionary Algo-
rithms: Empirical Results. Evolutionary Computation, 8(2) (2000): 173–195. 

Using an Immune System Model to Explore
Mate Selection in Genetic Algorithms
Chien-Feng Huang
Modeling, Algorithms, and Informatics Group (CCS-3),
Computer and Computational Sciences,
Los Alamos National Laboratory, MS B256,
Los Alamos, NM 87545, USA
cfhuang@lanl.gov
Abstract. When Genetic Algorithms (GAs) are employed in multi-
modal function optimization, engineering and machine learning, iden-
tifying multiple peaks and maintaining subpopulations of the search
space are two central themes. In this paper, an immune system model is
adopted to develop a framework for exploring the role of mate selection
in GAs with respect to these two issues. The experimental results re-
ported in the paper will shed more light into how mate selection schemes
compare to traditional selection schemes. In particular, we show that
dissimilar mating is beneﬁcial in identifying multiple peaks, yet harmful
in maintaining subpopulations of the search space.
1
Introduction
In the setting of multimodal function optimization, engineering and machine
learning, there are two important issues when a GA is used: (1) how fast can a
GA discover one or several peaks? And (2) can a GA maintain diverse subpop-
ulations in diﬀerent parts of the search space?1 In this paper, we intend to use
the mate-selection framework proposed in [7] and present the research work for
investigating these two themes. In [7], it was shown that mate selection plays
a crucial role in GA’s search performance. In a nutshell, the dissimilarity-based
mate selection schemes facilitate locating a single, best-so-far solution at the
expense of generating lethal oﬀspring; and the similarity-based mate selection
schemes enhance selection pressure toward highly-ﬁt individuals such that the
GA’s population converges rapidly to a certain region of a ﬁtness landscape.
As such, for the ﬁrst question, we would expect the dissimilarity-based mate
selection to improve the GA’s search performance with respect to that metric.
On the other hand, our empirical results so far have showed that simple GAs
with the mate selection schemes are all subject to convergence (i.e., the simple
1 The ﬁrst issue was brieﬂy discussed in [7]. For the second issue, there are some
practical problems where maintaining subpopulations are critical. An example is the
application of genetic approach to decentralized PI controller tuning for multivariable
processes in [12].
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1041–1052, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

1042
C.-F. Huang
GAs cannot maintain subpopulations). Thus for the second question, we intend
to employ Smith et al.’s immune system model [11], which was shown to be
able to maintain diverse subpopulations, in order to oﬀer additional insights
into how the mate selection schemes compare to traditional selection schemes.
In particular, we are interested in studying how diﬀerent mate choices aﬀect
the capability of Smith et al’s approach for maintaining subpopulations. Since
it has been shown, in [7], that the dissimilar mating mechanisms are harmful in
the sense of producing more useless hybrids, we expect that such mating pref-
erences will reduce the proportions of individuals in subpopulations. If so, the
next question would be to study if reducing the probability of dissimilar mating
(or increasing the probability of similar mating) can improve the capability for
maintaining subpopulations.
This paper presents the preliminary results we obtained while investigat-
ing the role of mate selection in the two issues discussed above. Before delving
fully into this paper, however, it is important to brieﬂy review Goldberg and
Richardson’s ﬁtness sharing mechanism [3] that serves as an idealized approach
for maintaining population diversity, and present Smith et al.’s immune sys-
tem model to discuss how it implements a form of implicit ﬁtness sharing so
as to facilitate formation of subpopulations. We then summarize the relevant
framework for studying mate selection proposed in [7]. Section 3 presents ex-
perimental results that answer the two questions mentioned above. Finally, this
paper is concluded with the insights obtained for the mate selection schemes and
future research lines.
2
Relevant Work in Prior GA Research
2.1
Fitness Sharing
Fitness sharing was an idea motivated by Holland’s discussion [6] in which the
number of individuals occupying a niche is limited to that niche’s carrying capac-
ity. Goldberg and Richardson [3] then introduced a ﬁtness sharing mechanism
that induces population diversity by penalizing individuals for the presence of
similar individuals in the population. The technique they proposed was shown to
be an eﬀective method for maintaining subpopulations over several high-ﬁtness
regions of the search space. However, it has two serious limitations: (1) the peaks
must be equidistant or nearly so, and (2) setting σs (a critical parameter in the
ﬁtness sharing scheme that represents a cutoﬀdistance, beyond which no sharing
will occur) requires knowledge about the number of peaks in the search space.
These limitations arise from the fact that ﬁtness sharing is deﬁned explicitly.
To avoid the diﬃculty of appropriately choosing σs Smith, Forrest and Perel-
son [11] introduced an algorithm that does not require explicit construction of
the sharing function. Their approach can implicitly achieve ﬁtness sharing that
discovers for itself how many peaks are in the search space (including the case of
not equally spaced peaks), and allocate trials appropriately. The idea is to use
the metaphor of biological immune systems which can maintain the diversity

Using an Immune System Model to Explore Mate Selection
1043
needed for it to detect multiple antigens. Then the GA, combined with the im-
mune system idea, eﬀectively distributes the population over several high-ﬁtness
areas of the search space.
2.2
Binary Immune System Model
The immune system model considered in this paper is based on a model intro-
duced by Farmer et al. [1], where both antigens and antibodies are represented
by binary strings. It is a simpliﬁcation from the real biology in which genes
are speciﬁed by a four-letter nucleic acid alphabet and recognition between an-
tibodies and antigens is based on their three-dimensional shapes and physical
properties. However, this abstract model of binary strings is rich enough for ex-
ploring how a relatively small number of recognizers (the antibodies) can evolve
to recognize a much larger number of diﬀerent patterns (the antigens).
In this binary immune system model, recognition is evaluated through a
string matching procedure. The antigens are considered ﬁxed, and a population
of N antibodies is evolved to recognize the antigens using a GA. For any set
of antigens, the goal is to obtain an antibody cover—a set of antibodies such
that each antigen is recognized by at least one antibody in the population.
Maintaining diverse antibodies is crucial for obtaining a cover [11].
An antibody is said to match an antigen if their bit strings are complemen-
tary (maximally diﬀerent). Since each antibody may have to match against sev-
eral diﬀerent antigens simultaneously, we do not require perfect bit-wise match-
ing. Many possible match rules are plausible physiologically (See [10] for ex-
amples). The degree of match is quantiﬁed by a class of match score functions
M : Antigen × Antibody →ℜ. For instance, M can simply count the number
of complementary bits or M can identify contiguous regions of complementary
bitwise matches within the string.
Smith et al. [11] adopted a model in which a ﬁxed set of antigens is given,
and the antibodies are initialized either to be completely random (to see if the
GA can learn the correct antibodies) or initially given the answer by setting the
population to include the correct antibodies (to test the stability of the answer).
Their mechanism for ﬁtness scoring is as follows:
1. A single antigen is randomly selected from the antigen population.
2. From the population of N antibodies a randomly selected sample of size σ
is taken without replacement.
3. For each antibody in the sample, match it against the selected antigen, de-
termine the number of bits that match, and assign it a match score.
4. The antibody in the sample population with the highest match score is
determined. Ties are broken at random.
5. The match score of the winning antibody is added to its ﬁtness. The ﬁtness
of all other antibodies remains unchanged.
6. This process is repeated for C cycles (typically one to three times the number
of antibodies).

1044
C.-F. Huang
In this scheme, since an antibody’s ﬁtness is increased only if it is the best
matching antibody in the sample, the ﬁtness values of antibodies are interde-
pendent. In [11] Smith et al. showed analytically how this procedure implicitly
embodies ﬁtness sharing. Furthermore, Forrest et al. [2] reported that this scheme
can maintain subpopulations of antibodies that cover a set of antigens.
2.3
Mate Selection Schemes
Based on the idea of “assortative mating” used in biology, [7] proposed a frame-
work to investigate the role of mate selection in GA’s search power.2 Simply
stated, the goal was to shed more light into how speciﬁc mate selection schemes
compare to traditional selection schemes. In case of similar mating, similar indi-
viduals are chosen for mating; in case of dissimilar mating, dissimilar individuals
will mate with each other. That is, the selection-for-mating step of a simple GA
[9] is modiﬁed as:
During each mating event, a binary tournament selection3—with probability
one the ﬁtter of the two randomly sampled individuals is chosen—is run to
pick out the ﬁrst individual, then choosing the mate according to the following
schemes:
Tournament Selection (TS): Run the binary tournament selection again to
choose the mate.
Tournament Dissimilar Mating (TDM): Run the binary tournament se-
lection two more times to choose two candidate partners; then the one more
dissimilar to the ﬁrst individual is selected for mating.
Tournament Similar Mating (TSM): Run the binary tournament selection
two more times to choose two candidate partners; then the one more similar
to the ﬁrst individual is selected for mating.
Random Dissimilar Mating (RDM): Randomly
choose
two
candidate
partners; then the one more dissimilar to the ﬁrst individual is selected for
mating.
Random Similar Mating (RSM): Randomly choose two candidate part-
ners; then the one more similar to the ﬁrst individual is selected for mating.
We use the Hamming distance as the similarity metric. Notice that in the
mate selection schemes above if the two candidates are of the same Hamming
distance to the ﬁrst individual, then one of them is randomly selected.
In the ﬁve approaches above, the ﬁrst individual is always sampled by the
regular tournament selection. For TDM and TSM, there are two ways to aﬀect
an individual’s probability of being selected. The ﬁrst results from the ﬁtness
evaluation explicitly deﬁned by a given test function. The second is from the
preference of each individual over other individuals that possess certain char-
acteristics. The two sources complicate the probability of an individual being
2 See [7] for a comprehensive literature review of the relating mate-selection work in
prior GA research and a detailed discussion on why the framework was proposed.
3 Tournament selection is employed here for low computational cost.

Using an Immune System Model to Explore Mate Selection
1045
selected for actual mating. It is expected that tournament selection contributes
more selection pressure toward highly-ﬁt individuals, and the mate preference
reﬁnes the searching for mates. As for RDM and RSM, the selection pressure is
reduced by removing the tournament selection acting upon the candidate mates.
The only source that aﬀects the mate selection probability is precisely the mat-
ing preference, which exerts a selection pressure on the population based on
genotype.
3
Experimental Results
To illustrate the eﬀects of mate selection on the subpopulation-maintaining abil-
ity of Smith et al.’s immune system algorithm (we call it the diversity algorithm
from here on), we use a simple example in which antigen populations cannot
be matched by a single antibody type. Consider an antigen population that is
composed of 50% 000 . . . 000 (all 0’s) and 50% 111 . . . 111 (all 1’s). In order for
an antibody population to recognize these antigens, there would need to be some
antibodies that are all 1’s and others that are all 0’s. Thus, a solution to this
problem requires the GA to maintain two diﬀerent solutions simultaneously. This
is an example of a “multiple peaks” problem because there are two incompatible
solutions that are maximally diﬀerent. Typically, on multiple-peaks problems
it is diﬃcult for simple GAs to distribute the population over several peaks of
a ﬁtness landscape (two diﬀerent subpopulations of antibodies that match two
types of antigens, in this case). This is because the selection pressure in a simple
standard GA usually entails strong convergence tendency to only one peak. Even
without selection pressure, genetic drift due to sampling error can still lead the
GA to converge on one of the peaks [4].
Forrest et al. [2] reported in their numerical experiments that the GA with
the diversity algorithm can eﬀectively avoid strong convergence to one peak
and distribute the population over multiple peaks. As has been discussed in the
beginning of this paper, we expect the mate selection schemes play an important
role in maintaining subpopulations. In particular, our objective is to address the
following questions concerning the capability of the GA, along with Smith’s
algorithm, for maintaining subpopulations:
– Can the GA with diﬀerent mate selection schemes maintain stable subpop-
ulations of antibodies for recognizing diﬀerent antigens, or does it always
converge on one peak? If it can maintain diverse subpopulations, then
– Is the proportion of antibodies in each subpopulation being aﬀected by dif-
ferent mating preferences?4
– Do diﬀerent mating preferences have inﬂuence on the discovery time of anti-
gens?
In light of pattern-recognition, Forrest et al. [2] pointed out that the immune
system needs to recognize bacteria partially on the basis of the existence of
4 How many antibody representatives must be in the population for an antigen to be
identiﬁed is critical. See [2] for a detailed discussion.

1046
C.-F. Huang
certain unusual molecules that are inherently diﬀerent from human cells, since
many bacteria have cell walls made from polymers that do not occur in humans.
With this as motivation, we study the GA’s ability to detect common patterns
(building blocks) in the antigen population and adopt the building-block idea in
[6] to calculate ﬁtnesses of antibodies.
Table 1. Building blocks of antigens
b1 = 11111***************; s1 =10
b2 = *****11111**********; s2 =10
b3 = **********11111*****; s3 =10
b4 = ***************11111; s4 =10
b5 = 00000***************; s5 =10
b6 = *****00000**********; s6 =10
b7 = **********00000*****; s7 =10
b8 = ***************00000; s8 =10
Table 1 illustrates the building blocks of antigens 111 . . . 1 and 000 . . . 0 (string
length is of 20 bits5). An antibody is said to match an antigen if its bit string
is complementary to the antigen at certain building blocks. Speciﬁcally, the
match score function Mb is to identify the building blocks for which an antibody
matches an antigen, and then assign corresponding scores to that antibody. For
example, given an antigen 111 . . . 1, an antibody with the ﬁrst ﬁve and the last
ﬁve bits being all 0’s will receive score s1 + s4 = 20, since these ten bits are
complementary to those of the antigen.
Smith et al. [11] considered two cases for the score calculation of antibodies—
perfect match and partial match. In case of perfect match, an antibody receives
a non-zero score only if it perfectly matches the antigen. In case of partial match,
an antibody receives a non-zero score if it partially matches the antigen. In terms
of the distance dij between antibody i and antigen j, partial match indicates
the degree by which an antibody matches an antigen—i.e., the number of bits
of an antibody that are complementary to the corresponding bits of an antigen.
The degree of match determines the speciﬁcity of an antibody. For example, if
dij = 0, the matching is completely speciﬁc (that is, the antibody must perfectly
match the antigen), but if dij ̸= 0, it is partially matched. The consequence of a
partial matching rule is that there is a trade-oﬀbetween the number of antibodies
used and their speciﬁcity—as the speciﬁcity of antibodies increases, so does the
number of antibodies required to achieve a certain level of detection [5].
For the scoring rule discussed in the building-block-based recognition prob-
lem, we can also expand its deﬁnition by allowing partial match. In other words,
5 The small string length here serves well for illustrating the eﬀect of the mate selection
schemes. We current have some results for larger string lengths that are consistent
with the results obtained for the small string length.

Using an Immune System Model to Explore Mate Selection
1047
Table 2. Illustration of the immune-based GAs.
1. Randomly generate an initial population of n antibodies.
2. Evaluate antibodies’ ﬁtnesses by the six steps of the diversity algorithm.
3. Repeat until n oﬀspring have been created.
a. select a pair of parents for mating by particular selection schemes;
b. apply crossover operator;
c. apply mutation operator.
4. Reset all the new individuals’ ﬁtnesses to zero and replace the current
population with the new population.
5. Go to Step 2 until terminating condition.
if an antibody matches an antigen at all the bits of a building block, it is a perfect
building-block match; if not all the bits of that building block are required for
matching, it constitutes a partial building-block match. Therefore, the prefect
building-block match case is that an antibody scores if all of its bits at a building
block are complementary to those of an antigen. On the other hand, a case for
partial match could allow an antibody to score with only 80% bits (i.e., 4 bits
in case of the building blocks shown in Table 1) of a building block at which it
matches an antigen. The result of this ﬂexible scoring is a smaller population size
required to achieve a certain level of recognition performance. In this paper, we
mostly concentrate on this latter case for calculating antibody scores. (In case
of 100% building-block match, a few experiments conducted so far have shown
similar qualitative results as the 80% building-block match case, but it requires
much larger population sizes, i.e., much higher computational costs, to achieve
similar levels of performance.)
3.1
Eﬀects of Mate Selection on Maintaining Subpopulations
To address the questions mentioned in the beginning of this section we conduct
a series of GA experiments using the diversity algorithm. The illustration of the
immune-based GAs is shown in Table 2.6 Our ﬁrst objective is to investigate
eﬀects of mate selection on the diversity algorithm’s subpopulation-maintaining
6 Since in the diversity algorithm the match scores of winning antibodies are contin-
uously accumulated, after each generation their ﬁtness values can be large. Thus at
step 4 of Table 2 we reset the ﬁtnesses of the new population’s individuals to zero
after each generation to prevent ﬁtnesses from unlimited increase.

1048
C.-F. Huang
ability. Unless stated otherwise, these experiments use an antibody population
size of 100, crossover rate of 0.7, mutation rate of 0.005, and ran for 150 gen-
erations. The antigen population is 50% 000 . . . 0 and 50% 111 . . . 1, and both
antigens and antibodies are binary strings of length 20. The number of samples,
σ, is 10, which is 10% of the population size. We choose this value because Smith
et al.’s analysis suggests that too small or too large a sample size cannot show
ﬁtness sharing’s eﬀect. In addition, as mentioned in the preceding section, the
number of cycles (C) does not have a bearing on the antibodies’ expected ﬁt-
nesses, 100 cycles (i.e., population size) used for each generation turned out to
serve well for displaying subpopulation-maintaining results. Thus the total func-
tion evaluations for each run are generations×cycles×sample size, which equal
150,000.
Fig. 1 illustrates the experimental results of the diversity algorithm (averaged
over 50 runs), evolved by the GAs with TS, TDM, TSM, RDM and RSM.
20
40
60
80
100
120
140
0
10
20
30
40
50
60
Generation
Number correct for 1st peak (111...1)
Population size=100, sample size=10
20
40
60
80
100
120
140
0
10
20
30
40
50
60
Generation
Number correct for 2nd peak (000...0)
Population size=100, sample size=10
Crossover rate=0.7
Crossover rate=0.7
TS
TSM
TDM
RSM
RDM
TS
TSM
TDM
RSM
RDM
Fig. 1. The number of antibodies that correctly recognize antigens
These are the results for the numbers of antibodies that recognize antigens
when all four building blocks are 80% correctly matched. Note that only the
curves with small error bars (95% conﬁdence intervals7) can be used for reliable
judgements (we will discuss the reason for the larger error bars shortly), and
thus the results for TS, TDM and RDM can be compared. It is clear that the
7 The vertical bars overlaying the metric curves throughout this paper represent the
95-percent conﬁdence intervals calculated from Student’s t-statistic [8].

Using an Immune System Model to Explore Mate Selection
1049
dissimilar mating schemes, TDM and RDM, generate less desired antibodies
than the regular tournament selection. The reason is in the following:
When crossover is turned on (crossover rate is .7, in this case), the
dissimilarity-based mate selection increases the probability of producing useless
hybrids—e.g., given an individual 111 . . . 1, and two candidate mates 111 . . . 1
and 000 . . . 0, the GAs with the dissimilar mating schemes tend to choose 000 . . . 0
for mating with 111 . . . 1, and the crossing-over between these two strings gener-
ates oﬀspring that fall into the valley between the two peaks. Therefore, TDM
and RDM maintain a smaller fraction of desired antibodies.
On the other hand, we see that TDM generates a larger fraction of desired
antibodies than RDM. The diﬀerence between theses two schemes is the method
of selecting the second individual for mating—that is, in TDM ﬁtter individuals
have higher probabilities of being selected as mates, but this is not the case for
RDM. As a result, TDM can pick out more individuals from the two peaks than
RDM, which in turn increases the proportion of desired antibodies.
A remedy for the problem of producing useless hybrids would be to reduce
dissimilar mating rates. In terms of the example above, the regular tournament
selection confers 111 . . . 1 and 000 . . . 0 with equal probability of being selected for
mating, thereby reducing the likelihood of two mating individuals chosen from
the two peaks. However, if individuals tend to select similar mates, the selection
pressure toward these individuals may be strong enough that the GA’s popu-
lation converges on only one peak. If this is the case, the diversity algorithm’s
capability for maintaining subpopulation is degraded.
The larger error bars for TSM and RSM in Fig. 1 illustrate this situation.
Since TSM and RSM induce too strong a selection pressure, most of the GA’s
population members converge to only one peak. At generation 150, the GA with
TSM has 20 (out of 50) runs in which most of the individuals converge to all
1’s, and in 14 (out of 50) runs most of the individuals converge to all 0’s, and
there are 16 runs in which the two peaks are present, simultaneously. In case of
RSM, there are 17 runs in which most of the individuals converge to all 1’s, 21
runs in which most of the individuals converge to all 0’s, and 12 runs where the
two peaks are lost.
As a further illustration, Fig. 2 is the experimental results of a typical run
for the number of desired antibodies obtained based on TSM. This ﬁgure shows
that 000 . . . 0 are drown out by 111 . . . 1 after generation 60, although they do
show up in earlier generations. This is because in TSM, similar individuals are al-
ways chosen as mates (with probability one)—a selection pressure toward similar
mates enhances the convergence on one peak.
3.2
Eﬀects of Mate Selection on the Discovery of Peaks
In the immune system problem considered, thus far we have been concerned with
maintaining desired antibody subpopulations. However, there is another relevant
issue we have not yet studied: the formation of the antibody subpopulations
requires these antibodies to be discovered ﬁrst. This is equivalent to the problem

1050
C.-F. Huang
20
40
60
80
100
120
140
0
10
20
30
40
50
60
70
80
90
100
110
Generation
Number of antibodies that recognize antigens
Tournament similar mating (population size=100, sample size=10)
111...1
000...0
Fig. 2. The number of antibodies that correctly recognize antigens (based on the
tournament similar mating), where all portion of the solid line (i.e., corresponding
to 000 . . . 0) after generation 10 is on the 0 level
of ﬁnding multiple peaks. Since it has been shown, in [7], that the dissimilarity-
based mate selection facilitates locating a single, best-so-far solution, we are
interested in investigating if dissimilar mating is also more beneﬁcial in ﬁnding
multiple peaks than traditional selection schemes.
Table 3 displays the averaged mean function evaluations (over 50 runs) of
discovering 111 . . . 1 and 000 . . . 0. These results show no obvious diﬀerence be-
tween various mate selection schemes for ﬁnding the two peaks, except that
there are two runs where 000 . . . 0 was not found by the RSM GA, and this GA
used a bit more evaluations to locate 111 . . . 1 than the other GAs. A closer
inspection again shows the selection pressure toward similar individuals led the
two particular runs of the GA to converge on 111 . . . 1, thereby precluding the
discovery of the other peak. However, as population size decreases, the discrep-
ancies between these mating schemes become more obvious. Table 4 illustrates
the results for the number of runs (out of 50) in which antibodies 111 . . . 1 and
000 . . . 0 are discovered, respectively, based on population size 20 and sample size
2 (other parameter values remain unchanged). It is clear that the dissimilarity-
based mating preferences facilitate locating two peaks. This is again because
the similar mating schemes introduce a selection pressure strong enough that
the corresponding GAs show inferior performance. All this conﬁrms with our
expectation that the dissimilarity-based mate selection is beneﬁcial in locating
multiple peaks.

Using an Immune System Model to Explore Mate Selection
1051
Table 3. The mean function evaluations of discovering antibodies 111 . . . 1 and 000 . . . 0
(over 50 runs)
Antibody
TS
TDM
RDM
TSM
RSM
111 . . . 1 2340 (368) 2460 (333) 2460 (272) 2440 (204)
3060 (338)
000 . . . 0 2300 (206) 2540 (323) 2320 (270) 2180 (224) 48 runs reached
Table 4. The number of runs (out of 50) in which antibodies 111 . . . 1 and 000 . . . 0
are discovered
Antibody TS TDM RDM TSM RSM
111 . . . 1 20
34
40
18
23
000 . . . 0 28
37
34
23
23
4
Conclusions and Future Work
In this paper, we have described Smith et al.’s immune system model in
which subpopulations can be maintained through speciﬁc interactions among the
strings. We have emphasized the performance of the GA in the binary immune
system model, investigating how mate selection aﬀects the GA’s subpopulation-
maintaining ability and the eﬀects of mate selection on the discovery of multiple
peaks. Both of these issues are important in the setting of multimodal function
optimization, engineering and machine learning.
In studying the subpopulation-maintaining problem, the results illustrate
that the dissimilar mating schemes are harmful in the sense of producing more
lethal oﬀspring. Consequently, the proportion of individuals that are representa-
tives of diﬀerent antibodies is reduced. We then showed that reducing the prob-
ability of dissimilar matings can remedy this problem. We also hoped to improve
the GAs’ performance by further increasing similar mating rates. However, as
shown by the results obtained for TSM and RSM, they introduce a selection
pressure strong enough that the population converges on only one peak.
In studying the peaks-identifying problem, we showed that the dissimilarity-
based mate selection schemes facilitate locating multiple peaks of the ﬁtness
landscape. This is a crucial extension of the results obtained in [7], where dis-
similar mating is shown to be more advantageous in ﬁnding a single, best-so-far
solution.
Since the pattern-recognition strategy in our approach was based on schema
detection, it is worth further exploration because in real problems when there
are many more antigens than antibodies, antibodies need to detect common re-
gions. In future work, we also hope to extend the results of schema detection and
multiple-peaks identiﬁcation to more realistic scale of antigens and antibodies.
Finally, we would like to develop an analytical analysis to enhance our under-
standing for mate selection in the context of the immune-GA-based system.

1052
C.-F. Huang
Acknowledgments. The author would like to thank John Holland, Rick Riolo
for their advice, and Bob Lindsay, Ted Belding, Leeann Fu, Tom Bersano-Begey
and Bill Rand for their comments and suggestions.
References
1. Farmer, J. D., Packard, N. H., and Perelson, A. S.: The Immune System, Adapta-
tion, and Machine Learning. In D. Farmer, A. Lapedes, N. Packard, and B. Wen-
droﬀ(Eds.): Evolution, Games and Learning. North-Holland (1986). (Reprinted
from Physica, 22D, 187–204)
2. Forrest, S., Javornik, B., Smith, R. E., and Perelson, A. S.: Using Genetic Al-
gorithms to Explore Pattern Recognition in the Immune System. Evolutionary
Computation, 1(3) (1993) 191–211.
3. Goldberg, D. E. and Richardson, J.: Genetic Algorithms with Sharing for Multi-
modal Function Optimization. Genetic Algorithms and Their Applications: Pro-
ceedings of the Second International Conference on Genetic Algorithms (1987)
41–49.
4. Goldberg, D. E. and Segrest, D.: Finite Markov Chain Analysis of Genetic Algo-
rithms. International Conference on Genetic Algorithms, 2 (1987) 1–8.
5. Hofmeyr, S. A., and Forrest, S.: Architecture for an artiﬁcial immune system.
Evolutionary Computation, 8(4) (2000) 443–473.
6. Holland, J. H.: Adaptation in Natural and Artiﬁcial Systems. Ann Arbor, MI:
University of Michigan Press (1975).
7. Huang, C.-F.: A Study of Mate Selection in Genetic Algorithms. Doctoral disserta-
tion. Ann Arbor, MI: University of Michigan, Electrical Engineering and Computer
Science (2002).
8. Miller, R. G.: Beyond ANOVA, Basics of Applied Statistics. John Wiley and Sons
(1986).
9. Mitchell, M.: An Introduction to Genetic Algorithms. Cambridge, MA: MIT Press
(1996).
10. Perelson, A. S.: Immune network theory. Immunol. Rev., 110 (1989) 5–36.
11. Smith, R., Forrest, S., and Perelson, A. S.: Searching for Diverse, Cooperative
Populations with Genetic Algorithms. Evolutionary Computation, 1(2) (1993) 127–
149.
12. Vlachos, C., Williams, D., and Gomm, J. B.: Genetic Approach to Decentralized
PI Controller Tuning for Multivariable Processes. IEE Proc. Control Theory and
Applications, 146 (1999), 58–64.

Designing a Hybrid Genetic Algorithm for the
Linear Ordering Problem
Gaofeng Huang1 and Andrew Lim2
1 Department of Computer Science, National University of Singapore
3 Science Drive 2, Singapore 117543
2 Department of Industrial Engineering and Engineering Management
Hong Kong Univ of Sci and Tech, Clear Water Bay, Kowloon, Hong Kong
Abstract. The Linear Ordering Problem(LOP), which is a well-known
NP-hard problem, has numerous applications in various ﬁelds. Using
this problem as an example, we illustrate a general procedure of
designing a hybrid genetic algorithm, which includes the selection of
crossover/mutation operators, accelerating the local search module
and tuning the parameters. Experimental results show that our hybrid
genetic algorithm outperforms all other existing exact and heuristic
algorithms for this problem.
Keywords: Linear Ordering Problem, Genetic Algorithm, Hybridiza-
tion.
1
Introduction
The Linear Ordering Problem(LOP) has numerous applications in economics,
archaeology, scheduling, the social sciences, and aggregation of individual
preferences[5,6,7,8]. Of all these applications, the most famous one may be “the
Triangulation for Input-Output Matrices”[7], which measures the movement of
goods from one “sector” to another in economics research.
Mathematically, LOP can be formulated as :
Instance
:
a matrix C = {cij}n×n
Solution
:
p = (p1, p2, ..., pn), a permutation of 1...n
Objective
:
to maximize C(p) =
n

i=1
n

j=i+1
cpi,pj
This problem is known to be NP-hard[9]. Many exact and heuristic algo-
rithms have been proposed to solve it.
Several exact methods have been devised based on the integer programming
technique. Grotschel et al. ﬁrst proposed a cutting plane algorithm in [6,7];
Mitchell and Borchers[8] improved the result by combining the cutting plane
with the interior point algorithm. However, all of these exact algorithms are
extremely time-consuming.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1053–1064, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

1054
G. Huang and A. Lim
On the other hand, heuristic algorithms seem to be more practical in solving
large instances of this problem[1,2,3,4,5]. The heuristic proposed by Chanas and
Kobylanski[5] can usually provide acceptable solutions both in terms of time
and quality. Laguna et al.[1] applied Tabu Search technique with path relinking
strategy on this problem successfully, while Campos et al.[2] used Scatter Search
to solve this problem. These two approaches are often regarded as the best at
this moment.
The purpose of this paper includes: (1) to develop an eﬀective heuristic for
LOP and (2) to illustrate a general procedure of designing a hybrid genetic algo-
rithm. Genetic algorithms(GA) have shown to be competitive technique for solv-
ing general combinatorial optimization problems. However, it is possible to in-
corporate problem-speciﬁc knowledge into GA so that the results can be further
improved. The hybridization between GA and Local Search is such a method.
Experimental results show that our hybrid genetic algorithm outperforms all
other existing exact and heuristic algorithms for LOP.
The rest of this paper is organized as follows. We ﬁrst present the details
of our hybrid GA implementation in Section 2. In Section 3, preliminary ex-
periments are conducted to tune our algorithm. The ﬁnal computational results
and comparisons are reported in Section 4. In the last section, we present our
conclusions.
2
Hybrid Genetic Algorithm
Many variations of hybridization between Genetic Algorithms and Local Search
have been proposed[11,13,14]. In this paper, our hybrid GA has the following
structure:
Algorithm 1 Hybrid Genetic Algorithm
1: Initialize population size, crossover rate, mutation rate
2: Generate Initial Generation gen
3: while not TerminateConditions() do
4:
for pop ←1 to population size do
5:
Randomly selected two individuals from gen according to the ﬁtness, say, genx and geny
6:
if random[0, 1) < crossover rate then
7:
next genpop ←CrossoverOperator(genx, geny)
8:
else
9:
next genpop ←Copy(better(genx, geny))
10:
end if
11:
if random[0, 1) < mutation rate then
12:
next genpop ←MutationOperator(next genpop)
13:
end if
14:
next genpop ←LocalSearch(next genpop)
15:
if ﬁtness(next genpop) > history best then
16:
history best ←ﬁtness(next genpop)
17:
solution ←next genpop
18:
end if
19:
end for
20:
gen ←next gen
21: end while

Designing a Hybrid Genetic Algorithm for the Linear Ordering Problem
1055
In this structure, we include all the basic components of a typical genetic
algorithm, such as Initial Generation, Terminate Conditions, CrossoverOperator,
MutationOperator, Fitness Function etc. In addition, we have also incorporated
a local search module to improve the quality of each individual in the population.
More details are provided in the following subsections.
2.1
Genetic Algorithm
– Individual Representation and Fitness Function : In LOP, any per-
mutation p = (p1p2...pn) is a feasible solution. It is very natural to use a
n length vector p as the representation (chromosome) of an individual. The
objective function C(p) acts as the ﬁtness function of chromosome.
– Initial Generation : For the initial generation, each individual is set us-
ing a permutation that is randomly generated, after which a local search is
applied to it to improve its quality. Therefore, every individual in the initial
generation is already a local maximum in the solution space.
– Terminate Conditions : We do not have duplicate detection scheme in our
genetic algorithm. The evolution process is to converge to a “best” solution.
When this happens, the algorithm terminates. However, checking for conver-
gence can be time consuming. An alternative is to use the ﬁtness function
to approximate convergence.
– Crossover Operators : Three crossover operators, including PMX, CX
and OX, are implemented in our algorithm and tested by our experiments.
• Partially Mapped Crossover(PMX) operator was proposed by Goldberg
and Lingle[10]. The basic idea of PMX is to exchange a partial segment
between two parents. However, a skill named mapping is used in order
to keep the results still as feasible permutations.
• Cycle Crossover(CX) operator was ﬁrst used in [12]. It ﬁrst ﬁnds all
the mapping cycles between two parents. After that, for each cycle, it
randomly selects one of the two parents, and copies its elements to the
oﬀspring in corresponding position.
• Order Crossover(OX) operator[13] ﬁrst randomly selects several same
elements in both parents, and then exchanges are made between two
parents in those positions in order.
– Mutation Operators : We experiment with the two most commonly used
mutation operators, DM and k-EM.
• Displacement Mutation(DM) operator[14] randomly select a segment
from the chromosome sequence and insert it into another randomly se-
lected position.
• k-EM is a variation of Exchange Mutation(EM) operator, where k ex-
change(swap) operations are performed synchronously in 2 × k random
selected positions.
2.2
Local Search
The local search strategy we used in LOP is based on the idea of iterative im-
provement. It starts with an initial solution(permutation p), and tries to improve
the solution via a series of neighbour moves until no improvement can be made.

1056
G. Huang and A. Lim
Neighbouring Move :
In each iteration, the solution p is improved by
a “neighbouring move”. We use the INSERT move (also named DELETE-
INSERT or SHIFT) and the EXCHANGE move (also named SWAP or Pairwise-
Interchange) that are commonly used in permutation problems. For more details
please see Table 1.
Table 1. Two commonly used neighbouring moves
neighbouring move
explanation
examples: p =
(5, 3, 1, 2, 4)
INSERT(p, i, j) insert pi to position j
INSERT(p, 4, 1) = (2, 5, 3, 1, 4)
EXCHANGE(p, i, j)
exchange pi and pj
EXCHANGE(p, 4, 1) = (2, 3, 1, 5, 4)
The following is an interesting observation.
Theorem 1. For the LOP, INSERT move subsumes EXCHANGE move.
Proof.
– If a solution p can be improved by the INSERT move, it may
not be improved by the EXCHANGE move.
An easy counterexample can be constructed when n = 3:
consider p = (1, 3, 2), c2,1 −c1,2 = 10, c3,1 −c1,3 = −1, c2,3 −c3,2 = −100,
An INSERT move can be made to improve the result since
INSERT(p, 1, 3)−p = C(3, 2, 1)−C(1, 3, 2) = (c3,1−c1,3)+(c2,1−c1,2) = −1+10 =
9 > 0.
Consider the EXCHANGE move,
EXCHANGE(p, 1, 2) −p = C(3, 1, 2) −C(1, 3, 2) = −1 < 0,
EXCHANGE(p, 1, 3) −p = C(2, 3, 1) −C(1, 3, 2) = −100 + 10 −1 < 0,
EXCHANGE(p, 2, 3) −p = C(1, 2, 3) −C(1, 3, 2) = −100 < 0.
Hence, no EXCHANGE move can be made to improve the solution p.
– If a solution p can be improved by the EXCHANGE move, it can
always be improved by the INSERT moves.
Suppose p can be improved by EXCHANGE(p, i, j)(i < j):
Let p′′ = INSERT(p, i, j −1)
p′′′ = INSERT(p, j, i)
Obviously, p′ = EXCHANGE(p, i, j) = INSERT(p′′, j, i) = INSERT(p′′′, i + 1, j)
So, C(p′) −C(p) = (C(p′) −C(p′′)) + (C(p′′) −C(p))
= (C(p′) −C(p′′′)) + (C(p′′′) −C(p))
A important property of LOP is that
C(p′) −C(p′′′) = C(p′′) −C(p) =
j−1

k=i+1
(cpk,pi −cpi,pk)
Therefore, C(p′) −C(p) = (C(p′′) −C(p)) + (C(p′′′) −C(p)).
Since solution p′ is better than p, C(p′)−C(p) > 0, we have C(p′′)−C(p) > 0
or C(p′′′) −C(p) > 0, that is, p can also be improved by INSERT.
⊓⊔

Designing a Hybrid Genetic Algorithm for the Linear Ordering Problem
1057
As a result, only INSERT moves are considered in our algorithm. The gain in
objective function after one INSERT move is given as:
dCi,j = C(INSERT(p, i, j)) −C(p) =



j
k=i+1(cpk,pi −cpi,pk) for i < j
0
for i = j
i−1
k=j(cpi,pk −cpk,pi)
for j < i
(1)
Search Strategy : In each iteration, there are n(n −1) choices for an INSERT
move. The FirstFit and BestFit are two widely adopted strategies.
As shown in Algorithm 2, the FirstFit search strategy always uses the ﬁrst
found move which can lead to a better solution.
Algorithm 2 FirstFit
1: p ←Initialize Solution(permutation)
2: finishF lag ←false
3: while not ﬁnishFlag do
4:
finishF lag ←true
5:
for i ←1 to n do
6:
for j ←1 to n do
7:
compute dCi,j using Equation(1)
8:
if dCi,j > 0 then
9:
p ←INSERT(p, i, j) {update the solution}
10:
finishF lag ←false
11:
end if
12:
end for
13:
end for
14: end while
In this algorithm, the computation of dCi,j using Equation(1) is very time-
consuming, since each entry takes O(n) time. If the “WHILE-LOOP” runs T1
times, the whole algorithm will need T1n2O(n) = T1O(n3) time.
dCi,j =
 dCi,j−1 + (cpj,pi −cpi,pj) for i < j
0
for i = j
dCi,j+1 + (cpi,pj −cpj,pi) for j < i
(2)
Unlike FirstFit, BestFit search strategy uses Equation(2) to compute all n2
dCi,j entries and ﬁnds the best solution among all n2 possible moves. Since each
entry can be computed in O(1) time, if the whole algorithm terminates after t2
iterations, it will take t2O(n2) time.
It’s expected that t2 < T1n2 because BestFit tends to take the “faster”
ascent direction to a local maximum. On the other hand, usually t2 ≫T1 since
in each “round”, FirstFit has up to n2 INSERT moves, while BestFit makes 1
best INSERT move. This may explain the results in [1] that BestFit even takes
more real CPU time than FirstFit.
We propose a FastFit search strategy that takes advantage of the strengths
of both approaches. By using the idea of the “cache”, we set a “dirty” ﬂag in
our algorithm. As long as no actual INSERT move is made, the dCi,j cache

1058
G. Huang and A. Lim
Algorithm 3 FastFit
1: p ←Initialize Solution(permutation)
2: finishF lag ←false
3: while not ﬁnishFlag do
4:
finishF lag ←true
5:
for i ←1 to n do
6:
dirtyF lag ←false
7:
dCi,i ←0
8:
for j ←i + 1, i + 2, ..., n; i −1, i −2, ..., 1 do
9:
if dirtyFlag then
10:
compute dCi,j using Equation(1)
11:
dirtyF lag ←false
12:
else
13:
compute dCi,j using Equation(2)
14:
end if
15:
if dCi,j > 0 then
16:
p ←INSERT(p, i, j) {update the solution}
17:
finishF lag ←false
18:
dirtyF lag ←true
19:
end if
20:
end for
21:
end for
22: end while
is always “clean”, so we can compute the next dC entry in O(1) time. Only
after one actual INSERT move, the dC cache becomes “dirty” so that O(n)
time is needed to compute dCi,j. If the “WHILE-LOOP” runs T3 times, the
whole FastFit algorithm would need T3O(n2) + t3O(n), where t3 is the number
of INSERT moves made to reach a local maximum.
It is expected that T3 ≈T1, as FastFit and FirstFit are very similar. We
have t2 ≫T1, so T3 ≪t2. Therefore, FastFit is expected to be much faster than
BestFit and FirstFit.
3
Preliminary Experiments
A typical Genetic Algorithm may have 100 individuals, while crossover rate
and mutation rate are set to be 0.8 and 0.05 respectively. However, performance
of a Genetic Algorithm may be very sensitive to the settings of these parameters.
And there also may be some interaction eﬀects between crossover type, mutation
type, crossover rate and mutation rate. Since it’s too computationally exhaus-
tive to experiment all such combinations, therefore, in this section, preliminary
experiments are conducted to tune our GA with FastFit local search strategy.
25 instances from [2] are used for our preliminary experiments. These in-
stances are the same as those instances with n = 75 in “Random Instances Set
B”(see Section 4.3).
3.1
Combination of Crossover Operators and Mutation Operators
First, we experiment the combination of crossover and mutation operators
with the conﬁguration : crossover rate = 0.8, mutation rate = 0.05,

Designing a Hybrid Genetic Algorithm for the Linear Ordering Problem
1059
Table 2. Combination of Crossover operators and Mutation operators
OX
CX
PMX
average
std dev
DM
32916465.40
32915105.08
32916825.64
32916132.04
907.43
1-EM
32916657.56
32916144.60
32916548.52
32916450.23
270.24
2-EM
32916467.28
32915031.28
32917855.44
32916451.33
1412.15
5-EM
32917524.20
32915519.84
32918195.64
32917079.89
1392.13
10-EM
32917299.60
32914183.04
32917273.64
32916252.09
1791.90
average
32916882.81
32915196.77
32917339.78
std dev
495.66
718.71
687.97
population size = 100. Table 2 shows the average results of all 25 instances
for each combination.
From Table 2, the performance of diﬀerent operators do not vary much.
Among these, the combination of crossover operator PMX and mutation operator
5-EM provides the best average result. However, PMX operator seems to be
slightly better than the other two crossover operators.
3.2
Tuning of Parameter mutation rate
Using the combination of PMX and 5-EM operator(denoted as GA PMX 5-EM),
now we consider the mutation rate parameter with crossover rate = 0.80 and
population size = 100.
Average
Result
mutation
rate (%)
32900000
32902000
32904000
32906000
32908000
32910000
32912000
32914000
32916000
32918000
32920000
0
1
2
3
4
5
6
Average
Time(sec)
mutation
rate (%)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
1
2
3
4
5
6
Fig. 1. Tuning of Parameter mutation rate
crossover
rate (%)
Average
Result
32860000
32870000
32880000
32890000
32900000
32910000
32920000
32930000
0
10
20
30
40
50
60
70
80
90
100
Average
Time(sec)
crossover
rate (%)
0
0.2
0.4
0.6
0.8
1
1.2
0
10
20
30
40
50
60
70
80
90
100
Fig. 2. Tuning of Parameter crossover rate
Figure 1 shows the average results and CPU time taken for diﬀerent
mutation rate. It’s reasonable that CPU time increases with the growth of

1060
G. Huang and A. Lim
mutation rate, because more mutation means more computation. But as you
can see in Fig. 1, the average result is nearly the same, which means the muta-
tion in our GA does not improve the solution’s quality signiﬁcantly, hence we
set mutation rate = 0.
3.3
Tuning of Parameter crossover rate
Parameter crossover rate determines the probability of crossover happens in
GA. Fig. 2 shows the average results and consumed CPU time under conﬁgura-
tion : GA PMX, mutation rate = 0, population size = 100.
As shown in Fig. 2, the average result increases with the growth of
crossover rate, especially when it is less than 0.5. On the other hand, the
CPU time increases with crossover rate in a approximately linear manner.
With the compromise of time and solution quality, we choose crossover rate
= 0.5.
3.4
Tuning of Parameter population size
population size may be the most inﬂuential parameter in a Genetic Algorithm.
If population size is too small, there will be insuﬃcient genetic diversity to
search the feasible solution space thoroughly. On the other hand, the algorithm
may be extremely slow when population size is too large.
Fig. 3 shows the preliminary results with diﬀerent population size under
the conﬁguration : GA PMX, no mutation, crossover rate = 0.50. With the
growth of population size, the average result increase consistently although
the increase becomes slower, while the CPU time consumed is approximately lin-
ear to population size. Therefore, it’s reasonable to choose population size
= 40, as it provide a good balance between solution quality and computing time.
3.5
Summary: Eﬀect of Genetic Algorithm and Local Search
In our preliminary experiments, we examine the eﬀects of both the Genetic
Algorithm and Local Search. The following four algorithms are compared:
– SimpleGA is the simple Genetic Algorithm without local search.
– LocalSearch(random) is a multi-round local search algorithm. In each round,
it starts with a random initial permutation and uses FastFit strategy to
improve the solution.
– hGA(untuned) is a hybrid Genetic Algorithm with the conﬁguration : GA
with OX crossover operator, DM mutation operator, BestFit local search
strategy, crossover rate = 0.8, mutation rate = 0.05, population size
= 100. This stands for an untuned hybrid Genetic Algorithm.
– hGA(tuned) is our ﬁnal hybrid Genetic Algorithm with the conﬁguration
: GA with PMX crossover operator, no mutation operator, FirstFit local
search strategy, crossover rate = 0.5, population size = 40.
Fig. 4 shows the time-quality curve for these 4 algorithms. The vertical axis
shows the CPU time consumed, while the horizontal axis shows the average
result for the 25 instances that we experimented with.
The following remarks can be made from Fig. 4:

Designing a Hybrid Genetic Algorithm for the Linear Ordering Problem
1061
Average
Result
population _size
32875000
32880000
32885000
32890000
32895000
32900000
32905000
32910000
32915000
32920000
0
50
100
150
200
population_size
Average Time
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
50
100
150
200
Fig. 3. Tuning of Parameter population size
hGA(tuned)
LocalSearch(random)
Average Result
SimpleGA
Time(sec)
hGA(untuned)
0
0.5
1
1.5
2
2.5
3
30000000
30500000
31000000
31500000
32000000
32500000
33000000
Average Result
Time(sec)
0
0.5
1
1.5
2
2.5
3
32800000
32820000
32840000
32860000
32880000
32900000
32920000
32940000
hGA(tuned)
hGA(untuned)
LocalSearch(random)
TabuSearch
ScatterSearch
Fig. 4. Time-Quality Curve
– SimpleGA is rather bad in performance.
– LocalSearch(random) has the ability to ﬁnd relatively good solutions in a
short time. Unfortunately, even if much more time is given, this algorithm
cannot improve the best solution by much. For the 25 instances that we have
tested, no better solutions can be found after running 0.5 second.
– hGA(untuned) shows the power of hybridization of Genetic Algorithm and
Local Search. Given enough time, it is capable of ﬁnding competitive solu-
tions. However, it’s slower than LocalSearch(random) since even the initial-
ization alone will take more than 0.5 second.
– hGA(tuned) is about 10 times faster than hGA(untuned), which shows the
power of proper tuning a search method. Consequently, hGA(tuned) outper-
forms Tabu Search ans Scatter Search both in time and solution quality.
Guided by the Genetic Algorithm, it is possible for Local Search to improve
the quality of solution consistently; and with the help of Local Search, Genetic
Algorithm becomes competitive in ﬁnding good solutions.
4
Computational Results
Three widely-used sets of instances are tested to demonstrate the eﬀectiveness
of our hybrid Genetic Algorithm. All the codes are implemented in C/C++ and
run on a PentiumIII 800Mhz PC. And for each instance, our hGA is run only
once.

1062
G. Huang and A. Lim
However, the computing machines used in previous papers are diﬀerent, i.e.,
Intel Pentium 166Mhz PC, Intel PentiumIII 500Mhz, Sun SPARC 20 Model 71.
Therefore, in order to compare the CPU time, a scaling scheme is used according
to SPEC(http://www.specbench.org/osg/cpu2000/)1
4.1
LOLIB Instances
LOLIB may be the most widely-used testing instances for LOP. It contains 49
instances of real-world input-output tables from sectors in the European and
United States Economies. The data together with optimal solutions can be ob-
tained from www.iwr.uni-heidelberg.de/groups/comopt/software/LOLIB/
Nearly all papers on LOP use LOLIB as test instances: in [1], the results
of Chanas-Kobylanski algorithm(CK) and Tabu Search with path relinking(TS-
LOP) are reported. [2] shows computational results of two Scatter Search ver-
sions(SS, SS10). A Variable Neighborhood Search(VNS-LOP) algorithm is ap-
plied in [3], while [4] gives the results of a Lagrangian Based Heuristic(LH-VP).
Table 3. LOLIB (49 instances)
Avg.Obj.
Deviation
No. of
Avg.CPU Times(sec)
Value
from Opt. Optimal in P166 PIII500 PIII800
CK
22018008.35
0.15%
11
0.1
SS
22041229.8
0.01%
42
3.82
SS10
22041232.3
0.01%
43
14.28
LH-VP
0.00%
43
5.581
VNS-LOP
22041260.8
0.00%
44
0.87
TS-LOP
22041261.51
0.00%
47
0.93
hGA
22041263.82
—
49
< 0.2
< 0.05
0.0247
Table 3 shows for each algorithm the average objective function value over
all 49 instances, the average percent deviation from the optimal solution, the
number of optimal solutions a particular algorithm can reach, and the average
CPU time. It’s evident that our hGA outperforms all other algorithms, since
not only is it faster, but also it’s the only one algorithm which can provide all
optimal solutions for 49 instances.
4.2
Random Instances Set A
This set of instances is randomly generated by J. E. Mitchell and B. Borchers[8].
There are 30 instances, where the size of instances vary from 100 to 250. More
speciﬁcally, there are 5 instances with size n = 100 and n = 250 respectively, and
10 instances with size n = 150 and n = 200. Both the instances and optimal val-
ues are available at the website: www.rpi.edu/˜mitchj/generators/linord/
The results of Interior Point algorithm, Simplex Cutting Plane algorithm and
the combined of these two are reported in [8], while in [4] two Lagrangian Based
Heuristics(LH-PC and LH-VP) are applied to this set of instances.
1 SPEC(Standard Performance Evaluation Corporation) points out that PIII 800 is
not more 2 times faster than PIII500, not more 8 times faster than P166 and not
more 12 times faster than SPARC 20/71

Designing a Hybrid Genetic Algorithm for the Linear Ordering Problem
1063
Table 4. Random Instances Set A (30 instances)
Avg.Obj. Deviation
No. of
Avg.CPU Times(sec)
Value
from Opt. Optimal SPARC20 PIII500 PIII800
Interior
—
26
2390.11
Simplex
—
30
2697.33
Combined
—
30
356.16
LH-PC
0.227%
16
605.13
LH-VP
0.014%
28
105.46
hGA
777324.17
—
30
< 24
< 4
1.9665
As you can see from Table 4, our hGA is 10 ∼100 times faster than those
exact algorithms and more than 25 times faster than Lagrangian Based Heuris-
tics. What’s more important, with respect to all these 30 instances, the solutions
obtained by our hGA are all optimal.
4.3
Random Instances Set B
This set includes 75 instances generated by Laguna et al.[1]2, which consists of
25 instances for each problem size n = 75, 150, 200 with each entries of the cost
matrix cij randomly distributed in (0, 25000). No optimal solutions are reported
for these instances by now, while the Tabu Search with path relinking strategy
(TS-LOP) gives the best result before our experiment.
Table 5. Random Instances Set B (75 instances)
Avg.Obj.
Deviation No. of Avg. Times(sec)
Value
from best
best
in P166 PIII800
CK
128663947.3
0.64%
0
10.67
CK-10
128919838.0
0.39%
0
108.44
TS-LOP 129269367.5
0.11%
5
20.19
hGA
129437686.3
—
75
< 18
2.2402
Table 5 shows the results of our experiment. As you can see, our hGA al-
gorithm gives better solutions than TS-LOP for 70 instances, for the other 5
instances the solutions between hGA and TS-LOP are same. It is clear that our
hGA outperforms TS-LOP.
5
Conclusion
The Linear Order Problem is studied in this paper. We designed a hybrid Ge-
netic Algorithm by integrating Genetic Algorithm and Local Search strategy
successfully. Experiments indicate that parameters are inﬂuential for genetic al-
gorithm. We described the procedure of tuning the parameters and develop a
FastFit algorithm to accelerate the local search. As a result, the algorithm after
tuning becomes 10 times faster than before.
Computational results show that our hybrid genetic algorithm outperforms
all other existing exact and heuristic algorithms.
2 The authors would like to express special thanks to Prof. Manuel Laguna for pro-
viding the instances and detail results of TS-LOP.

1064
G. Huang and A. Lim
References
1. M. Laguna, R. Mart´ı and V.Campos: Intensiﬁcation and diversiﬁcation with elite
tabu search solutions for the linear ordering problem, Computer and Operation
Research, vol.26, pp. 1217–1230, (1999)
2. V. Campos, M. Laguna and R. Mart´ı: Scatter Search for the Linear Ordering
Problem, New Ideas in Optimization, D. Corne, M. Dorigo and F. Glover (Eds.),
McGraw-Hill, pp. 331–339 (1999)
3. Carlos Garc´ıa Gonz´alez and Dionisio P´erez-Brito: A Variable Neighborhood Search
for Solving the Linear Ordering Problem, In the Proceedings of MIC’2001 - 4th
Metaheuristics International Conference, pp. 181–185, Porto, Portugal, (2001)
4. Alexandre Belloni and Abilio Lucena: Lagrangian Based Heuristics for the Linear
Ordering Problem, In the Proceedings of MIC’2001 - 4th Metaheuristics Interna-
tional Conference, pp. 445–449, Porto, Portugal, (2001)
5. S. Chanas and P. Kobylanski: A New Heuristic Algorithm Solving the Linear Or-
dring Problem, Computational Optimization and Applications, vol.6, pp. 191–205,
(1996)
6. M. Grotschel, M. Junger and G. Reinelt: A Cutting Plane Algorithm for the Linear
Ordering Problem, Operations Research, vol.32, no.6, pp. 1995–1220, (1984)
7. M. Groetschel, M. Juenger and G. Reinelt: Optimal Triangulation of Large Real
World Input-Output Matrices, Statistische Hefte 25, 261–295, (1984)
8. J. E. Mitchell and B. Borchers: Solving linear ordering problems with a combined
interior point/simplex cutting plane algorithm, High Performance Optimization,
Kluwer Academic Publishers, Dordrecht, The Netherlands, H. Frenk et al. (Eds.),
pp. 349–366, (2000)
9. R.M. Karp: Reducibility among combinatorial problems, In R.E. Miller and J.W.
Thatcher (Eds.), Complexcity of Computer Computations, pp. 85–103, New York,
(1972)
10. D. Goldberg and R. Lingle: Alleles, loci, and the traveling salesman problem, in the
Proceedings of the 1st International Conference on Genetic ALgorithms and Their
Applications, pp. 154–159, Lawrence Erlbaum, Hillsdale, New Jersey, (1985).
11. D. Goldberg: Genetic Algorithms in Search, Optimization and Machine Learing,
Addison-Wesley, Reading, MA, (1989)
12. I. Oliver, D. Smith, and J. Holland: A study of permutation crossover operators
on the tsp, in the Proceedings of the 2nd International Conference on Genetic
ALgorithms and Their Applications, pp. 224–230, Hillsdale, New Jersey, (1987).
13. G. Syswerda: Schedule Optimization Using Genetic Algorithms, in L.Davis (Ed.)
Handbook of Genetic ALgorithms, pp. 332–349, New York, (1991)
14. Z. Michalewicz: Genetic Algorithms + Data Structure = Evolution Programs,
Springer-Verlag, Berlin Heidelberg, (1992)

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1065–1076, 2003.
© Springer-Verlag Berlin Heidelberg 2003
A Similarity-Based Mating Scheme for Evolutionary
Multiobjective Optimization
Hisao Ishibuchi and Youhei Shibata
Department of Industrial Engineering, Osaka Prefecture University,
1-1 Gakuen-cho, Sakai, Osaka 599-8531, Japan
{hisaoi, shibata}@ie.osakafu-u.ac.jp
Abstract. This paper proposes a new mating scheme for evolutionary
multiobjective optimization (EMO), which simultaneously improves the
convergence speed to the Pareto-front and the diversity of solutions. The
proposed mating scheme is a two-stage selection mechanism. In the first stage,
standard fitness-based selection is iterated for selecting a pre-specified number
of candidate solutions from the current population. In the second stage,
similarity-based tournament selection is used for choosing a pair of parents
among the candidate solutions selected in the first stage. For maintaining the
diversity of solutions, selection probabilities of parents are biased toward
extreme solutions that are different from prototypical (i.e., average) solutions.
At the same time, our mating scheme uses a mechanism where similar parents
are more likely to be chosen for improving the convergence speed to the Pareto-
front. Through computational experiments on multi-objective knapsack
problems, it is shown that the performance of recently proposed well-known
EMO algorithms (SPEA and NSGA-II) can be improved by our mating scheme.
1   Introduction
Evolutionary multi-objective optimization (EMO) algorithms have been applied to
various problems for efficiently finding their Pareto-optimal or near Pareto-optimal
solutions. Recent EMO algorithms usually share some common ideas such as elitism,
fitness sharing and Pareto ranking for improving both the diversity of solutions and
the convergence speed to the Pareto-front (e.g., see Coello et al. [1] and Deb [3]). In
some studies, local search was combined with EMO algorithms for further improving
the convergence speed to the Pareto-front [10, 12–14]. While mating restriction has
been often discussed in the literature, its effect has not been clearly demonstrated. As
a result, it is not used in many EMO algorithms as pointed out in some reviews on
EMO algorithms [6, 17, 21]. The aim of this paper is to clearly demonstrate that the
search ability of EMO algorithms can be improved by appropriately choosing parent
solutions. For this aim, we propose a new mating scheme that is applicable to any
EMO algorithms. For maintaining the diversity of solutions, the selection
probabilities of parent solutions are biased toward extreme solutions that are different
from prototypical (i.e., average) solutions in our mating scheme. At the same time,

1066         H. Ishibuchi and Y. Shibata
our mating scheme uses a mechanism where similar parents are more likely to be
chosen for improving the convergence speed to the Pareto-front.
Mating restriction was suggested by Goldberg [7] and used in EMO algorithms by
Hajela & Lin [8] and Fonseca & Fleming [5]. The basic idea of mating restriction is to
ban the crossover of dissimilar parents from which good offspring are not likely to be
generated. In the implementation of mating restriction, a user-definable parameter
mating
σ
 called the mating radius is usually used for banning the crossover of two
parents whose distance is larger than 
mating
σ
. The distance between two parents is
measured in the decision space or the objective space. The necessity of mating
restriction in EMO algorithms was also stressed by Jaszkiewicz [13] and Watanabe et
al. [18]. On the other hand, Zitzler & Thiele [20] reported that no improvement was
achieved by mating restriction in their computational experiments. Moreover, there
was also an argument for the selection of dissimilar parents. Horn et al. [9] argued
that information from very different types of tradeoffs could be combined to yield
other kinds of good tradeoffs. Schaffer [16] examined the selection of dissimilar
parents but observed no improvement.
In our previous study [11], we demonstrated positive and negative effects of
mating restriction on the search ability of EMO algorithms through computational
experiments on knapsack problems and flowshop scheduling problems. The positive
effect of the recombination of similar parents is the improvement in the convergence
speed to the Pareto-front while its negative effect is the decrease in the diversity of
solutions. On the other hand, the positive effect of the recombination of dissimilar
parents is the improvement in the diversity while its negative effect is the
deterioration in the convergence speed. In this paper, we propose a new mating
scheme for simultaneously improving the convergence speed and the diversity. The
effect of the proposed mating scheme on the performance of the SPEA [21] and the
NSGA-II [4] is examined through computational experiments on knapsack problems
in Zitzler & Thiele [21]. Experimental results show that the search ability of those
EMO algorithms on the two-objective and three-objective knapsack problems is
significantly improved by the proposed mating scheme.
2   Proposed Mating Scheme
We describe our mating scheme using the following k-objective optimization
problem:
Optimize 
))
(
...,
),
(
),
(
(
)
(
2
1
x
x
x
x
f
kf
f
f
=
,
 (1)
subject to 
X
x∈
, 
 (2)
where 
)
(x
f
 is the objective vector, 
)
(x
if
 is the i-th objective to be minimized or
maximized, x is the decision vector, and X is the feasible region in the decision space.
Let us denote the distance between two solutions x and y as 
|)
(
)
(
|
y
f
x
f
−
 in the
objective space. In this paper, the distance is measured by the Euclidean distance as

A Similarity-Based Mating Scheme for Evolutionary Multiobjective Optimization         1067
2
2
1
1
|)
(
)
(
|
|)
(
)
(
|
|)
(
)
(
|
y
x
y
x
y
f
x
f
k
k
f
f
f
f
−
+
⋅⋅⋅
+
−
=
−
.
 (3)
We propose a two-stage mating scheme illustrated in Fig. 1. The selection in the
second stage (i.e., upper layer) is based on the similarity between solutions while the
selection in the first stage (i.e., lower layer) uses the fitness value of each solution.
Our mating scheme is applicable to any EMO algorithms because an arbitrary fitness
definition can be directly used with no modification in its lower layer. For choosing
the first parent (i.e., Parent A in Fig. 1), the standard fitness-based binary tournament
selection with replacement is iterated α  times for choosing α  candidates (say 
1
x ,
2
x , ..., 
α
x
). Next the center vector over the chosen α  candidates is calculated in the
objective space as
))
(
...,
),
(
),
(
(
)
(
2
1
x
x
x
x
f
kf
f
f
=
, 
 (4)
where
∑
=
=
α
α
1
)
(
1
)
(
j
j
i
i
f
f
x
x
  for  
k
i
,...,
2,1
=
.
 (5)
Then the most dissimilar solution to the center vector 
)
(x
f
 is chosen as Parent A in
Fig. 1 among the α  candidates. That is, the most extreme solution with the largest
distance from the center vector 
)
(x
f
 in the objective space is chosen as the first
parent A in Fig. 1. When multiple solutions have the same largest distance, one
solution is randomly chosen among them (i.e., random tiebreak). The choice of the
first parent is illustrated for the case of 
=
α
3 in Fig. 2 (a) where three solutions 
1
x ,
2
x  and 
3
x  are selected as candidates of the first parent. The most dissimilar solution
3
x  to the center vector 
)
(x
f
 is chosen as the first parent in Fig. 2 (a).
Selection of the most 
similar solution to 
parent A
Parent B
1
2
β
Parent A
Selection of the most
extreme solution
α
Parent A
1
2
Crossover
Selection of the most 
similar solution to 
parent A
Parent B
1
2
β
1
2
β
Parent A
Selection of the most
extreme solution
α
Parent A
1
2
Crossover
Second Stage
(Upper Layer)
First Stage
(Lower Layer)
Fig. 1. The proposed mating scheme.

1068         H. Ishibuchi and Y. Shibata
)
(
1 x
f
)
(
2 x
f
)
( 1
x
f
)
( 3
x
f
)
( 2
x
f
)
(x
f
0
Parent A
)
(
1 x
f
)
(
2 x
f
)
( 4
x
f
)
( 3
x
f
0
)
( 5
x
f
)
( 6
x
f
)
( 7
x
f
)
( 8
x
f
Parent A
Parent B
(a) Choice of the first parent (
=
α
3)
   (b) Choice of the second parent (
=
β
5)
Fig. 2. Illustration of the proposed mating scheme.
When 
=
α
1, the choice of the first parent is the same as the standard binary
tournament selection. The case of 
=
α
2 is actually the same as the standard binary
tournament selection because two candidates always have the same distance from
their center vector. Selection probabilities are biased toward extreme solutions only
when 
≥
α
3.
On the other hand, the standard fitness-based binary tournament selection with
replacement is iterated β  times for choosing β  candidates of the second parent (i.e.,
Parent B in Fig. 1). Then the most similar solution to the first parent (i.e., Parent A in
Fig. 1) is chosen as Parent B among the β  candidates. That is, the solution with the
smallest distance from Parent A is chosen. In this manner, similar parents are
recombined in our mating scheme. The choice of the second parent is illustrated in
Fig. 2 (b) for the case of 
=
β
5. The most similar solution 
7
x  to the first parent (i.e.,
3
x ) is selected as the second parent among the five candidates (
4
x , ..., 
8
x ) in Fig. 2
(b). A crossover operation is applied to 
3
x  and 
7
x  for generating new solutions.
The mating scheme in our former study [11] corresponds to the case of 
=
α
1.
That is, the first parent was chosen by the standard fitness-based binary tournament
selection. Not only the choice of the most similar solution as the second parent but
also the choice of the most dissimilar solution was examined. Experimental results in
our former study [11] suggested that the choice of similar parents improved the
convergence speed to the Pareto-front while it had a negative effect on the diversity of
solutions. On the other hand, the choice of dissimilar parents improved the diversity
of solutions while it had a negative effect on the convergence speed to the Pareto-
front. The main motivation to propose the new mating scheme in Fig. 1 is to
simultaneously improve the diversity and the convergence speed by appropriately
choosing parents for recombination.
Our mating scheme has high applicability and high flexibility. The positive
aspects of our mating scheme are summarized as follows:
(1) Our mating scheme is applicable to any EMO algorithms because an arbitrary
fitness definition can be directly used with no modification.
(2) The specification of the mating radius 
mating
σ
 is not necessary.

A Similarity-Based Mating Scheme for Evolutionary Multiobjective Optimization         1069
(3) The selection pressure toward extreme solutions is adjustable by the specification
of the parameter α .
(4) The selection pressure toward similar solutions is adjustable by the specification
of the parameter β .
(5) The proposed mating scheme has high flexibility. For example, not only the
binary tournament selection but also other selection mechanisms can be used for
choosing candidate solutions. The distance between solutions can be measured in
the decision space as well as in the objective space. The choice of dissimilar
parents can be also examined using our mating scheme.
On the other hand, the negative aspects of the proposed mating scheme are as follows:
(i) Appropriate specifications of the two parameters α  and β  seem to be problem-
dependent. The sensitivity of the performance of EMO algorithms on the
parameter specifications will be examined in the next section.
(ii) Additional computational load is required for performing our mating scheme in
EMO algorithms. The increase in CPU time will be also examined in the next
section. In general, the computational overhead caused by our mating scheme is
negligible when the evaluation of each solution needs long CPU time.
3   Computational Experiments
In this section, we examine the effect of our mating scheme on the performance of
EMO algorithms through computational experiments. For this purpose, we combined
our mating scheme with recently developed well-known EMO algorithms: SPEA [21]
and NSGA-II [4]. It should be noted that our mating scheme is the same as the
standard binary tournament selection when the two parameters α  and β  are
specified as 
=
α
1 and 
=
β
1. In this case, the modified SPEA and NSGA-II
algorithms with our mating scheme are the same as their original versions. Using 100
combinations of α  and β  (i.e., 
=
α
1,2,...,10 and 
=
β
1,2,...,10), we examine the
effect of our mating scheme on the performance of the EMO algorithms.
3.1   Test Problems and Parameter Specifications
In our computational experiments, we used four knapsack problems in Zitzler &
Thiele [21]: two-objective 250-item, two-objective 500-item, three-objective 250-
item, and three-objective 500-item test problems. Each solution in an m-item
knapsack problem was coded as a binary string of the length m. Thus the search space
size was 
m
2
. Each string was evaluated in the same manner as in Zitzler & Thiele
[21].
The modified SPEA and NSGA-II algorithms with our mating scheme were
applied to the four knapsack problems under the following parameter specifications:

1070         H. Ishibuchi and Y. Shibata
Crossover probability: 0.8,
Mutation probability: 
m
/
1
 where m is the string length,
Population size in NSGA-II: 200,
Population size in SPEA: 100,
Population size of the secondary population in SPEA: 100,
Stopping condition: 2000 generations.
3.2   Performance Measures
Various performance measures have been proposed in the literature for evaluating a
set of non-dominated solutions. As explained in Knowles & Corne [15], no single
performance measure can simultaneously evaluate various aspects of a solution set.
Moreover, some performance measures are not designed for simultaneously
comparing many solution sets but for comparing two solution sets with each other.
For comparing various combinations of α  and β , we use the average distance
from each Pareto-optimal solution to its nearest solution in a solution set. This
performance measure was used in Czyzak & Jaszkiewicz [2] and referred to as 
R
D1
in Knowles & Corne [15]. The 
R
D1  measure needs all Pareto-optimal solutions of
each test problem. For the two-objective 250-item and 500-item knapsack problems,
the Pareto-optimal solutions are available from the homepage of the first author of
[21]. For the three-objective 250-item and 500-item knapsack problems, we found
near Pareto-optimal solutions using the SPEA and the NSGA-II. These algorithms
were applied to each test problem using much longer CPU time and larger memory
storage (e.g., 30000 generations with the population size 400 for the NSGA-II) than
the other computational experiments (see Subsection 3.1). We also used a single-
objective genetic algorithm with a secondary population where all the non-dominated
solutions were stored with no size limitation. Each of the three objectives was used in
the single-objective genetic algorithm. This algorithm was applied to each three-
objective test problem 30 times (10 times for each objective using the same stopping
condition as the NSGA-II: 30000 generations with the population size 400). The
SPEA and the NSGA-II were also applied to each test problem 10 times. Thus we
obtained 50 solution sets for each test problem. Then we chose non-dominated
solutions from the obtained 50 solution sets as near Pareto-optimal solutions. The
number of the Pareto-optimal or near Pareto-optimal solutions of each test problem in
our computational experiments is as follows:
  567 solutions (2/250 test problem),
1427 solutions (2/500 test problem),
2158 solutions (3/250 test problem),
2142 solutions (3/500 test problem),
where the k/m test problem means the k-objective m-item test problem.

A Similarity-Based Mating Scheme for Evolutionary Multiobjective Optimization         1071
3.3   Experimental Results
The modified SPEA and NSGA-II algorithms with our mating scheme were applied
to the four test problems using 100 combinations of α  and β . For each combination,
we performed ten runs from different initial populations for each test problem.
Average values of the 
R
D1  measure over ten runs are summarized in Figs. 3–6
where smaller values of the 
R
D1  measure (i.e., shorter bars) mean better results. In
each figure, the result by the original EMO algorithm corresponds to the bar at the
top-right corner where 
=
α
1 and 
=
β
1. From these figures, we can see that our
mating scheme improved the performance of the SPEA and the NSGA-II over a wide
range of combinations of α  and β . Especially the performance of the original EMO
algorithms on the three-objective test problems (i.e., Fig. 5 and Fig. 6) was improved
by our mating scheme in almost all combinations of α  and β . This is also the case
for the performance of the NSGA-II on the 2/500 test problem (i.e., Fig. 4 (b)). The
significant deterioration in the performance was observed only when the value of α
in the modified SPEA was too large in Fig. 3 (a) and Fig. 4 (a).
80
160
240
320
400
1
5
1
10
10
5
R
D1
80
160
240
320
400
1
5
1
10
10
5
R
D1
1
5
1
10
10
5
R
D1
          
5
50
60
70
80
90
100
1
10
1
10
10
5
1
R
D1
5
50
60
70
80
90
100
1
10
1
10
10
5
1
R
D1
(a) Results by the modified SPEA 
(b) Results by the modified NSGA-II
Fig. 3. Average values of the 
R
D1  measure for the two-objective 250-item problem.
300
500
700
900
1
5
1
10
10
5
R
D1
300
500
700
900
1
5
1
10
10
5
R
D1
1
5
1
10
10
5
R
D1
            
200
250
300
350
1
5
1
10
10
5
R
D1
200
250
300
350
1
5
1
10
10
5
R
D1
1
5
1
10
10
5
R
D1
(a) Results by the modified SPEA 
(b) Results by the modified NSGA-II
Fig. 4. Average values of the 
R
D1  measure for the two-objective 500-item problem.

1072         H. Ishibuchi and Y. Shibata
400
420
440
460
480
500
1
5
1
10
10
5
R
D1
400
420
440
460
480
500
1
5
1
10
10
5
R
D1
1
5
1
10
10
5
R
D1
              
200
250
300
350
1
5
1
10
10
5
R
D1
200
250
300
350
1
5
1
10
10
5
R
D1
1
5
1
10
10
5
R
D1
(a) Results by the modified SPEA 
(b) Results by the modified NSGA-II
Fig. 5. Average values of the 
R
D1  measure for the three-objective 250-item problem.
1250
1350
1450
1550
1
5
1
10
10
5
R
D1
1250
1350
1450
1550
1
5
1
10
10
5
R
D1
1
5
1
10
10
5
R
D1
            
500
600
700
800
900
1000
1
5
1
10
10
5
R
D1
500
600
700
800
900
1000
1
5
1
10
10
5
R
D1
1
5
1
10
10
5
R
D1
(a) Results by the modified SPEA 
(b) Results by the modified NSGA-II
Fig. 6.  Average values of the 
R
D1  measure for the three-objective 500-item problem.
Using the Mann-Whitney U test, we examined the statistical significance of the
improvement in the 
R
D1  measure by the proposed mating scheme. More specifically,
the results by the original EMO algorithms (i.e., 
=
α
1 and 
=
β
1) were compared
with those by their modified versions (i.e., 
≥
α
2 and/or 
≥
β
2) for examining the
statistical significance of the improvement by the proposed mating scheme for three
confidence levels (90%, 95% and 99%). Confidence levels of the improvement are
summarized in Table 1 for the 2/250 test problem and Table 2 for the 3/500 test
problem. From those tables, we can see that the performance of the SPEA and the
NSGA-II was significantly improved by our mating scheme in many cases. As shown
in our experimental results in Figs. 3-6 and Tables 1-2, the selection bias toward
either extreme solutions (i.e., 
≥
α
3 and 
=
β
1) or similar parents (i.e., 
=
α
1 and
≥
β
2) improved the performance of the EMO algorithms. It is, however, clearly
shown by some experimental results (e.g., Fig. 4 (b), Fig. 5 (a) and Fig. 6 (a)) that the
simultaneous use of them (i.e., 
≥
α
3 and 
≥
β
2) improved their performance more
significantly. For example, the best result was obtained in Fig. 6 (a) from the
combination of 
=
α
10 and 
=
β
10.

A Similarity-Based Mating Scheme for Evolutionary Multiobjective Optimization         1073
Table 1. Confidence levels of the improvement for the two-objective 250-item test problem.
(* means that the corresponding confidence level is less than 90%)
(a) Results for the SPEA
(b) Results for the NSGA-II
Table 2. Confidence levels of the improvement for the three-objective 500-item test problem.
(a) Results for the SPEA
(b) Results for the NSGA-II
As mentioned in Section 2, additional computation load is required for executing
our mating scheme. For evaluating such a computational overhead, we measured the
average CPU time for each combination of α  and β . Experimental results are
summarized in Fig. 7 for the SPEA and Fig. 8 for the NSGA-II. Since the CPU time
of the original and modified SPEA algorithms totally depends on the computation
load for the clustering of non-dominated solutions in the secondary population, it is
not easy to evaluate the pure effect of our mating scheme (see Fig. 7). On the other
hand, the evaluation of the computational overhead caused by our mating scheme is
easy for the NSGA-II as shown in Fig. 8 where we observe the linear increase in the
average CPU time with the increase in the values of α  and β . The increase in the
average CPU time from the original NSGA-II with 
=
α
1 and 
=
β
1 to the modified
NSGA-II with 
=
α
10 and 
=
β
10 was 15.7% in Fig. 8 (a) and 7.4% in Fig. 8 (b).

1074         H. Ishibuchi and Y. Shibata
6
8
10
CPU Time (sec.)
1
5
1
10 10
5
6
8
10
CPU Time (sec.)
1
5
1
10 10
5
CPU Time (sec.)
1
5
1
10 10
5
            
130
160
190
220
CPU Time (sec.)
1
5
1
10 10
5
130
160
190
220
CPU Time (sec.)
1
5
1
10 10
5
CPU Time (sec.)
1
5
1
10 10
5
(a) Two-objective 250-item test problem 
(b) Three-objective 500-item test problem
Fig. 7.  Average CPU time of the original and modified SPEA.
38
40
42
44
CPU Time (sec.)
1
5
1
10 10
5
38
40
42
44
CPU Time (sec.)
1
5
1
10 10
5
CPU Time (sec.)
1
5
1
10 10
5
            
100
103
106
109
CPU Time (sec.)
1
5
1
10 10
5
100
103
106
109
CPU Time (sec.)
1
5
1
10 10
5
CPU Time (sec.)
1
5
1
10 10
5
(a) Two-objective 250-item test problem 
(b) Three-objective 500-item test problem
Fig. 8.  Average CPU time of the original and modified NSGA-II.
Modified SPEA
Original SPEA
8000
9000
10000
8000
9000
10000
)
(
1 x
f
)
(
2 x
f
)
(
1 x
f
)
(
2 x
f
   
Modified NSGA-II
Original NSGA-II
8000
9000
10000
8000
9000
10000
)
(
1 x
f
)
(
2 x
f
)
(
1 x
f
)
(
2 x
f
   (a) SPEA and its modified version
 (b) NSGA-II and its modified version
Fig. 9.  50% attainment surface for the two-objective 250-item test problem.
For visually demonstrating the improvement in the performance of the EMO
algorithms by our mating scheme, we show the 50% attainment surface (e.g., see [3])
obtained by the original EMO algorithms and the modified EMO algorithms in Fig. 9

A Similarity-Based Mating Scheme for Evolutionary Multiobjective Optimization         1075
for the 2/250 problem. The best values of α  and β  with the smallest 
R
D1  measures
in Fig. 3 were used in Fig. 9 for the modified SPEA and NSGA-II algorithms. We can
see from Fig. 9 that better attainment surfaces were obtained by the modified
algorithms. Similar improvement was also observed for the 2/500 problem.
4   Concluding Remarks
We proposed a two-stage mating scheme for simultaneously improving the diversity
of solutions and the convergence speed to the Pareto-front. The basic idea is to bias
selection probabilities toward extreme solutions for preserving the diversity and
toward similar parents for improving the convergence speed. The effect of our mating
scheme was examined through computational experiments on multiobjective
knapsack problems where our mating scheme was combined with the two well-known
EMO algorithms (i.e., SPEA and NSGA-II). It was shown that the performance of
those EMO algorithms was improved by our mating scheme. It was also shown that
the increase in the CPU time caused by our mating scheme was small if compared
with the total CPU time (e.g., 7.4% increase). The simultaneous improvement in the
diversity and the convergence speed is usually very difficult. This is also the case in
our mating scheme. In our mating scheme, the two parameters (i.e., α  and β )
should be carefully adjusted to strike a balance between the diversity and the
convergence speed. Further studies are needed for automatically specifying these
parameter values appropriately. Further studies are also needed for examining the
effectiveness of our mating scheme for recently developed other EMO algorithms
such as SPEA2 [22].
Our mating scheme can be viewed as assigning a selection probability to each pair
of solutions (not to each individual solution). Pairs of similar solutions tend to have
higher selection probabilities than those of dissimilar solutions. At the same time,
pairs of extreme solutions tend to have higher selection probabilities than those of
prototypical solutions. While various sophisticated methods for assigning a fitness
value to each individual solution have been proposed in the literature on EMO
algorithms, the assignment of a fitness value (or a selection probability) to each pair
of solutions has not been studied well. Experimental results in this paper clearly show
that such an idea of fitness assignment has a possibility to improve EMO algorithms
with sophisticated fitness assignment schemes to each individual solution.
The authors would like to thank the financial support from Japan Society for the
Promotion of Science (JSPS) through Grand-in-Aid for Scientific Research (B):
KAKENHI (14380194).
References
1. Coello Coello, C. A., Van Veldhuizen, D. A., and Lamont, G. B.: Evolutionary Algorithms
for Solving Multi-Objective Problems, Kluwer Academic Publishers, Boston (2002).
2. 
Czyzak, P., and Jaszkiewicz, A.: Pareto-Simulated Annealing – A Metaheuristic
Technique for Multi-Objective Combinatorial Optimization, Journal of Multi-Criteria
Decision Analysis 7 (1998) 34–47.

1076         H. Ishibuchi and Y. Shibata
3. 
Deb, K.: Multi-Objective Optimization Using Evolutionary Algorithms, John Wiley &
Sons, Chichester (2001).
4. 
Deb, K., Pratap, A., Agarwal, S., and Meyarivan, T.: A Fast and Elitist Multiobjective
Genetic Algorithm: NSGA-II, IEEE Trans. on Evolutionary Computation 6 (2002) 182–
197.
5. 
Fonseca, C. M., and Fleming, P. J.: Genetic Algorithms for Multiobjective Optimization:
Formulation, Discussion and Generalization, Proc. of 5th International Conference on
Genetic Algorithms (1993) 416–423.
6. 
Fonseca, C. M., and Fleming, P. J.: An Overview of Evolutionary Algorithms in
Multiobjective Optimization, Evolutionary Computation 3 (1995) 1–16.
7. 
Goldberg, D. E.: Genetic Algorithms in Search, Optimization, and Machine Learning,
Addison-Wesley, Reading (1989).
8. 
Hajela, P., and Lin, C. Y.: Genetic Search Strategies in Multicriterion Optimal Design,
Structural Optimization 4 (1992) 99–107.
9. 
Horn, J., Nafpliotis, N., and Goldberg, D. E.: A Niched Pareto Genetic Algorithm for
Multi-Objective Optimization, Proc. of 1st IEEE International Conference on
Evolutionary Computation (1994) 82–87.
10. Ishibuchi, H., and Murata, T.: A Multi-Objective Genetic Local Search Algorithm and Its
Application to Flowshop Scheduling, IEEE Trans. on Systems, Man, and Cybernetics -
Part C: Applications and Reviews 28 (1998) 392–403.
11. Ishibuchi, H., and Shibata, Y.: An Empirical Study on the Effect of Mating Restriction on
the Search Ability of EMO Algorithms, Proc. of Second International Conference on
Evolutionary Multi-Criterion Optimization (2003 in press).
12. Ishibuchi, H., Yoshida, T., and Murata, T.: Balance between Genetic Search and Local
Search in Memetic Algorithms for Multiobjective Permutation Flowshop Scheduling,
IEEE Trans. on Evolutionary Computation (2003 in press).
13. Jaszkiewicz, A.: Genetic Local Search for Multi-Objective Combinatorial Optimization,
European Journal of Operational Research 137 (2002) 50–71.
14. Knowles, J. D., and Corne, D. W.: M-PAES: A Memetic Algorithm for Multiobjective
Optimization, Proc. of 2000 Congress on Evolutionary Computation (2000) 325–332.
15. Knowles, J. D., and Corne, D. W.: On Metrics for Comparing Non-Dominated Sets, Proc.
of 2002 Congress on Evolutionary Computation (2002) 711–716.
16. Schaffer, J. D.: Multiple Objective Optimization with Vector Evaluated Genetic
Algorithms, Proc. of 1st International Conference on Genetic Algorithms and Their
Applications (1985) 93–100.
17. Van Veldhuizen, D. A., and Lamont, G. B.: Multiobjective Evolutionary Algorithms:
Analyzing the State-of-the-Art, Evolutionary Computation 8 (2000) 125–147.
18. Watanabe, S., Hiroyasu, T., and Miki, M.: LCGA: Local Cultivation Genetic Algorithm
for Multi-Objective Optimization Problem, Proc. of 2002 Genetic and Evolutionary
Computation Conference (2002) 702.
19. Zitzler, E., Deb, K., and Thiele, L.: Comparison of Multiobjective Evolutionary
Algorithms: Empirical Results, Evolutionary Computation 8 (2000) 173–195.
20. Zitzler, E., and Thiele, L.: Multiobjective Optimization using Evolutionary Algorithms – A
Comparative Case Study, Proc. of 5th International Conference on Parallel Problem
Solving from Nature (1998) 292–301.
21. Zitzler, E., and Thiele, L.: Multiobjective Evolutionary Algorithms: A Comparative Case
Study and the Strength Pareto Approach, IEEE Transactions on Evolutionary Computation
3 (1999) 257–271.
22. Zitzler, E., Laumanns, M., and Thiele, L.: SPEA2: Improving the Performance of the
Strength Pareto Evolutionary Algorithm, Technical Report 103, Computer Engineering
and Communication Networks Lab, Swiss Federal Institute of Technology, Zurich (2001).

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1077–1088, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Evolutionary Multiobjective Optimization for Generating
an Ensemble of Fuzzy Rule-Based Classifiers
Hisao Ishibuchi and Takashi Yamamoto
Department of Industrial Engineering, Osaka Prefecture University,
1-1 Gakuen-cho, Sakai, Osaka 599-8531, Japan
{hisaoi, yama}@ie.osakafu-u.ac.jp
Abstract. One advantage of evolutionary multiobjective optimization (EMO)
algorithms over classical approaches is that many non-dominated solutions can
be simultaneously obtained by their single run. In this paper, we propose an
idea of using EMO algorithms for constructing an ensemble of fuzzy rule-based
classifiers with high diversity. The classification of new patterns is performed
based on the vote of multiple classifiers generated by a single run of EMO
algorithms. Even when the classification performance of individual classifiers is
not high, their ensemble often works well. The point is to generate multiple
classifiers with high diversity. We demonstrate the ability of EMO algorithms
to generate various non-dominated fuzzy rule-based classifiers with high
diversity by their single run. Through computational experiments on some well-
known benchmark data sets, it is shown that the vote of generated fuzzy rule-
based classifiers leads to high classification performance on test patterns.
1  Introduction
A promising approach to the design of reliable classifiers is to combine multiple
classifiers into a single one [2], [6]. Several methods have been proposed for
generating multiple classifiers such as bagging [3] and boosting [8]. In the bagging
(bootstrap aggregating) algorithm of Breiman [3], different data sets are generated by
bootstrapping (i.e., random sampling with replacement from the whole data set) for
the design of multiple classifiers. Thus the design of multiple classifiers can be
performed in parallel. On the other hand, multiple classifiers are sequentially designed
in boosting methods such as the AdaBoost (Adaptive Boosting) algorithm of Freund
& Schapire [8]. After one classifier is designed, the weight of each training pattern is
updated based on the classification result (i.e., correct classification or
misclassification) in the AdaBoost algorithm. The training patterns with the updated
weights are used for designing another classifier. The design of a classifier and the
weight update of training patterns are iterated for generating multiple classifiers.
Classifier aggregation has been studied in various fields [19], [21]. For example,
evolutionary computation is used for generating multiple classifiers [20], [22]. In the

1078         H. Ishibuchi and T. Yamamoto
field of neural networks, the aggregation of multiple classifiers is often referred to as
“mixture of local experts [17], [18]”. Classifier aggregation has also been studied in
the field of fuzzy logic [4], [12].
The point in classifier aggregation is to generate an ensemble of classifiers with
high diversity. Ideally the classification errors by each classifier should be
uncorrelated. In this paper, we propose the use of evolutionary multiobjective
optimization (EMO) algorithms for generating an ensemble of classifiers with high
diversity. In our computational experiments, we apply the NSGA-II algorithm of Deb
et al. [5] to a three-objective rule selection problem [13] for generating a number of
non-dominated fuzzy rule-based classifiers with respect to the classification accuracy
on training patterns, the number of fuzzy rules, and the total length of fuzzy rules. Of
course, we can apply other EMO algorithms to our task. We use the NSGA-II because
its implementation is relatively easy and its high performance is well-known [5].
One advantage of EMO algorithms over classical approaches is that many non-
dominated solutions (i.e., classifiers in the context of this paper) can be obtained by
their single run. That is, multiple classifiers are obtained by applying an EMO
algorithm to training patterns just once. Through computational experiments on some
well-known benchmark data sets, it is shown that high classification performance on
test patterns (i.e., high generalization ability) can be obtained from the vote of non-
dominated fuzzy rule-based classifiers. That is, we can design a high-performance
aggregated fuzzy rule-based classifier using an EMO algorithm for generating
multiple classifiers and the majority rule for classifying new patterns.
In this paper, we first briefly describe fuzzy rule-based classifiers in Section 2.
Then we explain our two-stage approach [15], [16] to the design of fuzzy rule-based
classifiers in Section 3. In the first stage, a pre-specified number of fuzzy rules are
extracted as candidate rules from training patterns using a data mining technique. In
the second stage, a number of non-dominated rule sets are found from the candidate
rules by the NSGA-II algorithm. Experimental results on some well-known
benchmark data sets are reported in Section 4 where the generalization ability of an
ensemble of obtained non-dominated rule sets for each data set is examined using the
majority rule for classifying new patterns. Finally Section 5 summarizes this paper.
2   Fuzzy Rule-Based Classifiers
Let us assume that we have m training patterns 
)
...,
,
(
1
pn
p
p
x
x
=
x
, 
m
p
,...,
2,1
=
from M classes where 
pi
x
 is the attribute value of the p-th training pattern for the i-th
attribute (
n
i
1,2,...,
=
). For our n-dimensional M-class pattern classification problem,
we use fuzzy rules of the following form:
Rule 
q
R : If 1x  is 
1
q
A
 and  ...  and 
n
x  is 
qn
A
 then Class 
q
C  with 
q
CF ,  (1)
where 
q
R  is the label of the q-th rule, 
)
...,
,
( 1
n
x
x
=
x
 is an n-dimensional pattern
vector, 
qi
A
 is an antecedent fuzzy set, 
q
C  is a class label, and 
q
CF  is a rule weight.

Evolutionary Multiobjective Optimization         1079
We define the compatibility grade of each training pattern 
p
x
 with the antecedent
part 
)
...,
,
(
1
qn
q
q
A
A
=
A
 of the fuzzy rule 
q
R  in (1) using the product operator as
)
(
...
)
(
)
(
)
(
2
1
2
1
pn
A
p
A
p
A
p
x
x
x
qn
q
q
q
µ
µ
µ
µ
⋅
⋅
⋅
=
x
A
,  
m
p
...,
,2
,1
=
,
 (2)
where 
)
( ⋅
qi
A
µ
 is the membership function of 
qi
A
.
For determining the consequent class 
q
C  and the rule weight 
q
CF , we first
calculate the confidence of the fuzzy association rule “
h
q
Class
⇒
A
” for each class
h by extending its original definition for non-fuzzy association rules [1] as
∑
∑
=
∈
=
⇒
m
p
p
h
p
q
q
p
q
h
c
1
Class
)
(
)
(
)
Class
(
x
x
A
A
x
A
µ
µ
,  
M
h
...,
,2
,1
=
.
 (3)
The confidence 
)
(⋅
c
 can be viewed as a fuzzy conditional probability of Class h. The
consequent class 
q
C  is specified as the class with the maximum confidence:
}
1,2,...,
|)
Class
(
{
max
)
Class
(
M
h
h
c
C
c
q
q
q
=
⇒
=
⇒
A
A
.
 (4)
On the other hand, the rule weight 
q
CF  is specified as
∑
≠
=
⇒
−
⇒
=
M
C
h
h
q
q
q
q
q
h
c
C
c
CF
1
)
Class
(
)
Class
(
A
A
.
 (5)
The rule weight of each fuzzy rule has a large effect on the classification ability of
fuzzy rule-based classifiers [11].
Let S be a fuzzy rule-based classifier (i.e., a set of fuzzy rules). When an input
pattern 
p
x
 is to be classified, a single winner rule 
w
R  is chosen from the rule set S as
}
|
)
(
max{
)
(
S
R
CF
CF
q
q
p
w
p
q
w
∈
⋅
=
⋅
x
x
A
A
µ
µ
. 
 (6)
The input pattern 
p
x
 is assigned to the consequent class 
w
C  of the winner rule 
w
R .
In this paper, we use multiple fuzzy rule-based classifiers. An input pattern is
classified by each individual classifier using the single winner-based method as
shown in (6). Then the final classification is performed through the majority rule (i.e.,
simple majority vote scheme) based on the classification result by each individual
classifier (see [12] for various voting methods for fuzzy rule-based classification).
3   Heuristic Rule Extraction and Genetic Rule Selection
Genetic rule selection was proposed for designing fuzzy rule-based classifiers with
high accuracy and high comprehensibility in [14] where a scalar fitness function was
defined as a weighted sum of two objectives: to maximize the number of correctly
classified training patterns and to minimize the number of fuzzy rules. A two-

1080         H. Ishibuchi and T. Yamamoto
objective genetic algorithm was used in [10] for finding non-dominated rule sets with
respect to these two objectives. Genetic rule selection was further extended to the
following three-objective optimization problem in [13]:
Maximize 
)
(
1 S
f
, minimize 
)
(
2 S
f
, and minimize 
)
(
3 S
f
, 
 (7)
where S is a subset of candidate rules, 
)
(
1 S
f
 is the number of correctly classified
training patterns by the rule set S, 
)
(
2 S
f
 is the number of fuzzy rules in S, and 
)
(
3 S
f
is the total rule length of fuzzy rules in S. The number of antecedent conditions of
each fuzzy rule is referred to as the rule length. It should be noted that the third
objective 
)
(
3 S
f
 is not the average rule length but the total rule length. While we use
the average rule length for describing each rule set in some parts of this paper, its use
as 
)
(
3 S
f
 leads to many meaningless non-dominated rule sets [13].
A two-stage approach to the three-objective fuzzy rule selection problem in (7)
was proposed for handling high-dimensional classification problems in [15], [16].
This approach is briefly explained in this section (for details, see [15], [16]).
3.1   Heuristic Rule Extraction
When we use K linguistic values and “don’t care” as antecedent fuzzy sets for each of
n attributes, the total number of possible combinations of those 
)1
(
+
K
 antecedent
fuzzy sets is 
n
K
)1
(
+
. Among those combinations, a pre-specified number of
candidate rules are generated in a heuristic manner using a data mining criterion. In
the field of data mining, association rules are often evaluated by two rule evaluation
criteria: support and confidence. In the same manner as the fuzzy version of the
confidence in (3), the definition of the support [1] is also extended as
∑
∈
=
⇒
h
p
q
p
q
m
h
s
Class
)
(
1
)
Class
(
x
A
x
A
µ
.
 (8)
The support 
)
(⋅
s
 can be viewed as measuring the coverage of training patterns by the
fuzzy rule. We use the following rule evaluation criterion in this paper:
∑
≠
=
⇒
−
⇒
=
M
C
h
h
q
q
q
q
q
h
s
C
s
R
f
1
SLAVE
)
Class
(
)
Class
(
)
(
A
A
.
 (9)
This is a modified version of a rule evaluation criterion used in an iterative fuzzy
GBML (genetics-based machine learning) algorithm called SLAVE [9].
In our heuristic rule extraction, a pre-specified number of candidate rules with the
largest values of the SLAVE criterion are found for each class. For designing fuzzy
rule-based classifiers with high comprehensibility, only short fuzzy rules are
examined as candidate rules. This restriction on the rule length is consistent with the
third objective (i.e., the total rule length) of our three-objective rule selection
problem.

Evolutionary Multiobjective Optimization         1081
3.2   Genetic Rule Selection
Let us assume that N fuzzy rules have been extracted as candidate rules using the
SLAVE criterion. A subset S of the N candidate rules is handled as an individual in
EMO algorithms, which is represented by a binary string of the length N as
N
s
s
s
S
⋅⋅⋅
=
2
1
,
     (10)
where 
1
=
js
 and 
0
=
js
 mean that the j-th candidate rule is included in S and
excluded from S, respectively.
As in our former studies [15], [16], we use two problem-specific heuristic tricks
together with the NSGA-II [5] for efficiently finding non-dominated rule sets. One
trick is the biased mutation where a larger probability is assigned to the mutation from
1 to 0 than that from 0 to 1. This is for efficiently decreasing the number of fuzzy
rules in each rule set. The other trick is the removal of unnecessary rules, which is a
kind of local search. Since we use the single winner-based method for classifying
each pattern by the rule set S, some fuzzy rules in S may be chosen as winner rules for
no training patterns. We can remove those fuzzy rules without degrading the first
objective (i.e., the number of correctly classified training patterns). At the same time,
the second objective (i.e., the number of fuzzy rules) and the third objective (i.e., the
total rule length) are improved by removing unnecessary rules. Thus we remove all
fuzzy rules that are not selected as winner rules for any training patterns from the rule
set S. The removal of unnecessary rules is performed after the first objective is
calculated for each rule set and before the second and third objectives are calculated.
4   Computational Experiments
4.1   Data Sets
We use six data sets with many numerical attributes: Wisconsin breast cancer,
Diabetes, Glass, Cleveland heart disease, Sonar, and Wine. These data sets are
available from the UCI ML repository (http://www.ics.uci.edu/~mlearn/). In our
former study [16], we examined the performance of individual non-dominated rule
sets (i.e., individual fuzzy rule-based classifiers) on each data set. In this paper, we
examine the performance of their ensemble (i.e., their aggregation using the simple
majority vote scheme). We evaluate the performance of the aggregated classifier on
each data set by comparing it with the reported results on the same data set in Elomaa
& Rousu [7] where six variants of the C4.5 algorithm [23] were examined. The
performance of each variant was evaluated by ten independent iterations (with
different data partitions) of the whole ten-fold cross-validation (10-CV) procedure
(i.e., 
CV
-
10
10×
) in [7]. We use the same performance evaluation procedure as [7].
Incomplete patterns with missing values are included in the Wisconsin beast
cancer data set and the Cleveland heart disease data set. Those patterns were not used
in our computational experiments as in [16]. See UCI ML repository and [16] for
details of each data set.

1082         H. Ishibuchi and T. Yamamoto
4.2   Experimental Conditions
As in Elomaa & Rousu [7] and our former study [16], we iterated the whole 10-CV
procedure ten times using different data partitions into ten subsets. Since the whole
10-CV procedure consisted of ten iterations of the design of a classifier ensemble and
its performance evaluation, the NSGA-II was employed 100 times for each data set. A
number of non-dominated rule sets were simultaneously obtained from each run of
the NSGA-II. Among those non-dominated rule sets, too small rule sets were
excluded from the classifier ensemble. More specifically, we used the number of
classes as the lower bound on the number of fuzzy rules. That is, we excluded non-
dominated rule sets with less than M fuzzy rules for an M-class classification
problem. After the classifier ensemble was designed, each pattern was independently
classified by each individual non-dominated rule set in the ensemble. Then the
majority class was chosen as the final classification result of that pattern by the
classifier ensemble.
Our computational experiments in this paper were performed in the same manner
as in our former study [16] where the performance of individual non-dominate rule
sets was examined. Here we briefly describe the experimental conditions (for details,
see [16]). All attribute values of each data set were normalized into real numbers in
the unit interval [0, 1]. As antecedent fuzzy sets, we used “don’t care” and 14
triangular fuzzy sets generated from four fuzzy partitions with different granularities
in Fig. 1. We generated 300 fuzzy rules for each class as candidate rules in a greedy
manner using the SLAVE criterion. Thus the total number of candidate rules was
300M where M is the number of classes. The upper bound on the length of candidate
rules was two for the Sonar data set and three for the other data sets.
 
1.0 
0.0 
0.0 
1.0  
 
1.0 
0.0 
0.0 
1.0 
 
1.0 
0.0 
0.0 
1.0  
 1.0 
0.0 
0.0 
1.0 
Fig. 1. Four fuzzy partitions used in our computer simulations.
The NSGA-II was employed for finding non-dominated rule sets from 300M
candidate rules. We used the following parameter values in the NSGA-II:
Population size: 200 strings,
Crossover probability: 0.8,

Evolutionary Multiobjective Optimization         1083
Biased mutation probabilities: 
M
p
300
/
1
)1
0
(
m
=
→
  and  
=
→)
0
1(
m
p
0.1,
Stopping condition: 5000 generations.
4.3   Experimental Results
Wisconsin Breast Cancer Data Set: Average error rates by our classifier ensembles
on training patterns and test patterns are shown by the solid lines in Fig. 2 (a) and Fig.
2 (b), respectively. They were 2.32% on training patterns in Fig. 2 (a) and 3.75% on
test patterns in Fig. 2 (b). Average error rates of individual classifiers (i.e., individual
non-dominated rule sets) are shown by closed and open circles in each figure. Closed
circles are used for indicating individual classifiers with low error rates on training
patterns. As shown in Fig. 2 (and other figures in this paper), individual classifiers
with low error rates on training patterns did not always have low error rates on test
patterns. This makes it very difficult to choose a single classifier from multiple
alternative ones. The aggregation of many classifiers using a voting scheme avoids
this difficult task (i.e., the choice of a single classifier). The performance of individual
classifiers (i.e., closed and open circles) was examined in our former study [16]. For
the Wisconsin breast cancer data set, the best and worst error rates among the six
variants of the C4.5 algorithm were reported as 5.1% and 6.0% by Elomaa & Rousu
[7], respectively. Those results are shown by the two dotted lines in Fig. 2 (b).
 Ensembles of fuzzy classifiers
  Individual fuzzy classifiers
2
3
4
5
6
2
3
4
5
 
Error rate on training patterns (%) 
Number of fuzzy rules 
          
Ensembles of fuzzy classifiers
C4.5 Best      
C4.5 Worst
2
3
4
5
6
3
4
5
6
 
Error rate on test patterns (%) 
Number of fuzzy rules 
 
  (a) Error rates on training patterns.
   (b) Error rates on test patterns.
Fig. 2. Experimental results on the Wisconsin breast cancer data set.
From Fig. 2 (b), we can see that the performance of our classifier ensembles was
much better than the best result of the C4.5 algorithm in [7]. We can also see that the
performance of our classifier ensembles was better than many individual classifiers
while it was slightly inferior to the best individual classifier in Fig. 2 (b).

1084         H. Ishibuchi and T. Yamamoto
Diabetes Data Set: In the same manner as Fig. 2, experimental results on the
diabetes data set are summarized in Fig. 3. As in Fig. 2 (b), we can observe a positive
effect of aggregating multiple non-dominated rule sets in Fig. 3 (b). That is, the
performance of our classifier ensembles (i.e., 25.5% error rate) was better than many
individual classifiers. We can also see that the performance of our classifier
ensembles was close to the reported best result in [7] by the C4.5 algorithm (i.e.,
25.0% error rate) and much better than the worst result (i.e., 27.2% error rate).
 
Error rate on training patterns (%) 
Number of fuzzy rules 
  Individual fuzzy classifiers
 Ensembles of fuzzy classifiers
2
3
4
5
22
23
24
25
26
          
C4.5 Best      
C4.5 Worst
Ensembles of fuzzy classifiers
2
3
4
5
24
25
26
27
28
 
Error rate on test patterns (%) 
Number of fuzzy rules 
  (a) Error rates on training patterns.
   (b) Error rates on test patterns.
Fig. 3. Experimental results on the diabetes data set.
Glass Data Set: Experimental results on the glass data set are summarized in Fig.
4. Since the performance of our classifier ensembles was significantly inferior to the
reported results of the C4.5 algorithm in [7], the best result (27.3%) of the C4.5
algorithm is not shown in Fig. 4 (b). To the best of our knowledge, good results have
not been reported on the glass data set by descriptive fuzzy rules of the form in (1).
Thus we feel that descriptive fuzzy rules with homogeneous fuzzy partition are not
suitable for the glass data set. Further studies may be required for improving the
performance of descriptive fuzzy rules on the glass data set.
Cleveland Heart Disease Data Set: Experimental results on the Cleveland heart
disease data set are summarized in Fig. 5. The average error rate of our classifier
ensembles on test patterns in Fig. 5 (b) was 46.6%. The best and worst reported
results of the C4.5 algorithm in [7] were 46.3% and 47.9%, respectively.

Evolutionary Multiobjective Optimization         1085
 Ensembles of fuzzy classifiers
  Individual fuzzy classifiers
3
4
5
6
7
8
9
25
30
35
40
 
Error rate on training patterns (%) 
Number of fuzzy rules 
          
Ensembles of fuzzy classifiers
C4.5 Best      
C4.5 Worst
3
4
5
6
7
8
9
35
40
45
 
Error rate on test patterns (%) 
Number of fuzzy rules 
  (a) Error rates on training patterns.
   (b) Error rates on test patterns.
Fig. 4. Experimental results on the glass data set.
 Ensembles of fuzzy classifiers
  Individual fuzzy classifiers
5 6 7 8 9 10 11 12 13 14 15 16 17 18
28
30
32
34
36
38
40
 
Error rate on training patterns (%) 
Number of fuzzy rules 
          
Ensembles of fuzzy classifiers
C4.5 Best      
C4.5 Worst
5 6 7 8 9 10 11 12 13 14 15 16 17 18
45
46
47
48
49
50
51
 
Error rate on test patterns (%) 
Number of fuzzy rules 
  (a) Error rates on training patterns.
   (b) Error rates on test patterns.
Fig. 5. Experimental results on the Cleveland heart disease data set.
Sonar Data Set: Experimental results on the sonar data set are summarized in
Fig. 6. The reported worst result (35.8%) of the C4.5 algorithm in [7] is not shown in
Fig. 6 (b) because it is out of the range of the figure. In Fig. 6 (b), the average error
rate of our classifier ensembles was 22.74%, which outperformed almost all the
individual classifiers and the best result (24.6%) of the C4.5 algorithm in [7].
Wine Data Set: Experimental results on the wine data set are summarized in Fig.
7. The best and worst results of the C4.5 algorithm reported in [7] were 5.6% and
8.8%, respectively. The average error rate of our classifier ensembles on test patterns
was 4.21%, which was better than the reported best result of the C4.5 algorithm. In

1086         H. Ishibuchi and T. Yamamoto
Fig. 7 (b), the performance of our classifier ensembles was better than many
individual classifiers while it was slightly inferior to the best individual classifier.
 Ensembles of fuzzy classifiers
  Individual fuzzy classifiers
2
3
4
5
6
10
12
14
16
18
20
22
 
Error rate on training patterns (%) 
Number of fuzzy rules 
          
Ensembles of fuzzy classifiers
C4.5 Best      
C4.5 Worst
2
3
4
5
6
22
24
26
28
30
 
Error rate on test patterns (%) 
Number of fuzzy rules 
  (a) Error rates on training patterns.
   (b) Error rates on test patterns.
Fig. 6. Experimental results on the sonar data set.
 Ensembles of fuzzy classifiers
  Individual fuzzy classifiers
3
4
0
2
4
6
8
10
 
Error rate on training patterns (%) 
Number of fuzzy rules 
          
Ensembles of fuzzy classifiers
C4.5 Best      
C4.5 Worst
3
4
0
2
4
6
8
10
 
Error rate on test patterns (%) 
Number of fuzzy rules 
  (a) Error rates on training patterns.
   (b) Error rates on test patterns.
Fig. 7. Experimental results on the wine recognition data set.
5   Concluding Remarks
We proposed an idea of using EMO algorithms for designing an ensemble of
classifiers with high diversity. EMO algorithms seem to be suitable for this task

Evolutionary Multiobjective Optimization         1087
because a number of classifiers can be simultaneously obtained from their single run.
Moreover, many EMO algorithms have some mechanisms for maintaining the
diversity of populations (i.e., maintaining the diversity of classifiers). In our
computational experiments, we generated a number of non-dominated fuzzy rule-
based classifiers by applying the NSGA-II algorithm to the three-objective fuzzy rule
selection problem. Of course, other EMO algorithms are applicable to our classifier
generation task. Experimental results on six well-known benchmark data sets showed
that the performance of classifier ensembles was better than many individual
classifiers. It was also shown that the performance of classifier ensembles was
comparable with or superior to the reported best results of the C4.5 algorithm in [7]
for five benchmark data sets (except for the glass data set).
Our experimental results in this paper suggest that the use of EMO algorithms is a
promising approach to the design of classifier ensembles. Moreover, the aggregation
of non-dominated classifiers avoids the difficult task of choosing a single classifier
from multiple alternatives. As shown in many figures in this paper (and discussed in
our former study [16]), low error rates of classifiers on training patterns do not always
mean low error rates on test patterns. On the contrary, the minimization of the error
rate on training patterns often leads to the deterioration in the error rate on test
patterns due to the overfitting to training patterns. Thus it is very difficult to choose a
single classifier from multiple alternatives based on their classification performance
on training patterns. The proposed idea avoids this difficulty. The proposed idea can
be also used as a simple performance measure of EMO algorithms for classification
problems because the performance of many non-dominated classifiers can be
summarized as an aggregated scalar measure (i.e., the average error rate of their
ensemble).
Our experimental results can be further improved in several manners because we
used very simple settings for generating classifier ensembles and classifying new
patterns. For example, careful selection of classifiers from non-dominated rule sets
may improve the performance of classifier ensembles. Adjustment of rule weights
and/or membership functions in each individual classifier may also improve the
performance of classifier ensembles. We can also use a weighted vote scheme (or
other voting schemes [19], [21]) instead of the simple majority vote scheme for
improving the performance of ensembles.
The authors would like to thank the financial support from Japan Society for the
Promotion of Science (JSPS) through Grand-in-Aid for Scientific Research (B):
KAKENHI (14380194).
References
1. 
Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., and Verkamo, A. I.: Fast Discovery
of Association Rules, in Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., and
Uthurusamy, R. (eds.) Advances in Knowledge Discovery and Data Mining, AAAI Press,
Metro Park (1996) 307–328.
2. 
Bauer, E., and Kohavi, R.: An Empirical Comparison of Voting Classification Algorithms:
Bagging, Boosting, and Variants, Machine Learning 36 (1999) 105–139.
3. 
Breiman, L.: Bagging Predictors, Machine Learning 24 (1996) 123–140.

1088         H. Ishibuchi and T. Yamamoto
4. 
Cho, S. B., and Kim, J. H.: Combining Multiple Neural Networks by Fuzzy Integral for
Robust Classification, IEEE Trans. on Systems, Man, and Cybernetics 25 (1995) 380–384.
5. 
Deb, K., Pratap, A., Agarwal, S., and Meyarivan, T.: A Fast and Elitist Multiobjective
Genetic Algorithm: NSGA-II, IEEE Trans. on Evolutionary Computation 6 (2002) 182–
197.
6. 
Dietterich, T. G.: An Experimental Comparison of Three Methods for Constructing
Ensembles of Decision Trees: Bagging, Boosting, and Randomization, Machine Learning
40 (2000) 139–157.
7. 
Elomaa, T., and Rousu, J.: General and Efficient Multisplitting of Numerical Attributes,
Machine Learning 36 (1999) 201–244.
8. 
Freund, Y., and Schapire, R. E.: A Decision-Theoretic Generalization of On-Line
Learning and an Application to Boosting, Journal of Computer and System Sciences 55
(1997) 119–139.
9. 
Gonzalez, A., and Perez, R.: SLAVE: A Genetic Learning System Based on an Iterative
Approach, IEEE Trans. on Fuzzy Systems 7 (1999) 176–191.
10. Ishibuchi, H., Murata, T., and Turksen, I. B.: Single-Objective and Two-Objective Genetic
Algorithms for Selecting Linguistic Rules for Pattern Classification Problems, Fuzzy Sets
and Systems 89 (1997) 135–149.
11. Ishibuchi, H., and Nakashima, T.: Effect of Rule Weights in Fuzzy Rule-Based
Classification Systems, IEEE Trans. on Fuzzy Systems 9 (2001) 506–515.
12. Ishibuchi, H., Nakashima, T., and Morisawa, T.: Voting in Fuzzy Rule-Based Systems for
Pattern Classification Problems, Fuzzy Sets and Systems 103 (1999) 223–238.
13. Ishibuchi, H., Nakashima, T., and Murata, T.: Three-Objective Genetics-Based Machine
Learning for Linguistic Rule Extraction, Information Sciences 136 (2001) 109–133.
14. Ishibuchi, H., Nozaki, K., Yamamoto, N., and Tanaka, H.: Selecting Fuzzy If-Then Rules
for Classification Problems Using Genetic Algorithms, IEEE Trans. on Fuzzy Systems 3
(1995) 260–270.
15. Ishibuchi, H., and Yamamoto, T.: Fuzzy Rule Selection by Data Mining Criteria and
Genetic Algorithms, Proc. of Genetic and Evolutionary Computation Conference (2002)
399–406.
16. Ishibuchi, H., and Yamamoto, T.: Effects of Three-Objective Genetic Rule Selection on
the Generalization Ability of Fuzzy Rule-based Systems, Proc. of 2nd International
Conference on Evolutionary Multi-criteria Optimization (2003) (in press).
17. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton G. E.: Adaptive Mixtures of Local
Experts, Neural Computation 3 (1991) 79–87.
18. Jordan, M. I., and Jacobs, R. A.: Hierarchical Mixtures of Experts and the EM Algorithm,
Neural Computation 6 (1994) 181–214.
19. Kittler, J., Hatef, M., Duin, R. P. W., and Matas, J.: On Combining Classifiers, IEEE
Trans. on Pattern Analysis and Machine Intelligence 20 (1998) 226–239.
20. Kuncheva, L. I., and Jain, L. C.: Designing Classifier Fusion Systems by Genetic
Algorithms, IEEE Trans. on Evolutionary Computation 4 (2000) 327–336.
21. Lam, L., and Suen, C. Y.: Optimal Combinations of Pattern Classifiers, Pattern
Recognition Letters 16 (1995) 945–954.
22. Langdon, W.: A Hybrid Genetic Programming Neural Network Classifier for Use in Drug
Discovery, Proc. of 2nd International Conference on Hybrid Intelligence Systems (2002) 6
(Prenary Presentation).
23. Quinlan, J. R.: C4.5: Programs for Machine Learning, Morgan Kaufmann, San Mateo
(1993).

Voronoi Diagrams Based Function Identiﬁcation
Carlos Kavka1 and Marc Schoenauer2
1 LIDIC, Departamento de Inform´atica, Universidad Nacional de San Luis
D5700HHW, San Luis, Argentina
ckavka@unsl.edu.ar
2 Projet Fractales, INRIA Rocquencourt
BP 105, 78153 Le Chesnay Cedex, France
Marc.Schoenauer@inria.fr
Abstract. Evolutionary algorithms have been applied to function iden-
tiﬁcation problems with great success. This paper presents an approach
in which the individuals represent a partition of the input space in
Voronoi regions together with a set of local functions associated to each
one of these regions. In this way, the solution corresponds to a combina-
tion of local functions over a spatial structure topologically represented
by a Voronoi diagram. Experiments show that the evolutionary algo-
rithm can successfully evolve both the partition of the input space and
the parameters of the local functions in simple problems.
1
Introduction
The objective when dealing with a function identiﬁcation problem is to ﬁnd an
approximation that matches as closely as possible an unknown function deﬁned
in a certain domain. There are methods that can be used to optimize the pa-
rameters of the unknown function given a model, and also methods than can get
both the model and the parameters at the same time. A well known example is
the least squares method (LSM), than can compute the coeﬃcients of a linear
combination of base functions. Also non linear regression methods do exist, but
they tend to be very time consuming when compared with the linear approaches.
Neural networks have also been used for function approximation [7]. A neural
network can be considered as a model of connected units, where usually the un-
known parameters are the so called weights. The usual training algorithms can
be used to obtain the weights, but there are also methods that can obtain both
the connection pattern and the weights.
Evolutionary algorithms have been shown very eﬀective in function identiﬁ-
cation problems in a wide range of domains [3] [1]. The Genetic Programming
approach [8] uses a tree structure to represent an executable object (model)
that can be a function – and has been successful addressing regression problems.
Classiﬁer systems have also been used for function approximation. In the XCFS
classiﬁer system [11], the value of the dependent variable is considered as a payoﬀ
to be learned, given the values of the independent variables.
But the choice of a method to solve a given function identiﬁcation problem
also depends on the available data: when examples of input/output patterns of
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1089–1100, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

1090
C. Kavka and M. Schoenauer
the unknown function are given, function identiﬁcation, also then called data
ﬁtting, can be addressed by standard deterministic numerical methods. This is
not the case, however, for the so-called inverse problems: In control for instance,
candidate controllers cannot be speciﬁed just by a set of samples of values of
the independent and dependent variables; but a good simulation of the plant is
usually available, and allows one to evaluate candidate controllers. Even worse,
it is usual that some regions of the space are oversampled, while others are
undersampled.
This work is part of a series of investigations regarding the usefulness of
a particular representation and operators deﬁned to solve the inverse approxi-
mation problem with evolutionary algorithms. The objective is to ﬁnd both a
partition of the domain into a set of regions and a set of functions to be applied
in each region. In this way, instead of the solution being deﬁned by a single
global function, it is deﬁned by a set of local functions. The partition of the
domain is deﬁned in terms of Voronoi regions, a geometric structure that proved
to be useful in interpolation [4] [2] and evolution applied to structural mechanics
problems [10]. No restrictions are deﬁned for the local functions, but they are
expected to be simple functions. Two possible rules for their combination are
proposed in order to get a solution in the whole domain.
The paper is organized as follows: section 2 presents the details on the do-
main partition based on Voronoi regions, section 3 presents details on local ap-
proximators and the way in which they are combined, section 4 introduces the
representation and the evolutionary operators, section 5 some numerical results
on simple problems and section 6 presents the conclusions and the current lines
of research.
2
Domain Partition
The domain partition strategy is based on Voronoi diagrams. A Voronoi dia-
gram induces a subdivision of the space based on a set of points called sites.
An important property is that a number of operations can be executed on its
topological structure just by operating with the sites.
Formally [5], a Voronoi diagram of a set of n points P is the subdivision of
the plane into n cells, one for each site in P, with the property that a point q
lies in the cell corresponding to a site pi if and only if the distance between q
and pi is smaller than the distance between q and pj for each pj ∈P with j ̸= i.
In other words, the cell of the site pi contains all the points in the plane for
which pi is the closest site. The Voronoi diagram of P will be denoted by Vor(P)
and the cell (or region) that corresponds to pi by Vor(pi). Figure 1 illustrates
an example of a Voronoi diagram in IR2. The deﬁnition can be straightforward
extended to IRn, with n ≥2.
A related concept that will be used in the paper is the so called Delaunay
triangulation. A triangulation [5] of a set of points P is deﬁned as the maximal
planar subdivision whose vertex set is P. A maximal planar subdivision S is a
subdivision such that no edge connecting two vertices can be added to S without

Voronoi Diagrams Based Function Identiﬁcation
1091
destroying its planarity. In other words, any edge that is not in S intersects one
of the existing edges. A triangulation T of a set of points P is a Delaunay
triangulation if and only if the circumcicle of any triangle in T does not contain
a point of P in its interior. A circumcicle of a triangle is deﬁned as the circle that
goes through its three summits. Figure 1 illustrates an example of a Delaunay
triangulation in IR2.
Fig. 1. An example of a Voronoi diagram (left) and a Delaunay triangulation (right)
for a set of points in IR2
3
Approximation
A solution to the function identiﬁcation problem is deﬁned by a partition of the
domain in Voronoi regions together with a set of local functions. Two ways of
recombining local functions are proposed: a non continuous combination, where
each function deﬁnes the approximation in its own region, and a continuous
combination, where the local functions are adapted in order to get a continuous
approximation in the complete domain.
3.1
The Non-continuous Combination
Given a set of points P = {p1, p2, . . . , pn} and a set of continuous local func-
tions F = {f1, f2, . . . , fn}, each fi being deﬁned on V or(pi), the value of the
approximation F is deﬁned as follows:
F(x) = fi(x)
if
x ∈Vor(pi) .
(1)
The value of the approximation F is computed as the value of the local function
fi associated to the Voronoi region to which the input value x belongs. The
deﬁnition of F presents the following properties:
– It is deﬁned on every point in the domain, provided that the local functions
are deﬁned, since the Voronoi diagram induces a partition of the complete
domain.
– It is a (possibly) discontinuous function, presenting discontinuities at the
Voronoi region boundaries.

1092
C. Kavka and M. Schoenauer
3.2
The Continuous Combination
Given a set of points P = {p1, p2, . . . , pn}, a set of continuous local functions
F = {f1, f2, . . . , fn} and a set of real values V = {v1, v2, . . . , vn}, the value of
the approximation F is deﬁned as follows:
F(x) = fi(x) ∗D(x) + L(x)
if
x ∈Vor(pi) .
(2)
The value of the approximation F is computed as the value of the local function
fi associated to the Voronoi region to which the input value x belongs to, scaled
by a distance factor D, with the addition of the value of a global function L
evaluated on x.
The distance factor D is deﬁned as follows:
D(x) =

φ

d(x,boundary(x))
d(pi,boundary(x))

if boundary(x) is deﬁned
1
if boundary(x) is not deﬁned
(3)
where pi is the center of the Voronoi region to which x belongs to, boundary(x)
the point that corresponds to the Voronoi regions boundary, d(a, b) the Euclidean
distance between the points a and b, and φ a continuous function deﬁned in the
range [0..1] such that φ(0) = 0 and φ(1) = 1. boundary(x) is deﬁned as the
intersection of the line joining pi and x and the boundaries of V or(pi), if that
intersection lies within the domain of the deﬁnition of the problem.
The global function L is deﬁned based on the Delaunay triangulation as
follows:
L(x) =
d+1

j=1
vj ∗lj .
(4)
where d is the dimension of the input space, l1, . . . , ld+1 are the barycentric
coordinate of x in simplex T (triangle the case if d = 2), and v1, . . . , vd+1 are
given values, each one associated with the corresponding Voronoi site (see section
4.1). The deﬁnition of F presents the following properties:
– It is deﬁned on every point in the domain, provided that the local functions
are deﬁned, since the Voronoi diagram induces a partition for the complete
domain, and D and L are continuous functions.
– The value of the ﬁrst term is the local approximator in the center of the
region and 0 in the boundaries, since the distance factor D produces a value
in the range [0..1], with the value 0 in the boundaries of the Voronoi region
and the value 1 in the center. The function φ controls the shape of the
change of D between 0 and 1. It can be linear, quadratic, exponential, . . . .
Ultimately, diﬀerent functions could be attached to diﬀerent Voronoi sites.
In the external area of unbounded Voronoi regions the value of D is 1.
– The function L is global since it depends on the values vi associated to the
sites and their positions in the space. It is a piecewise linear continuous
function. The vertices of the triangles deﬁned by the Delaunay triangulation
are the sites from P. Given a point x that belongs to the triangle T in the

Voronoi Diagrams Based Function Identiﬁcation
1093
Delaunay triangulation, the value of the global function L is computed by
performing an interpolation of the values associated to the vertices by using
the barycentric coordinates of the point x in the triangle. The barycentric
coordinates of a point x are the local coordinates of x in T representing
the ratios of the areas of the sub-triangles formed by x and the sides of
the triangle. It corresponds to what is called a triangular element in the
Finite Elements Method terminology [12]. As an example, there are three
barycentric coordinates for a point in IR2: l1, l2, l3, with  li = 1. The value
of one of each of the barycentric coordinates is 1 in a vertex while the value
of the others barycentric coordinates is 0. Figure 2 presents the value of the
three barycentric coordinates for a triangle in IR2, together with an example
of the global function L evaluated on the same triangle. Note that even if
1
p2
p3
p1
p1
p2
1
p3
p1
p2
p3
1
p2
p3
v3
v1
v2
p1
Fig. 2. Barycentric coordinates in a triangle in IR2 (from left to right): barycentric
coordinate l1, barycentric coordinate l2, barycentric coordinate l3, the global function
L in the triangle T
it is true that every point in the domain belongs to a Voronoi region, it is
not the case with the Delaunay triangulation. Some points that belong to
unbounded Voronoi regions do not belong to any triangle deﬁned by the
Delaunay triangulation (see ﬁgure 1). In order to deﬁne L on these points,
a large triangle covering the whole domain and containing all points in P is
deﬁned with values 0 associated to its vertices [5].
4
The Evolutionary Algorithm
This section introduces the representation used for the individuals and the deﬁ-
nition of the evolutionary operators.
4.1
The Representation
Each individual has to represent a complete solution, or in other words, a com-
plete approximation F. A convenient representation is a list of variable length
of Voronoi sites, local approximator parameters and global function values (if
used), represented as real values.

1094
C. Kavka and M. Schoenauer
In order to formalize the deﬁnition, let us deﬁne a local vector as the vector
that contains all parameters associated to a Voronoi region: the coordinates that
deﬁne the site, the parameters of the local approximator and the value of the
global function (if applicable). A local vector lvi of an individual ind with non
continuous approximation is deﬁned as the vector:
lvi = [c1, . . . , cd, par1, par2, . . . , parm]
or as the following vector with the continuous approach:
lvi = [c1, . . . , cd, v, par1, par2, . . . , parm]
where the site coordinates are (c1, . . . , cd), d the dimension of the domain, v the
value of the global function on the site and par1, . . . , parm the m parameters
that deﬁnes the local approximator.
An individual ind can be represented as a vector of variable length that
is built from the concatenation of the local vectors that represent the local
approximators in the solution:
ind = lv1 + lv2 + . . . + lvn
As an example, an individual in IR with 3 local approximators deﬁned with two
parameters (a and b) each one is:
with non continuous approximation: [p1, a1, b1


	
lv1
, p2, a2, b2


	
lv2
, p3, a3, b3


	
lv3
]
with continuous approximation: [p1, v1, a1, b1


	
lv1
, p2, v2, a2, b2


	
lv2
, p3, v3, a3, b3


	
lv3
]
The representation proposed does not impose restrictions on the shape of the lo-
cal approximators. In this work, we have used linear, quadratic and RBF approxi-
mators. They were deﬁned in IR2 respectively as f(x) = ax+b, f(x) = ax2+bx+c
and f(x) = exp (−(x −p)2/a2)b + c, where a, b and c are the parameters, and p
the center of the corresponding Voronoi region.
4.2
The Operators
Three mutation operators and one crossover operator have been speciﬁcally de-
ﬁned for individuals that represent a Voronoi diagram. They are described below:
Voronoi crossover: This crossover operator is based on the crossover deﬁned
in [10] and exchanges the local vectors of the individuals by using geometric
properties of the Voronoi diagrams. A random hyperplane h of dimension
d −1 is randomly deﬁned. The ﬁrst child receives the local vectors from the
ﬁrst parent that lie on the left of h, and the local vectors from the second
parent that lie on the right of h. The second child receives the remaining
local vectors. Figure 3 presents an example of the application of this operator

Voronoi Diagrams Based Function Identiﬁcation
1095
Fig. 3. An example of the application of the Voronoi crossover in IR2
in IR2. Formally, given two individuals ind1 and ind2 with n and m local
vectors respectively:
ind1 = [lv1
1, ..., lv1
n]
ind2 = [lv2
1, ..., lv2
m]
The two children child1 and child2 are:
child1 = {lv1
i , lv2
j /lv1
i ∈Left(h), lv2
j ∈Right(h)}
child2 = {lv1
i , lv2
j /lv1
i ∈Right(h), lv2
j ∈Left(h)}
where h is a random hyperplane of dimension d−1, a local vector belongs to
Left(h) if its site is on the left of h and a local vector belongs to Right(h) if
its site is on the right of h. The concept of being on the right or on the left
of an hyperplane can be found in computational geometry textbooks, like
for example [5].
Mutation: This operator modiﬁes the coordinates of the Voronoi site, the pa-
rameters of the approximator or the global function value (if used) by up-
dating the values as follows:
xi(t) =

xi(t) + ∆(t, ubj −xi(t))
u < 0.5
xi(t) −∆(t, xi(t) −lbi)
u ≥0.5
(5)
where xi(t) is the i-th parameter at time t, u is a uniformly generated random
number in [0 : 1], lbi and ubi are respectively the lower bound and the upper
bound for the parameter xi, and ∆is a function deﬁned as follows:
∆(t, y) = yv(1 −t/T)b .
(6)
where v ∈[0 : 1] is a random number obtained from a gaussian distribution
with mean 0 and standard deviation 0.3, T the maximum number of gen-
erations and b a parameter that controls the degree of nonuniformity. This
operator is used to search the space uniformly in the ﬁrst generations and
very locally at the end, in order to ﬁne tune the values in the individual. It
is based on the nonuniform mutation operator deﬁned by Michalewicz [9].
Add mutation: This mutation operator is used to add a new random local
vector to the individual.
Del mutation: This mutation operator is used to remove a local vector from
an individual.

1096
C. Kavka and M. Schoenauer
5
Numerical Results
The objective of this section is to evaluate the representation and the operators
described in the previous sections, when applied to simple function identiﬁcation
problems.
The ﬁrst experiment consists in the approximation of the following func-
tion in IR, with linear, quadratic and RBF approximators, using both the non
continuous and the continuous approach:
f(x) =





sin(3πx)2
x
−1 ≤x < 0
1
x+exp(−x) −1
0 ≤x ≤1
(7)
The algorithm implemented is a classical generational genetic algorithm with
population size 100 and tournament selection (100 parents give birth to 100 oﬀ-
spring that replace the 100 parents). For the experimental test, the crossover rate
set to 0.7, mutation rate to 0.1, mutation b parameter to 0.2, adding mutation
rate equal to 0.1, delete mutation rate equal to 0.1, the maximum number of
function evaluations to 50 000. Individuals are evaluated by computing the error
on the approximation of a set of points, selected randomly for each individual on
each evaluation. It means that the individuals are not evaluated with the same
dataset – as would happen in inverse problems. The function φ (see equation
(3)) is the identity. The results are summarized in table 1, and corresponds to
10 independent runs for each set of parameters. The errors are speciﬁed as a
percentage of the output range, divided by 100. The error is computed by evalu-
ating the approximation obtained in 1000 points evenly selected from the whole
domain, and comparing its value with the real value obtained from the target
function. As a consequence of this fact, the ﬁtness values are not the same as
the errors presented in this table. The ﬁgure 4 presents examples of the kind of
solutions obtained with the diﬀerent kind of approximators. Each plot shows the
function to be approximated and the approximation obtained. The boxes in the
diagram correspond to the Voronoi regions in which the domain is partitioned.
The solutions obtained by all methods are comparable in quality. Better solu-
tions can be found by increasing the limit on the number of function evaluations.
The main diﬀerence is on the size of the approximators obtained. A continuous
combination can produce solutions with less local approximators, maintaining
the same quality in the approximation.
The second experiment concerns the
crossover operator. Since the representation allows the use of standard one point
crossover, the objective of the second experiment is to compare the Voronoi
crossover with the one point crossover. The one point crossover tends to gener-
ate very long individuals, or in other words, individuals with a big number of
Voronoi regions. It was not possible to use just the one point crossover, with-
out adding a regularization term [7] in order to penalize long individuals. The
results are summarized in table 2, and corresponds to 10 independent runs for
each set of parameters. The value of the regularization parameter corresponds
to the factor used to weight the size of the individual against the error in order

Voronoi Diagrams Based Function Identiﬁcation
1097
Table 1. Solutions found by the evolutionary algorithm for the ﬁrst experiment. The
smallest (resp. largest) size values correspond to the size of the smallest (resp. largest)
best individuals found in the set of runs
approximator continuity best error std. dev. smallest largest
linear
no
0.017882
0.00562
12
23
quadratic
no
0.017015
0.00693
12
29
RBF
no
0.013958
0.00561
11
53
linear
yes
0.018051
0.00577
6
11
quadratic
yes
0.011043
0.01041
5
13
RBF
yes
0.011468
0.00353
4
16
(a)
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
f
app
(b)
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
f
app
(c)
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
f
app
(d)
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
f
app
(e)
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
f
app
(f)
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
f
app
Fig. 4. Example of approximations obtained for the ﬁrst function by using (a) linear,
(b) quadratic and (c) RBF local approximators with no continuity, and using (d) linear,
(e) quadratic and (f) RBF local approximators with continuity

1098
C. Kavka and M. Schoenauer
Table 2. Solutions found for the second experiment
approximator continuity crossover regularization best error smallest largest
linear
yes
voronoi
no
0.018051
6
11
linear
yes
one point no
-
-
-
linear
yes
one point 0.995
0.024664
23
41
linear
yes
one point 0.990
0.030236
11
29
to compute the ﬁtness. The third experiment is intended to analyze the shape of
the Voronoi partitions obtained in the approximation of the following functions
in IR2, by using linear approximators and continuous combination:
f(x, y) = x exp(−x2 −y2)
f(x, y) = sin(

x2 + y2) .
(8)
The same parameters of the ﬁrst experiment are used. The results are summa-
rized in table 3, and correspond to 5 independent runs. The ﬁgure 5 presents
the plot of the target functions, examples of the approximations obtained by the
algorithm and their corresponding Voronoi partitions.
Table 3. Solutions found by the evolutionary for the third experiment
approximator continuity crossover regularization best error std.dev.
linear
yes
voronoi
no
0.021192
0.01092
linear
yes
voronoi
no
0.028055
0.02411
6
Conclusions
This work presents preliminary results on the use of evolutionary computation
and Voronoi diagrams in function identiﬁcation. The proposed representation
and the operators allow the algorithm to evolve both the partition of the input
space and the local functions to be applied in each region. The partition of the
input space is performed through Voronoi diagrams. Two ways to combine the
local approximators are proposed. The representation of the partition does not
depend on the type of local approximators used. In this work, linear, quadratic
and RBF local approximators were used, but other kind of approximators can
also be used.
Experiments on simple problems show that good approximations can be ob-
tained with linear, quadratic and RBF local approximators. They also show that
continuity in the combination of local functions can help to obtain approxima-
tions with smaller number of local approximators. The Voronoi crossover showed

Voronoi Diagrams Based Function Identiﬁcation
1099
-1
-0.8 -0.6 -0.4 -0.2
0
0.2
0.4
0.6
0.8
1
-1 -0.8-0.6-0.4-0.20
0.20.40.60.81
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
-1
-0.8 -0.6 -0.4 -0.2
0
0.2
0.4
0.6
0.8
1
-1 -0.8-0.6-0.4-0.20
0.20.40.60.81
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
-1
-0.8 -0.6 -0.4 -0.2
0
0.2
0.4
0.6
0.8
1
-1 -0.8-0.6-0.4-0.20
0.20.40.60.81
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
-1
-0.8 -0.6 -0.4 -0.2
0
0.2
0.4
0.6
0.8
1
-1 -0.8-0.6-0.4-0.20
0.20.40.60.81
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
Fig. 5. Target functions, examples of approximations and Voronoi diagram partitions
superior performance when compared to standard one point crossover. In par-
ticular, it was not necessary to add a regularization term in order to avoid the
solutions to be composed of a very large number of local approximators.
The evolution of individuals with RBF approximators produces an approxi-
mation F that is equivalent to an RBF neural network, since the computation
performed by the local approximators correspond to what the nodes of the ﬁrst
layer do, and the combination strategy to the computation done at the output
node. The usual training strategy of an RBF neural network consists in two
steps [7]: a selection for the centers of the nodes in the ﬁrst layer, and training
in order to compute the weights that connect the two layers. Usually the ﬁrst
step is done through a non supervised technique (or by randomly selecting the
centers), and the second step with supervised training. With the evolutionary
algorithm, both steps are performed at the same time. An advantage of this
approach is that it is not necessary to deﬁne in advance the number of centers
(or regions) since the algorithm does the partition by itself.
It can be noted from the experiments that the evolutionary algorithm assigns
more local approximators in areas that are more diﬃcult to approximate. The
concept of being diﬃcult depends on the kind of local approximator. For exam-
ple, a linear segment is easy to approximate for a linear local approximator but

1100
C. Kavka and M. Schoenauer
more diﬃcult for an RBF approximator. A representation that allows diﬀerent
kinds of local approximators to be evolved at the same time in diﬀerent regions
of the space is under analysis now.
Future experiments will include problems deﬁned in terms of ordinary dif-
ferential equations, like for example, problems from control, or models of some
physical or chemical processes [6]. The main characteristic is the fact that these
problems cannot be deﬁned just in terms of a set of input-output patterns. In
the experiments performed till now, even if it could have been possible to de-
ﬁne the problems in terms of a ﬁxed data set, it was preferred to follow this
approach as much as possible, generating new random patterns for each evalua-
tion of the individuals. It is expected that the local approximation properties of
the method proposed can demonstrate itself to be useful in the identiﬁcation of
complex functions that do not pertain to data ﬁtting.
References
1. Ahmed, M. and De Jong, K.: Function Approximator Design using Genetic Algo-
rithms. Proceedings of the 1997 IEEE Int. Conference on Evolutionary Computa-
tion, Indianapolis, IN, pp. 519–523 (1997)
2. Anton, F., Mioc D. and Gold C.: Line Voronoi Diagram based Interpolation and
Application to Digital Terrain Modeling. Proceedings of the 13th Canadian Con-
ference on Computational Geometry, University of Waterloo, (2001)
3. Back, T. Fogel, D. and Michalewicz, Z.(Eds.): Handbook of Evolutionary Compu-
tation. IOP Publishing Ltd and Oxford University Press. (1997)
4. Boissonnat, J. and Cazals, F.: Smooth Surface Reconstruction via Natural Neigh-
bour Interpolation of Distance Functions. Rapport de recherche de l’INRIA –
Sophia Antipolis (2000)
5. de Berg, M., van Kreveld, M., Overmars, M., Schwarzkopf, O.: Computational
Geometry, Algorithms and Applications. Second Edition. Springer-Verlag. (1998)
6. Fadda, A. and Schoenauer, M., Evolutionary Chromatographic Law Identiﬁcation
by Recurrent Neural Nets. In J. R. McDonnell and R. G. Reynolds and D. B. Fogel,
Eds, Proc. 4th Annual Conference on Evolutionary Programming, pp 219–235. MIT
Press, 1995.
7. Fiesler, E. and Beale R. (Eds.): Handbook of Neural Computation. Institute of
Physics Publishing. (1997)
8. Koza, J.: Genetic Programming: On the Programming of Computers by means of
Natural Evolution. MIT Press. (1992)
9. Michalewicz, Z.: Genetic Algorithms + Data Structures = Evolution Programs.
Third, Revised and Extended Edition. Springer-Verlag Berlin Heidelberg. (1999)
10. Schoenauer, M., Jouve, F. and Kallel, L. Identiﬁcation of Mechanical Inclusions.
Dasgupta and Z. Michalewicz Ed. Identiﬁcation of Mechanical Inclusions. Evolu-
tionary Algorithms in Engineering Applications. (1997)
11. Wilson, S.: Classiﬁers that approximate functions. Natural Computing, 1(2–3),
211–234 (2002)
12. Zienkiewicz, O. C. and Taylor R. L.: Finite Element Method: Volume 1, The Basis.
Fifth edition. Butterworth-Heinemann.(2000)

New Usage of SOM for Genetic Algorithms
Jung-Hwan Kim and Byung-Ro Moon
School of Computer Science and Engineering
Seoul National University
Shilim-dong, Kwanak-gu, Seoul, 151-742 Korea
{aram,moon}@soar.snu.ac.kr
Abstract. Self-Organizing Map (SOM) is an unsupervised learning neu-
ral network and it is used for preserving the structural relationships in
the data without prior knowledge. SOM has been applied in the study of
complex problems such as vector quantizations, combinatorial optimiza-
tion, and pattern recognition. This paper proposes a new usage of SOM
as a tool for schema transformation hoping to achieve more eﬃcient ge-
netic process. Every oﬀspring is transformed into an isomorphic neural
network with more desirable shape for genetic search. This helps genes
with strong epistasis to stay close together in the chromosome. Experi-
mental results showed considerable improvement over previous results.
1
Introduction
There have been a great many genetic algorithms (GAs) for neural network (NN)
optimization [9][11][18]. In order to represent solutions for NN optimization,
most GAs used linear encodings following the convention of the GA community
[11][15]. Mostly, every weight in an NN takes a position in a linear chromosome.
However, linear encodings have limited capability in reﬂecting the geographic
linkages of genes [1][7]. A number of 2D encodings were used to better reﬂect
the geographic linkages of genes [1][4][7][12].
In this paper, we use a two-dimensional encoding for the genetic representa-
tion and employ 2D geographic crossover which demonstrated good performance
for the neural network optimization problem [13] and the graph partitioning
problem [12][16][17]. The weights of an NN can be represented by a 2D matrix
and thus they are intrinsically suitable for 2D encoding.
A typical NN consists of input, output, and hidden layers. The neurons in the
hidden layer enable the network to learn complex tasks by extracting progres-
sively more meaningful features from the input patterns and to form the rules
classifying the input patterns. Once an NN is represented by a two-dimensional
encoding and the structure of the NN is ﬁxed, the genotype of an NN depends
on the order of neurons in the structure. All the hidden neurons are identical
when only the connections are considered. They become diﬀerent only after they
start having weights in the connections. Some hidden neurons have stronger re-
lation than between ordinary pairs of hidden neurons. However, the relationship
among the hidden neurons is not revealed from neurons’ placements. Since the
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1101–1111, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

1102
J.-H. Kim and B.-R. Moon
Output Layer
Hidden Layer
Input Layer
Fig. 1. The recurrent neural network architecture
indices of hidden nodes are assigned before they start having weights, there is
no guarantee that strongly related neurons are assigned close indices to one an-
other. This causes high-quality schemata not to survive well. In this paper, we
transform each neural network to an isomorphic neural network with diﬀerent
indices of neurons and we aim to develop a better genetic algorithm by achieving
a better representation of neural networks. We use self-organizing map (SOM)
for the transformation.
Self-Organizing Map (SOM) is an unsupervised learning neural network and
it is used for preserving the structural relationships in data without prior knowl-
edge. SOM has been applied in the study of complex problems such as vector
quantizations, combinatorial optimization, and pattern recognition. This paper
proposes a new usage of SOM as a tool for schema transformation hoping to
achieve more eﬃcient genetic process.
The rest of this paper is organized as follows. In the next section, we describe
the necessity of clustering strongly connected neurons and the property of SOM.
In Section 3, we describe the transformation that enables useful building blocks
to survive well. Section 4 shows our hybrid neuro-genetic framework for improv-
ing the performance of the neural network. We present our experimental results
in Section 5 and state our conclusions in Section 6.
2
Preliminaries
2.1
Network Architecture
We use a recurrent neural network (RNN) architecture based on Elman’s re-
current neural network [8]. It consists of two layers, excluding the input layer,
as shown in Figure 1. Each hidden unit is connected to itself and also fully
connected to all the other units. These connections are updated in the prop-
agation phase at every ∆t time interval. The network is usually trained by a
backpropagation-based algorithm.

New Usage of SOM for Genetic Algorithms
1103
2.2
Topological Ordering Property of SOM
SOM is an eﬀective software tool for the visualization of high-dimensional data.
It converts complex, nonlinear statistical relationships among high-dimensional
data items into simple geometric relationships on a low-dimensional display [14].
As it thereby compresses information while preserving the most important topo-
logical and metric relationships of the primary data items on the display, it may
also be thought to produce some kind of abstractions. These two aspects, visu-
alization and abstraction, can be utilized in a number of ways in complex tasks
such as process analysis, machine perception, control, and communication. The
data items are organized into a meaningful two-dimensional order in which sim-
ilar items are close to one another. In this sense, SOM is a similarity graph and
also a clustering diagram [14].
2.3
Gene Reordering
Each gene location has an explicit meaning in most genetic algorithms. This type
encoding is called locus-based encoding. Many researchers have studied the gene
reordering. Bagley [2] tried to change the gene ordering at random. Bui and Moon
[3][6] tried to exploit the useful clustering information of graphs and showed
that the performance of a GA can be improved by reordering genes in the graph
bipartitioning problem with a linear encoding. Kim and Moon [13] proposed a
reordering heuristic for neural-network representation. They believed that edges
connecting units with strong relationships would have stronger patterns than a
random set of edges and used the relative strengths of the connection weights
to measure the relationships among genes. This approach showed considerable
improvement of GA performance.
3
Transforming into Isomorphic Structures
Since the hidden neurons of the network are fully connected to one another,
there are great many hidden node orderings with equivalent function. All these
structures represent the same network but have diﬀerent genotypes. Some of
them are more advantageous than the others in genetic search. Figure 2 shows
an example with matrix encoding. In the ﬁgure, the three neurons j, i, and i+1
are assumed to have strong relationships. The positions of corresponding weights
are marked with rectangles. The set of rectangles constructs a schema of perhaps
high quality. Although the geographic crossover has the new-schema creation
power for 2D encoding, if useful features on the encoding are not clustered
(see the left weight matrix in Figure 2), corresponding schemata are not easy
to survive in most crossovers. It is important that the given structure must
be transformed to the functionally equivalent structure containing the useful
clustered blocks. When we exchange the positions of neurons j and i −1 in the
left weight matrix, the schema is transformed to another one with better shape
for survival (See the right weight matrix in Figure 2). We aim to transform high

1104
J.-H. Kim and B.-R. Moon
i
i+1
i−1
i
i+1
i+1
i
i+1
i
i−1
j
j
Fig. 2. Change of schema after transformation
quality schemata into better shapes with equivalent function. The more clusters
of the high correlated neurons, the ﬁner approximation of the complex system to
be modeled can be eventually obtained. The idea takes advantage of the ability
of SOM for clustering neurons.
3.1
SOM-Based Transformation
Each hidden neuron has a vector of weights and aﬀects other neurons according
to the weights. A large weight corresponds to a strong activation of another
neuron. On the other hand, a signiﬁcantly small negative weight corresponds to
strong inhibition. If a neuron strongly activates or inhibits another neuron, we
consider that these neurons are highly correlated. This information is utilized in
the transformation. However, it is diﬃcult to analyze the relative relationship
among neurons and the spatial ordering of the corresponding neurons. In this
work, we transform the topology of neural networks using SOM. This approach
is motivated by a desire to better preserve the relationships of hidden neurons
in the genetic process.
We describe the details of the SOM-based transformation in the following:
Input : an RNN with weights between neurons.
Output : a one-dimensional order of hidden neurons
Π = ⟨π1, · · · , πp⟩,
πi ∈{1, 2, · · · , p}.
1. The hidden layer is mapped to the competitive layer of SOM. That is, the
number of hidden neurons is the same as the number of competitive neurons
in SOM. The synaptic weights of the RNN are used as the input vectors
for the SOM. Let the number of input neurons, hidden neurons, and output
neurons of RNN be n, p, and q, respectively. Then the SOM has the (n+p+q)
input neurons and the p competitive neurons.

New Usage of SOM for Genetic Algorithms
1105
1
2
3
5
6
4
2
6
3
4
1
5
mapping
restructuring
(reorganizing)
competitive layer
of SOM
competitive layer
of SOM
3
mapping
restructured
1
5
4
6
2
2
5
6
3
1
4
Fig. 3. Mapping between RNN and SOM
2. SOM is trained with the synaptic weights related to the hidden neurons of
the RNN. SOM produces a similarity graph reﬂecting the relationship among
these synaptic weights. The number of input data pairs for SOM is p. That
is, SOM trains the data pairs ⟨xj, dj⟩where xj is an input vector for the
SOM and dj is the label corresponding to xj as follows:
xj = wj,
dj = i
(1 ≤i ≤p)
wj = ⟨w(0)
j1 , · · · , w(0)
jn , w(1)
j1 , · · · , w(1)
jp , w(2)
j1 , · · · , w(2)
jq ⟩
where
w(0)
jk : weight from input unit k to the hidden unit j of the RNN
w(1)
jk : weight from hidden unit k to the hidden unit j of the RNN
w(2)
jk : weight from hidden unit j to the output unit k of the RNN
3. The SOM produces a linear order of the competitive neurons. The hidden
neurons are rearranged according to the order produced by SOM. Sometimes
several competitive neurons may have the same index. It occurs when the
synaptic weights for diﬀerent hidden neurons without being transformed
represent a similar distribution. In this case, we assign the same index to
multiple competitive neurons. This attempts to make the neural network
obtain the eﬀect of the weight-sharing [19].
Figure 3 shows an example SOM-based transformation. In the ﬁgure, the
solid and dashed lines indicate the weights of high magnitudes. SOM trains the
synaptic weights of the given network (the left-top neural network in Figure 3)
and produces an ordering Π = ⟨3, 2, 6, 4, 1, 5⟩. The location of each node on
the competitive layer of SOM reﬂects the relative relationship among hidden
neurons. The high correlated neurons 2 and 6 locate distant to each other before
the transformation; they are clustered after the transformation.

1106
J.-H. Kim and B.-R. Moon
Create initial population of fixed size;
do {
choose parent1 and parent2 from population;
oﬀspring = g2d xover(parent1, parent2);
mutation(oﬀspring);
backpropagation(oﬀspring);
SOM-transformation(oﬀspring);
if suited(oﬀspring)
then replace(population, oﬀspring);
} until (stopping condition);
Return the best solution;
Fig. 4. The template of the hybrid GA
4
GA for Optimizing the Neural Network
4.1
The GA Structure
The template of the proposed genetic algorithm is shown in Figure 4. It is a
typical steady-state hybrid genetic algorithm except for the SOM-based trans-
formation. In the following, we describe the genetic framework.
– Initialization: The initial population is generated at random. We set the
population size to be 50.
– Selection: In each iteration, two parents are selected according to probabil-
ities that are proportional to their ﬁtness values. The probability that the
best solution is chosen was given four times that of the worst solution. This
is a typical normalized roulette-wheel proportional selection.
– Crossover: The oﬀspring is produced through geographic 2D crossover. The
“g2d xover” of Figure 4 indicates the geographic 2D crossover. The geo-
graphic 2D crossover is described in Section 4.3.
– Mutation: With a low probability, this oﬀspring is then modiﬁed by a mu-
tation operator as follows: ωi = ωi + λ0.5 where ωi is the ith gene value and
λγ returns a random value between -γ and γ .
– Local Optimization: After an oﬀspring is modiﬁed by a mutation operator, it
is locally optimized by backpropagation. The backpropagation process helps
the GA ﬁne-tune around local optima. From another perspective, the GA
provides diverse initial solutions to the backpropagation routine. After the
backpropagation, the neural network has a mean-square error and a discrim-
ination ratio on the training data. The ﬁtness of the oﬀspring is determined
with these two values1.
– Transformation: After local optimization, the genes in the oﬀspring are re-
ordered. This causes a structural change of the corresponding neural network
1 f = ε + η(100 −µ),
where ε is the mean-square error, µ is the discrimination ratio, and η is a constant.

New Usage of SOM for Genetic Algorithms
1107
m
m
m
m
m
m
m



!!!!!!!!!
@
@
@
@



a
a
a
a
a
a
a
a
a
T
T
T
T



""""""
"
b
b
b
b
b
b
b
D
D
DD
1
2
3
4
5
6
7
w41
w42
w43
w51
w52
w53
w44
w45
w55
w54
w64
w65
w74
w75
input units



hidden units
  
output units
  
1
2
3
4
5
6
7
4 w41 w42 w43 w44
w45
w64
w74
5 w51 w52 w53 w54
w55
w65
w75
Fig. 5. Example of 2D encoding
without aﬀecting the quality. The reorganizing approach was described in
Section 3.
– Replacement: Then the oﬀspring replaces a solution in the population by
the following rule: the more similar parent to the oﬀspring is replaced if the
oﬀspring is better; otherwise, the other parent is replaced if the oﬀspring
is better; if not again, the worst chromosome in the population is replaced.
The rational behind this is to maintain the population diversity to the extent
that not too much time is wasted [5].
– Stopping Condition: For stopping, we use a ﬁxed number of generations.
4.2
Problem Encoding
Most GAs for neural network optimization encode a solution (a set of weights)
with a linear string following the convention [9][11][15]. Instead of transforming
into a linear string, we represent a solution by a weight matrix. In the matrix,
each row corresponds to a hidden unit and each column corresponds to an input
unit, a hidden unit, or an output unit. Figure 5 shows an example of such
encoding. In this ﬁgure, the number within a circle (neuron) indicates the index
of the neuron and wij represents the weight of the edge from neuron j to neuron
i. The relationships among the edges in the network can be better reﬂected in
this 2D representation, which was experimentally supported in [13]. In addition,
we suspect that some neurons have stronger relationships with one another than
with the others. We further modify the encoding by transforming the given
network into an isomorphic network (See Section 3). In summary, a chromosome
is represented by a p × (p + n + q) matrix with columns and rows rearranged,
where p, n, and q are the numbers of hidden units, input units, and output units,
respectively.
4.3
Geographic 2D Crossover
Two-dimensional encoding can preserve more geographical relationships among
the genes [4][12][13]. However, when traditional straight-line-based cutting
strategies are used, the power of new-schema creation is far below that of

1108
J.-H. Kim and B.-R. Moon
offspring
offspring
Fig. 6. Example of 2D geographic crossover
crossovers on linear encodings [12]. Geographic crossover was suggested to resolve
this problem [12]. In the case of a 2D encoding, it chooses a number of mono-
tonic lines, divides the chromosomal domain into two equivalence classes, and
alternately copies the genes from the two parent chromosomes. Figure 6 shows
two example geographic crossover operators. We used geographic crossover in
this work. By combining two-dimensional representation, isomorphic transfor-
mation, and 2D geographic crossover, we are pursuing both reduced information
loss in the stage of encoding and the power of new-schema creation.
5
Experimental Results
5.1
Database
We selected three well known datasets to measure the performance of the pro-
posed approach. First, we used the MNIST handwritten character recognition
database2 for input patterns of 28 × 28 grids. The samples are evenly classiﬁed
to ten digits. The digit images were written by 500 diﬀerent writers. Recogni-
tion of handwritten character is an important task in the automated document
analysis. This application is used to read postal addresses and bank checks, etc.
We used 800 samples for training, 200 samples for the validation set, and 2000
samples for the test set. We used 45 input units and 20 hidden units by using a
vertical density distribution and a horizontal density distribution as in [13].
Next, we experiment with Wisconsin breast cancer data (WBCD) and Cleve-
land heart disease data (CHDD) obtained from the UCI repository3. The WBCD
contains 699 instances with 458 benign and 241 malignant cases. Each instance
is described by a case number, nine attributes with integer values in the range
[1, 10] and a binary class label. In the experiment using WBCD database, we
used nine input units, nine hidden units, and two output units in the neural
2 http://yann.lecun.com/exdb/mnist
3 http://www.ics.uci.edu/˜mlearn/MLRepository.html

New Usage of SOM for Genetic Algorithms
1109
Table 1. Result for three testbeds
BP
GA
Original
Heuristic
SOM
MNIST 90.28 (93.45) 94.08 (95.20) 96.52 (97.10) 96.85 (99.60)
WBCD 95.88 (96.37) 96.87 (97.21) 96.91 (97.43) 97.19 (97.79)
CHDD 80.56 (83.25) 82.22 (84.26) 82.75 (86.84) 82.82 (87.02)
network. We divided the data into 80 training samples, 20 validation samples,
and 599 test samples. The CHDD contains 303 instances with 164 healthy in-
stances. The rest are heart disease instances of various degrees of severity (4
classes). Each instance is described with 13 attributes. We used 80 samples for
the training set, 20 samples for the validation set, and 203 samples for the test
set. We used 13-21-4 network structure for CHDD data.
In these experiment, for robust check for the eﬀect of the proposed strat-
egy, we experimented with the data following the 5-fold hold out method cross-
validation approach [10]. In such a situation, we divide the available set of data
into 5 subsets. The model is trained on all the subsets except for one, and the
validation error is measured by testing it on the subset left out. This procedure
is repeated for a total of 5 trials, each time using a diﬀerent subset for validation.
5.2
Experimental Results
Now the general eﬀect of the proposed SOM-based transformation is illustrated
with the above databases. The weights of a neural network were initialized at
random between −0.5 and 0.5. The learning rate was set to 0.8 and the momen-
tum factor was used. The weights were tuned locally by backpropagation.
Table 1 shows the classiﬁcation result. In the table, “BP” indicates the neural
networks that are trained by backpropagation only. “GA” represents the hybrid
neuro-genetic algorithm and is classiﬁed into three types. “Original” represents
the version without any transformation in the genetic process. The other cases
contain transformation in the genetic framework. “Heuristic” indicates the ver-
sion with a heuristic transformation by [13]. “SOM” represents the version with
SOM-based transformation. The two values in each experiment show the mean
and the best recognition result, respectively, from 500 trials. The hybrid approach
showed improvement over the non-hybrid approach (BP) and the transforma-
tion approaches turned out to be useful. The SOM-based transformation showed
considerable improvement over the heuristic-based transformation.
6
Concluding Remarks
We devised a structural transformation algorithm for neuro-genetic hybrids to
eﬀectively reﬂect geographic correlations or relative contributions among neurons

1110
J.-H. Kim and B.-R. Moon
in the genetic search. The topological ordering property of SOM was used to
cluster neurons with high correlations. The proposed SOM-based transformation
dynamically alters the geographical shapes of ﬁtness landscapes; consequently,
the shapes of schemata are altered as well.
To the best of our knowledge, this is the ﬁrst usage of SOM in the context
of genetic search. We believe that there is room for further improvement in this
direction. Candidates of future studies include the enhancement of SOM-based
transformation and the design of other types of transformation.
Acknowledgments. This research was supported in part by KOSEF through
Statistical Research Center for Complex Systems at Seoul National University
and Brain Korea 21 Project. The RIACT at Seoul National University provided
research facilities for this study.
References
1. C. A. Anderson, K. F. Jones, and J. Ryan. A two-dimensional genetic algorithm
for the Ising problem. Complex Systems, 5:327–333, 1991.
2. J. Bagley. The Behavior of Adaptive Systems Which Employ Genetic and Corre-
lation Algorithms. PhD thesis, University of Michigan, Ann Arbor, MI, 1967.
3. T. N. Bui and B. R. Moon.
Hyperplane synthesis for genetic algorithms.
In
International Conference on Genetic Algorithms, pages 102–109, 1993.
4. T. N. Bui and B. R. Moon. On multi-dimensional encoding/crossover. In Interna-
tional Conference on Genetic Algorithms, pages 49–55, 1995.
5. T. N. Bui and B. R. Moon. Genetic algorithm and graph partitioning. IEEE Trans.
on Computers, 45(7):841–855, 1996.
6. T. N. Bui and B. R. Moon. GRCA: A hybrid genetic algorithm for circuit ratio-cut
partitioning. IEEE Transactions on Computer-Aided Design of Integrated Circuits
and Systems, 17(3):193–204, 1998.
7. J. P. Cohoon and W. Paris. Genetic placement. In IEEE International Conference
on Computer-Aided Design, pages 422–425, 1986.
8. J. L. Elman. Finding structure in time. Cognitive Science, 14:179–211, 1990.
9. A. Grauel and F. Berk. Mapping of dynamical systems by recurrent neural net-
works in an evolutionary algroithm approach. In European Congress on Intelligent
Techniques and Soft Computing, volume 1, pages 470–476, 1998.
10. S. Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall, 1999.
11. R. Jeﬀand V. B. Ciesielski. An evolutionary approach to training feed-forward
and recurrent neural networks. In International Conference on Knowledge-Based
Intelligent Electronics Systems, pages 596–602, 1998.
12. A. B. Kahng and B. R. Moon. Toward more powerful recombinations. In Interna-
tional Conference on Genetic Algorithms, pages 96–103, 1995.
13. J. H. Kim and B. R. Moon. Neuron reordering for better neuro-genetic hybrids.
In Genetic and Evolutionary Computation Conference, pages 407–414, New York,
9-13 July 2002. Morgan Kaufmann Publishers.
14. T. Kohonen, S. Kaski, K. Lagus, J. Saloj˝arvi, J. Honkela, V. Paatero, and
A. Saarela. Self organization of a massive document collection. IEEE Transactions
on Neural Networks, 11(3):574–585, 2000.

New Usage of SOM for Genetic Algorithms
1111
15. C. T. Lin and C. P. Jou. Controlling chaos by GA-based reinforcement learning
neural network. IEEE Transactions on Neural Networks, 10(4):846–869, 1999.
16. B. R. Moon and H. N. Kim. Eﬀective genetic encoding with a two-dimensional
embedding heuristic. International Journal of Knowledge-Based Intelligent Engi-
neering Systems, 3(2):113–120, 1999.
17. B. R. Moon, Y. S. Lee, and C. K. Kim. GEORG: VLSI circuit partitioner with a
new genetic algorithm framework. Journal of Intelligent Manufacturing, 9(5):401–
412, 1998.
18. V. Patridis, E. Paterakis, and A. Kehagias. A hybrid neural-genetic multimodel pa-
rameter estimation algorithm. IEEE Transactions on Neural Networks, 9(5):862–
876, 1998.
19. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal represen-
tations by error propagation. In D. E. Rumelhart and J. L. McCleland, editors,
Parallel Distributed Processing, volume 1, chapter 8. MIT Press, Cambridge, MA,
1986.

Problem-Independent Schema Synthesis for
Genetic Algorithms
Yong-Hyuk Kim, Yung-Keun Kwon, and Byung-Ro Moon
School of Computer Science & Engineering, Seoul National University
Shilim-dong, Kwanak-gu, Seoul, 151-742 Korea
{yhdfly, kwon, moon}@soar.snu.ac.kr
Abstract. As a preprocessing for genetic algorithms, static reorder-
ing helps genetic algorithms eﬀectively create and preserve high-quality
schemata, and consequently improves the performance of genetic algo-
rithms. In this paper, we propose a static reordering method indepen-
dent of problem-speciﬁc knowledge. One of the novel features of our
reordering method is that it is applicable to any problem with no infor-
mation about the problem. The proposed method constructs a weighted
complete graph from the gene distances calculated from solutions with
relatively high ﬁtnesses, transforms them into a gene-interaction graph,
and ﬁnds a gene rearrangement. Extensive experimental results showed
signiﬁcant improvement for a number of applications.
1
Introduction
By the schema theorem, Holland showed that highly ﬁt schemata of short deﬁn-
ing lengths and low orders have high probabilities of survival in the traditional
genetic framework [19]. High-quality schemata with the above features are called
building blocks. Building blocks are gene groups with high contribution to the
ﬁtnesses that have mutually strong interactions. The performance of a genetic
algorithm highly depends on the survival environment and reproducibility of
building blocks.
The survival probability of a gene group through crossovers is strongly af-
fected by the positions of genes in the chromosome. Schemata consisting of widely
scattered speciﬁc positions have poor survival probabilities through crossovers
due to their long deﬁning lengths. Thus, the strategy of locating genes signiﬁ-
cantly aﬀects the performance of genetic algorithms. Inversion is a genetic oper-
ator devised for changing the loci of genes dynamically [3]. The eﬀorts to exploit
the loci of genes dynamically are called linkage learning [18]. Messy genetic al-
gorithm is an example that implicitly pursues dynamic gene repositioning [16].
It has been observed that the performance of genetic algorithms on problems
with locus-based encoding can be improved by statically reordering the indices of
the genes. The technique of static reordering for genetic algorithms was ﬁrst sug-
gested in [6] [10], whose basic idea is to reassign the loci of genes in chromosomal
representation to help genetic algorithms eﬀectively preserve good schemata. A
good reordering also leads to better creation of high-quality schemata than in
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1112–1122, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Problem-Independent Schema Synthesis for Genetic Algorithms
1113
the original ordering. A number of studies on static reordering of gene positions
in locus-based encodings showed performance improvement [6] [8] [10] [29].
However, previous reorderings depend on the speciﬁc information of their ap-
plication [6] [8] [10]. Hence, a new heuristic has to be derived for the reordering of
each new problem. In this paper, we describe a static reordering method which
is free from problem-speciﬁc knowledge. The method requires locus-based en-
codings for chromosomal representation. We perform experiments on three rep-
resentative combinatorial optimization problems that are NP-hard [15]: graph
bisection, linear arrangement, and traveling salesman problem. Our experiments
showed notable improvement when compared against the cases without reorder-
ing. In this paper, we use rearrangement, reordering, and preprocessing inter-
changeably.
The remainder of this paper is organized as follows. In Section 2, we summa-
rize three testbed problems. The genetic framework that we used in this work
is described in Section 3. In Section 4, we describe the problem-independent
schema preprocessing for general purpose. We present experimental results in
Section 5. Finally, we make our conclusions in Section 6.
2
Preliminaries
2.1
Graph Bisection
Let G = (V, E) be an unweighted undirected graph, where V is the set of n
vertices and E is the set of e edges. A bisection {C1, C2} of the graph G satisﬁes
C1, C2 ⊂V , C1 ∪C2 = V , C1 ∩C2 = φ , and ||C1| −|C2|| ≤1. The cut size of
{C1, C2} is |{(v, w) ∈E : v ∈C1, w ∈C2}|. The graph bisection problem is the
problem of ﬁnding a bisection with the minimum cut size. The problem has been
extensively studied in the past [10] [25] [4] [23]. It is known to be NP-hard [15].
2.2
Linear Arrangement
Let G = (V, E) be an unweighted undirected graph. The linear arrangement
problem is the problem of ﬁnding a permutation σ : V →V of vertices with the
minimum value of 
(u,v)∈E |σ(u) −σ(v)|. There have been a number of studies
for the problem [1] [12] [37]. It is also NP-hard [15].
2.3
Traveling Salesman Problem (TSP)
Let G = (V, E) be a complete graph with weights on the edges. A Hamiltonian
cycle of G is a cycle that visits every vertex of the graph exactly once. The
traveling salesman problem (TSP) is the problem of ﬁnding a Hamiltonian cycle
with the minimum weight. TSP is well known to be NP-hard [15]. It has been
extensively studied in the past due to its wide applications as well as for its
complexity. Genetic algorithms have been applied to TSP with varying degrees
of success [24] [32] [21] [38].

1114
Y.-H. Kim, Y.-K. Kwon, and B.-R. Moon
Preprocess;
Create an initial population;
repeat {
choose parent1 and parent2 from population;
oﬀspring = crossover(parent1, parent2);
local-improvement(oﬀspring);
replacement(population, oﬀspring);
} until (stopping condition);
return the best solution;
Fig. 1. The framework of our hybrid genetic algorithm
3
A Hybrid Genetic Algorithm
A hybrid genetic algorithm is a genetic algorithm (GA) combined with a local
improvement heuristic. Some people call it a memetic genetic algorithm [34] [11]
[27] [28] [26] [5]. The general framework of hybrid steady-state genetic algorithm
is used in our GA as shown in Figure 1. In the following, we describe each part
of the GA that we used for this work.
– Locus-based encoding: Each solution in the population is represented by a
chromosome. A binary encoding is used for the graph bisection problem.
A gene has a value ‘0’ or ‘1’ depending on the side that the corresponding
vertex belongs to. We use a permutation encoding for the linear arrangement
problem. Each gene corresponds to a vertex in the graph and its value means
the position in the arrangement. We also use a permutation encoding for the
TSP. A gene corresponding a vertex v represents another vertex following
vertex v in the Hamiltonian cycle. These encodings, where each gene location
has an explicit meaning, are called locus-based encoding. It is necessary to
use the locus-based encoding since the preprocessing heuristic presented in
Section 4 is applicable only to locus-based encodings.
– Selection and crossover: To select two parents, we use a proportional selection
scheme where the probability for the best solution to be chosen is four times
higher than that for the worst solution. A crossover operator creates a new
oﬀspring by combining parts of parents. In the graph bisection problem, we
use ﬁve-point crossover. After the crossover, an oﬀspring may not satisfy
the balance. It selects a random point on the chromosome and changes the
required number of 1’s to 0’s (or 0’s to 1’s) from that point on. In the
permutation encoding, we use the partially matched crossover [17]. There is
no duplicated gene value in the oﬀspring and it need not be repaired in case of
the linear arrangement problem. However, since it may consist of more than
one mutually disconnected subcycle, it may not be a proper Hamiltonian
cycle in case of TSP. To resolve this problem, we used the repair algorithm
introduced in [8].
– Local improvement: Hybrid genetic algorithms have been considered natu-
ral in solving a diﬃcult problem to get desirable performance since genetic

Problem-Independent Schema Synthesis for Genetic Algorithms
1115
1. Generate M solutions with relatively high ﬁtness;
2. Compute distance for each gene pair;
3. Make a gene-interaction graph;
4. Find a gene arrangement;
4
5
1
5
4
2
3
1
2
3
5
1
3
4
2
3
1
5
4
2
3
2
4
5
1
Fig. 2. The structure of problem-independent schema preprocessing
algorithms are not so good at ﬁne tuning near local optima. In this study,
we use one of the most basic local improvement heuristic, 2-Opt, which
has 2-exchange as its neighbor structure. It is applied to the oﬀspring after
crossover in the GA.
– Replacement and stop condition: After generating an oﬀspring, the GA re-
places the worse of the two parents with the oﬀspring. It is called preselection
replacement. It stops after a ﬁxed number of generations.
4
Problem-Independent Gene Rearrangement
As mentioned in Section 3, we use locus-based encodings for GA and rearrange
the genes. Figure 2 shows the framework of the proposed schema preprocessing.
It does not depend on any problem-speciﬁc knowledge.
– Generating high quality solutions: First, it generates M solutions with rel-
atively high ﬁtness. In this study, we generated 100 solutions using 2-Opt
heuristics.
– Computing the distance for each gene pair: From the generated solution set,
it computes the gene distance between each pair of genes according to its
encoding type. Figure 3 describes how to measure the distance D(gi, gj)
between two genes gi and gj. In the ﬁgure, fl(gi) means the value of gene
gi in the lth solution. As explained in Section 3, binary encoding is used
for graph bipartition problem. Sequential permutation encoding and cyclic
permutation encoding are used for linear arrangement problem and TSP,
respectively. Thus, we get a weighted complete graph with vertices and edges
corresponding to genes and gene distances, respectively.
– Making a gene-interaction graph: It transforms the obtained weighted graph
into unweighted sparse graph called gene-interaction graph. We assume that
the edge weights in the weighted graph has the Gaussian distribution. To get
the gene-interaction graph, it chooses only the heavy-weighted edges with
95% conﬁdence level.

1116
Y.-H. Kim, Y.-K. Kwon, and B.-R. Moon
Binary encoding
D(gi, gj) = 1
M
M

l=1
I(fl(gi) ̸= fl(gj))
Sequential permutation encoding
D(gi, gj) = 1
M
M

l=1
|fl(gi) −fl(gj)|
Cyclic permutation encoding
D(gi, gj) = 1
M
M

l=1
argmink(gi = f k
l (gj) or gj = f k
l (gi))
Fig. 3. Gene-distance measure in locus-based encoding
Fig. 4. Reordering in graph bisection: instance U500.10
– Finding a gene arrangement: From the gene-interaction graph, it performs
gene rearrangement. Given the set of genes {g1, g2, . . . , gn}, a gene rear-
rangement {gσ(1), gσ(2), . . . , gσ(n)} is represented by a bijective map σ :
{1, 2, . . . , n} →{1, 2, . . . , n}. Gene vi is the jth gene in the gene rearrange-
ment if σ(j) = i. In general, the objective of gene rearrangement is to pre-
serve the clustering structure of the gene-interaction graph. In this paper,
we use three general graph-search methods: BFS, DFS, and Max-Adjacency
[2]. BFS and DFS reordering performs a breadth ﬁrst search and a depth
ﬁrst search, respectively, on the input graph starting at a random vertex.
The order in which the vertices are visited by the BFS or DFS is used to
reorder the vertices. In Max-Adjacency reordering [31], starting at a random
vertex, the vertex with the most edges incident to previously ordered vertices
is iteratively added to the ordering.
5
Experimental Results
5.1
Graph Bisection
We tested our approach on a total of 21 graphs which consist of three groups of
graphs: random graphs (Gn.d), random geometric graphs (Un.d), and caterpillar
graphs (cat.n and rcat.n). They have been used in a number of other studies

Problem-Independent Schema Synthesis for Genetic Algorithms
1117
Table 1. Experimental results in graph bisection problem
Graph
Basic ordering BFS reordering DFS reordering Max-Adj reordering
G500.2.5
52.48
52.38
51.50
51.94
G500.05
220.96
220.58
221.12
220.84
G500.10
630.32
629.88
630.34
629.64
G500.20
1751.48
1749.12
1750.14
1748.84
G1000.2.5
101.08
99.96
101.64
100.02
G1000.05
455.28
454.72
456.20
454.90
G1000.10
1374.04
1374.62
1372.88
1372.18
U500.05
8.66
5.88
4.66
4.72
U500.10
34.24
28.06
26.74
26.38
U500.20
178.28
178.12
178.18
178.36
U500.40
412.00
412.00
412.00
412.00
U1000.05
26.18
15.00
16.24
12.80
U1000.10
72.26
56.82
44.98
48.44
U1000.20
239.50
231.62
228.16
230.32
U1000.40
737.00
737.00
737.00
737.00
cat.352
3.56
1.04
1.28
1.76
cat.702
7.64
8.60
4.36
5.60
cat.1052
12.32
15.84
9.76
8.40
rcat.134
1.00
1.00
1.00
1.00
rcat.554
2.48
1.92
1.52
1.32
rcat.994
3.64
2.44
2.48
2.68
Average over 100 runs.
[20] [7] [10] [4]. Table 1 shows the experimental results. In the table, “BFS,”
“DFS,” and “Max-Adj” represent the gene preprocessing methods. We should
note again that this preprocessing is performed on the gene-interaction graphs
which are independent of problems diﬀerently from previous static reordering
methods such as [6] and [10]. We arrange the genes randomly in “Basic order-
ing.” The GAs have the same framework except the preprocessing. Preprocessed
GAs signiﬁcantly outperformed the GA in which genes are arranged randomly.
In particular, the preprocessing showed larger performance improvement on ge-
ometric graphs and caterpillar graphs. We should also note that we can get
improved solutions by using a stronger local optimization heuristic than 2-Opt.
Here, we ﬁxed the local optimization with 2-Opt since our major concern is the
eﬀect of the suggested reordering method.
To get visual insight into the reordering, we drew a problem instance and the
preprocessed order in Figure 4. Figure 4(a) shows the original graph. Figure 4(b)
and 4(c) were acquired by drawing segments between all consecutive vertices in
each ordering. Of course, randomly ordered genes do not reﬂect any relation
between most pair of vertices. We can observe that BFS helps highly related
genes to stay close in the chromosome.

1118
Y.-H. Kim, Y.-K. Kwon, and B.-R. Moon
Table 2. Experimental results in linear arrangement problem
Graph
Basic ordering BFS reordering DFS reordering Max-Adj reordering
U500.05
162852.40
158473.18
159004.50
158562.42
U500.10
317022.28
306477.68
308955.18
307840.62
U1000.05
658743.66
643714.92
647381.98
643003.50
U1000.10
1355660.12
1333089.58
1339380.60
1335310.82
Average over 100 runs.
Table 3. Experimental results in TSP
Instance Basic ordering BFS reordering DFS reordering Max-Adj reordering
lin318
42499.67
42451.82
42404.98
42406.36
pcb442
51886.53
51476.48
51510.38
51489.06
att532
28629.40
28173.80
28209.30
28199.80
rat783
9304.47
9104.72
9111.20
9095.08
Average over 100 runs.
Fig. 5. Reordering in linear arrangement: instance U500.05
5.2
Linear Arrangement
To test our approach on the linear arrangement problem, we used sparse geo-
metric graphs out of the graphs used in Table 1. Table 2 shows the experimental
results. The three methods outperformed “Basic ordering” in all instances. In
particular, “BFS” showed the best performance on the average. We also drew in
Figure 5 the order of genes after a reordering on the linear arrangement problem.
We can also observe that highly related genes stay close in the chromosome.
5.3
Traveling Salesman Problem
Table 3 shows the experimental result on four instances of TSPLIB1. The re-
sults are consistent with the two previous experiments. GAs preprocessed by
BFS, DFS, and Max-Adj have more chances to ﬁnd good solutions than the
basic ordering. Figure 6 shows the drawing of the gene orders on an instance
att532. We can observe that closely located cities tend to locate closely in the
1 http://www.iwr.uniheidelberg.de/iwr/comopt/soft/TSPLIB95/TSPLIB.html

Problem-Independent Schema Synthesis for Genetic Algorithms
1119
Fig. 6. Reordering in TSP: instance att532
0
1000
2000
3000
4000
5000
6000
7000
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
(a) Tour found by Fig.6(b)
0
1000
2000
3000
4000
5000
6000
7000
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
(b) Tour found by Fig.6(c)
Fig. 7. TSP solutions: instance att532
chromosome with the BFS reordering. We can also observe the eﬀect of reorder-
ing by visualizing the TSP tour. Figure 7 shows representative tours of TSP by
GAs with the orderings of Figure 6. The GAs found considerably diﬀerent tours
according to their orderings.
6
Conclusions
In this paper, we proposed a static reordering framework of genes in locus-
based encodings. It showed consistent performance improvement over genetic
algorithms without reordering. One may be able to devise a better reordering,
as a result of exploiting problem-speciﬁc knowledge, as far as each problem is
concerned. The most notable feature of the suggested method is that it does
not need any problem-speciﬁc information during the reordering process. When
a new problem is given for GA, we do not have to devise a new preprocessing
heuristic. The only thing we need is a measure of gene interaction for each
problem. However, it may not be a big burden since most problem encodings
can be classiﬁed into a number of representative encodings. Moreover, there exist
useful studies on gene interactions [13] [35] [36] [14] [30] [33].
We considered only the linear encoding in this study. Although it is tra-
ditional and the most popular encoding, multi-dimensional encodings are also
becoming common in the GA community [9] [22]. The proposed reordering frame-
work has a limitation that it can be just applied to the linear encoding. Extending
the reordering to multi-dimensional encodings seems to be a topic worth trying.

1120
Y.-H. Kim, Y.-K. Kwon, and B.-R. Moon
Acknowledgment. This work was partly supported by Optus Inc. and Brain
Korea 21 Project. The RIACT at Seoul National University provided research
facilities for this study.
References
1. D. Adolphson and T. Hu.
Optimal linear ordering.
SIAM J. Appl. Math.,
25(3):403–423, 1973.
2. C. Alpert and A. B. Kahng. A general framework for vertex orderings, with applica-
tions to netlist clustering. In IEEE/ACM International Conference on Computer-
Aided Design, pages 63–67, 1994.
3. J. Bagley. The Behavior of Adaptive Systems Which Employ Genetic and Corre-
lation Algorithms. PhD thesis, University of Michigan, Ann Arbor, MI, 1967.
4. R. Battiti and A. Bertossi. Greedy, prohibition, and reactive heuristics for graph
partitioning. IEEE Trans. on Computers, 48(4):361–385, 1999.
5. M.J. Blesa, P. Moscato, and F. Xhafa. A Memetic Algorithm for the Minimum
Weighted k-Cardinality Tree Subgraph Problem. In 4th Metaheuristics Interna-
tional Conference, volume 1, pages 85–90, 2001.
6. T. N. Bui and B. R. Moon. Hyperplane synthesis for genetic algorithms. In Fifth
International Conference on Genetic Algorithms, pages 102–109, July 1993.
7. T. N. Bui and B. R. Moon. A genetic algorithm for a special class of the quadratic
assignment problem. The Quadratic Assignment and Related problems, DIMACS
Series in Discrete Mathematics and Theoretical Computer Science, 16:99–116,
1994.
8. T. N. Bui and B. R. Moon. A new genetic approach for the traveling salesman
problem.
In IEEE Conference on Evolutionary Computation, pages 7–12, June
1994.
9. T. N. Bui and B. R. Moon. On multi-dimensional encoding/crossover. In Sixth
International Conference on genetic Algorithms, pages 49–56, 1995.
10. T. N. Bui and B. R. Moon. Genetic algorithm and graph partitioning. IEEE Trans.
on Computers, 45(7):841–855, 1996.
11. E. K. Burke, J. P. Newall, and R. F. Weare. A memetic algorithm for university
exam timetabling.
In 1st International Conference on the Practice and Theory
of Automated Timetabling (ICPTAT’95, Napier University, Edinburgh, UK, 30th
Aug – 1st Sept 1995), pages 496–503, 1995.
12. C. Cheng. Linear placement algorithms and applications to VLSI design. Networks,
17:439–464, 1987.
13. Y. Davidor. Epistasis variance: A viewpoint on ga-hardness. In Foundations of
Genetic Algorithms 3, pages 23–35. Morgan Kaufmann, 1991.
14. Cyril Fonlupt, Denis Robilliard, and Philippe Preux. A bit-wise epistasis measure
for binary search spaces. Lecture Notes in Computer Science, 1498:47ﬀ., 1998.
15. M. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory
of NP-Completeness. Freeman, San Francisco, 1979.
16. D. Goldberg, B. Korb, and K. Deb. Messy genetic algorithms: Motivation, analysis,
and ﬁrst results. Complex System, 3:493–530, 1989.
17. D. Goldberg and R. Lingle.
Alleles, loci, and the traveling salesman problem.
In First International Conference on Genetic Algorithms and Their Applications,
pages 154–159, 1985.

Problem-Independent Schema Synthesis for Genetic Algorithms
1121
18. G. R. Harik and D. E. Goldberg. Learning linkage. In Foundations of Genetic
Algorithms 4, pages 247–262. 1996.
19. J. Holland. Adaptation in Natural and Artiﬁcial Systems. University of Michigan
Press, 1975.
20. D. S. Johnson, C. Aragon, L. McGeoch, and C. Schevon. Optimization by simulated
annealing: An experimental evaluation, Part 1, graph partitioning.
Operations
Research, 37:865–892, 1989.
21. S. Jung and B. R. Moon.
Toward minimal restriction of genetic encoding and
crossovers for the 2D Euclidean TSP. IEEE Transactions on Evolutionary Com-
putation, 6(6), 2002.
22. A. B. Kahng and B. R. Moon. Toward more powerful recombinations. In Sixth
International Conference on genetic Algorithms, pages 96–103, 1995.
23. Y. H. Kim and B. R. Moon. A hybrid genetic search for graph partitioning based on
lock gain. In Genetic and Evolutionary Computation Conference, pages 167–174,
2000.
24. P. Merz and B. Freisleben. Genetic local search for the TSP: New results. In IEEE
Conference on Evolutionary Computation, pages 159–164, 1997.
25. P. Merz and B. Freisleben. Memetic algorithms and the ﬁtness landscape of the
graph bi-partitioning problem.
In Proceedings of the 5th International Confer-
ence on Parallel Problem Solving From Nature, 1998. Lecture Notes in Computer
Science, 1498:765–774, Springer-Verlag.
26. P. Merz and B. Freisleben. Fitness landscape analysis and memetic algorithms for
the quadratic assignment problem. IEEE-EC, 4(4):337, November 2000.
27. Peter Merz and Bernd Freisleben.
A comparison of memetic algorithms, tabu
search, and ant colonies for the quadratic assignment problem.
In Proceedings
of the Congress on Evolutionary Computation, volume 3, pages 2063–2070. IEEE
Press, 6-9 1999.
28. Peter Merz and Bernd Freisleben. Fitness landscapes and memetic algorithm de-
sign.
In David Corne, Marco Dorigo, and Fred Glover, editors, New Ideas in
Optimization, pages 245–260. McGraw-Hill, 1999.
29. B. R. Moon and C. K. Kim. A two-dimensional embedding of graphs for genetic
algorithms. In International Conference on Genetic Algorithms, pages 204–211,
1997.
30. M. Munetomo and D. Goldberg. Identifying linkage by nonlinearity check, 1998.
31. H. Nagamochi and T. Ibaraki. Computing edge-connectivity in multigraphs and
capacitated graphs. Siam J. of Disc. Math, 5(1):54–66, Feb 1992.
32. Y. Nagata and S. Kobayashi.
Edge assembly crossover: A high-power genetic
algorithm for the traveling salesman problem. In 7th International Conference on
Genetic Algorithms, pages 450–457, 1997.
33. Martin Pelikan, David Goldberg, and Fernando Lobo. A survey of optimization by
building and using probabilistic model. Technical Report 99018, IlliGAL, Septem-
ber 1999.
34. Nicholas J. Radcliﬀe and Patrick D. Surry. Formal memetic algorithms. In Evolu-
tionary Computing, AISB Workshop, pages 1–16, 1994.
35. Colin Reeves and Christine Wright. An experimental design perspective on ge-
netic algorithms. In Foundations of Genetic Algorithms 3, pages 7–22. Morgan
Kaufmann, 1995.
36. Colin Reeves and Christine C. Wright. Epistasis in genetic algorithms: An exper-
imental design perspective. In Proceedings of the Sixth International Conference
on Genetic Algorithms, pages 217–224. Morgan Kaufmann, 1995.

1122
Y.-H. Kim, Y.-K. Kwon, and B.-R. Moon
37. Y. Saab and C. Chen. An eﬀective solution to the linear placement problem. VLSI
Design Journal, 2(2):117–129, 1994.
38. D. I. Seo and B. R. Moon.
Voronoi quantized crossover for traveling salesman
problem. In Genetic and Evolutionary Computation Conference, pages 544–552,
2002.

Investigation of the Fitness Landscapes and
Multi-parent Crossover for Graph Bipartitioning
Yong-Hyuk Kim and Byung-Ro Moon
School of Computer Science & Engineering, Seoul National University
Shillim-dong, Kwanak-gu, Seoul, 151-742 Korea
{yhdfly, moon}@soar.snu.ac.kr
Abstract. An empirical study is performed on the local-optimum space
of graph bipartitioning. We examine some statistical features of the ﬁt-
ness landscape. They include the cost-distance correlations and the prop-
erties around the central area of local optima. The study revealed some
new notable results about the properties of the ﬁtness landscape; e.g.,
the central area yielded fairly good quality in the local-optimum space.
We performed an experiment on a spectrum of diﬀerent exploitation
strengths of the central areas. From the results, it seems attractive to
exploit the central area, but excessive or insuﬃcient exploitation is not
desirable.
1
Introduction
An NP-hard problem such as graph partitioning problem or traveling salesman
problem (TSP) has a ﬁnite solution set and each solution has a cost. Although
ﬁnite, the problem space is intractably large even for a small but nontrivial prob-
lem. It is almost impossible to ﬁnd an optimal solution for those problems by
exhaustive or simple search methods. Thus, in case of NP-hard problems, heuris-
tic algorithms are being used. Heuristic algorithms provide reasonable solutions
in acceptable computing time but have no performance guarantee.
Consider a combinatorial problem C = (Ω, f) and a local optimization algo-
rithm Lc : Ω→Ω, where Ωis the solution space and f is the cost function. If a
solution s∗∈Ωis in Lc(Ω), then s∗is called a local optimum with respect to the
algorithm Lc. For each local optimum s∗∈Lc(Ω), we deﬁne the neighborhood
set of s∗to be a set N(s∗) ⊂Ωsuch that, for every s in N(s∗), Lc(s) is equal to
s∗. That is, s∗is the attractor of the solutions in N(s∗). We examine the space
Lc(Ω) and hope to get some insight into the problem space. This is an alterna-
tive for examining the intractably huge whole problem space. Good insight into
the problem space can provide a motivation for a good search algorithm.
A number of studies about the ruggedness and the properties of problem
search spaces have been conducted. Sorkin [21] deﬁned the fractalness of a solu-
tion space and proposed that simulated annealing [18] is eﬃcient when the space
is fractal. Jones and Forrest [14] introduced the ﬁtness-distance correlation as a
measure of search diﬃculty. Manderick et al. [19] measured the ruggedness of a
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1123–1135, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

1124
Y.-H. Kim and B.-R. Moon
problem space by autocorrelation function and correlation length obtained from
a time series of solutions. Weinberger [23] conjectured that, if all points on a
ﬁtness landscape are correlated relatively highly, the landscape is bowl shaped.
Boese et al. [3] suggested that, through measuring cost-distance correlations for
the TSP and the graph bisection problem, the cost surfaces are globally convex;
from these results they proposed an adaptive multi-start heuristic and showed
that the heuristic is eﬃcient [3]. Kauﬀman [15] proposed the NK-landscape model
that can control the ruggedness of a problem space.
In this paper, we present a number of experiments to analyze problem spaces
more elaborately. We examine the cost-distance correlations and the proper-
ties around the central areas of local optima. Based on the empirical study,
we perform an experiment on a spectrum of diﬀerent exploitation strengths of
the central areas under a genetic algorithm (GA) framework. We perform these
experiments on the graph bipartitioning problem.
The remainder of this paper is organized as follows. In Section 2, we summa-
rize the graph bipartitioning problem, the Fiduccia-Mattheyses algorithm (FM)
which is used as a major local optimization algorithm in this paper, and test
graphs. We perform various experiments and analyze ﬁtness landscapes in Sec-
tion 3. In Section 4, we propose a multi-parent crossover for graph bipartitioning.
Finally, we make our conclusions in Section 5.
2
Preliminaries
2.1
Graph Bipartitioning
Let G = (V, E) be an unweighted undirected graph, where V is the set of vertices
and E is the set of edges. A bipartition (A, B) consists of two subsets A and B of
V such that A∪B = V and A∩B = φ. The cut size of a bipartition is deﬁned to
be the number of edges whose endpoints are in diﬀerent subsets of the bipartition.
The bipartitioning problem is the problem of ﬁnding a bipartition with minimum
cut size. If the diﬀerence of cardinalities between two subsets is at most one, the
problem is called graph bisection problem and if the diﬀerence does not exceed the
ﬁxed ratio of |V |, the problem is called roughly balanced bipartitioning problem.
Without balance criterion, we can ﬁnd the optimal solution in polynomial time
by maxﬂow-mincut algorithm [10]. In a roughly balanced bipartitioning problem,
10% of skewness is usually allowed [20]. Since it is NP-hard for general graphs
[11], heuristic algorithms are used practically. These include FM algorithm [9], a
representative linear time heuristic, PROP [5] based on probabilistic notion, LG
[17] based on lock gain, etc. In this paper, we consider only roughly balanced
bipartitioning problem allowing 10% of skewness.
2.2
Fiduccia-Mattheyses Algorithm (FM)
Fiduccia and Mattheyses [9] introduced a heuristic for roughly balanced bipar-
titioning problem. The FM algorithm as well as the Kernighan-Lin algorithm

Investigation of the Fitness Landscapes and Multi-parent Crossover
1125
do {
Compute gain gv for each v ∈V ;
Make gain lists of gvs;
Q = φ;
for i = 1 to | V | −1 {
Choose vi ∈V −Q such that gvi is maximal and
the move of vi does not violate the balance criterion;
Q = Q ∪{vi};
for each v ∈V −Q adjacent to vi
Update its gain gv and adjust the gain list;
}
Choose k ∈{1, . . . , | V | −1} that maximizes k
i=1 gvi;
Move all the vertices in the subset {v1, ..., vk} to their opposite sides;
} until (there is no improvement)
Fig. 1. The Fiduccia-Mattheyses algorithm (FM)
(KL) [16] is a traditional iterative improvement algorithm. The algorithm im-
proves on an initial solution by single-node moves. The main diﬀerence between
KL and FM lies in that a new partition in FM is derived by moving a single
vertex, instead of KL’s pair swap. The structure of the FM algorithm is given in
Figure 1. FM proceeds in a series of passes. In each pass, all vertices are moved
in chain and then the best bipartition during the pass is returned as a new so-
lution. The algorithm terminates when one or a few passes fail to ﬁnd a better
solution. With an eﬃcient data structure, each pass of FM runs in Θ(|E|) time.
2.3
Test Beds
We tested on a total of 17 graphs which consist of two groups of graphs. They
are composed of 17 graphs from [13] (9 random graphs and 8 geometric graphs).
The two classes were used in a number of other studies [20] [4] [1] [17]. The
classes are brieﬂy described below.
1. Gn.d: A random graph on n vertices, where an edge is placed between any two
vertices with probability p independent of all other edges. The probability p is
chosen so that the expected vertex degree, p(n −1), is d.
2. Un.d: A random geometric graph on n vertices that lie in the unit square and
whose coordinates are chosen uniformly from the unit interval. There is an edge
between two vertices if their Euclidean distance is t or less, where d = nπt2 is the
expected vertex degree.
3
Investigation of the Problem Space
In this section, we ﬁrst extend the experimentation of Boese et al. [3] to ex-
amine the local-optimum space. We denote by local-optimum space the space
consisting of all local optima with respect to a local optimization algorithm.
Next, we examine the area around the “central point” of local optima. In our
experiments, we use a suﬃciently large number of local optima. We do not care

1126
Y.-H. Kim and B.-R. Moon
about solutions other than local optima. The local optimizer in our experiments
is the FM algorithm.
In the graph bipartitioning problem for a graph G = (V, E), each solution
(A, B) is represented by a |V |-bits code. Each bit corresponds to a vertex in
the graph. A bit has value zero if the vertex is in the set A, and has value
one otherwise. In this encoding, a vertex move in the FM algorithm changes
the solution by one bit. Thus, it is natural to deﬁne the distance between two
solutions by the Hamming distance. However, if the Hamming distance between
two solutions is |V |, they are symmetric and equal. We hence deﬁne the distance
between two solutions as follows.
Deﬁnition 1 Let the universal set U be {0, 1}|V |. For a, b ∈U, we deﬁne the
distance between a and b as follows:1
d(a, b) = min(H(a, b), |V | −H(a, b))
where H is the Hamming distance.
By the deﬁnition, 0 ≤d(a, b) ≤⌊|V | /2⌋while 0 ≤H(a, b) ≤|V |.
3.1
Cost-Distance Correlation
Given a set of local minima, Boese et al. [3] plotted, for each local minimum,
i) the relationship between the cost and the average distance from all the other
local minima, and ii) the relationship between the cost and the distance to the
best local minimum. They performed experiments for the graph bisection and
the traveling salesman problem, and showed that both problems have strong
positive correlations for both i) and ii) in the above. This fact hints that the best
local optimum is located near the center of the local-optimum space. From their
experiments, they conjectured that cost surfaces of both problems are globally
convex. In this subsection, we repeat their experiments for other graphs and
extend their study to get more insight.
The solution space for the experiment is selected as follows. First, we choose
thousands of random solutions and obtain the corresponding set of local optima
by locally optimizing them. Next, we remove the duplicated solutions in the set
if any. Figure 2 shows the plotting results for the graph U500.10. It is consistent
with Boese et al.’s results with strong cost-distance correlation. More statistics
for a number of graphs are given in Table 1. The meaning of each item in the ta-
ble is as follows. “Population size” means the number of local optima we used for
each graph. “Best cut” is the cost of the best local optimum. “Average cut” is the
average cost of the local optima. “Cost-distance correlation” is the correlation
1 Given an element a ∈U, there is only one element such that it is diﬀerent from a
and the distance d to a is zero. If the distance between two elements is equal to zero,
we deﬁne them to be in relation R. Then, the relation R is an equivalence relation.
Suppose Q is the quotient set of U by relation R, it is easily veriﬁed that (Q, d) is a
metric space.

Investigation of the Fitness Landscapes and Multi-parent Crossover
1127
20
40
60
80
100
120
140
160
180
200
170
175
180
185
190
195
200
205
210
215
220
Cut Size
Avg Distance from other local minima
20
40
60
80
100
120
140
160
180
200
0
50
100
150
200
250
Cut Size
Distance to the best local minimum
Fig. 2. Relationship between cost and distance: U500.10 (see Table 1)
Table 1. The results for each graph
Items
G250.10 G500.2.5 G1000.2.5 U500.05 U500.10 U1000.05
Population size
9877
10000
10000
10000
9302
10000
Best cut
352
52
103
5
24
16
Average cut
367.65
64.58
128.17
35.62
83.58
70.76
Cost-distance correlation
0.77
0.78
0.83
0.89
0.91
0.88
Central point cut (CP)
380
60
118
5
24
17
CP + FM
352
51
99
5
24
16
Average distance
102.94
217.11
453.44
215.63
192.83
448.09
Population size : the number of local optima
Best cut : the minimum cost
Average cut : the average cost
Cost-distance correlation : correlation coeﬃcient between cost and average distance
from each local optimum to others
Central point cut (CP) : the cost of the approximate central point in solution space
CP + local opt : the cost after local optimization on the approximate central point
Average distance : the average value of distances between a pair of local optima
coeﬃcient between the costs of local optima and the average distances from the
other local optima. “Central point cut (CP)” is the cost of the approximate cen-
tral point of the local-optimum space (see Section 3.2 for the approximate central
point). “CP + local opt” is the cost after local optimization on the approximate
central point. Finally, “Average distance” means the average distance between a
pair of local optima. Overall, each graph showed strong positive correlation. De-
pending on graphs, correlation coeﬃcients were a bit diﬀerent. Geometric graphs
showed larger correlation coeﬃcients than random graphs. In the statistical data
of Table 1, each population was obtained from 10,000 random initial solutions.
Among the six graphs, four graphs had no duplications and the other two graphs
had 123 and 698 duplications, respectively. It is surprising that there were no
duplications in the ﬁrst 10,000 attractors for four of them. It seems to suggest
that the number of all possible local optima with respect to FM is immeasurably
large.
Figure 3, Table 2, and Table 3 compare the data with diﬀerent local opti-
mizers. A greedy local optimizer which moves only vertices with positive gain
was named GREEDY. Its principle is the same as that of the steepest descent
algorithm in the diﬀerentiable cases. NONE means a set of random solutions
without any local optimization. From the cut sizes in Table 2 and Table 3, FM
is clearly stronger than the GREEDY algorithm. The stronger the local opti-

1128
Y.-H. Kim and B.-R. Moon
0
10
20
30
40
50
60
70
80
208
210
212
214
216
218
220
222
224
Cut Size
Avg Distance from other local minima
(a) FM
30
40
50
60
70
80
90
100
110
218
219
220
221
222
223
224
225
226
227
Cut Size
Avg Distance from other local minima
(b) GREEDY
560
580
600
620
640
660
680
700
240.4
240.6
240.8
241
241.2
241.4
241.6
Cut Size
Avg Distance from other solutions   
(c) NONE
Fig. 3. Relationship between cost and distance with diﬀerent local optimizer in the
graph U500.05 (see Table 3)
Table 2. The data comparison with diﬀerent local optimizer in the graph G500.10
Local opt
FM
GREEDY NONE
Population size
2000
2000
2000
Best cut
623
666
1101
Average cut
648.60
706.26
1178.00
Cost-distance correlation
0.77
0.81
−0.02
Central point cut (CP)
659
670
1138
CP + local opt
623
643
−
Average distance
218.58
229.71
241.09
Table 3. The data comparison with diﬀerent local optimizer in the graph U500.05
Local opt
FM
GREEDY NONE
Population size
2000
2000
2000
Best cut
7
34
562
Average cut
35.86
65.16
640.89
Cost-distance correlation
0.88
0.79
−0.02
Central point cut (CP)
5
33
581
CP + local opt
5
30
−
Average distance
215.71
222.58
241.08
mizer, the smaller the average distance between two local optima and the more
sharing among local optima. However, from Tables 1–3, it is surprising that,
diﬀerently from our expectation, the average distance between two arbitrary
local optima is nearly 80% ∼90% of the possible maximum distance ⌊|V |/2⌋.
This is an evidence of the huge diversity of local optima. In Figure 3, a stronger
local optimization shows stronger cost-distance correlation. Since the average
distances in graphs are various, these values may have some potential to be used
as measures of the problem diﬃculty with respect to a local optimizer.2
3.2
Approximate Central Point
The results of Boese et al. [3] for the TSP and the graph bisection problem sug-
gest that the best solution is located near the center of the local-optimum space.
As a result of this, given a subspace of local optima for a problem, the “central
2 This is not a simple issue, though.

Investigation of the Fitness Landscapes and Multi-parent Crossover
1129
point”3 of the subspace may be near the optimal solution. Hence, computing
the “central point” not only supports the results of Boese et al. but may also be
helpful for obtaining the optimal solution.
Given a subspace Ω′ of the whole solution space in the graph bipartitioning
problem, the “approximate central point”4 is computed as follows. Let one of
the two encodings of the best solution in Ω′ be pbest. First, since each solution
has a pair of encodings, we make a set SΩ′ that contains only one encoding e
for each solution in Ω′ so that the Hamming distance between e and pbest is
not greater than ⌊|V | /2⌋. Next, for each position, count the number of 0’s and
that of 1’s for all elements of SΩ′. Make the approximate central point c so that
each position of c has the more-frequently-appeared bit. Then, the approximate
central point c is closer to the center5 of Ω′ than pbest.6 That is, we have the
following proposition.
Proposition 1 ∀pbest ∈SΩ′, let SΩ′ = {s1, s2, . . . , sn}. Then,
n

i=1
d(pbest, si) ≥
n

i=1
d(c, si).
Proof: Let Bj(x) be the jth value of x.
n

i=1
d(pbest, si) =
n

i=1
H(pbest, si)
=
n

i=1
|V |

j=1
|Bj(pbest) −Bj(si)|
=
|V |

j=1
n

i=1
|Bj(pbest) −Bj(si)|
=
|V |

j=1
|{s ∈SΩ′ : Bj(s) ̸= Bj(pbest)}|
3 We deﬁne “central point” to be the nearest solution to the center of local-optimum
space.
4 In this problem, it is not easy to ﬁnd the exact central point by a simple computation.
Each solution has two diﬀerent encodings. In order to get the distance to other
solution, we select one to which the Hamming distance is smaller than the other.
The more the solutions, the more complex the whole phase about which encoding is
used to calculate the distance.
5 Here, the center of Ω′ is deﬁned to be the point that has the minimum average
distance from the other solutions in Ω′.
6 Since the approximate central point obtained in this way can violate balance crite-
rion, adjustment is required. Although not mentioned, the experimental data showed
that most of adjusted approximate central points were closer to the center of Ω′ than
pbest.

1130
Y.-H. Kim and B.-R. Moon
≥
|V |

j=1
|{s ∈SΩ′ : Bj(s) ̸= Bj(c)}|
=
n

i=1
H(c, si)
≥
n

i=1
d(c, si).
Q.E.D.
Although the approximate central points are calculated through a simple
computation, it turned out that the costs of the approximate central points
are quite attractive (see Tables 1–3). It is amazing that the cut size of the
approximate central point without any ﬁne-tuning was sometimes equal to or
better than that of the best solution (see the cases of U500.05 and U500.10 in
Table 1). In order to check the local optimum near the center, we applied local
optimization to the approximate central point. The results are in the row “CP
+ local opt” of Tables 1–3. In all of the ten cases, the costs of the local optima
near the approximate central points were at least as good as those of the best
solutions; surprisingly enough, they were better than those of the best solutions
in ﬁve cases of them. This shows the attractiveness of the central area of the
local-optimum space, and provides a motivation for intensive search around the
central area.
4
Exploiting Approximate Central Points
We observed in Section 3.2 that the approximate central points obtained by
simple computation are quite attractive. In this section, we propose a pseudo-
GA that exploits the areas around the approximate central points. Based on the
GA, we perform an experiment on how strong exploitation of the central area is
desirable.
4.1
A Pseudo-GA That Exploits the Central Areas
Multi-parent crossover is a generalization of the traditional two parent recombi-
nation. It was ﬁrst introduced by Eiben et al. [8] and has been extensively studied
in the past [7] [6] [22]. But, it is not adequate for problems with multiple rep-
resentations for a solution like the graph partitioning problem. Unlike previous
works, our multi-parent crossover is designed based on the statistical features of
problem spaces. The oﬀspring of our multi-parent crossover is exactly the approx-
imate central point of the solutions in the population. Formally, the process of
our multi-parent crossover is as follows. Consider the distance measure d deﬁned
in Section 3, the parent set P, and a parent pk in P.7 For each parent a ∈P, if
7 Assume that the distance d between any two elements in P is larger than zero.

Investigation of the Fitness Landscapes and Multi-parent Crossover
1131
H(pk, a) > ⌊|V |/2⌋, make a transition that interchanges 0 and 1 at every gene po-
sition of a. Let the new set resulted from the transitions be P ′ = {p1, p2, . . . , pn}.
Then, for each i = 1, 2, . . . , n (pi ̸= pk), 0 < H(pk, pi) ≤⌊|V |/2⌋. Now, generate
an oﬀspring c such that for each j = 1, 2, . . . , |V |
Bj(c) =

1,
if |{p ∈P ′ : Bj(p) = 1}| > ⌊n/2⌋
0,
otherwise
where Bj(x) is the jth gene value of x.
Figure 4 shows the template of pseudo-GA that is designed to exploit the
central areas. It is a type of hybrid steady-state genetic algorithm using the
multi-parent crossover described above.
– Initialization and genetic operators: The GA ﬁrst creates K local optima at
random. We set the population size K to be from 10 to 100. Simply, the
total population is selected as the parent set and the GA performs K-parent
crossover on them. Then the mutation perturbs the oﬀspring by R percent.
Mutation is important in this model since the crossover strongly drives the
oﬀspring to the central area; thus an appropriate degree of perturbation is
needed.
– Local optimization: One of the most common local optimization heuristic for
graph partitioning is the FM algorithm. We apply it to the oﬀspring after
mutation.
– Replacement and stopping condition: After generating an oﬀspring and ap-
plying a local optimization on it, the GA replaces the most similar member
of the population with the oﬀspring. Maintaining population diversity, a
randomly generated local optimum replaces the most similar member of the
population per generation. It stops after a ﬁxed number, (M −K)/2, of
generations.
4.2
Experimental Results
We tested the GA with a number of diﬀerent population sizes. The population
size is denoted by K. K is also the number of parents for crossover; in other
words, it is the degree of exploitation around the central area. The values of
K represent a spectrum of the exploitation strengths of the central area. If K
is equal to M, since it just generates initial population without genetic search,
the heuristic equals the multi-start heuristic. The multi-start heuristic returns
the best local optimum among a considerable number of local optima ﬁne-tuned
from random initial solutions. Although the multi-start heuristic is simple, it has
been useful in a number of studies [12] [2].
The experimental results are given in Table 4. We used the FM algorithm as
the local optimizer. We set M and R to 1,000 and 20 respectively in all cases
and performed 100 runs for each case.
Overall, one can observe that it is helpful to exploit the central areas to
some extent. Figure 5 shows two sample plottings, which have roughly bitonic

1132
Y.-H. Kim and B.-R. Moon
MPGA(M, K, R)
// M: running time budget, K: population size, and R: perturbation rate
{
for each i = 1, 2, . . . , K // Generate initial population with size K
{
Generate a random individual Pi;
Pi ←local opt(Pi);
}
B ←the best among population;
do {
Make an oﬀspring C using K-parent crossover from population;
C ←R% random mutation(C);
C∗←local opt(C);
Replace the most similar individual from population with C∗;
Generate a random individual T ;
T ←local opt(T );
Replace the most similar individual from population with T ;
B ←the best among B, C∗, and T ;
} until (the number of generations is (M −K)/2)
return B;
}
Fig. 4. A simple genetic algorithm using multi-parent crossover
spectra of performance. The results of Table 4 and Figure 5 show that it is useful
to exploit the central area, but that excessive or insuﬃcient exploitation is not
desirable.
50
50.5
51
51.5
52
52.5
53
53.5
1
10
100
1000
K
Cut Size
(a) G500.2.5
1726.5
1727
1727.5
1728
1728.5
1729
1729.5
1730
1730.5
1
10
100
1000
K
Cut Size
(b) G500.20
Fig. 5. Two sample spectra extended from Table 4
5
Conclusions
The ﬁtness landscape of the problem space is an important factor to indicate
the problem diﬃculty, and the analysis of the ﬁtness landscape helps eﬃcient
search in the problem space. In this paper, we made a number of experiments
and got some new insights into the global structure of the graph-partitioning
problem space. We extended previous works and observed that the central area
of multiple local optima is quite attractive.

Investigation of the Fitness Landscapes and Multi-parent Crossover
1133
Table 4. The comparison of cut sizes
K = 2
K = 5
K = 10
K = 20
Graphs
Ave†
CPU‡
Ave†
CPU‡
Ave†
CPU‡
Ave†
CPU‡
G500.2.5
52.82
1.56
52.32
1.51
52.08
1.56
51.56
1.63
G500.05
216.09
2.13
215.10
2.09
214.54
2.13
214.70
2.19
G500.10
622.28
3.41
621.46
3.36
620.64
3.41
619.96
3.45
G500.20
1729.13
6.77
1728.77
7.53
1728.27
7.62
1727.46
7.20
G1000.2.5
105.51
3.51
104.28
3.46
103.79
3.53
102.72
3.66
G1000.05
445.68
4.81
454.79
4.76
452.82
4.87
451.53
5.02
G1000.10
1363.76
8.63
1361.27
10.02
1359.44
9.65
1358.09
8.59
G1000.20
3357.62
27.04
3353.72
28.13
3350.87
26.45
3349.23
28.37
U500.05
7.64
1.95
7.22
1.83
7.06
1.92
6.67
1.98
U1000.05
24.60
4.84
25.73
4.51
24.35
5.30
22.81
4.87
U1000.10
41.84
7.18
41.68
6.78
40.64
9.84
40.31
7.35
K = 50
K = 100
K = 200
K = 500
Multi-Start§
Graphs
Ave†
CPU‡
Ave†
CPU‡
Ave†
CPU‡
Ave†
CPU‡
Ave†
CPU‡
G500.2.5
50.87
1.92
50.82
2.40
50.41
3.22
50.64
4.64
53.06
1.63
G500.05
214.12
2.45
213.95
2.92
213.92
3.75
213.78
5.19
217.23
2.20
G500.10
619.55
3.71
619.38
4.24
619.38
5.06
619.94
6.58
623.07
3.54
G500.20
1726.93
9.11
1727.03
8.38
1727.06
8.45
1727.20
11.60
1730.35
8.46
G1000.2.5
100.19
4.25
99.20
5.19
98.38
7.04
97.95
10.30
105.99
3.64
G1000.05
449.27
5.69
448.86
6.59
447.58
8.45
448.15
11.62
457.34
4.95
G1000.10
1356.30
10.10
1354.35
11.20
1354.69
12.17
1355.46
18.19
1365.14
10.21
G1000.20
3346.51
27.57
3345.77
27.56
3344.67
30.15
3345.18
33.71
3363.30
26.80
U500.05
5.60
2.24
5.04
2.71
4.98
3.55
4.87
5.06
7.98
2.07
U1000.05
20.12
5.40
17.28
6.38
14.72
8.28
12.74
11.50
24.09
5.16
U1000.10
40.13
7.74
40.55
8.48
40.86
10.51
40.97
13.58
42.89
8.43
§ K = M(= 1000).
† Average over 100 runs.
‡ CPU seconds on Pentium III 750 MHz.
For the other geometric graphs (U500.10, U500.20, U500.40, U1000.20, and U1000.40)
not shown here, all the methods always found the best known.
It seems clear that there are high-quality solutions clustered near the central
area of local optima. Hence, it is attractive to exploit the central area. Too much
exploitation of the central area perhaps makes the search diversity low. It seems
desirable to exploit the central area avoiding excessive or insuﬃcient exploitation.
We showed that the performance of search could be improved by a multi-parent
crossover based on the exploitation of the central area. The results presented in
this paper can also be good supporting data for the previous studies on multi-
parent crossover [8] [7] [6] [22]. More theoretical arguments for our empirical
results are left for future study.
Our results were achieved in a speciﬁc problem, the graph partitioning prob-
lem. However, we expect that many other hard combinatorial optimization prob-
lems have similar properties. For example, in case of cost-distance correlation,
TSP showed similar property to the graph partitioning problem [3]. We hope
this study provides a good motivation for the investigation of problem spaces
and the design of more eﬀective search algorithms.
Acknowledgments. This work was partly supported by Optus Inc. and Brain
Korea 21 Project. The RIACT at Seoul National University provided research
facilities for this study.

1134
Y.-H. Kim and B.-R. Moon
References
1. R. Battiti and A. Bertossi. Greedy, prohibition, and reactive heuristics for graph
partitioning. IEEE Trans. on Computers, 48(4):361–385, 1999.
2. K. D. Boese and A. B. Kahng.
Best-so-far vs. where-you-are: Implications for
optimal ﬁnite-time annealing. Systems and Control Letters, 22(1):71–78, January
1994.
3. K. D. Boese, A. B. Kahng, and S. Muddu. A new adaptive multi-start technique
for combinatorial global optimizations. Operations Research Letters, 15:101–113,
1994.
4. T. N. Bui and B. R. Moon. Genetic algorithm and graph partitioning. IEEE Trans.
on Computers, 45(7):841–855, 1996.
5. S. Dutt and W. Deng. A probability-based approach to VLSI circuit partitioning.
In Design Automation Conference, pages 100–105, June 1996.
6. A. E. Eiben. Multi-parent recombination, 1997. Handbook of Evolutionary Algo-
rithms (pp. 25–33). IOP Publishing Ltd. and Oxford University Press.
7. A. E. Eiben and T. Back. An empirical investigation of multiparent recombination
operators in evolution strategies. Evolutionary Computation, 5(3):347–365, 1997.
8. A. E. Eiben, P.-E. Rau´e, and Zs. Ruttkay. Genetic algorithms with multi-parent
recombination. In Parallel Problem Solving from Nature – PPSN III, pages 78–87,
1994.
9. C. Fiduccia and R. Mattheyses. A linear time heuristics for improving network
partitions. In 19th ACM/IEEE Design Automation Conference, pages 175–181,
1982.
10. L. R. Jr. Ford and D. R. Fulkerson. Flows in Networks. Princeton University Press,
1962.
11. M. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory
of NP-Completeness. Freeman, San Francisco, 1979.
12. D. S. Johnson. Local optimization and the traveling salesman problem. In 17th
Colloquium on Automata, Languages, and Programming, pages 446–461. Springer-
Verlag, 1990.
13. D. S. Johnson, C. Aragon, L. McGeoch, and C. Schevon. Optimization by simulated
annealing: An experimental evaluation, Part 1, graph partitioning.
Operations
Research, 37:865–892, 1989.
14. T. Jones and S. Forrest.
Fitness distance correlation as a measure of problem
diﬃculty for genetic algorithms.
In Sixth International Conference on Genetic
Algorithms, pages 184–192, 1995.
15. S. Kauﬀman. Adaptation on rugged ﬁtness landscapes. Lectures in the Science of
Complexity, pages 527–618, 1989.
16. B. Kernighan and S. Lin. An eﬃcient heuristic procedure for partitioning graphs.
Bell Systems Technical Journal, 49:291–307, Feb. 1970.
17. Y. H. Kim and B. R. Moon. A hybrid genetic search for graph partitioning based on
lock gain. In Genetic and Evolutionary Computation Conference, pages 167–174,
2000.
18. S. Kirkpatrick, C. D. Jr. Gelatt, and M. P. Vecchi. Optimization by simulated
annealing. Science, 220(4598):671–680, May 1983.
19. B. Manderick, M. de Weger, and P. Spiessens. The genetic algorithm and the struc-
ture of the ﬁtness landscape. In International Conference on Genetic Algorithms,
pages 143–150, 1991.

Investigation of the Fitness Landscapes and Multi-parent Crossover
1135
20. Y. G. Saab.
A fast and robust network bisection algorithm.
IEEE Trans. on
Computers, 44(7):903–913, 1995.
21. G. B. Sorkin. Eﬃcient simulated annealing on fractal landscapes. Algorithmica,
6:367–418, 1991.
22. I. G. Sprinkhuizen-Kuyper, C. A. Schippers, and A. E. Eiben. On the real arity
of multiparent recombination.
In Proceedings of the Congress on Evolutionary
Computation, volume 1, pages 680–686, 6-9 1999.
23. E. D. Weinberger.
Fourier and Taylor series on ﬁtness landscapes.
Biological
Cybernetics, 65:321–330, 1991.

New Usage of Sammon’s Mapping for Genetic
Visualization
Yong-Hyuk Kim and Byung-Ro Moon
School of Computer Science & Engineering, Seoul National University
Shillim-dong, Kwanak-gu, Seoul, 151-742 Korea
{yhdfly, moon}@soar.snu.ac.kr
Abstract. It is a hard problem to understand the ﬁtness landscape of a
problem as well as the evolution of genetic algorithms. For the purpose,
we adopt Sammon’s mapping for the investigation. We demonstrate its
usefulness by applying it to the graph partitioning problem which is a
well-known NP-hard problem. Also, through the investigation of schema
traces, we explain the genetic process and the reordering eﬀect in the
genetic algorithm.
1
Introduction
An NP-hard problem such as graph partitioning problem or traveling salesman
problem (TSP) has a ﬁnite solution set and each solution has a cost. Although
ﬁnite, the problem space is intractably large even for a small but nontrivial prob-
lem. A number of studies about the ruggedness and the properties of problem
search spaces were done. Weinberger [17] conjectured that, if all points on a
ﬁtness landscape are correlated relatively highly, the landscape is bowl shaped.
Boese et al. [1] suggested that, through measuring cost-distance correlation for
the TSP and the graph partitioning problem, the cost surfaces are globally con-
vex. Jones and Forrest [11] introduced ﬁtness-distance correlation as a measure
of search diﬃculty. Good insight into the problem space can provide a motiva-
tion for a good search algorithm [1]. We examine the problem space and hope
to get some insight into the problem space.
For NP-hard problems with intractably large problem space, it is almost
impossible to ﬁnd an optimal solution by exhaustive or simple search methods.
Thus, in case of NP-hard problems, heuristic algorithms or meta-heuristics are
used. The genetic algorithm (GA) is one of the most powerful search methods
among them. A number of studies for understanding GA’s working mechanism
were done. These include schema theorem [10], Royal Road function [13], etc.
Visualization is one of the most basic tools for studies of search spaces. A no-
table method for ﬁtness landscapes is the plotting of ﬁtness-distance correlation
[1]. For GA visualization, the most popular method is the ﬁtness ﬂow over time
as in many GA papers. Another wholesale method is the population data matrix1
for identifying population features. In this paper, we propose new visualization
1 In the matrix, the entire population is displayed in textual form.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1136–1147, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

New Usage of Sammon’s Mapping for Genetic Visualization
1137
techniques primarily using Sammon’s mapping. We analyze the problem space
for graph partitioning more elaborately. We visualize the solutions associated
with the genetic search. We also trace schemata and analyze them.
The remainder of this paper is organized as follows. In Section 2, we sum-
marize graph partitioning problem and Sammon’s mapping which is used as a
major tool for visualization in this paper. We analyze ﬁtness landscapes for graph
partitioning in Section 3. In Section 4, we provide some visualization for genetic
algorithms. In Section 5, schema traces are visualized and analyzed. Finally, we
make our conclusions in Section 6.
2
Preliminaries
2.1
Graph Partitioning
Let G = (V, E) be an unweighted undirected graph, where V is the set of vertices
and E is the set of edges. A bipartition (A, B) consists of two subsets A and
B of V such that A ∪B = V and A ∩B = φ. The cut size of a bipartition is
deﬁned to be the number of edges whose endpoints are in diﬀerent subsets of the
bipartition. The bipartitioning problem is the problem of ﬁnding a bipartition
with minimum cut size. If the diﬀerence of cardinalities between two subsets is
at most one, the problem is called graph bisection problem. It is a representative
NP-hard problem [8]. In this paper, we use three graphs from [3] (one geometric
graph and two caterpillar graphs)2 as test beds.
2.2
Sammon’s Mapping
Sammon’s mapping [16] is a mapping technique for transforming a dataset from a
high-dimensional (say, m-dimensional) input space onto a low-dimensional (say,
d-dimensional) output space (with d < m). The basic idea is to arrange all the
data points on a d-dimensional output space in such a way that minimizes the
distortion of the relationship among data points.
Sammon’s mapping tries to preserve distances. This is achieved by minimiz-
ing an error criterion which penalizes the diﬀerences of distances between points
in the input space and the output space. Consider a dataset of n objects. If
we denote the distance between two points xi and xj in the input space by δij
2 The classes are brieﬂy described below.
i) Un.d: A random geometric graph on n vertices that lie in the unit square and
whose coordinates are chosen uniformly from the unit interval. There is an edge
between two vertices if their Euclidean distance is t or less, where d = nπt2 is the
expected vertex degree.
ii) rcat.n: A caterpillar graph on n vertices. It is constructed out of a straight line
(called the spine), where all the vertices on this line have degree two except the
outermost two vertices. Each vertex on the spine is then connected to √n new
vertices, the legs of the caterpillar.

1138
Y.-H. Kim and B.-R. Moon
-20
-15
-10
-5
0
5
10
-10
-5
0
5
10
15
20
(a) U500.10
-6
-4
-2
0
2
4
6
8
-8
-6
-4
-2
0
2
4
6
(b) rcat.134
-20
-15
-10
-5
0
5
10
15
20
-10
-8
-6
-4
-2
0
2
4
6
(c) rcat.994
Fig. 1. Examples of Sammon’s mapping
and the distances between x′
i and x′
j in the output space by dij, then Sammon’s
stress measure E is deﬁned as follows:
E =
1
n−1
i=1
n
j=i+1 δij
n−1

i=1
n

j=i+1
(δij −dij)2
δij
.
The stress range is [0,1] with 0 indicating a lossless mapping. This stress measure
can be minimized using any minimization technique. Sammon [16] proposed a
technique called pseudo-Newton minimization, a steepest-descent method. The
complexity of Sammon’s mapping is O(n2m). There were several studies about
Sammon’s mapping [7] [5] [14].
The resulting output space depicts clusters of the input space as groups of
data points mapped close to each other in the output space. Figure 1 shows
Sammon’s mapping of three graphs into 2-dimensional space. In the graphs,
we deﬁned the distance between two vertices to be the shortest path length
between each other. We can observe that the mapping well accords with the
characteristics of the graphs.
3
Fitness Landscapes
In this section, we ﬁrst extend the experimentation of Boese et al. [1] to examine
the local-optimum space. We mean by local-optimum space the space consisting
of all local optima with respect to a local optimization algorithm. We then ex-
amine the distribution of local optima. In our experiments, we used a suﬃciently
large number of local optima. We do not care about solutions other than local
optima. The Kernighan-Lin algorithm (KL) [12] was used for local optimization.
In the graph bisection problem for a graph G = (V, E), each solution (A, B)
is represented by a |V |-bits code. Each bit corresponds to a vertex in the graph.
A bit has value 0 if the vertex is in the set A, and has value 1 otherwise. In this
encoding, a vertex move in the solution changes the solution by one bit. Thus, it
is natural to deﬁne the distance between two solutions by the Hamming distance.
Formally, we deﬁne the genotype distance between two solutions as follows.

New Usage of Sammon’s Mapping for Genetic Visualization
1139
Deﬁnition 1 Let the universal set U be {0, 1}|V |. For a, b ∈U, we deﬁne the
genotype distance between a and b as follows:
dg(a, b) = H(a, b)
where H is the Hamming distance.
However, if the genotype distance between two solutions is |V |, they are equal.
We hence deﬁne the phenotype distance between two solutions as follows.
Deﬁnition 2 Let the universal set U be {0, 1}|V |. For a, b ∈U, we deﬁne the
phenotype distance between a and b as follows:3
dp(a, b) = min(dg(a, b), |V | −dg(a, b))
where dg is the genotype distance.
By the deﬁnition, 0 ≤dp(a, b) ≤⌊|V | /2⌋while 0 ≤dg(a, b) ≤|V |. In this paper,
we use the phenotype distance dp for the distance between two solutions unless
otherwise noted.
3.1
Cost-Distance Correlation
Given a set of local minima, Boese et al. [1] plotted, for each local minimum,
i) the relationship between the cost and the average distance to all the other
local minima, and ii) the relationship between the cost and the distance to the
best local minimum. They performed experiments for the graph bisection and
the traveling salesman problem, and showed that both problems have strong
positive correlations for both i) and ii) in the above. This fact hints that the
best local optimum is located near the center of local-optimum space. From
their experiments, they conjectured that the cost surfaces of both problems
are globally convex. In this subsection, we repeat their experiments for another
graph.
The solution space for the experiment was selected as follows. First, we chose
a large number of random solutions and obtained the corresponding set of local
optima by locally optimizing them. Then, we removed the duplicated solutions
in the set if any. Figures 2(a) and (b) show the plotting results with 9,302 local
minima4 for the graph U500.10. The correlation coeﬃcient for the experiment
i) was 0.91. It is consistent with Boese et al.’s results with strong cost-distance
correlation.
3 Given an element a ∈U, there is only one element such that it is diﬀerent from a and
the distance dp to a is zero. If the distance between two elements is equal to zero,
we deﬁne them to be in relation R. Then, the relation R is an equivalence relation.
Suppose Q is the quotient set of U by relation R (Q = U/R), it is easily veriﬁed
that (Q, dp) is a metric space.
4 There were 698 duplications among 10,000 local minima.

1140
Y.-H. Kim and B.-R. Moon
20
40
60
80
100
120
140
160
180
200
170
175
180
185
190
195
200
205
210
215
220
Cut Size
Avg Distance from other local minima
(a)
20
40
60
80
100
120
140
160
180
200
0
50
100
150
200
250
Cut Size
Distance to the best local minimum
(b)
Fig. 2. Fitness-distance correlation (U500.10)
170
175
180
185
190
195
200
205
210
215
220
0
50
100
150
200
250
Avg Distance from other local minima
Distance to the best local minimum
(a)
0
20
40
60
80
100
120
140
160
180
200
0
50
100
150
200
250
Density of Subspace
Distance to the approximate central point
(b)
Fig. 3. Distribution of local minima (U500.10)
3.2
Distribution of Local Optima
As a result of the experiments of Boese et al. [1], we agree with the conjecture
about the global convexity of local-optimum space but it is diﬃcult to obtain
further deduction. Figure 3(a) shows the relationship between the distance to the
best local minimum and the average distance to the other local minima for each
local minimum in the local-optimum space. In the ﬁgure, there are considerably
many solutions such that they are far from the best solution but their average
distances are small. This fact suggests that solutions may be clustered in more
than one place.
We devised another way to examine the distribution of local optima. For each
solution s in the problem space, we chose a ball centered at s with radius r (here
we set r to be |V |/8) and counted the number of local optima inside the ball.
Figure 3(b) plots the densities of the balls. It shows that the density of local
optima near the center of the problem space is remarkably high. Interestingly
enough, one can also observe fairly high-density areas far from the center. It
suggests the existence of “medium valleys”5 or “small valleys.” It can not be
explained by the experimental methods such as [1].
5 The relative notion to big valleys mentioned by Boese et al. [1].

New Usage of Sammon’s Mapping for Genetic Visualization
1141
Fig. 4. Fitness landscape with Sammon’s mapping (U500.10)
0
200
400
600
800
1000
1200
1400
50
100
150
200
250
300
350
400
450
 Generations 
 Cut Size 
Average
Best
Worst
(a) Best, average, and worst
0
50
100
150
200
1
2
4
8
16
32
64
128
256
Best
Cut Size 
 Generations 
Offspring
(b) Best and oﬀspring
Fig. 5. Traditional plotting (a hybrid GA on U500.10)
3.3
Visualization by Sammon’s Mapping
Sammon’s mapping is a good visualization tool for multi-dimensional datasets.
Local-optimum spaces are also good candidates for Sammon’s mapping. Sam-
mon’s mapping of the local-optimum space helps visually understanding the
problem space. Figure 4(a) shows Sammon’s mapping of the local-optimum
space for the graph U500.10. The local optima were Sammon-mapped on the
XY plane. The Z-axis means the cut size. Figures 4(b) and (c) indicate the
projected spaces of Figure 4(a) into XZ plane and Y Z plane, respectively. We
can observe the ﬁtness landscape with respect to Sammon’s mapping. In this
case, the result suggests the existence of valleys in more than one place.
4
Visualization of a Steady-State Genetic Search
4.1
Previous Studies
Traditionally, the ﬁtness-generation plotting has been popular for the visual-
ization of genetic process. A great number of papers include these plottings to
visualize their genetic search process. Figure 5 shows examples of the traditional
plotting.
Recently, Dybowski et al. [6] proposed a GA visualization method using
Sammon’s mapping. There have been a number of studies about GA visualization
using Sammon’s mapping [4] [15]. They presented initial studies about small
problems. An extensive survey of GA visualization techniques appeared in [9].
In this paper, we focus only on the visualization by Sammon’s mapping.

1142
Y.-H. Kim and B.-R. Moon
-200
-150
-100
-50
0
50
100
150
200
-200
-150
-100
-50
0
50
100
150
200
 
 
Best
(a1) Initial state: dp
-200
-150
-100
-50
0
50
100
150
200
-200
-150
-100
-50
0
50
100
150
200
 
 
Best
(a2) Intermediate state: dp
-200
-150
-100
-50
0
50
100
150
200
-200
-150
-100
-50
0
50
100
150
200
 
 
Best
(a3) Final state: dp
-200
-150
-100
-50
0
50
100
150
200
-200
-150
-100
-50
0
50
100
150
200
 
 
Best
(b1) Initial state: dg
-200
-150
-100
-50
0
50
100
150
200
-200
-150
-100
-50
0
50
100
150
200
 
 
Best
(b2) Intermediate state: dg
-200
-150
-100
-50
0
50
100
150
200
-200
-150
-100
-50
0
50
100
150
200
 
 
Best
(b3) Final state: dg
Fig. 6. 2D mapping with diﬀerent distances (hybrid GBA on U500.10)
4.2
Extended Experiments
We extend the works of Dybowski et al. [6]. Using Sammon’s mapping, they
showed that population converges into one or more clusters. They used the Ham-
ming distance (called the genotype distance in this paper) for the distance in
the input space of binary chromosomes.
In this subsection, we provide two experiments for visualization with a GA.
First, we make experiments with diﬀerent distance measures. Then, we provide
a new technique for a steady-state GA to visualize the change of population in
the genetic search process.
We used the Genetic Bisection Algorithm (GBA) [3] for graph bisection prob-
lem. It is a steady-state GA with population size 50, 5-point crossover, adjacent
repair6, and GENITOR-style replacement [18]. If GBA is hybridized with KL
local optimization, it is denoted by KL-GBA. We use KL-GBA on the graph
U500.10 for the experiments in this subsection.
Given a space, various distance measures can be deﬁned. The properties of
the space are largely dependent on the distance measure. Particularly, in Sam-
mon’s mapping, if the output space is a metric space, the input space need to be a
metric space to minimize the stress measure. We used two distance measures: the
genotype distance dg and the phenotype distance dp deﬁned in Section 3. With
dp as the distance measure, Figures 6(a1), (a2), and (a3) show 2-dimensional
mapped population spaces at initial, intermediate, and ﬁnal generation, respec-
tively. Figures 6(b1), (b2), and (b3) show the results with dg as the distance
6 After the crossover, an oﬀspring may not satisfy the balance requirement. It then
selects a random point on the chromosome and changes the required number of 1’s
to 0’s (or 0’s to 1’s) starting at that point on to the right. This adjustment produces
some mutation eﬀect.

New Usage of Sammon’s Mapping for Genetic Visualization
1143
Fig. 7. Visualization of genetic search process with diﬀerent distances (hybrid GBA
on U500.10)
measure. When we used the genotype distance dg, the population converged
into four clusters. However, with the phenotype distance dp, the population con-
verged into roughly two clusters. From the fact that one phenotype matches two
genotypes in graph partitioning, it seems to be reasonable. We observed two
notable valleys in this problem space in Section 3.2 and Section 3.3. It is inter-
esting that the population of GA converged into two clusters. The plotting by
Sammon’s mapping can be extended to three dimensions. We omit the results
here.
In the next experiment, we visualize the change of population in the process
of a steady-state GA. Generally, Sammon’s mapping starts with random initial
positions of n objects. Iteratively, it optimizes the stress measure E. A steady-
state GA typically generates only one oﬀspring per iteration. It does not make a
rapid change per population. Hence, if the positions of the previous generation
are used for the initial positions of the next-generation Sammon’s mapping, the
positions would change steadily over the generations. This makes it possible to
visualize solutions over the genetic search process. Figure 7 shows the visualiza-
tion of a genetic process. Figure 7(a) visualizes the time-varying dataset such
that X-axis is the time and Y Z plane represents the 2-dimensional Sammon’s
mapping. Figure 7(b) shows its projection into XZ plane. Figure 7(c) gives the
average variation between the previous population positions and the current
population positions. More formally, in the ith generation, average variation Vi
is deﬁned to be Vi = 1
K

k ||sk(i)−sk(i−1)||, where sk(i) is the mapped vector
of the kth chromosome in the ith generation and K is the population size. At
a generation with large variation, which probably suggests the occurrence of an
important solution, the continuity gets broken. From this visualization, we can

1144
Y.-H. Kim and B.-R. Moon
also observe the process of population convergence. It helps us to understand the
genetic search process more elaborately. It is notable that this visualization is
related to average cost plotting of Figure 5(a). In the range with stable average
costs, the mapped data also shows minor changes (e.g., see the range [150, 310]
in Figure 5(a) and Figure 7). The phenomenon of punctuated equilibria may
also be observable by this plotting.
5
Schema Traces
A schema is a pattern of bit strings consisting of speciﬁc symbols and asterisks;
here, speciﬁc symbols represent the pattern and the asterisks represent “don’t
care” positions. A genetic algorithm starts with a group of random initial so-
lutions. Of course, the quality of the solutions is low in the early stages of the
genetic algorithm. However, most low-quality solutions contain some schemata
common to high-quality solutions. The crossover operators of genetic algorithms
generate larger schemata by juxtaposition of smaller schemata. It is important
to preserve valuable schemata. A schema is prone to be destroyed by crossover
operators if the positions forming the schema are scattered.
Generally speaking, it is not easy to know high-quality schemata in a prob-
lem. However, for some problems, it is possible to ﬁnd high-quality schemata.
Specially, in Royal Road function [13], all of the desired schemata are given in its
description. We can also ﬁnd high-quality schemata in some instances of graph
partitioning. Caterpillar graphs are good examples. It is clear from their Sam-
mon’s mapping (see Figures 1(b) and (c)). In this subsection, we provide the
visualization of high-quality schema traces for a graph partitioning problem and
a Royal Road function.
First, we compare KL-GBA with BFS-KL-GBA on the graph rcat.994 (see
Figure 1(c)). KL-GBA was introduced in Section 4.2. BFS-KL-GBA is an ap-
proach proposed in [2] for the purpose of transforming the shapes of valu-
able schemata to those advantageous for survival by using Breadth-First Search
(BFS) reordering. We selected a schema consists of 156 vertices. Figure 8 shows
the schema traces in the genetic search process (upper row with KL-GBA and
lower row with BFS-KL-GBA). A bold dot represents the presence of the schema
in a solution. A bold line mostly means the continual presence of the schema.
One can observe remarkable diﬀerence between the two algorithms. Not only did
KL-GBA show low frequency of schema creation, it also showed a low rate of
schema survival. On the contrary, BFS-KL-GBA showed a high rate of schema
survival as well as high frequency of schema creation. Since BFS reordering tends
to shorten the deﬁning lengths of high-quality schemata, the survival probabili-
ties of those schemata become high through crossovers [3]. Without reordering,
despite its early appearance, the schema did not spread all over the population
as steadily as the reordered version. One can observe a high rate of schema dis-
tinction. On the contrary, the reordered version showed fairly stable preservation
of the schema.

New Usage of Sammon’s Mapping for Genetic Visualization
1145
Fig. 8. Schema and reordering (hybrid GBA on rcat.994)
In the next experiment, we observe the schema traces with a 64-bit Royal
Road function. The ﬁtness of a chromosome is determined by the presence of
predeﬁned 8th order schemata. It deﬁnes a tailor-made ﬁtness landscape for GA’s
search and provides an ideal laboratory for studying GA’s behavior. Dynamics
of the search process can be studied by tracing individual schemata. We used
a steady-state GA with population size 50, 1-point crossover, 0.5% mutation
probability per bit, and GENITOR-style replacement. The Hamming distance is
used as the distance measure. Figure 9 shows Sammon’s mappings over the gen-
erations ((a) and (b)), a traditional plotting (c), and the schema traces ((d), (e),
and (f)). Figures 9(a) and (b) clearly reﬂect strong convergence. It is surprising
that the average ﬁtness value nearly reﬂects the distribution of the population
(compare ﬁgure (b) with the average line in ﬁgure (c)). The second and third
rows of Figure 9 show the traces of two low-order high-quality schemata ((d) and
(e)) and the high-order schema merging them (f). It visualizes only the individ-
uals containing the schema. Here, a dotted line does not mean the discontinuity
of schema presence but usually corresponds to a new appearance of an indi-
vidual containing the schema. One can observe that schema2 and schema3 ﬁrst
appeared at around 2500th and 1000th generation, respectively, and they were
successfully combined to a large schema at around 4500th generation. Although
schema3 is appeared earlier than schema2, schema2 spreads over the popula-
tion faster than schema3. Figures (d), (e), and (f) visualize a process of GA by
tracing the lives of particular schemata. Figures (d′), (e′), and (f′) are the 2D
projections of ﬁgures (d), (e), and (f), respectively.

1146
Y.-H. Kim and B.-R. Moon
Fig. 9. Royal Road function with 8 schemata
6
Conclusions
Our approach goes beyond those of Boese et al. [1] and Dybowski et al. [6]. To
get insights into ﬁtness landscapes and GA’s working mechanism, we introduced
visualization techniques using Sammon’s mapping and analyzed various exper-
imental results. A steady-state GA for graph partitioning was mainly used in
this paper. We could obtain some useful insights from the visualization. They
could not be explained by previous visualization experiments. Our approach will
be also useful for other optimization problems.
Sammon’s mapping is one of the possible mapping methods. We may consider
other mapping methods. It is of particular interest as well to investigate the
visualization with respect to genetic operators.
Acknowledgments. This work was partly supported by KOSEF through Sta-
tistical Research Center for Complex Systems at Seoul National University
(SNU) and Brain Korea 21 Project. The RIACT at SNU provided research
facilities for this study.

New Usage of Sammon’s Mapping for Genetic Visualization
1147
References
1. K. D. Boese, A. B. Kahng, and S. Muddu. A new adaptive multi-start technique
for combinatorial global optimizations. Operations Research Letters, 15:101–113,
1994.
2. T. N. Bui and B. R. Moon. Hyperplane synthesis for genetic algorithms. In Fifth
International Conference on Genetic Algorithms, pages 102–109, July 1993.
3. T. N. Bui and B. R. Moon. Genetic algorithm and graph partitioning. IEEE Trans.
on Computers, 45(7):841–855, 1996.
4. T. D. Collins. Genotypic-space mapping: Population visualization for genetic al-
gorithms. The Knowledge Media Institute, The Open University, Milton Keynes,
UK, Technical Report KMI-TR-39, 30th September 1996.
5. D. De Ridder and R.P.W. Duin.
Sammon’s mapping using neural networks: a
comparison. Pattern Recognition Letters, 18(11–13):1307–1316, 1997.
6. R. Dybowski, T.D. Collins, and P. Weller. Visualization of binary string conver-
gence by Sammon mapping. In Fifth Annual Conference on Evolutionary Program-
ming, pages 377–383, 1996.
7. W. Dzwinel.
How to make Sammon mapping useful for multidimensional data
structures analysis. Pattern Recognition, 27(7):949–959, 1994.
8. M. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory
of NP-Completeness. Freeman, San Francisco, 1979.
9. E. Hart and P. Ross. GAVEL – a new tool for genetic algorithm visualization.
IEEE Transactions on Evolutionary Computation, 5(4):335–348, 2001.
10. J. Holland. Adaptation in Natural and Artiﬁcial Systems. University of Michigan
Press, 1975.
11. T. Jones and S. Forrest.
Fitness distance correlation as a measure of problem
diﬃculty for genetic algorithms.
In Sixth International Conference on Genetic
Algorithms, pages 184–192, 1995.
12. B. Kernighan and S. Lin. An eﬃcient heuristic procedure for partitioning graphs.
Bell Systems Technical Journal, 49:291–307, Feb. 1970.
13. M. Mitchell, S. Forrest, and J. H. Holland. The royal road for genetic algorithms:
Fitness landscapes and GA performance. In Toward a Practice of Autonomous
Systems: Proceedings of the First European Conference on Artiﬁcial Life, pages
245–254, Cambridge, MA, 1992. MIT Press.
14. E. Pekalska, D. De Ridder, R.P.W. Duin, and M.A. Kraaijveld. A new method of
generalizing Sammon mapping with application to algorithm speed-up. In Fifth
Annual Conference of the Advanced School for Computing and Imaging, pages 221–
228, 1999.
15. H. Pohlheim. Visualization of evolutionary algorithms – set of standard techniques
and multidimensional visualization.
In Genetic and Evolutionary Computation
Conference, pages 533–540, 1999.
16. J. W. Sammon, Jr.
A non-linear mapping for data structure analysis.
IEEE
Transactions on Computers, 18:401–409, 1969.
17. E. D. Weinberger.
Fourier and Taylor series on ﬁtness landscapes.
Biological
Cybernetics, 65:321–330, 1991.
18. D. Whitley and J. Kauth. GENITOR: A diﬀerent genetic algorithm. In Rocky
Mountain Conference on Artiﬁcial Intelligence, pages 118–130, 1988.

Exploring a Two-Population Genetic Algorithm
Steven Orla Kimbrough1, Ming Lu1, David Harlan Wood2, and D.J. Wu3
1 University of Pennsylvania, 3730 Walnut Street, Philadelphia, PA 19104-6340
{kimbrough,milu}@wharton.upenn.edu
2 University of Delaware, CIS Dept., Newark, DE 19716
wood@cis.udel.edu
3 DuPree College of Management, Georgia Institute of Technology, Atlanta, GA
30332
wudj@drexel.edu
Abstract. In a two-market genetic algorithm applied to a constrained
optimization problem, two ‘markets’ are maintained. One market estab-
lishes ﬁtness in terms of the objective function only; the other market
measures ﬁtness in terms of the problem constraints only. Previous work
on knapsack problems has shown promise for the two-market approach.
In this paper we: (1) extend the investigation of two-market GAs to non-
linear optimization, (2) introduce a new, two-population variant on the
two-market idea, and (3) report on experiments with the two-population,
two-market GA that help explain how and why it works.
1
Introduction
Evolution programs (EPs), and more speciﬁcally genetic algorithms (GAs), are
optimizing or optimum–seeking procedures. They are used routinely to maximize
or minimize an objective function of a number of decision variables. Because the
EPs/GAs are general–purpose procedures (aka: weak methods) they can be used
in the computationally more challenging cases in which the objective function
is nonlinear and/or the decision variables are integers or mixtures of integers
and reals. However, when the objective function is constrained by one or more
constraints use of GAs (and EPs) is problematic. Considerable attention has
been paid to the subject (see [4,9,10,12,16] for excellent reviews and treatments),
but no consensus approach has emerged for encoding constrained optimization
problems as GAs.
A natural—and the most often used—approach is to penalize the objective
function in proportion to the size of the constraint violations. A feasible solu-
tion will impose no penalty on the objective function, an infeasible solution will
impose an objective function penalty (negative if maximizing, positive if mini-
mizing) as a function of the magnitude of the violation(s) of the constraint(s).
Putting this more formally, here is the general form of a maximization con-
strained optimization problem.
max
xi z(x), subject to E(x) ≥a, F(x) ≤b, G(x) = c, xi ∈Si
(1)
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1148–1159, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Exploring a Two-Population Genetic Algorithm
1149
Here, z(x) is the objective function value produced by the candidate solution x.1
E, F and G each yield zero or more constraint inequalities or equalities. Taken
as functions, in the general case z, E, F, G can be any functions at all (on x)
and in particular need not be linear. Si is the set of permitted values for the
xi (the components of the vector x), and are often called the decision variables
for the problem. Si may include reals, integers, or mixtures. Problems of the
form of expression (1) are not directly translatable into the linear encodings
normally used for GA solutions. The purpose of a penalty function formulation
is to produce a representation of the problem that can be directly and naturally
encoded as a GA. To indicate a penalty function representation, let x be a
(candidate) solution to a maximization constrained optimization problem. Its
absolute ﬁtness, W(x), in the presence of penalties for constraint violation is
measured as:
max
xi W(x) = z(x) −P(x)
(2)
where P(x) is the total penalty (if any) associated with constraint violations
by x. Problems representable as in expression (2) are directly and naturally
encoded as GAs. If a GA ﬁnds a solution x that is feasible (P(x) = 0) and has
a high value for W(x), then we may congratulate ourselves on the successful
application of the GA heuristic.
Typically, and by design, the penalty imposed on an infeasible solution will
severely reduce the net ﬁtness of the solution in question, leading to quick elim-
ination of the solution from the GA population. This may be undesirable and
it may be responsible for the generally recognized weak performance of GAs for
constrained optimization problems.
Kimbrough et al. [8] have noted that in constrained optimization a GA will
drive the population of solutions to the neighborhood of the Pareto frontier.2
Under a penalty regime, GA-driven probing of the Pareto frontier is inhibited
because infeasible solutions are heavily penalized, leading to likely loss of useful
aspects (properties, genetic material) that placed them near the frontier. Armed
with this intuition, Kimbrough et al. [8] investigated a “two-market GA” in which
a single population of solutions to a constrained optimization problem undergoes
two distinct GA procedures. Recalling expression (2), in phase 1(“optimality
improvement”) the population is evaluated with the ﬁtness function maxxi z(x)
and a new generation is created via the usual GA mechanisms. Then, in phase 2
1 By candidate solution or just solution we mean any instance of x. Some candidate
solutions are feasible and some not. An optimal solution is a candidate solution,
which need not be unique, satisfying the expression; that is, it is feasible and no
feasible candidate solution yields a better value of z(x).
2 Or rather the eﬀective Pareto frontier. While it is often true, it is not true in general
that the solution to a constrained optimization problem lies on the Pareto frontier
of the constraint set. In this paper when we speak of the Pareto frontier we mean
the eﬀective Pareto frontier, that is the set of solutions such that each is feasible (is
within the constraint set) and such that no decision variable in the solution can be
singly changed so as to improve the objective function and have the solution remain
feasible.

1150
S.O. Kimbrough et al.
Table 1. Summary of Group 1 Problems
Problem Min/Max Objective
# Variables
# Linear
# Nonlinear
Inequalities Inequalities
1
min
Linear
8
3
3
2
min
Polynomial
7
0
4
3
min
Quadratic
10
3
5
4
min
Polynomial
2
0
2
5
min
Linear
2
0
2
6
min
Polynomial
2
0
2
7
min
Quadratic
2
0
2
8
min
Quadratic
2
1
1
9
max
Nonlinear
20
1
1
10
max
Nonlinear
50
1
1
(“feasibility improvement”) the population is evaluated with the ﬁtness function
minxi P(x) and a new generation is created in the usual way. The procedure
continues, one phase following the other, until a stopping condition is met.
Kimbrough et al. [8] obtained encouraging results for a specialized class of
constrained optimization problems (knapsack problems), demonstrating that (as
measured by the number of ﬁtness evaluations employed) their two-market GA
outperformed generally accepted GA penalty function approaches. Knapsacks
are simple, linear integer programming problems. They are known to be NP-
hard, yet in practice they have proved tractable and there exist excellent poly-
nomial time heuristics.
The investigations we describe in §2 examined the eﬀectiveness of the two-
market GA on a collection of standard nonlinear constrained optimization prob-
lems. This is a challenging class of problems, for which improved solution tech-
niques are actively being sought. Further, we introduce a variation of the two-
market GA, which we call a two-population (and two-market) GA. The two-
population GA is essentially equivalent to the single-population, two-market
version, but it aﬀords insight into how the GA is exploring the solution space to
these constrained optimization problems. We discuss this in some detail in §3.
2
Results
Genocop II/III [11] is a sophisticated, highly-developed product, and may be
taken to represent the state of the art for constrained optimization using GAs.
We use it as our benchmark for evaluating the performance of our two-market
GAs. It is convenient to separate the test problems we studied, and our discus-
sions of them, into distinct groups.

Exploring a Two-Population Genetic Algorithm
1151
Table 2. Summary of Group 1 Results
Problem Min/Max Best Known
Genocop
Two-Market GA
or Optimal*
II/III
Best of 10 Median
Std.
1
min
7049.330923*
7268.650
∅
∅
∅
2
min
680.6300573*
680.640
680.6374 680.7566 0.085123
3
min
24.3062091*
25.883
25.18437 25.61935 0.825277
4
min
0.25*
close
0.25
0.25
0
5
min
-5.5079*
close
-5.50773
-5.50708 0.001042
6
min
-6961.81381*
close
-6960.95
-6957.44 1.908346
7
min
5*
close
5
5
0
8
min
1*
close
1
1
0
9
max
0.80351067
0.80351067
0.80288
0.7888
0.12853
10
max
0.8348
0.83319378 0.809211 0.743844 0.054728
2.1
Group 1
Group 1 contains 10 test problems described by Michalewicz in [10].3 Table 1
summarizes these problems.
Table 2 compares results reported by Michalewicz with results obtained by
our two-market, single-population GA. In these experiments we used a ﬂoat-
ing point representation for real number variables, ﬁtness-proportional selection,
arithmetical crossover [10, page 112] with probability 0.4, non-uniform mutation
with probability 0.4 [10, page 103].4 The results reported for Genocop are for the
best solution found from 10 runs. In problems 1–3, the Genocop and the two-
market GA runs are for 5,000 generations with a population of 70. (In the two-
market GA case, this means 2,500 generations of phase 1 and 2,500 generations
of phase 2, alternating). For problems 4–8 Michalewicz reports that Genocop
achieved “convergence” to the known optimal solution within 1,000 generations
for a population of 70. The two-market GA results reported are also for 1,000
generations—or rather half-generations (500 each, phase 1 and phase 2)—with
a population of 70. The Genocop results for problems 9–10 are for 10,000 gen-
erations on a population of 70. In the two-market GA, the population size was
50 and the total number of half generations was 10,000 (5,000 phase 1; 5,000
phase 2). We report the best solution found, the median best solution, and the
standard deviation of the best solution found over 10 runs. In problem #1, the
two-market GA failed to ﬁnd a feasible solution. We note that Genocop requires
a feasible solution at inception; it is not clear how Michalewicz obtained the
necessary solution(s) for Genocop. On the remaining problems, the performance
of this simple, two-market, single-population GA compares well with Genocop.
3 The correspondence between our numbering and his is, using format (our-number,
page-ref, his-identiﬁer): (1, 145, test case #2), (2, 145, test case #3), (3, 146, test
case #5), (4, 137, test case #1), (5, 137, test case #2), (6, 138, test case #3), (7,
138, test case #4), (8, 140, test case #5), (9, 156, Keane (n=20)), (10, 156, Keane
(n=50)).
4 In all our experiments we employed elitest selection [10, page 59].

1152
S.O. Kimbrough et al.
Table 3. Summary of Group 2 Problems
Problem
Min/Max Objective
# Variables
# Linear
# Nonlinear
Inequalities Inequalities
11 (test11)
max
Nonlinear
2
0
2
12 (chance)
min
Linear
3
2
1
13 (circle)
min
Linear
3
0
10
14 (ex3 1 4)
min
Linear
3
2
1
15 (ex7 3 1)
min
Linear
4
6
1
16 (ex7 3 2)
min
Linear
4
6
1
17 (ex14 1 1)
min
Linear
3
0
4
18 (st e08)
min
Linear
2
0
2
19 (st e12)
min
Nonlinear
3
3
0
20 (st e19)
min
Polynomial
2
1
1
21 (st e41)
min
Nonlinear
4
0
2
Problems 4–8 are relatively easy, while 1–3 and 9–10 are much more challeng-
ing. On problems 9–10, the Genocop results are superior to the two-market GA,
while on problems 2–3 Genocop falls behind. We would also draw the reader’s
attention to the reasonably low standard deviations for the two-market GA.
2.2
Group 2
Group 2 consists of 11 nonlinear programming problems. Ten are from the
GLOBAL Library at GAMS World [6]. The Library is “a web site aiming to
bridge the gap between academia and industry by providing highly focused
forums and dissemination services in specialized areas of mathematical pro-
gramming.” As such it is an important source of mathematical programming
(constrained optimization) problems that have proved valuable and interesting
to the operations research and management science community (cf., INFORMS
[5]). Demonstrable success on these problems with evolution programming would
cause the INFORMS community to take notice. An eleventh problem, “test11”,
appeared in a paper by Schoenaur and Xanthakis [17]. Table 3 summarizes the
Group 2 problems, all of which have only inequality constraints (rather than
equality constraints), perhaps after algebraic transformation by us.5
We obtained the Genocop results by executing Genocop III [11] for 10,000
generations, using the default settings (as recommended), including a population
of size 70. The two-market GA was run for 10,000 half generations (5,000 phase
1, 5,000 phase two, alternating), with a population size of 50. Again, we used
a ﬂoating point representation for real number variables, ﬁtness-proportional
selection, arithmetical crossover with probability 0.4, non-uniform mutation with
probability 0.4. Because we have not tuned the crossover or mutation rates, and
we are using a smaller population, it has to be concluded that the two-market
5 We so transformed problems 12 and 19.

Exploring a Two-Population Genetic Algorithm
1153
Table 4. Summary of Group 2 Results. † = 29.8943939275064, ‡ = 0.324275369.
Problem
Best Known
Genocop III
Two-Market GA
or Optimal*
Best of 10 Median
Std.
Best of 10 Median
Std.
11
?
0.115047
0.11504 4.87E-05 0.115047 0.115047 1.12E-07
12
29.8943781591
29.89549 29.94807 0.034976
†
29.90093 0.003931
13
4.57424778502
4.574318 4.575747 0.027615 4.574249 4.574257 9.01E-06
14
-4.0000
-4
-4
0.032482
-4
-4
3.01E-05
15
0.341739553124
0.3558
0.416292 0.141607
‡
0.328268 0.010168
16
1.08986397147
1.09145
1.113644 0.033588 1.089928 1.089985 0.000111
17
0.0000
1.44E-10 1.19E-06 2.85E-06 6.18E-06 0.000172 0.00031
18
?
0.741782 0.741782 9.93E-09 0.741782 0.741782 1.09E-07
19
?
-4.5099
-4.49564 0.02252
-4.50691
-4.49855 0.004872
20
?
-118.705
-118.704 0.021621 -118.705
-118.705 0.001853
21
?
645.626
648.9749 4.398663 641.8242 642.0479 1.827517
GA is not operating at any advantage over Genocop III in these experiments.
Table 4 summarizes our results.
Note that for problem 15, the two-market GA has found a solution (shown
in bold) that is superior to the best solution otherwise known to us.
2.3
Group 2 with a Two-Population GA
The two-market GA introduced and explored for knapsack problems by Kim-
brough et al. [8], and further explored by us, is not the only possible form for
a two-market GA. Theirs is a single-population two-market GA. We introduce
here a two-population two-market GA.
In the two-population GA, the feasible population contains only feasible solu-
tions to the constrained optimization problem to hand, while the infeasible pop-
ulation contains only infeasible solutions. The process is simple. The population-
size parameter is chosen in the customary manner (we use 50). The initialization-
pool parameter is set (we used 100). At initialization we randomly generate at
most initialization-pool×population-size solutions. When a solution is created we
test it for feasibility. If it is feasible and the feasible population is not complete
(has less than population-size solutions), we add the solution to it. Similarly, if
the infeasible population is incomplete and the solution is infeasible, we add it
to the infeasible population. After initialization, each population has at most
population-size solutions or chromosomes. Either population may have fewer, if
a suﬃcient number were not found.
In phase 1 of the two-population (+two-market) GA, the ﬁtnesses of all so-
lutions in the feasible population are calculated based only on the objective
function, z(x), and a new generation is created under the chosen genetic regime.
The regime is any standard GA with the exceptions that (a) the highest ﬁt-
ness solution is placed directly into the next generation and (b) any infeasible
daughter solutions created by the genetic operations are placed in the infeasible

1154
S.O. Kimbrough et al.
population. A total of population-size daughter solutions are created each gen-
eration, but the size of the succeeding generation may be considerably smaller.
In phase 2 of the two-population GA, ﬁtnesses of all the solutions in the infea-
sible population are calculated. (Because of contributions resulting from phase 1,
more than population-size solutions may be present.) The ﬁtness calculations are
based only on the employed penalty function for the constraint set, P(x), and
not on the objective function.6 The best population-size infeasible solutions are
candidates to be parents for the next generation. As in phase 1, (a) the highest
ﬁtness solution is placed directly into the next generation and (b) any feasible
daughter solutions created by the genetic operations are placed in the feasible
population.
Phase 1 and 2 are processed in alternation until a stopping condition is
reached. Table 5 shows our results with Group 2 using the two-population GA
for 10,000 half generations (5,000 phase 1, 5,000 phase 2). Otherwise, the GAs
employed are exactly as in the single-population, two-market version discussed
above. Note that for problem 15 the two-population GA has found solution
Table
5.
Summary
of
Group
2
Results
for
a
Two-Population
GA.
†
=
29.8943786178232, ‡ =0.319777729979837.
Problem
Best Known
Genocop III
Two-Population GA
or Optimal*
Best of 10 Median
Std.
Best of 10 Median
Std.
11
?
0.115047
0.11504 4.87E-05 0.115047 0.115047 4.16E-08
12
29.8943781591
29.89549 29.94807 0.034976
†
29.91486 0.032017
13
4.57424778502
4.574318 4.575747 0.027615 4.574248 4.574257 6.67E-05
14
-4.0000
-4
-4
0.032482
-4
-3.99991 0.000131
15
0.341739553124
0.3558
0.416292 0.141607
‡
0.320127 0.00033
16
1.08986397147
1.09145
1.113644 0.033588 1.089952 1.089994 2.56E-05
17
0.0000
1.44E-10 1.19E-06 2.85E-06 4.69E-05 6.63E-05 1.36E-05
18
?
0.741782 0.741782 9.93E-09 0.741782 0.741782 5.58E-08
19
?
-4.5099
-4.49564 0.02252
-4.51347
-4.51319 0.000995
20
?
-118.705
-118.704 0.021621 -118.705
-118.705 0.000187
21
?
645.626
648.9749 4.398663 641.8244 641.8283 2.663347
(shown in bold) that is superior to the best solution otherwise known to us.
Note further that the two-population GA has found a solution7 to problem 15
that is superior to the best solution8 that the single-population GA found, which
itself is better than the best known solution we are otherwise aware of.
6 Throughout our two-market runs we used the sum of absolute violations on con-
straints as our penalty function. We leave for future research, on tuning the two-
market GAs, the problem of ﬁnding optimal penalty functions.
7 x1 = 1055.688727, x2 = 3.360444871, x3 = 5.040713598, x4 = 0.319777719.
8 x1 = 1053.464266, x2 = 3.361225273, x3 = 5.029994467, x4 = 0.324275369.

Exploring a Two-Population Genetic Algorithm
1155
2.4
Summary and Comments on Results
We have investigated a number of constrained optimization problems that are
nonlinear in their objective functions or in their constraints (or both). This
class of problem is interesting because it arises often in practice and because
standard OR/MS solution methods are often defeated by practical problems of
this sort. The problems we discuss above are the problems we have investigated.
Speciﬁcally, none have been left out, e.g., because results were disappointing.9
Although it is imperative to study many more problems in this class, these
results are broadly quite encouraging. The simple, untuned, and in many ways
na¨ıve, single-population, two-market GA of Kimbrough et al. is apparently ro-
bust for nonlinear optimization. It compares favorably with the performance of
Genocop, an excellent, well-tuned, state-of-the-art conventional GA package. In
addition, we have introduced a two-population variant of a two-market GA, and
shown that it too performs well (at least on the Group 1 and Group 2 problems
we have investigated).
3
Discussion
We conclude with a discussion focusing on understanding why the two-market
GAs should perform well for constrained optimization. This ought to be useful
in designing future investigations.
Here is an easy question: Why, in a constrained optimization problem at-
tacked with a GA (or EP) should we have any penalty associated with constraint
violation? The answer, of course, is that the constraints are part of the problem
and unless we provide information from them to the GA solving procedure there
is no reason to think that the right problem will be addressed. Something has
to be done with the constraints.
A more interesting question: Why not impose prohibitive penalties on con-
straint violation, why not simply eliminate infeasible solutions from considera-
tion? Answer: it’s been tried and doesn’t work very well.10 Why not? Expanding
on suggestions in Kimbrough et al. [8], we should think of the GA as creating
a population of solutions that is driven towards and then explores the Pareto
frontier11 of the constrained optimization problem.
During this stochastic exploration process solutions will inevitably be cre-
ated that “step over the line” from the feasible region to infeasibility. Are these
9 More carefully, we are not reporting experience with nonlinear problems having
equality constraints for which it is not straightforward to eliminate a variable and
thereby convert to an inequality constraint. We examined three such problems with
mixed results.
10 Repair of infeasible solutions is a more modest variant of this approach and it some-
times does work reasonably well. We defer discussion of repair to future work. We
note further that the “death penalty” method, which liquidates solutions found to be
infeasible, has been reported to perform poorly for constrained optimization prob-
lems, e.g., [10].
11 See earlier comments qualifying this terminology.

1156
S.O. Kimbrough et al.
Fig. 1. For problem ex7 3 2 with population size 50: count of number of infeasible
solutions created from the feasible population, by generation, 4850–4999.
solutions now worthless? Surely not, one would conjecture. Even a randomly
generated pool of solutions will typically contain useful genetic material, which
after recombination, mutation, etc. may prove valuable indeed. One would ex-
pect that infeasible solutions, descending from feasible parents and pressured by
selection to minimize infeasibility, might have a fruitfully dense presence of useful
genetic material. The two-market GAs, in both the single- and double-population
forms, present ways of preserving genetic material in infeasible solutions so that
it might be used by future generations.
The two-population GA is convenient for studying the exchange dynamics be-
tween feasible and infeasible solutions. For the two-population GA we described
above and for the problems we applied it to, we found a robust pattern of be-
havior. First, of the 50 daughter solutions created from the feasible population
each generation, roughly 10-20 are infeasible and get put into the infeasible pop-
ulation. This number tends to tail oﬀto nearly zero as the population gets very
close to ﬁnding an optimal solution. See Figure 1 for a representative example.
Second, of the 50 daughter solutions created from the infeasible population each
generation, roughly 0-2 are feasible and get put into the feasible population. This
number typically increases, roughly to 0-4 then 2-8, as the number of genera-
tions increases beyond 5,000. See Figure 2 for a representative example. These
two ﬁndings indicate that the infeasible population is steadily and increasingly
sending genetic material to the feasible population. Our third ﬁnding conﬁrms
this. If we track for each feasible solution whether it has an infeasible ancestor
we ﬁnd that very quickly, before 20 generations have elapsed, every solution in
the feasible population is descended from one infeasible solution or another! See
Figure 3 for a representative example.

Exploring a Two-Population Genetic Algorithm
1157
Fig. 2. For problem ex7 3 2 with population size 50: count of number of feasible solu-
tions created from the infeasible population, by generation, 4850–4999.
These ﬁndings help us understand the dynamics of constrained optimization
with GAs. Yet as good ﬁndings should, they raise more questions than they an-
swer. How can these simple two-market GAs (both single- and double-population
GAs) be improved and tuned? How broadly will they work? By characterizing
the structure of the eﬀective Pareto frontier can we gain useful information for
conﬁguring the GAs? How does the richness of the genetic material in the in-
feasible population vary over generations and how does it compare to that in
the feasible population? What are the most eﬀective forms of penalty functions?
Should scaling be introduced on constraints? There will be an exchange rate in
computational value between maintaining an incrementally larger feasible pop-
ulation and an incrementally larger infeasible population. At some point—it
would appear from our results—an extra infeasible solution is worth more for
optimization than an extra feasible solution. When does this happen? Specif-
ically, we have maintained identical target sizes for both the feasible and the
infeasible populations. Is this the right tradeoﬀor might it be improved? What
can be done to maximize, or even diagnose, the value of the genetic material in
the infeasible population? And so forth.
In addition, GAs and EPs are members of a larger family of metaheuristics
for constrained optimization [13,15,18]. This larger family includes simulated
annealing, neural networks, tabu search, extremal optimization [2], and other
forms of local search heuristics. We are especially intrigued by parallels in the
concepts that diﬀerent heuristics attempt to instantiate. For example crossover
in GAs and annealing in simulated annealing arguably are parallel approaches
to avoid greedy hill-climbing in favor of a less myopic form of local search.
The underlying concept for both forms of the two-market GA is to maintain a
diversity of useful partial solutions (genetic material) and to facilitate subsequent
use of this information. A tantalizing parallel concept, called demon algorithms,

1158
S.O. Kimbrough et al.
Fig. 3. For problem ex7 3 2 with population size 50: fraction, by generation, of feasible
individuals with an infeasible ancestor.
has recently been brought to our attention. (There are various forms; see [3] for
an overview.) Demons act rather like lines of credit for a search process. If the
process is minimizing, it is always permitted to go down hill without drawing on
its line of credit (its demon). Faced with a solution that would take the process
up hill, however, it may draw on its available line of credit and undertake a local
upwards climb. The draw may be deterministic or stochastic, depending on the
variety of algorithm in play. Convergence may be realized by gradually reducing
the available credit.
We see demon algorithms as motivated by an intuition or concept similar
to that underlying the two-market GAs. Related ideas appear elsewhere, e.g.
[14]. What other forms might this concept take? How is it best employed and
when will work or not work well? Our two-population GA would seem to be a
GA analog of primal-dual algorithms [1,7], with two bodies of solutions—one
primal feasible, the other dual feasible—being driven towards each other. These
and many other questions are now on the table, due in part to the encouraging
results obtained, here and earlier, with two-market GAs.
References
1. Mokhtar S. Bazaraa, Hanif D. Sherali, and C. M. Shetty. Nonlinear Programming:
Theory and Algorithms. John Wiley & Sons, Inc., New York, NY, 1993.
2. Stefan Boettcher and Allon G. Percus. Extremal optimization: An evolutionary
local-search algorithm. In Hemant K. Bhargava and Nong Ye, editors, Computa-
tional Modeling and Problem Solving in the Networked World: Interfaces in Com-
puter Science and Operations Research, Operations Research/Computer Science
Interface Series, pages 61–77. Kluwer, Boston, MA, 2003.

Exploring a Two-Population Genetic Algorithm
1159
3. Bala Chandran, Bruce Golden, and Edward Wasil.
A computational study of
three demon algorithm variants for solving the traveling salesman problem.
In
Hemant K. Bhargava and Nong Ye, editors, Computational Modeling and Problem
Solving in the Networked World: Interfaces in Computer Science and Operations
Research, Operations Research/Computer Science Interface Series, pages 155–76.
Kluwer, Boston, MA, 2003.
4. Carlos Artemio Coello Coello. A survey of constraint handling techniques used with
evolutionary algorithms. Technical report Lania-RI-99-04, Laboratorio Nacional de
Inform´atica Avanzada, Veracruz, M´exico, 1999.
http://www.lania.mx/˜ccoello/constraint.html.
5. Institute for Operations Research and Management Science. Informs online. Pages
on the World Wide Web, Accessed 2003-01-19. URL: http://www.informs.org.
6. GAMS World. Global world. Pages on the World Wide Web, Accessed 2003-01-19.
URLs: http://www.gamsworld.org/global/index.htm,
http://www.gamsworld.org/global/globallib.htm.
7. Arthur M. Geoﬀrion. Duality in nonlinear programming: A simpliﬁed applications-
oriented development. SIAM Review, 13(1):1–37, January 1971.
8. Steven O. Kimbrough, Ming Lu, David Harlan Wood, and D. J. Wu. Exploring a
two-market genetic algorithm. In W. B. Langdon, E. Cant´u Paz, and et al., editors,
Proceedings of the Genetic and Evolutionary Computation Conference (GECCO
2002), pages 415–21, San Francisco, CA, 2002. Morgan Kaufmann Publishers.
9. Zbigniew Michalewicz. A survey of constraint handling techniques in evolutionary
computation methods. In Proceedings of the 4th Annual Conference on Evolution-
ary Programming, pages 135–155, Cambridge, MA, 1995. MIT Press.
http://www.coe.uncc.edu/˜zbyszek/papers.html.
10. Zbigniew Michalewicz. Genetic Algorithms + Data Structures = Evolution Pro-
grams. Springer, Berlin, Germany, third edition, 1996.
11. Zbigniew Michalewicz.
Genocop – optimization via genetic algorithms.
World
Wide Web, Accessed January 2003.
http://www.cs.sunysb.edu/∼algorith/implement/genocop/implement.shtml.
12. Zbigniew Michalewicz and David B. Fogel. How to Solve It: Modern Heuristics.
Springer, Berlin, Germany, 2000.
13. I. H. Osman and J. P. Kelly, editors. Meta-Heuristics: Theory and Application.
Kluwer, Boston, MA, 1996.
14. Carlos-Andres Pena-Reyes and Moshe Sipper. Fuzzy CoCo: Balancing accuracy
and interpretability of fuzzy models by means of coevolution. IEEE Transactions
on Fuzzy Systems, 9(5):727–737, October 2001.
15. Colin R. Reeves. Modern Heuristic Techniques for Combinatorial Problems. John
Wiley & Sons, New York, NY, 1993.
16. Ruhul Sarker, Masoud Mohammadian, and Xin Yao, editors. Evolutionary Opti-
mization. Kluwer, Boston, MA, 2002.
17. M. Schoenauer and S. Xanthakis. Constrained GA optimization. In Proceedings
of the 5th International Conference on Genetic Algorithms, pages 573–80. Morgan
Kaufmann, 1993.
18. S. Voß. Metaheuristics: The state of the art. In A. Nareyek, editor, Local Search
for Planning and Scheduling, volume 2148 of Lecture Notes in Computer Science,
pages 1–23. Springer, Heidelberg, Germany, 2001.

Adaptive Elitist-Population Based Genetic
Algorithm for Multimodal Function
Optimization
Kwong-Sak Leung and Yong Liang
Department of Computer Science & Engineering,
The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
{ksleung, yliang}@cse.cuhk.edu.hk
Abstract. This paper introduces a new technique called adaptive elitist-
population search method for allowing unimodal function optimization
methods to be extended to eﬃciently locate all optima of multimodal
problems. The technique is based on the concept of adaptively adjust-
ing the population size according to the individuals’ dissimilarity and
the novel elitist genetic operators. Incorporation of the technique in any
known evolutionary algorithm leads to a multimodal version of the al-
gorithm. As a case study, genetic algorithms(GAs) have been endowed
with the multimodal technique, yielding an adaptive elitist-population
based genetic algorithm(AEGA). The AEGA has been shown to be very
eﬃcient and eﬀective in ﬁnding multiple solutions of the benchmark mul-
timodal optimization problems.
1
Introduction
Interest in the multimodal function optimization is expanding rapidly since real-
world optimization problems often require the location of multiple optima in the
search space. Then the designer can use other criteria and his experiences to
select the best design among generated solutions. In this respect, evolutionary
algorithms(EAs) demonstrate the best potential for ﬁnding more of the best
solutions among the possible solutions because they are population-based search
approach and have a strong optimization capability. However, in the classic EA
search process, all individuals, which may locate on diﬀerent peaks, eventually
converge to one peak due to genetic drift. Thus, standard EAs generally only
end up with one solution. The genetic drift phenomenon is even more serious
in EAs with the elitist strategy, which is a widely adopted method to improve
EAs’ convergence to a global optimum of the problems.
Over the years, various population diversity mechanisms have been proposed
that enable EAs to maintain a diverse population of individuals throughout
its search, so as to avoid convergence of the population to a single peak and to
allow EAs to identify multiple optima in a multimodal domain. However, various
current population diversity mechanisms have not demonstrated themselves to
be very eﬃcient as expected. The eﬃciency problems, in essence, are related to
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1160–1171, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Adaptive Elitist-Population Based Genetic Algorithm
1161
some fundamental dilemmas in EAs implementation. We believe any attempt
of improving the eﬃciency of EAs has to compromise these dilemmas, which
include:
– The elitist search versus diversity maintenance dilemma: EAs are also ex-
pected to be global optimizers with unique global search capability to guar-
antee exploration of the global optimum of a problem. So the elitist strategy
is widely adopted in the EAs search process. Unfortunately, the elitist strat-
egy concentrates on some “super” individuals, reduces the diversity of the
population, and in turn leads to the premature convergence.
– The algorithm eﬀectiveness versus population redundancy dilemma: For
many EAs, we can use a large population size to improve their eﬀectiveness
including a better chance to obtain the global optimum and the multiple
optima for a multimodal problem. However, the large population size will
notably increase the computational complexity of the algorithms and gen-
erate a lot of redundant individuals in the population, thereby decrease the
eﬃciency of the EAs.
Our idea in this study is to strike a tactical balance between the two contra-
dictory issues of the two dilemmas. We propose a new adaptive elitist-population
search technique to identify and search multiple peaks eﬃciently in multimodal
problems. We incorporate the technique in genetic algorithms(GAs) as a case
study, yielding an adaptive elitist-population based genetic algorithm(AEGA).
The next section describes the related work relevant to our proposed tech-
nique. Section 3 introduces the adaptive elitist-population search technique and
describes the implementation of the algorithm. Section 4 presents the compar-
ison of our results with other multimodal evolutionary algorithms. Section 5
draws some conclusion and proposes further directions of research.
2
Related Work
In this section we brieﬂy review the existing methods developed to address the
related issues: elitism, niche formation method, and clonal selection principle of
an artiﬁcial immune network.
2.1
Elitism
It is important to prevent promising individuals from being eliminated from
the population during the application of genetic operators. To ensure that the
best chromosome is preserved, elitist methods copy the best individual found
so far into the new population [4]. Diﬀerent EAs variants achieve this goal of
preserving the best solution in diﬀerent ways, e.g. GENITOR [8] and CHC [2].
However, “elitist strategies tend to make the search more exploitative rather
than explorative and may not work for problems in which one is required to ﬁnd
multiple optimal solutions” [6].

1162
K.-S. Leung and Y. Liang
2.2
Evolving Parallel Subpopulations by Niching
Niching methods extend EAs to domains that require the location and mainte-
nance of multiple optima. Goldberg and Richardson [1] used Holland’s sharing
concept [3] to divide the population into diﬀerent subpopulations according to
similarity of the individuals. They introduced a sharing function that deﬁnes the
degradation of the ﬁtness of an individual due to the presence of neighboring
individuals. The sharing function is used during selection. Its eﬀect is such that
when many individuals are in the same neighborhood they degrade each other’s
ﬁtness values, thus limiting the uncontrolled growth of a particular species.
Another way of inducing niching behavior in a EAs is to use crowding meth-
ods. Mahfoud [7] improved standard crowding of De Jong [4], namely deter-
ministic crowding, by introducing competition between children and parents of
identical niche. Deterministic crowding works as follows. First it groups all popu-
lation elements into n/2 pairs. Then it crosses all pairs and mutates the oﬀspring.
Each oﬀspring competes against one of the parents that produced it. For each
pair of oﬀspring, two sets of parent-child tournaments are possible. Determinis-
tic crowding holds the set of tournaments that forces the most similar elements
to compete. Similarity can be measured using either genotypic or phenotypic
distances. But deterministic crowding fails to maintain diversity when most of
the current populations have occupied a certain subgroup of the peaks in the
search process.
2.3
Clonal Selection Principle
The clonal selection principle is used to explain the basic features of an adaptive
immune response to an antigenic stimulus. This strategy suggests that the algo-
rithm performs a greedy search, where single members will be optimized locally
(exploitation of the surrounding space) and the newcomers yield a broader explo-
ration of the search space. The population of clonal selection includes two parts.
First one is the clonal part. Each individual will generate some clonal points and
select best one to replace its parent, and the second part is the newcomer part,
the function of which is to ﬁnd new peaks. Clonal selection algorithm also incurs
expensive computational complexity to get better results of the problems [5].
All the techniques found in the literature try to give all local or global optimal
solutions an equal opportunity to survive. Sometimes, however, survival of low
ﬁtness but very diﬀerent individuals may be as, if not more, important than that
of some highly ﬁt ones. The purpose of this paper is to present a new technique
that addresses this problem. We show that using this technique, a simple GA
will converge to multiple solutions of a multimodal optimization problem.
3
Adaptive Elitist-Population Search Technique
Our technique for the multimodal function maximization presented in this paper
achieves adaptive elitist-population searching by exploiting the notion of the

Adaptive Elitist-Population Based Genetic Algorithm
1163
face to face
back to back
one−way
O 
O 
O 
O 
O 
O 
Fig. 1. The relative ascending direction of both individuals being considered: back to
back, face to face and one-way.
relative ascending directions of both individuals (and for a minimization problem
this direction is called relative descending direction).
For a high dimension maximization problem, every individual generally has
many ascending directions. But along the line, which is uniquely deﬁned by
two individuals, each individual only has one ascending direction, called the
relative ascending direction toward the other one. Moreover, the relative ascend-
ing directions of both individuals only have three probabilities: back to back,
face to face and one-way (Fig.1). The individuals located in diﬀerent peaks
are called dissimilar individuals. We can measure the dissimilarity of the in-
dividuals according to the composition of their relative ascending directions and
their distance. The distance between two individuals xi = (xi1, xi2, · · · , xin) and
xj = (xj1, xj2, · · · , xjn) is deﬁned by:
d(xi, xj) =




n

k=1
(xik −xjk)2
(1)
In this paper we use the above deﬁnition of distance, but the method we
described will work for other distance deﬁnitions as well.
3.1
The Principle of the Individuals’ Dissimilarity
Our deﬁnition of the principle of the individuals’ dissimilarity, as well as the
operation of the AEGA, depends on the relative ascending directions of both
individuals and a parameter we call the distance threshold, which denoted by
σs. The principle to measure the individuals’ dissimilarity is demonstrated as
follows:
– If the relative ascending directions of both individuals are back to back, these
two individuals are dissimilar and located on diﬀerent peaks;
– If the relative ascending directions of both individuals are face to face or
one-way, and the distance between two individuals is smaller than σs, these
two individuals are similar and located on the same peak.
In niching approach, the distance between two individuals is the only measure-
ment to determine whether these two individuals are located on the same peak,

1164
K.-S. Leung and Y. Liang
0
5
10
15
20
0
50
100
150
200
(a)
Fitness
0
5
10
15
20
0
50
100
150
200
(b)
Fitness
P1
P2
P1
P2
a 
a 
b
b 
c 
c 
O
O
X
X
X
X
Fig. 2. Determining subpopulations by niching method and the relative ascending di-
rections of the individuals.
but this is often not accurate. Suppose, for example, that our problem is to
maximize the function shown in Fig.2. P1 and P2 are two maxima and assume
that, in a particular generation, the population of the GA consists of the points
shown. The individuals a and b are located on the same peak, and the individual
c is on another peak. According to the distance between two individuals only, the
individuals b and c will be put into the same subpopulation, and the individual
a into another subpopulation (Fig.2-(a)). Since the ﬁtness of c is smaller than
that of b, the probability of c surviving to the next generation is low. This is
true even for a GA using ﬁtness sharing, unless a sharing function is speciﬁcally
designed for this problem. However, the individual c is very important to the
search, if the global optimum P2 is to be found. Applying our principle, the
relative ascending directions of both individuals b and c are back to back, and
they will be considered to be located on diﬀerent peaks (Fig.2-(b)). Identifying
and preserving the “good quality” of individual c is the prerequisite for genetic
operators to maintain the diversity of the population. We propose to solve the
problems by using our new elitist genetic operators described below.
3.2
Adaptive Elitist-Population Search
The goal of the adaptive elitist-population search method is to adaptively adjust
the population size according to the features of our technique to achieve:
– a single elitist individual searching for each peak; and
– all the individuals in the population searching for diﬀerent peaks in parallel.
For satisfying multimodal optimization search, we deﬁne the elitist individ-
uals in the population as the individuals with the best ﬁtness on diﬀerent peaks
of the multiple domain. Then we design the elitist genetic operators that can
maintain and even improve the diversity of the population through adaptively
adjusting the population size. Eventually the population will exploit all optima
of the mulitmodal problem in parallel based on elitism.
Elitist Crossover Operator: The elitist crossover operator is composed based
on the individuals’ dissimilarity and the classical crossover operator. Here we

Adaptive Elitist-Population Based Genetic Algorithm
1165
(a)
(b)
(c)
O 
O 
X 
X 
X − parent 
O − offspring 
O 
O 
O 
O 
X 
X 
X 
X 
P1 
P1 
P1 
P2 
P2 
P2 
X − parent 
O − offspring 
O − offspring 
X − parent 
Fig. 3. A schematic illustration that the elitist crossover operation.
have chosen the random uniformly distributed variable to perform crossover
(with probability pc), so that the oﬀspring ci and cj of randomly chosen parents
pi and pj are:
ci = pi ± µ1 × (pi −pj)
cj = pj ± µ2 × (pi −pj)
(2)
where µ1, µ2 are uniformly distributed random numbers over [0, 1] and the signs
of µ1, µ2 are determined by the relative directions of both pi and pj.
The algorithm of the elitist crossover operator is given as follows:
Input: g–number of generations to run, N–population size
Output: Pg–the population to next generation
for t ←−1 to N/2 do
pi ←−random select from population Pg(N));
pj ←−random select from population Pg(N −1);
determine the relative directions of both pi and pj;
if back to back then µ1 < 0 and µ2 < 0;
if face to face then µ1 > 0 and µ2 > 0;
if one-way then µ1 > 0, µ2 < 0 or µ1 < 0, µ2 > 0;
ci ←−pi + µ1 × (pi −pj);
cj ←−pj + µ2 × (pi −pj);
if f(c1) > f(p1) then p1 ←−c1;
if f(c2) > f(p2) then p2 ←−c2;
if the relative directions of p1 and p2 are face to face or one-way, and
d(p1, p2) < σs, then
if f(p1) > f(p2) then Pg ←−Pg/p2 and N ←−N −1;
if f(p2) > f(p1) then Pg ←−Pg/p1 and N ←−N −1;
end for
As shown above, through determining the signs of the parameters µ1 and µ2
by the relative directions of both p1 and p2, the elitist crossover operator gen-
erates the oﬀspring along the relative ascending direction of its parents (Fig.3),
thus the search successful rate can be increased and the diversity of the popula-
tion be maintained. Conversely, if the parents and their oﬀspring are determined
to be on the same peak, the elitist crossover operator could select the elitist to be

1166
K.-S. Leung and Y. Liang
retained by eliminating all the redundant individuals to increase the eﬃciency
of the algorithm.
Elitist Mutation Operator: The main function of the mutation operator is
ﬁnding a new peak to search. However, the classical mutation operator cannot
satisfy this requirement well. As shown in Fig.4-(a), the oﬀspring is located on a
new peak, but since its ﬁtness is not better than its parent, so it is diﬃcult to be
retained, and hence the new peak cannot be found by this mutation operation.
We design our elitist mutation operator for solving this problem based on any
mutation operator, but the important thing is to determine the relative directions
of the parent and the child after the mutation operation. Here we use the uniform
neighborhood mutation (with probability pm):
ci = pi ± λ × rm
(3)
where λ is a uniformly distributed random number over [−1, 1], rm deﬁnes the
mutation range and it is normally set to 0.5 × (bi −ai), and the + and −signs
are chosen with a probability of 0.5 each.
The algorithm of the elitist mutation operator is given as follows:
Input: g–number of generations to run, N–population size
Output: Pg–the population to next generation
for t ←−1 to N do
ct ←−pt + λ × rm;
determine the relative directions of both pt and ct;
if face to face or one-way and f(ct) > f(pt) then
pt ←−ct and break ;
else if back to back, then
for s ←−1 to N −1 do
if [d(ps, ct) < σs] and [f(ps) > f(ct)] then break;
else Pg ←−Pg ∪{ct} and N ←−N + 1;
end for
end if
end for
As shown above, if the direction identiﬁcation between the parent and oﬀ-
spring demonstrates that these two points are located on diﬀerent peaks, the
parent is passed on to the next generation and its oﬀspring is taken as a new
individual candidate. If the candidate is on the same peak with another individ-
ual, the distance threshold σs will be checked to see if they are close enough for
ﬁtness competition for survival. Accordingly, in Fig.4-(a), the oﬀspring will be
conserved in the next generation, and in Fig.4-(b), the oﬀspring will be deleted.
Thus, the elitist mutation operator can improve the diversity of the population
to ﬁnd more multiple optima.

Adaptive Elitist-Population Based Genetic Algorithm
1167
0
5
10
15
20
0
50
100
150
200
(a)
0
5
10
15
20
0
50
100
150
200
(b)
P1 
P1 
P2 
P2 
parent
offspring
individual candidate
parent
another individual
O 
O 
X 
X 
X
Fig. 4. A schematic illustration that the elitist mutation operation.
3.3
The AEGA
In this section, we will present the outline of the Adaptive Elitist-population
based Genetic Algorithm(AEGA). Because our elitist crossover and mutation
operators can adaptively adjust the population size, our technique completely
simulates the “survival for the ﬁttest” principle without any special selection
operator. On the other hand, since the population of AEGA includes most of
the elitist individuals, a classical selection operator could copy some individuals
to the next generation and deleted others from the population, thus the selection
operator will decrease the diversity of the population, increase the redundancy
of the population, and reduce eﬃciency of the algorithm. Hence, we design the
AEGA without any special selection operator. The pseudocode for AEGA is
shown bellow. We can see that the AEGA is a single level parallel (individuals)
search algorithm same as the classical GA, but the classical GA is to search
for single optimum. The niching methods is a two-level parallel (individuals and
subpopulations) search algorithms for multiple optima. So in terms of simplicity
in the algorithm structure, the AEGA is better than the other EAs for multiple
optima.
The structure of the AEGA:
begin
t ←−0;
Initialize P(t);
Evaluate P(t);
while (not termination condition) do
Elitist crossover operation P(t + 1);
Elitist mutation operation P(t + 1);
Evaluate P(t + 1);
t ←−t + 1;
end while
end
4
Experimental Results
The test suite used in our experiments include those multimodal maximization
problems listed in Table 1. These types of functions are normally regarded as

1168
K.-S. Leung and Y. Liang
diﬃcult to be optimized, and they are particularly challenging to the applica-
bility and eﬃciency of the multimodal evolution algorithms. Our experiments of
multimodal problems were divided into two groups with diﬀerent purpose. We
report the results of each group below.
Table 1. The test suite of multimodal functions used in our experiments.
Deb’s function(5 peaks):
f1(x) = sin6(5πx), x ∈[0, 1];
Deb’s decreasing function(5 peaks):
f2(x) = 2−2((x−0.1)/0.9)2 sin6(5πx), x ∈[0, 1];
Roots function(6 peaks):
f3(x) =
1
1+|x6−1|, where x ∈C, x = x1 + ix2 ∈[−2, 2];
Multi function(64 peaks):
f4(x) = x1 sin(4πx1) −x2 sin(4πx2 + π) + 1; x1, x2 ∈[−2, 2].
0
0.5
1
0
0.5
1
f1
0
0.5
1
0
0.5
1
f2
−2
0
2
−2
0
2
0
0.5
1
f3
−2
0
2
−2
0
2
−5
0
5
f4
Fig. 5. The suite of the multimodal test function.
Explanatory Experiments: This group of experiments on f3(x) aims to ex-
hibit the evolution details (particularly, the adaptive adjusting population size
details) of the AEGA for 2-D case, and also to demonstrate the parameter con-
trol of the search process of the AEGA. In applying the AEGA to solve the
2-D problem f3(x), we set the initial population size N = 2 and the distance
threshold σs = 0.4 or 2.
Fig.6 demonstrate clearly how the new individual are generated on a newly
discovered peak and how the elitist individuals reach each optimum in the mul-
timodal domain. Fig.6-(a) shows the 2 initial individuals, when σs = 0.4, the
population size is increased to 8 at the 50th generation(Fig.6-(b)), and Fig.6-(c)
show the 4 individuals in the population at the 50th generation when σs = 2.
When σs is smaller, new individuals are generated more easily. At the 50th gen-
eration, the result of σs = 0.4 seems to be better; but ﬁnally, both settings of the
AEGA can ﬁnd all the 6 optima within 200 generations (Fig.6-(d)). This means
the change of the distance threshold does not necessarily inﬂuence the eﬃciency
of the AEGA.

Adaptive Elitist-Population Based Genetic Algorithm
1169
−2
0
2
−2
−1
0
1
2
(a)
−2
0
2
−2
−1
0
1
2
(b)
−2
0
2
−2
−1
0
1
2
(c)
−2
0
2
−2
−1
0
1
2
(d)
Fig. 6. A schematic illustration that the AEGA to search on the Roots function, (a) the
initial population; (b) the population at 50th generation (σs = 0.4); (c) the population
at 50th generation (σs = 2); (d) the ﬁnal population at 200th generation (σs = 0.4 and
2).
Comparisons: To assess the eﬀectiveness and eﬃciency of the AEGA, its per-
formance is compared with the ﬁtness sharing, determining crowding and clonal
selection algorithms. The comparisons are made in terms of the solution quality
and computational eﬃciency on the basis of applications of the algorithms to
the functions f1(x)−f4(x) in the test suite. As each algorithm has its associated
overhead, a time measurement was taken as a fair indication of how eﬀectively
and eﬃciently each algorithm could solve the problems. The solution quality and
computational eﬃciency are therefore respectively measured by the number of
multiple optima maintained and the running time for attaining the best result
by each algorithm. Unless mentioned otherwise, the time is measured in seconds
as measured on the computer.
Tables 2 lists the solution quality comparison results in terms of the num-
bers of multiple optima maintained when the AEGA and other three multimodal
algorithms are applied to the test functions, f1(x) −f4(x). We have run each
algorithms 10 times. We can see, each algorithm can ﬁnd all optima of f1(x).
In the AEGA, two initial individuals increase to 5 individuals and ﬁnd the 5
multiple optima. For function f2(x), crowding algorithm cannot ﬁnd all optima
for each time. For function f3(x), crowding cannot get any better result. Shar-
ing and clonal algorithms need to increase the population size for improving
their performances. The AEGA still can use two initial individuals to ﬁnd all
multiple optima. For function f4(x), crowding, sharing and clonal algorithms
cannot get any better results, but the successful rate of AEGA for ﬁnding all
multiple optima is higher than 99%. Figs.7 and 8 show the comparison results
of the AEGA and the other three multimodal algorithms for f1(x) and f2(x),
respectively. The circles and starts represent the initial populations of AEGA
and the ﬁnal solutions respectively. In the AEGA process, we have only used 2
individuals in the initial population. In 200 generations, ﬁnally the 5 individuals
in the population can ﬁnd the 5 multiple optima. These clearly show why the
AEGA is signiﬁcantly more eﬃcient than the other algorithms. On the other
hand, the computational eﬃciency comparison results are also shown in Tables
2. It is clear from these results that the AEGA exhibits also a very signiﬁcant
outperformance of many orders compared to the three algorithms for all test

1170
K.-S. Leung and Y. Liang
functions. All these comparisons show the superior performance of the AEGA
in eﬃcacy and eﬃciency.
0
0.5
1
0
0.5
1
fitness sharing
0
0.5
1
0
0.5
1
deterministic crowding
0
0.5
1
0
0.5
1
clonal selection
0
0.5
1
0
0.5
1
AEGA
Fig. 7. A schematic illustration that the results of the algorithms for f1(x).
0
0.5
1
0
0.5
1
fitness sharing
0
0.5
1
0
0.5
1
deterministic crowding
0
0.5
1
0
0.5
1
clonal selection
0
0.5
1
0
0.5
1
AEGA
Fig. 8. A schematic illustration that the results of the algorithms for f2(x).
Table 2. Comparison of results of the algorithms for f1(x) −f4(x).
5
Conclusion and Future Work
In this paper we have presented the adaptive elitist-population search method,
a new technique for evolving parallel elitist individuals for multimodal function
optimization. The technique is based on the concept of adaptively adjusting the
population size according to the individuals’ dissimilarity and the elitist genetic
operators.

Adaptive Elitist-Population Based Genetic Algorithm
1171
The adaptive elitist-population search technique can be implemented with
any combinations of standard genetic operators. To use it, we just need to in-
troduce one additional control parameter, the distance threshold, and the pop-
ulation size is adaptively adjusted according to the number of multiple optima.
As an example, we have endowed genetic algorithms with the new multiple tech-
nique, yielding an adaptive elitist-population based genetic algorithm(AEGA).
The AEGA then has been experimentally tested with a diﬃcult test suite con-
sisted of complex multimodal function optimization examples. The performance
of the AEGA is compared against the ﬁtness sharing, determining crowing and
clonal selection algorithms. All experiments have demonstrated that the AEGA
consistently and signiﬁcantly outperforms the other three multimodal evolu-
tionary algorithms in eﬃciency and solution quality, particularly with eﬃciency
speed-up of many orders.
We plan to apply our technique to hard multimodal engineering design prob-
lems with the expectation of discovering novel solutions. We will also need to
investigate the behavior of the AEGA on the more theoretical side.
Acknowledgment. This research was partially supported by RGC Earmarked
Grant 4212/01E of Hong Kong SAR and RGC Research Grant Direct Allocation
of the Chinese University of Hong Kong.
References
1. D.E. Goldberg and J.Richardson, Genetic algorithms with sharing for multimodal
function optimization, Proc. 2nd ICGA, pp.41-49, 1987
2. Eshelman.L.J. The CHC adaptive search algorithm: how to have safe search when
engaging in nontraditional genetic recombination. In Rawlins, G.J.E., editor, Foun-
dation of Genetic Algorithms pp.265-283, Morgan Kaufmann, San Mateo, California,
1991
3. Holland and J.H. Adaptation in Natural and ArtiﬁcialSystem.University of Michi-
gan Press, Ann Arbor, Michigan, 1975.
4. K.A. De Jong, An analysis of the behavior of a class of genetic adaptive systems,
Doctoral dissertation, Univ. of Michigan, 1975
5. L.N.Castro and F.J.Zuben, Learning and Optimization Using the Clonal Selection
Principle, IEEE Transactions on EC, vol .6 pp.239-251, 2002
6. Sarma.J. and De Jong, Generation gap methods. Handbook of Evolutionary Com-
putation, pp.C2.7:1-C2.7:5, 1997
7. S.W.Mahfoud. Niching Methods for Genetic algorithms, Doctoral Dissertation, Il-
liGAL Report 95001, University of Illinois at Urbana Champaign, Illinois Genetic
Algorithm Laboratory, 1995
8. Whitley.D., The GENITOR algorithm and selection pressure: why rank-based allo-
cation of reproductive trials is best. Proc. 3rd ICGA, pp.116-121, 1989

Wise Breeding GA via Machine Learning
Techniques for Function Optimization
Xavier Llor`a and David E. Goldberg
Illinois Genetic Algorithms Laboratory (IlliGAL),
National Center for Supercomputing Applications,
University of Illinois at Urbana-Champaign,
104 S. Mathews Avenue, Urbana, IL 61801.
{xllora,deg}@illigal.ge.uiuc.edu
Abstract. This paper explores how inductive machine learning can
guide the breeding process of evolutionary algorithms for black-box func-
tion optimization. In particular, decision trees are used to identify the
underlying characteristics of good and bad individuals, using the mined
knowledge for wise breeding purposes. Inductive learning is comple-
mented with statistical learning in order to deﬁne the breeding pro-
cess. The proposed evolutionary process optimizes the ﬁtness function
in a dual manner, both maximizing and minimizing it. The paper also
summarize some tuning and population sizing issues, as well as some
preliminary results obtained using the proposed algorithm.
1
Introduction
Recently, a new interest in the genetic algorithms (GA) community has been
growing. The work published by Baluja [1,2], Juels & Wattenberg [3], and
M¨uhlenbein & Paaß [4]—among others—sparked a new way to approach to GA.
Instead of recombining genes, as in a traditional GA, this new approach proposes
the usage of explicit statistics as the main breeding force. These kind of GA are
known as probabilistic model building GA (PMBGA), or estimation of distri-
bution algorithms (EDAs) [5]. Instead of using crossover or mutation operators,
these GA breed a new population of individuals sampling a learned probabilistic
model that describes the good individuals in the population.
Some early eﬀorts done in PMBGA assume that the probability distribu-
tion of each gene can be computed independently. This assumption, not true in
many real-world problems, leads to some well-know algorithms. Some examples
of algorithms that assume gene independence are: PBIL (Population Based In-
cremental Learning) [1], UMDA (Univariate Marginal Distribution Algorithms)
[6], and the cGA (compact Genetic Algorithm) [7]. Dependences among genes,
and what that implies to the distributions to be learned, have also been stud-
ied by several authors. Relevant work deals with gene dependence modeling and
linkage learning. For an overview of these approaches please see [5,8]. An example
of this kind of algorithms is BOA (Bayesian Optimization Algorithm) [9]. BOA
bases its probability distribution on a Bayesian network. This network describes
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1172–1183, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Wise Breeding GA via Machine Learning Techniques
1173
the probability distribution of the good individuals in the population, as well as
the dependences among genes.
There are other non-statistical approaches to model building GA for breeding
purposes. Some of them are provided by inductive machine learning techniques.
An example of this kind of approach is LEM (Learnable Evolution Model) pro-
posed by Michalski [10,11]. LEM combines two diﬀerent learning paradigms,
evolutionary learning and inductive rule learning. On the one hand, the evolu-
tionary learning uses a simple GA for function optimization. On the other hand,
LEM1 and LEM2 uses AQ15 [12] and AQ18 [13] for rule learning. However, LEM
is a hybrid approach that still relies on traditional GA mechanisms, although
they can be turned oﬀ, moving it away from the ideas that inspired PMBGA
and EDAs.
This paper presents an alternative approach. The work combines statistical
models and inductive machine learning for guiding the breeding process. The
goal is to create an algorithm that exploits the knowledge that can be inferred
from the current population. In order to mine the population looking for useful
breeding knowledge, we combine two diﬀerent machine learning methods under
a supervised learning paradigm. This is achieved casting the optimization task
into a problem that can be solved using machine learning techniques. The goal
is boosting the evolutionary search process using the mined knowledge. This
population based learning algorithm, SI3E (statistical and inductive tree based
evolution), is applied to some optimization problems, showing its competence, as
well as some interesting evolutionary dynamics, optimizing the ﬁtness function
in a dual manner, both maximizing and minimizing it. Moreover, casting the
optimization into an inductive learning problem provides an elegant way for
population sizing of the proposed algorithm.
The paper is structured as follows. Section 2 reviews some background,
needed for understanding SI3E. Then, section 3 describes the evolutionary model
proposed by SI3E. This section also discusses the parameter tuning and popu-
lation sizing for a competent usage of the algorithm. Section 4 summarizes the
preliminary results obtained using the algorithm on two diﬀerent well-known
black-box optimization functions. Finally, section 5 presents some conclusions
about the work presented in this paper.
2
Background
The goal of the work presented in this paper is to combine the ideas that inspired
PMBGA, EDAs, and inductive machine learning. SI3E (statistical and inductive
tree based evolution) is the result of mixing a PMBGA and inductive decision
trees. Properly speaking, SI3E deﬁnes a common framework based on PBIL [1]
and ID3 [14,15]. This section describes both algorithms brieﬂy, PBIL and ID3, in
order to explain how SI3E integrates both for obtaining a model-based breeding
GA in the next section.

1174
X. Llor`a and D.E. Goldberg
2.1
Population Based Incremental Learning (PBIL)
PBIL was introduced by Baluja [1], and was later improved in [2,16]. The goal of
PBIL is to solve a black-box optimization problem. The target function is deﬁned
over a binary space Ω= {0, 1}ℓ. Thus, each individual in the population can be
represented by a binary string. However, PBIL does not maintain the population.
Instead, PBIL models the population using a probability vector p(x) of size ℓ:
p (x) = (pt (x1) , pt (x2) , . . . , pt (xℓ))
(1)
where pt(xi) refers to the probability of ﬁnding a 1 in the ith position of Dt, the
population of individuals in the tth generation. Thus, PBIL models the popula-
tion using the probability vector p(x).
The algorithm samples the distribution deﬁned by p(x) generating m indi-
viduals. These individuals are evaluated using the ﬁtness function. Then, the
best n individuals (n ≤m) are selected. These individuals can be denoted, as
shown in [5], by:
xt
1:m, . . . , xt
i:m, . . . , xt
n:m
(2)
Then, p(x) is updated using these selected individuals. These update is done
using the following rule:
pt+1 (x) = (1 −α) pt (x) + α 1
n
n

k=1
xt
k:m
(3)
where α is a parameter that controls the learning rate.
2.2
Induction of Decision Trees (ID3)
The induction of decision trees was born in the machine learning community.
The goal is the learning of a concept from a set of illustrative examples, or
supervised learning. The ﬁrst well-known approach to the induction of decision
trees, ID3, was proposed by Quinlan [14]—improved later as C4.5 [15]. ID3 is a
tree inducer based on a divide and conquer strategy. Given a set of examples of
the target concept, the algorithm picks an attribute for building the root node
of the tree. Once the root node is chosen, the data set is divided according to the
values of the selected attribute. For each of these divided data sets, the process
is repeated recursively creating the branches of the root node. The process stops
when all the examples in the data set belong to the same class of the target
concept.
The selection of the splitting attribute becomes critical for the induced deci-
sion tree. ID3 measures the purity of the split introduced, by picking an attribute,
using an information gain heuristic. The goal is to minimize the impurity that
introduces the split, looking to build a compact decision tree. The information
gain heuristic is based on the entropy measure. The entropy of a given set D [17],

Wise Breeding GA via Machine Learning Techniques
1175
relative to a classiﬁcation task of c classes (generalized version of the concept
learning problem from positive and negative examples) is:
Entropy(D) ≡
c

i=1
−pi log2 pi
(4)
where pi is the proportion of D belonging to class i. Using the entropy measure,
the information gain, Gain(D, A), of an attribute A is the expected reduction
in entropy caused by partitioning the examples according to this attribute. The
information gain is then deﬁned as
Gain(D, A) ≡Entropy(D) −

v∈V alues(A)
|Dv|
|D| Entropy(Dv)
(5)
where V alues(A) is the set of all possible values for attribute A, and Dv is the
subset of D for which attribute A has value v (i.e. Dv = {d ∈D|(A(d) = v}).
3
Statistical Learning + Inductive Learning = SI3E
The previous section presented some basic background for SI3E. This section
explains how statistical learning and inductive learning can be mixed in order
to provide a breeding model for GA. As explained in the introduction, the goal
is to create an algorithm that exploits the knowledge that can be mined from
the current population. Inductive learning is going to be in charge of this task,
however, as we will show later, it has some limitations if it is to become the core
of a breeding model for GA. Nevertheless, this limitations can be overcome by
combining inductive learning and statistical learning.
3.1
Knowledge Extraction from a Population Using ID3
The work presented in this paper deals with black-box optimization functions
deﬁned on a binary space, Ω= {0, 1}ℓ. Thus, an individual x of the population
is deﬁned by a binary string, x ∈Ω. The target function is used as a ﬁtness
function, f : Ω→ℜ. Using the ﬁtness function f(x), we can sort the individuals
in a population P. Therefore, we know which are the best and worst individuals.
As proposed in LEM [11], we mark the best individuals as positive examples, P⊕.
In the same way, we can mark the worst individuals as negative examples, P⊖.
These sets contain the best and worst individuals seen so far in the evolutionary
process. This approach diﬀerentiates SI3E from PMBGA and EDAs. The main
reason for this two sets is the result of casting the optimization problems into a
supervised learning problem. Using this approach, we artiﬁcially create a data
set, D = P⊕∪P⊖, from which we can extract knowledge using any supervised
learning algorithm from the machine learning literature. All the instances in
the data set D are individuals of the population. Therefore, we deal with a
binary classiﬁcation task (positive and negative examples) where all ℓattributes

1176
X. Llor`a and D.E. Goldberg
are binary. Then, the goal is to ﬁnd an explicit representation of the diﬀerences
between positive and negative instances—individuals of the population. Since the
classiﬁcation problem to be solved is quite simple, ID3 is suited for inducing a
compact tree, leaving more complex approaches (i.e C4.5 [15]) for further study.
Table 1. Turning the population into a data set D for supervised learning. Using D,
the rules induced by ID3 are: R={0***:⊖, 10**:⊖, 11**:⊕}.
Artiﬁcially created data set D
Rank Genotype f(x) Class Rank Genotype f(x)
Class
Rank Genotype f(x) Class
0
1111
4
⊕
4
1101
3
not used
8
0101
2
⊖
1
1110
3
⊕
5
1110
3
not used
9
1010
2
⊖
2
1110
3
⊕
6
0111
3
not used
10
0001
1
⊖
3
1110
3
⊕
7
1100
2
not used
11
0001
1
⊖
Table 1 shows how a randomly generated population P can be turned into a
data set D for supervised learning. In this example, the size of the individuals
in the population is ℓ= 4, whereas the population size |P| = 12. The ﬁtness
f(x) is computed using the OneMax function [18]. After sorting the population
according to f(x), we split the population into three subsets: the best individuals,
the worst individuals, and the mediocre ones. We will discuss more about how
this split is performed later on. Once we have built D, we used ID3 for inducing a
decision tree, extracting the set of equivalent rules. For further details, please see
[14,15,17]. In the condition part of the rules, 0 or 1 denotes the allele to be used at
the given gene, whereas * indicates that the gene can take any value (i.e. 0 or 1).
The class part of the rules identiﬁes whether the rule describes a characteristic of
a good individual (⊕) or a bad one (⊖). We can take the interpretation of these
rules one step further. They can be seen as a kind of notation of the underlying
schemas [19] for good and bad individuals. This point can be assumed if we agree
that the knowledge mined by ID3 is actually the building blocks of good and bad
individuals.
The rules produced by ID3 are the base of the breeding model used by SI3E.
However, there is a question that must be answered before using such a thing.
The rules produced by ID3 split the binary space of the genotype in a non-
overlapping and recursive way. This means that rules cannot be combined in
any useful way. Moreover, there are a large number of genes marked as * in the
rule. This means that if we pick a rule in order to generate a new good individual,
the genotype will be under-speciﬁed, because the rule is giving no clue about
the value to be substituted in place of the *. This is where PMBGA can help
solve this problem.
3.2
Can PBIL Ideas Help?
The rules obtained using ID3 show the underlying patterns of good and bad
individuals. Nevertheless, there are several approaches to ﬁll the empty genes

Wise Breeding GA via Machine Learning Techniques
1177
described by the rules. PBIL provides a simple one using the positive examples
(P⊕) and the negative examples (P⊖) sets. Using both data sets we can compute
the appearance probability of the alleles, 0 and 1, for each gene. Then, given a
rule produced by ID3, we can ﬁll the unspeciﬁed positions probabilistically.
For choosing an allele for the empty genes we compute two probability vec-
tors, p(x|P⊕) and p(x|P⊖), using the positive and negative examples data sets.
Given an example data set S, the probability vector p(x|S) of size ℓis
p (x|S) = (p (x1|S) , p (x2|S) , . . . , p (xℓ|S))
(6)
where p(xi|S) refers to the probability of ﬁnding a 1 in the ith variable of S.
Thus, combining the rules with the statistical information, we can generate
new good and bad individuals. However, there are still two issues to solve before
we can use this approach for breeding new individuals. They are explained in
the next subsection.
3.3
Fixed Loci and Example Set Formation
Before showing the algorithmic description of SI3E, we present the last two
remaining issues to solve. The ﬁrst one arises from the appearance of genes with
ﬁxed values across P⊕and P⊖. The second is the formation of the data sets,
P⊕and P⊖, using the current population P.
A ﬁxed locus happens when for all the instances of a data set S, the same
allele appears on a given gene. This property can be expressed as
fixed(xi, S) ⇔Sj
xi = Sj+1
xi , ∀j = 1, 2, . . . , |S| −1
(7)
where xi is the ith gene, S the available data set, and Sj the jth instance of the
S data set. The set of ﬁxed locus can be deﬁned as
φ(X, P⊕, P⊖) = {xi ∈X|fixed(xi, P⊕) ∧fixed(xi, P⊖) ∧P⊕
xi ̸= P⊖
xi}
(8)
where P⊕
xi is allele of the ﬁxed position xi in the P⊕data set, as well as P⊖
xi is
the allele of the ﬁxed position xi in the P⊖data set.
Fixed loci mislead ID3 and the information gain heuristic. The problem arises
when the same gene appears as a ﬁxed locus on both P⊕and P⊖sets, but the
ﬁxed locus represent diﬀerent alleles. An example of a data set D with a ﬁxed
locus is: R={ 110 0:⊖, 000 1:⊖, 011 0:⊕, 111 1:⊕} The gene x3 contains the
allele 0 for all the instances in P⊖, whereas the allele 1 is ﬁxed in the P⊕data
set. If we compute the gain using the data set D = P⊖∪P⊕with the ﬁxed gene
x3, we obtain Gain(D, x3) ≡1. Gain(D, x3) is the maximum gain using the four
genes available. Moreover, if we select x3 and split the data set D, we obtain a
perfect description of good and bad individuals in data set D. Therefore, ID3
would stop there and produce two diﬀerent rules, **0*:⊖and **1*:⊕.
Fixed loci turn the breeding process into a PBIL equivalent. Nevertheless,
ﬁxed loci do not provide much breeding information, since they stop ID3 from
exploring linkages among genes. This situation can be avoided by removing the

1178
X. Llor`a and D.E. Goldberg
ﬁxed loci from the set of available genes to explore. This fact does not imply that
these ﬁxed genes are ignored, on the contrary, p(x|P⊕) and p(x|P⊖) contains
enough breeding information for their correct usage.
Another critical point in SI3E is the formation of p(x|P⊕) and p(x|P⊖) data
sets. In this paper we use a simple approach suggested by LEM [11]. As shown in
table 1, we split the current population P, ordered by ﬁtness, into three equally
sized disjoint subsets. The best individuals form P⊕, the worst ones form P⊖,
and the rest are not used. This process looks for obtaining opposite instances of
the concept to be learned (good or bad individuals), removing the regular ones
in order to avoid adding extra noise.
However, if we use this splitting policy strictly, we may be introducing some
noise. We can see in table 1, that the last best individual of P⊕has a ﬁtness
value of 3. There are three more individuals that also have this ﬁtness value
that were not chosen. The same happens with the last worst individual that
form P⊖having a ﬁtness value of 2, leaving 1 individuals with the same ﬁtness
out of the set. These random choices introduce noise in the learning process of
ID3. We softened this splitting policy, adding these equivalent ﬁtness individuals.
However, we still maintain that the ﬁtness of the worst individual in P⊕should
be better than the ﬁtness of the best individual in P⊖.
3.4
Everything in Place: SI3E Algorithms and Their Tuning
The algorithms that implement the evolutionary process of SI3E are presented
in ﬁgures 1 and 2. Inspecting the algorithms, it can be seen that there are two
parameters for tuning SI3E. These parameters are α, the learning rate, and the
population size. In this paper we tune these parameters using previous work.
The ﬁrst one, α, is usually set between [0.1,0.2]. These values are common in the
reinforcement learning community. Please refer to [17,20] for more information.
The other parameter to tune is the population size. Our population sizing
model is based on guarantee successful breeding. That is, we size the population
in terms of P⊕and P⊖. Moreover, if we assume the policy presented in section
3.3, the population size |P| is approximated by |P| ≈3|P⊕|. Thus, the sizing
model for P is obtained determining the sizing model of |P⊕|. The size of P⊕,
relies on the capabilities of the learning algorithms, ID3 and PBIL. In this paper,
we will only focus on the worst case provided by ID3, trying to estimate the lower
bound of |P⊕|. In other words, we need to estimate the minimum size of P⊕that
leads ID3 to learn a competent description of the target concept (characteristics
of good and bad individuals).
Our population sizing problem can be translated into a well-known problem
in the machine learning community. This problem is to determine the number of
instances (samples of the hypotheses space) that a learning algorithm requires
for learning the target concept. Haussler [21] compute this lower bound using
the theoretical framework proposed by Valiant [22]. This computation relies on
the probably approximately correct (PAC) model. PAC learners, like ID3, learn
target concepts from some concept class C, using training examples drawn at
random according to an unknown, but ﬁxed, probability distribution. It requires

Wise Breeding GA via Machine Learning Techniques
1179
SI3E(P)
t ←0
initialize P(t), p(x|P⊕)(t), and p(x|P⊖)(t)
evaluate and sort P(t)
WHILE ¬ end-criteria-satisfied
DO
split P(t) into P⊕and P⊖
compute p(x|P⊕)(t+1) and p(x|P⊖)(t+1)
identify fixed locus φ(X, P⊕, P⊖) using
p(x|P⊕)(t+1) and p(x|P⊖)(t+1)
obtain the rule set R using ID3, φ(X, P⊕, P⊖), P⊕, and P⊖
p(x|P⊖)(t+1) ←(1 −α) · p(x|P⊖)(t)+α · p(x|P⊖)(t+1)
p(x|P⊕)(t+1) ←(1 −α) · p(x|P⊕)(t)+α · p(x|P⊕)(t+1)
breed a new population P(t+1) using
R, p(x|P⊕)(t+1), p(x|P⊖)(t+1), P⊕, and P⊖,t
t ←t+1
evaluate and sort P(t)
DONE
RETURN P
Fig. 1. Algorithm implemented by SI3E.
that the learner (with probability at least [1-δ]) learns an hypothesis that is
approximately (within error ϵ) correct.
Within the setting of the PAC learning model, any consistent learner using
a ﬁnite hypothesis space H where C ⊆H (where |H| = 2ℓin SI3E), with a
probability (1-δ), output a hypothesis within error ϵ of the target concept after
observing m randomly drawn training examples, as long as suﬃcient examples
are provided. This bound is computed [21] as
m ≥1
ϵ

ln 1
δ + ℓln 2

(9)
Equation 9 suggest an interesting result for SI3E, when analyzed from an
asymptotically: m must grow at O(ℓ). Therefore, the number of new individuals
generated, MAX (see ﬁgure 2), should grow linearly to the length ℓof the
individuals in the population; that is, logarithmically to the size of the search
space. For further detail about the PAC model, please refer to [22,21,17].
4
Experiments
In this section we present some preliminary results obtained using SI3E. The
conducted experiments involved SI3E and two diﬀerent functions (OneMax, in-
troduced in section 3, and the concatenation of 4-bit deceptive traps, see [18].)

1180
X. Llor`a and D.E. Goldberg
Breed(P,R,p(x|P⊕),p(x|P⊖),P⊕,P⊖,t)
P(t+1) ←P⊕∪P⊖
i ←0
WHILE (i<MAX)
DO
draw a rule r from R at random
IF (r ∈⊕)
THEN
x ←fill the unspecified possitions of r using p(x|P⊕)
ELSE
x ←fill the unspecified possitions of r using p(x|P⊖)
FI
P(t+1) ←P(t+1)∪{x}
i ←i+1
DONE
RETURN P
Fig. 2. Breeding algorithm used by SI3E.
These experiments were designed for showing the viability of the approach sug-
gested by SI3E, as well as, for studying its scalability. SI3E was tuned as follows.
α was set to 0.2 (please refer to [17,20]). The population size was set using the
results presented in the previous section. The population was parameterized as
follows: MAX=4ℓ, |P⊕| = ℓ, and |P⊖| = 3ℓ, biasing the population towards the
negative instances. The reason for this bias is that negative usually outnumber
the positive ones (i.e 4-bit deceptive traps).
Figures 3(a) and 3(c) shows the distribution of the population evolved by
SI3E solving OneMax and the concatenation of 4-bit deceptive traps. As ex-
plained in section 3, SI3E is optimizing the ﬁtness function in a dual manner
(both maximizing and minimizing it). Thus, the evolved population collapses on
two diﬀerent regions. On the right-hand size, the best ones (highest ﬁtness set
P⊕), whereas on the left-hand size the worst evolved individuals (lowest ﬁtness
set P⊖) can be found. The amount of individuals on each set is the result of the
ratio of good and bad individuals (1:3).
We also conducted some preliminary analysis about the scalability of SI3E.
Figures 3(b) and 3(d) summarizes the obtained results using SI3E solving the
OneMax and the concatenation of 4-bit deceptive traps functions. Each result is
the average of 50 independent runs. SI3E scales linearly in the OneMax problem
as shown in ﬁgure 3(b). These results was not unexpected. PBIL performs in O(ℓ)
in the OneMax problem. Hence, the building block identiﬁcation introduced by
ID3 do not degrade the overall performance.
The building block identiﬁcation, performed by ID3 in SI3E, proves its use-
fulness when solving the concatenation of 4-bit deceptive traps. As mentioned
before, this deceptive function has a clear underlying patterns. Thus, SI3E can

Wise Breeding GA via Machine Learning Techniques
1181
Fig. 3. Results obtained using SI3E. Figures show the results solving two diﬀerent
optimization functions.
learn these underlying patterns, being later exploited in the breeding phase of
the algorithm. Theoretical studies show that simple GA scales exponentially

1182
X. Llor`a and D.E. Goldberg
with the length of individuals when solving deceptive trap functions, while com-
petent GA, like BOA [8], achieve this goal in subquadratic time [18]. Figure 3(d)
summarizes the results obtained. Results suggested polynomial scalability of the
number of evaluations related to the problem size. Empirical results suggested,
at most, O(ℓ3) scalability. Although this boundary is one order of magnitude
bigger than well-known competent GA, these preliminary results need a deeper
analysis. In particular, the population sizing criteria adopted and the artiﬁcial
D data set formation criteria should be studied more deeply.
5
Conclusions
This paper has explored the usage of inductive and statistical learning for im-
proving the breeding process of GA. The paper relies on the existence of learnable
patterns in the population. These patterns are usually the result of the existing
regularities introduced by the ﬁtness function. Thus, using the ﬁtness function,
the individuals can be ranked and split in two diﬀerent subsets. The ﬁrst one
contains the best individuals seen so far in the evolution. Similarly, the second
one is formed by the worst individuals seen so far. As a result of this kind of split-
ting of the population, it can be mined using inductive and statistical learning,
looking for the underlying patterns that describe both sets.
SI3E implements these ideas. Using a sorted population, where the best and
worst individuals have been identiﬁed, SI3E uses ID3, as well as some PBIL ideas,
obtaining the underlying existing schemas in the population. These schemes are
used later to produce a new population of good and bad individuals. Thus, the
breeding phases is wisely guided by these mined patterns. Preliminary results
obtained using SI3E suggest the competence of this machine learning based GA
for function optimization. This fact is also suggested by the empirical analysis
of the scalability of the algorithm, although it should be studied deeply, as
summarized in section 4.
Acknowledgments. This work was supported by the Technology Research,
Education and Commercialization Center (TRECC), a program of the Univer-
sity of Illinois at Urbana-Champaign, administered by the National Center for
Supercomputing Applications (NCSA) and funded by the Oﬃce of Naval Re-
search (N00014-01-1-0175). Funding was also provided the Air Force Oﬃce of
Scientiﬁc Research, USAF, (F49620-00-0163), and the National Science Founda-
tion (DMI-9908252).
References
1. Baluja, S.: Population-based incremental learning: A method for integrating ge-
netic search function optimization and competitive learning.
Technical Report
CMU-CS-94-163, Carnegie Mellon University (1994)

Wise Breeding GA via Machine Learning Techniques
1183
2. Baluja, S.: An empirical comparation of seven iterative and evolutionary func-
tion optimization heuristics. Technical Report CMU-CS-95-193, Carnegie Mellon
University (1995)
3. Juels, A., Wattenberg, M.: Stochastic hillclimbing as a baseline method for eval-
uating genetic algorithms. Technical Report CSD-94-834, Computers Science De-
partment, University of California at Berkeley, USA (1995)
4. M¨uhlenbein, H., Paaß, G.: From recombination of genes to estimation of distri-
butions I. In: Lecture Notes in Computer Science 1411: Parallel Problem Solving
from Nature - PPSN IV, Springer (1996) 178–187
5. Larra˜naga, P., Lozano, J.: Estimation of Distribution Algorithms. GENA 2. Kluwer
Academic Publishers (2002)
6. M¨uhlenbein, H.: The equation for response to selection and its use for prediction.
Evolutionary Computation 5 (1998) 303–346
7. Harik, G., Lobo, F., Goldberg, D.: The compact genetic algorithm. In: Proceedings
of the IEEE Conferrence on Evolutionary Computation, IEEE press (1998) 523–
528
8. Pelikan, M.: Bayesian Optimization Algorithm: from single level to hierarchy. PhD
thesis, University of Illinois at Urbana Champaign, Urbana, Illinois, USA (June,
2002)
9. Pelikan, M., Goldberg, D., Cant´u-Paz, E.: BOA: the Bayesian Optimization Algo-
rithm. In: Proceedings of the Genetic and Evolutionary Computation Conference
(GECCO-99), Morgan Kaufmann (1999) 525–532
10. Michalski, R.: Learnable Evolution: Combining Symbolic and Evolutionary Learn-
ing. In: Proceedings of the 4th International Workshop on Multistrategy Learning,
Decenzano del Garda, Italy. (June 11-14, 1998)
11. Michalski, R.: Learnable Evolution Model: Evolutionary Processes Guided by Ma-
chine Learning. Machine Learning 38 (2000) 9–40
12. Wnek, J., Kaufman, K., Bloedorn, E., Michalski, R.: Inductive Learning System
AQ15c: The Method and User’s Guide.
Reports of the Machine Learning and
Inference Laboratory, MLI 95-4, George Manson University, Fairfax, VA (March,
1995)
13. Kaufman, K., Michalski, R.: The AQ18 Machine Learning and Data Mining Sys-
tem: An Implementation and User’s Guide. Reports of the Machine Learning and
Inference Laboratory, George Manson University, Fairfax, VA (1999)
14. Quinlan, J.R.: Induction of decision trees. Machine Learning 1 (1986) 81–106
15. Quinlan, J.R.: C4.5: programs for machine learning. Morgan Kaufmann (1993)
16. Baluja, S., Caruana, R.: Removing the genetics from standard genetic algorithms.
In: International Conference on Machine Learning, Morgan Kaufmann (1995) 38–
46
17. Mitchell, T.M.: Machine Learning. McGraw Hill (1997)
18. Goldberg, D.E.: The Design of Innovation: Lessons from and for Competent Ge-
netic Algorithms. Kluwer Academic Publishers (2002)
19. Holland, J.H.:
Adaptation in Natural and Artiﬁcial Systems: An Introductory
Analysis with Applications to Biology, Control and Artiﬁcial Intelligence. MIT
Press/Bradford Books edition (1975)
20. Gonzalez, C., Lozano, J., Larranarraga, P.: Analyzing the pbil algorithm by means
of discrete dynamical systems. Complex Systems 12 (2001) 465–479
21. Haussler, D.:
Quantifying inductive bias: AI learning algorithms and Valiant’s
learning framework. Artiﬁcial Intelligence 36 (1988) 177–221
22. Valiant, L.: A theory of the learnable. Communications of ACM 27 (1984) 1134–
1142

Facts and Fallacies in Using Genetic Algorithms
for Learning Clauses in First-Order Logic
Flaviu Adrian M˘arginean
Department of Computer Science, The University of York
Heslington, York YO10 5DD, United Kingdom
flav@cs.york.ac.uk
Abstract. Over the last few years, a few approaches have been proposed
aiming to combine genetic and evolutionary computation (GECCO) with
inductive logic programming (ILP). The underlying rationale is that evo-
lutionary algorithms, such as genetic algorithms, might mitigate the com-
binatorial explosions generated by the inductive learning of rich repre-
sentations, such as those used in ﬁrst-order logic. Particularly, the binary
representation approach presented by Tamaddoni-Nezhad and Muggle-
ton has attracted the attention of both the GECCO and ILP communities
in recent years. Unfortunately, a series of systematic and fundamental
theoretical errors renders their framework moot. This paper critically
examines the fallacious claims in the mentioned approach. It is shown
that, far from restoring completeness to the learner progol’s search
of the subsumption lattice, the binary representation approach is both
overwhelmingly unsound and severely incomplete.
1
Introduction
Over the last few years there has been a surge of interest in combining the ex-
pressiveness aﬀorded by ﬁrst-order logic representations in inductive logic pro-
gramming with the robustness of evolutionary search algorithms [7,20,21,22]. It
is hoped that such hybrid systems would retain the powerful logic programming
formalism and its well-understood theoretical foundations, while bringing in to
the search the versatility of evolutionary algorithms, their inherent parallelism,
and their adaptive characteristics [21].
progol is a ﬁrst-order inductive reasoner widely regarded as state of the
art. Owing to its relative importance, its soundness and completeness have been
the object of numerous theoretical studies [3,8,9,10,14,24]. progol has also been
investigated from the point of view of its tractability [16,17]. Search in ﬁrst-order
logic is notoriously diﬃcult because of the expressive power of the hypotheses
that generates combinatorial explosions.
Owing to these two issues, (in)completeness and (in)tractability, the an-
nouncement by Tamaddoni-Nezhad and Muggleton [20,21,22] that genetic al-
gorithms can solve both problems via a simple binary change of representation
has attracted interest. In this paper we demonstrate that, unfortunately, such
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1184–1195, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Facts and Fallacies in Using Genetic Algorithms
1185
hopes are unfounded. We review Tamaddoni-Nezhad and Muggleton’s aforemen-
tioned approach and show that it is provably ﬂawed at every level. Speciﬁcally,
we consider the following claims by the authors, which are central to their ap-
proach:
Fallacy 1. The proposed binary representation for clauses is novel. It encodes
the subsumption lattice lower bounded by the bottom clause in a compact
and complete way.
Fallacy 2. A fast evaluation mechanism for clauses has been given.
Fallacy 3. The proposed task-speciﬁc genetic operators can be viewed as a form
of reﬁnement, titled genetic reﬁnement [20] or stochastic reﬁnement [21].
Respectively, we show:
Fact 1. The proposed binary representations have been known for some time
[1,2] and shown to be incomplete for subsumption even for function-free lan-
guages. The binary encoding of the subsumption lattice is both incomplete
and unsound. Owing to unsoundness, for even a single shared variable in the
bottom clause, the proposed space of binary representations is 2(n
2)
Bn
times
bigger than the number of valid clauses that it manages to encode, where
n
def
= the number of predicates that share the variable and Bn is the n-th
Bell Number. Therefore the space of binary representations is not compact.
An inﬁnity of good clauses are left out (the encoding is incomplete) and a
huge number of spurious binary strings get in (the encoding is unsound and
noncompact).
Fact 2. The proposed evaluation mechanism for clauses is provably unsound.
Fact 3. The proposed task-speciﬁc genetic operators are not reﬁnement opera-
tors because of their being provably unsound and incomplete.
The errors that we pinpoint in this paper appear to have no easy ﬁx. They
are very fundamental theoretical errors, which undermine the whole binary rep-
resentation approach. We wish to emphasise that the problem of combining
evolutionary computation with ﬁrst-order logic learning is worth investigating,
and in this respect the binary representation attempt is meritorious. However,
the ﬂaws need to be corrected. The paper is organised as follows. In Section 2
we review some preliminaries, such as inductive logic programming and inverse
entailment. In Section 3 we expose the fallacies undermining the binary repre-
sentation approach in correlation with our counter-arguments, while Sections 4
and 5 present the conclusions of this paper.
2
Preliminaries
The reader is assumed to be familiar with the basic formalism of ﬁrst-order
clausal logic. The paragraphs below are intended as brief reminders. A good
general reference for inductive logic programming is [15] and Muggleton’s seminal
paper on progol is [13].

1186
F.A. M˘arginean
2.1
Inductive Logic Programming (ILP)
Under ILP’s normal setting (also called the monotonic setting, or the learning-
from-entailment setting) one considers background knowledge B, positive and
negative examples E+ and E−, and a hypothesis H. B, E+, E−and H are all
ﬁnite sets of clauses (theories). Further restrictions can be applied, for instance
by requiring that B, H, E+, E−are logic programs, rather than general the-
ories, or imposing that positive examples are ground unit deﬁnite clauses and
negative examples are ground unit headless Horn clauses. The central goal is to
induce (learn) H such that the following two conditions are satisﬁed:

B ∧H |= E+
B ∧H ∧E−̸|= 2
This looks rather much like solving a system of inequations (logical entailment
|= is a quasi-order, i.e. reﬂexive and transitive), except that it is a rather com-
plicated one. However, if one only considers the positive examples, in the ﬁrst
instance, the following simpler system is obtained:

B ∧H |= E+
B ∧H ̸|= 2
Progress has been made towards characterising the solutions to this system as
follows:
2.2
Inverse Entailment
Deﬁnition 1 (Subsumption). Let C and D be clauses. Then C θ-subsumes
D, denoted by C ⪰D, if there exists a substitution θ such that Cθ ⊆D (i.e.
every literal in Cθ is also a literal in D).
Deﬁnition 2 (Inverse Entailment). Inverse Entailment is a generic name
for any computational procedure that, given B, E+ as input, will return a bottom
clausal theory ⊥(E+, B) as output, such that the following condition is satisﬁed:
H |= ⊥(E+, B) ⇐⇒

B ∧H |= E+
B ∧H ̸|= 2
, ∀H
Inoue (2001) has provided the only known example of Inverse Entailment in
the general case (under the name of Consequence-Finding). It was hoped that
entailment on the left-hand side might be replaced with subsumption or an-
other decidable quasi-order, as entailment is only semidecidable in the general
case. However, this hope was largely unfulﬁlled. In more restricted settings, the
following results have been obtained:
For H, E restricted to be clauses rather than theories, Yamamoto (1997)
gives a computational procedure that computes a bottom clause ⊥(E+, B) such
that the following condition is satisﬁed:
H ⪰⊥(E+, B) ⇐⇒
H ⪰B E+
H ̸⪰B 2
, ∀H

Facts and Fallacies in Using Genetic Algorithms
1187
Note that entailment has been replaced with the more restricted subsumption
on the left-hand side and Plotkin’s relative subsumption on the right-hand side.
For H, E restricted to be function-free clauses and B a function-free Horn
theory, Muggleton (1998) gives a computational procedure that computes a bot-
tom clause ⊥(E+, B) such that the following condition is satisﬁed:
H ⪰⊥(E+, B)
̸=⇒
⇐=

B ∧H |= E+
B ∧H ̸|= 2
, ∀H
Note that entailment has been replaced with the more restricted subsumption
on the left-hand side but the soundness of Inverse Entailment (=⇒) has been
lost.
For H, E restricted to be function-free Horn clauses and B a function-free
Horn theory, Muggleton (1995) gives a computational procedure that computes
a bottom Horn clause ⊥(E+, B) such that the following condition is satisﬁed:
H ⪰⊥(E+, B)
̸⇐=
=⇒

B ∧H |= E+
B ∧H ̸|= 2
, ∀H
Note that entailment has been replaced with the more restricted subsumption
on the left-hand side but the completeness of Inverse Entailment (⇐=) has this
time been lost. Completeness can be restored to this version if either entailment
is restored on the left-hand side or the unique bottom clause is replaced with
a family {⊥i(E+, B)}i of bottom clauses (subsaturants of the unique bottom
clause computed by Muggleton’s procedure):

i

H ⪰⊥i(E+, B)

⇐⇒

B ∧H |= E+
B ∧H ̸|= 2
, ∀H
Subsumption is, in general, preferred to entailment on the left-hand side since it
is decidable. However, as apparent from before, it only guarantees completeness
and soundness of Inverse Entailment when the general ILP setting is restricted
and multiple bottom clauses are generated.
3
The Binary Representation Approach: Facts and
Fallacies
In the context of the Inverse Entailment procedure discussed in the preceding
section, Tamaddoni-Nezhad and Muggleton consider the case where one has a
function-free bottom Horn clause ⊥(E+, B) and claim that the space of solutions
{H | H ⪰⊥(E+, B), H is a function−free Horn clause} can be described as a
boolean lattice obtained from the variable sharing in the bottom clause according
to a simple procedure (Fig. 1).
3.1
Fallacy 1 — Fact 1
We ﬁrst give a description of the proposed binary representation. In [20,21] the
following deﬁnition is given:

1188
F.A. M˘arginean
Fig. 1. Tamaddoni-Nezhad and Muggleton’s “subsumption lattice” bounded below by
the bottom clause p(U, V ) ←−q(U, X), r(X, V )
Deﬁnition 3 (Binding Matrix). Suppose B and C are both clauses and there
exists a variable substitution θ such that Cθ = B. Let C have n variable occur-
rences representing variables ⟨v1, v2, . . . , vn⟩. The binding matrix of C is an n×n
matrix M in which mij is 1 if there exist variables vi, vj and u such that vi/u
and vj/u are in θ and mij is 0 otherwise. We write M(vi, vj) = 1 if mij = 1
and M(vi, vj) = 0 if mij = 0.
This deﬁnition is unsound because the binding matrix of C is deﬁned with
respect to an arbitrary clause B. It is obvious that such a binding matrix may not
be unique. We therefore assume that the authors meant B to be a ﬁxed bottom
clause and the binding matrix of C was deﬁned with respect to this ﬁxed bottom
clause. Let us consider the bottom clause p(U, V ) ←−q(U, X), r(X, V ) in Fig. 1.
Using the equality predicate we can re-write the clause as follows:
p(X1, X2) ←−q(X3, X4), q(X5, X6), X1 = X3, X2 = X6, X4 = X5
We note that the variable sharing in the bottom clause is now completely de-
scribed by the three equalities. Any other clause in Fig. 1 can be re-written as
a combination of the common factor p(X1, X2) ←−q(X3, X4), q(X5, X6) and a
subset of the three equalities {X1 = X3, X2 = X6, X4 = X5} that describe
the variable sharing in the bottom clause. For instance, clause p(U, V ) ←−
q(U, X), r(Y, Z) will become:
p(X1, X2) ←−q(X3, X4), q(X5, X6), X1 = X3

Facts and Fallacies in Using Genetic Algorithms
1189
It is now clear that we do not need the common factor, every clause in Fig. 1
being describable by simply noting which of the three equalities in the bottom
clause it sets oﬀ. If we use the binary string (1, 1, 1) to indicate that the bottom
clause satisﬁes all three equalities, then the second clause above may be encoded
as (1, 0, 0). This is the binary representation approach, an instantiation of a
technique more commonly known as propositionalisation. This approach was
ﬁrst investigated rigorously within the European ILP2 project supported by
ESPRIT Framework IV, which ended in 1999. The deliverables of the ILP2
project [1,2] showed clearly that the approach could not yield completeness for
subsumption, not even in the simple case of function-free (Datalog) languages
[1]. We now show that this is indeed the case for subsumption lattices bounded
below by bottom clauses.
Binary Representation Space is Incomplete. The following clauses are
missing from Tamaddoni-Nezhad and Muggleton’s subsumption lattice, as
pictured in Fig. 1:
←−, the empty clause, subset of the bottom clause
p(U, V ) ←−, subset of the bottom clause
q(U, X) ←−, subset of the bottom clause
r(X, V ) ←−, subset of the bottom clause
p(U, V ) ←−q(W, X), maps into the bottom clause by substitution θ = {W/U}
p(U, V ) ←−r(X, Z), maps into the bottom clause by substitution θ = {Z/V }
←−q(U, X), r(Y, V ), maps into the bottom clause by substitution θ = {Y/X}
These clauses, together with the ones in Fig. 1, are the ones that weakly
subsume the bottom clause1, i.e. those that map literals injectively to the bot-
tom clause. In addition, an inﬁnity of other clauses that subsume p(U, V ) ←−
q(U, X), r(X, V ) are also missing, for instance: p(U, V ) ←−{q(U, Xi), r(Xj, V ) |
i ̸= j, 1 ≤i, j ≤n} for n ≥2, which maps onto the bottom clause by substi-
tution {Xi/X}1≤i≤n. We may wonder whether completeness has instead been
achieved under subsumption equivalence, i.e. one clause from each equivalence
class under subsumption is present in the search space. However, this is not
the case: neither of the exempliﬁed missing clauses are subsume-equivalent with
any of the clauses already in the search space, nor are they subsume-equivalent
between themselves.
The quasi-order used in Fig. 1 is therefore not subsumption but the much
weaker atomic subsumption ⪰a as deﬁned in [15, p. 244]. If, as before, we denote
entailment by |=, subsumption by ⪰, weak subsumption by ⪰w and the atomic
subsumption by ⪰a, we have the following relationship between the four orders:
⪰a
̸⇐=
=⇒⪰w
̸⇐=
=⇒⪰
̸⇐=
=⇒|=
Consequently, we have:
1 See [3] for a complete and sound encoding of the weak subsumption lattice with a
reﬁnement operator.

1190
F.A. M˘arginean
H⪰a ⊊H⪰w ⊊H⪰⊊H|=
where
H⪰i
def
= {H | H ⪰i ⊥(E+, B), H is a function−free Horn clause}
for ⪰i∈{⪰a, ⪰w, ⪰, |=}. progol’s existing reﬁnement encodes H⪰w, which is
incomplete with respect to subsumption: H⪰w ⊊H⪰. However, the binary
representation approach only captures H⪰a, which is even more incomplete:
H⪰a ⊊H⪰w.
Binary Representation Space is Unsound. As mentioned before, the in-
completeness of propositionalisation for subsumption is known. Now we show
that the binary representation is also unsound, i.e. a counter-example may be
given wherein the binary representation codiﬁes binary strings that do not cor-
respond to any clause in the subsumption lattice. The bottom clause chosen in
Fig. 1 is well-behaved, in the sense that all three equalities involve distinct vari-
ables. This was the original example given in [21]. Let us consider, however, the
following bottom clause:
p(U, U) ←−q(U, X), r(Y, Z)
Using equality to re-write the clause we arrive at:
p(X1, X2) ←−q(X3, X4), q(X5, X6), X1 = X2, X1 = X3, X2 = X3
If we describe this bottom clause by (1, 1, 1) as the binary representation ap-
proach prescribes, then (1, 1, 0), (1, 0, 1) and (0, 1, 1) will correspond to no clause.
Because of the transitivity of equality, when one has X1 = X2, X1 = X3 for in-
stance, one will also have X2 = X3. In other words, a certain number of binary
strings will be spurious. Interestingly, in [21] deﬁnitions 3, 4 and 5 capture pre-
cisely the space of binary strings that are mapped to valid clauses, i.e. those
corresponding to normalised binding matrices or, in the language of this paper,
those subsets of binary equalities that are closed under the transitivity of equal-
ity. However, it appears from [20,21] that the authors’ encoding of the search
space used in implementations is not the set of normalised binding matrices or
corresponding normalised strings but the unsound space of all boolean binary
strings. Now we show that the number of such spurious strings can grow wildly
compared to the number of valid clauses encodable by this approach.
Binary Representation Space is Not Compact. Let us consider a bottom
clause that has n predicates, all sharing the ﬁrst variable. For simplicity we
assume that all the other variables are distinct:
p0(X, . . . ) ←−p1(X, . . . ), p2(X, . . . ), . . . , pn−1(X, . . . )
Using equality to re-write the clause we arrive at:
p0(X0, . . . ) ←−p1(X1, . . . ), . . . , pn−1(Xn−1, . . . ), X0 = X1 · · · = Xn−1

Facts and Fallacies in Using Genetic Algorithms
1191
Note that we have slightly abused the notation in order to write the clause more
compactly. The number of binary equalities X0 = X1, X0 = X2, . . . etc. is now
n
2

, which will yield 2
n(n−1)
2
binary strings upon encoding. On the other hand,
the number of clauses that can be obtained by anti-uniﬁcation of variables (as we
have seen, Tamaddoni-Nezhad and Muggleton do not consider adding/dropping
literals, thereby generating incompleteness) is given by the n-th Bell number, i.e.
the number of all partitions of the set of variables {X0, . . . , Xn−1}. The space of
encodings will therefore be 2(n
2)
Bn
times bigger than the number of valid clauses
that it encodes. To get a feeling for this diﬀerence, for n = 12 the space of
encodings will contain
266 = 73786976294838206464
binary strings, while the space of clauses will contain B12 = 4213597 valid
clauses, i.e. about 1 good encoding for every 17.5 trillion spurious ones. In ILP
it is not uncommon for bottom clauses to contain tens or hundreds of predicates
with complex variable sharing. It can be shown that, as n grows, this gets worse
and worse, i.e. the asymptotic behaviour conﬁrms this tendency: 2(n
2)
Bn
n→∞
−→∞.
Because of the sheer number of spurious binary encodings that parasitise
the representation space even for low values of n, the search domain may be-
come unsamplable. Furthermore, the authors observe in footnote 4 of [21, p.
642] and paragraph following Example 2 in [20, p. 249] that certain operations
on binding matrices may result in matrices that are not normalised, this lead-
ing to unsoundness (‘inconsistency’ in their language). They say that this does
not aﬀect the genetic search in practice and could be avoided by normalisation
closure. However, this is not the case. We have shown above that unsoundness
can cause serious computational problems. Normalisation closure, on the other
hand entails itself computational diﬃculties: in order to be able to compute
normalisation closures, one has to revert to variables or keep a separate list of
normalisation rules, valid for the bottom clause at hand. For instance, to in-
fer X2 = X3 from X1 = X2 and X1 = X3 one has to encode the variables.
Otherwise, if X1 = X2, X1 = X3, X2 = X3 are encoded as 3-bit binary strings
(Y1, Y2, Y3), then one needs to know that Y1 = 1 ∧Y2 = 1 =⇒Y3 = 1 etc.
However, the real problem is that, with the proposed encoding, normalisation
closures are syntactically computable but meaningless from a semantic point of
view. Suppose that, as in our example before, we get by random sampling (seed-
ing) the string (1, 1, 0). The 0 bit in the string does not mean that we do not
know that X2 = X3. It means that we know that X2 ̸= X3. This is because the
encoding is made under a Closed World Assumption: it is assumed that a binary
string encodes the variable sharing information completely. If this were not so,
encodings would not represent clauses but subsets of clauses, i.e. all clauses in
the search domain that are compatible with the variable sharing described by
the 1-bits in the binary string. Since a 0 does not reﬂect incomplete information
in the binary string but encodes negative information, normalisation by closure
does not fulﬁll its intended meaning, which is to ﬁll in inferred information.

1192
F.A. M˘arginean
Normalisation by closure remains to be but one of many alternative ways of re-
solving inconsistency. It proceeds by switching the minimum number of 0-bits to
1-bits that allows the consistency rules to be satisﬁed. To resolve inconsistency
we can proceed with any other algorithm that, switching a number of bits, would
achieve consistency under the normalisation rules. For instance, we can decide
that, instead of switching 0-bits to 1-bits, we switch 1-bits to 0-bits. While we
can choose, for instance, that the number of bits so changed be minimal (i.e.
we determine all valid clauses at minimal Hamming distance), or some other
criterion, we do not have any rational basis for preferring one criterion over an-
other. The normalisation by closure is therefore syntactically computable but
semantically undeﬁned.
3.2
Fallacy 2 — Fact 2
Let us consider Example 2 in [21]. It is claimed that the clause p(U, V ) ←−
q(U, X), r(X, Z) is the mgi2 of the clauses p(U, V ) ←−q(U, X), r(Y, Z) and
p(U, V ) ←−q(W, X), r(X, Z) under subsumption ⪰. However, this is not true.
The mgi of the two clauses under ⪰, according to [15, p. 251], is p(U, V ) ←−
q(U, X), q(W, X′), r(Y, Z), r(X′, Z′). We may wonder if this is not subsume-
equivalent with p(U, V ) ←−q(U, X), r(X, Z). Suppose, by reductio ad absurdum,
that the latter clause subsumes the former. Then there should be a substitu-
tion mapping it onto a subset of the former. Necessarily, {U/U, V/V } because
of the common head. Then q(U, X) is mapped to some q(U, ) and the only
such literal available is q(U, X), therefore {X/X}. Then r(X, Z) is mapped to
some r(X, ) but there are no such literals in the former clause, contradiction.
In fact the mgi given in [21] is computed under atomic subsumption ⪰a. Cru-
cially, for mgi’s under ⪰a Theorem 4 in [21] does not hold, i.e. one may not
compute coverages of such mgi’s by intersecting the coverages of the parent
clauses as described in [21]. This is because coverages of clauses are computed
under entailment |= (or, in some circumstances, subsumption ⪰) but not un-
der the much weaker atomic subsumption ⪰a. For instance, consider the clause
p(U, V ) ←−q(U, X), q(W, X′), r(Y, Z), r(X′, Z′) referred to above. Under ⪰it is
covered by both p(U, V ) ←−q(U, X), r(Y, Z) and p(U, V ) ←−q(W, X), r(X, Z),
therefore it belongs to the intersection of their coverages. However, as shown, it
is not covered by the clause p(U, V ) ←−q(U, X), r(X, Z), the alleged mgi. This
renders the proposed fast evaluation mechanism unsound and therefore moot.
3.3
Fallacy 3 — Fact 3
According to the theory of reﬁnement in ILP [11,15], reﬁnement operators main-
tain the search space implicitly. Various qualities may be required of any well-
deﬁned reﬁnement operator, such as soundness, completeness (weak or strong),
local ﬁniteness, properness, minimality etc. Soundness and completeness are min-
imal requirements. However, we have seen in the preceding sections that the
2 most general instantiation

Facts and Fallacies in Using Genetic Algorithms
1193
search space explored by the task-speciﬁc genetic operators in [20,21] is both
unsound and incomplete. The most we could hope for is that such operators,
once applied to valid encodings, will produce a valid encoding. However, this is
not the case. Both mgi and mutation may produce spurious outcomes starting
from valid parent strings, for instance taking mgi of (1, 0, 0) and (0, 1, 0) in our
running counter-example will give the inconsistent (1, 1, 0), while 1-bit mutation
applied to (1, 0, 0) will yield two inconsistent strings and a consistent one.
4
Potential Solutions
This paper was initially written and reviewed in a two-part format, containing
both our criticism of the binary representation approach and our alternative
thereto. Unfortunately, for reasons strictly connected with the 12-page limit
allowed for full papers in the conference proceedings, the second part had to
be dropped. The author plans to submit an extended version of this paper to
a journal in the Artiﬁcial Intelligence ﬁeld, including our proposed solution to
the binary representation ﬂaws described in this paper. The interested reader is
invited to contact the author for details.
5
Conclusions
Although combining genetic algorithms with inductive logic programming is po-
tentially a valuable approach, it is not straightforward. Inductive logic program-
ming owes its existence both to the ﬁrst-order representations that it uses, but
also to the recognition of the fact that it employs search mechanisms that are not
easily translatable in the propositional domain. Even when such transformations
can be made, it is usually under heavy restrictions or at the expense of expo-
nential blow-ups in complexity [6]. It is why the inductive logic programming
community has painstakingly developed inductive mechanisms that work with
ﬁrst-order representations directly. Particularly, reﬁnement operators deﬁned on
clauses lie at the core of many inductive logic programming systems.
Tamaddoni-Nezhad and Muggleton have proposed [20,21] an approach that
recycles some well-known propositionalisation techniques under the name of bi-
nary representations. Although it is claimed that this approach achieves com-
pleteness in respect of progol’s search, it is in fact even more incomplete than
progol’s existing search. Although they use the phrases “genetic reﬁnement”
in [20] and “stochastic reﬁnement” in [21] to name this type of search, no def-
inition of these terms is given and no explanation on how they relate to the
very consistent body of research on reﬁnement [11,12,15,23]. At present it is not
possible to determine with enough precision how the ﬂaws in their theoretical
framework aﬀect their implementation since they do not give the algorithm on
which the implementation is based, nor enough detail regarding their experi-
mental settings. It is therefore not possible to either replicate or criticise their
experimental evidence. However, in [20] the authors state:

1194
F.A. M˘arginean
“In our ﬁrst attempt, we employed the proposed representation to
combine Inverse Entailment in c-progol4.4 with a genetic algorithm.
In this implementation genetic search is used for searching the subsump-
tion lattice bounded below by the bottom-clause (⊥). According to The-
orem 3 the search space bounded by the bottom clause can be
represented by S(⊥).”
However, a careful consideration of Theorem 3 given in their paper will show
that the theorem’s statement does not entail the identity between H⪰and S(⊥).
Furthermore, the argument can not be repaired: S(⊥) is the space of binary
representations shown in this paper to be incomplete, unsound, and noncompact
with respect to H⪰. Since, by the results of this paper, the statement quoted
above does not hold, we should conjecture that caution is needed in accepting the
authors’ alleged experimental results until such time that the ﬂaws unearthed in
this paper are given a proper solution.
Acknowledgements. Thanks to James Cussens for proofreading a preliminary
version of this paper. The author thanks the four anonymous reviewers for their
interest, careful evaluation and commentary, part of which is going to be used in
future extensions of this work. This paper is dedicated to my maternal grandpar-
ents: Maria Icl˘anzan (Buni) and Andrei Icl˘anzan (Bunicu’), who taught me the
value of imagination and rational endeavour, respectively; and to the memory
of my departed paternal grandparents, Anica M˘arginean and Ioan M˘arginean,
who gave me my name and taught me the meaning of faith and dignity in face
of adversity.
References
1. ´E. Alphonse and C. Rouveirol. Object Identity for Relational Learning. Technical
report, LRI, Universit´e Paris-Sud, 1999. Supported by ESPRIT Framework IV
through LTR ILP2.
2. ´E. Alphonse and C. Rouveirol. Test Incorporation for Propositionalization Methods
in ILP. Technical report, LRI, Universit´e Paris-Sud, 1999. Supported by ESPRIT
Framework IV through LTR ILP2.
3. L. Badea and M. Stanciu. Reﬁnement Operators Can Be (Weakly) Perfect. In
S. Dˇzeroski and P. Flach, editors, Inductive Logic Programming, 9th International
Workshop, ILP-99, volume 1634 of Lecture Notes in Artiﬁcial Intelligence, pages
21–32. Springer, 1999.
4. J. Cussens and A. Frisch, editors. Inductive Logic Programming—ILP 2000, Pro-
ceedings of the 10th International Conference on Inductive Logic Programming,
Work-in-Progress Reports, Imperial College, UK, July 2000.
Work-in-Progress
Reports.
5. J. Cussens and A. Frisch, editors. Inductive Logic Programming—ILP 2000, Pro-
ceedings of the 10th International Conference on Inductive Logic Programming,
volume 1866 of Lecture Notes in Artiﬁcial Intelligence, Imperial College, UK, July
2000. Springer.

Facts and Fallacies in Using Genetic Algorithms
1195
6. L. De Raedt. Attribute-Value Learning versus Inductive Logic Programming: The
Missing Links. In Page [18], pages 1–8.
7. F. Divina and E. Marchiori.
Knowledge Based Evolutionary Programming for
Inductive Learning in First-Order Logic. In Spector et al. [19], pages 173–181.
8. K. Furukawa and T. Ozaki. On the Completion of Inverse Entailment for Mutual
Recursion and its Application to Self Recursion. In Cussens and Frisch [4], pages
107–119.
9. K. Inoue. Induction, Abduction and Consequence-Finding. In C. Rouveirol and
M. Sebag, editors, Inductive Logic Programming—ILP 2001, volume 2157 of Lec-
ture Notes in Artiﬁcial Intelligence, pages 65–79. Springer, 2001.
10. K. Ito and A. Yamamoto. Finding Hypotheses from Examples by Computing the
Least Generalization of Bottom Clauses.
In S. Arikawa, editor, Proceedings of
Discovery Science, volume 1532 of Lecture Notes in Artiﬁcial Intelligence, pages
303–314. Springer-Verlag, 1998.
11. P.R. Laag. An Analysis of Reﬁnement Operators in Inductive Logic Programming.
Technical Report 102, Tinbergen Institute Research Series, 1995.
12. Philip D. Laird. Learning from Good Data and Bad. PhD thesis, Yale University,
1987.
13. S. Muggleton. Inverse entailment and Progol. New Generation Computing, 13:245–
286, 1995.
14. S. Muggleton. Completing Inverse Entailment. In Page [18], pages 245–249.
15. S-H. Nienhuys-Cheng and R. de Wolf. Foundations of Inductive Logic Program-
ming. Springer-Verlag, Berlin, 1997. LNAI 1228.
16. K. Ohara, N. Babaguchi, and T. Kitahashi. An Eﬃcient Hypothesis Search Algo-
rithm Based on Best-Bound Strategy. In Cussens and Frisch [4], pages 212–225.
17. H. Ohwada, H. Nishiyama, and F. Mizoguchi. Concurrent Execution of Optimal
Hypothesis Search for Inverse Entailment. In Cussens and Frisch [5], pages 165–
173.
18. D. Page, editor.
Inductive Logic Programming, Proceedings of the 8th Interna-
tional Conference, ILP-98, volume 1446 of Lecture Notes in Artiﬁcial Intelligence.
Springer, July 1998.
19. L. Spector, E.D. Goodman, A. Wu, W.B. Langdon, H-M. Voigt, M. Gen, S. Sen,
M. Dorigo, S. Pezeshk, M.H. Garzon, and E. Burke, editors. Proceedings of the
Genetic and Evolutionary Computation Conference, GECCO-2001, San Francisco,
CA, July 7–11 2001. AAAI, Morgan Kaufmann.
20. A. Tamaddoni-Nezhad and S.H. Muggleton. Searching the Subsumption Lattice
by a Genetic Algorithm. In Cussens and Frisch [5], pages 243–252.
21. A. Tamaddoni-Nezhad and S.H. Muggleton. Using Genetic Algorithms for Learning
Clauses in First-Order Logic. In Spector et al. [19], pages 639–646.
22. A. Tamaddoni-Nezhad and S.H. Muggleton. A Genetic Algorithms Approach to
ILP. Proceedings of the Twelfth International Conference on Inductive Logic Pro-
gramming, 2002. In Press.
23. F. Torre and C. Rouveirol. Private Properties and Natural Relations in Inductive
Logic Programming. Technical report, LRI, Universit´e Paris-Sud, July 1997.
24. A. Yamamoto. Which Hypotheses Can Be Found with Inverse Entailment? In
N. Lavraˇc and S. Dˇzeroski, editors, Inductive Logic Programming, 7th International
Workshop, ILP-97, volume 1297 of Lecture Notes in Artiﬁcial Intelligence, pages
296–308. Springer, 1997.

Comparing Evolutionary Computation
Techniques via Their Representation
Boris Mitavskiy
Department of Mathematics
University of Michigan
Ann Arbor MI 48109
bmitavsk@umich.edu
Abstract. In the current paper a rigorous mathematical language for
comparing evolutionary computation techniques via their representation
is developed. A binary semi-genetic algorithm is introduced, and it is
proved that in a certain sense any reasonable evolutionary search algo-
rithm can be re-encoded by a binary semi-genetic algorithm (see corol-
laries 15 and 16). Moreover, an explicit bijection between the set of all
such re-encodings and the collection of certain n-tuples of invariant sub-
sets is constructed (see theorem 14). Finally, all possible re-encodings of
a given heuristic search algorithm by a classical genetic algorithm are
entirely classiﬁed in terms of invariant subsets of the search space in
connection with Radcliﬀe’s forma (see [9] and theorem 20).
1
Introduction
Over the past 25 years evolutionary algorithms have been widely exploited to
solve various optimization problems. In order to apply an evolutionary algorithm
to attack a speciﬁc optimization problem, one needs to model the problem in a
suitable manner. That is, one needs to construct a search space Ω(the set whose
elements are all possible solutions to the problem) together with a computable
positive valued ﬁtness function f : Ω→(0, ∞) and an appropriate family of
“mating” and “mutation” transformations. One can say, therefore, that a repre-
sentation of a given problem by an evolutionary algorithm is an ordered 4-tuple
(Ω, F, M, f) where Ωis the search space, F is a family of binary operations
on Ωand M is the family of unary transformations on Ω, that is, M is just
a family of functions from Ωto itself. Intuitively F is the family of mating
transformations: every element of F takes two elements of Ω(the parents) and
produces one element of Ω(the child).1 while M is the family of mutations (or
asexual reproductions) on Ω. For theoretical purposes it is usually assumed that
M is ergodic in the sense that the only invariant subsets under M are the ∅and
1 In general there is no reason to assume that a child has exactly two parents. All of
the results in this paper are valid for the families of m-ary operations on Ω. The
only reason F is assumed to be the family of binary transformations is to simplify
the notation.
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1196–1209, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Comparing Evolutionary Computation Techniques via Their Representation
1197
the entire search space Ω. (The ergodicity assumption ensures that the Markov
process modelling the algorithm is irreducible ( see, for instance, [4] ). A typical
evolutionary algorithm works as follows: A population P =





x1
x2
...
x2m




with xi ∈Ω
is selected randomly. The algorithm cycles through the following stages:
Evaluation:
Individuals of P are evaluated:





x1
x2
...
x2m





→f(x1)
→f(x2)
...
...
→f(x2m)
Selection:
A new population
P ′ =





y1
y2
...
y2m





is obtained where yi = xj with probability
f(xj)
Σ2m
l=1f(xl).
In other words, all of the individuals of P ′ are these of P, and the expectation
of the number of occurrences of any individual of P in P ′ is proportional to the
number of occurrences of that individual in P times the individual’s ﬁtness value.
In particular, the ﬁtter the individual is, the more copies of that individual are
likely to be present in P ′. On the other hand, the individuals having relatively
small ﬁtness value are not likely to enter into P ′ at all. This is designed to imitate
the natural survival of the ﬁttest principle.
Partition:
The individuals of P ′ are partitioned into m pairwise disjoint couples for
mating according to some probabilistic rule: For instance the couples could be
Q1 =
yi1
1
yi1
2

Q2 =
yi2
1
yi2
2

. . . Qj =
	
yij
1
yij
2

. . . Qm =
	
yij
1
yij
2

Reproduction:
Replace every one of the selected couples Qj =
	
yij
1
yij
2

with the couples
Q′ =
	
T1(yij
1, yij
2)
T2(yij
1, yij
2)


1198
B. Mitavskiy
for some couple of transformations (T1, T2) ∈F2. The couple (T1, T2) is se-
lected according to a ﬁxed probability distribution on F2. This gives us a new
population
P ′′ =





z1
z2
...
z2m





Mutation:
Finally, with small probability we replace zi with F(zi) for some randomly
chosen F ∈M. This, once again, gives us a new population P ′′′ =





w1
w2
...
w2m





Upon completion of mutation start all over with the initial population P ′′′.
The cycle is repeated a certain number of times depending on the problem.
A more general and extensive description is given in [18]. The importance of
choosing a reasonable representation for a speciﬁc problem is emphasized in
some of the modern research. See, for instance, [10]. A few special evolutionary
algorithms are introduced in the next section.
2
Special Evolutionary Algorithms
Classical Genetic Algorithm with Masked Crossover:
Let Ω= n
i=1 Ai. For every subset M ⊆{1, 2, . . . , n}, deﬁne a binary oper-
ation LM on Ωas follows:
LM(a, b) = (x1, x2, . . . , xi, . . . , xn)
where a = (a1, a2, . . . , an) and b = (b1, . . . , bn) ∈S and xi =

ai
if i ∈M
bi
otherwise.
The reader will recognize LM as a masked crossover operator with mask M.
Let F = {LM | M ⊆{1, 2, . . . , n}}. That is, F in this example is simply the
family of masked crossover transformations. The probability distribution on the
set F2 is concentrated on the pairs of the form (LM, L ¯
M) where ¯
M denotes the
complement of the set M in {1, 2, . . . , n}. Most often the pairs are equally likely
to be chosen from that diagonal-like subset.
Example: Let n = 5 and Ai = {0, 1, . . . , i+1}. Suppose a given population
P consists of 6 individuals which are the rows of the matrix








2 3 4 5 6
0 1 2 3 4
1 2 3 4 5
0 0 1 2 3
1 1 0 1 2
1 2 1 5 4









Comparing Evolutionary Computation Techniques via Their Representation
1199
Say, after selection stage is complete one obtains the following population
P ′ =








2 3 4 5 6
2 3 4 5 6
1 2 3 4 5
0 0 1 2 3
0 1 2 3 4
1 2 3 4 5








Now the following individuals are paired for mating: (masked crossover in this
case)
Q1 =

2 3 4 5 6
0 0 1 2 3

, Q2 =

2 3 4 5 6
1 2 3 4 5

, and Q3 =

0 1 2 3 4
1 2 3 4 5

Suppose we have chosen the masks M1 = {1, 4, 5}, M2 = {1, 2} and M3 = {3, 4}
for the crossover of pairs Q1, Q2 and Q3 respectively. In the language of this
paper it means we have chosen the pairs of transformations (TM1, T ¯
M1) for Q1,
(TM2, T ¯
M2) for Q2 and (TM3, T ¯
M3) for Q3 respectively. Upon applying these we
obtain
Q1 →

TM1((2, 3, 4, 5, 6), (0, 0, 1, 2, 3))
T ¯
M1((2, 3, 4, 5, 6), (0, 0, 1, 2, 3))

=

2 0 1 5 6
0 3 4 2 3

Likewise
Q2 →

TM2((2, 3, 4, 5, 6), (1, 2, 3, 4, 5))
T ¯
M2((2, 3, 4, 5, 6), (1, 2, 3, 4, 5))

=

2 3 3 4 5
1 2 4 5 6

and, ﬁnally,
Q3 →

TM3((0, 1, 2, 3, 4), (1, 2, 3, 4, 5))
T ¯
M2((0, 1, 2, 3, 4), (1, 2, 3, 4, 5))

=

1 2 2 3 5
0 1 3 4 4

The family of mutation transformations, M in this (and in all of the fol-
lowing examples) consists of the transformations Ma : Ω→Ωwhere a ∈

S⊆{1, 2,... ,n}

i∈S Ai so that a = (ai1, ai2, . . . , aik) for i1 ≤i2 ≤. . . ≤ik ∈
Sa ⊆{1, 2, . . . , n} deﬁned as follows: ∀x = (x1, x2, . . . , xn) ∈Ωwe have
Ma(x) = y = (y1, y2 . . . , yn) where yq =

aq
if q = ij for some j
xq
otherwise
In other
words, Ma simply replaces the qth coordinate of its argument with aq ∈Ai
whenever q ∈Sa.
Random Respectful Recombination
This type of algorithm ﬁrst appeared in [9] under the name of Random Re-
spectful Recombination, but it didn’t seem to be useful at ﬁrst2. Here the search
space Ωand the family of mutation transformations, M, are exactly the same as
2 Recently a variation of this technique, known as “gene pool recombination” has been
considered in [16], [7] and [19]

1200
B. Mitavskiy
in the example of classical genetic algorithm, and the family of mating transfor-
mations is described below: In [8] these were named Holland transformations (be-
cause their corresponding ﬁxed family of subsets is precisely the collection of sub-
sets of Ωdetermined by the classical Holland schemata together with the empty
set. See examples following corollary 12 in the next section). For every given
point u = (u1, u2, . . . , un) ∈Ωdeﬁne a Holland transformation Tu : Ω2 →Ωas
follows: for every a = (a1, a2, . . . , an) and b = (b1, b2, . . . , bn) ∈S
Tu(a, b) = (x1, x2, . . . , xn)
where
xi =

ai if ai = bi
ui otherwise
In other words, if the ith coordinates of a and b coincide, then the ith coordinate
of Tu(a, b) also coincides with them. If, on the other hand, the ith coordinates
of a and b diﬀer, then the ith coordinate of Tu(a, b) is that of u, namely, ui.
Let F = {Tu | u ∈S} be the family of all Holland transformations. At every
iteration of the algorithm, once a new population P is obtained, a new probability
distribution on F is deﬁned: Tu is chosen from F so that ui occurs in u with
the probability proportional to its ﬁtness in P. Every transformation in the pair
(Tu, Tv) is chosen independently.
Binary Genetic Algorithm with Masked Crossover:
When every Ai = {0, 1} (which means that Ω= {0, 1}n) in the example
above, one obtains the classical binary genetic algorithm.
Binary Random Respectful Recombination
The search space Ωand the family of mating transformations F and the
family of mutations M are exactly the same as these for the binary genetic al-
gorithm with masked crossover described above. The only diﬀerence is that the
probability distribution on F2 is now completely uniform. (rather than being
concentrated on the diagonal-like subset described in the classical genetic algo-
rithm example) For instance, if n = 5, M1 = {2, 3, 4}, M2 = {1, 3, 5} and the
pair (TM1, TM2) is selected for mating, we have, for instance,

1 0 0 1 1
1 1 0 0 1

−→

TM1((1, 0, 0, 1, 1), (1, 1, 0, 0, 1))
TM2((1, 0, 0, 1, 1), (1, 1, 0, 0, 1))

=

1 1 0 0 1
1 1 0 0 1

This type of a binary search algorithm can be classiﬁed by the following property:
If both parents have a 1 in the ith position then the oﬀspring also has a 1 in
the ith position. Likewise, if both parents have a 0 in the ith position then the
oﬀspring also has a 0 in the ith position. If, on the other hand, the alleles of the
ith gene don’t coincide, then the ith allele could be either a 0 or a 1.
The following type of algorithm may seem useless at ﬁrst. Its importance will
become clear in the next section when we present the binary embedding theorem
which shows that the binary semi-genetic algorithm (described below) possesses
an interesting universal property.

Comparing Evolutionary Computation Techniques via Their Representation
1201
Binary Semi-genetic Algorithm:
The search space Ω= {0, 1}n, just as in the case of the binary genetic
algorithm. The family of mating transformations F is deﬁned as follows: Fix
u = (u1, u2, . . . , un) ∈Ω. Deﬁne a semi-crossover transformation Fu : Ω2 →Ω
as follows: For any given pair (x, y) ∈Ω2 with x = (x1, x2, . . . , xn) and y =
(y1, y2, . . . , yn) we have Fu(x, y) = z = (z1, z2, . . . zn) ∈Ωwhere
zi =

1
if xi = yi = 1
ui
otherwise
In other words, Fu preserves the ith gene if it is equal to 1 in all of the rows
of P, and replaces it with ui otherwise. Let F = {Fu | u ∈Ω} be the family of
all semi-crossover transformations. The family of mutation transformations M
is exactly the same as in the examples above.
Example: With n = 5 and u1 = (0, 1, 1, 0, 1), u2 = (0, 1, 0, 0, 1) we have

1 0 0 1 1
1 1 0 0 1

−→

Fu1 2((1, 0, 0, 1, 1), (1, 1, 0, 0, 1))
Fu2 2((1, 0, 0, 1, 1), (1, 1, 0, 0, 1))

=

1 1 1 0 1
1 1 0 0 1

Notice, that if 1 is present in the ith position of both parents, then it remains
in the ith position of both oﬀsprings. There are absolutely no other restrictions,
though.
In practice the choice of the search space Ωis primarily determined by the
speciﬁc problem and related circumstances. The general methodology for the
construction of the search spaces ﬁrst appeared in the work of Radcliﬀe (see,
for instance, [9]). Radcliﬀe introduced the notion of a forma which captures
the essential properties of the Holland schemata in a representation independent
setting. A forma is simply a partition of the search space into equivalence classes.
A given collection of forma with suitable properties (see [9]) is, in a sense, no
diﬀerent from the collection of the classical Holland schemata provided that one
encodes the search space using the ”genetic representation function” which is also
introduced in [9]. The connection between all of the possible families of mating
transformations on a given search space Ωand the corresponding families of
invariant subsets established in [8] will allow us to extend Radcliﬀe’s notion of
the genetic representation function to compare various evolutionary algorithms
via possible encodings of their search spaces. This idea will be made clear in the
following section.
3
The Binary Embedding Theorem
As we have seen in the introduction, a given evolutionary heuristic search al-
gorithm is determined primarily by the ordered 4-tuple (Ω, F, M, f). In the
current paper we shall only be concerned with the search space Ω, the family of
mating transformations F and the family of mutations M. As mentioned in the
introduction, the family of mutation transformations is ergodic, meaning that

1202
B. Mitavskiy
the only invariant subsets under M is the ∅and the entire search space Ω. This
motivates the following deﬁnitions:
Deﬁnition 1 For a given family of m-ary operations Γ on a set Ω(that is,
functions from Ωm into Ω) a subset S ⊆Ωis invariant under Γ if and only if
∀T ∈Γ we have T(Sm) ⊆S. We shall denote by ΛΓ the family of all invariant
subsets of Ωunder Γ. In other words, ΛΓ = {S | S ⊆Ω, T(Sm) ⊆S ∀T ∈Γ}.
Deﬁnition 2 A heuristic 3-tuple Ω= (Ω, F, M) is a 3-tuple where Ωdenotes
an arbitrary set, F is a family of binary operations on Ω(in other words, a family
of functions from Ω2 to Ω) and M is a family of unary transformations on Ω
(in other words, a family of functions from Ωto itself) such that ΛM = {∅, Ω}.
It is easy to verify (see Proposition A1 of [8]) that the family ΛΓ is closed under
arbitrary intersections and contains Ω. It then follows that for every element
x ∈Ωthere is a unique element of ΛΓ containing x (namely the intersection of
all the members of ΛΓ containing x.)
Deﬁnition 3 Given a heuristic 3-tuple Ω= (Ω, F, M), denote by SΩ
x the small-
est element of ΛF containing x.
The following deﬁnition is a natural extension of the notion of a genetic repre-
sentation function.
Deﬁnition 4 Given two heuristic 3-tuples Ω1 = (Ω1, F1, M1) and Ω2
=
(Ω2, F2, M2), a morphism3 δ : Ω1 →Ω2 is just a function δ : Ω1 →Ω2 which
respects the mating transformations in the following sense: ∀T ∈F1 and ∀x =
(x1, x2) ∈Ω2 ∃Fx ∈F2 such that δ(T(x1, x2)) = F(x1, x2)(δ(x1), δ(x2)).
Analogously, we must have ∀M ∈M1
and ∀x ∈Ω∃Hx ∈M2 such that
δ(M(x)) = Hx(δ(x)). We shall denote by Mor(Ω1, Ω2) the collection of all
morphisms from Ω1 into Ω2.
A morphism δ : Ω1 →Ω2 provides the means for encoding the heuristic 3-tuple
Ω1 by the heuristic 3-tuple Ω2. Unless the underlying function δ is one to one,
there is some nontrivial coarse graining involved. We, therefore have a special
name for these morphisms whose underlying functions are injective.
Deﬁnition 5 We say that a morphism δ : Ω1 →Ω2 is an embedding if the
underlying function δ : Ω1 →Ω2 is one-to-one.
Already at this stage one can see the importance of the family of invariant subsets
ΛF:
Proposition 6 Let δ : Ω1 →Ω2 be a morphism of heuristic 3-tuples. Then
S ∈ΛF2 =⇒δ−1(S) ∈ΛF1. In words, a preimage of an invariant set under a
morphism is invariant.
3 Heuristic 3-tuples along with the morphisms between them do form a mathematical
structure called a Category (see [6] for a detailed exposition). Some properties of the
Category of heuristic k-tuples will be presented in the forthcoming paper.

Comparing Evolutionary Computation Techniques via Their Representation
1203
Proof. Fix S
∈ΛF2. Let (x1, x2) ∈δ−1(S). Then ∀T
∈F1 we have
δ(T(x1, x2)) = F(x1, x2)(δ(x1), δ(x2)) for some F(x1, x2) ∈F2. But S ∈F2 by
assumption so that δ(T(x1, x2)) = F(x1, x2)(δ(x1), δ(x2)) ∈S =⇒T(x1, x2) ∈
δ−1(S). This shows that δ−1(S) is, indeed, invariant under F1.
Although the converse of proposition 6 is not true in general, the mathematical
apparatus developed in Appendix A of [8] allows us to establish a partial converse
of 6. First we need the notion of a composition closed family which is studied in
appendix A of [8]. For the sake of completeness we include the deﬁnition below:
Deﬁnition 7 We say that a given family of m-ary operations Γ on a set Ω(that
is a family of functions from Ωm to Ω) is composition closed if the following two
conditions hold:
1. ∀T0, T1, T2, . . . , Tm ∈Γ the operation T : Ωm →Ωsending any given
x = (x1, x2, . . . , xm) ∈Ωm to T(x) = T0(T1(x), T2(x), . . . , Tm(x)) is also a
member of Γ.
2. S ⊆Ωwe have 
T ∈Γ T(Sm) ⊇S.
Remark 8 Notice that if a given family of mating transformations Γ is pure in
the sense of [9] (meaning that ∀T ∈Γ and ∀x ∈Ωwe have T(x, x, . . . , x) = x.
See also [11] and [8]) then condition 2 of deﬁnition 7 is satisﬁed automatically.
Every one of the families of mating transformations for the algorithms introduced
in section 2 is pure.
It is fairly straightforward to verify that every one of the families of mating trans-
formations involved in the examples of section 2 is composition closed. In fact,
it has been already shown in [8] that the families of masked crossover trans-
formations and gene Holland transformations (these which are convenient for
modelling random respectful recombination) are composition closed (see propo-
sition 2.1 and Theorem 3.6 of [8]). It only remains to show that the family of
semi-crossover transformations is composition closed:
Proposition 9 The family of binary semi-crossover transformations as deﬁned
in the description of the binary semi-genetic algorithm is composition closed.
Proof. Fix arbitrary u, v, w ∈Ω= {0, 1}n. We want to show that the transfor-
mation F : Ω2 →Ωsending any given pair z = (x, y) ∈Ω2 to Tu(Tv(z), Tw(z))
is of the form Tt for some t ∈Ω. It is routine to verify using the deﬁnition that
t = Tu(v, w) does the job.
The following fact justiﬁes the importance of deﬁnition 7:
Proposition 10 Let Ω1 = (Ω1, F1, M1) and Ω2 = (Ω2, F2, M2) denote
heuristic 3-tuples with F2 and M2 being composition closed. Now given any
function δ : Ω1 →Ω2 such that ∀S ∈ΛF2 we have δ−1(S) ∈ΛF1, δ is a
morphism of heuristic 3-tuples.

1204
B. Mitavskiy
Proof. Fix arbitrary x1, x2 ∈Ωand a mating transformation T ∈F1. Our goal
is to ﬁnd a transformation F ∈F2 such that F(δ(x1), δ(x2)) = δ(T(x1, x2)).
Consider the smallest element of ΛF2 containing both, δ(x1) and δ(x2), call
it K{δ(x1), δ(x2)}. (K{δ(x1), δ(x2)} is simply the intersection of all the mem-
bers of ΛF2 containing δ(x1) and δ(x2). Since ΛF2 is closed under arbi-
trary intersections, K{δ(x1), δ(x2)} ∈ΛF2.) Since K{δ(x1), δ(x2)} ∈ΛF2, by as-
sumption δ−1(K{δ(x1), δ(x2)}) ∈ΛF1. But then δ(T(x1, x2)) ∈K{δ(x1), δ(x2)}.
Since F2 is composition closed, by Lemma A.8 of [8], ∃∈F2 such that
F(δ(x1), δ(x2)) = δ(T(x1, x2)) which is exactly what we were after. Notice that
condition δ(M(x)) = Hx(δ(x)) for some Hx ∈M2 is fulﬁlled automatically since
by deﬁnition 2 ΛM = {∅, Ω2} and, by assumption M2 is composition closed so,
by Lemma A.8 of [8], ∀y ∈Ω2 ∃Hy ∈M2 such that Hy(δ(x)) = y.
As noted before, for any family of m-ary operations on Ωthe corresponding
family of invariant subsets ΛΓ is closed under arbitrary intersections. Moreover,
for any function δ : Ω1 →Ω2 the inverse image of the intersection of two subsets
of Ω2 is the intersection of the inverse images of these subsets: δ−1(U ∩V ) =
δ−1(U) ∩δ−1(V ). This motivates the following deﬁnition:
Deﬁnition 11 Given a family of m-ary operations Γ on Ω, we say that a family
of subsets 
ΛΓ ⊆ΛΓ is a base of ΛΓ if for every K ∈ΛΓ there exists a collection
ΛK ⊆
ΛΓ such that K = 
S∈ΛK S. (Equivalently, if K = 
S∈
ΛΓ , S⊃K S).
Corollary 12 Let Ω1 = (Ω1, F1, M1) and Ω2 = (Ω2, F2, M2) denote heuris-
tic 3-tuples with F2 and M2 being composition closed, and δ : Ω1 →Ω2 be a
function. Then the following are equivalent:
1. S ∈
ΛF2 =⇒δ−1(S) ∈ΛF1.
2. S ∈ΛF2 =⇒δ−1(S) ∈ΛF1.
3. δ : Ω1 →Ω2 is a morphism of heuristic 3-tuples.
Proof. An immediate consequence of propositions 6 and 10 together with the
discussion preceding deﬁnition 11.
Below we list the families of invariant subsets together with a naturally chosen
bases for each of the examples presented in section 2.
Classical Genetic Algorithm. In this case, the family of invariant subsets
ΛF = {n
i=1 Ti | Ti ⊆Ai}. This is precisely the family of subsets determined by
Antonisse’s schemata (see corollary 2.4 of [8]). A bases for ΛF is the family 
ΛF =
{n
i=1 Ti | Ti = Ai for all but one i}. The reader can see that | 
ΛF| = 2Σn
i=1|Ai|.
Every element of 
ΛF can be thought of as a union of subsets determined by the
Holland schemata having exactly one ﬁxed position at the same gene.
Random Respectful Recombination. ΛF = {n
i=1 Ti | Ti = {ai} or Ti =
Ai} ∪{∅}. This is precisely the family of subsets determined by the Holland
schemata together with the empty set (see corollary 3.5 of [8]). A bases for ΛF
is the family 
ΛF = {n
i=1 Ti | ∃! with Tj = {aj}. For i ̸= j Ti = Ai}. This is

Comparing Evolutionary Computation Techniques via Their Representation
1205
precisely the family of subsets determined by Holland schemata having exactly
one ﬁxed position.
Binary Semi-genetic Algorithm. It is not hard to verify that ΛF =
{n
i=1 Ti | Ti = {1} or Ti = {0, 1}} ∪{∅}. This is precisely the family of sub-
sets determined by Holland schemata whose ﬁxed positions can only equal to 1
(can’t be equal to 0). A bases for ΛF is the family 
ΛF = {n
i=1 Ti | ∃! with Tj =
{1}. For i ̸= j Ti = {0, 1}} which is precisely the family of subsets determined
by Holland schemata having exactly one ﬁxed position, and that ﬁxed position
is equal to 1.
Corollary 12 allows us to characterize all possible morphisms from various
heuristic 3-tuples to the standard types described in section 2. Our ﬁrst result,
the Binary Embedding Theorem, establishes an explicit one-to-one correspon-
dence between the set of all embeddings of a given heuristic 3-tuple into a binary
semi-genetic algorithm of length n and a certain collection of ordered n-tuples
of Ω-invariant subsets.
Deﬁnition 13 Fix any heuristic 3-tuple Ω= (Ω, F, M). We say that collec-
tion
Υn = {I | I = (I1, I2, . . . , In) Ij ∈ΛΩ, ∀x, y ∈Ωwith x ̸= y ∃1 ≤j ≤n
such that either (x ∈Ij and y /∈Ij) or vise versa: (y ∈Ij and x /∈Ij)}
is a family of separating n-tuples. Notice that Υn ⊆(ΛF)n.
Theorem 14 Fix a heuristic 3-tuple Ω= (Ω, F, M). We now have the fol-
lowing bijection φ : ΛF →Mor(Ω, Sn) (here Sn denotes the binary semi-genetic
heuristic 3-tuple with the search space {0, 1}n, see also deﬁnition 4 for the mean-
ing of Mor(Ω, Sn)) which is deﬁned explicitly as follows: Given an ordered n-
tuple of sets from ΛΩ, call it I = (I1, I2, . . . , In) ∈(ΛF)n let φ(I) = δI where
δI(x) = (x1, x2, . . . , xn) ∈S = {0, 1}n with xj =

1
if x ∈Ij
0
otherwise ∀x ∈Ω. More-
over, δI is an embedding (injective) if and only if I ∈Υn (see deﬁnition 13).
In other words, the restriction of φ to Υn is a bijection onto the collection of all
embeddings of Ωinto Sn.
Proof. Given any map δ : Ω→{0, 1}, for 1 ≤j ≤n let Ij = δ−1(n
i=1 Ti) with
Ti = {0, 1} if i ̸= j and Tj = {1}. Recall from examples following deﬁnition 11
that {n
i=1 Ti | ∃! with Tj = {1}. For i ̸= j Ti = {0, 1}} forms a bases for
the family of subsets invariant under semi-crossover transformations. Therefore,
according to corollary 12, δ : Ω→Sn is a morphism of heuristic 3-tuples if and
only if (I1, I2, . . . , In) ∈(ΛF)n. This shows that φ : ΛF →Mor(Ω, Sn) is a
well-deﬁned bijection. It is routine to check that δI is injective if and only if
I ∈Υn (see deﬁnition 13).
It turns out that the conditions under which a given heuristic 3-tuple can be em-
bedded into a binary semi-genetic heuristic 3-tuple are rather mild and naturally
occurring as the following two corollaries demonstrate:

1206
B. Mitavskiy
Corollary 15 Given a heuristic 3-tuple Ω= (Ω, F, M), the following are
equivalent:
1. Ωcan be embedded into an n-dimensional semi-genetic heuristic k-tuple for
some n.
2. ∀x, y ∈Ωwith x ̸= y we have either x /∈SΩ
y (see deﬁnition 3) or vise versa:
y /∈SΩ
x .
3. ∀x, y ∈Ωwith x ̸= y we have SΩ
x ̸= SΩ
y . (Another way to say this, is that
the map sending x to SΩ
x is one-to-one.)
Moreover, if an embedding exists for some n, then there exists one for n =
|Ω|. We also must have n ≥⌈log2 |Ω|⌉.
Proof. One simply shows that ∀x, y ∈Ωwith x ̸= y we have either x /∈SΩ
y or
y /∈SΩ
x if and only if |Ω|-tuple S = (SΩ
x1, SΩ
x2, . . . , SΩ
x|Ω|) where {xi}n
i=1 is an
enumeration of all the elements of Ωis separating ( i. e. S ∈Υn, see deﬁnition 13
) if and only if Υn ̸= ∅which, in turn, according to theorem 14, happens if and
only if Ωcan be embedded into an n-dimensional semi-genetic heuristic k-tuple
for some n. This establishes the equivalence of 1 and 2. Clearly 2 implies 3. To
see the converse, we show that “Not 2” implies “Not 3”. Indeed, if x ∈SΩ
y and
y /∈SΩ
x , then, by minimality, (see deﬁnition 2) we have SΩ
x ⊆SΩ
y and SΩ
y ⊆SΩ
x ,
so that SΩ
x = SΩ
y .
Corollary 16 Given a heuristic 3-tuple Ω= (Ω, F, M), if for every T ∈F, T
is pure in the sense of [9] ( in other words, ∀x ∈ΩT(x, x) = x ) then Ωcan
be embedded into a binary semi-genetic heuristic k-tuple of dimension less than
or equal to |Ω|.
Proof. The desired conclusion follows immediately from corollary 15 by observ-
ing that ∀x, y ∈Ωwith x ̸= y we have SΩ
x = {x} so that x ∈{x} = SΩ
x while
y /∈{x} = SΩ
x .
Notice that purity by itself is suﬃcient for the existence of an embedding of a
given heuristic 3-tuple into a binary semi-genetic heuristic 3-tuple. Of course,
the embedding may not be surjective by any means. The main virtue of theo-
rem 14 is not so much the results such as corollary 16, but rather the explicit
bijective correspondence between Mor(Ω, Sn) and the collection (ΛF)n. The
main tool involved in the proof of theorem 14 is corollary 12. In the next section
we demonstrate how corollary12 can be applied to establish a similar bijec-
tive correspondence between certain kinds of sequences of Radcliﬀe’s forma and
Mor(Ω, G{Ai}n
i=1) where G{Ai}n
i=1 denotes the heuristic 3-tuple corresponding to
the genetic algorithm with n
i=1 Ai as its underlying search space.
4
Characterizing the Morphisms from a Given Heuristic
3-Tuple into a Genetic Heuristic 3-Tuple in Terms of
Radcliﬀe’s Forma
For the reader’s convenience we restate a few basic notions considered in [9]:

Comparing Evolutionary Computation Techniques via Their Representation
1207
Deﬁnition 17 Given a set Ω, denote by E(Ω) the set of all possible partitions of
Ωinto disjoint nonempty subsets. (Partitions and equivalence relations are in a
natural bijective correspondence. See, for instance [9], or any standard textbook
on basic mathematical structures and concepts for details) Given an element Ξ ∈
E(Ω), the elements of Ξ are called forma. Given an n-tuple Ψ = (Ξ1, Ξ2, . . . , Ξn)
of elements of E(Ω), let Ξ(Ψ) = n
i=1 Ξi. A genetic representation function
ρ : Ω→Ξ(Ψ) sends a given x ∈Ωto (X1, X2, . . . , Xn) ∈Ξ(Ψ) where x ∈Xi
(remember that such an Xi ∈Ξi exists and is unique since Ξi is a partition of
Ωso that ρ is, indeed, well-deﬁned).
The following deﬁnition sets the stage for the main theorem of this section:
Deﬁnition 18 Given a heuristic 3-tuple Ω= (Ω, F, M), and a function δ :
Ω→n
i=1 Ai, let Ψδ = (Xδ
1, Xδ
2, . . . , Xδ
n) where Xj is the collection of all
nonempty preimages under δ of the subsets of n
i=1 Ai which are determined
by the classical Holland schemata having exactly one ﬁxed position in the jth
gene. Explicitly, Xδ
j = {δ−1(n
i=1 Ti) | Tj = {aj} for some aj ∈Aj and Ti =
Ai for i ̸= j} −{∅}.
Deﬁnition 19 To shorten the notation we shall denote by G{Ai}n
i=1 the heuristic
3-tuple representing the classical genetic algorithm, and by P{Ai}n
i=1 the the
heuristic 3-tuple representing the random respectful recombination algorithm
with the underlying search space n
i=1 Ai.
The following theorem is an immediate consequence of corollary 12:
Theorem 20 Given a heuristic 3-tuple Ω= (Ω, F, M), and a function δ :
Ω→n
i=1 Ai, the following are true:
1. δ : Ω→P{Ai}n
i=1 is a morphism of heuristic 3-tuples if and only if ∀1 ≤
j ≤n every forma in Xδ
j (see deﬁnition 18) is invariant under F (if and only
if every forma in Xδ
j is a member of ΛF).
2. δ : Ω→G{Ai}n
i=1 is a morphism of heuristic 3-tuples if and only if ∀1 ≤
j ≤n every union of forma in Xδ
j (see deﬁnition 18) is invariant under F (if
and only if for every subset of forma Y ⊆Xδ
j we have (
S∈Y S) ∈ΛF).
Proof. In case of a random respectful recombination all forma in Xδ
j , and, in case
of a classical genetic algorithm, all unions of forma in Xδ
j are precisely the preim-
ages under δ of the sets in 
ΛΓ where Γ is the family of Holland transformations
in case of random respectful recombination, and the family of masked crossover
transformations in case of a classical genetic algorithm (see examples following
corollary 12). The desired conclusion now follows at once from corollary 12.
The diﬀerence between theorem 20 and results like theorem 25 of [9] is that the-
orem 20 classiﬁes all possible re-encodings of a given evolutionary search algo-
rithm in terms of a given genetic algorithm (or in terms of a “random respectful
recombination”) while Radcliﬀe’s results provide a foundation for designing a
genetic algorithm to model a speciﬁc problem in question.

1208
B. Mitavskiy
5
Conclusions
In the current paper the following contributions have been made:
1. An appropriate notion for comparing evolutionary computation techniques
via their representation (a morphism between heuristic 3-tuples) has been intro-
duced. (See deﬁnition 4)
2. An important connection between the family of invariant subsets of the
search space (see deﬁnition 1) and the morphisms of heuristic 3-tuples has been
established (see corollary 12).
3. A binary semi-genetic algorithm has been introduced and it was shown
that virtually any evolutionary heuristic search algorithm can be embedded into
a binary semi-genetic algorithm (see theorem 14 and corollaries 15 and 16).
4. All possible morphisms (re-encodings and coarse graining) of a particu-
lar evolutionary heuristic search tuple by a classical genetic algorithm, or by a
random respectful recombination have been characterized in terms of Radcliﬀe’s
forma (see theorem 20).
Acknowledgements. I want to thank Professor John Holland for the helpful
discussions and for the encouragement I’ve received from him to write this paper.
I also want to thank my thesis advisor, Professor Andreas Blass for the numerous
helpful advisor meetings which have stimulated many ideas for this and for my
future work. In addition I would like to thank my fellow graduate student of
mathematics, Ronald Walker and the University of Michigan Complex Systems
Group for helpful discussions. Finally, I want to thank Jon Rowe and the other
anonymous referees for the helpful suggestions and remarks they have made.
References
1. Antonisse, J. (1989). A new interpretation of schema notation that overturns the
binary encoding constraint. In Ed. J. D. Schaﬀer Procedings of the Third Interna-
tional Conference on Genetic Algorithms, Morgan Kaufmann, pages 86–97.
2. Booker, L. (1993). Recombination distributions for genetic algorithms. In L. Dar-
rell Whitley, editor, Foundations of Genetic Algorithms 2, pages 29–44, Morgan
Kaufmann.
3. Geiringer, H. (1944). On the probability of linkage in Mendelian heredity. Annals
of Mathematical Statistics, 15:25–57.
4. Coﬀey, S. (1999) An Applied Probabilist’s Guide to Genetic Algorithms. A Thesis
Submitted to The University of Dublin for the degree of Master in Science.
5. Liepins, G. and Vose, M. (1992). Characterizing Crossover in Genetic Algorithms.
Annals of Mathematics and Artiﬁcial Intelligence, 5: 27–34.
6. Mac Lane, S. (1971) Categories for the working mathematician. Graduate Texts in
Mathematics 5, Springer-Verlag.
7. M¨uhleinbeim, H. and Mahnig, T. (2001). Evolutionary computation and beyond.
In Y. Uesaka, P. Kanerva, and H. Asoh, editors, Foundations of Real-World Intel-
ligence, CSLI Publications, pp. 123–188.

Comparing Evolutionary Computation Techniques via Their Representation
1209
8. Mitavskiy B. (Resubmitted). Crossover Invariant Subsets of the Search Space for
Evolutionary Algorithms. Evolutionary Computation.
http://www.math.lsa.umich.edu/∽bmitavsk/
9. Radcliﬀe, N. (1992). The algebra of genetic algorithms. Annals of Mathematics and
Artiﬁcial Intelligence, 10:339–384.
10. Rothlauf F., Goldberg D., Heinzl A. (2002). Network random keys - a tree represen-
tation scheme for genetic and evolutionary algorithms. Evolutionary Computation,
10(1): 75–97.
11. Rowe, J., Vose, M., and Wright, A. (2002). Group properties of crossover and
mutation. Evolutionary Computation, 10(2): 151–184.
12. Stephens, C., Poli, R., Wright, A., and Rowe, J. (2002). Exact results from a
coarse grained formulation of the dynamics of variable-length genetic algorithms.
In W. B. Langdon, E. Cantu-Paz, K. Mathias, R. Roy, D. Davis, R. Poli, K.
Balakrishnan, V. Honavar, G. Rudolph, J. Wegener, L. Bull, M. A. Potter, A. C.
Schultz, J. F. Miller, E. Burke, and N. Jonoska, editors, GECCO 2002: Proceedings
of the Genetic and Evolutionary Computation Conference, pages 578–585, Morgan
Kaufmann Publishers.
13. Stephens, C. (2001). Some exact results from a coarse grained formulation of
genetic dynamics. Proceedings of the Genetic and Evolutionary Computation
(GECCO) conference, pages 631–638, Morgan Kaufmann.
14. Stephens, C. and Waelbroeck, H. (1999). Schemata evolution and building blocks.
Evolutionary Computation, 7(2):109–124.
15. C. Stephens. (2002). The Renormalization Group and the Dynamics of Genetic
systems, to be published in Acta Physica Slovaka. http://arXiv.org/abs/cond-
mat/0210271/.
16. Syswerda, G. (1993). Simulated crossover in genetic algorithms. In L. Darrel Whit-
ley, editor, Foundations of Genetic Algorithms 2, Morgan Kaufmann.
17. Vose, M. (1991). Generalizing the notion of a schema in genetic algorithms. Arti-
ﬁcial Intelligence, 50(3): 385–396.
18. Vose, M. (1999). The simple genetic algorithm: foundations and theory. MIT Press,
Cambridge, Massachusetts.
19. Wright, A., Rowe, J., Poli, R., and Stephens C. (2002). A ﬁxed point analysis
of a gene pool GA with mutation. Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO) Morgan Kaufmann.
http://www.cs.umt.edu/u/wright/.

Dispersion-Based Population Initialization
Ronald W. Morrison
Mitretek Systems, Inc.
3150 Fairview Park Drive South
Falls Church, VA 22043-4519
ronald.morrison@mitretek.org
Abstract. Reliable execution and analysis of an evolutionary algorithm
(EA) normally requires many runs to provide reasonable assurance that
stochastic eﬀects have been properly considered. One of the ﬁrst stochas-
tic inﬂuences on the behavior of an EA is the population initialization.
This has been recognized as a potentially serious problem to the per-
formance of EAs but little progress has been made in improving the
situation. Using a better population initialization algorithm would not
be expected to improve the many-run average performance of an EA,
but instead, it would be expected to reduce the variance of the results,
without loss of average performance. This would provide researchers the
opportunity to reliably examine their experimental results while requir-
ing fewer EA runs for an appropriate statistical sample. This paper uses
recent advances in the measurement and control of a population’s disper-
sion in a search space to present a novel algorithm for better population
initialization. Experimental veriﬁcation of the usefulness of the new tech-
nique is provided.
1
Introduction
Execution and analysis of an evolutionary algorithm (EA) normally requires
many runs to provide reasonable assurance that any negative eﬀects of merely
“bad luck” in the stochastic processes have been overcome. One of the ﬁrst
stochastic inﬂuences on the behavior of an EA is the population initialization.
This has been recognized as a potentially serious problem to the performance
of EAs in [1], and, to a lesser extent, in [2], but little progress has been made
in improving the situation. In the few cases where this situation is addressed
at all, populations, in very low dimensionality problems, have been initialized
using techniques like the Latin Hypercube [2]. The Latin Hypercube technique
guarantees uniform placement along each axis, but uniform placement along
individual axes does not ensure any level of uniformity throughout the search
space.
EAs are generally executed many times to overcome stochastic anomalies,
so using a better population initialization algorithm would not be expected to
improve the average performance of an EA. Instead, it would be expected to
reduce the variance of the results, without loss of average performance. This
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1210–1221, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

Dispersion-Based Population Initialization
1211
would provide researchers the opportunity to reliably examine their experimental
results while requiring fewer EA runs for an appropriate statistical sample.
This paper provides a description of the population initialization problem, an
introduction to low-discrepancy sequences, the derivation of a new and compu-
tationally eﬃcient population initialization algorithm, and experimental results
that demonstrate the usefulness of the algorithm.
2
Problem Description
We desire to improve the search capability of the initial population by spreading
the individual members throughout the search space so that their positioning
maximizes the probability of detection of a landscape feature of interest. Let’s
take a deeper look at more precisely what this means.
In the general case, for any speciﬁc population size, P, in N dimensions, this
is equivalent to solving an N-dimensional sphere packing problem. The problem,
however, is expressed in terms of the packing space size and the number of
spheres, and must be solved for the sphere radius and the coordinates of the
packed spheres. The sphere center coordinates would then be the population
positions. While much research has been done in sphere packing problems [3] [4],
eﬃcient methods for solving the above problem have not been identiﬁed. Note
that we would also like to avoid adding signiﬁcant computational complexity to
our EA.
The population placement problem becomes even more diﬃcult if we wish
to use a variety of population sizes. We would like the ability to initialize dif-
ferent sized population for a problem without needing to re-perform a complex
population initialization algorithm. We would like, therefore, to be able to ap-
propriately add or delete population members to an initialization distribution
that we have already computed.
The identiﬁcation of the largest unoccupied volume for the placement of the
“next” population member in N-space already occupied by a population is an in-
teresting problem. The normal N-dimensional space partitioning techniques such
Delaunay triangulation [5], or other space partitioning methods involving poly-
topes, do not assist in the identiﬁcation of the largest unoccupied N-dimensional
volume. Application of these techniques to our problem could result in an un-
intentional division of the largest unoccupied volume by inappropriate space
partitioning edge placement, rather than point placement.
Another area of research that could be used for placing points in N-
dimensional space includes meshing techniques such as t-m-s nets [6]. As a pow-
erful mechanism for placing points uniformly in N-dimensional space, t-m-s nets
cover space in base b in dimension N, requiring bN points to assure uniform
spatial coverage. Since the process starts by partitioning the space into bN cells
and we wish to use a varying number of points, the mesh-generating techniques
are generally inappropriate for our application.
Since we are trying to place all population members such that the distance
to a any point in the search space is minimized, ideally we would like to place an

1212
R.W. Morrison
additional population member into an existing population at the mid-point of
the largest unoccupied symmetrical volume. What we are in need of is a compu-
tationally eﬃcient algorithm that is better than a random number generator in
eﬃciently placing a population uniformly throughout a search space, occupying
unoccupied volumes ﬁrst. The specialty ﬁeld in mathematics that has done some
exploration into this type of problem is called low-discrepancy sequence genera-
tion [7]. The key idea in this ﬁeld of mathematics is discrepancy measurement,
so ﬁrst we will provide a short introduction to discrepancy measurement.
3
Discrepancy Measurement
Discrepancy measurement identiﬁes how uniformly a set of points sample a
search space. Discrepancy measurement involves concepts that are loosely based
on the same ideas as the Kolomogorov-Smirnov (K-S) test, when used to compare
a distribution of points to the uniform distribution. The K-S test compares an
ordered set of P points Xi to a continuous distribution by computing the diﬀer-
ence between the discrete cumulative distribution of the points with cumulative
distribution of the desired continuous distribution. The hypothesis regarding the
distributed form of the points is rejected if a test statistic, D, is greater than
the critical value.
Unfortunately, the cumulative probability distribution is not well deﬁned in
more than one dimension. So for higher dimensions the common measure of
spatial uniformity is discrepancy. While not providing a test statistic for uni-
formity, discrepancy can be used to compare sequences of points in N-space to
determine which sequence is more uniform. Discrepancy measures consider the
N-dimensional ℜN space modulo 1 or, equivalently, the N-dimensional torus
T N = ℜN
ZN . Instead of identifying the largest diﬀerence between cumulative distri-
butions of points and the desired distribution, as the K-S test does, discrepancy
identiﬁes the largest diﬀerence between the proportion of points included in an
N-dimensional space partition and the proportion of the total volume (Lebesgue
measure) included in that partition, over all possible space partitions.
Discrepancy, DP , of a sequence of points, (xp)p≥1, xp ⊆ℜN, as further elabo-
rated in [7], is useful for comparing the uniformity sequences, because a sequence
is uniformly distributed modulo 1 if and only if:
lim
P →∞DP (xP ) = 0.
(1)
As a mathematically sound method for computing the uniformity of a distri-
bution of points in an N-dimensional space, it is unfortunate that discrepancy
measurement is generally too computationally expensive to be practically em-
ployed in most EA problems involving assessment of the dispersion of points
in a search space. Recently [8] developed a computationally eﬃcient method for
measuring a population’s dispersion throughout a search space, called the disper-
sion index, ∆. This measure is based on on the concepts of discrepancy theory
and provides practical improvements over the more commonly used diversity
measurements.

Dispersion-Based Population Initialization
1213
Applying discrepancy theory concepts to our population initialization prob-
lem, we turn to research in the generation of low-discrepancy sequences. This
research is focused on methods for placing points uniformly in multi-dimensional
spaces. Most of the research into low-discrepancy sequences is involved in estab-
lishing upper and lower bounds on the discrepancy of various sequence generation
algorithms, through algorithmic proofs or example construction.
The research in this ﬁeld yields sequences with the smallest values of dis-
crepancy (in the limit) currently known [9] [10]. There is, however, no evidence
that these sequences will perform well in an application such as ours, where only
a small number of points near the beginning of the sequence will actually be
used. Additionally, our application has computational eﬃciency requirements,
and none of the research appears to adequately address this requirement. In the
following sections, applying the low-discrepancy sequence generation methods,
we derive a computationally eﬃcient, heuristic sequence generation algorithm
for an N-dimensional space that has the attributes we desire.
3.1
Population Placement Basis
For the construction of our population placement algorithm, we used some of the
research on the kα sequence for an irrational α. This is a well-studied sequence
and is known to be uniformly distributed modulo 1 [7].
This sequence has been extended into N-dimensions, through what is known
as the Kroenecker sequence [10]. The Kroenecker sequence is based on the fol-
lowing theorem, from [7]. Let β1, ..., βN ∈ℜ. Then:
Theorem 1:
The N-dimensional sequence: xk = (kβ1, ...kβN) is uniformly distributed modulo
1 if and only if 1, β1, ..., βN are linearly independent over the integers Z.
This is an extension of the kα sequence since, for β1...βN to be lin-
early independent over the integers, there are no integers mi ∈Z such that
1, m1β1 + ... + mNβN = 0. In other words, the β’s must be irrational. So, using
our notation convention, we will refer to these numbers in future references as
α’s (i.e., kα1, ...kαN).
The mathematical study of low-discrepancy sequences is usually focused on
the sequence discrepancy in the limit for large numbers of points. We, however,
aren’t really interested in theoretical convergence for large sequences. We are
interested in the uniformity of the distribution of the ﬁrst few points (perhaps as
high as several hundred) of the sequence. The extreme computational eﬃciency
of the Kroenecker sequence, once the α’s are identiﬁed, makes it very attractive
for our application.
So our contribution here is to devise a set of rules for using the concepts of
the Kroenecker sequence to construct a reasonably uniformly distributed set of
points in N-space for the range of dimensions and range of number of points of
interest for EA applications. We will then use those rules to derive a set of α’s
that EA practitioners and researchers can use. To understand the problem, we

1214
R.W. Morrison
will ﬁrst illustrate that the Kroenecker sequence does not, by itself, necessarily
provide the desired uniformity of the initial points in a sequence. Figure 1 illus-
trates one type of non-uniform initial point distributions that can be generated
a by badly chosen, but irrational, α’s in two dimensions. This sequence will be
uniformly distributed in the long run, but is a very bad choice with respect to
the initial few points.
Fig. 1. 50 Kroenecker Sequence Points #1
4
Heuristic Rules
What we need is “good mixing” for the ﬁrst few hundred points of the sequence.
To accomplish this, we have developed a set of heuristic rules to choose α’s
with good inter-dimensional mixing. The rules will ﬁrst be delineated, then the
reasons for each of these heuristics will be described.
1. Base the incremental intervals for each dimension on multiples of the “Golden
Ratio” which is: φ =
√
5+1
2
.
2. Select the multiples of φ for each dimension from sequences of prime num-
bers.
3. Examine the resultant modulo 1 sequences for the quasi-period with which
they revisit a value near the initial value (this is done by examining the
sequential distance diﬀerence as described below). If a multiplier results in
either a very short or trivial sequence of sequential diﬀerences, or closely
matches the sequential diﬀerence sequence of a previously selected multiplier,
do not use that multiplier, use the next prime number instead.
The reasons for these heuristics are fairly straightforward. The ﬁrst rule is
based on the fact that to achieve good mixing in the initial points of the se-
quence, we need a “very” irrational number. Irrational numbers are classiﬁed by
how easy they are to approximate with continued-fraction ratios of integers. For
example,
√
2, which is irrational, would be inappropriate to use since it is well
approximated by a continued fraction sequence.

Dispersion-Based Population Initialization
1215
The “Golden Ratio” mentioned previously, normally represented by φ, is the
irrational number that is most poorly approximated through continued fractions
[11]. This is the obvious choice for uniformly of a one-dimensional placement,
since the more irrational α is, the more uniformly distributed is the ﬁrst part
of the sequence of P points Pi = {Kiα}, where Ki = 1, 2, 3...P [10]. However,
the use of φ only guarantees best placement in one dimension. If the same α
(or any integer multiple of the same α) is used for a Kroenecker sequence in
more than one dimension, the points are merely placed along a diagonal in those
dimensions. This is why the Kroenecker sequence requires linear independence
of the α’s among Z. Since it can be very diﬃcult to prove that a number is
irrational or that multiple irrational numbers are linearly independent over Z,
this brings us to the second heuristic rule.
The second rule is to select the multiples of φ for each dimension from se-
quences of prime numbers, avoiding the prime numbers 1 and 2. What we need
to accomplish here is to force the pair-wise relationship between dimensions to
be irrational. Ratios of prime numbers approximate irrational numbers and this
method makes the ratios of the step sizes between any two dimensions relatively
irrational. 1 This approximation is adequate for our purposes, as long as the
sequences generated provide good placement and good inter-dimensional mix-
ing. Since, however, multiples of φ do not necessarily have the same placement
uniformity as φ, we come to the third heuristic rule.
This third heuristic rule is designed to improve mixing across multiple dimen-
sions. In a sequence kZ1α modulo 1, for k = 1, 2, 3, ... and Zi ∈Z, values within a
distance ε of the ﬁrst point are visited quasi-periodically, with the sequential dif-
ferences between these quasi-periods forming an easily recognizable pattern, but
with occasional and irregular breaks in the pattern. An example of this is shown
as Figure 2. In this ﬁgure, the distance to the starting point of a Kroenecker
sequence is plotted for 50 points. As can be seen, for ε = .10 the distance is less
than ε at k = 6, 14, 18, 22, 26, 31, 35, 39, 43, 48.... The sequential diﬀerence be-
tween the k values are 8, 4, 4, 4, 5, 4, 4, 4, 4, .... When these sequential diﬀerences
form trivial patterns like this (or, for example, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 5),
or are identical to the patterns already selected for other prime multipliers, the
ﬁll patterns across multiple dimensions are usually not adequately mixed. These
sequences are easily checked using a simple spreadsheet, and, if an undesired se-
quence is encountered, the next prime number in the sequence should be selected
as the multiplier.
Table 1 provides recommended α values for 1 through 12 dimensions that
were derived using the heuristic rules described above. The Table also provides
the Dispersion Index, ∆from [8] for populations of the ﬁrst 50 points generated
using these α’s. The Dispersion Index, ∆measures the uniformity of the pop-
ulation dispersion and has a maximum value of 1.00 for a completely uniform
distribution. While the recommended α’s in Table 2 are not guaranteed to be
1 Sequential very large prime numbers must also not be used, since the ratio of two
sequential very large prime numbers closely approximates 1.0 and will result in a
violation of the next heuristic rule.

1216
R.W. Morrison
Sequence Number
Distance to Starting Point
Fig. 2. Quasi-Periodic Distance to the First Point
the optimal α’s for use with any speciﬁc EA problem, these prime numbers are
known to provide reasonably uniform spatial distributions for the ﬁrst 200 points
in dimensions up through 12.
Table 1. Prime α’s and Dispersion Indices for 50 Points, Dimensions 1 through 12
Dimension α
∆
1
41
0.9871
2
43
0.9841
3
47
0.9702
4
59
0.9874
5
83
0.9738
6
107 0.9795
7
109 0.9853
8
173 0.9853
9
311 0.9853
10
373 0.9910
11
401 0.9999
12
409 0.9910
The resultant heuristic sentinel placement algorithm works as follows. First
scale and oﬀset all search-space dimensions as necessary to set each dimensional
range from 0 to 1. Second, randomly place the initial sentinel point, xi, within
the search space, for i equal 1 to N. Finally, compute the coordinates of each
subsequent point P, xi,p ∈P, as:
xi,p = mod1[xi,p−1 + Ziφ],
(2)
where the Zi’s are selected for each dimension in accordance with the heuristic
rules speciﬁed.
As can be seen, the coordinates of each point can easily (and very computa-
tionally eﬃciently) be determined from the location of the previous point, once

Dispersion-Based Population Initialization
1217
the Zi’s are selected through an oﬀ-line process for the range of dimensions and
population sizes appropriate for your problem domain. Note that nothing more
need be known about the problem space than the number of dimensions and
the approximate size of the population of sentinels to be used, and that this
information is used oﬀ-line in a simple spreadsheet calculation.
4.1
Population Placement in Very High-Dimensional Search Spaces
Very high-dimension search spaces would require a large set of prime numbers
that obey the heuristic rules for selection. These can be diﬃcult to ﬁnd, although
all eﬀort expended in ﬁnding them is performed oﬀline and does not aﬀect
the EA performance. More important, however, is the fact that it is probably
not worth the eﬀort. In large dimensional spaces, the distances between points
increase rapidly. With very large distances between only a few points, the beneﬁt
of placing population members “uniformly” versus “randomly” diminishes. For
this and other reasons, several mathematical researchers have expressed doubts
about the usefulness of uniform distribution methods for dimensions higher than
about 12 [12] [9]. Because of this, if the dimensionality of the problem is greater
than 12, random population placement is recommended. Up through dimension
12, Table 1 provides a pre-computed set of α’s.
4.2
Population Placement in Complex Search Spaces
Up until this point, we have only been describing population placement in sym-
metric, real-numbered search spaces. Fortunately, the transition to other search
spaces is quite straightforward.
The use of this technique for asymmetric search spaces is straightforward.
Since the population placement algorithm is so computationally eﬃcient, simply
compute the population as if the search space was symmetric in all axes (using
the largest axis, scaled 0 to 1 as the basis), and disregard any points falling out-
side the search space. Compute additional points, continuing to discard those
falling outside of the search space, until the desired number of population mem-
bers fall within the search space.
Similarly, the use of this technique for search spaces involving non-binary
combinatorial problems is also straightforward. If, for example, one of the axes
of the search space involves four non-ordinal values, a simple partitioning of the
axis into four equal-length segments to represent the four values will suﬃce for
problem initialization. Since the placement algorithm assures reasonably uniform
inter-dimensional dispersion for problem initialization, this technique will pro-
vide an appropriate representation of population members dispersed throughout
the search space.
5
Experimental Veriﬁcation
To examine the usefulness of the sentinel placement algorithm for population
initialization, a simple genetic algorithm (GA) with gray code binary represen-

1218
R.W. Morrison
tation, ﬁtness-proportional selection, uniform crossover, and a mutation rate of
0.001 was used. This GA was run against a variety of diﬀerent problems of vary-
ing complexity. A few representative results are provided here. The problems
presented are:
– De Jong Test Function 2, also called Rosenbrock’s Function, in 2 dimensions
[13].
– De Jong Test Function 3, a step function, in 5 dimensions [13].
– Michalewicz’s Function, in 10 dimensions [13].
A population of 25 was used. The De Jong test functions #2 and #3 and the
Michalewicz test function are usually implemented as minimization functions,
but were implemented as maximization functions for these experiments, applying
the following equations respectively:
f(x) = 3907 −100(x2
1 −x2)2 + (1 −x1)2, for x1,2 ∈[−2.048, 2.048].
(3)
f(x) = 25.0 −
N

i=1
|xi|, for xi ∈[−2.048, 2.048].
(4)
f(x) =
N

i=1
((sin(xi))(sin(ix2
i
π ))20), for xi ∈[0, π].
(5)
The GA was run on each of these problems 100 times using random popu-
lation initialization and 100 times using placed population initialization (recall
that, since the placement algorithm uses a random place for the ﬁrst placed
point, each “placed population” will be diﬀerent, but equivalently dispersed in
the search space). Since these are static problems, the average “best-so-far,”
along with its variance, for the 100 runs are reported as the measures of eﬀec-
tiveness in this chapter.
The results of the ﬁrst 25 generations of the above experiments are graphed
in Figures 3 through 8. As can be seen in Figures 3 and 4 there appears to be a
considerable reduction in variance for De Jong Test Function 2. The other charts
show less dramatic results, so the results were further analyzed to determine their
statistical signiﬁcance.
6
Analysis
At any speciﬁc generation, the F-statistic can be used to determine whether the
diﬀerences between the two variances are statistically signiﬁcant.
Table 2 provides a look at the signiﬁcance of the variance diﬀerence seen at
generation 10 in each of the ﬁgures from the previous section. For each technique,
the mean best-so-far (BSF) ﬁtness at generation 10, the variance of the mean

Dispersion-Based Population Initialization
1219
Fig. 3.
Best So Far, Random Initializa-
tion, DeJong Test Function 2
Fig. 4. Best So Far, Placed Initialization,
DeJong Test Function 2
Fig. 5.
Best
So
Far,
Random
Ini-
tialization, DeJong Test Function 3, 5-
Dimensions
Fig. 6. Best So Far, Placed Initialization,
DeJong Test Function 3, 5-Dimensions
Fig. 7.
Best
So
Far,
Random
Ini-
tialization, Michaeliwicz’s Function, 10-
Dimensions
Fig. 8. Best So Far, Placed Initialization,
Michaeliwicz’s Function, 10-Dimensions

1220
R.W. Morrison
Table 2. Results of Diﬀerent Population Initialization at Generation 10
Test
BSF Mean Variance F
Conﬁdence
De Jong 2 2D random
3905.10
2.12
3.867 99+%
De Jong 2 2D placed
3905.32
0.55
-
-
-
-
-
-
-
De Jong 3 5D random
20.90
2.53
1.326 76%
De Jong 3 5D placed
21.15
1.91
-
-
-
-
-
-
-
Michalewicz 10D random 3.00
0.26
1.135 77%
Michalewicz 10D placed
3.10
0.23
-
-
ﬁtness at generation 10, the F statistic computed as the ratio of the squares of
the two variances, and the conﬁdence level that the variances are diﬀerent.
As expected, there was no signiﬁcant diﬀerence in the mean best-so-far per-
formance of the EAs attributable to the population initialization algorithm (the
t statistic is therefore omitted for brevity), but there was generally a reduction
in the variance of the mean best-so-far performance. As addressed previously, at
high dimensions the advantage of the placement algorithm is reduced.
7
Summary
In this paper we have derived a population initialization algorithm for EAs in
static environments. Use of the technique does not improve mean best-so-far
performance over a large number of runs. Instead, it reduces the variance of the
mean best-so-far performance without loss of average performance, thereby pro-
viding researchers the opportunity to reliably examine their experimental results
needing fewer EA runs for an appropriate statistical sample. This may provide
an opportunity for researchers to address more complex problems without an
attendant increase in required computational resources.
References
1. Kallel, L. and Schoenauer, M.: Alernative Random Initialization in Genetic Algo-
rithms. In: Proceedings of the Seventh International Conference on Genetic Algo-
rithms. Morgan Kaufman (1997) 268–275
2. Branke, J.: Evolutionary Optimization in Dynamic Environments. Kluwer Aca-
demic Publishers (2002)
3. Shimada, K. and Gossard, D.: Bubble Mesh: Automated Triangular Meshing of
Non-manifold Geometry by Sphere Packing. In: Proceedings of the Third Sympo-
sium on Solid Modeling and Applications. IEEE (1995) 225–237
4. Rush, J.: Sphere Packing and Coding Theory. In: Handbook of Discrete and Com-
putational Geometry. CRC Press (1997) 185–208
5. Weisstein, E.: Hypersphere. In: World of Mathematics. Wolfram Research, Inc.
http://mathworld.wolfram.com (2001)

Dispersion-Based Population Initialization
1221
6. Neiderreiter, H.: Low-discrepancy and Low-dispersion Sequences. In: Journal of
Number Theory 30-1 (1987) 51–70.
7. Drmota, M. and Tichy, R.: Sequences, Discrepancies and Applications, Lecture
Notes in Mathematics 1651. Springer-Verlag (1997)
8. Morrison R.: Designing Evolutionary Algorithms for Dynamic Environments.
Ph.D. Dissertation. George Mason University (2002)
9. Bratley, P., Fox, B. and Neiderreiter, H.: Implementation and Tests of Low-
discrepancy Sequnces. In: ACM Transactions on Modeling and Computer Sim-
ulation 2(3). ACM (1992) 195–213
10. Alexander, J., Beck, J. and Chen, W.: Geometric Discrepancy Theory and Uniform
Distribution. In: Handbook of Discrete and Computational Geometry. CRC Press
(1997) 185–208
11. American-Mathematical-Society: The most irrational number.
http://www.ams.org/new-in-math/cover/irrational3.html. 2001
12. Owen, A.: Monte Carlo Variance of Scrambled Net Quadrature. In: Journal of
Numerical Analysis 34(5). Society for Industrial and Applied Mathematics (1997)
1884–1910
13. Liao, Y. and Sun, C.: An Educational Genetic Algorithm Learning Tool. In: IEEE
Transactions on Education 44(2) 2001 CD-ROM Directory 14

A Parallel Genetic Algorithm Based on Linkage
Identiﬁcation
Masaharu Munetomo, Naoya Murao, and Kiyoshi Akama
Hokkaido University, North 11, West 5, Sapporo 060-0811, JAPAN.
{munetomo, naoya.m, akama}@cims.hokudai.ac.jp
Abstract. Linkage identiﬁcation algorithms identify linkage groups —
sets of loci tightly linked — before genetic optimizations for their recom-
bination operators to work eﬀectively and reliably. This paper proposes
a parallel genetic algorithm (GA) based on the linkage identiﬁcation al-
gorithm and shows its eﬀectiveness compared with other conventional
parallel GAs such as master-slave and island models. This paper also
discusses applicability of the parallel GAs that tries to answer “which
method of the parallel GA should be employed to solve a problem?”
1
Introduction
To solve large, complex combinatorial optimization problems, parallelization of
optimization algorithms is crucial. Evolutionary computation models such as
genetic algorithms (GAs) are considered suitable for parallel computation be-
cause they deal with a number of strings that can be processed independently in
their ﬁtness evaluations and genetic operators such as mutations and crossovers.
Therefore, the parallel GA is an active research area and a number of papers
have been published in this area. Typical approaches of the parallel GAs are
master-slave models, island models, massively parallel GAs, and so on. Although
they succeeded in parallelizing GAs, it does not mean that they succeeded in
parallelizing their optimization, in other words, they can reduce time to obtain
optimal solutions properly as compared with the number of processors employed.
Speedup analysis is essential to discussing eﬃciency of parallel GAs. Cantu-Paz
et. al[1] performed theoretical and empirical analysis on speedup for master-slave
models, island models, and so on. The analysis shows the conditions for the mod-
els to work eﬀectively; for example, master-slave models work eﬀectively when
time to evaluate ﬁtness is much greater than that for communications among
processors. In this paper, we ﬁrst review and discuss current technologies and
the conditions that they work eﬀectively.
Discussing eﬃciency of parallelizations, we need to discuss eﬀectiveness of
the GA itself in solving a problem. This is because when a GA (with some op-
erators, selection schemes, etc.) cannot solve a problem essentially, we cannot
solve the problem even though parallelized version of the GA is employed. This
happens especially when we employ simple GAs to solve diﬃcult problems that
have deceptiveness and encoded strings do not have tight linkage. If linkage is
E. Cant´u-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1222–1233, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

A Parallel Genetic Algorithm Based on Linkage Identiﬁcation
1223
loose, building blocks are easily disrupted by simple crossovers and therefore
genetic optimizations are easily trapped into deceptive attractors. On the conse-
quence of this, we cannot expect the GA to obtain optimal solutions because it
is easily converged to local optima. To solve this problem, linkage identiﬁcation
procedures such as the LINC (Linkage Identiﬁcation by Nonlinearity Check) [7,
6] and the LIEM (Linkage Identiﬁcation with Epistasis Measures) have been
proposed to identify linkage groups — sets of loci tightly linked to form building
blocks. The merit of this approach is not only it can avoid disruption of build-
ing blocks but also it can easily be parallelized because its calculation is highly
independent. This paper discusses merit of the parallel GA based on the linkage
identiﬁcation compared with conventional parallel GAs.
2
Parallel GAs
A number of papers have been published on parallel GAs. Majority of them are
classiﬁed into the following categories:
– master-slave models that parallelize ﬁtness evaluations
– island models that divide a population into subpopulations
– massively parallel GAs that place strings on a mesh, etc.
– hierarchical models that combine more than two methods
A master-slave model consists of a master that performs GAs and slaves that
evaluate ﬁtness values. This model performs ﬁtness evaluations in parallel to
enhance its performance. To apply this model, communication overheads should
be relatively small compared with time to evaluate ﬁtness values. This is analyzed
by Cantu-Paz[1] in his doctoral thesis. In the following analysis based on his
thesis, we denote time to evaluate a ﬁtness value as Tf, communication time
between processors as Tc, the overall population size as n and the number of
processors of the target parallel machine as P. We can easily calculate overall
execution time Tp by the P-processor parallel machine as follows:
Tp = PTc + nTf
P .
(1)
Speedup factor S is obtained by calculating nTf/Tp:
S = nTf
Tp
=
nTf
PTc + nTf/P .
(2)
This is illustrated in ﬁgure 1. This ﬁgure shows that parallelization by the
master-slave model is eﬀective when Tf ≫Tc.
From the above calculations, we obtain the optimal number of processors P ∗
that maximizes the speedup factor S as follows:
P ∗=

nTf/Tc.
(3)

1224
M. Munetomo, N. Murao, and K. Akama
Fig. 1. Speedup by master-slave PGAs
Island models also called subpopulation-based parallel GAs divide a popula-
tion into subpopulations assigned to processors. Each processor performs a GA
for its subpopulation and migrates strings between subpopulations to exchange
building blocks. Even though speedup realized by this model is relatively small
as illustrated in ﬁgure 2, this is considered natural implementation of parallel
GAs and may be employed when master-slave models does not work eﬀectively.
Fig. 2. An Empirical Result of Speedup by an island model PGA
The parallel GAs based on the island model expect that each subpopulation
searches diﬀerent candidates of building blocks to be exchanged among them,
however, such favorable situations may not be realized in some problems. For
example, when ﬁtness contribution of building blocks to overall ﬁtness is ex-

A Parallel Genetic Algorithm Based on Linkage Identiﬁcation
1225
Fig. 3. Ratio of BBs correctly identiﬁed by an island model
ponentially decreasing (or increasing), all subpopulations must search the same
building block with maximum ﬁtness contribution at ﬁrst, then search that with
the second maximum, and so on. This is reported by Goldberg in discussing
continuation operators[4].
Figure 3 shows that subpopulation-based parallel GAs performs well when
ﬁtness contribution of building blocks is uniform and their performance gets
worse when the contribution becomes exponential. In the ﬁgure, we employ the
sum of 5-bit trap functions as a test function and we assign a weight wi to each
i-th trap subfunction (i = 1, 2, · · · , 20). In uniform case, wi = 1 for all i. In expo-
nential case, w1 = 1.0 and wi = r × wi−1, that is, the weights are exponentially
decreasing. When r < 1/2, the sum of all weights for i = 2, 3, · · · becomes smaller
than w1, the maximum weight value. This means that ﬁtness contribution by the
ﬁrst BB exceeds those for the sum of other BBs, and therefore at ﬁrst only the
BB with the maximum weight should be found in all the subpopulations. This
is because diversity cannot be ensured even though a number of subpopulations
are employed in the island models.
3
Linkage Identiﬁcation
A series of linkage identiﬁcation techniques have been proposed to ensure tight
linkage among loci to form building blocks. The Linkage Identiﬁcation by Non-
linearity Check (LINC)[7,6] identiﬁes linkage groups by introducing bit-wise per-
turbations for pairs of loci to detect nonlinear interactions of ﬁtness changes. In
the LINC, we calculate the following values for each pair of loci (i, j) and for all
the strings s in a randomly initialized population that has O(c2k) strings where
c is a constant and k is the maximum order of linkages:
∆fi(s) =
f(.. ¯si.....) −f(..si.....)
∆fj(s) =
f(..... ¯sj..) −f(.....sj..)

1226
M. Munetomo, N. Murao, and K. Akama
∆fij(s) = f(.. ¯si. ¯sj..) −f(..si.sj...),
(4)
where f(s) is a ﬁtness functions of a string s and ¯si = 1 −si (0 →1 or 1 →0)
stands for a bitwise perturbation at the i-th locus. We can consider the following
two cases:
1. If ∆fij(s) ̸= ∆fi(s)+∆fj(s), then si and sj are surely members of a linkage
group, so we add i to the linkage group of locus j and add j to the linkage
group of locus i.
2. If ∆fij(s) = ∆fi(s) + ∆fj(s), then si and sj may not be a member of a
linkage group, or they are linked but linearity exists in the current context.
We do nothing in this case.
The LINC checks the above conditions for all the string s in a population.
When the nonlinear condition ∆fij(s) ̸= ∆fi(s) + ∆fj(s) is satisﬁed for at least
one string s, the pair (i, j) should be considered to be tightly linked. This is
because a pair of loci can be optimized independently when linearity is detected
for all the string. The detection of nonlinearity is the key idea of the LINC.
The Linkage Identiﬁcation with Epistasis Measures (LIEM) extends the idea
of the LINC by introducing an epistasis measure that represents tightness of
linkage based on the nonlinearity conditions of the LINC. The epistasis measure
of the LIEM is deﬁned as follows:
eij = max
s∈P |∆fij(s) −(∆fi(s) + ∆fj(s))|,
(5)
where ∆fi(s), ∆fj(s), ∆fij(s) are the same as those in equations (4).
This measure shows the maximum distance from where the LINC condition is
satisﬁed, therefore, this shows a degree of dissatisfaction of the condition. When
the measure for a pair of loci (i, j) is equal to zero, the pair should be separable
from the LINC conditions. By introducing the epistasis measure, we can relax
strict condition of the LINC and can introduce a clear deﬁnition of tightness of
linkage for pairs of loci.
Figure 4 shows the algorithm of the LIEM. First, we randomly initialize a
large enough initial population. From theoretical investigations for the LINC, we
need to have O(k2k) strings in a population to identify linkage groups of order
k. Secondly, we calculate an epistasis measure eij for each pair of loci (i, j) by
applying perturbations to the pair of loci. After the calculations, the measures
are sorted by descendent order. Linkage groups are generated by picking up loci
from the ﬁrst to the k-th sorted measures.
Although the algorithm of the LIEM seems too simple to identify accurate
linkage groups, it can identify 100% correct linkage groups for problems with
linear combinations of nonlinear subfunctions such as the sum of trap functions
even though their bit positions are randomly encoded, and it also achieves accu-
rate identiﬁcations for quasi-linear or weak nonlinear combinations of nonlinear
subfunctions such as weak nonlinear functions of the sum of trap functions[5].
This is because the LIEM diﬀerentes strong nonlinearity from linear or weak
nonlinear interactions.

A Parallel Genetic Algorithm Based on Linkage Identiﬁcation
1227
algorithm LIEM
N = c*2^difficulty;
P = initialize N strings;
/*  Calculate epistasis measure e[i][j]  */
for i = 0 to l-1
    for j = 0 to l-1
        e[i][j] = 0; 
        if i != j then
            for each s in P
                s’ = perturb(s, i);
                f1 = fitness(s’) - fitness(s);
                s’’ = perturb(s, j);
                f2 = fitness(s’’) - fitness(s);
                s’’’ = perturb(s’, j);
                f12 = fitness(s’’’) - fitness(s);
                ep[s] = |f12 - (f1+f2)|;
                if(ep[s] > e[i][j]) then e[i][j] = ep[s];
            endfor
        endif
    endfor
endfor
/*  Generate linkage group l[i][k]
     where k = 0, 1,..., difficulty-1  */
for i = 0 to l-1
    for j = 0 to l-1
        id[j] = j;
    endfor
    sort e[i][j] with j by descendent order;
  /* select linkages */
    for k = 0 to difficulty-1
        if(e[i][k] > epsilon) l[i][k] = id[i][k];
        else break;
    endfor
endfor
Fig. 4. The Linkage Identiﬁcation with Epistasis Measure (LIEM)
4
A Parallel GA Based on Linkage Identiﬁcation
In this paper, we propose a parallel GA based on the linkage identiﬁcation tech-
nique. The linkage identiﬁcation algorithms such as the LINC and the LIEM are
easy to parallelize, because their calculations of epistasis measures are considered
highly independent. Figure 5 illustrates an execution ﬂowchart of our method.
To parallelize linkage identiﬁcations, we assign calculations of epistasis mea-
sures to processors in a parallel computer. First, a master processor randomly
initializes a population of N = O(k2k) strings and broadcasts them to all the
slave processors. As a consequence of this, all the processors have the same pop-
ulation and they can calculate their assigned epistasis measures. Computational
cost for calculating epistasis measures for all the pair of loci is O(l2) (more pre-
cisely, we need to calculate (l2 −l)/2 epistasis values). In order to calculate one
epistasis measure, we need to evaluate 3N ﬁtness values because calculations of
∆fi(s), ∆fj(s), ∆fij(s) are necessary for each s in the population. After cal-

1228
M. Munetomo, N. Murao, and K. Akama
1. Randomly Initialize a population of O(k2  ) strings
k
Calc
3. Parallel calculations of epstasis measures
Calc
Calc
Calc
2. Broadcast the population to processors
Initialize a population
Generate linkage groups
4. Collect epstasis measures
5. Generate linkage groups based on epstasis measures
Intra GA
Intra GA
Intra GA
Intra GA
Inter GA
6. Assign schemata to each linkage group
7. Parallel execution of Intra GAs
8. Collect BB candidates
9. Perform an Inter GA to obtain solution(s)
Fig. 5. An overview of PGA based on linkage identiﬁcation
culations of the epistasis measures, they are collected to the master processor
which generates linkage groups based on them. This is essentially the same as
the serial version of the LIEM.
When we have P processors in a parallel computer, overall computation time
Tp for the parallel linkage identiﬁcation becomes as follows:
Tp = 3NTf(l2 −l)/2
P
+ TcN + T ′
c(l2 −l),
(6)
where Tf and Tc are the same as in equation (1) and T ′
c is the time to send/receive
one epistasis measure (T ′
c < Tc). Similar to the master-slave model, we can
calculate speedup factor S for the proposed algorithm as follows:
S = (3TfN(l2 −l)/2)/Tp =
3TfN(l2 −l)/2
3Tf N(l2−l)/2
P
+ TcN + T ′c(l2 −l)/2
.
(7)
We can expect eﬀective parallelization with this algorithm because communica-
tion time to send an epistasis measure is much smaller than that for calculating
it.
After the generation of linkage groups, we perform Intra GAs in slave pro-
cessors to search candidates of building blocks and an Inter GA in the master
processor to mix and test the candidates to ﬁnd optimal solutions. The Intra
GAs and the Inter GA were originally introduced in a report that proposes a
GA based on the LINC[6]. To perform the Intra GAs, we divide initial strings
into schemata based on the obtained linkage groups. Figure 6 illustrates this
decomposition process.
The Intra GA applies ranking selections, uniform crossovers and simple mu-
tations to search candidates of building blocks in the linkage group. To evaluate
schemata, competitive template in messy GA[3] is employed. After the Intra

A Parallel Genetic Algorithm Based on Linkage Identiﬁcation
1229
0
0
1
0
1
1
1
0
0
1
1
1
0
1
1
0
1
0
0
0
0
0
1
1
0
0
0
0
1
2
3
4
5
6
7
8
Linkage groups
(0 3 4)   (1 2 6 7)   (5 8) 
A population of strings
*
0
1
*
*
*
1
0
*
*
1
1
*
*
*
0
1
*
*
0
0
*
*
*
0
0
*
0
1
2
3
4
5
6
7
8
A population of schemata
for linkage group (1 2 6 7)
Perform an Intra GA
for linkage group (1 2 6 7)
Fig. 6. Division of strings into schemata based on linkage groups
GAs, we select a limited number of well-performed schemata as building block
candidates in each linkage group.
The Inter GA processes obtained building block candidates by applying
crossovers based on the linkage groups and ranking selections repeatedly. Figure
7 shows crossover operator of the Inter GA. Building block candidates are mixed
by this operator and their combinations are tested through selections in order
to obtain optimal solutions.
0
0
1
0
1
1
1
0
0
0
1
1
0
0
1
0
0
0
0
1
2
3
4
5
6
7
8
Linkage groups
(0 3 4)   (1 2 6 7)   (5 8) 
Exchange substrings
for linkage group (1 2 6 7)
0
1
1
0
1
1
0
0
0
0
0
1
0
0
1
1
0
0
0
1
2
3
4
5
6
7
8
Fig. 7. Crossover operator of the Inter GA
5
Empirical Results
We perform numerical experiments for the proposed parallel GA based linkage
identiﬁcation. We employ the following weighted sum of trap subfunctions as a
test function.

1230
M. Munetomo, N. Murao, and K. Akama
f(s) =
L

i=1
wifi(ui).
(8)
where wi > 0 is the weight of subfunction fi of unitation ui (the number of 1’s
in the K-bit i-th substring of s) deﬁned as follows:
fi(ui) =

K −ui if 0 ≤ui ≤K −1
K
if ui = K
(9)
In the following experiments, we consider the following two cases:
Uniform : All weights have the same value: wi = 1 for all i.
Exponential : Weights are exponentially decreasing: w1 = 1, wi = rwi−1 (i =
2, 3, · · · , L, 0 < r < 1.)
Fig 8 shows the result on speedup by the parallel GA based on linkage iden-
tiﬁcation. In this experiment, we employ uniform case of the test function. For
exponential functions, similar result should be obtained because linkage iden-
tiﬁcation algorithms do not depend on scaling of the ﬁtness function. In this
experiment, we change ratio of time for ﬁtness evaluations (Tf) and that for
communications (Tc).
Fig. 8. Speedup by a PGA with linkage identiﬁcation
This result shows that the PGA with linkage identiﬁcation achieves near
linear speedup except when communication overheads are extraordinary large
compared with those for ﬁtness evaluations. (Note that ratios of Tf and Tc in
this ﬁgure are diﬀerent from those in the ﬁgure 1.)

A Parallel Genetic Algorithm Based on Linkage Identiﬁcation
1231
In ﬁgure 9, we compare overall performances of an island model PGA to
that with parallel linkage identiﬁcation for the exponential test function. In the
ﬁgure, the x-axis shows problem size (total length of the string = LK) and the
y-axis shows overall time to obtain optimal solutions (×Tf). In this experiment,
we assume the parallel machine has 8 processors and we optimize population
size for each model and each string length.
Fig. 9. Problem size vs. time (exponential function)
Fig. 10. # of processors vs. time (uniform function)
This ﬁgure shows that PGA with linkage identiﬁcation can ﬁnd optimal solu-
tions with less computational cost compared with that based on an island model

1232
M. Munetomo, N. Murao, and K. Akama
when string length is larger than around a hundred. This is because an island
model cannot search eﬀectively a variety of building blocks in their subpopu-
lations, on the other hand, the PGA based on linkage identiﬁcation can search
building block candidates separately in each linkage group. Another reason for
the performance diﬀerence is that an island model needs larger population size
when signal diﬀerence of subfunctions becomes small. This is easily understood
from the equation of population sizing calculated by Goldberg et. al[2] as follows:
n = 2cκσ2
M
d2 ,
(10)
where n is the necessary initial population size, c is a constant determined by
sampling noise, κ is the cardinarity of competing schemata, σM is the standard
deviation of the ﬁtness distribution, and d is the signal diﬀerence of the ﬁtness
between the best and the second best schemata.
When signal diﬀerence of ﬁtness d in the equation becomes smaller, the pop-
ulation size n necessary to obtain optimal solutions becomes larger. In the expo-
nential case, when string length becomes longer, minimum diﬀerence of the ﬁt-
ness decreases exponentially, therefore, an island model PGA needs much larger
size of population. In contrast, the PGA with linkage identiﬁcation do not de-
pend on such ﬁtness scaling eﬀect and its necessary population size is constant
with respect to signal diﬀerences.
Figure 10 shows time to obtain optimal solutions for an island model PGA
and that with linkage identiﬁcation for the uniform case. In this ﬁgure, the x-axis
shows the number of processors employed and the y-axis shows the time to obtain
optimal solutions. In this experiment, we employ a uniform test function whose
string length is 500, and we optimize initial population size for both models.
This ﬁgure clearly shows that the island model performs better than the PGA
with linkage identiﬁcation in the uniform test function. This is because an island
model works eﬀectively by maintaining variety of building block candidates in
each subpopulation. On the other hand, PGA with linkage identiﬁcation needs
relatively large computational overheads to obtain accurate linkage information.
6
Conclusions
In this paper, we propose a PGA based on linkage identiﬁcation. The PGA
achieves quasi-linear speedup except when communication overheads is ex-
tremely large compared with that for ﬁtness evaluations. In summary, we can
select one of the PGA models to solve optimization problems according to the
following guidelines:
– If the problem is diﬃcult to ensure tight linkage in advance with prior knowl-
edge, linkage identiﬁcations are necessary which can be parallelized easily.
– If time to evaluate ﬁtness values are much larger than that of communications
among processors, the master-slave models work eﬀectively.

A Parallel Genetic Algorithm Based on Linkage Identiﬁcation
1233
– If the ﬁtness function of the problem consists of uniformly weighted subfunc-
tions, island models can maintain diversity of building block candidates and
are expected to perform well.
– If the ﬁtness function consists of non-uniform subfunctions whose weights
are largely diﬀerent, parallel linkage identiﬁcations are expected to perform
better than island models.
Our future works includes applications of the proposed model to real world
design problems such as broadband network design problems, and we also plan
to compare the PGA with linkage identiﬁcation with parallelized version of ad-
vanced GAs and estimation of distribution algorithms such as the Bayesian op-
timization algorithm.
References
1. Erick Cant´u-Paz. Designing Eﬃcient and Accurate Parallel Genetic Algorithms.
PhD thesis, University of Illinois at Urbana-Champaign, 1999.
2. D. E. Goldberg. Sizing populations for serial and parallel genetic algorithms. pages
70–79, 1989. (Also TCGA Report 88004).
3. D. E. Goldberg, K. Deb, and B. Korb. Messy genetic algorithms revisited: Studies
in mixed size and scale. Complex Systems, 4:415–444, 1990.
4. David E. Goldberg. Using time eﬀectively: Genetic-evolutionary algorithms and the
continuation problem. Technical Report IlliGAL Report No.99002, University of
Illinois at Urbana-Champaign, 1999.
5. Masaharu Munetomo. Linkage identiﬁcation based on epistasis measures to realize
eﬃcient genetic algorithms. In Proceedings of the 2002 Congress on Evolutionary
Computation, 2002.
6. Masaharu Munetomo and David E. Goldberg. Designing a genetic algorithm using
the linkage identiﬁcation by nonlinearity check. Technical Report IlliGAL Report
No.98014, University of Illinois at Urbana-Champaign, 1998.
7. Masaharu Munetomo and David E. Goldberg. Identifying linkage by nonlinearity
check. Technical Report IlliGAL Report No.98012, University of Illinois at Urbana-
Champaign, 1998.

E. Cantú-Paz et al. (Eds.): GECCO 2003, LNCS 2723, pp. 1234–1245, 2003.
© Springer-Verlag Berlin Heidelberg 2003
Generalization of Dominance Relation-Based
Replacement Rules for Memetic EMO Algorithms
Tadahiko Murata 1, Shiori Kaige 2, and Hisao Ishibuchi 2
1 Department of Informatics, Kansai University
2-1-1 Ryozenji-cho, Takatsuki, Osaka 569-1095, Japan
murata@res.kutc.kansai-u.ac.jp,
http: www.res.kutc.kansai-u.ac.jp/~murata/
2 Department of Industrial Engineering, Osaka Prefecture University
1-1 Gakuen-cho, Sakai, Osaka 599-8531, Japan
{hisaoi, shior}@ie.osakafu-u.ac.jp,
http://www.ie.osakafu-u.ac.jp~hisaoi/ci_lab_e/
Abstract. In this paper, we generalize the replacement rules based on the
dominance relation in multiobjective optimization. Ordinary two replacement
rules based on the dominance relation are usually employed in a local search
(LS) for multiobjective optimization. One is to replace a current solution with a
solution which dominates it. The other is to replace the solution with a solution
which is not dominated by it. The movable area in the LS with the first rule is
very small when the number of objectives is large. On the other hand, it is too
huge to move efficiently with the latter. We generalize these extreme rules by
counting the number of improved objectives in a candidate solution for LS. We
propose a LS with the generalized replacement rule for existing EMO
algorithms. Its effectiveness is shown on knapsack problems with two, three,
and four objectives.
1   Introduction
Since Schaffer’s study [1], evolutionary algorithms have been applied to various
multiobjective optimization problems for finding their Pareto-optimal solutions.
Recently evolutionary algorithms for multiobjective optimization are often referred to
as EMO (evolutionary multiobjective optimization) algorithms. The task of EMO
algorithms is to find Pareto-optimal solutions as many as possible. In recent studies
(e.g., [2-6]), emphasis was placed on the convergence speed to the Pareto-front as
well as the diversity of solutions. In those studies, some form of elitism was used as
an important ingredient of EMO algorithms. It was shown that use of elitism
improved the convergence speed to the Pareto-front [5].
One promising approach for improving the convergence speed to the Pareto-front is
the use of local search in EMO algorithms. Hybridization of evolutionary algorithms
with local search has already been investigated for single-objective optimization
problems in many studies (e.g., [7], [8]). Such a hybrid algorithm is often referred to
as a memetic algorithm. See Moscato [9] for an introduction to this field and [10]–
[12] for recent developments. The hybridization with local search for multiobjective

Generalization of Dominance Relation-Based Replacement Rules         1235
optimization was first implemented in [13], [14] as a multiobjective genetic local
search (MOGLS) algorithm where a scalar fitness function with random weights was
used for the selection of parents and the local search for their offspring. Jaszkiewicz
[15] improved the performance of the MOGLS by modifying its selection mechanism
of parents. While his MOGLS still used the scalar fitness function with random
weights in selection and local search, it did not use the roulette wheel selection over
the entire population. A pair of parents was randomly selected from a pre-specified
number of the best solutions with respect to the scalar fitness function with the current
weights. This selection scheme can be viewed as a kind of mating restriction in EMO
algorithms. Knowles & Corne [16] combined their Pareto archived evolution strategy
(PAES [2], [4]) with a crossover operation for designing a memetic PAES (M-PAES)
[17]. In their M-PAES, the Pareto-dominance relation and the grid-type partition of
the objective space were used for determining the acceptance (or rejection) of new
solutions generated in genetic search and local search. The M-PAES had a special
form of elitism inherent in the PAES. In those studies, the M-PAES was compared
with the PAES, the MOGLS of Jaszkiewicz [15], and an EMO algorithm. In the
above-mentioned hybrid EMO algorithms (i.e., multiobjective memetic algorithms
[13]-[17]), local search was applied to individuals in every generation. In some
studies [18], [19], local search is restrictedly applied to every generation by limited
local search only to non-dominated solutions [18] or introducing the tournament
selection and the selection probability of candidate solutions for local search [19].
Another way of application of local search is proposed in [20], [21], where local
search was applied to individuals only in the final generation.
In order to design a local search for multiobjective optimization, a rule for
replacing a current solution with another solution should be defined in advance.
Murata et al. [22] showed experimental results on pattern classification problems
where a scalar fitness function-based replacement rule was better than the dominance
relation-based replacement rules. In the dominance relation-based replacement rules,
the current solution is replaced with a solution which dominates the current one or a
solution which is at least a non-dominated solution with the current one. Ishibuchi et
al. [23] also pointed this matter by experimental results on scheduling problems. As
mentioned in [17], [22] and [23], the replacement rule to accept a non-dominated
solution has a weak search pressure since almost all pairs of solutions (a current
solution and a candidate solution) will be non-dominated with respect to each other
especially in problems with a large number of objectives. On the other hand, the
replacement rule to accept a dominating solution does not work well because it is
difficult to find a dominating solution for the current solution.
We generalize the replacement rules based on the dominance relation by counting
the number of better objective values. Details of the dominance relation-based
replacement rules are shown in the next section. We employ a local search using the
proposed replacement rule to improve the performance of existing EMO algorithms
such as SPEA [3] and NSGA-II [6]. We apply them with the local search to multi-
objective knapsack problems as benchmark problems [3], and show the effectiveness
of our generalization.

1236         T. Murata, S. Kaige, and H. Ishibuchi
2  Dominance Relation-Based Replacement Rules
2.1  Previous Extensions of Dominance Relation
In order to improve the performance of local search using the dominance relation,
several ideas to extend it have been already proposed in [17], [24], and [25]. Knowles
and Corne [17] proposed a replacement rule by which a current solution is replaced
with a non-dominated solution if it dominates other solutions in the set of non-
dominated solutions obtained so far. Ikeda et al. [24] proposed their “α -dominance”
where a small detriment in one or several of the objectives is permitted if an attractive
improvement in the other objective(s) is achieved. While these two methods try to
extend the area of dominating solutions of the current solution, Laumanns et al. [25]
proposed their “ε -dominance” where a solution with a small improvement in every
normalized objective does not dominate the current one.
Each of these three methods can be considered as an extension of the dominance
relation. Before explaining these extensions, we show the dominance relation defined
in multiobjective optimization. Without loss of generality, we assume the following
N-objective maximization problem:
Maximize 
))
(
...,
),
(
),
(
(
2
1
x
x
x
z
N
f
f
f
=
,
               (1)
subject to 
X
x∈
,
               (2)
where z is the objective vector with N objectives to be maximized, x is the decision
vector, and X is the feasible region in the decision space. A solution 
X
x ∈
 is said to
dominate another solution 
X
y ∈
 if the following two conditions are satisfied.
)
(
)
(
y
x
i
i
f
f
≥
, 
}
...,
,2
,1
{
N
i ∈
∀
,
               (3)
)
(
)
(
y
x
i
i
f
f
>
, 
}
...,
,2
,1
{
N
i ∈
∃
.
               (4)
If there is no solution which dominates x in X, x can be said to be a Pareto-optimal
solution. Fig. 1 shows that there are four areas of candidate solutions for the solution
x in the case of two-objective problems. When we employ this dominance relation in
local search, two replacement rules can be used in the local search as follows:
Rule A: Move to dominating solutions:
Replace the solution x with a solution which dominates it (Area A in Fig. 1).
Rule B: Move to non-dominated solutions:
Replace the solution x with a solution which is not dominated by x (Areas A - C).
The movable area in the local search with Rule A is very small when the number of
objectives is large. On the other hand, it is too huge to move efficiently with Rule B.
Therefore some extensions for the dominance relation should be considered.
As shown in Fig. 2, Knowles and Corne [17] extended the area of dominating
solutions using non-dominated solutions obtained so far. Fig. 3 shows the dominating
area of the current one defined by the α -dominance relation [24]. While these two
methods enlarge the dominating area of the current solution, the ε -dominance
relation [25] reduces the dominating area as shown in Fig. 4. Since the aim of the ε -
dominance is reducing the number of non-dominated solutions obtained by this
dominance relation, an opposite strategy is used. However, we can see that the area of

Generalization of Dominance Relation-Based Replacement Rules         1237
non-dominated solutions (B and C) is also reduced by the hatched area which consists
of dominated solutions (D in Fig. 4). Therefore we can see that this is also a method
to reduce the area of non-dominated solutions.
As we observe from Figs. 2 - 4, the area of non-dominated solutions is reduced by
these three methods. However, each of them needs more computational efforts. The
method in [17] needs to compare the candidate solution with non-dominated
solutions. Its performance may depend on the quality of the obtained set of non-
dominated solutions. As for the α -dominance [24], the decision maker (DM) should
define parameters β  and γ  for every pair of objectives in advance. The DM should
also define a parameter ε  in advance to use the ε -dominance [25], and ε  should be
determined carefully since a large ε  makes a solution obtained by this method far
from the true Pareto-front.
 0 
f1(x)
f2(x) 
Maximized 
Maximized 
x 
Fig. 1. The area of candidate solutions
which replace the current solution x by the
dominance relation.
 0 
x 
f1(x)
f2(x) 
Maximized 
Maximized 
Fig. 2. The area of candidate solutions
which replace the current solution x by the
method in [17].
 x 
0 
γ  β  f1(x)
f2(x) 
Maximized 
Maximized 
Fig. 3. The area of candidate solutions
which replace the current solution x by the
α -dominance [24].
 0 
x 
ε  ε  f1(x)
f2(x) 
Maximized 
Maximized 
Fig. 4. The area of candidate solutions
which replace the current solution x by the
ε -dominance [25].
C
A
D
B
C
A
D
B
C
A
D
B
C
A
D
B

1238         T. Murata, S. Kaige, and H. Ishibuchi
2.2   Generalization of the Dominance Relation
In this section, we generalize the two replacement rules (i.e., Rules A and B) shown in
the previous section by counting the number of improved objectives. Fig. 5 shows that
there are eight possible spaces for the solution x in the case of three-objective
problems. Every solution in Space A dominates the solution x. On the other hand, the
solution x dominates solutions in Space H. Therefore Rule A allows the current
solution to move to a solution in only one space, Space A. On the other hand, Rule B
enables it to move to neighborhood solutions in all spaces except a dominated space
H in Fig. 5. This means that 
)1
2
(
−
N
 spaces are allowed out of 
N
2
 spaces in Rule B.
We can see that the number of accepted spaces is extreme in each of both cases. That
is, while the number of accepted spaces is only one as for Rule A, it is 
)1
2
(
−
N
 for
Rule B. We generalize these two extreme cases by counting the number of improved
objectives.
The number of improved objectives for the solution x is different in each space. For
example, Fig. 5 shows that the number of improved objectives for a solution in Space
A is three. It is zero for a solution in Space H. There are other spaces where the
number of improved objectives is one or two. That is, Spaces B, C, and E have two
improved objectives, and Spaces D, F, and G have one.
In the case of N-objective problems, the number of possible spaces from the current
solution is 
N
2
. The number of improved objectives varies from zero to N in this case.
We generalize the replacement rules A and B by considering the number of improved
objectives d. That is, the current solution x is replaced with a solution which has d or
more improved objective values. We have the following generalized rule:
Rule d: Move to d-Improved Solutions:
Replace the current solution x with a solution which has d or more improved
objectives.
0 
x 
 
1f  
2f  
3f  
A 
C 
B 
D 
E 
G 
F 
H 
A 
H 
Fig. 5.  Eight spaces for the current solution in the case of three-objective problems.

Generalization of Dominance Relation-Based Replacement Rules         1239
 Initialization 
EMO Part 
Local Search Part 
Initial 
population 
New 
population 
Improved 
population 
Fig. 6.  Generic form of our local search part and EMO part.
By varying the value of d, we have the following rules where N is the number of
objectives:
Rule N :  Accept a solution which has N better objective values.
Rule 
1
−
N
:  Accept a solution which has 
1
−
N
 or more better objective values.

Rule 2:  Accept a solution which has at least two better objective values.
Rule 1:  Accept a solution which has at least one better objective value.
Therefore, Rules A and B in Subsection 2.1 are Rule N and Rule 1 of the proposed
rule, respectively.
3   Local Search Using the Generalized Replacement Rule
The outline of our local search can be written in a generic form as Fig. 6. This figure
shows a basic structure of simple memetic algorithms. As shown in Fig. 6, our local
search part can be applied to every EMO algorithm. For other types of memetic
algorithms, see Krasnogor [26] where taxonomy of memetic algorithms was given
using an index number D. This type of memetic algorithms is a 
4
=
D
 memetic
algorithm in his taxonomy (for details, see [26]).
We design our local search as follows:
[Proposed Local Search]
Iterate the following seven steps 
pop
N
 times, where 
pop
N
 is the number of
populations to be governed by genetic operations such as crossover and mutation in
an EMO algorithm. Then replace the current population with 
pop
N
 solutions obtained
by the following steps.
Step 1: Randomly choose two individuals from the current population.
Step 2: Count the number of better objective values between the two solutions. Select
a solution which has a larger number of better objective values.

1240         T. Murata, S. Kaige, and H. Ishibuchi
Step 3: Select another solution from the current population, and back to Step 2 until t
solutions are compared from the current population.
Step 4: Apply local search with the local search probability 
LS
p
. If it is applied, go to
Step 5. If not, go to Step 7.
Step 5: Generate a neighborhood solution of the current solution, and calculate the
objectives of the generated solution. Count the number of improved objectives
by the generated solution.
Step 6: If the number of improved objective values is d or more, replace the current
solution with the generated solution, and back to Step 5 for examining the
neighborhood solution for the generated solution. If not, back to Step 5 until
the number of examined solution for the current solution becomes k. If there is
no better solution among k neighborhood solutions, go to Step 7.
Step 7: Back to Step 1 until 
pop
N
 solutions are selected for the local search.
When local search is applied to the selected solution in Step 4, the final solution
obtained by the local search is included in the next population. If local search is not
applied, the selected solution is included in the next population. Therefore Steps 1 - 3
can be considered as the tournament selection for selecting candidate solutions for
local search. In this tournament selection, we also employ the idea of the generalized
replacement rule. That is, we select a solution with respect to the number of better
objective values among t solutions. We use the local search probability 
LS
p
 for
decreasing the number of solutions to which local search is applied. In this way, local
search is not applied to all the selected solutions. If local search is applied to all the
solution among the population, the computation time may be wasted to improve
dominated solutions. Moreover, we also employ the number of examined solutions k
in Step 6 in order to control the balance between genetic search and local search.
Since this proposed local search can be applied to any EMO algorithm, we apply
our local search to SPEA [3] and NSGA-II [6]. In order to show its effectiveness, we
employ multiobjective knapsack problems [3]. We show results of computer
simulations in the next section.
4   Computer Simulations on Multiobjective Knapsack Problems
4.1   Multiobjective Knapsack Problems
We employ multiobjective knapsack problems [3] to which we applied EMO
algorithms with the proposed local search. Those test problems are available from the
web site (http://www.tik.ee.ethz.ch/~zitzler/). Generally, a 0/1 knapsack problem
consists of a set of items, weight and profit associated with each item, and an upper
bound for the capacity of the knapsack. The task is to find a subset of items which
maximizes the total profits within the prespecified total weight of the items. This
single-objective problem was extended to the multiobjective case by allowing an
arbitrary number of knapsacks in [3]. In the multiobjective knapsack problem, there
are m items and N knapsacks. Profits of items, weights of items, and capacities of
knapsacks are denoted as follows:

Generalization of Dominance Relation-Based Replacement Rules         1241
ij
p : profit of item j according to knapsack i,
                (5)
ij
w : weight of item j according to knapsack i, 
                (6)
ic : capacity of knapsack i, 
                (7)
where 
m
j
...,
,1
=
 and 
N
i
...,
,1
=
.
                (8)
The decision vector x of this problem is 
)
...,
,
,
(
2
1
m
x
x
x
=
x
, where 
jx  is a binary
decision value. 
0
=
jx
 and 
1
=
jx
 mean that the item j is not included in the
knapsacks, and it is included in the knapsacks, respectively. Thus N-objective
problem can be written as follows:
Maximize 
))
(
...,
),
(
),
(
(
2
1
x
x
x
z
N
f
f
f
=
,
                (9)
 subject to 
m
}
1,0
{
∈
x
, and ∑=
≤
⋅
m
j
i
j
ij
c
x
w
1
 for 
N
i
...,
,1
=
,
              (10)
where each objective function is described in the following form:
∑=
⋅
=
m
j
j
ij
i
x
p
f
1
)
(x
 for 
N
i
...,
,1
=
.
              (11)
On the website of the first author of [3], problems with 100, 250, 500, and 750 items
for 2, 3 and 4 knapsacks are available. We employed the SPEA [3] and the NSGA-II
[6] as representative EMO algorithms. These algorithms are known as high
performance algorithms for multiobjective problems. We preliminarily defined the
parameters in both the algorithms as shown in Table 1. We employed a one point
crossover with the crossover rate 0.8, a bit-flip mutation with the mutation rate 0.01 to
each bit. In our local search, we specified the tournament size as 
6
=
t
, the selection
probability as 
1.0
LS =
p
, and the number of examined solution as 
3
=
k
. We
commonly used these parameters to the SPEA and the NSGA-II. We generated 30 sets
of different initial solutions and applied each of EMO algorithms to them.
After that we apply the three- and four-objective knapsack problems with 250, 500,
and 750 items to show the effectiveness of generalizing the replacement rule based on
the dominance relation.
4.2   Experimental Results on a Two-Objective Problem
We show the effectiveness of our local search on two-objective knapsack problems.
We employed a 750-item problem for two knapsacks to show non-dominated
solutions obtained by EMO algorithms with/without our local search. Therefore we
applied each of four EMO algorithms to the problem. In this case, we can not show
the effectiveness of generalization using the number of improved objective values d
because the value of d can vary one or two for two-objective problems. We can see,
however, the performance of the basic architecture of our local search with 
2
=
d
.
First we obtained 30 sets of non-dominated solutions by each algorithm. In order to
depict figures clearly, we employed the 50%-attainment surface [27]. An attainment
surface is a kind of trade-off surface obtained by a single run of an EMO algorithm.
And 50%-attainment surface shows the estimated attainment surface obtained by at
least 50% of multiple runs. Figs. 7 and 8 show the 50%-attainment surface of 30 sets
of non-dominated solutions obtained by four EMO algorithms. Each axis of the
figures shows the total profit of each knapsack. In a two objective problem, the total
profit of each of two knapsacks is maximized.

1242         T. Murata, S. Kaige, and H. Ishibuchi
From Figs. 7 and 8, we can see that the 50%-attaiment surface is improved by
introducing the local search in the area of compromised solutions. Considering only a
single objective, each of the original EMO algorithm (i.e., without the local search)
found better solutions. Since we employed the local search with 
2
=
d
, the local
search allows a solution to move to only a dominating solution in two objectives.
Therefore the attainment surface in the area of compromised solutions is improved by
the local search.
As for the local search using the generalized replacement rule with 
1
=
d
, we could
not obtain better attainment surfaces than the original EMO algorithms. As explained
in [23], a drawback of the replacement rule which allows a solution to move to non-
dominated solutions is that the current solution can be degraded by multiple moves.
That may be a reason why the local search with 
1
=
d
 was not effective to this
problem.
Table 1.  Parameter settings for EMO algorithms.
Problem
(objectives, items)
# of evaluations
Population size
Secondary Population Size
in SPEA
(2, 750)
125 000
250
100
(3, 250)
100 000
200
80
(3, 500)
125 000
250
100
(3, 750)
150 000
300
120
(4, 250)
125 000
250
100
(4, 500)
150 000
300
120
(4, 750)
175 000
350
140
No LS
d=2 LS
25000
26000
27000
28000
24000
25000
26000
27000
28000
Fig. 7.  50%-attainment surface obtained by
SPEA with/without local search.
No LS
d=2 LS
25000
26000
27000
28000
24000
25000
26000
27000
28000
Fig. 8.  50%-attainment surface obtained by
NSGA-II with/without local search.
4.3   Experimental Results on Three- and Four-Objective Problems
In the case of two-objective problems, we can depict the set distribution by two-
dimensional graphs, it is difficult, however, to show the set distribution by figures for
more than three-objective problems. In this paper, we use the coverage metric [3] to
evaluate two sets of non-dominated solutions obtained by two EMO algorithms. Let
X’, X’’
X
∈
 be two sets of non-dominated solutions. The coverage metric can be
defined as follows:
No LS
d = 2 LS
No LS
d = 2 LS

Generalization of Dominance Relation-Based Replacement Rules         1243
|’’
|/|
}’’
’
:’
’
|’’
’’
{
|
)’’
,’
(
X
a
a
X
a
X
a
X
X

∈
∃
∈
=
C
.
              (12)
The value 
1
)’’
,’
(
=
X
X
C
 means that all points in 
’’
X  are dominated by or equal to
points in X’. On the other hand, 
0
)’’
,’
(
=
X
X
C
 represents that no solutions in 
’’
X  are
covered by the set X’. It is noted that both 
)’’
,’
(
X
X
C
 and 
)’
,’’
(
X
X
C
 have to be
considered, since 
)’’
,’
(
X
X
C
 is not necessarily equal to 
)’
,’’
(
X
X
C
.
Table 2.  SPEA for 3-objective problems (250, 500, and 750 items).
3 objectives
No LS
d = 1
d = 2
d = 3
No LS
--
0.0311
0.0088
0.0179
d = 1
0.7444
--
0.1960
0.2123
d = 2
0.8437
0.5142
--
0.3671
d = 3
0.8044
0.4484
0.2796
--
Table 3.  NSGA-II for 3-objective problems (250, 500, and 750 items).
3 objectives
No LS
d = 1
d = 2
d = 3
No LS
--
0.0961
0.0862
0.1147
d = 1
0.4373
--
0.2274
0.2369
d = 2
0.4797
0.3283
--
0.3127
d = 3
0.4466
0.3326
0.2473
--
Table 4.  SPEA for 4-objective problems (250, 500, and 750 items).
 4 objectives
No LS
d = 1
d = 2
d = 3
d = 4
No LS
--
0.0038
0.0010
0.0001
0.0012
d = 1
0.8820
--
0.1488
0.1369
0.1697
d = 2
0.9353
0.4231
--
0.2394
0.2904
d = 3
0.9299
0.4356
0.2792
--
0.2976
d = 4
0.9161
0.3756
0.2122
0.2011
--
We applied the EMO algorithms with/without our local search to three three-
objective problems and three four-objective problems using the parameters in Table 1.
We varied the number of improved objective values d as 
3
,2
,1
=
d
 in three-objective
problems, and 
4
,3
,2
,1
=
d
 in four-objective problems. Therefore we had four variants
for each EMO algorithm in the case of three-objective problems, and five in the case
of four-objective problems. We compare two sets of non-dominated solutions using
the coverage metric, and calculate an average value over 30 trials. Tables 2–5 show
the summarization of the results for each EMO algorithm. We average the values of
the coverage over three different items. The second column of Tables 2 and 3 shows
that the sets obtained original algorithm is covered by those obtained by algorithms
with the proposed local search. For example, 0.8437 in the cell of the 4th row and the
second column in Table 2 shows that 84.37 % solutions obtained by the original EMO
algorithm (SPEA with No LS) are covered by solutions obtained by SPEA with the
local search (
2
=
d
). From Tables 2–5, we can see that the better sets obtained by our
local search with 
2
=
d
 for three-objective problems, and 
3
,2
=
d
 for four-objective
problems.

1244         T. Murata, S. Kaige, and H. Ishibuchi
Table 5.  NSGA-II for 4-objective problems (250, 500, and 750 items).
4 objectives
No LS
d = 1
d = 2
d = 3
d = 4
No LS
--
0.0431
0.0114
0.0114
0.0155
d = 1
0.4701
--
0.1165
0.1233
0.1112
d = 2
0.5898
0.3736
--
0.2418
0.2475
d = 3
0.5719
0.3551
0.2214
--
0.2122
d = 4
0.6022
0.3643
0.2447
0.2325
--
Due to the page limitation, we don’t show detail results on each problem. Further
information is shown in the first authors web (http://www.res.kutc.kansai-u.ac.jp/
~murata/research.html). Through the experiments, we found that the proposed local
search was effective in the case of larger problems. That is, it was more effective in
500-item problem than 250, and 750 than 500 for each of three- and four-objective
problems.
5   Conclusion and Future Works
In this paper, we generalized the replacement rules based on the dominance relation
for local search in multiobjective optimization. Simulation results on knapsack
problems with three- and four-objectives showed the effectiveness of the generalized
replacement rule by introducing the number of improved objectives. As shown in the
experimental results for the two-objective problems, the proposed local search is
weak to improve each objective value. We can improve such weakness of the local
search with the generalization rule. Since we employed the proposed only to knapsack
problems in this paper, we can also apply other types of problems such as permutation
problems, and function approximation problems.
References
[1]
J. D. Schaffer, “Multiple objective optimization with vector evaluated genetic
algorithms,” Proc. of 1st Int’l Conf. on Genetic Algorithms and Their Applications, pp.
93–100, 1985.
[2]
J. D. Knowles and D. W. Corne, “The Pareto archived evolution strategy: A new baseline
algorithm for Pareto multiobjective optimization,” Proc. of 1999 Congress on
Evolutionary Computation, pp. 98–105, 1999.
[3]
E. Zitzler and L. Thiele, “Multiobjective evolutionary algorithms: A comparative case
study and the strength Pareto approach,” IEEE Trans. on Evolutionary Computation, 3 –
Algorithms: Empirical Results,” Evolutionary Computation, 8 (2), pp. 173–195, 2000.
[6]
K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, “A fast and elitist multiobjective
genetic algorithm: NSGA-II,” IEEE Trans. on Evolutionary Computation, 6 (2), pp. 182–
197, 2002.
[7]
P. Merz and B. Freisleben, “Genetic local search for the TSP: New results,” Proc. of 4th
IEEE Int’l Conf. on Evolutionary Computation, pp. 159–164, 1997.
[8]
N. Krasnogor and J. Smith, “A memetic algorithm with self-adaptive local search: TSP as
a case study,” Proc. of 2000 Genetic and Evolutionary Computation Conf., pp. 987–994,
2000.
[9]
P. Moscato, “Memetic algorithms: A short introduction,” in D. Corne, F. Glover, and M.
Dorigo (eds.), New Ideas in Optimization, McGraw-Hill, pp. 219-234, Maidenhead, 1999.

Generalization of Dominance Relation-Based Replacement Rules         1245
[10] W. E. Hart, N. Krasnogor, and J. Smith (eds.), First Workshop on Memetic Algorithms
(WOMA I), in Proc. of 2000 Genetic and Evolutionary Computation Conf. Workshop
Program, pp. 95–130, 2000.
[11] W. E. Hart, N. Krasnogor, and J. Smith (eds.), Second Workshop on Memetic Algorithms
(WOMA II), in Proc. of 2001 Genetic and Evolutionary Computation Conf. Workshop
Program, pp. 137–179, 2001.
[12] W. E. Hart, N. Krasnogor, and J. Smith (eds.), Proc. of Third Workshop on Memetic
Algorithms (WOMA III), 2002.
[13] H. Ishibuchi and T. Murata, “Multi-objective genetic local search algorithm,” Proc. of 3rd
IEEE Int’l Conf. on Evolutionary Computation, pp. 119–124, 1996.
[14] H. Ishibuchi and T. Murata, “A multi-objective genetic local search algorithm and its
application to flowshop scheduling,” IEEE Trans. on Systems, Man, and Cybernetics -
Part C: Applications and Reviews, 28 (3), pp. 392–403, 1998.
[15] A. Jaszkiewicz, “Genetic local search for multi-objective combinatorial optimization,”
European Journal of Operational Research, 137 (1), pp. 50–71, 2002.
[16] J. D. Knowles and D. W. Corne, “M-PAES: A memetic algorithm for multiobjective
optimization,” Proc. of 2000 Congress on Evolutionary Computation, pp. 325–332, 2000.
[17] J. D. Knowles and D. W. Corne, “A comparison of diverse approaches to memetic
multiobjective combinatorial optimization,” Proc. of 2000 Genetic and Evolutionary
Computation Conf. Workshop Program, pp. 103–108, 2000.
[18] T. Murata, H. Nozawa, Y. Tsujimura, M. Gen, and H. Ishibuchi, “Effect of local search on
the performance of cellular multi-objective genetic algorithms for designing fuzzy rule-
based classification systems,” Proc. of 2002 Congress on Evolutionary Computation, pp.
663–668, 2002.
[19] H. Ishibuchi, T. Yoshida, and T. Murata, “Selection of initial solutions for local search in
multi-objective genetic local search,” Proc. of 2002 Congress on Evolutionary
Computation, pp. 950–955, 2002.
[20] K. Deb and T. Goel, “A hybrid multi-objective evolutionary approach to engineering
shape design,” Proc. of 1st Int’l Conf. on Evolutionary Multi-Criterion Optimization, pp.
385–399, 2001.
[21] E. Talbi, M. Rahoual, M. H. Mabed, and C. Dhaenens, “A hybrid evolutionary approach
for multicriteria optimization problems: Application to the flow shop,” Proc. of 1st Int’l
Conf. on Evolutionary Multi-Criterion Optimization, pp. 416–428, 2001.
[22] T. Murata, H. Nozawa, H. Ishibuchi, and M. Gen, “Modification of local search directions
for non-dominated solutions in cellular multiobjective genetic algorithms for pattern
classification problems,” Proc. of 2nd Int’l Conf. on Evolutionary Multi-Criterion
Optimization, 2003 (to appear).
[23] H. Ishibuchi, T. Yoshida, and T. Murata, “Balance between genetic search and local
search in memetic algorithms for multiobjective permutation flowshop scheduling,” IEEE
Trans. on Evolutionary Computation (to appear).
[24] K. Ikeda, H. Kita, and S. Kobayashi, “Failure of Pareto-based MOEAs: Does non-
dominated really mean near to optimal?” Proc. of 2001 Congress on Evolutionary
Computation, pp. 957–962, 2001.
[25] M. Laumanns, L. Thiele, K. Deb, and E. Zitzler, “Combining convergence and diversity
in evolutionary multiobjective optimization,” Evolutionary Computation, 10 (3), pp. 263–
282, 2002.
[26] N. Krasnogor, “Studies on the theory and design space of memetic algorithms,” Ph. D.
Thesis, University of the West of England, Bristol, June 2002.
[27] C. M. Fonseca and P. J. Fleming, “On the performance assessment and comparison of
stochastic multiobjective optimizers,” Proc. of Parallel  Problem Solving from Nature IV,
pp. 584–593, 1996.

Author Index
Abbass, Hussein A.
483, 1612
Acan, Adnan
695
Ackley, David H.
150
Aguilar Contreras, Andr´es
874
Aguilar-Ruiz, Jes´us S.
979
Ahn, Byung-Ha
1610
Akama, Kiyoshi
1222, 1616
Alba, Enrique
955
Almeida, Jonas
1776
Ando, Shin
1926
Angelov, P.P.
1938
Aporntewan, Chatchawit
1566
Araujo, Lourdes
1951
Armstrong, Marc P.
1530
Arnold, Dirk V.
525
Arslan, Tughrul
1614
Asai, Kiyoshi
2288
Auger, Anne
512
Aune, Thomas
2277
Aupetit, S.
140
Azad, R. Muhammad Atif
1626
Bacardit, Jaume
1818
Balan, Gabriel Catalin
1729
Balan, M. Sakthi
425
Ballester, Pedro J.
706
Banerjee, Nilanjan
2179
Banzhaf, Wolfgang
390
Baraglia, R.
2109
Barbosa, Helio J.C.
718
Baresel, Andr´e
2428, 2442
Barfoot, Timothy D.
377
Barkaoui, Mohamed
646
Barry, Alwyn
1832
Barz, Christiane
754
Bates Congdon, Clare
2034
Baugh Jr., John W.
730
Behrens, Ivesa
754
Beielstein, Thomas
1963
Belle, Terry Van
150
Benekohal, Rahim F.
2420
Benson, Karl
1690
Berger, Jean
646
Beyer, Hans-Georg
525
Bhanu, Bir
332, 2227
Blackwell, T.M.
1
Blain, Derrel
413
Blindauer, Emmanuel
369
Bonacina, Claudio
1382
Boryczka, Mariusz
142
Botello Rionda, Salvador
573
Bottaci, Leonardo
2455
Bouvry, Pascal
1369
Bouzerdoum, Abdesselam
742
Bowersox, Rodney
2157
Branke, J¨urgen
537, 754, 766, 1568
Brown, Martin
778
Buason, Gunnar
458
Bucci, Anthony
250
Buckles, Bill P.
1624
Bull, Larry
1924
Burke, Edmund
1800
Buswell, R.A.
1938
Butz, Martin V.
1844, 1857, 1906
Camens, Doug
1590
Cant´u-Paz, Erick
790, 801
Cardona, Cesar
219
Carter, Jonathan N.
706
Carvalho, Andre de
634
Castillo, Flor
1975
Cattron, James
298
Chafekar, Deepti
813
Cheang, Sin Man
1802, 1918
Chen, Hui
379
Chen, Ping
1986
Chen, Ying-Ping
825, 837, 1620
Chen, Yu-Hung
681
Choe, Heemahn
850
Choi, Sung-Soon
850, 862, 1998
Choi, Yoon-Seok
2010
Chong, H.W.
2396
Chongstitvatana, Prabhas
1566
Chuang, Yue-Ru
681
Clark, John A.
146, 2022
Clergue, Manuel
1788
Coello Coello, Carlos A.
158, 573, 640
Collard, Philippe
1788
Collins, J.J.
1320
Cruz Cort´es, Nareli
158

Author Index
Culberson, Joseph
948
Cutello, Vincenzo
171, 1570
Czech, Zbigniew J.
142
D’Eleuterio, Gabriele M.T.
377
Daida, Jason M.
1639, 1652, 1665
Dasgupta, Dipankar
183, 195, 219, 1580
Dawson, Devon
1870
Delbem, A.C.B.
634
Dick, Grant
1572
Dijk, Steven van
886
Divina, Federico
898, 1574
Doerner, Karl
2465
Dongarra, Jack
1015
Doya, Kenji
507
Dozier, Gerry
561
Drake, Stephen
1576
Droste, Stefan
909
Dubreuil, Marc
1578
Eikelder, Huub M.M. ten
344
Elfwing, Stefan
507
Elliott, Lionel
2046
Eppstein, Maggie
967
Espinoza, Felipe P.
922
Esterline, Albert
657
Ewald, Claus-Peter
1963
Falke II, William Joseph
1920
Fan, Zhun
1029, 1764, 2058
Fatiregun, Deji
2511
Feinstein, Mark
61
Ferguson, Michael I.
442
Fern´andez, Francisco
1812, 2109
Ficici, Sevan G.
286
Fischer, Marco
2398
Forman, Sean L.
2072
Foster, James A.
456, 2313
Fry, Rodney
1804
Fu, Zhaohui
1986
Fukami, Kiichiro
2134
Furutani, Hiroshi
934
Gaag, Linda C. van der
886
Gagn´e, Christian
1578
Galeano, Germ´an
1812
Gallagher, John C.
431, 454
Galos, Peter
1806
Gao, Yong
948
Garnica, O.
2109
Garrell, Josep Maria
1818
Garrett, Deon
1469
Garway-Heath, David
2360
Garzon, Max H.
379, 413
G´erard, Pierre
1882
Giacobini, Mario
955
Gilbert, Joshua
967
Gir´aldez, Ra´ul
979
Goldberg,
David
E.
801, 825, 837, 922,
1172, 1271, 1332, 1554, 1620, 1844, 1857,
1906
G´omez, D.
243
Gomez, Faustino J.
2084
G´omez, Jonatan
195, 1580
Gonz´alez, Fabio
195, 219, 1580
Goodman, Erik D.
1029, 1764, 2058,
2121
Green, James
1975
Greene, William A.
1582
Guinot, C.
140
Guo, Haipeng
1584
Guo, Pei F.
322
Gustafson, Steven
1800
Gutjahr, Walter J.
2465
Gwaltney, David A.
442
Hahn, Lance W.
2412
Hallam, Jonathan
1586
Hamza, Karim
2096
Han, Kuk-Hyun
427, 2147
Hanby, V.I.
1938
Handa, Hisashi
991
Hang, Dehua
13
Harman, Mark
2511, 2513
Hart, Emma
1295
Hayasaka, Taichi
1600
Heckendorn, Robert B.
1003
Hermida, R.
2109
Hern´andez Aguirre, Arturo
573, 2264
Heywood, Malcolm I.
2325
Hidalgo, J.I.
2109
Hierons, Robert
2511, 2513
Hilbers, Peter A.J.
344
Hilss, Adam M.
1639, 1652, 1665
Hiroshima, Koji
2134
Hiroyasu, Tomoyuki
1015
Holcombe, Mike
2488
Holder, Mike
2400
Holiﬁeld, Gregory A.
1588
Homaifar, Abdollah
657

Author Index
Horn, Jeﬀrey
298
Hornby, Gregory S.
1678
Howard, Daniel
1690
Hsu, William H.
1584
Hu, Jianjun
1029, 1764, 2058
Hu, Shangxu
134
Huang, Chien-Feng
1041
Huang, Gaofeng
1053
Huang, Yong
2121
Huang, Zhijian
2121
Hussain, Mudassar
657
Iba, Hitoshi
470, 1259, 1715, 1816, 1926,
2288
Ichikawa, Kazuo
2134
Ichikawa, Manabu
2134
Ingham, Derek B.
2046
Ishibuchi, Hisao
1065, 1077, 1234
Isono, Katsunori
2288
Jacob, Jeremy L.
2022
J¨ahn, Hendrik
2398
Jang, Jun-Su
2147
Jansen, Thomas
310
Jin, Yaochu
636
Johnson, Paul
156
Jong, Edwin D. de
262, 274
Jong, Kenneth De
1604
Julstrom, Bryant A.
2402
Just, Winfried
154
Kaegi, Simon
122
Kaige, Shiori
1234
Kalisker, Tom
1590
Kamio, Shotaro
470
Karr, Charles L.
2157, 2400, 2404, 2406
Kavka, Carlos
1089
Keijzer, Maarten
898, 1574, 1752
Kelsey, Johnny
207
Kendall, Graham
1800
Kharma, Nawwaf
322
Khoshgoftaar, Taghi M.
1808
Kim, Jong-Hwan
427, 638, 2147
Kim, Jong-Pil
2408
Kim, Jung-Hwan
1101
Kim, Keum Joo
642
Kim, Yong-Geon
2426
Kim, Yong-Hyuk
1112, 1123, 1136, 1345,
2168, 2215, 2410
Kimiaghalam, Bahram
657
Klein, Jon
61
Klinkenberg, Ralf
1606
Kohl, Nate
356
Kondo, Shoji
2134
Korczak, Jerzy
369
Kordon, Arthur
1975
Kramer, Gregory R.
454
Krawiec, Krzysztof
332
Kumar, Rajeev
1592, 2179
Kumar, Sujay V.
730
Kwan, Raymond S.K.
693
Kwok, N.M.
2191
Kwon, Yung-Keun
1112, 2203, 2426
Kwong, Sam
2191, 2396
Kyne, Adrian G.
2046
Labroche, Nicolas
25
Lai, Eugene
681
Lanchares, J.
2109
Langdon, W.B.
1702
Lanzi, Pier Luca
1894, 1922
Lattaud, Claude
144
Le Bris, Claude
512
Lee, Chi-Ho
638
Lee, Kin Hong
1802, 1918
Lee, Seung-Kyu
2168, 2215
Leﬂey, Martin
2477
Leier, Andr´e
390
Lemonge, Afonso C.C.
718
Leung, Kwong-Sak
585, 1160, 1802, 1918
Li, Gaoping
2121
Li, Hsiaolei
1665
Li, Xiaodong
37
Liang, Yong
585, 1160
Liekens, Anthony M.L.
344
Lim, Andrew
1053, 1594, 1596, 1986
Lin, Kuo-Chi
1541
Lin, Yingqiang
2227
Lipson, Hod
1518
Liszkai, Tam´as
2418
Liu, Hongwei
1715
Liu, Xiaohui
2360
Liu, Yi
1808
Liz´arraga, Giovanni
573
Llor`a, Xavier
1172
Long, Stephen L.
1652
Louis, Sushil J.
2424
Lu, Ming
1148
Luke, Sean
1729, 1740

Author Index
Mahdavi, Kiarash
2513
Majumdar, Nivedita Sumi
183
Mancoridis, Spiros
2499
Marchiori, Elena
898, 1574
M˘arginean, Flaviu Adrian
1184
Mar´ın-Bl´azquez, Javier G.
1295
Markon, Sandor
1963
Marshall, Kenric
1975
Mart´ın-Vide, Carlos
401
Masahiro, Hiji
2337
Masaru, Tezuka
2337
Matsui, Shouichi
1598, 2240
Mauch, Holger
1810
Mazza, Raymond H.
2034
McMinn, Phil
2488
McQuay, Brian N.
2384
Mera, Nicolae S.
2046
Messimer, Sherri
2406
Mezura-Montes, Efr´en
640
Miikkulainen, Risto
356, 2084
Miki, Mitsunori
1015
Minaei-Bidgoli, Behrouz
2252
Minsker, Barbara S.
922
Misevicius, Alfonsas
598
Mistry, Paavan
693
Mitavskiy, Boris
1196
Mitchell, Brian S.
2499
Mitrana, Victor
401
Mitsuhashi, Hideyuki
470
Mohan, Chilukuri
110
Monmarch´e, Nicolas
25, 140
Moon, Byung-Ro
669, 850, 862, 1101,
1112, 1123, 1136, 1345, 1357, 1998, 2010,
2168, 2203, 2215, 2408, 2410, 2426
Moore, Jason H.
2277, 2412
Morrison, Ronald W.
1210
Moura Oliveira, P.B. de
510
Mueller, Rainer
742
Munetomo, Masaharu
1222, 1616
Murao, Naoya
1222
Murata, Tadahiko
1234
Myodo, Emi
2264
Myung, Hyun
638
Nasraoui, Olfa
219
Neel, Andrew
379
Nguyen, Minh Ha
483
Nicolau, Miguel
1752
Nicosia, Giuseppe
171
Ni˜no, F.
243
Nordin, Peter
495, 1806
Northern, James III
2414
Ocenasek, Jiri
1247
Oda, Terri
122, 231
Ofria, Charles
13
Oka, Kazuhiro
1814
Okabe, Tatsuya
636
Olsen, Anne
2416
Ols´en, Joel
1806
Olsen, Nancy
2277
Oppacher, Franz
1481, 1493
Orla Kimbrough, Steven
1148
Osadciw, Lisa Ann
110
Palacios-Durazo, Ram´on Alfonso
371
Palmes, Paulito P.
1600
Panait, Liviu
1729, 1740
Pappalardo, Francesco
1570
Parizeau, Marc
1578
Park, Sung-Joon
1602
Paul, Topon Kumar
1259
Pauw, Guy De
549
Pavone, Mario
171
Pei, Min
2121
Pelikan, Martin
1247, 1271
Peram, Thanmaya
110
Perego, R.
2109
P´erez-Jim´enez, Mario J.
401
Perry, Chris
61
Pohlheim, Hartmut
2428
Pollack, Jordan B.
250, 274, 286
Popovici, Elena
1604
Pourkashanian, Mohamed
2046
Powell, David J.
2347
Pr¨ugel-Bennett, Adam
1586
Punch, William F.
1431, 2252
Raich, Anne
2418
Ramos, Fernando
429
Ramos, Fernando Manuel
375
Ranji Ranjithan, S.
1622
Rasheed, Khaled
813
Reif, David M.
2277
Reyes-Luna, Juan F.
2096
Ringn´er, Kristofer Sund´en
1806
Riquelme, Jos´e C.
979
Ritthoﬀ, Oliver
1606
Rockett, Peter
1592
Rodebaugh, Brandon
148

Author Index
Rodrigues, Brian
1594, 1986
Rodriguez-Tello, Eduardo
1283
Rojas, Carlos
219
Rosenberg, Ronald C.
1029, 1764, 2058
Ross, Peter
1295, 1920
Rothlauf, Franz
1307, 1608
Rowe, Jonathan E.
874, 1505
Russell, Matthew
146
Ruwei, Dai
152
Ryan, Conor
1320, 1626, 1752
Sadeghipour, Sadegh
2428
Saeki, Shusuke
2288
Saitou, Kazuhiro
2096
S´anchez, J.M.
2109
Sancho-Caparrini, Fernando
401
Sano, Masaki
1015
Santos, Eugene Jr.
642
Santos, Eunice E.
642
Saraﬁs, Ioannis
2301
Sastry, Kumara
1332, 1554, 1857
Sayyarodsari, Bijan
657
Schiavone, Guy
1541
Schmeck, Hartmut
1568
Schmidt, Christian
766
Schmidt, Thomas M.
13
Schmitt, Lothar M.
373
Schoenauer, Marc
512, 1089
Schulenburg, Sonia
1295
Schwarz, Josef
1247
Sciortino, John C. Jr.
2384
Scott, Douglas A.
2404
Sendhoﬀ, Bernhard
636
Seo, Dong-Il
669, 1345, 1357
Seo, Kisung
1029, 1764, 2058
Seredy´nski, Franciszek
1369
Settles, Matthew
148
Shanblatt, Michael
2414
Shepherd, Rob
456
Shepperd, Martin J.
2477
Sheu, Shiann-Tsong
681
Shibata, Youhei
1065
Shimohara, Katsunori
74
Shimosaka, Hisashi
1015
Shyu, Conrad
2313
Sigaud, Olivier
1882
Silva, Sara
1776
Singh, Vishnu
2157
Slimane, M.
140
Smith, R.E.
1382
Smith, Robert E.
778
Soak, Sang-Moon
1610
Socha, Krzysztof
49
Solteiro Pires, E.J.
510
Song, Dong
2325
Soule, Terence
148
Sousa, Fabiano Luis de
375
Spector, Lee
61
Stacey, A.
2422
Stein, Gunnar
644
Stein, Michael
1568
Stephens, Christopher R.
874, 1394
Stepney, Susan
146, 2022
Sthamer, Harmen
2442
Stone, Christopher
1924
Stone, Peter
356
Storch, Tobias
1406
Streeter, Matthew J.
1418
Streichert, Felix
610, 644
Suen, Ching Y.
322
Sun, Dazhi
2420
Suzuki, Tomoya
1814
Takahashi, Katsutoshi
2288
Takama, Yasufumi
246
Tanabe, Shoko
2134
Tanaka, Kiyoshi
2134, 2264
Tanev, Ivan
74
Tang, Ricky
1665
Tarakanov, Alexander O.
248
Teich, Tobias
2398
Tekol, Y¨uce
695
Tenreiro Machado, J.A.
510
Teo, Jason
483, 1612
Thangavelautham, Jekanthan
377
Tharakunnel, Kurian K.
1906
Thierens, Dirk
886
Tian, Lirong
1614
Timmis, Jon
207
Tokoro, Ken-ichi
1598, 2240
Tomassini, Marco
955, 1788, 1812, 2109
Tominaga, Kazuto
1814
Tong, Siu
2347
Topchy, Alexander
1431
Torng, Eric
13
Torres-Jimenez, Jose
1283
Toussaint, Marc
86, 1444
Trinder, Phil
2301
Truong, T.Q.S.
2422
Tsuji, Miwako
1616

Author Index
Tsutsui, Shigeyoshi
1015
Tucker, Allan
2360
Tyrrell, Andy
1804
Uchibe, Eiji
507
Ueno, Yutaka
2288
Ulmer, Holger
610, 644
Upal, M. Afzal
98
Usui, Shiro
1600
Valenzuela-Rend´on, Manuel
371, 1457
Vallejo, Edgar E.
429
Vanneschi, Leonardo
1788, 1812
Veeramachaneni, Kalyan
110
Vejar, R.
243
Venturini, Gilles
25, 140
Vigraham, Saranyan
431
Vlassov, Valeri
375
Vose, Michael D.
1505
Wakaki, Hiromi
1816
Wallace, Jeﬀ
2424
Waller, S. Travis
2420
Wallin, David
1320
Wang, Wei
537
Ward, David J.
1652
Watanabe, Isamu
1598, 2240
Watson, Jean-Paul
1469
Wegener, Ingo
622, 1406
West, Michael
413
White, Bill C.
2277
White, Tony
122, 231
Whiteson, Shimon
356
Whitley, Darrell
1469
Wieczorek, Wojciech
142
Wiegand, R. Paul
310
Wilson, Chritopher W.
2046
Wilson, Eric
2406
Wineberg, Mark
1481, 1493
Witt, Carsten
622
Wolﬀ, Krister
495
Wood, David Harlan
1148
Wright, Alden H.
1003, 1505
Wright, J.A.
1938
Wu, Annie S.
1541, 1588, 2384
Wu, D.J.
1148
Wyatt, Danica
1518
Xianghui, Dong
152
Xiao, Fei
1594
Xiao, Ningchuan
1530
Xiao, Ying
154
Xu, Zhou
1596
Xuan, Jiang
813
Yamamoto, Takashi
1077
Yamamura, Masayuki
1602
Yang, Jinn-Moon
2372
Yang, Seung-Jin
2426
Yang, Shengxiang
1618
Yassine, Ali
1620
Yilmaz, Ayse S.
2384
Yu, Han
1541, 2384
Yu, Huanjun
134
Yu, Senhua
183
Yu, Tian-Li
1554, 1620
Yu, Tina
156
Yuan, Xiaohui
1624
Yuchi, Ming
638
Yue, Yading
2072
Zalzala, Ali
2301
Zamora, Adolfo
1394
Zechman, Emily M.
1622
Zell, Andreas
610, 644
Zhang, Jian
1624
Zhang, Liping
134
Zhang, Y.
1938
Ziemke, Tom
458
Zincir-Heywood, A. Nur
2325
Zomaya, Albert Y.
1369

GECCO 2003 Conference Organization
Conference Committee
General Chair: James A. Foster
Proceedings Editor-in-Chief: Erick Cant´u-Paz
Business Committee: David E. Goldberg, John Koza, J.A. Foster
Chairs of Program Policy Committees:
A-Life, Adaptive Behavior, Agents, and Ant Colony Optimization,
Russell Standish
Artiﬁcial Immune Systems, Dipankar Dasgupta
Coevolution, Graham Kendall
DNA, Molecular, and Quantum Computing, Natasha Jonoska
Evolution Strategies, Evolutionary Programming, Hans-Georg Beyer
Evolutionary Robotics, Mitchell A. Potter and Alan C. Schultz
Evolutionary Scheduling and Routing, Kathryn A. Dowsland
Evolvable Hardware, Julian Miller
Genetic Algorithms, Kalyanmoy Deb
Genetic Programming, Una-May O’Reilly
Learning Classiﬁer Systems, Stewart Wilson
Real-World Applications, David Davis, Rajkumar Roy
Search-Based Software Engineering, Mark Harman and Joachim Wegener
Workshops Chair: Alwyn Barry
Late-Breaking Papers Chair: Bart Rylander
Workshop Organizers
Biological Applications for Genetic and Evolutionary Computation (Bio GEC
2003), Wolfgang Banzhaf, James A. Foster
Application of Hybrid Evolutionary Algorithms to NP-Complete Problems, Fran-
cisco Baptista Pereira, Ernesto Costa, G¨unther Raidl
Evolutionary Algorithms for Dynamic Optimization Problems, J¨urgen Branke
Hardware Evolutionary Algorithms and Evolvable Hardware (HEAEH 2003),
John C. Gallagher
Graduate Student Workshop, Maarten Keijzer, Sean Luke, Terry Riopka
Workshop on Memetic Algorithms 2003 (WOMA-IV), Peter Merz, William E.
Hart, Natalio Krasnogor, Jim E. Smith
Undergraduate Student Workshop, Mark M. Meysenburg
Learning, Adaptation, and Approximation in Evolutionary Computation, Sibylle
Mueller, Petros Koumoutsakos, Marc Schoenauer, Yaochu Jin, Sushil Louis,
Khaled Rasheed
Grammatical Evolution Workshop (GEWS 2003), Michael O’Neill, Conor Ryan
Interactive Evolutionary Search and Exploration Systems, Ian Parmee

XII
Organization
Analysis and Design of Representations and Operators (ADoRo 2003), Franz
Rothlauf, Dirk Thierens
Challenges in Real-World Optimisation Using Evolutionary Computing,
Rajkumar Roy, Ashutosh Tiwari
International Workshop on Learning Classiﬁer Systems, Wolfgang Stolzmann,
Pier-Luca Lanzi, Stewart Wilson
Tutorial Speakers
Parallel Genetic Algorithms, Erick Cant´u-Paz
Using Appropriate Statistics, Steﬀan Christiensen
Multiobjective Optimization with EC, Carlos Coello
Making a Living with EC, Yuval Davidor
A Uniﬁed Approach to EC, Ken DeJong
Evolutionary Robotics, Dario Floreano
Immune System Computing, Stephanie Forrest
The Design of Innovation & Competent GAs, David E. Goldberg
Genetic Algorithms, Robert Heckendorn
Evolvable Hardware Applications, Tetsuya Higuchi
Bioinformatics with EC, Daniel Howard
Visualization in Evolutionary Computation, Christian Jacob
Data Mining and Machine Learning, Hillol Kargupta
Evolvable Hardware, Didier Keymeulen
Genetic Programming, John Koza
Genetic Programming Theory I & II, William B. Langdon, Riccardo Poli
Ant Colony Optimization, Martin Middendorf
Bionics: Building on Biological Evolution, Ingo Rechenberg
Grammatical Evolution, C. Ryan, M. O’Neill
Evolution Strategies, Hans-Paul Schwefel
Quantum Computing, Lee Spector
Anticipatory Classiﬁer Systems, Wolfgang Stolzmann
Mathematical Theory of EC, Michael Vose
Computational Complexity and EC, Ingo Wegener
Software Testing via EC, J. Wegener, M. Harman
Testing & Evaluating EC Algorithms, Darrell Whitley
Learning Classiﬁer Systems, Stewart Wilson
Evolving Neural Network Ensembles, Xin Yao
Neutral Evolution in EC, Tina Yu
Genetics, Annie S. Wu
Keynote Speakers
John Holland, “The Future of Genetic Algorithms”
Richard Lenski, “How the Digital Leopard Got His Spots: Thinking About Evo-
lution Inside the Box”

Organization
XIII
Members of the Program Committee
Hussein Abbass
Adam Adamopoulos
Alexandru Agapie
Jos´e Aguilar
Jes´us Aguilar
Hern´an Aguirre
Chang Wook Ahn
Uwe Aickelin
Enrique Alba
Javier Alcaraz Soria
Dirk Arnold
Tughrul Arslan
Atif Azad
Meghna Babbar
Vladan Babovic
B.V. Babu
Thomas B¨ack
Julio Banga
Francisco Baptista Pereira
Alwyn Barry
Cem Baydar
Thomas Beielstein
Theodore Belding
Fevzi Belli
Ester Bernado-Mansilla
Tom Bersano-Begey
Hugues Bersini
Hans-Georg Beyer
Filipic Bogdan
Andrea Bonarini
Lashon Booker
Peter Bosman
Terry Bossomaier
Klaus Bothe
Leonardo Bottaci
J¨urgen Branke
Wilker Bruce
Peter Brucker
Anthony Bucci
Dirk Bueche
Magdalena Bugajska
Larry Bull
Edmund Burke
Martin Butz
Stefano Cagnoni
Xiaoqiang Cai
Erick Cant´u-Paz
Uday Chakraborty
Weng-Tat Chan
Alastair Channon
Ying-Ping Chen
Shu-Heng Chen
Junghuei Chen
Prabhas Chongstitvatana
John Clark
Lattaud Claude
Manuel Clergue
Carlos Coello Coello
David Coley
Philippe Collard
Pierre Collet
Clare Bates Congdon
David Corne
Ernesto Costa
Peter Cowling
Bart Craenen
Jose Crist´obal Riquelme
Santos
Keshav Dahal
Paul Darwen
Dipankar Dasgupta
Lawrence Davis
Anthony Deakin
Kalyanmoy Deb
Ivanoe De Falco
Hugo De Garis
Antonio Della Cioppa
A. Santos Del Riego
Brahma Deo
Dirk Devogelaere
Der-Rong Din
Phillip Dixon
Jose Dolado Cosin
Marco Dorigo
Keith Downing
Kathryn Dowsland
Gerry Dozier
Rolf Drechsler
Stefan Droste
Marc Ebner
R. Timothy Edwards
Norberto Eiji Nawa
Aniko Ekart
Christos Emmanouilidis
Hector Erives
Felipe Espinoza
Matthew Evett
Zhun Fan
Marco Farina
Robert Feldt
Francisco Fern´andez
Sevan Ficici
Peter John Fleming
Stuart Flockton
Dario Floreano
Cyril Fonlupt
Carlos Fonseca
Stephanie Forrest
Alex Freitas
Clemens Frey
Chunsheng Fu
Christian Gagne
M. Gargano
Ivan Garibay
Josep Maria Garrell i Guiu
Alessio Gaspar
Michel Gendreau
Zhou Gengui
Pierre G´erard
Andreas Geyer-Schulz
Tushar Goel
Fabio Gonzalez
Jens Gottlieb
Kendall Graham
Buster Greene
John Grefenstette
Darko Grundler
Dongbing Gu
Steven Gustafson
Charles Guthrie
Pauline Haddow
Hani Hagras

XIV
Organization
Hisashi Handa
Georges Harik
Mark Harman
Emma Hart
William Hart
Inman Harvey
Michael Herdy
Jeﬀrey Hermann
Arturo Hern´andez
Aguirre
Francisco Herrera
J¨urgen Hesser
Robert Hierons
Mika Hirvensalo
John Holmes
Tadashi Horiuchi
Daniel Howard
William Hsu
Jianjun Hu
Jacob Hurst
Hitoshi Iba
Kosuke Imamura
I˜nnaki Inza
Christian Jacob
Thomas Jansen
Segovia Javier
Yaochu Jin
Bryan Jones
Natasha Jonoska
Hugues Juille
Bryant Julstrom
Mahmoud Kaboudan
Charles Karr
Balakrishnan Karthik
Sanza Kazadi
Maarten Keijzer
Graham Kendall
Didier Keymeulen
Michael Kirley
Joshua Knowles
Gabriella Kokai
Arthur Kordon
Bogdan Korel
Erkan Korkmaz
Tim Kovacs
Natalio Krasnogor
Kalmanje Krishnakumar
Renato Krohling
Sam Kwong
Gary Lamont
William Langdon
Pedro Larra˜nnaga
Jesper Larse
Marco Laumanns
Paul Layzell
Martin Leﬂey
Claude Le Pape
Kwong Sak Leung
Warren Liao
Derek Linden
Michael Littman
Xavier Llora
Fernando Lobo
Jason Lohn
Michael Lones
Sushil Louis
Manuel Lozano
Jose Antonio Lozano
Jose Lozano
Pier Luca Lanzi Sean Luke
John Lusth
Evelyne Lutton
Nicholas Macias
Ana Madureira
Spiros Mancoridis
Martin Martin
Pete Martin
Arita Masanori
Iwata Masaya
Keith Mathias
Dirk Mattfeld
Giancarlo Mauri
David Mayer
Jon McCormack
Robert McKay
Nicholas McPhee
Lisa Meeden
J¨orn Mehnen
Karlheinz Meier
Ole Mengshoel
Mark Meysenburg
Zbigniew Michalewicz
Martin Middendorf
Risto Miikkulainen
Julian Miller
Brian Mitchell
Chilukuri Mohan
David Montana
Byung-Ro Moon
Frank Moore
Alberto Moraglio
Manuel Moreno
Yunjun Mu
Sibylle Mueller
Masaharu Munetomo
Kazuyuki Murase
William Mydlowec
Zensho Nakao
Tomoharu Nakashima
Olfa Nasraoui
Bart Naudts
Mark Neal
Chrystopher Nehaniv
David Newth
Miguel Nicolau
Nikolay Nikolaev
Fernando Nino
Stefano Nolﬁ
Peter Nordin
Bryan Norman
Wim Nuijten
Leandro Nunes De Castro
Gabriela Ochoa
Victor Oduguwa
Charles Ofria
Gustavo Olague
Markus Olhofer
Michael O’Neill
Una-May O’Reilly
Franz Oppacher
Jim Ouimette
Charles Palmer
Liviu Panait
Gary Parker
Anil Patel
Witold Pedrycz
Martin Pelikan
Marek Perkowski

Organization
XV
Sanja Petrovic
Hartmut Pohlheim
Riccardo Poli
Tom Portegys
Reid Porter
Marie-Claude Portmann
Mitchell A. Potter
Walter Potter
Jean-Yves Potvin
Dilip Pratihar
Alexander Pretschner
Adam Pr¨ugel-Bennett
William Punch
G¨unther Raidl
Khaled Rasheed
Tom Ray
Tapabrata Ray
Victor Rayward-Smith
Patrick Reed
John Reif
Andreas Reinholz
Rick Riolo
Jose Riquelme
Denis Robilliard
Katya Rodriguez-Vazquez
Marc Roper
Brian Ross
Franz Rothlauf
Jon Rowe
Rajkumar Roy
G¨unter Rudolph
Thomas Runarsson
Conor Ryan
Bart Rylander
Kazuhiro Saitou
Ralf Salomon
Eugene Santos
Kumara Sastry
Yuji Sato
David Schaﬀer
Martin Schmidt
Thorsten Schnier
Marc Schoenauer
Sonia Schulenburg
Alan C. Schultz
Sandip Sen
Bernhard Sendhoﬀ
Kisung Seo
Franciszek Seredynski
Jane Shaw
Martin Shepperd
Alaa Sheta
Robert Shipman
Olivier Sigaud
Anabela Sim˜oes
Mark Sinclair
Abhishek Singh
Andre Skusa
Jim Smith
Robert Smith
Donald Sofge
Alan Soper
Terence Soule
Lee Spector
Andreas Spillner
Russell Standish
Harmen Sthamer
Adrian Stoica
Wolfgang Stolzmann
Matthew Streeter
V. Sundararajan
Gil Syswerda
Walter Tackett
Keiki Takadama
Uwe Tangen
Alexander Tarakanov
Ernesto Tarantino
Gianluca Tempesti
Hugo Terashima-Marin
Sam Thangiah
Scott Thayer
Lothar Thiele
Dirk Thierens
Adrian Thompson
Jonathan Thompson
Jonathan Timmis
Ashutosh Tiwari
Marco Tomassini
Andy Tomlinson
Jim Torresen
Paolo Toth
Michael Trick
Shigeyoshi Tsutsui
Andy Tyrrell
Jano Van Hemert
Clarissa Van Hoyweghen
Leonardo Vanneschi
David Van Veldhuizen
Robert Vanyi
Manuel Vazquez-
Outomuro
Oswaldo V´elez-Langs
Hans-Michael Voigt
Roger Wainwright
Matthew Wall
Jean-Paul Watson
Ingo Wegener
Joachim Wegener
Karsten Weicker
Peter Whigham
Ronald While
Darrell Whitley
R. Paul Wiegand
Kay Wiese
Dirk Wiesmann
Janet Wile
Janet Wiles
Wendy Williams
Stewart Wilson
Mark Wineberg
Alden Wright
Annie Wu
Zheng Wu
Chia-Hsuan Yeh
Ayse Yilmaz
Tian-Li Yu
Tina Yu
Hongnian Yu
Ricardo Zebulum
Andreas Zell
Byoung-Tak Zhang
Lyudmila A. Zinchenko

A Word from the Chair of ISGEC
You may have just picked up your proceedings, in hard copy and CD-ROM,
at GECCO 2003, or purchased it after the conference. You’ve doubtless already
noticed the new format – publishing our proceedings as part of Springer’s Lecture
Notes in Computer Science (LNCS) series will make them available in many more
libraries, broadening the impact of the GECCO conference dramatically!
If you attended GECCO 2003, we, the organizers, hope your experience was
memorable and productive, and you have found these proceedings to be of conti-
nuing value. The opportunity for ﬁrst-hand interaction among authors and other
participants in GECCO is a big part of what makes it exciting, and we all hope
you came away with many new insights and ideas.
If you were unable to come to GECCO 2003 in person, I hope you’ll ﬁnd many
stimulating ideas from the world’s leading researchers in evolutionary computa-
tion reported in these proceedings, and that you’ll be able to participate in a
future GECCO – for example, next year, in Seattle!
The International Society for Genetic and Evolutionary Computation, the
sponsoring organization of the annual GECCO conferences, is a young orga-
nization, formed through the merger of the International Society for Genetic
Algorithms (sponsor of the ICGA conferences) and the organization responsible
for the annual Genetic Programming conferences. It depends strongly on the
voluntary eﬀorts of many of its members. It is designed to promote not only the
exchange of ideas among innovators and practitioners of well-known methods
such as genetic algorithms, genetic programming, evolution strategies, evolutio-
nary programming, learning classiﬁer systems, etc., but also the growth of newer
areas such as artiﬁcial immune systems, evolvable hardware, agentbased search,
and others. One of the founding principles is that ISGEC operates as a confe-
deration of groups with related but distinct approaches and interests, and their
mutual prosperity is assured by their representation in the program committees,
editorial boards, etc., of the conferences and journals with which ISGEC is asso-
ciated. This also insures that ISGEC and its functions continue to improve and
evolve with the diversity of innovation that has characterized our ﬁeld.
ISGEC has seen many changes this year, in addition to its growth in mem-
bership. We have completed the formalities for recognition as a tax-exempt cha-
ritable organization. We have created the new designations of Fellow and Senior
Fellow of ISGEC to recognize the achievements of leaders in the ﬁeld, and by the
time you read this, we expect to have elected the ﬁrst cohort. Additional Fellows
and Senior Fellows will be added annually. GECCO continues to be subject to
dynamic development – the many new tutorials, workshop topics, and tracks
will evolve again next year, seeking to follow and encourage the developments of
the many ﬁelds represented at GECCO. The best paper awards were presented
for the second time at GECCO 2003, and we hope many of you participated
in the balloting. This year, for the ﬁrst time, most presentations at GECCO

XVIII
A Word from the Chair of ISGEC
were electronic, displayed with the LCD projectors that ISGEC has recently
purchased. Our journals, Evolutionary Computation and Genetic Programming
and Evolvable Machines, continue to prosper, and we are exploring ways to make
them even more widely available. The inclusion of the proceedings in Springer’s
Lecture Notes in Computer Science series, making them available in many more
libraries worldwide, should have a strong positive impact on our ﬁeld.
ISGEC is your society, and we urge you to become involved or continue your
involvement in its activities, to the mutual beneﬁt of the whole evolutionary
computation community. Three members were elected to new ﬁve-year terms on
the Executive Board at GECCO 2002 – Wolfgang Banzhaf, Marco Dorigo, and
Annie Wu.
Since that time, ISGEC has been active on many issues, through actions of
the Board and the three Councils – the Council of Authors, Council of Editors,
and Council of Conferences.
The organizers of GECCO 2003 are listed in this frontmatter, but special
thanks are due to James Foster, General Chair, and Erick Cant´u-Paz, Editor-
in-Chief of the Proceedings, as well as to John Koza and Dave Goldberg, the
Business Committee. All of the changes this year, particularly in the publication
of the proceedings, have meant a lot of additional work for this excellent team,
and we owe them our thanks for a job well done.
Of course, we all owe a great debt to those who chaired or served on the
various core and special program committees that reviewed all of the papers for
GECCO 2003. Without their eﬀort it would not have been possible to put on a
meeting of this quality.
Another group also deserves the thanks of GECCO participants and ISGEC
members – the members of the ISGEC Executive Board and Councils, who are
listed below. I am particularly indebted to them for their thoughtful contributi-
ons to the organization and their continuing demonstrations of concern for the
welfare of ISGEC.
I invite you to communicate with me (goodman@egr.msu.edu) if you have
questions or suggestions for ways ISGEC can be of greater service to its members,
or if you would like to get more involved in ISGEC and its functions.
Don’t forget about the 8th Foundations of Genetic Algorithms (FOGA)
workshop, also sponsored by ISGEC, the biennial event that brings together
the world’s leading theorists on evolutionary computation, which will be held in
2004.
Finally, I hope you will join us at GECCO 2004 in Seattle. Get your ideas to
Ricardo Poli, the General Chair of GECCO 2004, when you see him at GECCO
2003, and please check the ISGEC Website, www.isgec.org, regularly for details
as the planning for GECCO 2004 continues.
Erik D. Goodman

ISGEC Executive Board
Erik D. Goodman (Chair)
David Andre
Wolfgang Banzhaf
Kalyanmoy Deb
Kenneth De Jong
Marco Dorigo
David E. Goldberg
John H. Holland
John R. Koza
Una-May O’Reilly
Ingo Rechenberg
Marc Schoenauer
Lee Spector
Darrell Whitley
Annie S. Wu
Council of Authors
Erick Cant´u-Paz (chair), Lawrence Livermore National Laboratory
David Andre, University of California – Berkeley
Plamen P. Angelov, Loughborough University
Vladan Babovic, Danish Hydraulic Institute
Wolfgang Banzhaf, University of Dortmund
Forrest H. Bennett III, FX Palo Alto Laboratory, Inc.
Hans-Georg Beyer, University of Dortmund
Jergen Branke, University of Karlsruhe
Martin Butz, University of Illinois at Urbana-Champaign
Runwei Cheng, Ashikaga Institute of Technology
David A. Coley, University of Exeter
Marco Dorigo, IRIDIA, Universit´e Libre de Bruxelles
Rolf Drechsler, University of Freiburg
Emanuel Falkenauer, Optimal Design and Brussels University (ULB)
Stephanie Forrest, University of New Mexico
Mitsuo Gen, Ashikaga Institute of Technology
Andreas Geyer-Schulz, Abteilung fuer Informationswirtschaft
David E. Goldberg, University of Illinois at Urbana-Champaign
Jens Gottlieb, SAP, AG
Wolfgang A. Halang, Fernuniversitaet
John H. Holland, University of Michigan and Sante Fe Institute
Hitoshi Iba, University of Tokyo
Christian Jacob, University of Calgary
Robert E. Keller, University of Dortmund
Dimitri Knjazew, SAP, AG

XX
Organization
John R. Koza, Stanford University
Sam Kwong, City University of Hong Kong
William B. Langdon, University College, London
Dirk C. Mattfeld, University of Bremen
Pinaki Mazumder, University of Michigan
Zbigniew Michalewicz, University of North Carolina at Charlotte
Melanie Mitchell, Oregon Health and Science University
Ian Parmee, University of North Carolina at Charlotte
Frederick E. Petry, University of North Carolina at Charlotte
Riccardo Poli, University of Essex
Moshe Sipper, Swiss Federal Institute of Technology
William M. Spears, University of Wyoming
Wallace K.S. Tang, Swiss Federal Institute of Technology
Adrian Thompson, University of Sussex
Michael D. Vose, University of Tennessee
Man Leung Wong, Lingnan University
Council of Editors
Erick Cant´u-Paz (chair), Lawrence Livermore National Laboratory
Karthik Balakrishnan, Fireman’s Fund Insurance Company
Wolfgang Banzhaf, University of Dortmund
Peter Bentley, University College, London
Lance D. Chambers, Western Australian Department of Transport
Dipankar Dasgupta, University of Memphis
Kenneth De Jong, George Mason University
Francisco Herrera, University of Granada
William B. Langdon, University College, London
Pinaki Mazumder, University of Michigan
Eric Michielssen, University of Illinois at Urbana-Champaign
Witold Pedrycz, University of Alberta
Rajkumar Roy, Cranﬁeld University
Elizabeth M. Rudnick, University of Illinois at Urbana-Champaign
Marc Schoenauer, INRIA Rocquencourt
Lee Spector, Hampshire College
Jose L. Verdegay, University of Granada, Spain
Council of Conferences, Riccardo Poli (Chair)
The purpose of the Council of Conferences is to provide information about the
numerous conferences that are available to researchers in the ﬁeld of Genetic and
Evolutionary Computation, and to encourage them to coordinate their meetings
to maximize our collective impact on science.

Organization
XXI
ACDM, Adaptive Computing in Design and Manufacture, 2004, Ian Parmee
(Ian.Parmee@uwe.ac.uk)
EuroGP, European Conference on Genetic Programming, Portugal, April 2004,
Ernesto Costa (ernesto@dei.uc.pt)
EvoWorkshops, European Evolutionary Computing Workshops, Portugal, April
2004, Stefano Cagnoni (cagnoni@ce.unipr.it)
FOGA, Foundations of Genetic Algorithms Workshop, 2004
GECCO 2004, Genetic and Evolutionary Computation Conference, Seattle,
June 2004, Riccardo Poli (rpoli@essex.ac.uk)
INTROS, INtroductory TutoRials in Optimization, Search and Decision Support
Methodologies, August 12, 2003, Nottingham, UK, Edmund Burke
(ekb@cs.nott.ac.uk)
MISTA, 1st Multidisciplinary International Conference on Scheduling: Theory
and Applications August 8-12, 2003, Nottingham, UK, Graham Kendall
(gxk@cs.nott.ac.uk)
PATAT 2004, 5th International Conference on the Practice and Theory of Au-
tomated Timetabling, Pittsburgh, USA, August 18–20, 2004, Edmund Burke
(ekb@cs.nott.ac.uk)
WSC8, 8th Online World Conference on Soft Computing in Industrial Applica-
tions, September 29th - October 10th, 2003, Internet (hosted by University
of Dortmund), Frank Hoﬀmann (hoﬀmann@esr.e-technik.uni-dortmund.de)
An up-to-date roster of the Council of Conferences is available online at
http://www.isgec.org/conferences.html.
Please contact the COC chair Riccardo Poli (rpoli@essex.ac.uk) for additions
to this list.
Papers Nominated for Best Paper Awards
In 2002, ISGEC created a best paper award for GECCO. As part of the double
blind peer review, the reviewers were asked to nominate papers for best paper
awards. The chairs of core and special program committees selected the papers
that received the most nominations for consideration by the conference. One win-
ner for each program track was chosen by secret ballot of the GECCO attendees
after the papers were presented in Chicago. The titles and authors of the winning
papers are available at the GECCO 2003 website (www.isgec.org/GECCO-2003).
Finite Population Models of Co-evolution and Their Application to Haploidy
versus Diploidy, Anthony M.L. Liekens, Huub M.M. ten Eikelder, and Peter
A.J. Hilbers
A Game-Theoretic Memory Mechanism for Coevolution, Sevan G. Ficici and
Jordan B. Pollack
A Non-dominated Sorting Particle Swarm Optimizer for Multiobjective Opti-
mization, Xiaodong Li
Emergence of Collective Behavior in Evolving Populations of Flying Agents, Lee
Spector, Jon Klein, Chris Perry, and Mark Feinstein

XXII
Organization
Immune Inspired Somatic Contiguous Hypermutation for Function Optimisa-
tion, Johnny Kelsey and Jon Timmis
Eﬃciency and Reliability of DNA-Based Memories, Max H. Garzon, Andrew
Neel, and Hui Chen
Hardware Evolution of Analog Speed Controllers for a DC Motor, D.A. Gwaltney
and M.I. Ferguson
Integration of Genetic Programming and Reinforcement Learning for Real Ro-
bots, Shotaro Kamio, Hideyuki Mitshuhashi, and Hitoshi Iba
Co-evolving Task-Dependent Visual Morphologies in Predator-Prey Experi-
ments, Gunnar Buason and Tom Ziemke
The Steady State Behavior of (µ/µI, λ)-ES on Ellipsoidal Fitness Models Di-
sturbed by Noise, Hans-Georg Beyer and Dirk V. Arnold
On the Optimization of Monotone Polynomials by the (1+1) EA and Randomi-
zed Local Search, Ingo Wegener and Carsten Witt
Ruin and Recreate Principle Based Approach for the Quadratic Assignment
Problem, Alfonsas Misevicius
Evolutionary Computing as a tool for Grammar Development, Guy De Pauw
Adaptive Elitist-Population Based Genetic Algorithm for Multimodal Function
Optimization, Kwong-Sak Leung and Yong Liang
Scalability of Selectorecombinative Genetic Algorithms for Problems with Tight
Linkage, Kumara Sastry and David E. Goldberg
Eﬀective Use of Directional Information in Multi-objective Evolutionary Com-
putation, Martin Brown and R.E. Smith
Are Multiple Runs of Genetic Algorithms Better Than One? Erick Cant´u-Paz
and David E. Goldberg
Selection in the Presence of Noise, J¨urgen Branke and Christian Schmidt
Diﬃculty of Unimodal and Multimodal Landscapes in Genetic Programming,
Leonardo Vanneschi, Marco Tomassini, Manuel Clergue, and Philippe Collard
Dynamic Maximum Tree Depth: a Simple Technique for Avoiding Bloat in Tree-
Based GP, Sara Silva and Jonas Almeida
Generative Representations for Evolving Families of Designs, Gregory S. Hornby
Identifying Structural Mechanisms in Standard Genetic Programming, Jason M.
Daida and Adam M. Hilss
Visualizing Tree Structures in Genetic Programming, Jason M. Daida, Adam M.
Hilss, David J. Ward, and Stephen L. Long
Methods for Evolving Robust Programs, Liviu Panait and Sean Luke
Population Implosion in Genetic Programming, Sean Luke, Gabriel Catalin Ba-
lan, and Liviu Panait
Designing Eﬃcient Exploration with MACS: Modules and Function Approxima-
tion, Pierre G´erard and Olivier Sigaud
Tournament Selection: Stable Fitness Pressure in XCS, Martin V. Butz, Kumara
Sastry, and David E. Goldberg
Towards Building Block Propagation in XCS: a Negative Result and Its Impli-
cations, Kurian K. Tharakunnel, Martin V. Butz, and David E. Goldberg

Organization
XXIII
Quantum-Inspired Evolutionary Algorithm-Based Face Veriﬁcation, Jun-Su
Jang, Kuk-Hyun Han, and Jong-Hwan Kim
Mining Comprehensive Clustering Rules with an Evolutionary Algorithm, Ioan-
nis Saraﬁs, Phil Trinder and Ali Zalzala
System-Level Synthesis of MEMS via Genetic Programming and Bond Graphs,
Zhun Fan, Kisung Seo, Jianjun Hu, Ronald C. Rosenberg, and Erik D. Good-
man
Active Guidance for a Finless Rocket Using Neuroevolution, Faustino J. Gomez
and Risto Miikkulainen
Extracting Test Sequences from a Markov Software Usage Model by ACO, Karl
Doerner and Walter J. Gutjahr
Modeling the Search Landscape of Metaheuristic Software Clustering Algo-
rithms, Brian S. Mitchell and Spiros Mancoridis


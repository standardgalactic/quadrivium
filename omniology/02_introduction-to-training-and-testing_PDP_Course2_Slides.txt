Week 1
Regression

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Recap on mathematical notation

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Revise the mathematical notation necessary to cover the 
basics of machine learning algorithms

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Vectors and matrices
: vector – usually a vector of features
: a vector component, usually a single feature
: a vector of parameters
: matrix – usually a feature matrix
: feature vector for a specific datapoint
: transpose operator
: vector norm
(in general                             )
: a vector of labels

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Linearity
We will frequently talk about linear 
models
Precisely speaking, a function f(x) is linear if
For the purposes of this class, we care about 
functions of the form:
which is linear in theta

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Probability and statistics
: probability of some event
: conditional probability of some event
: mean of a vector
: variance of a vector
: sigmoid function

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Covered basic notation of vectors, matrices, probability, and statistics

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Introduction to supervised learning

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will
• Introduce the concept of supervised learning
• Contrast supervised learning and unsupervised learning

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: What is supervised learning?
Supervised learning is the process 
of trying to infer from labeled data 
the underlying function that 
produced the labels associated with 
the data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
What is supervised learning?
Given labeled training data of the form
Infer the function

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Example: movie recommendation
Suppose we want to build a movie 
recommender
e.g. which of these films will I rate highest?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Example: movie recommendation
Q: What are the labels?
A: ratings that others have given to each 
movie, and that I have given to other 
movies

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Example: movie recommendation
Q: What is the data?
A: features about the movie and the users 
who evaluated it
Movie features: genre, actors, rating, length, etc.
User features: 
age, gender, 
location, etc.

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Example: movie recommendation
Movie recommendation:
=

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Solution 1
Solution: Design a system based on 
prior knowledge, e.g.
def prediction(user, movie):
if (user[‘age’] <= 14):
if (movie[‘mpaa_rating’]) == “G”):
return 5.0
else:
return 1.0
else if (user[‘age’] <= 18):
if (movie[‘mpaa_rating’]) == “PG”):
return 5.0
….. Etc.
Is this supervised learning?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Solution 2
Solution: Identify words that I frequently mention in my 
social media posts, and recommend movies whose plot 
synopses use similar types of language
Plot synopsis
Social media posts
argmax similarity(synopsis, post)
Is this supervised learning?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Solution 3
Solution: Identify which attributes 
(e.g. actors, genres) are associated 
with positive ratings. Recommend 
movies that exhibit those attributes.
Is this supervised learning?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Solution 1
(design a system based on prior 
knowledge)
Disadvantages:
•
Depends on possibly false assumptions about how 
users relate to items
•
Cannot adapt to new data/information
Advantages:
•
Requires no data!

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Solution 2
(identify similarity between wall 
posts and synopses)
Disadvantages:
•
Depends on possibly false assumptions about 
how users relate to items
•
May not be adaptable to new settings
Advantages:
•
Requires data, but does not require labeled
data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Solution 3
(identify attributes that are 
associated with positive ratings)
Disadvantages:
•
Requires a (possibly large) dataset of 
movies with labeled ratings
Advantages:
•
Directly optimizes a measure we care about 
(predicting ratings)
•
Easy to adapt to new settings and data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Supervised versus unsupervised learning
Learning approaches attempt to model 
data in order to solve a problem
Unsupervised learning approaches find 
patterns/relationships/structure in data, but are not optimized to 
solve a particular predictive task
Supervised learning aims to directly model the relationship 
between input and output variables, so that the output variables 
can be predicted accurately given the input

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced the concept of supervised learning
• Contrasted supervised learning, unsupervised learning, (and also non-
learning-based approaches)
• Described the relative merits of these approaches
On your own...
•
Consider a related problem (e.g. 
predicting a movie’s box-office gross), 
and propose ways to solve it based on 
unsupervised learning, supervised 
learning, (and without learning)

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Supervised Learning: Regression

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Demonstrate the idea behind linear regression
• Understand the role of parameters in a model

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Regression
Regression is one of the simplest 
supervised learning approaches to learn 
relationships  between input variables 
(features) and output variables 
(predictions)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivation: height vs. weight
Suppose we wanted to understand the 
relationship between height and weight...
…or equivalently, can we predict a person's 
weight from their height?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivation: height vs. weight
Height
Weight
40kg
120kg
130cm
200cm
Q: Can we find a line that (approximately) fits the data?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivation: height vs. weight
Q: Can we find a line that (approximately) fits the data?
• If we can find such a line, we can use it to make predictions (i.e., 
estimate a person's weight given their height)
• How do we formulate the problem of finding a line?
• If no line will fit the data exactly, how to approximate?
• What is the "best" line?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Recap: equation for a line
What is the formula describing the line?
Height
Weight
40kg
120kg
130cm
200cm

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Recap: equation for a line
What about in more dimensions?
Height
Weight
40kg
120kg
130cm
200cm
Age
1 year
100 years

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Recap: equation for a line as an inner product
What about in more dimensions?
Height
Weight
40kg
120kg
130cm
200cm
Age
1 year
100 years

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Linear regression
In general:
What we want to 
predict
(label)
What we can use
to predict it
(features)
Parameters that 
we can tune to 
make the 
prediction

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Linear regression
But here we have many observations, so we can rewrite using a 
matrix:
Vector of labels
Matrix of features
Vector of parameters

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Linear regression
Linear regression assumes a predictor of the form
(or              if you prefer) 
matrix of features
(data)
unknowns
(which features are relevant)
vector of outputs
(labels)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Linear regression
Linear regression assumes a predictor of the form
Q: Solve for theta
A:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Linear regression
Linear regression assumes a predictor of the form
Q: Solve for theta
A:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
On your own...
•
Try writing down the regression 
equation (equation for a line) for a 
similar problem, e.g. predicting a 
person's income from various features
• Regression is one of the simplest forms of supervised learning
• Linear regression is essentially equivalent to finding a line that best fits 
the data
• Can express this as solving a system of matrix equations

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Regression in Python

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Explore how to express linear regression equations in 
terms of Python data structures
• Work through a (simple) real-world regression example
• Compare a "manual" implementation of linear regression 
to a library function

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Example – Air quality prediction
We'll look at the problem of predicting air quality, using an index 
called pm2.5, measured in Beijing
• This is a "simpler" dataset than some of the 
others we've been working with, as the 
relevant features are all real-valued
• It's also useful in our following lecture (on 
time-series prediction), since the data is in 
the form of a time series

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Example – UCI Dataset Repository
https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
What are we trying to predict?
Temperature
pm2.5
100
140
-20 degrees c
50 c
E.g. pm2.5 vs. Temperature:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Reading the file
Label that we 
want to predict
Feature we want to 
use for prediction

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Extracting features and labels

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Let's try again...

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Reminder: Constant feature
Why did we implement our feature function like this?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Finding the parameters
pm2.5 = 107.1 - 0.68 * temp

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Adding more features
pm2.5 = 3263.7
- 3.109 * temp
- 3.065 * pressure
- 0.460 * wind speed
Note: pressure
Note: wind speed

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Doing the same thing manually

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Demonstrated how to perform simple linear regression in 
Python
• Performed linear regression on an "air quality" example 
from the UCI Machine Learning Repository
• Introduced the numpy "least squares" function for linear 
regression
On your own...
•
Try extending the code provided here to 
use different features and feature 
combinations

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Time-series Regression (or "Autoregression")

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Explore options to apply regression to time-series data
• Consider the merits of alternative approaches
• Introduce the "autoregression" framework for time-series 
regression

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
Here, we’d like to predict sequences of 
real-valued events as accurately as 
possible
Given: a time series:
Suppose we’d like to predict the next values in the 
sequence as accurately as possible

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
Method 1: maintain a “moving average” 
using a window of some fixed length
Time à

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
Method 1: maintain a “moving average” 
using a window of some fixed length
“peel-off” the 
oldest point
add the 
newest point
This can be computed efficiently via dynamic programming:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
Method 2: weight the points in the moving 
average by age
Time à

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
Method 2: weight the points in the moving 
average by age
newest points have 
the highest weight
weight decays to 
zero after K points

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
Method 3: weight the most recent points 
exponentially higher
Time à

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
Method 3: weight the most recent points 
exponentially higher
most recent point 
has weight \alpha
previous prediction 
has weight 1 - \alpha

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Methods 1, 2, 3
Method 1: Sliding window
Method 2: Linear decay
Method 3: Exponential decay
Method 1:
Method 2:
Method 3:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
Method 4: all of these models are assigning 
weights to previous values using some 
predefined scheme, why not just learn the 
weights?
• We can now fit this model using least-squares
• This procedure is known as autoregression
• Using this model, we can capture periodic effects, e.g. 
that the traffic of a website is most similar to its traffic 
7 days ago

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced the problems of time-series prediction and 
"autoregression"
• Introduced and compared four techniques to solve the 
time-series prediction problem
On your own...
•
By considering different types of data, 
consider in which scenarios (a) sliding 
windows, (b) linear decay, and (c) 
exponential decay would work better than 
each of the others

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Autoregression in Python

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Show a real-world example of autoregression in Python
• Adapt the regression code we previously developed to 
handle autoregressive problems

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Time-series regression
We'll quickly adapt our regression code to 
use the autoregressive framework
•
The air quality data (which we used 
previously) is made up of sequential (hourly) 
predictions
•
So, we can predict the next air quality 
measurement from the previous ones

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Reading the data
(see "regression in python" lecture for further explanation)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Extracting autoregressive features
Vector of previous 
(windowSize) 
observations
•
Feature vector is made up of the previous pm2.5 
observations
•
The number of previous observations is provided 
as a configurable parameter

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Extracting autoregressive features
Vector of previous 
(windowSize) 
observations

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Extracting autoregressive features
Note: Weight associated with 
most recent observation

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Extracting autoregressive features
•
Note that we don't need to use autoregression or 
regular regression exclusively
•
We can include both types of features 
simultaneously!
Note: Features for temperature, 
pressure, and wind-speed

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Demonstrated how to perform autoregression in Python
On your own...
•
Experiment with different sliding window 
sizes and their impact on performance
•
Try alternative approaches (e.g. sliding 
windows) and compare them to 
autoregression

Week 2
Feature Engineering

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Features from categorical data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Demonstrate how to incorporate binary and categorical 
features into regressors
• Compare the benefits of various feature representation 
strategies

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
How would we build regression models 
that incorporate features like:
•
How does height vary with gender?
•
How do preferences vary with geographical 
region?
•
How does product demand change during 
different seasons?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
E.g. How does height vary with gender?
Gender
Height
130cm
200cm

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
E.g. How does height vary with gender?
•
Previous picture doesn't quite make sense: 
we're unlikely to have a dataset including a 
continuum of gender values, so fitting a 
"line" doesn't seem to fit
•
So how can we deal with this type of data 
using a linear regression framework?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
E.g. How does height vary with gender?
• Presumably our gender values might look 
more like
{"male", "female", "other", "not specified"}
• Let's first start with a binary problem where 
we just have {"male", "female"}

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
What should our model equation look like?
gender = 0 if male, 1 if female 
if male
if female

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
Gender
Height
130cm
200cm
male
female
is the (predicted/average) height for males
is the how much taller females are than males 
(in this case a negative number)
We’re really still fitting a line though!

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
What if we had more than two values?
(e.g {“male”, “female”, “other”, “not specified”})
Could we apply the same approach?
gender = 0 if “male”, 1 if “female”, 2 if “other”, 3 if “not specified”
if male
if female
if other
if not specified

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
What if we had more than two values?
(e.g {“male”, “female”, “other”, “not specified”})
Gender
Height
130cm
200cm
male
female
other
not specified

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
Gender
Height
130cm
200cm
male
female
other
not specified
• This model is valid, but won’t be very effective
• It assumes that the difference between “male” and “female” 
must be equivalent to the difference between “female” and 
“other”
• But there’s no reason this should be the case!

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
Gender
Height
130cm
200cm
male
female
other
not specified
E.g. it could not capture a function like:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
Instead we need something like:
if male
if female
if other
if not specified

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
This is equivalent to:
where feature = [1, 0, 0] for “female”
feature = [0, 1, 0] for “other”
feature = [0, 0, 1] for “not specified”

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: One-hot encodings
feature = [1, 0, 0] for “female”
feature = [0, 1, 0] for “other”
feature = [0, 0, 1] for “not specified”
• This type of encoding is called a one-hot encoding
(because we have a feature vector with only a single “1” 
entry
• Note that to capture 4 possible categories, we only need 
three dimensions (a dimension for “male” would be 
redundant)
• This approach can be used to capture a variety of 
categorical feature types, as well as objects that belong 
to multiple categories

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Described how to capture binary and categorical features 
within linear regression models
• Introduced the concept of a “one-hot” encoding
On your own...
•
Think how you would encode different 
categorical features, e.g. the set of 
categories a business belongs to, or the 
set of a user’s friends on a social network

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Features from temporal data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Investigate different strategies for extracting features from 
temporal (or seasonal) data
• Extend the concept of one-hot-encodings to represent 
temporal information

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
How would we build regression models 
that incorporate features like:
•
How do sales (or preferences) vary over 
time?
•
What are the long term trends of sales?
•
What are the short term trends (e.g. day of 
the week, season, etc.)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
E.g. How do ratings vary with time?
Time
Rating
1 star
5 stars

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
E.g. How do ratings vary with time?
•
In principle this picture looks okay (compared our 
previous lecture on categorical features) – we’re 
predicting a real valued quantity from real valued 
data (assuming we convert the date string to a 
number)
•
So, what would happen if (e.g. we tried to train a 
predictor based on the month of the year)?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
E.g. How do ratings vary with time?
•
Let’s start with a simple feature representation, e.g. 
map the month name to a month number:
Jan = [0]
Feb = [1]
Mar = [2]
etc.
where

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
The model we’d learn might look something like:
J   F   M   A   M   J   J
A   S   O   N   D
0   1   2   3   4   5   6   7   8   9   10  11
Rating
1 star
5 stars

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating examples
J  F  M  A  M  J  J
A  S  O  N  D  J  F  M  A  M  J  J
A  S  O  N  D
0  1  2  3  4  5  6  7  8  9  10 11 0  1  2  3  4  5  6  7  8  9  10 11
Rating
1 star
5 stars
This seems fine, but what happens if we look 
at multiple years?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Modeling temporal data
•
This representation implies that the model would 
“wrap around” on December 31 to its January 1st
value.
•
This type of “sawtooth” pattern probably isn’t very 
realistic
This seems fine, but what happens if we look 
at multiple years?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Modeling temporal data
J  F  M  A  M  J  J
A  S  O  N  D  J  F  M  A  M  J  J
A  S  O  N  D
0  1  2  3  4  5  6  7  8  9  10 11 0  1  2  3  4  5  6  7  8  9  10 11
Rating
1 star
5 stars
What might be a more realistic shape?
?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Modeling temporal data
•
Also, it’s not a linear model
•
Q: What’s a class of functions that we can use to 
capture a more flexible variety of shapes?
•
A: Piecewise functions!
Fitting some periodic function like a sin wave would 
be a valid solution, but is difficult to get right, and 
fairly inflexible

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Fitting piecewise functions
We’d like to fit a function like the following:
J   F   M   A   M   J   J
A   S   O   N   D
0   1   2   3   4   5   6   7   8   9   10  11
Rating
1 star
5 stars

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Fitting piecewise functions
In fact this is very easy, even for a linear 
model! This function looks like:
1 if it’s Feb, 0 otherwise
•
Note that we don’t need a feature for January
•
i.e., theta_0 captures the January value, theta_0 
captures the difference between February and 
January, etc.

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Fitting piecewise functions
Or equivalently we’d have features as follows:
where
x = [1,1,0,0,0,0,0,0,0,0,0,0] if February
[1,0,1,0,0,0,0,0,0,0,0,0] if March
[1,0,0,1,0,0,0,0,0,0,0,0] if April
...
[1,0,0,0,0,0,0,0,0,0,0,1] if December

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Fitting piecewise functions
Note that this is still a form of one-hot
encoding, just like we saw in the “categorical 
features” lecture
•
This type of feature is very flexible, as it can handle 
complex shapes, periodicity, etc.
•
We could easily increase (or decrease) the 
resolution to a week, or an entire season, rather 
than a month, depending on how fine-grained our 
data was

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Combining one-hot encodings
We can also extend this by combining several 
one-hot encodings together:
where
x1 = [1,1,0,0,0,0,0,0,0,0,0,0] if February
[1,0,1,0,0,0,0,0,0,0,0,0] if March
[1,0,0,1,0,0,0,0,0,0,0,0] if April
...
[1,0,0,0,0,0,0,0,0,0,0,1] if December
x2 = [1,0,0,0,0,0] if Tuesday
[0,1,0,0,0,0] if Wednesday
[0,0,1,0,0,0] if Thursday
...

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Motivated the use of piecewise functions to model 
temporal data
• Described how one-hot encodings can be used to model 
piecewise functions
On your own...
•
Think about what piecewise functions you 
might use to model demand on Amazon
•
Is the day of the week important?
•
Or the day of the month?
•
How would you incorporate significant 
holidays (which may influence demand) into 
this model?

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Feature transformations

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Demonstrate the use of transformations to incorporate 
non-linear functions into linear models

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating example
Height
40kg
120kg
130cm
200cm
We’ve previously seen simple examples of 
regression models such as Weight vs. Height:
Weight

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating example
We’ve previously seen simple examples of 
regression models such as Weight vs. Height:
•
A linear relationship is probably okay for modeling 
this data, and in practice we’d often get away with 
using this type of model
•
But, it certainly makes some assumptions that 
aren’t totally justified
•
How can we fit more suitable, or more general 
functions

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivating example
How should the right model look for weight vs. 
height?
•
A linear function?
•
A quadratic or polynomial function?
•
An asymptotic function?
•
etc.

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Fitting complex functions
Let’s start with a polynomial function (e.g. a cubic 
function):
•
Note that this is perfectly straightforward: the 
model still takes the form
•
We just need to use the feature vector
x = [1, height, height^2, height^3]

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Fitting complex functions
Note that we can use the same approach to fit 
arbitrary functions of the features! E.g.:
•
We can perform arbitrary combinations of the 
features and the model will still be linear in the 
parameters (theta):

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Fitting complex functions
The same approach would not work if we wanted 
to transform the parameters:
•
The linear models we’ve seen so far do not support 
these types of transformations (i.e., they need to be 
linear in their parameters)
•
There are alternative models that support non-
linear transformations of parameters, e.g. neural 
networks

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Showed how to apply arbitrary transformations to 
features in a linear model
• Further explained the restrictions and assumptions of 
linear models
On your own...
•
Extend our previous code (on pm2.5 levels vs. 
air temperature) to handle simple polynomial 
functions

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Missing values

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Introduce the issues around datasets with missing values
• Investigate different strategies for dealing with missing 
values in datasets

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivation
•
Even the simple PM2.5 dataset we introduced had 
missing values (indicated by "NA")
•
So far we dealt with them simply by discarding those 
instances:
•
This was an okay strategy when dealing with a single 
feature where missing data was rare, but how would 
in generalize?
•
In particular, this approach wouldn't work if many 
features might be missing

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Strategies for dealing with missing values
•
Filtering (I.e., discarding missing values), as we 
discussed on the previous slide
•
Missing data imputation: filling in the missing 
values with "reasonable" estimates
•
Modeling: changing our regression/classification 
algorithms to handle missing data explicitly
In this lecture we'll look at three strategies for 
dealing with missing data:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Missing data imputation
Even in cases where only a small amount of data 
is missing, simply discarding instances may not be 
an option. What else can we do?
Missing data imputation seeks to replace missing 
values by reasonable estimates

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Missing data imputation
A simple scheme would be to replace every missing 
value with the average for that feature.
What are the consequences of such a scheme?
•
The average may be sensitive to outlying values 
(though this could be addressed by using the 
median instead)
•
The imputed value may or may not be 
"reasonable" (e.g. consider our "gender = male" 
feature)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Missing data imputation
Alternately we could consider more sophisticated 
data imputation schemes
•
Rather than imputing using the mean, does it 
make more sense to compute the mean of a 
certain subgroup (e.g. if "height" is missing, can 
we impute using the average height of users 
with the same gender?)
•
We could also train a separate predictor to 
impute the missing values (though this is 
complex if there are missing values for many 
different features)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Modeling missing data
How can we directly model the missing values within 
a regression or classification algorithm?
•
One simple scheme: add an additional feature
indicating that a value is missing
•
e.g.:
feature = [1, 0, 0] for “female”
feature = [0, 1, 0] for “male”
feature = [0, 0, 1] for “feature missing”

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Modeling missing data
What predictions does the model make under this 
scheme?
feature = [1, 0, 0] for “female”
feature = [0, 1, 0] for “male”
feature = [0, 0, 1] for “feature missing”
Note that      learns what value should be predicted 
when this feature is missing
for female
for male
for “feature missing”

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Discussed some simple schemes for dealing with missing 
data
• Introduced the ideas of data imputation and modeling 
missing data
On your own...
•
Extend our previous code (on pm2.5 levels vs. 
air temperature) to handle missing features 
(other than the pm2.5 measurement itself)
•
Experiment with different missing data 
imputation schemes and note their effect on 
performance

Week 3
Classification

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Supervised Learning: Classification

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Introduce the idea of classification
• Compare classification and regression models
• Explain what types of problems can be solved using 
classification

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Regression vs. classification
So far, we’ve studied regression 
problems that allow us to make 
predictions of the form
That is, we’ve assumed real-valued 
(or numerical) outputs

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Classification
How can we predict binary or 
categorical variables?
{0,1}, {True, False}
{1, … , N}

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Why classification?
Will I purchase
this product?
(yes)
Will I click on
this ad?
(no)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Why classification?
What animal appears in this image?
(mandarin duck)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Why classification?
What are the categories of the item 
being described?
(book, fiction, philosophical fiction)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Linear Classification
We’ll attempt to build classifiers that make 
decisions according to rules of the form

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Linear Classification
This is called linear classification, since 
we’re still making decisions according to a 
linear function,           .

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Classification
We’ll look at a few different types of classifiers in 
detail, and briefly discuss others:
• Nearest neighbors – a simple (non-learning-based) 
classification scheme
• Logistic regression – a generalization of the 
techniques we’ve already seen
• (briefly) Support Vector Machines and other 
approaches
• Python libraries for classification

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced the idea of classification
• Compared and contrasted classification with regression

Python Data Products
Course 2: Design thinking and predictive pipelines 
Lecture: Classification: Nearest Neighbor

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Introduce a simple classification algorithm, before we 
proceed to more complex alternatives in later lectures
• Demonstrate a “non-learning” solution to classification 
problems

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Classification
Suppose we have some data we wish to classify, belonging to 
one of two classes (positive or negative)
positive 
examples
negative 
examples
feature 1
feature 2

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Classification
What is the simplest algorithm we could come up with to 
classify a new data point?
positive 
examples
negative 
examples
new data point –
positive or negative?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Nearest Neighbor Classification
The nearest neighbor classification algorithm assigns the point 
the label of the nearest point
positive 
examples
negative 
examples
new data point –
positive or negative?
nearest labeled point
à Label = negative

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Nearest Neighbor Classification
Precisely speaking, if we have a collection of points X (really a 
collection of feature vectors) and labels y,
and we see a new point (that we wish to label) z, then:
label assigned 
to the new 
point
distance 
between z and 
the ith point
nearest point
label of nearest 
point

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced nearest neighbor classification
• Introduced the notation used do describe classifiers for 
the rest of this course

Python Data Products
Course 2: Design thinking and predictive pipelines 
Lecture: gradient descent

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Introduce the notion of gradient descent, a general-
purpose approach for model fitting

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Closed form solutions
When we introduced regression, we optimized its parameters 
by solving a system of matrix equations:
• But in general we don't have nice closed-form 
solutions to choosing our models
• What can we do in cases where a closed-form isn't 
available?
• E.g. how would we solve this problem if we 
couldn't find a closed form?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Optimization with gradient descent
Precisely, the problem we're trying to solve looks like
Rows in our 
dataset
Prediction
error
• How do we solve this for theta?
• i.e., how do we choose

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Gradient Descent
Gradient Descent is a general-purpose optimization approach 
to solve continuous minimization problems that don't have a 
closed form
• Normally, to solve a continuous minimization 
problem, we would
1. Compute the gradient w.r.t. theta
2. Find the points where the gradient is equal to 
zero (i.e., the minima of the function)
• If we can't do (2) above, gradient descent helps us 
to find local minima

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Gradient Descent
With gradient descent, rather than "solving" the problem by 
finding its zeros, we instead start with an initial guess, and 
iteratively update our solution in the direction of the gradient
• In this way, we gradually find solutions that come 
progressively closer to being zeros of the gradient 
equation – even though we couldn't solve it in 
closed-form
• The points we find are called local minima of the 
original function

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
The gradient descent algorithm
In essence gradient descent (to minimize a function
) works 
as follows:
1. Initialize     at random
2. While (not converged) do
All sorts of annoying issues:
• How to initialize theta?
• How to determine when the process has converged?
• How to set the step size alpha
(these aren’t really the point of this course though)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
The gradient descent algorithm
So what exactly is going on here?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
The gradient descent algorithm
And how would we use it to solve our regularization objective?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced the gradient descent algorithm
• Showed how this algorithm can be applied to solve the 
types of regression problems we've seen so far

Python Data Products
Course 2: Design thinking and predictive pipelines 
Lecture: Classification: Logistic Regression

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Introduce logistic regression, a modification of regression 
algorithms to handle classification problems
• Show how logistic regression can be solved using gradient 
descent approaches

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Logistic regression
Previously: regression
This lecture: logistic regression

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Logistic regression
Q: How to convert a real-valued 
expression (                )
Into a probability
(                           )

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Logistic regression
A: sigmoid function:
Classification 
boundary

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Logistic regression
Training:
should be maximized when      
is positive and minimized 
when      is negative
= 1 if the argument is true, = 0 otherwise

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Logistic regression
How to optimize?
•
Take logarithm
•
Compute gradient
•
Solve using gradient ascent

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Logistic regression

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Logistic regression

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Logistic regression
Log-likelihood:
Derivative:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced logistic regression
• Showed how to solve the logistic regression objective 
using gradient ascent

Python Data Products
Course 2: Design thinking and predictive pipelines 
Lecture: Introduction to Support Vector Machines

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Briefly introduce the Support Vector Machine (SVM) 
classifier
• Discuss some of the relative merits of different classifiers, 
and reasons for choosing one classifier over another

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
So far we've seen...
• Last lecture we looked at logistic regression, which is a 
classification model of the form:
• In order to do so, we made certain modeling assumptions, but 
there are many different models that rely on different 
assumptions
• In this lecture we’ll look at another such model

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Support vector machines
• Support Vector Machines are an alternative linear 
classification scheme that aims to minimize the number of 
classification errors
• Note that this is a different goal (as we’ll see next) than the 
goal of logistic regression, which aims to maximize a 
probabilistic expression

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Motivation: SVMs vs Logistic regression
positive 
examples
negative 
examples
a
b
Q: Where would a logistic regressor place the 
decision boundary for these features?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
SVMs vs Logistic regression
Q: Where would a logistic regressor place the 
decision boundary for these features?
b
positive 
examples
negative 
examples
easy to 
classify
easy to 
classify
hard to 
classify

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
SVMs vs Logistic regression
•
Logistic regressors don’t optimize the 
number of “mistakes”
•
No special attention is paid to the 
“difficult” instances – every instance 
influences the model
•
But “easy” instances can affect the 
model (and in a bad way!)
•
How can we develop a classifier that 
optimizes the number of mislabeled 
examples?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Support Vector Machines: Basic idea
A classifier can be defined by the hyperplane (line)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Support Vector Machines: Basic idea
Observation: Not all classifiers are equally good

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Support Vector Machines
such that
“support vectors”
•
An SVM seeks the classifier (in 
this case a line) that is furthest 
from the nearest points
•
This can be written in terms of a 
specific optimization problem:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Support Vector Machines
But: is finding such a separating 
hyperplane even possible?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Support Vector Machines
Or: is it actually a good idea?

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Support Vector Machines
Want the margin to be as wide as possible
While penalizing points on the wrong side of it

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Support Vector Machines
such that
Soft-margin formulation:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of Support Vector Machines
•
SVMs seek to find a hyperplane (in two dimensions, a line) 
that optimally separates two classes of points
•
The “best” classifier is the one that classifies all points 
correctly, such that the nearest points are as far as possible
from the boundary
•
If not all points can be correctly classified, a penalty is 
incurred that is proportional to how badly the points are 
misclassified (i.e., their distance from this hyperplane)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced Support Vector Machines
• Demonstrated some advantages/disadvantages among 
different types of classification objectives

Python Data Products
Course 2: Design thinking and predictive pipelines 
Lecture: Other classification schemes

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Introduce a few other popular classification schemes

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Decision trees
Is sex male?
Is age > 9.5?
is sibsp > 2.5
survived
survived
died
died
Y
N
Y
N
Y
N
0.73  36%
0.17 61%
0.89  2%
0.05 2%
(example from “Titanic Survival Data”,
see https://www.kaggle.com/c/titanic)
• Decision trees are a “rule based” classifier
• Each rule corresponds to a branch in the tree 
based on one of its features
• This is an example of a non-linear classifier, 
where different combinations of rules (and a 
sufficiently deep tree) can allow the data to be 
separated in arbitrary ways

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: Neural Network-based classification
This lecture sucks I don’t think I’ll bother keeping it

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Briefly introduced other popular classification schemes

Week 4
Libraries and tools for 
regression and 
classification

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: classification in Python

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Demonstrate how to set up classification problems in 
Python
• Introduce the LogisticRegression model from the 
sklearn library

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Dataset – Polish bankruptcies
We'll look at a simple dataset from the UCI repository:
https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data
• This dataset is concerned with which (Polish) companies go bankrupt

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Reading the data
• Data is in CSV format, but first contains a header that we need to skip
Header ends and the "real" data begins after we see the "@data" tag
• Next we read the CSV data. We (a) skip rows with missing entries; (b) convert all 
fields to floats; and (c) convert the label to a bool

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Processing the data
• Next let's look at some simple statistics about our data
Number of samples (after discarding missing values)
Number of positive samples
• Next we extract our features (X) and labels (y), much as we would do for a 
regression problem
True/False labels

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Concept: The sklearn library
The sklearn library contains a number of different regression and 
classification models
For example:
• linear_model.LinearRegression() - linear regression
• linear_model.LogisticRegression() - logistic regression
• svm.SVC - Support Vector Classifier
• In this lecture we'll use the LogisticRegression module

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Fitting the logistic regression model
• First we import the library and create an instance of the model, before fitting it to 
data
• Note that this function doesn't produce any output, rather it just updates the 
class instance to store the model

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Making predictions
• Make predictions from the data:
• Check whether they match the labels
• And compute the error

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Training vs. Testing?
We achieved fairly high accuracy using a simple classifier "off the 
shelf"
• But note that we're evaluating our classifier on the same data 
that was used to train it
• How can we be sure that our classifier will work well on 
unseen data?
• This is something we'll cover in the next course, when we look 
at training, testing, and validation

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Other classification algorithms in sklearn
This example showed how to use logistic regression, but other 
classifiers are available in sklearn and have a similar interface:
• sklearn.svm.SVC: Support Vector Classifier
• sklearn.tree.DesicionTreeClassifier: Decision trees
• sklearn.naive_bayes: Naïve Bayes
• sklearn.neighbors.KNeighborsClassifier: Nearest Neighbors
•
see http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html for 
additional comparisons

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced the sklearn library
• Showed how to set up a simple classification problem in 
Python
On your own...
•
Try to set up a similar classification problem 
using another of the UCI datasets – look for 
classification datasets that have numerical 
attributes (i.e., datasets similar to the one 
used for this exercise)

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: Introduction to Training and Testing

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Introduce the concepts of training versus testing
• Discuss the importance of evaluating models on unseen 
data
• Show how to adjust our Python code to make use of these 
ideas

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Training and testing?
In the previous lecture we saw that we can obtain good 
performance with a simple classifier, but highlight a possible issue:
If we evaluate a system on the same data used to train the system, 
we may overestimate its performance
Really, we want to know how well a method is likely to work on 
unseen data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Training and testing?
To estimate how well a system is likely to perform on new data, we 
can split our dataset in to two components:
• A training set to train the machine learning model
• A test set used to estimate the performance on new data
We'll investigate both of these ideas more in Course 3, but for the 
moment, let's quickly explore how we can adapt our previous code 
to incorporate these two components

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Training and testing
First we read the dataset, exactly as we did in previous lectures:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Training and testing
The first thing we do differently is to shuffle the data:
We do this because we we want the training and test set to be random samples of the 
data – if we didn't use random samples, different subsets of the data could have distinct 
characteristics that could cause the model to under- (or over) perform on one of them

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Training and testing
Next we split the data into a train and a test portion
Note that we split both the data (X) and the labels (y) in the same way

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Training and testing
Next we train our model as before, but we use only the training data and labels

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Training and testing
Finally we can compute the accuracy of the model, but this time we do so separately for 
the training and test portions
The latter quantity measures how well the model is likely to perform on new data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary
This lecture presented a very brief introduction to training and 
testing. Though we'll cover more in Course 3, the basic concepts 
are:
• Simply training on a dataset doesn't give us a sense of how a 
model will generalize to new data
• This generazilation ability can be estimated using a test set
• Training and test sets should be non-overlapping, random splits 
of our data

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced the concepts of training and testing sets
• Briefly described the difference between training 
performance versus generalization ability
• Showed how to adapt our classification code to measure 
performance on the training set
On your own...
•
Try repeating this exercise for our regression 
example from the previous lecture, i.e., split 
the data into training/testing portions and 
measure its training and testing performance

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: gradient descent in Python

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Show how gradient descent can be implemented in 
Python
• Introduce the relationship between 
equations/mathematical objectives (theory) and their 
implementation (practice)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Goal: Regression objective
Let’s look at implementing this on the same 
PM2.5 dataset from our previous lecture on 
regression

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Reading the data
Reading the data from CSV, and discarding missing entries:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Extracting features from the data
Extract features from the dataset:
Offset and temperature
K = number of feature dimensions

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Initialization
Initialize parameters (and include some utility functions)
• Initializing theta_0 (the offset parameter) to the mean value will help the model to 
converge faster
• Generally speaking, initializing gradient descent algorithms with a "good guess" can 
help them to converge more quickly

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Derivative
Compute partial derivatives for each dimension:
Also compute MSE, just for utility
Derivative:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Derivative
Compute partial derivatives for each dimension:
Stopping condition
Update in direction 
of derivative

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Derivative
Read output
• (Almost) identical to the result we got when using the regression library in the previous 
lecture

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary
Although a crude (and fairly slow) implementation, this 
type of approach can be extended to handle quite 
general and complex objectives. However it has several 
difficult issues to deal with:
• How to initialize?
• How to set parameters like the learning 
rate and convergence criteria?
• Manually computing derivatives is time-
consuming – and difficult to debug

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Briefly introduced a crude implementation of gradient 
descent in Python
• Later, we'll see how the same operations can be 
supported via libraries

Python Data Products
Course 2: Design thinking and predictive pipelines
Lecture: gradient descent in tensorflow

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Learning objectives
In this lecture we will...
• Introduce the tensorflow library

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Tensorflow
Tensorflow, though often associated with deep 
learning, is really just a library that simplifies gradient 
descent and optimization problems, like those we saw 
in the previous lecture
In this lecture we'll reimplement the previous lecture's 
example in Tensorflow

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Gradient Descent in Tensorflow
Reading the data is much the same as before (except that we first import the tensorflow
library)

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Gradient Descent in Tensorflow
Next we extract features from the data
Note that we convert y to a native tensorflow vector. In particular we convert it to column 
vector. We have to be careful about getting our matrix dimensions correct or we may 
(accidentally) apply the wrong matrix operations.

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Gradient Descent in Tensorflow
Next we write down the objective – note that we use native tensorflow operations to do 
so
Next we setup the variables we want to optimize – note that we explicitly indicate that 
these are variables to be optimized (rather than constants)
Initialized to zero
Stochastic gradient descent optimizer with learning rate of 0.01
Specify the objective we want to optimize – note that no computation is 
performed (yet) when we run this function

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Gradient Descent in Tensorflow
Boilerplate for initializing the optimizer...
We want to minimize the objective

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Gradient Descent in Tensorflow
Run 1,000 iterations of gradient descent:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Code: Gradient Descent in Tensorflow
Print out the results:

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary
Note that in contrast to our "manual" implementation 
of gradient descent, many of the most difficult issues 
were taken care of for us:
•
No need to compute the gradients –
tensorflow does this for us!
•
Easy to experiment with different models
•
Very fast to run 1,000 iterations, especially 
with GPU acceleration!

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Other libraries
Tensorflow is just one example of a library that can be used for 
this type of optimization. Alternatives include:
• Theano - http://deeplearning.net/software/theano/
• Keras - https://keras.io/
• Torch - http://torch.ch/
• Etc.
Each has fairly similar functionality, but some differences in 
interface

Data to Products Specialization: Course 2: Design Thinking and Predictive Pipelines
Summary of concepts
• Introduced the Tensorflow library for optimization


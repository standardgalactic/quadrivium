Julian M. Kunkel
Thomas Ludwig (Eds.)
 123
LNCS 9137
30th International Conference, ISC High Performance 2015
Frankfurt, Germany, July 12–16, 2015
Proceedings
High Performance 
Computing

Lecture Notes in Computer Science
9137
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zürich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7407

Julian M. Kunkel
• Thomas Ludwig (Eds.)
High Performance
Computing
30th International Conference,
ISC High Performance 2015
Frankfurt, Germany, July 12–16, 2015
Proceedings
123

Editors
Julian M. Kunkel
Deutsches Klimarechenzentrum (DKRZ)
Hamburg
Germany
Thomas Ludwig
Deutsches Klimarechenzentrum (DKRZ)
Hamburg
Germany
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-20118-4
ISBN 978-3-319-20119-1
(eBook)
DOI 10.1007/978-3-319-20119-1
Library of Congress Control Number: 2015940735
LNCS Sublibrary: SL1 – Theoretical Computer Science and General Issues
Springer Cham Heidelberg New York Dordrecht London
© Springer International Publishing Switzerland 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.springer.com)

Preface
ISC High Performance, formerly known as the International Supercomputing Con-
ference, was founded in 1986 as the Supercomputer Seminar. Originally organized by
Professor Hans Meuer, Professor of Computer Science at the University of Mannheim
and former director of the computer center, the seminar brought together a group of 81
scientists and industrial partners who all shared an interest in high-performance com-
puting. Since then the annual conference has become a major international event within
the HPC community, and accompanying its growth in size over the years, the con-
ference has moved from Mannheim via Heidelberg, Dresden, Hamburg and Leipzig to
Frankfurt. With 2,405 attendees and 156 exhibitors from over 51 countries in 2014, we
were happy to see that this steady growth of interest also turned ISC High Performance
2015 into a powerful and memorable event.
In 2007, we decided to strengthen the scientiﬁc part of the conference by presenting
selected talks on relevant research results within the HPC ﬁeld. These research paper
sessions began as a separate day preceding the conference, where slides and accom-
panying papers were made available via the conference website. The research paper
sessions have since evolved into an integral part of the conference, and this year the
scientiﬁc presentations took place over a period of three days.
This year, the organizers of the ISC High Performance conference reintroduced an
ISC-sponsored award to encourage outstanding research in high-performance com-
puting and to honor the overall best research paper submitted to the conference. This
annual award is now called the Hans Meuer Award in memory of the late Dr. Hans
Meuer, general chair of the ISC conference from 1986 through 2014, and co-founder
of the TOP500 project. From all research papers submitted, the Research Paper
Committee consider the three papers with the highest review scores for the award and
elect the winner among them.
The call for participation was issued in fall 2014, inviting researchers and devel-
opers to submit the latest results of their work to the Program Committee. As in the
year before, we invited the submission of full papers and technical short papers. In all,
67 papers were submitted from authors all over the world. In a peer-review process, an
international committee selected 27 full papers and 10 short papers for publication and
for presentation in the research paper sessions.
We are pleased to announce that many fascinating topics in HPC are covered by the
proceedings. The papers address the following issues in regards to the development of
an environment for exascale supercomputers:
– Cost-efﬁcient data centers
– Scalable applications
– Advancements in algorithms
– Scientiﬁc libraries
– Programming models
– Architectures

– Performance models and analysis
– Automatic performance optimization
– Parallel I/O
– Energy efﬁciency
We believe that this selection is highly appealing across a number of specializations
and that the presentations will foster inspiring discussions with the audience.
Three award committees selected papers considered to be of exceptional quality and
worthy of special recognition.
– The Hans Meuer Award honors the overall best research paper submitted to the
conference. The award went to:
“Accelerating LBM and LQCD Application Kernels by In-Memory Processing” by
Paul F. Baumeister, Hans Boettiger, José R. Brunheroto, Thorsten Hater, Thilo
Mauer, Andrea Nobile and Dirk Pleiter
– PRACE, the Partnership for Advanced Computing in Europe, awards a prize to the
best scientiﬁc paper by a European student or scientist. This year’s award was
granted to:
“Lattice-CSC: Optimizing and building an efﬁcient supercomputer for Lattice-QCD
and to achieve ﬁrst place in Green500” by David Rohr, Matthias Bach, Gvozden
Neskovic, Volker Lindenstruth, Christopher Pinke and Owe Philipsen
– The Gauss Centre for Supercomputing sponsors the Gauss Award. This award is
assigned to the most outstanding paper in the ﬁeld of scalable supercomputing and
went to:
“Updating the Energy Model for Future Exascale Systems” by Peter M. Kogge.
We would like to express our gratitude to all our colleagues for submitting papers to
the ISC scientiﬁc sessions, as well as to the members of the Program Committee for
organizing this year’s attractive program.
June 2015
Julian M. Kunkel
Thomas Ludwig
VI
Preface

Organization
Program Committee
Balaji, Pavan
Argonne National Laboratory, USA
Bohlouli, Mahdi
University of Siegen, Germany
Bönisch, Thomas
HLRS, Germany
Brown, Jed
Argonne National Laboratory, USA
Burrows, Eva
University of Bergen, Norway
Cai, Xing
Simula Research Laboratory, Norway
Flouri, Tomas
HITS, Germany
Fröning, Holger
University of Heidelberg, Germany
Gross, Lutz
School of Earth Sciences, The University
of Queensland, Australia
Hackenberg, Daniel
TU Dresden, Germany
Ham, David
Imperial College London, UK
Hannig, Frank
Friedrich-Alexander University Erlangen-Nürnberg,
Germany
Haveraaen, Magne
University of Bergen, Norway
Herhut, Stephan
Intel Labs, Santa Clara, USA
Huang, Weicheng
National Center for High-Performance Computing,
Taiwan
Huynh, Huynh Phung
A*Star, Singapore
Kindratenko, Volodymyr
National Center for Supercomputing Applications,
USA
Koshulko, Oleksiy
Glushkov Institute of Cybernetics NAS, Ukraine
Kunkel, Julian
University of Hamburg, Germany
Li, Dong
Oak Ridge National Laboratory, USA
Lin, Fang-Pang
National Center for High-Performance Computing,
Taiwan
Liu, Qing
Oak Ridge National Laboratory, USA
Ludwig, Thomas
DKRZ, Germany
Manjunathaiah,
Manjunathaiah
University of Reading, UK
McIntosh-Smith, Simon
University of Bristol, UK
Membarth, Richard
Intel Visual Computing Institute, Saarland University,
Germany
Mohr, Bernd
Jülich Supercomputing Centre, Germany
Moskovsky, Alexander
RSC SKIF, Russia
Müller, Matthias
RWTH Aachen University, Germany
Nakajima, Kengo
University of Tokyo, Japan
Nou, Ramon
Barcelona Supercomputing Center, Spain
Olbrich, Stephan
University of Hamburg, Germany

Ortega, Julio
University of Granada, Spain
Poulet, Thomas
CSIRO, Australia
Qian, Ying
KAUST, Saudi Arabia
Resch, Michael M.
HLRS, Germany
Duro, Francisco Rodrigo
Universidad Carlos III de Madrid, Spain
Rouson, Damian
Center for Computational Earth and Environmental
Sciences at Stanford University, USA
Scholz, Sven-Bodo
Heriot-Watt University, UK
Solnushkin, Konstantin S.
Saint Petersburg State Polytechnic University, Russia
Stamatakis, Alexandros
TU München, Germany
Tatebe, Osamu
University of Tsukuba, Japan
Thiyagalingam, Jeyarajan
University of Liverpool, UK
Tolentino, Matthew E.
Intel, USA
Tsujita, Yuichi
RIKEN AICS, Japan
Vigouroux, Xavier
Bull, France
Wang, Zhonglei
Intel Mobile Communications, Germany
Wild, Thomas
TU München, Germany
Ziegenhein, Peter
The Institute of Cancer Research, UK
Program Committee for the Research Poster Session
Aguilera, Alvaro
Blas, Javier Garcia
Bohlouli, Mahdi
Bönisch, Thomas
Cai, Xing
Ham, David
Hannig, Frank
Huang, Weicheng
Kunkel, Julian
Li, Dong
Membarth, Richard
Moskovsky, Alexander
Nakajima, Kengo
Ortega, Julio
McIntosh-Smith, Simon
Solnushkin, Konstantin S.
Tatebe, Osamu
Thiyagalingam, Jeyarajan
Tsujita, Yuichi
Ziegenhein, Peter
Additional Reviewers
Armour, Wes
Chasapis, Konstantinos
Czech, Lucas
Dao, David
Darriba, Diego
Fenwick, Joel
Gao, Jinfang
Geyer, Robin
Gong, Zhenhuan
Hu, Qi
Kozlov, Alexey
Langguth, Johannes
Lu, Mian
Miller, Felix
Miranda, Alberto
Mu, Kimmy
Ong, Zhong Liang
Osprey, Annette
Parayil, Preethi
Reiche, Oliver
Riley, Graham
Schmid, Moritz
Schmitt, Christian
Schuchart, Joseph
Shaw, Simon
Tang, Wai Teng
Witterauf, Michael
Zou, Hongbo
VIII
Organization

Contents
Asynchronous Iterative Algorithm for Computing Incomplete
Factorizations on GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Edmond Chow, Hartwig Anzt, and Jack Dongarra
Matrix Multiplication on High-Density Multi-GPU Architectures:
Theoretical and Experimental Investigations . . . . . . . . . . . . . . . . . . . . . . . .
17
Peng Zhang and Yuxiang Gao
A Framework for Batched and GPU-Resident Factorization Algorithms
Applied to Block Householder Transformations . . . . . . . . . . . . . . . . . . . . .
31
Azzam Haidar, Tingxing Tim Dong, Stanimire Tomov, Piotr Luszczek,
and Jack Dongarra
Parallel Efficient Sparse Matrix-Matrix Multiplication
on Multicore Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
Md. Mostofa Ali Patwary, Nadathur Rajagopalan Satish,
Narayanan Sundaram, Jongsoo Park, Michael J. Anderson,
Satya Gautam Vadlamudi, Dipankar Das, Sergey G. Pudov,
Vadim O. Pirogov, and Pradeep Dubey
On the Design, Development, and Analysis of Optimized Matrix-Vector
Multiplication Routines for Coprocessors . . . . . . . . . . . . . . . . . . . . . . . . . .
58
Khairul Kabir, Azzam Haidar, Stanimire Tomov, and Jack Dongarra
Large-Scale Neo-Heterogeneous Programming and Optimization of SNP
Detection on Tianhe-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
Yingbo Cui, Xiangke Liao, Shaoliang Peng, Yutong Lu, Canqun Yang,
Bingqiang Wang, and Chengkun Wu
ACCOLADES: A Scalable Workflow Framework for Large-Scale
Simulation and Analyses of Automotive Engines . . . . . . . . . . . . . . . . . . . .
87
Shashi M. Aithal and Stefan M. Wild
Accelerating LBM and LQCD Application Kernels
by In-Memory Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
Paul F. Baumeister, Hans Boettiger, José R. Brunheroto,
Thorsten Hater, Thilo Maurer, Andrea Nobile, and Dirk Pleiter
On Quantum Chemistry Code Adaptation for RSC
PetaStream Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
Vladimir Mironov, Maria Khrenova, and Alexander Moskovsky

Dtree: Dynamic Task Scheduling at Petascale. . . . . . . . . . . . . . . . . . . . . . .
122
Kiran Pamnany, Sanchit Misra, Vasimuddin Md., Xing Liu,
Edmond Chow, and Srinivas Aluru
Feasibility Study of Porting a Particle Transport Code to FPGA . . . . . . . . . .
139
Iakovos Panourgias, Michele Weiland, Mark Parsons, David Turland,
Dave Barrett, and Wayne Gaudin
A Scalable, Linear-Time Dynamic Cutoff Algorithm
for Molecular Dynamics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
Paul Springer, Ahmed E. Ismail, and Paolo Bientinesi
BWTCP: A Parallel Method for Constructing BWT in Large Collection
of Genomic Reads. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
Heng Wang, Shaoliang Peng, Yutong Lu, Chengkun Wu, Jiajun Wen,
Jie Liu, and Xiaoqian Zhu
Lattice-CSC: Optimizing and Building an Efficient Supercomputer
for Lattice-QCD and to Achieve First Place in Green500 . . . . . . . . . . . . . . .
179
David Rohr, Matthias Bach, Gvozden Nešković, Volker Lindenstruth,
Christopher Pinke, and Owe Philipsen
An Efficient Clique-Based Algorithm of Compute Nodes Allocation
for In-memory Checkpoint System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
Xiangke Liao, Canqun Yang, Zhe Quan, Tao Tang, and Cheng Chen
A Scalable Algorithm for Radiative Heat Transfer Using Reverse Monte
Carlo Ray Tracing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
Alan Humphrey, Todd Harman, Martin Berzins, and Phillip Smith
Optimizing Processes Mapping for Tasks with Non-uniform Data Exchange
Run on Cluster with Different Interconnects . . . . . . . . . . . . . . . . . . . . . . . .
231
Victor Getmanskiy, Vladimir Chalyshev, Dmitriy Kryzhanovsky,
Igor Lopatin, and Evgeny Leksikov
Dynamically Adaptable I/O Semantics for High Performance Computing . . . .
240
Michael Kuhn
Predicting Performance of Non-contiguous I/O with Machine Learning . . . . .
257
Julian Kunkel, Michaela Zimmer, and Eugen Betke
A Best Practice Analysis of HDF5 and NetCDF-4 Using Lustre . . . . . . . . . .
274
Christopher Bartz, Konstantinos Chasapis, Michael Kuhn, Petra Nerge,
and Thomas Ludwig
Striping Layout Aware Data Aggregation for High Performance I/O
on a Lustre File System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
Yuichi Tsujita, Atsushi Hori, and Yutaka Ishikawa
X
Contents

Hop: Elastic Consistency for Exascale Data Stores . . . . . . . . . . . . . . . . . . .
291
Latchesar Ionkov and Michael Lang
Energy-Efficient Data Processing Through Data Sparsing with Artifacts. . . . .
307
Pablo Graubner, Patrick Heckmann, and Bernd Freisleben
Updating the Energy Model for Future Exascale Systems. . . . . . . . . . . . . . .
323
Peter M. Kogge
High-Order ADER-DG Minimizes Energy- and Time-to-Solution
of SeisSol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
Alexander Breuer, Alexander Heinecke, Leonhard Rannabauer,
and Michael Bader
Modeling the Productivity of HPC Systems on a Computing Center Scale . . .
358
Sandra Wienke, Hristo Iliev, Dieter an Mey, and Matthias S. Müller
Taking Advantage of Node Power Variation in Homogenous HPC Systems
to Save Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
376
Torsten Wilde, Axel Auweter, Hayk Shoukourian, and Arndt Bode
A Run-Time System for Power-Constrained HPC Applications. . . . . . . . . . .
394
Aniruddha Marathe, Peter E. Bailey, David K. Lowenthal,
Barry Rountree, Martin Schulz, and Bronis R. de Supinski
A Machine Learning Approach for a Scalable, Energy-Efficient
Utility-Based Cache Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
Isa Ahmet Guney, Abdullah Yildiz, Ismail Ugur Bayindir,
Kemal Cagri Serdaroglu, Utku Bayik, and Gurhan Kucuk
A Case Study - Cost of Preemption for Urgent Computing on SuperMUC . . .
422
Siew Hoon Leong and Dieter Kranzlmüller
Designing Non-blocking Personalized Collectives with Near Perfect
Overlap for RDMA-Enabled Clusters. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
434
Hari Subramoni, Ammar Ahmad Awan, Khaled Hamidouche,
Dmitry Pekurovsky, Akshay Venkatesh, Sourav Chakraborty,
Karen Tomko, and Dhabaleswar K. Panda
Design Methodology for Optimizing Optical Interconnection Networks
in High Performance Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
454
Sébastien Rumley, Madeleine Glick, Simon D. Hammond,
Arun Rodrigues, and Keren Bergman
Quantifying Communication in Graph Analytics . . . . . . . . . . . . . . . . . . . . .
472
Andreea Anghel, German Rodriguez, Bogdan Prisacari,
Cyriel Minkenberg, and Gero Dittmann
Contents
XI

Formal Metrics for Large-Scale Parallel Performance. . . . . . . . . . . . . . . . . .
488
Kenneth Moreland and Ron Oldfield
Hunting Down Load Imbalance: A Moving Target . . . . . . . . . . . . . . . . . . .
497
Christoph Pospiech
Orchestrating Docker Containers in the HPC Environment . . . . . . . . . . . . . .
506
Joshua Higgins, Violeta Holmes, and Colin Venters
Performance and Scaling of WRF on Three Different Parallel
Supercomputers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
514
Zaphiris Christidis
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
529
XII
Contents

Asynchronous Iterative Algorithm
for Computing Incomplete
Factorizations on GPUs
Edmond Chow1, Hartwig Anzt2(B), and Jack Dongarra2
1 Georgia Institute of Technology, Atlanta, GA, USA
2 University of Tennessee, Knoxville, TN, USA
hanzt@icl.utk.edu
Abstract. This paper presents a GPU implementation of an asynchro-
nous iterative algorithm for computing incomplete factorizations. Asyn-
chronous algorithms, with their ability to tolerate memory latency, form
an important class of algorithms for modern computer architectures.
Our GPU implementation considers several non-traditional techniques
that can be important for asynchronous algorithms to optimize conver-
gence and data locality. These techniques include controlling the order
in which variables are updated by controlling the order of execution of
thread blocks, taking advantage of cache reuse between thread blocks,
and managing the amount of parallelism to control the convergence of
the algorithm.
1
Introduction
Asynchronous algorithms, with their ability to tolerate memory latency, form an
important class of algorithms for modern computer architectures. In this paper,
we develop a GPU implementation for a recently proposed asynchronous iterative
incomplete factorization algorithm [4]. In particular, we consider the following
techniques to enhance data locality and convergence that may be considered non-
traditional as they are not strictly allowed in standard GPU implementations.
– In an asynchronous iterative method, variables are updated using values of
other variables that are currently available, rather than waiting for the most
updated values of these other variables (this will be made more precise later).
The rate of convergence of the method may depend on the order in which
variables are updated. In traditional GPU computations, there is no deﬁned
ordering in which thread blocks are executed, making it impossible to control
the update order. For the NVIDIA K40c GPU, however, we were able to
determine the order in which thread blocks are executed, thereby allowing us
to control the order of the updates of the variables.
– Eﬃcient GPU performance requires that GPU thread blocks reuse data
brought into shared memory. Shared memory must be conﬁgured as cache
when the working set is large, otherwise few thread blocks can run in parallel.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 1–16, 2015.
DOI: 10.1007/978-3-319-20119-1 1

2
E. Chow et al.
However, without control over the order in which thread blocks are executed,
temporal locality cannot be properly exploited. Since we are able to control
the execution order of thread blocks on the K40c GPU, we can assure that
data in cache is eﬃciently used. As mentioned, traditionally this order is not
known.
– In GPU computing, the amount of parallelism is typically not controlled;
work is scheduled onto all multiprocessors. Using less parallelism and updating
fewer variables simultaneously in order to use more recently updated variables
may lead to faster convergence than using more parallelism. We investigate
this tradeoﬀby using the unconventional approach of controlling the occu-
pancy, or the fraction of the maximum number of threads executing simulta-
neously per multiprocessor.
There has been some past work on asynchronous algorithms on GPUs for lin-
ear iteration methods. See [1] for a comprehensive study on block-asynchronous
Jacobi iterations. Venkatasubramanian et al. [18] solve a 2D Poisson equation on
a structured grid using asynchronous stencil operations on a hybrid CPU/GPU
system. Contassot-Vivier et al. [5] investigated the impact of asynchronism when
solving an advection-diﬀusion problem with a GPU-accelerated PDE solver. The
work in this paper on investigating data locality and update order for asynchro-
nous algorithms on GPUs is new.
This paper is organized as follows. In Sect. 2, we provide background on
incomplete factorizations and the recently proposed iterative ILU algorithm.
Then in Sect. 3, we discuss the implementation of the algorithm on GPUs and
consider how convergence and data locality are impacted by the order in which
computations are performed. Experimental tests with this implementation are
reported in Sect. 4, and we conclude in Sect. 5.
2
Iterative ILU Algorithm
Preconditioners are a critical part of solving large sparse linear systems via
iterative methods. A popular class of preconditioners is incomplete LU (ILU)
factorizations. These preconditioners are generally computed using a Gaussian
elimination process where small non-diagonal elements are dropped in some way.
The problem-independence of ILU preconditioners makes this class attractive for
a wide range of problems, particularly for optimization problems.
An ILU factorization is the approximate factorization of a nonsingular sparse
matrix A into the product of a sparse lower triangular matrix L and a sparse
upper triangular matrix U (A ≈LU), where nonzeros or ﬁll-in are only permit-
ted in speciﬁed locations, (i, j) of L and U. Deﬁne the sparsity pattern S to be
the set of matrix locations where nonzeros are allowed, i.e., (i, j) ∈S implies
that entry lij in matrix L is permitted to be nonzero (i ≥j), or that entry uij
in matrix U is permitted to be nonzero (i ≤j). The set S always includes the
diagonal of the L and U factors so that these factors are nonsingular. The basic
algorithm, called ILU(0), approximates the LU factorization by allowing only
nonzero elements in L and U that are nonzero in A. To enhance the accuracy of

Asynchronous Iterative Algorithm for Computing Incomplete
3
the preconditioner, one can allow for additional ﬁll-in in the incomplete factors.
The choice of S can be made either before the factorization, or may be made
dynamically, during elimination. The classical factorization algorithm, based on
Gaussian elimination, is inherently sequential, but some parallelism does exist, as
it is usually possible to ﬁnd multiple rows that can be eliminated in parallel, i.e.,
those that only depend on rows that already have been eliminated; see [17] for an
overview. Parallelism is usually reduced when more ﬁll-in is allowed. Multicolor-
ing and domain decomposition reordering can be used to enhance the available
parallelism [2,7,11,16]. However, these approaches generally have limited scala-
bility, as they are unable to exploit the computing performance of thousands of
light-weight cores that we expect in future HPC architectures [3].
For large amounts of parallelism, a new algorithm for computing ILU factor-
izations was recently proposed [4], and is the focus of this paper. This algorithm
uses the property of ILU factorizations that
(LU)ij = aij,
(i, j) ∈S
(1)
where (LU)ij denotes the (i, j) entry of the product of the computed factors L
and U, and aij is the corresponding entry in matrix A. For (i, j) ∈S, the iterative
ILU algorithm computes the unknowns lij (for i > j) and uij (for i ≤j) using
the bilinear constraints
min(i,j)

k=1
likukj = aij,
(i, j) ∈S
(2)
which corresponds to enforcing the property (1). We use the normalization that
the diagonal of the lower triangular L is ﬁxed to one. Thus we need to solve a
system of |S| equations in |S| unknowns.
To solve this system, Ref. [4] proposed writing
lij =
1
ujj

aij −
j−1

k=1
likukj

, i > j
uij = aij −
i−1

k=1
likukj, i ≤j
(3)
which has the form x = G(x), where x is a vector containing the unknowns lij
and uij for (i, j) ∈S. The equations are then solved by using the ﬁxed-point
iteration x(p+1) = G(x(p)), for p = 0, 1, . . ., starting with some initial x(0). See [4]
for details on the convergence of this method. In brief, it can be proven that
the iteration is locally convergent for standard (synchronous) and asynchronous
iterations [9].
The iterative ILU algorithm, which solves the Eq. (2), is given in Algorithm 1.
The actual implementation that served as the basis for this study can be found
in the MAGMA open-source software library [10]. Each ﬁxed-point iteration
updating all the unknowns is called a “sweep.” In this algorithm, an important
question is how to choose the initial values of the lij and uij variables (line 1).
In many applications, a natural initial guess is available; for example, in time-
dependent problems, the L and U computed at the previous time step may be an

4
E. Chow et al.
excellent initial guess for the current time step. In other cases, there may be no
natural initial guess. In [4], the matrix A is symmetrically scaled to have a unit
diagonal, and the initial guess for L and U are then chosen to be the lower and
upper parts of A, respectively. We also use this initial guess for the experiments
in this paper.
Algorithm 1. Fine-Grained Parallel Incomplete Factorization
1 Set unknowns lij and uij to initial values
2 for sweep = 1, 2, . . . until convergence do
3
parallel for (i, j) ∈S do
4
if i > j then
5
lij =

aij −j−1
k=1 likukj

/ujj
6
else
7
uij = aij −i−1
k=1 likukj
8
end
9
end
10 end
We conclude this section by pointing out several features of this algorithm
that make it diﬀerent than existing methods, and relevant to parallel computing
on emerging architectures:
– The algorithm is ﬁne-grained, allowing for scaling to very large core counts,
limited only by the number of nonzero elements in the factorization.
– The algorithm does not need to use reordering to enhance parallelism, and
thus reorderings that enhance the accuracy of the incomplete factorization
can be used.
– The algorithm can utilize an initial guess for the ILU factorization, which
cannot be exploited by conventional ILU factorization algorithms.
– The bilinear equations do not need to be solved very accurately since the ILU
factorization itself is only an approximation.
3
GPU Implementation
3.1
General Parallelization Strategy
When multiple processors are available, an iteration based on x(p+1) = G(x(p))
may be parallelized by assigning each processor to compute a subset of the
components of x(p+1) using values of x(p), such that each component is updated
by exactly one processor. This is called a synchronous iteration, as all values of
x(p) must generally be computed before the computation of x(p+1) may start. In
contrast, an asynchronous iteration is one where the computation of components
of x may use the latest components of x that are available. Convergence may

Asynchronous Iterative Algorithm for Computing Incomplete
5
be faster than the synchronous iteration because more updated values are used
(e.g., Gauss-Seidel type of iteration compared to Jacobi type) or may be slower
than synchronous iteration in the case that some components are rarely updated.
In general, there may be a tradeoﬀbetween parallelism and convergence: with
less parallel resources, the asynchronous iterations tend to use “fresher” data
when computing updates; with more parallel resources, the iterations tend to
use older data and thus converge more slowly.
For GPU computing, subsets of the components of x are assigned to GPU
thread blocks. Each thread block updates the components of x assigned to it.
The thread blocks are scheduled onto GPU multiprocessors. Within each thread
block, the components of x are updated simultaneously (in Jacobi-like fashion).
As there are generally more thread blocks than multiprocessors, some thread
blocks are processed before others, and thus update their components of x before
others. Thus some thread blocks within one ﬁxed-point sweep may use newer
data than others (in Gauss-Seidel-like fashion). However, there is no guarantee
of the order in which thread blocks are scheduled onto multiprocessors. Overall,
the iteration may be considered to be “block-asynchronous” [1].
3.2
Component Update Order
The convergence rate of an asynchronous ﬁxed-point iteration for the system of
equations x = G(x) may depend on the order in which the components of x are
updated, particularly if some equations have more dependencies on components
of x than others. Speciﬁcally, for the computations (3) that are performed in
the new ILU formulation, there is a tree of dependencies between the variables
(components) being updated. Thus convergence will be faster if the asynchronous
updates of the variables are ordered roughly following these dependencies. Such
orderings are called “Gaussian elimination orderings” in [4].
On GPUs, each thread block is responsible for updating a given set of vari-
ables. Unfortunately, there is no guarantee of thread block execution order for
current generation GPU programs. Using a simple kernel that records the order
in which thread blocks are executed, we observed on the NVIDIA K40c GPU that
block indices are always assigned in order of execution of the thread blocks. (On
some earlier GPU models, we observed that the order of execution appears ran-
dom.) Using this result, we can explicitly set a component update order that will
be approximately followed by the asynchronous iterations (approximate because
the iterations are asynchronous). In Sect. 4, we will test the eﬀect of component
update order on convergence rate.
3.3
Data Locality and Cache Reuse
GPU access of data in global memory (oﬀ-chip DRAM) is expensive, and the
performance of any GPU code depends on how eﬃciently memory is reused after
it has been brought into cache or shared memory. In this section, we consider
how to partition the computational work into thread blocks in order to maximize

6
E. Chow et al.
data reuse. We note that such partitionings also aﬀect component update order
and thus convergence.
Each thread is associated with an element (i, j) ∈S. Each thread thus com-
putes either lij or uij in Eq. (3). This computation requires reading row i of
L and column j of U. This row and column may be reused to compute other
elements in S. Thus we have the problem of partitioning the (i, j) ∈S among a
given number of thread blocks such that threads in the thread block require, in
aggregate, as few rows of L or columns of U as possible. (In the above, only part
of the row or column is needed by each thread, due to the upper limit on the
summations in (3), but this will only aﬀect our analysis by a constant factor.)
Another way to view this problem is to maximize the number of times each
row of L and each column of U is reused by a thread block. We deﬁne the reuse
factor of thread block l as
freuse(l) := 1
2
|Sl|
ml
+ |Sl|
nl

where |Sl| is the number of elements of S assigned to thread block l, and where
ml and nl are the number of rows of L and columns of U, respectively, required
by thread block l. The ﬁrst term in the brackets is the average number of times
that the rows are reused, while the second is the average number of times that the
columns are reused. If the elements of S are assigned arbitrarily to the thread
blocks, then the reuse factor is 1 in the worst case. For simplicity, we assume
below that ml = nl, since ml and nl approximately equal will give higher reuse
factors than ml and nl being very diﬀerent.
We ﬁrst consider a matrix corresponding to a 2D mesh problem using a
5-point stencil. Figure 1 (middle) shows the sparsity pattern of the matrix corre-
sponding to a 6×6 mesh (left). The mesh has been partitioned into 9 subdomains
of size 2 × 2, and the rows and columns of the matrix are ordered such that the
rows and columns corresponding to the same subdomain are ordered together.
Horizontal and vertical lines are used to separate the rows and columns corre-
sponding to subdomains.
Now, assuming the ILU(0) case, each nonzero in the matrix corresponds to an
element of S to assigned to a thread block. If we use the partitioning of the mesh
into subdomains just described, then thread block l is assigned the nonzeros aij
corresponding to edges (i, j) in the mesh within and emanating from subdomain
l. For one subdomain, these correspond to the nonzeros marked as red squares
in Fig. 1 (middle). For a partitioning of the mesh into subdomains of size b × b,
each thread block is assigned 5b2 edges (b2 nodes in the subdomain and 5 edges
per node). A typical thread block (corresponding to an interior subdomain) also
requires (b + 1)2 −1 rows of L and the same number of columns of U to be
read. Thus the reuse factor is freuse(l) :=
5b2
(b+1)2−1 in this case. The limit of the
maximum size of the reuse factor is 5, for large values of b. Note that b × b is a
blocking of the mesh, while ml × ml used above is a blocking of the matrix.
In general, if a regular mesh is partitioned into subdomains of size b × b and
an s-point stencil is used, then there are s·b2 matrix entries corresponding to the

Asynchronous Iterative Algorithm for Computing Incomplete
7
Fig. 1. A 6 × 6 mesh using a 5-point stencil partitioned into 9 subdomains of size
2 × 2 (left). The corresponding matrix (middle) orders rows and columns subdomain
by subdomain. The bold edges in the mesh correspond to the nonzeros in the matrix
marked with squares, and identify the elements assigned to one of the thread blocks.
The right side shows the partitioning of the matrix after applying reverse Cuthill-
McKee (RCM) and using 12 partitions.
subdomain, and the number of rows needed by the subdomain is b2 (highest order
term). The reuse factor therefore approaches s for large block sizes, showing that
a larger stencil gives rise to a larger reuse factor.
In the 3D case, if a regular mesh is partitioned into subdomains of size b×b×b
and an s-point stencil is used, then there are s · b3 matrix entries corresponding
to the subdomain, and the number of rows needed by the subdomain is b3 (high-
est order term). The reuse factor again approaches s for large block sizes. As
apparent from this analysis, 3D problems do not inherently have a larger reuse
factor than 2D problems, except for generally using larger stencils. We note that
in the sparse case, the maximum reuse factors are bounded independent of the
partitioning, whereas for the dense case, the reuse factors increase with the size
of the partitioning, may however be limited by the size of the shared memory.
3.4
Cache Reuse Between Thread Blocks
The working set of rows of L and columns of U needed by each thread block
can be large, especially for large values of the blocking parameter b. However,
if this working set can be shared between thread blocks by using the shared
L2 cache [13], the communication volume from global memory can be reduced,
compared to the case where each thread block uses its own scratchpad memory.
To this end, in this section, we explore the idea of sharing the cache between
thread blocks, i.e., one thread block using data brought into the L2 cache by
another thread block. This idea is non-traditional because GPU programs gen-
erally assume that thread blocks that are not communicating through global
memory are completely independent. This is because there is no guarantee of
the order in which thread blocks are executed.
By using our previous observation in Sect. 3.2 that thread blocks are assigned
indices in order of execution, we were able to verify through a simple performance

8
E. Chow et al.
test that thread blocks can indeed reuse data brought into the multiprocessor
cache by an earlier thread block. Thus we can assign work to thread blocks such
that reuse of cache between thread blocks is high. The right side of Fig. 1 shows
a simple example for the matrix corresponding to the 6×6 mesh on the left. The
matrix has been reordered to reduce its bandwidth using reverse Cuthill-McKee
(RCM) ordering. Horizontal lines show a partitioning of the elements of S, and
assume that the partitions are assigned to thread blocks such that the partitions
are executed from top to bottom. From the ﬁgure, it can be observed that each
partition requires diﬀerent rows of L; these rows are reused within a thread block
but not across thread blocks. We also observe that partitions numbered nearby
use a very similar set of columns of U. These columns of U may be shared across
thread blocks using cache. Experiments are reported in Sect. 4 to test the cache
behavior of this partitioning of S and this ordering of the thread blocks.
3.5
Parallelism Vs Convergence
The convergence rate of an asynchronous iteration depends on the amount of
parallelism used in the iteration, with more parallelism usually resulting in a
reduced convergence rate. We aim to control the amount of parallelism in order
to investigate the tradeoﬀbetween parallelism and convergence. NVIDIA pro-
vides no interface for direct manipulation of parallelism; thus we will control the
amount of parallelism indirectly.
To quantify the parallelism of a code running on GPUs, we may use the
concept of “occupancy.” While occupancy is deﬁned as the ratio between the
number of threads (grouped in thread blocks) that are scheduled in parallel
and the hardware-imposed thread limit (threads per streaming multiprocessor,
tpsm), certain algorithms may beneﬁt from using less than the maximum occu-
pancy [19]. A metric for quantifying the actual parallelism is the number of
executed instructions per cycle (IPC), which reﬂects not only the number of
active threads, but also stalls due to communication bottlenecks.
To indirectly control the number of thread blocks that are simultaneously
scheduled onto a multiprocessor, note that the available shared memory, reg-
isters, the number of individual threads and thread blocks are limited for each
streaming multiprocessor, which bounds the total number of thread blocks being
executed in parallel. Thus we can artiﬁcially reduce parallelism by explicitly allo-
cating scratchpad memory that we do not use in the kernel execution.
The multiprocessor’s local memory is partitioned between scratchpad mem-
ory and L1 cache. We have conﬁgured shared memory to maximize cache and
minimize scratchpad memory. The minimum amount of shared memory that can
be conﬁgured is 16,384 bytes per multiprocessor on the K40X architecture [13].
In compute capability 3.5, the value of tpsm is 2048. Therefore, requesting 1,024
bytes or less of scratchpad memory for each thread block consisting of 128 threads
results in 16 thread blocks running in parallel and 100 % multiprocessor occu-
pancy, while doubling the allocated scratchpad memory reduces the number of
active blocks to 8, and decreases the occupancy to 50 %. In Sect. 4.6, we use this
technique to control parallelism and observe its aﬀect on the convergence rate
and performance of the asynchronous iterations.

Asynchronous Iterative Algorithm for Computing Incomplete
9
3.6
Implementation Issues
Our CUDA kernel is designed to perform one sweep of the iterative ILU algo-
rithm (Algorithm 1) for computing the factorization A ≈LU. Additional sweeps
are performed by launching this kernel multiple times. The input to the kernel
are arrays corresponding to matrices A, L, and U. The arrays for L and U,
stored in CSR and CSC format, respectively, contain an initial guess on input
and the approximate factorizations on output.
The matrix A, stored in coordinate (COO) format, can be regarded as a
“task list.” The thread with global thread index tid is assigned to update the
variable corresponding to element tid in the COO array. Eﬀectively, thread
block 0 is assigned the ﬁrst blockDim elements in the COO array, thread block
1 is assigned the next blockDim elements, etc., (where blockDim is the number
of threads in a thread block). By changing the order in which the elements are
stored in the COO array, the order of the updates can be controlled. Note that
changing this order does not change the ordering of the rows and columns of the
matrix A.
Reads of the arrays corresponding to matrix A are coalesced. However, each
thread in general reads diﬀerent rows of L and columns of U that are not coa-
lesced, so that we rely on this data being read into cache and reused as much as
possible, as described above.
Signiﬁcant thread divergence will occur if the partial sum (lines 5 and 7 of
Algorithm 1) computed by a thread contains a diﬀerent number of addends than
for other threads in the same warp. Thus, to minimize the cost of divergence,
the partial sums for the updates handled by threads in the same warp should
be of similar length. This, however, conﬂicts with optimizing component assign-
ment for cache reuse, and we experimentally identiﬁed that data locality is more
important.
To end this section, we summarize the parameters that aﬀect the convergence
behavior and data locality of the kernel.
– Task list ordering: the ordering of the elements of S in the task list. Except
when speciﬁed otherwise, the default task list ordering is as follows: the ele-
ments of S are ordered such that if the elements were placed in a matrix, the
elements are ordered row by row, from left to right within a row. This is a
Gaussian elimination ordering. The order in which variables are updated can
be changed by changing the task list ordering.
– Thread block order: the order of execution of the thread blocks. Except when
speciﬁed otherwise, the thread blocks are ordered by increasing thread block
index, i.e., the ﬁrst thread block is assigned the ﬁrst chunk of the task list, etc.
We can also order the thread blocks in reverse order or in a random, arbitrary
order.
– Matrix ordering: the ordering of the rows and columns of the matrix. Chang-
ing the matrix ordering changes the problem. We use the symmetric RCM
ordering of the matrix, except for the ﬁnite diﬀerence discretizations of the
Laplace problem l2d and l3d, for which the natural ordering can be used.

10
E. Chow et al.
The RCM and natural orderings are good choices for computing accurate
incomplete factorization preconditioners [8].
4
Experimental Results
4.1
Experimental Setup
Our experimental setup is an NVIDIA Tesla K40c GPU (Rpeak 1,682 GFLOP/s)
with a two socket Intel Xeon E5-2670 (Sandy Bridge) host. The iterative incom-
plete factorization GPU kernels were implemented in CUDA version 6.0 [15] and
use a default thread block size of 128. The conjugate gradient linear solver and
the iterative incomplete factorization routine are taken from the MAGMA open-
source software library [10]. The sparse triangular solve routines are from the
NVIDIA cuSPARSE library [14]. The results below used double precision com-
putations for the iterative ILU code; timings were 20–40 percent slower than for
single precision computations.
The test matrices were selected from Tim Davis’s sparse matrix collection [6]
and include the problems used by NVIDIA for testing the ILU factorization code
in cuSPARSE [12]. We have reordered these test matrices using RCM reordering.
Additionally, we use test matrices arising from a ﬁnite diﬀerence discretization of
the Laplace operator in 2D and 3D with Dirichlet boundary conditions. For the
2D case, a 5-point stencil was used on a 1024×1024 mesh, and for the 3D case, a
27-point stencil was used on a 64 × 64 × 64 mesh. All test matrices (Table 1) are
symmetric positive deﬁnite and we used the symmetric, or incomplete Cholesky
(IC) version of the iterative ILU algorithm [4]. However, we still refer to the
algorithm as the iterative ILU algorithm for generality.
Table 1. Test matrices.
4.2
Convergence Metrics
For an incomplete Cholesky factorization, A ≈LLT , we measure how well the
factorization works as a preconditioner for the preconditioned conjugate gradi-
ent (PCG) method for solving linear systems. Thus we will report PCG solver
iteration counts. Linear systems were constructed using a right-hand side of all
ones. The iterations start with a zero initial guess, and the iterations are stopped
when the residual norm relative to the initial residual norm has been reduced
beyond 10−6.

Asynchronous Iterative Algorithm for Computing Incomplete
11
We also measure the nonlinear residual norm ∥(A −LLT )S∥F , where
the Frobenius norm is taken only over the elements in S. The expression
(A −LLT )S = 0 corresponds to the nonlinear equations being solved by the
ﬁxed-point sweeps of the asynchronous iterative algorithm. Finally, we can also
measure the ILU residual norm ∥A−LLT ∥F , which has been shown to correlate
well with the number of PCG iterations in the SPD case [8]. This quantity is
relatively expensive to compute since all elements of LLT are required, but it
can be useful for diagnostic purposes. Note that this quantity is not zero for an
incomplete factorization.
We show how the above quantities change for matrices L computed using
diﬀerent numbers of asynchronous ﬁxed-point sweeps, beginning with an initial
guess for L (corresponding to sweep 0). In this paper, the level 0 factorizations
are computed, although any sparsity pattern S can be used in the GPU kernel.
We use the notation “IC” to indicate results for an incomplete factorization
computed “exactly” using the conventional Gaussian elimination process.
4.3
Preconditioner Quality and Timings
We begin by demonstrating the convergence and performance of the iterative
ILU algorithm using our GPU implementation. Figure 2 shows the convergence
of the nonlinear residual norm (left) and ILU residual norm (right) as a function
of the number of nonlinear sweeps, for several test matrices. The results show
that the nonlinear residual norm converges very steadily. Also, the ILU residual
norm converges very rapidly, i.e., after a small number of steps, to the ILU
residual norm of the conventional ILU factorization (which is necessarily nonzero
because the factorization is approximate). This suggests that the factorization
computed by the new algorithm after a small number of steps may be comparable
to the factorization computed by the conventional algorithm.
Fig. 2. Relative nonlinear residual norm (left) and relative ILU residual norm (right)
for diﬀerent numbers of sweeps.
This is indeed the case, as shown on the left side in Table 2, which shows the
PCG solver iteration counts when the incomplete factors are used as a precondi-
tioner. The iteration counts indicate that only a few sweeps are usually suﬃcient

12
E. Chow et al.
to generate a preconditioner similar in quality to the preconditioner computed
conventionally. This is consistent with the ILU residual norm typically having
converged after the ﬁrst 5 iterations. It is thus not necessary to fully converge
the nonlinear residual.
The right side of Table 2 shows the timings for IC computed using the
NVIDIA cuSPARSE library, and for the 5 sweeps of the new algorithm. Sig-
niﬁcant speedups are seen over the cuSPARSE implementation, which uses level
scheduling to exploit parallelism.
Table 2. PCG solver iteration counts using preconditioners constructed with up to 5
sweeps, and timings for 5 sweeps. IC denotes the exact factorization computed using
the NVIDIA cuSPARSE library. Speedup shown is that of 5 sweeps relative to IC.
Solver iteration counts for given number of sweeps Timings [ms]
IC
0
1
2
3
4
5
IC
5 swps speedup
apa
958 1430 1363 1038
965
960
958
61.
8.8
6.9
eco 1705 2014 1765 1719 1708 1707 1706
107.
6.7
16.0
g3
997 1254
961
968
993
997
997
110. 12.1
9.1
off
330
428
556
373
396
357
332
219. 25.1
8.7
par
393
763
636
541
494
454
435
131.
6.1
21.6
thm 1398 1913 1613 1483 1341 1411 1403
454. 15.7
28.9
l2d
550
653
703
664
621
554
551
112.
7.4
15.2
l3d
35
43
37
35
35
35
35
94. 47.5
2.0
4.4
Thread Block Ordering and Convergence
We now show the eﬀect that thread block ordering has on the convergence of
the iterative ILU algorithm. First, the ordering of the tasks follows a Gaussian
elimination ordering, which is beneﬁcial for convergence. This task list is linearly
partitioned into thread blocks. We tested three thread block orderings: (1) a
forward ordering of the thread blocks, i.e., the ﬁrst thread block is associated
with the ﬁrst chunk of the task list, etc., (2) a backward ordering of the thread
blocks, and (3) a random ordering of the thread blocks. For the random ordering,
the results have been averaged over several trials.
Table 3 reports the time and the relative nonlinear residual norm after 5
sweeps for various problems. The forward ordering leads to the best convergence
rate and the backward ordering leads to the worst convergence rate. The random
ordering, corresponding to the convergence behavior of an unpredictable GPU
thread block scheduling gives results in between these two. Note that there is no
signiﬁcant diﬀerence between the timings using the three diﬀerent orderings.

Asynchronous Iterative Algorithm for Computing Incomplete
13
Table 3. Comparison of diﬀerent thread block orderings, showing time and relative
nonlinear residual norm after 5 sweeps of the iterative ILU algorithm. Forward ordering
gives the fastest convergence rate, and timings are not impacted.
4.5
Data Locality
In this section we show the eﬀect of the ordering of the task list, which aﬀects
data locality. In Sect. 3.3, orderings of the task list were proposed for problems
that are discretized on regular meshes. These are based on partitioning the graph
corresponding to the matrix (using b × b blockings of the 2D regular mesh was
the prototypical example). In Sect. 3.4, it was proposed to enhance cache reuse
between thread blocks by using a RCM reordering of the matrix combined with
the default task list ordering.
Results are shown along with metrics from NVIDIA’s NVPROF proﬁler in
Table 4 for the l3d problem. We used a randomly permuted task list as an
extreme case of disordered memory access and b × b × b blockings of the mesh.
The results show that large block sizes give better performance. RCM order-
ing also gives good performance. The random case is the slowest, due to low L2
hit rate and low global load throughput, resulting in low executed instructions
per cycle. For the blockings, the high L2 hit rate indicates that most data is
already present in local memory, and only small amounts must be reloaded from
DRAM. Luckily, we also ﬁnd that orderings that have better memory access
locality also lead to better convergence rates.
4.6
Parallelism Vs Convergence
Increased parallelism may result in slower convergence because variables being
updated at the same time use “older” rather than “refreshed” values in their
update formulas. As described in Sect. 3.6, the amount of parallelism can be
controlled explicitly by allocating diﬀerent amounts of scratchpad memory (that
will not be used) in the kernel code. Table 5 shows several kernel conﬁgurations,
for diﬀerent amounts of requested scratchpad memory, and the result of running
these conﬁgurations for the l3d problem. As can be observed, the theoretical

14
E. Chow et al.
Table 4. Comparison of strategies for enhancing data locality for the l3d problem.
The time and relative nonlinear residual norm are for 5 nonlinear sweeps. RCM and
orderings for large block sizes m × m × m give best results in terms of both timing and
convergence.
Task list ordering
Random 2
4
8
16
32
64
RCM
Time ×10−2 [s]
20.0
5.54
5.37
5.15
4.96
4.89
4.78
4.92
Rel. nonlin. res. norm ×10−3
2.66
3.08
4.35
4.86
4.63
1.87
1.35
1.58
Global load throughput [GB/s]
83.35
237.34 247.71 249.45 251.38 253.00 258.00 252.97
DRAM read throughput [GB/s] 119.00
16.70
15.93
16.43
17.13
16.62
22.10
20.02
L2 read throughput [GB/s]
83.35
237.34 247.71 249.45 251.38 253.00 258.00 252.97
L2 hit rate (L1 reads) [%]
19.02
96.68
96.97
96.91
96.79
96.93
95.88
96.02
Global store throughput [GB/s]
1.08
3.70
3.79
3.88
3.98
4.05
3.86
3.74
Ex. instructions per cycle (IPC)
0.21
0.75
0.78
0.81
0.85
0.85
0.87
0.86
and observed occupancy decreases with increasing requested memory. For our
algorithm, the IPC also generally decreases with increasing requested memory,
but the relation is not exact. For example, conﬁg 1 has the highest occupancy but
not the highest IPC. As expected, reducing the parallelism slightly improves the
convergence rate, but at the same time, the time scales almost linearly with the
inverse of parallelism quantiﬁed by IPC. Figure 3 graphs the relative nonlinear
residual norm as a function of time, for diﬀerent conﬁgurations. The results show
that the degradation in convergence due to additional parallelism is small, and
the penalty is more than compensated by the additional parallelism.
Table 5. Several conﬁgurations of the kernel code to control parallelism, and the
corresponding runtime and relative nonlinear residual norm for 5 sweeps. Results are
for the l3d problem.
conﬁg 1 conﬁg 2 conﬁg 3 conﬁg 4 conﬁg 5 conﬁg 6
Requested scratchpad mem. [B] 1024
2048
3072
4096
6144
9216
Active thread blocks
16
8
5
4
2
1
Active threads
2048
1024
640
512
256
128
Theoretical occupancy [%]
100.0
50.0
31.3
25.0
12.5
6.3
Reported occupancy [%]
98.7
49.6
31.0
24.8
12.4
6.2
Global load throughput [GB/s] 258.00
269.34
209.70
175.00
92.32
46.34
DRAM read throughput [GB/s] 22.10
23.23
18.06
15.07
7.98
4.01
L2 read throughput [GB/s]
258.00
269.34
209.70
175.00
92.32
46.34
Global store throughput [GB/s] 3.86
4.04
3.15
2.62
1.38
0.69
IPC
0.8572
0.91184 0.7120
0.5941
0.3134
0.1598
5 sweeps Time:
4.88e-02 4.57e-02 5.89e-02 7.01e-02 1.33e-01 2.62e-01
Rel. nonlin. res. norm: 1.35e-03 1.22e-03 1.16e-03 1.14e-03 9.84e-04 8.00e-04

Asynchronous Iterative Algorithm for Computing Incomplete
15
0
0.05
0.1
0.15
0.2
0.25
10
−3
10
−2
10
−1
10
0
Time [s]
Relative nonlinear residual norm
0.91 IPC
0.71 IPC
0.59 IPC
0.31 IPC
0.16 IPC
Fig. 3. Relationship between runtime and nonlinear residual norm for diﬀerent amounts
of parallelism, for the l3d problem.
5
Conclusions
A GPU implementation of an asynchronous iterative algorithm for computing
incomplete factorizations has been presented. Signiﬁcant speedups over the level
scheduling implementation of the cuSPARSE library were reported. Several tech-
niques were discussed for controlling the order of the asynchronous computations
to enhance convergence and data locality. These techniques may be applied in
general to other asynchronous iterative algorithms.
Acknowledgments. This material is based upon work supported by the U.S. Depart-
ment of Energy Oﬃce of Science, Oﬃce of Advanced Scientiﬁc Computing Research,
Applied Mathematics program under Award Numbers DE-SC-0012538 and DE-SC-
0010042. Support from NVIDIA is also acknowledged.
References
1. Anzt, H., Tomov, S., Dongarra, J., Heuveline, V.: A block-asynchronous relaxation
method for graphics processing units. J. Parallel Distrib. Comput. 73(12), 1613–
1626 (2013)
2. Benzi, M., Joubert, W., Mateescu, G.: Numerical experiments with parallel order-
ings for ILU preconditioners. Electron. Trans. Numer. Anal. 8, 88–114 (1999)
3. Bergman, K. et al.: ExaScale computing study: technology challenges in achieving
exascale systems peter kogge, editor & study lead (2008)
4. Chow, E., Patel, A.: Fine-grained parallel incomplete LU factorization. SIAM J.
Sci. Comput. 37, C169–C193 (2015)
5. Contassot-Vivier, S., Jost, T., Vialle, S.: Impact of asynchronism on gpu acceler-
ated parallel iterative computations. In: J´onasson, K. (ed.) PARA 2010, Part I.
LNCS, vol. 7133, pp. 43–53. Springer, Heidelberg (2012)
6. Davis, T.A.: The University of Florida Sparse Matrix Collection. NA DIGEST 92
(1994). http://www.netlib.org/na-digesthtml/

16
E. Chow et al.
7. Doi, S.: On parallelism and convergence of incomplete LU factorizations. Appl.
Numer. Math. 7(5), 417–436 (1991)
8. Duﬀ, I.S., Meurant, G.A.: The eﬀect of ordering on preconditioned conjugate
gradients. BIT 29(4), 635–657 (1989)
9. Frommer, A., Szyld, D.B.: On asynchronous iterations. J. Comput. Appl. Math.
123, 201–216 (2000)
10. Innovative Computing Lab: Software distribution of MAGMA, July 2015. http://
icl.cs.utk.edu/magma/
11. Lukarski, D.: Parallel Sparse Linear Algebra for Multi-core and Many-core Plat-
forms - Parallel Solvers and Preconditioners. Ph.D. thesis, Karlsruhe Institute of
Technology (KIT), Germany (2012)
12. Naumov, M.: Parallel incomplete-LU and Cholesky factorization in the precondi-
tioned iterative methods on the GPU. Technical report. NVR-2012-003, NVIDIA
(2012)
13. NVIDIA Corporation: NVIDIA’s Next Generation CUDA Compute Architecture:
Kepler GK110. Whitepaper (2012)
14. NVIDIA Corporation: CUSPARSE LIBRARY, July 2013
15. NVIDIA Corporation: NVIDIA CUDA TOOLKIT V6.0, July 2013
16. Poole, E.L., Ortega, J.M.: Multicolor ICCG methods for vector computers. SIAM
J. Numer. Anal. 24, 1394–1417 (1987)
17. Saad, Y.: Iterative Methods for Sparse Linear Systems. Society for Industrial and
Applied Mathematics, Philadelphia (2003)
18. Venkatasubramanian, S., Vuduc, R.W.: Tuned and wildly asynchronous stencil
kernels for hybrid CPU/GPU systems. In: Proceedings of the 23rd International
Conference on Supercomputing, ICS 2009, pp. 244–255. ACM, New York (2009)
19. Volkov, V.: Better performance at lower occupancy. In: GPU Technology Confer-
ence (2010)

Matrix Multiplication on High-Density
Multi-GPU Architectures: Theoretical
and Experimental Investigations
Peng Zhang1(&) and Yuxiang Gao2
1 Biomedical Engineering Department,
Stony Brook University, Stony Brook, NY 11794, USA
peng.zhang@stonybrook.edu
2 Cluster Solution Department, Cray Inc., San Jose, CA 95112, USA
Abstract. Matrix multiplication (MM) is one of the core problems in the high
performance computing domain and its efﬁciency impacts performances of
almost all matrix problems. The high-density multi-GPU architecture escalates
the complexities of such classical problem, though it greatly exceeds the
capacities of previous homogeneous multicore architectures. In order to fully
exploit the potential of such multi-accelerator architectures for multiplying
matrices, we systematically evaluate the performances of two prevailing tile-
based MM algorithms, standard and Strassen. We use a high-density multi-GPU
server, CS-Storm which can support up to eight NVIDIA GPU cards and we test
three generations of GPU cards which are K20Xm, K40m and K80. Our results
show that (1) Strassen is often faster than standard method on multicore
architecture but it is not beneﬁcial for small enough matrices. (2) Strassen is
more efﬁcient than standard algorithm on low-density GPU solutions but it
quickly loses its superior on high-density GPU solutions. This is a result of more
additions needed in Strassen than in standard algorithm. Experimental results
indicate that: though Strassen needs less arithmetic operations than standard
algorithm, the heterogeneity of computing resources is a key factor of deter-
mining the best-practice algorithm.
Keywords: Matrix multiplication  Performance evaluation  Heterogeneous
architectures  High-density multi-GPU architectures
1
Introduction
Since ENIAC was announced in 1946, researchers never stop to seek a faster approach
for multiplying matrices. Not only one of the kernels in numerical linear algebra, the
problem of matrix multiplication (MM) is also a bottleneck for almost all matrix
problems such as least square problem and eigenvalue problem [1–5]. The key problem
has widely been studied in computing theory and in practical implementation. Math-
ematicians have been looking for the possible lowest bound for multiplying matrices.
The standard method for multiplying two n × n matrices is O(n3). In 1969, Strassen
reduced the computing complexity to O(n2.807) [6]. In 1987, a big breakthrough of this
problem is the Coppersmith-Winograd algorithm which can do MM in O(n2.376)
© Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 17–30, 2015.
DOI: 10.1007/978-3-319-20119-1_2

operations [7]. More new algorithms are proposed for beating the records and
approaching the true lowest bound [8].
Practical implementation is essential to exploit the proposed algorithms on the
novel parallel computing facilities [5, 9–20]. Different from theoretical studies, the
complex characteristics of computing facilities need to be taken into the design of
parallel programs. In distributed computing, communication needs to be minimized
[12, 19, 21]. Particularly the task mapping problem for the Strassen algorithm is
addressed for balancing multiplications [9, 18]. In the latest heterogeneous architec-
tures, MM needs to be optimized on special-purposed processors and accelerators, such
as CELL processor [5] and graphics processing units (GPUs) [11]. Besides, many high-
performance implementations of MM are developed such as in GotoBLAS [22, 23],
ATLAS [10], LAPACK [24] and CUBLAS [23]. To accommodate the ever-changing
computing architectures, new algorithms have been designed and developed with the
birth of new technologies. Recently, high-density multi-GPU technologies are available
to the community of supercomputing. For example, a 2U server node can be conﬁ-
gurated with 8 NVIDIA GPU cards in CS-Storm [25, 26], featuring up a high-density
space-efﬁcient design for integrating multiple GPUs. This design has signiﬁcantly
escalated computing complexities, though it greatly improves computing performance.
There is a need to investigate MM algorithms on this novel architecture.
This motivated the work to investigate the performance of the standard and Strassen
tile-based algorithms on the high-density multi-GPU architecture. Our contributions in
the work are: (i) to systematically compare the performances of the standard and
Strassen algorithms on the high-density multi-GPU platforms; (ii) to ﬁnd out the
optimal algorithms through extensive experiments for a wide range of problem sizes
under different system conﬁgurations; (iii) to present the performance characteristics to
the researchers and the engineers for better algorithmic and engineering designs.
The paper is organized as follows: standard and Strassen algorithms are reviewed in
Sect. 2. Theoretical evaluation is presented in terms of ﬂoating-point operations and
execution time in Sect. 3. High-density multi-GPU architecture is described in terms of
hardware speciﬁcations and software stacks in Sect. 4. Experimental results are pre-
sented and analyzed in Sect. 5. Conclusion is drawn in Sect. 7.
2
Matrix Multiplication Algorithms
We consider the tiled matrix multiplication (MM) algorithms on shared-memory
multicore and multi-accelerator systems. The ﬁrst method is the standard tiled MM
algorithm and it is also referred to as the Naïve method thereafter. Naïve method
partitions each input matrix into a block matrix whose tiles are submatrices of identical
sizes. Based on the given partition, the computing products of submatrices are per-
formed concurrently. The other method is the Strassen algorithm, which is often faster
than Naïve on the multicore systems for large size matrices. Figure 1 gives the
examples to show the data partition and computing ﬂows for both methods. In the
examples, the input matrices A and B are partitioned into 2 × 2 block matrices. Each
tile (namely, submatrix) is referred to as atomic data module. Intermediate data
modules are needed to buffer intermediate results. Finally, the resultant matrix C is
18
P. Zhang and Y. Gao

computed and stored in the same manner. This case of multiplying two 2 × 2 matrices
shows that Strassen saves one multiplication at the expense of 14 more additions. The
cost of multiplying matrices is often highlighted; however, the cost of adding matrices
is somewhat ignored in the analysis of most algorithms. This could result in the
problems in practical implementations. For example, clearly the beneﬁt of Strassen
would be marginal for small enough matrices. It is observed that in practice on the
multicore systems, there is a performance crosspoint between Strassen and Naïve
[10, 27]. However, in this work, we’d ask one question: is there a performance
crosspoint for large matrices on heterogeneous architectures?
For convenience of description, we assume that: input matrix is a square matrix of
size N × N; the tiled partition is (2p) × (2p); and, each tile is a square submatrix of size
n × n. Thus, N = 2p × n. N, p and n are positive integers. p is called as partition factor.
In the example (Fig. 1), the titled partition is 2 × 2 and p = 1.
3
Theoretical Evaluation
We conduct the theoretical evaluation for two methods in terms of ﬂoating-point
operations (FLOP) and execution time.
Floating-point operations (FLOP): Let Fmm (p, n) and Fst (p, n) be the number of
ﬂoating-point operations (FLOP) that is required by Naïve and Strassen, respectively.
The formulas are written as:
Fmmðp; nÞ ¼ 8p3  fmðnÞ þ p2ð8p  4Þ  faðnÞ
ð1Þ
Fstðp; nÞ ¼ 7p3  fmðnÞ þ p2ð22p  4Þ  faðnÞ
ð2Þ
Here fmðnÞ ¼ n2ð2n  1Þ and fa n
ð Þ ¼ n2 are the FLOP for multiplying and adding
n × n matrices.
Fig. 1. Data partitions and computing procedures for Naïve and Strassen algorithms
Matrix Multiplication on High-Density Multi-GPU Architectures
19

Let the ratio of Fst p; n
ð
Þ over Fmm p; n
ð
Þ be c p; n
ð
Þ written as:
c p; n
ð
Þ ¼ Fst p; n
ð
Þ
Fmm p; n
ð
Þ ¼ 7p 2n  1
ð
Þ þ 22p  4
8p 2n  1
ð
Þ þ 8p  4  0:875  15
n
ð3Þ
The partition factor p is often small in tiled algorithms. Then, we see: c p; n
ð
Þ !
0:875 as n ! 1. Figure 2 illustrates the evolution of ratio c p; n
ð
Þ under certain con-
ditions. This reafﬁrmed the asymptotic complexities for Naïve and Strassen and it also
indicated the performances on multicore architectures [6].
Execution Time: The performances of these two methods are then evaluated in terms of
execution time. Let Tm n
ð Þ and Ta n
ð Þ be the time for multiplying and adding two
submatrices of size n × n, respectively. Let Tmn p; n
ð
Þ and Tst p; n
ð
Þ are the total time
needed for Naïve and Strassen and written as:
Tmm p; n
ð
Þ ¼ 8p3  Tm n
ð Þ þ p2 8p  4
ð
Þ  Ta n
ð Þ
ð4Þ
Tst p; n
ð
Þ ¼ 7p3  Tm n
ð Þ þ p2 22p  4
ð
Þ  Ta n
ð Þ
ð5Þ
On homogeneous multicore architectures, multiplication is often more time-con-
suming than addition for large enough matrices so we assume: Tm n
ð Þ  Ta n
ð Þ for large
n. Let np be the number of processor cores that process concurrently. Thus, we have the
facts on homogenous multicore systems that:
• Strassen is more efﬁcient than Naïve and its improvement is *12.5 %.
• Parallel efﬁciency is nearly perfect when the number 7p3 is a multiple of np. Nat-
urally, it is because 7p3 multiplication instances could be distributed evenly on the
np processor cores, thus leading to perfect balanced multiplying operations [9].
Fig. 2. Ratio of FLOP (Strassen) over FLOP (Naïve) with varied partition factor p and
submatrix size n
20
P. Zhang and Y. Gao

However, the situation may be different on the heterogeneous multi-GPU architecture.
The difference is due to the disparity of GPU and CPU performances (Table 1). We
calculate the ratio of Tst p; n
ð
Þ over Tmm p; n
ð
Þ as:
b p; n
ð
Þ ¼ Tst p; n
ð
Þ
Tmm p; n
ð
Þ ¼ p 7  j n
ð Þ þ 22
ð
Þ  4
p 8  j n
ð Þ þ 8
ð
Þ  4  7  j n
ð Þ þ 22
8  j n
ð Þ þ 8
ð6Þ
Here j n
ð Þ ¼ Tm n
ð Þ=Ta n
ð Þ is the ratio of multiplication time over addition time for
submatrices of size n × n. Smaller b p; n
ð
Þ means that Strassen is more efﬁcient than
Naïve. Writing j n
ð Þ as a function of b p; n
ð
Þ yields:
j n
ð Þ   8  b p; n
ð
Þ  22
8  b p; n
ð
Þ  7
ð7Þ
This helps ﬁnd out the asymptotic trend:
limbðp;nÞ!1 j n
ð Þ ¼ 14
ð8Þ
Secondly, we have:
@b p; n
ð
Þ
@j n
ð Þ

15
8 j n
ð Þ þ 1
ð
Þ2 \0
ð9Þ
From Eqs. (8) and (9), we ﬁnd out that:
• If j n
ð Þ [ 14, Strassen could be faster than Naïve. On the other hand, if j n
ð Þ\14,
Naïve could outperform Strassen, though it required more FLOP.
• Thus, when multiplication is much faster than addition, Strassen is greatly beneﬁ-
cial, compared to Naïve. However, when multiplication becomes as fast as addition,
Naïve may in turn surpass Strassen.
Figure 3 shows the changes of b p; n
ð
Þ under varied p and j n
ð Þ. Therefore, this made a
possible: when/if the multiplication could be as fast as addition, Naïve could outper-
form Strassen. This assumption is hardly achievable in today’s processors but it maybe
holds on the hybrid multi-GPU architectures.
Table 1. Performance comparison between GPUs and CPUs
Peak ﬂoating point performances (TFlops)
Double-precision
Single-precision
Tesla K20
1.17
3.52
Tesla K40
1.43
4.29
Tesla K80
1.87
5.60
Xeon E5-2670
0.166
0.333
Matrix Multiplication on High-Density Multi-GPU Architectures
21

4
High-Density Multi-GPU Architecture
4.1
Hardware
CS-Storm [25] is used for performing all experiments and it is a 2U sever that can be
equipped with up to 8 NVIDIA Tesla GPU cards and 2 Intel Xeon processors. In this
work, we test three multi-GPU systems. Table 2 lists the hardware speciﬁcations. On
the board, four PCIe switches are enclosed; each hooking up to 2 GPUs with the host.
4.2
Software
System software includes RHEL 6.5 and NVIDIA driver 340.32. For best perfor-
mances of subprograms on CPUs and GPUs, we select two BLAS (basic linear algebra
subprograms) libraries: Intel Math Kernel Library (MKL v11.2) for CPUs and CU-
BLAS (CUDA 6.5) for GPUs. Complier package is Intel Parallel Studio 2015.
A data-oriented mapping paradigm (DMP) is extended to distribute tasks
among CPUs and GPUs [28]. Following the work [28], we describe the scheduler
Fig. 3. Ratio of Tst p; n
ð
Þ (Strassen) over Tmm p; n
ð
Þ (naïve) with varied partition factor p and j n
ð Þ
Table 2. Hardware speciﬁcation for multi-K20/K40/K80 server nodes
Systems GPU hardware
CPU hardware
Model
# of
GPUS
Total
CUDA
Cores
GDDR5 /
GPU (GB)
Model
# of
CPUS
Total
CPU
Cores
Total Host
Memory (GB)
K20
K20Xm
(1x Kepler
GK110)
4
10,752
5.76
Intel Xeon
E5-2670
v2
2
20
165
K40
K40m
(1x Kepler
GK110B)
8
23,040
11.52
Intel Xeon
E5-2670
v2
2
20
165
K80
K80
(2x Kepler
GK210)
16
39,936
11.52
Intel Xeon
E5-2680
v3
2
24
165
22
P. Zhang and Y. Gao

work. In the tiled algorithms, the tiles are treated as data modules. All the tiles asso-
ciated with input matrices are treated as initial data modules. All the tiles that belong to
the resultant matrix are referred to as resultant data modules. Intermediate tiles are
treated as intermediate data modules. All of data modules are given by an identiﬁer. In
the method, when a function is deﬁned as ds = f (d1, d2), we say ds depends on d1 and
d2. Here, ds, d1 and d2 are the identiﬁers of data modules and the function f is either the
multiplication or the addition. In this manner, the data dependency is deﬁned.
A function in the method is referred to as a task in the computing. In the computing,
initial data modules are ﬁrst loaded and ready to use. Then a dedicated scheduler
checks the availability of new tasks until all tasks are done. A task is available as long
as the input data modules it requires are ready to use. The scheduler sends a new task to
the next available CPU core or GPU card, as long as the task is available. We further
add an arbitrator layer in the scheduler, which allows the scheduler to distribute
speciﬁed tasks to preferred platforms. For example, the scheduler could distribute the
addition tasks only on CPUs and the multiplication tasks only on GPUs.
5
Experimental Evaluation
5.1
Performance Metrics
Wallclock time (in seconds) is used for timing. TðAÞ=TðBÞ denotes the wallclock time
for method A and B. SðA; BÞ ¼ TðBÞ=TðAÞ is the speedup for A over B. Performance
improvement for A over B is deﬁned as PðA; BÞ and calculated as:
P A; B
ð
Þ ¼ T B
ð Þ  TðAÞ
TðBÞ
¼ 1  S A; B
ð
Þ1
ð10Þ
As this equation suggests, a positive PðA; BÞ implies A is faster than B; otherwise,
B is faster than A. In addition, for the clarity of showing schedulability, parallel
activities trace (PAT) is proposed to illustrate the activities of concurrent computations.
PAT is the 2D graphic scheme, in which the horizontal axis shows wallclock time and
the vertical axis implies the device type (CPU/GPU) and thread identiﬁers (IDs).
Different colors refer to different types of tasks (functions). Naturally, two ends of a
color bar indicate starting and ending time points of data processing of a particular task
so the length means the amount of time the task takes. PAT graphically helps illustrate
the complexities of parallel activities of parallel programs. 64-bit and 32-bit precision
ﬂoating-point formats are tested. Input matrices are partitioned as 4 × 4 tiles.
5.2
Homogeneous Multicore Architectures
Parallel efﬁciency is nearly perfect when the number of concurrent processes is a
multiple of 7 thanks to the nature of Strassen [9, 29]. Thus, we benchmark both
methods using 7 and 14 processor cores for a range of varied problem sizes. Figures 6
and 7 present the performances for Strassen and Naïve using the 64-bit (double) and
32-bit (single) precisions, respectively. Performance improvements of Strassen over
Matrix Multiplication on High-Density Multi-GPU Architectures
23

Naïve are accordingly calculated. From these results, we can ﬁnd out: (i) Strassen lost
its superiority for small enough sizes. With the increase of problem sizes, Strassen
gains more beneﬁts and it becomes consistently more efﬁcient than Naïve. On seven
cores, Strassen improved the performance by a factor of 13 * 14 %, compared with
Naïve. On 14 cores, performance improvements of Strassen over Naïve increase to
16 * 17 %. This reafﬁrms: Strassen is more beneﬁcial for large enough sizes. The
precision of ﬂoating-point numbers has a dominating impact on absolute performances
but it has a relatively small impact on performance improvements of Strassen
over Naïve.
5.3
Heterogeneous Multi-GPU Architectures
5.3.1
Heterogeneity of Performances
When migrating from homogeneous multicore to heterogeneous multi-GPU architec-
tures, we need to exam the performances of two key operations, multiplication and
addition, on processors and accelerators. Figure 4 presents the performances for varied
problem sizes. The results show that: (i) GPU multiplication (CUBLAS_DGEMM/
CUBLAS_SGEMM) is dominantly faster than CPU multiplication (MKL_DGEMM/
MKL_SGEMM). (ii) CPU addition (MKL_DOMATADD/MKL_SOMATADD) is
even faster than GPU addition (CUBLAS_DAXPY/CUBLAS_SAXPY), due to data
transfer overhead between the host and the GPU devices. Thus, CPU is still the optimal
platform to perform the matrix addition but GPU becomes the best choice for the
matrix multiplication for large enough size. (iii) Lastly, CPU multiplication is typically
2 * 3 orders of magnitude slower than CPU addition; however, GPU multiplication is
merely one order of magnitude slower than CPU addition. Figure 5 shows the ratios.
This trend made a possible that Naïve may outperform Strassen on multi-GPU archi-
tectures, on which multiplication is not that slower than addition.
Fig. 4. Multiplication and addition performances on CPU/GPU
24
P. Zhang and Y. Gao

5.3.2
Strassen vs. Naïve on 8x K40m
Figures 6 and 7 present the absolute performances of two methods in (a) and (b), and
then the performance improvements for Strassen over Naïve in (c), for 64-bit (double)
and 32-bit (single) precision, respectively. From the results, we can ﬁnd out that: (i) The
GPU-solutions are often superior to the CPU-only solutions for large enough matrices.
(ii) High-density GPU-solutions are always better than low-density GPU-solutions. (iii)
In the single-GPU solution (i.e., 1-GPU), Strassen still retains its superiority over Naïve.
This could be because single GPU per node cannot give enough competitive perfor-
mance for multiplying matrices, compared with the performance for adding matrices
given by the many processor cores. Under this condition, Strassen could be still more
efﬁcient than Naïve since it needs fewer multiplications than Naïve. (iv) However, with
the increase of GPU cards in one system, the efﬁciency of Naïve is greatly improved and
in turn, Naïve outperforms Strassen. For examples, in the 4-GPU and 8-GPU solutions,
Naïve appears much more efﬁcient than Strassen. The results from the high-density
multi-GPU tests veriﬁed the assumption that: Naïve may surpass Strassen under the
condition that the time of multiplying two matrices is approx. one order of magnitude
slower than the time of adding matrices of same sizes. Currently, this condition is hardly
satisﬁed on low-density multi-GPU conﬁgurations since the processor cores are rela-
tively more powerful than single GPU card. However, with the capability of densely
integrating accelerators, the performance gap between multiplying and adding matrices is
further reduced, thus directly affecting the best practice for MM on these novel archi-
tectures. (v) Naïve is more efﬁcient than Strassen on 4-/8-GPU solutions regardless of
ﬂoating-point precision (32-bit/64-bit). Figure 8 shows the parallel activities traces
(PAT) of the case for multiplying two matrices of size 48,000, partitioned as 4 × 4 tiles,
on the 8-K40 m system. PAT illustrates that (i) multiplication tasks are evenly distributed
on GPU cards; and (ii) Strassen needs signiﬁcantly more addition tasks than Naïve.
Particularly, Fig. 8 shows that, at the beginning of program, all of cores are busy with
these substantial additions for Strassen. Similarly, at the ﬁnishing of program, Strassen
needs more addition tasks than Naïve (Fig. 1). In the middle of program, multiple GPU
cards could be more efﬁcient for multiplication tasks, compared with traditional pro-
cessor cores. In this, the results show that Naïve becomes more efﬁcient than Strassen.
Fig. 5. Ratios of CPU-/GPU multiplication time over CPU addition time
Matrix Multiplication on High-Density Multi-GPU Architectures
25

Fig. 6. Experimental results for Naïve and Strassen on 8-K40 m (double precision): (a) and (b)
present the wallclock time in seconds for Naïve and Strassen, respectively. (c) shows the
performance improvements for Strassen over Naïve. In the legend, 7 CORE (14 CORE) means a
CPU-only solution using 7 (14) processor cores. The rest of tests are GPU solutions where 16
CPU cores used. Figures 7, 9, 10, 11 and 12 use the same legends.
Fig. 7. Experimental investigation for Naïve and Strassen on 8-K40 m (single precision).
Fig. 8. Parallel activities traces for Naïve and Strassen for problem size 48,000 on 8x K40 m
26
P. Zhang and Y. Gao

5.3.3
Strassen vs. Naïve on 4x K20Xm and 16x K80
In the literature of NVIDA GPU cards, K20Xm and K80 are the predecessor and
successor of K40m. Figures 9 and 10 show the absolute performances and performance
comparisons between Strassen and Naïve on 4-K20Xm, using double and single pre-
cisions, respectively. Similarly, Figs. 11 and 12 show the results on 16-K80 system.
Furthermore, Fig. 13 presents the best performances of three multi-GPU platforms,
which undoubtedly shows that 16-K80 is the optimal. The results on 4-K20Xm system
reafﬁrm previous discoveries: on 1-GPU conﬁguration, Strassen is the optimal algo-
rithm while on 2-/4-GPU conﬁgurations, Naïve appears more efﬁcient than Strassen.
The same results appear in the 16-K80 tests.
Fig. 9. Experimental investigation for Naïve and Strassen on 4-K20Xm (double precision)
Fig. 10. Experimental investigation for Naïve and Strassen on 4-K20Xm (single precision).
Fig. 11. Experimental investigation for Naïve and Strassen on 16-K80 (double precision)
Matrix Multiplication on High-Density Multi-GPU Architectures
27

6
Discussions
Through extensive experiments, we have demonstrated the big performance dis-
parities for MM on different architectures. The greatest advantage of Strassen is that
Strassen needs fewer multiplication operations than the naïve method. Thus on the
classical multicore architectures and single-GPU architecture, Strassen is often more
efﬁcient than the naïve method for large enough problem size. However, on the novel
high-density multi-GPU architectures, the efﬁciency of multiplying two matrices is
signiﬁcantly improved but the efﬁciency of adding two matrices is still constrained
by the overhead of data transfer between the host and multiple accelerator devices.
This made processors as the optimal platform for additions and accelerators as the
optimal platform for multiplications. In this scenario, the naïve method could out-
perform the Strassen method. This indicates that the performance difference between
the multiplication and addition operations would ﬁnally determine which method
would be the best-practice solution. In this regard, the high-density multi-GPU
architecture is widely different from the homogenous multi-core systems and low-
density systems.
Fig. 12. Experimental investigation for Naïve and Strassen on 16-K80 (single precision)
Fig. 13. Best performances of MM on multi-GPU platforms (double/single precisions: left and
right plots)
28
P. Zhang and Y. Gao

7
Conclusion
In this work, we test the standard (Naïve) and Strassen tile-based MM methods on
novel high-density multi-GPU systems. Three generations of NVIDIA GPU cards,
K20Xm, K40m and K80 are benchmarked on the systems. Both 64-bit double and 32-
bit single precisions are tested. The results show that multi-GPU solutions can sig-
niﬁcantly improve the performances, in comparison with CPU-only solutions. The
Strassen method is often beneﬁcial on the multicore and the low-density GPU solu-
tions; however it is beaten by the Naïve method on the high-density multi-GPU
solutions. The reason is that the Strassen needs more additions than the Naïve method
but GPU is not efﬁcient enough for these additions thanks to the host-device overhead.
The results in the work give a handy guide for the practitioners to use the methods for
multiplying matrices on heterogeneous systems.
With the birth of new technologies, it is undoubted that the intra-chip and the inter-
chip communication capability could and should be improved. By then, performance
comparisons between different MM methods should be re-evaluated to ﬁnd out the
best-practice algorithm on novel architectures.
References
1. Robinson, S.: Toward an optimal algorithm for matrix multiplication. SIAM News 38, 1–3
(2005)
2. Lancaster, P., Tismenetsky, M.: The Theory of Matrices: with Applications. Academic
Press, Waltham (1985)
3. Dorn, F.: Dynamic programming and fast matrix multiplication. In: Azar, Y., Erlebach, T.
(eds.) ESA 2006. LNCS, vol. 4168, pp. 280–291. Springer, Heidelberg (2006)
4. Gunnels, J.A., Henry, G.M., Van De Geijn, R.A.: A Family of high-performance matrix
multiplication algorithms. In: Alexandrov, V.N., Dongarra, J.J., Juliano, B.A., Renner, R.S.,
Kenneth Tan, C.J. (eds.) ICCS 2001. LNCS, vol. 2073, pp. 51–60. Springer, Heidelberg
(2001)
5. Kurzak, J., Alvaro, W., Dongarra, J.: Optimizing matrix multiplication for a short-vector
SIMD architecture–CELL processor. Parallel Comput. 35, 138–150 (2009)
6. Strassen, V.: Gaussian elimination is not optimal. Numer. Math. 13, 354–356 (1969)
7. Coppersmith, D., Winograd, S.: Matrix multiplication via arithmetic progressions. In:
Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, pp. 1–6
(2004)
8. Williams, V.V.: Multiplying matrices faster than Coppersmith-Winograd. In: Proceedings of
the Forty-Fourth Annual ACM Symposium on Theory of Computing, pp. 887–898 (2012)
9. Chou, C.C., Deng, Y.F., Li, G., Wang, Y.: Parallelizing strassens method for matrix
multiplication on distributed-memory mimd architectures. Comput. Math. Appl. 30, 49–69
(1995)
10. D’Alberto, P., Nicolau, A.: Using recursion to boost ATLAS’s performance. In: Labarta, J.,
Joe, K., Sato, T. (eds.) ISHPC 2006 and ALPS 2006. LNCS, vol. 4759, pp. 142–151.
Springer, Heidelberg (2008)
Matrix Multiplication on High-Density Multi-GPU Architectures
29

11. Ohshima, S., Kise, K., Katagiri, T., Yuba, T.: Parallel processing of matrix multiplication in
a CPU and GPU heterogeneous environment. In: Daydé, M., Palma, J.M.L.M., Coutinho, A.
L.G.A., Pacitti, E., Lopes, J.C. (eds.) VECPAR 2006. LNCS, vol. 4395, pp. 305–318.
Springer, Heidelberg (2007)
12. Irony, D., Toledo, S., Tiskin, A.: Communication lower bounds for distributed-memory
matrix multiplication. J. Parallel Distrib. Comput. 64, 1017–1026 (2004)
13. Fatahalian, K., Sugerman, J., Hanrahan, P.: Understanding the efﬁciency of GPU algorithms
for matrix-matrix multiplication. In: Proceedings of the ACM SIGGRAPH/EUROGRAPHICS
Conference on Graphics Hardware, pp. 133–137 (2004)
14. Beaumont, O., Boudet, V., Rastello, F., Robert, Y.: Matrix multiplication on heterogeneous
platforms. IEEE Trans. Parallel Distrib. Syst. 12, 1033–1051 (2001)
15. Thottethodi, M., Chatterjee, S., Lebeck, A.R.: Tuning Strassen’s matrix multiplication for
memory efﬁciency. In: Proceedings of the 1998 ACM/IEEE Conference on Supercomputing
(CDROM), pp. 1–14 (1998)
16. Luo, Q., Drake, J.B.: A scalable parallel Strassen’s matrix multiplication algorithm for
distributed-memory computers. In: Proceedings of the 1995 ACM Symposium on Applied
Computing, pp. 221–226 (1995)
17. Choi, J., Walker, D.W., Dongarra, J.J.: PUMMA: parallel universal matrix multiplication
algorithms on distributed memory concurrent computers. Concurrency: Pract. Experience 6,
543–570 (1994)
18. Zhang, P., Gao, Y., Fierson, J., Deng, Y.: Eigenanalysis-based task mapping on parallel
computers with cellular networks. Math. Comput. 83, 1727–1756 (2014)
19. Zhang, P., Powell, R., Deng, Y.: Interlacing bypass rings to torus networks for more efﬁcient
networks. IEEE Trans. Parallel Distrib. Syst. 22, 287–295 (2011)
20. Zhang, P., Deng, Y., Feng, R., Luo, X., Wu, J.: Evaluation of various networks conﬁgurated
by adding bypass or torus links. IEEE Trans. Parallel Distrib. Syst. 26, 984–996 (2015)
21. Ballard, G., Demmel, J., Holtz, O., Lipshitz, B., Schwartz, O.: Communication-optimal
parallel algorithm for strassen’s matrix multiplication. In: Proceedings of the 24th ACM
Symposium on Parallelism in Algorithms and Architectures, pp. 193–204 (2012)
22. Goto, K., Geijn, R.A.: Anatomy of high-performance matrix multiplication. ACM Trans.
Math. Softw. (TOMS) 34, 12 (2008)
23. Barrachina, S., Castillo, M., Igual, F.D., Mayo, R., Quintana-Orti, E.S.: Evaluation and
tuning of the level 3 CUBLAS for graphics processors. In: IEEE International Symposium
on Parallel and Distributed Processing, IPDPS 2008, pp. 1–8 (2008)
24. Demmel, J.: LAPACK: a portable linear algebra library for supercomputers. In: IEEE
Control Systems Society Workshop on Computer-Aided Control System Design, pp. 1–7
(1989)
25. CS-Storm speciﬁcation. (2014). http://www.cray.com/sites/default/ﬁles/CrayCS-Storm.pdf
26. Fang, Y.-C., Gao, Y., Stap, C.: Future enterprise computing looking into 2020. In: Park, J.J.,
Zomaya, A., Jeong, H.-Y., Obaidat, M. (eds.) Frontier and Innovation in Future Computing
and Communications. LNEE, vol. 301, pp. 127–134. Springer, Heidelberg (2014)
27. Skiena, S.S.: The Algorithm Design Manual, vol. 1. Springer, Heidelberg (1998)
28. Zhang, P., Ling, L., Deng, Y.: A data-driven paradigm for mapping problems. Parallel
Comput. (2015). doi: 10.1016/j.parco.2015.05.002 (In press)
29. Huss-Lederman, S., Jacobson, E.M., Johnson, J.R., Tsao, A., Turnbull, T.: Implementation
of Strassen’s algorithm for matrix multiplication. In: Proceedings of the 1996 ACM/IEEE
Conference on Supercomputing, pp. 32–32 (1996)
30
P. Zhang and Y. Gao

A Framework for Batched and GPU-Resident
Factorization Algorithms Applied to Block
Householder Transformations
Azzam Haidar1(B), Tingxing Tim Dong1, Stanimire Tomov1, Piotr Luszczek1,
and Jack Dongarra1,2,3
1 University of Tennessee, Knoxville, USA
haidar@icl.utk.edu
2 Oak Ridge National Laboratory, Oak Ridge, USA
3 University of Manchester, Manchester, UK
Abstract. As modern hardware keeps evolving, an increasingly eﬀective
approach to developing energy eﬃcient and high-performance solvers is
to design them to work on many small size and independent problems.
Many applications already need this functionality, especially for GPUs,
which are currently known to be about four to ﬁve times more energy
eﬃcient than multicore CPUs. We describe the development of one-sided
factorizations that work for a set of small dense matrices in parallel, and
we illustrate our techniques on the QR factorization based on House-
holder transformations. We refer to this mode of operation as a batched
factorization. Our approach is based on representing the algorithms as a
sequence of batched BLAS routines for GPU-only execution. This is in
contrast to the hybrid CPU-GPU algorithms that rely heavily on using
the multicore CPU for speciﬁc parts of the workload. But for a system to
beneﬁt fully from the GPU’s signiﬁcantly higher energy eﬃciency, avoid-
ing the use of the multicore CPU must be a primary design goal, so the
system can rely more heavily on the more eﬃcient GPU. Additionally,
this will result in the removal of the costly CPU-to-GPU communica-
tion. Furthermore, we do not use a single symmetric multiprocessor (on
the GPU) to factorize a single problem at a time. We illustrate how our
performance analysis, and the use of proﬁling and tracing tools, guided
the development and optimization of our batched factorization to achieve
up to a 2-fold speedup and a 3-fold energy eﬃciency improvement com-
pared to our highly optimized batched CPU implementations based on
the MKL library (when using two sockets of Intel Sandy Bridge CPUs).
Compared to a batched QR factorization featured in the CUBLAS library
for GPUs, we achieved up to 5× speedup on the K40 GPU.
1
Introduction
Accelerators and coprocessors have enjoyed widespread adoption in computa-
tional science, consistently producing many-fold speedups across a wide range
of scientiﬁc disciplines and important applications [13]. The typical method of
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 31–47, 2015.
DOI: 10.1007/978-3-319-20119-1 3

32
A. Haidar et al.
utilizing the GPU accelerators is to increase the scale and resolution of an appli-
cation, which in turn increases its computational intensity; this tends to be a
good match for the steady growth in performance and memory capacity of this
type of hardware. Unfortunately, there are many important application types
for which the standard approach turns out to be a poor strategy for improving
hardware utilization1. Numerous modern applications tend to be cast in terms
of a solution of many small matrix operations, e.g., computations that require
tensor contraction (such as quantum Hall eﬀect), astrophysics [28], metabolic
networks [26], CFD and the resulting PDEs through direct and multifrontal
solvers [45], high-order FEM schemes for hydrodynamics [10], direct-iterative
preconditioned solvers [20], and some image [29] and signal processing [5]. That
is, at some point in their execution, such programs must perform a computation
that is cumulatively very large, but whose individual parts are very small; these
parts cannot be eﬃciently mapped as separate individuals on to the modern
accelerator hardware. Under these circumstances, the only way to achieve good
performance is to ﬁnd a way to group these small inputs together and run them
in large “batches.”
The emergence of large-scale, heterogeneous systems with GPU accelerators
and coprocessors has made the near total absence of linear algebra software
for such small matrix operations especially noticeable. Due to the high levels
of parallelism they support, accelerators or coprocessors, like GPUs, eﬃciently
achieve very high performance on large data parallel computations, so they have
often been used in combination with CPUs, where the CPU handles the small and
diﬃcult to parallelize tasks. Moreover, by using eﬃcient GPU-only codes, linear
algebra problems can be solved on GPUs with four to ﬁve times more energy
eﬃciency than one can get from multicore CPUs alone citebatchedCholesky.
For both of these reasons, and given the fundamental importance of numerical
libraries to science and engineering applications of all types [25], the need for
software that can perform batched operations on small matrices is acute. The
concepts in this paper work towards ﬁlling this critical gap, both by providing
a library that addresses a signiﬁcant range of small matrix problems, and by
driving progress toward a standard interface that would allow the entire linear
algebra (LA) community to attack the issues together.
for Ai ∈A1,A2,...,Ak do
GenerateSmallLinearSystem(Ai)
for Ai ∈A1,A2,...,Ak do
Factorize(Ai)
Fig. 1. Batched computation example.
To better understand the problem,
consider Fig. 1, which shows a simple
batched computation that factorizes a
sequence of small matrices Ai. The
word small is used in relative terms as
the beneﬁcial size of Ai will depend on
the circumstances. A straightforward
guideline to determine this size is the
ability of processing the matrices in parallel rather then sequentially. To achieve
this goal it is necessary to co-locate a batch of Ai in a fast GPU store – a cache
1 Historically, similar issues were associated with strong scaling [14] and were
attributed to a ﬁxed problem size.

A Framework for Batched and GPU-Resident Factorization Algorithms
33
or shared memory – and process it there with a better use of parallel execution
units.
This kind of optimization cannot be made by the compiler alone for two
primary reasons: the lack of standardized interfaces and the opaque implemen-
tation of the factorization routine. The former derives from the fact that, until
recently, batched computations were not the primary bottleneck for scientiﬁc
codes because it was the larger problems that posed a performance challenge.
Once appreciable increases in the processing power and memory capacity of
GPUs removed this bottleneck, the small size problems became prominent in
the execution proﬁle because they signiﬁcantly increased the total execution
time. The latter is a simple consequence of separation of concerns in the soft-
ware engineering process, whereby the computational kernels are packaged as
standalone modules that are highly optimized and cannot be inlined into the
batched loop in Fig. 1. What we propose is to deﬁne the appropriate interfaces
so that our implementation can work seamlessly with the compiler and use the
code replacement technique so that the user has an option of expressing the
computation as the loop shown in the ﬁgure or a single call to a routine from
the new standard batch library.
Against this background, the goal of this work is two-fold: ﬁrst, to deliver a
high-performance numerical library for batched linear algebra subroutines tuned
for the modern processor architectures and system designs. The library must
include LAPACK routine equivalents for many small dense problems as well as
routines for many small sparse matrix solvers, which should be constructed, as
much as possible, out of calls to batched BLAS routines and their look-alikes
required in the context of sparse computations. Second, and just as importantly,
it must deﬁne modular interfaces so that our implementation can seamlessly
work with the compiler and use code replacement techniques. This will provide
the developers of applications, compilers, and runtime systems with the option
of expressing computation as a loop (as shown in the ﬁgure), or a single call to
a routine from the new batch operation standard.
As might be expected, a batched BLAS forms the foundation of the frame-
work for the batched algorithms proposed, with other tasks building up in layers
above. The goal of this approach is to achieve portability across various architec-
tures, sustainability and ease of maintenance, as well as modularity in building
up the framework’s software stack. In particular, on top of the batched BLAS
(by algorithmic design), we illustrate the building of a batched LAPACK. The
new batched algorithms are implemented and currently released through the
MAGMA 1.6.1 library [21]. The framework will allow for future work extension
to batched sparse linear algebra, and application-speciﬁc batched linear alge-
bra. Finally, all the components are wrapped up in a performance and energy
autotuning framework.
In terms of the framework’s sustainability, it is important to note that batched
operations represent the next generation of software that will be required to eﬃ-
ciently execute scientiﬁc compute kernels on self-hosted accelerators that do not
have accompanying CPUs, such as the next generation of Intel Xeon Phi proces-
sors, and on accelerators with a very weak host CPU, e.g., various AMD APU

34
A. Haidar et al.
models and NVIDIA Tegra platforms. For such hardware, performing any non-
trivial work on the host CPU would slow down the accelerator dramatically,
making it essential to develop new batched routines as a basis for optimized
routines for accelerator-only execution.
2
Related Work
There is a lack of numerical libraries that cover the functionalities of batched
computation for GPU accelerators and coprocessors. NVIDIA started to add
certain batch functions in their math libraries; NVIDIA’s CUBLAS 6.5 [36]
includes batched Level 3 BLAS for gemm and trsm (triangular matrix solver), the
higher-level (LAPACK) LU and QR factorizations, matrix inversion, and a least
squares solver. All of these routines are for uniform size matrices. AMD and
Intel’s MKL do not provide batched operations yet. For higher-level routines,
NVIDIA provides four highly-optimized LAPACK-based routines, but they do
not address the variable sizes, extended functionality, portability and device-
speciﬁc redesigns of the LAPACK algorithms. Our work shows the potential
of addressing these issues, e.g., as illustrated in this paper by a 3× speedup
compared to the batch-optimized QR in CUBLAS.
Batched LA ideas can be applied to multicore CPUs as well. Indeed, small
problems can be solved eﬃciently on a single core, e.g., using vendor supplied
libraries such as MKL [23] or ACML [4], because the CPU’s memory hierarchy
would back a “natural” data reuse (small enough problems can ﬁt into small fast
memory). To further speedup the computation, beyond memory reuse, vector-
ization can be added to use SIMD supplementary processor instructions—either
explicitly as in the Intel Small Matrix Library [22], or implicitly through the
vectorization in BLAS. Batched factorizations can then be eﬃciently computed
for multicore CPUs by having a single core factorize a single problem at a time.
However, as we will show in this paper, the energy consumption is higher than
the GPU-based factorizations, and our GPU-based routine is about 2 times faster
than the multicore implementation.
Despite the lack of support for batched operations, application developers
implemented particular routines for certain cases, trying various approaches.
For example, when targeting very small problems (matrix sizes up to 128), Villa
et al. [37,38] obtained good results for batched LU developed entirely for GPU
execution, where a single CUDA thread, or a single thread block, was used
to solve one system at a time. Similar techniques, including the use of a single
CUDA thread warp for single factorization, were investigated by Wainwright [43]
for LU with full pivoting on matrix sizes up to 32. Although the problems con-
sidered were often small enough to ﬁt in the GPU’s shared memory, e.g., 48 KB
on a K40 GPU, and thus able to beneﬁt from data reuse, the results showed
that the performance in these approaches, up to about 20 Gﬂop/s in double
precision, did not exceed the performance of memory bound kernels like gemv
(which achieves up to 46 Gﬂop/s on a K40 GPU). Batched-speciﬁc algorithmic
improvements were introduced for the Cholesky factorization [9] and the LU fac-
torization [8,17], that exceed the memory bound limitations mentioned above in

A Framework for Batched and GPU-Resident Factorization Algorithms
35
terms of performance. Here we further develop and conceptualize an approach,
based on batched BLAS plus a number of batched-speciﬁc algorithmic innova-
tions to signiﬁcantly improve in performance the previously published results on
batched linear algebra.
3
Methodology and Algorithmic Design
In a number of research papers [8,9,19], we have shown that high-performance
batched algorithms can be designed so that the computation is performed by calls
to batched BLAS kernels, to the extent possible by the current BLAS API. This
is important since the use of BLAS has been crucial for the high-performance
sustainability of major numerical libraries for decades, and therefore we can also
leverage the lessons learned from that success. To enable the eﬀective use of a
batched BLAS based approach, there is a need to develop highly eﬃcient and
optimized batched BLAS routines that are needed by many high-level linear
algebra algorithms such as Cholesky, LU, and QR, either in batched or classical
fashion.
LU, QR, or Cholesky  
on small diagonal matrices 
Sparse / Dense Matrix 
System 
TRSMs, QRs, or LUs 
TRSMs, TRMMs
Updates (Schur complement)  
GEMMs, SYRKs, TRMMs
DAG-based factorization 
Batched LA 
And many other BLAS/LAPACK, e.g., for application 
specific solvers,  preconditioners, and matrices 
Fig. 2. Direct sparse or dense factorizations—a DAG approach that needs eﬃcient com-
putation of many small linear algebra tasks. Thin DAG edges represent data dependen-
cies among individual small tasks and if small data-parallel tasks are grouped together
in batches, the thick edges represent dependencies among the resulting batched tasks.
To put the proposed methodology in context, Fig. 2 illustrates our work on
direct linear system solvers, be it sparse or dense, for many-core heterogeneous
architectures. To provide parallelism in these solvers, the computation can be
expressed as a Directed Acyclic Graph (DAG) of small tasks with labeled edges
designating data dependencies, which naturally leads to the need to handle many
small LA problems in parallel. Our work with vendors (through vendor recogni-
tion centers), collaborators (from the HPC community), and application devel-
opers has resulted in the accumulation of expertise, technologies, and numerical
software [1,3,6,7,12,15,27,30–32,40–42,42] that can be directly leveraged in the
development of state-of-the-art, portable, cross-platform batched BLAS. The
objective of our methodology is to minimize the development eﬀort and have
a parametrized kernels that can be used for tuning on diﬀerent architectures
without the need to re-implement the kernel.

36
A. Haidar et al.
3.1
Algorithmic Baseline
The QR factorization of an m-by-n matrix A is of the form A = QR, where Q
is an m-by-m orthonormal matrix, and R is an m-by-n upper-triangular matrix.
The LAPACK routine GEQRF implements a right-looking QR factorization algo-
rithm, whose ﬁrst step consists of the following two phases:
1) Panel factorization: The ﬁrst panel A:,1 is transformed into an upper-
triangular matrix.
1. GEQR2 computes an m-by-m Householder matrix H1 such that HT
1 A:,1 =
R1,1
0

, and R1,1 is an nb-by-nb upper-triangular matrix.
2. LARFT computes a block representation of the transformation H1, i.e.,
H1 = I −V1T1V H
1 , where V1 is an m-by-nb matrix and T1 is an nb-by-nb
upper-triangular matrix.
2) Trailing submatrix update: LARFB applies the transformation computed
by GEQR2 and LARFT to the submatrix A:,2:nt:
R1,2:nt
A

:= (I −V1T1V H
1 )
 A1,2:nt
A2:mt,2:nt

.
Then, the QR factorization of A is computed by recursively applying the same
transformation to the submatrix A. The transformations Vj are stored in the
lower-triangular part of A, while R is stored in the upper-triangular part. Addi-
tional m-by-nb storage is required to store Tj.
3.2
Optimized and Parametrized Batched BLAS Kernels
We developed the most needed and performance-critical Level 3 and Level 2
batched BLAS routines. Namely, we developed the batched gemm (general
matrix-matrix multiplication), trsm (triangular matrix solver), and gemv (gen-
eral matrix-vector product) routines, as well as a number of Level 1 BLAS such
as the dot product, the norm functionality, and the scal scaling routine. There
are a number of feasible design choices for batched BLAS, each best suited for a
particular case. Therefore, to capture as many of them as possible, we designed
a space for batched BLAS that includes parametrized algorithms enabling an
ease of tuning for modern and future hardware and take into account the matrix
size. Thus, a parametrized-tuned approach can ﬁnd the optimal implementation
within the conﬁnes of the said design space.
We developed a parametrized basic kernel, that uses multiple levels of block-
ing, including shared memory and register blocking, as well as double buﬀering
techniques to hide the data communication with the computation. This ker-
nel allowed us to optimize and tune the MAGMA gemm routine for large matrix
sizes — originally for Fermi GPUs [32], and later for the Kepler GPUs. Recently,
we extended it to a batched gemm [18,19], and it is now available through
MAGMA 1.6.1 [21]. The extension was done by autotuning the basic kernel
and adding one more thread dimension to account for the batch count. Our goal

A Framework for Batched and GPU-Resident Factorization Algorithms
37
is to develop optimized components that can be used easily as a plug-in device
routine to provide many of the Level 3 and Level 2 BLAS routines. Following the
techniques for batched gemm for example, we developed a batched trsm kernel.
It consists of a sequence of calls to invert a 16 × 16 diagonal block followed by
a call to the gemm components which are already optimized and tuned.
Moreover, we developed the batched equivalent of LAPACK’s geqr2 routine
to perform the Householder panel factorizations. For a panel of nb columns,
it consists of nb steps where each step calls a sequence of the larfg and the
larf routines. At every step (to compute one column), the larfg involves a norm
computation followed by a scal that uses the results of the norm computation
in addition to some underﬂow/overﬂow checking. These Level 1 BLAS kernels
have been developed as device component routines to all for easy plug-in when
needed. The norm computation is a sum reduce and thus a synchronization
step. To accelerate it, we implemented a two-layer tree reduction where, for
sizes larger than 32, all 32 threads of a warp progress to do a tree reduction
similar to the MPI REDUCE operation, and the last 32 elements are reduced
by a single thread. Our parametrized technique lets us run our autotuner and
tune these kernels. As a result, custom batched implementations of both larfg
and the larf have been developed. When the panel size is small enough, we use
the shared memory to load the whole panel and to perform its computation in
fast memory. For larger panel sizes, we load only the vector that is annihilated at
each step, meaning that the norm, scal, and thus the larfg computation operate
on data in shared memory; the larf reads data from shared memory, but writes
data in main memory since it cannot ﬁt into the shared memory. When the panel
is very large, the BLAS kernel operates using many thread-blocks and an atomic
synchronization.
3.3
Development of Batched LAPACK Algorithms
The development of batched LAPACK algorithms and implementations is our
main example of how to use the batched BLAS for higher-level algorithms.
We show an approach based on batched BLAS and batched-speciﬁc algorith-
mic improvements that overcomes the memory bound limitations that previous
developers had on small problems. Moreover, we exceed the performance of even
state-of-the-art vendor implementations by up to 3×. Similarly to the batched
BLAS, we build a design space for batched LAPACK that includes parametrized
algorithms that are architecture and matrix size aware. An autotuning approach
is used to ﬁnd the best implementation within the provisioned design space.
We developed the performance-critical LAPACK routines to solve small dense
linear systems or least squares problems. Namely, we developed the LU and
Cholesky factorizations previously in [8,9,19], and we present our progress and
development for the QR decomposition in this paper.
We developed technologies for deriving high-performance from GPU-only
implementations to solve sets of small linear algebra problems (as in LAPACK)
in parallel. Note that GPU-only implementations have been avoided up until
recently in numerical libraries, especially for small and diﬃcult to parallelize

38
A. Haidar et al.
tasks like the ones targeted by the batched factorization. Indeed, hybridiza-
tion approaches were at the forefront of developing large scale solvers as they
were successfully resolving the problem by using CPUs for the memory bound
tasks [2,11,16,40,44]. For large problems, the panel factorizations (the source
of memory bound, not easy to parallelize tasks) are always performed on the
CPU. For small problems, however, this is not possible, and our experience has
shown that hybrid algorithms would not be as eﬃcient as they are for large prob-
lems. Therefore, we developed an approach based on a combination of 1) batched
BLAS, 2) batched-speciﬁc, and 3) architecture-aware algorithmic improvements.
Batched-speciﬁc algorithms that were diﬀerent from LAPACK were needed since
we could not outperform the NVIDIA-optimized LAPACK-based implementa-
tion by only using our own aggressive optimizations on top of the standard
LAPACK algorithm. In particular, for our QR decomposition, besides high-
performance batched BLAS, we also used batch-speciﬁc and architecture-aware
algorithmic advances described below.
Recursive Multilevel Nested Blocking. The panel factorizations (geqr2)
described above factorize the nb columns one after another, similarly to the
LAPACK algorithm. At each of the nb steps, a rank-1 update is required to
update the vectors to the right of the factorized column i. This operation is
done by the larf kernel. Since we cannot load the entire panel into the shared
memory of the GPU, the columns to the right are loaded back and forth from
the main memory at every step except for the very small size cases (e.g., size
less than 32 × 8). Thus, one can expect that this is the most time consuming
part of the panel factorization.
Our analysis using the NVIDIA Visual Proﬁler [33] shows that a large fraction
of even a highly optimized batched factorization is spent in the panels, e.g., 40 %
of the time for the QR decomposition. The proﬁler reveals that the larf kernel
requires more than 75% of the panel time by itself. The ineﬃcient behavior of
these routines is also due to the memory access. To resolve this challenge, we
propose to improve the eﬃciency of the panel and to reduce the memory access
by using a two-level nested blocking technique as depicted in Fig. 3. First, we
recursively split the panel to an acceptable block size nb as described in Fig. 3. In
principle, the panel can be blocked recursively until a single element remains. Yet,
in practice, 2-3 blocked levels (an nb = 32 for double precision was the best) are
suﬃcient to achieve high performance. Then, the routine that performs the panel
factorization (geqr2) must be optimized, which complicates the implementation.
This optimization can bring between 30 % to 40 % improvement depending on the
panel and the matrix size. In order to reach our optimization goal, we also blocked
the panel routine using the classical blocking fashion to small blocks of size ib
(ib = 8 was the optimized choice for double precision) as described in Fig. 3b.
More than a 25% boost in performance is obtained with this optimization.
Block Recursive
dlarft Algorithm. The larft is used to compute the upper
triangular matrix T that is needed by the QR factorization in order to update
either the trailing matrix or the right hand side of the recursive portion of the

A Framework for Batched and GPU-Resident Factorization Algorithms
39
P
a
n
e
L
Trailing 
matrix
update
sub panel 1a
Factored part of A
128
sub trailing matrix 1b
sub trailing matrix 2b
sub panel 2a
64
32
32
p
sub trailing matrix 1b
64
(a) Recursive nested blocking fashion.
P
a
n
e
L
32
32
done
sub trailing matrix 
d
sub 
8
d
b 
ubb
sub trailing matrix 
sub panel 
8
sub panel 
done
done
donne
sub panel 
8
8
(b) Classical blocking fashion.
sub trailing matrix 
Fig. 3. The recursive two-level nested blocking fashion is used in our implementation
to achieve high-performance batched kernels.
QR panel. The classical LAPACK computes T column by column in a loop
over the nb columns as described in Algorithm 1. Such an implementation takes
up to 50% of the total QR factorization time. This is due to the fact that the
kernels needed – gemv and trmv – require implementations where threads go
through the matrix in diﬀerent directions (horizontal vs. vertical, respectively).
An analysis of the mathematical formula of computing T allowed us to redesign
the algorithm to use Level 3 BLAS and to increase the data reuse by putting
the column of T in shared memory. One can observe that the loop can be split
into two loops – one for gemv and one for trmv. The gemv loop that computes
each column of T (see the notation in Algorithm 1 can be replaced by one gemm
to compute all the columns of T if the triangular upper portion of A is zero
and the diagonal is made of ones. For our implementation, replacing a gemv
loop with one gemm is already done for the trailing matrix update in the larfb
routine, and thus can be exploited here as well. For the trmv phase, we load the
T matrix into shared memory as this allows all threads to read/write from/into
shared memory during the nb steps of the loop. The redesign of this routine
is depicted in Algorithm 2. Since we developed a recursive blocking algorithm,
we must compute the T matrix for every level of the recursion. Nevertheless,
the analysis of Algorithm 2 leads us to conclude that the portion of the T’s
computed in the lower recursion level are the same as the diagonal blocks of the
T of the upper level (yellow diagonal blocks in Fig. 4), and thus we can avoid
their (re-)computation. For that we modiﬁed Algorithm 2 in order to compute
either the whole T or the upper rectangular portion that is missed (red/yellow
portions in Fig. 4). Redesigning the algorithm to block the computation using
Level 3 BLAS accelerated the overall algorithm on average by about 20 −30 %
(depending on various parameters).
Trading Extra Computation for Higher Performance. The goal here is
to replace the use of low performance kernels with higher performance ones—
often for the cost of more ﬂops, e.g., trmm used by the larfb can be replaced by

40
A. Haidar et al.
for j ∈{1, 2, . . . , nb} do
dgemv to compute T1:j−1,j = AH
j:m,1:j−1 × Aj:m,j
dtrmv to compute
T1:j−1,j = T1:j−1,1:j−1 × T1:j−1,j
T(j, j) = tau(j)
Algorithm 1. Classical implementation of the dlarft routine.
dgemm to compute T1:nb,1:nb = AH
1:m,1:nb × A1:m,1:nb
load T1:nb,1:nb to the shared memory. for j ∈{1, 2, . . . , nb} do
dtrmv to compute
T1:j−1,j = T1:j−1,1:j−1 × T1:j−1,j
T(j, j) = tau(j)
write back T to the main memory.
Algorithm 2. Block recursive dlarft routine.
gemm. The QR trailing matrix update uses the larfb routine to perform A =
(I −V T HV H)A. The upper triangle of V is zero with ones on the diagonal, and
also the matrix T is upper triangular. The classical larfb uses trmm to perform
the multiplication with T and with the upper portion of V . If one can guarantee
that the lower portion of T is ﬁlled with zeroes and the upper portion of V is
ﬁlled with zeros and ones on the diagonal, then the trmm can be replaced by
gemm. Thus we implemented a batched larfb that uses three gemm kernels by
initializing the lower portion of T with zeros, and ﬁlling up the upper portion
of V with zeroes and ones on the diagonal. Note that this brings 3nb3 extra
operations. The beneﬁts again depend on various parameters, but on current
architectures we observe an average of 10 % improvement, and see a trend where
its eﬀect on the acceleration grows from older to newer systems.
4
Performance Results
4.1
Hardware Description and Setup
We conducted our experiments on a two-socket multicore system with two 8-core
Intel Xeon E5-2670 (Sandy Bridge) processors, each running at 2.6 GHz. Each
socket has 20 MiB of shared Level 3 cache, and each core has a private 256 KiB
Level 2 and 64 KiB Level 1 cache. The system is equipped with the total of 52 GiB
of main memory and a theoretical peak, in double precision, of 20.8 Gﬂop/s per
core, i.e., 332.8 Glop/s in total for the two sockets. It is also equipped with
three NVIDIA K40c cards with 11.6 GiB of GDDR memory per card running at
825 MHz. The theoretical peak in double precision is 1, 689.6 Gﬂop/s per GPU.
The cards are connected to the host via two PCIe I/O hubs with 6 GB/s band-
width. A number of software packages were used for the experiments. On the
CPU side, we used MKL (Math Kernel Library) [23] with the icc compiler (ver-
sion 2013.sp1.2.144) and on the GPU accelerator we used CUDA version 6.5.14.

A Framework for Batched and GPU-Resident Factorization Algorithms
41
level 2
level 3
level 1
Fig. 4. The shape of the matrix T for diﬀerent level of the recursion during the QR
decomposition.
Regarding energy use, we note that in this particular setup the CPU and the
GPU have about the same theoretical power draw. In particular, the Thermal
Design Power (TDP) of the Intel Sandy Bridge is 115 W per socket, or 230 W
in total, while the TDP of the K40c GPU is 235 W. Therefore, we expect that a
GPU would have a power consumption advantage if it outperforms (in terms of
time to solution) the 16 Sandy Bridge cores. Note that, based on the theoretical
peaks, the GPU’s advantage should be about 4 to 5×. This is observed in practice
as well, especially for regular workloads on large data-parallel problems that can
be eﬃciently implemented for GPUs.
4.2
Performance Results
Getting high performance across accelerators remains a challenging problem that
we address with the algorithmic and programming techniques described in this
paper. These eﬃcient strategies are used to exploit parallelism and increase the
use of Level 3 BLAS operations across the GPU. We highlighted this through a
set of experiments that we performed on our system. We compare our batched
implementations with the dgeqrfBatched routine from the CUBLAS [35] library.
Our experiments were performed on batches of 1, 000 matrices of diﬀerent sizes
ranging from 32 × 32 to 1024 × 1024.
We also compare our batched QR to two CPU implementations. First is the
simple CPU implementation which operates in a loop style to factorize matrix
after matrix, where each factorization is using the multi-thread version of the
MKL Library. This implementation is limited in terms of performance and does
not achieve more than 90 Gﬂop/s. The main reason for this low performance is
the fact that the matrix is small – it does not exhibit parallelism and so the
multithreaded code is not able to feed work to all 16 threads used. For that we

42
A. Haidar et al.
32 64
128
256
384
512
640
768
896
1024
0
50
100
150
200
250
300
350
matrix size
Gflop/s
Batched dgeqrf count = 1000
GPU: Magma
GPU: CUBLAS
CPU v2: 16 parallel facto using sequential MKL
CPU v1: each matrix uses MKL multithread_16
Fig. 5. Performance in Gﬂops/s of the GPU vs. the CPU versions of our batched QR
decomposition for diﬀerent matrix sizes, where m = n.
proposed another version of the CPU implementation. Since the matrices are
small (< 1024) and at least 16 of them ﬁt in the Level 3 cache, one of the best
techniques is to use each thread to independently factorize a matrix. This way
16 factorizations are conducted independently, in parallel. We believe that this
implementation is one of the best optimized implementations for the CPU. This
later implementation is 2× faster than the simple implementation. It reaches
around 160 Gﬂop/s in factorizing 1, 000 matrices of size 1024 × 1024.
The progress of our batched QR implementation over the various optimiza-
tions shows promise. For a 1000 matrix of size 512×512 each, the classical block
implementation does not attain more than 55 Gﬂop/s. The inner panel blocking
allows for performance of around 70 Gﬂop/s; and the recursive blocking alone
improves performance up to 108 Gﬂop/s; combined, the two-level blocking brings
performance up to around 136 Gﬂop/s. The optimized computation of T draws
it up to 195 Gﬂop/s. The other optimizations (replacing dtrmm with dgemm in
both dlarft and dlarfb), combined with the streamed/batched dgemm calls, bring
the performance of the GPU implementation to around 221 Gﬂop/s. Despite
the CPU’s hierarchical memory advantage, our experiments show that our GPU
batched QR factorization is able to achieve a speedup of 2× vs. the best CPU
implementation using 16 Sandy Bridge cores, and 4× vs. the simple one. More-
over, our algorithm — which reaches around 334 Gﬂop/s for matrices of size
1024 × 1024 — is between 5× to 20× faster than the CUBLAS implementation
for matrices in the range of 512 to 1024. We should mention that the CUBLAS
implementation is well suited for very small matrices such as matrices of size
less than 64 × 64. The performance of CUBLAS for these sizes outperforms our
proposed algorithm as well as both of the CPU implementations Fig. 5.

A Framework for Batched and GPU-Resident Factorization Algorithms
43
4.3
Energy Eﬃciency
For our energy eﬃciency measurements, we use power and energy estimators
built into the modern hardware platforms. In particular, on the tested CPU, the
Intel Xeon E5-2690, we use RAPL (Runtime Average Power Limiting) hardware
counters [24,39]. By the vendor’s own admission, the reported power/energy
numbers are based on a model which is tuned to match the actual measurements
for various workloads. Given this caveat, we can report that the idle power of the
tested Sandy Bridge CPU, running at a ﬁxed frequency of 2600 MHz, consumes
about 20 W of power per socket. Batched operations raise the consumption from
125 to 140 W per socket, and the large dense matrix operations, which reach the
highest fraction of the peak performance, raise the power draw to about 160 W
per socket. We should mention that the CPU measurements do not include the
power cost of the memory access, while the GPU measurements include it. In
order to include the power for the CPU, we had to change in the BIOS and
we were not allowed to do it on our testing machine. However, results on other
systems showed that the power of the CPU memory access can be estimated to be
40 W on average. On some systems, energy consumption numbers do not include
the power consumed by the main memory as the memory modules do not report
their voltage levels to the CPU’s memory controller on those systems, which
renders RAPL ineﬀective for the purpose of estimating temporal power draw.
However, based on estimates from similarly conﬁgured systems, we estimate that
the power consumption for the main memory under load is between 30 W and
40 W, depending on the memory size and conﬁguration.
For the GPU measurements we use NVIDIA’s NVML (NVIDIA Manage-
ment Library) library [34]. NVML provides a C-based programmatic interface
for monitoring and managing various states within NVIDIA Tesla GPUs. On
Fermi and Kepler GPUs (like the K40c used) the readings are reported to be
accurate to within +/-5% of current power draw. The idle state of the K40c
GPU consumes about 20 W. Batched factorizations raise the consumption from
150 to 180 W, while large dense matrix operations raise the power draw to about
200 W. For reference, it is worth noting that the active idle state draws 62 W.
In Fig. 6 we depict the comparison of the power consumption required by the
three implementations of the batched QR decomposition: the best GPU and the
two CPU implementations. The problem solved here is about 1, 000 matrices of
size 1024 × 1024 each. The green curve shows the power required by the simple
CPU implementation. In this case the batched QR proceeds as a loop over the
1, 000 matrices where each matrix is factorized using the multithreaded dgeqrf
routine from the Intel MKL library on the 16 Sandy Bridge CPU cores. The
blue curve shows the power required by the optimized CPU implementation.
Here, the code proceeds with sweeps of 16 parallel factorizations each using the
sequential dgeqrf routine form the Intel MKL library. The red curve shows the
power consumption of our GPU implementation of the batched QR decompo-
sition. One can observe that the GPU implementation is attractive because it
is around 2× faster than the optimized CPU implementation, and moreover,
because it consumes 3× less energy.

44
A. Haidar et al.
Fig. 6. Comparison of the power consumption for the QR decomposition of 1, 000
matrices of size 1024 × 1024.
According to the experiments we conducted to measure the power, we found
that the GPU implementations of all of the batched one-sided factorizations
reach around 2× speedup over their best CPU counterpart and are 3× less
expensive in terms of energy consumption.
5
Conclusions an Future Work
Designing algorithms to work on small problems is a concept that can deliver
higher performance through improved data reuse. Many applications have relied
on this design concept to get higher hardware eﬃciency, and users have requested
it as a supported functionality in linear algebra libraries. Besides having the
potential to improve the overall performance of applications with computational
patterns ranging from dense to sparse linear algebra, developing these algorithms
for the new low-powered and power-eﬃcient architectures can bring signiﬁcant
savings in energy consumption as well, as we showed. Therefore, by solving the
technical issues and providing the needed batched LA tools, the future devel-
opment on the framework presented will also address the following long term
goals: 1) deﬁne a new standard for the use of small matrix computations in
applications; 2) provide a methodology to solve many small size LA problems
and an initial implementation that is portable at all levels of the platform pyra-
mid, from embedded devices to supercomputers; and 3) establish grounds for the
next generation of innovations in HPC applications and sustainability of high-
performance numerical libraries. The algorithms described are already available

A Framework for Batched and GPU-Resident Factorization Algorithms
45
in the open-source MAGMA library. The proposed framework and the algorithms
to be developed for batched linear algebra will be open for input and contribu-
tions from the community, similar to LAPACK, and will be incorporated into
the MAGMA Batched library.
Future work extensions include building batched sparse, and application-
speciﬁc batched linear algebra capabilities. Of speciﬁc interest will be the eﬀect
of the batched framework on high-performance numerical libraries and run-time
systems. Current approaches, e.g., in dense tiled algorithms, are based on split-
ting algorithms into small tasks that get inserted into, and scheduled for exe-
cution by, a run-time system. This often amounts to splitting large gemms into
many small gemms, which is known to encounter overheads for scheduling and
saving parameters (although most are the same). The batched approach, besides
providing high performance for small tasks, will be a natural ﬁt to extend these
and similar libraries, as well as provide a new hierarchical scheduling model for
queuing jobs, organizing run-time systems, and interacting with accelerators/co-
processors.
Acknowledgments. This material is based upon work supported by the National
Science Foundation under Grant No. ACI-1339822, the Department of Energy, and
Intel. The results were obtained in part with the ﬁnancial support of the Russian
Scientiﬁc Fund, Agreement N14-11-00190.
References
1. Agullo, E., Demmel, J., Dongarra, J., Hadri, B., Kurzak, J., Langou, J., Ltaief, H.,
Luszczek, P., Tomov, S.: Numerical linear algebra on emerging architectures: the
PLASMA and MAGMA projects. J. Phys.: Conf. Ser. 180(1), 012037 (2009)
2. Agullo, E., Augonnet, C., Dongarra, J., Ltaief, H., Namyst, R., Thibault, S.,
Tomov, S.: Faster, cheaper, better - a hybridization methodology to develop linear
algebra software for GPUS. In: Hwu, W.W. (ed.) GPU Computing Gems. Morgan
Kaufmann, California (2010)
3. Agullo, E., Dongarra, J., Nath, R.,Tomov, S.: Fully empirical autotuned qr factor-
ization for multicore architectures (2011). CoRR, abs/1102.5328
4. ACML
-
AMD
Core
Math
Library
(2014).
http://developer.amd.com/
tools-and-sdks/cpu-development/amd-core-math-library-acml
5. Anderson, M.J., Sheﬃeld, D., Keutzer. K.: A predictive model for solving small
linear algebra problems in gpu registers. In: IEEE 26th International Parallel Dis-
tributed Processing Symposium (IPDPS) (2012)
6. Buttari, A., Dongarra, J., Kurzak, J., Langou, J., Luszczek, P., Tomov, S.: The
impact of multicore on math software. In: K˚agstr¨om, B., Elmroth, E., Dongarra, J.,
Wa´sniewski, J. (eds.) PARA 2006. LNCS, vol. 4699, pp. 1–10. Springer, Heidelberg
(2007)
7. Cao, C., Dongarra, J., Du, P., Gates, M., Luszczek, P., Tomov, S.: clMAGMA:
high performance dense linear algebra with OpenCL. In: The ACM International
Conference Series, Atlanta, May 13–14 (2013). (submitted)
8. Dong, T., Haidar, A., Luszczek, P., Harris, A., Tomov, S., Dongarra, J.: LU
factorization of small matrices: accelerating batched DGETRF on the GPU. In:
Proceedings of 16th IEEE International Conference on High Performance and Com-
munications (HPCC 2014), August 2014

46
A. Haidar et al.
9. Dong, T., Haidar, A., Tomov, S., Dongarra, J.: A fast batched cholesky factor-
ization on a GPU. In: Proceedings of 2014 International Conference on Parallel
Processing (ICPP-2014), September 2014
10. Dong, T., Dobrev, V., Kolev, T., Rieben, R., Tomov, S., Dongarra, J.: A step
towards energy eﬃcient computing: redesigning a hydrodynamic application on
CPU-GPU. In: IEEE 28th International Parallel Distributed Processing Sympo-
sium (IPDPS) (2014)
11. Dongarra, J., Haidar, A., Kurzak, J., Luszczek, P., Tomov, S., YarKhan, A.:
Model-driven one-sided factorizations on multicore accelerated systems. Int.
J.Supercomputing Frontiers Innovations 1(1), 85 (2014)
12. Peng, D., Weber, R., Luszczek, P., Tomov, S., Peterson, G., Dongarra, J.: From
CUDA to OpenCL: towards a performance-portable solution for multi-platform
GPU programming. Parallel Comput. 38(8), 391–407 (2012)
13. Oak Ridge Leadership Computing Facility. Annual report 2013–2014 (2014).
https://www.olcf.ornl.gov/wp-content/uploads/2015/01/AR 2014 Small.pdf
14. Gustafson, J.L.: Reevaluating Amdahl’s law. Commun. ACM 31(5), 532–533
(1988)
15. Haidar, A., Tomov, S., Dongarra, J., Solca, R., Schulthess, T.: A novel hybrid
CPU-GPU generalized eigensolver for electronic structure calculations based on
ﬁne grained memory aware tasks. Int. J. High Perform. Comput. Appl. 28(2),
196–209 (2012)
16. Haidar, A., Cao, C., Yarkhan, A., Luszczek, P., Tomov, S., Kabir, K., Dongarra,
J.: Uniﬁed development for mixed multi-gpu and multi-coprocessor environments
using a lightweight runtime environment. In: IPDPS 2014 Proceedings of the 2014
IEEE 28th International Parallel and Distributed Processing Symposium, pp. 491–
500. IEEE Computer Society, Washington, (2014)
17. Haidar, A., Dong, T., Luszczek, P., Tomov, S., Dongarra, J.: Batched matrix com-
putations on hardware accelerators based on GPUs. Int. J. High Performance Com-
put. Appl. 18(1), 135–158 (2015). doi:10.1177/1094342014567546
18. Haidar, A., Luszczek, P., Tomov, S., Dongarra, J.: Optimization for performance
and energy for batched matrix computations on GPUs. In: PPoPP 2015 8th Work-
shop on General Purpose Processing Using GPUs (GPGPU 8) co-located with
PPOPP 2015, ACM, San Francisco, February 2015
19. Haidar, A., Luszczek, P., Tomov, S., Dongarra, J.: Towards batched linear solvers
on accelerated hardware platforms. In: PPoPP 2015 Proceedings of the 20th ACM
SIGPLAN Symposium on Principles and Practice of Parallel Programming, ACM,
San Francisco, February 2015
20. Im, E.-J., Yelick, K., Vuduc, R.: Sparsity: optimization framework for sparse matrix
kernels. Int. J. High Perform. Comput. Appl. 18(1), 135–158 (2004)
21. Matrix algebra on GPU and multicore architectures (MAGMA), MAGMA Release
1.6.1 (2015). http://icl.cs.utk.edu/magma/
22. Intel Pentium III Processor - Small Matrix Library (1999). http://www.intel.com/
design/pentiumiii/sml/
23. Intel Math Kernel Library (2014). http://software.intel.com/intel-mkl/
24. Intel 64 and IA-32 architectures software developer’s manual, July 20 (2014).
http://download.intel.com/products/processor/manual/
25. Keyes, D., Taylor, V.: NSF-ACCI task force on software for science and engineering,
December 2010
26. Liao, J.C., Khodayari, A., Zomorrodi, A.R., Maranas, C.D.: A kinetic model of
escherichia coli core metabolism satisfying multiple sets of mutant ﬂux data. Metab.
Eng. 25C, 50–62 (2014)

A Framework for Batched and GPU-Resident Factorization Algorithms
47
27. Li, Y., Dongarra, J., Tomov, S.: A note on auto-tuning GEMM for GPUs. In: Allen,
G., Nabrzyski, J., Seidel, E., van Albada, G.D., Dongarra, J., Sloot, P.M.A. (eds.)
ICCS 2009, Part I. LNCS, vol. 5544, pp. 884–892. Springer, Heidelberg (2009)
28. Messer, O.E.B., Harris, J.A., Parete-Koon, S., Chertkow, M.A.: Multicore and
accelerator development for a leadership-class stellar astrophysics code. In: Manni-
nen, P., ¨Oster, P. (eds.) PARA. LNCS, vol. 7782, pp. 92–106. Springer, Heidelberg
(2013)
29. Molero, J.M., Garz´on, E.M., Garc´ıa, I., Quintana-Ort´ı, E.S, Plaza, A.: Poster: a
batched Cholesky solver for local RX anomaly detection on GPUs. In: PUMPS
(2013)
30. Nath, R., Tomov,S., Dong, T., Dongarra, T.: Optimizing symmetric dense matrix-
vectormultiplication on GPUs. In: Proceedings of 2011 International Conference
for High PerformanceComputing, Networking, Storage and Analysis, November
2011
31. Nath, R., Tomov, S., Dongarra, T.: Accelerating GPU kernels for dense linear
algebra. In: VECPAR 2010 Proceedings of the 2009 International Meeting on High
Performance Computing for Computational Science, pp. 22–25. Springer, Berkeley,
June 2010
32. Nath, R., Tomov, S., Dongarra, J.: An improved magma gemm for fermi graphics
processing units. Int. J. High Perform. Comput. Appl. 24(4), 511–515 (2010)
33. Nvidia visual proﬁler
34. https://developer.nvidia.com/nvidia-management-library-nvml (2014)
35. CUBLAS (2014). http://docs.nvidia.com/cuda/cublas/
36. CUBLAS 6.5, January 2015. http://docs.nvidia.com/cuda/cublas/
37. Villa, O., Fatica, M., Gawande, N., Tumeo, A.: Power/performance trade-oﬀs of
small batched LU based solvers on GPUs. In: Wolf, F., Mohr, B., an Mey, D. (eds.)
Euro-Par 2013. LNCS, vol. 8097, pp. 813–825. Springer, Heidelberg (2013)
38. Nitin, V.O., Gawande, A., Tumeo, A.: Accelerating subsurface transport simu-
lation on heterogeneous clusters. In: IEEE International Conference on Cluster
Computing (CLUSTER 2013), pp. 23–27, Indiana, September 2013
39. Rotem, E., Naveh, A., Rajwan, D., Ananthakrishnan, A., Weissmann, E.: Power-
management architecture of the intel microarchitecture code-named sandy bridge.
IEEE Micro. 32(2), 20–27 (2012). doi:10.1109/MM.2012.12. ISSN: 0272–1732
40. Tomov, S., Dongarra, J., Baboulin, M.: Towards dense linear algebra for hybrid
gpu accelerated manycore systems. Parellel Comput. Syst. Appl. 36(5–6), 232–240
(2010). doi:10.1016/j.parco.2009.12.005
41. Tomov, S., Nath, R., Ltaief, H., Dongarra, J.: Dense linear algebra solvers for
multicore with GPU accelerators. In: Proceedings of the IEEE IPDPS 2010, pp.
1–8. IEEE Computer Society, Atlanta, 19–23 April 2010. doi:10.1109/IPDPSW.
2010.5470941
42. Tomov, S., Dongarra, J.: Dense linear algebra for hybrid gpu-based systems. In:
Kurzak, J., Bader, D.A., Dongarra, J. (eds.) Scientiﬁc Computing with Multicore
and Accelerators. Chapman and Hall/CRC, UK (2010)
43. Wainwright, I .: Optimized LU-decomposition with full pivot for small batched
matrices, GTC 2013 - ID S3069. April 2013
44. Yamazaki, I., Tomov, S., Dongarra, J.: One-sided dense matrix factorizations on
a multicore with multiple GPU accelerators. In: Proceedings of the International
Conference on Computational Science, ICCS 2012, pp. 37–46. Procedia Computer
Science, 9(0):37 (2012)
45. Yeralan, S.N., Davis, T.A., Ranka, S.: Sparse mulitfrontal QR on the GPU. Tech-
nical report, University of Florida Technical report (2013)

Parallel Eﬃcient Sparse Matrix-Matrix
Multiplication on Multicore Platforms
Md. Mostofa Ali Patwary1(B), Nadathur Rajagopalan Satish1,
Narayanan Sundaram1, Jongsoo Park1, Michael J. Anderson1,
Satya Gautam Vadlamudi1, Dipankar Das1, Sergey G. Pudov2,
Vadim O. Pirogov2, and Pradeep Dubey1
1 Parallel Computing Lab, Intel Corporation, Santa Clara, USA
mostofa.ali.patwary@intel.com
2 Software and Services Group, Intel Corporation, Santa Clara, USA
Abstract. Sparse matrix-matrix multiplication (SpGEMM) is a key
kernel in many applications in High Performance Computing such as
algebraic multigrid solvers and graph analytics. Optimizing SpGEMM
on modern processors is challenging due to random data accesses, poor
data locality and load imbalance during computation. In this work, we
investigate diﬀerent partitioning techniques, cache optimizations (using
dense arrays instead of hash tables), and dynamic load balancing on
SpGEMM using a diverse set of real-world and synthetic datasets. We
demonstrate that our implementation outperforms the state-of-the-art
using Intel R
⃝Xeon R
⃝processors. We are up to 3.8X faster than Intel R
⃝
Math Kernel Library (MKL) and up to 257X faster than CombBLAS. We
also outperform the best published GPU implementation of SpGEMM
on nVidia GTX Titan and on AMD Radeon HD 7970 by up to 7.3X and
4.5X, respectively on their published datasets. We demonstrate good
multi-core scalability (geomean speedup of 18.2X using 28 threads) as
compared to MKL which gets 7.5X scaling on 28 threads.
1
Introduction
Sparse Matrix-Matrix Multiplication (SpGEMM) is an important kernel used in
many applications in High Performance Computing such as algebraic multigrid
solvers [4] and graph analytic kernels [7,10,12,17]. Compared to the eﬃciency
of the corresponding dense GEMM routines, SpGEMM suﬀers from poor per-
formance on most parallel hardware. The diﬃculty in optimizing SpGEMM lies
in the irregular memory access patterns, unknown pattern of non-zeros in the
output matrix, poor data locality and load imbalance during computation. For
sparse matrices that have non-zero patterns following power law distributions
(e.g. graphs from social network and recommendation system domains), this
leads to poor eﬃciency as some portions of the output are very dense while
others are very sparse.
This paper presents an optimized implementation of SpGEMM on two matri-
ces A and B that eﬃciently utilizes current multicore hardware. We have paral-
lelized SpGEMM through row and column based blocking of A and B respectively.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 48–57, 2015.
DOI: 10.1007/978-3-319-20119-1 4

Parallel Eﬃcient Sparse Matrix Multicore Matrix-Matrix Multiplication
49
By using a dense array to accumulate partial (sparse vector) results, we get supe-
rior performance compared to previous implementations. We also maintain a CSR
input and output format and include data structure transformation and memory
allocation costs in our runtime.
Our main contributions are as follows:
– We present the fastest SpGEMM results on a single node on a variety of diﬀer-
ent sparse matrices drawn from various domains. Our implementation running
on Intel R⃝Xeon R⃝E5-2697 v3 processor based system is faster than Intel R⃝
MKL by up to 3.8X and CombBLAS by up to 257X. Our implementation also
outperforms previously published GPU implementations [13] on nVidia GTX
Titan and on AMD Radeon HD 7970 by up to 7.3X and 4.5X, respectively.
– We have explored diﬀerent partitioning schemes for SpGEMM. We provide
intelligent heuristics that combine row-wise partitioning of one matrix and
column-wise partitioning of the second matrix to get the best performance
(up to 1.4X improvement) on a single node.
– We divide the matrices into small partitions and perform dynamic load bal-
ancing over the partitions, resulting in speedups of up to 1.49X and 1.24X
respectively.
– We demonstrate good multi-core scalability (geomean speedup of 18.2X using
28 threads) as compared to MKL, which gets 7.5X scaling on 28 threads.
2
SpGEMM Algorithm and Optimizations
2.1
Overview
Sparse Matrix-Matrix Multiply (SpGEMM) involves the multiplication of two
sparse matrices A of dimension m × k and B of dimension k × n to yield a
resultant matrix C of dimension m×n. In this paper, for presentation simplicity,
we assume we deal with square matrices where m = n = k (represented as n
hereafter). However, our techniques are general and applicable to other matrices
as well.
Fig. 1. Data access pattern of Gustavson [11] and Partitioned SpGEMM algorithms.

50
M.M.A. Patwary et al.
Consider the following notations. Ai,j denotes a single entry of matrix A. Ai,:
denotes the ith row of matrix A, and A:,i represents the ith column of A. Then the
computation of the entire row i of C can be seen to be Ci,: = 
k Ai,k ∗Bk,:. This
computation is shown in Fig. 1(a). The ﬁgure shows the situation for a sparse
matrix A, where Ai,k is non-zero only for some values of k, and computations
only occur on the corresponding rows of B. The product of the scalar Ai,k with
the non-zeros in Bk,: basically scales the elements of the row Bk,: and has the
same sparsity structure as Bk,:. This product then needs to be summed into Ci,:.
Note that Ci,: is the sum of various sparse vectors obtained from the products
above, and is in general sparse, although its non-zero structure and density may
be quite diﬀerent from that of A and B.
Gustavson [11] proposed a single-
threaded
algorithm
for
SpGEMM
based on the Compressed Sparse Row
(CSR) format. This is a straightfor-
ward implementation of the compu-
tations described in Fig. 1(a). This
algorithm can be parallelized over
rows of A to run on multi-core proces-
sors. However, as we show below,
this basic algorithm does not take
full advantage of architectural fea-
tures such as caches present on mod-
ern hardware and makes ineﬃcient
use of the limited bandwidth available at various levels of the memory hierarchy.
We now show the optimizations that we perform to overcome these bottlenecks.
2.2
Performance Optimizations
We make a number of improvements to the algorithm described previously.
Adding Sparse Vectors. Whenever a new sparse vector is to be added to the
running sum for Ci,:, the index of each non-zero value in the sparse vector needs
to looked up in the running sum. If present, the value of the non-zero element
needs to be added to that in Ci,:, and if not, a new entry is to be created for
this non-zero.
We considered various options to eﬃciently implement this lookup. One app-
roach is to use a hash table to store the non-zero elements of the running sum for
Ci,:, with the key being the column index of the non-zero, and the value being its
numerical value. However, we found that this technique had high overheads due
to (i) cost of hash key computations and (ii) cost of handling collisions through
chaining.
Since the range of elements is known apriori (equal to the matrix dimension n),
it is much more eﬃcient to use a dense array to store the running sum. We initial-
ize this array X to zero. When we need to add a new sparse vector, we take each

Parallel Eﬃcient Sparse Matrix Multicore Matrix-Matrix Multiplication
51
non-zero entry and simply add its value to X[c], where c is the column index of the
non-zero. Finally, once all additions are complete, we need to convert this dense
array back to a sparse CSR representation when writing back to C. While the
additions themselves have very low overhead with this data structure, it is very
ineﬃcient to have to scan the entire dense X array (most of whose elements are
zero) and write back only the few non-zero elements into a sparse format. Indeed,
our experiments indicate that this scan takes more than 20X the time required
for the additions themselves. Hence, in addition to X, we keep a index array that
stores the non-zero indices of X. This can be cheaply maintained during the addi-
tion process by simply appending a column index c when it is ﬁrst written to, i.e.
when the existing array value X[c] is zero. We then iterate only over this sparse
index array when writing to C and reset the corresponding value in X.
Partitioning Schemes. The computation of individual rows of C can be done
independently, and hence rows of C (and the computation on the corresponding
rows of A) can be trivially partitioned among the threads.
However, we need to pay careful attention to cache behavior during the sparse
addition. Depending on the number of distinct cache lines of the dense array X
that are touched during the update of a row of C, the X data structure may
not completely reside in a close enough level of the cache hierarchy. Since this
update is in the inner loop of the code, this can signiﬁcantly aﬀect performance.
Speciﬁcally, for the datasets we describe in the evaluation section, we see a
number of potential misses to the private second level (L2) cache. These misses
are usually captured in the shared last level (LLC) cache, however we do ﬁnd a
signiﬁcant performance impact due to L2 misses. In general for larger data sets,
one could see misses to LLC as well. Hence we need a general scheme to localize
accesses to the X array through blocking.
Blocking accesses to X in an eﬃcient manner is non-trivial. Without any
modiﬁcations, blocking along a row of B is diﬃcult to achieve (as there are
only few non-zeros per row on average). We hence propose to change the data
structure of matrix B, and store it in a partitioned manner. We store individual
CSRs for each partition of B. The number of partitions required for B depends on
the L2 cache size. Figure 1(b) shows this scheme, where accesses to X get blocked.
There are, of course, overheads in creating this blocked CSR data structure from
the original CSR, and it may not be worthwhile to perform this transformation.
We discuss this shortly.
Algorithm 1 shows the overall pseudocode for our algorithm. We use a hybrid
scheme where we partition both the rows of A and the columns of B. Each parti-
tion updates a 2D block of matrix C. Each block of C is computed independently
as an SpGEMM of the corresponding row partition of A and column partition
of B.
As mentioned before, there is overhead in creating the blocked CSR represen-
tations of B. Further, each block of B is much sparser than the full row of B, and
accesses to row pointers are not well amortized. This can lead to some bandwidth

52
M.M.A. Patwary et al.
overhead when reading B as well. Consequently it only makes sense to perform
the transformation when we know that the beneﬁts when accessing X are large.
If we know that, for a signiﬁcant fraction of rows, the number of non-zeros of the
ﬁnal X after all updates is greater than the L2 cache size (usually 256 KB, or
64 K single-precision numbers), then blocking accesses to X makes sense. Since
we do not apriori know the density of the output rows, we need to estimate it.
We use a simple upper-bound estimate described in [13], where we merely count
the number of multiplications involved in computing each row of C. This can
be done cheaply without any actual ﬂoating point operations by merely looking
at the non-zero structure of A and the row pointer array of B. This usually
takes just 1-2 % of overall runtime to compute. We use these estimates e nnz(i)
for each row i to deﬁne an overall metric : e nnz =

i:e nnz(i)>64K e nnz(i)

i e nnz(i)
. If
this is suﬃciently large (greater than 30 %), our results show that we should
partition B.
Dynamic Scheduling. Diﬀerent partitions in the computations described in
Algorithm 1 have diﬀering amounts of computation and store diﬀerent numbers
of non-zeros. This leads to severe load imbalance. We have found that reducing
the size of each partition so that the total number of partitions is 6–10 times the
number of threads leads to signiﬁcantly better load balance.
3
Experimental Results
3.1
Experimental Setup
We used an Intel R⃝Xeon R⃝1 E5-2697 v3 processor based system for the exper-
iments. The system consists of two processors, each with 14-cores running at
2.6 GHz (a total of 28 cores) with 36 MB L3 cache and 64 GB memory. The
system is based on the Haswell microarchitecture and runs Redhat Linux (ver-
sion 6.5). All our code is developed using C/C++ and is compiled using the
Intel R⃝C++ compiler2 (version: 15.0.1) using the -O3 ﬂag. We pins threads
to cores for eﬃcient NUMA behavior by setting KMP AFFINITY to granular-
ity=ﬁne,compact,1 in our experiments [2].
1 Intel, Xeon, and Intel Xeon Phi are trademarks of Intel Corporation in the U.S.
and/or other countries.
2 Intel’s compilers may or may not optimize to the same degree for non-Intel micro-
processors for optimizations that are not unique to Intel microprocessors. These opti-
mizations include SSE2, SSE3, and SSE3 instruction sets and other optimizations.
Intel does not guarantee the availability, functionality, or eﬀectiveness of any opti-
mization on microprocessors not manufactured by Intel. Microprocessor-dependent
optimizations in this product are intended for use with Intel microprocessors. Certain
optimizations not speciﬁc to Intel micro-architecture are reserved for Intel micro-
processors. Please refer to the applicable product User and Reference Guides for
more information regarding the speciﬁc instruction sets covered by this notice. Notice
revision #20110804.

Parallel Eﬃcient Sparse Matrix Multicore Matrix-Matrix Multiplication
53
Table 1. Structural properties of the datasets. C denotes the resultant matrix.
Name
Rows
nnz
Avg Degree Max Degree nnz (C)
Time (sec) Type
harbor
46,835
4,701,167 100
289
7,900,918 0.0681
Symmetric
hood
220,542 10,768,436
49
77
34,242,181 0.0802
Symmetric
qcd
49,152
1,916,928
39
39
10,911,745 0.0373
Asymmetric
consph
83,334
6,010,480
72
81
26,539,737 0.0675
Symmetric
pwtk
217,918 11,634,424
53
180
32,772,237 0.0821
Symmetric
PR02R
161,070
8,185,136
51
92
30,969,454 0.0756
Asymmetric
mono
169,410
5,036,288
30
719
41,377,965 0.0965
Asymmetric
webbase
1,000,005
3,105,536
3
4,700
51,111,997 0.1597
Asymmetric
audikw
943,695 39,297,771
42
346
164,772,225 0.2053
Asymmetric
mou.gene
45,101 14,506,196 322
8033
190,430,984 0.9016
Asymmetric
cage14
1,505,785 27,130,349
18
82
236,999,813 0.2469
Asymmetric
dielFilt
1,102,824 45,204,422
41
271
270,082,366 0.2687
Asymmetric
rmat er
262,144 16,777,150
64
181
704,303,135 0.7538
Asymmetric
rmat g
262,144 16,749,883
64
3283
1,283,506,475 1.1721
Asymmetric
rmat b
262,144 16,749,883
64
54250
1,648,990,052 2.4519
Asymmetric
We used both real-world and synthetic matrices for performance analysis.
Table 1 shows the structural properties of the datasets. Our real-world matrices
consist of 12 datasets collected from the Florida Matrix collection [8] covering
many applications including structural engineering, web connectivity, electro-
magnetics, and medical science. Six of these datasets (harbor, hood, qcd, pwtk,
mono, and webbase) are the same as those used in [13]. We speciﬁcally picked
datasets where the authors’ implementation shows the best performance and
augmented this set with the largest datasets considered in that work. To increase
the diversity of use cases considered, we additionally included 6 more datasets,
which are up to an order of magnitude larger (in terms of nonzeros) than those
previously experimented.
We also generated 3 types of synthetic datasets using the Graph500 RMAT
data generator [14] by varying the RMAT parameters (similar to previous work
[6]). These are (i) rmat b (parameters 0.55, 0.15, 0.15, 0.15), (ii) rmat g (para-
meters 0.45, 0.15, 0.15, 0.25), and (iii) rmat er (0.25, 0.25, 0.25, 0.25). These
matrices vary mainly in terms of degree distributions (e.g. rmat b is highly
skewed) to cover a wider range of applications.
The seven largest datasets (audikw, mou.gene, cage14, dielFilt, and the three
synthetic datasets) are treated as unsymmetric to avoid large SpGEMM out-
put matrices C that overﬂow memory limits. The symmetry of each dataset is
tabulated in Table 1.
3.2
Experimental Results
We ﬁrst compare the performance of our SpGEMM implementation with
state-of-the-art results. To do so, we consider four available implementations,

54
M.M.A. Patwary et al.
Fig. 2. Performance comparison of CombBLAS, Intel R
⃝MKL, BHSPARSE on nVidia
GTX Titan GPU and on AMD Radeon HD 7970 GPU, and our implementation.
(i) the Combinatorial BLAS Library (CombBLAS v1.3, [1]), (ii) Intel R⃝Math
Kernel Library (MKL, version 11.2.1 [3]), (iii) BHSPARSE implementation on
the nVidia GeForce GTX Titan GPU reported in [13], and (iv) BHSPARSE
implementation on the AMD Radeon HD 7970 GPU reported in [13]. These rep-
resent some of the most recent SpGEMM publications that perform well on mod-
ern hardware [13]. We consider all 15 datasets in the comparison. However, since
the GPU SpGEMM code is not available online, we used the performance results
of the 6 overlapping datasets from [13]. Figure 2 shows the performance of these
implementations in GFlops. As can be seen, our code is able to achieve up to
18.4 GFlops (geomean 6.6 GFlops) whereas CombBLAS, MKL, BHSPARSE on
nVidia GTX Titan, and BHSPARSE on AMD Radeon HD 7970 GPU achieve
up to 0.3, 11.7, 2.8, and 5.0 (geomean of 0.09, 3.6, 1.5, and 2.1) GFlops respec-
tively. CombBLAS uses the DCSC matrix format that involves an additional
layer of indirection (more cache line accesses) that leads to poor performance.
MKL does not partition the columns of B and uses a static partitioning scheme
that can cause performance degradation. We note that our performance results
are up to 7.3X (geomean 3.9X) compared to the best BHSPARSE GPU imple-
mentation. This is despite the nearly 2× higher peak ﬂops for the GPU cards
compared to the Intel R⃝Xeon R⃝processor used. We attribute this to the impact
of the algorithmic optimizations such as partitioning techniques, cache optimiza-
tions, and dynamic load balancing (more details are in Sect. 2).
We next demonstrate scalability using the 7 largest datasets in Table 1.
Figure 3 shows the scalability of our algorithm and Intel R⃝MKL. We ignore
CombBLAS for this comparison as it is signiﬁcantly slower (76X slower on aver-
age) and also the GPU implementations due to unavailability of code. Figure 3(a)
and Fig. 3(b) show scaling results for our algorithm and MKL respectively. As
can be seen, we achieve up to 28X speedup using 28 threads with respect to single

Parallel Eﬃcient Sparse Matrix Multicore Matrix-Matrix Multiplication
55
Fig. 3. Scalability of our implementation and Intel R
⃝MKL on real (geomean of largest
4) and synthetic (geomean of largest 3) datasets with {1, 2, 4, 7, 14, 20, 28} threads.
thread performance (geomean 18.2X) on our datasets (for real-world datasets:
max 19.6X, and geomean of 16.4X; for synthetic datasets: max 28X, and geomean
of 20.7X). MKL shows up to 9.9X speedup (geomean of 7.5X) on the same
datasets (for real-world datasets:max 7.3X, and geomean of 6.3X; for synthetic
datasets: max 9.9X, and geomean of 9.5X). The bar at each point shows the stan-
dard deviation on the scalability of our code as observed in our experiments. In
general, scalability is better for dense datasets (mou.gene scales by about 20X
on 28 cores); as also for more skewed datasets (rmat b scales near linearly - 28X
on 28 cores) where a signiﬁcant portion of runtime is spent in dense areas of the
matrix. For such matrices, there is more reuse of data structures and SpGEMM
is more compute bound.
Figure 4(a) shows the timing breakdown in our algorithm. Among the 4 steps
(memory allocation, computation, and input and output data structure conver-
sion), computation is the dominant part and the other 3 take only 0.4 %, 2.6 %,
and 0.3 %, respectively on average. For some datasets such as mou.gene, rmat g
and rmat b, where there are dense regions of computation, as determined by our
metric deﬁned in Sect. 2, we partition columns of B and input data structure
conversion times increase (e.g. rmat g 7.9 %). This is still however small and our
computation times still dominate; overall runtimes for such datasets are up to
1.4X (average 1.22X) faster with column partitioning. We veriﬁed our reasoning
by counting L2 cache misses using hardware counters. Our analysis shows that
for such datasets, L2 cache misses went down by 1.3-2.3X when we performed
column partitioning. This ties in well with our performance gains.
We now discuss the impact of various optimizations described in Sect. 2
(Fig. 4(b)) on our SpGEMM code. We take the hash-based SpGEMM imple-
mentation as the baseline for the comparison. The main performance gain comes
from using a dense array instead of hash tables during the addition phase. This
yields up to 9.2X speedup (with a geomean of 6.3X). This is due to the overheads
of using hash tables as explained in Sect. 2. Increasing the number of partitions
improves performance by up to 49 % (with a geomean of 14.5 %). Using dynamic
scheduling gives an additional 24.9 % (with a geomean of 10.2 %) performance
boost. This is due to better load balancing among the parallel threads.

56
M.M.A. Patwary et al.
Fig. 4. Performance analysis of our implementation
4
Related Work
Gustavson introduced an SpGEMM algorithm with a work complexity propor-
tional to the number of nonzeros in A, number of total ﬂops, number of rows in
C, and the number of columns in C [11]. This algorithm, and how it relates to
our work, is described in detail in Sect. 2. A similar algorithm is used by Matlab,
which processes a column of C at a time, and uses a dense vector with values,
indices, and valid ﬂags, for accumulating sparse partial results [9]. Buluc and
Gilbert address the case of hypersparse matrices, where the number of non-zeros
is less than the number of columns or rows [5]. The authors introduce the doubly
compressed sparse column (DCSC) format, and two new SpGEMM algorithms
to handle this case.
There have also been several eﬀorts to optimize the performance of SpGEMM
for parallel and heterogeneous hardware. Sulatycke and Ghose analyzed the cache
behavior of diﬀerent loop-orderings of SpGEMM with sparse A and B and dense
C, and proposed a cache-eﬃcient parallel algorithm that divided work across
rows of A and C [16]. Siegel et al. created a framework for running SpGEMM on
a cluster of heterogeneous (2x CPU + 2x GPU) nodes, and demonstrated a sig-
niﬁcant improvement in load-balance through dynamic scheduling as compared
to a static approach [15]. Zhu et al. propose a custom hardware implementation
to accelerate SpGEMM [18]. They use custom logic integrated with 3D stacked
memory to retrieve columns from A, and merge intermediate results into C using
content-addressable memories (CAMs). Liu and Vintner describe an SpGEMM
algorithm and implementation for GPUs [13]. In this implementation, rows of C
are divided into bins based on an upper-bound of the size of intermediate results
and processed using diﬀerent routines.
5
Conclusion and Future Work
In this paper we investigated Sparse Matrix-Matrix Multiplication (SpGEMM),
an important kernel used extensively in many applications including linear

Parallel Eﬃcient Sparse Matrix Multicore Matrix-Matrix Multiplication
57
solvers and graph analytics. To improve SpGEMM eﬃciency on multicore plat-
forms, we performed diﬀerent optimization techniques such as using dense arrays,
implementing column-wise partitioning, and dynamic scheduling. We showed
state-of-the-art results on both real-world and synthetic datasets. We are up to
3.8X faster than Intel R⃝MKL and up to 257X faster than CombBLAS. We are
also up to 7.3X better than the best published GPU implementation. Our code
shows good scalability of 18.2X using 28 threads, as compared to MKL that
achieves 7.5X speedup. In the future, we intend to extend these optimizations
in a distributed setup.
References
1. Combinatorial Blas v 1.3. http://gauss.cs.ucsb.edu/∼aydin/CombBLAS/html/
2. Thread aﬃnity interface. https://software.intel.com/en-us/node/522691
3. Intel math kernel library (2015). https://software.intel.com/en-us/intel-mkl
4. Bell, N., Dalton, S., Olson, L.N.: Exposing ﬁne-grained parallelism in algebraic
multigrid methods. SIAM J. Sci. Comput. 34(4), C123–C152 (2012)
5. Buluc, A., Gilbert, J.: On the representation and multiplication of hypersparse
matrices. In: Proceedings of IPDPS, pp. 1–11, April 2008
6. Bulu¸c, A., Gilbert, J.R.: Parallel sparse matrix-matrix multiplication and index-
ing: Implementation and experiments. CoRR abs/1109.3739 (2011)
7. Chan, T.M.: More algorithms for all-pairs shortest paths in weighted graphs.
SIAM J. Comput. 39(5), 2075–2089 (2010)
8. Davis, T.A., Hu, Y.: The university of ﬂorida sparse matrix collection. ACM
Trans. Math. Softw. 38(1), 1:1–1:25 (2011)
9. Gilbert, J., Moler, C., Schreiber, R.: Sparse matrices in matlab: design and imple-
mentation. SIAM J. Matrix Anal. Appl. 13(1), 333–356 (1992)
10. Gilbert, J.R., Reinhardt, S., Shah, V.B.: High-performance graph algorithms from
parallel sparse matrices. In: K˚agstr¨om, B., Elmroth, E., Dongarra, J., Wa´sniewski,
J. (eds.) PARA 2006. LNCS, vol. 4699, pp. 260–269. Springer, Heidelberg (2007)
11. Gustavson, F.G.: Two fast algorithms for sparse matrices: multiplication and
permuted transposition. ACM Trans. Math. Softw. 4(3), 250–269 (1978)
12. Kaplan, H., Sharir, M., Verbin, E.: Colored intersection searching via sparse rec-
tangular matrix multiplication. In: Symposium on Computational Geometry, pp.
52–60. ACM (2006)
13. Liu, W., Vinter, B.: An eﬃcient GPU general sparse matrix-matrix multiplication
for irregular data. In: Proceedings of IPDPS, pp. 370–381. IEEE (2014)
14. Murphy, R.C., Wheeler, K.B., Barrett, B.W., Ang, J.A.: Introducing the graph
500. Cray User’s Group (2010)
15. Siegel, J., et al.: Eﬃcient sparse matrix-matrix multiplication on heterogeneous
high performance systems. In: IEEE Cluster Computing, pp. 1–8 (2010)
16. Sulatycke, P., Ghose, K.: Caching-eﬃcient multithreaded fast multiplication of
sparse matrices. In: Proceedings of IPPS/SPDP 1998, pp. 117–123, March 1998
17. Vassilevska, V., Williams, R., Yuster, R.: Finding heaviest h-subgraphs in real
weighted graphs, with applications. CoRR abs/cs/0609009 (2006)
18. Zhu, Q., Graf, T., Sumbul, H., Pileggi, L., Franchetti, F.: Accelerating sparse
matrix-matrix multiplication with 3D-stacked logic-in-memory hardware. In:
IEEE HPEC, pp. 1–6 (2013)

On the Design, Development, and Analysis
of Optimized Matrix-Vector Multiplication
Routines for Coprocessors
Khairul Kabir1, Azzam Haidar1(B), Stanimire Tomov1, and Jack Dongarra1,2,3
1 University of Tennessee Knoxville, Knoxville, USA
haider@icl.utk.edu
2 Oak Ridge National Laboratory, Oak Ridge, USA
3 University of Manchester, Manchester, UK
Abstract. The manycore paradigm shift, and the resulting change in
modern computer architectures, has made the development of optimal
numerical routines extremely challenging. In this work, we target the
development of numerical algorithms and implementations for Xeon Phi
coprocessor architecture designs. In particular, we examine and opti-
mize the general and symmetric matrix-vector multiplication routines
(gemv/symv), which are some of the most heavily used linear algebra
kernels in many important engineering and physics applications. We
describe a successful approach on how to address the challenges for
this problem, starting with our algorithm design, performance analy-
sis and programing model and moving to kernel optimization. Our goal,
by targeting low-level and easy to understand fundamental kernels, is
to develop new optimization strategies that can be eﬀective elsewhere
for use on manycore coprocessors, and to show signiﬁcant performance
improvements compared to existing state-of-the-art implementations.
Therefore, in addition to the new optimization strategies, analysis, and
optimal performance results, we ﬁnally present the signiﬁcance of using
these routines/strategies to accelerate higher-level numerical algorithms
for the eigenvalue problem (EVP) and the singular value decomposi-
tion (SVD) that by themselves are foundational for many important
applications.
1
Introduction
As the era of computer architectures dominated by serial processors closes, the
scientiﬁc community has produced a consensus for the need to redesign numeri-
cal libraries to meet the new system design constraints and revolutionary levels
of parallelism and heterogeneity. One approach, from the early days of multicore
architectures, was to redesign the higher-level algorithms, e.g., LAPACK [5], to
use tile operations [7–9]. To provide parallelism in these algorithms, the com-
putation is expressed as a Directed Acyclic Graph (DAG) of tasks on small
matrices/tiles with labeled edges designating data dependencies, and a run-
time system schedules the DAG’s execution over the cores to ensure that data
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 58–73, 2015.
DOI: 10.1007/978-3-319-20119-1 5

On the Design, Development, and Analysis of Optimized Matrix-Vector
59
dependancies are not violated. Performance relied on fast sequential implemen-
tations of the Basic Linear Algebra Subprograms (BLAS) interface [13]. When
manycore accelerators entered the HPC ﬁeld, it became apparent that break-
ing the uniformity of the computation is not advantageous for GPUs. Instead,
hybrid approaches were developed [4,17,27,28,30], where there is still a DAG
and scheduling (for both GPUs and CPUs), but SIMD tasks on large data that
are suitable for GPUs, e.g., GEMM, remain coarse grained and are scheduled
as single tasks for parallel execution through parallel BLAS implementations.
This highlighted the interest in parallel BLAS, and subsequently to parallel
BLAS implementations in CUBLAS [1] and MAGMA BLAS [2]. Hybrid appro-
aches are also suitable for the more recent many-core coprocessors, e.g., as is
evident from the MAGMA MIC’s extension of MAGMA for the Xeon Phi
coprocessors [14,19].
The use of batched BLAS [10,11,18,20] as an extension to the parallel BLAS
in many HPC applications is currently the subject of great interest. Batched
algorithms address one of the signiﬁcant challenges in HPC today – that numer-
ous important applications tend to be cast in terms of a solution to many small
matrix operations: they contain the large majority of computations that con-
sist of a large number of small matrices which cannot be executed eﬃciently
on accelerated platforms except in large groups, or “batches”. Indeed, batched
representations of computational tasks are pervasive in numerical algorithms
for scientiﬁc computing. In addition to dense linear algebra routines and appli-
cations, batched LA can naturally express various register and cache blocking
techniques for sparse computations [21], sparse direct multifrontal solvers [31],
high-order FEM [12], and numerous applications in astrophysics [24], hydrody-
namics [12], image processing [25], signal processing [6], and big data, to name
just a few. Moreover, blocking for cache reuse - the most basic technique to accel-
erate numerical algorithms from the fundamental dense matrix-matrix product,
to sparse matrix-vector (SpMV), to more complex linear or eigenvalue solvers –
is often synonymous with a batched representation of algorithms.
To enable the eﬀective use of parallel BLAS and batched BLAS-based compu-
tational approaches, new parallel BLAS algorithms and optimization strategies
must be developed. In this work, we target the development of these foundational
numerical algorithms, optimization strategies, and implementations for the Xeon
Phi coprocessors, also known as Intel’s many integrated core architectures (MIC).
In particular, we examine and optimize the general and symmetric matrix-vector
multiplication routines (gemv/symv), which are some of the most heavily used
linear algebra kernels in many important engineering and physics applications.
Our goal, by targeting low-level, easy to understand fundamental kernels, is to
develop optimization strategies that can be eﬀective elsewhere, and in particular
for batched approaches for HPC applications on manycore coprocessors. There-
fore, we developed new optimization strategies (and analysis) to obtain optimal
performance. Finally, we illustrate the need and the signiﬁcance of using these
routines/strategies to accelerate higher-level numerical algorithms for the EVP
and SVD problems that by themselves are foundational for many important
applications.

60
K. Kabir et al.
2
Background and Related Work
This paper addresses two kernels – the general and the symmetric matrix-vector
multiplications (gemv and symv) – which are crucial for the performance of lin-
ear solvers as well as EVP and SVD problems. A reference implementation for
a generic matrix-vector multiplication kernel is straight-forward because of the
data parallel nature of the computation. However, achieving performance on
accelerators or coprocessors is challenging, as evident from the results on cur-
rent state-of-the-art implementations. For example, even though Intel optimized
dgemv in their recent release of MKL, its performance is highly nonuniform,
reaching up to about 37–40 Gﬂop/s for only particular matrix sizes and data
alignments. Performance, when the matrix size is not a multiple of the cache
line (8 double precision numbers), drops by about 10 Gﬂop/s, or 30 % of the
peak obtained. Furthermore, a sequence of calls to dgemv with “transpose” and
“Non transpose” have shown a drop in the performance as well at about 10
Gﬂop/s. In addition to the issues for the dgemv kernel, the irregular data access
patterns in the symv routine bring further challenges for its design and optimiza-
tion. For example, the current MKL dsymv achieves the same performance as
the dgemv (≈37–40 Gﬂop/s) while in theory it should be twice as fast.
To the best of our knowledge, there has not been other published work on
addressing the acceleration opportunities mentioned for the Xeon Phi architec-
tures. Related algorithmic work, but for GPU architectures, is the acceleration
of the symv routine in MAGMA [26]. The CUBLAS’s symv, similarly to the
MKL’s symv for Xeon Phi, was not exploiting the symmetry of the matrix to
reduce the data traﬃc needed, and as a result was also twice slower than theo-
retically expected. A new algorithm was proposed to correct this for GPUs by
Nath et al. [26], which was later slightly improved for Kepler GPUs using atomic
operations [3].
In this paper, we describe the optimizations performed on both the gemv and
symv routines to make them reach their theoretical peak performances on coproces-
sors. Our gemv kernel is not aﬀected by the matrix size or the sequence of calls. It
achieves uniform performance that matches the peaks of the MKL’s gemv. This
improvement was important to speed up many algorithms, and in particular, the
reduction to bidiagonal form which is a major component for SVD.
An optimality analysis for the symv routines shows (see Sect. 6) that this
kernel should achieve twice the performance of the gemv routine. We developed
an algorithm (and its implementation) that exploits cache memory to read small
blocks of the matrix in cache and reuse them in the computation involving their
symmetric counterparts. This implementation divides the main memory reads
in half, and our experiments show that it reaches to around 50–55 Gﬂop/s for
speciﬁc blocking sizes that allow each small block to ﬁt into the L2 cache of a
corresponding core of the coprocessor. Even though this new symv kernel brings
an excellent improvement over the contemporary MKL, it is still less than what
the performance bound analysis shows as being possible. This motivated us to
look for further improvements that led to the development of a second algorithm
(and its implementation) that reuses the data loaded into the L1 cache level, as

On the Design, Development, and Analysis of Optimized Matrix-Vector
61
well as from the registry, to reach to around 65 Gﬂop/s. We should mention
that both of our symv implementations incur memory overheads of less than
one percent (about 0.78%) of the matrix size. We also show the impact that this
optimization has on the tridiagonal reduction, which is the most time consuming
component of the symmetric eigenvalue problem.
3
Contributions to the Field
The evolution of semiconductor technology is dramatically transforming the bal-
ance of future computer systems, producing unprecedented changes at every level
of the platform pyramid. From the point of view of numerical libraries, and the
myriad of applications that depend on them, three challenges stand out: (1) the
need to exploit unprecedented amounts of parallelism; (2) the need to maximize
the use of data locality; and (3) the need to cope with component heterogeneity.
Besides the software development eﬀorts that we investigate to accomplish an
eﬃcient implementation, we highlight our main contributions related to the algo-
rithm’s design and optimization strategies aimed at addressing these challenges
on the MIC architecture:
Exploit Unprecedented Amounts of Parallelism: Clock frequencies are
expected to stay constant, or even decrease to conserve power; consequently,
as we already see, the primary method of increasing computational capability
of a chip will be to dramatically increase the number of processing units
(cores), which in turn will require an increase of orders of magnitude in the
amount of concurrency that routines must be able to utilize. We developed
MIC-speciﬁc optimization techniques that demonstrate how to use the many
(currently 60) cores of the MIC to get optimal performance. The techniques
and kernels developed are fundamental and can be used elsewhere.
Hierarchical Communication Model that Maximizes the use of Data
Locality: Recent reports (e.g., [16]) have made it clear that time per ﬂop,
network bandwidth (between parallel processors), and network latency are
all improving, but at exponentially diﬀerent rates. So an algorithm that is
computation-bound and running close to peak today may be communication-
bound in the near future. The same holds for communication between levels
of the memory hierarchy. We demonstrate that, related to the latter, perfor-
mance is indeed harder to get on new manycore architectures unless hierar-
chical communications are applied. Hierarchical communications to get top
speed are now needed not only for Level 3 BLAS but also for Level 2 BLAS,
as we show. Only after we developed and applied multilevel cache blocking,
did our implementations reach optimal performance.
Performance Bounds Analysis: We study and demonstrate the maximal
performance bounds that could be reached. The performance bounds allow
us to ascertain the eﬀectiveness of our implementation and how close it
approaches the theoretical limit. We developed and demonstrated this use
of performance bound analysis not only for the low-level kernels considered,
but also for the higher-level algorithms that use them as building blocks.

62
K. Kabir et al.
4
Experimental Testbed
All experiments are done on an Intel multicore system with two 8-core Intel Xeon
E5-2670 (Sandy Bridge) CPUs, running at 2.6 GHz. Each CPU has a 20 MB
shared L3 cache, and each core has a private 256 KB L2 and 64 KB L1 caches.
The system is equipped with 52 GB of memory. The theoretical peak in double
precision is 20.8 Gﬂop/s per core, giving 332 Gﬂop/s in total. The system is
equiped with one Intel Xeon-Phi KNC 7120 coprocessor. It has 15.1 GB, runs
at 1.23 GHz, and yields a theoretical double precision peak of 1, 208 Gﬂop/s.
We used the MPSS 2.1.5889-16 software stack, the icc compiler that comes with
the composer xe 2013 sp1.2.144 suite, and the BLAS implementation from MKL
(Math Kernel Library) 11.01.02 [22].
5
The General Matrix-Vector Multiplication
Routine gemv
Level 2 BLAS routines are of low computational intensity and therefore DLA
algorithm designers try to avoid them. There are techniques that can replace
Level 2 BLAS operations with Level 3 BLAS. For example, in factorizations
like LU, QR, and Cholesky, the application of consecutive Level 2 BLAS opera-
tions that occur in the algorithms can be delayed and accumulated so that, at
a later moment, the accumulated transformation can be applied at once as a
Level 3 BLAS [5]. This approach totally removes Level 2 BLAS from Cholesky,
and reduces its amount to O(n2) in LU and QR, thus making it asymptotically
insigniﬁcant compared to the total O(n3) amount of operations for these factor-
izations. The same technique can be applied to the two-sided factorizations [15],
but in contrast to the one-sided, a large fraction of the total number of ﬂoating
point operations (ﬂops) still remains Level 2 BLAS. For example, the block Hes-
senberg reduction has about 20 % of its ﬂops in Level 2 BLAS, while both the
bidiagonal and tridiagonal reductions have 50 % of their ﬂops in Level 2 BLAS
[29]. In practice, the ﬂops in Level 2 BLAS do not scale well on current architec-
tures and thus can signiﬁcantly impact the total execution time. Therefore the
availability of their eﬃcient implementations is still crucial for the performance
of a two sided factorization in current architectures. This section considers the
Xeon Phi implementation of one fundamental Level 2 BLAS operation, namely
the matrix-vector multiplication routine for general dense matrices (gemv). The
gemv multiplication routine performs one of:
y := αAx + βy,
or
y := αAT x + βy,
(1)
where A is an M by N matrix, x and y are vectors, and α and β are scalars.
5.1
Eﬀect of the Matrix Size on the MKL gemv Performance
The gemv performance peak on the Xeon Phi coprocessor is as expected – achiev-
ing around 37–40 GFlop/s in double precision for both of its transpose and non-
transpose cases, which translate to a bandwidth of about 160 GB/s. Achieving

On the Design, Development, and Analysis of Optimized Matrix-Vector
63
this bandwidth is what is expected on the Xeon Phi coprocessor [23]. How-
ever, this peak performance is obtained only on particular matrix sizes and data
alignments. In reality, applications that rely exclusively on the gemv, e.g., the
bidiagonal reduction (BRD), show much lower performance. Our analysis shows
that in the case of the BRD in particular (see Eq. (7)), performance must be
about twice the performance of the gemv, while experiments show that the BRD
attains less than 37–40 GFlop/s. A detailed analysis of the gemv kernel shows
that its performance indeed highly depends on the location of the data in the
memory, and in particular, on its alignment. We benchmarked gemv on matri-
ces of consecutively increasing sizes from 1 to 27 K, similar to the way that the
BRD reduction calls it. We found out that its performance ﬂuctuates, as shown
in Fig. 1a and b (the blue curves), according to the oﬀset from which the matrix
is accessed. The performance drops by about 15 Gﬂop/s for the transposed case
when the matrix size in the “n” dimension is not a multiple of 240 (as shown
in Fig. 1a) and falls by about 10 Gﬂop/s for the non-transposed case when the
matrix size in the m dimension is not a multiple of 8, as depicted in Fig. 1b. To
resolve the dependance on the memory alignment and the matrix sizes, we devel-
oped two routines (for the transpose and non-transpose cases, respectively) that
always access a matrix from its aligned data, performing a very small amount
of extra work, but keeping its performance stable at its peak. The red curves in
Fig. 1 show our improvement. The algorithms are described in Subsect. 5.3 below.
(a) Performance of dgemv¯T
w/o the proposed virtual
padding.
(b) Performance of dgemv¯N
w/o the proposed virtual
padding.
(c) Performance of a seq-
uence of dgemv¯T and 
dgemv¯N calls.
Fig. 1. Performance obtained from the dgemv routine on matrices of consecutively
increasing sizes (Color ﬁgure online).
5.2
Eﬀect of the Sequence of gemv Calls
After achieving optimal performance for the gemv’s transpose and non-transpose
cases, as described in Sect. 5.1, we tested their use in real-world applications. For
the BRD reduction for example, performance is improved for all sizes and reaches
its theoretical peak for large matrix sizes. However, the performance for small
sizes, in particular less than 8 K, is not as expected. The detailed experiments
depicted in Fig. 1c show that performance of gemv drops by 10 Gﬂop/s when
called in a sequence of non-transpose followed by transpose cases for matrices of

64
K. Kabir et al.
size less than 8 K. We believe that this is related to the diﬀerent parallelization
grid used for each case of gemv (transpose vs. non-transpose), and thus this is
the overhead of switching between the two diﬀerent grids of cores. The overhead
probably always exists for larger sizes, but its eﬀect is less evident because the
cost of the gemv is dominant. To overcome this drawback, we introduce another
optimization technique and use it to develop a new gemv routine, described in
detail in the following sections.
5.3
A New MAGMA MIC gemv
Transpose Matrix Case: The computation of the gemv routine for the trans-
pose case can be parallelized in a one-dimensional (1D) block-column fashion.
In this parallelization model, each thread processes its part of the gemv column
by column, and thus for each column a dot product is performed. The accu-
mulations are done in cache and the ﬁnal, resulting vector y is written once.
Moreover, each thread reads data that is stored consecutively in memory, which
will simplify the prefetching and vectorization process. To get good performance
out of a MIC core, vectorization that takes advantage of the core’s 16-wide SIMD
registers is essential. Each core processes one block (or multiple, if we use 1D
block cyclic distribution). The number of columns in the blocki can be set, for
example, as:
columns in blocki =
N
num blocks + (i < (N%num blocks) ? 1 : 0),
(2)
where num blocks is the number of blocks (e.g., 60 to correspond to 60 cores
of a MIC) that we want N columns to split into, and i = 1, . . . , num blocks is
the block index. We developed parametrized implementations and hand-tested
them at ﬁrst to get insight for the tuning possibilities. For example, one para-
meter is number of threads per core. Figure 2 illustrates a distribution using one
thread per core (displayed as version-1) and four threads per core (displayed as
version-2). In this case, we found that both implementations provide the same
performance. This is due to the fact that the gemv routine is memory bound
and one thread per core is enough to saturate the bandwidth, thus increasing
the number of threads does not aﬀect the performance.
Non-transpose Matrix Case: Following the same strategy used for the trans-
pose approach leads to poor performance for the non-transpose case. This is
because the values of y need to be written multiple times in this case. There-
fore, we can instead parallelize the algorithm in 1D block-row fashion. In this
way each core processes its independent part of the gemv and thus the resulting
vectors can be accumulated in cache and written to the main memory only once.
To keep the blocks cache aligned, their size can be made to be a multiple of
eight. For eﬀective load balance we can think of the matrix as strips of eight,
and divide the strips among the block-rows equally. In this case, the number of

On the Design, Development, and Analysis of Optimized Matrix-Vector
65
Fig. 2. Basic implementation of matrix-vector multiplication on Intel Xeon Phi
rows in blocki can be set as:
m8 strip = (M + 7)/8
rows in blocki = [ m8 strip
num blocks + (i < (m8 strip%num blocks) ? 1 : 0)] × 8
(3)
Dividing rows in block-rows like this has two advantages: ﬁrst, every block except
the last one will have elements that are multiples of eight, which is good for vec-
torization; and second, it helps keep the blocks aligned with the cache sizes which
is essential to reduce memory access time. When the matrix A is not aligned for
the cache size, we can increase the size of the ﬁrst block in order to handle
the unaligned portion, while making all the remaining blocks aligned. Compiler
guided prefetching for this case is not enough to reach the same performance
as for the transpose case. Prefetching to L1 and L2 cache plays an important
role here.
Similarly to the transpose case, using one or four threads per core provides
the same performance. Again, we developed a parametrized implementation
where one parameter is the number of threads per core. Figure 2b, for exam-
ple, illustrates a distribution using one thread per core (displayed as version-1)
and four threads per core (displayed as version-2). The thread processes four
columns together to reduce the write traﬃc for vector y. Before processing the
eight elements (eight is the length of SIMD instruction for double precision), it
prefetches the next eight elements of A from the same column to the L1 cache
level and the next four columns of the same block-row to the L2 cache. In this
way, when the code proceeds to process the next four columns, the data for them
will be obtained from the L2 cache. Processing more than four columns does
not improve the performance. For version-2 each thread handles four columns
together and then the consecutive eight rows from the same column. Like version-
1 each thread will prefetch its portion from the same columns to the L1 cache
and from the next four columns to the L2 cache.
The blocking and prefetching technique for the transpose and non-transpose
cases are described in Figs. 2a and b, respectively.
Figures 3a and b show the performance comparison of magma dgemv vs.
mkl dgemv. In both the transpose and non-transpose cases the techniques pre-
sented yield better performance than the MKL’s dgemvs.

66
K. Kabir et al.
Fig. 3. Performance of MAGMA MIC dgemv vs. MKL on Intel Xeon Phi.
6
Our Proposed Symmetric Matrix-Vector Multiplication
Routine symv
The symv multiplication routine performs:
y := αAx + βy,
(4)
where α and β are scalars, x and y are vectors of size N, and A is an N by N
symmetric matrix.
The performance of the MKL symv routine is as high as the performance of
the gemv routine, and therefore can be further accelerated. Due to the fact that
the symv routine accesses half of the matrix, meaning it needs only half of the
data transfers, its performance (theoretically) should be twice that of the gemv.
The idea behind getting this acceleration is to reuse the data from half of the
matrix to perform the entire multiplication. The traditional way to attain this
objective is to divide the whole matrix into small blocks so that each block ﬁts in
the cache. A symv kernel is used for the diagonal blocks, and for each of the non-
diagonal blocks two calls to the gemv routine are used — one for the transpose
and one for the non-transpose case. For high parallelism without the need for
synchronization, each core handles a block and its resulting vector is written
independently in separate locations. Thus, a summation is taken at the end to
get the ﬁnal y result. As each block is brought to the cache once, this technique
is expected to reach close to the theoretical bound which, as mentioned, is twice
the performance of gemv.
We performed a set of experiments for diﬀerent block sizes. In our timing,
we ignored the overhead of the summation and the launching of the threads. We
illustrate in Fig. 5a the performance we obtained for diﬀerent block sizes. The
maximum performance achieved is around 54 Gﬂop/s for large matrix sizes and
near 50 Gﬂop/s for smaller matrix sizes. When including the time for the sum-
mation, the later results decrease by about 5−10 %. This symv implementation
brings an excellent improvement over the contemporary MKL (e.g., it is about
1.3 times faster). However, the performance is not optimal. This motivated us
to search for other MIC-speciﬁc optimization techniques, leading to our second

On the Design, Development, and Analysis of Optimized Matrix-Vector
67
algorithm and implementation that adds one more level of blocking. In particu-
lar, we manage to reuse data from the L1 cache, which brings the performance
up to the optimal level, i.e., twice the one for gemv.
In order to achieve the desired performance one must optimize both at the
blocking and at the kernel levels. As there are sixty cores in a MIC, we divided
the whole matrix into 60 × 60 blocks. If (i, j) is the index of a block in a two
dimensional grid and block M×block N is the block’s dimension, block M and
block N are computed as follows:
n8 strip = (N + 7)/8
block M = [n8 strip
60
+ (i < (n8 strip%60) ? 1 : 0)] ∗8
block N = [n8 strip
60
+ (j < (n8 strip%60) ? 1 : 0)] ∗8.
(5)
When the size of the matrix A is a multiple of 8, then both block M and block N
are a multiple of eight as well, and all the blocks in the grid are aligned with
the cache. When the size of A is not a multiple of 8, the non-aligned portion is
added to block(0, 0), making all the remaining blocks aligned and of sizes that
are multiples of 8.
Fig. 4. Basic implementation of MAGMA symv on Intel Xeon Phi.
The symv computation is organized according to the description presented
in Fig. 4. Since the diagonal blocks require special attention because their lower

68
K. Kabir et al.
Fig. 5. Performance of MAGMA dsymv on Intel Xeon Phi (Color ﬁgure online).
or upper portion is accessed, and in order to enforce workload balance among
the cores, we split the diagonal blocks over all the cores in a way that provides
load balance. The non-diagonal blocks are also distributed among the cores as
described in Fig. 4 in order to achieve load balance. The number inside each
block indicates which core owns and processes that block. Since the gemv and
the symv are memory bound, we found that one thread per core is the best
conﬁguration.
Each core computes the symmetric matrix-vector multiplication of its block
by performing the gemv N and gemv T together, meaning it loads a column of A
and computes the multiplication for both the non-transpose and transpose cases,
and then moves to the next column. We used the same prefetching technique
as the one used in our gemv kernel for the non-transpose case. We prefetch the
data of a block to the L2 cache and then every column is prefetched to the L1
cache where we perform all computations involving that data. This technique
is illustrated in Fig. 4. The corresponding portions of x and y of the matrix-
vector multiplication of the red block in Fig. 5b are shown in yellow for the non-
transpose operation and in the purple color for the transpose operation. Finally,
the resulting vector yi must be summed, and this is done in parallel using the 60
cores. Figure 5b shows the performance of our MAGMA MIC dsymv along with
a comparison to the performance of the dsymv routine from the MKL Library.
Using the above technique we can achieve almost twice the performance of gemv,
which means that the bound limit for this routine is reached.
7
Impact on Eigenvalue and Singular Value Algorithms
Eigenvalue and singular value decomposition (SVD) problems are fundamen-
tal for many engineering and physics applications. The solution of these prob-
lems follow three phases. The ﬁrst phase involves reducing the matrix to a con-
densed form matrix (e.g., tridiagonal form for symmetric eigenvalue problem,
and bidiagonal form for singular value decomposition) that has the same eigen/

On the Design, Development, and Analysis of Optimized Matrix-Vector
69
Fig. 6. Percentage of the time spent in each of the three phases of the symmetric
eigenvalue and singular value problem
singular-values as the original one. The reductions are referred to as two-sided
factorizations, as they are achieved by two-sided orthogonal transformations.
Then, in the second phase, an eigenvalue (or a singular value) solver further
computes the eigenpairs (or, singular values and vectors) of the condensed form
matrix. Finally, in the third phase, the eigenvectors (or the left and right singu-
lar vectors) resulting from the previous phase are multiplied by the orthogonal
matrices used in the reduction phase. We performed a set of experiments in
order to determine the percentage of time spent in each of these phases for the
symmetric eigenvalue problem and the singular value decomposition problem.
The results depicted in Figs. 6a, and b show that the ﬁrst phase is the most
time consuming portion. It consists of more than 80 % or 90 % of the total time
when all eigenvectors/singular vectors or only eigenvalues/singular values are
computed, respectively. These observations illustrate the need to improve the
reduction phase. It is challenging to accelerate the two-sided factorizations on
new architectures because they are rich in Level 2 BLAS operations, which are
bandwidth limited and therefore do not scale on recent architectures. For that,
we focus in this section on the reduction phase and study its limitation. Fur-
thermore, we present the impact of our optimized kernel when accelerating it on
Intel Xeon-Phi coprocessor architectures.
7.1
Performance Bound Analysis
In order to evaluate the performance behavior of the two-sided factorizations and
to analyze if there are opportunities for improvements, we conduct a computa-
tional analysis of the reduction to condensed forms for the two-sided reductions
(TRD and BRD). The total cost for the reduction phase can be summarized as
follows:

70
K. Kabir et al.
For Tridiagonal:
≈
2
3n3
symv + 2
3n3
Level 3
≈
4
3n3.
For Bidiagonal:
≈
4
3n3
gemv + 4
3n3
Level 3
≈
8
3n3.
According to these equations we derive below the maximum performance Pmax
that can be reached by any of these reduction algorithms. In particular, for large
matrix sizes n, Pmax = number of operations
minimum time tmin , and thus Pmax is expressed as:
For Tridiagonal:
4
3 n3
2
3 n3∗
1
Psymv + 2
3 n3∗
1
PLevel3
2∗PLevel3∗Psymv
PLevel3+Psymv
≈2Psymv
when
PLevel3 ≫Psymv.
(6)
For Bidiagonal:
8
3 n3
4
3 n3∗
1
Pgemv + 4
3 n3∗
1
PLevel3
2∗PLevel3∗Pgemv
PLevel3+Pgemv
≈2Pgemv
when
PLevel3 ≫Pgemv.
(7)
The performance of the Level 2 BLAS routines, such as the matrix-vector
multiplication (symv or gemv), is memory bound and very low compared to the
Level 3 BLAS routines which can achieve close to the machine’s peak perfor-
mance. For example, on the Intel Xeon Phi system the performance of dgemv is
about 40 Gﬂop/s, while for dgemm is about 1000 Gﬂop/s. Thus, one can expect
from Eqs. (6, 7) that the performance of the reduction algorithms are bound by
the performance of the Level 2 BLAS operations. This proves that the perfor-
mance behavior for these algorithms is dictated by the matrix-vector Level 2
BLAS routines, and this is one example of why it is very important to optimize
them.
7.2
Impact on the Tridiagonal Reduction
Figure 7a shows the performance for the tridiagonal reduction using the Xeon
Phi. The MAGMA implementation using the MKL symv routine is much slower
than when using our proposed symv implementation. In particular MAGMA
with the new symv optimization is about 1.6× faster than MAGMA using the
MKL symv, and reaches 78 % of the theoretical performance bound derived from
Eq. 6.
7.3
Impact on the Bidiagonal Reduction
Figure 7b shows the performance for the bidiagonal reduction on the Xeon Phi.
Similarly to the tridiagonal factorization, the MAGMA bidiagonal reduction
using our proposed gemv shows better performance than when using the gemv

On the Design, Development, and Analysis of Optimized Matrix-Vector
71
Fig. 7. Impact of the proposed symv and gemv routine on the reduction algorithms for
eigenvalue and singular value problems.
routine from the MKL library combined with our proposed ﬁx described in
Sect. 5.1. In particular we are reaching 85 % of the theoretical performance bound
that we derived in Eq. 7.
8
Conclusions
We developed MIC-speciﬁc optimization techniques that demonstrate how to use
the many (currently 60) cores of the Intel Xeon Phi coprcessor to obtain optimal
performance. The techniques and kernels developed are fundamental and can
be used elsewhere. For example, we showed that hierarchical communications to
obtain top speed are now needed not only for Level 3 BLAS but also for Level
2 BLAS – indeed, only after we developed and applied multilevel cache block-
ing, our implementations reached optimal performance. Further, the new gemv
kernel handles unaligned general matrices eﬃciently and its use in higher-level
routines, like the bidiagonal reduction, does not require additional optimization
techniques, like padding for example. The impact of our optimizations are clearly
visible in the performance of the bidiagonal reduction. Finally, our new symv is
almost 2× faster than MKL’s symv. Optimization in symv makes the tridiagonal
reduction 1.6× faster than using MKL’s symv.
Acknowledgments. This material is based upon work supported by the National
Science Foundation under Grant No. ACI-1339822, the Department of Energy, and
Intel. The results were obtained in part with the ﬁnancial support of the Russian
Scientiﬁc Fund, Agreement N14-11-00190.
References
1. CUDA Cublas Library. https://developer.nvidia.com/cublas
2. MAGMA. http://icl.cs.utk.edu/magma/

72
K. Kabir et al.
3. Abdelfattah, A., Keyes, D., Ltaief, H.: Systematic approach in optimizing numer-
ical memory-bound kernels on GPU. In: Caragiannis, I., et al. (eds.) Euro-Par
Workshops 2012. LNCS, vol. 7640, pp. 207–216. Springer, Heidelberg (2013)
4. Agullo, E., Augonnet, C., Dongarra, J., Ltaief, H., Namyst, R., Thibault, S.,Tomov,
S.: Faster, cheaper, better - a hybridization methodology to develop linear algebra
software for GPUs. In: Mei, W., Hwu, W. (eds.) GPU Computing Gems, vol. 2.
Morgan Kaufmann, September 2010
5. Anderson, E., Bai, Z., Bischof, C., Blackford, S.L., Demmel, J.W., Dongarra,
J.J., Croz, J.D., Greenbaum, A., Hammarling, S., McKenney, A., Sorensen, D.C.:
LAPACK User’s Guide, 3rd edn. Society for Industrial and Applied Mathematics,
Philadelphia (1999)
6. Anderson, M.J., Sheﬃeld, D., Keutzer, K.: A predictive model for solving small
linear algebra problems in gpu registers. In: IEEE 26th International Parallel Dis-
tributed Processing Symposium (IPDPS) (2012)
7. Bosilca, G., Bouteiller, A., Danalis, A., H´erault, T., Lemarinier, P., Dongarra,
J.: DAGuE: a generic distributed DAG engine for high performance computing.
Parallel Comput. 38(1–2), 37–51 (2012)
8. Buttari, A., Langou, J., Kurzak, J., Dongarra, J.: A class of parallel tiled lin-
ear algebra algorithms for multicore architectures. Parallel Comput. 35(1), 38–53
(2009)
9. Chan, E., Quintana-Orti, E.S., Quintana-Orti, G., van de Geijn, R.: Supermatrix
out-of-order scheduling of matrix operations for smp and multi-core architectures.
In: Proceedings of the Nineteenth Annual ACM Symposium on Parallel Algorithms
and Architectures, SPAA 2007, pp. 116–125. ACM, New York (2007)
10. Dong, T., Haidar, A., Luszczek, P., Harris, A., Tomov, S., Dongarra, J.: LU Fac-
torization of small matrices: accelerating batched DGETRF on the GPU. In:
Proceedings of 16th IEEE International Conference on High Performance and Com-
munications (HPCC 2014), August 2014
11. Dong, T., Haidar, A., Tomov, S., Dongarra, J.: A fast batched Cholesky factor-
ization on a GPU. In: Proceedings of 2014 International Conference on Parallel
Processing (ICPP-2014), September 2014
12. Dong, T., Dobrev, V., Kolev, T., Rieben, R., Tomov, S., Dongarra, J.: A step
towards energy eﬃcient computing: Redesigning a hydrodynamic application on
CPU-GPU. In: IEEE 28th International Parallel Distributed Processing Sympo-
sium (IPDPS) (2014)
13. Dongarra, J., Du Croz, J., Duﬀ, I., Hammarling, S.: Algorithm 679: a set of level
3 basic linear algebra subprograms. ACM Trans. Math. Soft. 16(1), 18–28 (1990)
14. Dongarra, J., Gates, M., Haidar, A., Jia, Y., Kabir, K., Luszczek, P., Tomov,
S.: Portable HPC programming on intel many-integrated-core hardware with
MAGMA port to Xeon Phi. In: Wyrzykowski, R., Dongarra, J., Karczewski, K.,
Wa´sniewski, J. (eds.) PPAM 2013, Part I. LNCS, vol. 8384, pp. 571–581. Springer,
Heidelberg (2014)
15. Dongarra, J.J., Sorensen, D.C., Hammarling, S.J.: Block reduction of matrices to
condensed forms for eigenvalue computations. J. Comput. Appl. Math. 27(1–2),
215–227 (1989). Special Issue on Parallel Algorithms for Numerical Linear Algebra
16. Fuller, S.H., Millett, I. (eds.): The Future of Computing Performance: Game Over
or Next Level?. The National Academies Press, Washington (2011)
17. Haidar, A., Cao, C., Yarkhan, A., Luszczek, P., Tomov, S., Kabir, K., Dongarra,
J.: Uniﬁed development for mixed multi-gpu and multi-coprocessor environments
using a lightweight runtime environment. In: Proceedings of the 2014 IEEE 28th

On the Design, Development, and Analysis of Optimized Matrix-Vector
73
International Parallel and Distributed Processing Symposium, IPDPS 2014, pp.
491–500. IEEE Computer Society, Washington, DC (2014)
18. Haidar, A., Dong, T., Luszczek, P., Tomov, S., Dongarra, J.: Batched matrix com-
putations on hardware accelerators based on GPUs. Int. J. High Perform. Comput.
Appl. February 2015. doi:10.1177/1094342014567546
19. Haidar, A., Dongarra, J., Kabir, K., Gates, M., Luszczek, P., Tomov, S., Jia, Y.:
Hpc programming on intel many-integrated-core hardware with magma port to
xeon phi. Scientiﬁc Programming, 23, January 2015
20. Haidar, A., Luszczek, P., Tomov, S., Dongarra, J.: Towards batched linear solvers
on accelerated hardware platforms. In: Proceedings of the 20th ACM SIGPLAN
Symposium on Principles and Practice of Parallel Programming, PPoPP 2015.
ACM, San Francisco, February 2015
21. Im, E.-J., Yelick, K., Vuduc, R.: Sparsity: Optimization framework for sparse
matrix kernels. Int. J. High Perform. Comput. Appl. 18(1), 135–158 (2004)
22. Intel. Math kernel library. https://software.intel.com/en-us/en-us/intel-mkl/
23. John McCalpin. STREAM: Sustainable Memory Bandwidth in High Performance
Computers. (http://www.cs.virginia.edu/stream/)
24. Messer, O.E.B., Harris, J.A., Parete-Koon, S., Chertkow, M.A.: Multicore and
accelerator development for a leadership-class stellar astrophysics code. In:
Manninen, P., ¨Oster, P. (eds.) PARA. LNCS, vol. 7782, pp. 92–106. Springer,
Heidelberg (2013)
25. Molero, J.M., Garz´on, E.M., Garc´ıa, I., Quintana-Ort´ı, E.S., Plaza, A.: Poster: a
batched Cholesky solver for local RX anomaly detection on GPUs, PUMPS (2013)
26. Nath, R., Tomov, S., Dong, T., Dongarra, J.: Optimizing symmetric dense matrix-
vector multiplication on GPUs. In: Proceedings of International Conference for
High Performance Computing. Networking, Storage and Analysis, Nov 2011
27. Tomov, S., Dongarra, J., Baboulin, M.: Towards dense linear algebra for hybrid
gpu accelerated manycore systems. Parellel Comput. Syst. Appl. 36(5–6), 232–240
(2010)
28. Tomov, S., Nath, R., Ltaief, H., Dongarra, J.: Dense linear algebra solvers for
multicore with GPU accelerators. In: Proceedings of the 2010 IEEE International
Parallel & Distributed Processing Symposium, IPDPS 2010, pp. 1–8. IEEE Com-
puter Society, Atlanta, 19–23 April 2010. http://dx.doi.org/10.1109/IPDPSW.
2010.5470941. doi:10.1109/IPDPSW.2010.5470941
29. Tomov, S., Nath, R., Dongarra, J.: Accelerating the reduction to upper Hessenberg,
tridiagonal, and bidiagonal forms through hybrid GPU-based computing. Parallel
Comput. 36(12), 645–654 (2010)
30. Volkov, V., Demmel, J.: Benchmarking GPUs to tune dense linear algebra. In:
Supercomputing 2008. IEEE (2008)
31. Yeralan, S.N., Davis, T.A., Ranka, S.: Sparse mulitfrontal QR on the GPU. Tech-
nical report, University of Florida Technical Report (2013)

Large-Scale Neo-Heterogeneous Programming
and Optimization of SNP Detection on Tianhe-2
Yingbo Cui1, Xiangke Liao1, Shaoliang Peng1(B), Yutong Lu1, Canqun Yang1,
Bingqiang Wang2, and Chengkun Wu1
1 School of Computer Science, National University of Defense Technology,
Changsha, China
pengshaoliang@nudt.edu.cn
2 National Supercomputing Center in Shenzhen, Shenzhen, China
Abstract. SNP detection is a fundamental procedure in genome analy-
sis. A popular SNP detection tool SOAPsnp can take more than one week
to analyze one human genome with a 20-fold coverage. To improve the
eﬃciency, we developed mSNP, a parallel version of SOAPsnp. mSNP uti-
lizes CPU cooperated with Intel® Xeon PhiTM for large-scale SNP detec-
tion. Firstly, we redesigned the key data structure of SOAPsnp, which
signiﬁcantly reduces the overhead of memory operations. Secondly, we
devised a coordinated parallel framework, in which CPU collaborates
with Xeon Phi for higher hardware utilization. Thirdly, we proposed a
read-based window division strategy to improve throughput and parallel
scale on multiple nodes. To the best of our knowledge, mSNP is the ﬁrst
SNP detection tool empowered by Xeon Phi. We achieved a 45x speedup
on a single node of Tianhe-2, without any loss in precision. Moreover,
mSNP showed promising scalability on 4,096 nodes on Tianhe-2.
Keywords: SNP detection · SOAPsnp · Parallelized algorithm · Xeon
Phi · Many Integrated Core (MIC) Coprocessor · Tianhe-2
1
Introduction
For DNA sequence analysis, reads are segments of DNA sequence generated
by sequencer. Just like string matching in computer science, reads will usually
be mapped back to a reference DNA sequence to locate their positions in the
reference, which are called aligned reads. SNP (Single Nucleotide Polymorphism)
detection is a fundamental and essential process in whole genome analysis. It
takes aligned reads, the reference sequence, and sometimes curated database like
dbSNP [1] as input, detects the information of aligned reads and reference site by
site, and generates a list of SNP sites. Constrained by the memory size, the
reference is usually divided into multiple windows with even size. SNPs are
detected window by window. However, the division may separate one read into
two diﬀerent windows, generating overlapped bases. As a result, the previous
window has to share the information of overlapped bases with the next window
when switching windows.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 74–86, 2015.
DOI: 10.1007/978-3-319-20119-1 6

Large-scale Nero-heterogeneous SNP Detection on Tianhe-2
75
SOAPsnp [2] is a popular SNP detection tool developed by BGI as a member
of its SOAP (Short Oligonucleotide Analysis Package) series analysis tools [3].
The software adopts a Bayesian model to call consensus genotype by carefully
considering the data quality, alignment and recurring experimental errors. All
these information is integrated into a single quality score for each base to measure
the calling accuracy. SOAPsnp usually costs several days to analyze one human
genome with sequencing depth of 20X, which may account for more than 50 %
time at most of a commonly used genome analysis pipeline. The low eﬃciency
calls for a performance boost by advanced computing technologies.
Intel Xeon Phi coprocessor [4] is becoming prevailing with a number of
potential applications in accelerating various computations, such as sparse
matrix-vector multiplication [5], 1D FFT computations [6], Linpack Benchmark
calculation [7], molecular dynamics [8], computational biology [9], and so on.
We performed an in-depth dynamic test of SOAPsnp with gprof [10] and
VTune [11], and located the limiting factors that deter its performance. One of
those is that the core input data (aligned bases) is stored as a highly sparse
matrix, which results in a large amount of redundant computation and huge
overhead of switching windows. Moreover, the current version of SOAPsnp is a
CPU-based single-threaded program although SNP detections between diﬀerent
DNA sites are independent. In this paper, we aim to improve the eﬃciency
of SNP calling algorithm and develop a high performance version of SOAPsnp
utilizing Xeon Phi. The ultimate goal is to apply the improved tool in large-scale
SNP detection of human or other complex species genome.
To realize the above objectives, we proposed a series of optimization
strategies:
(1) We proposed a space-eﬃcient data structure to replace the original ineﬃ-
cient sparse matrix in SOAPsnp. The new structure can dramatically reduce
memory overhead and improves operation eﬃciency.
(2) We transported the Bayesian model to Xeon Phi with oﬄoad mode and
developed a coordinated parallel framework utilizing both CPU and Xeon
Phi.
(3) For large scale parallelism, we proposed a read-based window division (RWD)
strategy, which enables parallel ﬁle reading for diﬀerent processes. RWD
eﬃciently improves the throughput and parallel scale of mSNP.
mSNP is freely available from https://github.com/lemoncyb/mSNP under GPL
license. We evaluated our work on the Tianhe-2 supercomputer [12], where each
compute node (see Table 2) is equipped with two Xeon E5-2692 v2 2.2 GHz CPUs
and three Xeon Phi 31S1P coprocessors. On one compute node of Tianhe-2,
mSNP managed to ﬁnish the analysis of one 20X human genome within two
hours, whereas the original CPU-based SOAPsnp used several days. The soft-
ware maintains promising scalability on 4,096 nodes (98,304 CPU cores and
688,128 MIC cores). Our experiments demonstrated that mSNP is an eﬃcient
and scalable software for large-scale SNP detection of human genome. The details
of evaluation are presented in Sect. 5.

76
Y. Cui et al.
The remaining of this paper is organized as follows. Section 2 presents related
work. Section 3 presents the analysis of our work. Section 4 describes the architec-
ture of mSNP. Performance evaluation is presented in Sect. 5. Section 6 concludes
the paper.
2
Related Work
In this section, we survey most of the popular SNP detection tools and related
optimization work. We also introduce the Intel Xeon Phi coprocessor, which has
been deployed as the primary accelerator on Tianhe-2.
2.1
SNP Detection Tools
SNP detection tools take aligned reads, a reference sequence, in some cases
dbSNP as input to detect SNPs. Web-based tools, such as HaploSNPer [13] and
SNi-Play [14], were deployed on web servers that can be accessed from anywhere
conveniently via a web page. However, the data security and uploading time
prevents them from performing large-scale analysis. Therefore, stand-alone tools
like QualitySNP [15], SAMtools [16], SOAPsnp [2], GATK [17] and Illuminas
Isaac [18] etc. were developed.
SNP detection is time-consuming; as a result, some optimization eﬀorts have
been carried out to improve the performance. Crossbow [19] is a parallel solution
using Hadoop [20] and accelerates detection with cloud computing. Rainbow [21]
optimizes Crossbow for larger sequencing datasets. GSNP [22] accelerates SNP
detection with GPU (graphics processing unit) to achieve better performance.
Mucahid adopts cluster for computation and achieves a good load balance [23].
To the best of our knowledge, mSNP is the ﬁrst SNP detection tool powered by
Intel Xeon Phi.
2.2
Intel Xeon Phi Coprocessor
Intel announced its Xeon Phi coprocessor based on Many Integrated Core (MIC)
architecture in November 2012 [4]. The coprocessor is equipped with 50+ cores
clocked at about 1 GHz and 6 GB or more on-card memory. Each core supports
4 hardware threads. The double precision peak performance of each coprocessor
is higher than 1 TFlops. The architecture of MIC is x86-compatible, which alle-
viates the eﬀorts needed to transport applications to Xeon Phi compared to its
counterpart GPU. Some simple applications can even run directly on Xeon Phi
simply after re-compiling. There are two major modes to employ Xeon Phi in
applications:
(1) native mode, where Xeon Phi has one copy of the application and runs the
application natively like a compute node.
(2) oﬄoad mode, where the application runs as a master thread on CPU and oﬀ-
loads some selected work to Xeon Phi, treating Xeon Phi as a coprocessor [24].

Large-scale Nero-heterogeneous SNP Detection on Tianhe-2
77
read_site 
counting 
likelihood 
posterior 
output 
Processing all sites in a window concurrently  
Process next site 
recycle 
Process next window 
cal_p_mat 
Data flow 
Control flow 
Input files 
p_table 
base_array 
type_likely 
Output files 
Fig. 1. Workﬂow of SOAPsnp. The dash lines illustrate data ﬂow, and the real lines
illustrate control ﬂow.
As mentioned in Sect. 1, more and more applications are accelerated by Xeon
Phi, from basic scientiﬁc computation to biology application [5–9]. Xeon Phi is
showing great potential in parallel computing.
3
Performance Proﬁling Analysis of SOAPsnp
In this section, we will present our analysis of the workﬂow and bottleneck
proﬁling of SOAPsnp.
3.1
Workﬂow of SOAPsnp
Figure 1 illustrates the workﬂow of SOAPsnp. SOAPsnp takes aligned reads, a
reference genome, and in some cases dbSNP as input. The output is consensus
genotype information. SOAPsnp mainly contains seven modules: cal p mat,
read site, counting, likelihood, posterior, output, and recycle (italics bold
font represents function module, italics font represents data structure). The core
data structures include p matrix, base info, and type likely (the oval block of
Fig. 1).
cal p mat module takes input reads and generates a calibration matrix
p matrix for likelihood computation. p matrix is a four-dimensional matrix
(256 × 256 × 4 × 4 = 1, 048, 576) with a size of 8 MB. Constrained by the mem-
ory size, SOAPsnp divides the reference into multiple windows with even size.
In each window, SNP calling is performed site by site. The read site module
loads a ﬁxed number of sites (a window size) from input ﬁle. Then counting
collects the information of aligned bases for each site and stores the information
in base info. likelihood takes base info and p matrix as inputs, calculates the
likelihood and stores it in type likely. After posterior calculation, calling results
of one site are written to the output ﬁle. Then the next site of the current window
will be processed from likelihood too. The recycle module switches windows
by dealing with the overlapped bases and re-initializes buﬀers for new window.
base info is a four dimensional matrix (4 × 64 × 256 × 2, corresponding to
base × score × coord × strand), storing the information of bases aligned to each

78
Y. Cui et al.
Table 1. Time breakdown of SOAPsnp
Module cal p matrix read site couting likelihood posterior recycle
output
Time/s 16.73
2.35
40.59
1478.36
23.47
210.19
752.98
%
0.66 %
0.09 %
1.59 %
57.93 %
0.92 %
29.50 % 8.42 %
DNA site. The dimensions stand for four aspects of an aligned base: the base type,
the sequencing quality score, the coordinates on read and the strand of read.
3.2
Bottleneck Proﬁling
To identify the performance bottleneck of SOAPsnp, we analyzed the code of
SOAPsnp and divided it into seven main modules, as described in Subsect. 3.1.
Then we timed each module and obtained a time breakdown, as listed in Table 1.
likelihood is the most time-consuming module, which takes about 58 % of the
total processing time. The second is recycle taking 30 % percentage. output is
ranked third with 8.4 %. Then we investigated further with Intel VTune [11] to
detect the most time-consuming operations in likelihood and recycle. Further
code analysis shows large amounts of these operations are memory accesses, espe-
cially the accesses to base info data structure storing information of aligned reads.
As mentioned in Subsect. 3.1, likelihood traverses base info to fetch aligned
reads. recycle module copies the information of aligned reads across adjacent win-
dows. The optimization strategy to base info is described in Subsect. 4.1.
4
Design and Implementation of mSNP
In this section, we will describe the design and implementation of mSNP in
detail.
4.1
Consolidating Sparse Matrix
SOAPsnp detects SNP site by site. For each site computation, every element of
base info will be accessed exactly once (including zero elements), which would
generate a total number of 131,072(= 4 × 64 × 256 × 2) memory accesses. That
is, 393 trillions memory accesses will be made for a whole human genome with
about 3 billion sites.
One notable fact is that, base info is a highly sparse matrix. Each element
of base info stands for a combination of four dimensions (base × score × coord ×
strand) and is initialized with zero. The element value will increase by one if a
base in a read matches the combination. However, sequencing depth is usually
smaller than 100X, and the bases in one human genome are relatively ﬁxed,
so most elements in base info are zero. We tested several human genomes and
found that only less than 0.08 % of base info elements are non-zero. This means
that most memory accesses to the matrix are in vain.

Large-scale Nero-heterogeneous SNP Detection on Tianhe-2
79
17 bits
base: 2 bits
score: 6 bits
coord: 8 bits
strand: 1 bit
Fig. 2. Bit composition of base array.
To reduce the amount of unnecessary memory accesses, we design a space-
eﬃcient data structure base array to store the information of each DNA site.
The base array only stores the dimensions information of non-zero elements.
As illustrated in Fig. 2, the four dimensions (base × score × coord × strand) are
integrated into 17 bits in one word (32 bits) by bit operations. For repeated bases,
base array stores multiple copies of the same coordinates. Thus, all information
is maintained and the space complexity of base info is signiﬁcantly reduced, as
the percentage of non-zero elements of base info is only 0.08 %.
By analyzing the source code, we discovered that recycle module produces
a large amount of memory copy operations when switching windows, especially
the copy of base info with a size of 13MB. With the introduction of base array,
the cost of windows switching is reduced by three orders of magnitude too.
4.2
Coordinated Parallelism Between CPU and Xeon Phi
As described in Subsect. 3.1 and illustrated in Fig. 1, SOAPsnp divides the SNP
calling of one genome into multiple windows. In each window, SNP detection is
performed site by site. Theres no dependency between sites. We parallelize the
procedure by multi-threading on both CPU and Xeon Phi, where each thread
handles one site.
Due to Xeon Phi’s weak ability of ﬁle operation in native mode, mSNP
adopts the oﬄoad mode of Xeon Phi. In the naive oﬄoad mode, CPU will have
to pause and wait for the results to be returned from Xeon Phi, which results
in a waste of the CPU computing power. In mSNP, we make the data transfer
between CPU and Xeon Phi asynchronous, which allows CPU to take on other
job immediately after launching the oﬄoad region, as illustrated in Fig. 3. When
the oﬄoad region is ﬁnished, CPU retrieves results from Xeon Phi and resumes
other operations.
4.3
Collaborated Parallel Window Division
As described in Subsect. 4.2, mSNP parallelizes the SNP detections of diﬀerent
sites in a window with multi-thread. While the computing power of one com-
puting node is limited, to achieve higher performance, we have to parallelize
SOAPsnp across nodes.
SOAPsnp performs SNP calling window by window. One straightforward
strategy to parallelize SOAPsnp across nodes is to assign each node at least one
window. Diﬀerent nodes call SNPs of diﬀerent windows simultaneously.

80
Y. Cui et al.
CPU0 + 
CPU1
MIC0
MIC1
MIC2
Data in 
CPU-MIC coordinate
Data out
Fig. 3. Coordinated parallelism between CPU and Xeon Phi. Data in stands for CPU
transfers input data to MICs. CPU-MIC coordinate means CPU and MIC perform
computation simultaneously. Data out denotes MICs transfer results back to CPU.
SOAPsnp divides windows evenly according to the coordinates of base on ref-
erence sequence, denoted as coordinate-based window division (CWD) as illus-
trated in Fig. 4. The coordinate of base is stored in the aligned reads ﬁle, as
attached information to each read. The coordinate is not known until the read
sequence is loaded into memory. It’s impossible to locate a read with given non-
zero coordinate beforehand. That is to say, it’s impossible for diﬀerent processes
to load reads from the start of any window simultaneously. To parallelize the
SNP calling of diﬀerent windows, we have to choose a master-slave mode. The
master process loads reads into memory sequentially, prepares base informa-
tion for each window, and sends window task to idle slave processes. For the
master-slave mode, the throughput and parallel scale are limited by the master
process, where all input data come from. Diﬀerent processes cannot load reads
simultaneously.
Reference sequence 
Aligned reads 
Window 0 
Window 1 
Window 2 
Window …… 
Fig. 4. Coordinate-based window division of SOAPsnp. The dash lines denote aligned
reads. The real zones of dash lines represent the bases that belong to the next window,
but are loaded by the previous window, that is overlapped bases.
To improve the parallel scale across nodes, we designed a read-based window
division (RWD) strategy. As illustrated in Fig. 5, windows are divided by the
number of reads, each window containing almost equal number of reads. As
each aligned read occupies four lines in ﬁle, the RWD strategy actually divides
windows by ﬁle lines. Diﬀerent processes can load reads from diﬀerent lines of
input ﬁle simultaneously.

Large-scale Nero-heterogeneous SNP Detection on Tianhe-2
81
Another problem of RWD strategy is to deal with overlapped reads which
belong to the next window, but are loaded by the previous window, responding
to the real lines in Figs. 4 and 5. SOAPsnp detects SNP site by site. In order to
maintain the completeness of each site, the information of overlapped bases has
to be transferred to the next window before the next window launches. To realize
this, two adjacent processes Pn and Pn+1 have to send one message to each other.
As illustrated in Fig. 5, Pn+1 sends the position of its ﬁrst site Pos1 to Pn, Pn
sends the information of sites after Pos1 (overlapped bases) to Pn+1. Diﬀerent
processes accomplish loading step evenly at the same time, because the number
of assigned reads is evenly equal. Then all processes communicate with each
other at the same time. When communication ﬁnishes, all processes start SNP
calling even simultaneously. Based on the above description, the throughput and
parallel scale of mSNP improve eﬃciently compared with SOAPsnp. Moreover,
for each process is assigned evenly equal number of reads, diﬀerent processes can
get a better load balance.
Reference sequence 
Aligned reads 
Window 0 
Window 1 
Window 2 
Window …… 
Fig. 5. Read-based window division (RWD) of mSNP. The dash lines illustrate aligned
reads. The real zones of dash lines mean the bases that belong to the next window,
but are read by the previous window, that is overlapped bases.
5
Evaluation
We evaluated the performance of mSNP from four aspects: eﬀectiveness of space-
eﬃcient format base array, CPU and Xeon Phi cooperation, RWD performance
and scalability.
5.1
Experimental Setup
We evaluated our work on the Tianhe-2 supercomputer in the National Super
Computer Center in Guangzhou (NSCC-GZ) [12]. The conﬁguration of Tianhe-2
is described in Table 2. The whole system consists of 16,000 compute nodes.
The latest version of SOAPsnp is v1.03, which is available from it’s web-
site1. SOAPsnp v1.03 is a CPU-based single thread program. In consideration of
1 SOAPsnp website: http://soap.genomics.org.cn/soapsnp.html.

82
Y. Cui et al.
equality, we chose diﬀerent baselines for diﬀerent evaluations, and the details are
described in the subsections. We prepared three datasets for evaluation: 3.2 GB,
73 GB and 542 GB. The details are described in the subsections too.
Table 2. Conﬁguration of Tianhe-2’s compute node
Xeon® E5-2692 v2 CPU Xeon® PhiTM 31S1P
Sockets × Cores × Threads 2 × 12 × 2
3 × 57 × 4
Clock Frequency (GHz)
2.20
1.10
L1/L2/L3 Cache (KB)
32/256/30,720
32/512/-
Memory Size (GB)
64
6
5.2
Dimension Reduction of Sparse Matrix
We adopted SOAPsnp v1.03 as baseline in this evaluation and the optimized
version is also CPU-based single thread. The size of test data is 3.2 GB. Figure 6
shows the time consumptions of likelihood and recycle before and after opti-
mization on one node of Tianhe-2. Our space-eﬃcient new representation for-
mat base array outperforms base info 32+ times in likelihood and 56+ times in
recycle function. base array stores only non-zero elements to avoid unnecessary
memory accesses.
1478.36 
752.98 
45.57 
13.29 
0 
200 
400 
600 
800 
1000 
1200 
1400 
1600 
likelihood 
recycle 
Time/s
base_info 
base_array 
Fig. 6. Performance of likelihood and recycle before and after optmization
Table 3 shows the time breakdown of the CPU-based optimized single thread
version of SOAPsnp. The main two modules optimized are likelihood and recy-
cle. After optimization, for a 3.2 GB dataset, the percentage of likelihood is
5.10 %, and that of recycle is 1.49 %. The two modules are no longer the bottle-
necks. output becomes the most time-consuming module taking more than 84 %
and turns into the big bottleneck. It’s hard to parallelize output in multi-threads,
while possible in multi-processes.

Large-scale Nero-heterogeneous SNP Detection on Tianhe-2
83
Table 3. Time breakdown of CPU-based optimized version of SOAPsnp
Module cal p matrix read site couting likelihood posterior recycle output
Time/s
17.05
2.13
41.21
45.57
22.64
13.29
750.88
%
1.91 %
0.24 %
4.62 %
5.10 %
2.54 %
1.49 %
84.11 %
830 
840 
850 
860 
870 
880 
890 
900 
1 
2 
4 
8 
12 
16 
24 
32 
48 
Time/s
Number of threads on CPU
(a)
845 
850 
855 
860 
865 
870 
875 
880 
56 
112 
168 
224 
Time/s
Number of threads on Xeon Phi
(b)
Fig. 7. (a) Performance of mSNP on CPU. (b) Performance of mSNP on Xeon Phi.
5.3
CPU and Xeon Phi Cooperation
mSNP supports coordinated computation between CPUs and MICs. We tested
mSNP under diﬀerent number of CPUs and MICs on one node of Tianhe-2. To
determine the proper number of threads launched in CPU and MIC, we evaluated
the performance of mSNP with the number of threads varying ﬁrst. We adopted
3.2 GB dataset in the tests. Figure 7(a) illustrates the performance of mSNP
on CPU. There are two sockets 12-core CPUs in each compute node. The time
decreases with the number of threads increasing, and the peak performance
is obtain in 24 threads, 1 core assigned with 1 thread. After 24 threads, the
performance drops gradually. Figure 7(b) shows the performance on Xeon Phi.
The peak performance is obtained in 224 threads, 1 core assigned with 4 threads.
Thus, we launched 24 threads on CPUs and 224 threads on one Xeon Phi for
the later tests.
The smooth varieties in Fig. 8 are contributed by the big proportion of output
in mSNP (Table 3). It’s hard to parallelize output in multi-thread. To make
the illustration for CPU-Phi cooperation distinct, we chose 73 GB dataset and
presented the time of likelihood only, because other modules of mSNP are not
parallelized by multi-thread. Figure 8(a) shows the performance of likelihood for
CPUs cooperating with Phis. As illustrated, for SNP detection, the computing
power of one Xeon Phi corresponds to that of two CPUs in Tianhe-2. The high
accelerator speedup comes from the massively parallelism on Xeon Phi. There’s
no other communication, except for transferring input data to Phi and getting
results from Phi to CPU. Thus, the performance increases nearly linearly as the
number of Phi increases.

84
Y. Cui et al.
1033.42 
584.95 
358.14 
214.89 
0 
200 
400 
600 
800 
1000 
1200 
2CPUs 
2CPUs+1MIC 
2CPUs+2MICs 2CPUs+3MICs 
Time/s
(a)
0 
2 
4 
6 
8 
10 
12 
14 
0  
50  
100  
150  
200  
250  
300  
350  
1 
2 
4 
8 
16 
Strong scale speedup
Time/min
Number of nodes
Time-RWD 
Time-CWD 
Speedup-RWD 
Speedup-CWD 
(b)
Fig. 8. (a) Performance of likelihood. (b) Time and speedup of RWD vs. CWD.
0 
5 
10 
15 
20 
25 
30 
0  
2,000  
4,000  
6,000  
8,000  
10,000  
12,000  
14,000  
16,000  
128 
256 
512 
1024 
2048 
4096 
Strong scale speedup
Time/s
Number of nodes
2CPUs+3MICs 
2CPUs+2MICs 
2CPUs+1MIC 
2CPUs 
1CPU 
2CPUs+3MICs 
2CPUs+2MICs 
2CPUs+1MIC 
2CPUs 
1CPU 
Fig. 9. Strong scale speedup of mSNP
5.4
RWD Performance
Figure 8(b) shows the performance and strong scale speedup of RWD vs. CWD
from 1 to 16 nodes on Tianhe-2. All two CPUs with 24 threads and three MICs
with 224 threads per Phi are used in each compute node. The size of dataset is
73 GB. RWD and CWD achieve evenly equal speedup before 4 nodes. After 4
nodes, the performance of RWD exceeds CWD more and more. The speedup of
CWD increases slower and slower after 4 nodes. CWD only obtains about 4.2
folds speedup on 16 nodes, while RWD achieves about 13 folds speedup, which is
over 3 folds faster. The good scalability is contributed by RWD’s parallel reading
capability.
5.5
Scalability
We tested the scalability of mSNP from 128 nodes to 4,096 nodes on Tianhe-
2. Figure 9 presents the strong scale speedup of mSNP. The dash lines indi-
cate speedup and the real lines indicate time. The size of test data is 542 GB.

Large-scale Nero-heterogeneous SNP Detection on Tianhe-2
85
For a better presentation, we used the performance achieved on 128 nodes as
baseline. The lines in the ﬁgure stand for the number of processors (CPU,
MIC) used in each compute node of Tianhe-2. We observed a speedup of about
27.5 from 128 nodes to 4,096 nodes with 2CPUs+3MICs, and a speedup of
24 with 2CPUs+2MICs, a speedup of about 19 with 2CPUs+1MIC, about 15
folds speedup with 2CPUs, about 11.5 folds speedup with 1CPU. These results
demonstrate a promising result for strong scalability on the large scale CPU-MIC
heterogeneous system, Tianhe-2.
6
Conclusion
In this paper, we presented mSNP, which a large-scale parallel SNP detection
tool accelerated by Intel Xeon Phi. Firstly, we proposed a space-eﬃcient repre-
sentation format that can substantially reduces the amount of memory accesses
and overhead of switching windows. Secondly, we developed a coordinated paral-
lel framework using CPU and Xeon Phi, which optimized hardware utilization.
Thirdly, we proposed a read-based window division strategy to improve data
throughput and parallel scale across nodes. We evaluated our work on Tianhe-2
supercomputer. It achieves about 45x speedup on one node and exhibits strong
scalability on 4,096 nodes. The algorithm optimization, parallelization on both
CPU and Xeon Phi lead to a signiﬁcant reduction of computing time.
Acknowledgments. We would like to thank Mr. Yingrui Li from BGI for provid-
ing the source code of SOAPsnp and Dr. Jun Wang from BGI for providing related
test data. We would also like to thank Prof. Hans V. Westerhoﬀfrom University
of Manchester for discussions of the human genome re-sequencing analysis problem
and thus improving our own understanding. This work is supported by NSFC Grant
61272056, U1435222, 61133005, 61120106005, 91430218 and 61303191.
References
1. National Center for Biotechnology Information. http://www.ncbi.nlm.nih.gov/
SNP/
2. Li, R., Li, Y., Fang, X.: SNP detection for massively parallel whole-genome rese-
quencing. Genome Res. 19(6), 1124–1132 (2009)
3. Short Oligonucleotide Analysis Package Sites. http://soap.genomics.org.cn/index.
html
4. James, J., Reinders, J.: Intel Xeon Phi Coprocessor High Performance Program-
ming. Morgan Kaufmann, Newnes (2013)
5. Liu, X., Smelyanskiy, M., Chow, E., Dubey, P.: Eﬃcient sparse matrix-vector mul-
tiplication on x86-based many-core processors. In: Proceedings of the 27th Inter-
national ACM Conference on International Conference on Supercomputing, pp.
273–282. ACM (2013)
6. Park, J., Bikshandi, G., Vaidyanathan, K., Tang, P.T.P., Dubey, P., Kim, D.:
Tera-scale 1D FFT with low-communication algorithm and Intel R
⃝Xeon PhiTM
coprocessors. In: Proceedings of SC13: International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis, p. 34. ACM (2013)

86
Y. Cui et al.
7. Heinecke, A., Vaidyanathan, K., Smelyanskiy, M., Kobotov, A., Dubtsov, R. et al.:
Design and implementation of the linpack benchmark for single and multi-node
systems based on intel R
⃝Xeon Phi coprocessor. In: 2013 IEEE 27th International
Symposium on Parallel & Distributed Processing (IPDPS), pp. 126–137. IEEE
(2013)
8. Pennycook, S.J., Hughes, C.J., Smelyanskiy, M., Jarvis, S.A.: Exploring SIMD
for molecular dynamics, using intel R
⃝Xeon R
⃝processors and intel R
⃝Xeon Phi
coprocessors. In: 2013 IEEE 27th International Symposium on Parallel & Distrib-
uted Processing (IPDPS), pp. 1085–1097. IEEE (2013)
9. Misra, S., Pamnany, K., Aluru, S.: Parallel mutual information based construction
of whole-genome networks on the intel R
⃝Xeon PhiTM coprocessor. In: 2014 IEEE
28th International Parallel and Distributed Processing Symposium, pp. 241–250.
IEEE (2014)
10. Graham, S.L., Kessler, P.B., McKusick, M.K.: Gprof: a call graph execution pro-
ﬁler. ACM SIGPLAN Not. 39(4), 49–57 (2004)
11. Wikipedia Sites of VTune. http://en.wikipedia.org/wiki/VTune
12. TOP500 Supercomputer Sites. http://www.top500.org/system/177999
13. Tang, J., Leunissen, J.A.M., Voorrips, R.E.: HaploSNPer: a web-based allele and
SNP de-tection tool. BMC Genet. 9(1), 23 (2008)
14. Dereeper, A., Nicolas, S., Le Cunﬀ, L.: SNiPlay: a web-based tool for detection,
management and analysis of SNPs. Application to grapevine diversity projects.
BMC Bioinform. 12(1), 134 (2011)
15. Tang, J., Vosman, B., Voorrips, R.E.: QualitySNP: a pipeline for detecting single
nucleotide polymorphisms and insertions/deletions in EST data from diploid and
polyploid species. BMC Bioinform. 7(1), 438 (2006)
16. Li, H., Handsaker, B., Wysoker, A.: The sequence alignment/map format and
SAMtools. Bioinformatics 25(16), 2078–2079 (2009)
17. DePristo, M.A., Banks, E., Poplin, R.: A framework for variation discovery and
genotyping using next-generation DNA sequencing data. Nature Genet. 43(5),
491–498 (2011)
18. Raczy, C., Petrovski, R., Saunders, C.T.: Isaac: ultra-fast whole-genome secondary
analysis on Illumina sequencing platforms. Bioinformatics 29, 2041–2043 (2013).
btt314
19. Langmead, B., Schatz, M.C., Lin, J.: Searching for SNPs with cloud computing.
Genome Biol. 10(11), R134 (2009)
20. Shvachko, K., Kuang, H., Radia, S., The hadoop distributed ﬁle system. IEEE 26th
Symposium on Mass Storage Systems and Technologies (MSST), pp. 1–10. IEEE
(2010)
21. Zhao, S., Prenger, K., Smith, L.: Rainbow: a tool for large-scale whole-genome
sequencing data analysis using cloud computing. BMC Genomics 14(1), 425 (2013)
22. Lu, M., Zhao, J., Luo, Q.: GSNP: a DNA single-nucleotide polymorphism detec-
tion system with GPU acceleration. In: 2011 International Conference on Parallel
Processing (ICPP), pp. 592–601. IEEE (2011)
23. Kutlu, M., Agrawal, G.: Cluster-based SNP calling on large-scale genome sequenc-
ing data. In: 2014 14th IEEE/ACM International Symposium on Cluster, Cloud
and Grid computing (CCGrid), pp. 455–464. IEEE (2013)
24. Cui, Y., Liao, X., Zhu, X.: mBWA: a massively parallel sequence reads aligner. In:
Saez-Rodriguez, J., Rocha, M.P., Fdez-Riverola, F., De Paz Santana, J.F. (eds.)
PACBB 2014. AISC, vol. 294, pp. 113–120. Springer, Heidelberg (2014)

ACCOLADES: A Scalable Workﬂow Framework
for Large-Scale Simulation and Analyses of
Automotive Engines
Shashi M. Aithal(B) and Stefan M. Wild
Argonne National Laboratory, Argonne, IL 60439, USA
{aithal,wild}@anl.gov
Abstract. Analysis and optimization of simulation-generated data have
myriads of scientiﬁc and industrial applications. Fuel consumption and
emissions over the entire drive cycle of a large ﬂeet of vehicles is an exam-
ple of such an application and the focus of this study. Temporal variation
of fuel consumption and emissions in an automotive engine are functions
of over twenty variables. Determining relationships between fuel con-
sumption or emissions and the dependent variables plays a crucial role
in designing an automotive engine. This paper describes the develop-
ment of ACCOLADES (Advanced Concurrent COmputing for LArge-
scale Dynamic Engine Simulations), a scalable workﬂow framework that
exploits the task parallelism inherent in such analyses by using large-scale
computing. Excellent weak scaling is observed on 4,096 cores of both an
Intel Sandy Bridge-based cluster and a Blue-Gene/Q supercomputer.
Keywords: Workﬂow management · Industrial simulations · Large-
scale vehicle simulations · Task parallelism
1
Introduction
Discrete time series occur in many scientiﬁc and industrial applications [7,9,11,13].
Examples of these applications include solar radiation, temporal variations of the
load requirements on a power-grid, temperature variation of power-generating
equipment and variation in the price of a commodity, among others. An observed
value of a time series is typically a function of several variables; developing an
understanding of the eﬀect of the variables on the observed value is a computa-
tionally intensive task. Furthermore, optimization of time-averaged or integrated
values of these functions often requires analyses of large datasets.
Fuel consumption and emissions over the entire drive cycle of a large ﬂeet
of cars is an example of such a problem, and hence the focus of this study.
Temporal variation of fuel consumption and emissions in an automotive engine
This material is based upon work supported by the U.S. Department of Energy,
Oﬃce of Science, under Contract DE-AC02-06CH11357.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 87–95, 2015.
DOI: 10.1007/978-3-319-20119-1 7

88
S.M. Aithal and S.M. Wild
 18
 18.2
 18.4
 18.6
 18.8
 19
 19.2
 19.4
 0
 200
 400
 600
 800
 1000
 1200
 1400
Fuel mass (mg)
Time (s)
 6000
 6250
 6500
 6750
 7000
 7250
 7500
 0
 200
 400
 600
 800
 1000
 1200
 1400
NO (ppm)
Time (s)
 300
 350
 400
 450
 500
 0
 200
 400
 600
 800
 1000
 1200
 1400
CO (ppm)
Time (s)
Fig. 1. Typical time series data for fuel mass (top), NO emissions (middle), and CO
emissions (bottom) as generated by pMODES over a single drive cycle.
can be functions of over twenty independent variables, including engine speed
(i.e., RPM), torque, type of fuel/additive, air-to-fuel ratio, ambient temperature,
inlet pressure, humidity, ignition and valve timings, and driving conditions (e.g.,
city or highway). Deriving correlations between the observed values (such as fuel
consumption and emissions) and the independent variables plays a crucial role
during the design and development stages of an automotive engine. For instance,
a study on the eﬀect of the inﬂow air temperature on the fuel consumption and
(NO, CO, soot, and unburned hydrocarbons) emissions might consist of four dif-
ferent drive cycles and ﬁve diﬀerent temperatures (e.g., expressed as a percentage
of the nominal temperature). Such a study would result in twenty diﬀerent time
series for fuel consumption and eighty time series for emissions (i.e., one for each
type of emission). A typical drive cycle of an automotive engine has a duration
of 25–30 min; for data sampled every second, one obtains approximately 1,500
data points per drive cycle. Dynamometer testing and measurements (called
“dyno testing”) are usually conducted to obtain data for engine performance
and emissions. Numerical simulations can be used to complement dyno data or
to estimate engine performance and emissions during the engine design process.
Figure 1 shows an instance of the temporal variation of fuel ﬂow along with
the computed temporal variation of nitric oxide (NO) and carbon monoxide
(CO) emissions. For each sampled data point, which represents one (compression
and expansion) engine cycle, engine state variables (e.g., temperature, pressure,
andfuel-air mixture combination) are computed over 360 crank angle degrees
(CAD) in intervals of roughly 0.5 CAD. These engine state variables are needed
in order to compute the engine performance (e.g., torque and power) and engine-
out emissions (e.g., NO and CO). Hence, each drive cycle requires the evaluation
of over a million engine (≈1,500 × 720) CAD.

ACCOLADES: Simulation and Analysis of Automotive Engines
89
Fig. 2. Block diagram showing the structure of ACCOLADES.
The above example represents a simpliﬁed case wherein the eﬀect of variation
of a single parameter on the engine performance and emissions is studied. Typ-
ical ﬂeet studies require the simultaneous variation of multiple design variables
over speciﬁed ranges for a larger number of drive cycles, resulting in a large set
of input conﬁgurations. For instance, if one were to consider the eﬀect of varia-
tion of four design parameters (e.g., inlet pressure, inlet temperature, humidity,
and engine RPM) with four diﬀerent values for each of these design parameters
over sixteen diﬀerent drive cycles, one would need (44 × 16 = 4, 096) diﬀerent
independent cases. Each of these 4,096 cases would require 1,500 engine-cycle
evaluations. Conducting large parametric sweeps on the drive cycles of a ﬂeet of
cars with varying combinations of operating conditions places stringent demands
on the required computational resources. Furthermore, analyses of the results of
these large-scale simulations present signiﬁcant challenges from a data-analytics
standpoint. Transient multidimensional numerical simulation of a single engine
cycle (360 CAD) running on 24 to 48 cores (approaching the strong scaling limit
for physically meaningful grid sizes) can take several hours to days, depending
on the complexity of the physical models used. Hence, conducting multi-cycle
simulations for the scenario described above would require enormous computa-
tional resources, and thus precluding their use for initial design/development
studies or analyses of large transients.
Physics-based reduced-order models, which capture the temporal variation,
for example, of average engine temperature, pressure, and mixture composition,
are ideally suited for such large-scale studies. Given the wide range of operating
conditions (engine speed, load, equivalence ratio, etc.) the reduced-order models
have to be robust and fast in order to compute emissions and performance at
real-time speeds. Real-time analysis would require a typical data point in any
given drive cycle to be computed in approximately 250–30 ms.
This paper describes the development of ACCOLADES (Advanced Con-
current COmputing for LArge-scale Dynamic Engine Simulations), a scalable
workﬂow management framework that enables automotive design engineers to
exploit the task parallelism inherent in the study of such systems using large-
scale computing (e.g., GPGPUs, multicore architectures, or the cloud). As shown

90
S.M. Aithal and S.M. Wild
in Fig. 2 and detailed in Sect. 2, ACCOLADES consists of two main compo-
nents, pMODES (parallel Multi-fuel Otto Diesel Engine Simulator) and TADA
(Toolkit for Advanced Data Analytics). pMODES is a fast, robust, physics-based
reduced-order engine simulator that can concurrently compute the performance
and emissions of the various parametric cases required for a vehicle ﬂeet simula-
tion. TADA is a data analytics toolbox used to post-process the results generated
by pMODES or directly from dyno data.
Although large-scale system-level optimization has been performed for mili-
tary vehicles [5,10], to the author’s knowledge, this work is the ﬁrst to implement
physics-based engine models for large-scale analysis of a ﬂeet of cars. As illus-
trated by our results in Sect. 3, ACCOLADES can be used in the design and
conceptual analyses phase of new engine systems and can streamline workﬂow
management in the analyses of large amounts of data obtained in dyno tests for
various engine operating conditions.
2
Main Components of ACCOLADES
ACCOLADES consists of the reduced-order engine simulator p-MODES and the
data analytics toolbox TADA.
2.1
pMODES
pMODES is used to compute the temporal variation of various engine para-
meters such as pressure, temperature and mixture composition for each CAD
over an entire drive cycle. The energy equation shown in Eq. (1) describes the
relationship between the engine crank-angle θ and pressure.
dP (θ)
dθ
= γ −1
V (θ) (Qin −Qloss) −γ P (θ)
V (θ)
dV
dθ
(1)
Solution of this equation yields the temporal variation of cylinder pressure for a
given set of operating conditions (such as load, combustion duration, fuel type,
engine RPM, etc). The instantaneous values of temperature and composition
of the burned and unburned gas zones can be obtained from the instantaneous
value of computed pressure. Knowing the instantaneous temperature, pressure
and composition of the burned zone enables the computation of emissions such
as NO, CO, soot, and unburned hydrocarbons using simpliﬁed reduced chem-
istry models. Details of these models and the solution procedure are discussed
in Ref. [2]. Instantaneous values of equilibrium concentrations of the combus-
tion products are needed to compute various emissions. Computation of these
equilibrium concentrations pose serious numerical challenges on account of the
stiﬀness of the system of nonlinear equations describing the formation of combus-
tion products. References [1,3] discuss the details of the computation procedure
and steps taken to ensure a fast, robust solution. Following the solution proce-
dure discussed above enables one to obtain temporal variation of emissions such
as NO and CO for a given fuel input proﬁle. Figure 1 shows the NO and CO
emissions for a single-cylinder gasoline engine obtained using pMODES.

ACCOLADES: Simulation and Analysis of Automotive Engines
91
5
10
15
20
25
30
35
40
45
0
10
20
30
40
50
60
70
80
90
100
total NO per total fuel
Cumulative Frequency (%)
1
1.5
2
2.5
Pressure
50
100
150
200
250
300
350
400
0
10
20
30
40
50
60
70
80
90
100
total CO per total fuel
Cumulative Frequency (%)
1
1.5
2
2.5
Pressure
Fig. 3. Cumulative distributions of NO (left) and CO (right) emissions for diﬀerent
initial cylinder pressure conditions. Each curve shows the percentage of drive cycle runs
for which the NO/CO is at or below the value given on the horizontal axis.
2.2
TADA
The Toolkit for Advanced Data Analytics (TADA) provides a framework for
post-processing of experimental- and simulation-generated time series data. Here
we overview some of the operations possible with TADA.
Whether from physical experiment or numerical simulation, TADA takes as
input time series data {fo(x; t; θt(x)) : o = 1, . . . , O; t = 1, . . . , T}, where o
indexes O diﬀerent dependent variable outputs, t indexes T time periods τ1 <
τ2 < · · · < τT , x ∈Rn parameterizes the independent design and operational
variables, and θt(x) ∈Rm denotes the state variables at time τt with input x.
Typical data analysis operations on these sets of time series data include
Filtering to extract basic statistics and identify input conﬁgurations of interest.
For example, one can determine peak temperatures and pressures in order to
characterize engine damage; peaks can be computed for each conﬁguration,
or all peaks above a threshold can be extracted.
Empirical distribution characterization to provide cross-conﬁguration
information. Such distributions can be used, for example, to determine ﬂeet-
wide fuel economy [12] or to characterize emissions as a function of ambient
pressure as is done in Fig. 3 for the case study in Sect. 3.
Sensitivity analysis to analyze how operating conditions or other independent
variables eﬀect observables of interest. Sensitivity analysis can be used, for
example to determine ﬂeet-wide implications for performance and emissions
of increased adoption of novel fuel types or additives.
Tradeoﬀvisualization and analysis can be used to ﬂag a conﬁguration that
is worse in all metrics of interest than some other conﬁguration. Such analy-
ses can also be used, for example, to identify vehicle conﬁgurations that
sacriﬁce little in terms of performance while providing substantial gains in
fuel economy.

92
S.M. Aithal and S.M. Wild
We expect that the capabilities in TADA will be fully used as one “closes the
loop” between the pMODES simulation and analysis for purposes of simulation-
based design optimization [4] or optimal experimental design to determine con-
ﬁgurations that should be tested on a dyno. In this view, TADA can be used
to generate input conﬁgurations and/or in order to optimize a design objective
of interest. Distributional information can be used to generate scenarios (e.g.,
ambient or operating conditions, drive cycle variations) for use in sample average
approximation for optimization under uncertainty [8]. Similarly, tradeoﬀanaly-
sis forms the basis for simultaneously optimizing multiple conﬂicting objectives
[6,14], such as performance and engine lifetime/reliability.
3
Results and Discussions
As an illustrative example, we discuss the simulation of a single-cylinder gasoline
engine operating at 1,100 RPM, wherein the inlet gas temperature, air humidity,
initial cylinder pressure, and exhaust gas recirculation (EGR) fraction are var-
ied for realistic engine operating conditions. Each of the parameters have four
values and for each conﬁguration sixteen diﬀerent drive cycles are considered,
leading to 4,096 individual conﬁgurations (or parametric cases). The inlet gas
temperature is varied from 28 to 31 ◦C in steps of 1 ◦C, the initial cylinder pres-
sure is varied from 0.88 atm to 1.0 atm, the relative humidity is varied from 0
to 100 %, and the EGR fraction is varied from 0 to 3 %. These 4,096 parametric
cases, each with 1,500 temporal data points in the drive cycle, were run on IBM
Blue Gene/Q (BG/Q) and Sandy Bridge clusters at Argonne National Labora-
tory. The BG/Q supercomputer (called “Mira”) is equipped with 786,432 cores,
768 TB of memory and has a peak performance of 10 petaﬂops. Each compute
node has a PowerPC A2 1600 MHz processor containing 16 cores, each with 4
hardware threads, running at 1.6 GHz, and 16 GB of DDR3 memory. The Sandy
Bridge cluster (called “Blues”) is a 2.6 GHz, 4960 processors system with 16
cores per compute node and 4 GB memory per core. These systems were chosen
to ensure portability of the code on diﬀerent architectures (and compilers) and
also to compare and contrast the relative performance of ACCOLADES on these
machines. Each of the cases considered in the study was assigned to one MPI
rank on the machine. Each MPI rank read its input data (i.e., operating condi-
tions such as initial pressure, humidity, inlet air temperature, and EGR fraction)
from a separate input ﬁle and wrote two diﬀerent ﬁles: (a) the computed solu-
tion (e.g., emissions, maximum temperature, pressure, exhaust temperature and
pressure, peak ion current, location of peak ion current) and (b) operating condi-
tions to its own uniquely named ﬁle. This methodology was chosen to ensure no
communication between diﬀerent case conﬁgurations (thus ensuring task paral-
lelism), and also to facilitate data analytics by TADA. A typical case’s solution
ﬁle was 306 kB while the ﬁle containing information about each of 1,500 data
points was 351 kB. Since each of the parametric cases are independent of the
others, increasing the number of cases directly proportional to the number of
cores yields a weak scaling study. The study was run both with no optimization

ACCOLADES: Simulation and Analysis of Automotive Engines
93
512
1024
2048
4096
0
10
20
30
40
50
No. of Ranks (= No. of Cases)
Total Time [min]
Ideal Original
Ideal −O3
32
64
128
256
512
1024
2048
4096
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
No. of Ranks (= No. of Cases)
Total Time [min]
Ideal
Fig. 4. Weak scaling results on the BlueGene/Q machine Mira (left) and the Sandy
Bridge-based machine Blues (right).
1%
3%
10%
15%
210
215
220
225
230
235
240
Humidity
Time [secs]
0
500
1000
1500
2000
2500
3000
3500
4000
150
160
170
180
190
200
210
220
230
240
Rank Number (= Case Number)
Time [secs]
1% Hum.
3% Hum.
10% Hum.
15% Hum.
Fig. 5. Case timings on Sandy Bridge-based machine Blues: (left) global sensitivity
analysis (outliers removed) demonstrating eﬀect of humidity input on per case timing;
(right) case timings as a function of rank number show a cyclic pattern associated with
varying humidity input.
and with ‘O3’ optimization option on both machines. The Intel 13.1 compiler
was used on Blues whereas the xl.legacy.ndebug (libraries with MPICH compiled
with the XL compilers) was used on Mira.
Figure 4 shows the scaling for Blues and Mira (with and without optimiza-
tion). Excellent weak scaling is seen on both machines. It was seen that the
optimization level did not change the overall compute time on Blues (hence it is
not shown in Fig. 4), whereas using -O3 level optimization on Mira reduced the
computational time by a factor of nearly 2.7. We attribute the slight increase
in overall computational time as the number of cores (and thus cases) increases
primarily to imbalances in individual case solution times and to increased con-
tention for the I/O operations. Furthermore, we see greater imbalances across
cases for Blues than for Mira.

94
S.M. Aithal and S.M. Wild
Deeper analysis of the timings associated with the 4,096 cases provides insight
to the scaling behavior seen. For the Sandy Bridge-based system, Fig. 5 shows
that the input value of humidity (for this study, selected from {1 %, 3 %, 10 %,
15 %}) has a signiﬁcant eﬀect on the timing of a run. This information can be
used to perform application-informed load balancing in ACCOLADES, whereby
the population of tasks is partitioned and scheduled based on their input values.
For the study presented in Fig. 4, the cases were selected as ordered in Fig. 5
(right). As a result of this ordering, the results using fewer than 512 ranks
beneﬁt from the fact that they only involve low input humidity values, which
result in lower time per case.
4
Conclusions
In this work, we discuss the development of a parallel design and data analysis
tool, named ACCOLADES, for conducting large-scale parametric studies of a
ﬂeet of cars. A parallel, fast robust physics-based engine model used to compute
performance and emissions of automotive engines was coupled to a data-analytics
module to enable a wide range of operations in support of design- and decision-
makers as well as vehicle experimentalists.
An illustrative example consisting of 4,096 parametric cases was run on a
Sandy Bridge cluster and an IBM BG/Q supercomputer. It was shown that the
emission and performance characteristics of a 25-min-long synthetic drive cycle
can be obtained numerically in acceptable computing time (≈4–20 min, depend-
ing on the machine). Excellent weak scaling was observed on both machines as
expected in such inherently task parallel problems. Although no serious I/O
bottlenecks were observed for the simulations considered in this work, we expect
that additional care will need to be taken when performing I/O operations for
massively parallel studies (e.g., involving a million cases) in order avoid over-
loading a parallel ﬁle system.
Acknowledgments. We gratefully acknowledge the computing resources provided by
the Argonne Leadership Computing Facility and the Laboratory Computing Resource
Center at Argonne National Laboratory.
References
1. Aithal, S.M.: Analysis of the current signature in a constant-volume combustion
chamber. Combust. Sci. Technol. 185, 336–349 (2013)
2. Aithal, S. M.: Development of an integrated design tool for real-time analyses of
performance and emissions in engines powered by alternative fuels. In: Proceedings
of the SAE 11th International Conference on Engines and Vehicles (2013). SAE
Paper 2013–24-0134
3. Aithal, S.M.: Prediction of voltage signature in a homogeneous charge compression
ignition (HCCI) engine fueled with propane and acetylene. Combust. Sci. Technol.
185, 1184–1201 (2013)

ACCOLADES: Simulation and Analysis of Automotive Engines
95
4. Aithal, S.M., Wild, S.M.: Development of a fast, robust numerical tool for the
design, optimization, and control of IC engines. In: Proceedings of the SAE 11th
International Conference on Engines and Vehicles (2013). SAE Paper 2013–24-0141
5. Belludi, N., Receveur, J., Raymond, J.: High-performance grid computing for cum-
mins vehicle mission simulation: architecture and applications. In: Proceedings of
the SAE 2011 Commercial Vehicle Engineering Congress (2011). SAE Paper 2011–
01-2268
6. Ehrgott, M.: Multicriteria Optimization, 2nd edn. Springer-Verlag, Heidelberg
(2005)
7. Fu, T.-C.: A review on time series data mining. Eng. Appl. Artif. Intell. 24, 164–181
(2011)
8. Homem-de-Mello, T., Bayraksan, G.: Monte Carlo sampling-based methods for
stochastic optimization. Surv. Oper. Res. Man. Sci. 19, 56–85 (2014)
9. Kieckhafer, K., Walther, G., Axmann, J., Spengler, T.: Integrating agent-based
simulation and system dynamics to support product strategy decisions in the auto-
motive industry. In: Proceedings of the Winter Simulation Conference (2009), pp.
1433–1443
10. Lamb, D.A., Gorsich, D., Krayterman, D., Choi, K.K., Hardee, E., Du, L., Youn,
B.D., Bettig, B., Ghiocel, D.: System level RBDO for military ground vehicles using
high performance computing. In: Proceedings of the SAE 2008 World Congress and
Exhibition. SAE Technical Paper 2008–01-0543 (2008)
11. Liu, Y., Wang, Z., Liang, J., Liu, X.: Synchronization and state estimation for
discrete-time complex networks with distributed delays. IEEE Trans. Syst. Man
Cybern. B 38, 1314–1325 (2008)
12. Moawad, A., Balaprakash, P., Rousseau, A., Wild, S.M.: Novel large scale simu-
lation process to support DOT’s CAFE modeling system. In: Proceedings of the
International Electric Vehicle Symposium and Exhibition, May 2015
13. Thornton, P.E., Running, S.W.: An improved algorithm for estimating incident
daily solar radiation from measurements of temperature, humidity, and precipita-
tion. Agric. For. Meteorol. 93, 211–228 (1999)
14. Vijayagopal, R., Sharer, P., Wild, S.M., Rousseau, A., Chen, R., Bhide, S., Don-
garkar, G., Zhang, M., Meier, R.: Using multi-objective optimization for HEV com-
ponent sizing. In: Proceedings of the International Electric Vehicle Symposium and
Exhibition, no. EVS28 0153, May 2015

Accelerating LBM and LQCD Application
Kernels by In-Memory Processing
Paul F. Baumeister1, Hans Boettiger2, Jos´e R. Brunheroto3,
Thorsten Hater1(B), Thilo Maurer2, Andrea Nobile4, and Dirk Pleiter1
1 J¨ulich Supercomputing Centre, Forschungszentrum J¨ulich, 52425 J¨ulich, Germany
t.hater@fz-juelich.de
2 IBM Deutschland Research and Development GmbH, 71032 B¨oblingen, Germany
3 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA
4 Institute for Advanced Simulation, Forschungszentrum J¨ulich,
52425 J¨ulich, Germany
Abstract. Processing-in-memory architectures promise increased com-
puting performance at decreased costs in energy, as the physical prox-
imity of the compute pipelines to the data store eliminates overheads
for data transport. We assess the overall performance impact using a
recently introduced architecture of that type, called the Active Memory
Cube, for two representative scientiﬁc applications. Precise performance
results for performance critical kernels are obtained using cycle-accurate
simulations. We provide an overall performance estimate using perfor-
mance models.
1
Introduction
With increasing complexity the performance of modern HPC architectures
becomes increasingly communication limited. The resulting need for improving
eﬃciency of data transport and eliminating the associated overheads in terms
of time and energy can be addressed by integrating compute pipelines close to
the data stores, which is achieved, e.g., by processing-in-memory (PIM) designs.
For various reasons none of the proposed PIM designs have resulted in products
which could be integrated into HPC architectures. There are good arguments
for expecting PIMs to enter system roadmaps, see [2] for an extended discus-
sion. Besides a growing need for such approaches there are new opportunities for
realizing such architectures due to novel 3-dimensional stacking technologies.
In this paper we assume that PIM technologies will become part of future
massively-parallel HPC architectures to address the following question: How eﬃ-
ciently could such an architecture be exploited by relevant scientiﬁc applications?
For our work we selected two representative applications, namely computa-
tional ﬂuid-dynamics calculations based on the Lattice Boltzmann Method and
simulations of the theory of strong interactions, i.e. Lattice Quantum Chromody-
namics. We have implemented performance relevant kernels on the IBM Research
Active Memory Cube (AMC) architecture [14] to allow for cycle-accurate simula-
tions. Performance models are used to assess the overall performance for diﬀerent
proxy architectures. This approach allows us to make the following contributions:
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 96–112, 2015.
DOI: 10.1007/978-3-319-20119-1 8

Accelerating LBM and LQCD Application Kernels by In-Memory Processing
97
– We present and discuss our implementation of applications on AMC and
analyse the observed performance.
– Based on this performance analysis we conduct an architectural evaluation of
AMC for the considered application kernels.
– To assess the overall performance based on performance models for diﬀerent
variants of a proxy architecture.
– Finally, we report on feedback on future applications’ roadmaps which we
initiated to assess future application needs.
In Sects. 2 and 3 we provide an overview on the AMC architecture and the appli-
cations considered for this paper. We continue with a presentation of details of
application kernels on AMC and performance analysis results in Sects. 4 and 5,
respectively. In Sect. 6 we explore features and parameters of the AMC architec-
ture and its suitability for the given application kernels. Finally in Sect. 7, we
introduce a performance model approach for assessing the overall performance
for HPC architectures that integrate AMCs.
2
Active Memory Cubes
Recently a new, 3-dimensional packaged memory architecture, Hybrid Memory
Cube (HMC) [11], was introduced, where a set of memory dies are stacked on
top of a logic die. In the HMC architecture the logic die implements a mem-
ory controller and a network interface via which the processor can access the
memory. In the IBM Research Active Memory Cube (AMC) [14] architecture
32 computational lanes are added to the logic die, which also have access to the
memory. The most important hardware performance parameters are listed in
Table 1.
Each lane is composed of a control unit and 4 computation slices, which each
comprise a memory-access and an arithmetic pipeline. Each slice contains mul-
tiple register ﬁles which include 32 scalar plus 16 vector computation registers.
The latter are composed of 32 elements. All registers are 64-bit wide. Overall,
AMC enabled nodes oﬀer a signiﬁcant amount of parallelism, comparable to
GPU-accelerated architectures. Nodes are expected to contain multiple AMCs,
in the order of sixteen devices. Each AMC possesses 32 lanes, multiplied by four
slices.
The computational lanes are micro-coded and each can process in every
clock cycle one Very Long Instruction Word (VLIW), mapping to 1 Control +
4 · (1 ALU + 1 LSU) instructions for a total of nine. To reduce the number of
VLIWs a static or dynamic repeat counter can be speciﬁed following a temporal
single-instruction-multiple-data (SIMD) paradigm. Instructions can be repeated
up to 32 times and access, e.g., during each iteration consecutive elements of
vector registers. The VLIWs are read from a buﬀer which can hold up to 512
entries.
The arithmetic pipelines take 64-bit input operands and can complete in each
clock cycle one double-precision or a 2-way SIMD single-precision multiply-add.

98
P.F. Baumeister et al.
Lane
Control
Slice 0
LSU
ALU
Slice 1
LSU
ALU
Slice 2
LSU
ALU
Slice 3
LSU
ALU
Load/Store Interface
Register
File 0
Register
File 1
Register
File 2
Register
File 3
Operand Interconnect
c
Fig. 1. Sketch of the AMC lane architecture. For more details see [14].
Table 1. Summary of AMC performance characteristics.
Clock frequency
f = 1.25 GHz
Floating-point performance per lane Bfp = 8 Flop/cycle = 10 GFlop/s
Memory bandwidth per lane
Bmem = 8 Byte/cycle = 10 GByte/s
Memory capacity per lane
Cmem = 0.25 GByte
Number of lanes per AMC
Nlane = 32
Thus up to 8 double-precision ﬂoating-point operations can be completed per
clock cycle and lane. The peak performance of one AMC running at a clock
speed of 1.25 GHz is 320 GFlop/s. As one slice can read the vector registers of the
other slices, double-precision complex arithmetic can be implemented without
the need for data re-ordering instructions. The same holds for single-precision
complex arithmetic thanks to special 2-way SIMD instructions.
Like in the HMC architecture the memory is organized into vaults. Each of
the 32 vaults in the AMC architecture is functionally and operationally indepen-
dent. Each vault has a memory controller (also called vault controller) which like
the lanes is attached to the internal interconnect. Load/store requests issued by
a lane are buﬀered in the lane’s load-store queue which has 192 entries. A lane
can load or store 8 Byte/cycle from or to the internal interconnect, i.e. the ratio
of ﬂoating-point performance versus memory bandwidth is 1 and thus signiﬁ-
cantly larger than in typical processor architectures. The memory access latency
depends on the distance between lane and vault as well as on whether the data is
already buﬀered by the vault controller. The minimum access latency is 24 cycles,
i.e. load operations have to be carefully scheduled to avoid stalls due to data
dependencies.

Accelerating LBM and LQCD Application Kernels by In-Memory Processing
99
Each AMC features a network interface with a bandwidth of 32 GByte/s to
connect it to a processor and multiple devices may be chained to share a link
to the processor. The execution model foresees main programs to be executed
on the processor with computational lanes being used for oﬀ-loading small ker-
nels. Assuming a design based on 14 nm technology, the power consumption is
expected to be around 10 W.
Architectures based on AMC would feature multiple levels of parallelism with
4 slices per lane, 32 lanes per AMC, NAMC AMCs per processor and possibly
multiple processors per node and many nodes per system.
3
Applications and Application Roadmaps
3.1
Lattice Boltzmann Method
The Lattice Boltzmann method (LBM) is an approach to computational ﬂuid
dynamics based on a discrete approximation to the Boltzmann equation in terms
of the distribution functions fα in phase space
fα(x + cαΔt, t + Δt) = fα(x, t) + ω (fα(x, t) −f eq
α ).
(1)
Here, spatial position x, velocity cα and time t only take discrete values. The
update process can be implemented in two separate steps: First, the local update
on the right hand side, describing changes in the distribution due to particle
interactions (collide). Second, distributing the update to adjacent sites accord-
ing to a stencil with a range of three sites, equivalent to the ﬂuid advection
(propagate).
There are various LBM models which diﬀer in number of spatial dimensions
and velocities. In this work we considered an application which implements the
D2Q37 model , utilizes 37 populations in two spatial dimensions [17]. Parallelisa-
tion of the application is based on a domain decomposition in 1 or 2 dimensions.
For typical runs and architectures the time spent in the collide and propagate
kernel corresponds to about 90 % and 10 % of the overall execution time, respec-
tively. The propagate step involves communication between tasks processing
adjacent domains to update a halo. Communication can typically be overlapped
with updating the bulk region. The collide step requires 5784 ﬂoating-point oper-
ations per site at 37 · 8 Bytes of input and output, whereas propagate does not
involve any computation. Given this high arithmetic intensity we expect the col-
lide step to be compute performance limited on the AMC architecture, while the
propagate step will be limited by the memory bandwidth on any architecture.
This split into two central tasks with distinct performance characteristics makes
D2Q37 an interesting choice for evaluating the AMC architecture.
Future Requirements. The D2Q37 model is used to study ﬂuid turbulence using
lattices of size L2 with L = O(1000 . . . 10000). These studies will have to be
extended to 3 dimensions. Studies on lattices of size L3 = 40003 are expected to
require 64 days of computing time on a machine with 100 PFlop/s peak perfor-
mance assuming an eﬃciency of about 20 % [18]. The number of ﬂoating-point

100
P.F. Baumeister et al.
operations, which have to be executed per site and update, would increase from
about 6000 to about 50000, while the memory footprint per site increases by a
factor of about 3.
3.2
Lattice QCD
Quantum Chromodynamics (QCD) is a theory for describing strong interactions,
e.g. the interaction between key constituents of matter like quarks. Lattice QCD
(LQCD) refers to the formulation of QCD on a 4-dimensional discrete lattice.
LQCD opens the path for numerical simulations of QCD.
The computational challenge in LQCD computations consists of repeatedly
solving very large sparse linear systems. Iterative solver algorithms result in a
very large number of multiplications of a matrix D and a vector ψ, which thus
becomes the most performance critical computational tasks. In this paper we
focus on a particular formulation of LQCD with so-called Wilson-Dirac fermions
where this task takes the following form:
Dψ(x) = mψ(x) +

±μ
(1 ± ˆγμ) ⊗U ±†
μ (x)ψ(x ± ˆμ).
(2)
The index μ runs over all four dimensions of space-time.
Modern solvers (see, e.g., [8]) allow for a mixed precision approach where
majority of the ﬂoating-point operations can be performed in single-precision,
without compromising on the double-precision solution.
The operator D consists of a real, scalar on-site term m and eight links
towards the Cartesian nearest-neighbour lattice sites in the four-dimensional
discretised space-time. Spinor ﬁelds ψ are represented by one element per lattice
site, where each element lives in the product space of 3 colours and 2×2 spins and
thus comprises 12 complex numbers. Gauge ﬁelds U are represented by vectors
with one element per lattice link, where each element acts as a SU(3)-rotation
onto the colour-space. These are typically represented by a 3×3 complex matrix,
however, the full information can be encoded in just 8 real numbers. The spin
space is invariant with respect to the link rotations, but sensitive to the link-
direction.
The spin projection operators (1 ± ˆγμ) commute with the link matrices U
due to operating on diﬀerent dimensions. A closer look at the 4 × 4 operators
(1 ± ˆγμ) reveals that their matrix representation is of rank 2 rather than rank 4.
This enables us to separate the operators into two parts, henceforth called spin
deﬂation P±μ ∈C2×4 and spin inﬂation J±μ ∈C4×2. This reduces the number of
necessary operations by performing the SU(3)-rotations in the deﬂated subspace.
The computational task now takes the following form:
Dψ(x) = mψ(x) +

±μ
J±μU ±†
μ (x)P±μψ(x ± ˆμ)
(3)
Usually, data parallelism is exploited by decomposing the lattice into cubic sub-
domains in a one to four dimensional scheme. The main source of communication

Accelerating LBM and LQCD Application Kernels by In-Memory Processing
101
Table 2. Implementation characteristics of the three kernels per lattice site.Shown are
the volumes of data loaded and stored, the number of required ﬂoating point operations
(FP) and the percentage of operations which could be mapped to multiply-add type
instructions (MA). We further give the resulting arithmetic intensity (AI) and the
number of VLIWs in the kernel.
Load (Byte) Store (Byte) FP (% MA) AI
VLIWs
LQCD
1152
480
1368 (89 %)
0.8
63
LBM collide
765
296
5784 (80 %)
5.5 163
LBM propagate
296
296
-
0.0
26
is the surface exchange with the nearest neighbours in the respective dimensions.
Therefore an appropriate torus topology is preferred. In order to minimise the
impact of the communication, the operator computation is split into the for-
ward and backward directions. This allows for transferring partial results of the
latter and meanwhile computing the former, thus overlapping computation and
communication. Then the procedure is reversed and the ﬁnal result is computed.
Future Requirements. The performance of future algorithms are expected to be
dominated by similar computational tasks as considered in this paper. With
more computational resources becoming available, the lattice sizes are expected
to grow during the next couple of years to L3 · T = 1283 · 256 [13]. Currently,
even larger lattices are only expected to be required for relatively special cases.
4
Implementation on AMC
We describe our implementations of the collide and propagate kernels of the
D2Q37 LBM code and the Dirac operator from the LQCD code. As the exper-
iments were performed at an early stage of the development of this technology,
neither compilers for high-level languages nor assemblers were available. Conse-
quently, the implementation was done directly at the level of AMC micro-code
supported by a set of ad-hoc tools. Thus programming involved scheduling of
instruction and their allocation to slices as well as the allocation of registers.
The focus of the dicussion lies on the features of the instruction set and
AMC architecture that support the implementation, rather than providing a
full overview on our implementations.
4.1
LBM D2Q37 Model
Collide. For this kernel, we implemented a vectorised version over 32 elements,
using the temporal SIMD paradigm, but neglected the traversal of the lattice.
For a full version, this infrastructure would have to be added. An eﬃcient, static
work sharing scheme, based on partitioning the lattice, can be applied. In this
case, we did not use the four slices for additional parallelisation, in order to
avoid further constraint on the input. The operation is divided in four sub-steps:

102
P.F. Baumeister et al.
Slice 0
Slice 1
Slice 2
Slice 3
C3
C2
Ceq
2
C4
Ceq
4
Ceq
3
Fig. 2. Distribution of collide central loop body over the slices. Similar colours match
coeﬃcients of f and f eq, white signiﬁes auxiliary terms.
computing bulk properties, applying a non-linear transformation, computing the
update on the transform and, ﬁnally, reverting to the original representation. The
last three steps require the computation of six terms of diﬀering length. These
computations are distributed carefully over the four slices, resulting in a very
dense packing where 92 % of the instructions are ﬂoating point computations.
The six coeﬃcients are computed in pairs matched for even length in three of
the slices, while the fourth supplies intermediate terms. Common sub-terms are
preferably computed in this auxiliary slice, which further increases eﬃciency.
Propagate. We implemented the full kernel, including traversal of the lattice,
loading from the input array and storing to a diﬀerent output array. Exploiting
our freedom in choosing an optimal data layout, we sub-divide the lattice into
rows and distribute those evenly over the participating lanes. Each lane iterates
over the assigned rows handing one quarter of the row to each of its slices.
The capabilities of the LSU instructions are exploited to map the require-
ments to eﬃcient code. We load the local site in linear fashion, using two VLIWs
to capture the 37 populations. The LSU is capable of automatically increment-
ing the source address. Further, the store instruction is supplied with a vector
register holding the stencil and applies this to the relevant elements of the stored
data. Thus, the loop body of the kernel consist of just ﬁve VLIWs.
4.2
LQCD
In order to hide communication required when parallelising the matrix-vector
multiplication, the computation of the forward and backward part is overlapped
as described in Sect. 3.2. We re-produce this structure to keep our implementa-
tion close to the original memory access pattern, but do not implement inter-task
communication. Further, we simplify the kernel by only mocking up the indirec-
tion infrastructure for storing the results. Therefore, the simulation results have
to be understood as being approximative.
We took advantage of the temporal SIMD capabilities of the AMC by vec-
torising over eight lattice sites, utilising 24 out of 32 vector register elements.

Accelerating LBM and LQCD Application Kernels by In-Memory Processing
103
Fig. 3. Weak-scaling of the collide kernel performance for 32 × 32 lattice sites per
lane as a function of the number of used lanes (left pane) and strong-scaling of the
propagate kernel (right pane). The lines indicate the theoretical peak performance of
8 Flop/cycle/lane (left) and 8 Byte/cycle/lane (right).
By this we ensure that load latencies are mostly hidden. All necessary operations
are performed on complex numbers in single precision. Specialised instructions
have been added to the AMC ISA to eﬃciently implement complex multiply-
add operations using 8-Byte complex numbers and only 2 instructions. This
corresponds to a throughput of 1 single-precision complex multiply-add, i.e. 8
ﬂoating-point operations, per 2 cycles per slice. Each of the four slices operates on
one of the four link dimensions. The requirements of the kernel are summarised
in Table 2. Ultimately, the performance is limited by the memory bandwidth.
However, while the ﬁrst part executes one arithmetic operation per Byte loaded
and stored, the second is imbalanced by a larger initially loaded segment. Thus,
it incurs an additional start-up overhead.
5
Performance Analysis
We now proceed by presenting performance results for the kernels, for which
implementation was discussed in the previous section. The results have been
obtained on a cycle-accurate simulator for a full AMC including memories.
5.1
LBM
Collide. We start with a weak-scaling analysis of the collide kernel. Each instance
of the kernel is executed on a single lane and operates on an individual set of
32 lattice sites. From the analysis of the kernel in terms of ﬂoating-point oper-
ations per memory access, we expect the kernel to operate close to the maxi-
mum ﬂoating-point performance. We present the relevant metric, the number of

104
P.F. Baumeister et al.
Table 3. Median time per lattice site spent in the collide (tcoll, weak scaling: l = 32
sites per lane), propagate (tprop, strong scaling: L2 sites per AMC) and Wilson-Dirac
(tcomp, weak scaling: l4 sites per lane) kernel.
Lanes tcoll (ns) tprop (ns)
tcomp (ns)
l 32
L2 322
642
1282 l4 24
44
84
164
1
821.2
33.7 32.9 37.9
158.5 141.0 142.9 143.8
2
411.4
17.9 16.5 19.1
90.2
76.5
81.1
81.4
4
207.5
8.1
8.4
9.6
64.0
41.7
41.7
42.9
8
107.5
4.0
4.6
5.0
58.9
25.9
23.3
23.7
16
53.2
2.3
2.7
2.8
47.3
20.0
15.2
14.4
32
30.7
1.2
1.6
2.3
41.7
13.2
10.7 -
Fig. 4. LQCD operator weak scaling on AMC over the number of lanes in use.
ﬂoating-point operations performed per cycle, along with the speed-up over a
single lane in Fig. 3. Across all numbers of lanes up to 32, we can exploit above
60 % of the peak ﬂoating-point performance. On a single lane, the implementa-
tion reaches 70 % of peak. We observe a small negative impact of utilising more
than a single lane, despite operating on disjunct blocks of memory.
Propagate. The performance of the propagate kernel completely depends on
memory access performance. In Fig. 3 (right pane) we show strong scaling results.
Out of the theoretical maximum of 320 GByte/s bandwidth, propagate can
sustain 125 GByte/s on a lattice of 1282, or 39 % of peak. The simulations show
that for larger number of lanes memory accesses take longer to complete, which
could be result of congestion or non-optimal access patterns.

Accelerating LBM and LQCD Application Kernels by In-Memory Processing
105
5.2
LQCD
We investigated the performance of the partial implementation of the Wilson-
Dirac operator D in a weak scaling study. The extracted kernel is run on multiple
lanes, each processing a private lattice of extent L4 where L = 2, 4, 8, 16. The
results are summarised in Fig. 4 as performance in ﬂoating-point operations and
memory bandwidth. For all lattice sizes, the best performance was achieved at
the maximum of 32 lanes. From L = 2, the overall performance increases up to
the case L = 8, where it is virtually identical to L = 16. The optimum sustained
bandwidth on 32 lanes is 107 GByte/s, or 33 % of the peak value.
From Fig. 4 we can derive the optimum local lattice size per lane, namely 24
for two lanes, 84 for eight lanes and 164 for sixteen or more lanes.
6
Discussion of AMC Architecture
Based on our analysis of diﬀerent application kernels we are now able to analyse
selected hardware aspects and parameters and explore possible impact on the
kernel’s performance.
Instruction Set. The given set of application kernels could be mapped well on
the instruction set architecture (ISA) of the Active Memory Cube (AMC). Spe-
ciﬁc ISA features which have been exploited include ﬂexible load/store instruc-
tions and the availability of specialised single precision complex operations. The
architecture allows for an eﬃcient implementation of complex arithmetics. For
the Dirac kernel the upper bound of the ﬂoating point eﬃciency as determined
only by instruction selection and dependence lies at 94 %.
Temporal SIMD and Register File. For all kernels the temporal SIMD could be
extensively exploited. The penalty of not using all elements of a vector register
was found to be small. The collide kernel utilises near the maximum of scalar
and vector registers on all slices. The propagate kernel uses three vectors per
slice and about ten scalars. The Wilson-Dirac operator makes use of 14 out of
16 vector registers and 23 out of 32 scalar registers. The register ﬁle size was
in all three cases found to be adequate for holding data or staging data loaded
from memory. Sharing of vector registers between slices proved to be a crucial
feature for eﬃcient mapping of the application kernels on this architecture.
Instruction Buﬀer Size. The overall number of VLIWs needed to implement the
(most important parts) of the kernels was found to be small (see Table 2) such
that at most 32 % of the instruction buﬀer was ﬁlled.
Memory Bandwidth. Although the memory bandwidth is high compared to the
compute performance, the Dirac and propagate kernels are nevertheless limited
by the memory bandwidth on the AMC.
Memory Access Latency. Due to the size of the register ﬁle, load operations can
be scheduled such that memory access latencies are largely hidden. For a strictly
in-order design timely scheduling of such instructions can have signiﬁcant perfor-
mance impact. From our simulations we ﬁnd memory access latencies to grow sig-
niﬁcantly when increasing the number of used lanes, resulting in a loss of eﬃciency.

106
P.F. Baumeister et al.
AMC
AMC
AMC
AMC
AMC
AMC
AMC
AMC
AMC
CPU
Network
βnet
NC · βloc
Fig. 5. Proxy architecture for AMC enabled nodes.
Table 4. Key parameters of our node proxy architecture.
Description
Parameter
Default value
Number of AMC
NAMC
16
Number of chains
NC
8
AMC memory capacity
Cmem
8 GByte
Inter-AMC link performance (λmem, βmem) (1 μs, 32 GByte/s)
Network link performance
(λnet, βnet)
(1 μs, 100 GByte/s)
Load/Store Queue (LSQ). The LSQ size is limited to 192 entries and holds both
load and store requests. Its particular size has not been critical for neither the
propagate nor the collide kernel. For the Wilson-Dirac operator, however, the
majority of stalled cycles are due to a ﬁlled LSQ. Based on the memory access
pattern we do not expect a larger LSQ to improve performance.
7
System Performance Modelling
For assessing the characteristics of the full applications on an AMC enabled
node architecture, we deﬁne a proxy node architecture [1]. The basic structure
of this architecture, as shown Fig. 5, comprises a CPU with a set of AMCs, which
are organised in chains of equal length, plus a network interface attached. We
assume that various parameters of this architecture as listed in Table 4 can be
varied. To obtain quantitative results we use a speciﬁc choice guided by the node
architecture proposed in [14].
To model the system performance for the kernels considered in this paper
we assume that all calculations are performed on the AMC, for which we can
use the cycle accurate simulator results presented in the previous sections.

Accelerating LBM and LQCD Application Kernels by In-Memory Processing
107
To assess the overall performance we have to model the time needed to perform
the necessary data exchange when application kernels are parallelised over multi-
ple AMCs per node and multiple nodes. We assume that time for point-to-point
communications can be described by a latency-bandwidth model, i.e.
Tcomm = λ + I
β ,
(4)
where I is the amount of communicated data and (λ, β) are the start-up latency
and asymptotic bandwidth, respectively. We, furthermore, make the following
assumptions:
– All links between components are bi-directional and can be described by the
same parameters in both directions.
– All data transfers can be overlapped, therefore, the simulator results are valid
for node level implementations.
– One task is executed per AMC and care is taken to achieve optimal placement.
7.1
LBM
The LBM application has, apart from collide and propagate, only one relevant
step in performing a lattice update, the communication between diﬀerent tasks.
We model this part to estimate the performance on an AMC-enabled system.
As mentioned before, the lattice is distributed over multiple tasks using a
one dimensional decomposition. Each task, and equivalently each AMC, holds
a tranche with extent Lx · Ly. A layer of three halo columns is introduced to
exchange boundaries, necessitating a transfer of
I = 3 · 2 · Lx · 37 · 8 Byte = Lx · 1776 Byte
(5)
per sub-domain. We assume Lx to be suﬃciently large to allow execution of
the propagate kernel and the halo exchange to be perfectly overlapped. We can
therefore estimate the time required for an update step as follows:
Tstep = max(Lx · Ly · tprop, Tcomm) + Lx · Ly · tcoll,
(6)
where tcoll and tprop are the measured execution time for collide and propagate
kernel, respectively. They depend on the problem size as well as the number of
used lanes. In the following we will consider only the case Nlane = 32.
Assuming that data communication performance is limited either by the link
connecting the CPU and the ﬁrst AMC within a chain as well as the network
bandwidth, we obtain the following estimate for the time required for data com-
munication:
Tcomm = max

T mem
ex
, T net
ex

= max

λmem + I · NAMC
NC · βmem
, λnet + I · N
3
4
AMC
βnet

,
(7)

108
P.F. Baumeister et al.
Fig. 6. Performance model for LBM (left) and LQCD (right), using the selected
parameters for the proxy architecture as a function of the lattice size per AMC. Shaded
areas indicate the eﬀect of halving/doubling the respective bandwidth.
Considering square sub-domains per AMC, the minimal local size at which the
kernels can utilise the AMC eﬃciently is about 322 from our experiments and
maximally 35002 due to memory constraints.
For exploring execution time as a function of the lattice size per AMC we use
the maximum execution times per lattice site observed when using Nlane = 32
lanes (see Table 3): tcoll = 30.7 ns, tprop = 2.37 ns. The results are shown in
Fig. 6. We ﬁnd that lattice size beyond 1282 yield performance numbers that are
dominated by the computations as opposed to boundary exchange. Speciﬁcally
for Lx = Ly = 128 we obtain the following results: Tcoll = 502 μs and Tstep =
542 μs.
7.2
LQCD
To model the performance of a parallelised matrix-vector multiplication we apply
the same methodology as in the previous section. We assume that computation
and communication can be perfectly overlapped such that
T Dirac = max(Tcomp, Tcomm).
We assume a four dimensional domain decomposition into one sub-domain per
task. Let L4 be the lattice size per AMC, where the assumed memory capacity
Cmem limits us to L ≤64. The performance results shown in Table 3 indicate that
a lattice with L ≥8 should be used. Time for computation Tcomp = L4 · tcomp
with tcomp = 10.7 ns is obtained from simulations (see Table 3).

Accelerating LBM and LQCD Application Kernels by In-Memory Processing
109
A halo of thickness one, containing matrices comprising 12 single-precision
complex numbers is exchanged with each neighbour, resulting in a data volume of
I = L3 · 2 · 4 · 12 · 8 Byte = L3 · 768 Byte
(8)
per task. On the grounds of optimal task placement, we can argue that the
amount of data exchanged by a node over the network is I · N
3
4
AMC. The time
consumed by the communication is then given by
Tcomm = max

T mem
ex
, T net
ex

= max

λmem + I · NAMC
NC · βmem
, λnet + I · N
3
4
AMC
βnet

,
(9)
where the local exchange has an extra factor NAMC/NC since all AMCs in a
chain share the bandwidth to the CPU.
We investigate the resulting timing in Fig. 6. We conclude that even with
half the available bandwidth, a lattice extent of L = 16 yields timings that are
determined solely by the computation. At this setting we expect TDirac = Top =
0.7 ms for a lattice update step, using 8192 tasks on 512 nodes. The total peak
performance of such a partition is 2.6 PFlop/s. Halving the local extent to L = 8
is the point at which the time for local exchange (at slightly more than the
half eﬀective bandwidth) is equal to Top. At this point we are using 8192 nodes
(42 PFlop/s) and expect an operator evaluation every TDirac ≃44 μs.
8
Related Work
Performance analysis of Lattice Boltzmann Methods have been investigated in
detail in various papers. For instance, the 2-dimensional D2Q37 model has been
explored extensively for diﬀerent processor and accelerator architectures includ-
ing IBM PowerXCell 8i [3], Intel Xeon [4], GPUs and Xeon Phi [6]. Recently
an increasing number of publications focus on the 3-dimensional D3Q19 model,
concentrating on optimising memory hierarchies [15,20].
With advances in research on Lattice Quantum Chromodynamics (LQCD)
strongly linked to progress in available compute resources, there exists a large
number of publications where performance characteristics of LQCD applications
are studied and results from performance analysis for diﬀerent architectures are
presented. Recent examples include the performance analysis for LQCD solvers
on Blue Gene/Q [5] and architectures comprising GPUs and Xeon Phi [10,21].
Starting in the 90 s diﬀerent processing-in-memory (PIM) architectures have
been proposed and explored, including Computational RAM [7], Intelligent RAM
[16], DIVA [9], and FlexRAM [12]. The initial enthusiasm decreased lacking a
perspective of PIM architectures being realised. For a brief but more compre-
hensive overview on diﬀerent projects see [19]. Diﬀerent application kernels have
been mapped to these architectures to explore their performance, with focus on
kernels that feature irregular memory access patterns. To the best of our knowl-
edge no results from a performance analysis for large-scale scientiﬁc applications
on massively-parallel architectures comprising PIM modules has been published.

110
P.F. Baumeister et al.
9
Summary and Conclusions
Studying two applications on the IBM Research Active Memory Cube (AMC)
processing-in-memory architecture yielded promising results. Both applications
are representative for many simulation codes, which are similarly structured, so
conclusions can be expected to be transferable to those applications. We could
demonstrate that compared to peak ﬂoating-point performance eﬃciencies could
be reached which are mostly larger than those for processors and accelerators
available today. By keeping most of the data transfer local within the memory,
the need for data transport outside the memory could be signiﬁcantly reduced.
Our results for kernel execution times allows to estimate energy eﬃciency.
Assuming a power consumption of 10 W the costs of the D2Q37 LBM collide
kernel per lattice site is 0.3 μJ. This compares favourably to NVIDIA K20x
GPUs with an energy consumption of 2.6 μJ per lattice site [6], even if one takes
into account that these measurements were done on a technology which became
available in 2012.
While optimal exploitation of the AMC architecture requires the programmer
to take data locality and access patterns into consideration, the resulting beneﬁt
to the user is a power eﬃciency distinctly better than alternative platforms.
Acknowledgements. We thank the AMC team at IBM Research, in particular
J. Moreno, for sharing their knowledge on the AMC and continued help on this
project including many fruitful discussions. Furthermore, we gratefully acknowledge
F.S. Schifano and R. Tripiccione (INFN/University of Ferrara) for making a mini-
application version of their D2Q37 code available and for discussing their future
roadmaps [18]. We also thank G. Koutsou, S. Krieg, and H. Simma from the Sim-
ulation Lab LQCD at Cyprus Institute/DESY/JSC for discussing the future require-
ments of LQCD [13]. Finally, we thank A. Frommer and S. Krieg for making their
implementation of their AMG solver [8] available.
References
1. Ang, J.A., Barrett, R.F., Benner, R.E., Burke, D., Chan, C., Cook, J., Donofrio, D.,
Hammond, S.D., Hemmert, K.S., Kelly, S.M., Le, H., Leung, V.J., Resnick, D.R.,
Rodrigues, A.F., Shalf, J., Stark, D., Unat, D., Wright, N.J.: Abstract machine
models and proxy architectures for exascale computing. In: Proceedings of the 1st
International Workshop on Hardware-Software Co-Design for High Performance
Computing (Co-HPC 2014), pp. 25–32. IEEE Press, Piscataway (2014). http://
dx.doi.org/10.1109/Co-HPC.2014.4
2. Balasubramonian, R., Chang, J., Manning, T., Moreno, J.H., Murphy, R., Nair,
R., Swanson, S.: Near-data processing: insights from a MICRO-46 workshop. IEEE
Micro 34(4), 36–42 (2014)
3. Biferale, L., Mantovani, F., Pivanti, M., Sbragaglia, A., Schifano, S., Toschi, F.,
Tripiccione, R.: Lattice Boltzmann ﬂuid-dynamics on the QPACE supercomputer.
Procedia Comput. Sci. 1(1), 1075–1082 (2010). http://www.sciencedirect.com/
science/article/pii/S1877050910001201, ICCS 2010

Accelerating LBM and LQCD Application Kernels by In-Memory Processing
111
4. Biferale, L., Mantovani, F., Pivanti, M., Pozzati, F., Sbragaglia, M., Scagliarini,
A., Schifano, S.F., Toschi, F., Tripiccione, R.: Optimization of multi-phase com-
pressible lattice Boltzmann codes on massively parallel multi-core systems. Pro-
cedia Comput. Sci. 4, 994–1003 (2011). http://www.sciencedirect.com/science/
article/pii/S1877050911001633, Proceedings of the International Conference on
Computational Science, ICCS 2011
5. Boyle, P.A., Christ, N.H., Kim, C.: Co-design of the IBM BlueGene/q level 1
prefetch engine with QCD. IBM J. Res. Dev. 57(1/2), 13:1–13:10 (2013)
6. Calore,
E.,
Schifano,
S.F.,
Tripiccione,
R.:
A
portable
OpenCL
lattice
Boltzmann code for multi- and many-core processor architectures. Procedia
Comput. Sci. 29, 40–49 (2014). http://www.sciencedirect.com/science/article/
pii/S1877050914001811, 2014 International Conference on Computational Science
7. Elliott, D., Snelgrove, W., Stumm, M.: Computational ram: a memory-simd hybrid
and its application to dsp. In: Proceedings of the IEEE 1992 on Custom Integrated
Circuits Conference, pp. 30.6.1–30.6.4, May 1992
8. Frommer, A., Kahl, K., Krieg, S., Leder, B., Rottmann, M.: Adaptive aggregation
based domain decomposition multigrid for the lattice Wilson Dirac operator. SIAM
J. Sci. Comput. 36, A1581–A1608 (2014)
9. Hall, M., Kogge, P., Koller, J., Diniz, P., Chame, J., Draper, J., LaCoss, J.,
Granacki, J., Brockman, J., Srivastava, A., Athas, W., Freeh, V., Shin, J., Park, J.:
Mapping irregular applications to DIVA, a PIM-based data-intensive architecture.
In: ACM/IEEE 1999 Conference on Supercomputing, pp. 57–57, November 1999
10. Heybrock, S., Jo´o, B., Kalamkar, D.D., Smelyanskiy, M., Vaidyanathan, K., Wettig,
T., Dubey, P.: Lattice QCD with domain decomposition on intel xeon phi co-
processors. In: Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis (SC 2014), pp. 69–80. IEEE Press,
Piscataway (2014). http://dx.doi.org/10.1109/SC.2014.11
11. Hybrid Memory Cube Consortium: Hybrid Memory Cube Speciﬁcation (2013)
12. Kang, Y., Huang, W., Yoo, S.M., Keen, D., Ge, Z., Lam, V., Pattnaik, P., Torrellas,
J.: FlexRAM: toward an advanced intelligent memory system. In: International
Conference on Computer Design (ICCD 1999), pp. 192–201 (1999)
13. Koutsou, G., Krieg, S., Pleiter, D., Simma, H.: EIC co-design questionnaire: lattice
QCD (unpublished, 2013)
14. Nair, R., Antao, S.F., Bertolli, C., Bose, P., Brunheroto, J.R., Chen, T., Cher,
C.-Y., Costa, C.H.A., Evangelinos, C., Fleischer, B.M., Fox, T.W., Gallo, D.S.,
Grinberg, L., Gunnels, J.A., Jacob, A.C., Jacob, P., Jacobson, H.M., Karkhanis,
T., Kim, C., Moreno, J.H., O’Brien, J.K., Ohmacht, M., Park, Y., Prener,
D.A., Rosenburg, B.S., Ryu, K.D., Sallenave, O., Serrano, M.J., Siegl, P.D.M.,
Sugavanam, K., Sura, Z.: Active memory cube: a processing-in-memory architec-
ture for exascale systems. IBM J. Res. Dev. 59(2/3), 17:1–17:14 (2015)
15. Nguyen, A., Satish, N., Chhugani, J., Kim, C., Dubey, P.: 3.5-d blocking opti-
mization for stencil computations on modern cpus and gpus. In: International
Conference for High Performance Computing, Networking, Storage and Analysis
(SC 2010), pp. 1–13, November 2010
16. Patterson, D., Anderson, T., Cardwell, N., Fromm, R., Keeton, K., Kozyrakis, C.,
Thomas, R., Yelick, K.: A case for intelligent RAM. IEEE Micro 17(2), 34–44
(1997)
17. Scagliarini, A., Biferale, L., Sbragaglia, M., Sugiyama, K., Toschi, F.: Lattice Boltz-
mann methods for thermal ﬂows: continuum limit and applications to compressible
Rayleigh-Taylor systems. Phys. Fluids 22(5), 055101 (2010)

112
P.F. Baumeister et al.
18. Schifano, S.F., Tripiccione, R.: EIC co-design questionnaire: LBM (unpublished,
2013)
19. Torrellas, J.: Flexram: toward an advanced intelligent memory system: a retrospec-
tive paper. In: IEEE 30th International Conference on Computer Design (ICCD
2012), pp. 3–4, September 2012
20. Williams, S., Oliker, L., Carter, J., Shalf, J.: Extracting ultra-scale lattice Boltz-
mann performance via hierarchical and distributed auto-tuning. In: Proceedings
of 2011 International Conference for High Performance Computing, Networking,
Storage and Analysis (SC 2011), pp. 55:1–55:12. ACM, New York (2011). http://
doi.acm.org/10.1145/2063384.2063458
21. Winter, F., Clark, M., Edwards, R., Joo, B.: A framework for lattice QCD cal-
culations on GPUs. In: 2014 IEEE 28th International Parallel and Distributed
Processing Symposium, pp. 1073–1082, May 2014

On Quantum Chemistry Code Adaptation
for RSC PetaStream Architecture
Vladimir Mironov1, Maria Khrenova1,
and Alexander Moskovsky2(&)
1 Chemistry Department, Lomonosov Moscow State University,
Moscow, Russia
{vmironov,mkhrenova}@lcc.chem.msu.ru
2 ZAO “RSC Technologies”, Moscow, Russia
moskov@rsc-tech.ru
Abstract. Molecular simulations with quantum chemistry methods consume
a large portion of CPU cycles in modern high-performance computing centers.
Evolution of modern processors and HPC architectures necessitates adaptation
of software to new hardware generations. The present work concentrates on the
optimization of the widely used GAMESS code to Intel Xeon Phi architecture
and recently devised RSC PetaStream platform. Since improvement in parall-
elization is required, the most frequently used Hartree-Fock and DFT methods
are explored for additional parallelization options. The Xeon Phi requires vec-
torization that is important for electron-repulsion integrals (ERI) calculations to
achieve good performance.
Keywords: Quantum chemistry  Hartree-Fock  Density functional theory 
Intel xeon phi, GAMESS
1
Introduction
In 2012, Intel Many Integrated Cores (MIC) architecture [1] has been introduced as an
answer to mounting challenges in building scalable and efﬁcient high-performance
computing systems. To achieve energy efﬁciency of computation, Intel MIC has more
than 60 computational cores, each capable to execute AVX instructions. This new
hardware requires new level of parallelization and vectorization from the application
software for efﬁcient performance.
Quantum chemistry algorithms were being adapted for parallel hardware for many
decades. However, most popular codes “as is” don’t demonstrate good performance
efﬁciency on the Intel MIC hardware platform. In most cases, code is not vectorized,
while required thread parallelism level is not achieved. For example, GAMESS(US)
[2, 3] package has been parallelized for decades by now, but its code lacks vectorization
and enough thread-level parallelism of important pieces of algorithm even for widely
used Hartree-Fock and Density Functional Theory calculations. Intel Xeon Phi 5120D
requires as many as 240 threads to be run to achieve best performance in many algorithms
[4]. Attempts to run few hundred processes of GAMESS application instead of more
lightweight threads overwhelm memory subsystem with dramatic performance decrease.
© Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 113–121, 2015.
DOI: 10.1007/978-3-319-20119-1_9

The Intel MIC set new performance per watt level for x86-compatible systems.
While MIC is available as Intel Xeon Phi PCI express co-processor cards, it supports
“Native” mode of application execution, where each Xeon Phi is visible to application
as an independent manycore machine. The next generation of Intel MIC technology –
Intel Knights Landing [5] – will be self-sufﬁcient manycore bootable systems. Already
existing RSC PetaStream architecture [6] leverages efﬁcient co-processor-to-co-pro-
cessor communication, providing realistic model of future Intel KNL supercomputers,
where “native” mode of parallelization is the most natural and effective. Each node runs
its own Linux-based OS image of operating system, and Linux OS is run on host.
Majority of PetaStream computation power comes from Xeon Phi chips, therefore it
make sense to run application on Intel Xeon Phi cards, and use the host’s CPU for
support and service functions; application is run on uniform ﬁeld of Xeon Phi nodes –
at least one MPI rank per node – compatible with “native” mode for Xeon Phi. In case
of ofﬂoad-like work sharing is efﬁcient for an application and it is possible to harness
both CPU and MIC nodes. RSC PetaStream system uses Intel Node Manager Tech-
nology to control and monitor node power consumption of every node, that mechanism
can be used to implement ﬂexible power energy and optimization strategies to help
HPC sites save power and reduce operational costs. An example of the supercomputing
system where both types of nodes co-exist in the same fabric is St. Petersburg Poly-
technic University supercomputing center, with over 800 nodes on Intel Xeon E5v3 -
2697 (Haswell) share Inﬁniband FDR fabric with 256 nodes on Intel Xeon Phi 5120D.
The most common approaches in quantum chemistry are Hartree-Fock method
(HF) and density functional theory (DFT). The major steps of these methods are
construction and diagonalization of the Fock (Kohn-Sham) matrix [7]. For practically
interesting systems computational power required is usually of supercomputer scale.
The computational effort for the ﬁrst step is dominated by the calculation of two-
electron integrals corresponding to the Coulomb repulsion of electron pairs (and
therefore frequently called electron repulsion integrals, ERI) and, in case of DFT, also
by calculation of numerical quadrature of the exchange-correlation contribution to
energy. The two-electron integral calculation has theoretical O(N4) computational
complexity, where N is a number of basis functions used to characterize the system.
However, many of these integrals are small enough and may be neglected. It is possible
to reduce number of operations down to O(N2÷3) using cutoffs and also some
approximations, especially for very large systems. In that case a speed of Fock (or
Kohn-Sham) matrix diagonalization (O(N3)) signiﬁcantly affects the performance of
HF (DFT) method. However, for the majority of practically important molecular sys-
tems a construction of Fock (Kohn-Sham) matrix dominates overall computational
cost. Also, matrix diagonalization is a pure linear algebra calculation with a great
scalability, so the efﬁcient two-electron integral code is crucial to achieve the perfor-
mance in HF and DFT methods. We therefore targeted the Fock matrix two-electron
contribution code to demonstrate the applicability of the Intel MIC platform to classical
quantum chemistry problems.
The goal of the presented work is to enable migration of GAMESS(US) quantum
chemistry code [2, 3] to novel Intel MIC hardware technology. GAMESS is widely
used by the scientiﬁc community, with thousands of references in the papers each year.
114
V. Mironov et al.

We intend to minimize code modiﬁcation and optimize for future-proof “native” mode
of Intel Xeon Phi.
2
Basics of Hartree-Fock Method
2.1
Electron Repulsion Integrals (ERIs)
ERIs are the integrals of type:
Iijkl ¼ i; jjk; l
ð
Þ ¼
ZZ vi r1
ð Þvj r1
ð Þvk r2
ð Þvl r2
ð Þ
r2  r1
dr1dr2
ð1Þ
where χ denotes basis functions, i, j, k, l – their indices, r1, r2 – coordinates of ﬁrst and
second electrons. An important property of ERIs is their eightfold permutation sym-
metry with respect to i, j, k, l indices. Commonly Cartesian Gaussians are used as basis
functions:
v rð Þ ¼ x  Ax
ð
Þax y  Ay

ay z  Az
ð
Þazea rA
ð
Þ2
ð2Þ
where A and α are center and exponent of basis function respectively, a(ax, ay, az) – its
angular momentum. They have practically important property that a product of two
Gaussians is another Gaussian (see [8] for eq.). The Gaussian in form (2) is also called
“primitive”. Typically, linear combinations of Gaussian primitives which share the
same center and angular momentum (“contracted” functions) are actually used as a
basis functions. Contracted ERI are sum of integrals over their primitives:
i; jjk; l
ð
Þ ¼
XM
a
XN
b
XO
c
XP
d CaiCbjCckCdlðabjcdÞ
ð3Þ
where C is a matrix of contraction coefﬁcients, M, N, O, P – degree of contraction.
A set of (possibly contracted) basis functions that share the same center and same set of
exponents is termed “shell”. Grouping basis functions into shells reduces to some
extent the number of expensive ﬂoating point operations and improves efﬁciency of
integral screening. Primitive integrals are calculated numerically. Among the most
popular approaches are McMurchie-Davidson [9], Obara-Saika [10] and Dupuis-Rys-
King (DRK) [11] schemes. The effectiveness of the different schemes varies greatly for
the different integral types. Quantum chemical codes often have several algorithms
implemented and switch them wisely to improve performance. In this study we used
only DRK integral scheme for testing purposes due to its numerical stability, relative
simplicity, and uniformness for different kinds of integrals.
2.2
The Hartree-Fock Algorithm
The Hartree-Fock method is a method of ﬁnding an approximate wavefunction and
energy of the model system. It is based on eigenvalue problem:
On Quantum Chemistry Code Adaptation for RSC PetaStream Architecture
115

FC ¼ ϵSC;
ð4Þ
where F – Fock matrix, S – overlap matrix, C – matrix of molecular orbital coefﬁcients,
ϵ - diagonal matrix of orbital energies. Since F depends on C, the Eq. 4 has to be solved
self-consistently. Matrix F incorporates contribution from electron-electron (Vee) and
electron-nuclei electrostatic interaction (Ven) as well as kinetic energy of electrons (Te).
It is usually represented as a sum of one-electron Hamiltonian (h), Coulomb (J) and
exchange (K) matrices:
F ¼ h þ J  1
2 K
ð5Þ
h ¼ Ven þ Te; Jij ¼
X
kl Dkl  Iijkl; Kij ¼
X
kl Dkl  Iikjl
ð6Þ
where D – density matrix which is calculated from molecular orbital coefﬁcients.
Matrix h depends on the one-electron integrals and its computation scales quadratically
depending of the system size. The Fock matrix construction requires calculation of all
symmetry unique ERIs and has theoretical O(N4) complexity. It is worth noting that
numerous ERIs are very small and their contribution to the Fock matrix is negligible.
They could be avoided by applying screening techniques. It vastly reduces the number
of ERIs required for calculation down to O(N2÷3) depending on the geometrical size of
molecular system and the nature of atomic basis set used.
Different schemes have been proposed to calculate Fock matrix. Conventional
algorithm requires all ERIs to be calculated once and stored on a disk. However, it is
not very efﬁcient for the large systems due to high requirements on the amount of
available disk space for the integral storage and relatively slow disk operation speed.
The advantage of this method is that each ERI is calculated only once. In the alternative
approach (“direct” HF) integrals are recalculated every time as needed.
3
Implementation of the Hartree-Fock Method in GAMESS
The algorithm of direct HF method implemented in GAMESS is presented on Fig. 1.
The implementation of main loop over shell coefﬁcients corresponds to the so-called
“triple-sort” order [12] when up to three symmetrically unique integrals are calculated
at each cycle step. The alternative is a canonical way with slightly different index order,
when only one integral is calculated at each cycle step. The disadvantage of triple-sort
order is decreased granularity, which may be important on highly parallel systems.
GAMESS uses MPI parallelization to split workload during ERI calculation. It is
done on the ish and jsh loops implementing static and dynamic load balance. The main
drawback of this implementation is a huge memory footprint on multicore architec-
tures, because each MPI rank has its own copy of density matrix and a partial con-
tribution to Fock matrix that scales quadratically with job size. Straightforward
OpenMP implementation also inherits this drawback; however the density matrix is
read-only during ERI computation cycle and could be shared between threads. The
Fock matrix is constantly updated in this cycle and in simplest case it is replicated. It is
116
V. Mironov et al.

not a big problem when a large amount of memory is available. Replicated-memory
MPI/OpenMP version of GAMESS was previously reported to work on Cray XT5 and
further on K-computer [13]. In this algorithm each thread has its own copy of Fock
matrix. Even in this case the amount of required memory reduces up to two times in
comparison to original GAMESS implementation. Co-processors like MICs have large
number of cores and a limited amount of on-chip memory. In this case a maximum job
size is limited by the amount of available memory. A possible solution to this problem
is to use distributed memory libraries like Global Arrays [14] or DDI [15]. This
approach makes calculation possible even for extremely large jobs when none of these
matrices could ﬁt in a single-node memory in expense for some internode communi-
cation overhead. The distributed memory algorithms are based on the fact, that at every
moment only a small amount of data from density and Fock matrix is required for the
computation. Actually, only three rows of Fock matrix are updated in the innermost
loop of the ERI calculation cycle. The drawback of this implementation is that inter-
process communication grows, which may be quite expensive in runtime. In this study
we focus on the straightforward variant of the memory problem solution.
First we tried both triple-sort and canonical way of integral ordering. They show
nearly identical performance, however canonical order is slightly faster on medium-size
problems due to smaller granularity. Further we always used canonical order of shells
in the two-electron integral computational loop. It also has an advantage of the rect-
angular structure of second and third loop in nest, that could be used to improve load
balance between threads.
The straightforward OpenMP version of GAMESS Fock matrix two-electron
contribution shows quite a good performance on Xeon Phi (see Tables 1, 2, and 3).
This implementation still has considerable memory footprint (Fock matrix is local to
Fig. 1. Simpliﬁed algorithm of Hatree Fock implementation in GAMESS. NSH – number of
shells. NSH ≤1000 for typical workloads.
On Quantum Chemistry Code Adaptation for RSC PetaStream Architecture
117

each thread) but it is two times lower than for pure MPI implementation because
density matrix is now shared. We observe nearly perfect parallelization when up to 60
threads per MIC are used. Further increase of number of threads per MIC improves
performance only slightly. The same effect is observed on Xeon E5 CPU when more
than 8 cores per socket are used.
One of the reasons of this effect is poor cache utilization when multiple threads
are tied to one physical core. Indeed, the implementation of DRK algorithm of ERI
calculation in GAMESS operating with large arrays of data (about L2 cache size)
with nontrivial access pattern. The sizes of these arrays are set up at the compile time
and depend on the maximum possible angular momentum for the basis functions.
The scalability of code notably improves if we manually decrease the maximum
angular momentum that code could manage from L = 7 (default in GAMESS) to
L = 4. At the same time, the performance per core changes only slightly. Another
reason for the scalability degradation is a poor vectorization of the ERI code in
GAMESS.
It is worth noting that the scalability of code is unaltered if we consider benchmarks
with similar thread/core afﬁnity (Table 2). Therefore further improvement of single-
core performance would increase overall performance as well.
The code on Fig. 2 could be also straightforwardly parallelized over the top loop in
nest across MPI processes. The performance of the hybrid MPI/OpenMP version is
presented in Table 3. The heaviest MPI communication task is a Fock matrix reduction
that is performed only one time per HF iteration. We observe quite small (* 1 % of
execution time) synchronization and communication overhead in the case of
multi-MIC run.
Fig. 2. Algorithm of OpenMP parallelization of the calculation of two-electron contribution to
the Fock matrix in GAMESS.
118
V. Mironov et al.

3.1
Details of Benchmarks
As a benchmark systems we used fullerene molecule with two basis sets (6-31G and 6-
31G∗). The sizes of basis for these systems are 540 and 900 functions respectively.
Xeon Phi benchmarks were conducted on RSC PetaStream platform. MIC results were
compared to those of the RSC Tornado platform based on dual-socket Xeon E5-2690
server. Conﬁgurations of the test systems are summarized in Table 4.
Table 1. Performance of the OpenMP parallelized Fock matrix two-electron contribu-
tion code for the C60 (6-31G) benchmark (KMP_AFFINITY = balanced).
Number of
threads
Time of single Fock matrix build, seconds
Xeon E5-2690
Xeon Phi 7120D, L = 7
Xeon Phi 7120D, L = 4
1
370.8
–
–
2
195.5
–
–
4
105.5
–
–
8
55.0
815.2
790.2
16
48.5
409.6
396.4
32
–
215.2
211.4
60
–
109.7
106.6
120
–
75.6
69.0
180
–
–
61.5
240
–
79.6
59.2
Table 2. Thread afﬁnity impact on the performance of the OpenMP parallelized Fock matrix
two-electron contribution code for the C60 (6-31G) benchmark.
Number of
cores used
Time of single Fock matrix build, seconds
1 thread/core
2 thread/core
3 thread/core
4 thread/core
16
393.1
256.4
227.8
216.1
30
212.1
137.6
123.2
117.1
60
104.9
69.3
61.3
59.2
Table 3. Performance of hybrid MPI/OpenMP parallelized Fock matrix two-electron
contribution code on multiple Xeon Phi modules
Number of threads
Time of single Fock matrix build, seconds
C60 (6-31G), 540 b.f.
C60 (6-31G∗), 900 b.f.
240 (1 MIC)
59.2
147.2
480 (2 MICs)
29.7
81.8
960 (4 MICs)
15.6
32.0
1920 (8 MICs)
8.2
25.8
On Quantum Chemistry Code Adaptation for RSC PetaStream Architecture
119

4
Related Work
GAMESS [2, 3] is one of the most widely used software packages for quantum
chemistry calculations. Existing parallelization in GAMESS is sophisticated [16], it has
dynamic load balancing and distributed shared memory features.
GPU technology advances [17–19] created opportunity to take advantage of this
new technology. NWChem code has been re-written initially for GPU [17] with CUDA
technology, at its implementation on Xeon Phi [20] uses ofﬂoad mode for harnessing
Xeon Phi computational power. In this paper, implementation uses Xeon Phi native
mode for better ﬁtness to next generation architectures, with performance demonstra-
tion of multi-Phi. Existing GAMESS adaptation to GPU doesn’t affect most widely
used algorithms by computational chemists, and limited to some PCM model imple-
mentation. In more general contexts, only proﬁling work is reported [20]. In this
respect, this paper constitutes an important contribution to the development of
important software tools used by practicing researchers.
5
Conclusions
In this paper we present the design of parallelization scheme of GAMESS(US) code for
quantum chemistry calculations, namely, Hartree-Fock and Density Function Theory
(DFT) algorithms. Current work demonstrates the applicability of Xeon Phi copro-
cessors for the quantum chemistry problems. In this paper, we demonstrate scalability
of the current implementation on Xeon Phi cores, as well as with multiple Xeon Phi
chips running in native mode (OpenMP+MPI parallelization). Future work include
more thorough performance characterization and additional vectorization of ERI
calculation.
Acknowledgements. This work is supported by Intel Parallel Compute Center program. We
thank Georg Zitzlsberg (Intel Corp.) and Klaus-Dieter Oertel (Intel Corp.) for valuable advices.
Table 4. Conﬁgurations of the test systems
RSC PetaStream
RSC Tornado
Host processors
1x Xeon E5-2697v2
2x Intel Xeon E5-2690
Co-processor
8x Xeon Phi 7120D
2x Xeon Phi SE10X
RAM amount/speed
128 GB DDR3R-1600
64 GB DDR3R-1600
Main board
Intel Server Board S1600JP
Intel Server Board S2600JFF
PM settings
cpufreq and PC6 enabled
EIST and Turbo enabled
Inﬁniband HCA
Connect-IB, 2-port
ConnectX-3 on-board
Host OS
CentOS 6.4
CentOS 6.2
MPSS
3.2.3
2.1.2
OFED version
3.5-rc3
1.5.4.1
Inﬁniband switch
Mellanox FDR MSX6025F. 1 hop between hosts
120
V. Mironov et al.

References
1. Goodwins, R.: Intel unveils many-core Knights platform for HPC. http://www.zdnet.com/
article/intel-unveils-many-core-knights-platform-for-hpc/ (2010)
2. Schmidt, M.W., Baldridge, K.K., Boatz, J.A., Elbert, S.T., Gordon, M.S., et al.: General
atomic and molecular electronic structure system. J. Comput. Chem. 14, 1347–1363 (1993)
3. Gordon, M.S., Schmidt, M.W.: Advances in electronic structure theory: GAMESS a decade
later. In: Dykstra, C., Frenking, G., Kim, K., Scuseria, G. (eds.) Theory And Applications Of
Computational Chemistry: The First Forty Years, pp. 1167–1189. Elsevier, Amsterdam (2005)
4. Jeffers, J., Reinders, J.: Intel Xeon Phi Coprocessor High-Performance Programming.
Morgan Kaufmann Publishers, San Francisco (2013)
5. Anthony, S.: Intel unveils 72-core x86 Knights Landing CPU for exascale supercomputing.
http://www.extremetech.com/extreme/171678-intel-unveils-72-core-x86-knights-landing-cpu-
for-exascale-supercomputing (2013)
6. Semin, A., Druzhinin, E., Mironov, V., Shmelev, A., Moskovsky, A.: The performance
characterization of the rsc petastream module. In: 29th International Conference (ISC 2014),
Leipzig, Germany, pp. 420–429 (2014)
7. Schlegel, H.B., Frisch, M.J.: Computational Bottlenecks in Molecular Orbital Calculations.
Theor. Comput. Model. Org. Chem. 339, 5–33 (1991)
8. Reza Ahmadi, G., Almlöf, J.: The Coulomb operator in a Gaussian product basis. Chem.
Phys. Lett. 246, 364–370 (1995)
9. McMurchie, L.E., Davidson, E.R.: One- and two-electron integrals over cartesian gaussian
functions. J. Comput. Phys. 26, 218–231 (1978)
10. Obara, S., Saika, A.: Efﬁcient recursive computation of molecular integrals over Cartesian
Gaussian functions. J. Chem. Phys. 84, 3963 (1986)
11. Rys, J., Dupuis, M., King, H.F.: Computation of electron repulsion integrals using the rys
quadrature method. J. Comput. Chem. 4, 154–157 (1983)
12. Foster, I.T., Tilson, J.L., Wagner, A.F., Shepard, R.L., Harrison, R.J., et al.: Toward high-
performance computational chemistry: i. scalable fock matrix construction algorithms.
J. Comput. Chem. 17, 109–123 (1996)
13. Ishimura, K., Kuramoto, K., Ikuta, Y., Hyodo, S.: MPI/OpenMP hybrid parallel algorithm
for hartree −fock calculations. J. Chem. Theory Comput. 6, 1075–1080 (2010)
14. Nieplocha, J.: Advances, applications and performance of the global arrays shared memory
programming toolkit. Int. J. High Perform. Comput. Appl. 20, 203–231 (2006)
15. Alexeev, Y., Kendall, R.A., Gordon, M.S.: The distributed data SCF. Comput. Phys.
Commun. 143, 69–82 (2002)
16. Fletcher, G.D., Schmidt, M.W., Bode, B.M., Gordon, M.S.: Distributed data interface in
GAMESS. Comput. Phys. Commun. 128, 190–200 (2000)
17. Sengottaiyan, S., Liu, F., Sosonkina, M.: A GPU support for large scale quantum chemistry
applications. In: The 2012 International Conference on Parallel and Distributed Processing
Techniques and Applications (PDPTA 2012), Las Vegas, Nevada, USA (2012)
18. Uﬁmtsev, I.S., Martínez, T.J.: Quantum chemistry on graphical processing units. 1.
strategies for two-electron integral evaluation. J. Chem. Theory Comput. 4, 222–231 (2008)
19. Uﬁmtsev, I.S., Martinez, T.J.: Quantum chemistry on graphical processing units. 2. direct
self-consistent-ﬁeld implementation. J. Chem. Theory Comput. 5, 1004–1015 (2009)
20. Aprà, E., Klemm, M., Kowalski, K.: Efﬁcient implementation of many-body quantum
chemical methods on the intel&reg; xeon phi&trade; coprocessor. In: Proceedings of the
International Conference for High Performance Computing, Networking, Storage and
AnalysisPiscataway, NJ, USA, pp. 674–684. IEEE Press (2014)
On Quantum Chemistry Code Adaptation for RSC PetaStream Architecture
121

Dtree: Dynamic Task Scheduling at Petascale
Kiran Pamnany1(B), Sanchit Misra1, Vasimuddin Md.2, Xing Liu3,
Edmond Chow4, and Srinivas Aluru4
1 Parallel Computing Lab, Intel Corporation, Bangalore, India
kiran.pamnany@intel.com
2 Department of Computer Science and Engineering,
Indian Institute of Technology Bombay, Mumbai, India
3 IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
4 School of Computational Science and Engineering, Georgia Institute of Technology,
Atlanta, USA
Abstract. Irregular applications are challenging to scale on supercom-
puters due to the diﬃculty of balancing load across large numbers of
nodes. This challenge is exacerbated by the increasing heterogeneity of
modern supercomputers in which nodes often contain multiple processors
and coprocessors operating at diﬀerent speeds, and with diﬀering core
and thread counts. We present Dtree, a dynamic task scheduler designed
to address this challenge. Dtree shows close to optimal results for a
class of HPC applications, improving time-to-solution by achieving near-
perfect load balance while consuming negligible resources. We demon-
strate Dtree’s eﬀectiveness on up to 77,824 heterogeneous cores of the
TACC Stampede supercomputer with two diﬀerent petascale HPC appli-
cations: ParaBLe, which performs large-scale Bayesian network struc-
ture learning, and GTFock, which implements Fock matrix construction,
an essential and expensive step in quantum chemistry codes. For Para-
BLe, we show improved performance while eliminating the complexity of
managing heterogeneity. For GTFock, we match the most recently pub-
lished performance without using any application-speciﬁc optimizations
for data access patterns (such as the task distribution design for commu-
nication reduction) that enabled that performance. We also show that
Dtree can distribute from tens of thousands to hundreds of millions of
irregular tasks across up to 1024 nodes with minimal overhead, while
balancing load to within 2 % of optimal.
Keywords: Petascale · Dynamic scheduling · Load balance
1
Introduction
The scheduling challenge on modern supercomputers arises from the need for
ﬁne-grained parallelism in applications. Scaling to large numbers of processors
requires that there be a large number of parallel tasks that can be distributed.
X. Liu—During this research, Xing Liu was aﬃliated with Georgia Institute of Tech-
nology.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 122–138, 2015.
DOI: 10.1007/978-3-319-20119-1 10

Dtree: Dynamic Task Scheduling at Petascale
123
This brings forth the need for eﬃcient scheduling – to distribute tasks among
all available processors so as to minimize total run time.
Applications that can be decomposed into equal-sized tasks can be scheduled
simply by statically distributing the tasks evenly among the number of proces-
sors. This additionally simpliﬁes the distribution of input data for tasks as well
as routing for any required inter-task communication. For these reasons, this
class of applications can be easily scaled to large supercomputers.
However, many applications exhibit irregular characteristics and require more
sophisticated scheduling to prevent unbalanced computing load and the resulting
unnecessary increase in total run time. Such applications vary widely, and a num-
ber of techniques have been proposed to address the diverse problems presented.
We introduce our approach by classifying irregular applications according to the
following characteristics:
1. Irregular tasks – individual tasks may vary in length.
2. Dynamic task pool – the set of tasks may be dynamic, i.e. new tasks may be
added during application execution.
3. Locality – input and/or output data for a task may be large, i.e. the cost of
movement must be considered.
4. Dependencies – there may be inter-task dependencies or communication.
There are important applications that display various combinations of one or
more of these characteristics. While it is possible to design dynamic scheduling
algorithms that cope with all of them, the resulting overhead may be needlessly
large for applications that exhibit only few of the characteristics. Hence, many
solutions in the literature focus on particular characteristics, the most common
being the data locality problem.
Devine et al. [11] address load balancing entirely in the context of data parti-
tioning. Menon and Kal´e [20] target iterative applications with a synchronous load
balancer, measuring load information with a gossip protocol and running sender-
initiated load transfer at application synchronization points. Zheng et al. [27] also
balance load periodically, but use a hierarchy of load balancing domains. These
solutions share a theme in being measurement-based – which nodes are overloaded
and which underutilized must be determined before load balancing decisions can
be taken.
Other solutions target applications using recursive parallelism, e.g. combina-
torial search or divide-and-conquer, in which the task pool is dynamic. Work
stealing [8] is the dominant technique in this space [12,19] but generalized
approaches can struggle with scaling beyond 6K processors due to large increases
in failed steals [12]. Liﬄander et al. have scaled a specialized work stealing app-
roach for iterative applications to 163K cores [16]. Guo et al. use locality hints
to improve performance and explore scheduling policies in their work stealing
scheduler, SLAW [13].
Min et al. have proposed a topology-aware hierarchical work stealing strat-
egy [21]; their implementation is in UPC [2]. Saraswat et al. extend work steal-
ing with work sharing over lifeline graphs [24]; their system is implemented in
X10 [9].

124
K. Pamnany et al.
All work stealing approaches share the problem of distributed termination
detection – if a node repeatedly fails to steal work, it cannot conclude that there
is no work left in the system; other means of detecting quiescence must be used.
Numerous scheduling algorithms exist for applications that model inter-task
dependencies as static task graphs [15]. Dependencies in applications that have
dynamic task pools may be expressed as dynamic task additions and scheduled
via work stealing.
For applications that do not have a data locality problem, applying a tech-
nique designed to minimize that problem is ineﬃcient or unfeasible. Similarly,
applications with static task pools cannot beneﬁt from a technique intended to
facilitate the easy addition of dynamically created tasks.
Our scheduler, Dtree, uses work sharing rather than work stealing, and fur-
ther diﬀers from other approaches in focusing on applications with independent
irregular tasks in a static task pool. We decouple the task scheduling problem
from the data distribution problem, focusing on balancing load by distribut-
ing tasks with minimal overhead. This allows us to demonstrate near-optimal
load balance for an application that does not require data distribution. Further-
more, for an application that does require data distribution, the high eﬃciency
of our scheduler allows us to discard application-speciﬁc optimizations designed
to reduce data movement and still match the performance.
Many other approaches impose a particular programming model or frame-
work on the application. We target the widely used and familiar hybrid MPI+X
programming model (although the applications considered in this paper use
OpenMP R
⃝, Dtree is agnostic to the shared memory parallelism model).
Finally, Dtree enables full utilization of heterogeneous nodes containing
Intel R
⃝Xeon R
⃝processors as well as Intel R
⃝Xeon Phi
TMcoprocessors1, balanc-
ing load across the diﬀerent processors transparently and thereby eliminating
the need for applications to manage such heterogeneity. The potential for this
has been discussed in the literature, but to the best of our knowledge, ours is
the ﬁrst scalable solution showing experimental results.
We brieﬂy introduced an early version of Dtree in the context of a speciﬁc
application in previous work [22]. We have since signiﬁcantly improved Dtree’s
performance and ﬂexibility, extended it to support manycore processors and
heterogeneous clusters, and evaluated it in depth, both with another large appli-
cation, and with a micro-benchmark designed to ﬁnd its limitations.
The remainder of this paper is organized as follows: we provide an in-depth
description of Dtree in Sect. 2, followed by an analysis of Dtree’s performance
using a micro-benchmark to simulate a variety of workloads in Sect. 3. Sections 4
and 5 detail how we have used Dtree with a machine learning application and a
quantum chemistry application, respectively, and present experimental results.
We summarize and conclude in Sect. 6.
1 Intel, Xeon, and Intel Xeon Phi are trademarks of Intel Corporation in the U.S.
and/or other countries.

Dtree: Dynamic Task Scheduling at Petascale
125
1.1
Experimental Setup
We performed all our experiments on the TACC Stampede supercomputer [3].
The Stampede nodes used in our experiments each contain two Intel Xeon E5-
2680 at 2.7 Ghz, one Intel Xeon Phi SE10P coprocessor at 1.09 GHz, and 32 GB
of DRAM. We compiled and ran our code using Intel Composer XE 2013 and
either Intel MPI Library 4.1 or MVAPICH2 2.0b, depending on the application.
2
Dtree
A fundamental step in scaling an application is determining how work will be
decomposed into a large number of independent tasks. Dtree addresses the chal-
lenge of scheduling these tasks across all available processors so as to minimize
runtime.
2.1
Overview
Task Model. We follow Dinan et al. [12] in referring to the set of tasks as
a task pool and requiring that all tasks in the pool are independent and able
to execute till completion without blocking or waiting for results produced by
any concurrently executing tasks. We further specify that a task pool may be
dynamic and allow task addition during program execution, or static and contain
the entire set of tasks to be completed at program start. Our approach targets
static task pools.
It is possible, with a static task pool, to assign numbers to the tasks to arrange
them in a total order. These task IDs allow a compute node to independently and
uniformly identify the task, given the task ID. Dtree is agnostic to application
task structure as it schedules tasks by distributing task IDs.
Task Characteristics. From the perspective of scheduling, the typical length
of a task and the variance in task lengths are key characteristics of a task pool.
A large number of short tasks and a smaller number of long tasks present quite
diﬀerent scheduling challenges. Similarly, tasks with high variance in length are
diﬃcult to balance whereas for tasks with low variance in length, the challenge
is to outperform a static schedule.
Dtree’s design, described in the following sub-section, addresses these chal-
lenges eﬀectively. We validate these claims in Sect. 3.
2.2
Design
Distribution Tree. A centralized task distribution scheme oﬀers excellent per-
formance [7], but can face scalability problems due to the central node becoming
a bottleneck. We address this with the well-known technique of arranging the
nodes into a tree.
Figure 1 illustrates our approach, showing an example Dtree for 32 nodes.
We arrange the tree to maximize the number of leaf nodes. The fan-out of the

126
K. Pamnany et al.
Fig. 1. Task distribution with an example Dtree for a 32 node cluster using a fan-out
of 16. Observe that leaf nodes do not require a distribution thread.
tree, 16 in the example, is the maximum number of children for which a single
node can be a parent, and is a function of the communication cost between a
child node and a parent node. The protocol used by Dtree for task distribution
is designed to minimize this communication cost, and thereby maximize the
fan-out of the distribution tree.
Task Distribution. Dtree uses receiver-initiated task distribution – a node
requests work from its parent in the tree and a parent node responds to requests
from its children. As a parent node must listen for requests from its children,
Dtree requires one thread in each parent node. All other threads in a parent
node may be used for computation.
As Dtree distributes task IDs, the application need only specify T, the total
number of tasks to be distributed. An important component of Dtree’s eﬃciency
is that tasks are distributed in groups identiﬁed by a range of task IDs, i.e. a
start and end task ID. Initially, the root node alone holds a single task group
comprised of tasks IDs 0 to T −1.
Distribution Cost. A child node’s request for work from its parent node takes
the form of an 8-byte message. A parent node’s response is a task group, sent in
a 16-byte message. Thus, a request/response pair consists of two small messages.
Dtree is built on top of MPI. Running over a typical Inﬁniband intercon-
nect, point-to-point MPI small messages see a one-way latency of approximately
1.31 ms, which equates to over 3 million messages per second [6]. In practice, this
allows us to use a very large fan-out for the distribution tree, which minimizes
the number of parent nodes required and consequently, the number of threads
required by the scheduler.
Task Allocation. At program start, a node initializes its Dtree with an init-
work() call. This returns a task group which is allocated as follows: the Dtree
root node distributes df × T tasks to its children. All parent nodes in the tree
apply the same strategy which leaves a buﬀer of tasks with each parent.

Dtree: Dynamic Task Scheduling at Petascale
127
The number of tasks allocated to a child node is computed on the basis of
the number of nodes in the child’s sub-tree. In the example Dtree in Fig. 1,
the left side sub-tree has 15 leaf nodes while the right side sub-tree has 14 leaf
nodes. Parent nodes also work, and must be counted as part of their sub-trees
for allocation purposes. Thus the left side intermediate parent node apportions
1/16 of its task allocation to each of its children and to itself. The root node
must consider itself and its 2 children, which are parents themselves. Thus it
computes distribution fractions of 1/32, 16/32, and 15/32 respectively.
When a node completes executing its initial allocation of tasks, it requests
additional work from its Dtree using a getwork() call. This request is satisﬁed
by applying the child’s distribution fraction to the tasks remaining in the buﬀer.
The resulting number is scaled by dr, which controls the task drain rate, and
subjected to a minimum threshold, tmin. When a node’s task buﬀer is exhausted,
Dtree transparently requests more work from the node’s parent.
This approach results in reducing amounts of work being issued to requesting
children, eﬀectively balancing load.
Heterogeneity. A node may specify a multiplier, nm, to its Dtree in order to
indicate its performance relative to other nodes. The default multiplier is 1.0.
As an example, if a cluster has two types of nodes, A and B, and a node of type
B executes a task from the task pool in roughly twice the time a node of type A
would take to execute that task, then nodes of type A would specify a multiplier
of 2.0 while nodes of type B would specify a multiplier of 1.0.
Dtree parent nodes use node multipliers to compute distribution fractions for
their children, eﬀectively scaling task allocations. Load can be balanced eﬀec-
tively even if these estimates cannot be provided, however Dtree can be more
eﬃcient if they are available.
Mapping Task IDs. On receiving a task group to execute from Dtree, the
application may schedule a task per thread, or use multiple (or all) threads to
process the task. In either case, the task ID needs to be mapped to the input data
for that task. The application must establish a swift means of translating the
task IDs in the task group to the requisite task data. If this data is non-local, it
may potentially be pre-fetched at this point, enabling a signiﬁcant performance
boost from computation and communication overlap. Global Arrays [4] may be
Table 1. Dtree parameters
F
Tree fan-out. Up to 1024, depending on network traﬃc
T
Total number of tasks
df
Size of the initial (static) distribution. 0.2 is a reasonable default
dr
Task drain rate. 0.5 is a reasonable default
tmin Minimum task allocation. Function of mean task length
nm
Node multiplier. 1.0 is default

128
K. Pamnany et al.
used eﬀectively for data storage as task IDs (0 to T −1) can be used to index
them.
3
Micro-benchmark
We describe the use of Dtree with full applications in the following two sections.
However, we have additionally written a micro-benchmark in order to evalu-
ate Dtree performance and eﬃciency under a range of conditions. The micro-
benchmark is an MPI application that uses OpenMP R
⃝for intra-node parallelism.
It does not perform any actual computation – a task is simply a timed delay.
As discussed in Sect. 2.1, the key considerations for scheduling the tasks in a
pool are the mean and standard deviation of the task lengths. We instrumented
the execution of the ParaBLe and GTFock applications on two real datasets
each, and recorded the actual lengths of the tasks. From this data, we have
computed the statistics shown in Table 2.
Table 2. Measured task pool statistics in seconds
Application Dataset
Mean Std. Dev.
ParaBLe
<leaf,development>
0.696
0.314
<seed,development> 0.414
0.114
GTFock
15mer
0.733
0.197
graphene.336
0.644
0.153
We observe from the histograms of the measured task lengths that they could
roughly be approximated by a Gaussian probability distribution. We apply this
observation to generate task lengths for the micro-benchmark as follows: given
a mean and a standard deviation, we use the Intel Math Kernel Library [1] to
generate pseudo-random numbers in a Gaussian distribution. Each node uses
the same seed for the generator and generates R sets of n random numbers each,
where R is the number of participating nodes, and n × R is the total number
of tasks speciﬁed. For each of the R sets, we vary the mean for the n numbers
randomly within one standard deviation. This approach produces an artiﬁcial
task pool that approximates the characteristics seen in the real applications.
In each of the following experiments, we compare the runtime and the average
load imbalance of a micro-benchmark run that dynamically schedules tasks using
Dtree, against the runtime and average load imbalance of a run that is statically
scheduled, i.e. an equal number of tasks are distributed to each node. We run one
MPI rank per node, with each rank using all 16 threads available on the two Intel
Xeon processors. For the experiments involving Intel Xeon Phi coprocessors, we
run one MPI rank per coprocessor, using all 240 threads available.

Dtree: Dynamic Task Scheduling at Petascale
129
3.1
Scaling Experiments
The total number of tasks to be distributed is signiﬁcant in evaluating scheduling
eﬃciency. Clearly, as this number reduces, it becomes harder to balance load – at
1 task per thread, it becomes impossible for any scheduler. Our ﬁrst experiment
evaluates Dtree’s eﬀectiveness in this regard. We ran these tests using Intel Xeon
processors only on 256 nodes with a mean task length of 0.5 s and a standard
deviation of 0.125 s. In Fig. 2a, we see that even with as few as 2.5 tasks per
thread (40 tasks per node with each node running 16 threads) and a total runtime
of 2 s, Dtree does better than a static schedule, although we do observe a load
imbalance of 13 %. For 40 tasks per thread, the load imbalance is 1.6 %.
We then perform a weak scaling experiment using only Intel Xeon processors,
the results of which are shown in Fig. 2b. We ran these tests using a mean task
length of 0.5 s and a standard deviation of 0.125 s with 320 tasks per node. Dtree
exhibits a load imbalance of no more than 5 %.
Fig. 2. Scaling number of tasks and weak scaling with the micro-benchmark
We then performed a strong scaling experiment using only Intel Xeon proces-
sors, running 1,310,720 tasks with a mean task length of 0.5 s and a standard
deviation of 0.125 s. The results are shown in Fig. 3a. Given a larger number of
tasks, Dtree reduces load imbalance to no more than 1.4 % of runtime, even at
1024 nodes.
Fig. 3. Strong scaling experiments with the micro-benchmark. The X-axis is # nodes
and Y-axis is time-to-solution in seconds.

130
K. Pamnany et al.
Fig. 4. Eﬀect of task length and standard deviation. Y-axis is time-to-solution in sec-
onds. Standard deviation for (a) is 0.125 s. Mean for (b) is 0.05 s.
We repeated the strong scaling experiment using only Intel Xeon Phi coproces-
sors in order to evaluate the performance of Dtree on a manycore cluster. We exe-
cute 19,660,800 tasks with a mean task length of 0.5 s and a standard deviation
of 0.165 s. The results are shown in Fig. 3b. Again, we see that Dtree reduces load
imbalance to no more than 1.9 % of runtime.
Our ﬁnal scaling experiment with the micro-benchmark evaluates perfor-
mance on a heterogeneous cluster of both Intel Xeon and Intel Xeon Phi proces-
sors. We execute 10,000,000 tasks with mean task lengths of 0.35 and 0.87 s, and
standard deviations of 0.12 and 0.0.29 s for the host processor and coprocessor
respectively. We see in Fig. 3c that despite the diﬀering speeds and core counts of
the processors, Dtree reduces load imbalance to no more than 3.7 % of runtime.
3.2
Task Length Experiments
Given the importance of task lengths in dynamic scheduling, we used the micro-
benchmark to analyze Dtree performance for task pools with diﬀering character-
istics. In particular, we assessed the impact of changing the mean task length,
and also of changing the standard deviation. We ran all these experiments on
1024 nodes, using only Intel Xeon processors.
We begin by experimenting with the mean task length. In order to maintain a
roughly consistent runtime across experiments, we increase the number of tasks
when we reduce the mean length. Thus, for a mean task length of 0.5 s, we run
6,400 tasks, whereas for a mean task length of 0.000005 s, we run 640,000,000
tasks. Note that as the task length reduces, thread scheduling overhead increases
– this issue is unrelated to Dtree but can aﬀect runtime considerably, to the point
of obscuring the performance we are evaluating. For this reason, for this experi-
ment alone, we use a single thread per MPI rank. Figure 4a shows the results of
this experiment, clearly demonstrating that Dtree can eﬀectively schedule tasks
as short as 5 ms (13,500 processor cycles).
We then explore the eﬀect of varying the standard deviation of the tasks
in the task pool. We see in Fig. 4b that even with very small variance in task
lengths, Dtree outperforms a static schedule.

Dtree: Dynamic Task Scheduling at Petascale
131
4
ParaBLe
Bayesian network structure learning is an important machine learning problem.
ParaBLe (Parallel Bayesian Learning) implements a parallel heuristic algorithm
to solve this problem at scale [22,23]. We have modiﬁed this application to use
Dtree. In this section, we provide a brief introduction to the algorithm, describe
how we use Dtree, and evaluate the performance gains from doing so.
4.1
Bayesian Network Structure Learning Algorithm
A Bayesian network is represented as a directed acyclic graph in which nodes
represent the set of variables of interest in the domain. Data regarding the obser-
vations of these variables are taken to estimate the joint probability distribution
of the variables – a Bayesian network is the graphical representation of a factor-
ization of the joint probability distribution. Automatically learning the structure
of a Bayesian network from data is an NP-hard problem [10].
Let n denote the number of variables, X, Y , etc. denote individual variables,
X denotes the set of all variables, and A, B, etc. denote subsets of X. The struc-
ture learning problem can be deﬁned as follows. Let the function s(X, A) model
the ﬁtness of choosing elements of set A as the parents of X. Let CP(X) ⊆X
denote the set of candidate parents of a variable X. For each variable X, we
need to identify Pa(X) ⊆CP(X) that maximizes s(X, Pa(X)). This requires
computing s(X, A) for all A ⊆CP(X). ParaBLe represents this computation
using a hypercube of dimension |CP(X)|, in which each node represents a subset
of CP(X). Therefore, for each variable X, all the 2|CP (X)| nodes of the corre-
sponding hypercube have to be computed. Note that choosing CP(X) = X \{X}
results in exact structure learning, which has exponential complexity. ParaBLe
heuristically determines much smaller CP(X) sets to stem the computational
complexity.
4.2
Work Decomposition
Consider the n candidate parents sets, exploring the subsets of which consti-
tutes the total amount of work to be done. The CP sets vary signiﬁcantly in
size and the corresponding work varies exponentially in the size of the CP sets,
therefore the required work for each variable is vastly diﬀerent. The number of
variables and the exponential variation in the corresponding workloads does not
permit balancing the load to thousands of nodes. To more eﬀectively distribute
work, ParaBLe chooses an r-dimensional hypercube as the largest allowed unit
of work, for a speciﬁc threshold r. For any variable X with |CP(X)| > r, the
corresponding hypercube is divided into 2|CP (X)|−r sub-hypercubes each creat-
ing a work item2. Any variable X with CP(X) ≤r creates a work item with
hypercube dimension |CP(X)|. This creates a suﬃciently large list of work items.
2 We refer to Dtree tasks as work items in this application.

132
K. Pamnany et al.
4.3
Using Dtree
The work items are arranged in a global order so that using a small metadata and
a bijective mapping, the ID of a work item can be mapped to the corresponding
(variable, hypercube) pair. The data for learning the network is small enough to
be replicated on every processor. Hence, no communication is needed to provide
the data required for a work item. ParaBLe executes one work item with one
thread. The results of the work items assigned to a node are accumulated on the
node and reduced across all nodes on completion.
ParaBLe has optimized implementations for executing work items on both
the Intel Xeon and Intel Xeon Phi processors. It uses the oﬄoad model [5]
to achieve heterogeneity, running MPI only on the host processor and using a
dynamic scheduling algorithm to balance load between the processors.
We modiﬁed the application to use the symmetric model, eliminating the
complex oﬄoading code and creating a simpler, truly heterogeneous version,
running MPI on both the host processor and on the coprocessor, and using
Dtree to balance load across all the processors and coprocessors in the cluster.
4.4
Datasets Used
ParaBLe was created to learn genome-scale networks from microarray data, a
grand challenge in systems biology, and we test Dtree’s performance for the same
application. In this application, nodes in the Bayesian network represent genes
of the underlying organism. The data comes from a large number of experiments,
each of which measures quantitatively the expression level of each gene in the
organism under diﬀerent experimental conditions. We ran our experiments on
two datasets for the plant Arabidopsis thaliana – one for the seedling tissue
type and the other for the leaf tissue type, both classiﬁed under developmental
conditions. More details on these datasets may be found in [22].
4.5
Experimental Results
We ran strong scaling experiments with ParaBLe on the two datasets over 256,
512, and 1024 heterogeneous nodes on Stampede. We measured the time-to-
solution of the implementation using the oﬄoad model with static and dynamic
task distribution across nodes using an earlier version of Dtree, against the imple-
mentation using the symmetric model with the improved Dtree. The graphs in
Fig. 5 show the results of these experiments. We see that performance improves
in all cases, up to 18 % with respect to dynamic on 1024 nodes, while load
imbalance is almost completely eliminated.
5
GTFock
Fock matrix construction is an important algorithm in quantum chemistry. It is
the most time consuming step in the widely used Hartree Fock (HF) algorithm

Dtree: Dynamic Task Scheduling at Petascale
133
Fig. 5. ParaBLe performance (static and dynamic schedule) vs. using Dtree for 256,
512 and 1024 nodes. Y-axis is time-to-solution in seconds.
and optimizations of Fock matrix construction have been studied for decades.
GTFock [17], NWChem [26], GAMESS [25], ACESIII [18] and MPQC [14] are
some of the many existing computational chemistry packages that implement
parallel Fock matrix construction and HF.
In this section, we provide a brief introduction to GTFock and to Fock matrix
construction, describe how we use Dtree, and study the resulting performance.
5.1
Fock Matrix Construction
The Fock matrix is deﬁned as
Fij = Hcore
ij
+

kl
Dkl(2(kl|ij) −(ik|jl))
(1)
where i, j, k and l are indices such that 0 ≤i, j, k, l < nf, where nf is the
number of basis functions. The two dimensional matrices Hcore and D are ﬁxed
for this computation. Each (ij|kl) is an element of a 4-dimensional array called
an electron repulsion integral (ERI) tensor. Along each dimension, the indices
are grouped into “shells” of 1 to 10 (or more) indices, which is necessary to
facilitate eﬃcient computation of ERIs in quantum chemistry programs.
GTFock deﬁnes a task3 as computing the set of ERIs,
(M, : |N, :) = {(ij|kl) s.t. i ∈shell M, k ∈shell N, 0 ≤j, l < nf}
(2)
and then computing contributions to the Fock matrix due to these ERIs. Here, M
and N are shell indices. Some ERIs can be pre-determined to be very small, and
their computation can be neglected. The ERI tensor also has 8-way symmetry,
so only unique elements of (ij|kl) need to be computed. Thus the number of
ERIs in (M, : |N, :) to be computed in each task can vary, making it hard to
balance the load.
3 We refer to Dtree tasks as work items, to prevent confusion with GTFock tasks.

134
K. Pamnany et al.
All the tasks can be represented as a 2-dimensional task array indexed by M
and N. The way the tasks are designed, there is a higher overlap between the
corresponding blocks of D and F of batches (M1, : |N1, :) and (M2, : |N2, :) if
|M2 −M1| and |N2 −N1| are small. Based on this, GTFock statically allocates
subarrays of the task array across nodes. Therefore, each node can prefetch the
required submatrices of D for all the tasks assigned to it. As a result of this
partition, there is a signiﬁcant overlap between the submatrices of D for tasks
assigned to a node, thus greatly reducing the volume of data to be communicated.
Similarly, submatrices of F can be accumulated locally ﬁrst and the global copy
can be updated once the entire parition is ﬁnished.
The blocks of tasks assigned to a node are divided into sub-blocks and com-
putation is performed one sub-block at a time. When a node is done with its
own quota, it uses work stealing to steal one of the sub-blocks from some other
node that still has some left. It will also need to copy the corresponding blocks
of D and update the corresponding blocks of F. Since this happens only towards
the end, it results in only a small increase in communication while signiﬁcantly
balancing the load.
5.2
Using Dtree
For Dtree, the work items are deﬁned as equal sized subarrays of the task array.
When a node receives a work item, it uses the size of the blocks (S) and nshells,
to uniquely map the ID to the corresponding block of the task array. It then
loads the corresponding blocks of D from the global copy, processes the work
item, and updates the corresponding blocks of F in the global copy. Larger work
items allow more overlap between the blocks of D and F within the work item
and thereby reduce communication, but also make it harder to balance load.
5.3
Datasets Used
We have used two datasets to study the performance of Dtree for Fock matrix
construction. The system 15mer is a 15 nucleotide strand of DNA containing 981
atoms, 4826 shells and 10498 functions. The system 1hsg is a human immunod-
eﬁciency virus (HIV) II protease complexed with a 92-atom ligand which is an
HIV inhibitor. We have modiﬁed the system to include only residues 8 Angstroms
from any atom in the ligand, and call this system 1hsg 80. It has 1035 atoms,
4576 shells and 9584 functions.
5.4
Experimental Results
Figure 6 shows the results of strong scaling experiments for Fock matrix con-
struction by GTFock, using four diﬀerent scheduling schemes – static, static
with work stealing, using ADLB [19], and using Dtree – on the two datasets.
As expected, the static schedule has the highest amount of imbalance, but
negligible communication. Adding work stealing signiﬁcantly reduces the imbal-
ance while increasing the communication only slightly. Dtree reduces imbalance

Dtree: Dynamic Task Scheduling at Petascale
135
(a) 256 nodes. x= 30 × 30, y= 70 × 70.
(b) 1024 nodes. x= 15 × 15, y= 35 × 35.
(c) 2025 nodes. x= 10 × 10, y= 20 × 20.
Fig. 6. GTFock strong scaling experiment. Avg. comp. time and Avg. comm. time are
averages of time spent by each node in computation and communication, respectively.
Misc. time is obtained by subtracting the average computation and communication
time from the average total time, and includes scheduling overhead. Imb. time is time-
to-solution minus average total time and quantiﬁes the load imbalance across nodes.
The bars for ADLB do not have any imbalance since every node in eﬀect waits for every
other node before exiting the scheduler. So, for ADLB, the imbalance is included in
the Misc. time. Parameters used: GTFock(# of sub-blocks = 5 × 5), ADLB(# servers
=
1
64(#nodes), task size = 1), Dtree(F = 256)

136
K. Pamnany et al.
even further, but as it must fetch data for each task separately, the communica-
tion time is much higher.
We observe that as Dtree sends a range of tasks at a time, it is possible for a
node to prefetch the data for most of these tasks, overlapping this communication
with the computation for the ﬁrst few tasks. This would signiﬁcantly reduce the
communication cost, and coupled with the lower imbalance exhibited with Dtree,
would result in a smaller time-to-solution, even relative to work stealing.
The use of ADLB suﬀers the same communication overheads as the use of
Dtree, but the time-to-solution with ADLB is much higher. There are three
possible reasons for this. ADLB probably has higher load imbalance, as well
as more overhead in task distribution. Moreover, ADLB uses dedicated servers
for work distribution, as a result of which it loses some performance. We have
experimentally determined that 1 of every 64 nodes needs to be a server. Thus,
of 1024 nodes, 16 must be servers, resulting in a loss of
1
64 = 1.6 % of the
performance.
Eﬀect of Work Item Size. We tested two work item sizes for both Dtree
and ADLB. Our results conﬁrm that smaller work items achieve better load
balancing while requiring more communication. Therefore, if the communication
and computation is overlapped, using smaller work items might be better.
6
Conclusion
We have presented Dtree, a dynamic task scheduler for applications with irreg-
ular computations. Dtree can eﬀectively balance load for widely varying task
proﬁles at petascale on heterogeneous supercomputers. We have demonstrated
these capabilities with a micro-benchmark, and with two important HPC appli-
cations: ParaBLe, for Bayesian network structure learning, and GTFock, for
Fock matrix construction. We have further shown near-perfect scaling up to 2K
MPI ranks, and ﬁnd no obstacle to scaling well beyond that. Dtree can easily be
deployed in hybrid MPI+X applications, has negligible overhead, and enables
balanced heterogeneous computation.4
Acknowledgements. The authors acknowledge the Texas Advanced Computing Cen-
ter (TACC) at The University of Texas at Austin for providing HPC resources that
have contributed to the research results reported within this paper. URL: http://www.
tacc.utexas.edu.
4 Software and workloads used in performance tests may have been optimized for
performance only on Intel microprocessors. Performance tests, such as SYSmark and
MobileMark, are measured using speciﬁc computer systems, components, software,
operations and functions. Any change to any of those factors may cause the results
to vary. You should consult other information and performance tests to assist you
in fully evaluating your contemplated purchases, including the performance of that
product when combined with other products. For more information go to http://
www.intel.com/performance.

Dtree: Dynamic Task Scheduling at Petascale
137
References
1. Intel R
⃝math kernel library MKL. http://software.intel.com/en-us/intel-mkl
2. Upc consortium. upc language speciﬁcations, v1.2. Technical report LBNL-59208,
Lawrence Berkeley National Lab (2005)
3. TACC Stampede supercomputer (2014). http://top500.org/system/177931
4. Global arrays webpage (2015). http://hpc.pnl.gov/globalarrays/
5. Intel mpi on intel xeon phi coprocessor systems (2015). https://software.intel.com/
en-us/articles/using-the-intel-mpi-library-on-intel-xeon-phi-coprocessor-systems
6. Mvapich: Performance (2015). http://mvapich.cse.ohio-state.edu/performance/pt
to pt/
7. Bhatele, A., Kumar, S., Mei, C., Phillips, J., Zheng, G., Kale, L.: Overcoming
scaling challenges in biomolecular simulations across multiple platforms. In: IEEE
International Symposium on Parallel and Distributed Processing, IPDPS 2008, pp.
1–12, April 2008
8. Blumofe,
R.D.,
Leiserson,
C.E.:
Scheduling
multithreaded
com-
putations
by
work
stealing.
J.
ACM
46(5),
720–748
(1999).
http://doi.acm.org/10.1145/324133.324234
9. Charles, P., Grothoﬀ, C., Saraswat, V., Donawa, C., Kielstra, A., Ebcioglu, K., von
Praun, C., Sarkar, V.: X10: an object-oriented approach to non-uniform cluster
computing. In: Proceedings of the 20th Annual ACM SIGPLAN Conference on
Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA
2005, pp. 519–538. ACM, New York (2005). http://doi.acm.org/10.1145/1094811.
1094852
10. Chickering, D.M., Heckerman, D., Geiger, D.: Learning Bayesian networks is NP-
hard. Technical report MSR-TR-94-17, Microsoft Research (1994)
11. Devine,
K.D.,
Boman,
E.G.,
Heaphy,
R.T.,
Hendrickson,
B.A.,
Teresco,
J.D.,
Faik,
J.,
Flaherty,
J.E.,
Gervasio,
L.G.:
New
challenges
in
dynamic
load
balancing.
Appl.
Numer.
Math.
52(2–3),
133–152
(2005).
http://dx.doi.org/10.1016/j.apnum.2004.08.028
12. Dinan, J., Larkins, D.B., Sadayappan, P., Krishnamoorthy, S., Nieplocha, J.: Scal-
able work stealing. In: Proceedings of the Conference on High Performance Com-
puting Networking, Storage and Analysis, SC 2009, pp. 53:1–53:11. ACM, New
York (2009). http://doi.acm.org/10.1145/1654059.1654113
13. Guo, Y., Zhao, J., Cave, V., Sarkar, V.: Slaw: A scalable locality-aware adaptive
work-stealing scheduler for multi-core systems. In: Proceedings of the 15th ACM
SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP
2010, pp. 341–342. ACM, New York (2010). http://doi.acm.org/10.1145/1693453.
1693504
14. Janssen, C.L., Nielsen, I.M.: Parallel Computing in Quantum Chemistry. CRC
Press, Boca Raton (2008)
15. Kwok, Y.K., Ahmad, I.: Static scheduling algorithms for allocating directed
task graphs to multiprocessors. ACM Comput. Surv. 31(4), 406–471 (1999).
http://doi.acm.org/10.1145/344588.344618
16. Liﬄander, J., Krishnamoorthy, S., Kale, L.V.: Work stealing and persistence-based
load balancers for iterative overdecomposed applications. In: Proceedings of the
21st International Symposium on High-Performance Parallel and Distributed Com-
puting, HPDC 2012, pp. 137–148. ACM, New York (2012). http://doi.acm.org/10.
1145/2287076.2287103

138
K. Pamnany et al.
17. Liu, X., Patel, A., Chow, E.: A new scalable parallel algorithm for Fock matrix
construction. In: 2014 IEEE International Parallel & Distributed Processing Sym-
posium (IPDPS), Phoenix, AZ (2014)
18. Lotrich, V., Flocke, N., Ponton, M., Yau, A., Perera, A., Deumens, E., Bartlett,
R.: Parallel implementation of electronic structure energy, gradient, and hessian
calculations. J. Chem. Phys. 128, 194104 (2008)
19. Lusk, E.L., Pieper, S.C., Butler, R.M., et al.: More scalability, less pain: a simple
programming model and its implementation for extreme computing. SciDAC Rev.
17(1), 30–37 (2010)
20. Menon, H., Kal´e, L.: A distributed dynamic load balancer for iterative applica-
tions. In: Proceedings of the International Conference on High Performance Com-
puting, Networking, Storage and Analysis, SC 2013, pp. 15:1–15:11. ACM, New
York (2013). http://doi.acm.org/10.1145/2503210.2503284
21. Min, S.J., Iancu, C., Yelick, K.: Hierarchical work stealing on manycore clusters. In:
5th Conference on Partitioned Global Address Space Programming Models (2011)
22. Misra, S., Vasimuddin, M., Pamnany, K., Chockalingam, S., Dong, Y., Xie, M.,
Aluru, M., Aluru, S.: Parallel Bayesian network structure learning for genome-
scale gene networks. In: International Conference for High Performance Comput-
ing, Networking, Storage and Analysis, SC14, pp. 461–472, November 2014
23. Nikolova, O., Aluru, S.: Parallel Bayesian network structure learning with appli-
cation to gene networks. In: Proceedings of the International Conference on High
Performance Computing, Networking, Storage and Analysis, SC 2012, pp. 63:1–
63:9 (2012)
24. Saraswat, V.A., Kambadur, P., Kodali, S., Grove, D., Krishnamoorthy, S.: Lifeline-
based global load balancing. In: Proceedings of the 16th ACM Symposium on
Principles and Practice of Parallel Programming, PPoPP 2011, pp. 201–212. ACM,
New York (2011). http://doi.acm.org/10.1145/1941553.1941582
25. Schmidt, M.W., Baldridge, K.K., Boatz, J.A., Elbert, S.T., Gordon, M.S., Jensen,
J.H., Koseki, S., Matsunaga, N., Nguyen, K.A., Su, S., et al.: General atomic and
molecular electronic structure system. J. Comput. Chem. 14(11), 1347–1363 (1993)
26. Valiev, M., Bylaska, E.J., Govind, N., Kowalski, K., Straatsma, T.P., Van Dam,
H.J., Wang, D., Nieplocha, J., Apra, E., Windus, T.L., et al.: NWChem: a com-
prehensive and scalable open-source solution for large scale molecular simulations.
Comput. Phys. Commun. 181(9), 1477–1489 (2010)
27. Zheng, G., Bhatel´e, A., Meneses, E., Kal´e, L.V.: Periodic hierarchical load balanc-
ing for large supercomputers. Int. J. High Perform. Comput. Appl. 25(4), 371–385
(2011). http://dx.doi.org/10.1177/1094342010394383

Feasibility Study of Porting a Particle
Transport Code to FPGA
Iakovos Panourgias1(&), Michele Weiland1, Mark Parsons1,
David Turland2, Dave Barrett2, and Wayne Gaudin2
1 EPCC, James Clerk Maxwell Building, Peter Guthrie Tait Road, Edinburgh
EH9 3FD, UK
{i.panourgias,m.weiland,m.parsons}@epcc.ed.ac.uk
2 AWE, Aldermaston, Reading RG7 4PR, UK
{David.Turland,Dave.Barrett,Wayne.Gaudin}@awe.co.uk
Abstract. In this paper we discuss porting a particle transport code, which is
based on a wavefront sweep algorithm, to FPGA. The original code is written in
Fortran90. We describe the key differences between general purpose CPUs and
Field Programmable Gate Arrays (FPGAs) and provide a detailed performance
model of the FPGA. We describe the steps we took when porting the Fortran90
code to FPGA. Finally, the paper will present results from an extensive
benchmarking exercise using a Virtex 6 FPGA.
Keywords: FPGA  Particle transport  Wavefront sweep
1
Introduction
Chimaera-2 is a particle transport code based on a wavefront algorithm. The code has
been developed and maintained by the UK Atomic Weapons Establishment (AWE) and
is used primarily for benchmarking during procurement processes. Chimaera-2 is
written in Fortran90 and MPI/OpenMP; it scales well to thousands of cores for large
problem sizes and is used to calculate nuclear criticality. It is one of the highest priority
application codes in the benchmark suite used by AWE for procurement of HPC
systems.
This paper will discuss the programming strategies required to port a CPU-centric
HPC code to Field Programmable Gate Arrays (FPGAs) by an application developer
with no prior knowledge of FPGA/embedded programming. This feasibility study will
provide a detailed performance model of the FPGA implementation of Chimaera-2 and
we will show how the model can forecast the runtime of an FPGA ported application.
A mini-app, which closely matches the Chimaera-2 code, was implemented and used to
evaluate our porting strategy and the performance model predictions. Finally, we will
describe the steps we took when porting parts of the Fortran90 code to FPGA using
Maxeler’s FPGA solution. The limiting factors of the algorithm will be discussed and
we will provide solutions for future development.
© Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 139–154, 2015.
DOI: 10.1007/978-3-319-20119-1_11

1.1
Contributions
Although writing applications for FPGAs and embedded devices dates back to the 1990s
and the difﬁculties and steep learning curve have been mentioned in research papers
feasibility studies of using new methodologies have not been sufﬁciently explored. This
paper is amongst the ﬁrst to consider new abstract methods of porting HPC CPU
applications to FPGA. Furthermore, this paper studies the feasibility of an application
developer with no knowledge of FPGA or embedded computing being able to port a
large and complex HPC application. This paper makes the following contributions:
• We discuss and evaluate a set of techniques which enable an application developer
to predict the performance characteristics of an FPGA application;
• We describe the conceptual differences between CPU and FPGA programming;
• We describe the implementation of a mini-app that closely matches Chimaera-2 to
test different porting strategies and to evaluate the performance model predictions.
2
Related Previous Work
This section will discuss previous attempts of porting a particle sweep transport code to
accelerators. The Denovo code [1], produced by the Scientiﬁc Computing Group at
Oak Ridge National Laboratory, serves a similar purpose to Chimaera-2 – it is used for
advanced nuclear reactor design and as a benchmark code. Denovo also works on a
three dimensional domain, but unlike Chimaera it is written in C ++. Denovo has been
ported to work with GPUs using CUDA for the three dimensional case with perfor-
mances that achieve from 1 to 10 % of the peak GPU performance; however these are
for three dimensional systems that parallelise more extensively, including groups (or
energy levels), which expose additional levels of parallelism. Work by Gong et al.
[2, 3] attempted to convert another three dimensional sweep code, Sweep3D1, a
benchmark code produced by the Los Alamos National Laboratory, to work on GPUs.
Their results obtained overall performance speedups for a single NVIDIA Tesla M2050
GPU ranging from 2.56, compared to an Intel Xeon X5670 CPU, to 8.14, compared to
an Intel Core Q6600 CPU with no ﬂux ﬁx-up (with ﬂux ﬁx-up on an M2050 was 1.23
times faster than on one X5670). In [4], Gong et al. go on to look at 2D problems using
a “Discontinuous Finite Element – Sn” (DFE-Sn) method [5], using a 2D Lagrangian
geometry. The performance speed-ups they obtain for simulations on an M2050 GPU
range from 11.03 to 17.96 compared with an NTXY2D [6] on Intel Xeon X5355 and
Intel Core Q6600. In their case the energy groups in the calculation are regarded as
inherently independent and thus can be executed in parallel.
3
Chimaera-2
The full problem space for a Chimaera execution is a ﬁxed uniform rectilinear 3D
spatial mesh of grid points. All faces of the problem space are squares or rectangles.
1 The code can be downloaded from http://wwwc3.lanl.gov/pal/software/sweep3d.
140
I. Panourgias et al.

For running in parallel using MPI, Chimaera performs a simple domain decom-
position of this mesh as shown in Fig. 1. The front face is divided into N rectangles
(with N being the number of MPI tasks) that are projected the whole way from the front
face to the back face to form 3D elongated cuboids. These 3D domains are often
referred as “pencils”.
Each iteration of the solver algorithm used in Chimaera-2 involves sweeping
through the whole spatial mesh eight times, each of the sweeps starting at one of the
eight vertices of the mesh and proceeding to the diametrically opposed vertex. As
the boundaries between pencils are crossed, MPI message passing is used to transmit
the updated data to adjacent pencils.
Figure 2 shows a single pencil (MPI domain) with a 6 × 6 square cross section of
mesh points. 103 geometry sizes can be used to test correctness; however, realistic
problems are likely to be larger. 2003 to 5003 geometries are common and sizes over
1,0003 are considered very large. The sweep direction being illustrated starts at the
front top-right vertex; therefore this MPI task will receive data from the pencil to its
immediate right and from the one immediately above and transmit data to the left and
downwards. Within the pencil, the order of computation is shown by the following
code fragment in Code Listing 1.
Note that this assumes that the data in the 3D arrays are held in the same sequence
as the sweep direction; that is that the front top-right element has the index (1,1,1) in
Fortran syntax. With the data still in this sequence for other sweep directions, one or
more of the loops end up being computed in reverse order. For example, “DO K = 1,
KEND” might become “DO K = KEND,1,-1” when we compute sweep “Front, Top,
Left”. This is because each of the J, K, and L loops exhibits sequential dependence
where every iteration depends on the previous iteration and must be computed in the
order corresponding to the sweep direction.
Fig. 1. Chimaera domain decomposition for a 16-way parallel run
Feasibility Study of Porting a Particle Transport Code to FPGA
141

Although all three spatial loops (i.e. J, K and L) have this sequential dependence,
the order of nesting can be changed in any way without affecting the results (except for
the complication that it has been chosen to perform the MPI communication after each
iteration of the L-loop). The M-loop is concerned with the directions of the particles
being tracked and is independent of the J, K and L loops. It does not have sequential
dependence and can be logically nested at any point. With the above algorithm, the
computational Kernel within the MPI layer processes a single 2D tile (indexed by K
and J), which moves from front to back as the outer L-loop is iterated. The Chimaera-2
application is used for procurement purposes at AWE and has been maintained and
optimised for the past 10 years. The algorithm has been optimised for serial, MPI,
OpenMP and GPU runs.
Chimaera sweep order:
Original design: 2D tiles
MPI data
FromTop
8 sweep directions starting from
each of the 8 vertices of the 3D mesh
This diagram shows sweep from
Front, Top, Right vertex.
MPI data ToBottom
MPI
data
ToLeft
MPI data
FromRight
MPI decomposition is 
elongated cuboids 
from front to back.
Fig. 2. Original 2D tile design, where computation order across the tile is a simple x and y pair
of loops.
142
I. Panourgias et al.

3.1
Mini-app
Our goal was to port the computationally expensive Kernel of the Chimaera-2 appli-
cation to the FPGA. However, in order to better understand the effects of porting
different parts of the code using different techniques we implemented a mini-app. The
mini-app uses less memory than the full Chimaera-2 application; however, the com-
putational Kernel is identical to the full application showing the same data depen-
dencies. It also uses the same number of arrays as input and output.
The mini-app does not implement 8 different sweeps; rather a single sweep (Front,
Top, Left) is used. Furthermore, it does not use MPI or OpenMP. The initial values of
the arrays are populated programmatically rather than reading an input ﬁle. The mini-
app allowed us to experiment with different porting techniques and to run large sim-
ulations. Chimaera-2 uses more than 40 GBs of RAM for a 1203 simulation; whereas,
the mini-app only uses 12 GBs of RAM for a 1,0003 simulation.
4
Porting from CPU to FPGA
Porting to an FPGA historically required writing code in VHSIC Hardware Description
Language (VHDL) or Verilog. VHDL and Verilog are hardware description languages
(HDLs) and are used to describe the structure, design and operation of electronic
circuits. Unlike an Application-Speciﬁc Integrated Circuit (ASIC), an FPGA is a
semiconductor device that can be conﬁgured by the end user. FPGAs contain pro-
grammable logic blocks, that can perform complex Boolean logic functions or simple
AND or XOR operations. These logic blocks can be connected in order to provide
more complex functionalities. The logic blocks are made up of two basic components:
ﬂip-ﬂops (FF) and lookup tables (LUT). Many logic blocks can be combined to create
simple arithmetic operations (addition, multiplication) or higher math operations like
abs or even trigonometry operations like sin and cos.
There is a growing interest in using FPGAs as energy efﬁcient accelerators and new
programming models designed to make FPGA programming more widely accessible
are gradually emerging. The work presented in this paper uses Maxeler’s Multiscale
Dataﬂow solution to port Chimaera-2 to FPGA. This approach allows an application
programmer to use a high-level (Java-like) language, called MaxJ, as an intermediary
step before using the Maxeler tool chain to translate the code to VHDL. The VHDL
ﬁles are then used by a set of third party vendor tools, which build the conﬁguration
bitstream that is uploaded to the FPGA. The FPGA used here is a Max3 solution which
is based on a Xilinx Virtex-6 (V6-SXT475) FPGA with 24 GB RAM. Maxeler’s
solution allows an application to be split into three parts:
• Kernel(s), which implement the computational components of the application in
hardware;
• Manager conﬁguration, which connects Kernels to the CPU, engine RAM, other
Kernels and other dataﬂow engines via MaxRing;
• CPU application, which interacts with the dataﬂow engines to read and write data to
the Kernels and engine RAM.
Feasibility Study of Porting a Particle Transport Code to FPGA
143

– Dataﬂow variable type
FPGAs allow the representation of a number by using an arbitrary number of bits. It is
possible to either use pre-deﬁned primitive variable types (such as 32-bit integers or 64-
bit ﬂoats) or deﬁne a custom data format. For instance, if it is known that an algorithm
requires 48-bit precision a matching custom type can be created.
– Streams of data
Dataﬂow computing operates on windows into data streams. The data window can be
held in the on-chip memory of the FPGA and thus minimises data transfer costs. We
can use static or variable stream offsets to look back or forward into a stream of data.
Variable stream offsets can be set once for the duration of a Kernel, or they can be
modiﬁed per FPGA clock cycle.
– Loops/Counters
Loop statements are heavily used in High Performance Computing. Simple loops can
be easily ported to an FPGA. A simple loop applies the same operation to a group of
data elements. In an FPGA this translates to streaming data element through the same
logic block. Unfortunately, not all loop statements are simple. In order to implement
more complex loop statements on an FPGA it is necessary to keep track of where we
are in the stream. Furthermore, many HPC applications use nested loops. We use
counters to port nested or loops with boundary conditions.
– Data Dependency Loops
So far discussion has been limited to simple loops, where each iteration of the loop is
using values from the incoming stream(s) and loop counters. However, many loops in
HPC applications have dependencies and each iteration relies on the values calculated
in previous iterations. In order to port these loops, it is necessary to either unroll the
loops or create cyclic graphs. Unrolling loops is only possible if the number of itera-
tions is known (or if it is known that the number of iterations will be N at most).
Loop unrolling on the FPGA uses replicated hardware resources and the limit is
therefore the size of the FPGA. Unfortunately in many HPC applications the number of
iterations per loop will be large enough to consume all available hardware resources.
Therefore the better option is to create a cyclic data graph, in which the output of the
loop stream can be connected to the input stream. Thus, the output of a calculation is
available as input.
– Scalar inputs and outputs
In addition to streaming input and output data between CPU and FPGA, it is also
possible to transfer read-only conﬁguration values at runtime.
– Boolean control logic
Since FPGA designs are built in advance, FPGA code must be able to handle all
possible execution paths through the code. For every conditional “if/then/else” state-
ment a multiplexer is created, which diverts the data streams (using multiplexers) to the
correct logic block path.
144
I. Panourgias et al.

5
Performance Model
Estimating the performance of a dataﬂow implementation is simpler than for a CPU
implementation due to the static scheduling of the FPGA. The FPGA design is built
out-of-band using the MaxCompiler and the FPGA vendor tools. The bitstream is then
loaded on the FPGA and can be used by an HPC application. We can estimate the
performance of an application with a known runtime in cycles (like Chimaera 2)
executing an FPGA design. If the runtime is not known, it is still possible to model the
performance of the application; as long as the application uses a regular pattern for
reading and writing data to external interfaces. The FPGA has a set of well-deﬁned
external interfaces (PCIe and DRAM) with ﬁxed performance capabilities. Further-
more, the FPGA does not create or handle any dynamic events, like interrupts. Also the
FPGA does not use pre-emption for interrupting the kernels since only a speciﬁc set of
kernels can be executed at one time. Finally, out of order execution and cache misses
are not relevant on an FPGA. The FPGA design dictates the compute nodes that will be
used and the data that will be computed.
Thus, the performance of an FPGA design depends on the following factors:
• Number of cycles per kernel
• Data read from and written to DRAM
• Data read from and written to PCIe
• Data read from and written to Network Interface
The total time that an application spends in a dataﬂow execution is computed by:
TTotal ¼ max Tcompute; TDRAM; TPCIe; TMaxRing


ð1Þ
5.1
Model Compute
As mentioned earlier, the dataﬂow design uses static scheduling. Therefore TCOMPUTE
is the time that it takes the input data to run through the dataﬂow design:
Tcompute ¼
Cycles
Frequency
ð2Þ
The frequency of the design determines the compute speed. For simple single pipe
designs the number of cycles is equal to the number of inputs, i.e. 1. However, for
multi-pipe designs the number of cycles is equal to the number of input divided by the
number of pipes.
As an example, in order to compute the loops K, J and M (from Code Listing 1) we
need to run for K*J*M cycles. Thus, for the 103 simulation with M = 36, the FPGA will
run for 3,600 cycles. Using a frequency of 150 MHz we estimate that the FPGA will
run for 24 μs. If we port the L loop, the FPGA will run for L*K*J*M cycles. Thus, for
the 103 simulation the FPGA will run for 36,000 cycles or for 240 μs. However, the
LKJ FPGA port will be called L fewer times than the KJ FPGA port. It is estimated that
the overhead of calling the FPGA is close to 7 ms. As the size of the simulation
Feasibility Study of Porting a Particle Transport Code to FPGA
145

increases, the overheads of calling the FPGA many times for small runtimes also
increases.
5.2
Model PCIe
PCIe is one of the three different ways of getting data into an FPGA. PCIe is a bi-
directional link which can send and receive data in parallel without sharing bandwidth.
To estimate the PCIe transfer rate we can use the following formula:
TPCIe ¼ max
BytesInPCIe
BandwidthInPCIe
;
BytesOutPCIe
BandwidthOutPCIe


ð3Þ
The number of bytes transferred to and from the FPGA is known at compile time. For
the Chimaera-2 application it is known that the KJ FPGA port is streaming
Bstream = K*J*M*sizeof(double)*number of array bytes to the FPGA. The values for
K, J and M depend on the size and conﬁguration of the simulation. The “number of
arrays” value depends on the implementation and is equal to 8 for the KJ FPGA port.
The naïve implementation copies redundant data to the FPGA. This approach simpliﬁes
the logic on the FPGA, but the cost of copying more data slows down the application.
The optimised version uses more logic on the FPGA in order to re-arrange arrays which
hold replicated data. For example, the naïve implementation copies three 2D arrays as
three 3D arrays. The optimised implementation copies three 2D arrays.
We also know that the design is streaming out to the host Bstream Bytes. The naïve
approach copies all output values to the host (even if some of them are temporary and
not useful). The optimised version on the other hand only copies the minimum data
back to the host that is needed for future iterations. Table 1 shows the two versions:
The ideal PCIe bandwidth is 1.8 GBs/s; however, for our calculations we also used
a non-ideal value of 1.5 GBs/s. The non-ideal bandwidth reﬂects real life benchmarks
using Chimaera-2 and the mini-app. Table 2 shows the PCIe transfer times in seconds.
It can be seen that the optimised KJ FPGA port reduces the time spend on PCIe data
transfers. The reason behind the performance improvement is that the optimised KJ
FPGA port transfers less data to the FPGA. The naïve KJ FPGA implementation copies
superﬂuous data, thus spending more time on the host in order to create the data arrays
and spending more time when transferring the data to/from the FPGA.
Table 1. GBs transferred for KJ FPGA port
Size
Total naïve (GBs)
Total optimised (GBs)
103
0.00038
0.00014
603
0.01352
0.00405
5003
0.93877
0.27142
1,0003
3.75509
1.08302
146
I. Panourgias et al.

5.3
Model DRAM
The FPGA has two different types of memory: DRAM and BRAM. The BRAM
memory is a very fast close to the FPGA logic chips memory of a very limited size. The
BRAM on the FPGA is less than 10 MBs in size. The FPGA also has 24 GBs of DDR3
DRAM on-board. Like any DRAM, it shares read and write bandwidth on a single bus
(unlike the PCIe bus, which has different bandwidths for read and write). The time
spent on writing and reading to the DRAM can be expressed by the following formula:
TDRAM ¼ BytesInDRAM þ BytesOutDRAM
BandwidthDRAM
ð4Þ
The performance of writing to and reading from the DRAM is also affected by two
other factors: memory frequency and memory access pattern.
Memory frequency is chosen at build time. A faster memory frequency usually
yields a higher bandwidth; however, a faster memory frequency makes the design
harder to build due to the stricter timing requirements imposed by the faster frequency.
Memory access patterns play a more signiﬁcant role to the DRAM performance. If
we try to a use a semi-random access pattern, e.g. one where the data is not located
sequentially on DRAM, the performance will be very poor. The maximum bandwidth
that can be achieved is expressed by the following formula:
Bandwidth ¼ BytesDIMM  No DIMMS  FrequencyDRAM  Efficiency
ð5Þ
“Bytes per DIMM” is 8 Bytes on the MAX3. However, it can be conﬁgured and set
during the build phase of the design. The “No_DIMMS” is based on the actual FPGA
hardware that is used (in our case this is 6 DIMMS). The frequency of the DRAM
controller is also set during the build phase of the design. It can vary from 606 up to
800 MHz (for both read and write). The efﬁciency depends on the size of the transfer
and the access pattern. Assuming a perfect efﬁciency and using the maximum fre-
quency for both read and write operations a MAX3 DRAM subsystem can achieve a
theoretical bandwidth of 38.4 GB/s. More modern FPGAs can achieve higher fre-
quencies and are also able to accommodate more complicated designs.
Our implementation used a streaming approach rather than copying data to the
DRAM. Thus, we will not use TDRAM and TNETWORK in our calculations. The algo-
rithm which we used did not require the use of DRAM, however in order to inform
design choices it is useful to estimate the time spent on the DRAM. As long as we
know the amount of required data we can model the performance of the DRAM
subsystem.
Table 2. Model KJ FPGA port PCIe performance in seconds
Size
Naïve In
Naïve Out
Optimised IN
Optimised OUT
103
0.00011
0.000080
0.000037
0.000034
603
0.00429
0.00321
0.00115
0.0011
5003
0.29802
0.22357
0.07606
0.07472
1,0003
1.19209
0.89407
0.3032
0.29847
Feasibility Study of Porting a Particle Transport Code to FPGA
147

6
Results
This section discusses the performance results of porting the mini-app and Chimaera-2
to FPGA. Due to the memory requirements of the Cimaera-2 code we were able to run
simulations with geometry sizes up to 1203. The mini-app enabled us to run simulations
with geometry sizes up-to 1,0003.
The CPU run-times were obtained on a workstation with an Intel Core i7-2600S
CPU @ 2.8 GHz (3.8 GHz with turbo mode) and running CentOS release 6.6. The
workstation had 32 GBs of DDR3 RAM and the mini-app and Chimaera-2 applications
were using a single core. The Fortran90 code was compiled using the GNU Fortran
compiler (v4.4.7) and the O3 optimisation ﬂag. The C code was compiled using the
GNU Compiler Collection compiler (v4.4.7) and the O3 optimisation ﬂag.
Both the mini-app and Chimaera-2 code stream data to/from the FPGA. Hence, we
did not use the on-board DRAM. The input data are streamed to the FPGA before the
execution of the compute Kernel and the computed values are streamed back to the
host. The amount of data copied to/from the FPGA depends on the port implementation
(naïve or optimised) and the number of ported directional loops (KJ or LKJ).
6.1
Mini-app Naïve KJ Implementation
Using the performance model calculations it is possible to estimate TCOMPUTE and
TPCIe. Based on this model, we created two versions of the application: a naïve and an
optimised implementation. The ﬁrst 2 columns of Table 3 show the performance in
seconds for streaming data (TPCIe) to/from the FPGA using the naïve implementation
and using 1.8 GB/s bandwidth. The third column shows the performance (in seconds)
of running the FPGA at 100 MHz. The next two columns select the maximum of each
phase (TPCIe to/from and TCOMPUTE) and use it to model the performance of the naïve
mini-app using either 1.8 GB/s and 1.5 GB/s PCIe rates.
Table 3 shows the naïve implementation of the mini-app is PCIe bound. For small
simulations the overhead of calling the FPGA multiple times makes the runtime
compute bound. However, for simulations larger than 1003 the application becomes
PCIe bound. The last two columns of Table 3 show the actual duration of the naïve
mini-app in seconds (for two different designs; using 100 MHz and 170 MHz speeds).
It can be seen that increasing the operation speed of the FPGA by 70 % does not affect
the duration of the mini-app; since it is PCIe bound. For example, for the 1,0003
Table 3. Mini-app (KJ) Naive TPCIe, TCOMPUTE, modelled and actual duration
Size
To
FPGA
From
FPGA
Compute
total
Duration
1.8 GB/s
Duration
1.5 GB/s
Actual
duration
Actual
duration*
103
0.0008
0.0006
0.0736
0.0008
0.0008
0.00657
0.00453
1003
0.895
0.5960
0.43
0.895
1.0747
1.1515
0.9213
5003
111.80
74.51
48.50
111.80
134.16
124.56
115.37
1,0003
894.22
596.05
367.00
894.22
1,073.06
963.66
950.08
148
I. Panourgias et al.

simulation the 100 MHz design runs for 963 s; whereas the 170 MHz design runs for
950 s (a performance improvement of 1.3 %).
Furthermore, the performance model is able to accurately predict the actual per-
formance of the application. For the 1,0003 simulation, the performance model predicts
a duration between 894 and 1,073 s using a speed of 1.8 GB/S and a less favourable
speed of 1.5 GB/s. The actual duration is 963 and 950 s (for the 100 MHz and
170 MHz designs). The actual duration corresponds to a * 1.65 GB/s PCIe transfer
rate.
6.2
Mini-app Optimised KJ Implementation
The optimised mini-app application reduces the need for redundant data copies to/from
the FPGA. In order to reduce the amount of data that needs to be copied it is necessary
to implement more logic on the FPGA. Three of the input arrays are used as read-only
memory and it was possible to reduce the size of the arrays from three dimensions to
two. The three read-only arrays are accessed using a constant pattern. The pattern is not
the same for each array; however, it is the same for the runtime of the application. We
implemented the pattern on the FPGA using logic blocks and several fast memory
blocks to store the data. The effect that this reduction has on TPCIe can be seen in the
ﬁrst two columns of Table 4. The third column shows that the optimised version of the
mini-app is compute bound. Since the optimised version of the mini-app is compute
bound, increasing the operational speed of the FPGA by 70 % (from 100 MHz to
170 MHz) also improves the runtime performance of the mini-app.
As mentioned earlier, the performance of an application on an FPGA can be
improved by increasing the number of pipes that are used, which in turn increases the
number of calculations per cycle. In order to perform more calculations the FPGA
design increases the width between logic blocks. Instead of streaming 64 bits of data
between logic blocks, multiples of 64 bits are streamed (i.e. 128 bits for 2 pipes, 256
bits for 4 pipes, and so on) using more logic blocks (such as adders or multipliers);
thus, more physical space is required on the FPGA. Fortunately, there is an initial cost
for each resource; however, doubling the resource usage does not double the actual
hardware usage. We experienced 50 % increase of actual resource usage whenever we
doubled the number of pipes.
The TPCIe performance model of the “double” implementation is identical to the
model of the “single”, because the same amount of data is being streamed. However,
Table 4. Mini-app (KJ) TPCIe, TCOMPUTE, modeled and actual duration
Size
To
FPGA
From
FPGA
TCOMPUT
Duration
1.8 GBs
Duration
1.5 GBs
Actual
duration
Actual
duration*
103
3.4E-4
2.8E-4
0.0736
0.07036
0.07036
0.00587
0.0041
1003
0.306
0.2995
0.43
0.43
0.43
0.11267
0.0970
5003
37.88
37.29
45.35
45.35
45.46
52.2
39.9
1,0003
302.6
298.2
360.7
360.7
363.13
375.5
318.1
Feasibility Study of Porting a Particle Transport Code to FPGA
149

TCOMPUTE is now reduced to 50 % as two cells are computed per cycle and the FPGA
Kernel will therefore run for half the time. Unfortunately in that case the problem is
again PCIe bound. Figure 3 shows a comparison between the various versions of the
mini-app.
If the double pipes mini-app application was not PCIe bound a performance
increase of * 100 % could be expected. And since there is enough physical space on
the FPGA to use even more pipes, a doubling of performance with each step could be
expected.
Since the problem is now PCIe bound, an option is to employ compression on the
host side to reduce the amount of data that needs to be transferred. Uncompressing the
data on the FPGA is free in terms of compute cost. However, one has to take into
account that compressing the data on the host side would increase the runtime of the
host application.
6.3
Mini-app LKJ Implementation
Using the same methodology as above we model TPCIe and TCOMPUTE; the results are
shown in Table 5. We can see that porting the L loop, the application becomes compute
bound. However, the amount of data that needs to be streamed to/from the FPGA has
been reduced. For example, for the 1,0003 simulation the LKJ version streams 7.9 GBs
and streams back 0.53 GBs (as opposed to 0.54 GBs and 0.53 GBs for the KJ version).
The LKJ version streams the data only once, whereas the KJ version has to repeat the
streaming operation 1,000 times.
The last two columns of Table 5 show the actual duration and the modelled
duration of the mini-app LKJ port. Since the mini-app LKJ port is compute bound, a
speed increase from 100 MHz to 170 MHz reduces the runtime of the application.
Fig. 3. Mini-app (KJ) speedup comparison
Table 5. Mini-app (LKJ) TPCIe, TCOMPUTE, modeled and actual duration
Size
To
FPGA
From
FPGA
TCOMP
Duration
1.8 GBs
Duration
1.5 GBs
Actual
Duration
Actual
duration*
103
4.0E-05
3.0E-05
0.0011
0.0011
0.0011
0.00195
0.00123
603
0.0020
0.00108
0.0785
0.0785
0.0785
0.07962
0.04636
5003
0.59
0.07
45.01
45.01
45.01
1,0003
4.44
0.30
360.01
360.01
360.01
150
I. Panourgias et al.

We are not able to run simulations larger than 603. The M loop calculations are
independent; however, the K, J and L loop calculation are not. Our design creates a
cyclic loop on the K, J and L output streams in order to drive the output calculation as
input for the next iteration. The Virtex FPGA allows * 1 MB of look-back buffers per
stream. For the 603 simulation 0.9887 MBs (K*J*M entries) are used as buffer. For the
703 we need 1.34 MBs, which is larger than the available buffer. In order to be able to
execute problems that are larger than 603 it is necessary to modify the algorithm and
use the DRAM as a temporary buffer. Furthermore, if the size of the simulation
increases to more than the available DRAM there will not enough space on the FPGA
to hold the temporary data. Another option to support larger problem sizes is to modify
the computational algorithm. The current algorithm computes a 2D face and then
moves back in the L (Z) dimension. However, it is possible to instead compute a whole
pencil in the L (Z) dimension and then move to the next pencil in the J (X) dimension.
This modiﬁcation would reduce the look back buffer to L*M entries. Thus, we could
run 3,6003 simulations.
6.4
Chimaera-2 Port
We implemented two versions of the K, J and M loop FPGA port for the Chimaera-2
application: the naïve and the optimised versions. The Chimaera-2 kernel uses 8 input
arrays and 6 output arrays.
The ﬁrst three columns of Table 6 show TPCIe and TCOMPUTE for the optimised
version of Chimaera-2 kJ port, which reduces the amount of data that needs to be
copied to/from the FPGA. As an example, the PCIe duration of the optimised version
compared to the naïve for the 1203 simulation dropped from 8,261 to 2,154 s, which
represents an improvement of almost 4x.
The last column of Table 6 shows the actual duration of the FPGA kernel calls. For
small simulation sizes the overheads cause the model to drift. However, as the simu-
lation sizes increase the predictions of the model match the duration of the Kernel.
Figure 4 (top left) shows the overall runtime of the Chimaera-2 application. The
difference between the naïve and optimised approach grows larger as the simulation
increases in size.
Figure 4 (top right and bottom left) show the performance improvement of reducing
data transfers to/from the FPGA. The ﬁrst ﬁgure shows the speedup of the overall
application runtime whereas the second compares only the durations of the Kernels.
Table 6. Chimaera-2 (KJ) TPCIe, TCOMPUTE, modeled and actual Duration
Size
To FPGA
From FPGA
TCOMPUTE
Duration 1.8
GBs/s
Duration
1.5GBs/s
Actual
Duration
103
1.69
1.43
3.9
3.9
3.9
10.93
603
305.91
292.47
248.37
305.91
367.09
403.81
1003
1,390.41
1,345.56
1,098.66
1,390.41
1,668.50
1,588.80
5003
169,162
166,198
133,595
169,162
202,994
1,0003
1,348,658
1,327,596
1,067,831
1,348,658
1,618,389
Feasibility Study of Porting a Particle Transport Code to FPGA
151

Both implementations (naïve and optimised) are PCIe bound. It can be seen that as
the size of the simulation increases, both implementations perform better. However,
this improvement will only last until the PCIe links become saturated. Once the PCIe
links are saturated performance improvements will plateau. It can be seen that the naïve
implementation speedup plateaus with the 603 simulation.
We also implemented a version of the optimised code, which replaced double
ﬂoating point numbers with single precision numbers in order to further reduce the
amount of data that needs to be transferred. The single precision version is faster
than the double precision implementation. However, it is not twice as fast even though
data transfer is reduced by 50 %. From Table 6 it can be seen that even though the
KJ optimised implementation is PCIe bound, the value of TPCIe is very close to
TCOMPUTE. Thus, when using single precision TPCIe is reduced and the performance
limiting factor is again TCOMPUTE.
7
Conclusions
We have shown how an application programmer with limited knowledge of FPGAs
and embedded computing can port a complicated production-ready Fortran90 code to
an FPGA and achieve acceptable performance. We have also seen that porting a greater
Fig. 4. Overall duration (CPU, naïve and optimised) (top left); overall speedup (naïve, optimised
and single precision) compared to single core CPU run (top right); kernel only speedup (naïve
and optimised) compared to single core CPU run (bottom left); speedup of M, JM and KJM loop
implementations (bottom right)
152
I. Panourgias et al.

proportion of the code to the FPGA improves performance. Our latest design uses
around 20 % of the available resources (thus we have enough physical space on the
FPGA to port more code or to use multiple pipes).
We have also shown how an application programmer can use a mini-app to test
different porting strategies. We used the mini-app to implement a streaming version; a
version which ﬁrst copies to DRAM and then streams the data back; a version which
copies to DRAM the input and output arrays and a version which copies some of the
small arrays to the BRAM. Using the mini-app results, we decided to implement the
data streaming version for the full application.
Figure 4 (bottom right) shows the speedups of the M, JM and KJM loop imple-
mentations. The M loop implementation clocked a “speedup” of 0.0076, whereas
the KJM implementation reached 0.93 of the single CPU performance. Furthermore, as
we minimise data transfers and allow the FPGA to re-use data that are already on-chip
we realise that one of the most important optimisations that an application programmer
can implement is to reduce data trafﬁc. FPGAs can provide Terabytes/sec of data
bandwidth between the logic blocks and BRAM. The bandwidth drops to tens of
gigabytes/sec for DRAM memory. Furthermore, newer FPGAs increase the available
physical space for logic blocks thus allowing even more code to be ported. Also, they
run at faster speeds and include more and faster memory (BRAM and DRAM).
We plan to change the algorithm and port the L loop on the FPGA. We need to
modify the algorithm due to the limited physical resources of the FPGA. The existing
algorithm solves a 2D plane and then moves (backwards or forwards) to the next plane
in the Z dimension. However, this approach requires “X*Y*M” look-back entries on
several streams. We will modify the algorithm and solve a pencil of Z length before
moving to the next pencil in the X plane. Thus we will reduce the amount of look-back
buffer to Z*M entries. If we have enough physical resources we plan to use multiple
pipes in order to compute multiple cells per cycle. Since, the LKJ port will be compute
bound, each extra pipe will improve performance.
In order to reduce the amount of data being copied to/from the FPGA we can use
compression. We plan to measure the effects of compression and perform a cost beneﬁt
analysis to help us decide if the extra time spent on the host CPU to compress data
reduces the overall runtime of the application.
Finally, once the performance of the FPGA port is on par with the performance of a
compute node we plan to measure and compare the energy consumption of the FPGA
and the host platform.
References
1. Joubert, W.: Oak Ridge National Laboratory. Presentation given at the OLCF Titan Summit
2011. Porting the Denovo Radiation Transport Code to Titan: Lessons Learned. http://www.
olcf.ornl.gov/wp-content/uploads/2011/08/TitanSummit2011_Joubert.pdf
2. Gong, C., Liu, J., Chi, L., Huang, H., Fang, J., Gong, Z.: Accelerated simulations of 3D
deterministic particle transport using discrete ordinates method. J. Comput. Phys. 230, 6010–
6022 (2011). http://www.sciencedirect.com/science/article/pii/S0021999111002348
Feasibility Study of Porting a Particle Transport Code to FPGA
153

3. Gong, C., Liu, J., Chen, H., Xie, J., Gong, Z.: Accelerating the Sweep3D for a graphic
processor unit. J. Inf. Process. Syst. 7(1), 63–74 (2011). doi:10.3745/JIPS.2011.7.1.063
4. Gong, C., Liu, J., Chi, L., Huang, H., Gong, Z.: Particle transport with unstructured grid on
GPU. Comput. Phys. Commun. 183, 588–593 (2012). http://www.sciencedirect.com/science/
article/pii/S0010465511003870
5. Plimpton, S. Hendrickson, B., Burns, S., McLendon, W., Rauchwerger, L.: Parallel Sn sweeps
on unstructured grids: Algorithms for prioritization, grid partitioning, and cycle detection.
Nuclear Science and Engineering, vol. 150, p. 267 (2005). http://www.sandia.gov/*bahendr/
papers/Rad-Transport.pdf
6. Fu, L., Yang, S.: Researches on 2-D neutron transport solver NTXY2D, Technical report,
Institute of Applied Physics and Computational Mathematics, Beijing, China (1999)
7. Maxeler, MaxCompiler Tutorial, v2014.1.1
154
I. Panourgias et al.

A Scalable, Linear-Time Dynamic Cutoﬀ
Algorithm for Molecular Dynamics
Paul Springer(B), Ahmed E. Ismail, and Paolo Bientinesi
Aachen Institute for Advanced Study in Computational Engineering Science,
RWTH Aachen University, Schinkelstr. 2, 52062 Aachen, Germany
{springer,ismail,pauldj}@aices.rwth-aachen.de
http://hpac.rwth-aachen.de/
Abstract. Recent results on supercomputers show that beyond 65 K
cores, the eﬃciency of molecular dynamics simulations of interfacial sys-
tems decreases signiﬁcantly. In this paper, we introduce a dynamic cutoﬀ
method (DCM) for interfacial systems of arbitrarily large size. The idea
consists in adopting a cutoﬀ-based method in which the cutoﬀis cho-
sen on a particle-by-particle basis, according to the distance from the
interface. Computationally, the challenge is shifted from the long-range
solvers to the detection of the interfaces and to the computation of the
particle-interface distances. For these tasks, we present linear-time algo-
rithms that do not rely on global communication patterns. As a result,
the DCM algorithm is suited for large systems of particles and mas-
sively parallel computers. To demonstrate its potential, we integrated
DCM into the LAMMPS open-source molecular dynamics package, and
simulated large liquid/vapor systems on two supercomputers: SuperMuc
and JUQUEEN. In all cases, the accuracy of DCM is comparable to
the traditional particle-particle particle-mesh (PPPM) algorithm, while
the performance is considerably superior for large numbers of particles.
For JUQUEEN, we provide timings for simulations running on the full
system (458, 752 cores), and show nearly perfect strong and weak scaling.
Keywords: Dynamic cutoﬀ· Interface detection · Linear-time complex-
ity · Scalability · Molecular dynamics · Fast sweeping method
1
Introduction
Molecular dynamics (MD) is a vital tool for computational chemistry, materials
science, biophysics, and many other ﬁelds. The basic idea underpinning MD is
the direct numerical integration of Newton’s laws of motion, which require the
frequent evaluation of forces between atomistic- or molecular-scale “particles”.
Although the underlying model is conceptually simple, signiﬁcant challenges
arise because of the enormous number of particles found even in nanoscopic
systems. In this paper, we discuss the development implementation, and paral-
lelization of a new force computation algorithm especially designed for systems
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 155–170, 2015.
DOI: 10.1007/978-3-319-20119-1 12

156
P. Springer et al.
consisting of large number of particles which demand the equivalent of “Tier-0”
computing resources.
Practically,
MD
calculations
are
limited
by
available
computational
resources, with typical simulations today involving anywhere from 104 to 107
particles, although simulations of 109 atoms or more have been reported in the
literature [24,26]. In general, larger simulations are preferable to smaller ones
because smaller simulations can be aﬀected by ﬁnite-size eﬀects that reduce the
accuracy of the calculations by introducing spurious correlations between par-
ticles [11]. Moreover, in principle, every particle can interact with every other
particle, making the inherent complexity of MD O(N 2). Thus, a primary driver
of active research in MD is reducing the algorithmic complexity of the force
calculations while preserving both accuracy and scalability.
To integrate Newton’s equations of motion, one needs to calculate the inter-
action forces among all of the particles in the system. Formally, these forces can
be calculated using any scheme that correctly accounts for all forces present.
Calculations are typically divided into bonded and non-bonded forces:
F = Fbonded + Fnon-bonded,
(1)
where bonded forces result from the topological structure of molecules, while
non-bonded forces account for all other interactions (such as gravity and elec-
tromagnetic eﬀects). Since the calculation of bonded forces already has linear
complexity, our focus is on the non-bonded forces, which can have complexity
up to O(N 2). In particular, we focus on a class of forces known as dispersion
forces, which represent forces that exist as a result of the gravitational interac-
tion between particles, independent of any other internal and external forces in
the system. These dispersion forces are typically calculated as a sum of pairwise
interactions between particles, with the strength of the interaction depending on
the distance between them:
Fdisp =

i<j
F(rij),
(2)
where rij = |ri −rj| is the distance between atoms i and j.
Until recently, dispersion forces were treated using a cutoﬀon the distance,
beyond which they were assumed to be negligible:
F(rij) =

F(rij),
rij ≤rc
0
rij > rc
,
(3)
where rc is the user-speciﬁed “cutoﬀ” parameter. This approach, also referred to
as a short-range method, reduces the complexity of the force calculation in Eq. 2
from O(N 2) to O(Nr3
c), where N is the number of particles. Such cutoﬀ-based
methods are suﬃciently accurate for homogeneous systems, whose composition
is uniform throughout the simulation volume. However, in heterogeneous sys-
tems, with nonuniform spatial density that leads to the existence of interfaces,
assuming isotropic behavior can cause major technical problems, as illustrated

A Scalable, Linear-Time Dynamic CutoﬀAlgorithm for Molecular Dynamics
157
(a)Static cutoﬀ.
(b)Dynamic cutoﬀ.
Fig. 1. Interfacial system using a (a) static cutoﬀand a (b) dynamic cutoﬀ. Gray area
denotes the cutoﬀ. The arrows represent the force acting on a particle. The yellow arrow
depicts the additional force contribution due to a larger cutoﬀ(Color ﬁgure online).
in Fig. 1a. The red particle in the right-hand circle indicates the behavior in the
“bulk” part of the system, where isotropic behavior can be assumed and the
errors introduced by Eq. 3 largely cancel, as can be seen from the green particles
around the red particle, whose force contributions essentially negate one another.
However, near the interface, such as the blue particle on the left, this cancel-
lation of errors is impossible, as the distribution of atoms across the interface
is far from isotropic. This breakdown, which can lead to completely inaccurate
results, has been demonstrated by a number of diﬀerent researchers [2,6,10,17].
Successful resolution of this problem is critical in a range of applications, includ-
ing industrial uses such as spreading and coating of ﬁlms [14] as well as modeling
of the dynamics of cell membranes [4].
 0
 5
 10
 15
 20
 25
1024
2048
4096
8192
Time [s]
#cores
Short-range
FFT
other
Fig. 2. Weak scaling of the PPPM
long range solver (from the LAMMPS
framework) for an interfacial system
with 1200 particles per core (IBM
BG/Q).
A na¨ıve solution to account for the
“missing” interactions in Fig. 1b would be
to increase the magnitude of the cutoﬀ
rc; doubling and even tripling the magni-
tude of the cutoﬀhas been proposed [33].
Such an approach is inherently undesir-
able, as the O(Nr3
c) complexity of the
method means that doubling the cutoﬀ
leads to an eight-fold increase in the cost
of the pairwise computations. In response,
a number of so-called “long-range solvers”
have been developed to reduce the over-
all complexity. Most of these approaches
are based on Ewald summation methods
[8], which rely on Fourier transforms to
reduce the complexity of the force calcu-
lations to as little as O(N log N), with implementations based on the classical
method [13] and mesh-based approaches such as the particle-particle particle-
mesh (PPPM) [15,16] and particle mesh Ewald (PME) [30] methods. Other
approaches, such as the multilevel summation algorithm [27] further reduce the

158
P. Springer et al.
complexity to O(N). However, these methods all suﬀer from a critical drawback:
they require global communications between diﬀerent processors. Consequently,
their scalability eventually decreases as the number of cores increase and the
cost of all-to-all communications becomes prohibitively expensive [15], as shown
in Fig. 2. Although Sun et al. [26] were able to optimize communications within
the PME solver in NAMD, an open-source MD package, to achieve good strong
scaling for a system with 108 particles on up to 298992 cores, their approach still
requires all-to-all communications and has a complexity of O(N log N).
Since short-range methods exhibit errors at the interface that are typically
two orders of magnitude larger than the errors for particles in the bulk phase [15],
it would be helpful to direct the computational resources to where they are most
needed. The approach we introduce in this paper, which we call the dynamic
cutoﬀmethod (DCM), circumvents the need for all-to-all communications by
making the cutoﬀa particle-dependent property. As shown in Fig. 1b, parti-
cles located in bulk regions, where the isotropic assumption is valid, can be
handled with a small cutoﬀ, while particles close to the interface are assigned
a larger cutoﬀ. Consequently, computational demands are kept to a minimum
while maintaining high accuracy. The DCM is closely related to static cutoﬀ
methods [12,28] and, as a result, inherits their good properties, such as strictly
local communication and good scalability. To make DCM competitive with state-
of-the-art solvers, we have also developed a fast and scalable algorithm to detect
interfaces. A similar method involving adaptive cutoﬀs, using a derived error
estimate rather than the relative location of the particles to determine the cut-
oﬀ, was recently proposed [29]. However, as that algorithm still relies on the use
of fast Fourier transforms, its large-scale scalability remains questionable.
This paper outlines the development of the dynamic cutoﬀmethod and the
associated interface detection method, which has been parallelized and extended
to three dimensions. These algorithms were incorporated into the open-source
LAMMPS package [9,20], one of the most widely used MD simulators currently
available. We show that our implementation of the dynamic cutoﬀalgorithm
achieves linear-time scaling for interfacial systems, even when utilizing the entire
JUQUEEN supercomputer at the Forschungszentrum J¨ulich.
2
Dynamic CutoﬀMethod
The core idea of the DCM is to circumvent the use of long-range solvers by
adaptively choosing the cutoﬀon a particle-by-particle basis, using small cutoﬀs
for particles far away from the interface and increasingly larger cutoﬀs as one
approaches the interface. Clearly, this strategy requires knowledge of the position
and the time evolution of the interface.
The computational tasks involved in one iteration of the DCM are shown
schematically in Fig. 3. First, the interface is identiﬁed (box 1); for each particle,
the distance from the interface is computed, and the cutoﬀfor each atom is
determined and assigned (box 2). Like classical short-range methods, the DCM
builds a neighbor list (box 3), enabling each particle to access its neighbors in

A Scalable, Linear-Time Dynamic CutoﬀAlgorithm for Molecular Dynamics
159
O(1) time. Finally, pairwise forces are computed (box 4), and the positions and
velocities of all particles are updated (box 5). At this point, the next iteration
begins. Since in typical MD simulations the interface changes very slowly, the
interface detection and neighbor-list build need not be executed every iteration.
Particle positions
Neighbor-
list build
required?
Interface
detection
required?
1) Interface detection
2) Cutoﬀassignment
3) Neighbor-list build
4) Force calculation
5) Update particles
New Particle positions
yes
no
yes
no
Fig. 3. Schematic overview of the dynamic cutoﬀmethod.
Before describing the individual tasks, we brieﬂy discuss our overall par-
allelization strategy for distributed memory environments. Following the main
scheme used by LAMMPS, the physical domain is spatially decomposed and
assigned to the underlying three-dimensional grid of MPI processes. Each process
p is responsible for one subdomain Lp ⊆L of the computational domain
L = Lx × Ly × Lz ⊂R3, and has an average memory requirement of O(N/P),
where N and P are the total number of particles and processes, respectively.
Particles can migrate between pairs of neighboring processes [20]. As we will
show, irrespective of the task performed, each process only communicates with
its direct neighbors, so that global communication patterns are entirely avoided.1
DCM exhibits a linear dependence on the number of particles in the system.
The ﬁrst four tasks enumerated in Fig. 3 are covered in detail in the next sub-
sections. The ﬁnal task, responsible for the updates of the particle positions and
velocities, uses velocity Verlet integration [28], the standard integration scheme
in MD simulations, and is therefore not discussed further.
1 With the exception of a reduction operation to identify the maximum of a scalar in
the interface detection method.

160
P. Springer et al.
2.1
Interface Detection
Our approach for a fast interface detection is inspired by algorithms for binary
image segmentation. The main idea is that an interface delineates the regions of
the physical domain in which the density of particles changes; with this in mind,
we treat particle densities as gray-scale values and apply image segmentation
techniques to the data. In three dimensions, this eﬀectively becomes a gray-
scale volume of voxels (3D pixel). As shown in Fig. 4a, to create the gray-scale
volume from the particle positions, all particles are binned into small 3D h ×
h×h “boxes”,2 eﬀectively decomposing each subdomain Lp into small 3D boxes
bx,y,z ⊂Lp; this operation only requires neighbor-neighbor communication.
At this stage, each box is treated as a voxel and is assigned a gray-scale
value according to its relative particle density (Fig. 4b). Based on this gray-scale
volume, the segmentation (Fig. 4c) can be computed as the minimization of the
piecewise constant Mumford-Shah functional for two-phase segmentation [1,18].
The result is a binary classiﬁcation of the boxes, diﬀerentiating high-density
phases (e.g., liquid) from low-density ones (e.g., vapor). A distributed-memory
implementation of this third stage boils down to the parallelization of a 3D
ﬁnite-diﬀerence stencil [3,9]. Starting from the Mumford-Shah algorithm from
the QuocMesh open-source library [21], an accurate 3D segmentation algorithm
for shared-memory architectures, we developed an MPI-based parallelization,
adding support for the periodic boundary conditions typically used in MD sim-
ulations; this algorithm is now included in QuocMesh.
(a) Binning
(b) Gray-scale volume
(c) Segmented volume
Fig. 4. Interface detection: a 2D slice from a 3D domain.
The output of the three stages shown in Fig. 4 is a segmented volume S:
S = {sx,y,z ∈{0, 1} | 0 ≤x < Nx, 0 ≤y < Ny, 0 ≤z < Nz},
(4)
where Nx, Ny and Nz are the number of local boxes on a given processor; sx,y,z
equals 0 if the corresponding box bx,y,z belongs to the low-density phase, and
sx,y,z = 1 otherwise. The interface is then determined by adjacent boxes with dis-
continuous values (Fig. 4c). The minimization of the Mumford-Shah functional
might result in the set S presenting low-density “bubbles” inside the high-density
2 The edge length h determines the resolution of the interface and can be automati-
cally chosen at the beginning of the simulation.

A Scalable, Linear-Time Dynamic CutoﬀAlgorithm for Molecular Dynamics
161
region and vice versa. Depending on their size, such bubbles can be interpreted as
false-detections and cause a noticeable performance degradation (bubbles yield
additional interfaces and hence unnecessarily large cutoﬀs). For this reason, we
apply a parallel multi-stage ﬁltering algorithm to identify and remove connected
components smaller than a user-speciﬁed volume. This step, which again requires
communication only between neighboring processes, is described in [25]. In con-
trast to other interface-detection methods [5,19,22], our algorithm is so fast that
it does not aﬀect the performance of an MD simulation.
2.2
CutoﬀAssignment
The objective of this task is to adaptively assign a suitable cutoﬀto each indi-
vidual particle; to this end, the set Dp of box-interface distances,3 and from
this the particle-interface distances δ are then derived. Two numerical methods
for approximating the box-interface distances are the Fast Marching Method
(FMM) [23] and the Fast Sweeping Method (FSM) [32]. Let Nv be the number
of voxels of the system; FMM has a complexity O(Nv log Nv) and is generally
more accurate than FSM. However, since FSM has a preferred complexity of
O(Nv) and in practice its accuracy is suﬃcient for the DCM, we adopted an
FSM-based approach.
Because the cutoﬀs for particles vary only for particles within a given distance
from the interface, we need not compute the exact distance between the interface
and each box. Instead, it suﬃces to carry out the calculations up to a certain
threshold distance rgrid
c
, since beyond this distance, the “minimum cutoﬀ” will
be applied, regardless of the actual distance from the interface. This problem
formulation makes it possible to devise a fast sweeping method that only requires
local communication and the reduction of a scalar.
Zhao et al. proposed two parallel algorithms/implementations for the
FSM [31]. Since scalability is one of our main concerns, we developed our own ver-
sion of FSM which was speciﬁcally tailored to the needs of our problem. Henceforth
we call our implementation a cutoﬀ-based fast sweeping method (CFSM).
As shown in Algorithm 1, the CFSM propagates box-interface distances out-
wards until the distance is larger than the threshold rgrid
c
.4 Visually, the algo-
rithm unfolds as a wave that starts at the interface and ﬂows outwards until it
has traveled the maximum distance rgrid
c
. The set of box-interface distances Dp,
local to process p, is initialized such that boxes at the interface and in the low-
density region are assigned a distance of 0, while boxes in the high-density region
are assigned a distance of +∞(line 1). All boxes adjacent to the interface and
have nonzero distance are added to queue Q, which keeps track of the remain-
ing boxes to be processed. After initialization, Q is processed breadth-ﬁrst. For
each box, indexed as (x, y, z), the distance to the interface is computed using a
3 Dp = {dx,y,z ∈R|0 ≤x < Nx, 0 ≤y < Ny, 0 ≤z < Nz}; the superscript p indicates
that this set is computed on each process, in parallel.
4 The exact value for the threshold is not important here. More information is provided
in [25].

162
P. Springer et al.
Algorithm 1. Distributed cutoﬀ-based fast sweeping method.
1: initialize(Dp, Q)
▷Add interfacial boxes to Q
2: for 0 ≤iter < itermax do
3:
for all boxes (x, y, z) ∈Q do
4:
dnew ←solveDistance(Dp, (x, y, z))
▷local FSM
5:
if dnew ≤rgrid
c
then
6:
if |dx,y,z −dnew| > Δe then
7:
Δe ←|dx,y,z −dnew|
8:
dx,y,z ←dnew
9:
addNeighborsToQueue( Q, (x, y, z))
10:
swapQueues(Q, Q)
11:
emptyQueue( Q)
12:
Δe ←MPI Allreduce(Δe, MAXIMUM)
13:
if Δe < ϵ then
14:
break
15:
ghostExchange(Dp)
▷Local communication
16:
addModiﬁedBoundariesToQueue(Dp, Q)
FSM on the local subdomain (line 4), the distance of box bx,y,z is updated (line
8), and all its adjacent boxes are added to the auxiliary queue Q (line 9). Once all
boxes in Q have been processed, the queues Q and Q are swapped (line 10), and
the maximum diﬀerence Δe between the old and new distances (line 7) is reduced
among all processes (line 12). If Δe < ϵ for some threshold ϵ (line 13), all processes
terminate; otherwise, each process communicates its boundary boxes to its neigh-
bors (line 15), adds the received boundary to Q (line 16), and enters a new itera-
tion. Typically, two to four iterations suﬃce for convergence. For completeness, we
point out that for any box within the threshold rgrid
c
, this implementation yields
the same results as the algorithms proposed by Zhao et al. [31].
From the set of computed box-interface distances Dp, the particle–interface
distances δi ∈R, i ∈[1, . . . , N], can be estimated via trilinear interpolation. The
cutoﬀrc of each particle is then chosen as a function of δi.5 In all cases, particles
at the interface or within the low-density phase are assigned a larger cutoﬀ, up to
rmax
c
, than particles further away; beyond a given distance from the interface, par-
ticles in the high-density phase are assigned the minimum cutoﬀrmin
c
.
2.3
Neighbor-List Build
Neighbor lists in MD simulations allow particles to access all of their neighbors
in constant time. Hockney et al. [12] introduced the linked-cell method, which
bins particles into cells of edge rc, thereby restricting the search for neighbors
of particle i to the cell containing particle i and its 26 neighbors. An alternative
technique, introduced by Verlet [28], uses a neighbor (or Verlet) list for each
particle i: the indices of all particles that interact with particle i are stored (i.e.,
rij ≤rc) (Algorithm 2). In practice, a skin distance rs > 0 is introduced, so that
5 Possible interpolation functions and the resulting accuracy are discussed in [25].

A Scalable, Linear-Time Dynamic CutoﬀAlgorithm for Molecular Dynamics
163
particles with rij ≤rc + rs are stored; this allows reuse of the neighbor list over
multiple timesteps. A drawback of this technique is that the neighbor list must
be updated frequently [7]. Currently, the most common approach combines these
techniques and bins all particles only when a neighbor-list build is required.
Algorithm 2 outlines the steps needed to build a neighbor list in the speciﬁc
context of the DCM; it is assumed that all particles are already spatially sorted
into bins. For each particle i, the algorithm loops over all particles j in the
neighboring bins jBin and adds j to the neighbor list of particle i if rij is less
than the cutoﬀrc[i] of atom i.
Algorithm 2. Neighbor-list build.
1: for all local atoms i do
2:
nNbrs[i] ←0
3:
iBin ←getBin(i)
4:
for all jBin ∈neighbors(iBin) do
5:
for all atoms j of jBin do
6:
rij ←ri −rj
7:
if |rij| < rc[i] and i ̸= j then:
8:
nbrsi[nNbrs[i]] ←j
9:
nNbrs[i] ←nNbrs[i] + 1
Algorithm 3. Force calculation.
1: for all atoms i do:
2:
fi ←0
3:
for all neighbors k of i do
4:
j ←nbrsi[k]
5:
rij ←ri −rj
6:
if |rij| < rc[i] then:
7:
f ←forceLJ(|rij|)
8:
fi ←fi −f × rij
9:
force[i] ←force[i] + fi
The varying cutoﬀs of DCM pose additional challenges for eﬃcient imple-
mentation of the neighbor-list build. With a static cutoﬀ, all particles traverse
the same stencil of neighboring cells. If applied to the DCM, this static approach
would result in poor performance because particles with a small cutoﬀtraverse
the same volume as particles with the maximum cutoﬀ. For instance, assuming
rmin
c
=
1
2rmax
c
and a typical skin distance rs = 0.1rmax
c
, all particles would
traverse a volume Vcube = (3(rmax
c
+ rs)3) to ﬁnd their neighbors. However, par-
ticles assigned the minimum cutoﬀhave their neighbors within the much smaller
volume Vmin = 4
3πrmin
c
3. Thus, only Vmin/Vcube ≈1.5 % of all particle-particle
calculations would contribute to the neighbor-list build (i.e., Line 7 of Algo-
rithm 2 would return false 98.5 % of the time). Since the neighbor-list build
is memory-bound, this approach would nullify any performance beneﬁt gained
using dynamic cutoﬀs.
The solution lies in the choice of the bins’ edge length: instead of lmax =
rs +rmax
c
, we use an edge length of lmin = rs +rmin
c
or smaller. While this results
in having to traverse a slightly larger stencil of neighboring cells, the traversed
volume is considerably smaller than Vcube and results in many fewer spurious
distance calculations. Note that the complexity of this improved neighbor-list
build is hidden in Line 4 of Algorithm 2. This optimization yields a 4× to 6×
speedup of the neighbor-list build over the binning with edge length lmax.
2.4
Force Calculation
Compared to classical short-range methods, the force calculations within the
DCM (Algorithm 3) show two striking diﬀerences: a particle-dependent cutoﬀ

164
P. Springer et al.
(shown in red), and the inapplicability of Newton’s third law of motion6 (hence-
forth called N3). Since the cutoﬀis independently assigned to each particle, the
fact that particle j is “inﬂuenced” by particle i does not imply that particle
i is inﬂuenced by particle j. Eﬀectively, neglecting N3 means that the update
fj ←fj + f × rij, which would appear in Algorithm 3 right after Line 8, is not
performed. Computationally, this results in twice as many force calculations,
but also allows a better memory access pattern, since costly scattered memory
accesses for fj are avoided.7
Our DCM implementation is based on the existing short-range Lennard-Jones
solver in LAMMPS. We developed both a pure MPI implementation, as well as
a hybrid MPI + OpenMP-based shared-memory parallelization that allows us to
start multiple threads per MPI rank, reducing both the memory requirements
and communication overhead. While the shared-memory implementation con-
sists of simple OpenMP directives—for instance, (#pragma omp for sched-
ule(dynamic,20)) before the outermost loops of Algorithms 2 and 3 suﬃces to
distribute the loops across multiple threads—we stress that the default sta-
tic schedule would result in severe load imbalance (due to diﬀerent cutoﬀs for
diﬀerent particles). By contrast, a dynamic schedule, using tasks of about 20
particles, results in almost perfectly load-balanced simulations. The beneﬁts of
dynamic scheduling become more apparent as more threads are involved. Simu-
lations with 256 MPI ranks and 16 threads per rank8 show a speedup of 1.4× for
the neighbor list and 2.2× for the force-calculation kernels over static scheduling.
3
Simulation Methodology
We present performance results for two interfacial systems: one with a planar
interface (Fig. 5a) and another with a non-planar interface (Fig. 5b). As the
processor count increases, the area of the interface in the planar system is scaled
proportionally in two dimensions (i.e., creating a large plane), and in the non-
planar system is extended along its cylindrical axis.
Both accuracy and performance are compared to the particle-particle
particle-mesh solver (PPPM), a state-of-the-art long-range algorithm included
in the LAMMPS package. Speciﬁcally, the measurements for the static cutoﬀ
method and PPPM are obtained using LAMMPS, version 30Oct14, with the
OpenMP user-package installed. For all experiments, the settings for PPPM are
chosen according to [16] and are considered to be optimal. Unless otherwise
speciﬁed, the minimum and maximum cutoﬀof DCM are respectively set to
rmin
c
= 3.0 and rmax
c
= 8.0, such that the resulting accuracy is comparable to
that of PPPM. The experiments were carried out on two diﬀerent supercomput-
ing architectures: SuperMuc and JUQUEEN.
6 If a body i exerts a force f onto another body j, then j exerts a force −f on i.
7 This is why most GPU implementations of force calculations also neglect N3.
8 Running on 1024 cores on the BlueGene/Q supercomputer with simultaneous multi-
threading enabled for four threads per core.

A Scalable, Linear-Time Dynamic CutoﬀAlgorithm for Molecular Dynamics
165
(a) Planar interface.
(b) Non-planar interface.
Fig. 5. The two interfacial systems used in this publication.
SuperMUC. The SuperMUC supercomputer at the Leibniz Supercomputing
Centre is based on Intel’s Sandy Bridge architecture, with 147.456 cores and
288 TB of main memory arranged in 18 “islands” of 512 nodes each. A node
consists of two Intel Xeon E5-2680 CPUs with a total of 16 cores. We used
Intel’s C++ compiler icpc 14.0.3 with compiler ﬂags -O3 -restrict -ip -unroll0
-openmp -xAVX.
JUQUEEN. The JUQUEEN supercomputer at Forschungszentrum J¨ulich is a
IBM Blue Gene/Q machine with 28.672 nodes organized in 28 racks, each com-
prising 1024 nodes. A node consists of 16 IBM PowerPC A2 cores, and 16GBs of
DDR3 memory, for a total of 458.752 cores and 448 TB of main memory. We used
IBM’s C compiler xlc++ 12.01 with compiler ﬂags -O3 -qarch=qp -qtune=qp -
qsmp=omp -qsimd=auto -qhot=level=2 -qprefetch -qunroll=yes.
4
Performance and Accuracy Results
4.1
Accuracy
As discussed in Sect. 1, every physical property (e.g., pressure, density) in a
molecular dynamics simulation relies on accurate force calculations. We there-
fore choose to measure the per-particle error in the forces perpendicular to the
interface (in these experiments, along the z-direction) to validate the correctness
of DCM. A detailed accuracy analysis of DCM is beyond the scope of this paper.
The error Δfiz of particle i is computed as follows:
Δfiz = f ∗
iz −fiz
(5)
where f ∗
iz denotes the correct force for particle i along the z-direction.9 We only
show the component of the error perpendicular to the interface because this is
much larger than the error along either of the other directions.
Figure 6 shows the error in the z-component of the forces for the planar sys-
tem with 19.200 particles for PPPM and DCM with diﬀerent maximum cutoﬀs.
9 f ∗
iz is computed by the accurate (but expensive) Ewald long-range solver.

166
P. Springer et al.
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0.09
 0.1
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
|Δfz|
z
Interface
Interface
pppm
dcm 7.0
dcm 8.0
Fig. 6. Absolute error in the z-direction (i.e., perpendicular to the interface) for the
planar system with 19, 200 particles. Each cross corresponds to a single particle. DCM
uses a minimum cutoﬀof rmin
c
= 3.0σ.
Table 1. Speedup of DCM over a static cutoﬀwith rc = rmax
c
for the planar system
with 12 million particles. The DCM setting reads as rmin
c
/rmax
c
. The experiments were
run on JUQUEEN using 1024 cores (256 MPI ranks and 16 threads per rank).
DCM setting 3.0/7.0 3.0/8.0 3.5/8.0 3.0/9.0 3.5/9.0
Speedup
2.31
2.66
2.38
2.42
2.23
First, we note that the errors of DCM and PPPM are comparable in magni-
tude. Second, larger cutoﬀs for DCM lead to more accurate results, as expected.
Third, in DCM the errors are smaller at the interface; this is critical, as the error
strongly inﬂuences the physical behavior [15,16,25]. Finally, as the dashed lines
indicate, one can see that our interface detection method correctly identiﬁes the
interface.
4.2
Performance
We compare the performance of DCM with the static cutoﬀmethod and PPPM
on two diﬀerent architectures. Table 1 shows the speedup of DCM over its static
counterpart forthe planar system. The simulation was run on JUQUEEN, on
1024 cores with 1200 particles per core. The static cutoﬀis set to the maximum
DCM cutoﬀ: rc = rmax
c
.Despite not exploiting Newton’s third law, the DCM
outperforms the static cutoﬀversion by at least a factor of 2.2. We note that
these speedups are highly dependent on the ratio between the number of particles
at the interface and away from the interface; depending on this ratio, even higher
speedups can be expected by incorporating Newton’s third law.
Figure 7 presents the strong and weak scalability on SuperMUC for the non-
planar system and on JUQUEEN for the planar system. On both architectures,
the weak scaling experiments were performed with 1200 particles per core, while
the strong scaling experiments use a system with roughly 4 × 107 particles.

A Scalable, Linear-Time Dynamic CutoﬀAlgorithm for Molecular Dynamics
167
 10
 100
 1024
 2048
 4096
 8192
 16384
 32768
Time/step [ms]
#cores
DCM
PPPM
(a) SuperMUC: Strong scaling
 8
 16
 32
 64
 128
 256
 512
 1024
 2048
 4096
 8192
 16384
 32768
 65536
Time/step [ms]
#cores
DCM
PPPM (pure MPI)
PPPM (8 threads/rank)
(b) JUQUEEN: Strong scaling
 0
 10
 20
 30
 40
 50
 60
 1024
 2048
 4096
 8192
 16384
 32768
Time/step [ms]
#cores
DCM
PPPM
(c) SuperMUC: Weak scaling
 0
 10
 20
 30
 40
 50
 60
 1024
 2048
 4096
 8192  16384  32768  65536 131072
 458752
Time/step [ms]
#cores
DCM
DCM (serial interface)
PPPM (pure MPI)
PPPM (8 threads/rank)
(d) JUQUEEN: Weak scaling
Fig. 7. Strong (a, b) and weak (c, d) scalability for PPPM and DCM.
Figure 7a and b show a head-to-head comparison between DCM and PPPM
in terms of strong scalability. The results for the pure MPI version of PPPM
expose a degradation in scalability starting from 4 k and 8 k cores on Super-
MUC and JUQUEEN, respectively. On JUQUEEN, we also ran an hybrid
multithreaded+MPI version of PPPM, using 8 threads per MPI rank; this con-
ﬁguration attained somewhat better timings than the pure MPI version, but
consistently crashed on 32 k cores or more. The DCM achieved convincing
(nearly-linear) scalability on both systems.
The weak scalability behaviour of DCM and PPPM is illustrated in Fig. 7c
and d. On both systems, the trend of PPPM (red line) matches exactly the
behaviour presented in Fig. 2: use of FFTs causes the scalability to progressively
deteriorate as the number of cores increases. This phenomenon is only delayed
in the hybrid multithreaded + MPI version of PPPM (orange line in Fig. 7d),
which eventually follows the same diverging trend. The results for the DCM on
JUQUEEN clearly indicate the need for a parallel interface detection method,
since the serial implementation (blue line), although extremely fast, eventually
becomes the bottleneck for the entire DCM. Finally, we direct our attention to
the DCM (green line) with parallel interface detection (as described in Sect. 2.1):
for both the planar and the nonplanar systems, scalability is nearly perfect.
Indeed, Fig. 7d reveals that on JUQUEEN it was possible to scale the system

168
P. Springer et al.
up to 5.5×108 particles on all 458, 752 available cores, while attaining ideal weak
scalability.
5
Conclusion
We have developed a dynamic cutoﬀmethod to study large-scale interfacial and
heterogeneous systems in molecular dynamics simulations containing millions of
particles on massively parallel supercomputers. Our method is based on making
the cutoﬀfor force calculations between particles a particle-dependent property.
We have implemented DCM as part of the open-source LAMMPS MD package
and showed that it exhibits desired properties such as (1) linear-time complex-
ity, (2) local communication, and (3) ideal weak- and strong-scaling on up to
458, 752 cores. Moreover, our performance results show that DCM outperform
state-of-the-art algorithms for large interfacial Lennard-Jones systems. These
experiments suggest that DCM is a promising algorithm for massively parallel
supercomputers.
We have also presented a scalable interface detection method for non-planar
interfaces. This method is fast enough to be applicable in real time throughout
the course of an MD simulation, which may open the door to a wide variety of
new MD applications. This interface detection method enabled us to preserve
the linear scaling of the DCM for short-ranged potentials.
Even though not investigated further in this paper, DCM can be used as a
replacement for short-range calculations within mesh-based Ewald solvers (e.g.,
PPPM). This allows to shift computational workload from the FFTs to the
short-range calculations and therefore should improve the scalability of these
solvers as well.
Acknowledgments. The authors gratefully acknowledge ﬁnancial support from the
Deutsche Forschungsgemeinschaft (German Research Association) through grant GSC
111, computing resources on the supercomputer JUQUEEN at J¨ulich Supercomputing
Centre (JSC) (project ID: e5430301) and the Gauss Centre for Supercomputing/Leibniz
Supercomputing Centre (project ID: pr84za), and Edoardo Di Napoli and Benjamin
Berkels for helpful discussions.
References
1. Berkels, B.: An unconstrained multiphase thresholding approach for image seg-
mentation. In: Tai, X.-C., Mørken, K., Lysaker, M., Lie, K.-A. (eds.) SSVM 2009.
LNCS, vol. 5567, pp. 26–37. Springer, Heidelberg (2009)
2. Blokhuis, E., Bedeaux, D., Holcomb, C., Zollweg, J.: Tail corrections to the surface
tension of a lennard-jones liquid-vapour interface. Mol. Phys. 85(3), 665–669 (1995)
3. Bohlen, T.: Parallel 3-d viscoelastic ﬁnite diﬀerence seismic modelling. Comput.
Geosci. 28(8), 887–899 (2002)
4. Bradley, R., Radhakrishnan, R.: Coarse-grained models for protein-cell membrane
interactions. Polymers 5(3), 890–936 (2013)

A Scalable, Linear-Time Dynamic CutoﬀAlgorithm for Molecular Dynamics
169
5. Bresme, F., Chac´on, E., Tarazona, P.: Molecular dynamics investigation of the
intrinsic structure of water-ﬂuid interfaces via the intrinsic sampling method. Phys.
Chem. Chem. Phys. 10(32), 4704–4715 (2008)
6. Chapela, G.A., Saville, G., Thompson, S.M., Rowlinson, J.S.: Computer simulation
of a gas-liquid surface. Part 1. J. Chem. Soc. Faraday Trans. 2: Mol. Chem. Phys.
73(7), 1133–1144 (1977)
7. Chialvo, A.A., Debenedetti, P.G.: On the use of the verlet neighbor list in molecular
dynamics. Comput. Phys. Commun. 60(2), 215–224 (1990)
8. Ewald, P.: Die berechnung optischer und elektrostatischer gitterpotentiale.
Annalen der Physik 369, 253–287 (1921)
9. Griebel, M., Knapek, S., Zumbusch, G.: Numerical Simulation in Molecular
Dynamics. Springer, Heidelberg (2007)
10. Guo, M., Peng, D.-Y., Lu, B.C.-Y.: On the long-range corrections to computer sim-
ulation results for the Lennard-Jones vapor-liquid interface. Fluid Phase Equilib.
130(1), 19–30 (1997)
11. Hill, T.L.: Thermodynamics of Small Systems. Dover Publications, Mineola (2013)
12. Hockney, R., Goel, S., Eastwood, J.: Quiet high-resolution computer models of a
plasma. J. Comput. Phys. 14(2), 148–158 (1974)
13. in ’t Veld, P.J., Ismail, A.E., Grest, G.S.: Application of ewald summations to
long-range dispersion forces. J. Chem. Phys. 127, 144711 (2007)
14. Isele-Holder, R.E., Ismail, A.E.: Atomistic potentials for trisiloxane, alkyl ethoxy-
late, and perﬂuoroalkane-based surfactants with tip4p/2005 and application to
simulations at the airwater interface. J. Phys. Chem. B 118(31), 9284–9297 (2014)
15. Isele-Holder, R.E., Mitchell, W., Hammond, J.R., Kohlmeyer, A., Ismail, A.E.:
Reconsidering dispersion potentials: reduced cutoﬀs in mesh-based ewald solvers
can be faster than truncation. J. Chem. Theory Comput. 9(12), 5412–5420 (2013)
16. Isele-Holder, R.E., Mitchell, W., Ismail, A.E.: Development and application of a
particle-particle particle-mesh ewald method for dispersion interactions. J. Chem.
Phys. 137(17), 174107 (2012)
17. Ismail, A.E., Tsige, M., in ’t Veld, P.J., Grest, G.S.: Surface tension of normal and
branched alkanes. Mol. Phys. 105(23–24), 3155–3163 (2007)
18. Mumford, D., Shah, J.: Optimal approximations by piecewise smooth functions
and associated variational problems. Commun. Pure Appl. Math. 42(5), 577–685
(1989)
19. P´artay, L.B., Hantal, G., Jedlovszky, P., Vincze, ´A., Horvai, G.: A new method
for determining the interfacial molecules and characterizing the surface roughness
in computer simulations. Application to the liquid-vapor interface of water. J.
Comput. Chem. 29(6), 945–956 (2008)
20. Plimpton, S.: Fast parallel algorithms for short-range molecular dynamics. J. Com-
put. Phys. 117(1), 1–19 (1995)
21. Rumpf, A.G.: Quocmesh software library. Institute for Numerical Simulation, Uni-
versity of Bonn. http://numod.ins.uni-bonn.de/software/quocmesh/
22. Sega, M., Kantorovich, S.S., Jedlovszky, P., Jorge, M.: The generalized identiﬁ-
cation of truly interfacial molecules (ITIM) algorithm for nonplanar interfaces. J.
Chem. Phys. 138(4), 044110 (2013)
23. Sethian, J.A.: A fast marching level set method for monotonically advancing fronts.
Proc. Nat. Acad. Sci. 93(4), 1591–1595 (1996)
24. Shekhar, A., Nomura, K.-I., Kalia, R.K., Nakano, A., Vashishta, P.: Nanobubble
collapse on a silica surface in water: Billion-atom reactive molecular dynamics
simulations. Phys. Rev. Lett. 111, 184503 (2013)

170
P. Springer et al.
25. Springer, P.: A scalable, linear-time dynamic cutoﬀalgorithm for molecular simu-
lations of interfacial systems (2013). arXiv:1502.0323
26. Sun, Y., Zheng, G., Mei, C., Bohm, E.J., Phillips, J.C., Kal´e, L.V., Jones, T.R.:
Optimizing ﬁne-grained communication in a biomolecular simulation application
on cray xk6. In: 2012 International Conference on High Performance Computing,
Networking, Storage and Analysis (SC), pp. 1–11. IEEE (2012)
27. Tameling, D., Springer, P., Bientinesi, P., Ismail, A.E.: Multilevel summation for
dispersion: a linear-time algorithm for r−6 potentials. J. Chem. Phys. 140(2),
024105 (2014)
28. Verlet, L.: Computer “experiments” on classical ﬂuids. I. thermodynamical prop-
erties of Lennard-Jones molecules. Phys. Rev. 159, 98–103 (1967)
29. Wang, H., Sch¨utte, C., Zhang, P.: Error estimate of short-range force calculation
in inhomogeneous molecular systems. Phys. Rev. E 86(2), 026704 (2012)
30. Wennberg, C.L., Murtola, T., Hess, B., Lindahl, E.: Lennard-Jones lattice summa-
tion in bilayer simulations has critical eﬀects on surface tension and lipid properties.
J. Chem. Theory Comput. 9, 3527–3537 (2013)
31. Zhao, H.: Parallel implementations of the fast sweeping method. J. Comput. Math.
25(4), 421–429 (2007)
32. Zhao, H.-K., Osher, S., Merriman, B., Kang, M.: Implicit and nonparametric shape
reconstruction from unorganized data using a variational level set method. Com-
put. Vis. Image Underst. 80(3), 295–314 (2000)
33. Zubillaga, R.A., Labastida, A., Cruz, B., Mart´ınez, J.C., S´anchez, E., Alejandre, J.:
Surface tension of organic liquids using the OPLS/AA force ﬁeld. J. Chem. Theory
Comput. 9, 1611–1615 (2013)

BWTCP: A Parallel Method for Constructing
BWT in Large Collection of Genomic Reads
Heng Wang, Shaoliang Peng(B), Yutong Lu, Chengkun Wu, Jiajun Wen,
Jie Liu, and Xiaoqian Zhu
School of Computer Science, National University of Defense Technology,
Changsha 410073, People’s Republic of China
{pengshaoliang,zhu xiaoqian}@nudt.edu.cn
Abstract. Short-read alignment and assembly are fundamental pro-
cedures for analyses of DNA sequencing data. Many state-of-the-art
short-read aligners employ Burrows-Wheeler transform (BWT) as an
in-memory index for the reference genome. BWT has also found its use
in genome assembly, for indexing the reads. In a typical data set, the vol-
ume of reads can be as large as several hundred Gigabases. Consequently,
fast construction of the BWT index for reads is essential for an eﬃcient
sequence processing. In this paper, we present a parallel method called
BWTCP for BWT construction at a large scale. BWTCP is characterized
by its ability to harness heterogeneous computing power including multi-
core CPU, multiple CPUs, and accelerators like GPU or Intel Xeon Phi.
BWTCP is also featured by its novel pruning strategy. Using BWTCP,
we managed to construct the BWT for 1 billion 100bp reads within 30 m
using 16 compute nodes (2 CPUs per node) on Tianhe-2 Supercomputer.
It signiﬁcantly outperforms the baseline tool BCR, which would need 13 h
to ﬁnish all processing for the same dataset. BWTCP is freely available
at https://github.com/hwang91/BWTCP.
Keywords: BWT · Genome assembly · BWTCP · BCR · CX1 · Radix
sort · Parallel computing
1
Introduction
Short-read alignment and assembly are fundamental problems in genomics [1–5].
Burrows Wheeler Transform (BWT) [6] was used to index a reference genome
in some state-of-the-art short-read aligners, for instance, BWA [7], Bowtie2 [8]
and MICA [9]. These short-read aligners store the BWT of the reference genome
in main memory to allow eﬃcient mapping of short-reads. Recently, Simpson
and Durbin [10] reported that BWT of reads could also be used to accelerate
genome assembly. While BWT construction for aligning only involves one ref-
erence genome, constructing BWT for a large collection of short reads is far
more complicated and computationally expensive. In a typical sequencing run,
H. Wang, S. Peng, and X. Zhu—Joint ﬁrst authors.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 171–178, 2015.
DOI: 10.1007/978-3-319-20119-1 13

172
H. Wang et al.
the amount of short reads generated is usually ten times larger than the size of
the reference genome. For instance, human genome with 30X depth can produce
about 100 Gigabases, or equivalently about 1 billion reads of length 100. Many
existing BWT construction methods utilize suﬃx arrays as an intermediate data
structure, which occupies even more memory. For a collection of 1 billion reads
of length 100, it takes 800 GB memory to store its suﬃx array. Hence, these
methods are impractical for the BWT construction of large collection of short
reads. In 2013, Bauer et al. [11] presented a method, embodied in BCR, to con-
struct BWT of a large string collection with a small memory footprint with a
sacriﬁce in time eﬃciency. It takes up to 13 hours to construct the BWT of
one billion reads with a length of 100 using BCR. This makes it reasonable to
accelerate BCR with parallelization. However, as BCR constructs partial BWT
of substrings of length 0 to length k in a sequential order, it’s non-trivial to
parallelize BCR for acceleration.
Liu et al. [12] proposed a parallel algorithm named CX1 to construct the
BWT of a large collection of reads utilizing GPU. CX1 construct the BWT of
R = {S1, S2, . . . , Sm} by its deﬁnition: sorting suﬃxes in R. Firstly, CX1 ﬁnd
a set of splitters P0, P1, · · · , Pu, which are strings over Σ satisfying P0 < P1 <
· · · < Pu (they are not necessarily suﬃxes in R). Then it uses those splitters to
partition the suﬃxes in R. Each partition gives a portion of the BWT, which
can be simply concatenated together to form the complete BWT. The splitters
are chosen such that the suﬃxes in each partition can be sorted within the GPU
memory limit. Then CX1 sort the partitions on GPU by means of LSD (least
signiﬁcant digit) radix sort. CX1 managed to construct the BWT of one billion
reads with a length of 100 in 2 h using a machine equipped with an Intel Xeon
i7-3930K (Hexa-core) 3.2 GHz processor and an NVDIA GeForce GTX 680 GPU.
One notable fact is that both BCR and CX1 are sensitive to the length of
reads while modern sequencers are producing longer reads, which can be as long
as several kilobases. For instance, when processing the same amount of reads,
BCR spent about 290 % more time on reads of length 400 than those of length
100. 150 % more time is needed for CX1.
In this paper, we proposed a more eﬃcient method based on CX1, named
BWTCP (BWT Construction in Parallel), to construct the BWT of a large col-
lection of reads in parallel. By adopting a diﬀerent sorting algorithm together
with an optimized pruning strategy, BWTCP can build the BWT of one billion
reads of length 100 within 30 m using 16 compute nodes on Tianhe-2 supercom-
puter. Each compute node on Tianhe-2 is equipped with 2 Intel Xeon E5 CPUs.
As conducted in [12], we tested BWTCP on 100M 100-bp reads and 25M 400-bp
reads, which used 197s and 201s respectively. BWTCP is far more eﬃcient on
longer reads than existing tools.
2
Preliminaries
Burrows-Wheeler transform (BWT) was introduced in [6] and originally used
to conduct data compression. Ferragina and Manzini found in [13] that BWT

BWTCP: A Parallel Method for Constructing BWT
173
could be used to construct full-text indices, which could be used to accelerate
sequence alignment. BWT originally works for a single string, but the deﬁnition
can be easily extended to a collection of strings. A detailed discussion of BWT
can be found in [6].
Let Σ = {c1, c2, . . . , cσ} be a ﬁnite alphabet, and ci is lexicographically less
than cj for 1 <= i < j <= σ. Given a string S = s0s1...sm−1 over Σ, we denote
the substring sisi+1 . . . sj by S[i, j], for 0 <= i <= j <= m −1. A substring
of type S[0, i] is called a preﬁx while a substring of type S[i, m −1]is called a
suﬃx of S. Generally, when constructing BWT of a string, a special end-marker
symbol $ is ﬁrstly appended to the string, where $ is lexicographically smaller
than every symbol in Σ. We deﬁne the BWT code of a suﬃx as the character
just before the start position of the suﬃx, and for suﬃx start from 0, the BWT
code is deﬁned as end-marker $.
Let R = {S1, S2, . . . , Sn} be a collection of n strings. Each of the strings Si
is of length m + 1 with Si[k] ∈Σ(0 <=k<= m) and Si[m] = $j, with $i < $j
for 0 <= i < j <= n. BWT of R is similar to that of a single string. Firstly,
we lexicographically sort the suﬃxes of all strings in R. Each string has m + 1
suﬃxes, thus the sorted list contains n(m+1) diﬀerent suﬃxes. Once the sorted
list L is generated, we can deﬁne the BWT of R, denoted by BR, as a string
of length n(m + 1), such that BR[i] is the BWT code of the suﬃx L[i], for
0 <= i <= n(m + 1).
3
Method - BWTCP
Similar to CX1, we sort all suﬃxes of reads in R to construct the BWT of
R. However, given a large amount of genomic reads, it’s impractical to sort all
suﬃxes in memory. Consider that the BWT construction of 1 billion reads of
length 100. Each read has 100 non-empty suﬃxes with length ranging from 1
to 100. If we explicitly enumerate all the suﬃxes of the 1 billion reads, there
will be about 5 trillion characters. It requires at least 1250 gigabytes to store all
these characters, using the canonical 2-bit-per-character representation for DNA
strings. Obviously, such requirement is far beyond the memory capacity of most
PCs or commodity servers. To be memory eﬃcient, we adopt a strategy similar
to block-wise suﬃx sorting proposed in [12,14]. BWTCP mainly involves two
steps: partitioning and sorting. The workﬂow is illustrated in Fig. 1.
3.1
Partitioning
As mentioned above, it is impractical to list all the suﬃxes of a large collection
of reads in memory. Splitting the suﬃxes list to smaller partitions is an obvious
approach, which was employed in CX1. We adopt the same strategy. l −mers
(string of length l) are chosen as splitters, i.e. the suﬃxes with same l-preﬁx, or in
other words, suﬃxes share the same l characters at their beginning, will be placed
into same partition (Suﬃxes shorter than l require special but easy handling).
Thus the order of suﬃxes from diﬀerent partition is absolutely determined by the
partition they belong to, and we can sort each partition separately and simply

174
H. Wang et al.
preﬁx-0
preﬁx-1
preﬁx-n
…
preﬁx-0
preﬁx-1
preﬁx-n
…
preﬁx-0
preﬁx-1
preﬁx-n
…
…
…
BWT-0 BWT-1
BWT-n
…
BWT-0
BWT-1
BWT-n
…
FASTA File 
a
b
c
d
e
Final BWT
Fig. 1. Workﬂow of BWTCP. (a) several processors load diﬀerent part of the reads
contained in a fasta ﬁle; (b) suﬃxes are partitioned and placed into a number of ﬁles
by their l-preﬁx; (c) several processors load and sort the suﬃxes with same l-preﬁx;
(d) the BWT codes of each partition is outputted into temporary ﬁles; (e) concatenate
the temporary ﬁles to get the ﬁnal BWT.
concatenate the sorted partitions to form the sorted list of all suﬃxes. The value
of l is a parameter dependent on the amount of available memory. A larger l
simply means more partitions but less memory is needed to sort each partition.
We observed that it costs about 15 m merely to load 1 billion reads of length
100 from hard disk to main memory. To accelerate the loading process, we load
the reads in parallel as follows. Consider the scenario of loading a ﬁle with n
reads using p processors. We can evenly partition n reads into p parts, each part
containing n/p reads. The p parts are then loaded into the main memory of p
processors simultaneously. After that, we perform the partitioning process on
each processor concurrently. On each processor, we simply scan all suﬃxes of
reads loaded and σl ﬁles each containing suﬃxes with speciﬁc l-preﬁx would be
written to hard disk. Next, suﬃxes in these ﬁles will be sorted in parallel.
3.2
Hybrid Radix Sorting
Suﬃxes Representation. Note that DNA sequences are strings over the
alphabet {A, C, G, T}, so each character can be represented by 2 bits, with
A, C, G, T represented by 00, 01, 10 and 11 respectively. We store the suﬃx
in units of byte, i.e. every four adjacent characters are stored in one byte. For
convenience in sorting, all suﬃxes will be stored in same length of k/4, where k
is the length of reads. Suﬃxes with a length less than k will be padded with 0.
Note that suﬃxes in same partition share same l-preﬁx, so we need not store
the ﬁrst l bits of the suﬃx. Instead, we keep the BWT code of the suﬃx in the
l bits for further acceleration.

BWTCP: A Parallel Method for Constructing BWT
175
Table 1. Representation of suﬃxes. For no more than 4G reads of length 100 and
splitters set as 4-preﬁxes, each suﬃx occupies 30 byte: 1 for BWT code, 24 for suﬃx
content excluding the ﬁrst byte, 1 byte for the start position and 4 byte for the ID of
the read the suﬃx belongs to.
BWT Code Suﬃx excluding l-preﬁx Suﬃx start position Read ID
To make each suﬃx unique (so we can adopt all sorting strategies, stable
or not), we append the start position of each suﬃx and ordinal number of the
original read. As we have stored start positions of suﬃxes, we can skip storing
the end-marker $ for each read. Thus, for suﬃxes of reads with length less than
256 and number of reads less than 4G, we store each suﬃx with k/4 + 5 bytes.
For longer or larger collection of reads, we just need to modify the number of
bytes used to store start position or ordinal number of the reads. Table 1 presents
a template of how the suﬃxes are stored.
Suﬃxes Sorting. Radix sort is eﬃcient for sorting large byte arrays. We opti-
mized radix sort using strategies as described in [15]. The idea of radix sort is as
follows. Firstly, classify the suﬃxes into piles by their ﬁrst several, say, 4 char-
acters. One pile gets all the suﬃxes that begin with AAAA, another pile gets
suﬃxes begin with AAAC, and so on. Split these piles recursively on succeeding
4-character substrings until the suﬃxes end. When there are no more piles to
split, pick up all the piles in order. Then the suﬃxes are sorted.
However, radix sorting is most advantageous for large arrays, so when
the piles get smaller than a pre-determined threshold, we just sort it with
comparison-based sorting algorithms such as insertion or quicksort. Referring
to [15] and simple experiment results, an empirical threshold is set as 25.
Pruning Strategy. From the deﬁnition of BWT we can see that a strictly
sorted list of the suﬃxes is not our ﬁnal goal. Once the suﬃxes are sorted, we
take a further step to collect the BWT code of each suﬃx. CX1 adopts LSD radix
sorting method, which sorts from the last digit to the ﬁrst one for all suﬃxes.
Actually it’s not necessary to compare all the digits. As described above, we store
the BWT code in the suﬃx representation and when the suﬃxes in a pile share
the same BWT code, we need not to sort the pile any longer. Repeated patterns
in genomes are commonly seen, there would be pretty many piles sharing the
same BWT code and this strategy could save a lot of time. Actually, the suﬃx
sorting part only represents 30 % of the total runtime of BWTCP, while CX1
spent 86 % of total runtime on sorting, both in the case of BWT construction of
1 billion 100-length reads.
3.3
Parallelism
BWTCP can be highly parallelized, either in the partitioning part or the sorting
part. Note that to avoid missing some suﬃxes, the sorting part should not be

176
H. Wang et al.
invoked until the partitioning part has ﬁnished. Given that the reads collection
is evenly split, partitioning time on each part only diﬀers very slightly, so the
waiting time is quite short (For 1 billion 100-bp reads, the waiting time is less
than 30 s. Note that the total running time is 1778 s).
In the partitioning part, the speedup is almost linear to the number of
processors employed (Simple experiment shows that comparing to employing
one processor, a speedup of 15.4X speedup was achieved when employing 16
processors). However, there exists a concern that when too many processors are
writing on the same hard disk, the I/O performance will be limited by the writ-
ing speed of the hard disk. One possible solution is to use more hard disks to
avoid writing jam. While in the sorting part, the degree of parallelism is as high
as the number of the partitions of suﬃxes, i.e. σl, with σ being the alphabet size
and l being the length of splitter when partition the suﬃxes. More partitions
straightly mean higher degree of parallelism.
4
Experiments and Results
BWTCP was implemented in C. We tested BWTCP with the Asian YH short
reads dataset presented in [16] on Tianhe-2 supercomputer. The results were
compared against CX1 and BCR (version 0.6.0). The evaluation of BWTCP was
carried out on 32 Intel Xeon E5-2692V2 CPUs connected by Gigabit Ethernet in
Tianhe-2 supercomputer system, each equipped with 64 GB memory. As Tianhe-
2 is not equipped with GPU, We previously tested BCR and CX1 on a machine
with an Intel Xeon i7- 3930K (Hex-core) 3.2 GHz processor and 64 GB of memory.
A comparison of Intel E5-2692V2 and Intel Xeon i7-3930K can be found at [17].
The machine was also equipped with an NVIDIA GeForce GTX 680 graphics
card with 4 GB of global memory.
We prepared three input data sets with 100 million, 500 million, and 1 billion
reads respectively. For each data set, we randomly selected a subset of the YH
short reads of the required size. Each read contains 100 bases of A, C, G, T.
Then we invoked BWTCP, CX1 and BCR to construct the BWT of the selected
reads. The BWT was outputted in ASCII format. All parameters and options
of CX1 and BCR are set as default, except I/O formats and algorithm of BCR
(which was set to BCR).
4.1
Time and Memory
Table 2 lists the construction time and memory consumption of BWTCP, CX1
and BCR on the three datasets. Note that the memory consumption of BWTCP
is the maximum value on all the processors employed. From Table 2 we can see
that BWTCP is way faster than BCR and CX1 both on small dataset and big
ones. BWTCP employed many processors in order to release memory pressure
and the improvement is evident in Table 2.

BWTCP: A Parallel Method for Constructing BWT
177
Table 2. Time (sec) and memory consumption by the three tools.
Tools
100M × 100bp
500M × 100bp
1000M × 100bp
BCR
6141 (3.3 GB)
23094 (17.6 GB) 46899 (33.3 GB)
CX1
565 (45.0 GB)
3108 (57.0 GB)
6886 (57.0 GB)
BWTCP
197 (2.2 GB)
883 (10.7 GB)
1778 (22.1 GB)
4.2
Impact of Read Length
Table 3 illustrates the impact of read length on BCR, CX1 and BWTCP. As
performed in [12], data sets of length-200 reads and length-400 reads were created
based on the 100 M data set by concatenating adjacent 100-bp reads. We can see
from Table 3 that the performance of BCR and CX1 dropped sharply on longer
reads. On the contrary, BWTCP was robust to read length variation. This is
mainly because that the sorting procedure in BWTCP would be terminated as
early as possible, when all suﬃxes in a pile share the same BWT code.
Table 3. Construction time (sec) for diﬀerent length of reads.
Tools
100M × 100 bp 50M × 200bp 25M × 400bp
BCR
6141
9334
23950
CX1
565
724
1269
BWTCP
197
183
201
5
Conclusion
We presented an eﬃcient method, BWTCP, for constructing the BWT of a large
string collection in parallel. BWTCP employs multiple nodes to cut down mem-
ory footprint on single node and enhance speed performance. As supported by
our experiments, our method is more eﬃcient (both on time and space perfor-
mance) compared to existing tools, especially on longer reads. Similar to BCR,
our method is also I/O bound. So BWTCP can be even faster on processors
equipped with SSD.
We carried out a performance test on 1 billion reads of length 100 with
16 nodes of Tianhe-2 Supercomputer system. This implies the possibility that
BWTCP can be scaled up to process larger read volumes by employing more
nodes (16,000 nodes are available on Tianhe-2).
In addition, BWTCP is theoretically portable. One can take advantage of
accelerators like GPU or Intel Xeon Phi to construct BWT of reads with simple
modiﬁcation to BWTCP.
Acknowledgement. We acknowledge Prof. T.W. Lam, Project Manager Ruibang
Luo and C.M. Liu in BAL lab, Department of Computer Science, The University of

178
H. Wang et al.
Hong Kong for providing the source codes, related data and constructive advice both
in designing and testing of BWTCP. And this work is supported by NSFC Grant
61272056, U1435222, 61133005, 61120106005 and 91432018.
References
1. Deshpande, V., Fung, E.D.K., Pham, S., Bafna, V.: Cerulean: a hybrid assembly
using high throughput short and long reads. In: Darling, A., Stoye, J. (eds.) WABI
2013. LNCS, vol. 8126, pp. 349–363. Springer, Heidelberg (2013)
2. Deshpande, V.: Sequencing, assembling, and annotating a mid-sized genome. In:
Plant and Animal Genome XXII Conference. Plant and Animal Genome (2014)
3. Huang, L., Popic, V., Batzoglou, S.: Short read alignment with populations of
genomes. Bioinformatics 29(13), i361–i370 (2013)
4. Blazewicz, J., Frohmberg, W., Gawron, P., et al.: DNA sequence assembly involving
an acyclic graph model. Found. Comput. Decis. Sci. 38(1), 25–34 (2013)
5. Li, B., Fillmore, N., Bai, Y., et al.: Evaluation of de novo transcriptome assemblies
from RNASeqdata. bioRxiv (2014)
6. Burrows, M., Wheeler, D.J.: A block-sorting lossless data compression algorithm
(1994)
7. Li, H., Durbin, R.: Fast and accurate short read alignment with Burrows Wheeler
transform. Bioinformatics 25(14), 1754–1760 (2009)
8. Langmead, B., Salzberg, S.L.: Fast gapped-read alignment with Bowtie 2. Nat.
Methods 9(4), 357–359 (2012)
9. Chan, S.H., Cheung, J., Wu, E., et al.: MICA: a fast short-read aligner that takes
full advantage of intel many integrated core architecture (MIC) (2014). arXiv
preprint arXiv:1402.4876
10. Simpson, J.T., Durbin, R.: Eﬃcient de novo assembly of large genomes using com-
pressed data structures. Genome Res. 22(3), 549–556 (2012)
11. Bauer, M.J., Cox, A.J., Rosone, G.: Lightweight algorithms for constructing and
inverting the BWT of string collections. Theoret. Comput. Sci. 483, 134–148 (2013)
12. Liu, C.M., Luo, R., Lam, T.W.: GPU-accelerated BWT construction for large
collection of short reads (2014). arXiv preprint arXiv:1401.7457
13. Ferragina, P., Manzini, G.: Opportunistic data structures with applications. In:
Proceedings of the 41st Annual Symposium on Foundations of Computer Science,
p. 390–398. IEEE (2000)
14. K¨arkk¨ainen, J.: Fast BWT in small space by blockwise suﬃx sorting. Theoret.
Comput. Sci. 387(3), 249–257 (2007)
15. Mcllroy, P.M., Bostic, K., Mcllroy, M.D.: Engineering radix sort. Comput. Syst.
6(1), 5–27 (1993)
16. Luo, R., Liu, B., Xie, Y., et al.: SOAPdenovo2: an empirically improved memory-
eﬃcient short-read de novo assembler. Gigascience 1(1), 18 (2012)
17. http://www.cpu-world.com/Compare/501/Intel Core i7 i7-3930K vs Intel Xeon
E5-2692 v2.html

Lattice-CSC: Optimizing and Building
an Eﬃcient Supercomputer for Lattice-QCD
and to Achieve First Place in Green500
David Rohr1(B), Matthias Bach1, Gvozden Neˇskovi´c1, Volker Lindenstruth1,2,
Christopher Pinke3, and Owe Philipsen3
1 Frankfurt Institute for Advanced Studies, Department for High Performance
Computing, Goethe University Frankfurt, Ruth-Moufang-Str.1,
60438 Frankfurt, Germany
rohr@fias.uni-frankfurt.de
2 GSI Helmholtz Center for Heavy Ion Research, Planckstraße 1,
64291 Darmstadt, Germany
3 Institute for Theoretical Physics, Goethe University Frankfurt,
Max-von-Laue-Str.1, 60438 Frankfurt, Germany
Abstract. In the last decades, supercomputers have become a necessity
in science and industry. Huge data centers consume enormous amounts
of electricity and we are at a point where newer, faster computers must
no longer drain more power than their predecessors. The fact that user
demand for compute capabilities has not declined in any way has led to
studies of the feasibility of exaﬂop systems. Heterogeneous clusters with
highly-eﬃcient accelerators such as GPUs are one approach to higher eﬃ-
ciency. We present the new L-CSC cluster, a commodity hardware com-
pute cluster dedicated to Lattice QCD simulations at the GSI research
facility. L-CSC features a multi-GPU design with four FirePro S9150
GPUs per node providing 320 GB/s memory bandwidth and 2.6 TFLOPS
peak performance each. The high bandwidth makes it ideally suited
for memory-bound LQCD computations while the multi-GPU design
ensures superior power eﬃciency. The November 2014 Green500 list
awarded L-CSC the most power-eﬃcient supercomputer in the world
with 5270 MFLOPS/W in the Linpack benchmark. This paper presents
optimizations to our Linpack implementation HPL-GPU and other power
eﬃciency improvements which helped L-CSC reach this benchmark. It
describes our approach for an accurate Green500 power measurement
and unveils some problems with the current measurement methodol-
ogy. Finally, it gives an overview of the Lattice QCD application on
L-CSC.
1
Introduction and Contribution of This Paper
In order to cope with today’s scientiﬁc challenges, supercomputers are of para-
mount importance. For a long time now, performance has grown exponentially.
A fraction of this increase has been acquired through higher power consumption.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 179–196, 2015.
DOI: 10.1007/978-3-319-20119-1 14

180
D. Rohr et al.
With data centers draining tens of megawatts of electricity, we have reached a
point where this increase of power consumption has to stop. It is our obligation
as a society to use our resources as carefully as possible, which means that super-
computers must become more eﬃcient. One approach to improving supercom-
puter power eﬃciency is the usage of accelerator cards such as GPUs (Graphics
Processing Units), which have already shown great power eﬃciency [1].
The L-CSC (Lattice Computer for Scientiﬁc Computing) is a new compute
cluster dedicated to simulations in the ﬁeld of the strong interactions between
quarks and gluons, which is under investigation both experimentally and the-
oretically. The cluster is installed at the GSI Helmholtz Center for Heavy Ion
Research in Darmstadt, Germany1. The theory of the strong interaction is an
SU(3) gauge theory called Quantum Chromo Dynamics (QCD). In general,
QCD is non-perturbative and the only approach to study it from ﬁrst principles is
Lattice QCD (LQCD) [2]. Here, space time is discretized on a four-dimensional
lattice, allowing numerical evaluation of the theory. Due to the large computa-
tional demands, state-of-the-art LQCD simulations require High Performance
Computing (HPC). LQCD applications are always memory bandwidth limited.
Accordingly, GPUs, which oﬀer a much higher peak memory bandwidth com-
pared to traditional CPUs, have become a vital ingredient to modern LQCD
applications [3]. In addition, with respect to both peak performance and peak
memory bandwidth, GPUs are much more power-eﬃcient than traditional CPUs.
Hence, GPUs can increase the power eﬃciency signiﬁcantly for applications that
can harness their capabilities. L-CSC features a multi-GPU design, providing a
huge aggregate global memory bandwidth and superior power eﬃciency.
For LQCD computations we use the CL2QCD application [4–7]. It utilizes
GPUs through OpenCL, which is a vendor independent API (Application Pro-
gramming Interface) for GPU programming [8]. In particular, it can run on
diﬀerent architectures independent of the hardware vendor, as opposed to most
other modern LQCD applications, which are bound to NVIDIA’s CUDA [9].2
CL2QCD has been shown to give signiﬁcant speedups on the predecessors of
L-CSC, LOEWE-CSC and Sanam (see below) [4–6], compared to CPU systems
and literature values. It has been successfully applied in physical studies [10,11].
Linpack [12] is the standard tool for measuring the compute perfor-
mance of supercomputers and it is the basis for the semi-annual Top500 list
of the fastest supercomputers [13]. The standard implementation of Linpack
is HPL (High Performance Linpack). We have presented an adapted version
called HPL-GPU that can run on GPUs [14,15]. The Green500 list [16,17]
awards the most power-eﬃcient supercomputers worldwide, ranked accord-
ing to performance per watt achieved in Linpack. We have tuned L-CSC for
power eﬃciency, and we optimized HPL-GPU for the best Green500 result. In
November 2014, L-CSC was awarded 1st place for the most power-eﬃcient super-
computer in the world.
1 See http://www.gsi.de.
2 See e.g. http://lattice.github.io/quda/.

Lattice-CSC: Optimizing and Building an Eﬃcient Supercomputer
181
The design of the L-CSC cluster continues the approach of the predecessors
LOEWE-CSC [18] and Sanam [1]. We give some details on L-CSC hardware
selection in [19]. In summary, L-CSC consists of 160 compute nodes intercon-
nected by 56 GBit FDR (Fourteen Data Rate) InﬁniBand with half bisectional
bandwidth. Each node oﬀers two 10-core Ivy-Bridge-EP Xeon CPUs, 256 GB
RAM, and four AMD FirePro GPUs. 148 nodes feature the FirePro S9150 card
while 12 nodes also feature the S10000 dual GPU, for a total of eight GPU chips
per node. The two diﬀerent GPU models are chosen to serve diﬀerent types of
QCD problems [19]. An important aspect related to power eﬃciency, which is
not considered in the Green500 ranking, is the data center itself. L-CSC inherits
the cooling principles from Sanam and LOEWE-CSC, which use passive water
cooling heat exchangers in the racks’ back doors. The only active cooling com-
ponents, aside from the fans inside the servers, are the water pumps and cooling
towers yielding a Power Usage Eﬀectiveness (PUE) of 1.05, i. e. only 5 % cooling
overhead, which is signiﬁcantly lower than in most data centers [18].
This paper is structured as follows: After Sect. 2 gives an overview of
other power-eﬃcient clusters, Sect. 3 presents the optimizations for HPL-GPU.
Section 4 illustrates other power eﬃciency improvements mostly related to the
hardware. We describe our eﬀorts to achieve an accurate power measurement in
Sect. 5. Finally, Sect. 6 gives an overview of the LQCD application on L-CSC.
2
Related Work
A newcomer to the November 2014 Green500 list, the ExaScaler-1 Cluster at
the High Energy Accelerator Research Organization KEK [20], employs PEZY-
SC [21] many-core coprocessors. It is ranked at 2nd place. Each node of the
ExaScaler-1 uses two Intel Xeon E5-2660v2 CPUs, 256 GiB RAM and eight
PEZY-SC coprocessors. The ExaScaler-1 cluster is comprised of eight nodes
and uses FDR InﬁniBand for interconnect. PEZY-SC contains 1024 logical cores
running at 733 MHz, having a theoretical double precision ﬂoating point peak
performance of 1.5 TFLOPS and 1533.6 GiB/s memory bandwidth. Altogether,
the ExaScaler-1 cluster achieves power eﬃciency of 4945.63 MFLOPS/W.
Current 2nd (ExaScaler-1) and 3rd (TSUBAME-KFC [22]) place systems in
the Green500 list are using liquid submersion cooling. In contrast to our solu-
tion, this approach directly cools the components by circulating dielectric liquid.
Components are submerged in large tanks of liquid. Pumps and heat exchangers
are used to remove heat away from components, eliminating the need for server
fans. Regardless, our passive water cooling solution still achieves comparable
PUE while enabling the use of commodity hardware without any modiﬁcations.
It is worth mentioning that many of the top Green500 systems in 2014 are com-
modity hardware HPC clusters. This approach, which extends upon Beowulf
clusters [23] concept, proves to be highly energy and cost-eﬀective.
We present related work to the LQCD application directly in Sect. 6.

182
D. Rohr et al.
3
Optimizing the Linpack Benchmark
The HPL-GPU software itself is already quite optimized. Thus, there is no single
new feature that leads to the good result on its own. Instead, tuning the software
at multiple places for small improvements of 1 % – 5 % each leads to the overall
improvement. This process increased matrix multiplication performance on a
quad-S9150 server from about 4 TFLOPS to 7.8 TFLOPS and power eﬃciency
from 3680.3 MFLOPS/W to 5367.8 MFLOPS/W. We give a short introduction
to the terminology ﬁrst and then present a selection of these new improvements.
Linpack solves a dense system of linear equations iteratively. Every iteration
consists of many steps: the large Update-DGEMM (matrix-matrix multiplica-
tion), the Update-DTRSM (solving a triangular matrix), LASWP, Factor-
ization, and Broadcast. The factorization is a complex step involving many
substeps, among them smaller DGEMM tasks. The most compute-intense step is
DGEMM. GPUs are ideally suited for DGEMM calculations because DGEMM
can load the GPUs’ FMA (Fused Multiply Add) ALUs almost to the full extent,
which most tasks cannot do. DGEMM on GPUs can achieve around 90 % of the
theoretical peak performance [14]. Hence, HPL-GPU attempts to run DGEMM
on the GPU 100 % of the time. It leaves the other steps on the processor, and
hides them behind DGEMM execution on the GPU. In addition, HPL-GPU
can use the remaining compute capabilities of the CPU to assist the GPU
at the DGEMM task. It can work with static workload distribution (static
scheduler) and dynamic workload distribution (dynamic scheduler), the lat-
ter one yielding better CPU utilization but larger scheduling overhead. Hid-
ing CPU tasks behind DGEMM is facilitated by a feature called Lookahead,
which runs the CPU tasks of the next iteration concurrent to the large Update-
DGEMM of the current iteration. Lookahead adds an additional step to each
iteration: a Preparatory-DGEMM, which is executed on the CPU to sim-
plify the scheduling. We use the Intel MKL BLAS library [24] for the CPU
and a custom GPU DGEMM kernel. HPL-GPU oﬀers two operating modes:
a performance-optimized mode, and an alternative eﬃciency-optimized
mode that sacriﬁces a small fraction of the performance for a better net power
eﬃciency. The ﬁrst mode yields better results for the Top500 list, the second
mode for the Green500 list. During an HPL run, the trailing matrix gets consec-
utively smaller and the computational eﬀort for DGEMM shrinks faster than the
workload for the other CPU tasks. This shifts the workload to the CPUs and at
a certain point the minimum CPU time exceeds the GPU time. From this time
forward, the GPU idles regularly. Formerly, e. g. on LOEWE-CSC, this was no
problem because the GPUs were not that much faster than the CPUs. Usually,
the performance of each new GPU-generation improves by a larger factor than
for the corresponding CPU-generation. It becomes more diﬃcult with every new
generation of GPUs to overlap computations on GPU and CPU. On Sanam this
became a relevant problem [1], and on L-CSC the discrepancy between CPU
performance and GPU performance prohibits the original approach.
In the following, we present the new features we have added to HPL-GPU
for Lattice-CSC, in order to cope with these ineﬃciencies. An asynchronous

Lattice-CSC: Optimizing and Building an Eﬃcient Supercomputer
183
 0
 5
 10
 15
 20
 25
 0
 20000
 40000
 60000
 80000
 100000
 120000
Time per Iteration [s]
Iteration (Remaining Matrix Size)
Dynamic Scheduling (2332 GFLOPS)
Static Scheduling (2323 GFLOPS)
Early Lookahead / Adaptive Tile Size (2375 GFLOPS)
Early Lookahead / CPU Load Minimized (2309 GFLOPS)
CPU Time GPU Time
Fig. 1. Computation time of CPU and GPU during HPL runs with diﬀerent settings
on a node with two S10000 cards. The upper three versions are optimized for compute
performance and try to use the CPU to the full extent, the lower version (CPU Load
Minimized) is optimized for energy eﬃciency and tries to keep the CPU idle. Hence,
its CPU execution time is shorter and its GPU execution time longer.
GPU command queue allows a dynamic oﬄoad of other CPU tasks onto the
GPU. Due to full-duplex DMA transfer, there is only little inﬂuence on the
Update-DGEMM performance of the GPU. Three CPU tasks are worthwhile for
dynamic oﬄoad: the Preparatory-DGEMM, the small DGEMM steps during the
factorization, and the Update-DTRSM. Oﬄoading the Preparatory-DGEMM
for lookahead is called Early Lookahead and analyzed in Fig. 1 in Sect. 3.1,
oﬄoading the other tasks is handled afterward in Sect. 3.2. Another modiﬁcation
is an adaptive tile size. It allows a more ﬁne-grained selection of the matrix
splitting ratios for the static scheduler [14].
3.1
Early Lookahead / Adaptive Tile Size
Figure 1 shows GPU and CPU runtime during an HPL run with two S10000
GPUs for diﬀerent settings in HPL-GPU. The X-axis shows the remaining work-
load (matrix size) whereas the Y-axis shows the time needed for the related iter-
ation. Two curves, one for the CPU runtime (solid) and a second for the GPU
runtime (dashed), show the time contribution of both processor types in each
iteration. All curves in the plot are falling, since matrix size becomes smaller
towards the right side. The GPU curves fall faster, because the DGEMM work-
load shrinks faster than the CPU steps’ workload. The graph shows whether the
CPU or the GPU generates the bottleneck in each iteration. (The CPU is the
bottleneck if and only if the CPU curve is above the GPU curve.) The wasted
GPU time, when the CPU is still working but GPU has ﬁnished the iteration,
is directly visible as the integral between the curves for GPU and CPU.
The ﬁgure compares (among others) the versions with static and dynamic
GPU / CPU scheduling. The dynamic one utilizes the processor almost to the

184
D. Rohr et al.
full extent (the CPU curve is very close to the GPU curve). The beneﬁt com-
pared to the static one is small since the CPU contributes only little to the
combined performance. The version with adaptive tile size and static scheduler
has a slightly shorter CPU active period compared to the dynamic scheduler, but
the performance is better due to less scheduling overhead. (See “Early Looka-
head/Adaptive Tile Size” in Fig. 1.) Early lookahead reduces the minimum CPU
runtime such that the CPU dominated period of HPL-GPU begins later.
In terms of power eﬃciency, it is better to move as much DGEMM workload
from the CPU to the GPU because the latter is the more eﬃcient chip. The
eﬃciency optimized operation mode of HPL-GPU performs the entire Update-
DGEMM step on the GPU, and does not split this workload between CPU and
GPU. (See “Early Lookahead/CPU Load Minimized” in Fig. 1.) This creates a
small performance penalty but improves the net power eﬃciency by 2 %. Con-
sequently, this approach increases the duration of the GPU computation.
3.2
Dynamic Oﬄoad of CPU Steps to GPU
The previous section showed that oﬄoading the Preparatory-DGEMM onto
GPU is better in any case. Oﬄoading the DGEMM substeps during the fac-
torization and the Update-DTRSM is more complex, as they run less eﬃciently
on the GPU than the other DGEMM tasks. (DGEMM inside the factorization
has matrices that are too small and DTRSM is limited by PCIe bandwidth.)
Fig. 2. Performance of HPL-GPU during the iterations of a Linpack run diﬀerent
settings for DTRSM oﬄoad.
Due to the non-constant ratio of CPU and GPU workload in a Linpack run,
the optimal settings during such a run can change with every iteration [1]. In
the following, phase 1 denotes the GPU dominated period at the beginning
and phase 2 refers to the period at the end with the CPU being the bottleneck.
Figure 2 shows the performance achieved during every iteration of the Linpack
run (with the iteration on the x-axis) with and without DTRSM oﬄoad. It

Lattice-CSC: Optimizing and Building an Eﬃcient Supercomputer
185
turns out the version without DTRSM oﬄoad is faster at the beginning, but
the version with DTRSM oﬄoad is faster at the end. Hence, an adaptive version
switches the setting when the curves intersect to ensure optimal performance in
every iteration. Oﬄoading the smaller DGEMMs shows a similar behavior. As
mentioned before, it is possible that optimal settings with respect to performance
and with respect to energy eﬃciency are diﬀerent. Section 4.1 presents a way to
obtain optimal settings for best eﬃciency. Table 1 gives an overview of which
HPL-GPU mode performs which oﬄoads.
Table 1. Dynamic Oﬄoad of CPU steps onto GPU.
Step
Performance Mode
Eﬃciency Mode
Preparatory-DGEMM
Always Oﬄoad
Always Oﬄoad
DGEMM during factorization
Oﬄoad in phase 2
Always Oﬄoad
Update-DTRSM
Oﬄoad in phase 2
Oﬄoad in phase 2
3.3
DMA Optimizations and OpenCL
We have already shown that system memory bandwidth can become a seri-
ous bottleneck for HPL in multi-GPU conﬁgurations [15,25]. A large blocking
factor Nb in HPL can compensate this to some degree [15], but this approach
encounters a principle limit at around Nb = 2048. The factorization workload
goes with N 3
b . Since, as shown in the previous sections, the CPU-based factor-
ization workload creates a bottleneck, Nb should not be increased further.
Due to limits in the DMA capabilities of former GPU programming APIs like
AMD CAL3 or older versions of CUDA, HPL-GPU had to copy data to interme-
diate host buﬀers before performing a DMA transfer to the GPU and vice versa.
The problem lies in the limited size of GPU-accessible host memory. Meanwhile,
both OpenCL and CUDA allow the allocation of large GPU-accessible host
buﬀers. HPL-GPU has been modiﬁed to be platform independent supporting
CAL, OpenCL, and CUDA as GPU APIs but also pure CPU systems. If the plat-
form provides suﬃcient DMA support, HPL-GPU can do direct DMA transfers
in any case avoiding all intermediate host buﬀers, which halves the system mem-
ory load [25]. Since no GPU exceeded 1 TFLOPS DGEMM performance at that
time, we used a simulation in [25] to show that the DMA framework of HPL-GPU
can keep step with a quad-GPU system achieving 2 TFLOPS DGEMM per-
formance per GPU. The new S9150 GPU of L-CSC achieves real 2 TFLOPS
DGEMM kernel performance and the ﬁnal DGEMM performance achieved by
HPL-GPU in the system is in complete accordance with the simulations.
HPL-GPU comes with GPU assembler CAL DGEMM kernels for Cypress,
Cayman, and Tahiti GPU families. Today, OpenCL compilers have matured
and it is possible to write fast DGEMM kernels in OpenCL. In order to use
proprietary DGEMM kernels oﬀered by vendors, HPL-GPU can load binary
3 Compute Abstraction Layer is the assembler language of former AMD GPUs.

186
D. Rohr et al.
kernels from a shared binary library. AMD provided such a library with an
OpenCL DGEMM kernel which achieves more than 2050 GFLOPS (approx. 80 %
of the GPU peak performance) on S9150 (Hawaii family). HPL-GPU makes
around 98 % of this DGEMM kernel performance available to the host.
4
Power Optimizations
Besides the optimizations to HPL-GPU, it is possible to tune the compute nodes
to drain less power. Deactivation of hardware components are immediate mea-
sures. Universal Serial Bus (USB) can drain up to 16 W [1]. Disabling USB
completely has the exact same eﬀect as enabling USB auto suspend in Linux.
Hence, L-CSC employs the second option. Hard disks and SATA controllers can
be eliminated by booting an NFS (Network File System) root system from
LAN. Since InﬁniBand is used anyway as a high performance network, L-CSC
boots directly via InﬁniBand which allows switching oﬀthe Ethernet ports com-
pletely. These steps yield an additional saving of 7 W.
CPU as well as GPU voltage and frequency play an important role. Power
consumption goes with the voltage squared. Hence, it can be beneﬁcial to reduce
the frequency a bit, which allows a voltage reduction as well. Voltage should be
minimal to ensure stable operation at the chosen frequency, and the best sweet
spot in terms of frequency is determined experimentally.
4.1
CPU Voltage / Frequency
HPL-GPU can internally alter CPU frequencies via libCPUFreq [26]. This
library forces the CPU to a lower P-State [27], lowering the processor’s voltage
accordingly. As the ratio of CPU and GPU workload in Linpack is not constant,
the optimal CPU frequency is likely to change during the run. A lower frequency
should yield better power eﬃciency during phase 1 at the beginning of Linpack
while phase 2 is supposed to favor higher frequencies. Naturally, the performance-
optimized HPL-GPU mode will always favor the faster CPU with high clocks,
hence the following discussion applies only to the eﬃciency-optimized mode.
We use a technique similar to that in Fig. 2 to ﬁnd the best settings with
respect to eﬃciency. Production of the plot is a bit more complex than for pure
performance analysis. The number of compute operations Ci per Linpack itera-
tion i is constant and straightforward to compute. Hence, the energy eﬃciency Ei
during the iteration i is given by Ci divided by the total power consumed during
the iteration. The power meter integrates the power continuously and provides
power values in conﬁgurable intervals, but a Linpack iteration does not coincide
with one or multiple intervals. Consider that in particular between two itera-
tions, power consumption can change signiﬁcantly, e. g. because of lookahead
reinitialization (See the ﬂuctuating power measurement in Fig. 4.) We estimate
the actual power by accumulating all power meter intervals completely within
the iteration’s duration, and scaling this power value linearly to the duration
of the iteration. Figure 3 shows such a plot for two Linpack runs with diﬀerent

Lattice-CSC: Optimizing and Building an Eﬃcient Supercomputer
187
CPU frequencies, which matches the expectations exactly. Due to the above-
explained estimation, the curves are a bit noisy, but the trend is clearly visible.
HPL-GPU on L-CSC varies CPU frequencies gradually from 1.2 GHz to 3 GHz
with changeover points determined by the technique presented in Fig. 3.
Fig. 3. Power eﬃciency of HPL-GPU during the iterations of a Linpack run with
diﬀerent CPU frequencies.
4.2
GPU Voltage / Frequency
Most programs cannot utilize all GPU ALUs to the full extent. (The same holds
true for CPUs.) Hence, vendors such as AMD, Intel, or NVIDIA have imple-
mented a Turbo Mode, where the chips run at higher frequencies, which can
exceed the TDP (Thermal Design Power) under full load. The FirePro GPUs
of L-CSC have an intelligent power management, which monitors the power con-
sumption, and lowers the clocks if needed in order to stay within thermal and
power limits. A GPU oscillating between a high turbo frequency and a lower
frequency to maintain the TDP is roughly as fast as a GPU running at the mid-
dle frequency, but it runs less eﬀectively. Compared to the middle frequency, the
turbo mode drains more power than the low frequency saves. During Linpack
on L-CSC, this feature is disabled via the AMD ADL library (AMD Display
Library), temperature and power consumption are monitored manually, and the
optimal combination of voltage and frequency is ﬁxed.
We investigated the eﬃciency with diﬀerent frequency/voltage settings dur-
ing the Linpack run. It turned out that in diﬀerent phases of Linpack, diﬀerent
settings are optimal. During phase 1 the GPUs harness the full potential of their
maximum clocks, while in phase 2 they are limited by the CPU. Hence, in the
other way around as for the CPU, phase 1 favors high GPU clocks while phase 2
shows better eﬃciency with lower clocks. AMD provided a custom BIOS with
several custom performance levels (voltage/frequency settings) and a tool that
can switch between these performance levels during runtime. This enables oper-
ation of the GPUs with optimal eﬃciency at every point in time. This approach

188
D. Rohr et al.
can also improve the performance in the case when the GPU hits the TDP limit,
because in that situation performance and eﬃciency are linked linearly.
In addition, the pipeline reinitialization in HPL-GPU introduces short time
periods (some tenths of milliseconds) where the GPU is not used. These periods
are too short to be handled manually in the above-described fashion. Instead, it
turns out that the power saving technique of the AMD PowerTune feature, based
on an SVI2 regulator chip, can eﬃciently reduce voltage (in 6.25 mV steps) and
frequency on demand for short periods with 10 ms transition time. Therefore, all
the performance levels in the custom BIOS have exactly two settings:
– High: for optimal Linpack eﬃciency at the current point in time.
– Low: minimum voltage/clocks save power during pipeline reinitialization.
Overall, our software performs the coarse-grain switching by selecting the cor-
rect performance level, while PowerTune does the ﬁne-grain switching to save
power during pipeline reinitialization. Voltage and frequency tuning was the
most important step and yielded a net energy eﬃciency improvement of 22 %.
5
Power Eﬃciency Results
Since L-CSC was assembled in October 2014, the full system was not yet ready
for the Linpack run for the November Green500 submission. Instead, we used
only a subsystem of 56 nodes. All results in this section refer to these 56 nodes.
The EEHPC Power Measurement Methodology [28] deﬁnes the procedure to
obtain the power measurements for the Green500 list. It speciﬁes three levels
for the measurement. Level 1 is the basic level which requires the least eﬀort,
but has the least accuracy. The higher levels have stricter rules on the procedure
and have higher demands on the measurement equipment, resulting in higher
accuracy of the measurement. For instance, a level 1 measurement may measure
only a small fraction of the cluster and scale the power consumption linearly
to the full size (i. e. all nodes which participated in the Linpack run.) Level 2
requires a larger fraction and level 3 the full cluster. The power meter used
for L-CSC is an LMG95 [29] revenue-grade power meter qualiﬁed for level 3
measurements with an accuracy of 0.025 %, but the total power it can measure
did restrict the L-CSC submission to level 1. All other aspects of the L-CSC
measurement are valid for level 3 and by screening the nodes to select ones
with middle power consumption for the submission, we estimate an inaccuracy
compared to a full level 3 measurement of less than 1.2 % [19].
Compared to the higher levels, there are three main factors which can distort
a level 1 measurement. It is possible to exploit them, in order to obtain a higher
power eﬃciency than actually achieved. These factors are:
– The measurement only has to cover a fraction of the cluster ( 1
64 for level 1).
– The duration of the measurement only has to cover at least 20 % of the mid-
dle 80 % of the core phase of Linpack [28].
– Level 1 requires only a measurement of the compute nodes which ignores other
components such as network and infrastructure nodes.

Lattice-CSC: Optimizing and Building an Eﬃcient Supercomputer
189
The remainder of this section discusses how the L-CSC power measurement dealt
with the above aspects and illustrates how exploiting these points leads to a false
but higher power eﬃciency, which is still compliant to level 1 speciﬁcations.
5.1
Number of Measured Nodes
Compute nodes identical in construction can show signiﬁcant node variability
in power consumption. It is possible to screen the compute nodes for the most
eﬃcient ones and measure only them. Scaling this result to the full size gives a
false result if the measured nodes are more eﬃcient than the average.
The node variability in terms of power eﬃciency of ±1.2 % on L-CSC [19]
is small and in fact than we expected. Still, measuring the best nodes would
improve the result by 1.2 %. However, it can be quite challenging to mea-
sure 1 MW or more, which is drained by large installations, with high accuracy.
Hence, we consider the approach to scale up a smaller measurement reasonable.
A pre-screening and selection of average nodes can improve the measurement.
In case of L-CSC, this leads to an estimated error of less than 1.2 %.
Table 2. Linpack power eﬃciency in GFLOPS/W on Sanam and L-CSC clusters.
Cluster 1 Node 4 Nodes Full Cluster
Sanam
2432.4
2318.7
2346.8
L-CSC
5367.8
5183.4
5293.4
Linpack may run on fewer nodes than available. Obviously, this deteriorates
Linpack performance and Top500 ranking. But, such a small conﬁguration usu-
ally yields better per-node performance, as it involves less communication. On
the one hand, it seems self-suggesting that such a small conﬁguration could also
lead to better eﬃciency. On the other hand, the more nodes participate the more
dominant is the GPU DGEMM workload, which is the most eﬃcient one. Hence,
the best option is not apparent. Table 2 shows that on both the Sanam [30] and
the L-CSC clusters the eﬃciency is almost independent from the number of
nodes. The single-node conﬁgurations have a small advantage as they do not
need communication at all, but results with four and many nodes are similar.
This demonstrates how well HPL-GPU hides the communication phases with its
lookahead feature and how well it scales to many nodes.
5.2
Duration of Power Measurement
Figure 4 shows the measured power consumption over the time of the ﬁnal Lin-
pack run on L-CSC submitted to Green500. The red vertical lines mark the
core phase of Linpack. This is the duration relevant for computing the FLOPS
for the performance, so it should also be the duration of the power measure-
ment. Levels 2 and 3 require this, but level 1 allows measuring a shorter dura-
tion for facilities with less advanced power measurement equipment. L-CSC’s

190
D. Rohr et al.
Fig. 4. Power consumption measurement during the Green500 Linpack run of L-CSC.
result submitted to the Green500 list uses the measurement of the full run
with 5293.4 MFLOPS/W for the compute nodes. The following paragraphs illus-
trate false but higher eﬃciency measurements allowed by the level 1 speciﬁca-
tion. A level 1 measurement may select the best 20 % out of the middle 80 %,
which is usually the period 70 % – 90 %. The power consumption steadily reduces
towards the end of the run, because in this phase 2 of Linpack the computation
is CPU-dominated and the GPU load becomes ever smaller.
The ﬁgure shows clearly that by simply redeﬁning the power measurement
interval from 0 % – 100 % to 70 % – 90 %, the measured power consumption drops
from 1017 W to 892 W boosting the obtained eﬃciency from 5293 MFLOPS/W
to 6005 MFLOPS/W. In this ﬁgure the 70 % – 90 % interval does not cover the
period with very low power consumption at the very end. The power consump-
tion at a time depends more or less only on the remaining matrix size. Starting
with a smaller matrix does reduce performance but shifts the 70 % – 90 % inter-
val to the smaller matrix sizes on the right. Thus, it enables harnessing a large
fraction of the period with signiﬁcantly lower power consumption.
Figure 5 shows the power consumption during such a shorter run. The
completely ironic fact is that the short run achieves worse eﬃciency than the
long run in the full measurement (4907 MFLOPS/W versus 5293 MFLOPS/W
for 0 % – 100 % measurement). However, selecting an “optimal” interval
boosts the eﬃciency further to 6899.6 MFLOPS/W. And theoretically, by using
dynamic voltage and frequency adjustment, it is possible to reduce the power
consumption much further for exactly the interval that is measured. This would
reduce the performance, but only during 20 % of the time, and would lead to
even higher results beyond 10000 MFLOPS/W. The period of measurement has
by far the largest inﬂuence on the result. Therefore, it should be required to
measure the full run, in particular because the additional eﬀort is negligible.

Lattice-CSC: Optimizing and Building an Eﬃcient Supercomputer
191
Fig. 5. Power consumption during a short Linpack run on L-CSC. Matrix size is tuned
such that the 70 % – 90 % measurement yields the highest eﬃciency.
5.3
Measured Components
The above measurements covered only the compute nodes. In principle, a level 1
measurement could leave the other components out, which would improve the
eﬃciency slightly. L-CSC can run from an InﬁniBand NFS network root ﬁle
system served by a single SSD. Mass storage and login nodes are not required
for the Linpack run. Hence, the only other signiﬁcant contribution comes from
the three InﬁniBand switches, arranged in a ring-conﬁguration. The measured
total power of all InﬁniBand switches is 256.8 W while idle and 257.5 W during
Linpack. It came as a surprise to us that the InﬁniBand power consumption is so
low and in particular that there is literally no diﬀerence between idle and load.
Adding this power consumption to the measurement of the compute nodes yields
the ﬁnal result of 5271.8 MFLOPS/W submitted to the November 2014 list.
6
Quantum Chromo Dynamics on L-CSC with CL2QCD
The main application on L-CSC is CL2QCD, used for QCD studies. Hence, good
performance of CL2QCD on L-CSC is important and will be elucidated in the fol-
lowing. On the lattice, physical values are measured by means of evaluating oper-
ators over a representative sample of the phase space. One method to generate
such samples is the Hybrid Monte-Carlo (HMC) algorithm, which integrates a
virtual molecular-dynamic system corresponding to a system of quarks and glu-
ons. The lattice action, which must be evaluated for these calculations, contains
the inverse of the Dirac operator (a large sparse matrix). The oﬀ-diagonal part
of this Dirac operator is called ̸D. It implements a matrix-vector multiplication
and is by far the most compute-intensive part [31] and is hence the core routing
of the application.
The lattice spacing must be decreased to inﬁnitesimal values to remove the
discretization again. However, a lattice of 304 points, merely enough for a cube

192
D. Rohr et al.
of 3 fm linear extent, already requires a large amount of TFLOPS. Improving the
resolution by a factor of two increases the number of lattice points by a factor
of 24. Thus, LQCD has already led to the development of dedicated compute
platforms like APE [32], QCDOC [33] and QPACE [34], as well as inﬂuenced
the development of high-performance systems like the BlueGene family [35].
163 × 8
163 × 16
163 × 24
163 × 32
243 × 12
243 × 16
323 × 8
243 × 24
323 × 12
243 × 32
323 × 16
243 × 48
323 × 24
483 × 8
40
80
120
160
200
240
Lattice Size
GB/s
AMD FirePro S9150 – Utilized in
/D
AMD FirePro S9150 – 75 % of Peak
AMD FirePro S10000 – Utilized in
/D
AMD FirePro S10000 – 75 % of Peak
Fig. 6. Memory bandwidth achieved in ̸D kernel on L-CSC.
CL2QCD is a major application on LOEWE-CSC (one Radeon HD 5870
per node), and it has shown convincing performance on the Sanam cluster (two
S10000 dual-GPUs per node). The core routine ̸D achieves 100 GFLOPS per chip
on the S10000 dual-GPU [1] and 135 GFLOPS on the S9150, i. e. 8×100 GFLOPS
and 4 × 135 GFLOPS respectively per L-CSC node. The speedup compared to
older GPUs (e. g. on LOEWE-CSC) and especially to CPUs is signiﬁcant [4].
The aggregate ̸D performance on the entire L-CSC cluster for parallel individ-
ual jobs is 89.5 TFLOPS. CL2QCD is currently optimized for S10000 and achieves
the excellent S9150 performance out of the box. Fine-tuning for the new card
should further improve this result. Since LQCD is memory-bound, these per-
formance numbers cannot reach the peak performance. Instead, Fig. 6 puts the
achieved memory throughput during the ̸D kernel into relation with the theo-
retical peak memory bandwidth. The ﬁgure reveals that CL2QCD utilizes the
available bandwidth to a great extent, leaving little room for improvements.
We want to put the performance on L-CSC into relation to its predecessors
LOEWE-CSC and Sanam. A single GPU chip of one S10000 GPU is twice as
fast as a LOEWE-CSC GPU. The S9150 is roughly 35 % faster than one S10000
chip. Considering the aggregate processing throughput, an S9150 L-CSC node
outperforms a Sanam node by 35 % and a LOEWE-CSC node by a factor of 10.8.
The S10000 L-CSC nodes outperform the Sanam nodes by a factor 2 and the
LOEWE-CSC nodes by a factor 16. To put our results into perspective, we
quote some recent ̸D performances on CPUs and GPUs. Smelyanskiy et al. [36]
have reached 75 GFLOPS on an Intel Xeon X5680 CPU with a single preci-
sion ̸D for problem sizes that ﬁt into the last-level cache. For realistic problem
sizes their performance drops to 42 GFLOPS, eﬀectively reaching the through-
put limit of the main memory. A variant that merges multiple ̸D invocations
(as they usually show up in applications) manages to exceed the data rate limit

Lattice-CSC: Optimizing and Building an Eﬃcient Supercomputer
193
and achieve 53 GFLOPS on that same CPU. In single-precision, current NVIDIA
GPUs achieve about 250 GFLOPS in ̸D [38]. In double precision, an NVIDIA
GTX 280 can achieve 40 GFLOPS [31]. The NVIDIA GTX 480 reaches slightly
more than 50 GFLOPS. Current results on NVIDIA k20m GPUs achieve close
to 90 GFLOPS [37]. Note that these quoted values use a slightly diﬀerent dis-
cretization than our implementation. The excellent ̸D performances translate
very well to the full HMC application, where similar speedups are observed.
The 1 GB of memory of the older AMD Radeon HD 5870 on LOEWE-CSC lim-
its the maximum possible lattice size severely. The 3 GB memory of the Sanam
GPUs can handle many state-of-the-art ﬁnite temperature lattice sizes such as
(323 × 12), but are still subject to limitations. Both GPU types used in L-CSC
have more memory (6 GB per chip on the new L-CSC S10000 cards and 16 GB on
S9150), which removes most limitations. CL2QCD can also utilize four GPUs in a
single process [5]. While the communication overhead reduces the aggregate sys-
tem throughput for small lattices by around 20 %, this approach diminishes the
wall time for individual jobs. In addition, it enables processing of larger lattices
utilizing the accumulated GPU memory of 64 GB (e. g. 483 × 96 for zero temper-
ature) which exceeds the capacity of a single GPU. With 64 GB we are able to
process all lattices of interest without the additional development time and the
penalties of inter-node communication. In many cases we process multiple indi-
vidual smaller lattices which ﬁt into 6 GB of memory. Requiring no communica-
tion at all, this is the most eﬃcient conﬁguration. The S10000 nodes are ideally
suited for this. Hence, L-CSC provides an optimal execution environment for a
broad variety of LQCD problems [19]: the S10000 nodes oﬀer the highest aggre-
gate throughput, while the S9150 nodes can process larger lattices and are faster
for single jobs. We will still have to scale over multiple nodes, as Babich et al. [3]
already showed, to further speed up individual compute-intensive jobs.
Multiple GPUs in a single server can improve energy eﬃciency. A single AMD
FirePro S10000 chip is about four times as fast as a pure CPU system. Therefore,
the four AMD FirePro S10000 dual-GPUs provide 32 times the throughput of
an equivalent CPU system. Still, the GPUs raise the TDP of the system only
by 1500 W. 31 additional systems are necessary to achieve the same through-
put increase by use of CPUs. Their idle power-consumption already adds up to
more than 1500 W. Even if the pure CPU systems could run LQCD at 200 W
each, their combined energy consumption would be 6,400 W. This is more than
four times the TDP of the four AMD FirePro S10000. Spread over multiple sys-
tems, the GPUs still have the same performance beneﬁt. However, additional
servers add additional power consumption, reducing overall energy eﬃciency.
7
Conclusions
The L-CSC cluster is a great success. It was awarded 1st place in November 2014
Green500 list for its energy eﬃcient design and software as the most power eﬃ-
cient cluster in the world. The presented optimizations to the HPL-GPU software
and in particular the tuning of clocks and voltage yield a signiﬁcant boost in
performance and power eﬃciency. It was shown that the software scales perfectly

194
D. Rohr et al.
from few nodes to large systems. The cluster is dedicated to physics simulations,
in particular of Lattice QCD investigations. The previously developed LQCD
application CL2QCD makes great usage of the hardware and performs very well.
It achieves more than 75 % of the global GPU memory bandwidth, leaving only
very little possibility for improvement because LQCD is memory-bound.
Acknowledgments. We would like to thank Advanced Micro Devices, Inc. (AMD)
and ASUSTeK Computer Inc. (Asus) for their support.
References
1. Rohr, D., Kalcher, S., Bach, M., Alaqeeli, A., Alzaid, H., et al.: An energy-eﬃcient
multi-GPU supercomputer. In: Proceedings of the 16th IEEE International Confer-
ence on High Performance Computing and Communications, IEEE, Paris, France
(2014)
2. Gupta,
R.:
Introduction
to
Lattice
QCD
(1998).
http://arxiv.org/abs/
hep-lat/9807028
3. Babich, R., Clark, M., Jo´o, B., Shi, G., Brower, R. C., Gottlieb, S.: Scaling lattice
QCD beyond 100 GPUs. In: SC 2011 Proceedings of 2011 International Conference
for High Performance Computing, Networking, Storage and Analysis, pp. 70:1–
70:11 (2011)
4. Bach, M., Lindenstruth, V., Philipsen, O., Pinke, C.: Lattice QCD based on
OpenCL. Comput. Phys. Commun. 184, 2042–2052 (2013)
5. Bach, M., Lindenstruth, V., Pinke, C., Philipsen, O.: Twisted-Mass Lattice QCD
using OpenCL. In: PoS LATTICE2013, p. 032 (2013)
6. Philipsen, O., Pinke, C., Sciarra, A., Bach, M.: CL2QCD - lattice QCD based on
OpenCL. In: PoS LATTICE2014, p. 038 (2014)
7. http://code.compeng.uni-frankfurt.de/projects/clhmc
8. Khronos OpenCL Registry, OpenCL API and C Language Speciﬁcations. https://
www.khronos.org/registry/cl/
9. NVIDIA, CUDA Toolkit Documentation. http://docs.nvidia.com/cuda/index.
html
10. Philipsen, O., Pinke, C.: The nature of the Roberge-Weiss transition in Nf = 2.
Phys. Rev. D 89(9), 094504 (2014)
11. Philipsen, O., Bach, M., Lindenstruth, V., Pinke, C.: The thermal quark hadron
transition in lattice QCD with two quark ﬂavours. In: Proceedings of Conference:
C14–02-12.1, pp. 33–40
12. Dongarra, J., Luszczek, P., Petitet, A.: The LINPACK benchmark: past, present
and future. Concurrency Comput.: Pract. Experience 15(9), 803–820 (2003)
13. TOP500 Supercomputer Sites. http://www.top500.org
14. Bach, M., Kretz, M., Lindenstruth, V., Rohr, D.: Optimized HPL for AMD GPU
and multi-core CPU usage. Comput. Sci. - Res. Dev. 26(3–4), 153–164 (2011)
15. Rohr, D., Bach, M., Kretz, M., Lindenstruth, V.: Multi-GPU DGEMM and HPL
on highly energy eﬃcient clusters. In: IEEE Micro, Special Issue, CPU, GPU, and
Hybrid Computing (2011)
16. Sharma, S., Hsu, C., Feng, W.: Making a case for a Green500 list. In: Proceedings
of the 20th IEEE International Parallel Distributed Processing Symposium p. 343
(2006)
17. The Green500 List. http://www.green500.org

Lattice-CSC: Optimizing and Building an Eﬃcient Supercomputer
195
18. Bach, M., De Cuveland, J., Ebermann, H., Eschweiler, D., Kretz, M., et al.: The
LOEWE-CSC: a comprehensive approach for a power eﬃcient general purpose
supercomputer. In: 21st Euromicro International Conference on Parallel, Distrib-
uted and Network-Based Processing (2013)
19. Rohr, D., Nescovic, G., Radtke, M., Lindenstruth, V.: The L-CSC cluster: greenest
supercomputer in the world in Green500 list of November 2014. In: Proceedings of
Supercomputing Frontiers (2015)
20. High Energy Accelerator Research Organization. http://www.kek.jp
21. PEZY Computing, PEZY-SC Many Core Processor (2014). http://www.pezy.co.
jp/en/products/pezy-sc.html
22. http://www.gsic.titech.ac.jp/tsubame
23. Sterling, T.L.: How to Build a Beowulf: A Guide to the Implementation and Appli-
cation of PC Clusters. MIT Press, Cambridge (1999)
24. Intel Corporation, Intel MKL BLAS Library. https://software.intel.com/en-us/
intel-mkl
25. Rohr, D., Lindenstruth, V.: A ﬂexible and portable large-scale DGEMM library for
linpack on next-generation multi-GPU systems. In: 23rd Euromicro International
Conference on Parallel, Distributed and Network-Based Processing (2015)
26. https://www.kernel.org/pub/linux/utils/kernel/cpufreq/
27. Kidd, T.I.: What exactly is a P-state? (2008). https://software.intel.com/en-us/
blogs/2008/05/29/what-exactly-is-a-p-state-pt-1
28. EEHPC Working Group: Energy Eﬃcient High Performance Computing Power
Measurement Methodology v1.2 RC 2
29. ZES Zimmer: LMG95 1 Phase Power Analyzer. http://www.zes.com/en/Products/
Precision-Power-Analyzer/LMG95
30. Rohr, D.: On Development, Feasibility, and Limits of Highly Eﬃcient CPU and
GPU Programs in Several Fields. Dissertation Thesis (2013)
31. Clark, M.A., Babich, R., Barros, K., Brower, R.C., Rebbi, C.: Solving lattice QCD
systems of equations using mixed precision solvers on GPUs. Comput. Phys. Com-
mun. 181, 1517–1528 (2010)
32. Battista, C., Cabasino, S., Marzano, F., Paolucci, P., Pech, J., et al.: APE-100
computer: (i) the architecture. Int. J. High Speed Comput. 05(04), 637–656 (1993)
33. Boyle, P. A., Chen, D., Christ, N. H., Clark, M. A., Cohen, S. D., et al.: QCDOC:
a 10 teraﬂops computer for tightly-coupled calculations. In: SC 2004 Proceedings
of 2004 International Conference for High Performance Computing, Networking,
Storage and Analysis (2004)
34. Baier, H., Boettiger, H., Drochner, M., Eicker, N., Fischer, U.: QPACE - a QCD
parallel computer based on cell processors. In: Proceedings of Science, p. 21,
November 2009
35. Vranas, P.: QCD and the BlueGene. J. Phys.: Conf. Ser. 78, 012080 (2007)
36. Smelyanskiy, M., Vaidyanathan, K., Choi, J., Jo´o, B., Chhugani, J., et al.: High-
performance lattice QCD for Multi-Core based parallel systems using a cache-
friendly hybrid threaded-MPI approach. In: SC 2011 Proceedings of 2011 Inter-
national Conference for High Performance Computing, Networking, Storage and
Analysis (2011)

196
D. Rohr et al.
37. Winter, F. T., Clark, M. A., Edwards, R. G., Jo´o, B.: A framework for lattice
QCD calculations on GPUs. In: Proceedings of the 2014 IEEE 28th International
Parallel and Distributed Processing Symposium, pp. 1073–1082 (2014)
38. Jo´o, B., Kalamkar, D.D., Vaidyanathan, K., Smelyanskiy, M., Pamnany, K., et al.:
Supercomputing. In: Kunkel, J.M., Ludwig, T., Meuer, H.W. (eds.) ISC 2013.
Lecture Notes in Computer Science, vol. 7905, pp. 40–54. Springer, Heidelberg
(2014)

An Eﬃcient Clique-Based Algorithm
of Compute Nodes Allocation for In-memory
Checkpoint System
Xiangke Liao(B), Canqun Yang, Zhe Quan, Tao Tang, and Cheng Chen
College of Computer Science, National University of Defense Technology,
Changsha 410073, China
{xkliao,canqun,zhequan,taotang84,chengchen}@nudt.edu.cn
Abstract. Fault-tolerant is an essential technology for high-performance
computing systems. Checkpoint/Restart (C/R) is the most popular fault-
tolerant technique in which the programs save their states in stable stor-
age, typically a global ﬁle system, and recover from the last checkpoint
upon a failure. Due to the high-cost of global ﬁle system, node-local stor-
age based checkpoint techniques are now getting more and more interests,
where checkpoints are saved in local storage, such as DRAM. Typically,
computing nodes are divided into groups and the checkpoint data is
redundantly saved on a speciﬁed another node or is distributed among all
other nodes in the same group, according to diﬀerent cross-node redun-
dancy schemes, to overcome the volatility of node-local storage. As a
result, multiple simultaneous failures within one group often cannot be
withstood and the strategy of node grouping is consequently very impor-
tant since it directly impacts the probability of multi-node-failure within
one group. In this paper, we propose a novel node allocation model, which
takes the topological structure of high-performance computing systems
into account and can greatly reduce the probability of multi-node-failure
within a group, compared with traditional architecture-neutral group-
ing algorithms. Experimental results obtained from a simulation system
based on TianHe-2 supercomputer show that our method is very eﬀective
on random simulative instances.
Keywords: Fault-tolerance · In-memory checkpoint · Algorithm
1
Introduction
In high performance computing (HPC) systems, the probability of overall failure
increases over the computing time and the number of compute nodes due to
more involved components. The mean time between failures (MTBF) of toady’s
systems have decreased to only a few hours [5,6,14] because of hardware and/or
software errors [8,15]. As a result, fault-tolerant has become a well-known issue
in HPC area [11].
One commonly used fault-tolerant technique is Checkpoint/Restart (C/R)
[1]. In a C/R-based method, the state of an application, known as a checkpoint,
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 197–211, 2015.
DOI: 10.1007/978-3-319-20119-1 15

198
X. Liao et al.
is periodically saved to stable storages, typically the global ﬁle system. Once a
failure occurs, the program can be restarted from the latest saved checkpoint.
The critical issue of C/R-based methods is the high-cost of checkpoint access
from global ﬁle system, especially for those large-scale systems, in which the
I/O bandwidth will become the performance bottleneck [7,13]. Consequently,
many local-storage based C/R methods have emerged [3,4,12,16]. In this paper,
we focus on one that takes host memory as the storage to save checkpoints. It
should be noted that local storage based C/R method is usually adopted as a
supplement to the disk based C/R, to reduce the frequency of global ﬁle system
access. This is also known as multi-level checkpoint technique [10].
The performance beneﬁt of local storage based C/R derives from the linearly
increasing checkpoint access bandwidth and at least an order of magnitude lower
access latency compared with disks. However, local storage is usually supposed
to be unstable. For example, DRAM is volatile and the data will be lost once
the power is oﬀ. Consequently, the checkpoint of one node has to be redun-
dantly saved in other nodes, so as to recover the node failure. The most common
strategy is dual-redundancy. To be more speciﬁc, local storage based C/R typi-
cally divides compute nodes into groups, and only duplicates a checkpoint onto
another node in the same group (usually called partner node). Upon a node fail-
ure, the execution state can be recovered by the checkpoint saved on its partner
node. The dual-redundancy strategy means that a given node and its partner
cannot fail at the same time, otherwise the execution cannot be recovered. To
reduce the data amount of checkpoint, another commonly used scheme is XOR,
which calculates a parity of redundant data from all checkpoints and then dis-
tributes it among all nodes in the same group. In this case, two nodes from the
same group cannot fail at the same time.
How to group the compute nodes has a direct impact on the fault-tolerant
eﬀect of these local storage based C/R techniques, since diﬀerent grouping strate-
gies often lead to diﬀerent probabilities of multi-node-failure in a group. In tra-
ditional methods, grouping strategy is relatively intuitive. For instance, in the
Scalable Checkpoint/Restart (SCR) library [9], a multi-level checkpointing sys-
tem, the nodes can be grouped by continuous node ID or a speciﬁed stride.
This strategy is straightforward to implement, while the architecture of system
is ignored. In real-world large-scale parallel computing systems, multiple simul-
taneous failures occur with higher probability in some set of nodes than others.
For instance, by omitting other factors, two nodes that share the same electric-
ity supply module are more likely to fail simultaneously than two isolated nodes
due to the possible power failure. Generally, two nodes with larger logic distance
may have lower probability of failing simultaneously.
Based on these observations, we propose a new algorithm in this paper, to
group the computing nodes with the topological structure of a parallel comput-
ing system taken into account. Our method transfers the computing nodes with
the probability of failure into a complete weighted undirected graph and uses
clique technology to improve the nodes groups. Compared with intuitive group-
ing strategies, our algorithm can eﬀectively reduce the probability of multiple

A Node Allocation Algorithm for In-memory Checkpoint System
199
simultaneous intragroup failures, in which case high-cost global C/R system has
to be invoked. To evaluate our method, we build a simulation system based on
TianHe-2 [2], the world’s fastest supercomputer in the latest TOP500 list, which
has more than 16,000 nodes. The topological structure and the essential parame-
ters of the simulation system are extracted from this real system, and can also
be modiﬁed easily to simulate other systems. The experimental results obtained
show that the approach is very eﬀective on random instances, especially for hard
instances.
The remainder of this paper is organized as follows: Sect. 2 introduces the
background. We propose our model and algorithm in detail in Sects. 3 and 4
respectively. Section 5 evaluates the performance of this model and conclusions
are given in Sect. 6.
2
Background
In-memory checkpoint system is the most important local-storage based check-
point technique. Generally, memory access speed is at least an order of mag-
nitude faster than the ﬁle system. In addition, the capacity and bandwidth of
memory can expand linearly with system scale from the view of the whole sys-
tem. The major problem of takeing memory as checkpoint storage is its volatility.
Note that in this paper we assume a fail-stop fault model, which means once an
error occurs, the node stops responding and need to be replaced. Thus, we need
a redundancy scheme to ensure that checkpoint data can be retrieved after the
node failure. One common scheme is dual-redundancy (also called mirror scheme
in some literatures), as illustrated in Fig. 1. Each node has a partner node, where
its checkpoint data is stored redundantly.
Fig. 1. Scheme of in-memory dual-redundancy checkpoint system
This dual-redundancy scheme demands that one node cannot fail simulta-
neously with its partner node, otherwise the checkpoint data will be lost. In
practise, nodes are divided into groups and each node is assigned a partner
node within the group. The strategy of node grouping is intuitive in existing
checkpoint system, i.e., dividing nodes according to node’s ID. Users can assign
a hop distance so as to avoid adjacent nodes being allocated into the same

200
X. Liao et al.
group. Besides, a so-called XOR-scheme is another option, in which all nodes in
a group collectively calculate a parity redundancy data according to their own
checkpoints and then evenly distribute the parity redundancy data among all
nodes in the group. Upon a node failure, other nodes in the group can recover the
checkpoint according to their segments of the parity redundancy data. Compared
with the dual-redundancy scheme, XOR-scheme demands less memory storage to
save checkpoint data, while introducing extra computations. The XOR-scheme
can withstand node failures as long as two or more nodes from the same group
do not fail simultaneously.
We can see that in-memory checkpoint system is sensitive to simultaneous
failures within a group. As a result, it is often taken as a complement of the global
checkpoint system. That is, upon the failures that in-memory checkpoint system
cannot withstand, the global checkpoint system is invoked. Notice that global
checkpoint system is high-cost and thus the overall fault tolerance overhead can
be reduced if we can lower the probability of simultaneous node failure within
a group. This is also the object of the node allocation model we propose in this
paper.
3
Node Allocation Model
3.1
Assumptions and Errors
Due to the complexity of organization structure, the fault model of high perfor-
mance computing system can be very complicated, thus requiring some assump-
tions and simpliﬁcations when modeling the fault-tolerant system. We believe
that these assumptions can cover the majority of actual situations.
– First, we assume errors follow a fail-stop model. Upon a node crash, all data
on that node are supposed to be lost and we have to migrate its working state
to a new node. The crashed node can be allocated again after repaired.
– We assume node failures are completely independent. In other words, a node
failure does not increase or decrease the failure probability of other nodes.
– We assume that all kinds of failures have constant probabilities, including
single node failure, power supply module failure, fan system failure, air con-
dition system failure and water cooling system failure. We assume that these
probabilities do not vary by time or utilization frequency.
3.2
Probability Function
As mentioned in Sect. 2, the uppermost reason that in-memory checkpoint sys-
tem fails is simultaneous node failure within the same group, and the probability
of that is closely related to the scheme of node grouping in a given system. So,
we ﬁrst calculate the simultaneous failure probability of any two given nodes
before we propose the node group model in next section.
We take TianHe-2 high performance system as platform in this paper, which
has a typical hierarchy architecture of large-scale parallel computing system.

A Node Allocation Algorithm for In-memory Checkpoint System
201
As shown in Fig. 2, two nodes are integrated on a mainboard and share a power
supply module. Several mainboards, then form a chassis, which is equipped with
a standalone fan system. Each cabinet consists of several chassis and has its own
air condition system. Finally, a row of cabinets share a water cooling system.
In such a hierarchy architecture, diﬀerent node grouping schemes will result in
diﬀerent probability of simultaneous node failure within the same group.
Fig. 2. Organization structure of TianHe-2
Below, we will discuss in detail the probability function of simultaneous fail-
ure of node i and j (denoted as P j
i ). The probability can be calculated according
to the coordinates of the two nodes involved. We take the ratios of ﬁve kinds of
failures into account when calculating the probability: single node failure, power
supply failure, fan failure, air condition failure and water cooling system failure.
Single node failure ratio Pn is commonly considered as the reciprocal of the
mean time between failure of node MTBFn:
Pn =
1
MTBFn .
In the same way, the probability of power supply module failure Pm is equal to the
reciprocal of the mean time between failure of power supply module MTBFm:
Pm =
1
MTBFm .
As mentioned above, nodes on the same board share a single power supply module.
In other words, the failure of the power supply module will directly result in the failure
of all nodes on that board. Thus, without regard to other factors, the probability of two
simultaneous node failures (on the same board) caused by power supply module failure
is equal to Pm. Similarly, nodes within the same chassis share a unique fan system
and will fail together due to the high temperature if the fan system stops working.
Consequently, the probability of two simultaneous node failures (in the same chassis)
caused by fan system failure is equal to the probability of fan failure Pf, which is equal
to the reciprocal of the mean time between failure of fan MTBFf:
Pf =
1
MTBFf .

202
X. Liao et al.
For nodes within the same cabinet (sharing a unique air condition system) and the
same row (sharing a unique water cooling system), the probabilities of two simultaneous
failures caused by the air condition cooling system (Pc) and water cooling system (Pl)
failures are the mean time between failure of each cooling system:
Pc =
1
MTBFc , Pl =
1
MTBFl .
Now we consider the simultaneous failure probability of any two nodes i
and j. Let symbol m/f/c/l be 1 if node i and j belong to the same main-
board/chassis/cabinet/row, and 0 otherwise. First we only consider the factor of node
failure. As mentioned above, all failures are assumed to be independent. So the simul-
taneous failure probability of i and j is Pn
2. To simplify the representation, we denote
it as P j
i |n, that is,
P j
i |n = P 2
n.
Based on P j
i |n, we further take the power supply module into account. When i and
j are on the same mainboard (m = 1), the simultaneous failure probability is the sum of
Pm and the product of 1−Pm and P j
i |n. That is because both nodes will fail deﬁnitely
(with the probability of 1) if the power supply module fails (with the probability of
Pm); otherwise (with the probability 1 −Pm), the simultaneous failure probability is
P j
i |n. When i and j are on diﬀerent mainboards (m = 0), however, their failure are
independent and the probability is the product of each one’s failure probability, which
is Pm + (1 −Pm)Pn. We denote the probability of single node failure considering node
failure and power supply module failure as Pnm. Consequently, the simultaneous failure
probability of i and j with node failure and power supply module failure considered
(denoted as P j
i |nm) is
P j
i |nm =

Pm + (1 −Pm)P j
i |n, m = 1
Pnm
2,
otherwise
In the same way, we take the fan system failure into account based on the equation
above. When i and j are in the same chassis (f = 1), the simultaneous failure proba-
bility is the sum of Pf and the product of 1 −Pf and P j
i |nm, and otherwise (f = 0)
is the product of each one’s failure probability considering node failure, power supply
module failure and fan system failure. We denote the latter one as Pnmf, which can be
calculated as
Pnmf = Pf + (1 −Pf)Pnm.
So, we have
P j
i |nmf =

Pf + (1 −Pf)P j
i |nm, f = 1
Pnmf
2,
otherwise
After all factors are involved, we can get the ﬁnal probability equation as follows:
P j
i |nmfcl =

Pl + (1 −Pl)P j
i |nmfc, l = 1
Pnmfcl
2,
otherwise
(1)
We can see that Eq. 1 is a recursion function and can be easily extended to a
failure model with more organization hierarchies. Generally, for an S-level model, the
simultaneous failure probability of i and j considering all S kinds of failures (denoted
as P j
i |1∼S) is
P j
i |1∼S =

PS + (1 −PS)P j
i |1∼(S−1), TS = 1
P1∼S
2,
otherwise

A Node Allocation Algorithm for In-memory Checkpoint System
203
where TS represents whether the two nodes are in the same set at level S and P1∼S =
PS+(1−PS)P1∼(S−1). P j
i |1∼1 means the probability of simultaneous failure considering
the factor of level 1 failure (node failure) only, which is P1
2. Given Pk and Tk (1 ≤k ≤
S), we can get the simultaneous failure probability of any two nodes.
3.3
Model Overview
Based on the probability function, we propose a node allocation model, to ﬁnd the
optimal node grouping scheme for a given node set, a given probability function and a
given group size, so that the probability of simultaneous node failure within the same
group is minimal.
In the paper, we abstract the allocation model as a weighted undirected graph,
where vertices represent the computer nodes and the weight on the edge indicates the
probability that the two connected nodes fail simultaneously. Figure 3 shows a partial
view of a basic model with 3 individual computing nodes 1,2 and 3. The position of
Node in the system is denoted by its coordinate xi, yi, zi, ki. The value P on the edge
is the weight.
1
2
3
(x1,y1,z1,k1)
(x2,y2,z2,k2)
(x3,y3,z3,k3)
F(P;1,2)
P 2
1 =
F(P;2,3)
P 3
2 =
F(P;1,3)
P 3
1 =
Fig. 3. A weighted undirected graph
It should be explained that (xi, yi, zi, ki) indicates the speciﬁc position of node i in
the system, where xi, yi, zi and ki denote the number of board, the number of chassis,
the number of cabinet and the number of row where node i is located in respectively.
Consequently, we abstract the node allocation model as a graph problem. For a
given system, we use a graph to represent any given set of nodes. According to the
probability function proposed above, the weight of each edge in the graph can be
calculated. Then, the problem is to ﬁnd a graph partition scheme with a given group
size so that the probability of system failure due to two simultaneous node failures in
a group is minimal. In the next section, we propose a novel algorithm to solve this
problem.
4
Node Allocation Algorithm Based on Clique
SCR uses hop algorithm to divide compute nodes to groups, and hops are generally
selected to be 1 in many systems. Our model, however, transfers the compute nodes
into a weighted undirected graph, and tries to ﬁnd an optimal combination checkpoint
sets of nodes with the minimal weight. Given the positions of nodes, we can use Eq. 1
to calculate the simultaneous failure probability of every two nodes in the node set.

204
X. Liao et al.
Table 1. Probabilities of simultaneous failure
Node 1
2
3
4
5
6
1
-
0.18 0.1
0.18 0.18 0.18
2
0.18 -
0.85 0.18 0.1
0.1
3
0.1
0.85 -
0.1
0.1
0.36
4
0.18 0.18 0.1
-
0.1
0.25
5
0.18 0.1
0.1
0.1
-
0.85
6
0.18 0.1
0.36 0.25 0.85 -
For instance, we assume a task that occupies 6 compute nodes: {1,2,3,4,5,6}, and the
simultaneous failure probabilities are listed in Table 1:
As shown in Fig. 4, these nodes can be transferred into a complete weighted undi-
rected graph, where the weight of edge denotes the simultaneous failure probability of
these two nodes.
1
2
6
3
5
4
0.85
0.85
0.1
0.1
0.18
Fig. 4. A complete weighted undirected graph.
Consider a weighted undirected graph G = (N, E, W), where N is a set of nodes
{n1, n2, ..., nn}, and E and W are the edge and weight sets respectively, we have:
Deﬁnition 1. Given a node n in G, the number of its neighbor nodes is called the
degree of n.
Deﬁnition 2. Given a graph G, a subset of N is called a clique if every two nodes in
the subset are connected by an edge in G.
The problem is then attributed to ﬁnd a clique partition of the graph with speciﬁed
size, so as to minimize the probability of system failure due to two simultaneous node
failures in a clique.
Algorithm 1 shows the pseudo-code of a basic algorithm for node allocation. The
algorithm based on clique(CB algorithm for short) ﬁnds all cliques of speciﬁed size in
a set of compute nodes N. Given a set N, clique size s and the probability function
of simultaneous failure P, the algorithm will ﬁnd a clique sets C : {C1, C2, C3, ..., Cn}.
In line 5, the function BuildProbMatrix(N, P) calculates the simultaneous failure
probability of every two nodes in N based on function P (i.e., Formula 1). The function
AddEdges in line 12 adds new edges with minimal weight for nodes in the latest graph
G. Note that there may be multiple edges added at one time since they have the same

A Node Allocation Algorithm for In-memory Checkpoint System
205
weight. We start with minimal weight edges to make the weight of clique as small as
possible. Lines 13–24 search all cliques in the current graph. We travel the node set N
in ascending order of node degree since node with small degree has less opportunity
to form cliques with other nodes, so as to get as more cliques with minimal weight as
possible.
Algorithm 1. Find all cliques
Input: nodes set N, clique size s, probability function P
Output: clique set C
1. C ←∅
2. G ←{N, ∅, ∅}
3. W ←BuildProbMatrix(N, P)
4. while N ̸= ∅do
5.
if (#N) ≤s then
6.
c ←N
7.
C ←C + {c}
8.
return C
9.
end if
10.
G ←AddEdges(G, W) // edges with minimal weight for nodes in current G
11.
N ′ ←N
12.
while N ′ ̸= ∅do
13.
Let v ∈N ′ be the node with minimal degree in current G
14.
c ←FindAClique(G, v, s)
15.
if c ̸= ∅then
16.
C ←C + {c}
17.
N ←N −c // also remove all edges connected to c in G
18.
N ′ ←N ′ −c
19.
else
20.
N ′ ←N ′ −{v}
21.
end if
22.
end while
23. end while
24. return C
The function FindAClique in line 16 is used to ﬁnd a clique that contains node
v with size s in graph G. It uses a basic branch-and-bound algorithm to search for a
clique. Once a clique is found, we add it into the clique set C (line 18), remove the
nodes from original graph G (line 19), and also remove all edges connected to these
nodes. After the while-loop ﬁnishes (line 24), all possible cliques are generated and
removed from the current graph. Then some new edges with minimal weight should be
added and the search is redone until the graph is empty. Note that if the number of
nodes in current graph is no more than s, i.e., the clique size, we directly output it as
the last clique and quit (lines 7–11). This works for the situations that the number of
nodes is not divisible by s.
For the instance with 6 compute nodes given in Fig. 4, traditional hop algorithm
will divide them into two groups: {1, 2, 3} and {4, 5, 6} (assume that hop distance is
1 and group size is 3). Based on the probabilities in Table 1, the failure probability of
the system will be 0.9888.

206
X. Liao et al.
Fig. 5. An example of the algorithm.
The CB algorithm, however, tries to ﬁnd all cliques of size 3 with minimal weight.
First, all edges with weight 0.1 are added to the graph, as shown in Fig. 5(a), and we
travel the graph in the order 1 →6 →2 →4 →3 →5 according to the node degree.
The function FindAClique will ﬁnd the ﬁrst clique ({3, 4, 5}) when v = 4. Then, the
clique and all related edges are removed, as shown in Fig. 5(b). New edges with weight
0.18 are added after that because no more clique of size 3 can be found in the left
graph. Actually, in this example, ({1, 2, 6}) can be directly denoted as a clique without
search since the number of left nodes is 3, which is equal to the clique size. The failure
probability of whole system in this solution is 0.5588, which is only 56.51 % of that in
the hop algorithm.
As mentioned before, the “short-plate” of in-memory checkpoint system is prob-
ability of failure of any two nodes from the same group. The CB algorithm takes
probabilistic model as a guide, initiatively avoids the allocation of checkpoint set with
high simultaneous failure probability, and makes each “short-plate” as long as possible.
Consequently, it reduces the frequency of using ﬁle system checkpoints, which means
the cost of fault tolerance will be decreased.
5
Experimental Results
We have compared the performance of our algorithm with hop algorithms from the
state-of-the-art fault-tolerant library. In practice, the execution time of application
could be very long (up to days or months), and the overhead of grouping algorithm is
negligible; so we only compare the probability of simultaneous failure, in which case
the in-memory checkpoint system cannot recover the execution and higher level fault-
tolerant system with much higher cost has to be involved.
Table 2 lists the probabilities of simultaneous failure of our algorithm and hop
algorithm. PCB represents the probability of our clique algorithm, PHOP represents
the best result of the hop algorithm, and ratio represents the diﬀerence between them.
As mentioned in Sect. 3.2, the system has a total of 16,000 nodes, and we choose a
random subset of nodes in this experiment. It can be seen obviously in Table 2 that
clique allocation algorithm is very eﬃcient when the size of XOR set is small, especially
with size of 2. The ratio between the two algorithms becomes smaller as the size of
XOR set increases, and the solutions of two algorithms will become identical when the
size is equal to (or larger than) the number of computing nodes. This tendency is more
clearly illustrated in Fig. 6.
To be more clear, Fig. 7 gives the simultaneous failure probabilities of two algo-
rithms when #Node is 2048. We can see from the ﬁgure that the probability of the

A Node Allocation Algorithm for In-memory Checkpoint System
207
Table 2. Probabilities of simultaneous failure with diﬀerent node numbers and group
sizes
#Node
SizeXOR = 2
SizeXOR = 4
SizeXOR = 8
SizeXOR = 16
PCB
PHOP
ratio
PCB
PHOP
ratio
PCB
PHOP
ratio
PCB
PHOP
Ratio
64
1E-09
0.00031
114959
3.97E-05
0.00113
67.87
0.00034
0.00296
13.50
0.00465
0.00684
1.52
128
1E-08
0.00090
167483
3.19E-05
0.00245
88.53
0.00032
0.00589
18.90
0.00595
0.01295
2.27
256
1E-08
0.00170
145818
6.64E-05
0.00526
117.40
0.00059
0.01204
20.30
0.00912
0.02551
2.90
512
2E-08
0.00327
139793
0.00015
0.00981
148.50
0.00120
0.02288
19.10
0.01211
0.04923
4.21
1024
5E-08
0.00668
145621
0.00017
0.02010
207.60
0.00242
0.04596
19.10
0.02112
0.09744
4.81
2048
9E-08
0.01401
152651
0.00016
0.04020
411.70
0.00480
0.09059
18.90
0.03186
0.18509
6.06
4096
1.8E-07
0.00730
39570
0.00040
0.03976
287.50
0.00962
0.13239
13.80
0.04554
0.30447
6.73
8192
3.7E-07
0.01442
39092
0.00025
0.03938
154.50
0.01913
0.16271
8.51
0.07993
0.45199
5.65
16384
7.4E-07
0.12184
165302
0.00177
0.29667
167.20
0.03818
0.57240
15.00
0.71965
0.80620
1.12
#Node
SizeXOR = 32
SizeXOR = 64
SizeXOR = 128
SizeXOR = 256
PCB
PHOP
ratio
PCB
PHOP
ratio
PCB
PHOP
ratio
PCB
PHOP
Ratio
64
0.01114
0.01392
1.25
0.02705
0.02705
1.00
0.02705
0.02705
1.00
0.02705
0.02705
1.00
128
0.01989
0.02613
1.32
0.05203
0.05263
1.02
0.10208
0.10208
1.00
0.10208
0.10208
1.00
256
0.03645
0.05126
1.41
0.09059
0.10132
1.12
0.19173
0.19381
1.01
0.35117
0.35117
1.00
512
0.06925
0.10048
1.45
0.19105
0.19369
1.02
0.32208
0.34899
1.08
0.53657
0.57689
1.08
1024
0.13777
0.19352
1.41
0.30183
0.35450
1.18
0.56614
0.58503
1.03
0.74841
0.82544
1.10
2048
0.25198
0.34635
1.37
0.47044
0.58109
1.24
0.77218
0.82778
1.07
0.87476
0.97030
1.11
4096
0.43440
0.55524
1.28
0.67615
0.81967
1.21
0.87961
0.96930
1.10
0.89930
0.99908
1.11
8192
0.65864
0.77372
1.17
0.84162
0.96272
1.14
0.89956
0.99894
1.11
0.89999
0.99999
1.11
16384
0.86865
0.96454
1.11
0.89906
0.99890
1.11
0.89999
0.99999
1.11
0.90000
0.99999
1.11
2
4
8
16
32
64
128
256
10
0
10
1
10
2
10
3
10
4
10
5
Size of Checkpoint set
Gap between CB Algorithm and Hop Algorithm
Gap = Hop Algorithm/CB Algorithm
Fig. 6. Gap between two algorithms when #Node=1024.
2
4
8
16
32
64
128
256
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Size of Checkpoint set
Failure Probability
CB Algorithm
Hop Algorithm(Hop distance = 6)
Fig. 7. Simultaneous failure probabilities of the two algorithms when #Node=2048.

208
X. Liao et al.
simultaneous failure is always larger in hop algorithm than in our algorithm with all
set sizes.
Tables 3 and 4 compare the simultaneous failure probabilities of two algorithms
when sizeXOR = 3 and sizeXOR = 4 respectively, where we vary the hop distance
in the hop algorithm within diﬀerent ranges. The results show that the probability
in the hop algorithm is not aﬀected by the hop distance evidently. We can also see
that our algorithm obtains much lower simultaneous failure probabilities than the hop
algorithm in all cases. Figure 8 illustrates this result more clearly and intuitively.
Table 3. Probabilities of simultaneous failure with small hops in hop algorithm when
sizeXOR = 3.
#Node
PHOP
PCB
hop=1
hop=2
hop=3
hop=4
hop=5
hop=6
hop=7
hop=8
hop=9
hop=10
1000
0.01058
0.01574
0.01511
0.01430
0.01120
0.01414
0.01405
0.01167
0.01414
0.01272
1.01E-05
6000
0.13404
0.02085
0.02418
0.02445
0.02534
0.02562
0.02605
0.01772
0.01866
0.03975
5.99E-07
10000
0.31091
0.03400
0.03196
0.03063
0.03286
0.04901
0.07563
0.11029
0.10535
0.07420
1.1E-05
15000
0.54986
0.05346
0.05140
0.04826
0.04553
0.04403
0.04140
0.04100
0.04892
0.08605
1.5E-06
Table 4. Probabilities of simultaneous failure with large hops in hop algorithm when
sizeXOR = 4.
#Node
PHOP
PCB
hop=1
hop=17
hop=33
hop=49
hop=65
hop=81
hop=97
hop=113
hop=121
hop=136
1024
0.01853
0.02186
0.01971
0.02012
0.01971
0.02070
0.01971
0.01925
0.01981
0.01930
0.00018
2048
0.03534
0.04087
0.04171
0.04143
0.03893
0.04063
0.04042
0.04023
0.04054
0.04002
0.00017
8192
0.06354
0.04198
0.04047
0.04210
0.03984
0.03963
0.04264
0.03842
0.03951
0.03889
0.00026
16384
0.66958
0.15221
0.08228
0.33279
0.21335
0.38715
0.21852
0.28729
0.20119
0.33215
0.00177
1
2
3
4
5
6
7
8
9
10
17
33
51
79
101 121
10
−4
10
−3
10
−2
10
−1
Hop distance
Failure Probability
CB Algorithm
Hop Algorithm with different hop distances
Fig. 8. Simultaneous failure probabilities of hop algorithm with diﬀerent hop distances
when #Node=8192 and sizeXOR = 4.
Figure 9 shows the simultaneous failure probabilities with diﬀerent hop distances
and checkpoint set sizes. As concluded above, hop distance has little inﬂuence on
probability in the hop algorithm. Those curves represent hop algorithm almost coincide
in this ﬁgure. Also, the blue curve, which represents our algorithm, shows better results
with all checkpoint set sizes compared to the hop algorithm.
Table 5 collects the times that our algorithm outperforms the hop algorithm in
1,000 random experiments. Since our algorithm is heuristic, the search result is not
necessarily the optimum solution, and the hop algorithm gets chance to obtain better

A Node Allocation Algorithm for In-memory Checkpoint System
209
2
4
8
16
32
64
128
256
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Size of Checkpoint set
Failure Probability
CB Algorithm
Hop Algorithm(Hop distance=1)
Hop Algorithm(Hop distance=33)
Hop Algorithm(Hop distance=68)
Hop Algorithm(Hop distance=123)
Fig. 9. Simultaneous failure probabilities of the two algorithms with diﬀerent hop
distances and sizeXORs when #Node=1024
result due to the randomness of the node set we choose. However, we can notice in
the table that for most situations, our method can outperform the hop algorithm in all
1,000 random tests. We can also see that with a ﬁxed checkpoint set size, the times that
our method win decrease mildly when the number of nodes gets larger. That is because
when almost all nodes in the system are involved, the topology of these nodes is also
pretty much ﬁxed, in which case, the hop algorithm can decrease simultaneous node
failure probability easily by assigning a hop distance large enough.
Table 5. Times that our algorithm outperforms the hop algorithm in 1,000 random
experiments.
#Node sizeXOR
#Node sizeXOR
2
4
64
256
3
5
65
257
512
1000 1000 1000 1000
500
1000 1000 1000 1000
1024
1000 1000 1000
999
1000
1000 1000 1000 1000
8192
1000 1000
998
992 10000
1000 1000
996
988
16384
1000 1000
990
978 15000
1000 1000
991
976
Experimental results above show that our clique-based algorithm is very eﬃcient,
especially for small size of checkpoint set. The probability of simultaneous node failure
is far below the hop algorithms, which means we can greatly reduce the chance to
invoke the high-cost global checkpoint system.
6
Conclusion and Future Work
We build a new node allocation model based on the architecture of TianHe-2 and pro-
pose a new algorithm to decrease the probability that in-memory checkpoint system
cannot work. We calculate the probability of simultaneous failure of any two nodes,
transfer it into a complete weighted undirected graph, use a heuristic algorithm to
ﬁnd clique in the graph, and then rationally divide the compute nodes into groups to
decrease the in-group simultaneous failure probability. The experimental results per-
formed based on the probability model abstracted from TianHe-2 show that, compared

210
X. Liao et al.
to the traditional node distribution scheme, our model can ﬁnd near optimal combina-
tion of nodes with lower simultaneous failure probability. This also means that we can
greatly reduce the cost of recovery in multi-level checkpoint system. In the future, we
will take the communication cost into account when grouping the nodes based on the
topology of the interconnect network.
Acknowledgment. This work is supported by National High Technology Research
and Development Program of China (863 Program) No.2012AA01A301 and 2012A
A01A309.
References
1. http://source-forge.net/projects/scalablecr/scalable-checkpoint/restart-library
2. http://www.netlib.org/utk/people/jackdongarra/papers/tianhe-2-dongarra-
report.pdf
3. Daly, J.: A higher order estimate of the optimum checkpoint interval for restart
dumps. Future Gener. Comput. Syst. 22(3), 303–312 (2006)
4. Duda, A.: The eﬀects of checkpointing on program execution time. Inf. Process.
Lett. 16(5), 221–229 (1983)
5. Vivek Sarkar, E.: Exascale software study: Software challenges in exascale systems
(2009)
6. Glosli, J.N., Caspersen, K.J., Gunnels, J.A., Rudd, D.F.R.A.E., Streitz, F.H.:
Extending stability beyond cpu millennium: a micron-scale atomistic simulation
of kelvin-helmholtz instability. In: Proceedings of the 2007 ACM/IEEE Confer-
ence on Supercomputing (SC), pp. 1–11 (2007)
7. Iskra, K., Romein, J.W., Yoshii, K., Beckman, P.: Zoid: I/o-forwarding infrastruc-
ture for petascale architectures. In: PPoPP 2008: Proceedings of the 13th ACM
SIGPLAN Symposium on Principles and Practice of Parallel Programming, pp.
153–162 (2008)
8. Michalak, S.E., Harris, K.W., Hengartner, N.W., Takala, B.E., Wender, S.A.: Pre-
dicting the number of fatal soft errors in los alamos national laboratory’s ASC Q
supercomputer. IEEE Trans. Device Mater. Reliab. 5(3), 329–335 (2005)
9. Moody, A.: The scalable checkpoint/restart (scr) library, user manual version 1.1-6
(2010)
10. Moody, A., Bronevetsky, G., Mohror, K., de Supinski, B.R.: Design, modeling, and
evaluation of a scalable multi-level checkpointing system. In: Proceedings of the
International Conference for High Performance Computing, Networking, Storage
and Analysis(SC), pp. 13–29, November 2010
11. Naksinehaboon, N., Liu, Y., Leangsuksun, C.B., Nassar, R., Paun, M., Scott, S.L.:
Reliability-aware approach: an incremental checkpoint/restart model in hpc envi-
ronments. In: Proceedings of the 2008 Eighth IEEE International Symposium on
Cluster Computing and the Grid (CCGRID), pp. 783–788 (2008)
12. Plank, J.S., Thomason, M.G.: Processor allocation and checkpoint interval selec-
tion in cluster computing systems. J. Parallel Distrib. Comput. 61(11), 1570–1590
(2001)
13. Ross, R., Moreira, J., Cupps, K., Pfeiﬀer, W.: Parallel i/o on the ibm blue gene/l
system. Blue Gene/L Consortium Quarterly Newsletter. Technical report (2006)

A Node Allocation Algorithm for In-memory Checkpoint System
211
14. Schroeder, B., Gibson, G.: Understanding failure in petascale computers. J. Phys.
Conf. Series: SciDAC 78, 012–022 (2007)
15. Schroeder, B., Gibson, G.A.: A large-scale study of failures in high-performance
computing systems. In: Proceedings of the International Conference on Dependable
Systems and Networks (DSN), pp. 249–258 (2006)
16. Young, J.W.: A ﬁrst order approximation to the optimum checkpoint interval.
Commun. ACM 17(9), 530–531 (1974)

A Scalable Algorithm for Radiative
Heat Transfer Using Reverse Monte Carlo
Ray Tracing
Alan Humphrey(B), Todd Harman, Martin Berzins, and Phillip Smith
University of Utah, Salt Lake City, USA
ahumphrey@sci.utah.edu
Abstract. Radiative heat transfer is an important mechanism in a class
of challenging engineering and research problems. A direct all-to-all treat-
ment of these problems is prohibitively expensive on large core counts
due to pervasive all-to-all MPI communication. The massive heat trans-
fer problem arising from the next generation of clean coal boilers being
modeled by the Uintah framework has radiation as a dominant heat
transfer mode. Reverse Monte Carlo ray tracing (RMCRT) can be used
to solve for the radiative-ﬂux divergence while accounting for the eﬀects
of participating media. The ray tracing approach used here replicates
the geometry of the boiler on a multi-core node and then uses an all-to-
all communication phase to distribute the results globally. The cost of
this all-to-all is reduced by using an adaptive mesh approach in which
a ﬁne mesh is only used locally, and a coarse mesh is used elsewhere.
A model for communication and computation complexity is used to pre-
dict performance of this new method. We show this model is consistent
with observed results and demonstrate excellent strong scaling to 262 K
cores on the DOE Titan system on problem sizes that were previously
computationally intractable.
Keywords: Uintah · Radiation modeling · Parallel · Scalability · Adap-
tive mesh reﬁnement · Simulation science · Titan
1
Introduction
Our study is motivated primarily by the target problem of the University of Utah
Carbon Capture Multi-Disciplinary Simulation Center (CCMSC). This project
aims to eventually simulate a 350 MWe clean coal boiler being developed by
Alstom Power during the next ﬁve years, by using large parallel computers in a
scalable manner for reacting, large eddy simulations (LES)-based codes within
the Uintah open source framework, and to use accelerators at large scale.
Within the boiler, the hot combustion gases radiate energy to the boiler walls
and to tubes carrying water and steam that is superheated to a supercritical ﬂuid.
This steam acts as the working ﬂuid to drive the turbine for power generation.
The residual energy in the mixture passes through a convective heat exchange
system to extract as much of the remaining energy as possible into the working
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 212–230, 2015.
DOI: 10.1007/978-3-319-20119-1 16

A Scalable Algorithm for Radiative Heat Transfer
213
ﬂuid. This radiative ﬂux depends on the radiative properties of the participating
media and temperature. The mixture of particles and gases emits, absorbs and
scatters radiation, the modeling of which is a key computational element in these
simulations. The radiation calculation, in which the radiative-ﬂux divergence at
each cell of the discretized domain is calculated, can take up to 50 % of the
overall CPU time per timestep using the discrete ordinates method (DOM), one
of the standard approaches to computing radiative heat transfer. This method,
which Uintah currently uses, is computationally expensive, involves multiple
global, sparse linear solves and presents challenges both with the incorporation
of radiation physics such as scattering and to the use of parallel computers at
very large scales. Reverse Monte Carlo ray tracing (RMCRT), the focus of this
work, is one of the few numerical techniques that can accurately solve for the
radiative-ﬂux divergence while accounting for the eﬀects of participating media,
naturally incorporates scattering physics, and lends itself to scalable parallelism.
The principal challenges with our initial, single ﬁne mesh (single-level) RMCRT
approach are the all-to-all communication requirements and on-node memory
constraints. To address these challenges, our study explores a multi-level, adap-
tive mesh reﬁnement (AMR) approach in which a ﬁne mesh is only used close
to each grid point and a successively coarser mesh is used further away. The
central question of our study will be to determine if our AMR approach can
scale to large core counts on modern supercomputers, and if our communication
and computation models can accurately predict how this approach to radiation
scales on current, emerging and future architectures.
In what follows, Sect. 2 provides an overview of the Uintah software, while
Sect. 3 describes our RMCRT model in detail and provides an overview of the
key RMCRT approaches considered and used within Uintah. Section 4 details our
model of communication and computation for our multi-level AMR approach.
Section 5 provides strong scaling results over a wide range of core counts (up to
262 K cores) for this approach, and an overview of related work is given in Sect. 6.
The paper concludes in Sect. 7 with future work in this area.
2
The Uintah Code
The Uintah open-source (MIT License) software has been widely ported and used
for many diﬀerent types of problems involving ﬂuids, solids and ﬂuid-structure
interaction problems. The present status of Uintah, including applications, is
described by [4]. The ﬁrst documented full release of Uintah was in July 2009 and
the latest in January 2015 [37]. Uintah consists of a set of parallel software com-
ponents and libraries that facilitate the solution of partial diﬀerential equations
on structured adaptive mesh reﬁnement (AMR) grids. Uintah presently contains
four main simulation components: (1.) the multi-material ICE [20] code for both
low and high-speed compressible ﬂows; (2.) the multi-material, particle-based
code MPM for structural mechanics; (3.) the combined ﬂuid-structure interac-
tion (FSI) algorithm MPM-ICE [12] and (4.) the ARCHES turbulent reacting
CFD component [19] that was designed for simulating turbulent reacting ﬂows

214
A. Humphrey et al.
with participating media radiation. Uintah is highly scalable [6,24], runs on
many National Science Foundation (NSF), Department of Energy (DOE) and
Department of Defense (DOD) parallel computers (Stampede, Mira, Titan, Vul-
can, Vesta, Garnet, Kilraine, etc.) and is also used by many NSF, DOE and
DOD projects in areas such as angiogenesis, tissue engineering, green urban
modeling, blast-wave simulation, semi-conductor design and multi-scale materi-
als research [4].
Uintah is unique in its combination of the MPM-ICE ﬂuid-structure-
interaction solver, ARCHES heat transfer solver, AMR methods and directed
acyclic graph (DAG)-based runtime system. Uintah is one of the few codes that
uses a DAG approach as part of a production strength code in a way that is
coupled to a runtime system. Uintah also provides automated, large-scale par-
allelism through a design that maintains a clear partition between applications
code and its parallel infrastructure, making it possible to achieve great increases
in scalability through changes to the runtime system that executes the taskgraph,
without changes to the taskgraph speciﬁcations themselves. The combination of
the broad applications class and separation of the applications problems from a
highly scalable runtime system has enabled engineers and computer scientists to
focus on what each does best, signiﬁcantly lowering the entry barriers to those
who want to compute a parallel solution to an engineering problem. Uintah is
open source, freely available and is the only widely available MPM code. The
broad international user-base and rigorous testing ensure that the code may be
used on a broad class of applications.
Particular advances made in Uintah are scalable adaptive mesh reﬁnement
[25] coupled to challenging multiphysics problems [5]. A key factor in improv-
ing performance has been the reduction in MPI wait time through the dynamic
and even out-of-order execution of task-graphs [29]. The need to reduce memory
use in Uintah led to the adoption of a nodal shared memory model in which
there is only one MPI process per multicore node, and execution on individual
cores is through Pthreads [27]. This has made it possible to reduce memory
use by a factor of 10 and to increase the scalability of Uintah to 768 K cores
on complex ﬂuid-structure interactions with adaptive mesh reﬁnement. Uintah’s
thread-based runtime system [27,30] uses: decentralized execution [29] of the
task-graph, implemented by each CPU core requesting work itself and perform-
ing its own MPI. A shared memory abstraction through Uintah’s data warehouse
hides message passing from the user but at the cost of multiple cores accessing
the warehouse originally. A shared memory approach that is lock-free [30] was
implemented by making use of atomic operations (supported by modern CPUs)
and thus allows eﬃcient access by all cores to the shared data on a node. Finally,
the nodal architecture of Uintah has been extended to run tasks on one or more
on-node accelerators [15]. This uniﬁed, heterogeneous runtime system [28] makes
use of a multi-stage queue architecture (two sets of task queues) to organize work
for CPU cores and accelerators in a dynamic way, and is the focus of current
development.

A Scalable Algorithm for Radiative Heat Transfer
215
2.1
The ARCHES Combustion Simulation Component
The radiation models in Uintah have previously been a part of the ARCHES
component, which was designed for the simulation of turbulent reacting ﬂows
with participating media. ARCHES is a three-dimensional, large eddy simu-
lation (LES) code that uses a low-Mach number variable density formulation
to simulate heat, mass, and momentum transport in reacting ﬂows. The LES
algorithm solves the ﬁltered, density-weighted, time-dependent coupled conser-
vation equations for mass, momentum, energy, and particle moment equations
in a Cartesian coordinate system [19]. This set of ﬁltered equations is discretized
in space and time and solved on a staggered, ﬁnite volume mesh. The staggering
scheme consists of four oﬀset grids, one for storing scalar quantities and three for
each component of the velocity vector. Stability preserving, second order explicit
time-stepping schemes and ﬂux limiting schemes are used to ensure that scalar
values remain bounded. ARCHES is second-order accurate in space and time and
is highly scalable through Uintah and its coupled solvers like hypre [10] to 256 K
cores [36]. Research using ARCHES has been done on radiative heat transfer
using the parallel discrete ordinates method and the P1 approximation to the
radiative transport equation [22]. Recent work has shown that RMCRT methods
are potentially more eﬃcient [17,39].
3
RMCRT Model
Scalable modeling of radiation is currently one of the most challenging problems
in large-scale simulations, due to the global, all-to-all nature of radiation [31].
To simulate thermal transport, two fundamental approaches exist: random walk
simulations, and ﬁnite element/ﬁnite volume simulations, e.g., discrete ordinates
method (DOM)
[3], which involves solving many large systems of equations.
Accurate radiative-heat transfer algorithms that handle complex physics are
inherently computationally expensive [16], particularly when high-accuracy is
desired in cases where spectral or geometric complexity is involved. They also
have limitations with respect to scalability, bias and accuracy.
The Uintah ARCHES component is designed to solve the mass, momentum,
mixture fraction, and thermal energy governing equations inherent to coupled
turbulent reacting ﬂows. ARCHES has relied primarily on a legacy DOM solver
to compute the radiative source term in the energy equation [19]. Monte Carlo
ray tracing (MCRT) methods for solving the radiative transport equation oﬀer
higher accuracy in two key areas where DOM suﬀers: geometric ﬁdelity and spec-
tral resolution. In applications where such high accuracy is important, MCRT
can become more eﬃcient than DOM approaches. In particular, MCRT can
potentially reduce the cost signiﬁcantly by taking advantage of modern hardware
on large distributed shared memory machines [14], and now on distributed mem-
ory systems with on-node graphics processing unit (GPU) accelerators, using a
prototype GPU implementation of the single-level RMCRT [15], written using
NVidia CUDA.

216
A. Humphrey et al.
3.1
Radiation and Ray Tracing Overview
The heat transfer problems arising from the clean coal boilers being modeled
by the Uintah framework has thermal radiation as a dominant heat transfer
mode and involves solving the conservation of energy equation and radiative
heat transfer equation (RTE) simultaneously. Thermal radiation in the target
boiler simulations is loosely coupled to the computational ﬂuid dynamics (CFD)
due to time-scale separation and is the rightmost source term in the conservation
of energy equation shown by:
cv
DT
Dt
= −∇· (κ∇T) −p∇· v + Φ + Q′′′ −∇· qr
(1)
where cv is the speciﬁc heat, T is the temperature ﬁeld, p is the pressure, k is
the thermal conductivity, c is the velocity vector, Φ is the dissipation function,
Q′′′ is the heat generated within the medium, e.g. chemical reaction, and ∇· qr
is the net radiative source. The energy equation is then conventionally solved
by ARCHES (ﬁnite volume) and the temperature ﬁeld, T is used to compute
net radiative source term. This net radiative source term is then fed back into
energy equation (for the ongoing CFD calculation) which is solved to update the
temperature ﬁeld, T.
A radiatively participating medium can emit, absorb and scatter thermal
radiation. The RTE (2) as shown in [41], is the equation describing the inter-
action of absorption, emission and scattering for radiative heat transfer and is
an integro-diﬀerential equation with three spatial variables and two angles that
determine the direction of ˆs [41].
dIη(ˆs)
ds
= ˆs∇Iη(ˆs)
(2)
= kηIη −βηIη(ˆs)
+ σsη
4π

4π
Iη(ˆs)Φη( ˆsi, ˆs)dΩi,
In (2), kη is the absorption coeﬃcient, σsη is the scattering coeﬃcient (dependent
on the incoming direction s and wave number η), βη is the extinction coeﬃcient
that describes total loss in radiative intensity, Iη is the change in intensity of
incoming radiation from point s to point s + ds and is determined by summing
the contributions from emission, absorption and scattering from direction ˆs and
scattering into the same direction ˆs at wave number η. Φη( ˆsi, ˆs) is the phase
function that describes the probability that a ray coming from direction si will
scatter into direction ˆs and integration is performed over the entire solid angle
Ωi [32,41].
DOM, MCRT and RMCRT all aim to approximate the radiative transfer
equation. In the case of RMCRT, a statistically signiﬁcant number of rays (pho-
ton bundles) are traced from a computational cell to the point of extinction.
This method is then able to calculate energy gains and losses for every element
in the computational domain. The process is considered “reverse” through the

A Scalable Algorithm for Radiative Heat Transfer
217
Helmholtz Reciprocity Principle, e.g. incoming and outgoing intensity can be
considered as reversals of each other [13]. Through this process, the divergence
of the heat ﬂux for every sub-volume in the domain (and radiative heat ﬂux
for surfaces, e.g., boiler walls) is computed by Eq. 3, as rays accumulate and
attenuate intensity (measured in watts per square meter, SI units based on the
StefanBoltzmann constant) according to the RTE for an absorbing, emitting and
scattering medium.
∇· q = κ(4πIemmited −

4π
IabsorbeddΩ),
(3)
where the rightmost term,

4π IabsorbeddΩ is represented by the sum N
r=1 Ir 4π
N
for each ray r up to N rays. The integration is performed over the entire solid
angle Ω. Ray origins are randomly distributed throughout a given computational
cell. In our implementation, the Mersenne Twister random number generator [26]
is used to generate ray origins. The ray marching algorithm proceeds in a similar
fashion to that shown by [1].
Within the Uintah RMCRT module, rays are traced backwards from the detec-
tor, thus eliminating the need to track ray bundles that never reach the detec-
tor [32]. Rather than integrating the energy lost as a ray traverses the domain,
Fig. 1. 2D Outline of reverse Monte Carlo ray tracing [15]

218
A. Humphrey et al.
RMCRT integrates the incoming intensity absorbed at the origin, where the ray
was emitted. RMCRT is more amenable to domain decomposition, and thus Uin-
tah’s parallelization scheme due to the backward nature of the process [38], and
the mutual exclusivity of the rays themselves. Figure 1 shows the back path of a
ray from S to the emitter E, on a nine cell structured mesh patch. Each ith cell
has its own temperature Ti, absorption coeﬃcient κi, scattering coeﬃcient σi and
appropriate pathlengths li,j [15]. In each case the incoming intensity is calculated
in each cell and then traced back through the other cells. The Uintah RMCRT
module computes how much of the outgoing intensity has been attenuated along
the path. When a ray hits a boundary, as on surface 17 in Fig. 1, the incoming
intensities will be partially absorbed by the surface. When a ray hits a hot bound-
ary surface, its emitted surface intensity contributes back to point S. Rays are
terminated when their intensity is suﬃciently small [15].
RMCRT uses rays more eﬃciently than forward MCRT, but it is still an
all-to-all method, for which all of the geometric information and radiative prop-
erties (temperature T, absorption coeﬃcient κ, and cellType (boundary or ﬂow
cell)) for the entire computational domain must be accessible by every ray [38].
When using a ray tracing approach (forward or backward), two approaches for
parallelizing the computation are considered when using structured grids, 1.)
parallelize by patch-based domain decomposition with local information only
and pass ray information at patch boundaries via MPI, and 2.) parallelize by
patch-based domain decomposition with global information and reconstruct the
domain for the quantities of interest on each node by passing domain informa-
tion via MPI. The ﬁrst approach becomes untenable due to potentially billions
of rays whose information would need to be communicated as they traverse the
domain. In the second approach the primary diﬃculty is eﬃciently constructing
the global information for millions of cells in a spatially decomposed (patch-
based) domain. The second approach is the one taken used in this work. While
reconstruction of all geometry on each node has shown to limit the size of the
problem that can be computed [17], we will show that the multi-level mechanisms
in Uintah allows representing a portion of the domain at a coarser resolution,
thus lowering the memory usage and message volume, ultimately scaling to over
256 K CPU cores. The hybrid memory approach of Uintah also helps as only one
copy of geometry and radiative properties is needed per multi-core node [30].
RMCRT will be invoked largely on coarser mesh levels and the CFD calculation
will be performed on the highest resolved mesh.
3.2
Uintah RMCRT Approaches
Within the Uintah RMCRT module there are numerous approaches, each
designed for a speciﬁc use case, and range from a single-level method to a full
adaptive mesh reﬁnement using an arbitrary number of grid levels with varying
reﬁnement ratios. Our study focuses on the multi-level mesh reﬁnement approach
and its scalability to large core counts. CPU Scaling results for this approach
are shown in Sect. 5.

A Scalable Algorithm for Radiative Heat Transfer
219
Single-Level RMCRT: The single level RMCRT approach was initially imple-
mented as a proof-of-concept to begin comparisons against the legacy DOM
solver within the Uintah ARCHES component. This approach focused on the
benchmark problem described by Burns and Christen in [9]. In this approach,
the quantity of interest, the divergence of the heat ﬂux, ∇q is calculated for every
cell in the computational domain. The entire domain is replicated on every node
(with all-to-all communication) for the following quantities; κ, the absorption
coeﬃcient, a property of the medium the ray is traveling through, σT 4, a phys-
ical constant σ· temperature ﬁeld, T 4 and, cellType, a property of each compu-
tational cell in the domain to determine if along a given path, a ray will reﬂect
or stop on a given computational cell. These three properties are represented by
1 double, 1 double and 1 integer value respectively.
For Ntotal mesh cells, the amount of data communicated is O(N 2). While
accurate and eﬀective at lower core counts, the volume of communication in this
case overwhelms the system for large problems in our experience. Calculations
on domains up to 5123 cells are possible on machines with at least 2 GB RAM
per core and only when using Uintah’s multi-threaded runtime system, described
in Sect. 2. Strong scaling breakdown for the single-level approach occurs around
8-10 K CPU cores for a 3843 domain. Currently, Uintah has a production-grade
GPU implementation of this single-level approach that delivers a 4-6X speedup1
in mean time per timestep for this benchmark with a domain size of 1283 cells.
The work done to achieve accelerator task scheduling and execution is detailed
in [15]. Initial scalability and accuracy studies of the single-level RMCRT algo-
rithm are also shown in [17] which examines the accuracy of the computed
divergence of the heat ﬂux as compared to published data and reveals expected
Monte-Carlo convergence.
Multi-level Adaptive Mesh Reﬁnement: In this adaptive meshing app-
roach, a ﬁne mesh is used locally and only coarser representations of the entire
domain are replicated on every node (with all-to-all communication) for the
radiative properties, T, κ cellType. The ﬁne level consists of a collection of
patches where each patch is considered a region of interest and individually
processed using a local ﬁne mesh and underlying global coarse mesh data.
Figure 2 illustrates a three-level mesh coarsening scheme and how a ray might
traverse this multi-level domain. Surrounding a patch is a halo region which eﬀec-
tively increases the size (at the ﬁnest resolution) of each patch in each direction,
x, y and z. This distance is user speciﬁed. An arbitrary number of successively
coarser levels (received by each node during the all-to-all communication phase)
reside beneath the ﬁne level for the rays to travel across once they have left the
ﬁne level. Each ray ﬁrst traverses a ﬁne level patch until it moves beyond the
boundary and surrounding halo of this ﬁne-level patch. At this point, the ray
moves to a coarser level. Once outside this coarse level, the ray moves again to
a coarser level. The rays move from level to level, similar to stair stepping, until
the coarsest level is reached. Once on the coarsest level, a ray cannot move to
a ﬁner level. The key goal of this approach is to achieve a reduction in both
1 1-NVIDIA K20 GPU vs. 16-Intel Xeon E5-2660 CPU cores @2.20 GHz.

220
A. Humphrey et al.
Fig. 2. RMCRT - 2D diagram of three-level mesh reﬁnement scheme, illustrating how
a ray from a ﬁne-level patch (right) might be traced across a coarsened domain (left).
communication and computation costs as well as memory usage. This approach
is fundamental to our target problem, the 350 MWe boiler predictive case where
the entire computational domain needs to be resolved to adequately model the
radiative heat ﬂux.
Initial scalability results on a two-level methane jet problem are shown in [31].
This problem was run on the the DOE Titan, Mira and NSF Stampede systems
with 10 rays per cell, two grid levels, a reﬁnement ratio of four and a problem size
of 2563 cells on the highest resolved mesh. These results provided an excellent
starting point by showing scaling to 16 K CPU cores. Scaling results beyond
16 K cores at the time was not possible due to algorithmic issues, which were
ultimately resolved in this work and are detailed in Sect. 5.
4
Complexity Model
In this section we generalize the somewhat simplistic analysis given in [31] (as
suggested there) of the two-level scheme of [17] to a detailed discussion of both
the computational and communications costs of a multiple mesh level approach.
Initially our approach involves replicating the geometry of the target problem
and constructing an adaptive mesh for the radiation calculations. The adaptive
mesh used by the radiation calculation may be constructed directly from the
eﬃcient mesh data structure used to describe the whole mesh. This is a one-
time procedure and so is not analyzed further here.
We suppose that on N 3
nodes compute nodes there is a global ﬁne mesh of n3
mesh
cells in n3
patch mesh patches. Deﬁne nlocal = nmesh/Nnodes, and deﬁnes nplocal =
npatch/Nnodes so that each node has n3
local ﬁne mesh cells in n3
plocal patches. In
the ray tracing algorithm, each compute node then has to compute the heat
ﬂuxes, ∇q on its local mesh by ray tracing and to export the temperatures
T and and the absorption coeﬃcient κ on the original mesh to neighbouring
“halo” nodes, or in a coarsened form (possibly at multiple levels) to other nodes.

A Scalable Algorithm for Radiative Heat Transfer
221
Finally the coarsest mesh representations are distributed to all the other nodes.
The amount of information per cell transmitted is two doubles κ, σT 4 and one
integer, cell type.
To assess the complexity of RMCRT on a ﬁxed ﬁne mesh, computational
experiments to measure the per cell and per ray cost of the RMCRT:CPU imple-
mentation were conducted on a single CPU with a single-level grid. Ray scatter-
ing and reﬂections were not included in these experiments. In both experiments
the absorption coeﬃcient was initialized according to the benchmark of Burns
& Christen [9] with a uniform temperature ﬁeld. A grid with a single patch and
1 MPI process was used, thus eliminated any communication costs. The grid
resolution varied from 163, 323, 643 to 1283 cells with each cell using emitting 25
rays. The mean time per timestep (MTPTS) was computed using 7 timesteps.
The code was instrumented to sum the number of cells traversed during the com-
putation and it was shown that MTPTS = (n3
mesh)1.4. In the second experiment
the number of grid cells in the domain was ﬁxed at 413 and the number of rays,
nray per cell varied. The MTPTS was computed over 47 timesteps. Here it was
shown that the MTPTS varies linearly with the number of rays per cell. Based
on these experiments, the cost for a single patch, without any communication,
is approximately given by
T global
rmcrt = C∗nraysn3
mesh
4/3,
(4)
where C∗is a constant. This result may be interpreted as saying that the rays
from each of the n3
mesh cells travel a distance of nmesh cells on average. In the
case of a ﬁne mesh on a node and a coarse representation of the rest of the mesh.
T local
rmcrt = C∗nrays

n3
local
4/3 + (nmesh2−m)3)
4/3
(5)
where 2m is the reﬁnement ratio used to obtain the coarse mesh. It is possible
to extend this analysis to more mesh reﬁnement levels.
Communications Costs: The main step with regard to communication is to
update the temperatures T and and the absorption coeﬃcients κ every timestep.
On a uniform ﬁne mesh this is done by each node sending out the values of these
quantities to all the other compute nodes, and in a multiple mesh level approach
this is done by each node sending out the values of these quantities on the coarse
mesh and ﬁne mesh values locally.
Fine Mesh Global Communications: Each node has to transmit (N 3
nodes−1)
messages of size (nlocal)3 · 3. This is currently done by a series of asynchronous
sends but could be done with an MPI Allgather. This has a complexity of
α3log(Nnodes) + β N3
nodes−1
N3
nodes (nlocal)3 for N 3
nodes nodes with n3
local elements per
mesh patch, where α is the latency and β is the transmission cost per ele-
ment [40]. This result applies for both the recursive doubling and Bruck algo-
rithms [40]. Other recursive doubling algorithms result in a complexity of
α3log(Nnodes) + β(N 3
nodes −1)(nlocal)3, so the cost may be dependent on the
MPI implementation used.

222
A. Humphrey et al.
Coarse Mesh All-to-All: In the case of using a coarse mesh in which the
mesh is reﬁned by a factor of 2m in each dimension, each node has to transmit
(N 3
nodes−1) messages of size (nlocal2−m)3∗3. Thus reducing the communications
volume, but not the number of messages, by a factor of 23m overall.
Multi-level Adaptive Mesh Reﬁnement: This approach considers each ﬁne
level patch (individually) in the domain as a region of interest (ROI) and for each
ﬁne level patch, the highest resolved CFD mesh is used. Figure 2 illustrates one
patch being such a region of interest. In the case of a region of interest consisting
of Pint patches, the compute node must transmit the ﬁne mesh information to
all the local nodes close to the ROI. In this context let Li be the nodes that
are i levels of nodes removed from node containing the region of interest. There
will then be 26 level-1 nodes and 98 level-2 nodes. Of course at the edges of a
spatial simulation domain or in the case of a small domain of interest each node
will only have to communicate ﬁne mesh values of κ, σT 4 to a fraction of the
nodes. In this case let Li,j
active be the number of active nodes (halo-level nodes) at
level j, where j < Nlevels, active for the ith level of interest, where active nodes
are the local halos from the ﬁne mesh. Furthermore let the reﬁnement factor be
1
2m(i,j) active at this level. Then the ﬁne mesh communication associated with
this region of interest is given by
Comfhalo =
Nlevels

j=1
Li,j
active(α + β(nlocal2−m(i,j))3 ∗3)
(6)
This means that the ratio of communications to computations Ratio is now be
given as:
Ratio = ((N 3
nodes −1))(α + β(nlocal2−m)3 ∗3) + Comfhalo
T local
rmcrt
,
(7)
where α and β are deﬁned above and scaled by the cost of a FLOP. Overall
this expression allows us to analyze the relationship between computation and
communications.
Strong scaling of RMCRT does not change the overall volume of data com-
municated. Increasing the number of Nnodes by a factor of two simply reduces
nlocal by two. This does mean that the number of messages increases even with
the total communications value being constant. Moving to MPI Allgather also
has the same issue but the factor of 3logNnodes also increased by adding 3. Thus
for enough rays nrays with enough reﬁnement by a factor of 2m on the coarse
radiation mesh, the computation will likely dominate. A key challenge is that
storage of O(nmesh2−m)3) is required on a multicore node and that an AMR
mesh representation is needed at very large core counts. Some aspects of this
analysis are not dissimilar to earlier work by one of us on PDE solvers with
global coarse mesh operations [11] using algorithms related to those of [8]. The
results of this analysis will make it possible to prioritize subsequent serial and
parallel performance tuning and and also perhaps to make projections regarding
performance on forthcoming petascale and exascale architectures.

A Scalable Algorithm for Radiative Heat Transfer
223
5
Scaling Studies
In this section, we show strong scalability results on the DOE Titan XK72 system
for the Burns and Christen [9] benchmark problem using the multi-level mesh
reﬁnement approach. We deﬁne strong scaling as a decrease in execution time
when a ﬁxed size problem is solved on more cores. This work focuses on using all
CPU cores available on Titan. Subsequent work will focus on additionally using
all of Titan’s GPUs in addition to its CPUs, following our prototype work on a
single mesh in [15,31].
The scaling challenges faced in this work have only become apparent by
running this challenging problem at such high core counts, stressing areas of
infrastructure code in ways never before seen, speciﬁcally Uintah’s task-graph
compilation phase. With Uintah’s directed acyclic graph (DAG)-based design
[31], during an initial simulation timestep, the initial timestep of a restart, or
when the grid layout or its partition changes, a new task graph needs be to
created and compiled. Task-graph compilation is a complex operation with mul-
tiple phases, including creation and scheduling of tasks themselves on local and
neighboring patches (for halo exchange), keeping a history of what these tasks
require and compute, setting up connections between tasks (edges in the DAG),
and ﬁnally assigning MPI message tags to dependencies.
As the RMCRT ray trace task requests ghost cells across the entire domain
(a global halo) for ray marching, Uintah’s task-graph compilation algorithm was
overcompensating when constructing lists of neighboring patches for local halo
exchange. The cost of this operation grew despite the number of patches per node
remaining constant, resulting in task-graph compilation times of over four hours
at 32 K cores with 32,000 total patches. This necessitated extensive algorithmic
improvements to the task-graph compilation algorithm. The original complexity
of this operation was O(n1·log(n1)+n2·log(n2)), and after optimization became
O(n1 · log(n1)) + O

n2
p · log(n2)
	
, where n1 is the number of patches on coarse
level, n2 is the number of patches on ﬁne level, and p is the number of processor
cores. This reduced the four hour task-graph compilation time to under one
minute at 32 K cores, thus making possible the results presented here.
5.1
CPU Strong Scaling of Multi-level Adaptive Mesh
Reﬁnement, RMCRT
Our scaling study focuses on a two-level AMR problem based on benchmark
described in [9], which exercises all of the main features of the AMR support
2 Titan is a Cray KX7 system located at Oak Ridge National Laboratory, where each
node hosts a 16-core AMD Opteron 6274 processor running at 2.2 GHz, 32 GB DDR3
memory and 1 NVIDIA Tesla K20x GPU with 6 GB GDDR5 ECC memory. The
entire machine oﬀers 299,008 CPU cores and 18,688 GPUs (1 per node) and over 710
TB of RAM. Titan uses a Cray Gemini 3D Torus network, 1.4 µs latency, 20 GB/s
peak injection bandwidth, and 52 GB/s peak memory bandwidth per node.

224
A. Humphrey et al.
Table 1. Total number of MPI messages and average number of messages per MPI
rank for each problem size, 1283, 2563 and 5123 (ﬁne mesh)
Cores
256
1 K
4 K
8 K
16 K
32 K
64 K
128 K
256 K
1283 total msgs
1001
5860
36304
avg msgs/node
62.5
91.6
141.8
2563 total msgs
9843
52.1 K
105.2 K
212.1 K
437.7 K
avg msgs/node
153.8
203.3
205.6
207.0
213.7
5123 total msgs
338.2 K
673.8 K
1.36 M
2.71 M
5.42 M
10.88 M
avg msgs/node
660.5
658.0
663.65
662.6
661.36
662.83
within Uintah in addition to the radiation physics required by our target prob-
lem. A ﬁne level halo region of four cells in each direction, x, y, z was used. The
AMR grid consisted of two levels with a reﬁnement ratio of four, the CFD mesh
being four times more resolved than the radiation mesh. For three separate cases,
the total number of cells on the highest resolved level was 1283, 2563 and 5123
(green, red and blue lines respectively in Fig. 3), with 100 rays per cell in each
case. The total number of cells on the coarse level was 323, 643 and 1283. In all
cases, each compute core was assigned at least 1 ﬁne mesh patch from the CFD
level. Figure 3 shows excellent strong scaling characteristics for our prototype,
two-level benchmark problem [9]. The eventual breakdown in scaling in each
problem size is due to diminishing work, when a patch’s MPI messages begins
to exceed the cost of its computation, and hence the runtime system cannot
overlap computation with communication. Figure 4 additionally shows the MPI
wait associated with the global and local communications for this calculation
along side the execution times for each of the three cases above.
Though the actual communication patterns for this problem are perhaps
more complicated than our predictive model, due to MPI message combining
and packing done by Uintah, both Table 1 and Fig. 4 illustrate points made in
Sect. 4, that global communications dominate and that the local communications
do not have a signiﬁcant impact, and for enough rays and enough reﬁnement on
the coarse radiation mesh, the computation does in fact dominate (Eq. 7 of our
predictive model). These results also show how the number of MPI messages
grows with the number of cores. A key point to note, as is evidenced by the
dominating global communications, is that the reﬁnement ratio of four reduces
the global communication phase by a factor of 64 (ignoring communications
latency for large messages) over a ﬁne mesh all-to-all. If this communications
phase took 8–64 times as long it would destroy scalability.
5.2
Multi-Level Accuracy Considerations
To quantify the error associated with coarsening the radiative properties (tem-
perature T, absorption coeﬃcient κ, and cellType (boundary or ﬂow cell)), an
error analysis was performed using a simpliﬁed version of the adaptive-meshing
approach described in Sect. 3. The grid consisted of a ﬁne and coarse mesh and

A Scalable Algorithm for Radiative Heat Transfer
225
Fig. 3. Strong scaling of the two-level benchmark RMCRT problem on the DOE Titan
system. L-1 (Level-1) is the ﬁne, CFD mesh and L-0 (Level-0) is the coarse, radiation
mesh.
Fig. 4. Strong scaling with communication costs of the two-level benchmark. L-1
(Level-1) is the ﬁne, CFD mesh and L-0 (Level-0) is the coarse, radiation mesh

226
A. Humphrey et al.
Fig. 5. L2 norm error of ∇q vs reﬁnement ratio, The error in each direction (x,y,z) is
shown.
during a radiation timestep the quantities necessary to compute ∇q were inter-
polated to the coarser grid level. The radiation calculation was performed on
the coarse level including all ray tracing. The ∇q was then compared using the
computed solution of the Burns and Christen [9] benchmark problem at the
prescribed 41 locations. 100 rays per cell were used in the computation and
the reﬁnement ratio between the coarse and ﬁne grids was varied from 1 to 8.
Figure 5 shows the L2 norm error of ∇q versus reﬁnement ratio. This represents
a worse case scenario, as only coarsened quantities are used in the computation.
In addressing the issue of accuracy, our approach will be to continue send-
ing the coarse mesh in the all-to-all communication phase of each simulation
timestep, but to recover the ﬁne mesh values of the radiative properties through
interpolation. This approach is well suited for GPU accelerators such as those
on the Titan system, where FLOPS are inexpensive relative to the cost of data
movement. Further compression of the coarse mesh information will also be
investigated.
6
Related Work
Industrial codes, such as Fluent, incorporate straightforward radiation models
such as the discrete ordinates method in Fluent and Airpack [2], but scale to
relatively small numbers of cores. At the national labs, many cutting edge codes
are developed, such as Fuego, CFDLIB, Kiva, and radiation codes ATTILA,
DANTE, WEDGEHOG, PARTISN [21,23], but many of these are not generally
available, are unsupported, or are targeted at other problems such as neutron

A Scalable Algorithm for Radiative Heat Transfer
227
transport. There are also radiation transport problems that use CFD codes and
AMR techniques [18,34], however, a broad range of problems exist that require
the concept of tracing rays or particles, such as the simulation of light transport
and electromagnetic waves. In the case of adaptive mesh codes there are many
such solvers. Speciﬁc examples of these codes are the Flash code [7,35] based
on adaptive oct-tree meshes and the physics AMR code Enzo [33,42]. These
examples are perhaps closest as these combine AMR and radiation, but for very
diﬀerent problem classes. Most of these codes do not target the problems that
Uintah has been designed for, with large deformations, complex geometries, high
degrees of parallelism and now radiation.
7
Conclusions and Future Work
We have demonstrated that through leveraging the multi-level AMR infrastruc-
ture provided by the Uintah framework, we have developed a scalable approach
to radiative heat transfer using reverse Monte Carlo ray tracing. The scaling
and communication cost results shown in Sect. 5 provide a promising alternative
to approaches to radiation modeling such as discrete ordinates. Using our cost
model for communication and computation, we can predict how our approach
to radiation modeling may scale and perform on current, emerging and future
architectures.
The addition of a scalable, hierarchical radiation solver within Uintah will
also beneﬁt the general computational science engineering community in appli-
cations areas such as turbulent combustion simulation and other energy-related
problem. The broader impact of our work may ultimately include algorithmic
developments for related problems with pervasive all-to-all type communications
in general, such as long-range electrostatics in molecular dynamics, and will be
of importance to a broad class of users, developers, scientists and students for
whom such problems are presently a bottleneck.
Our primary focus in moving beyond this study will be continued devel-
opment of RMCRT capabilities explored here, to provide support for several
additional energy-related problems within the scope of the Utah CCMSC. The
relationship between accuracy, number of rays cast, reﬁnement ratios between
grid levels and extent of the ﬁne-level halo region is being explored as part of
the ongoing research goals. The calculations demonstrated in this work are ideal
candidates for large-scale accelerator use, employing large numbers of rays for
every cell in the computational domain. As such, implementation of a multi-
level GPU:RMCRT module is now underway with the aim of using the whole of
machines like Titan with accelerators.
Acknowledgments. This material is based upon work supported by the Department
of Energy, National Nuclear Security Administration, under Award Number(s) DE-
NA0002375, and by DOE ALCC award CMB109, “Large Scale Turbulent Clean Coal
Combustion”, for time on Titan. This research used resources of the Oak Ridge Lead-
ership Computing Facility, which is a DOE Oﬃce of Science User Facility supported

228
A. Humphrey et al.
under Contract DE-AC05-00OR22725. We would also like to thank all those involved
with Uintah past and present, Isaac Hunsaker and Qingyu Meng in particular.
References
1. Amanatides, J., Woo, A.: A fast voxel traversal algorithm for ray tracing. Euro-
graphics 87, 3–10 (1987)
2. Ansys, I.: Fluent Web Page (2014). http://www.ansys.com/Products/
3. Balsara, D.: Fast and accurate discrete ordinates methods for multidimensional
radiative transfer. part i, basic methods. J. Quant. Spectr. Radiative Trans. 69(6),
671–707 (2001). http://www.sciencedirect.com/science/
4. Berzins, M.: Status of release of the Uintah computational framework. Techni-
cal report, UUSCI-2012-001, Scientiﬁc Computing and Imaging Institute (2012).
http://www.sci.utah.edu/publications/SCITechReports/UUSCI-2012-001.pdf
5. Berzins, M., Luitjens, J., Meng, Q., Harman, T., Wight, C., Peterson, J.: Uintah - a
scalable framework for hazard analysis. In: TG 2010: Proceedings of 2010 TeraGrid
Conference. ACM, New York (2010)
6. Berzins, M., Meng, Q., Schmidt, J., Sutherland, J.C.: DAG-based software frame-
works for PDEs. In: Alexander, M., D’Ambra, P., Belloum, A., Bosilca, G., Can-
nataro, M., Danelutto, M., Di Martino, B., Gerndt, M., Jeannot, E., Namyst, R.,
Roman, J., Scott, S.L., Traﬀ, J.L., Vall´ee, G., Weidendorfer, J. (eds.) Euro-Par
2011, Part I. LNCS, vol. 7155, pp. 324–333. Springer, Heidelberg (2012)
7. Fryxell, B., Olson, K., Ricker, P., Timmes, F.X., Zingale, M., Lamb, D.Q., Macne-
ice, P., Rosner, R., Truran, J., Tufo, H.: FLASH an adaptive mesh hydrodynamics
code for modeling astrophysical thermonuclear ﬂashes. Astrophys. J. Suppl. Ser.
131, 273–334 (2000)
8. Brandt, A., Lubrecht, A.: Multilevel matrix multiplication and fast solu-
tion
of
integral
equations.
J.
Comput.
Phys.
90(2),
348–370
(1990).
http://www.sciencedirect.com/science/article/pii/002199919090171V
9. Burns, S.P., Christen, M.A.: Spatial domain-based parallelism in large-scale,
participating-media, radiative transport applications. Numer. Heat Trans., Part
B: Fundam. 31(4), 401–421 (1997)
10. Falgout, R., Jones, J., Yang, U.: The design and implementation of hypre, a library
of parallel high performance preconditioners. In: Numerical Solution of Partial
Diﬀerential Equations on Parallel Computers, UCRL-JRNL-205459, vol. 51, pp.
267–294. Springer (2006)
11. Goodyer, C.E., Berzins, M.: Parallelization and scalability issues of a multilevel
elastohydrodynamic lubrication solver. Concurrency Comput.: Pract. Experience
19(4), 369–396 (2007). http://dx.doi.org/10.1002/cpe.1103
12. Guilkey, J.E., Harman, T.B., Xia, A., Kashiwa, B.A., McMurtry, P.A.: An
Eulerian-Lagrangian approach for large deformation ﬂuid-structure interaction
problems, part 1: algorithm development. In: Fluid Structure Interaction II. WIT
Press, Cadiz (2003)
13. Hapke, B.: Theory of Reﬂectance and Emittance Spectroscopy. Cambridge Uni-
versity Press, Cambridge (1993). http://dx.doi.org/10.1017/CBO9780511524998.
cambridge Books Online
14. Howell, J.R.: The monte carlo in radiative heat transfer. J. Heat Trans. 120(3),
547–560 (1998)

A Scalable Algorithm for Radiative Heat Transfer
229
15. Humphrey, A., Meng, Q., Berzins, M., Harman, T.: Radiation modeling using the
Uintah Heterogeneous CPU/GPU runtime system. In: Proceedings of the 1st Con-
ference of the Extreme Science and Engineering Discovery Environment (XSEDE
2012). ACM (2012)
16. Hunsaker, I.: Parallel-distributed, Reverse Monte-Carlo radiation in coupled, Large
Eddy combustion simulations. Ph.D. thesis, Department of Chemical Engineering,
University of Utah (2013)
17. Hunsaker, I., Harman, T., Thornock, J., Smith, P.: Eﬃcient parallelization of
RMCRT for large scale LES combustion simulations, paper AIAA-2011-3770. In:
41st AIAA Fluid Dynamics Conference and Exhibit (2011)
18. Jessee, J.P., Fiveland, W.A., Howell, L.H., Colella, P., Pember, R.B.: An adaptive
mesh reﬁnement algorithm for the radiative transport equation. J. Comput. Phys.
139(2), 380–398 (1998)
19. J. Spinti, Thornock, J., Eddings, E., Smith, P., Saroﬁm, A.: Heat transfer to objects
in pool ﬁres. In: Transport Phenomena in Fires. WIT Press, Southampton (2008)
20. Kashiwa, B., Gaﬀney., E.: Design basis for cfdlib. Technical report, LA-UR-03-
1295, Los Alamos National Laboratory (2003)
21. Kerbyson, D.: A look at application performance sensitivity to the bandwidth
and latency of inﬁniband networks. In: 20th International Parallel and Distributed
Processing Symposium, IPDPS 2006, p. 7, April 2006
22. Krishnamoorthy, G., Rawat, R., Smith, P.: Parallelization of the P-1 radiation
model, numerical Heat Transfer. Part B: Fundamentals 49(1), 1–17 (2006)
23. Los Alamos National Security, L.: Los Alamos National Laboratory Transport
Packages (2014). http://www.ccs.lanl.gov/CCS/CCS-4/codes.shtml
24. Luitjens, J., Berzins, M.: Improving the performance of Uintah: A large-scale adap-
tive meshing computational framework. In: Proceedings of the 24th IEEE Interna-
tional Parallel and Distributed Processing Symposium (IPDPS10) (2010). http://
www.sci.utah.edu/publications/luitjens10/Luitjens ipdps2010.pdf
25. Luitjens, J., Berzins, M.: Scalable parallel regridding algorithms for block-
structured adaptive mesh reﬁnement. Concurrency and Comput.: Pract. Expe-
rience 23(13), 1522–1537 (2011). http://dx.doi.org/10.1002/cpe.1719
26. Matsumoto, M., Nishimura, T.: Mersenne twister: A 623-dimensionally equidis-
tributed uniform pseudo-random number generator. ACM Trans. Model. Comput.
Simul. 81(3), 3–30 (1998). http://doi.acm.org/10.1145/272991.272995
27. Meng, Q., Berzins, M., Schmidt, J.: Using hybrid parallelism to improve memory
use in the Uintah framework. In: Proceedings of the 2011 TeraGrid Conference
(TG11), Salt Lake City, Utah (2011)
28. Meng, Q., Humphrey, A., Berzins, M.: The Uintah framework: a uniﬁed heteroge-
neous task scheduling and runtime system. In: Digital Proceedings of Supercom-
puting 2012 - WOLFHPC Workshop. IEEE (2012)
29. Meng, Q., Luitjens, J., Berzins, M.: Dynamic task scheduling for the uintah
framework. In: Proceedings of the 3rd IEEE Workshop on Many-Task Comput-
ing on Grids and Supercomputers (MTAGS 2010) (2010). http://www.sci.utah.
edu/publications/meng10/Meng TaskSchedulingUintah2010.pdf
30. Meng, Q., Berzins, M.: Scalable large-scale ﬂuid-structure interaction solvers in the
Uintah framework via hybrid task-based parallelism algorithms. In: Concurrency
and Computation: Practice and Experience (2013). http://dx.doi.org/10.1002/cpe.
3099
31. Meng, Q., Humphrey, A., Schmidt, J., Berzins, M.: Investigating applications
portability with the uintah dag-based runtime system on petascale supercomput-

230
A. Humphrey et al.
ers. In: Proceedings of the International Conference on High Performance Com-
puting, Networking, Storage and Analysis, SC 2013, pp. 96:1–96:12. ACM, New
York (2013). http://doi.acm.org/10.1145/2503210.2503250
32. Modest, M.F.: Backward Monte Carlo simulations in radiative heat transfer.
J. Heat Trans. 125(1), 57–62 (2003). http://link.aip.org/link/JHTRAO/v125/i1/
p57/s1&Agg=doi
33. O’Shea, B., Bryan, G., Bordner, J., Norman, M., Abel, T., Harkness, R., Kritsuk,
A.: Introducing Enzo, an amr cosmology applications. In: Plewa, T., Linde, T.,
Gregory Weirs, V. (eds.) Adaptive Mesh Reﬁnement - Theory and Application.
Lecture Notes in Computational Science and Engineering, vol. 41, pp. 341–350.
Springer, Heidelberg (2005)
34. Pernice, M., Philip, B.: Solution of equilibrium radiation diﬀusion problems using
implicit adaptive mesh reﬁnement. SIAM J. Sci. Comput. 27(5), 1709–1726 (2005)
35. Rijkhorst, E.J., Plewa, T., Dubey, A., Mellema, G.: Hybrid characteristics: 3d
radiative transfer for parallel adaptive mesh reﬁnement hydrodynamics. Astron.
Astrophys. 452(3), 907–920 (2006)
36. Schmidt, J., Berzins, M., Thornock, J., Saad, T., Sutherland, J.: Large scale parallel
solution of incompressible ﬂow problems using Uintah and hypre. In: Proceedings
of CCGrid 2013. IEEE/ACM (2013)
37. Scientiﬁc Computing and Imaging Institute: Uintah Web Page (2015). http://www.
uintah.utah.edu/
38. Sun, X.: Reverse Monte Carlo ray-tracing for radiative heat transfer in combustion
systems. Ph.D. thesis, Department of Chemical Engineering, University of Utah
(2009)
39. Sun, X., Smith, P.J.: A parametric case study in radiative heat transfer using the
reverse monte-carlo ray-tracing with full-spectrum k-distribution method. Journal
of Heat Transfer 132(2) (2010)
40. Thakur, R., Rabenseifner, R., Gropp, W.D.: Optimization of collective commu-
nication operations in mpich. Int. J. High Perform. Comput. Appl. 19(1), 49–66
(2005)
41. Viswanath, K., Veljkovic, I., Plassmann, P.E.: Parallel load balancing heuristics
for radiative heat transfer calculations. In: CSC, pp. 151–157 (2006)
42. Wise, J.H., Abel, T.: enzo+moray: radiation hydrodynamics adaptive mesh reﬁne-
ment simulations with adaptive ray tracing. Monthly Notices of the Royal Astro-
nomical Society 414(4), 3458–3491 (2011). http://dx.doi.org/10.1111/j.1365-2966.
2011.18646.x

Optimizing Processes Mapping for Tasks
with Non-uniform Data Exchange Run
on Cluster with Diﬀerent Interconnects
Victor Getmanskiy1(B), Vladimir Chalyshev1, Dmitriy Kryzhanovsky1,
Igor Lopatin2, and Evgeny Leksikov2
1 Singularis Lab Ltd., Lenina Av. 86, 400005 Volgograd, Russia
{victor.getmanskiy,vladimir.chalyshev,
dmitry.kryzhanovsky}@singularis-lab.com
http://www.singularis-lab.com
2 Intel Inc., Turgeneva 30, 603950 Nizhniy Novgorod, Russia
http://www.intel.com
Abstract. The problem of mapping the parallel task to the nodes of
computing cluster is considered. MPI software with non-uniform commu-
nication and heterogeneous interconnect of computing cluster could run
faster using custom parallel processes mapping for optimization of data
exchange. The graph mapping algorithm is developed. It uses parallel
program representation as a task graph and cluster topology represen-
tation as system graph. The proposed optimization technique is tested
on synthetic benchmark and on CORAL QBox software to study its
eﬃciency on large number of computing cores. The positive results of
optimization are achieved and the summary is presented in the paper.
Keywords: Task mapping · Communication graph · Cluster · MPI
1
Introduction
Eﬃcient heterogeneous computing requires some approaches and methods of
appropriate mapping of parallel processes into computing hardware. Majority of
these approaches uses task graph and system graph model [1–3,7,8,11]. There
are diﬀerent optimization methods for solving NP-full problem of mapping task
graph to system graph, include heuristic [8,10], clustering [11], genetic [7] and
proﬁle-based greedy [3] strategies. Common issue in heterogenous cluster system
is the bandwidth and latency of interconnect which aﬀects the performance in
case of intensive non-uniform data exchange between parallel processes [9].
In considered approaches task graph is an undirected graph in which vertices
represent parallel processes and edges are weighted by data sizes sent between
processes. In scientiﬁc simulation software task graph can have a non-uniform
edges conﬁguration. The width of the edge lines is selected according to the
amount of exchanged data.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 231–239, 2015.
DOI: 10.1007/978-3-319-20119-1 17

232
V. Getmanskiy et al.
System graph represents the computing cluster topology taking into account
the performance of diﬀerent interconnect channels. Edges of system graph are
weighted by factors representing the ratio between inter node and intra node
sending time. Modern cluster hardware consists of several CPUs and coprocessors
such as GPGPU FPGA or Intel R⃝Xeon PhiTM. Interconnect between nodes can
also have a diﬀerent speed (InﬁniBandTM and Ethernet). So the cluster graph
also can have a complex non-uniform structure (Fig. 1).
Fig. 1. Heterogeneous cluster node conﬁguration
1.1
Problem Formulation
The main assumption in this work is that the exchange will be performed faster
if the task graph nodes are mapped to system graph nodes in such a way that
the heavy edges of task graph are mapped to the fastest interconnect edges in
system graph. The problem described is reduced to the following optimization
problem. Let us specify the task graph as
G1(P, L), Li = (ni, di), i = 1, . . . , N
where P–set of vertices (parallel processes), L–set of edges (processes communi-
cations), ni–data exchange frequency, di–amount of data exchanged, N–number
of edges. Let us specify the system graph as
G2(V, D), Dj = (lj, bj), j = 1, . . . , M
where V –set of vertices (computing cores), D–set of edges (communication links),
lj–latency, bi–bandwidth, M–number of edges. In ﬁrst assumption any pair of
nodes cluster nodes could exchange data through interconnect, so G2 is a com-
plete graph and N ≤M. The required mapping can be described as:
G1 →G2(P →V ) : Li →Dj, F(D, L) →min
Objective function to be minimized if as follows:
F(D, L) =
N

k=1
Tk, Tk(Di, Lj) = linj + dj/bi, F →min

Optimizing Processes Mapping for Tasks
233
The simpliﬁed formulation is used for the developed prototype. Task graph
weight is an amount of exchanged data and system graph edges weight is data
exchange “speed” ratio ri:
˜F ( ˜D, L) =
N

k=1
˜Tk, ˜Tk( ˜Di, Lj) = ridj, ˜
Di = (ri), ˜F →min
2
Mapping Algorithm
Two algorithms were developed. The ﬁrst one is based on brute force and the
second one is a greedy algorithm. The ﬁrst algorithm gives exact solution but
requires a lot of computation and suits only for small graphs with up to 10
vertices. The greedy solution is practically used and provides the optimal solution
in most cases.
The greedy solution is based on paper [3]. The main diﬀerence with our
modiﬁcation is that the paper examines a vertexes number in the cluster graph
equal to a vertex number in the task graph. The general scheme is as follows:
1. We ﬁnd the ﬁrst approximation. At the prototype stage we use the mapping
of the i-th cluster graph vertex to the i-th task graph vertex.
2. After that we try to improve the result iteratively:
(a) Take two vertexes of the task graph and change their destinations, that
is, if we had the mapping a →p, b →q, it will become a →q, b →p as
proposed in paper [3].
(b) Take any vertex and assign it to another system node (for example,
a →p, a →q).
The method complexity is O(IE(M + N)), where I is the iterations number, E
is the edges number in the task graph, M is the vertexes number in the task
graph, N is the vertexes number in the cluster graph. The greedy algorithm
converges quite fast, approximately within 100 iterations. The main drawback
of the algorithm is in ﬁnding suboptimal solutions, not truly optimal, but in
many cases the solution gives the mapping that improves the performance of
MPI software.
3
Synthetic Test Description
Synthetic test is a data exchange benchmark with diﬀerent amount and intensity.
It represents a best-for-optimization test case of task graph. There are two types
of communication:
1. Groups of parallel processes with tight communication (Intragroup commu-
nication).
2. Intergroup communications.

234
V. Getmanskiy et al.
Tight interaction blocks count is speciﬁed as Nbl. Number of processes in block
is speciﬁed as Nperbl. In each block processes communicate with each other (fully
connected sub graph) but inter-block communication is performed only between
the last process in i-th block and the ﬁrst (see Fig. 2). Iteration skipping is
speciﬁed (inter-block exchanges are not at each iteration). Process numbers are
mixed randomly (with constant Seed for random generator).
Fig. 2. Task graph example for 2 blocks with 6 processes in each
Synthetic test is also parameterized by intra-group data volume amount D,
and inter-block data amount Dbl, and iteration skipping Nskip. The oﬀset and
length linear arrays are constructed at each iteration for collective MPI com-
munication routine MPI AllToAllv [4]. These arrays are complemented at each
1+Nskip iteration by inter-block exchange data. For example, in case of Nskip = 2
the arrays are ﬁlled as shown at Table 1.
Table 1. Synch table
Target process id
0
1
2
3
4
5
6
7
8
9
10
11
Iter. 3k, 1 + 3k
oﬀset
0
0
0
0
0
0
D
D
2D
3D
4D
5D
Process id 2
length
0
0
0
0
0
D
0
D
D
D
D
D
Iter. 3k, 1 + 3k
oﬀset
0
D
2D
2D
3D
4D
5D
5D
5D
5D
5D
5D
Process id 11
length
D
D
0
D
D
0
D
0
0
0
0
0
Iter. 2 + 3k
oﬀset
0
0
0
0
0
0
D
D
2D
3D
4D
5D
Process id 2
length
0
0
0
0
0
D
0
D
D
D
D
Dbl
Iter. 2 + 3k
oﬀset
0
D
2D
2D+
3D+
4D+
4D+
5D+
5D+
5D+
5D+
5D+
Dbl
Dbl
Dbl
Dbl
Dbl
Dbl
Dbl
Dbl
Dbl
Process id 11
length
D
D
Dbl
D
D
0
D
0
0
0
0
0
The exchange data array is ﬁlled according to the table for process 2 and
process 11 at inter-group exchange iterations (Iteration number 0 + 3k, 1 + 3k)
and intra-group exchange iterations (Iteration number 2 + 3k). For processes
without intra-group communications the tables are similar but without addi-
tional Dbl at intra-group exchange iterations. So Dbl exchange data block is

Optimizing Processes Mapping for Tasks
235
inserted to data oﬀset and length arrays of the 2-nd and 11-th processes in every
three iterations. Thus the synthetic test routine is parameterized by following
parameters: Nbl–number of blocks, Nperbl–number of processes in block, D–intra-
block data volume amount, Dbl–inter-block data amount, Nskip–iteration skip-
ping step for inter-block communication. Synthetic test allows setting arbitrary
conﬁguration of task suitable for custom CPUs with diﬀerent number of cores
and loading the interconnect with diﬀerent data amount and diﬀerent inter-node
exchange intensity. The simulated data exchange is used to build task graph by
Intel R⃝MPI Library statistics (generated using I MPI STATS environment vari-
able) [4]. So the synthetic test allows simulating the data exchange suitable for
cluster with groups of computing cores linked by interconnect by a specifying
number of processes in group equal to number of cores per node in cluster. After
processes are randomized, the optimization algorithm must ﬁnd the solution that
maps these groups on cluster nodes.
4
Benchmarking
Benchmarks run using Intel R⃝MPI Library [4] launcher mpirun. A convenient
way to pass a lot of parameters to MPI launcher is through using the conﬁgura-
tion ﬁle and –conﬁgﬁle key. So the run command is
mpirun --configfile config.txt
The tool for generating conﬁguration ﬁles is developed. It uses a host list
ﬁle for input The ﬁrst step is running small time test collecting MPI statistics.
Input data for generating conﬁguration ﬁle (conﬁg.txt) is a host list with names
of node machines in cluster and number of cores in each node (hostlist.txt).
Conﬁg ﬁle for 4-nodes cluster with 4-cores CPU in each node is as follows:
-env I_MPI_STATS 4 -env I_MPI_STATS_FILE stat -n 4 -host node1 app
-env I_MPI_STATS 4 -env I_MPI_STATS_FILE stat -n 4 -host node2 app
-env I_MPI_STATS 4 -env I_MPI_STATS_FILE stat -n 4 -host node3 app
-env I_MPI_STATS 4 -env I_MPI_STATS_FILE stat -n 4 -host node4 app
Running the MPI program using this conﬁg.txt ﬁle launches application proﬁles
it and generates stats.txt ﬁle with data exchange statistics between each pair of
processes. After that tool generates task graph for G1 which edges are weighted
by amount of exchanged data from stats.txt. Then tool generates full system
graph G2 based on hostlist.txt creating groups of vertices coresponded to each
host (row in hostlist.txt). Vertices in each group are connected with “fast” edges
weighted by constant 1 and vertices between groups are connected with “slow”
edges weighted by constant 2. For the second run the tool uses stats.txt ﬁle
and hostlist.txt as input and generates optimized conﬁg.txt ﬁle. It generates a
conﬁg.txt ﬁle with properly ordered tasks so that they are mapped to computing
cores in the optimal way. The generated conﬁg.txt ﬁle for optimized run maps
the tasks to hosts.

236
V. Getmanskiy et al.
-n 2 -host node1 /path/to/application
-n 2 -host node4 /path/to/application
-n 1 -host node2 /path/to/application
...
The optimized conﬁguration ﬁle is used for a long running benchmark. Bench-
marking methodology is to measure long running benchmark time with default
conﬁguration ﬁle (T, sec.) and with optimized conﬁguration ﬁle (Topt, sec.). The
eﬃciency metric is an actual speedup calculated as S = T/Topt.
All benchmarks were launched on remote Tornado SUSU cluster [6]. Cluster
has the following conﬁguration. Hardware: 50–400 nodes: 2 x Intel R⃝XeonTM
X5680 CPUs, Intel R⃝Xeon PhiTM SE10X, 2 Gb DDR3-1333 RAM per node,
QDR InﬁniBandTM, Software: Linux CentOS 6.2, Software: Intel MPI 4.1.3.045,
CORAL QBox 1.40b. (default parameters; built with Intel C++ Compiler XE
14.0.1 for Linux).
4.1
Synthetic Benchmark on Two Intel R
⃝Xeon PhiTM Nodes
Benchmark was launched in two nodes with 60-cores Intel R⃝Xeon PhiTM in
native mode. Synthetic benchmark consists of K process groups with M processes
in each and K · M = 120. Intra-group communication data volume is much
greater than inter-node. Benchmark results are presented in Table 2.
Table 2. Synch table
K
M T, sec
Topt, sec S
2
60 660.847
84.125
7.855
4
30 320.938 153.668
2.088
8
15 165.581
34.700
4.771
15
8
93.015
39.100
2.378
20
6
68.347
30.007
2.277
4.2
Synthetic Benchmark for 10 Nodes Cluster
Benchmarking is performed on CPUs of 10 nodes using various exchanged data
amount and a number of exchange iterations (Niter) according to Table 3. Amount
of exchanged data is deﬁned in 64-bit doubles which means that for 100000000
doubles the data volume is about 800 Mb per node. 10 groups with 12 processes
in each are used in synthetic benchmark so there are 120 parallel processes. The
benchmarks were performed with ﬁxed amount of total exchanged data. Number
of exchange iteration diﬀers. Data size in doubles for inter group (Vinter) and
intra group (Vintra) communication also diﬀers, and Vintra = 1000Vinter.
The results prove that the higher exchange intensity of smaller amount of
data benchmark case gives better optimization results.

Optimizing Processes Mapping for Tasks
237
Table 3. Synthetic benchmark results for 10-nodes CPU cluster
Niter
Vinter Vintra
T, sec. Topt, sec. S
100
1000
1000000 1755
880
1.99
1000
100
100000 23.21
9.56
2.42
10000
10
10000 5.96
1.01
5.9
Fig. 3. Task graph for QBox run on 2 nodes
Fig. 4. Time and optimization time of QBox benchmark
4.3
CORAL QBox Software Benchmark on Remote Cluster
The benchmarks were launched on up to 400 nodes. CORAL QBOX [5] bench-
mark is a molecular dynamics code. The method has O(N 3) computational com-
plexity, where N is the total number of valence electrons in the system. Task
graph for large amount of nodes is quite general. But two nodes communication
graph (Fig. 3) demonstrates two clusters of nodes with tight communication.
And the ordering of nodes in each group corresponds to cores on nodes. QBox
benchmark was selected for better performance with grid dimensions 2512 for
25 nodes and 50 × 12, 100 × 12, 150 × 12, 200 × 12, 400 × 12 for 50–400 nodes
(benchmark was performed on 300–4800 cores). Optimized benchmark conﬁgu-
ration ﬁle is obtained using ration of 2 inter-nodes edges weight to intra-nodes

238
V. Getmanskiy et al.
edges weight. The result of optimization is steady increasing eﬀect with increas-
ing number of nodes and on 100 nodes it gives 20 % performance gain. Also
QBox benchmark has a high scalability but at some point the time converges to
some constant value as shown in Fig. 4. So the prediction of scalability to higher
number of nodes made by extrapolating the data for considered QBox bench-
mark is the constant eﬀect of optimization and constant run time of benchmark.
The considered QBox benchmark has a good scalability up to 100 nodes. On
larger task scalability is better (benchmarks in paper [5]) and it is possible to
get better results.
5
Conclusions
The proposed approach improves the performance if task graph of MPI applica-
tion has non-uniform edges weighting and subgraphs with tight communications
and the number of nodes close to the number of cores per cluster node. Positive
eﬀect is obtained on a large amount of nodes for QBox benchmark because it
has non-uniform collective communications between processes and high scalabil-
ity. The task mapping method can be improved taking into account the cluster
topology and latency and bandwidth characteristics of interconnect.
6
Product and Performance Information
Software and workloads used in performance tests may have been optimized for
performance only on Intel R⃝microprocessors. Performance tests, such as SYS-
mark and MobileMark, are measured using speciﬁc computer systems, compo-
nents, software, operations, and functions. Any change to any of those factors
may cause the results to vary. You should consult other information and per-
formance tests to assist you in fully evaluating your contemplated purchases,
including the performance of that product when combined with other products.
Conﬁguration. Hardware: 50–400 nodes: 2 x Intel R⃝XeonTM X5680 CPUs,
Intel R⃝Xeon PhiTM SE10X, 2 Gb DDR3-1333 RAM per node, InﬁniBandTM
QDR, Software: Linux CentOS 6.2, Software: Intel MPI 4.1.3.045, CORAL QBox
1.40b. (default parameters; built with Intel C++ Compiler XE 14.0.1 for Linux).
Optimization Notice. Intel’s compilers may or may not optimize to the same
degree for non-Intel microprocessors for optimizations that are not unique to
Intel microprocessors. These optimizations include SSE2, SSE3, and SSSE3
instruction sets and other optimizations. Intel does not guarantee the availability,
functionality, or eﬀectiveness of any optimization on microprocessors not man-
ufactured by Intel. Microprocessor-dependent optimizations in this product are
intended for use with Intel microprocessors. Certain optimizations not speciﬁc to
Intel microarchitecture are reserved for Intel microprocessors. Please refer to the
applicable product User and Reference Guides for more information regarding
the speciﬁc instruction sets covered by this notice. Notice revision #20110804.

Optimizing Processes Mapping for Tasks
239
References
1. Karlsson, C., Davies, T., Chen, Z.: Optimizing process-to-core mappings for appli-
cation level multi-dimensional MPI communications. In: IEEE International Con-
ference on Cluster Computing (CLUSTER 2012), pp. 486–494, 24–28 September
2012
2. Zhang, J., Zhai, J., Chen, W., Zheng, W.: Process mapping for MPI collective
communications. In: Sips, H., Epema, D., Lin, H.-X. (eds.) Euro-Par 2009. LNCS,
vol. 5704, pp. 81–92. Springer, Heidelberg (2009)
3. Chen, H., Chen, W., Huang, J., Robert, B., Kuhn, H.: MPIPP: an automatic
proﬁle-guided parallel process placement toolset for SMP clusters and multiclus-
ters. In: Proceedings of the 20th Annual International Conference on Supercom-
puting, ICS 2006, pp. 353–360, June 2006
4. Intel R
⃝Library Reference Manual (2014)
5. Gygi, F., Yates, R.K., Lorenz, J., Draeger, E.W., Franchetti, F., Ueberhuber, C.,
Supinski, B., Gunnels, S., Sexton, J.: Large-scale ﬁrst-principles molecular dynam-
ics simulations on the BlueGene platform using the Qbox code. In: Proceedings of
the ACM/IEEE SC 2005, p. 24, 12–18 November 2005
6. Tornado
SUSU
Supercomputer
(2014).
http://supercomputer.susu.ac.ru/en/
computers/tornado/
7. Sanyal, S., Jain, A., Das, S.K., Biswas, R.: A hierarchical and distributed approach
for mapping large applications to heterogeneous grids using genetic algorithms. In:
CLUSTER, pp. 496–499. IEEE Computer Society (2003)
8. Kaﬁl, M., Ahmad, I.: Optimal task assignment in heterogeneous distributed com-
puting systems. IEEE Concurrency 6(3), 42–50 (1998)
9. Martin, R., Vahdat, A., Culler, D., Anderson, T.: Eﬀects of communication latency,
overhead, and bandwidth in a cluster architecture. In: Proceedings of the 24th
Annual International Symposium on Computer Architecture (ISCA), pp. 85–97,
June 1997. Graph with spatial structure like mesh
10. Bhatele, A., Kale, L.V.: Heuristic-based techniques for mapping irregular commu-
nication graphs to mesh topologies. In: 2011 IEEE 13th International Conference
on High Performance Computing and Communications (HPCC), pp. 765–771, 2–4
September 2011
11. Eshaghian, M.M.: Mapping arbitrary heterogeneous task graphs onto arbitrary
heterogeneous system graph. Int. J. Found. Comput. Sci. 12(05), 599–628 (2001)

Dynamically Adaptable I/O Semantics for High
Performance Computing
Michael Kuhn(B)
University of Hamburg, Hamburg, Germany
michael.kuhn@informatik.uni-hamburg.de
Abstract. While an input/output (I/O) interface’s syntax describes
the available operations, its semantics determines how these operations
behave and which assumptions developers can make about them. There
are several diﬀerent interface standards in existence, some of them dating
back decades and having been designed for local ﬁle systems; one such
representative is POSIX. Many parallel distributed ﬁle systems imple-
ment a POSIX-compliant interface to improve portability. All currently
available interfaces follow a ﬁxed approach regarding semantics, mak-
ing them only suitable for a subset of use cases and workloads. While
the interfaces do not allow application developers to inﬂuence the I/O
semantics, applications could beneﬁt greatly from the possibility of being
able to adapt them to their requirements.
The work presented in this paper includes the design of a novel
I/O interface and a ﬁle system called JULEA. They oﬀer support for
dynamically adaptable semantics and are suited speciﬁcally for HPC
applications. The introduced concept allows applications to adapt the
ﬁle system behavior to their exact I/O requirements instead of the other
way around. The general goal is an interface that allows developers to
specify what operations should do and how they should behave – leaving
the actual realization and possible optimizations to the underlying ﬁle
system.
JULEA has been evaluated using both synthetic benchmarks and
real-world applications. Overall, JULEA provides data and metadata
performance comparable to that of other established parallel distributed
ﬁle systems. However, in contrast to the existing solutions, its ﬂexible
semantics allows it to cover a wider range of use cases in an eﬃcient way.
The results demonstrate that there is need for I/O interfaces that can
adapt to the requirements of applications. Even though POSIX facili-
tates portability, it does not seem to be suited for contemporary HPC
demands.
Keywords: I/O semantics · I/O interface · Parallel ﬁle system
1
Introduction
Throughout their history, the computational power of supercomputers has been
increasing exponentially, doubling roughly every 14 months [20]. While this com-
putational power has allowed more accurate simulations to be performed, this
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 240–256, 2015.
DOI: 10.1007/978-3-319-20119-1 18

Dynamically Adaptable I/O Semantics for High Performance Computing
241
has also caused the simulation results to grow in size. Due to the large amounts
of data produced by parallel applications, high performance I/O is an important
aspect because storing and retrieving such large amounts of data can greatly
aﬀect the overall performance of these applications.
However, the I/O requirements of parallel applications can vary widely: While
some applications process large amounts of input data and produce relatively
small results, others might work using a small set of input data and output large
amounts of data; additionally, the aforementioned data can be spread across
many small ﬁles or be concentrated into few large ﬁles. Naturally, any combina-
tion thereof is also possible. These diﬀerent requirements can make high demands
on supercomputers’ storage systems.
Parallel distributed ﬁle systems provide one or more I/O interfaces that can
be used to access data within the ﬁle system. Usually at least one of them is
standardized, while additional proprietary interfaces might oﬀer improved per-
formance at the cost of portability. Popular interface choices include POSIX [9],
MPI-IO [16], HDF [19] and NetCDF [18]. Almost all the I/O interfaces found in
HPC today oﬀer simple byte- or element-oriented access to data and thus do not
have any a priori information about what kind of data the applications access
and how the high-level access patterns look like. However, this information can
be very beneﬁcial for optimizing the performance of I/O operations.
While the I/O interface deﬁnes which I/O operations are available, the I/O
semantics describes and deﬁnes the behavior of these operations. Usually each
I/O interface is accompanied by a set of I/O semantics, tailored to this spe-
ciﬁc interface. The POSIX I/O semantics is probably both the oldest and the
most widely used semantics, even in HPC. However, due to being designed for
traditional local ﬁle systems, it imposes unnecessary restrictions on today’s par-
allel distributed ﬁle systems. POSIX’s very strict consistency requirements that
require write operations to be propagated to all other clients immediately are one
of these restrictions and can lead to performance bottlenecks in distributed envi-
ronments [21]. Parallel distributed ﬁle systems often implement the strict POSIX
I/O semantics to accommodate applications that require it or simply expect it
to be available for portability reasons. However, this can lead to suboptimal
behavior for many use cases because its strictness is often not necessary. Even
though application developers usually know their applications’ requirements and
could easily specify them for improved performance, current I/O interfaces and
ﬁle systems do not provide appropriate facilities for this task.
Performing I/O eﬃciently is becoming an increasingly important problem.
CPU speed and HDD capacity have roughly increased by factors of 500 and
100 every 10 years, respectively [20,24]. The speed of HDDs, however, has only
grown by a factor of 10 every 10 years [23]; even newer technologies such as SSDs
only oﬀer a factor of 18. Although it is theoretically possible to compensate
for this fact in the short term by simply buying more storage hardware, the
ever increasing gap between the exponentially growing processing power on the
one hand and the stagnating storage capacity and throughput on the other
hand, requires new approaches to use the storage infrastructure as eﬃciently as
possible.

242
M. Kuhn
The goal of this paper is to explore the usefulness of additional semantical
information in the I/O interface. The JULEA framework introduces a newly
designed I/O interface featuring dynamically adaptable semantics that is suited
speciﬁcally for HPC applications. It allows application developers to specify the
semantics of I/O operations at runtime and supports batch operations to increase
performance. The overall goal is to allow the application developer to specify the
desired behavior and leave the actual realization to the I/O system.
This paper is structured as follows: The most important design aspects of
the JULEA I/O interface are elaborated in Sect. 2, focusing on the diﬀerences to
traditional I/O interfaces and ﬁle systems. Section 3 covers related work and com-
pares JULEA’s design with existing approaches. Section 4 contains an analysis of
the behavior of diﬀerent ﬁle systems using synthetic benchmarks. A conclusion
and future work are given in Sect. 5.
2
Interface and File System Design
JULEA’s general architecture closely follows that of established parallel distrib-
uted ﬁle systems such as Lustre [5] and OrangeFS [4]. Machines can have one
or several of three diﬀerent roles: client, data server or metadata server. While
it is possible to have a machine perform all three roles simultaneously, it is
recommended to separate the clients from the servers to provide stable perfor-
mance. JULEA supports multiple data and metadata servers and allows data
and metadata to be distributed among them; it is possible to inﬂuence the actual
distribution of data using distributions.
Metadata
Server
Data
Server
Client
Server Process
Server Process
Application
JULEA
Fig. 1. JULEA’s ﬁle system components
A very brief general view of JULEA’s diﬀerent components and their interac-
tions with each other are shown in Fig. 1. Applications are able to use JULEA’s
I/O interface that talks directly to the data and metadata servers by linking
against its client library libjulea.so; it abstracts all the internal details and
provides a convenient interface for developers. The metadata and data servers
run on dedicated machines with attached storage hardware. While the metadata
servers make use of the MongoDB database system, the data servers run a user
space daemon called julea-daemon that handles all I/O on behalf of the clients.
Figures 2a and b show a comparison of an exemplary HPC I/O stack and
the proposed JULEA I/O stack. In addition to the logical layers, the separation

Dynamically Adaptable I/O Semantics for High Performance Computing
243
(a) HPC I/O stack
(b) JULEA I/O stack
Fig. 2. Current HPC I/O stack and proposed JULEA I/O stack
between kernel and user space is shown. All kernel space layers are either imple-
mented directly inside the kernel or as kernel modules; the user space layers are
either normal applications or libraries. As can be seen, JULEA’s architecture
features less layers, which makes it easier to analyze the actual I/O behavior of
applications. It also allows concentrating all optimizations into a single layer,
reducing the implementation and runtime overhead.
The current I/O stack’s design results in several transformations of the data
as it is being transported through the diﬀerent layers. The parallel application’s
data types are stored in NetCDF that in turn stores its data in HDF’s datasets
and groups. This data is then transformed into a byte stream for MPI-IO. It
then stores the data in the actual parallel distributed ﬁle system that splits up
the data and stripes it across its servers, potentially storing it in yet another
underlying local ﬁle system.
An important design goal of JULEA is to remove the duplication of func-
tionality found in the traditional HPC I/O stack. Because many distributed ﬁle
systems use an underlying local POSIX ﬁle system to store the actual data and
metadata, a lot of common ﬁle system functionality such as path lookup and per-
mission checking is duplicated. This can be achieved by completely eliminating
the underlying POSIX ﬁle systems and using suitable object stores.
Because it is often unreasonable to port applications to new and experimental
I/O interfaces due to their size and complexity, it makes sense to leverage a layer
providing compatibility for existing applications. ADIOS is an established I/O
interface and speciﬁcally allows implementing diﬀerent backends. To minimize
the overhead, ADIOS could be used as a relatively thin layer on top of JULEA
to provide convenient access for application developers.
2.1
File System Namespace
Traditional ﬁle systems allow deeply nested directory structures. To avoid the over-
head caused by this, only a restricted and relatively ﬂat hierarchical namespace is

244
M. Kuhn
supported. While this approach might be unsuited for a general purpose ﬁle sys-
tem, JULEA is explicitly focused on speciﬁc use cases that are commonly found
in HPC. Therefore, JULEA is meant to be used in conjunction with traditional
ﬁle systems like NFS to provide other parts of the infrastructure such as the users’
home directories.
The ﬁle system namespace is divided into stores, collections, and items. Each
store can contain multiple collections that can, in turn, contain multiple items. In
traditional POSIX ﬁle systems, each component of the potentially deeply nested
path has to be checked for each access. This can seriously hamper performance,
especially for distributed ﬁle systems.
2.2
Interface
JULEA’s interface has been designed from scratch to oﬀer simplicity of use while
still meeting the requirements of high performance and dynamically adaptable
semantics. Its functionality can be subdivided into ﬁve groups:
1. Batches: Multiple operations can be batched explicitly to improve perfor-
mance by reducing network overhead.
2. Distributions: It is possible to inﬂuence the distribution of data directly to
optimize its placement on the data servers if necessary.
3. Namespace: The ﬁle system namespace is accessible using a convenient
abstraction called uniform resource identiﬁers (URIs).
4. Semantics: JULEA’s semantics is dynamically adaptable according to the
applications’ I/O requirements.
5. Stores, Collections and Items: It is possible to create, remove, open and
iterate over all of JULEA’s ﬁle system objects.
All of the above functionality is available publicly and directly to developers. The
two most important features are the ability to specify semantical information and
to batch operations. Both approaches give the ﬁle system additional information
that can be used to optimize accesses. Due to their importance, these two features
will be explained in more detail; more information about the other ones can be
found in [13].
It is possible for developers and users to specify additional information equiv-
alent to the coarse-grained statement “this is a checkpoint” or the more ﬁne-
grained “this operation requires strict consistency semantics”. This allows the ﬁle
system to tune operations for speciﬁc applications by itself. Additionally, devel-
opers are able to emulate well-established semantics as well as mixing diﬀerent
semantics within one application.
Developers perform all accesses to the ﬁle systems via so-called batches. Each
batch can consist of multiple operations. It is also possible to combine diﬀerent
kinds of operations within one batch. For instance, one batch might create a col-
lection and several items within it, and write data to each of the items. Because
the ﬁle system has knowledge about all operations within one batch, more elab-
orate optimizations can be performed.

Dynamically Adaptable I/O Semantics for High Performance Computing
245
Traditional POSIX ﬁle systems can also try to aggregate multiple operations
to improve network utilization. However, this can only be done by caching these
operations in the client’s main memory for a given amount of time and then
performing these optimizations. Because the POSIX interface does not provide
enough information to make reliable decisions for these kinds of optimizations,
it is necessary to employ heuristics, resulting in suboptimal behavior for border-
line cases. Additionally, it is not possible to do this in all cases because it would
violate the POSIX semantics. Therefore, users can never be sure when exactly
operations are performed in such a system without calling synchronization func-
tions explicitly, which can be very expensive.1
To be able to easily overlap calculations and I/O, it is possible to execute
batches asynchronously. This support is oﬀered natively by the I/O interface
without forcing developers to resort to using background threads or similar
techniques. The ﬁle system also exports additional information to enable perfor-
mance optimizations such as aligning data to the ﬁle system’s stripe size, which
is crucial for high performance [2].
2.3
Semantics
JULEA allows many aspects of the ﬁle system operations’ semantics to be
changed at runtime. Several key areas of the semantics have been identiﬁed
as important to provide opportunities for optimizations: atomicity, concurrency,
consistency, ordering, persistency and safety. Even though it is possible to mix
the settings for each of these semantics, not all combinations produce reasonable
results.
The atomicity semantics can be used to specify whether accesses should be
executed atomically, that is, whether or not it is possible for clients to see inter-
mediate states of operations. If atomicity is required, some kind of locking has
to be performed to prevent other clients from accessing data that is currently
being modiﬁed. The atomicity semantics is clearly performance-related. Atomic
accesses operating on the same data have to be serialized, which implies a per-
formance penalty. If atomicity is not required, all operations can be executed in
parallel.
The concurrency semantics can be used to specify whether concurrent accesses
will take place and, if so, how the access pattern will look like. Depending on
the level of concurrency, diﬀerent algorithms might be appropriate for ﬁle sys-
tem operations such as locking or metadata access. Concurrency semantics are
performance-related by allowing simpler and faster centralized algorithms to be
used when no concurrent access is happening. For instance, atomicity is only
required for overlapping accesses.
The consistency semantics can be used to specify if and when clients will
see modiﬁcations performed by other clients and applies to both metadata and
data. This information can be used to enable client-side read caching whenever
1 POSIX’s synchronization functions fsync and fdatasync only allow synchronizing
whole ﬁles even if this is not necessary.

246
M. Kuhn
possible. The consistency semantics is performance-related and can allow caching
data and metadata locally.
The ordering semantics can be used to specify whether operations within a
batch are allowed to be reordered. Because batches can potentially contain a
large number of operations, the additional information can be exploited to opti-
mize their execution. The ordering semantics is performance-related as it allows
operations to be reordered for more eﬃcient access. It is especially important to
group operations of the same type to reduce the amount of network overhead.
The persistency semantics can be used to specify if and when data and
metadata must be written to persistent storage. This can be used to enable client-
side write caching whenever possible. The persistency semantics is performance-
related and allows caching modiﬁed data and metadata locally. This can be
especially advantageous when diﬀerent levels of storage such as node-local SSDs
are available as it allows writing the temporary data to the fast local storage
without communicating via the network at all.
The safety semantics can be used to specify how safely data and meta-
data should be handled. It provides guarantees about the state of the data and
metadata after the execution of a batch has ﬁnished. The safety semantics is
performance-related by allowing to adjust the overhead incurred by data safety
measures and to optimize network utilization by not waiting for unnecessary
replies.
3
Related Work
The current HPC I/O stack has already been identiﬁed as problematic regard-
ing future demands due to its complex layering and static architecture [3]. Even
though there are a few approaches to provide conﬁgurable behavior and seman-
tics in parallel distributed ﬁle systems, they are usually limited to single aspects
of the ﬁle system or too static because they do not allow changes at runtime [17].
JULEA aims to solve these problems using its novel approach.
MosaStore is a versatile storage system that is conﬁgurable at application
deployment time and thus allows application-speciﬁc optimizations [1]. This is
similar to JULEA’s approach. However, MosaStore provides a storage system
bound to speciﬁc applications instead of a globally shared one. Additionally, the
storage system can not be reconﬁgured at runtime and keeps the traditional
POSIX I/O interface.
CAPFS introduces a new content-addressable ﬁle store that allows users to
deﬁne data consistency semantics at runtime [22]. While providing a client-side
plug-in API allows users to implement their own consistency policies, CAPFS is
limited to tuning the consistency of ﬁle data and keeps the traditional POSIX
interface. Additionally, the consistency semantics can only be changed on a per-
ﬁle basis. JULEA covers a wider range of semantics and features a more ﬁne-
grained as well as a more dynamic approach.
Memory ordering and consistency are important factors in parallel program-
ming for shared memory architectures, both for performance and correctness.

Dynamically Adaptable I/O Semantics for High Performance Computing
247
CPUs usually reorder memory load and store operations to improve perfor-
mance [7,8]. Modern concepts such as those supported by C++11 and C11 allow
developers to specify diﬀerent constraints to achieve optimal performance while
still maintaining correct execution of their applications [10]. JULEA’s order-
ing semantics provide the same beneﬁts by allowing the developer to provide
additional semantical information to optimize execution.
ADIOS oﬀers a novel and developer-friendly I/O interface that allows speci-
fying the I/O conﬁguration in an XML ﬁle that can be changed without recom-
piling the application [11,15]. Version 1.4 of ADIOS has added support for
scheduling read operations. Several read operations can be scheduled using the
adios schedule read function and then executed using the adios perform
reads function. Read scheduling is very similar to JULEA’s batches as it allows
aggregating multiple operations for improved performance. However, JULEA’s
batches can contain arbitrary operations, making them more versatile.
4
Performance Evaluation
Benchmarks will be used to evaluate diﬀerent performance aspects of JULEA
and Lustre, which strives to support POSIX semantics. In addition to comparing
JULEA to the other parallel distributed ﬁle system, a number of diﬀerent seman-
tics will be evaluated. However, due to the sheer amount of diﬀerent semantics
combinations, only those expected to have a signiﬁcant impact on performance
will be analyzed in more detail. JULEA’s data performance will be evaluated
using diﬀerent atomicity, concurrency and safety semantics. A prior comparison
of the metadata performance has been published in [12].
All evaluations have been conducted on the cluster of the Scientiﬁc Comput-
ing research group at the University of Hamburg. The benchmarks have been
performed using a total of 20 nodes, with 10 nodes running the ﬁle system clients
and 10 nodes hosting the ﬁle system servers. The nodes’ hardware and software
setup is as follows:
The client nodes each have two Intel Xeon Westmere EP HC X5650 CPUs
(2.66 GHz, 12 cores total), 12 GB DDR3/PC1333 ECC RAM, a 250 GB SATA2
Seagate Barracuda 7200.12 HDD and two Intel 82574L Gbit Ethernet NICs.
They run Ubuntu 12.04.3 LTS with Linux 3.8.0-33-generic and Lustre 2.5.0
(client); the MPI implementation is provided by OpenMPI 1.6.5.
The server nodes each have one Intel Xeon Sandy Bridge E-1275 CPUs
(3.4 GHz, 4 cores total), 16 GB DDR3/PC1333 ECC RAM, three 2 TB SATA2
Western Digital WD20EARS HDDs, one 160 GB SATA2 Intel 320 SSD and
two Intel 82579LM/82574L Gbit Ethernet NICs. They run CentOS 6.5 with
Linux 2.6.32-358.18.1.el6 lustre.x86 64 and Lustre 2.5.0 (server).
To allow a proper assessment of the results, the following theoretical perfor-
mance considerations should be kept in mind: The theoretical maximum per-
formance of Gbit Ethernet is 125 MB/s. However, it is usually not possible to
reach more than 117 MB/s due to overhead. Consequently, the maximum achiev-
able performance between the clients and servers is approximately 1,170 MB/s.

248
M. Kuhn
The average round-trip time between the client and server nodes is 0.228 ms.
Ignoring actual processing times, it is therefore possible to send and receive
4,386 requests/s.
The ﬁle systems’ data performance will be evaluated using a large number of
concurrently accessing clients that ﬁrst write data and then read it back again;
the write and read phases are completely separated and barriers ensure that
only one type of operation takes place at any given time. To force the clients
to read the data from the data servers during the read phase, the clients’ cache
was dropped after the write phase. The benchmark uses MPI to start multiple
processes accessing the ﬁle systems in a coordinated fashion. There are two
basic modes of operation: Individual ﬁles (each process accesses its own ﬁle,
the individual ﬁles are accessed serially) and shared ﬁle (all processes access
a single shared ﬁle concurrently in an interleaved fashion). All accesses use a
variable block size and are non-overlapping, that is, no write conﬂicts occur.
The ﬁle systems have been set up to provide ten data servers and one metadata
server. Each benchmark has been repeated at least three times to calculate the
arithmetic mean as well as the standard deviation.
4.1
Lustre
Lustre has been set up using its default options except for the stripe count that
has been set to -1 to enable striping over all available object storage targets
(OSTs); the stripe size has been set to 1 MiB. While each OST has been provided
by one of the servers’ HDDs, the meta data target (MDT) has been provided by
one of the SSDs. Lustre has been mounted using the client module as a normal
POSIX ﬁle system with the flock option that enables support for ﬁle locking.
The option should not have any inﬂuence on the benchmark results because they
do not use ﬁle locking.
Fig. 3. Lustre: individual ﬁles via POSIX
Individual Files. Figure 3 shows Lustre’s read and write performance when
using individual ﬁles via the POSIX interface. Regarding read performance, it
is interesting to note that conﬁgurations with a single node exhibit diﬀerent

Dynamically Adaptable I/O Semantics for High Performance Computing
249
performance characteristics depending on the number of processes. While the
conﬁgurations with one, eight and twelve processes all achieve a throughput
of roughly 100 MiB/s, the conﬁgurations with two and four processes deliver
200–300 MiB/s; while this eﬀect has to be related to some data being read from
the cache of the operating system, the exact reasons for this are unclear. As
explained earlier, the benchmark drops all caches between the read and write
phases, therefore, this eﬀect should not occur. The remaining conﬁgurations
gradually deliver more performance as more nodes are added until reaching their
maximum performance with ten nodes. As expected, smaller block sizes result
in lower read performance due to additional overhead. However, it is interesting
to note that even with a single process and a block size of 4 KiB, Lustre achieves
a read performance of roughly 100 MiB/s. As mentioned previously, the Gbit
Ethernet network can transfer at most 4,386 requests/s. Taking this into account,
Lustre should only be able to read at a maximum of 17 MiB/s. This discrepancy is
due to Lustre performing client-side readahead to increase performance. When
considering write performance, it can be seen that all block sizes deliver the
same performance. This is most probably due to Lustre’s use of client-side write
caching. Because individual ﬁles are used and each ﬁle is only accessed by one
node, Lustre can utilize caching without sacriﬁcing POSIX compliance.
Fig. 4. Lustre: shared ﬁle via POSIX
Shared File. Figure 4 shows Lustre’s read and write performance when using a
single shared ﬁle via the POSIX interface. The read performance for the conﬁg-
urations using one node behaves in a similar way to the test case with individual
ﬁles. For small block sizes, not all results could be collected because Lustre’s
performance was too low and the jobs exceeded the job scheduler’s time limit.
For 256 KiB and 1,024 KiB, the performance increases until six and seven nodes,
respectively, and afterwards drops with each additional node. This result is sur-
prising because only read operations are performed by all accessing clients, that
is, no locking should be required. However, it appears that Lustre still introduces
some overhead for these accesses, decreasing overall performance signiﬁcantly.
For the write phase, an interesting eﬀect occurs: While using only a single node,
performance is stable for all block sizes. When using more than one accessing

250
M. Kuhn
nodes, performance drops for all block sizes less than 1,024 KiB. As soon as
multiple nodes are involved, Lustre has to send all write operations directly
to the data server to achieve POSIX compliance. An additional factor for the
low performance could be write locking that needs to be performed due to the
concurrently accessing clients.
4.2
JULEA
JULEA has been conﬁgured to use a POSIX storage backend on the data servers’
system HDDs. Additionally, JULEA was set to use a maximum of six client
connections per node because it was observed that the default of twelve caused
severe performance problems due to the large amount of TCP connections.
Fig. 5. JULEA: individual items
Default Semantics. Figure 5 shows JULEA’s read and write performance
when using individual items via the native JULEA interface. Regarding read
performance, performance almost scales linearly until seven to eight nodes are
used. Afterwards, the speedup slows down, reaching a maximum of more than
900 MiB/s using a block size of 1,024 KiB. As expected, smaller block sizes pro-
vide a lower overall performance with the exception of 16 KiB and 64 KiB that
are reversed. Regarding write performance, the same eﬀects as in the read case
can be observed. Even though the performance does not increase with more than
seven clients, it remains at a stable level.
Figure 6 shows JULEA’s read and write performance when using a shared
item via the native JULEA interface. During the read phase, the performance
curve looks almost identical to its counterpart using individual items. While the
performance speedup slowed slightly when going from nine to ten nodes using
individual items, the shared item case is not aﬀected by this drop. Additionally,
the block size of 16 KiB provides a more stable performance curve. During the
write phase, the performance curve looks less smooth than when using individual
items. For instance, using the largest block size of 1,024 KiB, performance drops
when increasing the number of nodes from ﬁve to six, only to rise again when
using seven nodes. The fact that overall performance is lower than when using

Dynamically Adaptable I/O Semantics for High Performance Computing
251
Fig. 6. JULEA: shared item
individual items indicates that the handling of shared ﬁles is suboptimal in the
Linux kernel. Additional measurements using OrangeFS, diﬀerent underlying
ﬁle systems and JULEA’s NULL storage backend have shown that these perfor-
mance inconsistencies are not speciﬁc to JULEA, independent of the underlying
ﬁle system and only occur if the ﬁle system is actually accessed using shared
ﬁles.
To reduce the number of results and exclude the inﬂuences of the performance
inconsistencies when using a single shared ﬁle, the following measurements have
only been performed using individual items.
Fig. 7. JULEA: individual items using unsafe safety semantics
Safety Semantics. The following measurements have used the safety semantics
to disable write acknowledgments for all write operations.
Figure 7 shows JULEA’s read and write performance when using individual
items via the native JULEA interface. During the read phase, there are only
minor diﬀerences in performance in comparison to the default semantics. This is
to be expected because the read operations are not handled diﬀerently depending
on the safety semantics. During the write phase, performance is improved across
the board for all block sizes. It is especially interesting to note that even a single
process achieves the maximum performance of 110 MiB/s using a block size of

252
M. Kuhn
4 KiB because the clients do not have to wait for the write acknowledgments
from the data servers. Using a block size of 4 KiB, the maximum performance
is increased by 33 % when using ten nodes. The largest block size of 1,024 KiB
manages to achieve a maximum performance of approximately 800 MiB/s, an
improvement of 23 % when compared to the default semantics.
Fig. 8. JULEA: individual items using per-operation atomicity semantics
Atomicity Semantics. The following measurements have used the atomicity
semantics to enforce atomic access for each read and write operation. JULEA
currently implements atomicity using a centralized per-block locking algorithm.
Figure 8 shows JULEA’s read and write performance when using individual
items via the native JULEA interface. Regarding read performance, it is interest-
ing to note that diﬀerent block sizes show diﬀerent scaling behavior: While the
block sizes of 4 KiB and 16 KiB quickly reach a maximum and stay at this level,
the remaining block sizes deliver more performance as more nodes are used.
This behavior can be explained using a rough performance estimation: Mon-
goDB manages to deliver roughly 20,000 inserts/s and 6,000 removes/s. Taking
into account that each read or write operation requires one insert and one remove
operation, a maximum of 13,000 operations/s can be performed.2 This implies
a maximum performance of roughly 50 MiB/s for a block size of 4 KiB and
200 MiB/s for a block size of 16 KiB. According to the measurements, 42 MiB/s
and 170 MiB/s are reached for block sizes of 4 KiB and 16 KiB, respectively.
Because a block size of 64 KiB can already support up to 800 MiB/s according to
this approximation, the remaining block sizes’ performance scales with the num-
ber of nodes. Interestingly, the largest block size of 1,024 KiB almost reaches the
same performance as when using the default semantics. For smaller block sizes,
the slowdown is more severe, however, resulting in a decrease of almost 30 %.
Regarding write performance, the small block sizes manage to deliver almost
the same performance as during the read phase. While the block size of 4 KiB
reaches a maximum of 40 MiB/s, the block size of 16 KiB is limited to 140 MiB/s.
The remaining block sizes perform much worse, however. This is due to the lower
write performance that is already present when using the default semantics.
2 This number is only intended to provide a rough estimate. In practice, the number
might be lower due to the high discrepancy between insert and remove performance.

Dynamically Adaptable I/O Semantics for High Performance Computing
253
4.3
Discussion
The results demonstrate that the current state of parallel distributed ﬁle systems
is mixed and that performance can be very hard to predict and understand.
Even simple access patterns as the ones used for the presented benchmarks do
not achieve the maximum performance. This is true for all tested ﬁle systems
but has diﬀerent reasons for each of them.
Lustre deals well with a large number of concurrent clients. This is most likely
because Lustre can easily use the operating system’s ﬁle system cache due to
being implemented in kernel space. This allows Lustre to aggregate accesses and
thus reduce the load on the servers. However, Lustre’s performance is abysmal
when accessing a single shared ﬁle as commonly done in scientiﬁc applications:
Read performance decreases with more than seven client nodes and write per-
formance does not scale beyond one client node. Consequently, only individual
ﬁles are eﬃciently usable because it is not possible to inform Lustre about the
application’s I/O requirements to mitigate these performance problems.
JULEA’s overall performance is held back by problems found within the
underlying operating system and ﬁle systems. However, its dynamically adapt-
able semantics allow it to cater to a wide range of I/O requirements:
– Its default semantics enable performance results similar to those of Lustre
when using large block sizes. Lustre has advantages for small block sizes due
to its client-side caching and readahead functionalities. However, these advan-
tages vanish as soon as shared ﬁles are used.
– The safety semantics can be used to reduce the network overhead by not
awaiting the data servers’ replies. This is similar to Lustre’s default behavior
when using individual ﬁles.
– Atomic operations can be achieved by using the atomicity semantics. While the
performance of large read operations is not reduced signiﬁcantly, write opera-
tions suﬀer a performance penalty of up to 40 %. However, using JULEA’s ﬁne-
grained semantics, it is possible to use atomic operations only when absolutely
necessary.
In contrast to Lustre, JULEA can be adapted to diﬀerent applications by set-
ting its semantics appropriately. While it is neither possible to improve Lustre’s
shared ﬁle performance due to its POSIX compliance nor to use other ﬁle sys-
tems such as OrangeFS for workloads requiring overlapping writes, it is possible
for JULEA to support and to be tuned for these speciﬁc use cases.
5
Conclusion and Future Work
This paper presents a new approach for handling application-speciﬁc I/O require-
ments in HPC. The JULEA framework includes a prototypical implementation
of a parallel distributed ﬁle system and provides a novel I/O interface featur-
ing dynamically adaptable semantics. It allows applications to specify their I/O
requirements using a ﬁne-grained set of semantics. Additionally, batches enable
the eﬃcient execution of ﬁle system operations.

254
M. Kuhn
The results obtained in this paper demonstrate that there is need for I/O
interfaces that can adapt to the requirements of applications in order to pro-
vide adequate performance for a variety of diﬀerent use cases. The current cir-
cumstances eﬀectively force application developers to adapt their applications to
work around limitations found in speciﬁc ﬁle systems in order to achieve the best
possible performance. An indication for this is the wide variety of I/O libraries,
such as SIONlib [6], that deal with particular ﬁle system constraints. This can
signiﬁcantly increase the development and maintenance overhead because appli-
cations have to be optimized for diﬀerent ﬁle systems’ semantics instead of being
able to optimize the ﬁle systems according to their I/O requirements.
The concept introduced by the JULEA framework ﬁlls the gap by allowing
applications to adapt the ﬁle system to their exact I/O requirements instead
of the other way around. The available results show that the supplementary
semantical information can be used to adapt the ﬁle system’s behavior in such
a way as to optimize performance for speciﬁc use cases. Additional results and
more in-depth information about JULEA are available in [13].
Even though JULEA provides a convenient testbed to experiment with diﬀer-
ent semantics and prototype new functionality, it is necessary to provide dynam-
ically adaptable semantics for established I/O interfaces and parallel distributed
ﬁle systems for widespread adoption of these new features. These interfaces have
to be standardized and supported by a suﬃciently large subset of ﬁle systems to
provide consistent functionality across diﬀerent implementations.
First of all, it is necessary to agree on default semantics suited for modern
HPC applications and a common set of parameters that should be conﬁgurable.
The semantics presented in this paper are meant to provide a good starting point
for further evaluation.
5.1
Future Work
As mentioned previously, it is often unreasonable to port applications to new
I/O interfaces due to their size and complexity. Thus, to avoid having to rewrite
applications to be able to make use of JULEA’s novel features, some form of
compatibility would be preferable. Because many applications already use high-
level I/O libraries such as ADIOS or NetCDF, JULEA could be integrated into
applications by providing backends for these I/O libraries. As ADIOS’s API
design is relatively close to JULEA, a thin backend would be suﬃcient to enable
all ADIOS-aware applications to use JULEA without any further modiﬁcations.
ADIOS makes use of XML-based conﬁguration ﬁles to specify the applications’
I/O, which could be easily extended to add more semantical information about
the actual data, similar to what has been done in [14].
References
1. Al-Kiswany, S., Gharaibeh, A., Ripeanu, M.: The case for a versatile storage sys-
tem. SIGOPS Oper. Syst. Rev. 44(1), 10–14 (2010)

Dynamically Adaptable I/O Semantics for High Performance Computing
255
2. Bartz, C.: An in-depth analysis of parallel high level I/O interfaces using HDF5
and NetCDF-4. Master’s thesis, University of Hamburg, April 2014
3. Brinkmann, A., Cortes, T., Falter, H., Kunkel, J., Narasimhamurthy, S.: E10
– Exascale IO, May 2014. http://www.eiow.org/home/E10-Architecture.pdf?
attredirects=0&d=1. Accessed: April 2015
4. Carns, P.H., Ligon III, W.B., Ross, R.B., Thakur, R.: PVFS: a parallel ﬁle system
for linux clusters. In: Proceedings of the 4th Annual Linux Showcase and Confer-
ence, pp. 317–327. USENIX Association
5. Cluster
File
Systems
Inc.:
Lustre:
a
scalable,
high-performance
ﬁle
sys-
tem, November 2002. http://www.cse.buﬀalo.edu/faculty/tkosar/cse710/papers/
lustre-whitepaper.pdf. Accessed: November 2014
6. Frings, W., Wolf, F., Petkov, V.: Scalable massively parallel I/O to task-local ﬁles.
In: Proceedings of the Conference on High Performance Computing Networking,
Storage and Analysis, SC 2009. ACM, New York (2009). http://doi.acm.org/10.
1145/1654059.1654077
7. Gharachorloo, K., Gupta, A., Hennessy, J.: Performance evaluation of memory con-
sistency models for shared-memory multiprocessors. In: Proceedings of the Fourth
International Conference on Architectural Support for Programming Languages
and Operating Systems, ASPLOS IV, pp. 245–257. ACM, New York (1991). http://
doi.acm.org/10.1145/106972.106997
8. Gharachorloo, K., Lenoski, D., Laudon, J., Gibbons, P., Gupta, A., Hennessy, J.:
Memory consistency and event ordering in scalable shared-memory multiproces-
sors. In: Proceedings of the 17th Annual International Symposium on Computer
Architecture, ISCA 1990, pp. 15–26. ACM, New York (1990). http://doi.acm.org/
10.1145/325164.325102
9. The IEEE and The Open Group: Standard for Information Technology - Portable
Operating System Interface (POSIX) Base Speciﬁcations, Issue 7. IEEE Std 1003.1,
2013 Edition (incorporates IEEE Std 1003.1-2008, and IEEE Std 1003.1-2008/Cor
1–2013) pp. 1–3906, April 2013
10. ISO/IEC JTC 1/SC 22 - Programming languages, their environments and system
software interfaces: ISO/IEC 9899:2011 - Information technology - Programming
languages - C, December 2011
11. Klasky, S., Liu, Q., Lofstead, J., Podhorszki, N., Abbasi, H., Chang, C., Cummings,
J., Dinakar, D., Docan, C., Ethier, S., Grout, R., Kordenbrock, T., Lin, Z., Ma,
X., Oldﬁeld, R., Parashar, M., Romosan, A., Samatova, N., Schwan, K., Shoshani,
A., Tian, Y., Wolf, M., Yu, W., Zhang, F., Zheng, F.: ADIOS: powering I/O to
extreme scale computing. In: SciDAC 2010 Conference Proceedings, pp. 342–347
(2010). http://computing.ornl.gov/workshops/scidac2010/papers/data q liu.pdf
12. Kuhn, M.: A semantics-aware I/O interface for high performance computing. In:
Kunkel, J.M., Ludwig, T., Meuer, H.W. (eds.) ISC 2013. LNCS, vol. 7905, pp.
408–421. Springer, Heidelberg (2013)
13. Kuhn, M.: Dynamically adaptable I/O semantics for high performance computing.
Ph.D. thesis, University of Hamburg, Germany, November 2014 (to be published)
14. Kunkel, J., Minartz, T., Kuhn, M., Ludwig, T.: Towards an energy-aware scientiﬁc
I/O interface - stretching the ADIOS interface to foster performance analysis and
energy awareness. Comput. Sci. Res. Dev. 27(4), 337–345 (2011)
15. Lofstead, J.F., Klasky, S., Schwan, K., Podhorszki, N., Jin, C.: Flexible IO and
integration for scientiﬁc codes through the adaptable IO system (ADIOS). In:
Proceedings of the 6th International Workshop on Challenges of Large Applications
in Distributed Environments, CLADE 2008, pp. 15–24. ACM, New York (2008)

256
M. Kuhn
16. Message
Passing
Interface
Forum:
MPI:
a
message-passing
interface
stan-
dard. Version 3.0, September 2012. http://www.mpi-forum.org/docs/mpi-3.0/
mpi30-report.pdf. Accessed: November 2014
17. Patil, S., Gibson, G.A., Ganger, G.R., Lopez, J., Polte, M., Tantisiroj, W., Xiao, L.:
In search of an API for scalable ﬁle systems: under the table or above it? In:
Proceedings of the 2009 Conference on Hot Topics in Cloud Computing, HotCloud
2009. USENIX Association, Berkeley (2009)
18. Rew,
R.,
Davis,
G.:
Data
management:
NetCDF:
an
interface
for
sci-
entiﬁc
data
access.
IEEE
Comput.
Graph.
Appl.
10(4),
76–82
(1990).
http://dx.doi.org/10.1109/38.56302
19. The HDF Group: Hierarchical data format version 5, July 2014. http://www.
hdfgroup.org/HDF5. Accessed: November 2014
20. The TOP500 Editors: TOP500, June 2014. http://www.top500.org/. Accessed:
November 2014
21. Vilayannur,
M.,
Lang,
S.,
Ross,
R.,
Klundt,
R.,
Ward,
L.:
Extending
the
POSIX
I/O
interface:
a
parallel
ﬁle
system
perspective.
Technical
report ANL/MCS-TM-302, October 2008. http://www.mcs.anl.gov/uploads/cels/
papers/TM-302-FINAL.pdf
22. Vilayannur, M., Nath, P., Sivasubramaniam, A.: Providing tunable consistency for
a parallel ﬁle store. In: Proceedings of the 4th Conference on USENIX Confer-
ence on File and Storage Technologies, FAST 2005, vol. 4. USENIX Association,
Berkeley (2005)
23. Wikipedia:
Festplattenlaufwerk
-
Geschwindigkeit,
November
2014.
http://
de.wikipedia.org/wiki/Festplattenlaufwerk#Geschwindigkeit. Accessed: November
2014
24. Wikipedia: Mark Kryder - Kryder’s Law, November 2014. http://en.wikipedia.org/
wiki/Mark Kryder#Kryder.27s Law. Accessed: November 2014

Predicting Performance of Non-contiguous
I/O with Machine Learning
Julian Kunkel1(B), Michaela Zimmer2, and Eugen Betke2
1 DKRZ, Hamburg, Germany
kunkel@dkrz.de
2 University of Hamburg, Hamburg, Germany
Abstract. Data sieving in ROMIO promises to optimize individual non-
contiguous I/O. However, making the right choice and parameterizing its
buﬀer size accordingly are non-trivial tasks, since predicting the result-
ing performance is diﬃcult. Since many performance factors are not taken
into account by data sieving, extracting the optimal performance for a
given access pattern and system is often not possible. Additionally, in Lus-
tre, settings such as the stripe size and number of servers are tunable, yet
again, identifying rules for the data-centre proves challenging indeed.
In this paper, we (1) discuss limitations of data sieving, (2) apply
machine learning techniques to build a performance predictor, and (3)
learn and extract best practices for the settings from the data. We used
decision trees as these models can capture non-linear behavior, are easy
to understand and allow for extraction of the rules used. Even though this
initial research is based on decision trees, with sparse training data, the
algorithm can predict many cases suﬃciently. Compared to a standard
setting, the decision trees created are able to improve performance signif-
icantly and we can derive expert knowledge by extracting rules from the
learned tree. Applying the scheme to a set of experimental data improved
the average throughput by 25–50 % of the best parametrization’s gain.
Additionally, we demonstrate the versatility of this approach by applying
it to the porting system of DKRZ’s next generation supercomputer and
discuss achievable performance gains.
Keywords: MPI · Non-contiguous I/O · Parallel I/O
1
Introduction
With MPI 2, an I/O interface has been standardized which promises to improve
performance for parallel applications. Among the supported features, it explic-
itly supports non-contiguous I/O – one API call accesses multiple ﬁle regions,
and, with collective I/O, multiple processes can coordinate their ﬁle accesses. The
standard explicitly allows an implementation to exploit its knowledge about con-
current operations; for example, by scheduling the I/O calls intelligently. Since
We want to express our gratitude to the “Deutsches Zentrum f¨ur Luft- und
Raumfahrt e.V.” as responsible project agency and to the “Bundesministerium f¨ur
Bildung und Forschung” for the ﬁnancial support under grant 01 IH 11008 A-C.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 257–273, 2015.
DOI: 10.1007/978-3-319-20119-1 19

258
J. Kunkel et al.
there are many factors inﬂuencing performance in a supercomputer, extracting
the best performance is anything but trivial. The available optimizations oﬀer a
selection of parameters to be adapted to target machine and speciﬁc workload,
and through a wrong choice, performance may be degraded.
It is very diﬃcult for users to estimate how an I/O pattern will perform
under a given set of optimization parameters; they therefore typically try various
parameters, which resembles a limited brute force approach. While there are
some rules of thumb and expert knowledge, e.g. “data sieving helps for small
data accesses”, they need to be adjusted for each system. DKRZ will install the
ﬁrst phase of its next generation supercomputer HLRE3 this year, providing
more than 2 Petaﬂop/s and 30 Petabyte storage capacity. However, we struggle
in the data center to determine good defaults for certain Lustre parameters such
as number of servers and stripe size, as they are very speciﬁc to system and
application. Even the knowledge of specialists often merely helps to direct the
exploration of the complex parameter space for data sieving and stripe size.
It would be helpful if expert knowledge could be automatically inferred from
observations. To alleviate this, in the long run, our research strives to provide a
tool that will be aware of system capabilities as well as its performance history,
using all to suggest the best parameter set for the task at hand.
Our main contributions are: (1) The evaluation of decision trees to cap-
ture and predict non-contiguous performance behavior. (2) The semi-automatic
extraction of expert knowledge from the measurements.
This paper is structured as follows: Related work regarding I/O research and
machine learning is discussed in Sect. 2. Section 3 presents performance results
of experiments with several relevant parameters that are currently missing in
data sieving. The overall machine learning approach is introduced in Sect. 4.
In Sect. 5, the accuracy of the predictor is investigated and interesting results are
shown. We apply the approach to the porting system for DKRZ’s next generation
system in Sect. 6, to evaluate whether we can extract best practices for this test
system; this would permit use of the strategy on the full system as well. Section 7
concludes the paper and discusses future steps.
2
Related Work
Widely used concepts to improve I/O performance are non-blocking I/O, data
pre-fetching and write-behind. ROMIO [1], a common MPI-IO implementation,
oﬀers collective I/O and data sieving with the promise to speed up performance.
Data sieving optimizes independent sparse non-contiguous I/O; by accessing
larger ﬁle regions and discarding unwanted data, it avoids seeks on hard disk
drives and improves performance, especially for very small accesses. While holes
in the access pattern can just be read and discarded for reads, it is not as easy
for writes because they require reading the whole region, modifying changed
data and writing it back. Traditional ﬁle systems that oﬀer POSIX semantics
require locking to avoid conﬂicts with concurrent writes to an overlapping region.
Ching et al. [2] implemented ListIO for PVFS in MPI which supports access to

Predicting Performance of Non-contiguous I/O with Machine Learning
259
multiple ﬁle regions in one request and, thus, does not need such a read-modify-
write cycle. Over the last years, there has not been much research into further
optimizing non-contiguous I/O.
Optimizing collective I/O has been investigated more deeply. The basic idea
of coordinated I/O is that the processes exchange information about accessed
ﬁle regions; then they compute a schedule assigning responsibility for speciﬁc
ﬁle regions and deﬁning the access order; ﬁnally, data is exchanged amongst
those processes that ultimately perform the I/O. There are many variations
to this basic process: The Two-Phase protocol as discussed by Thakur et al.
[1] iterates over communication and I/O phases – in each phase, a maximum
amount of data is accessed. Multiphase-I/O [3,4] iteratively increases locality,
and Orthrus [5] oﬀers several strategies to optimize either for ﬁle or process
locality. One diﬃculty with these approaches is that they require careful analysis
and tuning of parameters.
Monitoring and analysis of system state and performance data is important
to optimize HPC systems; tools include Vampir [6] and Darshan [7]. While they
help in the analysis, they cannot set parameters.
There are several research projects which try to integrate machine learn-
ing into the analysis and optimization cycle. One of the ﬁrst is the work of
Madhyastha and Reed [8], comparing classiﬁcation of I/O access patterns by
feed-forward neural networks and by hidden Markov models. Higher level appli-
cation I/O patterns are inferred and looked up in a table to determine the ﬁle
system policy to set for the next accesses. The table, however, has to be supplied
by an administrator implementing his heuristics. Magpie, a system by Barham
et al. [9], traces events under Windows, merging them according to pre-deﬁned
schemas specifying event relationships. Their causal chains are reconstructed
and clustered into models for the various types of workload observed. Devia-
tions will point to anomalies deserving human attention. Classifying new traces
according to these models yields insights into their actual and expected per-
formance, leading to various applications such as capacity planning and on-line
latency tuning, as described in [10,11]. Behzad et al. [12] oﬀer a framework that
uses genetic algorithms to auto-tune select parameters of a stack consisting of
HDF5, MPI and Lustre. But its monolithic view of the system disregards the
relations between the layers as well as the users’ individual requirements, setting
optimizations but once per application run.
All of these systems have in common the need for human intervention to
beneﬁt from the results or to apply the solutions to the problems identiﬁed. The
SIOX framework [13,14] aims to implement a holistic approach covering the full
cycle of monitoring, analysis, machine learning of the adequate settings and their
automatic enactment.
3
Limits of Non-contiguous I/O
First, we discuss the handling of non-contiguous I/O with data sieving. From
the user perspective, the bytes to access are deﬁned by the MPI ﬁle view: MPI

260
J. Kunkel et al.
Fig. 1. Example non-contiguous access pattern in which every other elementary data
type is accessed.
data types are used to describe the elementary data type (etype) the ﬁle consists
of (e.g. a structure), and the ﬁle type speciﬁes which of those are to be accessed
by the current process. A programmer usually accesses data at the granularity
of etypes. An oﬀset (the so-called displacement) is added to the beginning of the
ﬁle to account for e.g. ﬁle meta-data. An example ﬁle view and mapping of its
data to bytes in the ﬁle is illustrated in Fig. 1. Here, the etype could be a 4-Byte
integer; the ﬁle type covers four contiguous regions (ﬁrst etype is occupied, then
a hole, . . . ) and allows access to every other etype. We will refer to the number
of bytes one contiguous region occupies as ddata, and to the hole size as dhole.
The data sieving algorithm is parameterized by its state (s: on, oﬀ) and a buﬀer
size (sbuﬀer) which deﬁnes the granularity of data access for reading and writing.
It will access data at this size, starting from the bytes needed to be accessed next
that are not contained in the current buﬀer.
3.1
Experiments
To demonstrate the suboptimality and to illustrate the diﬃculties in parameter-
izing the current data sieving strategy, we conducted several experiments. The
mpipattern benchmark has been created to measure performance for arbitrary
ﬁle views and MPI hints. In the following experiments1, this benchmark uses a
ﬁle pattern similar to Fig. 1; the etype is always an integer and we vary ddata
between 1 KiB and 16 MiB, the data sieving options (s, sbuﬀer) and the ﬁll level
f :=
ddata
ddata+dhole =
ddata
dextent .
The experiments have been conducted on our 20 node cluster: 10 I/O nodes
are each equipped with an Intel Xeon E3-1275@3.4 GHz, 16 GByte RAM and
one Seagate Barracuda 7200.12. Nodes are interconnected with Gigabit Ethernet
and the performance of one HDD is about 100 MiB/s. The I/O nodes run Cen-
tOS 6.5 and Lustre 2.5. On one additional compute node, a single mpipattern
process is run which reports the observed performance. In a production envi-
ronment, multiple users and applications access the shared storage; this may
lead to high ﬂuctuations in observable performance. For a ﬁrst discussion, this
eﬀect is ignored; during the measurement, the whole cluster has been blocked to
ensure exclusive access to the I/O servers. The test ﬁle is pre-created with 8 GiB
of data; between runs, we clear the Linux cache. In the following, we limit our
discussion to read calls.
1 Experimental data is taken from Schmidtke’s thesis [15].

Predicting Performance of Non-contiguous I/O with Machine Learning
261
(a) ddata = 16 KiB
(b) ddata = 256 KiB
Fig. 2. Independent I/O performance for a variable hole size and two block sizes mea-
sured with one client for diﬀerent data sieving options.
Overall, out of 198 diﬀerent conﬁgurations, the best result is achieved with-
out data sieving in 56 cases, and in 59 and 54 cases with sbuﬀer = 1 MiB and
sbuﬀer = 4 MiB, respectively. A 100 MiB buﬀer never achieved best performance,
even for larger datasets and holes. In 29 cases, performance of all settings was
similar (relative performance within 95 % of the best). Figure 2a and b show the
observed performance for ddata = 16 and 256 KiB, respectively. The lines in the
ﬁgures represent the performance with data sieving turned oﬀ, or its buﬀer set
to 1 MiB or 4 MiB of data. Additionally, a theoretic line is given: it is based on
the maximum network performance (117 MiB/s) and assumes all holes would be
read; e.g., for a ﬁll level of 10 %, user data is transferred at 11.7 MiB/s.
For small blocks, data sieving performs better, because it avoids seeks and
the system beneﬁts from the Linux read-ahead mechanism. The eﬀects of read-
ahead can be seen by comparing performance from ﬁll levels f of 100 % and
98 % (dhole = 160 Byte). With a low ﬁll level and thus large holes, data sieving
actually slows down the operations; the reason is that it actually reads the full
buﬀer size for every required data block even though we only need the ﬁrst 16 KiB
of data. When accessing 256 KiB of data, this strategy also explains the eﬀect
starting at f = 16 %: The 4 MiB buﬀer is much slower than the 1 MiB buﬀer.
However, a user would expect that larger buﬀers may increase performance but
never decrease it.
Moreover, for the large access granularity, turning data sieving oﬀis bene-
ﬁcial starting with f = 33 %. For larger accesses, data sieving extracts similar
performance in many cases. Several interesting eﬀects can be seen: performance
of f = 98 % (dhole = 8 KiB) is slower; with f = 66 % (dhole = 128 KiB), per-
formance converges; and with smaller ﬁll levels, some values attain much better
performance. The 128 KiB hole can be explained by Linux read-ahead mecha-
nisms: normally, another 128 KiB block of data is fetched, which is available in
any case. We are striping in 1,048,576 Byte blocks; due to the layout, every single
256 KiB access is covered completely by one Lustre object storage target (OST).
In these cases, with data sieving, another performance pattern can be observed.
Note that for a 1 MiB buﬀer, each I/O involves one additional OST, all of the
requested data of which is discarded.

262
J. Kunkel et al.
Presumably, the reason for the zig-zag pattern for small ﬁll levels without
data sieving is the OST-centric read-ahead in Lustre. Several patterns lead to
sequential access of data on a subset of servers, such as for 25 % and 12.5 %;
here, data is read from every (or every other) OST in a sequential fashion. The
read ahead stats from /proc reveal that per 256 KiB access, about 0.03 and 0.7
cache misses occur for the very good and bad cases, respectively. The osc read
shows about 0.77 to 1.75 operations per access; thus, some patterns trigger more
operations than others.
3.2
Performance Factors
There are many factors involved in the performance of non-contiguous I/O that
can be classiﬁed into the applications’ spatial and access pattern, the behav-
ior of ﬁle system client and parallel ﬁle system, and hardware characteristics.
Each individual I/O operation comes with some overhead for the system call
and transferring and processing the triggered I/O request within the parallel ﬁle
system. If data is not cached, the operation is dominated on the server side by
the latency of the block storage. Aggregating multiple non-contiguous accesses
into one operation alleviates these costs but may transfer irrelevant data from
block devices and across the network, and thus beneﬁt depends on the through-
put of these components. The ﬁle’s data distribution (stripe size in Lustre and
number of servers) has a big impact on performance, as it should be avoided to
involve too many I/O servers with very small requests. Therefore, the alignment
of the accessed data on the ﬁle system’s server is important. An additional factor
is the cost for distributed locking needed for writes. The operating system’s and
ﬁle system’s pre-fetching mechanisms can transform some read patterns auto-
matically into beneﬁcial sequential access patterns without explicitly requesting
large chunks at application level.
The decision whether or not to merge a consecutive operation with the cur-
rent operation depends on the knowledge of these factors; the best choice may
fuse certain blocks and process others individually. As none of these factors
are explicitly included in ROMIO’s data sieving, and the buﬀer sizes can only
be changed when opening the ﬁle, this approach is hard to tune for users and
the achieved performance is often suboptimal. Therefore, machine learning may
be a suitable technique to analyze the data.
4
Methodology
Every approach to optimization will consist of three basic steps: Identifying the
task’s ﬁxed parameters, choosing the best set of variable parameters and suggest-
ing or enacting them. While we aim to perform the machine learning with the
execution of the application (online), in this study, we measured the performance
and investigate the machine learning oﬄine to evaluate the accuracy.
In our use case, the ﬁxed parameters consist of the access pattern, speciﬁed by
a sequence of (oﬀset, size) tuples (cf. Fig. 1). As this may constitute a sequence

Predicting Performance of Non-contiguous I/O with Machine Learning
263
of ﬁnite but unbounded length, we use a simple ﬁrst abstraction, computing
only the total size ddata of each data type and its ﬁll level f. Further research
will target more accurate representations and characteristics of the resulting
parameter spaces.
Our variable parameters are the state of data sieving (on, oﬀ) and the buﬀer
size sbuﬀer used for it. Our optimization criterion is the performance p, the
average (arithmetic mean) throughput achieved under the mpipattern bench-
mark. The data to be used in training and validation was gathered by running
the benchmark ﬁve times per parameter set, then the performance’s arithmetic
mean is computed for each conﬁguration. The relevant variable parameters and
target labels are stored in a CSV ﬁle. The evaluation is conducted by loading the
observed performance data into the statistics tool R. We then create the models
oﬄine and compare their performance to the best achievable performance. Since
the observed performance data volume is small (CSV ﬁles of roughly 100 KiB),
the time needed for machine learning is negligible in the analysis.
We use standard machine learning techniques to extract knowledge from the
data. For our ﬁrst method to evaluate, we chose Classiﬁcation And Regression
Trees (CART) [16], as implemented in the open source library Shark [17], the
statistics tool R and the language python. Our ﬁrst step is to create a predic-
tor for the performance to be expected from a given set of ﬁxed and variable
parameters. This Performance Model (PM) is trained on a number of samples,
allowing it to estimate a performance value for any given parameter set (see
Fig. 3a). For this model, the CSV ﬁle contains: size, ﬁll level, state, buﬀer size
and arithmetic mean performance. We train the model using a subset of rows in
the CSV ﬁle (the training set) and predict the performance for the validation
set. Since the mean performance of the data is available for the validation set,
we can determine the error.
As not all machine learning algorithms are suited to regression, we trans-
formed this task into a classiﬁcation problem which allows for a full comparison
later on. For this, we form classes by quantizing the performance space into
intervals, similar to the “shingles” used in the R package lattice; parameter
sets are classiﬁed by mapping them to the interval “class” covering the achieved
performance p. For every parameter set thus classiﬁed, a representative of the
pertinent interval is then chosen as a performance estimate. Since our inter-
val partition is ignorant of the true performance’s distribution, we chose the
intervals’ middle points as representatives to facilitate error bound assertions.
Using the median of the values classed within each interval might decrease actual
errors as it better approximates clusters within the interval, though. With this
set-up, however, uniform intervals are imprecise in the lower ranges, while small
relative variations in the higher ranges will mean several classes displacement.
We therefore vary interval length with the absolute values they cover: Given a
relative error limit ϵ and the maximum performance measured pmax, we deﬁne
l := ϵ · pmax. Between 0 and l, we choose uniform interval lengths |Ii| := l · 2ϵ;
above l, we increase them stepwise by |Ii| := |Ii−1| · (1 + 2ϵ). This implies the
quantization mean error errq :=
1
4n
n
i=0 (ΔIi), where ΔIi is the interval size of
the class that belongs to the i-th instance.

264
J. Kunkel et al.
PM
Input
Buffer Size
Data Sieving
Data Size
Fill Level
Output
estimated Performance
Parameters
Buffer Size
Data Sieving
Data Size
Fill Level
Observed Values
Performance
train
(a) Performance Model
PSM
Input
Data Size
Fill Level
Output
best Buffer Size
best Data Sieving
Parameters
Buffer Size
Data Sieving
Data Size
Fill Level
Observed Values
Performance
train
(b) Parameter Setting Model
Fig. 3. PM provides a performance estimate, whereas PSM provides the corresponding
“tunable” variable parameters to achieve it.
We imagine an implementation could automatically tune the data sieving
parameters online by estimating the performance for all sets of variable para-
meters and picking the values expected to perform best. This strategy has the
advantage that we can validate the prediction accuracy online by comparing
estimation with measurement, and disable the predictor if the results diﬀer sig-
niﬁcantly from the observation.
However, this strategy requires us to assess performance for many diﬀerent
settings. Instead, we chose a complementing strategy to directly predict the
variable settings for a given set of ﬁxed parameters; we call this the Parameter
Setting Model (PSM) Fig. 3b. Since the performance data is still available to us,
we can also quantify the eﬃciency of this model in our evaluation. Note that in
an implementation, the PSM could use the performance prediction of the PM
to check its correctness.
5
Evaluation
To assess the quality of the machine learning algorithms, we created simpler
models and use them as a baseline: A very naive prediction for a sample would
be the arithmetic mean performance. In our experiments, the mean performance
is 54.7 MiB/s, which leads to an average error of 28.5 MiB/s. Experimenting with
diﬀerent linear models based on the ﬁxed and variable settings led to a model
with a mean error of 12.7 MiB/s.
5.1
Validation
A series of k-fold cross-validation tests (Table 1) shows that on our data set, the
CART classiﬁer performs better than the our baseline. Unless noted otherwise,
all results cited in this section have been generated with the following parame-
ters: size of training set = size of validation set = 387 instances. Classiﬁcation
parameter: ϵ = 0.05, pmax = 109.554.
Figure 4a shows the observed and predicted classes when using half the train-
ing data. The graph is sorted by the true performance class (black dots) and

Predicting Performance of Non-contiguous I/O with Machine Learning
265
Table 1. Prediction errors in MB/s and class errors for training sets under k-fold
cross-validation. Values for k = 3..7 lie in between.
k Performance errors Class errors
min
mean max
min
mean max
2 6.74 6.80
6.87
1.46 1.59
1.72
4 5.19 6.25
6.92
0.94 1.34
1.72
8 4.67 5.66
6.77
0.87 1.19
1.62
(a) CART prediction (trained by 387 in-
stances).
(b) Performance prediction for ddata =
256 KiB.
Fig. 4. Quality of PM performance prediction.
the red dots show the predicted classes. The actual performance prediction for
ddata = 256 KiB are presented in Fig. 4b. Often, a predicted performance matches
one of the nearby observed values; the reason is that the original data point is not
contained in the training set and thus the model learns from nearby values and
uses them as approximation. Clearly, the sensitivity of the pattern and thus
major performance diﬀerences are impossible to predict accurately if instances
are missing.
5.2
Investigating Training Set Size
We are working towards a self-optimizing system that stops the optimization
process as soon as some convergence criterion is fulﬁlled, e.g. the learning rate
is negligible or the error rate small enough. This bypasses the need for rules or
a static formula to calculate an optimal training set size, allowing us to replace
learning algorithms and apply this approach to a variety of problems without
interdependency with our data acquisition scheme.
Nevertheless, we have investigated the prediction accuracy of PM under var-
ious training set sizes, using a variant (“inverse”) k-fold cross-validation where
one fold is used for training and the remaining k −1 for validation instead of
the other way round. The results of the CART classiﬁer are shown in Fig. 5.
The 774-instances case validates the overall scheme: The CART classiﬁer was
both trained and validated with the whole data set, yielding a prediction mean

266
J. Kunkel et al.
Fig. 5. Mean prediction error of PM by training set size under inverse k-fold cross-
validation. Class prediction errors show very similar behavior.
error of 1.7 MB/s. Deducting the quantization error of 1.31 MB/s due to our
assignment to classes leaves the real CART classiﬁer error at 0.39 MB/s.
Beginning with the full data set, we reduce the number of training instances
by a factor of 1/2 in each iteration step until the CART classiﬁer stops pro-
ducing reasonable predictions. By training with 24 instances, we can recognize
the ﬁrst learning progress: the maximum prediction error drops beneath the raw
data’s standard deviation. 96 instances were suﬃcient to outperform the naive
approach, and after 387 instances, we could observe a considerable stagnation of
the learning rate (cf. Table 1). Moving to random forests [18] yielded very similar
results, not justifying the additional computational cost incurred.
The potential beneﬁt of the approach can be assessed by applying the strategy
to the experimental data. Assuming the user had parameterized the data sieving
for all experiments in the same way, the average performance beneﬁt of choosing
the optimal values instead is given in Table 2. As a default, setting data sieving to
1 MiB would yield the best result, as even optimal parameter settings outperform
it by at most 7.6 MB/s. But even here, our CART-driven PSM with training and
test set sizes of 387 instances each could improve performance by 1.9 MB/s.
5.3
Decision Rules
By classifying only into three classes (slow, average, fast), the CART classiﬁer
applied to the complete data set of mean performance values creates a tree of
221 nodes; the ﬁrst 4 levels are shown in Fig. 6. The following analysis relies on
Table 2. Average performance improvements that can be achieved with the PSM-
learned and best choices for sbuﬀer, compared to one default choice.
Default choice CART PSM, 387 Inst. Best choice
Oﬀ
4.2 MB/s
9.6 MB/s
1 MiB
1.9 MB/s
7.6 MB/s
4 MiB
6.9 MB/s
12.2 MB/s
100 MiB
6.9 MB/s
12.2 MB/s

Predicting Performance of Non-contiguous I/O with Machine Learning
267
Fig. 6. First three levels of the CART classiﬁer rules for three classes slow, avg, fast
([0, 25], (25, 75], > 75 MB/s). The dominant label is assigned to the leaf nodes – the
probability for each class is provided in brackets.
the fact that the test cases cover the selected parameter space equally. Based
on the ﬁgure, some rules can be derived: e.g., the left-most path tells us that
of non-contiguous accesses with f < 20 % and ddata = 2 MiB, 79 % will show
slow and 20 % average behavior. Other rules thus created include: in most cases,
sparse access to larger blocks is slow; sequential access to small blocks is fast
(due to read-ahead); for f > 2/3 and ddata < 200 KiB, data sieving is beneﬁcial
for almost all accesses; in the case of f < 2/3 and 2 MiB < ddata < 8 MiB,
performance is mostly fast – surprisingly, larger accesses achieve merely average
performance. Experts in the ﬁeld typically know the ﬁrst rules, but the last two
statements are interesting.
6
Learning Best Practices for DKRZ
DKRZ runs a test system to prepare for their next supercomputer that will be
installed in Q1 of 2015. We conducted measurements on this porting system
to study whether our methodology can be applied to learn appropriate Lustre
settings. The test system consists of 20 compute nodes and a Lustre 2.5 ﬁle
system hosted by one ClusterStor 6000 enclosure (SSU) from Seagate with two
OSS servers and 84 HDDs. All nodes are interconnected with FDR-Inﬁniband.
The following measurements are conducted with our NCT library which is
currently in development and oﬀers POSIX-compatible calls for non-contiguous
access. Amongst other strategies, it implements the ROMIO algorithm for data
sieving, allowing for an analysis similar to the one discussed before: A single
process performs reads or writes on a previously created 10 GB ﬁle, varying
hole size and access granularity. As opposed to the results discussed so far, this
evaluation is conducted on the ﬁle system shared amongst all users. To gain
comparable results, the client cache is cleaned between the runs, and several
repetitions are measured. If a value diﬀers more than 20 % from the average of
all others measured so far for this conﬁguration, an additional run is executed
after which this procedure is repeated, resulting in up to 10 measurements for
cases with high variation.
Overall, 408 conﬁgurations of hole and block size were measured for up to 8
combinations of user controllable settings (one or two Lustre servers, 128 KiB or
2 MiB stripe size, data sieving with 4 MiB or oﬀ). 240 of these were run with all
8 settings; of the remainder, 84 more cases each were only evaluated for 128 KiB
and 2 MiB stripe sizes, respectively. For validation purposes, the two settings of
stripe sizes were also evaluated for one server, which should not make a diﬀerence.

268
J. Kunkel et al.
Table 3. Frequency in which a setting of the row is better by 20 % (at least 5 MB/s)
than the one of a column, out of 240 hole/size conﬁgurations.
Data sieving
Oﬀ
On
Server count
1
2
1
2
Stripe size
128 KiB 2 MiB 128 KiB 2 MiB 128 KiB 2 MiB 128 KiB 2 MiB
Sieving Server # Stripe
Oﬀ
1
128 KiB
-
0
0
0
31
31
33
31
2 MiB
2
-
0
0
31
32
34
32
2
128 KiB
54
57
-
3
43
43
35
37
2 MiB
56
59
2
-
39
45
42
37
On
1
128 KiB 114
115
109
93
-
5
8
19
2 MiB
104
103
90
83
0
-
3
14
2
128 KiB 112
112
104
107
65
71
-
8
2 MiB
112
111
104
96
56
69
2
-
First, we look at the frequency at which a particular setting is superior to
another. Table 3 shows the number of times the conﬁguration in a row achieves
at least 20 % and 5 MB/s better performance than the one in the column. A few
unexpected cases are observable: when only one server is used, the variation in
stripe sizes should lead to similar performance results. However, e.g. without
data sieving, the 2 MiB stripe size is in 2 (out of 240) cases better than the
one with 128 KiB which is presumably due to ﬂuctuations on the shared storage
resource. Without data sieving, it can be seen that typically one server achieves
less performance than two; with data sieving, there are some cases in which
one server signiﬁcantly outperforms two. In the sampled conﬁgurations, turning
data sieving on is usually superior to turning it oﬀin about 100 cases, the naive
I/O is better in about 35 cases. While the conﬁgurations are similar to the ones
measured on our test cluster, the amount of data accessed is typically small
compared to the fast interconnect and storage system, which explains why data
sieving dominates the naive approach. With 20 % tolerance, there are a few cases
in which the stripe size is relevant; reducing tolerance to 10 %, this number rises
to about 50. Nevertheless, we expect that this number will grow much higher on
DKRZ’s ﬁnal system than in this preliminary experiment.
6.1
Applying Machine Learning
In the following analysis, used the scikit-learn Python library with its Decision-
TreeClassiﬁer (with its entropy criterion) on the CSV ﬁle to learn the decision
tree and extract knowledge. The triple (sieving, server count, stripe size) of the
best possible choice for each conﬁguration is encoded as an integer and learned.
Note that we treat all conﬁgurations equally – in a real system, each would be
weighted based on the probability of observing it, making sure that frequent
access patterns will be well optimized.

Predicting Performance of Non-contiguous I/O with Machine Learning
269
Looking at some statistics of the achieved performance allows us to quantify
the optimization potential: The best observed performances for a single run are
up to 800 MB/s and 350 MB/s for read and write operations, respectively. Over
the 240 conﬁgurations, an average performance of 213 MB/s is observable. The
average performance over all conﬁgurations, choosing the best setting for each,
is 293 MB/s; choosing the worst for each, it is 146 MB/s. Creating a decision
tree of depth 1 yields the rule if (write) select (data sieving=on, servers=2,
stripe=128 KiB) else select (data sieving=on, servers=1, stripe=128 KiB). Fol-
lowing even this simple rule reduces the gap in average performance compared
to the best per-case choice possible to only 16.6 MB/s.
In practice, we will not normally have all settings sampled for a given conﬁg-
uration, resulting in missing values similar to our case with 408 conﬁgurations.
Using pruned trees with reduced height however, as in this evaluation, rules may
still suggest settings that have not been measured so far, and if this recommenda-
tion is followed, the sampled portion of the parameter space will grow in the long
run. Using all values, the average performance over all measured conﬁgurations
and settings is 244.7 MB/s. The best setting for each conﬁguration achieves an
average performance of 357.7 MB/s, and the worst choice of 179.9 MB/s. Table 4
lists the average performance loss of a given default choice when compared to
the best available choices.
Table 4. Tunable settings: expected performance of a user’s default choice vs. the
per-case optimal setting (absolute in MB/s, relative and performance loss in MB/s
compared to the best choice) using arithmetic and harmonic mean. The number of
cases in which a setting is the worst or best choice out of all 408 conﬁgurations is listed
for reference.
Default choice
Best Worst Arithmetic mean
Harmonic mean
Servers Stripe size Sieving Freq. Freq.
Rel.
Abs. Loss
Rel.
Abs.
1
128 KiB
Oﬀ
20
35
58.4 % 200.1 102.1
9.0 % 0.09
1
2 MiB
Oﬀ
45
39
60.7 % 261.5 103.7
9.0 % 0.09
2
128 KiB
Oﬀ
87
76
69.8 % 209.5
92.7
8.8 % 0.09
2
2 MiB
Oﬀ
81
14
72.1 % 284.2
81.1
8.9 % 0.09
1
128 KiB
On
79
37
64.1 % 245.6
56.7 15.2 % 0.16
1
2 MiB
On
11
75
59.4 % 259.2 106.1 14.4 % 0.15
2
128 KiB
On
80
58
68.7 % 239.6
62.6 16.2 % 0.17
2
2 MiB
On
5
74
62.9 % 258.0 107.3 14.9 % 0.16
When averaging test run performance, two scenarios may apply: Comput-
ing centers desire a continually saturated job queue, where the mean achieved
over a ﬁxed time is of interest. Users, who typically have a ﬁxed workload to
be completed, regard the mean derived from the total time to completion as
more important. The ﬁrst is given by the arithmetic mean, while the harmonic
mean yields the second. Another interpretation of the arithmetic columns is

270
J. Kunkel et al.
Fig. 7. Performance diﬀerence between learned and the best choices, by maximum tree
depth, for the DKRZ porting system.
the expected performance when picking a random experiment, while the har-
monic performance deﬁnes the average throughput expected when executing all
experiments with the given setting. The arithmetic mean favors fast execution,
the harmonic mean is restricted by slow performance. With one server, 2 MiB
stripe size and activated data sieving, for instance, 64.1 % of the best possible
performance is expected for any run and the arithmetic mean performance loss
compared to the optimal settings is 57 MB/s (achieving 245.6 MB/s). However,
when executing all experiments with this setting, only 15.2 % of the best perfor-
mance is expected resulting in an average harmonic mean performance of only
0.16 MiB/s. This low harmonic mean is due to experiments with large holes and
small amounts of data achieving a performance below 1 MB/s. By choosing the
optimal tunable settings, the achievable performance can thus be signiﬁcantly
increased, which further underlines the relevance of this early study.
Figure 7 shows the average performance loss between machine learning and
the per-case optimum, based on the depth of the tree learned. The ﬁgure also
includes the relative performance achieved, compared as harmonic and arith-
metic means. Even at a very low height, the tree proves very eﬃcient, achieving
more than 87 % of the arithmetic mean performance and 79 % of the harmonic2.
This is much better than all possible ﬁxed defaults (72 % arithm. mean and
15 % harmonic mean performance at best). Therefore, the trees avoid subopti-
mal choices eﬃciently. One exception is the tree with a depth of 5: it suggests
several slow settings, resulting in a relative harmonic mean performance of 34 %.
A tree of level four (shown as in Fig. 8) achieves good performance (about
3.5 MB/s average gap) at a reasonable size; it can be expected to achieve 99 % of
the potential performance (arithmetic and harmonic). The leaves are the choices
based on the access pattern. The number of instances in which this choice is
2 Note that for a tree of depth one, 80 choices are made for which no measurement is
available; these values are excluded from the calculation of the average performance.
For bigger trees, less than a handful of choices are not quantiﬁable. Therefore, we
believe this comparison to be fair.

Predicting Performance of Non-contiguous I/O with Machine Learning
271
Fig. 8. Decision tree for DKRZ test system with height 4. In the leaf nodes, the settings
(Data sieving, server number, stripe size) and number of instances of the best and
second best choice are shown.
the best is given in the leaf for convenience, followed by the second best choice.
Interestingly, in most cases, both diﬀer only in a single parameter, i.e., either
number of servers, data sieving or stripe size. Given an access pattern, this tree
allows users (or a library) to select appropriate and eﬃcient settings. Also, using
machine learning and extracting rules from such a tree proved far less time
consuming and error-prone than studying the measurement results by hand.
7
Conclusions and Future Work
Even constrained to the few parameters governing data sieving, optimizing HPC
I/O is anything but trivial. We have discussed the challenges and limitations
faced when optimizing non-contiguous access using data sieving, and used Clas-
siﬁcation and Regression Trees to create a predictor for the I/O performance
resulting from a given parameter set. Evaluating this predictor under various
training set sizes, we found it a fairly accurate indicator of the performance
to be expected. We created another model that will choose the parameter set
promising the highest performance, achieving signiﬁcant improvements over the
best default settings and increasing the average I/O performance by several
MiB/s. While the decision trees reproduced known heuristics correctly, we also
harvested interesting insights from them, yielding best practices for data sieving
on our system.
Future work will focus on automatically generating simple rules-of-thumb
from the extensive decision trees. Integrating our ﬁndings with the SIOX system
will allow us to harness this knowledge for optimization as well as for active
learning during phases of low utilization. Thus, sparse training data can be sup-
plemented to greatly improve predictor accuracy and overall eﬀectiveness. Since
data sieving does not incorporate the important performance factors, observed
performance behaves unpredictably in many cases, leading to suboptimal accu-
racy of the CART when using sparse training data. The parameters discussed in
this paper are system dependent, but not aﬀected by the ﬁle type and pattern,
marking them as candidates for machine learning. We are currently working
on an adaptive data sieving algorithm relying on this, and researching more
accurate representations and characteristics of the resulting parameter spaces.

272
J. Kunkel et al.
Finally, future eﬀorts will further explore ML techniques and their applicability,
as well as the eﬀects of selective data acquisition and active learning.
References
1. Thakur, R., Gropp, W., Lusk, E.: Data sieving and collective I/O in ROMIO.
In: FRONTIERS 1999: Proceedings of the The 7th Symposium on the Frontiers
of Massively Parallel Computation, p. 182. IEEE Computer Society, Washington,
DC (1999)
2. Ching, A., Choudhary, A., Coloma, K., Liao, W.K., Ross, R., Gropp, W.: Non-
contiguous I/O accesses through MPI-IO. In: Proceedings of the 3rd International
Symposium on Cluster Computing and the Grid, CCGRID, p. 104. IEEE Com-
puter Society, Washington, DC (2003)
3. Singh, D.E., Isaila, F., Calderon, A., Garcia, F., Carretero, J.: Multiple-phase
collective I/O technique for improving data access locality. In: Proceedings of the
15th Euromicro International Conference on Parallel, Distributed and Network-
Based Processing, PDP, pp. 534–542. IEEE Computer Society, Washington, DC
(2007)
4. Singh, D.E., Isaila, F., Pichel, J.C., Carretero, J.: A collective I/O implementation
based on inspector-executor paradigm. J. Supercomputing 47(1), 53–75 (2009)
5. Zhang, X., Ou, J., Davis, K., Jiang, S.: Orthrus: a framework for implementing
eﬃcient collective I/O in multi-core clusters. In: Kunkel, J.M., Ludwig, T., Meuer,
H.W. (eds.) ISC 2014. LNCS, vol. 8488, pp. 348–364. Springer, Heidelberg (2014)
6. Kn¨upfer, A., Brunst, H., Doleschal, J., Jurenz, M., Lieber, M., Mickler, H., M¨uller,
M.S., Nagel, W.E.: The vampir performance analysis tool-set. In: Resch, M., Keller,
R., Himmler, V., Krammer, B., Schulz, A. (eds.) Tools for High Performance
Computing, Proceedings of the 2nd International Workshop on Parallel Tools,
pp. 139–155. Springer, Heidelberg (2008)
7. Argonne
National
Laboratory:
Darshan.
http://www.mcs.anl.gov/project/
darshan-hpc-io-characterization-tool
8. Madhyastha, T., Reed, D.: Learning to classify parallel Input/Output access pat-
terns. IEEE Trans. Parallel Distrib. Syst. 13(8), 802–813 (2002)
9. Barham, P., Donnelly, A., Isaacs, R., Mortier, R.: Using magpie for request extrac-
tion and workload modelling. In: Proceedings of the 6th Symposium on Opearting
Systems Design and Implementation, vol. 6, pp. 259–272 (2004)
10. Barham, P., Isaacs, R., Mortier, R., Narayanan, D.: Magpie: online modelling and
performance-aware systems. In: Proceedings of the 9th Conference on Hot Topics
in Operating Systems, vol. 9 (2003)
11. Isaacs, R., Barham, P., Bulpin, J., Mortier, R., Narayanan, D.: Request extrac-
tion in magpie: events, schemas and temporal joins. In: Proceedings of the 11th
Workshop on ACM SIGOPS European Workshop, EW11. ACM, New York (2004)
12. Behzad, B., Huchette, J., Luu, H.V.T., Aydt, R., Byna, S., Yao, Y., Koziol,
Q.: Prabhat: a framework for auto-tuning hdf5 applications. In: Proceedings of
the 22nd International Symposium on High-Performance Parallel and Distributed
Computing, HPDC 2013, pp. 127–128. ACM, New York (2013)
13. Kunkel, J.M., Zimmer, M., H¨ubbe, N., Aguilera, A., Mickler, H., Wang, X., Chut,
A., B¨onisch, T., L¨uttgau, J., Michel, R., Weging, J.: The SIOX architecture –
coupling automatic monitoring and optimization of parallel I/O. In: Kunkel, J.M.,
Ludwig, T., Meuer, H.W. (eds.) ISC 2014. LNCS, vol. 8488, pp. 245–260. Springer,
Heidelberg (2014)

Predicting Performance of Non-contiguous I/O with Machine Learning
273
14. Zimmer, M., Kunkel, J.M., Ludwig, T.: Towards self-optimization in HPC I/O.
In: Kunkel, J.M., Ludwig, T., Meuer, H.W. (eds.) ISC 2013. LNCS, vol. 7905,
pp. 422–434. Springer, Heidelberg (2013)
15. Schmidtke, D.: Analyse und Optimierung von nicht-zusammenh¨angende Ein-
/Ausgabe in MPI, April 2014
16. Breiman, L., Friedman, J.H., Olshen, R.A., Stone, C.J.: Classiﬁcation and Regres-
sion Trees. Wadsworth & Brooks, Paciﬁc Grove (1984)
17. Igel, C., Heidrich-Meisner, V., Glasmachers, T.: Shark. J. Mach. Learn. Res. 9,
993–996 (2008)
18. Breiman, L.: Random forests. Mach. Learn. 45(1), 5–32 (2001)

A Best Practice Analysis of HDF5
and NetCDF-4 Using Lustre
Christopher Bartz1(B), Konstantinos Chasapis2, Michael Kuhn2,
Petra Nerge2, and Thomas Ludwig1
1 Deutsches Klimarechenzentrum, Bundesstraße 45a, 20146 Hamburg, Germany
bartz@dkrz.de
2 University of Hamburg, Bundesstraße 45a, 20146 Hamburg, Germany
Abstract. With the constantly increasing number of cores in high per-
formance computing (HPC) systems, applications produce even more
data that will eventually have to be stored and accessed in parallel.
Applications’ I/O in HPC is performed in a layered manner; scientiﬁc
applications use standardized high-level libraries and data formats like
HDF5 and NetCDF-4 to store and manipulate data that is located inside
a parallel ﬁle system. In this paper, we present a performance analysis
of the parallel interfaces of HDF5 and NetCDF-4 using diﬀerent test
conﬁgurations in order to provide best practices for choosing the right
I/O conﬁguration. Our evaluation follows a breakdown approach where
we examine the performance penalties of each layer. The tested conﬁg-
urations include: (i) diﬀerent access patterns, disjoint and interleaved
(ii) aligned and unaligned accesses (iii) collective and independent I/O
(iv) contiguous and chunked data layout. The main observation is that
using interleaved data access in a certain conﬁguration achieves near the
maximum performance. Also, we see that NetCDF-4 does not provide
the ability to align the access to the Lustre object boundaries. To over-
come this we have developed a patch that resolves this issue and improves
the performance dramatically.
Keywords: Best practices · HDF5 · NetCDF-4
1
Introduction
Many high performance computing (HPC) applications deal with large volumes
of data. A typical example of I/O-intensive applications are simulations from the
ﬁeld of earth system science that generate vast amounts of data as snapshots and
model output ﬁles. The performance of those applications is highly dependent
on the storage system performance. For this reason, they make use of parallel
I/O to achieve maximum performance.
Apart from the I/O performance, another important aspect for such appli-
cations is eﬃcient data management. To improve this, standardized ﬁle formats
are used to store and manipulate data so that they can be easily exchanged
between institutes and scientists for further processing. Among the commonly
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 274–281, 2015.
DOI: 10.1007/978-3-319-20119-1 20

A Best Practice Analysis of HDF5 and NetCDF-4 Using Lustre
275
used formats are the Hierarchical Data Format (HDF5) [4] and the Network
Common Data Format (NetCDF-4) [10]. They are surrounded by application
programming interfaces (APIs), which allow manipulation and retrieval of the
data by programs written in various programming languages like C or Fortran.
In this paper we present these common I/O interfaces, evaluate their perfor-
mance and describe best practices to achieve optimal performance. To conduct
our analysis, we used a self-modiﬁed version of the I/O Performance Benchmark
(IOR) [6] to generate I/O patterns that mimic the I/O routines of real-world
applications. We used the parallel distributed ﬁle system Lustre [2] due to its
prevalence in HPC systems. The main contributions of this paper are: (i) an
analysis of the complete I/O path of the aforementioned interfaces while using
diﬀerent access patterns and I/O conﬁgurations, (ii) our proposed performance
enhancements for HDF5 and NetCDF-4, and (iii) a set of best practices that
can be used to achieve peak performance when using HDF5 and NetCDF-4.
The rest of this paper is organized as follows: First, we provide background
information in Sect. 2. We discuss the related work in Sect. 3. Our evaluation is
presented in Sect. 4. Finally, we summarize our ﬁndings in Sect. 5.
2
Background
2.1
Lustre
Lustre [2] is a POSIX-compliant, open-source distributed parallel ﬁle system
that powers over 60 % of the TOP100 supercomputers [9]. Figure 1 presents its
generic architecture. Lustre separates metadata and data handling in Metadata
Servers (MDSs) and Object Storage Servers (OSSs), respectively. Lustre’s ﬁle
data storage follows an object-based paradigm: ﬁle data is split up into multiple
objects called “stripes”. The objects are spread and manipulated by the OSSs.
The Object Storage Targets (OSTs) and the Metadata Targets (MDTs) hold the
actual data and metadata.
2.2
HDF5 and NetCDF-4
The HDF5 and NetCDF-4 APIs provide parallel access to the data. They perform
the I/O in a layered manner as illustrated in Fig. 2. The NetCDF-4 API uses
Fig. 1. Lustre architecture
Fig. 2. Layered HPC I/O

276
C. Bartz et al.
HDF5 to store the data. HDF5 uses the I/O implementation of MPI. MPI-IO is
built on top of a POSIX-complaint parallel ﬁle system. Finally, I/O is performed
by the I/O driver.
Both HDF5 and NetCDF-4 formats store data in a self-describing portable
way using multi-dimensional arrays. Beyond that, HDF5 and NetCDF-4 distin-
guish between contiguous and chunked data storage. Contiguous means that the
whole data of a dataset is stored in one contiguous array of bytes apart from
the header. Chunked means that the dataset is split into chunks, which are rec-
tangular regions. The chunks are written into independent locations in the ﬁle.
The locations of the chunks are stored in the header section of the dataset, using
a B-tree [1] data structure. B-trees hold the stored data in a sorted way and
allow access and manipulation in logarithmic time. They are commonly used in
databases and ﬁlesystems. Chunking is required if compression of the data is
desired.
3
Related Work
Many scientiﬁc papers deal with I/O optimization techniques. Dickens et al.
in [3] propose that each client communicates with fewer OSTs and disjointly
to reduce network and OST contention. They call these patterns x-OST (or
all-to-all) depending on the number of OSTs each client is communicating with
(see Fig. 3). Modiﬁcations to the Two-Phase I/O algorithm used by ROMIO
are proposed in [7]. Their modiﬁcations aim to (i) align the accesses to the ﬁle
system lock boundaries, (ii) reduce network and OSS contention by using I/O
aggregators and (iii) eliminate interleaved accesses. In [8] it has been argued
that a high degree of parallelism leads to high I/O contention at the servers and
does not scale with the amount of processors. To overcome this they introduce
a system which consists of a set of delegated I/O processes. The authors of [11]
show that a large stripe count can degrade the performance because of the
increased protocol overhead. In this case, clients must communicate with many
OSTs, and have to deal with reduced memory cache locality when the network
buﬀer is multiplexed for many OSTs. A set of optimizations in HDF5 and MPI-
IO is presented in [5]. For HDF5, they propose aggregate metadata operations.
Fig. 3. 1-OST communication
Fig. 4. Unaligned vs. Aligned
4
Evaluation
In this section we present the main results of our analysis of the NetCDF-4 and
HDF5 interfaces concerning parallel I/O access. We include two discrete data

A Best Practice Analysis of HDF5 and NetCDF-4 Using Lustre
277
access modes: disjoint and interleaved. Disjoint means that each client accesses
a large contiguous region of the data, which leads to interaction between all data
servers (all-to-all pattern). In the interleaved access pattern each client accesses
a large non-contiguous region (note that we refer to logical and not physical
address access). An interleaved access can result in any x-OST pattern.
4.1
Testbed Speciﬁcations
We used the following software libraries: HDF5 1.8.11, OpenMPI 1.6.4, Lustre 2.5,
NetCDF-4 4.1.3 and IOR 3.0.1. Our system consists of 10 storage servers and 10
client nodes. Each storage server hosts an OSS and an OST. For the sake of sim-
plicity we will refer to OSTs instead of OSSs. The single MDS and MDT are also
hosted in one of the data servers. All nodes are connected via a single Gigabit
Ethernet with a measured network performance between clients and servers of
1,125 MiB/s.
Each data server node runs CentOS 6.5 and has an Intel Xeon Sandy Bridge
E-1275 CPU (3.4 GHz, 4 cores) and 16 GB of RAM. We use a 2 TB SATA2
Western Digital Green HDD for the OST and a 160 GB SATA2 Intel 320 SSD as
the MDT. The client nodes consist of two Intel Xeon Westmere EP HC X5650
CPUs (2.66 GHz, 12 cores) and 12 GB of RAM. They use Ubuntu 12.04.3 LTS
with Linux 3.8.0-33-generic.
4.2
Experimental Design
In each I/O conﬁguration we perform a write phase followed by a read phase. To
ensure the independence of each phase, we restart the system before each write
or read and wait for 30 s. In the restart operation we ﬁrst unmount Lustre from
both the clients and servers, then ﬂush the kernel caches and ﬁnally remount
Lustre. Our plots always show the mean of three diﬀerent measurements if not
stated otherwise.
We use a single IOR process per client node which is suﬃcient to saturate
the network interface. The goal of our experiments is to extract the maximum
performance from the system, therefore we use all the client nodes. In each
write/read phase we transfer 20 GiB per client node (in total 200 GiB). This
exceeds the available memory on the client nodes (see Sect. 4.1) and prevents
any artiﬁcial client caching eﬀect.
If not stated otherwise, we set the transfer size per read/write call to 1 MiB.
We believe that this is a reasonable value, since it matches the chunk size that
the Lustre clients use internally to send all their data if possible. Furthermore,
the default Lustre stripe size equals 1 MiB. When using chunked I/O, we set the
chunk size to 1 MiB, too.
If not speciﬁed otherwise, we align the beginning of the data section to the
Lustre stripe sizes. This also includes the physical start address of the chunks,
when chunked I/O is used. The aligned mechanism is not implemented by the

278
C. Bartz et al.
Fig. 5. Disjoint pattern
default NetCDF-4 API. Our evaluation revealed that this results in huge perfor-
mance penalties thus we have added the respective function call and evaluated
the beneﬁt.
4.3
Contiguous Data Layout
Figure 5 shows the results of a test using contiguous and disjoint data layout.
The ﬁgure does not show the mean but the minimum and maximum obtained
throughput, because many results have a high variation when using independent
I/O. We believe that the reason lies in the combination of the competition on
the OST and the network resources with the lack of synchronization when using
independent I/O.
We see that the results are much lower than the practical maximum
(1,125 MiB/s). We are sure that this can be explained by the contention on
the OSTs and network resources, because the access pattern is an all-to-all OST
pattern; each client node is communicating with each OST node.
The results of the analysis of the 1-OST pattern are illustrated in Fig. 6.
Writing or reading with POSIX and MPI-IO performs nearly identical, achieving
almost the theoretical maximum. The 1-to-1 communication between clients
and servers prevents competition on OST and network resources. We observe
that HDF5 beneﬁts from independent I/O. On the opposite side, collective I/O
adds unnecessary synchronization between the clients when performing a 1-to-1
communication, which slows down the performance.
Contrary to the little overhead of the layering induced by HDF5, the default
NetCDF-4 issues unaligned data accesses which performs much worse. This is due
to the fact that unaligned accesses leads to communication with more than one
OST (see Fig. 4). Implementing the function call to align the data we observe that
NetCDF-4 performance improves greatly and performs similar to HDF5. Thus,
the lack of alignment in the NetCDF-4 API is a real disadvantage regarding the
performance.
Next, we investigate the impact of the transfer size on the performance when
using the disjoint pattern. Since the combination of independent I/O with this

A Best Practice Analysis of HDF5 and NetCDF-4 Using Lustre
279
Fig. 6. 1-OST pattern
Fig. 7. Varying transfer size
Fig. 8. Chunked layout
access pattern results in an inconsistent behavior, we evaluate only the collective
I/O conﬁguration. Figure 7 reports the results for both HDF5 and NetCDF-4.
We can see that as soon as the transfer size is greater than 0.5 MiB for HDF5
and 4 MiB for NetCDF-4 the write operation reaches the maximum performance
which is 800 MiB/s. Furthermore, we see that the read performance increases at
a slower pace than the write operation. We believe that this behavior is due to
the fact that the write requests can be aggregated and cached by the OSTs, so
they can be written out in a eﬃcient way in contrast to the reads that the OSTs
have to deliver immediately which can imply seek overhead. However, the seek
eﬀect ﬂattens out once the transfer size is larger than 128 MiB.
4.4
Chunked Data Layout
In the following test we analyze the performance when using chunked and disjoint
data layout with various chunk sizes. In this conﬁguration the transfer size is
ﬁxed to 512 MiB, because our previous tests have indicated that the performance

280
C. Bartz et al.
beneﬁts from a large transfer size. Furthermore, we measure only collective I/O
which produces ﬁgures with low variation. Figure 8 shows the results. Overall,
whether writing or reading, the performance increases with the size of the chunks
and stays relatively constant with chunk size larger than 4 MiB.
Smaller chunk sizes imply a larger amount of chunks which results in more
metadata operations that have to be handled. This overhead slows down the
performance dramatically, especially when writing, since the B-tree has to be
calculated and written to the header. When the chunk size is suﬃciently large,
the results are analogous to the contiguous case using 512 MiB as transfer size
(see Fig. 7). The reason is that the metadata overhead, mentioned above, is
minimized and also the same access pattern is used: Large contiguous regions
are accessed by each process.
Figure 9 illustrates the results using HDF5 with chunked layout and various
x-OST pattern. We set the chunk size to the maximum meaningful value, which
is the value of the block size, because larger chunk sizes decrease the meta-
data overhead. The achieved ﬁgures are signiﬁcantly lower than the ﬁgures of
the disjoint pattern. Furthermore, independent I/O realizes better results than
collective I/O. Beyond that, we did also perform tests using NetCDF-4 with
various x-OST pattern (ﬁgures are not presented here due to lack of space). The
performance is similar to HDF5. We also reevaluated the 1-OST pattern using
NetCDF-4 with our alignment patch. Again, the alignment patch improved the
performance, which is signiﬁcantly expressed when writing collectively. But the
ﬁgures of HDF5 are not achieved, which indicates an API overhead in this case.
Fig. 9. 1- to 10-OST pattern, HDF5 with chunked I/O
5
Conclusions
In this paper we presented an extensive evaluation of the HDF5 and NetCDF-4
libraries. Our results allow us to provide best practices concerning their usage
in order to achieve high throughput. Although our test environment is small in
comparison to a large HPC cluster, we believe that our results can be transferred
to these systems also.

A Best Practice Analysis of HDF5 and NetCDF-4 Using Lustre
281
For HDF5 we see that it performs optimal with contiguous layout and if the
1-OST pattern is used with I/O accesses aligned to the Lustre stripes. Moreover,
independent I/O should be used in this case, as the synchronization implied by
collective I/O is unnecessary and slows down the performance. Chunked I/O
achieves lower performance. However, in some cases it is required (for example,
if the use of compression is desired). The best performance with chunking is
achieved with the disjoint pattern in combination with large chunk sizes.
Furthermore, collective I/O should be used as we have observed ﬁgures with
high variation using independent I/O with this access pattern. Finally, we believe
that chunking does beneﬁt from large transfer sizes, too. Due to the lack of
alignment in the NetCDF-4 API, the disjoint pattern with large transfer sizes is
better suited as an interleaved pattern if high performance is desired. Chunked
I/O with NetCDF-4 does also perform best with the disjoint pattern.
References
1. Bayer, R., McCreight, E.: Organization and Maintenance of Large Ordered Indexes.
Springer, New York (2002)
2. Braam, P.J., Zahir, R.: Lustre: a scalable, high performance ﬁle system. Cluster
File Systems, Inc. (2002)
3. Dickens, P., Logan, J.: Towards a high performance implementation of MPI-IO on
the lustre ﬁle system. In: Meersman, R., Tari, Z. (eds.) OTM 2008, Part I. LNCS,
vol. 5331, pp. 870–885. Springer, Heidelberg (2008)
4. Group, H., et al.: Hierarchical data format version 5 (2000). Software package,
http://www.hdfgroup.org/HDF5
5. Howison, M.: Tuning HDF5 for lustre ﬁle systems. In: Workshop on Interfaces and
Abstractions for Scientiﬁc Data Storage (IASDS 2010), Heraklion, Crete, Greece,
24 September 2010 (2012)
6. IOR: https://github.com/chaos/ior
7. Liao, W.K., Choudhary, A.: Dynamically adapting ﬁle domain partitioning meth-
ods for collective I/O based on underlying parallel ﬁle system locking protocols. In:
International Conference for High Performance Computing, Networking, Storage
and Analysis, SC 2008, pp. 1–12. IEEE (2008)
8. Nisar, A., Liao, W.K., Choudhary, A.: Scaling parallel I/O performance through
I/O delegate and caching system. In: International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis, SC 2008, pp. 1–12. IEEE
(2008)
9. OpenSFS
(2014).
http://www.opensfs.org/press-releases/lustre-ﬁle-system-
version-2-5-released/. Accessed December 2014
10. Rew, R., Davis, G., Emmerson, S., Davies, H., Hartnett, E.: The NetCDF users
guide-data model, programming interfaces, and format for self-describing, portable
data-NetCDF version 4.1. Unidata Program Center (2010)
11. Yu, W., Vetter, J., Canon, R.S., Jiang, S.: Exploiting lustre ﬁle joining for eﬀective
collective IO. In: Seventh IEEE International Symposium on Cluster Computing
and the Grid, CCGRID 2007, pp. 267–274. IEEE (2007)

Striping Layout Aware Data Aggregation
for High Performance I/O on a Lustre
File System
Yuichi Tsujita(B), Atsushi Hori, and Yutaka Ishikawa
RIKEN AICS, Kobe, Hyogo 650-0047, Japan
yuichi.tsujita@riken.jp
Abstract. An MPI-IO library, ROMIO, improves I/O performance for
noncontiguous accesses by using its two-phase I/O optimization. When
we have multiple MPI processes on each node which has multicore proces-
sors, a data aggregation assignment mismatch leads to performance
degradation due to network contention. In this paper, we propose an
alternative aggregation scheme that manages the striping layout of a
Lustre ﬁle system to minimize network contention. The optimization has
achieved up to about 30 % performance improvements on our 4-node
PC cluster system connected via InﬁniBand FDR links in performance
evaluation by an HPIO benchmark program.
Keywords: MPI-IO · ROMIO · Two-phase I/O · Aggregator · Lustre
1
Introduction
Two-phase I/O optimization of ROMIO [9] repeats read-modify-write sequences
for noncontiguous access patterns in collective write operations. The read-
modify-write sequence consists of (1) ﬁle read, (2) data exchanges among MPI
processes, and (3) ﬁle write. Two-phase I/O assigns an aggregation task to some
or all of the MPI processes, called aggregators, for data collection and ﬁle I/O.
Various research studies have achieved performance improvements by apply-
ing their optimization scheme. However no optimizations currently address net-
work contention problems due to the aggregation task mismatch on a multi-core
processor PC cluster system in accessing a Lustre ﬁle system [6]. An increase
in the number of processes per node increases the number of processes which
access a Lustre ﬁle system in the same striping round in each node by using the
ROMIO. As a result, ﬁle I/O throughput is degraded.
In this paper, we propose ﬁle striping pattern aware aggregator assignment
for performance improvement by minimizing network contention. Performance
improvements up to about 30 % has been achieved by a 4-node PC cluster system
accessing a Lustre ﬁle system. We brieﬂy report the implementation mechanism
and performance results of the proposed scheme in the following sections.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 282–290, 2015.
DOI: 10.1007/978-3-319-20119-1 21

Striping Layout Aware Data Aggregation for High Performance I/O
283
2
Two-Phase I/O in Collective Write Operations
Two-phase I/O consists of repetitions of data exchange and ﬁle I/O phases
to reveal the performance degradation due to noncontiguous accesses. Figure 1
shows a typical two-phase I/O scheme in collective write operations for two
processes. When we have data gaps in access patterns, the two-phase I/O pro-
ceeds with read-modify-write operations. Here, both MPI processes are assumed
to be aggregators, and they read contiguous data including data gaps from their
ﬁle domains to a temporary buﬀer named collective buﬀer. Read data on the
buﬀers are updated by new data to be written after the data exchanges, and are
written-back to the target ﬁle domain. These operations, numbered from 1 to 3
in Fig. 1, are repeated until the whole target ﬁle domain is accessed.
Fig. 1. Two-phase I/O ﬂow in collective write operation for two MPI processes
In a Lustre ﬁle system, which is the target ﬁle system for our performance
improvements, an abstract I/O interface layer of ROMIO named ADIO [8] pro-
vides a Lustre driver layer to exploit its performance advantages [1]. In its opti-
mization context, the collective buﬀer size is aligned to the striping size of a
Lustre ﬁle system inside the ADIO driver layer.
3
Optimized Aggregator Assignment in Striping Layout
Aware Two-Phase I/O
ROMIO assigns one aggregator per node in the default conﬁguration, although
optionally we can have multiple aggregators. Figure 2 shows aggregator task
assignment layout when we have 4 MPI processes per node by using two nodes
to access 2 Object Storage Targets (OSTs) of a Luster ﬁle system. The original
ROMIO appoints aggregator tasks to the MPI processes in a block manner
layout based on its one-dimensional rank list holding MPI ranks. The aggregator
layout is independent of the MPI process layout as shown in Fig. 2(a). In this
case, network contention occurs in each network link from each PC node because

284
Y. Tsujita et al.
Fig. 2. Aggregator assignment layout of ROMIO by (a) original scheme and (b) our
proposed scheme for 2 OSTs when we have 4 aggregators per node. The rectangles rep-
resent PC nodes, and circles inside rectangles represent MPI processes, where numbers
in circles are the MPI rank. Numbers under the circles stand for the assignment order
of the aggregators. Arrows from MPI processes represent striping ﬁle accesses, where
numbers from i to iv of them are striping round.
multiple (or all) aggregators in the same node access diﬀerent OSTs at the same
time in each striping access round by sharing the same interconnect.
For further performance improvements, we propose an alternative aggregator
assignment scheme with striping layout awareness on a Luster ﬁle system, as
shown in Fig. 2(b). We implemented a location-aware grouping scheme for MPI
processes by using the host name of each node to generate the striping layout
aware aggregator assignment order. In the scheme, two lists, one storing host
names and the other one storing MPI ranks, are prepared. The host name list
consists of the host name of every node, and the rank list keeps every MPI rank
with the same index of the host list. By using the two lists, the proposed scheme
carries out host name matching to generate a two-dimensional rank-list, where
the outer index stands for a node ID and the inner one corresponds to a process
ID in the node. Note that the node ID is numbered from 0 in ascending order.
Each element of the original one-dimensional rank list of ROMIO is updated
by ﬁlling each MPI rank in the two-dimensional list in a outer index (node-ID)
major order. As a result, aggregator layout is in a round-robin manner among
nodes as shown in Fig. 2(b). Since aggregators in the same striping access round
are distributed among nodes and each node has aggregators which access the
same OST, we can eliminate the network contention due to shared network link
use among aggregators.
The proposed aggregator assignment also has an advantage in minimizing
data exchange cost. Figure 3(a), and (b) show the data exchange behavior in
the original and our aggregator assignments, respectively. When ﬁle I/O is done
by using striping accesses for the two OSTs, two aggregators access each target
OST at the same time in every striping round. In Fig. 3, the numbers from (i) to
(iv), which are depicted by rounded rectangles inside a PC node, represent the
striping round order. Prior to the ﬁle I/O, every aggregator gathers data from
every process including itself. In the original case in Fig. 3(a), the two aggregators

Striping Layout Aware Data Aggregation for High Performance I/O
285
Fig. 3. Data exchange behavior in (a) original aggregator assignment scheme and
(b) our proposed scheme for 2 OSTs when we have 4 aggregators per node
grouped in a rounded rectangle on the same node collect data simultaneously
in each striping round. Thus, they have a total of 8 incoming data transfers
(4 incoming data transfers × 2 aggregators per group) through a network link
on the same node. Here we only count incoming data transfer through a network
link, and exclude incoming data transfers from other processes on the same node.
However, our proposal has 4 incoming data transfers via a network link on
each node in each striping round. In this scheme, we can minimize the network
contention compared with the original case because the original case shares a
network link of each node for 8 incoming data, while our case can separate the
same number of incoming data evenly into 2 nodes. This can result in perfor-
mance gains relative to the original case, and a large improvement is expected
when we have more OSTs and PC nodes.
We provided the new aggregator assignment scheme in two data exchange
modes to examine its performance impacts in terms of data exchange algo-
rithms; one mode is based on the original implementation, which utilizes pairs
of MPI Isend and MPI Irecv, followed by MPI Waitall for data exchange com-
pletion check (hereinafter IS IR), and the other one is an alternative implemen-
tation that uses MPI Alltoallv instead of pairs of MPI Isend and MPI Irecv
(hereinafter AtoAv).
4
Performance Results
Our proposed scheme was implemented in MVAPICH2 version 2.0rc1 [11]. We
used HPIO benchmark [4] on a 4-node PC cluster system with one Intel Xeon
E3–1280V2 and 32 GiB memory in each node, connected by Mellanox InﬁniBand
FDR Host Channel Adapter through an InﬁniBand FDR non-intelligent switch,
Mellanox SX6005. The collective write operation for noncontiguous access pat-
terns was evaluated on Lustre version 2.6.54 conﬁgured with 1 meta data server
(MDS) and 2 object storage servers (OSSs) with two OSTs each via Inﬁni-
Band FDR links. Every server system consisted of one Intel Xeon E3-1270V3
and 32 GiB memory. It is noted that the 2 OSTs of each OSS were conﬁgured by
iSCSI storage devices built on RAID 10 storages of a dual Intel Xeon E5-2640V2
Linux server with 64 GiB memory, which was connected to the same InﬁniBand
switch via an InﬁniBand FDR link.

286
Y. Tsujita et al.
Fig. 4. Noncontiguous access pattern generation by HPIO benchmark
We deployed 4 MPI processes on each node to utilize all CPU cores, and a
total of 16 MPI processes were executed on 4 nodes. The HPIO benchmark eval-
uated collective writes for noncontiguous access patterns by using three parame-
ters; region size, region space, and region count, as shown in Fig. 4. We speciﬁed
256 bytes in the data gap (region space) in each 11,744 bytes of data region per
process (region size); a data block ((region size + region space) × the number
of processes) of about 185 kbytes (= (11, 744 + 256) × 16) was formed by the
16 processes. The 16 MPI processes generated 48,000 data blocks and wrote
them in a single ﬁle of about 8.6 GiB in total in each run. The HPIO benchmark
reported mean throughput values through the 12 runs by excluding the highest
and lowest performance values.
Figure 5 shows the performance results in the IS IR case with two kinds
of MPI process layouts, bunch and scatter, available in MVAPICH2. This
ﬁgure shows the results obtained by the original case with one aggregator (ORIG,
agg n=1) and 4 aggregators (ORIG, agg n=4) per node in addition to the results
of our proposed scheme with 4 aggregators per node (RR, agg n=4) in each
process mapping pattern. The original case degraded its performance with an
increase in the number of aggregators from 1 to 4 processes per node because of
aggregator assignment mismatch. However, our proposed scheme with 4 aggrega-
tors per node outperformed the original case with one aggregator by about 25 %
because of performance improvements in the ﬁle read and data exchanges inside
the two-phase I/O. Such two-phase I/O internal behavior is discussed later.
Fig. 5. Obtained I/O throughput values in an IS IR data exchange pattern with two
kinds of MPI process layout

Striping Layout Aware Data Aggregation for High Performance I/O
287
Figure 6 shows the performance values obtained in the AtoAv case in the
same evaluation procedure done for the IS IR case. Our approach denoted as RR,
agg n=4 also outperformed the original layout cases using both 1 and 4 aggrega-
tors per node (ORIG, agg n=1 and ORIG, agg n=4, respectively) up to 30 %.
Fig. 6. I/O throughput values in an AtoAv data exchange pattern with two kinds of
MPI process layout
The above performance improvements came from the minimization in time
for both data exchanges and ﬁle I/O with the help of our striping layout oriented
aggregator assignment. Figure 7 shows the internal operation mean times of the
two-phase I/O at each MPI process in the IS IR case. We examined three major
operations, ﬁle read(read), data exchange(exch), and ﬁle write(write). Here we
measured each operation time at every MPI process independently and showed
the mean values of each MPI rank in Fig. 7.
Compared with Fig. 7(a) and (d), where we had one aggregator per node,
4 aggregators per node (Fig. 7(b) and (e)) led to increased operation times,
especially in data exchanges. However, our alternative aggregator assignment
drastically decreased data exchange times by about 1/4, as shown in Fig. 7(c),
and (f). It was considered that the original aggregator assignment shared a
network link connected to each node for 48 incoming data (12 incoming data ×
4 aggregators per node) through a network link in each node for data exchanges
at each striping round in the same way explained in Sect. 3. Our striping layout
oriented assignment minimized such network contention with the help of the
round-robin layout by reducing the number of incoming data to 1/4 (12 incoming
data through a network link in each node). Therefore roughly speaking, our
proposal minimized the amount of incoming data per node to 1/4 relative to the
original case with 4 aggregators per node. This minimization eﬀect minimized
the exchange time.
The same analysis was done for the AtoAv data exchange mode, as shown
in Fig. 8. Increases in the number of aggregators per node from one (Fig. 8(a)
or (d)) to 4 (Fig. 8(b) or (e)) in the original aggregator assignment scheme led
to decreases in the data exchange time (exch) because of the internal collec-
tive communication optimization of MVAPICH2. This minimization led to the

288
Y. Tsujita et al.
Fig. 7. Three major internal operation mean times of collective write with the two-
phase I/O using IS IR data exchanges in 2 MPI process layouts, bunch and scatter
Fig. 8. Three major internal operation mean times of collective write with two-phase
I/O using using AtoAv data exchanges in 2 MPI process layouts, bunch and scatter

Striping Layout Aware Data Aggregation for High Performance I/O
289
performance improvements observed in Fig. 6. In addition, our aﬃnity-aware
aggregator assignment minimized all the internal operation times at almost the
same level among MPI ranks. Then we achieved further performance improve-
ments, as shown in Fig. 6.
Overall, the striping layout aware aggregator assignment outperformed other
cases in both communication modes, IS IR and AtoAv. It was found that the
striping layout aware aggregator layout minimized the network contention in
both data exchanges among MPI processes and data accesses on a target Lustre
ﬁle system.
5
Related Work
Regarding performance improvement aspects for MPI-IO on a Lustre ﬁle system,
eﬀective aggregator assignment was also proposed in [2]. This work focuses on
aﬃnity-awareness with multiple aggregators per node. In addition, the scheme
of that work pays attention to the appointment of aggregation tasks to MPI
processes on diﬀerent CPU sockets. However, it does not consider striping layout
awareness in its implementation. On the other hand, our proposal focuses on the
aﬃnity of the striping layout in accessing a Lustre ﬁle system. Our striping
layout aware aggregator assignment can improve I/O performance in any kind
of MPI process layout.
Optimization in data aggregation was realized by analyzing the physical data
layout on a parallel ﬁle system [3] or by using a scientiﬁc dataset in structured
formats and a parallel ﬁle system data distribution [5]. These works are similar to
our work regarding ﬁle system’s data distribution awareness. The work focusing
on physical data layouts takes the physical data distribution pattern into account
in the redistribution phase among aggregators to improve the I/O performance
by eliminating data access contention. The other work focusing on scientiﬁc
dataset in structured formats and the parallel ﬁle system data analyzes the
data structure in a high-level user application layer. While our work focuses on
optimized aggregator layout by considering aﬃnity-awareness among computing
nodes and a target Lustre ﬁle system conﬁguration. High-level optimization in
the work focusing on scientiﬁc datasets may be useful for our implementation
if we address to tune application-oriented I/O operations using HDF5 [10] or
PnetCDF [7].
6
Summary
We have shown the performance advantages of our striping pattern-aware aggre-
gator assignment scheme as compared with the original ROMIO. Our proposed
scheme appointed aggregation tasks to MPI processes in a round-robin man-
ner across PC nodes, where the aggregator layout is suited to the striping lay-
out of a Lustre ﬁle system. We deployed the alternative aggregator layout in
two data exchange modes; the original data exchange mode using MPI Isend
and MPI Irecv and the newly implemented mode using MPI Alltoallv. The

290
Y. Tsujita et al.
proposed scheme outperformed the performance results of the original mode
in collective write operations for noncontiguous accesses when using the HPIO
benchmark. In the original data exchange mode, our scheme outperformed the
original ROMIO by about 25 %, while up to about an 30 % improvement was
achieved as compared with the original ROMIO in the newly implemented data
exchange mode.
As future work, we will extend the current implementation to cover PC nodes
equipped with multiple CPU sockets, where we will have aﬃnity-awareness in the
aggregator assignment to achieve higher I/O performance. Further performance
improvements in data exchanges with aﬃnity-awareness are also our future work
in order to attain higher scalability.
Acknowledgment. Part of this research work was supported by JSPS KAKENHI
Grant Number 25330148. The authors would like to thank members of the System
Software Research Team at RIKEN AICS for their useful comments.
References
1. Lustre ADIO collective write driver. Technical report, Lustre (September 2008)
2. Cha, K., Maeng, S.: An eﬃcient I/O aggregator assignment scheme for collec-
tive I/O considering processor aﬃnity. In: Sheu, J., Wang, C. (eds.) 2011 Inter-
national Conference on Parallel Processing Workshops, ICPPW 2011, September
13–16 2011, pp. 380–388. IEEE Computer Society, Taipei, Taiwan (2011)
3. Chen, Y., Sun, X.H., Thakur, R., Roth, P.C., Gropp, W.D.: LACIO: a new col-
lective I/O strategy for parallel I/O systems. In: Proceedings of the 25th IEEE
International Parallel and Distributed Processing Symposium (IPDPS 2011), pp.
794–804. IEEE Computer Society (2011)
4. Ching, A., Choudhary, A., keng Liao, W., Ward, L., Pundit, N.: Evaluating I/O
characteristics and methods for storing structured scientiﬁc data. In: Proceedings
20th IEEE International Parallel and Distributed Processing Symposium, p. 49.
IEEE Computer Society (2006)
5. Liu, J., Crysler, B., Lu, Y., Chen, Y.: Locality-driven high-level i/o aggregation
for processing scientiﬁc datasets. In: IEEE International Conference on BigData
2013, pp. 103–111. IEEE (2013)
6. Lustre. http://wiki.lustre.org/index.php/Main Page
7. Parallel netCDF. http://cucis.ece.northwestern.edu/projects/PnetCDF/
8. Thakur, R., Gropp, W., Lusk, E.: An abstract-device interface for implementing
portable parallel-I/O interfaces. In: Proceedings of the Sixth Symposium on the
Frontiers of Massively Parallel Computation, pp. 180–187 (1996)
9. Thakur, R., Gropp, W., Lusk, E.: On implementing MPI-IO portably and with
high performance. In: Proceedings of the Sixth Workshop on Input/Output in
Parallel and Distributed Systems, pp. 23–32 (1999)
10. The National Center for Supercomputing Applications. http://hdf.ncsa.uiuc.edu/
HDF5/
11. The Ohio State University: MVAPICH: MPI over InﬁniBand, 10GigE/iWARP and
RoCE. http://mvapich.cse.ohio-state.edu/index.shtml

Hop: Elastic Consistency for Exascale
Data Stores
Latchesar Ionkov(B) and Michael Lang
Los Alamos National Laboratory,
Los Alamos, NM 87544, USA
lionkov@lanl.gov
Abstract. Distributed key-value stores are a scalable alternative for
relational databases and ﬁle systems. Diﬀerent systems try to provide
the right balance between scalability, consistency and reliability for a
certain category of applications. Yet many applications don’t have a sin-
gle static set of requirements for all the data they create and use. In order
to meet diﬀerent requirements, the applications end up using multiple
data stores, each with its own interface, or suﬀer the unnecessary per-
formance degradation. This paper describes Hop, our attempt to deﬁne
an interface that allows speciﬁcation of diﬀerent consistency and repli-
cation levels, and by allowing the assembly of diﬀerent implementations
of the interface, provides the right balance of consistency, scalability and
availability for diﬀerent parts of application data. We describe the Hop
interface and evaluate some of the implementations we created, as well as
how they can be combined to provide ﬂexible distributed deployments.
1
Introduction
In recent years many applications try to replace the standard data stores, like
relational databases and ﬁle systems with distributed key-value stores. The main
reason for the change is that the new data stores provide diﬀerent balance
between scalability, reliability and consistency, usually focusing more on the scal-
ability, than reliability and consistency. There is a large variety of storage systems
available, each with diﬀerent functionality, optimizations and guarantees. Most
of these systems are designed for particular application or workload, and there
is rarely a single system, that ﬁts perfectly for other applications, with slightly
diﬀerent requirements.
Our experience with implementing system services and scientiﬁc simulation
applications shows that even within a single application, the consistency and
performance requirements vary widely across subsets of the data sets, or for the
same subsets during diﬀerent stages of the application execution. For example,
a scientiﬁc simulation doesn’t require strong data consistency within the dis-
tributed store while performing a checkpoint (dumping its state to stable stor-
age), performance is more important. Analytic and visualization applications,
LA-UR-13-27636
The rights of this work are transferred to the extent transferable according to title
17 § 105 U.S.C.
c
⃝Springer International Publishing Switzerland 2015 (outside the US)
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 291–306, 2015.
DOI: 10.1007/978-3-319-20119-1 22

292
L. Ionkov and M. Lang
on other hand, require the same data set to be in consistent state before they
can process it Depending on the way the application partitions the overall work,
consistency can be relaxed based on regions of the data sets that are shared by
groups of nodes.
Same diverse requirements apply for data replication. In most cases, it is
beneﬁcial for the overall performance if the replicas don’t share the same physical
location (rack, data center, etc.) which usually leads to higher latency and lower
performance. In some cases, for instance when the data is temporary and all
users are clustered physically, it makes more sense if the replicas are close. For
example, there is no reason to keep the list of which nodes within a rack are
powered-on replicated outside the rack itself. Power or network failure to the
whole rack will make the data unavailable, but as the resources the list describes
are unavailable, that loss is irrelevant.
Distributed data stores are currently being evaluated for many exascale tasks
where centralized services have reached the limits of scalability. Global ser-
vices such as namespaces, service registration, provisioning, monitoring, job and
resource management, I/O forwarding, fabric management; runtime systems sup-
porting newer distributed programming models all need distributed data services
at exascale. In the application space these stores are being evaluated for static
and dynamic data tables, such as Equation of State (EOS) for multi physics,
and as a general way of dealing with asynchronous access to distributed data
structures. Hop is a robust, scalable class of solutions designed for these exascale
requirements.
The main goal of the Hop project is to try to deﬁne an interface for data
access that can be easily used by application and middle-ware developers and
implemented by data stores. It should allow data store developers to implement
and express various levels of consistency and reliability. The application develop-
ers should be able to express the consistency and data replication requirements
for particular parts of the data sets and phases of the application. Middle-ware
developers should be able to write software that optimizes for particular work-
loads without sacriﬁcing the requirements deﬁned by the application.
The Hop interface allows users of a data store to deﬁne the level of consistency
they need while reading the data. The choice varies from “don’t care” to “not less
recent than” to strong consistency. It allows formation of consistency domains
where a set of application instances ensure mutual consistency, without paying
the performance degradation for strong consistency outside the domain.
Each data entry in Hop is assigned numeric version value, which can be used
to deﬁne the freshness of the entry’s value. The entry’s version is monotoni-
cally increasing each time when its value is updated. When retrieving data, the
application can specify version value, and will be guaranteed to receive data
that is more recent that the speciﬁed version. All update operations return the
new version value. If an instance that updated some data needs to ensure that
another instance will see its updates, it needs to pass its version value to the
other instance, which in turn uses that value when reading the entry. There is a
special value, Newest that forces strong consistency among all instances. Clients
can also use future version values as a mechanism to wait until the entry is

Hop: Elastic Consistency for Exascale Data Stores
293
modiﬁed. This feature is useful in cases when it may take some time until updates
are propagated across multi-layered systems as well as for implementing pub-
lish/subscribe features on top of Hop.
Hop also deﬁnes a set of atomic operations on a single data entry, improving
the usability of a distributed store without sacriﬁcing the performance. The
atomic operations are serialized at the node, responsible for the entry, thus
ensuring their correct behavior within the distributed store.
Unlike the commonly used data stores, where clients use a single monolithic
service, we envision that Hop will be used similarly to ﬁle systems, where each
application will use multiple Hop implementations, each providing diﬀerent func-
tionality, and only some of them used for persistently storing the data to disks.
Custom Hop implementations may include caching middle-ware suitable for par-
ticular applications, aggregation of data entries, etc.
As part of the project, we developed a number of simple Hop implementations
that provide useful functionality and can be used to assemble more complicated
conﬁgurations that can be used for real-world installations. Our caching service
provides support for local and cooperative caching. It also implements a novel
concept of consistency domains that allow clients to easily group together and
ensure consistency for a subset of the key-value store entries.
The project is novel in that it provides a minimal, easy to implement API,
as well as a set of basic building blocks from which to compose larger services
based on key-value stores. It allows deﬁnition of ﬂexible coherency domains for
subsets of the data. Other key contributions are : conﬁgurable consistency and
replication of individual elements, deﬁnition of entries that provide side-eﬀects
upon access, support for publish/subscribe over key/value store interface, and a
unique aggregation service that redirects based on a simple preﬁx key.
2
Related Work
Distributed key-value store is an important system service, which serves requests
very fast. Memcached [5] is a popular service that stores key/values pairs in
RAM and is designed to serve as a caching layer between applications and
persistent data stores like databases. Dynamo [2] is a highly available key-
value storage implemented by Amazon to provide users with a reliable resource.
Cassandra [9] is a distributed storage system developed by Facebook to satisfy
the requirements of the Inbox Search problem. It provides SQL-like query lan-
guage and failure recovery. ZHT [11] is a zero-hop distributed hash table for
managing the parallel ﬁle system meta-data, and serves as a building block for
future distributed system services. Chord [14] is a distributed protocol, which
deﬁnes how servers communicate with other and how data is distributed among
all the servers. As part of the Hop evaluation, we implemented both Chord-
like and ZHT-like schemes providing choice to the services so they can select a
method that best ﬁts its requirements.
There are systems that use some form of versioning to manage consistency.
Project Voldemort [4] is an advanced key-value store that provides multi-version

294
L. Ionkov and M. Lang
concurrency control for updates. In order to get up-to-date view, the application
needs to read data from the majority of the replicas. Similarly to Voldemort,
Riak is a data store that uses MVCC and vector clocks to order updates. It also
provides tunable consistency at read and write operations by specifying how
many replicas must respond to operation.
Comet [6] is a distributed data storage system that allows executable code
(handlers) to be attached to the stored entries, making them “active”. The han-
dlers may run on speciﬁc operations (get, set, etc.), or on timers, at the place
where the entry is stored.
COPS [12] tries to improve the eventual consistency, usually used in dis-
tributed systems, by presenting a scalable causal consistency with convergent
conﬂict handling.
OceanStore [8] is an infrastructure that can be used to connect data stores
across wide-area network and provide provide clients with ﬂexible consistency
guarantees. It uses replicas as caching capabilities to provide fail-over and to
improve performance. It uses entries’ versions to to keep track of the freshness
of the data in the replicas and caches. OceanStore also deﬁnes algorithms that
are used for data placement. The consistency level is deﬁned per session and if
client has diﬀerent consistency requirements for groups of data, it needs to use
multiple sessions.
3
Design
The main objective of the project is to deﬁne an interface for storing and retrieving
data values in distributed systems, while allowing the data users to specify diverse
consistency and replication levels for the diﬀerent types of data they store.
The data store and retrieval requirements for the applications are not uni-
form, and in many cases they vary even for diﬀerent data entries within an
application. For example, in some cases fast data retrieval is more important
than the data freshness. Some of the data is relevant only to the clients in a
location (i.e. server rack) and don’t need to be replicated outside that loca-
tion. There are cases when consistency is required for groups of clients while the
consistency restrictions across groups can be relaxed.
Generally, a balance is necessary between the number of operations deﬁned in
an API and the complexity of the parameters they accept. Small set of operations
makes it easier for clients to use it. It also makes the implementation of simple
services that support it more straightforward. For complex functionality, though,
a small number of operations usually leads to a complex parameter space with
too many values with special semantics. In Hop’s design we tried to create what
we believe is the right balance between the number of operations and parameter
idiosyncrasies.
In order to keep the interface simple, Hop deﬁnes a single type of keys (string)
and a single type of values (byte array). Our evaluation in Sect. 4 shows that
these restrictions still allow reasonable level of ﬂexibility.
We intentionally omitted support for partial entry retrieval and update.
We believe that large monolithic entries allow applications to encapsulate their
data in “ﬁle” formats that leave oﬀimportant metadata information and make it

Hop: Elastic Consistency for Exascale Data Stores
295
harder for data to be shared across applications. Instead of using a singly entry
with all data in it, we encourage developers to adopt more ﬁne-grained approach
to data storage. The atomic operation Replace provides support for partial data
update, which we hope is not going to be abused by the developers.
3.1
Hop Operations
The Hop interface deﬁnes 6 operations:
Create. Creates a new entry for a key and sets its initial value.
Remove. Removes an entry associated with a key.
Get. Retrieves the value, associated to a key.
Set. Updates the value, associated with a key.
TestSet. Compares the value of an entry and sets it to a new value if the
comparison is true.
Atomic. Performs an atomic operation on an entry.
All Hop operations receive the name (key) of the entry as a parameter. Other
approaches, for example assigning a temporary numeric value to an entry (like
the ﬁle descriptors in the POSIX API) have some advantages, like performing
the name look-up and permission checking once at the beginning of a set of
operations, smaller message overhead, etc. That approach implies state kept on
the servers for each client. In a distributed system, these beneﬁts are smaller
than the disadvantages of keeping (and replicating in case of a failure) the state.
Many of the key-value stores implicitly create entries, when a value for a key is
set ﬁrst. We chose to deﬁne an explicit create operation. The main reason is
to allow the users to list speciﬁc requirements for the entry: how many replicas
should be supported, the location of the replicas, etc. Additionally, we prefer to
preserve symmetry: an interface with a function that deletes an entry is more
complete if it also explicitly creates it.
The interface doesn’t deﬁne any speciﬁc values for the entry’s ﬂags, the
description what parameters are supported is left for the speciﬁc implemen-
tations.
In distributed systems, many problems arise when multiple clients concur-
rently modify an entry’s value. Even strong consistency guarantees don’t allevi-
ate all the complications. For example, there is no guarantee that incrementing a
value is going to produce the correct results. The most common solutions for the
problems are support for transactions, or atomic operations. We chose to provide
the latter, mostly because distributed transactions are complex to implement,
don’t scale well and complicate the key/value store interface. Transactions oﬀer
many beneﬁts, and we plan to explore them in the future based on the atomic
primitives we already deﬁned.
All atomic operations serialize the access to an entry. The value of an entry
can’t be modiﬁed or retrieved while an atomic operation is being processed.
The most commonly implemented atomic operation is test-and-set. It compares
the value of an entry to a speciﬁed value, and if both match, sets it to a new

296
L. Ionkov and M. Lang
value. Test-and-set returns the value of the entry, as set at the end of the oper-
ation. If the two values don’t match, the user receives the current entry’s value,
otherwise the user receives the new value.
In a distributed conﬁguration, where clients and servers might be located on
diﬀerent nodes with high latency network connecting them, test-and-set might
be an ineﬃcient method to atomically modify an entry. While a client retrieves
the value, modiﬁes it and performs test-and-set, another client may change the
entry, forcing the ﬁrst client to loop multiple times until it succeeds. To improve
the latency, we deﬁne a set of commonly used operations that are atomically
executed by the Hop implementation at the location where the entry is stored.
Additional atomic operations can be built on these.
Add. Atomically adds a number to the entry’s value.
Sub. Atomically subtracts a number from the entry’s value.
BitSet. Atomically sets to 1 one bit in the entry’s value that was previously
set to 0. Returns the new value as well as the address of the bit that was
modiﬁed.
BitClear. Atomically sets to 0 one bit in the entry’s value that was previously
set to 1. Returns the new value as well as the address of the bit that was
modiﬁed.
Append. Atomically appends the value speciﬁed in the operation to the entry’s
current value.
Remove. Atomically removes from the entry’s current value all subarrays that
match the value speciﬁed in the operation.
Replace. Atomically replaces all subarrays from the entry’s current value that
match the ﬁrst value speciﬁed in the operation with the second value.
Although the Hop values are arrays of bytes, the arithmetic atomic operations
assume the arrays to be big-endian encoding of 8-, 16-, 32-, or 64-bit integers
(depending on the size of the array). The Hop implementations can deﬁne addi-
tional atomic operations.
3.2
Entry Versions
The Hop interface assigns a 64-bit version value to each data entry. The version
is increased every time the value of the entry is updated. When data is retrieved,
in addition to the value, the user receives its current version. The application
can specify what version of the value it requires. The Hop service cannot return
values associated with versions older than the speciﬁed one. They might, however
return more recent values. If the value the service is not recent enough, it is
expected to wait until the entry reaches the speciﬁed version. Caching services
may use requests for newer versions as triggers for updating the cached entries.
In distributed systems it is possible for requests to arrive in a diﬀerent order to
the servers, therefore versions ensure later requests wait for the earlier ones.
Valid entry version have values between 1 and 263. The rest of the values
can be used to deﬁne some special version values that ﬁne-tune the semantics
of the data retrieval. The special values that all Hop services are required to
support are:

Hop: Elastic Consistency for Exascale Data Stores
297
Any. Retrieve any value, no matter how fresh.
Newest. Retrieve the newest value. Instructs all intermediary services to ignore
their stored entries and consult the authoritative service that contains the
entry.
Wait. Ignore all intermediaries, wait until the entry is modiﬁed once, and return
the new value. Can be used for implementation of publish/subscribe mech-
anisms.
Uncommitted. Return values even if they are not yet replicated as required.
All operations that return an entry’s value also return its version. Successful
operations are required to return a valid version (i.e. between 1 and 231). If an
operation returns 0 for a version, the client is expected to retry the operation
(similarly to EAGAIN error code in Unix). In a distributed environment there
might be cases when successful completion of the operation is hard, or even
impossible to achieve, but returning an error would be the wrong outcome. For
example, if an instance in distributed service fails and its responsibilities are
assigned to another instance, it is easier to abandon the currently pending oper-
ations and ask the clients to retry using the updated conﬁguration.
Instead of keeping only the latest version of an entry, Hop allows the imple-
mentations to keep as many older versions as they may see ﬁt. However, Hop
would have to be extended to provide any operations for version management.
3.3
Entry Names and Values
Entries’ names in Hop are variable-sized UTF-8 encoded strings up to 65535
bytes long. Like most key-value stores, Hop supports a ﬂat namespace. Because
Hop doesn’t support key enumeration (see Sect. 4.1 though), there is no reason
to group keys in hierarchical namespaces. Flat namespaces are also easier to
partition in distributed implementations.
An entry’s value is an array of bytes, with maximum size of 4 GB. The Hop
interface doesn’t support partial retrieval of values, and there is no practical
reason for bigger values if they are stored and retrieved over the network.
3.4
Entry Side-Eﬀects
Hop implementations can deﬁne some, or all of the entries as special entries that
have side eﬀects or produce results based on the server or client conﬁgurations.
One example of that behavior is the #/keys:regular-expression entry that will
be deﬁned in Sect. 4.1.
4
Implementations and Evaluation
The Hop interface is intentionally kept simple. We envision multiple services
implementing it assembled from hop building block services. Real-world instal-
lations using a combination of multiple implementations to provide the required
levels of consistency, scalability, and reliability. Currently we have created 7 Hop

298
L. Ionkov and M. Lang
implementations, the more complicated ones using the basic ones as building
blocks. In this section we will describe these implementations as examples of how
the basic Hop interface can be used and to assemble complex real-world services.
4.1
General Functionality and Guidelines
All of the implementations below provide extra functionality not deﬁned in the
basic Hop interface that is useful in building system services.
Local Entries. Entries with names starting with #/ are considered local and
may return diﬀerent values depending to which node in a distributed system
the client connects. Most of these entries are used by the implementation itself
to maintain its basic functionality. All our Hop implementations provide #/id
entry. It is immutable and provides the name of the Hop implementation, as well
as additional information about its conﬁguration.
Name Lookup. The basic Hop interface doesn’t provide functionality that
allows checking the names of existing entries. Our Hop implementations allow
name lookup using two special local entries: #/keys and #/keys:regexp. The
value of #/keys contains the names of all local entries, #/keys:regular-expression
is a special dynamic entry that returns all keys that match the speciﬁed regular
expression. For example, getting the value of #keys:foo.*bar will return all
keys, with preﬁx foo and suﬃx bar.
The standard rules for Hop versions also apply for the special keys. For
example, the user can watch a Hop instance for newly created entries by calling
the Get operation on #/keys with version value Wait. The operation will hang
until a new entry is created.
If the service has a lot entries, waiting on #/keys is impractical, because it
will transfer huge amounts of data back to the client. For that reason, we deﬁned
another special entry, #/keynum that returns the number of keys present in a
Hop instance. Waiting on it will also return when an entry is created or removed.
In our future implementations we are planning to use dynamic key names for
other purposes, like returning partial values, debugging, etc.
Distributed Implementations.
Instead of deﬁning additional protocols,
we use the Hop interface and special local entries to implement the commu-
nication between the instances in a distributed service. That serves two main
purposes: makes the coding of distributed services fast and simple, and tests if
the Hop interface is ﬂexible enough for real-world applications. We believe that
the inability of using the Hop interface for our own distributed implementations
would be an argument for its incompleteness and/or ineﬃciency.
4.2
Basic Building Blocks
Rmt. The standard Hop interface doesn’t deﬁne or provide any support for
networked deployment of Hop implementations. The simplicity of the standard

Hop: Elastic Consistency for Exascale Data Stores
299
Hop interface makes it easier to implement and doesn’t tie it to speciﬁc network
protocol or framework.
Rmt is a package that provides networked client-server support for Hop.
It consists of a tightly integrated pair: rmt.srv and rmt.clnt. Rmt.clnt is a Hop
implementation that serializes all Hop operations and sends them over a network
to rmt.srv. Rmt.srv receives a Hop implementation as a parameter and makes it
available over a network connection. When it receives a message, it deserializes
it, calls the appropriate Hop operation, serializes the result and sends it back.
Rmt supports TCP and Inﬁniband Verbs protocols. It allows up to 65536
simultaneous requests for a client-server pair. The requests are multiplexed and
the responses are demultiplexed over a single connection.
Fig. 1. Example of single-process and
remote Hop deployment
Fig. 2. Example of MHop deployment
Figure 1 shows deployment of a Hop interface can be made within a single
process, where the Hop implementation and the application are linked together
in a single binary, or over the network where rmt.clnt serves as a proxy to the
remotely deployed Hop implementation. No changes to the Hop implementation
is required in order to make it available over the network.
KHop: Hop Reference Implementation. KHop implements a key-value
store in RAM. It is a basic building block that we use in other Hop implemen-
tations. KHop implements all required functionality and can be regarded as a
reference Hop implementation.
In addition to the default storage in RAM, KHop allows its users to provide
per-entry behavior for the entries they create. If the entry objects (created out-
side the Hop interface operations) implement all or some of the Hop operations,
the custom operations are called when required. This functionality allows easy
implementation of special entries with dynamic content and/or side eﬀects.
MHop: Hop Aggregation Service. MHop is a proxy implementation that
combines the entries from multiple Hop instances into a single namespace. It is
somewhat equivalent to the Unix Virtual File System (VFS) for ﬁle systems.

300
L. Ionkov and M. Lang
MHop allows a Hop instance to be “mounted” to a preﬁx, passing all operations
to keys that start with the preﬁx to that instance. MHop uses trie structures
to minimize the impact of the additional key processing on the performance.
Figure 2 shows how MHop can be used to redirect operations to three Hop
instances: all operations for keys starting with /ns are redirected to Hop2, the
keys starting with /user are redirected to Hop3, all operations for other keys
are redirected to the default Hop1 instance.
The purpose of MHop is to provide a centralized access to all available Hop
instances and hide the details of their deployment from the applications.
CHop: Distributed Caching Service. One of the main reasons to intro-
duce version numbers in Hop was to allow trading strict consistency for perfor-
mance. One of the ways to improve performance is to use some caching services.
We implemented simple distributed caching that utilizes the Hop features.
Upon creation of CHop, the user provides a Hop instance to be cached, and
restrictions on the memory and number of entries the cache should use. The
CHop instance can also join a CHop group of instances.
Each instance of CHop keeps local cache of the most recently used entries.
It uses the version number provided to the Get operation to decide whether to
return the locally stored value, or update it from the original Hop instance. All
Set operations also update the local cache.
In addition to the local cache, CHop allows deﬁnition of consistency domains.
All applications that use a speciﬁc consistency domain are guaranteed to see the
updates that other applications within the domain make, regardless of the ver-
sion number. There are no guarantees when updates made outside of the consis-
tency domain would be available. Using speciﬁc (more recent than the cache or
Newest) version while retrieving a value will always retrieve the speciﬁed version
from the original Hop instance, and ensure that from that moment on, no other
application within the consistency domain will receive an older value.
Figure 3 shows an example of a group of six CHop instances, with two con-
sistency domains. The applications using C1 and C3, or C4 and C6 will see
consistent views of the entries, provided by the Hop instance, while C2 and C5
may see inconsistencies.
The deﬁnition of consistency domains in CHop is implemented by using spe-
cial preﬁx #/cache/domain-name/. By prepending the preﬁx while accessing
the keys, multiple users of CHop instances can ensure that they will get consis-
tent view of the values. For example, applications A and B can use consistent
domain bar to ensure consistent view of key foo by accessing entries with names
#/cache/bar/foo. For example, for a resource manager job information(process,
memory layout) would be consistent within the nodes of a job but not consistent
to jobs on other nodes.
In addition to the consistency domain preﬁxes, CHop allows the users to
create a special Hop instance that hides the preﬁxes and uses the consistency
domain for all entries simplifying implementation.

Hop: Elastic Consistency for Exascale Data Stores
301
Fig. 3. Example of CHop deployment with 6 instances and 2 consistency domains
4.3
Examples of Using the Building Blocks
The Hop implementations described in this section utilize the basic Hop building
blocks to create services that can be used in real deployments.
SHop. SHop builds on KHop by adding special entries that are standard for
deployable Hop services. SHop deﬁnes the following special entries:
#/id Hop implementation identiﬁcation.
#/keys Returns all keys in the data store.
#/keys:regexp Returns all keys that match the speciﬁed regular expression.
#/keynum Returns the number of keys in the data store.
SHop also uses rmt.srv to provide single-instance data store, available over the
network.
D2Hop. D2Hop is a distributed Hop implementation that partitions the key
space across multiple D2Hop instances, running on diﬀerent nodes. D2Hop calcu-
lates a 32-bit hash value for each key and redirects the operation to the appro-
priate instance responsible to that value. Each instance is handles a range of
hash values. Currently D2Hop supports FNV-1a [3] and Adler-32 hash func-
tions. D2Hop implementation is built on top of the KHop and Rmt building
blocks.
Although D2Hop handles most instance failures, it doesn’t replicate the
entries’ data across multiple instances and failures may lead to data loss, this is
currently being developed.
The D2Hop instances belong to one of the three categories: leader, followers,
or clients. The leader is responsible for keeping track of the followers health and
handles joining and leaving the service. Failure of a leader causes failure of the
whole service. Followers are responsible for handling the operations for part of
the keys’ namespace as well as redirecting the operations for the rest of the keys
to the appropriate instance. Each follower is connected to all other followers and

302
L. Ionkov and M. Lang
the leader. The clients are connected to some, or all followers and redirect the
operations to the appropriate instance.
The maintenance of the D2Hop infrastructure is implemented by using Hop
operations on a special entry #/conf. The content of the entry is a string, with
ﬁrst line describing the leader, and following lines for each of the followers. A line
contains the instance network address as well as a list of hash ranges the instance
is responsible for. To join the service, the instance performs atomic Append with
its address to the leader’s #/conf entry. The leader updates its conﬁguration
and returns the new content of the entry, that includes the new instance and
what keys it is responsible for. Each follower and client keep track of version
of the conﬁguration they have and constantly tries to retrieve the next version.
Once an instance joins or leaves the service, they receive the new conﬁguration
and its updated version.
D2Hop can run any Hop implementation as distributed service, its instances
simply redirect the calls they receive to the Hop instance, speciﬁed on
initialization.
Although D2Hop doesn’t handle failures, it can be used for deployment where
failures are unlikely and serves as a good example how to implement distributed
Hop implementations.
ChordHop . ChordHop is a Hop implementation that uses the Chord [14] dis-
tributed service for partitioning the key space. It implements Chord’s strong
stabilization algorithm. CHordHop is built on top of KHop and Rmt.
Similarly to D2Hop, the Chord maintenance protocol is implemented as Hop
operations on special entries. Each Chord node deﬁnes the following entries:
#/chord/successor:ID The value of the entry is the address of the Chord node
that is successor of the speciﬁed ID. The current node might contact other
nodes to ﬁnd out the successor.
#/chord/predecessor Returns the address of the predecessor of the Chord.
TestSet operation on the entry notiﬁes the node that another node might
be its predecessor while returning the old predecessor.
#/chord/finger Returns the current node’s ﬁnger table. Used for debugging
only.
#/chord/ring Returns description of Chord’s ring. Used for debugging only.
ChordHop doesn’t replicate the entries, so even though Chord stays connected
when failures occur, data might be lost. In the future we plan to add replication.
ChordHop clients act similarly to the Chord nodes, maintain correct ﬁnger
table, etc. But unlike the Chord nodes, they don’t join the Chord ring.
5
Results
We evaluated the performance of some of our Hop implementations at small
scale and compared one of them to two other key-value stores. All tests were

Hop: Elastic Consistency for Exascale Data Stores
303
run on a 128-node cluster with Inﬁniband QDR interconnect. Each node has 16
cores and 32 GB RAM.
To evaluate the performance, we created a simple benchmark that executes
random key-value store operation on randomly generated keys and random value
sizes. The probability for each type of operation is weighted, giving higher chance
of retrieval operations over the ones that create, remove or modify an entry. For
the Hop centric tests (Figs. 4 and 6), 55 % of the operations are Get, 35 % are
Set, 5 % are Create, 4 % are Remove, 3 % are TestSet, and 3 % are Atomic.
For the comparison with other key-value stores (Fig. 5, we removed the atomic
operations and had tested with 60 % Get operations, 30 % Set, 5 % Create, and
5 % Remove.
We ran one key-value store server and 16 clients on each node.
Fig. 4. Performance of D2Hop and Chord
for diﬀerent value sizes
Fig. 5. Performance of Memcached, ZHT,
and D2Hop
We run the benchmark for D2Hop and ChordHop implementations, scaling
the number of key-value store nodes from 8 to 128. We run the tests for two
sizes of the entry values – from 16 to 64 bytes, and from 24 KB to 32 KB. The
tests run using the Inﬁniband Verbs API. Figure 4 shows the results from these
tests. As expected, the D2Hop implementation scaled better than Chord due to
the additional hops for key look-up required for Chord. Runs with smaller sized
entries were latency bound and delivered much worse bandwidth values.
The second test we run compared the D2Hop implementation to some of the
other key-value stores that we were able to run on the cluster. We compared
our Hop implementation to Memcached [5], Cassandra and ZHT [11]. We used
TCP over IB for running this set of tests, because Memcached, Casasandra and
ZHT don’t support running directly on Inﬁniband. We used value sizes between
24 KB and 32 KB for these tests. Both Memcached and ZHT were conﬁgured to
disable data replication. ZHT was set to keep the data in RAM instead of storing
it to disk. Cassandra was conﬁgured to store its data to ramdisk. We varied the

304
L. Ionkov and M. Lang
number of servers handling constant number of clients, running on 128 nodes.
Figure 5 shows the performance of the three key-value stores. Although D2Hop is
not optimized for performance, it slightly outperforms Memcached. Cassandra’s
performance suﬀers because even though it doesn’t store its data to real disks, it
performs all operations (like creating commit logs, merging data ﬁles, etc.) that
are required for persistence of the data.
Fig. 6. Bandwidth for various Hop conﬁgurations for diﬀerent key space sizes
The last test we run compared the performance of more complex conﬁgura-
tions of Hop implementations, using combinations of some of the basic building
blocks as well as D2Hop and ChordHop. The goal of the tests was to inves-
tigate how the utilization of caching with CHop, with and without consistency
domains will aﬀect the overall performance of the key-value store. The tests were
run on 64 nodes, with value sizes between 24 KB and 32 KB. We compared the
performance of running D2Hop and Chord directly (Chord, D2Hop lines) with
running them in combination with CHop with consistency domains (CHord+D,
D2Hop+D) and without consistency domains (Chord+N, D2Hop+N). When
consistency domains are used, we create 8 domains, each with 8 nodes as mem-
bers. As the eﬃciency of the cache depends on the key space size, we ran the tests
for three diﬀerent sizes of the key space –256 keys, 64 K keys and 16 M keys. Half
of the retrieval operations specify Any as version (making them candidates for
being served from the CHop cache), while the other half speciﬁes Newest ensur-
ing that the operation reaches D2Hop or ChordHop. Figure 6 shows the results
of the tests. For the small number of keys (256), all values can ﬁt in the local
cache, which makes the CHop without consistency domains best performer when
request latency is not very high (as with D2Hop). For larger number of keys,
as well as stores with higher latency (as ChordHop), the cooperative caching of
CHop’s consistency domains allow higher cache hit rate within the domain and
result in better performance.

Hop: Elastic Consistency for Exascale Data Stores
305
6
Future Work
The most important missing functionality is proper replication for instance fail-
ures. Our plan is to implement some of the consensus algorithms (Paxos [10],
Raft [13]) as operations on special entries, then build the entry replication on top
of them. Similarly to the consistency domains, we are planning to provide sup-
port for replication domains that provide diﬀerent replication levels for diﬀerent
sets of entries.
We are also planning to use Hop to implement an instance of Hobbes [1]
Global Information Bus and related services. Hobbes is an operating system and
runtime (OS/R) framework for extreme-scale systems, funded by the Depart-
ment of Energy. The Global Information Bus is a software layer that provides
mechanisms for sharing status information needed by other components in the
OS/R. This include nodes status, locations and availability of various services,
performance data, etc.
Another extension of this project we are exploring is using special keys as a
way to read and write portions of complex entries. We are planning to create a
subset of the DRepl [7] language designed for describing dataset layouts and use
it to specify what subsets of the data entries are accessed.
7
Conclusion
The Hop interface allows the implementation and deployment of data store ser-
vices, middleware and clients with diﬀerent consistency and reliability require-
ments. The Hop project provides a set of building blocks as well as some examples
on how to use them to create real-world services. The simplicity of the protocol
and the support for special entry names allows it to be used as an RPC frame-
work for exascale system services as well as to provide custom representation of
data sets.
The services we implemented show that Hop is well ﬁt for various system
services implementations and can be used to hide some of the complexity of
exascale systems from the user applications.
Acknowledgments. This work was performed at the Ultrascale Systems Research
Center in the High Performance Computing Division at Los Alamos National Labora-
tory, and is supported by the U.S. Department of Energy DE-FC02-06ER25750.
References
1. Brightwell, R., Oldﬁeld, R., Maccabe, A.B., Bernholdt, D.E.: Hobbes: composition
and virtualization as the foundations of an extreme-scale OS/R. In: Proceedings
of the 3rd International Workshop on Runtime and Operating Systems for Super-
computers, ROSS 2013, pp. 2:1–2:8. ACM, New York (2013). http://doi.acm.org/
10.1145/2491661.2481427

306
L. Ionkov and M. Lang
2. DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin,
A., Sivasubramanian, S., Vosshall, P., Vogels, W.: Dynamo: amazon’s highly avail-
able key-value store. In: Proceedings of Twenty-First ACM SIGOPS Symposium on
Operating Systems Principles, SOSP 2007, pp. 205–220. ACM, New York (2007).
http://doi.acm.org/10.1145/1294261.1294281
3. Eastlake, D., Fowler, G., Vo, K.P., Noll, L.: The fnv non-cryptographic hash algo-
rithm (2012)
4. Feinberg, A.: Project voldemort: reliable distributed storage. In: Proceedings of
the 10th IEEE International Conference on Data Engineering (2011)
5. Fitzpatrick, B.: Distributed caching with memcached. Linux J. 2004(124), 5
(2004). http://dl.acm.org/citation.cfm?id=1012889.1012894
6. Geambasu, R., Levy, A.A., Kohno, T., Krishnamurthy, A., Levy, H.M.: Comet: an
active distributed key-value store. In: OSDI. pp. 323–336 (2010)
7. Ionkov, L., Lang, M., Maltzahn, C.: Drepl: optimizing access to application data
for analysis and visualization. In: 2013 IEEE 29th Symposium on Mass Storage
Systems and Technologies (MSST), pp. 1–11 (2013)
8. Kubiatowicz, J., Bindel, D., Chen, Y., Czerwinski, S., Eaton, P., Geels, D., Gum-
madi, R., Rhea, S., Weatherspoon, H., Weimer, W., et al.: Oceanstore: an architec-
ture for global-scale persistent storage. ACM Sigplan Not. 35(11), 190–201 (2000)
9. Lakshman, A., Malik, P.: Cassandra: a decentralized structured storage sys-
tem. SIGOPS Oper. Syst. Rev. 44, 35–40 (2010). http://doi.acm.org/10.1145/
1773912.1773922
10. Lamport, L.: Paxos made simple. ACM Sigact News 32(4), 18–25 (2001)
11. Li, T., Verma, R., Duan, X., Jin, H., Raicu, I.: Exploring distributed hash tables
in highend computing. SIGMETRICS Perform. Eval. Rev. 39(3), 128–130 (2011).
http://doi.acm.org/10.1145/2160803.2160880
12. Lloyd, W., Freedman, M.J., Kaminsky, M., Andersen, D.G.: Don’t settle for even-
tual: scalable causal consistency for wide-area storage with cops. In: Proceed-
ings of the Twenty-Third ACM Symposium on Operating Systems Principles,
pp. 401–416. ACM (2011)
13. Ongaro, D., Ousterhout, J.: In search of an understandable consensus algorithm
14. Stoica, I., Morris, R., Karger, D., Kaashoek, M.F., Balakrishnan, H.: Chord: a
scalable peer-to-peer lookup service for internet applications. In: ACM SIGCOMM
Computer Communication Review, vol. 31, pp. 149–160. ACM (2001)

Energy-Eﬃcient Data Processing Through
Data Sparsing with Artifacts
Pablo Graubner(B), Patrick Heckmann, and Bernd Freisleben
Department of Mathematics and Computer Science, University of Marburg,
Hans-Meerwein-Str. 6, 35032 Marburg, Germany
{graubner,pheckmann,freisleb}@informatik.uni-marburg.de
Abstract. Improving the energy eﬃciency of software running in a
data center is a challenging task. Several application-speciﬁc techniques,
such as energy-aware heuristics, controlled approximation and energy-
conserving I/O, have been proposed to tackle this problem. In this paper,
we introduce data sparsing with artifacts, a novel approach to increase
the energy eﬃciency of applications that are robust to input variations,
such as speech and image processing. Data sparsing with artifacts is
aimed at reducing the processing times and thus the energy eﬃciency of
such applications while preserving the quality of the results by replacing
a random subset of the original data with application-speciﬁc artifacts.
In contrast to previous work, the proposed approach introduces artifacts
at the data layer, without application layer modiﬁcations and with gen-
eral purpose hardware. Data sparsing with artifacts has been integrated
into a prototypical ﬁle system in userspace (FUSE) and the Hadoop
Distributed File System (HDFS). Experiments with MapReduce-based
face detection, face recognition and speech recognition algorithms show
promising energy savings of up to 10 % with moderate accuracy losses
for diﬀerent data sparsing rates and artifacts.
1
Introduction
Several companies that deal with vast amounts of user data are currently inves-
tigating methods to reduce their costs for energy, infrastructure, cooling and
power supply. For example, both the social-network provider Facebook1 and the
instant-messaging provider Snapchat2 have reported that their users share an
average of 350 million photos daily [22]. These companies use highly scalable
storage infrastructures that are based on hundreds of thousands of hard disks,
storing petabytes of user data. In 2013, Facebook spent 787 million kWh on their
data centers, each operating with a Power Usage Eﬀectiveness (PUE) between
1.06 and 1.13. The companies are highly interested in developing more eﬃcient
servers, storage and data center infrastructures. For instance, the Facebook engi-
neer Tal [12] has proposed to reduce the hard disk spin speed to a minimum,
1 https://www.facebook.com.
2 https://www.snapchat.com.
3 https://www.facebook.com/green/app 439663542812831.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 307–322, 2015.
DOI: 10.1007/978-3-319-20119-1 23

308
P. Graubner et al.
trading lower bandwidth and higher latencies for a reduced energy consumption
of the storage infrastructure.
In this paper, we introduce data sparsing with artifacts, a novel approach to
increase the energy eﬃciency of applications that are robust to input variations,
such as speech and image processing. Data sparsing with artifacts is aimed at
reducing the processing times and energy eﬃciency of such applications while
preserving the quality of the results by replacing a random subset of the original
data with application-speciﬁc artifacts. Similar to other approaches, data spars-
ing trades accuracy for energy eﬃciency. In contrast to previous approaches, data
sparsing with artifacts achieves energy savings with general purpose hardware
and without application layer modiﬁcations. To demonstrate the feasibility of
the proposed approach, a prototypical ﬁle system in userspace (FUSE) integrat-
ing data sparsing with artifacts is presented. Furthermore, an integration into
the Hadoop Distributed File System (HDFS) [2] is described. Finally, the app-
roach is evaluated by experimental results. MapReduce-based face recognition
and speech recognition algorithms show promising energy savings of up to 10 %
with moderate accuracy losses for diﬀerent data sparsing rates and types.
This paper is organized as follows. Section 2 introduces the concept of data
sparsing with artifacts. Section 3 describes the design and the implementation
of data sparsing with artifacts in the context of a FUSE ﬁle system and HDFS.
Section 4 presents experimental results. Section 5 discusses related work. Section 6
concludes the paper and outlines areas for future work.
2
Data Sparsing with Artifacts
Data sparsing with artifacts is inspired by dimensionality reduction methods
that are often used in text and image processing, aimed at performing compu-
tations in a lower-dimensional subspace while preserving the properties of the
higher-dimensional space. These methods are typically based on the theoreti-
cal work of Johnson and Lindenstrauss, who state - informally speaking - that
any set of points in Euclidean space can be embedded into a lower-dimensional
subspace such that all pairwise distances are preserved [10]. In particular, the
random projection method [18] piqued many researchers’ interest in the past.
This dimensionality reduction method uses a random matrix to project points
from a higher-dimensional to a lower-dimensional space. It has some interesting
properties: The matrix can be constructed in an inexpensive way, since all its
vectors are chosen randomly from the higher-dimensional space. Furthermore,
the projection matrix can be chosen as a sparse matrix such that all elements are
in {−1, 0, +1} with probabilities { 1
6, 2
3, 1
6} [18]. Empirical results on real-world
text and image data have indicated that this method preserves the similarities
between data vectors well, even with a moderate number of dimensions [7].
2.1
Data Sparsing
Random projections follow the approach of reducing the amount of data with
inexpensive preprocessing based on statistical properties of the problem space.

Energy-Eﬃcient Data Processing Through Data Sparsing with Artifacts
309
In a time of vast amounts of data processed in large data centers with high
energy consumption, this approach is quite intriguing. Therefore, we propose
data sparsing: Reducing the execution time and thus energy consumption by
processing a sparsed version of the original data. The term sparsing is borrowed
from the mathematical term of a sparse matrix, a matrix in which most elements
are zero. In data sparsing, data blocks are selected in a statistically independent
manner and with a standard normal distribution on the basis of an application-
dependent sparse rate with a given block size. It is similar to random sampling,
where a sample is selected from a statistical population such that assumptions
can be inferred from the sample to the population. However, in contrast to ran-
dom sampling, data sparsing can not only omit blocks with a given probability,
but also introduces a novel concept: Application-speciﬁc artifacts.
2.2
Artifacts
Artifacts are application-speciﬁc data blocks that replace blocks of the original
data stream with an application-speciﬁc sparse rate. In computer science, the
term artifact usually refers to an undesired alternation of data; in particular,
compression artifacts stand for a loss in clarity in compressed image or audio ﬁles.
In data sparsing, an artifact is a desired alternation of data within a data stream.
Its purpose is, informally speaking, to indicate to an application that a given
block should be excluded from the computation. With this mechanism, the result
of the execution can be approximated without modifying the implementation.
Thus, instead of sampling the data, i.e., omitting data blocks in order to
reduce the amount of data to be processed, artifacts are introduced into a data
stream. Artifacts are based on the following hypothesis: Applications such as
automatic speech recognition or face detection can usually exclude data sections
very fast, if a detection within this section is very unlikely. For example, a block
of zeros indicating a black rectangle within an image is excluded by a face detec-
tion algorithm at an early stage. Therefore, a block of zeros is an appropriate
application-speciﬁc artifact for a face detection algorithm. On the other hand, a
machine learning algorithm trained to detect single-colored shapes would exclude
blocks of subsequent black/white colors, but would produce false positives for
face detection-speciﬁc artifacts. Therefore, our basic hypothesis is: There are
application-speciﬁc artifacts for each of these applications, and data sparsing
with artifacts can decrease the execution time of this kind of applications.
2.3
Energy Eﬃciency
In general, the energy consumed by a system in the interval [t0, t1] is deﬁned as
the integral of power consumed in the domain of integration:
E(t) :=
 t1
t0
P(t) dt.
(1)
The energy consumed by a system can be reduced by either minimizing the
interval [t0, t1] (execution time) or P(t) for all t ∈[t0, t1] (power consumption).

310
P. Graubner et al.
Execution time and power consumption are often conﬂicting: For example, Chen
et al. [9] have examined I/O- vs. performance trade-oﬀs in MapReduce appli-
cations. Depending on the compression rate, the use of compression for data
transfers via a network is a way to conserve energy. But the compression rate
is unknown beforehand, and experiments show that some compressed images
increase energy consumption by increasing the use of the CPU.
In contrast, data sparsing with artifacts is applied on the block level, i.e., blocks
of data are manipulated without any knowledge about the data or preprocessing
the data. We assume that data sparsing with artifacts neither increases power con-
sumption nor decreases an application’s execution speed, such that the reduction
of execution time directly leads to reduced energy consumption.
On the other hand, active low-power modes, such as CPU dynamic voltage-
frequency scaling (DVFS) or spun-down disks, reduce power consumption at
moderate performance costs. Barroso et al. [4] stated that typical server hard-
ware is designed to operate most energy-eﬃciently at the maximum utilization
level, and ﬁne-grained active low-power modes can be used to gradually decrease
the dynamic power consumption when the utilization level decreases (energy-
proportionality) [5]. Conversely, disproportionaotely scaling down CPU voltage
and frequency of a server might lead to a reduced energy eﬃciency. Using data
sparsing with artifacts, execution time and thus energy consumption is reduced
at moderate accuracy costs. Conceptually, the use of active low-power modes for
hard disks, such as spinning down disks according to the sparse rate, would be a
complementary method for data sparsing. Unfortunately, typical hard discs only
provide inactive low-power modes, i.e., the device is not usable in this mode. On
the other hand, DVFS may be applied for additional power savings, according to
an application’s CPU utilization. In these cases, additional power savings would
be application-speciﬁc and independent of the data sparsing concept.
More energy savings could be achieved with modiﬁed disk controllers that
can take advantage of sparsed sectors. However, we did not measure a signiﬁcant
eﬀect of data sparsing on the power consumption of commodity hard disks and
solid state drives. This is due to the design of current disks that aim to achieve
high throughput by making use of disk buﬀers and inexpensive read-aheads.
2.4
Applications
In principle, loss-tolerant applications such as image processing, sensor data
processing, sound synthesis, as well as applications without unique answers, such
as web search or machine leaning can beneﬁt from data sparsing. As stated by
Esmaeilzadeh et al. [14], such applications can often tolerate lower precision or
accuracy trade-oﬀs. Furthermore, such applications process a growing amount
of data: In contrast to former times when the majority of data produced was
transactional data, the majority of data today shifts more and more to non-
transactional data, produced by all kinds of devices [19]. For example, social
network providers like Facebook or Snapchat store petabytes of images and
videos. An energy-conserving approach like data sparsing could help to improve
the energy eﬃciency of their data centers. In contrast, applications based on

Energy-Eﬃcient Data Processing Through Data Sparsing with Artifacts
311
Data Sparsing with Artifacts
width: 200px                                          height: 600px
resolution: 300dpi                            color-depth: 32bit
Face Detection Algorithm
Fig. 1. The principle of sparsed data processing.
transactional data such as log ﬁle analysis for accounting and statistics do usually
not tolerate modiﬁcations of the original data and cannot beneﬁt from data
sparsing with artifacts.
3
Data Sparsing in FUSE and HDFS
Data sparsing with artifacts relies on blocks of bytes, i.e., sequences of bytes of a
given length. This is due to the fact that the hardware architecture of servers is
usually organized into virtual memory pages, hard disk sectors, packet sizes etc.
If the block size is chosen as a multiple of a virtual memory page, data sparsing
with artifacts can make use of page-table mechanisms to sparse the data.
The basic principle of processing sparsed data is depicted in Fig. 1: A program
implementing a face detection algorithm is used to process a sparsed version of
an image ﬁle. The sparse rate and the block size are speciﬁed such that the
program is still able to process the image ﬁle successfully. The image meta-data,
which is sensitive to data sparsing, is kept unmodiﬁed. This is an important
assumption: Arbitrary text ﬁles may be sparsed without restrictions, but sev-
eral other types of ﬁle contain sections that aﬀect a program’s control ﬂow.
Introducing an artifact into sensitive sections may result in runtime errors and
unexpected program termination. Therefore, as shown in Fig. 1, the raw data
block containing the image’s dimension and resolution is kept unmodiﬁed.
In principle, data sparsing could be implemented within a user program
(application layer) or within a special ﬁle system (ﬁle system layer). In case
of Fig. 1, a realization on the application layer would read the original ﬁle from
persistent storage and sparse it in-memory afterwards. But this design collides

312
P. Graubner et al.
with our basic idea: Data sparsing should handle the software as a black box
and keep the application unmodiﬁed. Therefore, data sparsing with artifacts
is implemented on the ﬁle system layer. Potentially, this design allows energy
savings even for applications where the source code is unavailable or restricted.
Furthermore, we prototypically integrated data sparsing into the Hadoop Dis-
tributed File System (HDFS) in order to show the practicability of our approach
within a distributed ﬁle system. Section 3.1 presents the design and implemen-
tation of data sparsing with artifacts in a ﬁle system in userspace (FUSE), while
Sect. 3.2 describes the design of a modiﬁed Hadoop Distributed File System
(HDFS) to make use of the data sparsing approach.
3.1
Data Sparsing in FUSE
A data sparsing ﬁle system (DSFS) requires a mechanism to mark ﬁle blocks as
sensitive to data sparsing. Given that the sensitive blocks and the sparse rate
have previously been deﬁned when a user program accesses ﬁles on a DSFS, a
sparsed version of this ﬁle can be delivered to the user process. The original
data is kept unmodiﬁed and can be accessed directly on the local ﬁle system.
The information about which blocks are sensitive to data sparsing is stored as
ﬁle system meta-data, such that all ﬁle types can be handled equally by this
special kind of ﬁle system. Write operations to this special kind of ﬁle system
are simply passed through without modiﬁcation.
The DSFS developed in the context of our work transparently provides data
sparsing with artifacts on the ﬁle level. Figure 2 shows the relationship between a
read / write
metadata
read
data blocks
other
operations
mark / unmark
as sensitive
read
data blocks
Application
DSFS Meta-Data
Data Sparsing
DSFS
Local FS
Meta-Data
Data
pass-through
uses
Fig. 2. The design of a Data Sparsing File System (DSFS).

Energy-Eﬃcient Data Processing Through Data Sparsing with Artifacts
313
local ﬁle system and DSFS. DSFS introduces artifacts during ﬁle read access, ﬁle
write access is passed through to the local ﬁle system. Artifacts, sparse rate and
the information about sensitive blocks within a ﬁle is stored within the DSFS
meta-data that is persistently used within the local ﬁle system as extended user
attributes, each consisting of a key and an associated value.
The concept of extended user attributes is an extension of normal attributes
of ﬁle system inodes that are supported by a growing number of Unix ﬁle systems,
such as ext2, ext3 and ext4. Our prototypical implementation is implemented
as a ﬁle system in userspace (FUSE) [15], based on the FUSE-based open source ﬁle
system bindfs [6]. bindfs mirrors already-mounted ﬁle system parts to another
location, similar to a bind mount (mount --bind). However, in contrast to a
bind mount, DSFS stores additional meta-data in extended attribute values.
File systems like ext2, ext3 and ext4 store each extended attribute in a single
ﬁle system block (usually 1 KB, 2 KB or 4 KB). DSFS uses a list of extended
user attributes to store the meta-data, to bypass the single block size limit
for meta-data that would signiﬁcantly limit the maximum ﬁle size in DSFS.
Each of the extended user attributes has a fully qualiﬁed name in the form of
user.DSFS block n, where n is the number of the meta-data block.
The ﬁle system interface is not altered, and operations like open(), close()
or write() are unmodiﬁed. ioctl() is reimplemented to provide read/write
access to the DSFS meta-data. A user can mark or unmark data blocks as sen-
sitive, set the sparse rate or specify the artifact. This information is stored with
the setxattr() command of the underlying ﬁle system. The read() function
relies on the set of sensitive data blocks marked with ioctl(), which is retrieved
with the getxattr() function of the underlying ﬁle system.
3.2
Data Sparsing in HDFS
In contrast to a local ﬁle system, HDFS provides a non-mountable single virtual
ﬁle system distributed over many server racks, which is not capable of extended
attributes. It provides a separate ﬁle system name space. All ﬁles are accessed
either via the FSShell command line interface, or the Java interface.
To introduce data sparsing to HDFS, its data storage needs to be set up on
top of a DSFS. Furthermore, the following design changes for HDFS are neces-
sary to support data sparsing: (1) Additional meta-data to mark data blocks as
sensitive. (2) A modiﬁed block data veriﬁcation scheme in order to avoid data
integrity errors. (3) A modiﬁed user interface, i.e., a modiﬁed Java interface and
commands for the FSShell command line interface.
Meta-Data. The key challenge of integrating additional meta-data to mark
data blocks is to store it without breaking the high-throughput optimization
and the data integrity of the ﬁle system. Usually, a single HDFS NameNode is
responsible for storing ﬁle meta-data such as the number of replicas, the block-
ids of the blocks of a single ﬁle as well as the whole ﬁle system namespace.
According to the HDFS architecture, DataNodes are only responsible for reading
and writing HDFS blocks. Since HDFS runs on top of a low-level ﬁle system,

314
P. Graubner et al.
it is a question of granularity on which layer data blocks should be marked
as sensitive. However, marking a whole HDFS block as sensitive decreases the
amount of sparseable data signiﬁcantly, since the standard block size of HDFS
is 64 MB, and it is even recommended to increase the block size for larger ﬁles.
Our design uses the DSFS features to mark sensitive data blocks on a Data-
Node. To enable data sparsing for HDFS blocks, new meta-data needs to be
introduced into the HDFS data ﬂow. This meta-data is speciﬁed by the user per
ﬁle, translated to HDFS meta-data per HDFS block by the user client and trans-
ferred to the DataNodes, where it is stored persistently using DSFS features.
DataNode Block Veriﬁcation. Data sparsing is only allowed in DSFS blocks
that are not marked as sensitive. Hence, there is still a need for HDFS block
veriﬁcation to ensure data integrity. This feature reﬂects the fault-tolerance of
HDFS, in order to countervail hardware faults of commodity hardware.
HDFS uses a two way integrity check: (1) HDFS veriﬁes checksums on receipt
of the block. (2) The HDFS DataNode periodically triggers a checksum-based
block integrity test to detect failures due to corrupt hard disk blocks. Both
mechanisms are modiﬁed with respect to data sparsing during DSFS reads.
User Interface. The information about sensitive blocks is speciﬁed manually
by the user in a MetaF ﬁle, which uses a simple CSV ﬁle format. In this format,
the user is able to mark arbitrary ranges of a ﬁle stored in HDFS as sensitive by
declaring start- and end-oﬀset.
HDFS Block Veriﬁcation. The periodical block veriﬁcation method of HDFS,
implemented in the class BlockPoolSliceScanner, is modiﬁed to skip sparsed
data blocks during veriﬁcation. Therefore, the byte stream is partitioned into
single blocks that are veriﬁed on the block level.
4
Experimental Evaluation
In the following, two use cases for data sparsing with artifacts are evaluated:
(i) face detection and recognition and (ii) speech recognition. Data sparsing is
performed with a block size of 4 KB and varying sparse rates with diﬀerent arti-
facts. 7 diﬀerent sparse rates are used: 0.5 %, 1 %, 2 %, 5 %, 10 %, 20 % and 50 %.
The experiments are conducted in a Hadoop cluster, consisting of 1 NameNode
and 2 DataNodes. The cluster size is related to the capacity of the EasyMeter [11]
electricity meter that is used for the power and energy measurements. It is con-
nected to a separate Ethernet network via the Multi Utility Communication
(MUC) tool and periodically polled at a 5 Hz rate. The servers have an Intel
Core i7–4771 with 3.5 GHz, 32 GB RAM and 256 GB SSD and an idle power
consumption of 55 W.

Energy-Eﬃcient Data Processing Through Data Sparsing with Artifacts
315
4.1
Use Case 1: Face Detection/Recognition
Several social-network or instant-messaging providers need to handle a vast
amount of image ﬁles. An interesting use case is to detect and to recognize
faces within images uploaded by users, in order to provide better services to the
users (e.g., suggesting potential friends to the user) or to their advertisers (to
improve user-related ads). Therefore, we have implemented a face detection and
face recognition use case as a MapReduce application running on top of HDFS
with DSFS support. The Hadoop MapReduce framework basically operates on
input-, output- and intermediate-data structured using key-value pairs. We use
Mapper tasks to process the image with OpenCV [20] or javafaces [16] and
Reducer tasks to store detected or recognized images.
For our evaluation of both face detection and face recognition, three types of
databases are used: The Yale Face Database [29] (Yale), Caltech Faces [8] (CIT)
and a random data set consisting of photos from everystockphoto4, Flickr5 and
Google image search6 (Random). In total, over 700 images stored in JPEG format
as well as BMP format are used as our test data. A face detector with high detec-
tion rates and low execution times as described by Viola and Jones [28] is used,
implemented with the Open Source Software OpenCV [20]. For face recognition,
the javafaces [16] implementation is used, which is based on eigenfaces [27].
Yale consists of gray-scaled images containing one face per image, 11 individu-
als in diﬀerent facial expressions or conﬁgurations. It is often used by researchers
for face detection and recognition tests. Similarly, CIT contains 27 persons with
diﬀerent lighting/expressions/backgrounds. In contrast to Yale, the location of
faces in images is more variable and the images are larger. Random contains a
large number of diﬀerent images with larger ﬁle sizes.
Compressed JPEG and uncompressed BMP image ﬁles are examined. Since
sensitive ﬁle headers need to be handled accurately, the ﬁrst 4KB block is marked
as sensitive. Additionally, since JPEG uses a termination marker at the end of a
ﬁle, the last block is also stored without modiﬁcation. Table 1 shows the average
ﬁle size and the percentages of insensitive blocks per ﬁle.
Beforehand, an appropriate artifact for these algorithms must be selected.
Simply omitting insensitive blocks from image ﬁles reduce the quality of the
results signiﬁcantly, because this leads to malformed shapes in an image with-
out any chance of detecting any faces. Good results can be achieved with zero
Table 1. Average ﬁle size and average percentage of insensitive blocks.
Yale
CIT
Random
Avg. ﬁle size
159 KB 34 KB 1058 KB
Avg. percentage 95.9 %
70.5 % 99.9 %
4 http://www.everystockphoto.com.
5 https://www.ﬂickr.com.
6 https://images.google.com.

316
P. Graubner et al.
Fig. 3. Face detection and face recognition results.
pages, because the JPEG parser interprets zeros as a termination symbol. Fur-
thermore, the JPEG standard enables the user to insert line-by-line or block-
wise restart markers as basic error-correction indicators for the JPEG decoder.
Figure 3 shows the experimental results. Quality denotes the number of detected
or recognized faces.
90 % Quality. 90 % quality is reached for the Yale data set at sparse rates of
up to 2 %; line-by-line restart markers achieve best results with up to 5 %. For
CIT, line-by-line markers achieve a quality higher than 90 % for sparse rates
of up to 1 %. Furthermore, for Random, only with line-by-line restart markers,
90 % quality can be reached at a 1 % sparse rate.
75 % Quality. 75 % quality is reached for the Yale data set at sparse rates of
up to 5 %; line-by-line markers achieve best results with up to 10 %. For CIT, 1 %

Energy-Eﬃcient Data Processing Through Data Sparsing with Artifacts
317
Fig. 4. Face detection and face recognition energy consumption results.
sparse rate is acceptable, and line-by-line markers achieve a quality higher than
75 % for sparse rates of up to 5 %. Furthermore, for Random, 75 % quality can
be reached at 1 % sparse rate without restart markers; with line-by-line restart
markers, up to 5 % sparse rate.
Summary. The reason for the drop in quality of the results in CIT and Random
without restart markers is that the artifact introduced during data sparsing has
a larger negative eﬀect on the JPEG decompression algorithm than predicted.
To summarize, the best results are achieved with line-by-line restart markers.
Energy Eﬃciency. We now investigate the energy consumption for diﬀerent
sparse rates on diﬀerent data sets. For CIT without restart markers (Fig. 4b),
a sparse rate of 50 % cuts the energy consumption in half. Within the range
between 0.5 % and 5.0 % sparse rate, where 90 % quality can be achieved, CIT
and Random show a slightly reduced energy consumption (CIT: 17 %, Random:
6 %) without restart markers. Unfortunately, although restart markers achieved
the best quality, they reduce energy eﬃciency. Without restart markers, the
energy consumption baseline is better in all conﬁgurations.
Due to the higher energy consumption of restart markers, at 90 % quality,
no energy savings are achieved. For 75 % quality, data sparsing achieves 10 %
energy savings for CIT images, and 9 % energy savings for the Random image
database.
As Fig. 4a shows, the Yale images are not large enough to have a positive
eﬀect on energy consumption. The CIT data set is closer to realistic pictures, and
it shows the best results with 1 % sparse rate. Face detection on larger images
as in Fig. 4c also shows an increase of energy eﬃciency within this range.
To summarize, restart markers can increase the quality of the results, but
lead to larger energy consumption. Blockwise restart markers introduce a data

318
P. Graubner et al.
overhead of 10 %, but do not lead to signiﬁcantly longer execution times. Nev-
ertheless, for 75 % quality, data sparsing achieves 10 % energy savings for face
detection and face recognition on realistic image samples.
4.2
Use Case 2: Speech Recognition
Speech recognition has become a key feature for instant-messaging and mobile
application providers. For example, mobile application developers can use
cloud-based voice recognition providers like Wit.ai7 to build voice-controlled
applications. Audio ﬁles are recorded locally and sent to an analysis backend
afterwards8. Therefore, a speech recognition use case is evaluated below. For
our evaluation, two types of databases are used: An alphanumeric database [23]
(AN4), containing 1078 recordings of speakers spelling out personal information
(such as name, address, telephone number, birth date) and a PDA speech data-
base [24] (PDA), where the voice was recorded by microphones mounted on a
PDA, and diﬀerent speakers read about 50 sentences, resulting in 836 recodings.
The PDA dataset contains Wall Street Journal news text, i.e., natural language
with a large vocabulary. In principle, a large vocabulary means less accurate
results, i.e., higher expected error rates. All data sets consist of .wav ﬁles with
16 bit sample rate.
For recognition, we used Pocketsphinx [25], an open source speech recognition
engine that recognizes words with an expected error of 20 %. Pocketsphinx needs
a phase of 3 seconds for calibration, which cannot be used for data sparsing.
Thus, the ﬁle header and the calibration phase are marked as sensitive. As shown
in Table 2, this leads to a relatively low percentage of insensitive blocks per ﬁle.
Appropriate artifacts for this kind of algorithm are both zero and omitted
pages. In case of .wav ﬁles, zero pages represent phases of silence, omitted pages
interruptions within the recording. As Fig. 5 shows, omitting pages gives slightly
better results with respect to the word error rate. With the AN4 database, a
90 % quality (10 % word error rate) can be achieved with 10 % sparse rate in case
of omitting pages, and with less than 5 % in case of zero pages. PDA has slightly
worse word error rates: 90 % quality can be achieved with only 2 % sparse rate.
Furthermore, a 75 % quality (25 % word error rate) can be achieved in AN4 with
20 % sparse rate in case of omitting pages, and with less than 10 % in case of
zero pages. PDA can only achieve 75 % quality with less than 10 % sparse rate.
Table 2. Average ﬁle size and average percentage of insensitive blocks.
PDA
AN4
Avg. ﬁle size
315 KB 178 KB
Avg. percentage 68,2 %
44,5 %
7 https://wit.ai.
8 Data sparsing might decrease the energy consumption of micro-controllers as well,
due to a reduced transmission energy, but this is beyond the scope of this paper.

Energy-Eﬃcient Data Processing Through Data Sparsing with Artifacts
319
Fig. 5. Speech recognition results.
Fig. 6. Speech recognition energy consumption results.
Energy Eﬃciency. As indicated by Fig. 6, the energy consumption of both
data sets decreases with higher sparse rates. 5 % energy can be saved with 90 %
quality, while 8 % energy can be saved with 75 % quality.
5
Related Work
We discuss related work on two relevant topics: Controlled Approximation and
Approximate Programming, and Energy-conserving I/O-performance trade-oﬀs.
5.1
Controlled Approximation/Approximate Programming
Several researchers try to ﬁnd a trade-oﬀbetween accuracy and energy consump-
tion by using controlled approximation [3] and approximate programming [13].
Code perforation is a technique developed by Agarwal et al. [1] to increase
the performance of error resilient applications. The approach basically performs
loop approximations for performance improvements. It consists of an extended
C/C++ compiler and a corresponding runtime environment, identifying a set
of loops that are perforated according to a user deﬁned acceptability metric.
Since this works automatically, it is not possible for a programmer to spec-
ify loops or parts of the program where code perforation should be omitted.
The Green framework developed by Baek and Chilimbi [3] supports energy-
conscious programming using controlled approximation, allowing function- and

320
P. Graubner et al.
loop-approximation. To use the framework, a programmer has to indicate poten-
tial approximation points (e.g., function deﬁnitions and loops) using annotations
and provide approximate versions of relevant functions.
The focus of architectural support for approximate programming is on volt-
age scaling for processing units and SRAM, ﬂoating point mantissa reduction
and reduced DRAM refresh rates, to reduce the dynamic power consumption
of CPU, cache and main memory. Approximate programming mainly targets
highly utilized systems with high CPU, caches and main memory usage, but the
general approach of trading accuracy for energy consumption is applicable to
more cases, such as ﬂoating-point mantissa reduction [26]. Sampson et al. [21]
have proposed EnerJ, an extension of the Java programming language that
enables a programmer to explicitly integrate approximation considerations into
code. Esmaeilzadeh et al. [13] have introduced architectural support for approx-
imate programming. In recent work, Esmaeilzadeh et al. [14] use artiﬁcial neural
networks to learn how a region of approximable code behaves and automatically
replace the original code with an eﬃcient computation of the learned model.
These approaches target the same classes of applications as data sparsing. In
contrast to data sparsing, they rely on speciﬁc programming languages, software
modiﬁcations and custom hardware to increase energy eﬃciency. Data sparsing
introduces artifacts at the data layer, without application layer modiﬁcations and
with general purpose hardware. Furthermore, approximate programming handles
bit ﬂips and ﬂoating point mantissa reductions, resulting in a lower precision
of computations. However, typical hypervisor and operating system resource
management components operate on larger blocks. For example, a typical hard
disk block or memory page is 4 KB. Data sparsing utilizes this fact by using
memory page sizes as block sizes for artifacts. Furthermore, data sparsing is
more than introducing errors into a byte stream, since it uses application-speciﬁc
artifacts to indicate that a given block should be excluded from the computation.
5.2
Energy-Conserving I/O-Performance Trade-Oﬀs
GreenHDFS developed by Kaushik and Bhandarkar [17] is based on the concept
of hot and cold zones. Rarely accessed ﬁles are stored in a cold zone, i.e., a set of
servers with a low-power hardware design. Regularly accessed ﬁles, on the other
hand, are stored in a hot zone, with higher utilization, performance requirements
and power consumption. In contrast to our approach, GreenHDFS is based on
system sleep states and low-power hardware at the cost of performance impacts
and wake-up penalties. Chen et al. [9] have examined I/O- vs. performance
trade-oﬀs in MapReduce. For some applications, the use of compression for data
transfers via a network is a way to save energy. Nevertheless, a very low com-
pression ratio of roughly 0.3 is necessary to achieve energy savings. For most
data-intensive applications, this condition is not met. For instance, commonly
used compression formats like MP3 and JPEG have compression ratios between
0.8 and 0.9 [9].

Energy-Eﬃcient Data Processing Through Data Sparsing with Artifacts
321
6
Conclusion
In this paper, data sparsing with artifacts has been introduced, a novel approach
aimed at reducing processing times and thus energy eﬃciency of applications that
are robust to input variations while preserving the quality of the results. A proto-
typical ﬁle system, called DSFS, implementing this approach and its integration
into HDFS have been presented. Experimental results have shown promising
energy savings of up to 10 % with moderate accuracy losses for diﬀerent data
sparsing rates and types.
Future work will be devoted to provide a theoretical basis and to implement
further applications for data sparsing with artifacts. In addition, we will inves-
tigate the automatic creation of application-speciﬁc artifacts and dynamically
adaptable sparse rates for applications. Also, random functions with diﬀerent
probability densities for diﬀerent placements of artifacts will be considered to
improve the quality of the results. Furthermore, the following applications will
be considered: Object detection, concept detection, optical character recogni-
tion and similarity search in video and image data. We will also compare data
sparsing with approaches such as lossy compression. Finally, an interesting ques-
tion is how operating system and hardware modiﬁcations can be used to further
improve the energy eﬃciency of data sparsing with artifacts.
References
1. Agarwal, A., Rinard, M., Sidiroglou, S., Misailovic, S., Hoﬀmann, H.: Using Code
Perforation to Improve Performance, Reduce Energy Consumption, and Respond
to Failures. Technical report, MIT Dspace (2009)
2. Apache Hadoop: Apache Hadoop (2015). http://hadoop.apache.org
3. Baek, W., Chilimbi, T.M.: Green: a framework for supporting energy-conscious
programming using controlled approximation. In: Proceedings of the 2010 ACM
SIGPLAN Conference on Programming Language Design and Implementation,
PLDI 2010, pp. 198–209. ACM (2010)
4. Barroso, L.A., Clidaras, J., H¨olzle, U.: The aatacenter as a computer: an intro-
duction to the design of warehouse-scale machines. Synth. Lect. Comput. Archit.
8(3), 1–154 (2013)
5. Barroso, L.A., H¨olzle, U.: The case for energy-proportional computing. IEEE Com-
put. 40(12), 33–37 (2007)
6. bindfs: bindfs (2015). http://bindfs.org (2015)
7. Bingham, E., Mannila, H.: Random projection in dimensionality reduction: appli-
cations to image and text data. In: Proceedings of the Seventh ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD 2001,
pp. 245–250. ACM (2001)
8. Caltech Computational Vision Group: Caltech Faces (2015). http://vision.caltech.
edu/Image Datasets/faces
9. Chen, Y., Ganapathi, A., Katz, R.H.: To compress or not to compress - compute
vs. i/o tradeoﬀs for mapreduce energy eﬃciency. In: Proceedings of the First ACM
SIGCOMM Workshop on Green Networking, pp. 23–28. ACM (2010)

322
P. Graubner et al.
10. Dasgupta, S., Gupta, A.: An elementary proof of a theorem of johnson and linden-
strauss. In: Random Structures & Algorithms, vol. 22, pp. 60–65. Wiley Subscrip-
tion Services (2003)
11. EasyMeter GmbH: EasyMeter Smart Metering Device (2015). http://www.
easymeter.com
12. Tal, E.: Saving Data Center Power by Reducing HDD Spin Speed (2015).
http://www.opencompute.org/blog/saving-data-center-power-by-reducing-hdd-
spin-speed/
13. Esmaeilzadeh, H., Sampson, A., Ceze, L., Burger, D.: Architecture support for dis-
ciplined approximate programming. In: 17th International Conference on Architec-
tural Support for Programming Languages and Operating Systems, pp. 301–312.
ACM (2012)
14. Esmaeilzadeh, H., Sampson, A., Ceze, L., Burger, D.: Neural acceleration for
general-purpose approximate programs. In: Commun. ACM, vol. 58, pp. 105–115.
ACM (2014)
15. Filesystem in Userspace: Filesystem in Userspace (FUSE) (2015). http://fuse.
sourceforge.net
16. javafaces: javafaces (2015). http://code.google.com/p/javafaces
17. Kaushik, R.T., Bhandarkar, M.: GreenHDFS: Towards an energy-conserving,
storage-eﬃcient, hybrid hadoop compute cluster. In: 2010 International Conference
on Power Aware Computing and Systems, pp. 1–9. USENIX Association (2010)
18. Li, P., Hastie, T.J., Church, K.W.: Very sparse random projections. In: Proceedings
of the 12th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 287–296. ACM (2006)
19. Nair, R.: Big Data needs approximate computing: technical perspective. Commun.
ACM 58, 104–104 (2014)
20. OpenCV: OpenCV (2015). http://opencv.org
21. Sampson, A., Dietl, W., Fortuna, E., Gnanapragasam, D., Ceze, L., Grossman,
D.: EnerJ: approximate data types for safe and general low-power computation.
SIGPLAN Not. 46(6), 164–174 (2011)
22. Cooper, S.: Facebook Users are Uploading 350 Million Photos each day (2015).
http://www.businessinsider.com/facebook-350-million-photos-each-day-2013-9
23. Sphinx Speech Group, Carnegie Mellon University: CMU Robust Speech Recogni-
tion Group: Census Database (2015). http://www.speech.cs.cmu.edu/databases/
an4/
24. Sphinx Speech Group, Carnegie Mellon University: CMU Robust Speech Recog-
nition Group: PDA Speech Database (2015). http://www.speech.cs.cmu.edu/
databases/pda/
25. Sphinx Speech Group, Carnegie Mellon University: CMU Sphinx Open Source
Speech Recognition Toolkit (2015). http://cmusphinx.sourceforge.net
26. Tong, J., Nagle, D., Rutenbar, R.: Reducing power by optimizing the necessary
precision/range of ﬂoating-point arithmetic. IEEE Trans. Very Large Scale Integr.
(VLSI) Syst. 8(3), 273–286 (2000)
27. Turk, M., Pentland, A.: Face recognition using eigenfaces. In: IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 1991, pp. 586–591 (1991)
28. Viola, P., Jones, M.: Rapid object detection using a boosted cascade of simple
features. In: Proceedings of the 2001 IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition, vol. 1, pp. 511–518 (2001)
29. Yale University Department of Computer Science: Yale Face Database (2015).
http://vision.ucsd.edu/content/yale-face-database

Updating the Energy Model for Future
Exascale Systems
Peter M. Kogge(B)
University of Notre Dame,
Notre Dame, IN 46556, USA
kogge@cse.nd.edu
Abstract. The 2008 DARPA Exascale report had as its goal deter-
mining if it were possible to achieve 1000X the computational power of
the then-emerging peta-scale systems at a system power of no more than
20 MW. The main conclusion was that there was no such path with tech-
nology and architectures as projected at that time. Key to this conclusion
were architecturally-tailored models as to how projected advances would
translate into system performance. This paper introduces a major update
to the “heavyweight” (modern server-class multi-core chips) model, with
a detailed discussion on the underlying projections as to technology, chip
layout and microarchitecture, and system characteristics. The model is
run over the same time period as the 2008 model to verify its accuracy.
Keywords: Exascale · Energy · Technology projection
1
Introduction
The 2008 DARPA Exascale report [17] had as its goal determining if it were pos-
sible to achieve 1000X the computational power of the then-emerging peta-scale
systems at a system power of no more than 20 MW. The main conclusion was that
there was no such path with technology and architectures as projected at that
time. Key to this were two projection models as to how future advances could
translate into exascale-level system performance, and more importantly what
was the power required to achieve that performance. The two models reﬂected
two classes of architectures as they appeared in the TOP500 lists through that
time: heavyweight systems where the processing was all done on high end server-
class conventional processor chips with sophisticated cooling and often separate
chips for inter-node networking, and lightweight systems where all logic func-
tions were integrated into a single chip and the number of cores and clock rate
reduced to the point where relatively simple cooling techniques could be used.
The baseline for the heavyweight extrapolations was the 2006 Red Storm system.
The lightweight baseline was the IBM BlueGene L system. The models projected
how such systems would evolve over time. Since power was a big concern, both at
the rack and the system level, special eﬀort was made to estimate both, and then
convert the total power and ﬂop rate into a single energy per ﬂop metric. The
goal was to ﬁnd if and when systems would pass below 20 MW for a complete
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 323–339, 2015.
DOI: 10.1007/978-3-319-20119-1 24

324
P.M. Kogge
system. The 2011 update [15] introduced hybrid or heterogeneous architectures
consisting of heavyweight processor chips and separate GPUs.
While the reported characteristics of real systems buttressing the 2008 report
has been updated frequently since 2008, using both the “Top10” from the TOP500
[15,16] and other benchmarks [14,18,19], the models themselves have only had
relatively minor tweaks to parameters. This paper completely redoes the heavy-
weight model, with the goal of projecting what is possible under relatively uncon-
strained future developments. The model’s projections start back at the same time
as the 2008 model did, so that real-world data from then until now can be com-
pared for model validation. It should be expected that the model will predict sys-
tems with better characteristics than we have actually seen in the last 7 years since
marketplace factors are not taken into account.
This paper considers only the heavyweight systems; after BlueGene Q there
are no planned updates to the lightweight class, and there is insuﬃcient detailed
data on the major GPU chips to do as accurate a model as for the other two.
A key point for consideration is the performance metric used here, namely
peak ﬂop rate (Rpeak in TOP500 terms). This is what was used in the Exascale
study, and may be reasonable if what we are doing is dense matrix math such
as in LINPACK, where eﬃciencies above 80 % are not uncommon. However,
newer benchmarks such as HPCG [8] have early results [1] that imply today’s
architectures may be at best 1–4 % eﬃcient, and thus making ﬂops less relevant.
As these benchmarks mature, the model should be adjusted accordingly.
The following sections discuss this new model in terms of updates to the
2008 model. Section 2 provides background. Section 3 discusses changes in the
underlying technology model. Section 4 does the same for the assumed layout
and microarchitectures of the processing chips. Section 5 looks at the system-
level bounds. Section 6 summarizes the new model. Section 7 concludes.
We note that while extensive data and graphs back up virtually all of the
points discussed in the following sections, only selected graphs are shown here.
Also for brevity, we refer to [15] for a concise summary of the 2008 assumptions.
2
Background
2.1
Benchmarks, Power, and Energy
The TOP5001 rankings of sustained performance against dense linear algebra
problems have been in existence for over 20 years, with overall power numbers
fairly consistently reported after about 2000. After the 2008 Exascale report, the
Green5002 rankings tracked the most energy-eﬃcient systems, in “megaﬂops per
second per watt” (where a ﬂop is a single ﬂoating point operation).
A second benchmark involving a graph problem has been tracked since 2010
as the Graph5003. In parallel with the Green500, the Green Graph5004 has been
1 http://www.top500.org/.
2 http://www.Green500.org/.
3 http://www.Graph500.org/.
4 http://green.Graph500.org/.

Updating the Energy Model for Future Exascale Systems
325
gathering data since 2013 on energy eﬃciency in terms of “megaTEPS per watt”
(where 1 TEPS = 1 traversed edge per second).
As useful as these pairs of rankings have been in terms of understanding
general trends, in both cases the power numbers have been total system only, for
only the one benchmark, and with no breakdown as to where the energy is going.
Further, even the total power numbers are not always completely comparable,
as diﬀerent reported numbers are inconsistent in what was measured, such as
accounting for associated storage and/or cooling and power conversion.
While both rankings have used “operations per second per watt”, the recipro-
cal “watts per operations per second” is more useful, especially when we replace
“watts” by “Joules per second”, and cancel out “per second”. This leaves “Joules
per operation” where a Joule is a basic unit of energy. Thus a “giga operations
per second per watt” is the same as 1 “nano Joule (nJ) per operation”.
The Exascale study noted that with this metric, if a single operation takes X
pico Joules (pJ - where 1 pJ = 10−12 Joules), then a machine that can perform
1 exa operations per second (1018) will require X megawatts. The goal for the
Exascale study was a 20 MW system that could do 1 exa (ﬂ)ops per second, or
20 pJ per (ﬂ)op. Thus looking for architectures with this 20 pJ characteristic was
a key goal. The best of the Green500 is at about 190 pJ per ﬂop for systems much
smaller than atop the TOP500, which have energies several times this. The best
of the Green Graph500 is at about 17,000 pJ per traversed edge, again with the
much faster systems from the Graph500 again several multiples of this.
The key aspect of this measure is that energy is additive; if a benchmark
operation like a ﬂop requires on average so many instructions executed, so many
cache accesses, so many register ﬁle accesses, etc., and we have the energy cost of
each individual sub-operation, then the sum is the total energy of the operation,
and we can easily compute the relative importance of each sub-operation.
2.2
Moore’s Law and Scaling
The original Moore’s Law [24] addressed the periodic growth in number of tran-
sistors on a chip, and was based on the decrease in the basic feature size (linear
dimension) of those transistors. This was later extended [23] to include the speed
of those transistors, and thus the overall performance of microprocessors. About
the same time, a more formal study (now called Dennard Scaling) [7] deﬁned
parametrically how many diﬀerent properties of a transistor, and circuits built
from such devices, would change as feature size changes. In particular, it pre-
dicted a decrease in device capacitance, and in needed supply voltage (denoted
Vdd), with a consequent increase in possible clock speed, as feature size decreased.
When coupled with an estimate of the power of a circuit as 0.5CfV 2
dd, where
C is the aggregate capacitance of the circuit and f the clock rate, Dennard scaling
rules have been the backbone of projections of silicon-based systems for years.
The annual ITRS roadmap [12] has published the best projections of what is
liable to be achievable in the future by industry for years.
A major change in the rate of decrease of Vdd around 2004 forced the archi-
tectural change from single to multi-core [16].

326
P.M. Kogge
2.3
Modeling
Virtually all of the most modern CAD packages such as SPICE for chip designs
include estimators for power and speed, based on a chosen current technology.
Some tools are available that allow projection to future technologies5. There are
also speciﬁc estimators for standard circuit blocks such as SRAM and DRAM [10],
backed up by detailed models that allow some forward projections [32].
Moving up, there are chip level architectural models such as Wattch [5] and
McPat (especially for multi-core designs) [21], often designed so as to allow
estimation using future technologies. There are also modeling tools for emerging
technologies such as 3D memory stacks [28]. However, there are few if any multi-
node system modeling tools that allow projections into future technologies.
2.4
Exascale
There were actually three Exascale reports, the one on technology and archi-
tecture that formed the basis for this paper [17], one on software [3], and one
on resiliency [6,9]. After that there were quite a few workshops on algorithms
and software for exascale, but at best a few narrow followup studies on exascale
energy and architecture [20]. After that there have been multiple projections by
acknowledged experts in high performance computing [11,29,31], but very little
descriptions of underlying models. For example, [30] projects that future genera-
tions of Intel’s MIC chip may get within 2-3X of 20 pJ per ﬂop goal. Interestingly,
this is only slightly better than the 2008 estimates.
3
Technology Updates
The key technology drivers for the 2008 model were logic feature size, voltage,
maximum chip power dissipation, die size, and DRAM chip densities.
Assuming that silicon CMOS will continue as the dominant device technol-
ogy, key to all the predictions is accurate estimations of how the “feature size”
(F) for transistors will change, as this directly drives estimates of such things as
die area for cores or capacitance (and thus power) for logic and signaling. The
2008 model assumed the ITRS [12] roadmap available at that time (the 2006
version), with an assumed one year lag from ﬁrst availability of a new micro-
processor to inclusion in systems. Figure 1 includes the trends for both the 2008
model and that from the most recent 2013 roadmap. The most recent version
pushes out the smaller feature sizes by perhaps 18 months and lowers the ulti-
mate size. These curves, however, correspond to general industry usage, and
foundries such as Intel have done better. Figure 1 also includes red stars repre-
senting the feature sizes of actual releases of high end Xeon® chips by Intel [2],
and a red line representing the smallest feature sizes of such chips versus time.
This line is up to 5 years earlier than either ITRS projection.
5 e.g. PTM http://ptm.asu.edu/.

Updating the Energy Model for Future Exascale Systems
327
Fig. 1. Trends in silicon feature size.
Also shown are red squares representing the ﬁrst time systems in the Top10
included such chips. Over the last 10 years they have drawn much closer to the
ﬁrst appearance of a Xeon at the same technology level. The black dashed line is
an extrapolation of this trend, ending where the ITRS 2013 does at about 7 nm.
This line is what our new model will use. We note that the net eﬀect is about 3
years earlier than the 2008 model for the same feature size (2 years earlier than
the 2008 curve, which was 1 year earlier than the assumed systems).
TDP (Thermal Design Power) represents the maximum power that a chip
should be allowed to dissipate, typically assuming an air-cooled heat sink. Higher
values of TDP allow chips to run faster. The 2006 ITRS numbers forecast up to
200 W per die, whereas the later projections reduced this to around 150. TDPs
for actual microprocessors used in Top10 systems over the same period have on
average risen from around 80 W to 130 W. In contrast, GPU chips today routinely
run above 200 W (as high as 375 W). Since our new model is intended to explore
what is possible, not just commercially available, we will assume a linear rise in
TDP for exascale-speciﬁc microprocessors up to 300 W, starting with the 2006
95 W number. With this model, the values in the 2009–2014 time frame match
up reasonably well with real devices. Of course, assuming a continuing rise to
300 W means that cooling must migrate from today’s air-cooled to the more
elaborate cooling as used in high end GPU accelerator cards, such as closed
circuit heat pipes.
Because of cost, most system vendors have used commodity DRAM rather
than leading edge as with the microprocessors. Basic cell area between the 2006
and 2013 ITRS is in rough agreement, but the steps for the 2013 projections for
commodity DRAM chip capacity are about 3 years later than projected in 2006.
This is for economics, as the major vendors have gone to smaller die areas to
improve yield and decrease cost. The eﬀect on systems is to reduce the growth

328
P.M. Kogge
in memory per node, which we have already observed over the last few years in
the Top10 rankings. The new model will continue to assume commodity DRAM,
although as suggested in the exascale report, new technologies such as stacked
memory are liable to appear, and are worth a separate exploration.
The other major needed model for DRAM chips is power. The Exascale
report devoted signiﬁcant time to detailing current DRAM chip models and the
associated power. Since then, detailed models such as DRAMsim2 [27] have been
released to model internal DRAM activity and performance, and part-speciﬁc
models from memory vendors (such as [22]) can estimate power consumption
given memory access and especially I/O parameters. However, all of these require
signiﬁcant application-dependent proﬁling information beyond the scope of the
model developed here. Consequently, we assume the same model as in 2008.
We started with a baseline of 210 MW per chip and extrapolated assuming future
power would be the product of the energy per access (including both access to
the DRAM core and the oﬀ-chip signaling energy) and the number of accesses
per second, which in turn was assumed to climb linearly with ﬂop rate. Two
bounding models were assumed. In one, denoted the Scaled model, the inherent
energy improved as fast as the ﬂop rate increased, leaving a constant chip power.
This is clearly optimistic. In the other, denoted the Constant model, total energy
per access is ﬁxed, and power is thus proportional to the access rate. This is
clearly pessimistic.
One caveat is that it is highly unlikely that the peak access rate of DRAMs
as architected to date will ever exceed perhaps 10 times that of the Red Storm
parts (today’s DDR4 are 3.2 GT/s vs Red Storm’s 0.4). This limits the maximum
power in the second model, and in either case means that there is a maximum
memory bandwidth that will be available per processor, which may not constrain
peak performance, but will constrain sustained performance.
4
Layout and Microarchitecture Updates
This section discusses the key characteristics of a single heavyweight micro-
processor chip as needed for the new model. The ﬁrst of these is die area. The
2008 model assumed a constant 220 mm2, while the heavyweight microprocessors
in Top10 systems from 1990 until the present have varied from 100 to almost
700 mm2. High end GPU chips today are routinely in the 300 to 500+mm2
range. Thus, as with TDP, we will feel free to explore what is possible for an
exascale-speciﬁc chip by assuming a linear growth in max die size from 300 to
500 mm2.
A second key microarchitectural characteristic is the peak ﬂoating point capa-
bility of each core, in ﬂops per cycle per core. The original model assumed a bump
from 4 ﬂops per cycle to 8 in 2015, and ﬂat thereafter (with the assumption that
more ﬂops per cycle than that would be done by chips like GPUs). When we look
at the actual data from the Top10 we see the jump to 8 actually ﬁrst occurred in
2010 (other than for the Earth Simulator’s microprocessor [33]) and it has been
ﬂat since then. Our new model adopts that.

Updating the Energy Model for Future Exascale Systems
329
Fig. 2. Core area as die area over core count.
Perhaps the most important parameter is the number of cores per die, which
in turn requires a “die area per core”. The 2008 model assumed perfect scaling:
halving the feature size of a logic block reduces the area of that block by 4. To
check on this, Fig. 2 plots data on microprocessor chips that have been used in
past Top10 systems. The Y axis is the total area of the die divided by the number
of cores. This area includes not just the basic cores but the shared caches, the
memory and I/O controllers, inter-core routing, I/O pads, etc.
Around 2004, with the conversion to 90 nm and below, we entered the multi-
core region. Before that, the “area per core” had no real trend with feature size,
with the growing number of transistors used for improved microarchitecture and
bigger caches. After that, we see replication come into play, with an aggregate per
core area decreasing with F to the 1.54 power (versus a perfect 2). The diﬀerence
most probably is the need for circuitry that grows greater than linearly with core
count, such as inter-core communication.
A side study used die photos of quad-core Intel Xeons dies to analyze how
the die area was used. Figure 3 plots trends in “basic core area” (the true core
logic plus just L1) and “area per MB” of cache above L1, all as a function of the
feature size. In this data, the area of a core decreased much more slowly than an
exponent of 2, or even 1.52, but 0.66. This less than perfect decline is probably
due to the growth of microarchitectural features such as bigger unshared caches,
more FPUs, bigger TLBs, etc. The area per MB of cache also decreased, but
again with an exponent of only 1.2, again which is much less than a perfect
scaling of 2. This latter decline is due to the diﬃculty at smaller feature sizes to
decrease SRAM cell areas in a reliable fashion.
An extended study looked at a wider range of Xeon-class microprocessor
chips. The growth in total on-chip cache has slowed since 2009, and the amount of
cache per core has ﬂattened at about 2.5 MB. This probably reﬂects the declining
eﬀectiveness of caches over that size for typical applications.

330
P.M. Kogge
Fig. 3. Side study of quad-core microprocessors.
In terms of the new model, one approach would be to use the derived relation-
ships for shared caches and cores from the above, and develop matching models
for all the other functions on a microprocessor die. A second approach would
simply use the aggregate formula predicted by Fig. 2 to account for everything.
We have chosen the latter here for simplicity.
The next major characteristic is the assumed clock speed of these cores over
time. Graphs in [19] summarized how ITRS projections have changed over time.
They used as a measurement the intrinsic delay of a 12-inverter chain of high per-
formance transistors (roughly proportional to 1/F). As feature size projections
improved, these projections improved yearly until 2004. After that, power issues
became more important than raw speed, and a switch in projections was made
to assume lower power, but slower, transistors. When it became apparent that
even that was overly optimistic, a switch was made to “power-limited” designs.
When the clocks for Top10 systems are compared, they line up reasonably well
with this last power-limited projection. If we focus just on high-volume Xeon
parts as used above, these ITRS clock rates are high by a factor of about 1.55.
Given that the Xeon-class parts form the basis for the core energy estimates
below, the new model will use the ITRS projections derated by a factor of 0.64.
The 2008 model used the classic CfV 2
dd equation to estimate trends in per
core power. We assumed C varied inversely proportional to feature size and Vdd
varied as predicted by ITRS. The clock rate f was then the maximum value
allowed that did not exceed the maximum die dissipation. This failed to account
for increases in power for I/O (both the number of signal I/Os and signaling
rates increased), and the extra logic for more memory controllers and inter-
core routing. Instead, the new model uses an estimate based again on the last
decade of microprocessors from the Top10. Figure 4 takes the TDP of the actual

Updating the Energy Model for Future Exascale Systems
331
Fig. 4. Per cycle core energy from top10.
microprocessors and divides by the number of cores and the clock rate to give an
energy per cycle for today (22 um) at about 4 nJ, or for a core with up to 8 ﬂops
per cycle, about 500 pJ per ﬂop. The exponent of about 1.23 on feature size in
the trend line makes sense, as we expect about 1 for the decrease in capacitance,
and the remaining 0.23 from remaining small Vdd decreases6. We note this energy
is still many times larger than the overall energy per ﬂop needed to get to the
original DARPA goal of 20 pJ/ﬂop.
The 2008 baseline Red Storm included a “Seastar” router chip [4] which
accepted packets from the microprocessor and routed them along one of several
links to other nodes. Virtually all heavyweight Top10 systems since then have
incorporated chips like that in their design. (Lightweight systems integrated
these router functions into the main microprocessor chip). The energy of any
one packet routing is approximately constant, meaning that the dynamic power
is proportional to the number of packets routed per second. The 2008 model
assumed a baseline of 55 W for such a chip and then, as with memory, it assumed
two possible bounds. In the Scaled model, the native energy per packet routing
decreased with technology in lock step with the increase in packet frequency
needed to support faster cores. In the Constant model, the energy per packet is
constant, and power is thus proportional to the aggregate packet ﬂow rate. As
with DRAM, there are limits to the maximum routing rate, but there is more
headroom here, with perhaps something approaching a 30X increase over the Red
Storm (56 Gbps vs 3.2 Gbps, plus the potential of doubling the number of lanes).
This is because of a new generation of transceivers (cf. [25]). However, power
dissipation will max out at something approaching the maximum we choose for
the microprocessor die. Also, as with the memory limits, after the bandwidth is
maxed out the sustainable performance of the system will suﬀer.
6 Prior to 2004, constant ﬁeld Dennard Scaling had C vary with F and Vdd with F 2.
After 2004, Vdd has become almost ﬂat, with a much smaller decline with F.

332
P.M. Kogge
5
System Updates
This section looks at trends in overall system implementation, including system
packaging, memory capacity, and memory and routing power.
Clearly if cost were no object, an “exascale” system of sorts could be created
by adding an arbitrarily large number of “racks”. The 2008 model grew the size
of a possible future system up from 150 to 600 racks. Real system have already
topped that projection, implying that the new model needs to be modiﬁed. Also,
not all racks are the same size (up to a factor of 4 diﬀerence in footprint), and
not all rack counts are just compute, such as service or storage racks.
If we wish the new model to be more consistent, we need a better metric. The
one chosen here uses “square meters of ﬂoor space” taken up by racks with just
compute nodes in them. Most racks are approximately the same height, so this
metric is equivalent to volume. This in turn can be computed for Top10 systems
by looking at their speciﬁc rack characteristics and the maximum number of just
compute nodes they contain. Dividing into the number of nodes in the system
gives us a count of “compute racks” which is then multiplied by the footprint
of one rack. The new model then starts with 100 m2 in 2005, and rises linearly
to 1000 m2. This upper bound is a little less than twice the current physically
largest system, the K computer [34].
Another metric that was important to the 2008 model was how many process-
ing sockets, and thus compute nodes, could be physically ﬁt within the available
rack volume. The 2008 model started with the packaging associated with Red
Storm and did some extrapolations from that. With the exception of moving
the ﬁrst step back 2 years, actual data matched well, and is carried forward
to the new model. Note that achieving the next step will require a signiﬁcant
advance in cooling, so that today’s “heat sink” area is greatly reduced.
The 2008 model projected future peak rack-level power budgets as step-wise
growth from that of the Red Storm system. We have compared this against
data from the Top10 since then, and found that increased power limits were
achieved by real systems much faster than projected. Consequently, the new
model assumes the same levels as before but moved earlier in time. The new
model also assumes that out of the total power dissipated in a rack only 80 % is
available for compute, with the rest for power conditioning, cooling, etc.
The 2008 model postulated that memory capacity would be increased in
tandem with the growth in ﬂop rate to match the ratio found in the Red Storm
2006 system (a ratio of 0.31 to 1). This was less than what traditionally had
been a “1 byte per ﬂop per second” rule of thumb. Part of this increase was
to come from the increase in DRAM chip density and part by increasing the
number of chips. As shown in [19], before 2006 this ratio was not uncommon
in the Top10, but afterwards it has been dropping by a factor of 0.74 per year,
and is now around 0.125 bytes per ﬂop for heavyweight systems, and even lower
(0.05 bytes per ﬂop per second) for the GPU-based systems. There are several
reasons for this. First, as discussed above, the density of DRAM chips has been
growing at a slower rate than foretold in 2008. Second, the number of socket
memory channels onto which memory can be attached has increased only from

Updating the Energy Model for Future Exascale Systems
333
2 in the Red Storm part to 4 today, as had the number of ranks of DRAM chips
per DIMM (also from 2 to 4). Third, there is a limit to the number of DRAM
DIMMs that can be attached to any memory channel (typically at most 2 as
in Red Storm). Thus, there is perhaps at most a factor of 4 between 2006 and
today in the total number of DRAM chips that might be associated with a single
socket, with little future increase likely without a dramatic change in memory
technology.
Fig. 5. DRAM chips per socket.
Figure 5 takes the memory per socket for each of the heavyweight Top10
systems and assumes that the DRAM technology used was that available one
year prior to the system’s introduction. These counts do not include ECC, which
should add about 12 % more chips. As can be seen, this count is in a band between
about 16 to 128 chips, with no growth over time. Also shown are the same Top10
systems if the memory had been expanded to achieve a 1 byte per ﬂop per sec-
ond. The DRAM chip count that must be associated with each processor socket
rapidly climbs to near 1,000 which, as discussed above, is probably infeasible
with current packaging.
Also shown is the chip count projected by the 2008 model. As can be seen,
this is in the middle of the actual Top10 points, but as discussed elsewhere this
model did not predict as fast a rise in ﬂops per cycle or when new technology
would be introduced. The result was an underestimate in performance per socket,
and thus less memory to achieve its constant 0.31 bytes/ﬂop per second.
Given these constraints, the new model uses a constant 128 (144 with ECC)
DRAM chips of a technology 1 year earlier than the system’s introduction, and
lets the bytes to ﬂops ratio be whatever it is.

334
P.M. Kogge
Fig. 6. Cores per socket.
6
New Model
From the prior sections we can project the power per core by multiplying the
energy per core by the assumed clock rate. We can then estimate the number of
cores per socket in one of two ways: by dividing the core area into the die area,
or dividing the core power into the maximum dissipation. Figure 6 diagrams
projections for each way as a function of time. As can be seen, the two models
seem to bound the real data to date fairly accurately, with most at the power
limit. Going forward we assume the power-limited case.
Fig. 7. Peak ﬂops per socket.

Updating the Energy Model for Future Exascale Systems
335
Given the number of cores per socket, the clock rate, and the peak ﬂops per
cycle, Fig. 7 computes the peak ﬂops per socket. The agreement is rather good.
The 2008 model clearly undershot, mainly because it did not predict as fast a
rise in ﬂops per cycle, when new technology would be introduced, or the size of
the processor dies.
We assume a node consists of a single microprocessor, 144 memory chips, and
a single router chip. In calculating power we sum the components from all three,
and add an additional 10 % for the on-board voltage regulation and conversion.
There is relatively good correspondence to Top10. We note that the constant
model suggests that both memory and network bandwidth became power limited
around 2009, meaning that if the constant model were correct, then after that
point we see a rapidly diminishing memory and network bandwidth to support
the growing chip performance.
As discussed earlier, the new model normalized a “rack” to a square meter
of ﬂoor space. A rack size for many of the Top10 systems is about 0.8 m2, so
numbers here will be slightly larger than for a rack today. The number of nodes
per this unit area is then bounded as the minimum of two numbers: one based
on the packing density and the other in the number of nodes that ﬁt before
the peak power for the unit area is reached. Interestingly, the model suggests
we crossed from being power to volume-constrained somewhere around 2008 to
2010. This is where the data points in Fig. 7 drops below the blue area/volume
bound towards or below the red power/bound line.
Figure 8 diagrams the peak performance of both new models, the 2008 model,
and the Top10. To date the new models seem a better ﬁt than the old one.
Figure 9 is perhaps the most important graph from this model, as it shows
the decrease in energy per ﬂop over time. The projections from 2006 until now
Fig. 8. Rpeak for a square meter.

336
P.M. Kogge
Fig. 9. Energy per ﬂop.
seem fairly good, so if the model holds for the future, the best we will do for
heavyweight machines is between 180 and 420 pJ per ﬂop.
To see if a system with a peak exaﬂop is even feasible, the new model scaled up
the total footprint of a multi-rack system as we go out in time, and projected peak
performance and memory. Figure 10 implies that if we had been more aggressive
we could have had a system with signiﬁcantly more peak performance than any
of the current heavyweights, and going forward, it may be possible to reach
an exaﬂop in about 2020, but it would require about 1200 racks, with a power
requirement of about 180 to 425 MW.
Fig. 10. System performance projections.

Updating the Energy Model for Future Exascale Systems
337
7
Conclusions
This paper has redone the overall projection model from the 2008 Exascale
report for possible exascale systems using conventional heavyweight processors.
These conﬁgurations have all been “at any cost”, and assumed more of the most
advanced technologies than in current systems. The results are better than the
2008 model, both in predictive accuracy and in the potential of reaching a peak
of an exaﬂop, but the cost and power demands are liable to be huge. In addition,
it is unclear if we would end up with anything more than a stunt machine, since
we very rapidly end up with constrained memory and network bandwidth to
go with constrained memory capacity. This may be even worse if performance
against more relevant applications require more bandwidth and other resources.
Going forward there are a variety of possibly valuable modiﬁcations to this
model. First is to ﬁll in missing data from many of the Top10 heavyweight sys-
tems, and continue updating from future years TOP500 list. Second is to extend
the models to include lightweight and heterogeneous architectures, as both of
these may be signiﬁcantly more eﬃcient architectures, especially in power. The
lightweight case is fairly easy, but with no successors to the BlueGene series,
this is at best an exercise to coax some vendor to reconsider. Developing the
heterogeneous model is more challenging, since data on both architecture and
implementation are in signiﬁcant ﬂux. Newer chips such as the Xeon Phi seem
to be cross-overs between all three classes of architectures.
Next, more detailed models for the major components are appropriate,
including partitioning between memory, logic, and signaling (both on and oﬀ
chip). Transceiver power for high speed signaling is key here. In particular, once
better models for signaling are incorporated, then other metrics such as bytes
per second of bandwidth versus peak ﬂops can also be computed, leading to
insight into other possible cliﬀs as we have seen with the bytes of memory per
ﬂop per second. All of these would help extend the model to be relevant to other
well-known benchmarks such as Graph500, and newer ones such as HPCG. The
eﬀects of “dark silicon” for “wear-leveling” and extra logic for redundancy need
to be factored in. Also of real interest going forward are projections based on
emerging memory architectures such as 3D stacks [26] or HBM (High Bandwidth
Memories) [13]. These will provide signiﬁcantly more bandwidth, ﬁxing some
of the problems discussed above, but with signiﬁcantly more complex models.
These may then lead to alternative architectures other than the three currently
discussed, such as some variations of “Processing In/Near Memory”.
Acknowledgements. This material is based upon work supported by the Department
of Energy, National Nuclear Security Administration, under Award Number(s) DE-
NA0002377, as part of the Center for Shock-Wave Processing of Advanced Reactive
Materials, University of Notre Dame. It also builds on work performed under the Sandia
National Labs XGC project.

338
P.M. Kogge
References
1. Hpcg: High performance conjugate gradient. https://software.sandia.gov/hpcg/
2. Xeon, February 2015. http://en.wikipedia.org/wiki/Xeon
3. Amarasinghe, S., Campbell, D., Carlson, W., Chien, A., Dally, W., Elnohazy, E.,
Harrison, R., Harrod, W., Hiller, J., Karp, S., Koelbel, C., Koester, D., Kogge,
P., Levesque, J., Reed, D., Schreiber, R., Richards, M., Scarpelli, A., Shalf, J.,
Snavely, A., Sterling, T.: Exascale software study: Software challenges in extreme
scale systems (2009)
4. Brightwell, R., Pedretti, K., Underwood, K.D.: Initial performance evaluation
of the cray seastar interconnect. In: Proceedings of the 13th Symposium on
High Performance Interconnects, HOTI 2005, pp. 51–57. IEEE Computer Society,
Washington, DC (2005)
5. Brooks, D., Tiwari, V., Martonosi, M.: Wattch: A framework for architectural-level
power analysis and optimizations. In: Proceedings of the 27th Annual International
Symposium on Computer Architecture, ISCA 2000, pp. 83–94. ACM, New York
(2000). http://doi.acm.org/10.1145/339647.339657
6. Cappello, F., Geist, A., Gropp, B., Kale, L., Kramer, B., Snir, M.: Toward
exascale resilience. Int. J. High Perform. Comput. Appl. 23(4), 374–388 (2009).
http://dx.doi.org/10.1177/1094342009347767
7. Dennard, R., Gaensslen, F., Rideout, V., Bassous, E., LeBlanc, A.: Design of ion-
implanted mosfet’s with very small physical dimensions. IEEE J. Solid-State Cir-
cuits 9(5), 256–268 (1974)
8. Dongarra, J., Heroux, M.: Toward a new metric for ranking high performance com-
puting systems. Technical report SAND2013-4744, Sandia National Laboratories,
June 2013
9. Elnozahy, M., Bianchini, R., El-Ghazawi, T., Fox, A., Godfrey, F., Hoisie, A.,
McKinley, K., Melhem, R., Plank, J., Ranganathan, P., Simons, J.: System
resilience at extreme scale. Technical report, DARPA Technical report (2009)
10. Hewlett Packard: Cacti: An integrated cache and memory access time, cycle time,
area, leakage, and dynamic power model. http://www.hpl.hp.com/research/cacti/
11. Hsu, J.: When will we have an exascale supercomputer? In: IEEE Spectrum,
vol. 52, pp. 13–15. IEEE, January 2015
12. ITRS: International technology roadmap for semiconductors. http://www.itrs.net/
13. JEDEC: High bandwidth memory (HBM) dram. Technical report, JEDEC Solid
State Technology Association, October 2013
14. Kogge, P.: Tracking the eﬀects of technology and architecture on energy through
the top 500, green 500, and graph 500. In: 2012 International Conference on Super-
computing, ISC 2012 (2012)
15. Kogge, P., Dysart, T.: Using the top500 to trace and project technology and archi-
tecture trends. In: Proceedings of the 2011 ACM/IEEE Conference on Supercom-
puting, SC 2011 (2011)
16. Kogge, P., Shalf, J.: Exascale computing trends: Adjusting to the new normal for
computer architecture. Comput Sci. Eng. 15(6), 16–26 (2013)
17. Kogge, P.M., Bergman, K., Borkar, S., Campbell, D., Carlson, W., Dally, W.,
Denneau, M., Franzon, P., Harrod, W., Hill, K., Hiller, J., Karp, S., Keckler, S.,
Klein, D., Lucas, R., Richards, M., Scarpelli, A., Scott, S., Snavely, A., Sterling, T.,
Williams, R.S., Yelick, K.: Exascale computing study: Technology challenges in
achieving exascale systems. Technical report. CSE 2008–13, University of Notre
Dame, September 2008

Updating the Energy Model for Future Exascale Systems
339
18. Kogge, P.M., Resnick, D.R.: Yearly update: Exascale projections for 2013. Techni-
cal report SAND2013-9229, University of Notre Dame, Sandia National Laborato-
ries, October 2013
19. Kogge, P.M., Resnick, D.R.: Yearly update: Exascale projections for 2014. Techni-
cal report SAND2014-18651, University of Notre Dame, Sandia National Labora-
tories, 30 September 2014
20. Kogge, P., La Fratta, P., Vance, M.: Facing the exascale energy wall. In: 2010
International Workshop on Innovative Architecture for Future Generation High
Performance (IWIA), pp. 51–58, January 2010
21. Li, S., Ahn, J.H., Strong, R.D., Brockman, J.B., Tullsen, D.M., Jouppi, N.P.:
Mcpat: An integrated power, area, and timing modeling framework for multicore
and manycore architectures. In: Proceedings of the 42nd Annual IEEE/ACM Inter-
national Symposium on Microarchitecture, MICRO 42, pp. 469–480. ACM, New
York (2009). http://doi.acm.org/10.1145/1669112.1669172
22. Micron: Tn-41-01: Calculating memory system power for ddr3. http://www.
micron.com/∼/media/Documents/Products/Technical%20Note/DRAM/TN4603.
pdf
23. Moore, G.: Progress in digital integrated electronics. In: 1975 International Elec-
tron Devices Meeting, vol. 21, pp. 11–13 (1975)
24. Moore, G.: Cramming more components onto integrated circuits. Proc. IEEE
86(1), 82–85 (1998)
25. Navid, R., Chen, E.H., Hossain, M., Leibowitz, B., Ren, J., Chou, C.H., Daly, B.,
Aleksic, M., Su, B., Li, S., Shirasgaonkar, M., Heaton, F., Zerbe, J., Eble, J.: A 40
gb/s serial link transceiver in 28 nm CMOS technology. IEEE J. Solid-State Circ.
50(4), 814–827 (2015)
26. Pawlowski, J.T.: 3D stacked memory architectures for multi-core processors. In:
3D Architectures for Semiconductor Integration and Packaging, San Francisco, CA,
USA (2012)
27. Rosenfeld, P., Cooper-Balis, E., Jacob, B.: Dramsim2: A cycle accurate memory
system simulator. Comput. Archit. Lett. 10(1), 16–19 (2011)
28. Rosenfeld, P.: Performance exploration of the hybrid memory cube. Ph.D. thesis,
University of Maryland, College Park, MD (2014)
29. Simon, H.: no exascale for you! an interview with berkeley lab’s horst simon (2013).
http://www.top500.org/blog/no-exascale-for-you-an-interview-with-berkeley-
labs-horst-simon/
30. Sodani, A.: Race to exascale: Opportunities and challenges, December 2011. http://
www.microarch.org/micro44/ﬁles/Micro%20Keynote%20Final%20-%20Avinash%
20Sodani.pdf
31. Stevens, R.: On the race to exascale (2013). http://www.ci.anl.gov/blog/rick-
stevens-race-exascale
32. Thoziyoor, S.: A comprehensive memory modeling tool for design and analysis of
future memory hierarchies. Ph.D. thesis, University of Notre Dame, Notre Dame,
IN, USA (2008), aAI3442483
33. Yokokawa, M., Habata, S., Kawai, S., Ito, H., Tani, K., Miyoshi, H.: Basic design of
the earth simulator. In: Fukuda, A., Joe, K., Polychronopoulos, C.D. (eds.) ISHPC
1999. LNCS, vol. 1615, pp. 269–280. Springer, Heidelberg (1999)
34. Yokokawa, M., Shoji, F., Uno, A., Kurokawa, M., Watanabe, T.: The k computer:
Japanese next-generation supercomputer development project. In: Proceedings of
the 17th IEEE/ACM International Symposium on Low-power Electronics and
Design, ISLPED 2011, pp. 371–372. IEEE Press, Piscataway (2011). http://dl.
acm.org/citation.cfm?id=2016802.2016889

High-Order ADER-DG Minimizes
Energy- and Time-to-Solution of SeisSol
Alexander Breuer1(B), Alexander Heinecke2, Leonhard Rannabauer1,
and Michael Bader1
1 Technische Universit¨at M¨unchen,
Boltzmannstr. 3, 85748 Garching, Germany
{breuera,bader}@in.tum.de, lrannabauer@mytum.de
2 Intel Corporation,
2200 Mission College Blvd., Santa Clara, CA 95054, USA
alexander.heinecke@intel.com
Abstract. In this paper we give a comprehensive overview of our node-
level optimization of the high-order ﬁnite element software SeisSol aiming
at minimizing energy- and time-to-solution. SeisSol simulates dynamic
rupture and seismic wave propagation at petascale performance in pro-
duction runs. In this context we analyze the impact that convergence
order, CPU clock frequency, vector instruction sets and chip-level par-
allelism have on the execution time, energy consumption and accuracy
of the obtained solution. From a performance perspective, especially on
state-of-the-art and future architectures, the shift from a memory- to
a compute-bound scheme and the need for double precision arithmetic
with increasing orders of convergence is compelling. Our results show
that we are able to reduce the computational error by up to ﬁve orders
of magnitudes when increasing the order of the scheme from 2 to 7, while
consuming the same amount of energy.
Keywords: Energy- and time-to-solution · High-order · Vectorization ·
ADER · Discontinuous galerkin · Finite element method
1
Introduction
In recent years, a lot of research eﬀort has been spent on designing energy-
eﬃcient processing units. Microprocessor architectural research rapidly moves
into the direction of application-speciﬁc accelerators to address dark silicon chal-
lenges [25]; e.g. in 2014 an ASIC for deep learning was declared as best paper
at MICRO’14 [9] and ASPLOS’14 [8]. In contrast to such developments, general
purpose processors (CPUs, coprocessors and GPGPUs) have been enhanced by
more and more powerful SIMD units, aiming at increasing the computational
throughput at minimal additional energy costs. However, we have to keep in mind
that many HPC applications are not bound by ﬂoating-point performance [10]
on modern chips. Especially CPUs may consume a lot of power when running
memory-bound workloads, see Table 1. This transforms algorithms that trade
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 340–357, 2015.
DOI: 10.1007/978-3-319-20119-1 25

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
341
Table 1. Performance and full-box power consumption when running the STREAM
benchmark (triad) and DGEMM (MKL 11.2) on the HSW system. A full utilization of
the memory sub-system results into a power draw which does not scale with frequency
but also does not stay constant since caches run at higher frequency as well. For
DGEMM the power consumption nicely correlates with the achieved performance.
Benchmark
@1.2 GHz
@1.9 GHz
Performace
Power Performace
Power
STREAM - Triad
105 GiB/s
295 W 105 GiB/s
345 W
DGEMM - 60 k × 60 k × 192
610 GFLOPS 250 W 950 GFLOPS 400 W
data movements for computations into a superior alternative. Especially, if the
increased amount of compute allows for more accurate results.
For this purpose, we analyze the potential for saving energy by using higher-
order ﬁnite element methods in the context of seismic simulations. High-order
spatial-temporal discretizations are an energy-eﬃcient approach for three rea-
sons. First, they allow for shorter runtimes with respect to accuracy. Second,
they lower the pressure on the memory subsystem due to an increased number
of local operations. And third, they are able to exploit a core’s SIMD capa-
bilities due to increased sizes of the involved matrix operators. We take the
Arbitrary high-order accurate DERivative Discontinuous Galerkin (ADER-DG)
code SeisSol as proxy for our work. SeisSol’s back-end requires small sparse and
dense matrix multiplication kernels which are selected based on an auto-tuning
model to ensure fastest execution times for a given order of convergence and
architecture. Having the optimal conﬁguration for the matrix kernels at hand,
we analyze important metrics of the overall software such as time-to-solution
(in the context of our work the most accurate solution) and most importantly
energy-to-solution. Keeping the relation of performance and full-box power con-
sumption for DGEMM and the STREAM benchmark on state-of-the-art hard-
ware – as shown in Table 1 – in mind, we know that for SeisSol’s properties a
fast compute-bound variant is more energy-eﬃcient than a memory-bound one.
We performed a cross-plattform analysis of SeisSol’s performance on several
node types and coprocessors found in today’s supercomputers with diﬀerent
maturities of the vector instructions sets:
WSM A dual-socket Intel R
⃝Xeon R
⃝X5690 server, 12 cores @3.46 GHz, 48 GB of
DDR3-1333 memory, 128-bit SSE4.2 vector instruction set, 41 GiB/s mem-
ory bandwidth, 166 GFLOPS double precision peak performance, idle power
consumption of 160 W and DGEMM power consumption of 350 W.
SNB A dual-socket Intel R
⃝Xeon R
⃝E5-2670 server, 16 cores @2.6 GHz, 128 GB
of DDR3-1600 memory, 256-bit AVX vector instruction set, 75 GiB/s mem-
ory bandwidth, 333 GFLOPS double precision peak performance, idle power
consumption of 100 W and DGEMM power consumption of 280 W.
HSW A dual-socket Intel R
⃝Xeon R
⃝E5-2699 v3 server, 36 cores @1.9 GHz (guar-
anteed, P0-frequency is 2.3 GHz), 128 GB of DDR4-1866 memory, 256-bit

342
A. Breuer et al.
AVX2 vector instruction set, 105 GiB/s (cluster-on-die enabled) memory
bandwidth, 1.1 TFLOPS double precision peak performance @1.9 GHz, idle
power consumption of 75 W and DGEMM power consumption of 400 W.
KNC A Intel R
⃝Xeon Phi
TM 5110P coprocessor, 60 cores @1.06 GHz, 8 GB of
GDDR5 memory, 512-bit MIC vector instruction set, 150 GiB/s memory
bandwidth and 1 TFLOPS double precision peak performance, idle power
consumption of 100 W and DGEMM power consumption of 225 W.
This cross-platform analysis was carried out on two entirely diﬀerent benchmark
scenarios: (a) a synthetic convergence test in Sect. 4.2 that allows for a compar-
ison of rigorous mathematical error-norms using regularly-reﬁned meshes and
(b) the well-established Layer Over Halfspace (LOH.1) benchmark in Sect. 4.3,
which operates on a fully unstructured mesh. We also study the eﬀect of using
single-precision arithmetic with respect to accuracy and execution time. In order
to ensure results that can be easily transferred to a full system perspective, we
limit our (detailed) power measurements to full-box numbers rather than RAPL
counters [24] for estimating the CPU-only power consumption.
To the best of our knowledge this is the ﬁrst work that performs a detailed
analysis of a higher-order DG code in the stress ﬁeld of accuracy, execution
time and energy-to-solution. Therefore this work should deliver a valuable exten-
sion to the fast-growing ﬁeld of energy awareness in scientiﬁc high-performance
computing. In [6,14,15] energy-proﬁle changes due to use of vectorization and
parallelization are analyzed for several memory- and compute-bound micro-
benchmarks on WSM- and SNB-type of architectures. Speciﬁcally to large dense
linear algebra applications, [1,4,11,12,21] analyze performance with respect to
energy consumption for various BLAS and LAPACK routines either using full-
box power or processor built-in counters. There is also application-speciﬁc work,
e.g. in [2,7], where the energy footprint of the weather code COSMO is presented.
A similar analysis is performed for N-body algorithms in [20,27]. Additionally,
[23,26] present approaches how auto-tuning of applications can help to reduce
their energy-footprint.
2
Computational Core
In this section we give a brief introduction into SeisSol’s computational core.
Please refer to [5,16] for a more detailed description and to [13,18] for a detailed
derivation from the underlying system of hyperbolic partial diﬀerential equations
and for handling of boundary conditions and source terms.
SeisSol solves the elastic wave equations in velocity-stress formulation
qt + Aqx + Bqy + Cqz = 0.
(1)
q(x, t) = (u, v, w, σxx, σyy, σzz, σxy, σxz, σyz)T is the space-time-dependent vec-
tor of elastic quantities. u, v and w are the particle velocities in Cartesian x-, y-
and z-direction, σxx, σyy and σzz the normal stresses and σxy, σxz and σyz the
shear stress components. A(x), B(x) and C(x) are the three Jacobians including
space-dependent material parameters.

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
343
SeisSol uses an ADER-DG Finite Element Method for spatial-temporal dis-
cretization. For meshing we rely on ﬂexible unstructured tetrahedral meshes.
For a given convergence order O – results for O ∈{2, 3, . . . , 7} are presented
in this work – we use BO basis functions per element discretizing each of the
components in the vector of elastic quantities q(x, t) in every tetrahedron. For
the considered orders of convergence the number of basis functions are B2 = 4,
B3 = 10, B4 = 20, B5 = 35, B6 = 56 and B7 = 84. In total, storing the modes
of the basis functions for every elastic quantity results in BO × 9 degrees of
freedom (DOFs) per element. We can split the wave-propagation core into three
integration operators performing time-, volume and boundary integration.
ADER. The ADER-time integration derives the time integrated DOFs T n,Δt
k
in
a tetrahedron k. It integrates the DOFs of an element from time step tn to the
next time step tn+1 = tn + Δt, where Δt is the time step width:
T n,Δt
k
:= Tk (Qn
k, Δt) =
O−1

j=0
Δtj+1
(j + 1)!
∂j
∂tj Qk(tn).
(2)
With the DOFs at time step tn as starting point,
∂0
∂t0 Qk = Qn
k, the derivatives
∂j
∂tj Qk are computed by a recursive scheme:
∂j+1
∂tj+1 Qk = −
3

c=1
ˆKξc
 ∂j
∂tj Qk

A⋆
k,c.
(3)
ξ1−ξ2−ξ3 is the reference coordinate system. ˆKξc are the three transposed global
stiﬀness matrices (size BO × BO) multiplied by the diagonal inverse mass matrix
in preprocessing. The star matrices A⋆
k,c (size 9 × 9) summarize the eﬀects of the
ﬂux function in reference coordinates and depend on the per-element material
parameters.
Volume. The volume kernel discretizes the integration of the time integrated
DOFs T n,Δt
k
over the volume of the element:
Vk

T n,Δt
k

=
3

c=1
˜Kξc 
T n,Δt
k

A⋆
k,c.
(4)
˜Kξc are the three non-transposed stiﬀness matrices (size BO × BO) multiplied
by the inverse mass matrix in preprocessing. As in the ADER-time integration,
A⋆
k,c are the three star matrices of size 9 × 9.
Boundary. As we are using a Discontinuous Galerkin scheme, the numerical
solution is allowed to be discontinuous at the interface of two neighboring tetra-
hedrons k and ki. This gives rise to General Riemann Problems, which are solved
by the boundary integration operator:
Fk,i

T n,Δt
k
, T n,Δt
ki

= ˆF −,i 
T n,Δt
k

ˆA+
k,i + ˆF +,i,jk(i),hk(i) 
T n,Δt
ki

ˆA−
k,i.
(5)

344
A. Breuer et al.
In Eq. 5 the per-element-face ﬂux-solver ˆA±
k,i transforms the time integrated
DOFs T n,Δt
k
and T n,Δt
ki
of tetrahedron k and the i-th a face neighbor ki to a face-
aligned coordinate system, solves the Riemann problem exactly and transform
the solution back to reference coordinates. ˆF −,i and ˆF i,jk(i),hk(i) are the 52
global ﬂux matrices [16] (size BO × BO) multiplied by the inverse mass matrix
in pre-processing.
Explicit Update Scheme. The update scheme from time step tn to tn+1 can be
summarized by combining the three integration operators:
Qn+1
k
= Qn
k + Vk −
4

i=1
Fk,i.
(6)
Time integration (Eq. 2), volume integration (Eq. 4) and the ﬁrst summand of
the boundary integration in Eq. 5 depend on element-local data only. The second
summand of the boundary integration in Eq. 5 accounts for the contribution of
face neighbor ki to the boundary integral. Therefore the complete evaluation
of the sum in Eq. 6 requires the time integrated DOFs T n,Δt
ki
of the four face
neighbors ki, i ∈1 . . . 4.
3
Implementation
The recently discussed operations reduce to small dense and sparse matrix mul-
tiplications. However, calling a rich math library, which is optimized for large
matrix operations, such as Intel MKL, would not result in best performance. We
therefore described (in previous work) a code generation approach that takes
many specialties of the ADER-DG scheme into account, e.g. zero blocks that are
gradually generated during the time integration step [5]. We also pointed out
that with more sophisticated vector-instructions sets (longer vectors, support for
fused-multiply-add (FMA) operations and fast aligned memory operations) it is
beneﬁcial to increase the amount of operations handled as dense [16].
We re-implemented the dense code path of our matrix multiplication code
generator from scratch. In contrast to our previous implementation, we now
directly generate assembly code (instead of compiler intrinsics) which gives us
more control with respect to register allocation. This is important since the
biggest change in this re-implementation is the support for arbitrary M, K,
LDA, LDB and LDC (BLAS identiﬁers). Support for leading dimensions allows
to mix-and-match the various diﬀerent shapes in the time and volume kernel and
therefore reduce the amount of zero-padding. For best performance we make sure
that LDA and LDC are always aligned to the vector-length of the underlying
instruction set extension (16 byte for SSE, 32 byte for AVX and AVX2 and 64
byte on MIC). Suﬃcient alignment avoids load operations to A or store oper-
ations to C which suﬀer from cacheline splits. Furthermore, our optimizations
naturally extend to the low-order routines and guarantee high performance in
these cases as well.

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
345
0
5
10
15
20
25
30
2
3
4
5
6
7
GFLOPS
convergence order
WSM
SNB
HSW
0.00
0.17
0.33
0.50
0.67
0.83
1.00
2
3
4
5
6
7
fraction of peak
convergence order
WSM
SNB
HSW
Fig. 1. Double precision GEMM kernel performance for O2 to O7 on WSM, SNB and
HSW. DGEMM performance for BO × 9 × BO shapes (left) and BO × 9 × 9 cases
(right) is shown. GFLOPS are depicted as bars whereas fraction of peak performance is
given by lines. DGEMM operations were repeated for 10,000 times on a hot L1 cache.
As the number of elastic quantities is nine (see Sect. 2), N = 9 holds always
true and all assembly building blocks are centered around this special case.
If needed in future – e.g. for extending our code generation approach to new sets
of equations – this limitation can be removed easily. The underlying assembly
buildings blocks follow identical ideas as already described in [5,16]. However,
in case of generating code for KNC we added more carefully-tuned prefetch
instructions. Additionally, the AVX2 FMA-enabled instruction-set extension on
HSW allowed us to include a 16 × 3 building block in addition as the intermediate
results no longer need to be kept in registers. Therefore these free registers can be
used as accumulators for ﬁnal results. Furthermore, we added full single precision
support for all matrix sizes of interest.
These improvements in the code generation process turn the current version
of SeisSol into a code that is naturally optimized for several convergence orders,
whereas our previous work [5,16] focused on the O6 case. Whether a certain
routine should be executed by a sparse or dense matrix operation is determined
up-front by auto-tuning. This step generates an optimal conﬁguration for each
combination of target architecture, order of convergence and machine precision.
The code generator is publicly available as open source at https://github.com/
hfp/libxsmm.
Figure 1 compares the performance for a selection of DGEMM operations
used in SeisSol running single-threaded on all diﬀerent architectures. For SSE,
AVX and AVX2 roughly the same eﬃciency can be reached and this eﬃciency is
comparable to DGEMM performance as being measured when running HPL [17].
Especially the higher-order kernels run up to 30–40 % faster than when calling
BLAS libraries. On KNC we observed lower eﬃciencies (of roughly 70 % of peak
performance) which is caused by two facts: a) at least two running threads
are needed to fully load a core (therefore we excluded KNC from Fig. 1) and
b) N = 9 does not allow us to optimally exploit KNC’s register ﬁle which has 32
entries, out of which we use only 10. In order to leverage the added single preci-
sion kernels, slight changes were required in SeisSol as well. We implemented a

346
A. Breuer et al.
mixed-precision version of SeisSol: the elastic wave propagation solver runs
entirely with single precision arithmetic whereas other parts, such as source
term calculations, remain in double precision.
Besides improving the performance of all matrix multiplication routines, we
also re-arranged the order of applying updates to an element’s DOFs. Whereas
before we strictly separated the three diﬀerent integrators, we switched to an
element-centric formulation: ﬁrst we compute the time integrated DOFs and
apply all local updates and second we perform the neighboring part of the ﬂux
computation. In low-order runs this change substantially saves bandwidth, for
high-order simulations global matrices can be kept in the private L2 caches for
the local integration parts.
Additionally, we optimized memory allocation in SeisSol. All global matrix
operators are allocated in a single malloc operation and a memory manager
handles whether a matrix is stored sparse or dense. Furthermore, we split the
element-local non-DOF data (star matrices and ﬂux solvers) into two parts,
one for the local update and one for the neighboring integral. This ensures more
accurate hardware prefetching and shorter access latencies by a NUMA-aware
initialization of all data structures. These changes prepare SeisSol for future chips
that may oﬀer several memory subsystems, also.
Finally, we want to mention that these improvements of SeisSol’s computa-
tional back-end result into roughly 15–20 % shorter runtimes while maintaining
the same hardware eﬃciencies. Combining Fig. 1 and Table 1 it becomes obvious
that these shorter runtimes directly translate to less consumed energy.
4
Experiments and Results
In the following sections we present a detailed analysis of the modernized SeisSol
code incorporating the discussed changes and extensions. This analysis was car-
ried out on two diﬀerent benchmarks, a synthetic convergence test (in Sect. 4.2)
and a standardized veriﬁcation benchmark (in Sect. 4.3) that utilizes an adaptive
unstructured mesh. In both cases we investigate the relationships between order
of convergence, clock frequency, vector instruction sets and selected machine
precision. However, ﬁrst we describe our power measurement methodology.
4.1
Power Measurement Setup
As pointed out before, SeisSol is used for high-resolution seismic simulations in
large-scale production runs. Since we focus on single-node runs in this work,
we need to make sure that the obtained insights can be easily transferred to
cluster-sized executions. We therefore decided to measure only full-box power
consumptions of standard 2U rack-servers which are a common building block
of supercomputers. We did not perform CPU-only measurements through e.g.
RAPL performance counters. Additionally, most of our power measurements are
taken at full CPU speed, RAPL counter measurements would not gain signiﬁcant
additional information as the CPU runs close to or at their thermal design power

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
347
(TDP) limit due to our highly-eﬃcient kernels, cf. Fig. 1. Furthermore, WSM and
KNC systems do not oﬀer RAPL counters which would limit comparisons, too.
For the WSM, SNB and HSW systems we used non-intrusive power measure-
ment tools1 which plug in between the system’s power inlet and the wall’s power
outlet. Both oﬀer a resolution of 1 Hz. This is not a limitation as runs of interest
are substantially longer. Furthermore, we disabled the second power supply unit
(PSU) (where available) of each server. The Wattsup power data was collected
on-the-ﬂy by a second machine (master) to which the device was connected via
USB. This master also launched SeisSol over network and saved all relevant
application information, such as runtimes and numerical output, for interpret-
ing the power measurements in an application context. On KNC we followed
a slightly diﬀerent procedure. In contrast to [16] we switched from an oﬄoad
model to native execution in order to prepare SeisSol for future socketable Xeon
Phi processors. To exclude various host system overheads, the micsmc utility was
used at a sampling rate of 1 Hz to measure full coprocessor power. Limitations
and overheads as described in [19] did not show up since we just measured the
power for fully-loaded coprocessors. Furthermore, power supply transformation
ineﬃciencies are not included.
4.2
Synthetic Benchmark
Benchmark Description. The synthetic benchmark was introduced for conver-
gence analysis in [13]. It consist of a cubic domain of size [−50 m, 50 m]3 with
constant material parameters. This results in uniform wave speeds throughout
the entire domain. Additionally the benchmark uses periodic boundary condi-
tions and assumes smooth initial data leading to waves propagating in diagonal
direction. As described in greater detail in [13] after
√
3·100 seconds of time the
propagation of the waves matches exactly the initial data. Thus diﬀerent error
norms of all elastic variables can be computed easily. Together with a simple
and controllable mesh reﬁnement this is the motivation for using the synthetic
benchmark in this work.
We discretized the cubic domain of the benchmark by a series of regular cubic
meshes. Each of the cubes is again subdivided into ﬁve tetrahedrons. The result
is a total number of 5 · N 3
1D tetrahedrons, where N1D is the number of cubes per
dimension. For temporal discretization we used a time step, that resembles 50 % of
the stability limit of Runge-Kutta DG-schemes. Taking into account the CFL con-
dition, wave speeds and in-sphere-diameters, the time step is given explicitly as:
Δt(O, N1D) =
50
(2O −1)(
√
3 + 3)N1D
.
(7)
We performed simulations of the synthetic benchmark for convergence orders
O2 −O7 and increased the number of per-dimension cubes N1D by two in every
step: N1D ∈{4, 6, 8 . . .}.
1 WSM, HSW: Wattsup powermeter (accuracy: +/−1.5 %), SNB: MEGWARE clust-
safe.

348
A. Breuer et al.
1E-12
1E-10
1E-08
1E-06
1E-04
1E-02
1E+00
25
16.67
12.5
 10
5
2.5
2
1.67
1.25
1
error
mesh width (m)
s2
d2
s3
d3
s4
d4
s5
d5
s6
d6
s7
d7
Fig. 2. Convergence of the synthetic benchmark for orders O2 – O7. Shown is the mesh
width 100/N1D against the error in the L∞-norm for the variable σyz. Orange dashed
lines represent single precision, blue solid lines double precision. The slopes of the gray
triangles next to the respective curves illustrate the mathematical convergence rate
(Color ﬁgure online).
In production runs, SeisSol spends almost all of the computational time in
the time marching loop – for example 99.56 % for the machine-size production
run presented in [16]. Thus for all simulations we measured only the time and
energy spent in the time marching loop, neglecting initialization.
Performance. In Fig. 2, we present numerical convergence in the L∞-norm for
the elastic variable σyz using the synthetic benchmark. All runs were performed
on the SNB system using the AVX-instruction set. With respect to convergence
alone, the results are representative for all studied architectures because cross-
architecture changes of the error-norms (e.g. by FMA instructions) are barely
measurable. In extension to the described benchmark setting we performed all
runs in double precision and, for the ﬁrst time, also in single precision with our
new uniﬁed computational core of SeisSol. For all runs, we observed the high-
order convergence expected numerically for increasing mesh sizes. In the single
precision runs, we hit the machine precision with an error between 10−4 and
10−5, in double precision at 10−11. For clarity we cut oﬀthe single precision
curves shortly after this stagnation. Before this point the curves of single and
double precision of all runs lie on top of each other and thus the quality of the
solution is almost identical for single and double precision.
Figure 3 illustrates the L∞-error of variable σyz in dependency of the con-
sumed energy for the HSW machine (for all tests in this section the HSW machine
is clocked at 1.9 GHz). To ensure reliable power-measurements we increased the
original starting point of the study, N1D = 4, for every order, such that the sim-
ulation consumed at least 50 kJ. In comparison to numerical convergence with
respect to mesh width only (Fig. 2), the slope of the curves is smaller. Keeping

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
349
in mind that the total number of element updates not only depends on the mesh
width, but also on the time step width (Eq. (7)) this behavior is expected. Again
we observe increasing slopes and a magnitude diﬀerence in terms of energy-to-
solution for higher orders. For convergence orders O2 and O3, single precision
reduces the amount of consumed energy by more than 35 % and 38 %. The the-
oretically possible 50 % (we measured the same power consumption for single
and double precision runs with an error within 3 % on all platforms) are not
fully reached, as we align the elements’ DOFs to the 32-byte boundary. This, in
general, imposes some overheads with longer vectors and switching from double
to single precision naturally doubles the vector length.
1E-10
1E-09
1E-08
1E-07
1E-06
1E-05
1E-04
1E-03
1E-02
 75
 100
 150
 250
 500
 750
 1000
error
energy (kJ)
shsw2
dhsw2
shsw3
dhsw3
shsw4
dhsw4
dhsw5
dhsw6
dhsw7
Fig. 3. L∞-error of variable σyz in dependency of the consumed energy for the HSW
machine in single- and double-precision.
Finally, in Fig. 4 we evaluated the measured energy-to-solution curves by
linear interpolation in log-log-space at 150 kJ for all architectures (using on-
demand frequency for WSM and SNB). For this energy budget we can now easily
compare the achieved accuracy depending on convergence order and architecture.
Note that we excluded settings beyond convergence and low order settings not
ﬁtting into the memory of KNC due to slow convergence. Comparing O2 and
O7 on HSW we see an error reduction of ﬁve orders of magnitude. For the cross-
architecture comparison we select O6. According to Fig. 1 roughly the same
eﬃciency can be reached on all platforms. In this case the error is reduced by a
factor of 3.4 when switching from WSM to SNB and 4× when comparing SNB
to KNC and HSW. Note that the factors are a combination of architectural
improvements and the non-linearity of the high-order convergence.

350
A. Breuer et al.
error
1E-08
1E-07
1E-06
1E-05
1E-04
1E-03
1E-02
1E-01
dwsm2
swsm2
dwsm3
swsm3
dwsm4
dwsm5
dwsm6
dwsm7
dsnb2
ssnb2
dsnb3
ssnb3
dsnb4
dsnb5
dsnb6
dsnb7
dknc4
dknc5
dknc6
dknc7
dhsw2
shsw2
dhsw3
shsw3
dhsw4
dhsw5
dhsw6
dhsw7
1.305E-08
1.014E-07
1.092E-06
1.442E-05
1.924E-04
2.54E-04
3.62E-03
4.273E-03
1.256E-08
9.857E-08
1.173E-06
1.484E-05
7.908E-08
4.014E-07
3.144E-06
3.067E-05
3.094E-04
3.836E-04
4.587E-03
5.147E-03
2.791E-07
1.376E-06
8.302E-06
6.164E-05
4.199E-04
5.909E-04
5.362E-03
7.14E-03
Fig. 4. Error with respect to a 150 kJ energy-budget for all matching settings. Shown
is the interpolated L∞-error of variable σyz for the diﬀerent architectures, orders of
convergence and single- and double-precision.
These ﬁndings align well with the Dennard scaling2 between SNB and HSW
on an iso-frequency and iso-TDP level. From WSM to SNB we even see a supe-
rior scaling. This is due to the fact that we use full-box power measurements,
but strictly speaking Dennard scaling applies to the processor only. Other parts
of the system became more power-eﬃcient, too, such that SNB clearly exceeds
the WSM machine. One KNC coprocessor is able to achieve the same energy
eﬃciency as the entire HSW machine. The reason for not (clearly) outperform-
ing HSW is the (already analyzed) lower eﬃciency of the matrix kernels. This
emphasizes that for compute-bound applications the fastest execution is also
most likely the most energy-eﬃcient one.
4.3
Layer over Halfspace
Benchmark Description. The Layer Over Halfspace (LOH.1) benchmark is part
of the wave propagation model set WP2 of the “SeISmic MOdeling Web INter-
facE”3 (SISMOWINE), a project aiming at the veriﬁcation of numerical models
in seismology.
In this benchmark two diﬀerent materials are used, one in the upper layer
down to 1000 m in depth and one in the halfspace of the remaining domain.
Analog to [13] we used an adaptively reﬁned mesh of a computational domain
extending [−15 km, 15 km]2 × [−17 km, 0 km] with free-surface boundary condi-
tions on the surface and outﬂow boundary conditions everywhere else. The mesh
2 Doubling the number of transistors doubles the amount of computations within the
same energy budget; number of transistors: WSM: 2 × 1.17 B, SNB: 2 × 2.26 B, HSW:
2 × 5.57 B.
3 Available at http://www.sismowine.org.

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
351
consists of 386,518 elements and is unstructured with faces aligned only to the
interface of the two layers and the boundaries. The setup is illustrated in Fig. 5.
Elements in the layer are gray, elements in the halfspace are white. The seismic
source is located at (0, 0, −2000) and shown in red with a grid for orientation
with respect to receiver locations. For veriﬁcation SISMOWINE oﬀers reference
solutions based on the discrete wavenumber method at nine receivers located at
the free surface for the ﬁrst nine seconds in simulated time.
In all runs of the LOH.1 benchmark, we simulated until reaching either the
entire nine seconds in simulated time or a minimum of ﬁve minutes in com-
putational time. We extended the parameters space as the mesh size is now
Fig. 5. Setup of the LOH.1 scenario.
Shown
is
a
only
a
part
of
size
[−2 km, 15 km]2 × [−17 km, 0 km].
constant. In contrast to the convergence
benchmark in Sect. 4.2, we perform a fre-
quency study within each CPU’s sup-
ported frequency spectrum. Note, that this
was only possible for CPU-based platforms
as Xeon Phi does not oﬀer DVFS [19],
which can be easily modiﬁed by software.
Additionally, we study how the consumed
power depends on the vector instruction
set. We therefore run the LOH.1 bench-
mark with the SSE, AVX and AVX2 back-
end on the HSW machine.
Performance. In comparison to the syn-
thetic benchmark of Sect. 4.2 the quality
of the solution can not be quantiﬁed by
simple error-norms due to the complexity
of the benchmark. In Fig. 6 we show exem-
plary a comparison of the particle velocity
u at receiver nine (8,647 m, 5,764 m, 0 m)
as given by the reference solution and obtained with SeisSol using convergence
orders O2 and O7. Additional we show the envelope misﬁt [22] of the seismo-
gram in Fig. 7. Visually we reproduce the reference solution and an increasing
ﬁt of the high order simulation is clearly visible. A more detailed veriﬁcation of
SeisSol is available at the homepage of the SISMOWINE-project.
Figure 8 depicts the performance when running the LOH.1 benchmark
depending on chip frequency and on diﬀerent machines. We clearly see that
low-order runs show nearly no performance improvement with increasing clock
frequency. Here we operate close to the memory bandwidth limit. E.g. on HSW
the measured average bandwidth is 72 GiB/s (with peaks at 92 GiB/s) and the
analytical overall byte/FLOP ratio is 1.3. With Table 1 in mind it is therefore
beneﬁcial from an energy perspective to run the memory-bound convergence
orders at lowest possible frequency. This is well-aligned with ﬁndings presented
in [3]. In case of high-order we reach full compute-bound performance with con-
vergence O4 on WSM and SNB and convergence order O5 on HSW. However, on

352
A. Breuer et al.
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
u (m/s)
time (s)
reference
SeisSol O2
SeisSol O7
Fig. 6. Comparison of the particle velocity u at receiver nine. Shown is the reference
solution of the SISMOWINE project against Seissol using O2 and O7.
time (s)
0
1
0
2
3
4
5
6
7
8
0
1
0
2
3
4
5
6
7
8
1
2
3
4
1
2
3
4
frequency (Hz)
frequency (Hz)
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
2.0
Fig. 7. Comparison of the envelope misﬁt with respect to the reference solution of the
SISMOWINE project. Shown is the misﬁt for particle velocity u at receiver nine over
the simulation time against the frequency for O2 (top) and O7 (bottom).
HSW the performance of O4 is closer to O5 than to O3. Due to 64-byte alignment
on KNC best performance is achieved for order O6 since B6 = 56 is divisible by
eight (size of a double) and thus no additional zero-padding is required. For these
higher-order runs we can also observe the expected frequency dependency in the
achieved GFLOPS performance. From a MFLOPS/W perspective, we are able
to reproduce the convergence test’s excellent results at iso-frequency (1.2 GHz).
HSW is delivering 2.6× more performance within the same power budget and
having a 2.5× higher transistor count. As a side note we want to highlight that
the achieved MFLOPS/W numbers are very close to Green500 entries of similiar
clusters, which underlines the high hardware eﬃciency of SeisSol.
However, maintaining the Dennard scaling is only possible when fully utilizing
all functionality and therefore AVX2. Often performing cross-architecture opti-
mizations might appear diﬃcult. Therefore Fig. 9 analyses how the performance

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
353
GFLOPS
0
100
200
300
400
500
600
700
800
MFLOPS / W
0
250
500
750
1000
1250
1500
1750
2000
234567
23456
7
234567
23 4 567
234567
234567
23456
234
5 67
234567
234567
234567
23456
7
234567
23 4 567
234567
234567
23456
234
5 67
234567
234567
355
375
345
283
152
84
299
317
307
256
155
82
192
210
214
196
140
83
178
159
133
100
57
89
109
115
115
61
91
69
84
90
90
74
52
41
51
54
54
46
33
51
56
61
63
55
31
32
32
30
28
25
29
35
39
42
44
43
25
759
810
731
592
397
234
861
893
845
700
465
262
850
889
862
749
527
318
947
845
734
545
307
310
371
380
358
280
186
331
392
404
377
296
202
283
339
347
332
265
182
93
149
164
174
176
156
108
145
157
168
173
161
122
133
142
147
142
106
1.60GHz 2.26GHz 3.46GHz
1.20GHz 2.00GHz 2.60GHz
1.06GHz
1.20GHz 1.90GHz 2.30GHz
WSM
SNB
HSW
KNC
Fig. 8. Non-zero FLOPs (dark colors) in double precision and hardware-level FLOPs (light colors) of the LOH.1 benchmark depending
on frequency. Shown are GFLOPS (top) and the MFLOPS per Watt (bottom) over all orders of convergence (Color ﬁgure online).

354
A. Breuer et al.
on HSW changes when using older vector instruction sets. We recognize nearly
a factor of two between SSE3 and AVX and a diﬀerence of up to 1.5 between
AVX and AVX2 with respect to performance. Power-wise the factors are 1.6 for
SSE3 and AVX and 1.3 between AVX and AVX2. When comparing SSE3 to
AVX2, we gain nearly 3× in performance but also have to pay a factor of 2× in
power consumption. This can be regarded as a win-win situation. If it is possible
to fully utilize the AVX2 instruction set, then the most power-eﬃcient execu-
tion is possible. But if only an SSE3 code is available, the HSW system delivers
roughly twice as much performance as the WSM machine and is approximately
2.5× more energy eﬃcient. The comparison between SNB and HSW results in
similar insights and is therefore not described.
GFLOPS
0
175
350
525
700
2 3 4 5
6 7
2 3 4 5 6 7
2 3 4 5 6
7
MFLOPS / W
0
500
1000
1500
2000
2 3 4 5 6 7
2 3
4 5 6 7
2
3 4 5 6 7
861
893
845
700
465
262
652
694
703
628
445
253
404
416
443
455
403
262
299
317
307
256
155
82
197
218
231
220
150
80
109
124
82
114
136
127
SSE3
AVX
AVX2
SSE3
AVX
AVX2
Fig. 9. GFLOPS (left) and MFLOPS per Watt (right) over all convergences orders
and instruction sets on HSW @1.9 GHz.
5
Conclusion
In this paper we summarized recent advancements with respect to node-level
performance of the high-order ﬁnite element seismic simulation software SeisSol.
These improvements include Intel Xeon E5 v3 (AVX2), single precision and
NUMA support and last but not least a cache-friendly restructuring of the
involved operators. Keeping our previous work on the scalability of SeisSol in
mind [5,16], we expect that all node-level enhancements directly translate to a
faster execution on cluster-level as well.
When evaluating our changes we put a strong focus on energy- and time-to-
solution. We analyzed the impact of convergence order, frequency, vector instruc-
tion sets and machine precision on the execution time, energy consumption and
quality of the obtained solution. We demonstrated that on modern architectures
the computational error can be reduced by up to ﬁve orders of magnitudes when
switching from convergence order O2 to O7, while staying within the same power
budget. From a hardware perspective we were able to reproduce Dennard scaling
across several generations of CPUs when utilizing the most advanced instruc-
tion set in each generation. Recapitulating our frequency scaling experiment
we can state the following two aspects from an energy-eﬃciency point of view.
First, memory-bound applications should be executed at the lowest frequency

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
355
supported by the processor and second, compute-bound application consume
minimal energy when running at a medium frequency but at high eﬃciency.
To put it short, the best-optimized and compute-bound high order execution is
also the most energy-eﬃcient one from an energy-to-solution perspective.
Acknowledgments. Our project was supported by the Intel Parallel Computing Cen-
tre “ExScaMIC - Extreme Scalability on x86/MIC”. We gratefully acknowledge the
respective support by Intel Corporation.
Optimization Notice: Software and workloads used in performance tests may have
been optimized for performance only on Intel microprocessors. Performance tests, such
as SYSmark and MobileMark, are measured using speciﬁc computer systems, com-
ponents, software, operations and functions. Any change to any of those factors may
cause the results to vary. You should consult other information and performance tests
to assist you in fully evaluating your contemplated purchases, including the perfor-
mance of that product when combined with other products. For more information go
to http://www.intel.com/performance. Intel, Xeon, and Intel Xeon Phi are trademarks
of Intel Corporation in the U.S. and/or other countries.
References
1. Aliaga, J.I., Barreda, M., Dolz, M.F., Quintana-Orti, E.S.: Are our dense linear
Algebra libraries energy-friendly? Comput. Sci. Res. Dev. 30(2), 187–196 (2015)
2. Anzt, H., Beglarian, A., Chilingaryan, S., Ferrone, A., Heuveline, V., Kopmann,
A.: A uniﬁed energy footprint for simulation software. Comput. Sci. -Res. Dev.
29(2), 131–138 (2014)
3. Auweter, A., Bode, A., Brehm, M., Brochard, L., Hammer, N., Huber, H., Panda,
R., Thomas, F., Wilde, T.: A case study of energy aware scheduling on supermuc.
In: Kunkel, J.M., Ludwig, T., Meuer, H.W. (eds.) ISC 2014. LNCS, vol. 8488, pp.
394–409. Springer, Heidelberg (2014)
4. Bosilca, G., Ltaief, H., Dongarra, J.: Power proﬁling of cholesky and qr factor-
izations on distributed memory systems. In: Third International Conference on
Energy-Aware High Performance Computing, Hamburg, September 2012
5. Breuer, A., Heinecke, A., Rettenberger, S., Bader, M., Gabriel, A.-A., Pelties, C.:
Sustained petascale performance of seismic simulations with seissol on supermuc.
In: Kunkel, J.M., Ludwig, T., Meuer, H.W. (eds.) ISC 2014. LNCS, vol. 8488, pp.
1–18. Springer, Heidelberg (2014)
6. Cebrian,J.W., Natvig, L., Meyer, J.C.: Improving energy eﬃciency through paral-
lelization and vectorization on Intel Core i5 and i7 processors. In: High Performance
Computing, Networking Storage and Analysis, SC Companion: 0:675–684 (2012)
7. Charles, J., Sawyer, W., Dolz, M.F., Catal´n, S.: Evaluating the performance and
energy eﬃciency of the COSMO-ART model system. Comput. Sci. Res. Dev. 30(2),
177–186 (2015)
8. Chen, T., Du, Z., Sun, N., Wang, J., Wu, C., Chen, Y., Temam, O.: Diannao: a
small-footprint high-throughput accelerator for ubiquitous machine-learning. In:
Proceedings of the 19th International Conference on Architectural Support for
Programming Languages and Operating Systems, ASPLOS 2014, pp. 269–284.
ACM, New York (2014)

356
A. Breuer et al.
9. Chen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun,
N., Temam, O.: Dadiannao: a machine-learning supercomputer. In: ACM/IEEE
International Symposium on Microarchitecture (MICRO), December 2014
10. Cheveresan, R., Ramsay, M., Feucht, C., Sharapov, I.: Characteristics of workloads
used in high performance and technical computing. In: Proceedings of the 21st
Annual International Conference on Supercomputing, ICS 2007, pp. 73–82. ACM,
New York (2007)
11. Demmel, J., Gearhart, A.: Instrumenting linear algebra energy consumption via
on-chip energy counters. Technical report (2012)
12. Dongarra, J., Ltaief, H., Luszczek, P., Weaver, V.M.: Energy footprint of advanced
dense numerical linear algebra using tile algorithms on multicore architecture. In:
2012 Second International Conference on Cloud and Green Computing (CGC), pp.
274–281. IEEE (2012)
13. Dumbser, M., K¨aser, M.: An arbitrary high-order discontinuous Galerkin method
for elastic waves on unstructured meshes - II. The three-dimensional isotropic case.
Geophys. J. Int. 167(1), 319–336 (2006)
14. Hager, G., Treibig, J., Habich, J., Wellein, G.: Exploring performance and
power properties of modern multicore chips via simple machine models. CoRR,
abs/1208.2908, 2012
15. H¨ahnel, M., D¨obel, B., V¨olp, M., H¨artig, H.: Measuring energy consumption for
short code paths using rapl. SIGMETRICS Perform. Eval. Rev. 40(3), 13–17 (2012)
16. Heinecke, A., Breuer, A., Rettenberger, S., Bader, M., Gabriel, A.-A., Pelties, C.,
Bode, A., Barth, W., Liao, X-K., Vaidyanathan, K., Smelyanskiy, M., Dubey, P.:
Petascale high order dynamic rupture earthquake simulations on heterogeneous
supercomputers. In: Proceedings of the International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis SC14, pp. 3–14. IEEE, New
Orleans, November 2014. Gordon Bell Finalist
17. Heinecke, A., Vaidyanathan, K., Smelyanskiy, M., Kobotov, A., Dubtsov, R.,
Henry, G., Chrysos, G., Shet, A.G., Dubey, P.: Design and implementation of
the linpack benchmark for single and multi-node systems based on intel(r) xeon
phi(tm) coprocessor. In: 27th IEEE International Symposium on Parallel and Dis-
tributed Processing, IPDPS 2013, pp. 126–137. IEEE Computer Society, Cam-
bridge, Boston, USA, 20–24 May 2013
18. K¨aser, M., Dumbser, M.: An arbitrary high-order discontinuous galerkin method
for elasticwaves on unstructured meshesi. the two-dimensional isotropic case with-
external source terms. Geophysical Journal International 166(2), 855–877 (2006)
19. Lawson, G., Sosonkina, M., Shen, Y.: Energy evaluation for applications with dif-
ferent thread aﬃnities on the intel xeon phi. In: 2014 International Symposium
on Computer Architecture and High Performance Computing Workshop (SBAC-
PADW), pp. 54–59, October 2014
20. Lawson, G., Sosonkina, M., Shen, Y.: Performance and energy evaluation of comd
on intel xeon phi co-processors. In: Proceedings of the 1st International Workshop
on Hardware-Software Co-Design for High Performance Computing, Co-HPC 2014,
pp. 49–54, IEEE Press, Piscataway, NJ, USA (2014)
21. Ltaief, H., Luszczek, P., Dongarra, J.: Proﬁling high performance dense linear
algebra algorithms on multicore architectures for power and energy eﬃciency. In:
International Conference on Energy-Aware High Performance Computing (EnA-
HPC 2011), Hamburg, Germany, September 2011
22. Moczo, P., Kristek, J., Galis, M., Pazak, P., Balazovjech, M.: The ﬁnite-diﬀerence
and ﬁnite-element modeling of seismic wave propagation and earthquake motion.
Acta phys. slovaca 57(2), 177–406 (2007)

High-Order ADER-DG Minimizes Energy- and Time-to-Solution of SeisSol
357
23. Rahman, S.F.,Guo, J., Yi, Q.: Automated empirical tuning of scientiﬁc codes for
performance and power consumption. In: Proceedings of the 6th International
Conference on High Performance and Embedded Architectures and Compilers,
HiPEAC 2011, pp. 107–116. ACM, New York, NY, USA (2011)
24. Rotem, E., Naveh, A., Ananthakrishnan, A., Rajwan, D., Weissmann, E.: Power-
management architecture of the intel microarchitecture code-named sandy bridge.
Micro, IEEE 32(2), 20–27 (2012)
25. Taylor, M.B.: Is dark silicon useful?: harnessing the four horsemen of the coming
dark silicon apocalypse. In: Proceedings of the 49th Annual Design Automation
Conference, DAC 2012, pp. 1131–1136. ACM, New York (2012)
26. Tiwari, A., Laurenzano, M.A., Carrington, L., Snavely, A.: Auto-tuning for energy
usage in scientiﬁc applications. In: Proceedings of the 2011 International Confer-
ence on Parallel Processing - vol. 2, Euro-Par 2011, pp. 178–187. Springer-Verlag,
Berlin, Heidelberg (2012)
27. Zecena, I., Burtscher, M., Jin, T., Zong, Z.: Evaluating the performance and energy
eﬃciency of n-body codes on multi-core cpus and gpus. In: 2013 IEEE 32nd Inter-
national Performance Computing and Communications Conference (IPCCC), pp.
1–8. IEEE (2013)

Modeling the Productivity of HPC Systems
on a Computing Center Scale
Sandra Wienke1,2,3(B), Hristo Iliev1,2,3, Dieter an Mey1,2,3,
and Matthias S. M¨uller1,2,3
1 IT Center, RWTH Aachen University, 52074 Aachen, Germany
{wienke,iliev,anmey,mueller}@itc.rwth-aachen.de
2 Chair for High Performance Computing, RWTH Aachen University,
52074 Aachen, Germany
3 JARA – High-Performance Computing, Schinkelstr. 2, 52062 Aachen, Germany
Abstract. In pursue of exaﬂop computing, the expenses of HPC cen-
ters increase in terms of acquisition, energy, employment, and program-
ming. Thus, a quantiﬁable metric for productivity as value per cost gets
more important to make an informed decision on how to invest available
budgets. In this work, we model overall productivity from a computing
center’s perspective. The productivity model uses as value the number
of application runs possible during the lifetime of a given supercom-
puter. The cost is the total cost of ownership (TCO) of an HPC cen-
ter including costs for administration and programming eﬀort. For the
latter, we include techniques for software cost estimation of large codes
taken from the domain of software engineering. As tuning eﬀort increases
when more performance is required, we further focus on the impact of
the 80-20 rule when it comes to development eﬀort. Here, performance
can be expressed with respect to Amdahl’s law. Moreover, we include an
asymptotic analysis for parameters like number of compute nodes and
lifetime. We evaluate our approach on a real-world case: an engineering
application in our integrative hosting environment.
Keywords: Productivity · Cost-beneﬁt ratio · Cost eﬃciency · TCO ·
Development eﬀort · COCOMO · 80-20 rule · Pareto principle · Com-
puting center · Scalability
1
Introduction
The move towards the exascale era will introduce huge and complex systems,
therefore the importance of improving productivity and total cost of ownership
(TCO) of computing centers increases. According to the deﬁnition of produc-
tivity as value per cost, HPC centers are interested in optimizing the spread of
their expenses across staﬀ(brainware) and hardware while trying to get the most
signiﬁcant outcome from the scientiﬁc simulations running on the cluster. Thus,
a tool that supports making informed decisions for the cost-eﬃcient division of
the available budget is needed.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 358–375, 2015.
DOI: 10.1007/978-3-319-20119-1 26

Modeling the Productivity of HPC Systems on a Computing Center Scale
359
Traditional factors that aﬀect productivity are the cluster size, the system
types it comprises, its lifetime, its energy consumption, and the performance of
the applications exploiting it. However, as future architectures might get more
complex to program, it is required to also include costs for developing, paralleliz-
ing, and tuning software into the equation. Intuitively, the development eﬀort
should be proportional to the number of lines of code (LOCs) to parallelize or
optimize, i.e. the kernel code size. Instead, in software engineering, it is suggested
that the eﬀort grows exponentially while developing new software. Although it
is not proven that exponential growth is also applicable when iteratively par-
allelizing hotspots in an HPC application, HPC lore suggests that 20 % of the
eﬀort is needed to achieve 80 % of the performance and 80 % of eﬀort to achieve
the remaining 20 % (80-20 rule). For large-scale applications, achieving high per-
formance is essential, resulting in exponentially increasing eﬀort. Thus, a certain
division of investment between hardware and staﬀnot only impacts the number
of compute nodes and their lifetime, but also the size or the detail of application
optimization and therefore the run time and the value of the application. Find-
ing combinations of these parameters that maximize productivity is needed for
cost-eﬃcient computing centers.
In this paper, we show an approach to model productivity at the scale of
a computing center. This includes quantifying hardware purchase and mainte-
nance costs and applying a performance model like Amdahl’s law to predict the
run time of big MPI codes. These large-scale application runs usually deliver
more value than single-node executions. Therefore, we also include a weighting
factor that represents the quality gained from large-scale runs. We also put into
the equation the impact of increasing tuning eﬀort on productivity for com-
plex codes. More detailed, the model supports dependencies on the kernel code
size and the speedup and applies rules from software engineering and HPC lore
respectively. Further, we include an analysis on the model’s asymptotic behavior
for large or long-lasting clusters.
In the context of one case study, we show examples of the worth of the pro-
ductivity metric. Our modeled productivity can be used to answer the question of
when to stop tuning a program (here an application from the engineering area).
Furthermore, we show that our productivity measure can help to decide how to
spend additional money – on brainware or hardware. Third, we investigate the
productivity metric over a system’s lifetime, which can also indicate when to
invest in new technologies. As these results are dependent on many parameters,
support for computations with own input values is important. Therefore, we pro-
vide an online version of the model called Aachen HPC Productivity Calculator –
aixH(PC)2 [25].
The paper is structured as follows. In Sect. 2, we cover related work. We
explain our productivity model that computes the number of application runs
per unit of TCO and analyze its asymptotic behavior with respect to the number
of compute nodes and system’s lifetime in Sect. 3. With the case study in Sect. 4,
we illustrate one concrete implementation of our model. Finally, we summarize
our ﬁndings in Sect. 5.

360
S. Wienke et al.
2
Related Work
The importance of cost-beneﬁt quantiﬁcation in HPC is increasing. This is also
shown by DoE funding a pilot study [13] conducted by IDC in 2013 that inves-
tigated the return of investment (ROI) for HPC investments. There, 208 case
studies were evaluated with respect to jobs created, revenue, proﬁt or cost sav-
ings per invested dollar. The beneﬁt from HPC investment can also be measured
in research competitiveness like NSF funding or number of publications [1]. How-
ever, the most common approach is the productivity metric that deﬁnes the ratio
of value over cost and often incorporates the time-to-solution.
Most existing productivity models have been studied between 2002 and 2006
in the context of the DARPA HPCS program [5]. Many focus on the user’s per-
spective and therefore provide simple metrics like relative speedup over relative
development eﬀort [28]. Furthermore, they are often applied to compare pro-
gramming models and can only express relative results. For instance, Zelkowitz
et al. [29] derive relative cost per LOC for OpenMP and MPI, whereas Ebcioglue
et al. [6] look at C + MPI, UPC, and x10 with the metric time-to-ﬁrst-correct-
parallel-solution. Kennedy et al. [14] model time-to-solution for a problem P
by T(P) = I(P) + r · E(P) where I is the implementation time to solve P,
E is the average execution time per program run, r is a problem-speciﬁc weight-
ing factor that reﬂects the relative importance of minimizing execution time
versus implementation time. Given a reference version P0, they derive relative
power pL = I(P0)/I(PL) and relative eﬃciency ϵL = E(P0)/E(PL) from that. In
general, underlying experiments for those studies are usually done with novice
parallel programmers in classroom environments [6,28,29], which only allows for
small codes, often running on single compute nodes. Further challenges with
this kind of experiments are summarized in [21]. Another approach from a user’s
perspective is taken by McCracken et al. [17], who tries to determine a set of
tasks that deﬁne the user’s workﬂow and measure the time spent in each task.
Our previous work [26] also compared programming models: OpenMP, Intel’s
Language Extensions for Oﬄoad, OpenCL, and OpenACC. For two real-world
applications, we investigated performance, programming eﬀort and costs per pro-
gram run on heterogeneous, single-node computer architectures, i.e. Intel Xeon
(regular and Phi) and NVIDIA GPUs.
Here, we extend this approach while focusing on a computing center’s per-
spective and large-scaling codes, where much tuning eﬀort is needed for execution
on the whole supercomputer. At the same time, our model is still reducible to
a user’s view and so in line with the base approaches of [14,28]. We use the
number of application runs as value and the TCO of a computing center in the
denominator. Only few other works consider TCO as cost factor. Sterling [23]
suggests a conceptual framework where productivity over time t is Ψt =
Rt
Ct×t
with R representing results i.e. complete solutions to a computation problem and
C the cost of production over time t (procurement, initial installation, machine
operation over time, construction of the application software). However, he does
not quantify a real-world example to illustrate his approach. Snir and Bader [22]
also include total costs. Their productivity value is expressed by a utility function

Modeling the Productivity of HPC Systems on a Computing Center Scale
361
that can represent the preferences on the outcome. The result is the productiv-
ity Ψ(P, S, T, U) =
U(P,T )
C(P,S,T ) with S the system, P the problem, T the time-to-
solution, U the utility function, and C the cost. They do not apply their model
to a real-world example either. Kepner [15] takes Sterling’s model and combines
it with approaches from Kennedy [14], Snir, and Bader [22] to obtain the (SK)3
synthesis model. He also includes one HPC example use of (SK)3, but again
reduces the model to the user’s view of a relative comparison between OpenMP
and MPI. In our work, we show the applicability and worth of our model in
decision maker’s view in an HPC case study. Two other approaches [8,18] also
emphasize the view of stakeholders or decision makers.
Additionally, we develop a comprehensive model that does not focus on com-
paring programming paradigms. Research in software engineering shows that
project size, software kind and personnel factors might have stronger impact on
eﬀort and software cost than the programming model [16]. We adapt the con-
structive cost model COCOMO II [2] from software engineering to HPC and link
the MPI code size to the tuning eﬀort. Kepner [15] has also thought about this
approach and used it in his relative comparison of OpenMP and MPI. Instead,
we initialize the model with real (absolute) values. Furthermore, we detail on the
kind of software, i.e. HPC applications, whose tuning eﬀort has an impact on
performance. While Snir and Bader [22] mention this fact in general, we are the
ﬁrst to our knowledge to model this impact explicitly. We further incorporate
a performance model, e.g. Amdahl’s law, for large-scale systems. Consequently,
our model can be applied to support decisions on expenditure on tuning or sys-
tem run time. We also include the asymptotic cases of long system lifetime and
big number of compute nodes, similar to Snir and Bader [22].
3
Productivity Model from a Computing Center’s
Perspective
Economically, productivity describes a certain output per unit of input [3], usu-
ally a ratio of value and cost. In HPC, ﬁnding an applicable deﬁnition and
quantiﬁcation of a beneﬁt-to-cost ratio is a challenging task. On the one hand,
gathering and quantifying all cost parameters is diﬃcult in computing centers as
whole. Here, we started evaluating parameters arising at the RWTH’s computing
center. On the other hand, the value of HPC is not easily deﬁnable as a single
number. We investigate the performance of applications or rather the number of
application runs over the cluster’s lifetime as value. Our focus is to also include
common features of large-scale applications. Therefore, we extend our previous
TCO formulation [26] with (Amdahl’s) scaling law, introduce a weighting factor
that describes the quality of large-scale runs compared to small-scale runs, and
include programming eﬀort into the equation.
3.1
Productivity Metric
We deﬁne productivity as the number of applications runs nex exploiting the
cluster under investigation. The denominator represents the cluster’s TCO as

362
S. Wienke et al.
Table 1. One-time and annual costs in e for the use-case application
deﬁned in [24]. Both depend on the number of acquired compute nodes n and
the system’s lifetime in years τ.
productivity = value
cost = nex(n, τ)
TCO(n, τ)
(1)
The number of application executions nex is computed by dividing the overall
time that the system is available by the application’s run time t:
nex(n, τ) = α · τ
t(n) .
(2)
For simplicity, we abstract the actual job mix running on a typical cluster to a
single application. Future work will address this issue. The parameter α denotes
the system availability rate in percent and has been introduced to account for
additional scheduling delays, maintenance periods or unreliability of the system.
The TCO value diﬀerentiates between one-time costs Cot and annual (opera-
tional) costs Cpa. We further break down costs depending on the system’s life-
time, the number of acquired nodes and their type (e.g. CPUs or GPUs). The
latter accommodates e.g. porting an application to a speciﬁc type of architecture.
Table 1 gives an overview on the diﬀerent components. If a ﬁxed investment I is
given, the computed TCO value is lower than or equal to this budget.
I ≥TCO(n, τ) = Cot(n) + Cpa(n) · τ = (CA · n + CB) + (CC · n + CD) · τ (3)
3.2
Parallel Scaling and Run Quality
When modeling MPI applications that include communication across the net-
work, Amdahl’s law comes into play. While we incorporate that law into the
equation, any other suitable performance model could be used instead. Given
Amdahl’s law, the run time t is expressed as sum of the serial time ts and the
time tp spent into parallelizable code:
t(n) = ts + tp
n = t1 · (1 −p) + t1 · p
n
(4)

Modeling the Productivity of HPC Systems on a Computing Center Scale
363
where t1 is the run time of the serial version of the application and p is the
percentage of the code that is parallelized. In real life, where serial runs are not
always feasible due to run time or memory constraints, ts and tp can be obtained
by ﬁtting to a set of measured run times from multiple compute nodes. If the
absolute productivity is not important, t1 could simply be set to 1 since it goes
linearly into the productivity. Summarizing, productivity evaluates to
productivity = nex(n, τ)
TCO(n, τ) =
α·τ
t(n)
TCO(n, τ) =
α·τ
t1·(1−p)+ t1·p
n
TCO(n, τ) .
(5)
Given Amdahl’s law t(n)
n→∞
−−−−→ts, the application might not eﬃciently lever-
age a whole cluster of maybe thousands of nodes. In such case, a threshold nscale
representing the maximum number of nodes on which the application scales
acceptably can be deﬁned and the n acquired nodes can be divided into multiple
partitions of size nscale and a remainder of size nrem:
n = ⌊n/nscale⌋· nscale + nrem with nrem = n −⌊n/nscale⌋· nscale
(6)
⇒productivity =
α · τ ·

⌊n/nscale⌋
t(nscale) +
1
t(nrem)

TCO(n, τ)
.
(7)
For non-MPI applications, the formula is still valid. For instance, a single-node
application executing a parameter study could have multiple copies working on
diﬀerent parts of the search space and thus nscale = 1 and t(n) = t1/n.
MPI applications usually perform domain decomposition or enable simula-
tions of increased resolution. Therefore, running with up to nscale nodes has a
beneﬁt that is not expressed in the number of application runs nex. To account
for this, we introduce a weighting quality factor q(n). It could e.g. have a value
of 1 for executions on nscale nodes and then decrease linearly when fewer nodes
are used. Of course, q(n) can take a diﬀerent form depending on the speciﬁc case
as shown in Subsect. 4.3.
productivity =
α · τ ·

q(nscale) · ⌊n/nscale⌋
t(nscale) + q(nrem) ·
1
t(nrem)

TCO(n, τ)
(8)
3.3
Eﬀort Dependence on Code Size and 80-20 Rule
For the parallelization and optimization of an application, the corresponding
development eﬀort must also be accounted for in the computation of a real pro-
ductivity metric since it is a part of the development costs gathered in TCO(n, τ).
This is especially important for huge codes typically implemented with MPI. Par-
allelization might be done iteratively by addressing more and more code hotspots
or by directly parallelizing the major part of the code. Either way, the size of
the application part that is parallelized or tuned impacts the eﬀort needed to
accomplish this goal. In software engineering, COCOMO II [2] is popular for
estimating the eﬀort with respect to the number of LOCs or function points.

364
S. Wienke et al.
The model assumes exponential eﬀort growth with linear increase of the number
of LOCs. An abstraction of this model is given by
eﬀort/ work [days] := w = r · (KLOC)s
(9)
where KLOC means thousand LOCs. The parameter r represents the so called
eﬀort multipliers. Those describe, for instance, execution time constraints or
the capability of the programmer. Ideally, r should be calibrated with data
from previously-completed projects within the local institution. The parame-
ter s ranges from 0.91 to 1.226 depending mainly on the growth of interpersonal
communication and large-system integration overheads. The nominal value is
s = 1.1 and for s > 1 the so called diseconomy of scale takes place. In soft-
ware engineering, this estimation model is applied to the development of new
software. We believe that most parallel scientiﬁc applications start from already
existing serial applications and we hypothesize that the eﬀort estimation from
COCOMO II can be applied to HPC development with the restriction that the
denoted number of LOCs will not be newly developed, but rather describe the
parallelizable part of the code. Unfortunately, we do not have a proof of this
postulate yet. However, we will investigate this relation in future work.
In the following, we assume that we can apply (9) to our eﬀort estimations.
Parameter s is assumed to be 1.16, i.e. low ratings in the scale factors of the
COCOMO II model. For r, we can apply a calibration with a given pair of kernel
code size LOC0 and base eﬀort w0 needed to parallelize or tune this kernel. Eﬀort
for further parallelizing more hotspots can then be predicted as:
w =

w0
 LOC0
1000
1.16

· (KLOC)1.16.
(10)
Given the increasing eﬀort needed for further parallelization or tuning, its
impact on application performance is of peculiar interest. Following the Pareto
principle [19], HPC lore states that 20 % of eﬀort will result in 80 % of perfor-
mance. To gain the remaining 20 % of performance, an eﬀort of further 80 % is
needed. The function of this so called 80-20 rule can be modeled by a Pareto
distribution:
F(w) =

1 −
 m
w
h
if w ≥m
0
if w < m
(11)
where m represents the minimum value of w and h is a positive parameter that
represents the ability of a programmer to get a certain performance improvement
per day.
In our HPC case, w symbolizes the development eﬀort in person-days and
F(w) the percentage of e.g. peak performance. The peak performance of an appli-
cation on a certain system could be determined e.g. by the rooﬂine model [27]
for single nodes or by using the LogP model [4] for clusters. Additionally, HPC
experts may assess the percentage of peak performance from experience.

Modeling the Productivity of HPC Systems on a Computing Center Scale
365
For the application of the 80-20 rule to the eﬀort-performance relation, we
must adapt (11). The actual minimum value for the eﬀort is w = 0, and no eﬀort
results in no performance gain. We therefore shift (11):
performance [%] = perf(w) = 1 −
	
1
k · w + 1

h
.
(12)
An example Pareto distribution is shown in Fig. 1. Parameter h can be computed
given a reference pair of eﬀort w0 and the achieved performance perf0. k > 0
accounts for the programmer’s background knowledge that eﬀects the amount
of eﬀort. By default k = 1. If more reference pairs are available, a distribu-
tion function that models better the given programmer can be obtained. The
value of k could either be computed by inverting the performance equation, or
approximated using a least-squares ﬁt.
  0%
 20%
 40%
 60%
 80%
100%
effort [days]
performance [%]
Fig. 1. Example Pareto distribution
(80-20 rule) following (12) with k = 1
effort [days]
productivity
Fig. 2. Example productivity function
that contains a maximum due to the
impact of eﬀort on performance
The reference performance perf0 may come as a result of tuning the original
kernel. The run time is then t(n) = t1 · (1 −p) + t1 · p/(ksp0 · n) where ksp0 is
the kernel speedup after the tuning. Finally, the range of values of perf(w) ∈[0, 1)
can be mapped from performance percentages to kernel speedups. Using kernel
speedup instead of overall speedup is appropriate since the percentage of peak
performance can usually be approximated by evaluating the kernel code alone. To
get kernel speedups in the range of [1, kspmax) where kspmax = 1+(ksp0−1)/perf0,
we apply a simple linear scaling based on the known kernel speedup ksp0 and
performance perf0. Thus, ksp(w) predicts the potential speedup of the kernel
code after w days of eﬀort invested in tuning it:
ksp(w) = 1 +
	ksp0 −1
perf0

· perf(w).
(13)
Given the diﬀerent impact that eﬀort has on both kernel speedup and cost,
productivity as function of the eﬀort (with ﬁxed values of n and τ) might have
a maximum (see Fig. 2). This maximum estimates the amount of eﬀort that is
worth putting into parallelization and tuning. Further eﬀort will result in lower
productivity. A threshold for an acceptable productivity has to be set by each
institution depending on its own needs, hardware and application conﬁguration.

366
S. Wienke et al.
3.4
Asymptotic Behavior
Computing centers typically operate large cluster systems. In the limit of large
but ﬁnite number of compute nodes n, acquisition and maintenance costs domi-
nate and then TCO ≈(CA +CC ·τ)·n. Consequently, putting more eﬀort aﬀects
the total application speedup while the TCO remains the same. A similar case
occurs in the limit of very long system lifetime τ. Operational costs then greatly
outweigh one-time costs, even when the latter include substantial programming
costs, therefore TCO ≈Cpa · τ. Substituting this into (8), the limiting value of
the long-term productivity is obtained as:
productivity =
q(nscale) · ⌊n/nscale⌋
t(nscale) + q(nrem) ·
1
t(nrem)
C′pa
(14)
where C′
pa = Cpa/α are the annual costs adjusted with the availability of the
system. The numerator (number of program executions per year) is only aﬀected
by the application’s scalability and by the speedup due to the programming eﬀort
put into optimizing the kernel. In both approximations above, programming
eﬀort is “for free” and productivity is a monotonically increasing function of the
tuning eﬀort.
In the special case of a large number of nodes and an application that scales
up to the whole cluster (nscale = n), Cpa ≈CC · n and (14) reduces to
productivity = q(n)
C′
C
1
n · t(n) ∝q(n)
t1
n · t(n).
(15)
The last term could be recognized as the weighted parallel eﬃciency of the
application. Except for embarrassingly parallel problems where the value of the
parallel eﬃciency is always 1, for any application with a non-zero serial part,
productivity will monotonically and asymptotically approach zero. With a lin-
early increasing quality factor, productivity follows the parallel speedup as given
by Amdahl’s law:
productivity =
1
nscale · C′
C · t(n) ∝
t1
t(n).
(16)
For some non-scalable applications, t(n) is not a monotonically decreasing func-
tion due to the communication and synchronization overhead increasing with the
number of compute nodes. Therefore, it could be beneﬁcial to have several con-
current smaller application runs instead of one big run. This also applies to the
general case of running non-scalable parallel applications and – in the absence of
superlinear scaling eﬀects – the minimum value of nscale after taking into account
constraints like project deadlines and memory/storage requirements should be
selected.
4
Case Study on Modeling Real-life Acquisitions
In this section, we show the viability of our model with an acquisition done
at the IT Center of RWTH Aachen University. As part of its services, the IT

Modeling the Productivity of HPC Systems on a Computing Center Scale
367
Center oﬀers to the university institutes the possibility to integrate their own
compute nodes as part of the RWTH Compute Cluster (integrative hosting). As
hardware and software are managed by the same set of tools as the rest of the
cluster, administrative costs are minimal.
4.1
Cluster Partition
For the purpose of this study, we used one partition from the integrative hosting
acquired in 2010. It consists of 56 Dell PowerEdge compute nodes with two Intel
Xeon X5670 processors and 48 GiB of RAM each. A fat-tree QDR InﬁniBand
network provides the high-speed connectivity. The total of the system costs is
250,000c. The detailed cost decomposition can be found in Table 1. The given
one-time and annual costs are real values for our cluster setup [26]. We would like
to stress that the following analysis was done after the acquisition was completed
and was not used during the procurement. It serves solely to illustrate our model.
4.2
Engineering Application psOpen
One of the important applications running on the partition above is psOpen [10]
which simulates turbulent ﬂows in the ﬁeld of combustion technology. It imple-
ments a pseudo-spectral Direct Numerical Simulation (DNS) method that solves
the discretized Navier-Stokes equations for incompressible ﬂuids in reciprocal
(Fourier) space. Part of the calculation is still performed in real space and three-
dimensional Fast Fourier Transform (3D-FFT) is used to switch between the
two representations. The 3D-FFT kernel takes up to 80 % of the total execution
time and is where network communication is involved. The rest of the algorithm
performs local point operations only.
Originally, psOpen used the open-source library P3DFFT [20]. To improve
the performance, the application developer made changes to the 3D-FFT routine
and fused the ﬁltering step of the pseudo-spectral DNS method with the Fourier
transform in 2013. As a result, the amount of data being sent across the network
is reduced signiﬁcantly when compared to the original full 3D-FFT and varying
levels of speedup are observed depending on the size of the simulation domain.
For the purpose of our analysis, we take the speedup factor of 1.69× achieved
for the biggest domain size of 40963 points.
In a second optimization phase, the communication pattern of the 3D-FFT
routine was analyzed by one of the authors and a comprehensive performance
model was derived. The model suggested that a speciﬁc domain decomposition
should prove more eﬃcient than any other when the application is running on
the given cluster partition. Repeated benchmarking showed an overall speedup
of 1.82×.
Each optimization phase involved eﬀort equal to one person-month or 17.5
working days based on 210 working days per year [7]. This provides two data points
to ﬁt the Pareto distribution. Since there are further actions, e.g. tweaking the
MPI runtime parameters, that could possibly bring a small additional speedup,
we assume that 98 % of the asymptotically possible speedup was achieved in the

368
S. Wienke et al.
second phase, which translates to 95.7 % of the maximum performance. The per-
formance after the ﬁrst phase is thus 80.5 %. Based on annual TV-L E13 salary
for scientiﬁc employees of 57,300e including tax [9], i.e. 272.86e per day, the cost
of the optimization eﬀorts is 9,550e.
Since the 3D-FFT kernel is not the only parallel part of psOpen, we employ
the following scaling model: t(n) = tio + tpc(n) + tFFT(n). Here, tio = cio · t1
is the serial input-output time, tpc(n) = cpc · t1/n is the time for local point
computations, and tFFT(n) = cFFT · t1/(ksp · n) is the time it takes to per-
form the FFT. The following run-time fractions are used below: cio = 0.02 %,
cpc = 19.98 %, cFFT = 80.0 %. The size of the kernel portion is about 1 KLOC
and the eﬀort in both phases is concentrated on tuning those lines only. For
simplicity, we use a constant run quality factor of 1.
4.3
Results
Here we show how our productivity model can be applied to psOpen and answer
questions such as how much eﬀort should be spent on tuning the code, how much
money to spend on brainware vs. hardware, and how long to run the system.
0
10
20
30
40
50
  0%
 20%
 40%
 60%
 80%
100%
effort [days]
performance [%]
k=1
k≈0.005
0
617 1122 1591 2039 2472
#LOCs
Fig. 3. 80-20 rule w.r.t. % of peak per-
formance applied to psOpen. Perfor-
mance after each tuning phase is shown
by diamonds.
0
10
20
30
40
50
1
1.2
1.4
1.6
1.8
2
effort [days]
kernel speedup
max ksp
k=1
k≈0.005
0
617 1122 1591 2039 2472
#LOCs
Fig. 4. 80-20 rule w.r.t. kernel speedup
(see (13)) applied to psOpen. Speedup
after each tuning phase is shown by dia-
monds.
How Much Eﬀort to Spend on Tuning? First, we illustrate the impact of
increasing tuning eﬀort on the productivity. For the model computations, we set
the system’s lifetime τ to 5 years. The actual serial run time depends on the type
of computation, but since it is simply a multiplier in the productivity equation
and it does not change the position of the maximum, we use t1 = 1000. Figure 3
shows the application of the 80-20 rule: gained percentage of peak performance as
function of the optimization eﬀort in person-days. The dashed curve represents
the Pareto distribution with k = 1 and h computed from the values for phase 1
to be 0.56. The solid curve is a plot of (12) with parameters adapted to ﬁt the
data points from both phases (indicated by diamond marks): k ≈0.0048 and

Modeling the Productivity of HPC Systems on a Computing Center Scale
369
0
10
20
30
40
50
14
16
18
20
22
24
effort [days]
productivity
k=1
k≈0.005
linear
max
Fig. 5. Productivity of psOpen w.r.t.
eﬀort. Diamonds mark the eﬀort from
both tuning phases.
0
20
40
60
14
16
18
20
22
24
#nodes
productivity
k=1
max
Fig. 6. Productivity of psOpen w.r.t.
cluster size given constant quality fac-
tor 1. Diamond marks the original sys-
tem size of 56 nodes.
h ≈20.11. Both curves diﬀer mainly in their initial slope and how quickly they
approach the maximum value. The unadapted distribution (k = 1) approaches
the limiting value very slowly due to h < 1. In the following sections, we look
at the second (adapted) distribution, if not stated otherwise. Figure 4 shows the
eﬀort-performance curves from Fig. 3 scaled to kernel speedup.
The impact of the Pareto distribution on productivity is shown in Fig. 5.
While the denominator in the productivity formula, i.e. the TCO, increases
roughly linearly with the person-days, the numerator, i.e. the number of applica-
tion runs, increases log-wise due to the 80-20 rule. The location of the maximum
is marked by squares. The tuning eﬀort during phase 1 is located on the positive
slope before the maximum. Given the initial distribution with k = 1, produc-
tivity peaks at 39.6 person-days and a further tuning eﬀort of 22.1 person-days
would be optimal with respect to productivity. If we examine the adapted dis-
tribution (solid curve), we see that the combined eﬀort from both phases is close
to the corresponding maximum at 42.7 person-days. After the maximum, the
productivity starts to decrease since further eﬀort does not result in signiﬁcant
performance improvement. Thus, our model can show how long tuning might be
worth it with respect to (a threshold on) productivity. For comparison, Fig. 5
also shows the linear increase of performance with eﬀort that could be naively
assumed.
Finally, all three ﬁgures also show the impact of increased LOCs on eﬀort and
thus on productivity. If we assume that double the amount of (kernel) LOCs is
parallelized or tuned, then the eﬀort must be increased by a factor 21.16 ≈2.23.
While this is not an issue in our example case, huge codes might be strongly
aﬀected.
How Much to Spend on Brainware vs. Hardware? Our productivity
model can also be used to assess whether additional money should be spent on
hardware or people. Assume that we were at the end of phase 1 (17.5 days of
tuning eﬀort, 80.5 % performance, k = 1) and got a productivity of 22.27. To get
more science out of our program, we have to decide whether to buy an additional

370
S. Wienke et al.
effort [days]
#nodes
0
10
20
30
40
10
20
30
40
50
60
200
400
600
800
0
617
1122
1591
2039
#LOCs
Fig. 7. Contour plot of the impact of
no. of nodes and eﬀort on run time of
psOpen. Diamond marks phase 1 setup.
Dots mark additional investment of
4464e.
effort [days]
#nodes
0
50
100
150
0
50
100
150
5
10
15
20
max
0
2472
4493
6373
#LOCs
Fig. 8. Contour plot of the productivity
of psOpen as function of no. of nodes
and eﬀort. Diamond marks phase 1
setup. Dots mark additional investment
of 4464e.
compute node at the cost of 4,464e or to invest the money into further tuning.
Spending 4,464e for tuning would mean approximately one person-month (as
4, 464[e]/272.86[e/person-day] ≈16.4 [person-days]).
The ﬁrst interesting clue is the impact of the node count and the amount
of eﬀort on the run time. Figure 7 shows a contour plot of this case, i.e. the
intersections of horizontal planes with the 3D run time surface as a function of
the number of nodes and the amount of eﬀort. The shape and density of the
contour lines suggest that the number of nodes has a stronger impact on the
run time than the amount of eﬀort. In contrast, the corresponding contour plot
of the productivity function (Fig. 8) suggests that investing more eﬀort is more
beneﬁcial than investing in more nodes (as long as the global maximum is not
exceeded). Thus, although run time decreases faster with the number of nodes
than with the amount of eﬀort, the cost of the nodes increase much faster than
the eﬀort cost. Figures 5 and 6 reveal a more detailed view. When buying one
additional node, the productivity stays at ∼22.27. Instead, spending 4,464e on
brainware, the productivity metric increases to 22.5.
Now, we generalize the approach of having a certain ﬁxed investment by
setting the TCO to a ﬁxed budget and varying the amount of eﬀort. From
that, the number of nodes that can be acquired is determined. The contour
plot in Fig. 9 illustrates the distribution of productivity on nodes and person-
days for ﬁxed investments up to 750,000e . Since Amdahl’s Law is in place and
no weighting factor for large-scale runs was speciﬁed, spending 750,000e into
nodes and people does not pay oﬀin terms of productivity. Here, the maximum
arises at ∼578,000e (marked as square) and is located at 45 person-days and
69 nodes when using the adapted eﬀort-performance relation. Thus, investing
roughly 12,000e into tuning eﬀort, i.e. 2 % of the total investment, is the sweet
spot. In our case study, larger-scale runs have a beneﬁt over smaller-scale runs.
Thus, we use a quality factor (8) of the form q(n) = (n/nscale)1/3. The quality of

Modeling the Productivity of HPC Systems on a Computing Center Scale
371
the run is proportional to the simulation resolution and running on more nodes
gives access to more system memory and therefore results in better resolution.
Now, spending the maximum investment of 750,000e pays oﬀ(Fig. 10). It can
also be seen that spending a particular amount of the available budget to people
instead of hardware is advantegous.
I = 578 K
I = 750 K
effort [days]
#nodes
0
50
100
150
70
80
90
max
5
10
15
20
Fig. 9.
Contour
plot
of
the
pro-
ductivity of psOpen
as function of
no. of nodes&amount of eﬀort: non-
weighted
scaling.
Investment
I
is
ﬁxed (up to 750 Ke) and distributed
between brain- & hardware.
I = 578 K
I = 750 K
effort [days]
#nodes
0
50
100
150
70
80
90
max
Fig. 10.
Contour
plot
of
the
pro-
ductivity of psOpen
as function of
no.
of
nodes
&
amount
of
eﬀort:
weighted
scaling.
Investment
I
is
ﬁxed (up to 750 Ke) and distributed
between brain- & hardware.
How Long to Run an HPC Cluster? The goal of the third analysis is to make
an informed decision on where to spend available ﬁxed budgets. In this case, we
vary the amount of tuning eﬀort and the system’s lifetime. Simultaneously, we
want to derive a value for the system lifetime that shows how long the system
is beneﬁcial to get employed. For the psOpen setup, the productivity levels out
at the longest system lifetime possible (see Fig. 11). For a system with non-
zero annual costs per node type (CD in Table 1), the lifetime that results in
maximum productivity could be just a couple of years. Figure 12 shows that
setting the annual license costs to 5,000e decreases productivity after 29 years as
fewer nodes could be acquired. Bigger clusters with higher software costs could
yield optimal system lifetime of approximately 4 or 5 years. If no peak value
exists, determining the range of system lifetime values in which productivity
rises signiﬁcantly makes sense. Restrictively, our approach does not take into
account that maintenance contracts usually expire after 5 years and that aging
system components may break down. Both will impact notably the productivity
of long-running clusters and must be addressed in future work. Additionally, this
analysis should be carried out every other year to account for the technological
advances.
Advances in new technologies also play a role in deciding whether to spend
the available budget on a single- or multi-phase installation and – in the multi-
phase case – when to buy the diﬀerent stages. As an example, we want the high-
est productivity after a certain time period, e.g. governmental funding period,

372
S. Wienke et al.
0
50
100
150
50
100
150
0
50
effort [days]
system
lifetime [years]
productivity
Fig. 11. Productivity of psOpen as func-
tion of the system’s lifetime and the
amount of eﬀort. Investment is ﬁxed at
750,000 e and distributed to nodes and
people while increasing the lifetime. Global
maximum is marked.
0
50
100
150
50
100
150
0
50
effort [days]
system
lifetime [years]
productivity
Fig. 12. Productivity of psOpen
as
function of the system’s lifetime and
the amount of eﬀort with added annual
compiler costs. Investment is ﬁxed at
750,000 e and distributed to nodes and
people while increasing the lifetime.
which is set to 8 years. Figures 13 and 14 show the comparison of a single-
phase and two-phase cluster purchase with a total investment of 10 million e.
For the two-phase cluster, the investment is split into two chunks of 5 million e,
one for each phase. The single-phase cluster is comparable to the ﬁrst stage
of the two-phase cluster, only diﬀering in the amount of investment. The sec-
ond stage is likely to comprise novel technologies delivering higher performance,
bandwidth and energy eﬃciency. In our example, we assume that the price of
the hardware components stays constant while the performance increases. To
predict the future performance of the application, we perform a least square
ﬁt of base-2 exponential functions to historical data of the InﬁniBand network
bandwidth [11] and the performance of diﬀerent generations of similarly-priced
Intel Xeon CPUs [12]. The ﬁt yields two characteristic times: τCPU = 1.61
years for doubling of the CPU performance and τIB = 3.57 years for dou-
bling of the network bandwidth. The execution time on the future system is
t′(n) = tio + t′
pc(n) · 2−δ/τCPU + t′
FFT(n) · 2−δ/τIB, where δ is the time in years
between the start of the ﬁrst and the second phase. The model assumes that the
time for compute-bound point operations is inversely proportional to the CPU
performance, while the time for the network-bound FFT kernel is only aﬀected
by the increasing network bandwidth. While this is a simple performance model
for psOpen, which also does not account for the increasing energy eﬃciency, it
still captures the basic trend of the performance increase. Better performance
models may improve the worth of the productivity prediction. We should notice,
that we eﬀectively model a two-partition system, as many parallel applications –
including psOpen – are not able to fully utilize clusters with e.g. processors of
varying speed. Therefore, we treat the two-phase case as two mostly indepen-
dent clusters and sum their individual application run counts, which is then
divided by the collective TCO in order to obtain the productivity. In Fig. 14, we
clearly see that in our case a two-phase cluster acquisition is beneﬁcial over a

Modeling the Productivity of HPC Systems on a Computing Center Scale
373
0 1
2 3
4
5 6
7 8
9 10
0
10
20
30
40
system lifetime [years]
productivity
single phase
phase 1
phase 2
duration of
funding period
Fig. 13. Productivity comparison of
psOpen for a single-phase vs. two-phase
cluster purchase and as function of
years. Overall investment is ﬁxed at
10 Me (split equally for the two-phase
cluster). Funding period is 8
years.
Operation start of 2nd phase after
4 years.
0
1
2
3
4
5
6
7
8
10
15
20
4 years
operation start δ of phase 2 [years]
productivity
single phase
two phases
max
Fig. 14. Productivity comparison of
psOpen for a single-phase vs. two-phase
cluster purchase after 8 years. Produc-
tivity as function of the year in which
the 2nd phase started operating. Over-
all investment is ﬁxed at 10 Me (split
equally for the two-phase cluster).
single-phase one in most cases. The best time for starting operating the second
phase is 4 years after installing the ﬁrst phase.
5
Conclusion
In this paper, we presented a productivity model with focus on a computing cen-
ter scale, i.e. running big MPI codes with much tuning eﬀort needed to get good
performance. For large-scaling runs, we included a performance model (Amdahl’s
law) and account for their higher quality using a weighting factor. We linked code
size, tuning eﬀort, and performance using an adaptation of COCOMO II and
HPC lore’s 80-20 rule. Analysis of the asymptotic behavior of large or long-
lasting clusters is also incorporated.
In the context of one case study at RWTH Aachen University, we examined
the viability and worth of our model by deriving quantitative statements on cap-
ital expenditure on tuning time, hardware, and system lifetime. Here, spending a
certain portion of the available budget on brainware would pay oﬀ. Furthermore,
a two-phase cluster acquisition would be beneﬁcial over a single-phase one since
the technological advances result in higher productivity. As the productivity
results depend on numerous parameters, we also provide an online tool on our
web page [25] that could help others make informed decisions on procurement
of new HPC systems or upgrading existing ones.
In future, we will improve the predicting power of our productivity metric
by further investigating applications of performance, power, and software esti-
mation models. Extensions of the productivity model will also include mixed
job executions and an adapted function of system lifetime. The latter will incor-
porate overlapping development time with system lifetime and the impact of

374
S. Wienke et al.
expired (or extended) maintenance contracts and the breakdown of aging sys-
tem components. Furthermore, we will investigate more use cases such as arguing
for diﬀerent funding sources. Finally, we will support our productivity model and
the assumptions made by gathering more case studies. We also ask for feedback
from other institutions on our aixH(PC)2 web page.
References
1. Apon, A., Ahalt, S., Dantuluri, V., Gurdgiev, C., Limayem, M., Ngo, L., Stealey,
M.: High performance computing instrumentation and research productivity in US
universities. J. Inf. Technol. Impact 10(2), 87–98 (2010)
2. Boehm, B., Abts, C., Brown, A.W., Chulani, S., Clark, B., Horowitz, E., Madachy,
R., Reifer, D., Steece, B.: COCOMO II Model Deﬁnition Manual, Version 2.1.
Technical report, University of Southern California (2000)
3. Chew, W.: No-nonsense guide to measuring productivity. Harvard Bus. Rev. 66(1),
110–118 (1988)
4. Culler, D.E., Karp, R.M., Patterson, D.A., Sahay, A., Schauser, K.E., Santos, E.,
Subramonian, R., von Eicken, T.: LogP: Towards a Realistic Model of Parallel
Computation. Technical report, Berkeley (1992)
5. Dongarra, J., Graybill, R., Harrod, W., Lucas, R., Lusk, E., Luszczek, P., Mcma-
hon, J., Snavely, A., Vetter, J., Yelick, K., Alam, S., Campbell, R., Carrington, L.,
Chen, T.Y., Khalili, O., Meredith, J., Tikir, M.: DARPA’s HPCS program: his-
tory, models, tools, languages. In: Zelkowitz, M.V. (ed.) Advances in COMPUT-
ERS High Performance Computing, Advances in Computers, vol. 72, pp. 1–100.
Elsevier (2008)
6. Ebcioglu, K., Sarkar, V., El-Ghazawi, T., Urbanic, J., Center, P.: An experiment in
measuring the productivity of three parallel programming languages. In: Workshop
on Productivity and Performance in High-End Computing (P-PHEC), pp. 30–36
(2006)
7. European Commission: Guide to Financial Issues relating to FP7 Indirect Actions
(2013)
8. Faulk, S., Gustafson, J., Johnson, P., Porter, A., Tichy, W., Votta, L.: Measuring
high performance computing productivity. Int. J. High Perform. Comput. Appl.
18(4), 459–473 (2004)
9. German Science Foundation (DFG): Personalmittels¨atze der DFG f¨ur das Jahr
(2013)
10. G¨obbert, J.H., Gauding, M.: psOpen (2015). http://www.fz-juelich.de/ias/jsc/
EN/Expertise/High-Q-Club/psOpen/ node.html
11. InﬁniBand Trade Association (2015). http://www.inﬁnibandta.org/
12. Intel Corporation: Intel Processor ARK (2015). http://ark.intel.com/
13. Joseph, E.C., Conway, S., Dekate, C.: Creating Economic Models Showing the
Relationship Between Investments in HPC and the Resulting Financial ROI and
Innovation and How It Can Impact a Nation’s Competitiveness and Innovation.
International Data Corporation (IDC), Technical report (2013)
14. Kennedy, K., Koelbel, C., Schreiber, R.: Deﬁning and measuring the productivity
of programming languages. Int. J. High Perform. Comput. Appl. 18(4), 441–448
(2004)
15. Kepner, J.: High performance computing productivity model synthesis. Int. J. High
Perform. Comput. Appl. 18(4), 505–516 (2004)

Modeling the Productivity of HPC Systems on a Computing Center Scale
375
16. McConnell, S.: Software Estimation: Demystifying the Black Art. Redmond, Wa.
Microsoft Press (2006)
17. McCracken, M., Wolter, N., Snavely, A.: Beyond performance tools: Measuring
and modeling productivity in HPC. In: Third International Workshop on Software
Engineering for High Performance Computing Applications, SE-HPC 2007, pp. 4–4
(2007)
18. Murphy, D., Nash, T., Lawrence Votta, J., Kepner, J.: A System-wide Productivity
Figure of Merit. CT Watch Quarterly 2(4B) (2006)
19. Newman, M.: Power laws, Pareto distributions and Zipf’s law. Contemp. Phys.
46(5), 323–351 (2005)
20. Pekurovsky, D.: P3DFFT: a framework for parallel computations of Fourier trans-
forms in three dimensions. SIAM J. Sci. Comput. 34(4), C192–C209 (2012)
21. Sadowski, C., Shewmaker, A.: The Last Mile: Parallel Programming and Usability.
In: Proceedings of the FSE/SDP Workshop on Future of Software Engineering
Research, FoSER 2010, pp. 309–314. ACM, New York (2010)
22. Snir, M., Bader, D.A.: A framework for measuring supercomputer productivity.
Int. J. High Perform. Comput. Appl. 18(4), 417–432 (2004)
23. Sterling, T.: Productivity metrics and models for high performance computing.
Int. J. High Perform. Comput. Appl. 18(4), 433–440 (2004)
24. Wang, L., Khan, S.: Review of performance metrics for green data centers: a tax-
onomy study. J. Supercomputing 63(3), 639–656 (2011)
25. Wienke, S., Iliev, H., Hahnfeld, J., an Mey, D., M¨uller, M.S.: aixH(PC)2 - Aachen
HPC Productivity Calculator (2015). http://www.hpc.rwth-aachen.de/research/
tco/
26. Wienke, S., an Mey, D., M¨uller, M.S.: Accelerators for technical computing: is it
worth the pain? A TCO perspective. In: Kunkel, J.M., Ludwig, T., Meuer, H.W.
(eds.) ISC 2013. LNCS, vol. 7905, pp. 330–342. Springer, Heidelberg (2013)
27. Williams, S., Waterman, A., Patterson, D.: Rooﬂine: an insightful visual perfor-
mance model for multicore architectures. Commun. ACM 52(4), 65–76 (2009)
28. Zelkowitz, M., Basili, V., Asgari, S., Hochstein, L., Hollingsworth, J., Nakamura,
T.: Measuring productivity on high performance computers. In: IEEE International
Symposium on Software Metrics, p. 6 (2005). http://doi.ieeecomputersociety.org/
10.1109/METRICS.2005.33
29. Zelkowitz, M., Hollingsworth, J., Basili, V., Asgari, S., Shull, F., Carver, J.,
Hochstein, L.: Parallel Programmer Productivity: A Case Study of Novice Par-
allel Programmers. SC Conference 35 (2005). http://dx.doi.org/10.1109/SC.2005.
53

Taking Advantage of Node Power Variation
in Homogenous HPC Systems to Save Energy
Torsten Wilde1,2(B), Axel Auweter1, Hayk Shoukourian1,2, and Arndt Bode1,2
1 Leibniz Supercomputing Centre of the Bavarian Academy of Science
and Humanity, Garching Bei M¨unchen, Munich, Germany
2 Technical University Munich (TUM), Munich, Germany
torsten.wilde@lrz.de
Abstract. Saving energy and, therefore, reducing the Total Cost of
Ownership (TCO) for High Performance Computing (HPC) data centers
has increasingly generated attention in light of rising energy costs and
the technical hurdles imposed when powering multi-MW data centers.
The broadest impact on data center energy eﬃciency can be achieved
by techniques that do not require application speciﬁc tuning. Improving
the Power Usage Eﬀectiveness (PUE), for example, beneﬁts everything
that happens in a data center. Less broad but still better than individual
application tuning would be to improve the energy eﬃciency of the HPC
system itself. One property of homogeneous HPC systems that hasn’t
been considered so far is the existence of node power variation.
This paper discusses existing node power variations in two HPC sys-
tems. It introduces three energy-saving techniques: node power aware
scheduling, node power aware system partitioning, and node ranking
based on power variation, which take advantage of this variation, and
quantiﬁes possible savings for each technique. It will show that using
node power aware system partitioning and node ranking based on power
variation will save energy with very minimal eﬀort over the lifetime of
the system. All three techniques are also relevant for distributed and
cloud environments.
Keywords: HPC · Energy-eﬃciency · Energy-saving · Data center
1
Introduction
The steady rise in energy consumption of data centers world wide over the last
decade [21,22] and the future 20 MW exascale-challenge in High Performance
Computing (HPC) [20] makes saving energy an important consideration for HPC
data centers.
Improving data center energy eﬃciency starts with improved measurement
capabilities. Fortunately, the standard measurement capabilities of HPC sys-
tems have continuously been improved with the help of the energy eﬃcient
HPC community. In Europe the yearly held “European workshop on HPC cen-
tre infrastructures” [18] supported by the Partnership for Advanced Computing
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 376–393, 2015.
DOI: 10.1007/978-3-319-20119-1 27

Taking Advantage of Node Power Variation in Homogenous HPC Systems
377
in Europe (PRACE [28]) provides an exchange platform for best practices and
lessons learned regarding the improvement of HPC data center energy eﬃciency.
The Energy Eﬃcient HPC Working-Group (EEHPCWG) [15] deﬁned a stan-
dard measurement methodology [10] for HPC systems and provided HPC sys-
tem commissioning guidelines [11] that include power and energy measurement
requirements. The improved capabilities led to new discoveries and improved
energy eﬃciency. For example, energy aware scheduling [3] would not be possi-
ble without detailed power and energy measurements.
During the ﬁrst in-depth analysis of the EEHPCWG standard measurement
methodology [32], variation in overall system power was observed depending
on the measured part of the system (1/16, 1/8, full system). This was mainly
attributed to the diﬀerence of the included subsystems.
Further analysis of HPC systems at Leibniz Supercomputing Centre (LRZ)
led to the quantiﬁcation of node power variability for two homogeneous HPC
systems: SuperMUC and CooLMUC.
This paper discusses the ﬁndings of the analysis and will propose possible
energy savings techniques taking advantage of node power variability.
The rest of the paper is organized as follows. Section 2 will highlight some
related areas of energy eﬃciency research. Section 3 gives an overview of the
CooLMUC and SuperMUC HPC systems. Section 4 discusses the discovered
node variability in homogenous HPC systems. Section 5 proposes diﬀerent energy
savings techniques based on system node variability. Section 6 quantiﬁes the pos-
sible savings using the CooLMUC HPC system. Section 7 discusses future work
and Sect. 8 draws the conclusion.
Deﬁnitions
The following deﬁnitions are used throughout this document:
– Worst nodes - refers to nodes which consume the largest amount of power
in the HPC system node power distribution (this implies a ranking of nodes
based on power consumption)
– Best nodes - refers to nodes which consume the least amount of power in the
HPC system node power distribution (this implies a ranking of nodes based
on power consumption)
– EtS - Energy to Solution, the energy consumed by an application to solve a
speciﬁc problem
– TtS - Time to Solution, the runtime of an application
2
Background
The “4 Pillar Framework for Energy Eﬃcient HPC Data Centers” [39] provides a
representation of all parts of a data center that inﬂuence its energy eﬃciency. The
four pillars are: Building Infrastructure (Pillar1), HPC System Hardware (Pil-
lar2), HPC System Software (Pillar3), and HPC applications (Pillar4). Typically,

378
T. Wilde et al.
optimization performed in Pillar1 will have the broadest scope as optimizations
on the building infrastructure will aﬀect the entire center independent from the
HPC systems, system software, and applications. Contrary to that, optimizing
a single application in Pillar4 will have no eﬀect beyond the energy consumed
when running that particular application.
Figure 1 illustrates how the four pillar framework helps classifying the state
of the art in energy eﬃcient HPC research. Reducing the system-PUE (sPUE)
[38] of an HPC system (Pillar1 and Pillar2) by using the most eﬃcient cooling
technology available in the data center (for example, ASHRAE W5 [2] direct
liquid cooling) reduces the system overhead for all workloads running on the
system. Dynamic Voltage and Frequency Scaling (DVFS) (Pillar2) is used for
energy aware scheduling on Supercomputers which can minimize an application’s
Energy-to-Solution (EtS) [3] (combining information from Pillar 2, 3, and 4).
And last but not least, by improving an application performance the energy
eﬃciency of the application is improved (Pillar4).
External Influences/Constraints
Data Center:
Neighboring Buildings
Utility Providers
Pillar 1
Building Infrastructure
Pillar 2
HPC System Hardware
Pillar 3
HPC System Software
Pillar 4
HPC Applications
PUE
Energy Aware Scheduling
DVFS
sPUE
Performance 
Improvements
Node Power 
Variation
Fig. 1. The four pillar framework for energy eﬃcient HPC data centers and the coverage
areas of popular research areas in the ﬁeld of energy eﬃcient HPC.
The existence of node power variation has been shown previously by Hacken-
berg et al. [17]. Their analysis of using the SPEC MPI benchmark to quantifying
power consumption variations of HPC systems, showed a node power variation
of 7 % when idle and 5 % under maximum load for 16 double nodes of an AMD

Taking Advantage of Node Power Variation in Homogenous HPC Systems
379
Opteron cluster. Davis et al. [7] looked into variability of large-scale cluster power
models. They stated that inter-node variations in power consumption is one rea-
son that single node power models, when scaled to a large-cluster, show high
errors. However, both papers do not consider the use of node power variation
for energy savings.
Even though there are a multitude of energy eﬃcient IT system models (for
example [31,40]) node power variation hasn’t been included yet, most prob-
ably because previously measuring the variation was diﬃcult if not impossible
on large scale systems. However, this paper will show that although the possible
savings are small, node power variation is a property of HPC systems that has
the potential to improve the system energy eﬃciency independent of any other
energy saving techniques.
3
Test Systems
The data on which this work is based on was collected on two systems operated
at the Leibniz Supercomputing Centre of the Bavarian Academy of Sciences and
Humanities (LRZ) [23].
Fig. 2. CooLMUC experimentation HPC cluster at LRZ.
CooLMUC (Fig. 2) was built by MEGWARE and is the ﬁrst AMD based
direct liquid cooled (W5 water) HPC cluster with 178 nodes (8 nodes inter-
active, 166 nodes batch, and 4 nodes reserved for internal use). A single node

380
T. Wilde et al.
contains two AMD Opteron 6128HE CPUs (MagnyCours) with 8 cores each and
12 MB L3 cache. In their standard setting, the CPUs run at 2 GHz clock fre-
quency. Each node is equipped with 16 GB RAM arranged in eight 2 GB DDR3
modules. The main interconnect network is InﬁniBand QDR using a fat tree
topology. In addition, each node has two Gbit Ethernet ports for IPMI and a
service network which is used to boot the diskless nodes and to provide the
root ﬁlesystem over NFS. The cluster is completely room neutral, meaning that
there is no requirement for computer room air conditioning (CRAC) units. Power
measurements on CooLMUC are based on smart PDUs which report 1-minute
average power values per node. Suﬃciently long benchmark times are used to
minimize the error of the 1-minute readouts.
Fig. 3. SuperMUC super-computer at LRZ.
SuperMUC (Fig. 3) (Nr.14, Top500 List Nov 2014) was built by IBM based
on iDataPlex technology with a peak performance of 3 PetaFLOPS. It is a
Gauss Center for Supercomputing (GCS) system made available to PRACE
users. SuperMUC’s thin node islands have 147.456 processor cores in 9216 com-
pute nodes. Each node has two Intel Sandy Bridge-EP Xeon E5-2680 8C proces-
sors, 32 GB memory, and is direct liquid cooled using ASHRAE W5 water. The
interconnect is Inﬁniband FDR10, a fat tree inside an island, and a Pruned
Tree (4:1 blocking factor) between islands. SuperMUC provides multiple levels
of power measurements. For this analysis the “IBM Active Energy Manager” was
used which collects power and energy consumption data at the power supply of
each node.

Taking Advantage of Node Power Variation in Homogenous HPC Systems
381
4
Node Power Variation
A feature of homogenous HPC systems, that wasn’t known in detail before the
improvement of system power measurement capabilities, is the existence of power
variation between nodes when running the same workload.
Figure 4 shows the average node power variation for 174 CooLMUC nodes
when running the same single node MPrime benchmark (see also [34]). MPrime
was chosen because it is one of the most power consuming benchmarks. In order
to account for the speciﬁc load pattern, the benchmark was run for over 60 min.
As can be observed, there is a diﬀerence of 21 W between the node with the least
and the node with the highest power consumption (i.e. the worst node consumes
8.75 % more power than the best node).
0
5
10
15
20
25
240
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
Average Node Power (watt)
Number of Nodes
Fig. 4. Histogram of CooLMUC’s node power when running single node MPrime at
2.0 GHz (AMD MagnyCours).
A similar variation pattern exists in other HPC systems as well. Figure 5
shows the node power variation of the 516 nodes (512 compute nodes, 4 spare
nodes) of island5 of SuperMUC when running the single node FIRESTARTER
benchmark on each node at 2.3 GHz (the average of 5 runs was used). FIREST-
ARTER was developed at TU Dresden, Germany, with the aim to create the
highest CPU power consumption possible [16]. It was used instead of MPrime
because its constant load pattern allowed a signiﬁcant reduction of benchmark
runtime to 15 min for each run. For island5 of SuperMUC the power diﬀerence
between the best and worst node is 42 W (18.9 %).
The main contributor to this node power variation is the CPU [13,27]. We
suspect this to be due to semiconductor manufacturing tolerances, the same
reason for which the ﬁnished products are categorized based on their thermal

382
T. Wilde et al.
and frequency characteristics, a process called “product binning”. It was also
observed that the node power variation does not change over the relative short
life time of an HPC system even though semiconductors age over time [19]. Yet
each node replacement needs to be evaluated and the node power ranking needs
to be adjusted. The node power variation will also not change when running
diﬀerent applications since it is a hardware property, good nodes will stay good
nodes.
0
5
10
15
20
25
30
35
40
45
222
230
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
264
Average Node Power
Count of Node
Fig. 5.
SuperMUC
(island5)
512
node
power
variation
running
single
node
FIRESTARTER at 2.3 GHz (Intel Sandy Bridge).
As a side note: Outlying nodes might warrant some closer examination. For
example, power supplies prior to failure tend to supply reduced voltages to the
node and, therefore, reduce the nodes power consumption as experienced on
CooLMUC. Similarly, a high power consumption paired with a high node tem-
perature could indicate improperly seated heatsinks on systems that use direct
liquid cooling as experienced during the PRACE 2nd Implementation Phase
project (PRACE 2IP) energy eﬃcient HPC prototyping eﬀort [29].
5
What to do with Node Power Variation
5.1
Node Power Aware Scheduling
Thermal and hot spot aware scheduling [4,36] works well in distributed and
cloud computing environments and the same techniques could be used for node
power aware scheduling.
Table 1 shows the power and energy consumption of running the High Per-
formance Conjugate Gradient Benchmark (HPCG) [9] on the 10 worst vs. the 10

Taking Advantage of Node Power Variation in Homogenous HPC Systems
383
best nodes of island5. The results show that node variability is not just a factor
for stress test workloads but also for applications. At a diﬀerence of 26.9 W run-
ning on the 10 worst nodes imposes an overhead of 14.4 % in power consumption
compared to running on the 10 best nodes.
Table 1. HPCG at 2.3 GHz on 10 best and worst nodes of island5 of SuperMUC (Intel
Sandy Bridge)
EtS (kWh) average node power (W) TtS (s)
10 worst nodes 0.658
213.4
1110
10 best nodes
0.575
186.5
1110
To get an idea how larger HPC applications are aﬀected, Table 2 shows the
diﬀerence when running HPCG on the 256 best or 256 worst nodes of island5.
Here the impact of running on the worst nodes is lower (4.3 %) because the
node average starts to dominate the distribution. It can also be seen that the
application performance is not aﬀected by the selection of nodes. Therefore,
power and energy optimization can be used interchangeably.
Table 2. HPCG at 2.3 GHz on 256 best and worst nodes of island5 of SuperMUC
(Intel Sandy Bridge)
EtS (kWh) average power (W) TtS (s)
256 worst nodes 15.55
197.7
1106
256 best nodes
15.03
189.6
1115
Obviously, the idea becomes less applicable when considering that in HPC
one tries to use all resources 100 % of the time. This means that while one
application may run on the best nodes another will run on the worst nodes.
The possible gain for an application A and B running on island5 would be the
diﬀerence between the gain of application A (worst vs best nodes) and the gain
of application B (worst vs best nodes) (Fig. 6 and Eq. 1).
Savingmaxtheoretical = |GainA −GainB| = |(PA.on.Worst −PA.on.Best) −(PB.on.Worst −PB.on.Best)|
(1)
For example, by combining HPCG gain (8.1 W per node) with Epoch gain
(6.4 W per node) (Table 3), which were both chosen because they reﬂect real
HPC data center workloads, the expected saving is: 1.7 W per node, equaling
870.4 W for the 512 nodes of island5.
This shows that in order to use node power variability for node power aware
scheduling one application needs to be less sensitive to the used node set (see
Fig. 6 application B). Since most of the power consumption comes from the CPU,
a combination of CPU bound (high frequency and high power consumption)

384
T. Wilde et al.
EtS
worst
ts
e
b
ts
r
o
w
best
B
B
nodes
nodes
Fig. 6. Node power aware scheduling application A and B.
Table 3. Epoch at 2.3 GHz on 256 best and worst nodes of island5 of SuperMUC (Intel
Sandy Bridge)
EtS (kWh) average power (W) TtS (s)
256 worst nodes 9.20
164.2
788
256 best nodes
8.83
157.8
787
applications running on the best nodes with memory/io bound (low frequency
and low power consumption) applications running on the worst would be better.
In addition, the analysis estimated the maximum theoretical savings, which in
comparison to reality where the node allocation would be random, overestimates
the possible savings. Therefore, the possible savings might not be substantial
enough to account for the additional eﬀort required for the scheduling system to
allow for this selection. A more in-depth analysis using application models might
be required to really quantify the potential of node power aware scheduling.
5.2
Node Switch-oﬀ, Node Power Aware System Partitioning,
and Node Ranking Based on Power Variation
The best technique in terms of saving energy is to switch oﬀwhat is not used.
This can have a great impact in distributed and/or virtualized data centers,
since in those environments it is possible to consolidate workload to switch oﬀ
resources. One concrete technique for energy-aware workload placement in cloud
computing can be found in [5]. Both All4Green [1] and Fit4Green [12] project
deliverables provide many insights into energy saving techniques for distributed
data centers. Finally a very comprehensive survey on research related to power
and energy-eﬃcient system design and usage can be found in [6]. Unfortunately,
the impact of such techniques will be lower in HPC since the goal is to have
a 100 % utilization rate of the resources. In addition, one needs to be aware of
possible challenges when using node switch-oﬀor sleep mode in very large HPC
installations. For example, the sudden removal or addition of possible thousands
of nodes can stall the interconnect fabric leading to the termination of jobs. Also,

Taking Advantage of Node Power Variation in Homogenous HPC Systems
385
0
50
100
150
200
250
300
350
Runtime in Days
Nodes
CooLMUC Node Runtimes 2014
Batch
Interactive
Fig. 7. CooLMUC node utilization in 2014 for batch and interactive partition
Table 4. CooLMUC partition usage for 2014
runtime per node interactive batch
min
45.49 days 224.43 days
max
85.62 days 311.19 days
average
63.47 days 289.07 days
the reliable wake-up from sleep-mode of nodes can be aﬀected by the robustness
of the software stack, a challenge especially in the early lifetime of the system.
One way to minimize the runtime of the worst nodes in HPC data centers is
to exploit usage pattern diﬀerences in the HPC system partitions (job queues).
CooLMUC for example has two partitions: one batch partition (166 nodes) and
one interactive partition (8 nodes). Empty nodes in the batch partition only
appear when no small jobs for backﬁlling are in the queue, or when the system
needs to drain nodes in order to start a high priority job with a high node count.
The nodes of the interactive partition are mainly used during normal working
hours. Figure 7 shows the utilization of each node in the interactive partition
and batch partition for the year 2014. Table 4 shows the average, minimum, and
maximum utilization for the nodes of each partition for the year 2014.
Two important observations can be made from this data:
1. A signiﬁcant diﬀerence between partitions, depending on their usage pattern,
exists. For example, the interactive partition is mainly used during normal

386
T. Wilde et al.
working hours (8 h a day equals 83 days/year per node) whereas the batch
partition can be used 24/7 with a maximum of 365 days per node.
2. The utilization varies between the nodes even in the same partition.
By using the observed system properties it is possible to deﬁne at least three
techniques that use the average node power distribution to save energy. Firstly,
the worst nodes of the average node power distribution should be moved to
the partition with the least usage (ideal would be spare nodes). Secondly, the
scheduler should prioritize the nodes according to the power distribution for each
partition. Preference should always be given to the nodes with the lowest power
consumption. And ﬁnally, because nodes are never utilized 100 % in a year, one
should switch-oﬀnodes especially for partitions that do not have a 24/7 usage
pattern. Switching oﬀthe worst nodes ﬁrst (because these are used the least)
provides an extra beneﬁt.
6
Savings Analysis
This section will quantify possible savings related to node power aware parti-
tioning and node power ranking using the CooLMUC HPC system (174 nodes [8
nodes interactive, 166 nodes batch] are considered). First, the theoretically best
and worst system distribution scenarios are compared to get an idea about the
maximum possible savings potential. In reality, a system without node power
aware partitioning and node ranking based on power variation will have a dis-
tribution between the best and worst case scenarios. Therefore, the real-world
savings using the actual CooLMUC setup are discussed. PowerDAM [33] was
used to collect and analyze power and energy data.
At ﬁrst, the measured node power distribution (Fig. 4) was normalized using
the average power consumption of all nodes during the MPrime benchmark
(251.14 W). Given the power consumption of an application on an average node,
the normalized power distribution can be used to derive the power consumption
of the application when running on good nodes or bad nodes respectively. After
that the following system and node statistics were collected:
1. System:
– average power per node when running jobs on interactive partition
– runtime of all jobs on the interactive partition
– average power per node when running job on batch partition
– runtime of all jobs on the batch partition
2. Nodes:
– runtime for each node (annual utilization time of each node)
– average power for each node (average of all power measurements when the
node was used by a job during the year)
Table 5 shows the CooLMUC system partitions power and runtime statistics for
2014. The system energy consumption is the sum of the energy consumption of
the interactive and batch partitions. The energy consumption of one partition is

Taking Advantage of Node Power Variation in Homogenous HPC Systems
387
Table 5. CooLMUC partitions statistic for 2014
interactive batch
average node power (W)
156.20
217.58
runtime (h)
12186.22
1151654.97
energy consumption (kWh)
1903.49
250577.09
the average yearly node power when running jobs on partition X multiplied by
partition X runtime.
Table 6 shows the data for node power aware system partitioning without
taking node ranking into consideration. In the worst case, the best nodes are
moved into the interactive partition. In the best case, the worst nodes are moved
into the interactive partition. Using this technique a maximum theoretical saving
of 663.28 kWh per year can be achieved.
Table 6. Best possible savings for CooLMUC using node power aware system parti-
tioning for 2014
interactive
batch
system
average node power (worst in interactive)
162.26 W
217.17 W
energy consumption (worst in interactive)
1977.39 kWh
250108.26 kWh
252085.65 kWh
average node power (best in interactive)
152.08 W
217.86 W
energy consumption (best in interactive)
1853.27 kWh
250895.65 kWh
252748.93 kWh
possible max. savings
663.28 kWh
Table 7 shows the results for power aware system partitioning and node rank-
ing based on power variation. In the worst case the best nodes are moved into
the interactive partition and in each partition the nodes are arranged so that the
longer the runtime the worse its power consumption. In the best case the worst
nodes are moved into the interactive partition and in each partition the nodes
are arranged so that the longer the runtime the better its power consumption.
Table 7. Best possible savings for CooLMUC using node power aware system parti-
tioning and node ranking based on power variation for 2014
interactive
batch
system
energy consumption (worst in interactive)
1962.81 kWh
250059.26 kWh
252022.07 kWh
energy consumption (best in interactive)
1841.05 kWh
251112.44 kWh
252953.49 kWh
possible max. savings
931.42 kWh
Using node ranking based on power variation in each partition saves nearly
50 % more energy than power aware system partitioning alone, increasing the
possible theoretical energy savings to 931.42 kWh.

388
T. Wilde et al.
In Reality. CooLMUC consumed 252480.58 kWh in 2014 (Table 5). This leads to
a possible savings of 251.77 kWh for 2014 (252480.58 kWh minus 252022.07 kWh
(taken from Table 7).
Since each 1 kWh IT power saved will also save cooling costs, the HPC system
internal cooling overhead (1.23 for CooLMUC [24]) and sPUE (see Fig. 1) need
to be considered. sPUE is deﬁned in [38] as:
sPUE = 1 + OverheadPDCL +
n

k=1
(wk ∗
1
COPk
)
1 =
n

k=1
wk
Where wk is the distribution for each heat removal technology k used in the
HPC system and the sum of all wk is 1 (equaling 100 % heat removal). The data
center overhead incurred by cooling the system is represented by 1/COPk which
is the power needed to remove 1 W of heat via the heat removal technology
k. OverheadP DCL is the additional power needed by the data center to provide
1 W of IT power.
Table 8. LRZ electrical and cooling overhead
LRZ
PDCL overhead
0.075
Air cooling overhead
0.500
W1 cooling overhead 0.400
W4 cooling overhead 0.050
Using the LRZ data center overhead information from Table 8, and the knowl-
edge that CooLMUC is 100 % cooled using W4, sPUE can be determined:
sPUECooLMUC = 1 + 0.075 + 1 ∗0.05 = 1.125
Using the system internal cooling overhead and sPUE, the ﬁnal yearly (for 2014)
energy savings for CooLMUC would be:
SavingsCooLMUC = 251.77 ∗1.23 ∗1.125 = 348.39kWh
If one, in the most simplistic way, scales the CooLMUC (43 kW average sys-
tem power) result to SuperMUC (2.4 MW average power consumption) then the
possible yearly savings would be: 56 times 348.39 kWh = 19509.84 kWh.
7
Future Work
The next step is to apply the proposed techniques in practice. Since CooL-
MUC is a prototype system it is available for research activities. The CooLMUC

Taking Advantage of Node Power Variation in Homogenous HPC Systems
389
interactive and batch partitions will be changed according to node power aware
system partitioning. SLURM, which is the scheduler used on CooLMUC, sup-
ports scheduling weights for resources. This might be suﬃcient to apply node
ranking based on power variation. The success of this technique can be veriﬁed
by checking that the worst nodes have the lowest runtime.
Node power aware scheduling might still be interesting for saving energy but
generic models for application power and energy proﬁles are needed. There is
already some work going into understanding the power consumption behavior
of nodes in terms of analysing the energy eﬃciency features of newer processors
and quantifying the power and energy costs of computation in terms of data
movement, ﬂops, and network communication. Using this information, a more
generic estimate of possible power and energy savings using node power aware
scheduling needs to be done. A quantiﬁcation of the impact on scheduling deci-
sions (scheduling complexity increase, scheduling time required, etc.) is required
before node power aware scheduling can be judged.
8
Conclusion
This paper made the following contributions to the state of the art in HPC power
savings research:
– It showed and quantiﬁed the existence of node power variation in current
homogenous HPC systems.
– It proposed and evaluated three techniques for saving energy using the node
power variation:
• node power aware scheduling
• node power aware system partitioning
• node ranking based on power variation
Node power variation can be used to save energy. The proposed techniques are
not limited to HPC data centers and can be used in conjunction with any other
energy savings eﬀort. From the three techniques, node power aware system par-
titioning and node ranking based on power variation are the most practical,
requiring very little eﬀort, and will beneﬁt the system over its complete lifetime.
In the best theoretical case a combined savings of these two techniques of 0.5 %
(using data from Table 7 and multiplying the saving with the system, 1.2, and
data center overhead, 1.125) is possible. In reality of course a system will start
with a conﬁguration between the worst and best case. The possible savings will
vary with the system size, the node power spread (diﬀerence between best and
worst), and the statistical node power distribution. If a system starts in the
middle between the theoretically best and worst distribution, which is a reason-
able assumption, then a real world saving of 0.25 % for an HPC system seems
possible. In the provided CooLMUC example, a saving of only 0.1 % could be
realized.
Even though node power variation might not provide huge energy savings
for HPC data centers, it has a high savings potential for data centers with lower

390
T. Wilde et al.
resource utilization. As shown in Sect. 5.1, for example, with a 50 % resource
utilization rate picking the best nodes saves 4.3 % for the HPC example appli-
cation. Lower utilization rates will provide even more savings potential. Even
though there do not exist many scientiﬁc publications analysing the utilization
rates of non-HPC data centers (one example is [25] where an average utilization
rate of less than 16 % for a week is reported), industry surveys approximate the
average utilization rate of most server clusters to be on average 15 % [26,30].
For example, a 2014 white paper by the Natural Resources Defense Council [37]
states that current hyper-scale cloud providers (which accounted for only 4 %
of the overall data centers power consumption in 2011) can realize average uti-
lization rates of 40 %. And research from Google indicates that typical server
clusters (which accounted for 95 % of the overall data centers power consump-
tion in 2011) average anywhere from 10 to 50 percent utilization [37]. The use
of virtualized environments improves the average utilization rate but an esti-
mated 20 to 30 percent of servers in large data centers today are idle, obsolete,
or unused [35,37].
Although a power saving of around 0.25 % for an HPC data center might
not seem like much, it will add up if one considers, for example, the aggregated
power consumption of all HPC systems in the November 2014 Green500 list [14]
(592.31 MW not including data center cooling infrastructure overhead), or the
yearly power consumption of all data centers world wide (38.84 GW for 2013
according to DCD Intelligence [8]).
Acknowledgments. The authors would like to thank Jeanette Wilde (LRZ) for her
valuable comments and support.
The work presented here has been carried out within the SIMOPEK project, which
has received funding from the German Federal Ministry for Education and Research
under grant number 01IH13007A, at the Leibniz Supercomputing Centre (LRZ) with
support of the State of Bavaria, Germany, and the Gauss Centre for Supercomputing
(GCS).
The EPOCH code used in this research was developed under UK Engineering
and Physics Sciences Research Council grants EP/G054940/1, EP/G055165/1 and
EP/G056803/1.
References
1. All4Green: http://www.all4green-project.eu/
2. ASHRAE TC 9.9.2011: 2011 thermal guidelines for liquid cooled data processing
environments. White paper (2011). www.tc99.ashraetcs.org
3. Auweter, A., et al.: A case study of energy aware scheduling on SuperMUC. In:
Kunkel, J.M., Ludwig, T., Meuer, H.W. (eds.) ISC 2014. LNCS, vol. 8488, pp.
394–409. Springer, Heidelberg (2014)
4. Banerjee, A., Mukherjee, T., Varsamopoulos, G., Gupta, S.K.S.: Cooling-aware and
thermal-aware workload placement for green hpc data centers. In: GREENCOMP
2010 Proceedings of the International Conference on Green Computing, pp. 245–
256. IEEE Computer Society, Washington, DC, USA (2010). http://dx.doi.org/10.
1109/GREENCOMP.2010.5598306

Taking Advantage of Node Power Variation in Homogenous HPC Systems
391
5. Beloglazov, A., Abawajy, J., Buyya, R.: Energy-aware resource allocation heuris-
tics for eﬃcient management of data centers for cloud computing. Future Gener.
Comput. Syst. 28(5), 755–768 (2012). http://www.sciencedirect.com/science/
article/pii/S0167739X11000689, special Section: Energy eﬃciency in large-scale
distributed systems
6. Beloglazov, A., Buyya, R., Lee, Y.C., Zomaya, A.Y.: A taxonomy and survey of
energy-eﬃcient data centers and cloud computing systems. Adv. Comput. 82, 47–
111 (2011). http://dblp.uni-trier.de/db/journals/ac/ac82.html#BeloglazovBLZ11
7. Davis, J.D., Rivoire, S., Goldszmidt, M., Ardestani, E.K.: Accounting for variability
in large-scale cluster power models. In: Exascale Evaluation and Research Tech-
niques (2011). http://research.microsoft.com/pubs/146087/EXERT Variability
CR3.pdf
8. DCD Intelligence: Is the industry getting better at using power? Data Cen-
ter Dynamics FOCUS 33, January/February 2014 33, 16–17 (2014). http://
content.yudu.com/Library/A2nvau/FocusVolume3issue33/resources/index.htm?
referrerUrl=
9. Dongarra, J., Heroux, M.A.: Toward a new metric for ranking high performance
computing systems. Sandia Report, SAND2013-4744 312 (2013)
10. EEHPCWG: Energy eﬃcient high performance computing power measure-
ment methodology. Tech. rep., Energy Eﬃicient High Performace Comput-
ing Working Group (2013). http://www.green500.org/sites/default/ﬁles/eehpcwg/
EEHPCWG PowerMeasurementMethodology.pdf
11. EEHPCWG: Energy eﬃciency considerations for hpc procurement documents: 2014.
Tech. rep., Energy Eﬃicient High Performace Computing Working Group (2014).
http://eehpcwg.lbl.gov/sub-groups/equipment-1/procurement-considerations/pro
curement-considerations-presentations
12. Fit4Green: http://www.ﬁt4green.eu/
13. Ge, R., Feng, X., Cameron, K.: Modeling and evaluating energy-performance eﬃ-
ciency of parallel processing on multicore based power aware systems. In: IEEE
International Symposium on Parallel Distributed Processing, IPDPS 2009, pp. 1–8
(May 2009)
14. Green500 List: http://www.green500.org/
15. Energy Eﬃcient HPC Working Group: http://eehpcwg.lbl.gov/
16. Hackenberg, D., Oldenburg, R., Molka, D., Schone, R.: Introducing ﬁrestarter: a
processor stress test utility. In: 2013 International Green Computing Conference
(IGCC), pp. 1–9 (June 2013)
17. Hackenberg, D., Sch¨one, R., Molka, D., M¨uller, M., Kn¨upfer, A.: Quantifying power
consumption variations of HPC systems using spec mpi benchmarks. Comput. Sci.
Res. Dev. 25(3–4), 155–163 (2010). http://dx.doi.org/10.1007/s00450-010-0118-0
18. Workshop on HPC centre infrastructures, E.: http://www-hpc.cea.fr/fr/evenem
ents/Workshop-HPC-2013.htm
19. Keane, J., Kim, C.: An odomoeter for cpus. IEEE Spectr. 48(5), 28–33 (2011)
20. Kogge, P.: ExaScale computing study: Technology challenges in achieving exascale
systems. Univ. of Notre Dame, CSE Dept. Tech. Report TR-2008-13 (September
28, 2008)
21. Koomey, J.G.: Worldwide electricity used in data centers (2008). http://iopscience.
iop.org/1748-9326/3/3/034008/pdf/1748-9326 3 3 034008.pdf
22. Koomey, J.G.: Growth in data center electricity use 2005 to 2010 (2011). http://
www.twosides.us/content/rspdf 218.pdf

392
T. Wilde et al.
23. Leibniz Supercomputing Centre: http://www.lrz.de
24. Johnsson, L., Netzer, G., Boyer, E., Carpenter, P., Januszewski, R., Koutsou, G.,
Saastad, O.W., Stylianou, G., Wilde, T.: D9.3.4 Final Report on Prototype Eval-
uation. PRACE 1IP-WP9 public deliverable, p. 44 (2013). http://www.prace-ri.
eu/IMG/pdf/d9.3.4 1ip.pdf
25. Liu, H.: A measurement study of server utilization in public clouds. In: 2011 IEEE
Ninth International Conference on Dependable, Autonomic and Secure Computing
(DASC), pp. 435–442 (December 2011)
26. Mark Aggar (Microsoft): The IT Energy Eﬃciency Imperative. White paper (2011)
27. Naﬀziger, S.: AMD at ISSCC: Bulldozer Innovations Target Energy Eﬃciency.
http://community.amd.com/community/amd-blogs/amd-business/blog/2011/02/
22/amd-at-isscc-bulldozer-innovations-target-energy-eﬃciency
28. Partnership for Advanced Computing in Europe: http://www.prace-ri.eu/
29. Partnership
for
Advanced
Computing
in
Europe:
http://www.prace-ri.eu/
prace-2ip/
30. Ravi A. Giri (StaﬀEngineer, Intel IT) and Anand Vanchi (Solutions Architect,
Intel Data Center Group): Increasing Data Center Eﬃciency with Server Power
Measurements. IT@Intel White Paper, p. 7 (2011)
31. Samak, T., Morin, C., Bailey, D.: Energy consumption models and predictions
for large-scale systems. In: 2013 IEEE 27th International Parallel and Distributed
Processing Symposium Workshops & Ph.D. Forum (IPDPSW), pp. 899–906. IEEE
(2013)
32. Scogland, T.R., Steﬀen, C.P., Wilde, T., Parent, F., Coghlan, S., Bates, N., Feng,
W.C., Strohmaier, E.: A power-measurement methodology for large-scale, high-
performance computing. In: ICPE 2014 Proceedings of the 5th ACM/SPEC Inter-
national Conference on Performance Engineering, pp. 149–159. ACM, New York,
NY, USA (2014). http://doi.acm.org/10.1145/2568088.2576795
33. Shoukourian, H., Wilde, T., Auweter, A., Bode, A.: Monitoring power data: a ﬁrst
step towards a uniﬁed energy eﬃciency evaluation toolset for HPC data centers.
Environ. Model. Softw. 56, 13–26 (2014). http://www.sciencedirect.com/science/
article/pii/S1364815213002934, thematic issue on Modelling and evaluating the
sustainability of smart solutions
34. Shoukourian, H., Wilde, T., Auweter, A., Bode, A.: Predicting the energy and
power consumption of strong and weak scaling HPC applications. Supercomput.
Front. Innovations 1(2), 20–41 (2014)
35. Stansberry, M.: Uptime institute annual data center industry survey report and
full results (2013). http://www.data-central.org/resource/collection/BC649AE0-
4223-4EDE-92C7-29A659EF0900/uptime-institute-2013-data-center-survey.pdf
36. Wang, L., Khan, S.U., Dayal, J.: Thermal aware workload placement with task-
temperature proﬁles in a data center. J. Supercomput. 61(3), 780–803 (2012)
37. Whitney, J., Delforge, P.: Scaling up energy eﬃciency across the Data Center Indus-
try: evaluating Key Drivers and Barriers. Data Center Eﬃciency Assessment (2014).
http://www.nrdc.org/energy/ﬁles/data-center-eﬃciency-assessment-ip.pdf
38. Wilde, T., Auweter, A., Patterson, M., Shoukourian, H., Huber, H., Bode, A.,
Labrenz, D., Cavazzoni, C.: DWPE, a new data center energy-eﬃciency metric
bridging the gap between infrastructure and workload. In: 2014 International Con-
ference on High Performance Computing Simulation (HPCS), pp. 893–901 (July
2014)

Taking Advantage of Node Power Variation in Homogenous HPC Systems
393
39. Wilde, T., Auweter, A., Shoukourian, H.: The 4 Pillar Framework for energy eﬃ-
cient HPC data centers. In: Computer Science - Research and Development, pp.
1–11 (2013). http://dx.doi.org/10.1007/s00450-013-0244-6
40. Wu, X., Lively, C., Taylor, V., Chang, H.C., Su, C.Y., Cameron, K., Moore, S.,
Terpstra, D., Weaver, V.: Mummi: multiple metrics modeling infrastructure. In:
2013 14th ACIS International Conference on Software Engineering, Artiﬁcial Intel-
ligence, Networking and Parallel/Distributed Computing (SNPD), pp. 289–295
(July 2013)

A Run-Time System for Power-Constrained
HPC Applications
Aniruddha Marathe1(B), Peter E. Bailey1, David K. Lowenthal1,
Barry Rountree2, Martin Schulz2, and Bronis R. de Supinski2
1 Department of Computer Science, The University of Arizona, Tucson, USA
{amarathe,pbailey,dkl}@cs.arizona.edu
2 Lawrence Livermore National Laboratory, Livermore, USA
{rountree,schulzm,bronis}@llnl.gov
Abstract. As the HPC community attempts to reach exascale perfor-
mance, power will be one of the most critical constrained resources.
Achieving practical exascale computing will therefore rely on optimiz-
ing performance subject to a power constraint. However, this additional
complication should not add to the burden of application developers;
optimizing the run-time environment given restricted power will primar-
ily be the job of high-performance system software.
This paper introduces Conductor, a run-time system that intelligently
distributes available power to nodes and cores to improve performance.
The key techniques used are conﬁguration space exploration and adap-
tive power balancing. Conﬁguration exploration dynamically selects the
optimal thread concurrency level and DVFS state subject to a hardware-
enforced power bound. Adaptive power balancing eﬃciently determines
where critical paths are likely to occur so that more power is distrib-
uted to those paths. Greater power, in turn, allows increased thread
concurrency levels, the DVFS states, or both. We describe these tech-
niques in detail and show that, compared to the state-of-the-art tech-
nique of using statically predetermined, per-node power caps, Conductor
leads to a best-case performance improvement of up to 30 %, and average
improvement of 19.1 %.
1
Motivation
The US government, as well as European and Asian agencies, have set a goal to
reach exascale computing in less than 10 years. However, if we were to build an
exascale machine out of today’s hardware, it would consume half of a gigawatt of
power [13,21] and eﬀectively require a dedicated power plant. In reality, there is
a practical power bound, which is much tighter, and one such bound commonly
used by both the research as well as the industrial high-performance computing
(HPC) community is 20 MW [2]. It is clear that future HPC systems will have
a whole-system power constraint that will ﬁlter down to job-level power con-
straints. The goal at the job-level will be to optimize performance subject to a
prescribed power bound.
HPC users have enough to handle with ensuring correctness and maintaining
suﬃcient performance, so the task of enforcing the job level power bound should
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 394–408, 2015.
DOI: 10.1007/978-3-319-20119-1 28

A Run-Time System for Power-Constrained HPC Applications
395
be left to HPC system software. More importantly, system software is in an
ideal position to dynamically conﬁgure applications for the best performance
subject to a power constraint. We deﬁne a processor’s conﬁguration as: (1) a
value for c, the number of active cores, and (2) the DVFS state. The power
constraint states that the total job power consumption must always be no more
than the job-level power bound P, and the goal is to minimize application run
time. We use Intel’s Running Average Power Limit (RAPL) [14], introduced in
the SandyBridge microarchitecture, to enforce the power constraint.
This paper describes the Conductor run-time system, which eﬃciently
chooses an initial conﬁguration resulting in near-optimal application perfor-
mance for a given job power bound, then adapts this conﬁguration as necessary
during application execution according to changing application behavior and
power constraints. The fundamental ideas behind Conductor are twofold. First,
Conductor performs conﬁguration space exploration, which dynamically selects
the optimal thread concurrency level (DCT) and dynamic voltage frequency
state (DVFS) subject to a RAPL-enforced power bound. Second, Conductor
performs adaptive power balancing, which locates non-critical parts of the appli-
cation (i.e., oﬀthe critical path), reduces their power consumption, and uses
that excess power to speed up the parts on the critical path.
In Conductor, adaptive power balancing itself is done in three stages. First,
Conductor monitors an application timestep to gauge representative application
behavior. Second, Conductor continually applies a local, adaptive algorithm to
select task conﬁgurations to reduce power consumption without increasing task
execution time where possible. Third, Conductor improves performance by real-
locating power at the MPI process level, using a global algorithm that is peri-
odically executed at the end of timesteps. Conductor uses RAPL to enforce the
chosen power allocation on each MPI process.
Speciﬁcally, this paper makes the following contributions.
– We design Conductor, the ﬁrst run-time system that utilizes nonuniform
power distribution, RAPL, DVFS, and DCT for optimizing HPC application
performance under a power constraint.
– Conductor is fully automatic and chooses conﬁgurations with no involvement
of the application programmer other than marking the end of an application
timestep.
– Conductor chooses and adapts conﬁgurations dynamically based on applica-
tion characteristics, resulting in eﬃcient execution on a number of applica-
tions.
We implement Conductor on a large scale cluster at Lawrence Livermore
National Laboratory, which has infrastructure for constraining power on a per-
processor basis. Our results on up to 64 processors (512 cores) show that over ﬁve
applications, Conductor achieves an average performance improvement of 19.1 %
in a range of power limits over the state-of-the-art method, which is statically
selected, per-node power caps. Moreover, we observe that Conductor achieves
best-case performance improvement of 30 % over the static allocation scheme.

396
A. Marathe et al.
Fig. 1. Execution model on two MPI processes. The process with rank 1 sends a mes-
sage to the process on rank 0. The process on rank 0 arrives ﬁrst at MPI Finalize,
inducing slack time.
2
Optimizing Overprovisioned Systems
Traditionally, achieving maximum throughput in HPC clusters has been con-
strained by the available (ﬁxed) hardware. However, as we move towards exas-
cale, power, rather than hardware, will become a limiting factor in achieving
optimal performance. With power consumption in a supercomputing cluster
becoming critical, the allocation of power to individual jobs must comply to
strict power constraints.
Under the given power constraint, which is typically imposed at the facility
level, a supercomputing system may run fewer nodes at maximum power or
more nodes at a lower power (known as overprovisioning). The system-level
power constraint translates into job-level power constraints, which motivate the
design of techniques to optimize performance under job-level power constraints.
The primary objective is to run each job at a conﬁguration that is power-eﬃcient
and allocate power to the critical path of the application.
2.1
Execution Model
In order to reason about optimization approaches in overprovisioned systems,
we adopt the task based execution model (Fig. 1) that we used in our work
on the Adagio run-time system [23]. A task is the basic unit of scheduling,
comprising total communication and computation that takes place on an MPI
process between two consecutive MPI calls. Note that a process must block at
a communication call if there is an inbound receive edge and the data has not
yet arrived, or if a process arrives late to a collective. This can lead to what
we refer to as slack time. As an example of the ﬁrst case, the MPI process with
rank 0 arrives before the process with rank 1 at MPI Finalize in Fig. 1, inducing
slack time.

A Run-Time System for Power-Constrained HPC Applications
397
2.2
Assumptions
We use the execution model discussed above to design our optimizing run-time
system, which we introduce in the next section. Additionally, we currently focus
on applications implemented with the SPMD (Single-Program, Multiple-Data
[7]) model and use OpenMP for intra-node parallelism and MPI for inter-node
parallelism. We assume that programs use MPI THREAD SINGLE, so there are no
MPI calls within an OpenMP region. There is nothing that prevents us from
conceptually supporting pure MPI programs (i.e., one MPI rank per core), but
because our system chooses a given number of cores per processor dynamically,
a pure MPI approach would require expensive data redistribution or core over-
subscription when the number of cores per processor changes. We assume that
an application is composed of several timesteps, and that the programmer iden-
tiﬁes the end of a timestep (currently accomplished with inserting MPI Pcontrol
into the application code).
Following the work of Li et al. [18], we also restrict each MPI process
(and therefore each OpenMP parallel region), to a single CPU socket/NUMA
node, and we assume that we use the same number of active cores for OpenMP
regions between consecutive MPI calls. This avoids increasing the number of
cache misses due to a change in the number of active cores between two OpenMP
regions.
2.3
Challenges
Previous work in the area of power-constrained performance optimization out-
lines the following challenges in developing a run-time system to adaptively
select the best processor conﬁguration for an application. First, the conﬁgura-
tion space from which to select the optimal conﬁguration is large, because mod-
ern day processors have over a dozen frequency steps and provide 16 or more
physical cores. Thus ﬁnding the optimal conﬁguration for each processor in a
job becomes a large combinatorial problem. Combined with the fact that diﬀer-
ent conﬁgurations can result in vastly diﬀerent performance [20], the quality of
current techniques is unknown. Second, allocating the optimal amount of power
to individual processors in a job is complicated by the fact that the critical path
may move through multiple processes in a time step. Finally, eﬃciently moni-
toring power usage for individual processors allocated to a job and re-allocating
power with acceptable overhead is a challenge. Conductor addresses all three
challenges and provides a novel approach for eﬀective use of large, power-limited
systems.
3
Conductor: Power-Constrained Runtime Scheduling
Conductor continuously monitors the execution behavior of an application and
adjusts its conﬁguration parameters to stay within the job-level power limit while
optimizing performance. In particular, we use two knobs in the conﬁguration of

398
A. Marathe et al.
an application, on a per-processor basis: the number of active threads in a com-
putation phase and the voltage/frequency setting. For the latter, we use two
mechanisms: we use DVFS to control an application’s speed and power usage
based on observational data (predictive control), while we use RAPL (Runtime
Average Power Limit) to set hard power caps for each processor in case our pre-
dicted conﬁgurations would violate the power constraint (prescriptive control).
The algorithm used in Conductor can be split into four steps: initialization,
conﬁguration exploration, adaptive reconﬁguration, and power reallocation.
3.1
Initialization
During the ﬁrst timestep of the application, each MPI process is assigned an
equal amount of power derived from the job-level power constraint. This is
the timestep before Conductor starts the conﬁguration exploration step. The
process-level power constraint is enforced using RAPL. The execution time and
power usage of each application task (i.e., unit of computation between commu-
nication events) in the timestep is recorded and stored in a task graph. At the
end of the initialization step, the power constraint per process is (temporarily)
removed to facilitate the conﬁguration exploration step.
3.2
Conﬁguration Exploration
The next step taken by Conductor is to choose the conﬁguration, or combina-
tion of thread concurrency level and the DVFS state for each application task.
The choice of conﬁguration has a signiﬁcant impact on program execution time
(as much as an 30.9 % diﬀerence over our ﬁve applications). There are a number
of ways to choose the ideal thread concurrency level given a power bound. One
way is to proﬁle the code beforehand, which has the distinct disadvantage that it
requires at least one extra program execution. Another is to build oﬄine models
based on program executions, but the disadvantage is that the model could lack
accuracy and generality, especially in the case that a given program diﬀers from
the set of programs used to build the model.
In Conductor, we take a simpler approach: given n MPI processes executing
a given application, we use a small number of application iterations to per-
form a parallel exploration of the conﬁguration space by selecting a diﬀerent
thread/DVFS conﬁguration on each MPI process. There are k such conﬁgura-
tions that we consider, and given n processes, we simply test all of them and
choose the best-performing conﬁguration depending on the current process-level
power constraint. We retain the set of power-eﬃcient conﬁgurations for each task,
yielding a per-task power/time Pareto frontier. As mentioned above, we disable
the power bound during this step in our current prototype; this can be ﬁxed—at
the expense of more overhead—by more carefully executing the conﬁguration
exploration.
This clearly adds overhead while we are searching for eﬃcient conﬁgurations.
In general, it takes m = ⌈n/k⌉timesteps to ﬁnish testing all conﬁgurations.
Assume an example application with a single task per process in each iteration.

A Run-Time System for Power-Constrained HPC Applications
399
Suppose that during a timestep, the optimal conﬁguration of thread concurrency
level and DVFS state takes time topt, and the process with the slowest conﬁg-
uration on timestep i (during the search phase) takes time ti
worst. Then, the
execution time is T = m
i=1(ti
worst) + n
i=m+1(ti
opt), assuming that there are n
timesteps in total and m timesteps in the search phase.
Given that high-performance computing applications generally execute many
timesteps (n ≫m), the overhead in Conductor, compared to an oracle that could
choose the optimal thread/power conﬁguration a priori, will be generally small
because it is amortized over the lifetime of the computation. Because Conductor
potentially selects the optimal conﬁguration, this overhead can be expressed as
T
n
i=1(ti
opt).
3.3
Adaptive Reconﬁguration
The conﬁguration exploration phase makes the assumption that the optimal con-
ﬁguration does not change, which is not true in general. Further, for dynamic
applications with load imbalance, this can lead to wasted power during unneces-
sary wait operations (slack time). To handle both of these issues, we additionally
introduce a novel adaptive power-balancing algorithm that changes conﬁgura-
tions when appropriate due to application behavior. In addition to application
behavior, Conductor takes into account the current power constraint, processor
DVFS state and thread concurrency level.
Conductor Monitoring. After the thread/DVFS relationships are character-
ized for each task during the conﬁguration exploration phase, the per-process
power constraint is re-enforced using RAPL. Conductor monitors application
execution during each individual timestep and uses this information to predict
the behavior of following, similar tasks. During each timestep, Conductor records
the elapsed time and power usage for each task in a statically selected conﬁg-
uration. Conductor also measures the slack by observing time spent within the
MPI library, if any, for each task. This makes the assumption that the signiﬁcant
portion of the time in the MPI library is spent blocking waiting for messages,
which works as a useful predictor in the absence of the ability to directly measure
slack. This measurement step is borrowed from our previous run-time system,
Adagio [23]. It distinguishes tasks based on callstacks and uses a threshold to
diﬀerentiate slack time from MPI processing time.
Adjusting Task Execution Times. The previous step simply identiﬁes tasks
that contain slack. In the following timesteps, Conductor adjusts task execu-
tion times in such a way that overall execution time will decrease. Conductor
avoids adding to existing application inter-process communication where possi-
ble. Accordingly, Conductor handles each task completely locally via the follow-
ing method.
First, for any task that contains slack, Conductor can guarantee that it is
not on the critical path; by deﬁnition, any task that contains slack can be slowed

400
A. Marathe et al.
down by some nonzero amount without slowing down overall application execu-
tion. Consequently, Conductor attempts to ﬁll as much of the slack as possible
with computation time without aﬀecting the completion time of the task. For this
purpose, Conductor leverages both DVFS and thread concurrency levels to ﬁll
slack. In other words, Conductor will not allow any (non-critical) path through
any task with slack to become the critical path. Note that the reason that we
adjust DVFS and thread concurrency levels and not the RAPL bound itself is
(1) the task granularity is too small to use RAPL, and (2) RAPL does not adjust
thread concurrency.
Second, for any task that has no slack, Conductor conservatively changes
its conﬁguration to the one with next fastest thread/DVFS on the Pareto fron-
tier, which was determined (and saved) as part of the conﬁguration exploration
phase. The intuition here is that Conductor knows that such a task may be crit-
ical. Therefore, Conductor treats it as a task that should decrease its execution
time, because the critical path could potentially decrease. Note that this decision
is made locally because the overhead of determining the exact critical path is
prohibitive.
3.4
Reallocation of Per-Process Power
While the above step adjusts power consumption using DVFS and thread con-
currency selection, the overall power cap per process is as yet unchanged; this
Fig. 2. Opportunity for re-scheduling excess power in an MPI process that runs oﬀthe
critical path with Adagio. Plot (a) shows near-cap power consumption for computation
task (C) at the highest processor DVFS state (highest voltage and frequency). Plot (b)
shows lower power consumption for computation task (C) at a lower DVFS/thread
concurrency set by Adagio.

A Run-Time System for Power-Constrained HPC Applications
401
cap ensures that the power constraint is not violated. Consequently, even after
Conductor has adjusted conﬁgurations for individual tasks to ﬁll slack, critical
tasks may continue to run at or near their process’s power constraint, while
processes with no critical tasks do not use all of their power allocation. Such a
situation may be caused by load imbalance inherent to the application or diﬀer-
ences in power eﬃciency between individual processors [22]. Regardless of the
cause, Conductor takes advantage of the opportunity to speed up the application
by using a global algorithm to reallocate power between processes.
As an example, Fig. 2(a) shows the power consumption proﬁle of an MPI
process in an iterative MPI application with repeating computation and com-
munication phases. Since the tasks on this process are oﬀthe critical path, there
is slack in the communication phase following the computation phase. Figure 2(b)
shows that the computation tasks can be slowed down within the slack bound-
aries using DVFS and thread concurrency selection without aﬀecting the overall
execution time.
We deﬁne the power fraction for a process as the fraction of time between
power reallocations that an MPI process spends within a small tolerance of
its power constraint. Figure 2(a) shows the fraction of time the process spends
operating at power Phigh, which is essentially at Pconstraint. The process con-
sumes Plow power after Conductor has slowed down the computation opera-
tion as shown in Fig. 2(b). We deﬁne the power headroom for a processor as
the diﬀerence between the processor’s power constraint and processor’s average
power consumption. In Fig. 2(b), the power headroom is the diﬀerence between
Pconstraint and Paverage.
Conductor gathers power headroom information (which is computed based
on all tasks) from all processes after a conﬁgurable number of timesteps of the
application. Using process-level power headroom information, Conductor calcu-
lates job-level power headroom and reallocates process-level power constraints
based on the power fraction. While this technique has the potential disadvantage
that the critical path could, for a pathological situation, move through a process
that “donates” power to another process, this situation is rare. Moreover, to
address it would require power reallocation on task granularity, which is quite
complex.
4
Experimental Setup
We performed all experiments on Cab, a 1200-node Xeon E5-2670 cluster at
LLNL with an InﬁniBand QDR interconnect. Each cab node is composed of two
8-core processors and 32 GB of DRAM.
4.1
Benchmarks and Tools
The codes we use for comparison are CoMD, LULESH 2.0, SP and BT from
NAS-MZ, ParaDiS and a synthetic benchmark. These benchmarks were selected
because they exhibit performance and scaling behavior typical for a wide range

402
A. Marathe et al.
of HPC applications. We note that the most interesting behavior for these bench-
marks occurs between 30 and 60 W per processor.
CoMD [1] is a molecular dynamics benchmark. CoMD is unique among our
tested benchmarks in that all of its MPI communication is in the form of collec-
tives. As a result, the only tasks that remain for the power-balancing algorithm
are to minimize load imbalance by reallocating power between processes at every
collective call and to select eﬃcient conﬁgurations under processor-level power
constraints. We use the input problem size of 20 × 40 × 40 with 100 timesteps.
LULESH 2.0 [17] is a shock hydrodynamics benchmark. In terms of MPI
communication, LULESH diﬀers from CoMD in that it relies on a multitude
of point-to-point messages between collective calls. This behavior complicates
analysis of opportunities to balance power, but we show in Sect. 5.1 that Con-
ductor improves performance over state-of-the-art methods for running under
a job-level power constraint. We use an input problem size of 32, with 100
timesteps.
ParaDiS [5] is a production dislocation dynamics simulations application
that operates on dynamically changing, unbalanced data set sizes across MPI
processes. The random nature of data set sizes results in varying computational
load, introducing load-imbalance across MPI processes. We use the “Copper”
input set provided with ParaDiS with 600 timesteps.
NAS Multi-Zone [27] is an extension of the NAS Parallel Benchmark suite [3].
It involves solving the application benchmarks BT, LU and SP on collections of
loosely coupled discretization meshes. In our work, we use Block Tri-diagonal
(BT-MZ) and Scalar Penta-diagonal (SP-MZ) algorithms. Both applications use
OpenMP for intra-node computation and MPI for inter-node communication.
We use a custom class D input size with 500 timesteps.
To quantify how a load-imbalanced application can beneﬁt from power
reallocation in Conductor, we developed a synthetic benchmark. The synthetic
benchmark has two properties. First, it is written in such a way that the best
conﬁgurations under various power limits use six (out of eight) threads per
socket. Second, half of MPI processes execute nearly six times more compu-
tation load than the other half, which leads to process-level load imbalance.
This synthetic program focuses on the opportunity for Conductor to improve
performance through power re-allocation for process-level load imbalance.
4.2
Overheads
As we instrument every instance of any potentially blocking MPI call in order to
capture slack time and select conﬁgurations, our proﬁler incurs some overhead.
The median measurement overhead is 34 µs per MPI call and adds less than
0.05 % time to the tested applications. We use 60 conﬁgurations that consist of
thread concurrency levels of 5 to 8 threads per socket and 15 discrete DVFS
states. For SP-MZ, BT-MZ, CoMD and Lulesh, the conﬁguration exploration
phase took up to 3 timesteps. For ParaDiS we run the conﬁguration explo-
ration phase locally over each MPI process due to nonuniformity across MPI
processes in computation phases. For the conﬁguration exploration phase, we

A Run-Time System for Power-Constrained HPC Applications
403
observe an overhead of 1.96 s in the worst case (recall this is amortized over the
entire application execution). For the run-time power re-allocation algorithms,
all power allocation decisions are coordinated within existing application collec-
tive calls, with an average overhead of 566 µs per invocation. For the job sizes
tested, we consider this an acceptable trade-oﬀ. For larger jobs, a hierarchical
power-balancing strategy would be required.
5
Experimental Evaluation
In this section, we compare the performance of Conductor to two alternate poli-
cies: Static, and Conﬁg-Only. The Static algorithm, which is the current state of
the art, chooses the largest number of threads possible under the power bound
assuming that the frequency is the lowest possible. Then, it increases the fre-
quency as high as possible subject to the power bound. Conﬁg-Only executes
the conﬁguration exploration part of Conductor and performs one-time conﬁgu-
ration selection for each task, but does not execute the adaptive reconﬁguration
or power reallocation steps.
5.1
Load-Balanced Applications
Figure 3 shows execution times of our three load-balanced applications for each
of the three policies. This includes Lulesh on 27 (33) MPI processes and BT-MZ
0
10
20
30
40
50
60
70
30
40
50
60
70
80
Power Limit (W)
Execution Time
LULESH
Policy
Static
Config−Only
Conductor
0
5
10
15
20
25
30
30
40
50
60
70
80
Power Limit (W)
Execution Time
BT−MZ
Policy
Static
Config−Only
Conductor
0
20
40
60
80
100
120
140
30
40
50
60
70
80
Power Limit (W)
Execution Time
CoMD
Policy
Static
Config−Only
Conductor
Fig. 3. Comparison of power allocation policies for our three load-balanced applica-
tions: Lulesh, BT-MZ, and CoMD. We use 27, 32, and 32 processes, respectively.

404
A. Marathe et al.
and CoMD on 32 processes. The execution times are grouped by process-level
power limits on the x-axis ranging 30 W to 80 W in steps of 10 W; each process
is conﬁned to a singled processor. We make the following important general
observations. First, for lower power limits (30 W and 40 W per processor), Con-
ductor performs signiﬁcantly better than Static; the diﬀerence is as much as
30.4 % in the case of Lulesh at 27 tasks. The improvement in performance is
due to the power-eﬃcient conﬁgurations selected by Conductor under the MPI
process-level power constraint. Second, at higher power limits (60–80 W), the
diﬀerence between Static and Conductor remains constant, because the default
conﬁguration selected by Static and the optimal conﬁguration selected by Con-
ductor remain constant. Also, Conductor performs almost identically compared
to Conﬁg-Only for the three load-balanced applications. This is because there is
virtually no load imbalance. In general, load-balanced applications will execute
slightly faster when using Conﬁg-Only if there is opportunity to select a better
conﬁguration over the default conﬁguration chosen by Static. This is because
both Conﬁg-Only and Conductor choose the same conﬁguration, and Conductor
does not change the power allocation per MPI process (but does have a slight
overhead for monitoring).
5.2
Load-Imbalanced Applications
Figure 4 shows several runs of ParaDiS, SP-MZ, and our synthetic load
imbalanced application at 32 (for SP and the synthetic benchmark) and 64 (for
0
100
200
300
400
500
30
40
50
60
70
80
Power Limit (W)
Execution Time
ParaDiS
Policy
Static
Config−Only
Conductor
0
5
10
15
20
25
30
40
50
60
70
80
Power Limit (W)
Execution Time
SP−MZ
Policy
Static
Config−Only
Conductor
0
10
20
30
40
50
30
40
50
60
70
80
Power Limit (W)
Execution Time
Synthetic Benchmark
Policy
Config−Only
Conductor
Fig. 4. Comparison of power allocation policies in our three load-imbalanced appli-
cations: ParaDis, SP-MZ, and a synthetic microbenchmark. We use 64, 32 and 32
processes, respectively.

A Run-Time System for Power-Constrained HPC Applications
405
ParaDiS) processes under power limits of 30 W to 80 W. For load-imbalanced
applications, Conductor has each process record power and execution time for
all conﬁgurations due to potential nonuniformity in the characteristics of com-
putation tasks across MPI processes.
For lower power limits (30 W to 50 W), Conﬁg-Only beneﬁts from selecting
the best conﬁguration for individual computation tasks. However, the beneﬁts of
our power re-allocation policy in Conductor are more pronounced. The diﬀerence
in power usage across diﬀerent MPI processes is up to 15 % and 20 % for process-
level power limits of 30 W and 40 W respectively, indicating load-imbalance and
potential beneﬁt through power re-allocation.
Compared to Conﬁg-Only, Conductor achieves an improvement of up to
10.4 % (for ParaDiS) and 3.6 % (for SP-MZ) improvement at the power limit
of 30 W. For the same power limit, compared to Static, Conductor achieves an
improvement of up to 13.2 % (for ParaDiS) and 14.9 % (for SP-MZ). Compared
to Conﬁg-Only, Conductor achieves an improvement of 5.5 % (for ParaDiS) and
5.2 % (for SP-MZ) for a power limit of 40 W.
Compared to Static, Conductor achieves an improvement of 10.8 % (for Par-
aDiS) and 15.1 % (for SP-MZ). For a 50-W power limit with ParaDiS, Conductor
performs similar to Conﬁg-Only and marginally better than Static. However,
in case of SP-MZ, Conﬁg-Only performs marginally worse than Static due to
non-repeatability in performance at the thread concurrency/DVFS conﬁguration
selected at 50 W. At the same time, Conductor beneﬁts from load-imbalance and
performs better than Static.
At higher power limits (60 W to 80 W), Conductor generally performs slightly
worse than Conﬁg-Only because the computation tasks consume lower power
than the process-level power limit and do not beneﬁt from the power re-allocation
scheme. The performance impact of power re-allocation scheme is aﬀected by
how accurately it can shuﬄe power between MPI processes without changing
the critical path of the application. We observe that for ParaDiS for power
limits of 60 W and 70 W, the power re-allocation scheme alters the critical path
of the application before shifting it back during some time steps. As expected,
the higher execution times shown by Static are due to the choice of sub-optimal
conﬁguration of maximum thread concurrency and DVFS state (8 threads per
socket and 2.6 GHz on our test system).
For the synthetic program, we show only Conductor and Conﬁg-Only; We
leave out the execution times for Static, which are inferior to Conﬁg-Only due
to the way we program the synthetic benchmark. For a process-level power limit
of 30 W, the computation times in the load-imbalanced synthetic benchmark
are 181 ms and 122 ms for Conﬁg-Only and Conductor respectively (Conduc-
tor is 32 % faster). The corresponding change in frequency was from 1.2 GHz to
1.8 GHz. This improvement in computation time results in an overall execution
time improvement of 25 % with Conductor. For the power limit of 40 W, the cor-
responding execution times for computation tasks are 123 ms and 86 ms (30 %
faster with Conductor), and the corresponding change in frequency was from
1.9 GHz to 2.6 GHz. For higher power limits (60 W to 80 W), the diﬀerence

406
A. Marathe et al.
between the two policies diminishes as power is not a limiting factor on any
process, regardless of the load imbalance.
6
Related Work
The closest work to Conductor is our own on Adagio [23] and Jitter [16]. In fact,
Conductor can be thought of as fusing modiﬁed versions of Adagio and Jitter
together.
Adagio saves energy in HPC programs with a negligible increase in execution
time. Conductor diﬀers from Adagio in three important ways. First, Conductor
optimizes performance under a power bound, which is a completely diﬀerent
goal. Second, Conductor determines an eﬃcient thread/frequency conﬁguration,
while Adagio assumed single-threaded programs. Third, while Conductor and
Adagio both decrease frequency of tasks that block (and are therefore oﬀof
the critical path), Conductor (but not Adagio) simultaneously chooses a faster
thread/frequency conﬁguration of tasks that may be on the critical path.
Conductor diﬀers from Jitter in two ways. First, Conductor shifts power
to improve performance, while Jitter lowers frequency to save energy. Second,
Conductor measures power and makes some power decisions at the task level,
while Jitter solely operates at timestep granularity.
There is other work in optimizing performance under a power bound, espe-
cially on overprovisioned HPC clusters. This includes an empirical study on the
eﬀect of diﬀerent conﬁgurations [20] and choosing conﬁgurations via interpola-
tion [26]. In addition, Isci et al. optimized performance under a power bound on
a single multicore node [15], and Bailey et al. did the same for a CPU+GPU
node [4]. There has also been work on scheduling algorithms to improve perfor-
mance under a power bound [8,9,25]. Finally, there has been work on overprovi-
sioning for commercial applications in a datacenter, where the goal is increased
throughput [10], as well as in improving performance under power constraints
in virtualized clusters in datacenters [19].
Other related work is focused on saving power/energy under a time bound
in HPC. There has been work using linear programming to ﬁnd near-optimal
energy savings with zero time increase [24]. Other run-time approaches to save
energy include those that trade oﬀpower/energy saving for (hopefully mini-
mized) performance degradation [6,11,12,18].
7
Conclusion
Current run-time systems are leaving performance on the table and wasting
power, and these problems will only become more costly with future generations
of supercomputers. Conductor eﬀectively allocates power to the parts of the
application that primarily impact application performance. In our experiments,
we found that selecting the optimal conﬁguration and adaptively re-allocating
power to the critical path can result in up to a 30 % performance improvement

A Run-Time System for Power-Constrained HPC Applications
407
compared to the state-of-the-art algorithm for the same power constraint. In the-
ory, our system can adapt to the job-level power constraint, which may vary dur-
ing application execution time because of external factors, and adaptively select
application conﬁguration and power allocation. Our results also highlight that
incorporating OpenMP (or other conﬁgurable node-level parallelism) in addi-
tion to MPI goes a long way toward the goal of ﬂexible power and performance
management.
Acknowledgements. Part of this work was performed under the auspices of the U.S.
Department of Energy by Lawrence Livermore National Laboratory under contract
DE-AC52-07NA27344 (LLNL-CONF-667408).
References
1. CoMD (2013). https://github.com/exmatex/CoMD
2. Ashby, S., Beckman, P., Chen, J., Colella, P., Collins, B., Crawford, D., Dongarra,
J., Kothe, D., Lusk, R., Messina, P., Mezzacappa, T., Moin, P., Norman, M.,
Rosner, R., Sarkar, V., Siegel, A., Streitz, F., White, A., Wright, M.: The oppor-
tunities and challenges of exascale computing (2010)
3. Bailey, D., Barszcz, E., Barton, J., Browning, D., Carter, R., Dagum, L., Fatoohi,
R., Frederickson, P., Lasinski, T., Schreiber, R., et al.: The NAS parallel bench-
marks summary and preliminary results. In: Supercomputing, pp. 158–165 (1991)
4. Bailey, P.E., Lowenthal, D.K., Ravi, V., Rountree, B., Schulz, M., de Supinski,
B.R.: Adaptive conﬁguration selection for power-constrained heterogeneous sys-
tems. In: ICPP (2014)
5. Bulatov, V., Cai, W., Fier, J., Hiratani, M., Hommes, G., Pierce, T., Tang, M.,
Rhee, M., Yates, K., Arsenlis, T.: Scalable line dynamics in ParaDiS. In: Super-
computing (2004)
6. Cameron, K.W., Feng, X., Ge, R.: Performance-constrained distributed DVS
scheduling for scientiﬁc applications on power-aware clusters. In: Supercomput-
ing (2005)
7. Darema, F., George, D.A., Norton, V.A., Pﬁster, G.F.: A single-program-multiple-
data computational model for EPEX/FORTRAN. Parallel Comput. 7(1), 11–24
(1988)
8. Etinski, M., Corbalan, J., Labarta, J., Valero, M.: Optimizing job performance
under a given power constraint in HPC centers. In: IGCC (2010)
9. Etinski, M., Corbalan, J., Labarta, J., Valero, M.: Linear programming based par-
allel job scheduling for power constrained systems. In: HPCS (2011)
10. Femal, M.E., Freeh, V.W.: Safe overprovisioning: using power limits to increase
aggregate throughput. In: Falsaﬁ, B., VijayKumar, T.N. (eds.) PACS 2004. LNCS,
vol. 3471, pp. 150–164. Springer, Heidelberg (2005)
11. Ge, R., Feng, X., Feng, W., Cameron, K.W.: CPU Miser: a performance-directed,
run-time system for power-aware clusters. In: ICPP (2007)
12. Hsu, C.-H., Feng, W.-C.: A power-aware run-time system for high-performance
computing. In: Supercomputing, November 2005
13. InsideHPC.
Power
consumption
is
the
exascale
gorilla
in
the
room.
http://insidehpc.com/2010/12/10/power-consumption-is-the-exascale-gorilla-in-
the-room/

408
A. Marathe et al.
14. Intel. Intel-64 and IA-32 Architectures Software Developer’s Manual, Volumes 3A
and 3B: System Programming Guide, December 2011
15. Isci, C., Buyuktosunoglu, A., Cher, C., Bose, P., Martonosi, M.: An analysis of eﬃ-
cient multi-core global power management policies: maximizing performance for a
given power budget. In: IEEE/ACM International Symposium on Microarchitec-
ture, pp. 347–358 (2006)
16. Kappiah, N., Freeh, V.W., Lowenthal, D.K.: Just in time dynamic voltage scaling:
exploiting inter-node slack to save energy in MPI programs. In: Supercomputing,
November 2005
17. Karlin, I., Keasler, J., Neely, R.: Lulesh 2.0 updates and changes. Technical report
LLNL-TR-641973, August 2013
18. Li, D., de Supinski, B., Schulz, M., Cameron, K., Nikolopoulos, D.: Hybrid
MPI/OpenMP power-aware computing. In: IPDPS (2010)
19. Nathuji, R., Schwan, K., Somani, A., Joshi, Y.: VPM tokens: virtual machine-aware
power budgeting in datacenters. Cluster Comput. 12(2), 189–203 (2009)
20. Patki, T., Lowenthal, D.K., Rountree, B., Schulz, M., de Supinski, B.R.: Exploring
hardware overprovisioning in power-constrained, high performance computing. In:
ICS (2013)
21. Pawlowski, S.S.: Exascale science: the next frontier in high performance computing.
In: International Conference on Supercomputing, June 2010
22. Rountree, B., Ahn, D.H., de Supinski, B.R., Lowenthal, D.K., Schulz, M.: Beyond
DVFS: a ﬁrst look at performance under a hardware-enforced power bound. In:
HPPAC (2012)
23. Rountree, B., Lowenthal, D.K., de Supinski, B., Schulz, M., Freeh, V.W.: Adagio:
making DVS practical for complex HPC applications. In: ICS (2009)
24. Rountree, B., Lowenthal, D.K., Funk, S., Freeh, V.W., de Supinski, B., Schulz, M.:
Bounding energy consumption in large-scale MPI programs. In: Supercomputing,
November 2007
25. Sarood, O., Langer, A., Gupta, A., Kale, L.: Maximizing throughput of overprovi-
sioned HPC data centers under a strict power budget. In: Supercomputing (2014)
26. Sarood, O., Langer, A., Kal´e, L., Rountree, B., De Supinski, B.: Optimizing power
allocation to CPU and memory subsystems in overprovisioned HPC systems. In:
CLUSTER (2013)
27. van der Wijngaart, R.F., Haopiang, J.: NAS parallel multi-zone benchmarks (2003)

A Machine Learning Approach for a Scalable,
Energy-Eﬃcient Utility-Based Cache
Partitioning
Isa Ahmet Guney, Abdullah Yildiz, Ismail Ugur Bayindir,
Kemal Cagri Serdaroglu, Utku Bayik, and Gurhan Kucuk(B)
Department of Computer Engineering, Yeditepe University, Istanbul, Turkey
{iguney,ayildiz,ubayindir,kserdaroglu,
utkubayik,gkucuk}@cse.yeditepe.edu.tr
http://cse.yeditepe.edu.tr
Abstract. In multi- and many-core processors, a shared Last Level Cache
(LLC) is utilized to alleviate the performance problems resulting from
long latency memory instructions. However, an unmanaged LLC may
become quite useless when the running threads have conﬂicting inter-
ests. In one extreme, a thread can make beneﬁt from every portion of the
cache whereas, in the other end, another thread may just want to thrash
the whole LLC. Recently, a variety of way-partitioning mechanisms are
introduced to improve cache performance. Today, almost all of the studies
utilize the Utility-based Cache Partitioning (UCP) algorithm as their
allocation policy. However, the UCP look-ahead algorithm, although it
provides a better utility measure than its greedy counterpart, requires a
very complex hardware circuitry and dissipates a considerable amount of
energy at the end of each decision period. In this study, we propose an
oﬄine supervised machine learning algorithm that replaces the UCP look-
ahead circuitry with a circuitry requiring almost negligible hardware and
energy cost. Depending on the cache and processor conﬁguration, our thor-
ough analysis and simulation results show that the proposed mechanism
reduces up to 5 % of the overall transistor count and 5 % of the overall
processor energy without introducing any performance penalty.
Keywords: Last level cache · Way-partitioning · Utility-based cache
partitioning · Machine learning
1
Introduction
In a typical multi-core processor setup, a shared Last Level Cache (LLC) is
usually needed as a last resort for hiding the latency of the main memory.
In these processors, each running thread assumes that it has the full control
over the cache. Obviously, when the multi-core processor allows full LLC accesses
from each thread, the required control circuitry for the cache becomes very sim-
ple. However, this naive approach does not perform very well, when the only
major concern is the improved processor throughput.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 409–421, 2015.
DOI: 10.1007/978-3-319-20119-1 29

410
I.A. Guney et al.
The actual source of performance drop can be explained with the variability
of the run-time behavior of each running thread. First, a thread may go into
various program phases at diﬀerent time periods. For example, it can start with
a memory-intensive phase, which initializes all the data structures it would like
to use. Then, the thread may step into a computation-intensive phase, which
might not require any memory instruction. After that, the thread may jump
into a cache thrashing phase, which generates heavy memory traﬃc with no
address locality. This temporal change in behavior of each thread shows that
the LLC may be treated diﬀerently by each running thread at diﬀerent times.
Secondly, in a multi-core processor, there might be multiple threads, which run
simultaneously. The behavior of each thread is also aﬀected by the program
phases of the other running threads. For instance, when a thread jumps into
a memory-intensive phase and the other running threads go into computation-
intensive phases, the working set of the ﬁrst thread may ﬁt into the LLC. In such
a case, the thread may perform at its peak performance. However, in another
run, when the memory-intensive phases of the simultaneously running threads
overlap, the working set of the ﬁrst thread may no longer ﬁt into the LLC
resulting in a noticeable performance drop. This type of variability in thread
behavior on neighboring cores shows that the LLC may be treated diﬀerently by
each running thread with diﬀerent workloads.
To manage the LLC in a more-eﬃcient way by dedicating portions of it
to simultaneously running threads, various cache partitioning mechanisms are
recently proposed [1–6]. A partitioning mechanism consists of a sequence of steps:
(1) an allocation algorithm, (2) a victim selection policy, and, ﬁnally, (3) a size
enforcement mechanism. In the initial step, the allocation algorithm decides
how much of the cache is beneﬁcial to each thread for satisfying an overall target
metric. This target metric is usually chosen to be the processor throughput, such
as the Instructions Per Cycle (IPC). However, recently, other metrics, such as
fairness and power, are also becoming popular, as well. The well-known Utility-
based Cache Partitioning (UCP) algorithm completes this step by calculating a
utility metric for each running thread [1]. This utility metric can be calculated
by the help of extra cache structures, known as the Auxiliary Tag Directories
(ATD). In the end, the thread with the best utility score receives the largest
portion of the LLC.
The allocation algorithms of the UCP mechanism are heavily adopted by
other partitioning mechanisms such as PIPP, Vantage and Futility Scaling
[2,5,6]. These methods usually ignore the second step and mainly focus on the
third step of the LLC partitioning, the size enforcement mechanism. In the orig-
inal UCP study, the size enforcement mechanism is known to be quite strict.
When, the allocation algorithm decides that m ways of an N -way cache must be
given to a thread, those m ways are immediately dedicated to that thread and
no other thread can claim new cache lines from those cache ways, afterwards.
However, there is no clue given on how those m ways are selected among N cache
ways in the original UCP paper. The second step of the partitioning algorithm
should decide on the portion of the cache that is to be taken from a thread and
given to another one. This can be a very complicated task, especially when the

A Machine Learning Approach Utility-Based Cache Partitioning
411
number of receiver-threads and/or giver-threads is more than one. Besides, cer-
tain questions should be addressed by the victim selection policy, such as which
cache ways are to be abandoned when a thread is to loose cache partitions?
or which cache ways are to be allocated to achieve better performance when a
thread is to receive cache partitions?. In pseudo partitioning mechanisms, such
as PIPP and Vantage, a target size cannot be immediately allocated to a thread,
and the time that is required to reach to a target partition size may be consider-
ably long when the diﬀerence between the current partition size and the target
size is large. This slow-pace size adaptation schemes might have a detrimental
eﬀect on the overall cache performance, since the target size is almost always
missed in an environment in which the behaviors of the running threads are
quickly and continuously changing.
In this study, we propose a new allocation policy that is based on an oﬄine
supervised machine learning approach. Speciﬁcally, we take the highly complex
look-ahead allocation algorithm of the UCP, and turn it into a series of simpler
hardware functions. The resulting mechanism has four main advantages. First,
we show that the resulting circuitry is much less complex than the original
look-ahead hardware, and it requires a smaller number of transistors. Second,
our simulation results show that energy dissipation of the proposed mechanism is
much lower. Third, the time complexity of our algorithm is just O(1) whereas the
time complexity of the original algorithm is O(N2). Thus, we named our proposed
algorithm as the look-up mechanism, since the original look-ahead algorithm is
really turned into a series of simple look-up functions, which might be evaluated
at a single step. Finally, fourth, we show that the proposed look-up allocation
mechanism performs no worse than the original look-ahead mechanism, on the
average. Here, we must also state that, just like in the original UCP paper,
we only focus on a multicore processor with single-threaded cores. As a result,
hereafter, we use the terms thread and core, interchangeably.
The paper is organized as follows. In Sect. 2, we describe the current state
of the art on the UCP mechanism, and give the details of the look-ahead
algorithm. This section also gives an extra motivation for our proposed design.
The third section focuses on a machine learning-based technique, the look-up
algorithm, which tries faithfully mimicking the behavior of the original look-
ahead algorithm. In Sect. 4, we elaborate our experimental methodology, and
provide the results of our tests comparing our proposed mechanism with the
original algorithm in terms of processor performance, energy and complexity.
Finally, in Sect. 5, we conclude our study.
2
The Current State of the Art: Original UCP
Look-Ahead Algorithm
The original partitioning algorithm, which is shown in Algorithm 1, is known
as the UCP look-ahead allocation algorithm [1]. Its name implies that it looks
further in terms of cache partitions and is wiser than a simple greedy approach.
The input of the algorithm comes from the ATDs that enable the control logic to

412
I.A. Guney et al.
count the number of cache misses for each thread when the number of allocated
cache ways are changed. The utility metric of a thread is measured by the number
of cache ways, which the thread really makes any beneﬁt of. At each round, the
algorithm calculates the maximum marginal utility for each thread, and, then,
selects the thread with the maximum of individual maximum marginal utility
values. The marginal utility is the change in number of cache misses when the
number of ways allocated to a thread is increased. For instance, if a thread
currently owns m cache ways, and when we allocate n more cache ways if the
number of cache misses is reduced by r, the marginal utility for this instance
becomes simply r/n. To calculate the marginal utility on hardware, we only need
the cache hit counts for each way in the ATD structure of each thread.
Algorithm 1. The original look-ahead algorithm
1: allocations[i] ←1, for each thread i
2: balance ←(Number of cache ways - Number of threads)
3: while balance is not zero do
4:
for i in threads do
5:
maxmu ←-1
6:
blocksRequired[i] ←0, for each thread i
7:
for j=1 to balance do
8:
mu ←0
9:
for w=allocations[i] to allocations[i]+j do
10:
for s=0 to MAXDSS do
11:
mu ←mu + ATD[i][s][w]
12:
end for
13:
end for
14:
mu ←mu / j
15:
if mu <maxmu then
16:
maxmu ←mu
17:
blocksRequired[i] ←j
18:
end if
19:
end for
20:
end for
21:
winner is thread with maximum value of maxmu
22:
allocations[winner] ←allocations[winner] + blocksRequired[winner]
23:
balance ←balance - blocksRequired[winner]
24: end while
The worst case time complexity of the algorithm is O(N2), since there are two
nested loops within the algorithm: (1) The outer loop that continues until balance
becomes zero, and (2) The inner j loop that iterates through the remaining
balance number of cache ways. Here, we assume that the most inner s loop,
which calculates the sum of hit counts of each way, is totally avoided by storing
cumulative hit values in small vectors of N elements per ADT before running
the look-ahead algorithm, where N is the associativity of the cache.

A Machine Learning Approach Utility-Based Cache Partitioning
413
The hardware complexity of the look-ahead mechanism might be enormous
when the allocation decisions are instantly needed. However, there is always
enough time slack for this type of a process, assuming that this task is not in
the critical path of the processor. In this study, we assume that there are enough
32-bit adders and T 32-bit dividers to calculate a single iteration of the Look-
ahead algorithm for T threads. Considering the ATD structure described in the
original study, and assuming that the sum of hit counts for MAXDSS number
of sets are read from a precalculated table, the number of adders required for
a single iteration of the algorithm simply becomes T × (W × (W + 1)) / 2,
where T denotes the number of threads and W is the number of cache ways.
Fig. 1. The number of arithmetic operations required for each run of the original UCP
look-ahead algorithm
Finally, we also studied the number of addition and division operations
required for diﬀerent processor and cache conﬁgurations. As shown in Fig. 1,
these numbers are quite high even for a two-core processor with an eight-way
set-associative LLC conﬁguration. Note that, in the bar chart, we discarded some
conﬁgurations, which do not make any sense, such as a 16-core processor with a
16-way set-associative LLC. This ﬁgure further motivates our study, and it also
demonstrates that the look-ahead algorithm has an inherent complexity issues
originated from its nature. There are alternative implementation methods with
less number of addition and division operations, as well. For instance, we can
apply a memoization technique in hardware by keeping an SRAM structure,
which holds all the precalculated values of all hit counter combinations, instead
of recalculating them by reusing the adders and dividers. However, in such a case
the design complexity shifts to the SRAM structure. The power and latency cost
of calculating the result of an addition operation by the use of an adder circuitry
would be comparable, if not lower than, to the look up, retrieval and routing
cost of the same data from an SRAM structure. What we aim in this study is
to get rid oﬀthe complex hardware required by the look-ahead mechanism and
still be able to perform as well as the original algorithm.

414
I.A. Guney et al.
3
The Look-Up Algorithm: A Complexity-Eﬀective,
Power-Eﬃcient Look-Ahead Variant
Application of machine learning techniques to the area of computer architecture
research is not new. Choi et al. investigate learning-based simultaneous multi-
threading (SMT) resource distribution techniques [7]. In that notable work, the
authors ﬁrst study the limits of the SMT resource distribution by applying an ideal
oﬀ-line exhaustive search technique. Next, they propose a runtime learning mech-
anism that varies the resource share of the multiple threads towards the direction
that improves the overall processor throughput. In another study, Bitirgen et al.
propose an Artiﬁcial Neural Network (ANN) hardware that learns each thread’s
performance response to diﬀerent resource distributions on a multicore proces-
sor [8]. The authors claim that their approach makes it possible to anticipate the
system-level performance impact of resource allocation decision at runtime. In this
study, we carry out a similar approach. First, we apply an oﬀ-line learning app-
roach to extract coeﬃcients of our look-up algorithm. Then, at runtime, we plug
those coeﬃcients and periodically collected way-based hit counters into hardware
counters to make allocation decisions on LLC cache ways. As a result, for a spe-
ciﬁc cache conﬁguration, our algorithm requires a look-ahead UCP-based analysis
prior to its deployment. However, at runtime, we do not need further oﬄine analy-
ses. This analysis can be even done on a simulator. All that is needed is to set a
vector of precalculated theta values, which are obtained at the end of this oﬄine
phase. In this section, we would like to focus on the detailed description of our
oﬄine learning phase.
The gradient descent is an algorithm, which is used for minimizing functions
in the context of supervised machine learning. Given a function deﬁned by a set of
parameters, the gradient descent algorithm starts with an initial set of parameter
values and iteratively minimizes a deﬁned function. We have applied gradient
descent to minimize our squared error cost function J in our linear regression
method. Equation 1 shows how gradient descent updates each coeﬃcient Θi on
each iteration. In our setup, we set the total number of iterations to 10000. In
Eq. 1, α represents the learning rate, which controls the size of each step we take
downhill in the search space to reach a local minima.
Θi = Θi −α ∂
∂Θi
J(Θi)
(1)
In our linear regression model, we used the training data gathered from the
look-ahead algorithm. This data consist of cache hit counters obtained from the
ATD structures interpreted as explanatory variables and allocation decisions of
the look-ahead algorithm as dependent variables. In our oﬄine learning experi-
ments, we found that only ﬁrst four cache hit counters of an ADT is represen-
tative enough for training purposes, and we carried out our tests by integrating
those four hit counters to our oﬄine learning phase. At the end of the learning
phase, we extract T diﬀerent functions for T threads by applying T diﬀerent
regression models based on our training data. Note that, the proposed look-up

A Machine Learning Approach Utility-Based Cache Partitioning
415
algorithm, which is given in Algorithm 2, has a time complexity of O(1) since
the number of operations is ﬁxed, and that number does not change as the
associativity of the cache increases.
Algorithm 2. The proposed look-up algoritm
1: totalScore ←0
2: total ←0
3: input ←number of hits obtained from ADT
4: size ←the number of inputs - 1
5: theta ←set of coeﬃcients obtained from oﬄine learning
6: numWays ←number of cache ways
7: numThreads ←number of threads
8: for i in threads do
9:
score[i] ←input[0] * theta[i][0] + ... + input[size] * theta[i][size]
10:
if score[i] <0 then
11:
score[i] ←0
12:
else
13:
totalScore ←totalScore + score[i]
14:
end if
15: end for
16: if totalScore = 0 then
17:
for i in threads do
18:
allocations[i] ←numWays / numThreads
19:
end for
20: else
21:
for i = 0 to numThreads -2 do
22:
rate ←score[i] / totalScore
23:
allocations[i] ←1 + round(rate * (numWays - numThreads))
24:
total ←total + allocations[i]
25:
end for
26:
allocations[numThreads-1] ←numWays - total
27: end if
After careful observation of the results, we found that the number of ways
allocated to threads are not linearly correlated with the number of hits obtained
from the ATD structures, as we already expected. Actually, number of hits each
thread receives in their corresponding ATDs range over a wide scale, and it would
be very naive to expect deﬁnite answers from any type of learning algorithm in
such situations. However, we observed that the relation among cache hits from
diﬀerent ATDs is somewhat correlated with the resulting allocations. With these
ﬁndings, we designed a mechanism which distributes ways to threads based on
the weighted average of calculated scores. Speciﬁcally, our control mechanism
computes the score of each thread using the formula in Eq. 2 at runtime. Each
thread uses its own set of coeﬃcients (Θ) determined by our oﬄine learning
phase. Meanwhile, all threads share the same input set (x) provided by the set
of ATD structures.

416
I.A. Guney et al.
score = Θ0 +
n

i=1
Θixi
(2)
Due to the existence of negative values among coeﬃcients, in some cases
the score of a thread can become negative, which may cause our mechanism to
allocate a negative number of ways to a thread. To avoid such inconsistency
problems, the score of a thread is reset to zero when it becomes negative.
The functions obtained from the regression analysis do not always smoothly
map the number of ways that are actually allocated by the original look-ahead
algorithm. To ensure fairness among threads(with respect to their scores) and
all ways are allocated to threads, number of ways allocated to each thread is
computed using the weighted average of scores, as shown in Eq. 3. Our mecha-
nism guarantees that each thread receives at least one cache way and the last
thread receives the remaining cache ways (not shown in the equation).
waysj = 1 + Round(
scorej
total score ∗(num ways −num threads))
(3)
4
Tests and Results
We use MacSim [9] for evaluating our proposed mechanism. Details of the proces-
sor conﬁguration are kept similar to that of the original UCP study, and given in
Table 1. All parameters except number of cores and the associativity of the LLC
cache are kept constant. For creating workloads, we use random combinations
of 14 spec2006 benchmarks (astar, bzip2, hmmer, omnetpp, soplex, sjeng, mcf,
namd, libquantum, milc, href264, povray, gcc, and lbm) [10]. Fifty workloads
are simulated for the 2-core and the 4-core conﬁgurations, whereas twenty work-
loads are simulated for the 8-core conﬁguration. All workloads are simulated for
50 million cycles, starting from the SimPoint locations [11].
We start by elaborating the hardware complexity reduction that we can
achieve with our proposed approach. For the evaluations of our proposed app-
roach and the comparisons with the look-ahead algorithm, arithmetical circuits
for both algorithms are designed in Verilog HDL at RTL level and then the evalu-
ations are made on Synopsys Design Compiler by using Cadence 45-nm Generic
Process Design Kit (GPDK) without any explicit or particular optimization.
Table 1. Processor conﬁguration
Core
8-way, 256-entry ROB, out-of-order execution
1-thread per core
L1 Cache 32 KB I-Cache and 16 Kb D-Cache;
64 byte line size and 3 cycle access latency
L2 Cache 64 byte line size, 1024 sets
15 cycle access latency

A Machine Learning Approach Utility-Based Cache Partitioning
417
Table 2. Comparison between two methods with respect to transistor counts (t.c.)
and arithmetic operations per partitioning
t.c. for addition
t.c. for division
t.c. for multiplication
# of cores
# of ways
look-ahead
look-up
look-ahead
look-up
look-ahead
look-up
2
16
382.1 K
433 K
711.6 K
51 K
0
406.7 K
32
1.6 M
2.2 M
64
6.4 M
5.4 M
4
16
764.2 K
917 K
6.8 M
101.7 K
1.6 M
32
3.2 M
9.9 M
64
12.8 M
16.2 M
8
16
1.5 M
2 M
19 M
203.3 K
6.5 M
32
6.3 M
25.1 M
64
25.7 M
37.7 M
Furthermore, all the arithmetical circuits for both algorithms (look-ahead and
look-up) are designed with 32-bit I/O size and transistor counts are derived from
the equivalent NAND gate count of the circuits and the respective technology
library characteristics.
The Table 2 shows the overall breakdown in transistor counts of the two
rival approach for various machine conﬁgurations. For the 2-core conﬁguration
with a 16-way LLC, the look-up algorithm has 19 % less number of transistors
compared to the look-ahead algorithm. Here, we show that our algorithm is
scalable, since we do not utilize any extra hardware for the 32-way and 64-way
cache conﬁgurations. As a result, our transistor count savings become 77 % for
the 32-way and 92 % for the 64-way LLC conﬁgurations. When we move to the
4-core conﬁguration, the savings become much more clear. For the 16-way,
32-way and 64-way LLC conﬁgurations, the savings are 65 %, 80 % and
91 %,respectively. Finally, for the 8-core conﬁgurations, we achieve similar sav-
ings for the same three LLC conﬁgurations, 58 %, 72 % and 86 %, respectively.
In its extreme case, with an 8-core, 64-way LLC conﬁguration, the original
mechanism requires almost 64M transistors. If we consider a processor core that
consists of 1 billion transistors, that means the UCP lookahead circuitry covers
more than 6 % of the overall die area. These results show that the scalability of
the original mechanism is quite limited whereas our proposed mechanism always
requires less than 1 % of the overall die area in all conﬁgurations.
Next, we focus on the energy comparison of the two approaches. Figure 2
shows the energy dissipation of the look-ahead and the proposed look-up mech-
anisms compared to the energy dissipation of a Register File (RF). To calculate
the energy dissipation on the RF, ﬁrst we used the CACTI tool [12] to collect
single read and write energy of an SRAM structure in the size of a Register File.
Then, by the help of the processor simulator, we obtained average number of
read and write accesses to this structure, in the clock cycle granularity. Finally,
we multiplied energy and access numbers to calculate the average energy dis-
sipation of the RF structure in a single clock cycle. For calculating the energy

418
I.A. Guney et al.
dissipation of the look-ahead and the look-up algorithms, we collected energy
ﬁgures from the already published papers for a single CMOS adder, a multi-
plier and a division circuitry [13–15]. We scaled all energy numbers to 45 nm
process technology, and, then, we multiplied those numbers with the number of
operations required for each algorithm. Figure 2 presents energy results for two
diﬀerent UCP periods, 5M and 3M clock cycles. Here, we show that the look-
ahead algorithm is not scalable for large machine conﬁgurations with large LLC
associativities. Especially, when the UCP period is stretched to 3M cycles, the
look-ahead algorithm starts dissipating as much as the half of the RF energy, in
8-core and 64-way LLC conﬁguration.
Fig. 2. Energy dissipation of the look-ahead and look-up mechanism compared to the
energy dissipation of a single Register File in 5M- and 3M-cycle of UCP periods
In one of the milestone papers by D.Folegnani and A.Gonzalez, the percentage
of energy dissipation of a RF is evaluated to be approximately 12 % of the overall
energy dissipation [16]. These results shows that, in the extreme case, the original
mechanism dissipates as much as 5 % of the overall processor energy. On the con-
trary, the look-up algorithm dissipates at most 0.06 % of the overall RF energy,
across all conﬁgurations, and it is scalable to manycore architectures, as well.
Figure 3 shows the performance of the look-up algorithm in all 50 workloads
tested for the 2-core conﬁguration. The results are normalized to the performance
of the look-ahead algorithm, and workloads are sorted from the worst perform-
ing to the best performing for each legend separately. On the average, for the
8-way and 16-way LLC conﬁgurations, our look-up algorithm performs 0.41 %
better and 0.12 % worse than the look-ahead algorithm, respectively. Similarly,
Fig. 4 shows the performance of the look-up algorithm in all 50 workloads tested
for the 4-core conﬁguration. On the average, for the 16-way and 32-way LLC

A Machine Learning Approach Utility-Based Cache Partitioning
419
0%
10%
Workloads
Performance gain/drop percentage
8-way
16-way
Fig. 3. S-curve model of the look-up algorithm for the 2-core conﬁguration
conﬁgurations, our look-up algorithm performs 0.20 % and 0.09 % better than
the look-ahead algorithm, respectively. However, in the 64-way conﬁguration,
the look-up performs slightly worse (0.27 %), on the average across all simulated
workloads. Finally, Fig. 5 shows the performance results for the 8-core 32-way
and 64-way LLC conﬁgurations, respectively. Here, our proposed look-up algo-
rithm performs 0.20 % worse (for the 32-way LLC) and 1.07 % worse (for the
64-way LLC) than the look-ahead algorithm, on the average. 8-core results show
that when the scale of the multicore processor increases, the learning algorithm
that is based on a ﬁrst-order optimization algorithm starts to fail. However, com-
pared to the scale of energy and complexity reduction in such a large processor
conﬁguration, we believe that these performance results are still within accept-
able limits. These results show that, although, our look-up algorithm is not
perfect while tracking down every allocation of the original algorithm, similar
performance ﬁgures still can be achieved, on the average.
−5%
0%
5%
Workloads
Performance gain/drop percentage
16-way
32-way
64-way
Fig. 4. S-curve model of the look-up algorithm for the 4-core conﬁguration

420
I.A. Guney et al.
−2%
0%
2%
Workloads
Performance gain/drop percentage
32-way
64-way
Fig. 5. S-curve model of the look-up algorithm for the 8-core conﬁguration
5
Conclusion
The cache partitioning is an interesting research area for multicore processors,
especially when there is a last level cache structure shared by the running
cores. The utility-based cache partitioning (UCP) is one of the most popular
way-partitioning mechanisms, in the literature. It proposes a way allocation
algorithm, known as the look-ahead, to assign cache ways to running threads
according to their cache utilization. However, the look-ahead algorithm requires
a rather complex hardware especially when the number of cores and cache
ways are large. Additionally, its power constraints and large time complexity
aﬀect its scalability. In this study, we propose the look-up algorithm, which is
devised from an oﬄine supervised machine learning approach with time com-
plexity of O(1). Speciﬁcally, the proposed algorithm aims to mimic the behavior
of the UCP look-ahead algorithm by applying the runtime collected cache sta-
tistics on hardware functions that are generated by the oﬄine learning step. The
results show that our proposed mechanism is extremely scalable. The hardware
complexity of the look-up algorithm is in the range of 19 % to 92 % less than
the original algorithm for a various machine conﬁgurations. The transistor count
reduction might be up to 5 % when we consider a billion-transistor core. More-
over, compared to the original algorithm, our proposed algorithm saves up to
5 % of the overall processor energy for large machine conﬁgurations and high
associativity LLCs. Finally, we also show that, on the average, the performance
of the look-up algorithm is almost identical to the performance of the look-ahead
algorithm, in all simulated machine conﬁgurations and across all simulated work-
loads.
Acknowledgement. This work is supported by the Scientiﬁc and Technical Research
Council of Turkey (TUBITAK) for Wise-Cache Project under Grant No: 114E119.

A Machine Learning Approach Utility-Based Cache Partitioning
421
References
1. Qureshi, M.K., Patt, Y.N.: Utility-based cache partitioning: a low-overhead, high-
performance, runtime mechanism to partition shared caches. In: Proceedings of
the 39th Annual IEEE/ACM International Symposium on Microarchitecture, pp.
423–432. IEEE Computer Society, Washington, DC (2006)
2. Xie, Y., Loh, G.H.: PIPP: promotion/insertion pseudo-partitioning of multi-core
shared caches. In: SIGARCH Computer Architecture News, pp. 174–183. ACM,
New York (2009)
3. Qureshi, M.K., Jaleel, A., Patt, Y.N., Steely, S.C., Emer, J.: Adaptive insertion
policies for high performance caching. In: Proceedings of the 34th Annual Inter-
national Symposium on Computer Architecture, pp. 381–391. ACM, New York
(2007)
4. Jaleel, A., Hasenplaugh, W., Qureshi, M., Sebot, J., Steely, Jr., S., Emer, J.: Adap-
tive insertion policies for managing shared caches. In: Proceedings of the 17th
International Conference on Parallel Architectures and Compilation Techniques,
pp. 208–219. ACM, New York (2008)
5. Sanchez, D., Kozyrakis, C.: Vantage: scalable and eﬃcient ﬁne-grain cache parti-
tioning. In: SIGARCH Computer Architecture News, pp. 57–68. ACM, New York
(2011)
6. Wang, R., Chen, L.: Futility scaling: high-associativity cache partitioning. In: 47th
IEEE/ACM International Symposium on Microarchitecture (MICRO) (2014)
7. Choi, S., Yeung, D.: Learning-based SMT processor resource distribution via hill-
climbing. In: SIGARCH Computer Architecture News, pp. 239–251. ACM, New
York (2006)
8. Bitirgen, R., Ipek, E., Martinez, J.F.: Coordinated management of multiple inter-
acting resources in chip multiprocessors: a machine learning approach. In: Proceed-
ings of the 41st Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO 41), pp. 318–329. IEEE, Computer Society, Washington DC (2008)
9. Macsim simulator. http://code.google.eom/p/macsim/
10. Henning, J.: SPEC CPU2006 benchmark descriptions. ACM SIGARCH Comput.
Archit. News 34(4), 1–17 (2006)
11. Hamerly, G., Perelman, E., Lau, J., Calder, B.: SimPoint 3.0: faster and more
ﬂexible program phase analysis. J. Instr. Level Parallelism 7, 1–28 (2005)
12. Muralimanohar, N., Balasubramonian, R., Jouppi, N.: Optimizing NUCA organi-
zations and wiring alternatives for large caches with CACTI 6.0. In: Proceedings
of the 40th Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO 40), pp. 3–14. IEEE Computer Society, Washington, DC (2007)
13. Tran, A.T., Baas, B.M.: Design of an energy-eﬃcient 32-bit adder operating at sub-
threshold voltages in 45-nm CMOS. In: Third International Conference on Com-
munications and Electronics (ICCE), pp. 87–91 (2010)
14. Mehmood, N., Hansson, M., Alvandpour, A.: An energy-eﬃcient 32-bit multiplier
architecture in 90-nm CMOS. In: IEEE 24th Norchip Conference, pp. 35–38 (2006)
15. Pham, T.N., Swartzlander, E.E.: Design of Radix 4 SRT dividers for single precision
DSP in deep submicron CMOS technology. In: IEEE International Symposium on
Signal Processing and Information Technology, pp. 236–241 (2006)
16. Folegnani, D., Gonzalez, A.: Energy-eﬀective issue logic. In: IEEE International
Symposium on Computer Architecture, pp. 230–239 (2001)

A Case Study - Cost of Preemption
for Urgent Computing on SuperMUC
Siew Hoon Leong1,2(B) and Dieter Kranzlm¨uller1,2
1 Leibniz Supercomputing Centre, Garching Near Munich, Germany
siew-hoon.leong@lrz.de, h.leong@campus.lmu.de
2 Ludwig-Maximilians-Universit¨at M¨unchen, Munich, Germany
Abstract. Urgent computing requires computations to commence in
short order and complete within a stipulated deadline so as to sup-
port mitigation activities in preparation, response and recovery from
an event that requires immediate attention. As such, acquiring compu-
tation resources swiftly is crucial. Preemptive scheduling, terminating
an existing job(s) to make way for an urgent job, is one of the most
common approach considered. However, public resource providers are
typically faced with policy restrictions that forbid them from allowing
preemption. The interruption of existing jobs is believed to have a sig-
niﬁcant consequence, i.e. cost, to the users and resource providers. This
case study on a public HPC resource, SuperMUC, hosted at Leibniz
Supercomputing Centre aims to study the cost of preemption. Two cost
models, least cost and least disruptive, will be used. With this, we want
to demonstrate that the cost of preemption is in fact much lower in com-
parison to the loss mitigation that can be achieved by allowing an urgent
computation. The ultimate aim is to provide evidence to convince policy
makers on the feasibility and beneﬁts of supporting urgent computing
on public resources.
Keywords: Urgent computing · Cost · Preemption · HPC · SuperMUC
1
Introduction
Public high performance computing (HPC) infrastructure is an untapped source
of computation resources that urgent computing can leverage on. This is partic-
ularly the case in Germany where there are three national HPC centres, together
known as the Gauss Centre for Supercomputing (GCS). Each of these national
HPC centres oﬀers petaﬂops of peak performance that can compute most if not
all urgent computing use cases swiftly, i.e. within the stipulated deadline, and
potentially mitigate the loss that are typically in the range of millions to billions.
In spite of the many advantages of public HPC resources, they are not com-
monly being leveraged upon for urgent computing [7]. Since urgent events typically
occur unexpectedly and require computation resources in short order, the existing
mode of operation is unable to support it. The usual advance reservation strategy,
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 422–433, 2015.
DOI: 10.1007/978-3-319-20119-1 30

A Case Study - Cost of Preemption for Urgent Computing on SuperMUC
423
which requires knowing how much and when the resources are required in advance,
is unable to cater to the unexpectedness and in short order characteristics.
Preemptive scheduling, a common strategy in real-time computing, is
arguably the most eﬀective method to get resources on public HPC resources
swiftly when an urgent event occurs unexpectedly. However, this requires a
change in the mode of operation, i.e. policy change, since existing users will be
aﬀected. A cost study of preemption on an existing HPC resource, SuperMUC,
hosted at Leibniz Supercomputing Centre (LRZ), one of the three national HPC
centres in GCS, is thus conducted. The aim is to show that the cost of pre-
emption is in fact insigniﬁcant when compared to the loss of an urgent event.
Two cost approaches, the least cost (LC) and least disruptive (LD), are used to
demonstrate the cost of preemption. The LC approach is chosen to illustrate the
stand, i.e. minimum cost, of policy makers while the LD approach is selected to
reﬂect the resource provider’ concerns, i.e. disrupting minimum jobs and users.
This paper is organised as follows. In Sect. 2, related work in preemption is
shared. Technical information on SuperMUC and how it compares to similar on-
demand resources from Amazon are discussed in Sect. 3. The cost of preemption
is elaborated in Sect. 4. Section 5 illustrates the two cost approaches, LC and
LD. The result is shared in Sect. 6. Finally the conclusion and future work are
discussed in Sect. 7.
2
Related Work
Preemption is not a new concept. It is a technique that is utilised by the ker-
nels of most modern computers to support multitasking. Naturally, it is also
not something new in the HPC world. It is a common tool utilised by system
administrators to drain a machine for maintenances and/or to make way for an
advance reservation. Widely used batch schedulers, e.g. LoadLeveler and Slurm,
support preemption either implicitly or explicitly. LoadLeveler1 supports two
types of preemption, system-initiated and user-initiated via job classes, while
Slurm2 manages preemption via the job’s partition priority or its quality of ser-
vice (QoS). Generally, batch schedulers only provide accounting data in terms
of number of cores/nodes used and ﬁnal wallclock time of completed, both suc-
cessful and failed, jobs. Actual accounting information in terms of core-hours
are usually computed with additional scripts or programs with post-processing.
As such, the cost of preemption, which requires a snapshot of an instance in time
of a machine, cannot be directly or easily inferred.
SPRUCE (Special Priority and Urgent Computing Environment) science gate-
way project [2] was the ﬁrst known framework to successfully enable urgent com-
puting on public HPC resources within TeraGrid, predecessor project of Extreme
Science and Engineering Discovery Environment (XSEDE)3. Within SPRUCE,
1 http://www-01.ibm.com/support/knowledgecenter/SSFJTW 5.1.0/com.ibm.
cluster.loadl.v5r1.load100.doc/am2ug preemptgang.htm.
2 http://slurm.schedmd.com/preempt.html.
3 https://www.xsede.org/.

424
S.H. Leong and D. Kranzlm¨uller
there was only limited resources that supported preemption [14] and only on
specially reserved smaller segments of the resources. The less intrusive elevated-
priority, next-to-run, etc. strategies were more widely adopted. The criteria used
for preemption included elapsed time for the existing job, number of nodes and
jobs per user [9, p. 14]. By 2012, it was reported that there was no longer any HPC
resources in SPRUCE that supported preemption [3, p. 1680]. Cloud resources,
Computing as a Service, became the seemly more popular recommendation within
SPRUCE, particularly if preemption was required. However, the limited parallel
support (8 cores) within the provided cloud resource was insuﬃcient for the storm
surge researchers [3] and consequently a dedicated HPC machine was purchased
to continue their urgent computations.
Preemptive scheduling is already a well-known strategy in real-time system,
in particular hard real-time system, a computing paradigm with strong similari-
ties to urgent computing. Most work [1,11] revolves around task level preemption,
communication, etc., and are frequently focus on one (uni-) processor. Preemp-
tion cost is usually not a concern since it is a justiﬁed strategy when compared to
the cost that may be incurred if a hard deadline is not met. The main goal of this
paper is to show that preemption is also a justiﬁed strategy in urgent computing
on HPC resources for similar reasons as in real-time computing.
3
SuperMUC
SuperMUC4 is an IBM high performance computer hosted at LRZ in Germany
and is made up of four segments from two phases of updates. It utilises an array
of Intel processors, Westermere-EX, Sandy Bridge-EP, Ivy-Bridge and Phi, and
Haswell. The segment that will be used in this case study is the “thin node
islands” that are composed of Intel Sandy Bridge-EP Intel Xeon E5-2680 8 C
processors. This segment has a peak performance of 3.185 PFlop/s by utilis-
ing 9216 nodes, i.e. 147,456 cores, with 2 GByte of memory per core and has
LoadLeveler as a batch scheduler.
The cost of a core per hour on the thin node island on SuperMUC is esti-
mated (rounded up) to be 0.016 EUR, i.e. 0.256 EUR per node per hour. This
estimation is calculated using the average annual funding for the system, its
system software, direct system personnel, electricity, cooling system and manda-
tory independent commercial software, i.e. compilers, debuggers, etc. The total
core-hours SuperMUC can oﬀer per year is approximated from the usage sta-
tistic collected in 2014. This estimated cost does not include the building cost,
non-system support personnel, extra commercial software, e.g. MATLAB, etc.
3.1
Comparing SuperMUC to On-demand Resources
To illustrate the economical value of utilising a public resource such as
SuperMUC, this cost is compared in Table 15 to a similar on-demand resource,
4 http://www.lrz.de/services/compute/supermuc/systemdescription.
5 http://aws.amazon.com/ec2/pricing/.

A Case Study - Cost of Preemption for Urgent Computing on SuperMUC
425
Table 1. Cost of SuperMUC and AWS
Site
Type
Cores or Processor
Processor Memory Storage
Cost per
vCPU
type
frequency (GB)
(GB)
hour
(GHz)
LRZ
SuperMUC
thin island
16
Intel Xeon
E5-2680 8C
2.7
32
100 (NAS)
+ 1000
(GPFS)
0.256 EUR
AWS m3.2xlarge
8
Intel Xeon
E5-2670 v2
2.6
30
160 (SSD)
0.665 USD
(≈0.585
EUR)
AWS c4.4xlarge
16
Intel Xeon
E5-2680 v2
2.8
30
- (EBS)
1.032 USD
(≈0.909
EUR)
i.e. Sandy Bridge processors, oﬀers by Amazon Web Services (AWS)6, which is
hosted in the region EU (Frankfurt) in January 2015.
The information collected from AWS is known to be correct and up-to-date
on 23 January 2015. Since the cost oﬀered by AWS is based on USD, the cost is
converted to EUR using the exchange rate of 1 USD to 0.88 EUR. This is the
rounded live exchange rate provided by XE.com7 on 23 January 2015.
The per node-hour cost at LRZ is comparatively cheaper than that oﬀered by
AWS since LRZ is a public resource provider and is thus not expecting a proﬁt.
However AWS resources have the advantage of being available on demand and
thus swiftly without any additional and in particular manual interference from
AWS. It also has the ﬂexibility and ease of setting up and conﬁguring the resource
as a user/use case requires.
Table 2. SuperMUC and AWS EC2 C3 cluster
Site
Type
Nominal
Peak
Cores
Memory
frequency performance
LRZ
SuperMUC thin island (Intel
Xeon E5-2680 8C)
2.7 GHz
3.185 PFlop/s
147,156
288 TB
AWS Amazon EC2 C3 Instance
Cluster (Intel Xeon
E5-2680v2)
2.8 GHz
593.51 TFlop/s
26,496
0.105984 TB
Table 2 compares SuperMUC and the HPC machine AWS made in November
2014 for the Top500 list8. In spite of the many advantages of on-demand resources,
when HPC resources are required, public HPC resources like SuperMUC are sim-
ply computationally more powerful, i.e. signiﬁcantly bigger number of cores, faster
network, bigger and faster storage. As such, public HPC resources are a highly
valuable class of resources for urgent computing.
6 http://aws.amazon.com/ec2/instance-types.
7 http://www.xe.com/.
8 http://top500.org/system/178321.

426
S.H. Leong and D. Kranzlm¨uller
4
Cost of Preemption
In urgent computing, the cost of an urgent computation (Cc) should be less than
the cost incurred from an urgent event (Ce) for a computation to be considered
as worthwhile. However, this is only true in the ideal situation where the actual
cost of the event is zero as a result of a perfect mitigation decision. A more
appropriate representation of the computation and event cost is shown in (1) [8]
where α is to be derived e.g. from previous events of similar nature.
Cc ≤(1 −α)Ce
(0 ≤α ≤1)
(1)
4.1
Public Resource
Public resources are an important class of resources that urgent computing can
utilise. The cost of urgent computation on such resources can be represented as
follows:
Cc = Cp + Cu + Cqos
(2)
Cp, Cu and Cqos refer to the cost of preemption, urgent job and loss in quality
of service (QoS) respectively. For a particular urgent event, the cost of an urgent
job (Cu) on a speciﬁc HPC resource is ﬁxed. Cu is illustrated in (3) where n is
the number of cores/nodes used, t is the wallclock time used (typically round
up to next hour) and Cn is the computation cost of each core/node.
Cu = n · t · Cn
(3)
The cost of preemption and loss of QoS are dynamic and are dependent on
the running state of the targeted resource. Cp can be represented as shown in (4)
and is strongly dependent on the jobs that will be preempted. Thus the decision
on which jobs to preempt can strongly inﬂuence Cp. ni and ti refer to the number
of cores/nodes used and the current wallclock time used by job i respectively.
Cp is thus a sum of the cost of these preempted jobs. Cp can be simply seen as
the direct cost of preemption.
Cp = (

ni · ti)Cn
(4)
Cqos is the indirect cost of preemption and is a more diﬃcult cost to measure
and quantify. This is particularly so for public resources like SuperMUC where
computation resources are given to scientist for free based on their scientiﬁc
relevance. Generally, if more jobs and thus users are disrupted, the higher the
perceived incurred indirect cost. Naturally other factors, e.g. time left until job
is completed, possibility to resume job instead of a complete restart, number of
jobs per user being preempted, frequency of preemption, when the preempted
jobs are restarted, will also inﬂuence the perceived loss of quality of service.

A Case Study - Cost of Preemption for Urgent Computing on SuperMUC
427
5
Approaches
In order to study the cost of preemption, two approaches, the LC and the LD,
were used. The LC approach aims to minimise the direct cost of preemption, Cp.
The LD approach’s goal is to minimise the number of aﬀected jobs (or users), i.e.
Cqos, while keeping the aﬀected number of preempted cores and/or cost as low as
possible. These two approaches allow more ﬂexibility over which jobs should be
preempted and thus more insight into the cost of preemption when compared to
the preemption support within LoadLeveler. Both approaches were realised with
Apache Spark9.
5.1
Least Cost (LC) Approach
The LC approach aims to minimise the direct cost of preempting running jobs
to make way for the urgent job(s). To reduce the direct cost, the “cheapest” jobs
are preempted. The “cheapness” of a job is dependent on the elapsed wallclock
time and the number of nodes used and can also be illustrated by Eq. (3).
The least cost algorithm applied is as follows:
Cp(no) = min(CJ(no), LEJ(no))
(5)
where
– no is the number of nodes that have to be preempted
– Cp(no) is the direct cost of preempting no or more nodes
– CJ(no) is the sum of the cost of preempting the cheapest running jobs to free
no or more nodes
– LEJ(no) is the sum of the cost of preempting the least wallclock elapsed jobs
to free no or more nodes
– min(.., ..) selects the minimum preemption cost
The LC approach will minimise the cost of preemption by preempting the
cheapest (Fig. 1a) or least elapsed wallclock time (Fig. 1b) jobs, thus favour-
ing the preemption of smaller or newly submitted jobs respectively. If more
nodes are removed than required, the list will be ﬁltered using with Knapsack
algorithm [12] where the most expensive redundant job combination will be
removed from the preemption list. If more nodes are still preempted in LEJ, the
Knapsack algorithm (minimisation) is additionally used to ﬁnd possible job/job
combination with a lower cost within the wallclock range that is smaller and
equal to the last job selected for preemption.
5.2
Least Disruptive (LD) Approach
The LD approach as shown in Fig. 2 aims to minimise the number of jobs pre-
empted, thus the number of users aﬀected. A job that utilises a bigger number
of nodes than no will be preempted as compared to e.g. preempting two smaller
cheaper jobs to free no nodes. If no jobs using ≥no can be found, the least
number of jobs to get no or more nodes are selected.
9 https://spark.apache.org/.

428
S.H. Leong and D. Kranzlm¨uller
Fig. 1. Least cost algorithm
Fig. 2. Least disruptive algorithm

A Case Study - Cost of Preemption for Urgent Computing on SuperMUC
429
6
Result
The LC and LD algorithms were applied on SuperMUC, while it was on full
load, in late December 2014 and early January 2015 at random times to reﬂect
the unexpectedness of urgent events. The cost of preemption, Cp, is collected
for 16, 32, 64, 128, 256, 512, 1024, 2048 compute nodes. Only nodes that
are in the same job class as the required compute nodes can be considered for
the preemption. Samples were taken within minutes, hours and days, to capture
the dynamic nature, i.e. a constant stream of new jobs, in the queue for smaller
jobs and the more invariant behaviour in the queue for larger jobs. Since there
is no QoS penalty at this moment for preempting jobs on SuperMUC, Cqos is
assumed to be zero for simplicity. In principle, Cqos can be inferred from the
number of trouble tickets, i.e. complaints, received as a result of the preemptive
activities.
Figure 3 shows the preemption cost, number of jobs and nodes preempted
when 2048 nodes are required for urgent computing over the case study period.
The preemption cost can vary signiﬁcantly and is strongly dependent on when
the preemption should take place, i.e. the status of the running jobs on the
machine and the proﬁle of jobs running on the machine. Typically the same
set of jobs is running for 48 h and thus the cost will only increase during this
period of time independent of the approach. Comparing the LC approach to the
LD approach, the preemption cost is frequently only half or less. As a result,
the number of aﬀected jobs is more than double, implying more aﬀected users.
The LC approach also resulted in more nodes to be preempted than required
since cheaper jobs are selected as opposed to jobs with the “right size”.
The average preemption cost, number of jobs and nodes preempted for diﬀer-
ent number of compute nodes are summarised in Fig. 4, where results is collected
at the same timestamps as shown in Fig. 3. Similar conclusions can be drawn
from the average numbers. In general, the cost of preemption and the number
of jobs preempted are inversely related. The LD approach adopts a more con-
servation strategy, i.e. minimise the number of jobs aﬀected, to preemption at
the expense of cost.
Table 3 shows the estimated cost of four severe but diﬀerent disasters that
occurred in the last decade. Typically, it will take a number of years before the
full extent of the cost can be collected. These disasters cost in the range of ten
to hundred of billions of EUR. In comparison, the cost of preemption, which is
in the range of ten of thousands of EUR appears like a pittance. Naturally, the
cost of an urgent computation includes more than just cost of the preemption
as shown in Eq. 2. Assuming 2048 nodes are required for a period of 48 h, the
maximum allowed walltime limit without additional arrangement, for an urgent
computation, the computation cost, Cu, can be computed using Eq. 3 and is
approximately 25165 EUR. Using the worst case preemption cost, 23599 EUR,
as shown in Fig. 3, the total computation cost, Cp, is expected to be around 48764
EUR. This is still insigniﬁcant when compared to a costly disaster. Finally, based
on Eq. 1, urgent computation will be worthwhile if the result is expected to be
useful for mitigation activities that can reduce the incurred cost by α.

430
S.H. Leong and D. Kranzlm¨uller
0
5000
10000
15000
20000
25000
28-DEC-14
13:43:22
28-DEC-14
14:03:11
28-DEC-14
14:53:21
28-DEC-14
18:45:29
29-DEC-14
04:34:03
03-JAN-15
04:49:36
5041.92
5218.56
5681.15
9146.11
3669.5
568.83
13543.42
13543.42
14159.87
16625.66
23599.1
11067.39
Preemption Cost, Cp(2048) in EUR
Least Disruptive
Least Cost
(a) Preemption Cost
0
5
10
15
20
25
30
35
28-DEC-14
13:43:22
28-DEC-14
14:03:11
28-DEC-14
14:53:21
28-DEC-14
18:45:29
29-DEC-14
04:34:03
03-JAN-15
04:49:36
31
31
31
34
26
9
3
3
3
4
3
4
No. of preempted Jobs
(b) Preempted Jobs
0
500
1000
1500
2000
2500
28-DEC-14
13:43:22
28-DEC-14
14:03:11
28-DEC-14
14:53:21
28-DEC-14
18:45:29
29-DEC-14
04:34:03
03-JAN-15
04:49:36
2059
2059
2059
2068
2053
2072
2408
2408
2408
2408
2504
2568
No. of preempted Nodes
(c) Preempted Nodes
Fig. 3. Preemption cost, preempted jobs and nodes for 2048 nodes

A Case Study - Cost of Preemption for Urgent Computing on SuperMUC
431
0
2000
4000
6000
8000 10000 12000 14000 16000 18000
16
32
64
128
256
512
1024
2048
6.4
10.41
37.89
71.77
154.45
390.06
1438.93
4887.68
27.99
19.12
62.81
660.82
480.6
3538.94
8066.39
15423.15
Preemption Cost (Cp(n0)) in EUR
Nodes (n0)
Least Disruptive
Least Cost
(a) Average Preemption Cost
0
5
10
15
20
25
30
16
32
64
128
256
512
1024
2048
6.7
3.3
1
1.8
5.3
7.7
14.7
27
1
1
1
1
1
1
1.2
3.3
No. of preempted Jobs
Nodes (n0)
(b) Average Preempted Jobs
0
500
1000
1500
2000
2500
16
32
64
128
256
512
1024
2048
16
32
73.3
149
259.2
528.3
1051.3
2061.7
16
32
64
133.3
256
512
1196
2450.7
No. of preempted Nodes
Nodes (n0)
(c) Average Preempted Nodes
Fig. 4. Average preemption cost, preempted jobs and nodes

432
S.H. Leong and D. Kranzlm¨uller
Table 3. Cost of some disasters in recent years
Disasters
Year Country
Estimated loss (EUR)
Flood [6]
2013 Germany
15 billion
T¯ohoku Earthquake/Tsunami [10,13] 2011 Japan
273 billion
Deepwater Horizon Oil Spill [4]
2010 Gulf of Mexico 53–88 billion
Hurricane Katrina [5]
2005 United States
88 billion
7
Conclusion and Future Work
Public resources, in particular HPC resources, can potentially be leveraged on
to solve most if not all urgent computing use cases. Unfortunately, the exist-
ing policies are too restrictive to support this class of computing. Preemption,
a common strategy, in urgent computing is not supported due to policy restric-
tions. The impact of preemption on other users is believed to be signiﬁcant, i.e.
costly. A case study was thus carried out on one of the public HPC resources,
SuperMUC, to investigate the cost.
To simulate the unpredictability of urgent events, the case study was car-
ried out at random times while SuperMUC was on full load. Eight diﬀerent
numbers of compute nodes were considered for preemption. Their corresponding
preemption cost, number of compute nodes preempted and number of jobs pre-
empted were collected. Two preemption approach, LC and LD, were used. The
LC approach aims to minimise the preemption cost while the LD approach’s
goal is to minimise the number of preempted jobs. These approaches attempt
to illustrate to the perspectives of the policy makers and the resource providers.
A production version of the preemption algorithm will have to merge the LC
and LD approaches and include also the perspectives of other stack-holders, i.e.
the urgent and non-urgent users, where other criteria, e.g. nodes proximity and
time until completion, has to be considered.
The computation cost for 2048 compute nodes, the maximum number of
nodes that can be used without any special arrangement, is then compared
to the cost of some of the deadliest disasters in recent decade. The cost of
an urgent computation is in the range of ten of thousands of EUR while the
cost of the deadliest disasters are in the range of ten to hundreds of billions of
EUR. The cost of computation with preemption is insigniﬁcant in comparison.
Naturally, not all cost of a disaster can be mitigated with urgent computations.
In the case of disasters that cost billions of EUR, if an urgent computation can
mitigate 0.001 % of the incurred cost, it is more than worthwhile to support
urgent computing and preemption. An urgent computation(s) is thus justiﬁed if
its cost including preemption is less than or equal to the expected reduction in
severity of the event from mitigation activities planned using the results of the
urgent computation(s).
More work is still required in the ﬁeld of urgent computing, in particular on
HPC infrastructures. Future work includes improving the preemption algorithms
for production purposes, estimating/predicting the computation time of an urgent

A Case Study - Cost of Preemption for Urgent Computing on SuperMUC
433
computation code for eﬃcient scheduling and how it corresponds to the stipulated
deadline. The ultimate goal is an urgent computing framework that can easily
enable urgent computing use cases on generic resources.
Acknowledgments. The author would like to thank Anthes, Heller, DRG group,
etc. at Leibniz Supercomputing Centre for their valuable motivations, suggestions,
feedbacks and support. A big thanks to Brehm for sharing the cost information on
SuperMUC.
References
1. Baruah, S., Mok, A., Rosier, L.: Preemptively scheduling hard-real-time sporadic
tasks on one processor. In: Proceedings of the 11th RTSS, pp. 182–190, December
1990
2. Beckman, P., Nadella, S., Trebon, N., Beschastnikh, I.: SPRUCE: a system for
supporting urgent high-performance computing. In: Gaﬀney, P.W., Pool, J.C.T.
(eds.) Grid-Based Problem Solving Environments. IFIP, vol. 239, pp. 295–311.
Springer, Boston (2007)
3. Blanton, B., McGee, J., et al.: Urgent computing of storm surge for North
Carolina’s coast. In: ICCS. Procedia Computer Science, vol. 9, pp. 1677–1686.
Elsevier (2012)
4. Cohen, M.: A taxonomy of oil spill costs - what are the likely costs of the deepwater
horizon spill? Technical report, Resources for the Future (2010)
5. Dolfman, M., Wasser, S., Bergman, B.: The eﬀects of Hurricane Katrina on the
New Orleans economy. Technical report, Bureau of Labor Statistics of the U.S.
Department of Labor (2007)
6. Gennies, S., Funk, A., et al.: Hochwasser-Bilanz 2013 Wie schlimm war die Flut
wirklich? June 2013. http://www.tagesspiegel.de/politik/hochwasser-bilanz-2013-
wie-schlimm-war-die-ﬂut-wirklich/8416770.html
7. Leong, S.H., Frank, A., Kranzlm¨uller, D.: Leveraging e-infrastructures for urgent
computing. In: ICCS. Procedia Computer Science, vol. 18, pp. 2177–2186. Elsevier
(2013)
8. Leong, S.H., Kranzlm¨uller, D.: Towards a general deﬁnition of urgent computing.
To be published in Proceedings of ICCS 2015
9. Marru, S., Gannon, D., et al.: LEAD cyberinfrastructure to track real-time storms
using SPRUCE urgent computing. CTWatch Q. 4(1), 5–16 (2008)
10. Nanto, D., Cooper, W., Donnelly, J.: Japans 2011 earthquake and tsunami- eco-
nomic eﬀects and implications for the United States. Technical report, Congres-
sional Research Service(2011)
11. Scaife, N., Caspi, P.: Integrating model-based design and preemptive scheduling in
mixed time- and event-triggered systems. In: Proceedings of the 16th Real-Time
Systems, ECRTS, pp. 119–126, June 2004
12. Trick, M.A.: A dynamic programming approach for consistency and propagation
for knapsack constraints. In: Annals of Operations Research, pp. 113–124 (2001)
13. Ujikane, K.: Japan Forecasts Earthquake Damage May Swell to $309 Billion, March
2011. http://www.bloomberg.com/news/articles/2011-03-23/japan-sees-quake-da
mage-up-to-309-billion-almost-four-katrinas
14. Yoshimoto, K.K., Choi, D.J., et al.: Implementations of urgent computing on pro-
duction HPC systems. In: ICCS. Procedia Computer Science, vol. 9, pp. 1687–1693.
Elsevier (2012)

Designing Non-blocking Personalized Collectives
with Near Perfect Overlap for RDMA-Enabled
Clusters
Hari Subramoni1(B), Ammar Ahmad Awan1, Khaled Hamidouche1,
Dmitry Pekurovsky2, Akshay Venkatesh1, Sourav Chakraborty1,
Karen Tomko3, and Dhabaleswar K. Panda1
1 Department of Computer Science and Engineering,
The Ohio State University, Columbus, OH, USA
{subramoni.1,awan.10,hamidouche.2,venkatesh.19,
chakraborty.52,panda.2}@osu.edu
2 San Diego Supercomputer Center, San Diego, California
dmitry@sdsc.edu
3 Ohio Supercomputer Center, Columbus, OH, USA
ktomko@osc.edu
Abstract. Several techniques have been proposed in the past for design-
ing non-blocking collective operations on high-performance clusters.
While some of them required a dedicated process/thread or periodic prob-
ing to progress the collective others needed specialized hardware solutions.
The former technique, while applicable to any generic HPC cluster, had
the drawback of stealing CPU cycles away from the compute task. The lat-
ter gave near perfect overlap but increased the total cost of the HPC instal-
lation due to need for specialized hardware and also had other drawbacks
that limited its applicability. On the other hand, the Remote Direct Mem-
ory Access technology and high performance networks have been push-
ing the envelope of HPC performance to multi-petaﬂop levels. However,
no scholarly work exists that explores the impact such RDMA technol-
ogy can bring to the design of non-blocking collective primitives. In this
paper, we take up this challenge and propose eﬃcient designs of personal-
ized non-blocking collective operations on top of the basic RDMA prim-
itives. Our experimental evaluation shows that our proposed designs are
able to deliver near perfect overlap of computation and communication
for personalized collective operations on modern HPC systems at scale. At
the microbenchmark level, the proposed RDMA-Aware collectives deliver
improvements in latency of up to 89 times for MPI Igatherv, 3.71 times
for MPI Ialltoall and, 3.23 times for MPI Iscatter over the state-of-the-art
designs. We also observe an improvement of up to 19 % for the P3DFFT
kernel at 8,192 cores on the Stampede supercomputing system at TACC.
Keywords: Non-blocking collectives · Remote Direct Memory Access ·
HPC · InﬁniBand
This research is supported in part by National Science Foundation grants #CCF-
1213084, #CNS-1419123, and #IIS-1447804.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 434–453, 2015.
DOI: 10.1007/978-3-319-20119-1 31

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
435
1
Introduction and Motivation
Supercomputing systems have grown in size and scale over the last decade. Two
key drivers fueling the growth of supercomputers are the current trends in multi-
/many-core architectures and the availability of commodity, RDMA-enabled,
and high-performance interconnects such as InﬁniBand [9] (IB). Such HPC sys-
tems are allowing scientists and engineers to tackle grand challenges in various
scientiﬁc domains. Users of HPC systems rely on parallel programming models
to parallelize their applications and obtain performance improvements.
Message Passing Interface (MPI) [26] is a very popular parallel program-
ming model for developing parallel scientiﬁc applications. The MPI Standard [27]
oﬀers primitives for various point-to-point, collective, and synchronization oper-
ations. Collective operations deﬁned in the MPI standard oﬀer a very convenient
abstraction to implement group communication operations. Owing to their ease
of use and performance portability, collective operations are widely used across
various scientiﬁc domains. Collective operations can broadly be classiﬁed as per-
sonalized and non-personalized depending on the kind of data transmitted by
one process to its peers. A personalized colelctive operations is one where each
process sends distinct data to many other processes. Consequently, personalized
collective operations transfer more volume of data and hence put a heavier load
on the communication subsystem when comapred to the non-personalized ones.
Until recently, the MPI Standard deﬁned all collective operations to be block-
ing, i.e., the application processes had to wait in the MPI library, until their
role in the collective operation is complete. This aﬀects the overall performance
and scalability of parallel applications. To address these limitations, the MPI
community has introduced Non-Blocking Collectives (NBC) in the current MPI
Standard, MPI-3. A high performance implementation of a non-blocking inter-
face for collective operations would ideally be expected to deliver near-perfect
communication/computation overlap, together with acceptable communication
latency. Furthermore, applications also need to be re-designed to take advantage
of this feature and to overlap expensive collective communication operations
with independent calculations.
Although the concept of NBC seems simple and the beneﬁts obvious, there
are several caveats that need to be addressed before end applications begin to
see the beneﬁts oﬀered by this novel programming interface. Given the dominant
nature of the MPI programming model in the scientiﬁc computation domain, it
is likely that the key driver for the acceptance of this interface by the applica-
tion community will be the real beneﬁts oﬀered by intelligent MPI designs and
implementations of NBC.
Simplistic designs of NBC that require applications to progress the commu-
nication explicitly through CPU intervention, e.g., calling MPI Test [6], oﬀset
much of the beneﬁt of non-blocking communication by stealing CPU cycles away
from the computation. Further, estimating the optimal number of MPI Test
/ MPI Probe / MPI Iprobe calls required is hard for application developers.
Similarly, if each MPI process launches a separate thread within the library
for communication progress, the application performance can be aﬀected by

436
H. Subramoni et al.
interrupt processing, thread scheduling and other similar factors [3]. While
functional partitioning approaches [5,14,29,35] that dedicate one compute core
for communication alleviate this to some extent, they still take one com-
pute core away from the application resulting in potential sub-par computa-
tion performance. It is also critical for a non-blocking collective interface to
ensure performance portability. In this context, network vendors are oﬀering
advanced hardware features to enable asynchronous communication progress
for collective operations. Mellanox has introduced network oﬄoad features in
their ConnectX-2 [23], ConnectX-3 [24] and Connect-IB [25] adapters. Using
this feature, generic lists of communication tasks can be oﬄoaded to the net-
work interface [21]. Such an interface eliminates the need for the host processor
to explicitly progress the communication. It also provides a low-level mechanism
that can be leveraged to design non-blocking collective communication algo-
rithms. However, as discussed in [12,13], there are several performance and scal-
ability limitations in the current generation hardware that may limit the adop-
tion of these features on mainstream supercomputing systems. Similar hardware-
based oﬄoad collectives are also possible with the Portals interface [32].
As described above and in [7,8], the overlap potential is limited by the wasted
CPU cycles in progressing the communication rather than performing useful
computation. Except for designs using core-direct feature [12,13,21] available
only with Mellanox OFED, the current implementations and designs of NBC
operations require CPU cycles to progress the communication. Many researchers
have been trying to maximize the potential of overlap by ﬁnding the appropri-
ate numbers of MPI Test / MPI Probe / MPI Iprobe calls. On the other hand,
the Remote Direct Memory Access (RDMA) technology powered high perfor-
mance interconnects have become the de-facto choice to design HPC systems.
Prior work by researchers in the ﬁeld [2,37] has shown that it is possible to design
eﬃcient collective communication operations using the basic RDMA primitives
oﬀered by high performance interconnects like IB. However, there exists no schol-
arly work that explores whether one can use such primitives to design eﬃcient
non-blocking collective operations with near perfect overlap of communication
and computation and good communication latency.
2
Contributions
In this paper, we take up this challenge and design high performance non-
blocking personalized collective operations that oﬀer near perfect overlap of
computation and communication and good communication latency for RDMA
enabled HPC interconnects like IB. In particular, we focus on primitives
oﬀering All-to-all, All-to-one and One-to-all communication patterns namely
MPI Ialltoall, MPI Igather / MPI Igatherv and MPI Iscatter / MPI Iscatterv.
The overlap optimality of our approach comes from reducing the number of
MPI Test / MPI Probe / MPI Iprobe calls to Zero. To summarize, this paper
makes the following important contributions:

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
437
– Propose a generic approach to design non-blocking collectives with near-
perfect overlap for RDMA-based networks
– Design RDMA-Aware non-blocking collectives for three personalized commu-
nication patterns - All-to-all, All-to-one and One-to-all
– Enhance the OSU microbenchmarks by adding NBC benchmarks to measure
overlap, init time and wait time. Add option to the proposed OSU NBC
microbenchmarks to insert MPI Iprobe calls equally during computation
– Perform a careful analysis of the beneﬁts of our approaches with the proposed
OSU NBC microbenchmarks and P3DFFT application kernel at a large scale
Our experimental evaluation shows that our proposed designs are able to deliver
near perfect overlap of computation and communication for personalized col-
lective operations on modern HPC systems at scale. At the microbenchmark
level, the proposed RDMA-Aware collectives deliver improvements in latency of
up to 89 times for MPI Igatherv, 3.71 times for MPI Ialltoall, and 3.23 times for
MPI Iscatter over the state-of-the-art designs. We also observe an improvement
of up to 19 % for the P3DFFT kernel at 8,192 cores on Stampede [39].
3
Background
In this section, we provide the necessary background information for this paper.
3.1
InﬁniBand
InﬁniBand [9] is a popular switched interconnect standard being used by more
than 44 % of the Top500 Supercomputing systems [40]. The InﬁniBand FDR
adapter from Mellanox can deliver up to 56 Gb/s bandwidth, and communica-
tion latency of less than 1 micro-second. InﬁniBand supports memory communi-
cation semantics through Remote Direct Memory Access (RDMA) operations,
such as RDMA-Write and RDMA-Read. RDMA operations are one-sided and do
not require software involvement at the target. The remote host does not have
to issue any work request for the data transfer. The memory regions that are
used for the RDMA operations are required to be registered with the InﬁniBand
network interface through the ibv reg mr operation, which returns a memory
handle, ibv mr, that contains the virtual address and the RDMA key informa-
tion. Processes that wish to communicate via the RDMA operations need to
exchange ibv mr objects before issuing the network operations on the corre-
sponding memory buﬀers.
3.2
MPI Collective Operations
The current MPI Standard, MPI-3 [27] deﬁnes support for non-blocking collec-
tive operations. The latest open-source versions of the MPICH2 [11] and MVA-
PICH2 [17] software libraries oﬀer basic host-based support for non-blocking
collectives.

438
H. Subramoni et al.
3.3
InﬁniBand Hardware-Multicast Based Collectives
One of the notable features provided by InﬁniBand is the ability to send a mes-
sage to a multicast address and have it delivered to multiple processes on diﬀerent
nodes. Compared to performing multiple point-to-point operations, hardware-
multicast can achieve reduced network load by allowing the switches to duplicate
the message as well as signiﬁcantly lower latency by reducing the number of oper-
ations performed at the host [18]. MVAPICH2 has support for eﬃcient collectives
based on hardware-multicast capabilities [15,22]. Figure 1 shows the performance
of hardware-multicast based MPI Bcast and MPI Scatter on Stampede [39].
 0
 5
 10
 15
 20
 25
256
512
1K
2K
4K
8K
16K 32K
64K 96K
Latency (microseconds)
Number of Processes
Default
Multicast
(a) 16-Byte Broadcast
 5
 10
 15
 20
 25
 30
 35
1
2
4
8
16
32
Latency (microseconds)
Message Size (Bytes)
Default
Multicast
(b) Small message Scatter at 1,024 processes
Fig. 1. Scalability and performance of hardware-multicast based collectives
3.4
P3DFFT
Many applications in areas including Direct Numerical Simulations of Turbu-
lence, and astrophysics rely on highly scalable 3D FFTs [1,34]. The Parallel
Three-Dimensional Fast Fourier Transforms (P3DFFT) library [31] from the
San Diego Supercomputer Center (SDSC) is a portable, high performance, open
source implementation based on the MPI programming model. It leverages the
fast serial FFT implementations of either IBM’s ESSL or FFTW. P3DFFT uses
a 2D, or pencil, decomposition and overcomes an important limitation to scala-
bility inherent in FFT libraries by increasing the degree of parallelism up to N 2,
where N is the linear size of the data grid. It has been used in various Direct
Numerical Simulation (DNS) turbulence applications [1]. The original version
of the P3DFFT kernel relies on blocking MPI Alltoallv / MPI Alltoall opera-
tions on the row and column communicators. Typically, the processor grids are
chosen to ﬁt the row communicator within a compute node, while each of the
column communicators include one process per compute node. Previously, we re-
designed this kernel to take advantage of network-oﬄoad-based MPI Ialltoallv
operation to achieve latency hiding [13]. We use this version of P3DFFT with
support for overlapping computation with communication for our experiments
in this paper.

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
439
4
Problem Space for Designing Non-blocking Collectives
In this section we consider the diﬀerent state-of-the-art approaches in the lit-
erature for designing non-blocking collective operations. We compare diﬀerent
solutions using the following metrics: communication latency, computation/com-
munication overlap, network scalability, and need for a dedicated core to progress
communication.
LibNBC: The initial support for NBC was introduced in LibNBC. The default
support for NBC in MPICH and MVAPICH2 extends this design. This approach
requires applications to call MPI Test / MPI Probe / MPI Iprobe to progress
NBC [6]. However, as noted in Sect. 1, it is not easy to determine the right fre-
quency and the exact number of MPI Test / MPI Probe / MPI Iprobe calls that
have to be performed to achieve communication/computation overlap. Further,
this approach also steals compute cycles away from the application resulting
in non-optimal overlap of computation and communication. Another approach
relies on each MPI process spawning its own asynchronous progress-thread.
In such a design, the MPI process and its progress-thread share the same address
space, and the progress thread can directly execute the collective operation on
behalf of the MPI processes. While this method does not require the application
developer to “guess” the correct number of MPI Test / MPI Probe / MPI Iprobe
calls required to achieve communication/computation overlap, it requires suﬃ-
cient number of idle CPU cores on each node, to ensure that the MPI processes
and their progress threads do not contend for the same set of resources [3]. This
leads to ineﬃcient utilization of the available CPU cycles and is not a practical
solution on modern multi-core systems. We demonstrate this design alternative
in Fig. 2(a).
InﬁniBand CORE-Direct: While the CORE-Direct interface potentially
oﬀers near-perfect overlap and can lead to application-level beneﬁts, the cur-
rent version of the interface has several limitations [13]. Owing to the lack of
hardware-level tag-matching, MPI libraries need to create separate Queue-Pairs
(QPs) and Completion-Queues (CQs) for each communicator, which leads to
scalability issues. Additionally, the communication performance of this interface
has been demonstrated to be worse than the basic InﬁniBand verbs-layer, and
this can adversely aﬀect small message non-blocking collectives [12]. Further, the
feature being available only on Mellanox hardware limits the portability of the
designs to other RDMA based networks.
Functional Partitioning (FP) Approaches: Several approaches that advo-
cate the use of one dedicated processor core to progress the communication
phases of NBC have been proposed in the literature [5,14,29,35]. While these
schemes have signiﬁcantly less overhead to progress communication when com-
pared to the asynchronous progress-thread mentioned above, they still take one
compute core away from the application resulting in sub-par computation per-
formance. However, on systems where there are several less powerful CPU cores

440
H. Subramoni et al.
available, functional partitioning may be a viable approach as the CPU resources
consumed will be low. This alternative is depicted in Fig. 2(b).
Proposed Approach: In our work, we propose to use the basic RDMA mech-
anisms deﬁned by high performance interconnects such as InﬁniBand, iWARP,
and RoCE to design scalable NBC designs that deliver good communication
latency and near perfect overlap of computation and communication. Our app-
roach, depicted in Fig. 2(c), does not require any changes to the application code,
does not require dedicated cores to progress communication (and hence has min-
imal overhead regardless of the types of cores on the system), and has better
portability and network scalability than existing hardware based approaches
such as CORE-Direct.
Table 1 summarizes the strengths and weaknesses of various approaches and
highlights the merits of our proposed framework for designing MPI-3 non-
blocking collective operations in a high-performance and scalable manner.
Fig. 2. Design alternatives for non-blocking collectives
Table 1. Overall design-space for MPI-3 non-blocking collectives
Metric
LibNBC CORE-Direct FP
Proposed Approach
Communication Latency
Good
Fair
Good Good
Computation / Communication Overlap Poor
Good
Good Good
Network Scalability
Good
Poor
Good Good
Availability of Cores for Compute
Poor
Good
Fair
Good
5
Design of RDMA-Aware Non-blocking Personalized
Collectives
In this section, we discuss our proposed designs for implementing high-
performance,
scalable,
and
personalized
MPI-3
non-blocking
collectives.
We develop our designs to meet the following major characteristics: (1) maximize
computation / communication overlap, (2) eliminate the need for MPI Test /

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
441
MPI Probe / MPI Iprobe calls to progress communication and (3) achieve good
communication latency and network scalability.
Although the current collective designs in high performance MPI libraries
like MVAPICH2 are already using RDMA, the data transfer is still based on
underlying two-sided point-to-point communication semantics that require CPU
intervention to progress control messages and completion. Thus, we need oper-
ations that rely on direct RDMA / one-sided semantics to avoid the require-
ment for CPU intervention to progress communication. Further, to achieve
good communication latency, it is better to design collectives directly on top
of low level RDMA operations (or RDMA-Aware) than on top of underlying
point-to-point operations. The main challenges in designing high performance
and scalable RDMA-Aware NBC are: (1) eﬃciently exchanging the tuple of
< remote memory address, remote rkey > required to perform RDMA oper-
ations (RDMA-Write and RDMA-Read) between processes (as described in
Sect. 3.1), (2) exchanging data using appropriate RDMA primitives over the
network to remote peers, and (3) notifying the remote peer of the completion of
the data transfer.
In the following sections, we elaborate on how we address these challenges
to design eﬃcient RDMA-Aware non-blocking collective algorithms to perform
All-to-all, All-to-one, and One-to-all communication patterns.
5.1
Design of Non-blocking RDMA-Aware All-to-all
There are several algorithms available to implement the All-to-all collective oper-
ation in the literature. However, most researchers agree that for large message
sizes and large system sizes, one needs some sort of a pairwise exchange algo-
rithm where all processes talk directly to each other without any intermediate
process routing the messages on their behalf. Such a communication pattern is
extremely amenable to designing using basic RDMA operations as there are no
intermediate steps in the communication that require intervention either from a
process/thread or a hardware collective oﬄoad engine like Core-Direct.
On entering the collective operation, each process ﬁrst registers the send /
receive buﬀers passed by the application with the IB HCA to obtain the
“remote rkey”. Although registering buﬀers with the IB HCA is a costly oper-
ation, the “IB memory registration cache” [28] mechanism in MVAPICH2
allows us to hide the cost of registration by caching the registered entries
for future use. Each process also allocates and registers (with the IB HCA)
a temporary buﬀer to check for completion of data transfers from remote
processes. A small message (24 byte) MPI Allgather is then performed between
all processes taking part in the All-to-all operation to collect the tuple of
< remote memory address, remote rkey > from all processes required to per-
form RDMA operations and notify them of its completion. On completion of
the Allgather, each process schedules data transfer and completion notiﬁcation
operations for each peer on a remote node by posting RDMA-Read or RDMA-
Write operations to the IB HCA. For peers residing on the same physical node
as the processes, we choose the shared memory communication channel [20] for

442
H. Subramoni et al.
small messages (<3 KB) and the loopback communication channel through the
IB HCA for large messages (>32 KB). Although, shared memory channel oﬀers
a higher performance route for intra-node communication from the point of view
of latency, we choose the loopback channel through the IB HCA due its ability
to overlap the communication with computation operation. Once the RDMA
operations have been posted to the IB HCA, the process is free to exit the MPI
library and perform its computation without having to progress the collective
operation. Since the IB HCA is taking care of progressing the collective commu-
nication, this design will meet the ﬁrst and second design criteria we outlined
above.
Caching Mechanism to Avoid MPI Allgather: Although the overhead
introduced by the Allgather operation is small, we introduce a caching mecha-
nism to store the tuple of < remote memory address, remote rkey > from all
processes in order to avoid invoking the Allgather operation in each iteration of
the All-to-all. After registering the send / receive buﬀers passed by the applica-
tion with the IB HCA, the processes participating in the All-to-all will compare
the current address / remote key with the cached address / remote key. Due
to the “IB memory registration cache” in MVAPICH2, it is highly likely that
if the addresses of the send / receive buﬀer passed by the application remains
unchanged, the remote key will also be the same. After the local comparison
phase, all the processes involved in the All-to-all will perform an MPI Allreduce
operation (with an operator of MPI LAND) to see if all processes had a “cache
hit”. If so, then they skip the MPI Allgather operation. If the result of the
MPI Allreduce was a “cache miss”, then the processes go ahead and perform
the MPI Allgather. As an MPI Allreduce operation (which transfers constant
amount of data and completes in Log(N Nodes) number of steps) is signiﬁcantly
less expensive and more scalable than an Allgather, we expect to have good
performance beneﬁts with this caching approach thus achieving the third design
criteria identiﬁed earlier. Note that the MPI Allgather that is being performed
as part of the All-to-all operation only exchanges very small messages (24 bytes)
and will only take a small fraction of the time compared to the All-to-all (<
0.1 %) at large scales.
RDMA Write vs RDMA Read: We evaluated the performance of the All-
to-all operation using both the RDMA Write and RDMA Read primitives that
IB oﬀers. We saw that the performance being oﬀered by the RDMA Write prim-
itive has higher than the RDMA Read primitive. We believe that this is due
to the limitation on the number of back-to-back RDMA Read operations that
can be posted to the IB HCA. Hence we choose the RDMA Write primitive to
implement the All-to-all collective operation.
Temporary Memory Overhead: The overhead in terms of memory intro-
duced by our RDMA-Aware scheme is negligible. We will only consume about 3.0
MB of memory per process for an All-to-all involving 131,072 (128 K) processes.
This can quite easily be oﬀset by tuning the number and size of the intra-node
shared-memory rendezvous buﬀers that the MPI library allocates [19]. As our

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
443
RDMA-Aware design uses IB based loopback communication even for intra-node
transfers such tuning will have no eﬀect on the communication performance
either. Note that our approach will not use any additional internal communica-
tion buﬀers in the MPI library for transferring Request To Send (RTS) / Clear To
Send (CTS) packets that are part of high performance rendezvous based commu-
nication protocols for large messages [38]. This will result in additional memory
savings when compared to the default non RDMA-Aware All-to-all algorithm.
5.2
Design of Non-blocking RDMA-Aware All-to-one and
One-to-all
There are several algorithms available to implement the All-to-one and One-
to-all collective operation in the literature. The result of extensive performance
tuning done for collective operations in the MVAPICH2 MPI library on dif-
ferent generations of interconnects and processors indicates that the “direct”
algorithm, where all processes send / receive data directly to the “root” of the
All-to-one / One-to-all operation, leads to the best performance on most mod-
ern HPC processors (like Westmere / SandyBridge / IvyBridge) and intercon-
nects (like IB-QDR-32 Gbps / IB-FDR-56 Gbps). As with the pairwise All-to-all
exchange, this communication pattern is also very amenable to designing using
basic RDMA operations as there are no intermediate steps in the communication
that require intervention either from a process/thread or a hardware collective
oﬄoad engine like Core-Direct.
On entering the collective operation, the processes register either the send
or the receive buﬀer passed by the application with the IB HCA to obtain the
“remote rkey”. In case of All-to-one, the root will register the receive buﬀer
while the non-root processes will register the send buﬀer. For One-to-all on the
other hand, the root will register the send buﬀer while the non-root processes
will register the receive buﬀer. Once the root has registered the buﬀers, it will
use a Broadcast operation of 12 bytes to inform the non-root processes of its
< remote memory address, remote rkey > tuple. We use the hardware mul-
ticast based MPI Bcast in MVAPICH2 (described in Sect. 3.3) to implement
this broadcast to achieve good scalability and performance. Once the non-root
processes have the < remote memory address, remote rkey > tuple from the
root, they go ahead and initiate the RDMA operations for transferring the data
and posting completion notiﬁcation to the root. In the data transfer phase, the
non-root processes will use the RDMA-Write operation for the All-to-one collec-
tive pattern and the RDMA-Read operation for the One-to-all collective pattern.
We choose diﬀerent RDMA primitives for these collectives to reduce the load on
the IB HCA at the root of the collective. We use the RDMA-Write primitive for
the completion notiﬁcation for both collective patterns as it is just a one byte
transfer.
Optimization to Avoid Memory Registration for Small Messages:
InﬁniBand supports transferring small messages as “INLINE” transfers with-
out having to pre-register it with the IB HCA [9]. We use this feature as an

444
H. Subramoni et al.
optimization for designing small message All-to-one collectives. If the message
size is less that “INLINE THRESHOLD”, the non-root processes will skip reg-
istering the send buﬀer with the IB HCA and directly post the RDMA-Write
operation. We expect this to improve the latency of small message transfers.
Extensions for “v” variants: The “v” variants of the All-to-one (MPI Igatherv)
and One-to-all (MPI Iscatterv) collectives need additional enhancements to the
design to account for the potential variable sized data transfers that the non-root
processes can initiate to the root. To this end, we add an additional 4 byte based
blocking One-to-all collective (MPI Scatter) operation. We use the hardware mul-
ticast based MPI Scatter in MVAPICH2 (described in Sect. 3.3) to implement this
One-to-all operation to achieve good scalability and performance. The root will use
this operation to inform its peers about the displacements at which they need to
read or write the data.
5.3
Extending OSU Micro-Benchmarks for Non-blocking
Collectives
To evaluate our RDMA-Aware design, we have introduced non-blocking col-
lective benchmarks to our OSU Micro-Benchmarks (OMB) suite [30]. In order
to implement these new benchmarks, we extend our current blocking collective
benchmarks by replacing the blocking calls with corresponding non-blocking
ones. Since, the non-blocking calls can be polled (tested) for completion, we are
using the MPI Wait call to wait for completion of the collective. The purpose
of non-blocking collectives is to provide an opportunity for the application pro-
grammers to do useful computation while the communication proceeds in the
background. This is usually referred to as the overlap. To calculate the over-
lap of communication and computation, we ﬁrst measure the communication
time using the initialization call to a collective e.g., MPI Ibcast() immediately
followed by an MPI Wait.
The current collective benchmarks in OMB display only average, minimum,
and maximum latency. This is not meaningful for a non-blocking benchmark so
we have added some new ﬁelds to the display. We display the overall latency (the
total time taken when computation is overlapped with communication), the com-
munication and the computation time, and the overlap percentage. In addition,
we are also timing the initialization overhead (displayed as Coll.Init) and the wait
time (displayed as MPI.Wait). The popular Intel MPI Benchmarks Suite (IMB)
[10] displays the percentage overlap, overall, computation, and communication
latency for the IMB-NBC. However, IMB does not display the initialization
overhead and the time spent in the wait call (as they are not timing these opera-
tions individually). Several other bechmarks have been proposed to measure the
overlap of computation and communication. NBCBench [4] was proposed with
the LibNBC which measures overlap of NBC operations in the LibNBC library.
Sandia MPI Micro-Benchmark Suite (SMB) [36] and Communication Oﬄoad
MPI-based Benchmark (COMB) [16] also provide the facility to measure over-
lap. SMB uses a host-processor overhead method and measures availability of

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
445
the host for computation during a non-blocking MPI send or receive operation.
COMB proposed two benchmarking methods for measuring the overlap. The
ﬁrst one is a simple polling method while the second one is called post-work-
wait method. Both methods use two phases to calculate availability as ration of
time(work without messaging) to time(work plus MPI calls while messaging).
For this paper, we have calculated the percentage overlap based on the formula
in Fig. 3.
Fig. 3. Formula to calculate overlap of communication and computation
The computation time is calculated by timing a dummy compute function
that multiplies a 2D array allocated through malloc(). We have used a cache
unfriendly access and malloc() on purpose to avoid any optimizations done by
the compiler. We have also added support in the OMB suite to use MPI Iprobe()
calls inside our dummy computation function. This will allow us to simulate the
behavior that most applications will use to progress communication while doing
the overlapped computation. Although this support is added at the benchmark
level, it may also provide a ballpark ﬁgure for the number of probe calls that
real applications can also use. The probe calls are evenly distributed between
actual compute function calls.
6
Experimental Results
In this section, we describe the experimental setup used to conduct our exper-
iments. An in-depth analysis of the results is also provided to correlate design
motivations and observed behavior. All results reported here are averages of
multiple (ﬁve) runs to discard the eﬀect of system noise. Note that while all the
experiments presented here are based on Mellanox IB based systems, the tech-
niques and designs are equally applicable to other interconnects that support
the RDMA technology such as iWARP [33]. We do not present these results due
to the lack of access to such large scale systems.
6.1
Experimental Setup
We used the TACC Stampede [39] system for all our experiments. Each node
on Stampede is equipped with with Intel SandyBridge series of processors, using
Xeon dual eight-core sockets, operating at 2.70 GHz with 32 GB RAM. Each
node is equipped with MT4099 FDR ConnectX HCAs (56 Gbps data rate) with
PCI-Ex Gen2 interfaces. The operating system used is CentOS release 6.3, with
kernel version 2.6.32-279.el6 and OpenFabrics version 1.5.4.1.

446
H. Subramoni et al.
6.2
Microbenchmark Level Evaluation
We start the evaluation of the proposed design with the extended version of OMB
introduced in Sect. 5.3. Using both latency and overlap as metrics, we evaluate
the performance of the three diﬀerent communication patterns of personalized
collective operations; All-to-one, One-to-all and All-to-all as well as their V-
version respectively. The comparison is performed up to 2,048 cores for All-to-
one and One-to-all and up to 4,096 cores for All-to-all among these four diﬀerent
schemes:
– Default - The default scheme with only the non-blocking collective followed
by the wait operation.
– RDMA-Aware - The proposed RDMA-Aware scheme.
– Default-Iprobe - The default scheme with the proposed NBC benchmarks
calling MPI Iprobe 1,000 times at equal intervals between the non-blocking
collective and the wait. We ran the proposed benchmarks with three diﬀerent
values for the number of MPI Iprobe calls (1,000, 10,000 and 50,000) and
identiﬁed that we get maximum overlap with 1,000 calls itself. So we use this
value for all our experiments.
– Default-Thread - The default scheme with one MPICH async thread progress-
ing communication in the background for each process.
Performance of All-to-one Operation. Figure 4(a) and (b) show latency
and overlap comparison respectively of non-blocking MPI Igather operation
for 2,048 processes. In the short message region, the cost of broadcasting the
tuple consisting of rkey and the receive buﬀer address has a near-constant over-
head for the RDMA-Aware scheme while the baseline Default scheme does not
incur these costs (Fig. 4(a)). At larger message sizes, however, the cost of reg-
istration adds to initialization overheads and more importantly the latency of
RDMA-Aware scheme grows super-linearly as bandwidth limitations and HCA
contention weigh in and possibly results in network serialization. Scheme Default-
Thread closely matches the latency of the Default scheme because of the
large number of MPI Iprobe calls which progress outstanding data transfers.
Despite this, the RDMA-Aware scheme yields good overlap in the short mes-
sage range and near-perfect overlap in the large message range compared to
other schemes (Fig. 4(b)). The short message region yields below 100 % overlap
because of the broadcast overhead, but in comparison this overhead is negligible
for the larger message range which is the primary target region for non-blocking
collectives. The trends of both latency and overlap metrics are in favor of the
RDMA-Aware scheme for MPI Igatherv operation as shown in Fig. 5(a) and (b).
As we can observe in Fig. 5(a), the RDMA-Aware scheme is able to deliver
improvements in latency of up to 89 times compared to the state-of-the-art for
small to medium message sizes. In the short message range the Default scheme
uses MPI Issend operation for scalability reasons (so that the root process is not
overwhelmed with messages from the non-root processes). However, this leads to
poor latency and overlap. The performance of Default-Thread was signiﬁcantly

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
447
worse compared to the other approaches for MPI Igatherv. Hence, we do not
show them in Fig. 5.
 0
 10000
 20000
 30000
 40000
 50000
 60000
 1
 4
 16
 64
 256
1K
4K
16K 64K 256K
Latency (microseconds)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
Default-Thread
Small Message Latency
 0
 1000
 2000
 1
 4
 16
 64
 256
1K
(a) Latency
 0
 25
 50
 75
 100
 1
 4
 16
 64
 256
1K
4K
16K 64K 256K
Overlap (percentage)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
Default-Thread
(b) Overlap
Fig. 4. Performance of MPI Igather with diﬀerent NBC designs at 2,048 processes
 0
 10000
 20000
 30000
 40000
 50000
 60000
 1
 4
 16
 64
 256
1K
4K
16K 64K 256K
Latency (microseconds)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
(a) Latency
 0
 25
 50
 75
 100
 1
 4
 16
 64
 256
1K
4K
16K 64K 256K
Overlap (percentage)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
(b) Overlap
Fig. 5. Performance of MPI Igatherv with diﬀerent NBC designs at 2,048 processes
Performance of One-to-all Operation. The One-to-all operation, being the
inverse of the All-to-one operation, exhibits favorable trends of good latency and
near-perfect overlap but for one diﬀerence — The latency of the MPI Iscatter
operation does not show super-linear growth at 16 KB range possibly due to
the read nature of underlying network operation which does not alter the con-
tents of the root of the One-to-all operation. As Fig. 6(a) depicts, our proposed
RDMA-Aware scheme is able to deliver up to 3.23 times improvement in commu-
nication latency when compared to the state-of-the-art for MPI Iscatter in the
medium to large message range. The beneﬁts observed increase as the message
size increases with the peak beneﬁt seen at a message size of 256 KB. There is,
however, a small degradation in the overlap achieved with for 2,048 processes in
the 16 KB range which can be attributed to tuning eﬀects. For the MPI Iscatterv
collective, the proposed RDMA-Aware scheme is able to deliver performance and

448
H. Subramoni et al.
overlap similar to the state-of-the-art schemes as depicted in Fig. 7. Note that
the performance of Default-Thread was signiﬁcantly worse compared to the other
approaches for MPI Iscatter and MPI Iscatterv. Hence, we do not show them in
Figs. 6 and 7.
 0
 50000
 100000
 150000
 200000
 250000
 300000
 1
 4
 16
 64  256 1K
4K 16K 64K 256K
Latency (microseconds)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
Small Message Latency
 0
 500
 1000
 1
 4
 16
 64  256 1K
(a) Latency
 0
 25
 50
 75
 100
 1
 4
 16
 64
 256
1K
4K
16K 64K 256K
Overlap (percentage)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
(b) Overlap
Fig. 6. Performance of MPI Iscatter with diﬀerent NBC designs at 2,048 processes
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 1
 4
 16
 64
 256
1K
4K
16K 64K 256K
Latency (microseconds)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
Small Message Latency
 400
 600
 800
 1000
 1
 4
 16
 64  256
1K
(a) Latency
 0
 25
 50
 75
 100
 1
 4
 16
 64
 256
1K
4K
16K 64K 256K
Overlap (percentage)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
(b) Overlap
Fig. 7. Performance of MPI Iscatterv with diﬀerent NBC designs at 2,048 processes
Performance of All-to-all Operation. Figure 8 shows the latency and over-
lap performance of the non-blocking All-to-all operation for 512 and 4,096
processes. For this operation, the RDMA-Aware scheme yields the best latency
compared to the rest of the schemes as well nearly 100 % overlap in the medium
and large message range. The allgather operation issued decouples control mes-
sage exchange and data transfer operations and this yields good overlap and
latency. As shown in Fig. 8(c), the latency of the RDMA-Aware non-blocking
All-to-all is up to 3.71 times lesser than the Default scheme at a message size of
32 KB. However, as message size increases, the performance of the two schemes
converge as network bandwidth becomes the chief bottleneck.

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
449
 0
 100
 200
 300
 400
 500
 600
 700
 800
16K
32K
64K
128K
256K
Latency (milliseconds)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
Default-Thread
(a) Latency at 512 processes
 0
 25
 50
 75
 100
16K
32K
64K
128K
256K
Overlap (percentage)
Message Size (Bytes)
Default
RDMA-Aware
Default-Iprobe
Default-Thread
(b) Overlap at 512 processes
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 4500
 5000
16K
32K
64K
128K
Latency (milliseconds)
Message Size (Bytes)
Default
RDMA-Aware
(c) Latency at 4,096 processes
 0
 25
 50
 75
 100
16K
32K
64K
128K
Overlap (percentage)
Message Size (Bytes)
Default
RDMA-Aware
(d) Overlap at 4,096 processes
Fig. 8. Performance of MPI Ialltoall with diﬀerent NBC designs at 512 and 4,096
processes
6.3
Evaluation with Application Kernels : P3DFFT
In this section, we study the characteristics of our proposed solution with the
P3DFFT Kernel with support for overlapping computation and communication
described in Sect. 3.4. Note that we have used the “-DUSE EVEN” build option
to make P3DFFT use MPI Ialltoall instead of MPI Ialltoallv.
We perform weak scaling experiments where the problem size increases with
the size of the job. In all experiments the problem size was chosen so as to keep
the percentage of memory consumed by the P3DFFT kernel at about 75 % to
80 % of available system memory. Figure 9 shows the performance of P3DFFT
for diﬀerent NBC designs. We do not consider the Default-Iprobe design here
due to the fact that it requires re-design of the P3DFFT kernel. We observe
that our proposed design can lead to signiﬁcant improvements in the execution
time of the P3DFFT kernel in all cases. As we can see, RDMA-Aware performs
the best for all system sizes. Default-Thread, on the other hand, performs the
worst. We believe this is due to MPICH async threads stealing compute cycles
away from the main application thread. Based on this observation, we eliminate
Default-Thread from further runs at larger scales. In Fig. 9(b), we compare the
performance of Default and RDMA-Aware at large scales. As with Fig. 9(a), we
see that our proposed RDMA-Aware approach performs better than “Default”

450
H. Subramoni et al.
for all system sizes. For instance, with 8,192 processes, our proposed RDMA-
Aware design of MPI Ialltoall was able to deliver 19 % better performance than
the default version.
  0
  3
  6
  9
  12
  15
  18
128
256
512
CPU Time Per Loop (Seconds)
Number of Processes
Default
Default−Thread
RDMA−Aware
(a) Small scale runs
  0
  2
  4
  6
  8
  10
  12
128
256
512
1K
2K
4K
8K
CPU Time Per Loop (Seconds)
Number of Processes
Default
RDMA−Aware
(b) Large scale runs
Fig. 9. Performance of P3DFFT for diﬀerent NBC designs
7
Conclusion and Future Work
In this paper, we presented the design of high performance non-blocking
personalized collective operations that oﬀer near-perfect overlap of computa-
tion and communication and good communication latency for RDMA enabled
HPC interconnects like IB. We proposed a generic approach to design non-
blocking collectives with near-perfect overlap for RDMA-based networks and
designed RDMA-Aware non-blocking collectives for three personalized commu-
nication patterns - All-to-all, All-to-one and One-to-all. We enhanced the OSU
microbenchmarks by adding NBC benchmarks to measure overlap, init time and
wait time. We also added an option to the proposed OSU NBC microbench-
marks to insert MPI Iprobe calls equally during computation. We also performed
a careful analysis of the beneﬁts of our approaches with the proposed OSU
NBC microbenchmarks and P3DFFT application kernel at a large scale. Our
experimental evaluation showed that our proposed designs are able to deliver
near-perfect overlap of computation and communication for personalized col-
lective operations on modern HPC systems at scale. At the microbenchmark
level, the proposed RDMA-Aware collectives delivered improvements in latency
of up to 89 times for MPI Igatherv, 3.71 times for MPI Ialltoall and, 3.23 times for
MPI Iscatter over the state-of-the-art designs. We also observed an improvement
of up to 19 % for the P3DFFT kernel at 8,192 cores on the Stampede supercom-
puting system at TACC.
In future, we plan to evaluate P3DFFT, PSDNS, as well as other ker-
nels and applications that use personalized collectives at larger scales and on
other RDMA-enabled networks. Further, we aim to study the applicability of
such RDMA-Aware techniques to designing other non-personalized collective
communication operations such as Reduce, Allreduce and Broadcast. We also

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
451
plan to publicly release the RDMA-Aware design for NBC and the OSU NBC
microbenchmarks with future releases of the MVAPICH2 MPI library.
References
1. Donzis, D., Yeung, P.K., Pekurovsky, D.: Turbulence simulations on O(104) proces-
sors. In: TeraGrid, June 2008
2. Gupta, R., Balaji, P., Panda, D.K., Nieplocha, J.: Eﬃcient collective operations
using remote memory operations on VIA-based clusters. In: 2003 Proceedings of
the International Parallel and Distributed Processing Symposium, p. 9, April 2003
3. Hoeﬂer, T., Lumsdaine, A.: Message progression in parallel computing - to thread
or not to thread?. In: Cluster (2008)
4. Hoeﬂer, T., Schneider, T., Lumsdaine, A.: Accurately measuring collective opera-
tions at massive scale. In Proceedings of the 22nd IEEE International Parallel &
Distributed Processing Symposium, PMEO 2008 Workshop, April 2008
5. Hoeﬂer, T., Siebert, C., Lumsdaine, A.: Group operation assembly language - a
ﬂexible way to express collective communication. In: ICPP-2009 - The 38th Inter-
national Conference on Parallel Processing. IEEE, September 2009
6. Hoeﬂer, T., Squyres, J.M., Rehm, W., Lumsdaine, A.: A case for non-blocking
collective operations. In: Min, G., Di Martino, B., Yang, L.T., Guo, M., R¨unger, G.
(eds.) ISPA Workshops 2006. LNCS, vol. 4331, pp. 155–164. Springer, Heidelberg
(2006)
7. Hoeﬂer, T., Gottschling, P., Lumsdaine, A., Rehm, W.: Optimizing a conjugate
gradient solver with non-blocking collective operations. Parallel Comput. 33(9),
624–633 (2007)
8. Hoeﬂer, T., Lumsdaine, A., Rehm, W.: Implementation and performance analysis
of non-blocking collective operations for MPI. In: 2007 Proceedings of the 2007
ACM/IEEE Conference on Supercomputing, SC 2007, pp. 1–10. IEEE (2007)
9. InﬁniBand Trade Association. http://www.inﬁnibandta.com
10. Intel MPI Benchmarks (IMB). https://software.intel.com/en-us/articles/intel-
mpi-benchmarks
11. Liu, J., Jiang, W., Wyckoﬀ, P., Panda, D.K., Ashton, D., Buntinas, D., Gropp,
B.,Tooney, B.: High Performance Implementation of MPICH2 over InﬁniBand with
RDMA Support. In: IPDPS (2004)
12. Kandalla, K., Yang, U., Keasler, J., Kolev, T., Moody, A., Subramoni, H., Tomko,
K., Vienne, J., Panda, D.K.: Designing non-blocking allreduce with collective
oﬄoad on inﬁniband clusters: a case study with conjugate gradient solvers. In:
IEEE International Symposium on Parallel and Distributed Processing (IPDPS)
(2012)
13. Kandalla, K., Subramoni, H., Tomko, K., Pekurovsky, D., Sur, S., Panda, D.K.:
High-performance and scalable non-blocking all-to-all with collective oﬄoad on
InﬁniBand clusters: a study with parallel 3D FFT. Comput. Sci. 26, 237–246 (2011)
14. Kandalla, K.C., Subramoni, H., Tomko, K., Pekurovsky, D., Panda, D.K.: A novel
functional partitioning approach to design high-performance MPI-3 non-blocking
alltoallv collective on multi-core systems. In: 42nd International Conference on
Parallel Processing, ICPP 2013, Lyon, France, 1–4 October 2013, pp. 611–620
(2013)

452
H. Subramoni et al.
15. Kini, S.P., Liu, J., Wu, J., Wyckoﬀ, P., Panda, D.K.: Fast and scalable barrier
using rdma and multicast mechanisms for inﬁniband-based clusters. In: Dongarra,
J., Laforenza, D., Orlando, S. (eds.) EuroPVM/MPI 2003. LNCS, vol. 2840, pp.
369–378. Springer, Heidelberg (2003)
16. Lawry, W., Wilson, C., Maccabe, A.B., Brightwell, R.: COMB: a portable bench-
mark suite for assessing MPI overlap. In: IEEE Cluster, pp. 23–26 (2002)
17. Liu, J., Jiang, W., Wyckoﬀ, P., Panda, D.K., Ashton, D., Buntinas, D., Gropp, W.,
Toonen, B.: Design and implementation of MPICH2 over InﬁniBand with RDMA
support. In: Proceedings of Int’l Parallel and Distributed Processing Symposium
(IPDPS 2004), April 2004
18. Liu, J., Mamidala, A., Panda, D.K.: Fast and scalable MPI-level broadcast using
InﬁniBand’s hardware multicast support. In: Proceedings of Int’l Parallel and Dis-
tributed Processing Symposium (IPDPS 04), April 2004
19. Luo, M., Wang, H., Vienne, J., Panda, D.K.: Redesigning MPI shared memory
communication for large multi-core architecture. computer science - research and
development, pp. 1–10. doi:10.1007/s00450-012-0210-8
20. Luo, M., Wang, H., Vienne, J., Panda, D.K.: Redesigning MPI shared memory
communication for large multi-core architecture. Comput. Sci. 28(2–3), 137–146
(2013)
21. Venkata, M., Graham, R., Ladd, J., Shamis, P., Rabinovitz, I., Vasily, F., Shainer,
G.: ConnectX-2 CORE-direct enabled asynchronous broadcast collective commu-
nications. In: Proceedings of the 25th IEEE International Parallel and Distributed
Processing Symposium, Workshops (2011)
22. Mamidala, A., Liu, J., Panda, D.K.: Eﬃcient barrier and allreduce on IBA clusters
using hardware multicast and adaptive algorithms. In: IEEE Cluster Computing
(2004)
23. ConnectX-2 VPI with CORE-Direct Technology. http://www.mellanox.com/page/
products dyn?product family=61&mtag=connectx 2 vpi
24. Programmable ConnectX-3 Pro Adapter Card Dual-Port Adapter with VPI.
http://www.mellanox.com/page/products dyn?product family=202&mtag=progr
ammable connectx 3 pro vpi card
25. Connect-IB Single/Dual-Port InﬁniBand Host Channel Adapter Cards. http://
www.mellanox.com/page/products dyn?product family=142
26. Message Passing Interface Forum. MPI: A Message-Passing Interface Standard,
March 1994
27. MPI-3
Standard
Document.
http://www.mpi-forum.org/docs/mpi-3.0/mpi30-
report.pdf
28. Network-Based Computing Laboratory. MVAPICH: MPI over InﬁniBand, 10GigE/
iWARP and RoCE. http://mvapich.cse.ohio-state.edu/
29. Nomura, A., Ishikawa, Y.: Design of kernel-level asynchronous collective commu-
nication. In: Keller, R., Gabriel, E., Resch, M., Dongarra, J. (eds.) EuroMPI 2010.
LNCS, vol. 6305, pp. 92–101. Springer, Heidelberg (2010)
30. OSU Micro-benchmarks. http://mvapich.cse.ohio-state.edu/benchmarks/
31. Pekurovsky, D.: P3DFFT: a framework for parallel computations of fourier trans-
forms in three dimensions. SIAM J. Sci. Comput. 34(4), C192–C209 (2012)
32. Portals Network Programming Interface. http://www.cs.sandia.gov/Portals/
33. Romanow, A., Bailey, S.: An overview of RDMA over IP. In: Proceedings of Interna-
tional Workshop on Protocols for Long-Distance Networks (PFLDnet2003) (2003)
34. Laizet, S., Lamballais, E., Vassilicos, J.C.: A numerical strategy to combine high-
order schemes, complex geometry and parallel computing for high resolution dns
of fractal generated turbulence. Comput. Fluids 39, 471–484 (2010)

Designing Non-blocking Personalized Collectives with Near Perfect Overlap
453
35. Schneider, T., Eckelmann, S., Hoeﬂer, T., Rehm, W.: Kernel-based oﬄoad of col-
lective operations – implementation, evaluation and lessons learned. In: Jeannot,
E., Namyst, R., Roman, J. (eds.) Euro-Par 2011, Part II. LNCS, vol. 6853, pp.
264–275. Springer, Heidelberg (2011)
36. Sandia MPI Micro-Benchmark Suite (SMB). http://www.cs.sandia.gov/smb/index.
html
37. Sur, S., Bondhugula, U.K.R., Mamidala, A.R., Jin, H.-W., Panda, D.K.: High
performance RDMA based all-to-all broadcast for inﬁniband clusters. In: Bader,
D.A., Parashar, M., Sridhar, V., Prasanna, V.K. (eds.) HiPC 2005. LNCS, vol.
3769, pp. 148–157. Springer, Heidelberg (2005)
38. Sur, S., Jin, H.-W., Chai, L., Panda, D.K.: RDMA read based rendezvous proto-
col for MPI over InﬁniBand: design alternatives and beneﬁts. In: Proceedings of
the Eleventh ACM SIGPLAN Symposium on Principles and Practice of Parallel
Programming, PPoPP 2006, pp. 32–39. ACM, New York, NY, USA (2006)
39. Texas Advanced Computing Center. Stampede Supercomputer. http://www.tacc.
utexas.edu/
40. TOP 500 Supercomputer Sites. http://www.top500.org

Design Methodology for Optimizing Optical
Interconnection Networks in High
Performance Systems
Sébastien Rumley1(&), Madeleine Glick2, Simon D. Hammond3,
Arun Rodrigues3, and Keren Bergman1
1 Lightwave Research Laboratory, Columbia University, New York, USA
{rumley,bergman}@ee.columbia.edu
2 College of Optical Science, University of Arizona, Tucson, USA
mglick@optics.arizona.edu
3 Sandia National Laboratories, Albuquerque, USA
{sdhammo,afrodri}@sandia.gov
Abstract. Modern high performance computers connect hundreds of thousands
of endpoints and employ thousands of switches. This allows for a great deal of
freedom in the design of the network topology. At the same time, due to the
sheer numbers and complexity involved, it becomes more challenging to easily
distinguish between promising and improper designs. With ever increasing line
rates and advances in optical interconnects, there is a need for renewed design
methodologies that comprehensively capture the requirements and expose trade-
offs expeditiously in this complex design space. We introduce a systematic
approach, based on Generalized Moore Graphs, allowing one to quickly gauge
the ideal level of connectivity required for a given number of end-points and
trafﬁc hypothesis, and to collect insight on the role of the switch radix in the
topology cost. Based on this approach, we present a methodology for the
identiﬁcation of Pareto-optimal topologies. We apply our method to a practical
case with 25,000 nodes and present the results.
Keywords: Topology  Network  HPC  Interconnect
1
Introduction
As aggregated computing power approaches the Exascale mark, and more importantly,
as parallelism reaches unprecedented levels, modern interconnects need to provide ever
growing bandwidths and connectivity. For rack-to-rack links, and in the near future, for
all types of connections, this trend is likely to lead to the increased use of photonic
networks. This transition provides an opportunity to re-examine interconnect design
methodologies. Photonic systems differ in many aspects from conventional electrical
ones. Depending on which optical technology is used, and how it is used, a particular
design can be well suited or on the contrary fairly ill-adapted.
In this work, we examine the selection of the interconnect topology using two
criteria: the switch radix, and the number of links. There is considerable research into
increasing the port count of different ﬂavors of optical switches [1, 2]. It would be
© Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 454–471, 2015.
DOI: 10.1007/978-3-319-20119-1_32

helpful to system designers and component researchers to have a clearer view of the
goals and trade offs. The situation is the same at the link level. There is considerable
progress in components and also work on power efﬁcient and/or power proportional
systems. We aim to establish a methodology that can clearly expose the beneﬁts and
shortcomings of various topologies along these axes.
A great variety of topology descriptions are available in existing literature. Multi-
dimensional tori and hierarchical structures (fat-trees) have been the dominant
supercomputer interconnects for many years (mid 1980 s to mid 2000 s) [3]. Tori
fundamentally replicate the same structure as many of the simulations they support (2D
or 3D stencils) [4]. They offer bandwidth in a very speciﬁc manner, which can be very
efﬁcient if the supporting code follows the same structure [5]. On the other hand, fat-
trees can be constructed as virtual crossbars offering general purpose connectivity
[6, 7]. Connectivity patterns, have, however, recently received renewed attention and
these toroidal or hierarchical architectures have been progressively superseded by more
complex schemes. This can be partly explained by the advent of two trends. On one
hand, there is a growing imbalance between the energy costs of computation and
communication. Tighter transistor integration allows computing circuits to be more
efﬁcient, but contributes little in decreasing bit displacement costs [8]. Networks are
thus growing contributors to overall energy consumption - and subsequently dissipa-
tion, which is also a serious concern. On the other hand, increased expectations in terms
of aggregated computing power, coupled with the progressive saturation of both CPU
and chip computing power, leads to a general inﬂation of the interconnect size [9]. As
underlined in several studies (e.g. [10]) as well as in what follows, the cost of tradi-
tional tori and fat-trees becomes discouraging at very large scale and under arbitrary
trafﬁc. More scalable alternatives, in particular, Flattened Butterﬂy [3], Dragonﬂy [11],
Jelly-ﬁsh [12] and Slim-ﬂy [13] have thus been proposed.
All these studies, however, address the question “what is the ideal topology?” by
promoting a recipe to assemble a topology and by comparing it to other instances. In
this paper, we dig deeper and propose an analytical approach to evaluate the funda-
mental connectivity requirements. Unless a radical breakthrough is achieved, larger
interconnects will continue to be required, they will represent an increasing part of the
operational costs, and they will integrate an increasing proportion of photonics. The
motivation is thus to more clearly deﬁne metrics, requirements and options to gauge
suitability of design options. Our proposed methodology, by allowing a rapid evalu-
ation of the switch radix/capacity requirement, addresses these needs. It is also general
enough to be applied in multiple contexts, even photonic-free ones.
Our approach is based on a capacity-ﬂow analysis, and uses Generalized Moore
Graphs. These graphs minimize the average routing distance. As such, they can be used
as an optimal bound in terms of connectivity, as developed in Sect. 3. In Sect. 4, we
further develop our approach in order to support a broader range of input cases. In
particular, we integrate in our formulation a variable concentration factor, which rep-
resents the number of nodes or servers that share an entry switch. Varying this factor
allows us to explore the fundamental cost/performance/hardware requirements trade-
offs that exists in terms of topologies, for various global interconnect sizes. We use
these results to derive conclusions on fundamental future needs in terms of the switch
radix. In Sect. 5, we evaluate several classical topologies against the bound, and show
Design Methodology for Optimizing Optical Interconnection Networks
455

that Flattened Butterﬂies, despite their relative simplicity, achieve good performance.
Since routing in these networks can be made very simple, they are still of interest for
future large scale machines. Finally we sketch future research directions where our
approach could be of further use, or further developed, and draw ﬁnal conclusions in
Sects. 6 and 7.
2
Related Work
Most classical means to interconnect multiple Processing Elements (PE) have been
investigated in the 1970 s—1980 s. Preparata et al. proposed the Cube-Connected-
Cycles network architecture [14], Bhuyan et al. summarized the properties of Hyper-
cube networks [15] previously proposed by Pease [16] and Leiserson described the
Fat-Tree concept [6]. Dally investigated the performance of n-dimensional tori [17].
These analyses targeting parallel computing were preceded by those realizing efﬁcient
automated switches for telephone networks [18]. The fundamental performance and
cost metrics that apply to connection networks (average trafﬁc delay, average trafﬁc
density, total connection cost, connections per node, reliability) are analyzed in the
most “obvious” types of interconnection by Wittie [19].
Most of these references are over 20 years old. Since then, the computing power of
supercomputers has generally doubled every year on average in two eras. Until the
early 2000 s, most of this increase was covered by the increase of the single CPU
power (either through higher clocking or increased instruction level parallelism). The
increase in terms of CPU (or core, as multi-core appeared) parallelism, although
continuous, was less strong. Consequently, over this period, interconnect node count
increased only modestly and results obtained in the literature cited above were sufﬁ-
cient to meet demand.
At the turn of the millennium, however, the scaling limitation of a single processor
[9] became apparent. This triggered renewed interest in interconnection structures, in
particular in those supported by high radix switches [20]. These switches, instead of
providing only 8 to 24 bi-directional ports, pushed the port count to 64 [21] by 2006,
and to more than 100 by today [22]. With such high port counts, more highly con-
nected and compact topologies become feasible. The Flattened Butterﬂy [3], Dragonﬂy
[11], and more recently, Slim-ﬂy [13] architectures all enter in this new generation of
topologies.
There are various reasons that might promote one topology over another. Some
structures make routing decisions (Flattened Butterﬂy [3], Tori [17]), trafﬁc-balancing
(Dragonﬂy [11]) or incremental deployment (Exchanged Crossed Cube [23], Jellyﬁsh
[12]) easier. As the interconnect size grows, however, the economic argument becomes
dominant, favoring designs involving as few resources as possible.
The aim of efﬁcient resource utilization naturally leads one to a graph theoretic
approach. The relation to the Moore Graph and Moore bound is mentioned by Von Conta
[24], who also analyzes the structure of Tori and proposes other graphs (and methods to
create them) to approach the Moore bound. The suitability of some Cayley graphs for
designing interconnection networks is underlined by Akers and Krishnamurthy [25].
Hsieh and Hsiao showed k-valent Cayley graphs are maximally fault tolerant [26].
456
S. Rumley et al.

McKay, Miller and Siran propose a method (exploited to create the Slim-ﬂy architecture)
to construct non-Cayley graphs of unlimited sizes, all of diameter two (shortened as
MMS) [27]. Most of these MMS graphs are the largest diameter 2 graphs known so far.
A table of the largest graphs (of any diameter <10) is provided in Reference [28].
The usage of optimal graphs as the Petersen or the Hoffman-Singleton graph has
also been proposed [29]. Random search methods have been exploited to identify large
graphs of small diameter [30, 31]. Random addition of links to rings [32] or simply
random wiring of switches [12] has also been investigated. The optimal properties of
the Generalized Moore Graph for comparison purposes have been previously exploited
in the context of Metropolitan Network planning [33].
Topologies are generally compared and considered in the literature under maximal
all-to-all trafﬁc, as a proxy for worst conditions. In practice, observed utilization pattern
almost always clearly differ from all-to-all, as illustrated by Vetter et al. [34] or
Kandula et al. [35]. A signiﬁcant effort is also invested to improve the matching
between (actual or future) parallel codes requirements and topology capabilities, in
particular through large-scale simulation [36, 37].
3
Identifying Ideal Connectivities
We start by deﬁning terms and notations, and by clarifying the framework of this study.
The decision problem can be summarized as follows: how does one connect N pro-
cessing elements (PE) such that each PE pair can communicate (even minimally). We
assume that each PE has a single bidirectional link with the outside world. We also
assume that every link has a given, constant capacity, for example 10 Gb/s. In our
calculations, all capacity and trafﬁc measures are normalized to this reference capacity.
Consequently, there is no need to retain its absolute value, and a PE can be assumed to
have a communication capacity of 1 unit (u), in both directions. We also neglect how
links are implemented. In practice links may be composed of multiple parallel lanes or
even cables. We ﬁnally assume that PEs are attached to a single switch through a
unique, bidirectional link of 1u.
We denote r the number of ports available in each switch, i.e. the switch radix. All
switches are assumed equivalent and capable of achieving 100 % throughput as long as
ﬂows are (on the medium term) well balanced among the input and outputs. Unless r is
larger than the number of PEs N, each switch is connected to at least one other switch
to ensure global connectivity among all PEs. Two switches can be connected by more
than one link in order to support a sustained trafﬁc ﬂow larger than 1u.
We further assume that PEs are distributed as equally as possible among the
switches, and hence that we have S ¼ N=C
d
e switches, where C is the concentration
factor. With this assumption, we focus our study on direct interconnection networks as
opposed to indirect ones. This assumption also allows us to consider all switches as
equivalent [25].
We are interested in determining how many links must be placed between each
switch pair. In raw optimization terms, this means ﬁnding S(S-1) positive integer
values, which become an unmanageable problem, especially if S is equal or larger than
one thousand as it is the case in recent supercomputers.
Design Methodology for Optimizing Optical Interconnection Networks
457

We start by investigating how many switch pairs should be connected, or, stated
slightly differently, how highly connected should the topology be. We deﬁne the
connectivity of a topology, noted R, as the average number of direct neighbors a switch
has, excluding the PEs. This is equivalent to the average vertex degree in graph theory.
R can be as large as S−1, in which case the topology is a full-mesh. If the topology
forms a ring, R = 2. Topologies with a connectivity R < 2 are possible but are not fault
tolerant, a quality that we expect from HPC interconnects. Establishing how highly
connected should the topology be means therefore ﬁnding the appropriate value of
R between 2 and S-1.
R determines the global capacity of the topology. Hence, without further
assumption about the way given PE pairs communicate, and since all switches are
equivalently connected to PE, there is no ﬁrst order reason to concentrate more con-
nections around a particular switch or between a particular switch pair. If n links have
to be installed between two switches to adequately support the trafﬁc, n links will also
be required between any of the connected pairs. This allows us to observe that, under
this assumption, the total number of links is nSR/2 and the total installed capacity
is nSR.1
Considering the trafﬁc demand, the amount of data that is injected into the inter-
connect, and the instants at which that data is injected is affected by multiple factors:
node or server computing power and architecture, type of software executed, imple-
mentation of the software, or even input-data currently processed by the software.
Quantifying the requirements is therefore a difﬁcult task. To obtain an idea of the
design requirements one generally deﬁnes a challenging scenario, as the maximal
uniform trafﬁc case: each of the PEs of the system uses its maximum networking
capacities to distribute messages equally among all other PEs, i.e. each PEs sends a
ﬂow of 1/(N-1) u to each other PE. Under this trafﬁc assumption, and that one PE is
connected to each switch, i.e. C = 1, we can formulate the total trafﬁc ﬂow as
NðN  1Þ 
1
N  1  D ¼ ND
which is the product of the number of ﬂows, the ﬂow magnitude and the average
routing distance (in hops), Δ. Hence, the more hops a message has to travel, the more
time it occupies links and switches. Note that the knowledge of the average distance is
sufﬁcient as long as all ﬂows have the same magnitude.
We have shown that in a regular topology with S switches, each associated with 1
PE (S = N as C = 1) and under maximum uniform trafﬁc, the total trafﬁc is NΔ. On other
hand, the capacity obtained maintaining symmetry is nSR = nNR. In order to have the
topology at least asymptotically supporting the trafﬁc, the inequality NΔ ≤nNR must
hold.
To evaluate the connectivity R, and to this aim, express Δ as a function of R, we
assume that the topology will be organized such that distances between switches are
1 Each switch is connected on average to R others with n links, thus nSR ports are occupied in total. As
each link connects two ports, the number of links is nSR/2. Since each link is assumed bidirectional,
it represents two units of capacity so the total capacity is nSR.
458
S. Rumley et al.

kept short. We can expect the ﬁnal topology, whatever its R being, to be closely related
to an ideal one of same size that minimizes the average distance Δ. In the best case, this
ideal topology is a Generalized Moore Graph (GMG - described in the Appendix) and
the average distance ΔGMG(R) between a switch and its S−1 neighbors (and thus
between any node pair) can be written as
DGMGðRÞ ¼ R þ 2RðR  1Þ þ 3RðR  1Þ2 þ . . . þ ðD  1ÞRðR  1ÞD2 þ Dx
S  1
ð1Þ
where x ¼ S  1  R  RðR  1Þ  RðR  1Þ2  . . .  RðR  1ÞD2 is the number of
neighbors at a maximum distance D, and R is the maximum degree in the topology
(D is also the diameter).
With Δ expressed as a function of R, we rewrite the inequality as ΔGMG(R) ≤nR.
We simplify further by showing that n can be assumed to be equal to one. As we are
interested in minimizing the topology costs, which we assume highly correlated with
the number links, we also want to minimize the product nR. As shown in Fig. 1, the
average distance ΔGMG(R) decreases with larger values R, and with it the total trafﬁc.
In contrast, changing n has no inﬂuence on the trafﬁc. We can exploit this fact. Taking
the case n = 2 and R = 4, thus nR = 8. By rebalancing the factors using n’ = 1 and
R’ = 8, the product is unchanged. On the contrary, the trafﬁc side of the equation may
be smaller but will never be larger. Therefore, under the assumptions listed so far,
choosing n = 1 never leads to less economical topologies.
Finally, this allows us to state that the smallest integer value R for which the
inequality ΔGMG(R) ≤R holds, Ropt, is the connectivity of the most economical
topology that supports N PEs, each connected to a switch and exchanging maximum
uniform trafﬁc.
There is no evident closed-form expression for Ropt. However, as shown in Fig. 2a,
Ropt grows relatively slowly with N = S, it is therefore easy to ﬁnd Ropt by calculating
ΔGMG(R) for increasing values of R. This approach allows one to also identify the
diameter of the most economical GMG, Dopt (Fig. 2b). Note that Dopt is not mono-
tonically increasing. When considering topologies of increasing sizes, at some points
the larger size justiﬁes a larger connectivity. As this increment of R allows more
switches to be present at closer distances, it generally allows one to “remove” a level in
the hierarchy, causing a drop in Dopt.
5
10
15
20
25
0
2
4
6
8
10
Topology connectivity R
Average distance in
Generalized Moore Graph
ΔGMG(R)
1,000 Switches
10,000 Switches
100,000 Switches
Fig. 1. Evolution of the average distance in a Generalized Moore Graph.
Design Methodology for Optimizing Optical Interconnection Networks
459

The evaluation of Ropt also allows us to analyze the capacity requirements of the
most economical GMG topologies, which is given by CAPopt = NRopt, and plotted in
Fig. 2c. Notably, the total amount of resources increases supra linearly with the number
of PEs, even in the optimal case: larger topologies thus induce a “complexity” penalty.
The values above are based on the conjecture that a GMG topology with N vertices
and maximal degree Ropt exists. As in practice very few such graphs have been proven
to exist, and even fewer have been constructed, these values must be considered as
indicators and not as absolute goals. In Sect. 5 real topologies are compared to the
GMG “bound” graph.
4
Generalization to Other Concentrations and Trafﬁc
Patterns
As described so far, our approach indicates the ideal level of connectivity required to
obtain a balanced topology under two assumptions: (1) there is only one PE per switch
and, (2) the trafﬁc is uniformly distributed among sources and destinations and injected
at maximum rate. In this section we show how our approach can be extended to higher
concentrations, and to other trafﬁc assumptions.
Having a concentration factor C > 1 modiﬁes the distances between PEs. Supposing
a GMG shaped interconnect, PE has now (C−1) PEs at topological distance2 0, CR at
distance 1, CR(R−1) at distance 2 etc. The minimum average topological distance
between PEs must be rewritten as follows:
DGMGðC; RÞ ¼ CR þ 2CRðR  1Þ þ 3CRðR  1Þ2 þ . . . þ CðD  1ÞRðR  1ÞD2 þ CDx
N  1
ð2Þ
10
0
10
1
10
2
10
3
10
4
10
5
0
2
4
6
8
Ideal connectivity Ropt
10
0
10
1
10
2
10
3
10
4
10
5
0
2
4
6
8
Number of switches S = Number of end-points N
Ideal diameter Dopt
10
1
10
2
10
3
10
4
10
5
10
0
10
2
10
4
10
6
Ideal capacity CAPopt
a
b
c
Fig. 2. Connectivity Ropt (a), diameter Dopt (b) and total capacity CAPopt (c) of the ideal GMG
topologies of increasing sizes with concentration factor C = 1 (one end-point per switch, S = N).
A strictly linear progression is drawn in (c) for reference.
2 Topological distance refers to the number of hops achieved over the topology itself and excludes
access and egress hops. A topological distance of 0 reﬂects the situation where messages are
immediately forwarded to their ﬁnal destination after hitting the ﬁrst switch.
460
S. Rumley et al.

The total trafﬁc is NΔGMG(C,R) and the resulting capacity is now nNR/C. For the
same reasons described above, minimum capacity is guaranteed when n = 1 which
leaves us with the following inequality: ΔGMG(C,R) ≤R/C.
Larger concentrations lead to a reduced number of switches, thus to more compact
topologies with smaller average topological distance. This in turn reduces the total
trafﬁc rate, which eventually translates into weaker requirements in terms of total
capacities. Note also that N(C-1) PE pairs have topological distance 0. If the con-
centration C is equal to N, there is a single switch in the topology so no inter switch
links are required at all. One the other hand, large concentrations require the existence
of switches with a large number of ports r, as r has to be greater or equal to C + R.
Ropt, Dopt and CAPopt have been evaluated for several values of N up to 100,000,
and different concentrations. Figure 3 shows the evolution of Dopt (a) and of the
required switch radix ropt = C + Ropt (b) across this parameter space. To ease the
rendering of the ﬁgure, and since real switches often have these number of ports,
resulting ropt values are rounded up to multiples of 8 up to 48, and to multiples of 16
above 48. Figure 3 shows that if higher concentrations require higher requirements in
terms of radix, they also limit the diameter of the topology. As the diameter represents
the longest shortest-path in the topology, it is a proxy for the largest zero-load latency.
If one desires to maintain this diameter to 2 [13], while achieving ideal connectivity,
concentrations larger than 2, 14 and 29 are required for N = 100, 10,000 and 100,000
respectively.
By superimposing the data of Fig. 3a and b, one obtains the Pareto front of the
largest topologies with the smallest switch radix, for different diameters, as plotted in
Fig. 4a. To maintain a diameter 2 for N ≥100,000 nodes, 96 ports are required. This
requirement falls to 32 ports if diameter 3 is acceptable.
If both diameter and radix are crucial indicators of the performance and the tech-
nological requirement, they do not fully reﬂect the cost of the topology, in particular
the operational cost (mainly energy) which can be expected to be proportional to the
number of links. Figure 4b offers this perspective for three values of N and different
concentrations. A fundamental trade-off exists between minimizing the switch radix
and the number of links. The ﬁnal choice of the concentration hence depends on cost
Fig. 3. Dependence of the ideal diameter Dopt (a) and of the ideal practical radix ropt (b) on the
number of end-points N and the concentration factor C, when considering Generalized Moore
Graph topologies.
Design Methodology for Optimizing Optical Interconnection Networks
461

difference between low and high radix switches. There are three zones of interest.
(1) Starting from very small ropt values, increases in the ﬁrst translates to substantial
savings in terms of links. This suggests that unless links can be kept low cost at both
CAPEX and OPEX levels, radixes of 16, 24 and 32 at least should be considered for
N ≥10,000, 25,000 and 100,000 respectively. These radixes correspond to the least
connected topology with diameter 3. (2) Then follows a trade-off zone (tinted in the
ﬁgure) in which capacity savings can be traded against larger radixes, until hitting
the least connected topology of diameter 2. (3) Past this point, an increased radix has
little inﬂuence on CAPtot. This suggests that building switches offering 48, 64 or 96 can
be taken as a good target for realizing interconnects supporting 10,000, 25,000 and
100,000 PEs respectively. Under the assumptions considered, larger values will reduce
the average distance and therefore the required capacities, but not in the same
proportion.
The approach presented here can also be used to analyze interconnect requirements
under different trafﬁc assumptions. Trafﬁc predictions can be linearly reduced by
assuming that each PE emits uniform trafﬁc at a fraction of its maximum capacity. If
this fraction is noted z, the inequality becomes zΔGMG(C,R) ≤R/C. One can also
examine more subtle trafﬁc scenarios in which trafﬁc ﬂows between closely or remotely
located PEs are imbalanced. Instead of estimating the trafﬁc injected by a PE through
the average distance, one can suppose that each PEs send a fraction pi of its trafﬁc to the
other PEs located at distance i. The trafﬁc sent by each PE thus become p1 þ 2p2 þ
. . . þ ðD  1ÞpD1 þ DpD with the conditions that 0  P
i
pi  1 and that each pi is
positive. If p1 ¼ CR
N1 ; p2 ¼ CRðR1Þ
N1
; . . .pD ¼
Cx
N1, i.e. that (1/N−1) of the trafﬁc is sent
to the CR PEs located at distance 1, to the CR(R−1) ones located at distance 2, etc., the
trafﬁc is uniform at maximal rate again. Another combination of interest is the one
Fig. 4. (a) Evolution of the minimum practical radix ropt required to interconnect N end-points
for a given maximum diameter. Ensuring Dopt = 2 induce high radixes requirements as N scales.
Accepting larger diameters allows to decrease the absolute radix needs. (b) Concentration factors
can be utilized to obtain different optimal capacity/radix trade-offs. Resulting topologies
diversely populate the capacity/required radix Pareto front. Topologies with Dopt = 3 (tinted
zone) appears as interesting trade-offs.
462
S. Rumley et al.

where p0…pD−1 = 0 and pD = 1. This situation assumes that every PE sends at
maximum rate messages to its most distant peers exclusively. It is therefore the worst-
case scenario in terms of total trafﬁc magnitude (adversarial trafﬁc scenario).
Figure 5a and b show how these alternative trafﬁc assumptions modify the optimal
diameter value for various N and C. For uniform trafﬁc at 50 % load and N = 25,000,
diameter 2 topologies are equilibrated capacity wise for concentrations of 30 at least.
In presence of adversarial trafﬁc, this number falls to 19.
Figure 5c compares the switch radix/installed capacity trade-offs across the three
trafﬁc cases (50 % and 100 % uniform, adversarial) for N ≥100,000, which is in
general the maximum scale desired for Exascale computers. As pointed out in several
studies [3, 13], switches with port counts in this range are currently available, with line
rates of several tens of Gb/s. However, Exascale HPC systems will require much larger
line rates, in the range of the Tb/s [9]. In this context, the requirement of 100 port
switches will become a challenge. As shown in Fig. 5c, dimensioning the topology for
50 % of the maximum injection rate drastically diminishes the radix requirements.
If high radix switches cannot be developed for line rates of 1 Tb/s or greater, then
smaller radix ones supporting even higher rates may provide an alternative.
In this context, transparent optical switches may be an option. MEMS based
switches with more than three hundred ports are already available, however, they suffer
from low switching speed. Integrated optics-based ultra-fast switches [2], in contrast,
are fundamentally capable of sub-nanosecond switching times. Recent results indicate
that 32x32 optical switches based on silicon photonics ring resonators, supporting
20x10 Gb/s parallel (WDM) signals, are feasible without recourse to intermediate
ampliﬁcation [1].
More generally, the analysis provided here indicates that realizing a 32-port switch
capable of 1 Tb/s line rate (32 Tb/s total bandwidth) is a reasonable minimum target for
realizing an Exascale-class interconnect.
Fig. 5. Impact of the trafﬁc assumption on the ideal diameter Dopt for various N and C (a) with
z = 0.5 (50 % uniform trafﬁc) (b) for worse-case trafﬁc. (c) Capacity/radix trade-off of GMG
topologies supporting N = 100,000 end-points using various concentration factors and different
trafﬁc assumptions
Design Methodology for Optimizing Optical Interconnection Networks
463

5
Identifying Topologies Close to the Bound
As indicated above, all Ropt, CAPopt, Dopt or ropt values provided so far have been
calculated on the premise that GMGs of any size and degree can be constructed. In
reality, most of these GMG topologies either not yet been identiﬁed, or have been
proven not to exist. In this section we therefore identify a host of topologies whose
wiring is known, with adequate connectivity, and compare them to the ideal GMG
ones. At the same time, we show how the knowledge of Ropt eases this identiﬁcation
process.
In order to ﬁnd good candidate topologies to interconnect at least 25,000 PEs, able
to support maximum uniform trafﬁc, we start with an evaluation of Ropt for a range of
realistic concentrations (typically 1 to 40), and plot these values against the number of
switches corresponding to each concentration, i.e. 25; 000=C
d
e. These values form the
black line on Fig. 6.
The second step consists of evaluating the connectivity R of several known
topologies such as Flattened Butterﬂies of different dimensions. We consider the 3D-
Flattened Butterﬂy (3D-FB) as an example: switches are arranged in a 3-dimensional
lattice. If concentration C = 1 is employed, this 3D lattice should be at least of size
29x29x30 = 25,230. Therefore, each switch is connected to R3D-FB,C=1 = 28 + 28 +
29 = 85 other switches. If C = 13, the lattice is 12x13x13 = 2,038 (2,038 x 13 = 26,494)
and switches have a connectivity R3D–FB,C=13 = 11 + 12 + 12 = 35. Hence, to each
Fig. 6. Connectivity R of topologies of various types and sizes (but all supporting at least
N = 25,000 end-points) reported against their number of these switches S (left y-axis) and
concentration factor C (right y-axis). The solid curve represents the connectivity Ropt of ideal,
GMG based topologies, and also delimits the feasibility region. Dots located close to the solid
curve indicate potentially good candidate topologies.
464
S. Rumley et al.

topology type (e.g. 3D-Flattened Butterﬂy) and concentration C there corresponds a
connectivity value (for a given N). This connectivity evaluation is shown in Fig. 6 - for
2D, 3D and 4D-Flattened Butterﬂies [3] (denoted as FF, FFF and FFFF), for the largest
known graphs of diameter 2 and limited degree (denoted as B) including ones exploited
by Slim-ﬂy [13], the largest ones of diameter 3 and limited degree (C), and for a 2-
dimensional combination3 of these largest known graphs (BB, BC and CC). We also
included the connectivity of BB topologies whose links have been doubled to obtain a
higher connectivity. Toroidal interconnects have not been included in Fig. 6 in an effort
to not overload the image. If included, they would appear as vertical lines: for example,
a 5D-torus has a constant connectivity R = 10 for any number of switches. If the links
of this 5D-torus are doubled, the connectivity becomes R = 20.
All points located at the left of the Ropt curve can be excluded: they show a too
weak connectivity which will oblige one to interconnect switches with more than one
link (n > 1). In contrast, practical topologies located close to the optimality curve, but to
the right of it, i.e. the ones whose connectivity is slightly higher than the strict required
minimum, are of great interest. Obviously, as “real” topologies, one can expect them to
show a higher average distance than “virtual” GMG of similar size, but their higher
connectivity might be sufﬁcient to compensate this distance penalty.
Finally we further analyze the topologies of greatest interest (those indicated with
boxes in Fig. 6 plus several tori) by (1) constructing each topology (2) routing indi-
vidual ﬂows (using shortest-path routing only) over the topology, (3) looking for the
most congested connection between two switches, (4) rounding up this congestion
value and multiplying it by the number of edges in the topology (for multi-dimensional
topologies, we do that for each dimension separately). By this method we obtain the
capacity required in practice to secure a complete absorption of the trafﬁc. This also
allows us to determine the minimum required switch radix (rounded to multiples of 8
and 16 as previously). These values are represented on Fig. 7.
Starting from the right side of the graph, the F29F30-C29 fails to approach the
bound. The dimension of size 29 is not connected enough which obliges us to double
its links to ensure sufﬁcient capacity. This is a general problem with Flattened But-
terﬂies. They are in theory capable of any connectivity R, but unless the lattice is
equally sized in all dimensions, this total connectivity is not adequately balanced across
the dimensions. In contrast, F30F30-C28 closely approximates the bound, but requires a
large radix of 96. The Slim-ﬂy topology B1682-C15 is the next Pareto dominant data-
point. Similarly to the 2D–FB, it is a diameter two topology, but due to its close to
optimal wiring, it requires smaller radix than the 2D-FB. It lies ahead of the bound,
however. As visible on Fig. 6, there are relatively few known large graphs of diameter
2 (among them the MMS ones exploited by Slim-ﬂy) which obliged us to consider a
topology of higher connectivity than strictly required (43 instead of 35). This explains
the distance to the bound. The F13F13F13–C12 is also Pareto dominant, followed by the
B57B57–C8. Although the base graph B57 is the largest one found so far for degree 8, by
3 In the Flattened Butterﬂy topology, all vertices sharing a dimension in the lattice as interconnected
(Full-Mesh). In a torus, all these vertices are connected along a ring. In our 2-dimensional
construction, vertices sharing a dimension are interconnected by following the structure of the largest
known graph for a given diameter and maximum degree.
Design Methodology for Optimizing Optical Interconnection Networks
465

using it in two dimensions and by doubling its edge we diluted its close to optimum
properties. Still, it appears as a Pareto dominant topology. The 4D Flattened Butterﬂy
can be realized with the same radix of 40, and for a slight increase in the capacity
requirements. As routing and workload mapping is probably made easier with this
topology than with the B57B57 one, it might also be considered, although not strictly
part of the Pareto front. After the 4D–FB, the next Pareto dominant topology is the
B50B133-C3 but as it lie far from the bound, it shows little value unless large radixes
cannot be utilized. More generally, the Pareto front created by Pareto dominant
topologies progressively diverges from the bound. This reveals a need for topologies
close to the GMG bound, of diameter 3 and larger, and of diverse sizes. These
topologies are also harder to emulate with symmetric constructions as the Flattened
Butterﬂy or tori.
Two additional datapoints have been highlighted on Fig. 7. They reﬂect topologies
used in current dominant Supercomputers. R5R5R5R5R5-C8 is a 5D-torus as the one
available in Sequoia, while R2R2R3R5R5R6-C14 mimics the Tofu interconnect [38]
(6D-torus) of the K computer. R5R5R5R5R5-C8 is the best 5D-torus for N = 25,000
maximal and uniform trafﬁc. It dominates all other combinations of sizes and con-
centrations leading to realistic radixes. Still, its average distance Δ (just above 6) is
more than three times larger than ΔGMG(C = 21, R = 42) = 1.963 which would also
require a radix of 64. This directly translate to greater than 3 times the capacity
requirements (156,250, where the bound indicates 50,022). Other tori, including the
best Tofu-like 6D-torus, lie even further from the bound, and no 3D-tori appears in the
ranges of Fig. 7. This demonstrates that tori are fairly ill-suited to trafﬁc conditions
similar to uniform trafﬁc at large scales. However, as mentioned in the introduction, tori
can compensate this hindrance by providing bandwidth in a very structured way,
a feature that is generally vastly exploited by workloads.
Fig. 7. Capacity/radix trade-off of practical topologies supporting at least 25,000 PEs.
466
S. Rumley et al.

Although not directly related to our approach, fat-trees with no over-subscription
(i.e. full-bisectional bandwidth) can also be characterized by a number of links and
radix, and therefore be represented in Fig. 7. For all radixes, a fat-tree involves at least
twice as many links than the bound. They are clearly outperformed by the Pareto
optimal topologies, expect for small radixes. However, as pointed out above, other
close to ideal topologies of larger diameter (not investigated here) might exist and
outperform the fat-tree for smaller radixes, too.
6
Discussion and Future Directions
The methodology described here does not cover all facets of the topology selection
process. The hypothesis that all links are equivalent, and that trafﬁc is uniformly or
arbitrarily distributed, excludes the Dragonﬂy topology which precisely assumes
imbalanced trafﬁc ﬂows. All aspects related to embedding the topology onto back-
planes and inter-rack/cabinet cables is also neglected although this may play an
important role in the process of choosing one topology over another. The goal of this
study is, however, not to provide a ﬁnal choice of a topology but rather to deﬁne a
framework allowing us to explore the main trade-offs and minimal requirements. More
exhaustive comparisons will be realized in future publications. Still, Fig. 7, covering
Tori, Fat-trees, Flattened Butterﬂies and Slim-ﬂies of various sizes and shape, provide a
rather inclusive set of “realistic” topologies that populate current Supercomputers.
No trafﬁc simulations have been conducted on the practical topologies compared in
the previous Section. However, if such simulations would be driven with uniform
trafﬁc, they would mainly conﬁrm that no surge in latency occurs for loads <1, as all
topologies are precisely dimensioned for this aim. In contrast, simulating real work-
loads in large-scale architectures equipped with the different Pareto optimal topologies
would be of high-interest. In particular, we would expect such experiments to conﬁrm
the statement that limited diameters are highly desirable. Our future plans include
realizing such experiments.
Our approach also allows us to derive further insights on practical topologies.
In results not shown here we have found that it allows one to identify how improper
wiring, connectivity that is too large or too small, and an unfavorable number of end-
points each contribute to making a particular topology sub-optimal.
7
Conclusion
In this study, we concentrate on the relationship between trafﬁc, capacities (i.e. number
of links) and switch radixes, and do not deal with other aspects such as maintenance,
organization in racks, incremental deployment, load balancing, etc. Surprisingly, even
in this reduced scope, there is no clearly dominant recipe for building a topology. We
conclude that the choice of the topology should not be made too early in the design
process, and not on the sole assumption that one topology or another has been proved
optimal in a given context. A ﬁnal choice among Pareto dominant data points of Fig. 7
(i.e. 5−10 options) can be made after comparison under real trafﬁc with large scale
Design Methodology for Optimizing Optical Interconnection Networks
467

simulators, or based on a detailed economical analysis once the costs of links and
switches are known.
Results also show that if 2D-Flattened Butterﬂies and Slim-ﬂies, both of diameter 2,
land close to the bound, approaching the bound with larger diameters, corresponding to
tighter radix availabilities, appears trickier. Provided that wide radixes might be hard to
adapt to larger line-rates, there is a need for topologies of diameter 3 and 4, not
necessarily of largest size, but showing close to optimal average routing distance.
Similar topologies of diameter 2 would offer a good complement to the Slim-ﬂy set
which appears too scattered to adequately support the whole range of system sizes.
This study also shows that tori appear sensibly dominated by other topologies,
although they account for a vast portion of modern Supercomputer interconnects. This
suggests that for now workload structures can be mapped reasonably well over their
hardware equivalent. However, with the advent of asynchronous parallel programming
method, or adaptive workload balancing, trafﬁc proﬁles may loose their structure. In
this case, topologies as close to a GMG as possible seems the most logical choice.
Acknowledgement. This work has been realized in the context of Department of Energy (DoE)
ASCR project “Data Movement Dominates”. It has been partly supported by the U.S. Depart-
ment of Energy (DoE) National Nuclear Security Administration (NNSA) Advanced Simulation
and Computing (ASC) program through contract PO1426332 with Sandia National Laboratories.
Sandia National Laboratories is a multi-program laboratory managed and operated by Sandia
Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Depart-
ment of Energy’s National Nuclear Security Administration under contract DE-AC04-
94AL85000.
Appendix
A Generalized Moore Graph can be described as follows. Consider a vertex, V, in any
graph of degree R (i.e. whose vertices have never more than R incident links). V cannot
have more than R direct neighbors. It also cannot have more than R(R-1) neighbors at
Fig. 8. (a) Maximal expansion possibilities for connectivity/degree R = 3 and three layers.
Generalized Moore Graphs follow this structure, except that the last layer does not have to be
totally ﬁlled (b) Example of Generalized Moore Graph (a 3x3 torus) (c) The 3x3 torus
reorganized to show the layers (d) A representation of unﬁlled slots in the last layer
468
S. Rumley et al.

distance 2 (each of its neighbors have R neighbors but V does not count as it is one of
them), and generally cannot have more than R(R−1)D−1 neighbors at distance
D. A GMG is a graph which maximally uses this expansion possibilities offered by
the degree R: in a GMG graph, each vertex has exactly R direct neighbors, exactly
R(R−1)i−1 neighbors at distance i (i = 2..D−1), and all the remaining vertices are at
distance D. Figure 8 exempliﬁes the GMG concept. Because inner layers are maximally
ﬁlled, there is no way to get a vertex closer without interchanging it with another
vertex. This means that no distance between two vertices can be reduced, thus that the
average distance in the graph is minimized.
References
1. Nikolova, D., Rumley, S., Calhoun, D., Li, Q., Hendry, R., Samadi, P., Bergman, K.:
Scaling silicon photonic switch fabrics for data center interconnection networks. Opt.
Express 23(2), 1159–1175 (2015)
2. Lee, B.G., Dupuis, N., Pepeljugoski, P., Schares, L., Budd, R., Bickford, J.R., Schow, C.L.:
Silicon photonic switch fabrics in computer communications systems. IEEE Journal of
Lightwave Technology (in press)
3. Kim, J., Dally, W.J., Abts, D.: Flattened butterﬂy: a cost-efﬁcient topology for high-radix
networks. In: Proceedings of the International Symposium on Computer Architecture
(ISCA), pp. 126–137 (2007)
4. Bhatelé, A., Kalé, L.V.: Beneﬁts of topology aware mapping for mesh interconnects. Parallel
Program. Lett. 18(4), 549–566 (2008)
5. Dally, W.J.: Principles and Practices of Interconnection Networks. Morgan Kaufmann, San
Francisco (2004)
6. Leiserson, C.E.: Fat-trees: universal networks for hardware-efﬁcient supercomputing. IEEE
Trans. Comput. C-43(10), 892–901 (1985)
7. Sano,
K.:
Interconnection
Network:
Design
Space
Exploration
of
Networks
for
Supercomputers. Sustained Simulation Performance, Springer, 151–161, 2015
8. Borkar, S.: Role of interconnects in the future of computing. IEEE J. Lightwave Technol.
(JLT) 31(24), 3927–3933 (2013)
9. Rumley, S., et al.: Silicon photonics for exascale systems. IEEE J. Lightwave Technol. (JLT)
33(3), 547–562 (2015)
10. Bradonjic, M., Saniee, I., Widjaja, I.: Scaling of capacity and reliability in data center
networks. In: Proceedings of the SIGMETRICS (2014)
11. Faanes, G., et al.: Cray cascade: a scalable HPC system based on a Dragonﬂy network. In:
Proceedings of the International Conference on High Performance Computing, Networking
Storage and Analysis (SC 2012) (2012)
12. Singla, A., Hong, C.-Y., Popa, L., Godfrey, P.B.: Jellyﬁsh: networking data centers
randomly. In: Proceedings of the USENIX Symposium on Networked Systems Design and
Implementation (NSDI 2012) 2012
13. Besta, M., Hoeﬂer, T.: Slim ﬂy: a cost effective low-diameter network topology. In:
Proceedings of the International Conference on High Performance Computing, Networking
Storage and Analysis (SC 2014) (2014)
14. Preparata, F.P., Vuillemin, J.: The cube-connected cycles: a versatile network for parallel
computation. Commun. ACM 24(5), 300–309 (1981)
Design Methodology for Optimizing Optical Interconnection Networks
469

15. Bhuyan, L.N., Agrawal, D.P.: Generalized hypercube and hyperbus structures for a
computer network. IEEE Trans. Comput. C-33(4), 323–333 (1984)
16. Pease, M.C.: The indirect binary n-Cube microprocessor array. IEEE Trans. Comput. C-26
(5), 478–482 (1977)
17. Dally, W.J.: Performance analysis of k-ary n-cube interconnection networks. IEEE Trans.
Comput. 39(6), 775–785 (1990)
18. Benes, V.E.: Optical rearrangeable multistage connecting networks. Bell Syst. Tech. J. 43
(4), 1641–1656 (1964)
19. Wittie, L.D.: Communication structures for large networks of microcomputers. IEEE Trans.
Comput. C-30(4), 264–273 (1981)
20. Kim, J., Dally, W.J., Towles, B., Gupta, A.K.: Microarchitecture of a high radix router. In:
Proceedings of the International Symposium on Computer Architecture (ISCA), pp. 420–
431 (2005)
21. Scott, S., Abts, D., Kim, J., Dally, W.J.: The blackwidow high-radix clos network. In:
Proceedings of the International Symposium on Computer Architecture (ISCA), pp. 16–28
(2006)
22. Barriuso, R., Knies, A.: 108-Port InﬁniBand FDR SwitchX Switch Platform Hardware User
Manual (2014)
23. Li, K., Mu, Y., Li, K., Min, G.: Exchanged crossed cube: a novel interconnection network
for parallel computation. IEEE Trans. Parallel Distrib. Syst. (TPDS) 24(11), 2211–2219
(2013)
24. Von Conta, C.: Torus and other networks as communication networks with up to some
hundred points. IEEE Trans. Comput. 32(7), 657–666 (1983)
25. Akers, S.B., Krishnamurthy, B.: A group-theoretic model for symmetric interconnection
networks. IEEE Trans. Comput. 38(4), 555–566 (1989)
26. Hsieh, S.-Y., Hsiao, T.-T.: The k-valent graph: a new family of cayley graphs for
interconnection networks. In: Proceedings of the International Conference on Parallel
Processing (ICPP) (2004)
27. McKay, B.D., Miller, M., Sirán, J.: A note on large graphs on diameter two and given
maximum degree. J. Comb. 61, 1–63 (1998)
28. Miller, M., Sirán, J.: Moore graphs and beyond: a survey of the degree/diameter problem.
Electronic Journal of Combinatorics, Dynamic Survey D 14 (2005)
29. Boa, W.-T., et al.: A high-performance and cost-efﬁcient interconnection network for high-
density servers. J. Comput. Sci. Technol. 23(2), 281–292 (2014)
30. Samples, M.: Large networks with small diameter. In: Möhring, R.H. (ed.) WG 1997.
LNCS, vol. 1335, pp. 288–302. Springer, Heidelberg (1997)
31. Loz, E., Sirán, J.: New record graphs in the degree-diameter problem. Autralasian J. Comb.
41, 63–80 (2008)
32. Koibuchi, M., Matsutani, H., Amano, H., Hsu, D.F., Casanova, H.: A case for random
shortcut topologies for HPC interconnects. In: Proceedings of the International Symposium
on Computer Architecture (ISCA), pp. 177–188 (2012)
33. Guan, K.C., Chan, V.W.S.: Cost-efﬁcient ﬁber connection topology design for metropolitan
area WDM networks. IEEE/OSA J. Opt. Commun. Netw. 1(1), 158–175 (2009)
34. Vetter, J., et al.: Quantifying architectural requirements of contemporary extreme-scale
scientiﬁc applications. In: High Performance Computing Systems. Performance Modeling,
Benchmarking and Simulation, Springer LNCS (2014)
35. Kandula, S., Sengupta, S., Greenberg, A., Patel, P., Chaiken, R.: The nature of data center
trafﬁc: measurements and analysis. In: Proceedings of the ACM Conference on Internet
Measurement (IMC), pp. 202–208 (2009)
470
S. Rumley et al.

36. Hendry, G.: The role of optical links in HPC system interconnects. In: Proceedings of the
IEEE Optical Interconnects Conference (2013)
37. Hammond, S., et al.: Towards a standard architectural simulation framework. In:
Proceedings of the Workshop on Modeling & Simulation of Exascale Systems &
Applications (2013)
38. Ajima, Y., Inoue, T., Hiramoto, S., Shimizu, T.: Tofu: interconnect for the K computer.
Fujistu Sci. Tech. J. 48(3), 280–285 (2012)
Design Methodology for Optimizing Optical Interconnection Networks
471

Quantifying Communication in Graph Analytics
Andreea Anghel(B), German Rodriguez, Bogdan Prisacari,
Cyriel Minkenberg, and Gero Dittmann
IBM Research — Zurich, Zurich, Switzerland
{aan,rod,bpr,sil,ged}@zurich.ibm.com
Abstract. Data analytics require complex processing, often taking the
shape of parallel graph-based workloads. In ensuring a high level of eﬃ-
ciency for these applications, understanding where the bottlenecks lie
is key, particularly understanding to which extent their performance is
computation or communication-bound. In this work, we analyze a ref-
erence workload in graph-based analytics, the Graph 500 benchmark.
We conduct a wide array of tests on a high-performance computing sys-
tem, the MareNostrum III supercomputer, using a custom high-precision
proﬁling methodology. We show that the application performance is
communication-bound, with up to 80 % of the execution time being spent
enabling communication. We equally show that, with the increase in scale
and concurrency that is expected in future big data systems and appli-
cations, the importance of communication increases. Finally, we char-
acterize this representative data-analytics workload and show that the
dominating data exchange is uniform all-to-all communication, opening
avenues for workload and network optimization.
Keywords: Workload characterization · Graph 500 · Proﬁling ·
Uniform all-to-all · Graph analytics
1
Introduction
Today’s systems generate large volumes of data that need to be transferred,
stored, but most of all processed to gain insights. As an example, over the last
years, social networks have experienced an exponential growth up to as much
as one billion active users with an average of more than one hundred connec-
tions each [8]. The complex processing involved in the exploitation of these large
data sets requires the use of distributed workloads executed on massively parallel
systems. To guarantee a high level of performance for these workloads, it is essen-
tial to identify what their bottlenecks are, particularly whether their execution
is computation or communication-dominated. In the latter case, optimization is
especially of interest, as data motion is expected to become the dominant power
consumption component in future HPC systems [15]. One solution to address
this challenge is the development of mechanisms that better orchestrate the
data motion through the system [10]. However, to implement such mechanisms
it is necessary to ﬁrst understand the application communication patterns.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 472–487, 2015.
DOI: 10.1007/978-3-319-20119-1 33

Quantifying Communication in Graph Analytics
473
In the context of data analytics workloads, one particularly relevant class of
applications is graph-based analytics. Indeed, the large sets of data generated by
social networks and business analytics are often modeled as graphs that need to
be processed on large-scale distributed systems using scalable parallel algorithms.
A representative of this class is the Graph 500 benchmark suite [20], which has
been introduced to guide the design of systems envisioned to support data-
intensive applications. The benchmark is designed to assess the performance
of supercomputing systems by solving a well-deﬁned graph problem, i.e., the
breadth-ﬁrst search (BFS) graph traversal.
In this paper, we use the MareNostrum III supercomputer [2] to conduct
a thorough communication characterization of the most scalable reference MPI
implementation of Graph 500. We analyze the data exchange across processes
and the variability of the communication-to-computation ratio with the problem
size (scale, edge factor) and number of processes. To the best of our knowledge,
this is the ﬁrst study that shows the actual communication pattern of the bench-
mark, oﬀering preliminary guidance for future application or network design
optimization eﬀorts. To improve the precision of our results, we introduce an
alternative tracing methodology enabling us to minimize the tracing overhead
and adjust communication time for data dependencies.
The remainder of this paper is organized as follows. We start with providing
background information about the Graph 500 benchmark and its MPI-simple
implementation (Sect. 2). In Sect. 3 we present a selection of related work. We
continue in Sect. 4 with a brief description of the computing system and tools we
used in the benchmarking process, as well as with a description of the parameters
we used for the Graph 500 experiments. In Sect. 5 we present characterization
results representative of out-of-the-box utilization of standard proﬁling tools.
Section 6 introduces the custom application characterization methodology for
communication proﬁling and presents the experimental results. We proceed in
Sect. 7 with describing the experimental results obtained for the communication
patterns study. Finally, we summarize and discuss the main take-aways of this
paper in Sects. 8 and 9.
2
Background on Graph 500
Graph 500 is a large-scale benchmark for data-intensive applications. The prob-
lem size is given by the scale and the edge factor parameters of the input graph.
If scale is equal to V then the number of vertices equals 2V and if the edge
factor is equal to E then the graph has 2V· E edges. The benchmark implements
two main kernels: graph generation and BFS. First, the benchmark generates a
list of edges and constructs an undirected graph. The graph construction phase
uses the Kronecker graph generator which is similar to the graph generation
algorithm presented in [11]. 64 graph nodes are randomly selected and, for each
node, the BFS tree with that node as root is computed. The BFS step is validated
to ensure that the generated trees are correct. The output of the benchmark is
the time necessary to perform the BFS and the number of traversed edges per

474
A. Anghel et al.
second (TEPS). In this paper, we focus on the analysis of the BFS kernel as in
a graph analytics setting the graph itself would already exist.
Graph 500 provides four implementations of the BFS algorithm: simple,
replicated-csr, replicated-csc and one-sided. All four implementations use a level-
synchronized BFS algorithm, that is, all the vertices at a given level in the BFS
tree are all processed before any vertex in a lower level of the tree is processed [7].
For the remainder of the paper we will focus on the MPI-simple implementation,
which, despite its name, is actually the most scalable among all the reference
MPI implementations [24].
The MPI-simple version of Graph 500 implements the BFS algorithm as
follows. Each MPI process uses two queues: a current queue (CurrentQueue)
and a next queue (NextQueue). CurrentQueue hosts all the vertices that have to
be visited at the current level. NextQueue hosts all the vertices that will need to
be visited at the next level. At the end of a level, the two queues are swapped.
In addition, each MPI process uses two arrays: Predecessors (list of parents) and
Visited (to track if a vertex has already been visited or not). If an MPI process A
needs to visit a vertex that is assigned to another MPI process B, then process
A will send a message to process B (via the MPI Isend function), requesting
that process B visit that speciﬁc vertex. The information passed via this MPI
message will include the vertex to be visited, as well as the predecessor in A that
triggered the visit. For optimization, multiple such requests can be aggregated
in messages of a certain size. This coalescing size is a tunable parameter. In this
study we use the default value of 4 KB.
3
Related Work
The work presented in this paper lies at the intersection of two research topics:
(A) Graph 500 related characterization and (B) communication proﬁling tools.
3.1
Graph 500 Analysis and Characterization
Previous Graph 500-related research eﬀorts, such as [12,21,24], have imple-
mented optimized versions of the benchmark, tested them on large-scale comput-
ing systems and reported their optimization techniques and performance results.
In this paper, we do not propose yet another optimized implementation of the
benchmark, but we rather focus on an in-depth workload characterization of the
most scalable MPI reference implementation of the benchmark.
In the characterization space, Jose et al. [16] report their ﬁndings on proﬁling
the execution of the MPI-simple implementation version of Graph 500—tested
with 128 processes and a problem scale of 26 and edge factor of 16—using uniﬁed
MPI+PGAS communication runtime over InﬁniBand
TM. Excluding the synchro-
nization cycles spent in the MPI all-reduce calls, a total amount of 60 % of the
total BFS time is predicted to represent communication. However, the runtime
breakdown of one single problem size may not be suﬃcient to understand how the
communication varies across diﬀerent problem sizes and/or number of processes.

Quantifying Communication in Graph Analytics
475
Suzumura et al. [23] perform a performance evaluation study of Graph 500
on a distributed-memory supercomputer for diﬀerent types of implementations,
including MPI-simple. The study reports proﬁling results for computation, com-
munication and stall times. Even though the execution breakdowns are shown
only for the replicated-based implementations (replicated-csr and replicated-csc),
the authors expect the MPI-simple implementation to have similar performance
characteristics as replicated-csc. The results show that communication and stall
times account for less than 50 % of the total execution time, even when increasing
the number of nodes from 1 to 64.
In this paper, we analyze the breakdown of the execution time of the MPI-
simple implementation, but across multiple scale, edge factor and number of
processes values. This allows us to identify trends in the execution breakdown of
the benchmark. We show that the communication time might be underestimated
and that communication represents in some cases more than 70 % of the total
execution time. In contrast to the cited related work, we also perform an in-depth
analysis of the MPI-simple communication patterns across threads (Sect. 7).
3.2
Proﬁling Tools for Parallel Applications
Crovella et al. [14] propose a methodology for measuring and modeling sources of
overhead in parallel Fortran applications. The sources identiﬁed are load imbal-
ance, insuﬃcient parallelism, synchronization loss (deﬁned as processor cycles
spent acquiring a lock or waiting for a barrier), resource contention or communi-
cation loss. We propose a way to achieve a similar breakdown by distinguishing
within the communication loss between time spent at the destination waiting
for data to be sent by the source (which we call data dependency loss) and time
spent waiting for data already in ﬂight (actual communication loss). Further-
more, we propose an alternative methodology for quantifying these losses and
use it to perform measurements for the Graph 500 benchmark.
A ﬁrst step towards extracting characterization information from a parallel
application is the use of proﬁling tools. The information provided by these tools
is useful to break down the execution time of the application into time spent in
communication and computation. For MPI applications, a typical way to do so
is by interposing instrumentation functions between the MPI library functions
and the MPI calls of the application. Such proﬁling tools will intercept applica-
tion calls, particularly those involving communication: sends, receives, collective
operations. This allows keeping a trace, generally in memory with regular ﬂushes
to disk, of the communication activity of the application. Some tools are able
to provide more detail, such as performance counters before and after each pro-
ﬁled call, more information about the protocols and/or parameters with which
the communication took place, or the user function from which the communi-
cation primitive was called, etc. Moreover, these tools are able to proﬁle with
more or less overhead, or dynamically, re-compiling the application not being
necessary. Regardless of these particularities that diﬀerentiate the performance
of one tool over another, the operating principle of all these tools is essentially
the same. Some examples of proﬁling tools are: Vampir [17], Tau [22], the HPC

476
A. Anghel et al.
Toolkit [13], or Extrae [3], among many others. For this work we will use Extrae,
but we could have used any other tool, as we did not require any special feature
or modiﬁcation of the code of Extrae.
4
Preliminaries on Characterization
Our objective is to understand the application performance bottlenecks by pro-
ﬁling its execution. Thus, we have instrumented the Graph 500 MPI-simple
implementation and used the Extrae tool to monitor, with minimal changes to
the code, the time the application spent in: (i) in MPI asynchronous point-to-
point communication calls (calls to MPI Irecv, MPI Isend); (ii) in polling MPI
Test calls; (iii) in MPI all-reduce calls; and (iv) outside of MPI functions.
We executed the instrumented code on 4 to 64 nodes with a total of 16 to
256 concurrent processes, on the MareNostrum III supercomputer and obtained
a set of preliminary results, presented in Sect. 5. By performing a more detailed
data-dependency analysis, we obtained a second set of more detailed results,
shown in Sect. 6. Using the Paraver and Paramedir analysis tools [18] on the
resulting execution traces, we were able to (i) quantify the proportion of the
overall completion time that was spent waiting for communication to complete,
performing computation or waiting due to data dependencies, as well as (ii)
identify the application’s data exchange pattern.
4.1
Benchmarking Platform
To benchmark Graph 500 we used a large-scale supercomputing cluster,
MareNostrum III [2], currently ranked 57th in the November 2014 Top 500 list [6].
The cluster consists of more than 3,056 compute nodes and 48,896 cores. Each
compute node has two Intel SandyBridge-EP E5-26701600 20 M processors, each
with 8 cores running at 2.6 GHz, and is equipped with 32 GB of memory (DDR3-
1600). Nodes are interconnected using an InﬁniBand
TM FDR-10 network.
4.2
Graph 500 Conﬁguration
We downloaded the Graph 500 reference code [4] and compiled it on the super-
computing cluster using the Intel compiler version 13.0.1 and the Intel MPI 4.1.1
default version. The only modiﬁcation we have made to the reference code was
to insert events that mark the beginning and the end of each BFS (after the col-
lection of the time statistics). In terms of problem size, we ran the benchmark
for scales as small as 16 and as large as 26, while the edge factors used range
between 16 and 256 in successive powers of 2. We used a number of concurrent
processes ranging from 32 to 256.

Quantifying Communication in Graph Analytics
477
4.3
Tracing and Analysis Tools
To extract the MPI traces of the Graph 500 benchmark we use a light-weight
tracing tool, Extrae, formerly known as mpitrace, developed at the Barcelona
Supercomputing Center. The traces have been further processed using two
additional tools: a GUI-based trace analyzer called Paraver, and Paramedir,
a command-line analysis tool based on Paraver. The two tools can ﬁlter the
trace data and visualize time-lines, compute statistics, generate traﬃc matrices,
i.e., spatial traﬃc distributions, etc.
The tracing library Extrae is implemented as an interposition library that
intercepts the MPI library calls. The tool stores the minimum amount of informa-
tion needed to, in a later (oﬀ-line) phase, match all the communication partners
and generate a single trace ﬁle that comprises all the communication activity,
including parameters of MPI calls. The library also provides an API allowing
custom emission of user events. We employ this capability, in particular by using
Extrae’s API to mark the entry and exit to the relevant code segments of the
Graph 500 benchmark. As we are measuring application completion time, it is
important to quantify the overhead of the tracing tool. In all our experiments,
the impact of tracing on the application runtime did not exceed 15 %.
5
Standard Application Characterization
As an initial approach, we used Extrae to proﬁle all MPI calls that the Graph
500 simple implementation makes. This does not require changing the code of
the benchmark in any way. However, in the course of a complete run of the
benchmark, there is an initial graph construction phase, after which multiple
BFS computation steps (which are the ones we are interested in analyzing),
alternate with validation steps (necessary only for solution veriﬁcation and esti-
mation of the number of traversed edges per second). As such, for convenience,
we did minimal changes to the code to signal to the tracing tool the beginning
and end of actual BFS tree computations, by means of emitting custom events.
Only two events were needed, each requiring a single API call.
Using this straightforward approach, we were able to determine that during
the execution of the benchmark, the application is performing one of four types of
activities: (1) asynchronous point-to-point communication (MPI Isend or MPI
Irecv); (2) polling of pending asynchronous messages (MPI Test); (3) calls to
MPI all-reduce; (4) computation, quantiﬁed as the time spent outside of any
MPI calls.
5.1
Experimental Results
The results obtained using standard characterization are outlined in Fig. 1. The
ﬁgure shows the percentual breakdown of the application execution into each
of the activities enumerated above. The three sub-ﬁgures present the impact on
this breakdown of three main application parameters: the scale and the edge fac-
tor (which determine the problem size) and the number of concurrent processes

478
A. Anghel et al.
Fig. 1. Standard instrumentation. Single BFS execution time percentual breakdown for
varying graph scales under a constant edge factor (16) and 128 concurrent processes (a),
for varying edge factors under constant graph scale (20) and 128 concurrent processes
(b), and for varying number of concurrent processes under constant graph scale (20)
and edge factor (16) (c).
executing the application. Several insights can be extracted from these results.
First, the majority of the execution time of the application is spent in either
computation or waiting for data to arrive from other processes (polling). In gen-
eral, polling time dominates, accounting for more than 50 % of the time in all
scenarios and for more than 60 % of the time in typical scenarios (where a rea-
sonably high number of concurrent processes are employed). Second, there are
a few clear trends of breakdown evolution with the three application parame-
ters, namely: (i) as the scale increases, computation becomes more important,
(ii) as the edge factor increases, computation becomes more important, and (iii)
increasing the number of concurrent processes signiﬁcantly decreases the impor-
tance of computation. Overall, in a scenario where increasingly larger problems
would be solved by means of increasingly more computational resources, the
problem will remain highly communication-bound.
6
Custom Application Characterization
In an application which uses synchronous communication, waiting for data from
another process is typically performed via a single blocking call (either a blocking
receive or a wait), which is logged in a communication trace very eﬃciently. In
contrast, in an application using asynchronous communication, waiting for data
takes the form of polling, that is, MPI test calls that query the communication
infrastructure multiple times unsuccessfully before ultimately receiving a conﬁr-
mation of data being available. Logging every failed test in the communication
trace can lead to it becoming extremely large as well as inducing a high tracing
overhead (and thus a warped view of the application). However, particularly in
the case of a communication-bound application, it is precisely the failed tests
that convey the time spent waiting for data.

Quantifying Communication in Graph Analytics
479
Fig. 2. Three types of traces for a minimal application with two concurrent processes.
(a) shows a standard trace, (b) shows the trace where the communication is emulated
from locally stored data, and (c) shows the trace were communication is emulated from
locally stored data but the temporal data dependencies (DD) between MPI Isends and
their corresponding MPI Irecvs are enforced.
To address this issue, we imagined the following thought experiment. Let
us assume that failed tests are not logged in the trace, and instead time spent
performing them appears as being outside of any communication event. This
means that what now appears to be computation time in the trace is a mix of
actual computation and polling. Should we be able to execute the application
on the same system but ensuring ideal (zero-latency, inﬁnite-throughput) com-
munication, then whatever the completion time of this ideal run is, that would
exclusively be the actual computation. To achieve this ideal setup, we replaced
the standard MPI calls with custom calls that wrap the former and additionally
have the capability to record traﬃc and replay it at ideal speed (i.e., incurring
node-side delays such as memory copies, but not incurring any network-side
delay). This allowed us to identify the time spent performing exclusively non-
communication related computation (Fig. 2 (a) and (b)). The advantage of such
an approach is not only that the trace itself is much more eﬃciently stored and
collected, but more importantly, the actual computation is measured without
any tracing overhead, and thus estimated much more precisely.
However, with this record-replay approach, removing polling time that is
falsely registered as computation was not possible while at the same time main-
taining the data dependencies between concurrent processes. Indeed, when exe-
cuting a parallel workload, a process can be caught in a sequence of polling
phases in two main circumstances:
1. the next steps the process has to execute depend on data in ﬂight that takes
a certain amount of time to arrive due to imperfections or simply inherent
limitations of the underlying communication system;
2. the next steps the process has to execute depend on data that has not yet
been sent by the corresponding source process (labeled as DD in Fig. 2).
While the former circumstance is related to communication, the latter stems
from application inherent data-dependencies between concurrent processes.

480
A. Anghel et al.
Fig. 3. Custom instrumentation. Single BFS execution time percentual breakdown for
varying graph scales under a constant edge factor (16) and 128 concurrent processes (a),
for varying edge factors under constant graph scale (20) and 128 concurrent processes
(b), and for varying number of concurrent processes under constant graph scale (20)
and edge factor (16) (c).
To be able to quantify the actual communication time, we therefore need
to partition the waiting time into the two categories above. For every message
exchanged, the trace comprises both the source and the destination processes.
As such, it provides all the necessary dependency information to achieve this
partitioning. However, in order to be able to order the dependencies properly
in time, we used another tool, called Dimemas [9], which is capable of ingest-
ing traces captured with Extrae, replay them maintaining the semantic data
dependencies and additionally model the inter-process communication for arbi-
trary levels of bandwidth, latency and network congestion. Using this tool, we
were able to determine the application runtime in the absence of communication
delays (Fig. 2(c)). This runtime, coupled to the ideal runtime above and the real
runtime, the latter two measured on the real system, are suﬃcient to allow us to
identify with high accuracy the time the application spends in each of the fol-
lowing ﬁve types of activities: (1) asynchronous point-to-point communication;
(2) calls to MPI all-reduce; (3) actual (as opposed to apparent) communica-
tion related polling; (4) inherent data dependency related polling; (5) actual
computation.
6.1
Experimental Results
Using this custom methodology, we re-ran the experiments presented in the pre-
vious section and obtained the results illustrated in Fig. 3. First, as expected, the
results show that a signiﬁcant amount of what appeared to be computation time
was actually overhead due to tracing (particularly due to the inclusion of the
failed tests). While relative to the entire application execution, this overhead was
moderate (< 15 %), as it was accounted for practically entirely in the computa-
tion part of the trace, it made up a signiﬁcant portion of that part. Indeed, while

Quantifying Communication in Graph Analytics
481
Fig. 4. Custom instrumentation. Single BFS execution time percentual breakdown for
varying graph scales under a constant edge factor (16) and 256 concurrent processes.
the previously identiﬁed trends are still present, actual computation is in fact
approximately 40 % smaller than what the standard instrumentation estimated.
Second, data dependencies make up a non-negligible part of the execution
time, which now the custom characterization correctly identiﬁes. In terms of
trends, data dependencies seem to become percentually less important with the
increase of the scale and more important with the increase of the edge factor.
Also, as the degree of parallelism increases, their importance relative to the time
spent computing increases signiﬁcantly.
With the custom methodology and the resulting smaller traces, we are also
able to handle larger problem sizes and degrees of parallelism. Figure 4 shows
the results obtained for scales as large as 26 and 256 concurrent processes (the
edge factor was set to the standard benchmark value of 16). We can see that
the previously identiﬁed trends hold, with communication remaining by far the
dominating component, accounting for more than 80 % of the total execution
time (more than 75 % of which is spent waiting for data).
7
Communication Patterns
So far, we have shown that communication delays constitute an overwhelming
part of the application runtime. Thus, optimizing the data exchange is key to
improving the performance of graph-based analytics applications. Understand-
ing the characteristics of the data exchanges is crucial to developing eﬃcient
systems and networks that enable these optimizations. Fortunately, the tracing
and analysis tools we have at our disposal have a high enough granularity to
allow data motion characterization. Indeed, from the communication traces, we
were able to determine the amount of data that each (source, destination) task
pair exchange, in time. The resulting traﬃc-matrix heatmap for a representative
scenario (scale 20, edge factor 64 and 64 concurrent processes) for the entire dura-
tion of a single BFS is shown in Fig. 5(a). Figure 5(b) shows the distribution of
the amounts of data exchanged across all individual (source, destination) pairs.

482
A. Anghel et al.
From both illustrations, one can see that the data exchange pattern strongly
resembles uniform all-to-all communication. Indeed, the distribution has a stan-
dard deviation of only 8.6 % around the mean and is almost entirely contained
in an interval of 20 % around the mean, with a very slight positive skew.
Fig. 5. Representative single BFS computation for scale 20, edge factor 64 and 64
concurrent processes. (a) illustrates the traﬃc matrix between the processes and sug-
gests that data exchanges are approximately uniformly distributed between all possible
(source, destination) pairs. (b) shows the actual distribution of (source, destination)
pairs communication volumes across possible data amounts. The volumes are distrib-
uted approximately Gaussian around a mean of 512 KB with a standard deviation of
only 8.6 % and a slight positive skew.
While these results suggest a uniform all-to-all traﬃc pattern, they are not
suﬃcient to reach such a conclusion. A given (source, destination) pair could
indeed exchange over the execution time of the program as many bytes as any
other pair. However, the performance of the exchange and the communication
pattern itself can vary strongly depending on how this global amount of data is
aggregated into messages. Exchanging the 512 KB in numerous small messages
or a few very large messages will lead to very diﬀerent traﬃc signatures. To shed
light on this aspect, we continue the characterization by extracting a similar
heatmap (Fig. 6(a)) and distribution across communicating pairs (Fig. 6(b)) for
the average message size. This analysis shows that the variability in the case of
the message size is even smaller than in the case of the aggregated amount of data
exchanged. Indeed, across communicating pairs, the standard deviation is only
0.7 % around a mean of 3.75 KB per message. It should be noted that message
size is a direct function of the (conﬁgurable) coalescing size parameter. For this
study, we did not change the default 4 KB value. Choosing a diﬀerent value
for this parameter might have performance implications, but (for a reasonable
range) it will not impact the conclusions of the data motion characterization.
Finally, even under low variability distributions of both aggregate commu-
nication volume and message size, a third aspect must also be taken into con-
sideration when characterizing a potentially uniform all-to-all communication

Quantifying Communication in Graph Analytics
483
Fig. 6. Representative single BFS computation for scale 20, edge factor 64 and 64 con-
current processes. (a) illustrates the distribution of the size of the exchanged messages
across every possible pair of communicating tasks. The distribution is highly regular,
with an average message size 3.75 KB and an extremely small variability (standard
deviation is 0.7 %), as illustrated in detail by the histogram in (b).
pattern, and that aspect is the distribution of the data exchange in time. To
perform this analysis, we divide the entire runtime of the BFS into 20 intervals
(each interval corresponding to 5 % of the BFS execution time) For each interval,
we perform the same two analyses that we performed before for the entire run,
i.e., we compute the total and per-message communication volume and repre-
sent each resulting per-interval distribution by its mean and standard deviation.
In addition, we also look at the number of active communicator pairs per time
interval. The results are shown in Fig. 7.
Figure 7(a) illustrates the percentage of active communicating pairs per time
window. During 80 % of the execution time, all pairs are in active communication.
Figure 7(b) shows the data volume exchanged per communicating pair per time
window. During 80 % of the execution time, the amount of data per time window
exchanged by an arbitrary communicating pair is similar to that of any other
communicating pair (the standard deviation is lower than 20 % in the majority of
cases). Finally, the distribution of the size of the messages exchanged is even more
regular, as illustrated in Fig. 7(c). During 65 % of the time, there is no variability,
i.e., every source is sending exclusively 4 KB messages, while during an additional
15 % of the time the variability is very low (the standard deviation is lower
than 20 %). The remaining four windows (highlighted in red) manifest larger
deviations from what would be expected from a uniform all-to-all exchange—only
a subset of (source, destination) pairs communicate, and across that subset there
are large variations in both the amount of data exchanged and the size of the
messages used. However, we would argue that these intervals are not necessarily
indicative of periods when a diﬀerent communication pattern is taking place, but
rather signs that imperfections/limitations of the communication infrastructure
or load imbalance issues cause a limited subset of messages to experience long
end-to-end latencies. Such tail eﬀects can subsequently impact the application
globally, leading to increased completion time and low network utilization.

484
A. Anghel et al.
Fig. 7. Interval analysis of the communication pattern of a single BFS (scale 20, edge
factor 64, 64 processes). The X axis represents the execution of the BFS in percentages.
For every 5 % of the execution time, we isolate the messages that are sent in that time
window. For every time window: (a) shows the fraction of all (source, destination) pairs
that communicate in that interval; (b) illustrates the mean and standard deviation of
the amount of data exchanged in the interval across (source, destination) pairs; (c)
shows the mean and standard deviation of the average message size exchanged in the
interval across (source, destination) pairs. For 80 % of the intervals, the communication
has the characteristics of a uniform all-to-all exchange.
In summary, these results suggest that system or network designs and
optimizations targeting high-performance uniform all-to-all traﬃc have a high
potential of positively impacting the communication performance of data ana-
lytics applications, and, consequently, the overall high performance of these
communication-bound workloads.
8
Discussion
The main take-away from this work is that graph analytics workloads for which
the Graph 500 benchmark is representative are communication dominated.
Indeed, we have shown that the vast majority (more than 75 %) of the execution
time is spent in polling operations that represent waiting periods for messages
in ﬂight. This means that improvements in the messaging infrastructure (such
as network bandwidth and latency or workload speciﬁc routing or process to
node allocation) will translate directly into a proportional decrease of what we
identiﬁed as polling. In turn, the application itself will beneﬁt greatly.
In performing this characterization, we also addressed several issues that a
tracing tool will encounter when proﬁling applications using asynchronous com-
munication. These include minimizing the tracing overhead, reducing the trace
size by avoiding the storing of failed MPI test events and perhaps most impor-
tantly by pinpointing polling time due to data dependencies. Indeed, to make
sure that we do not erroneously account in the communication time inherent
synchronization waiting periods between applications (time spent in one process
waiting for a message that another process has not yet sent), we carefully iso-
lated this portion of execution time in the form of a data dependency time.

Quantifying Communication in Graph Analytics
485
While for the purposes of this study we made use of the Extrae tracing tool, the
conclusions are not characteristic of Extrae alone, but rather of the entire class
of tracing/proﬁling tools, e.g., Vampir [17], Tau [22], the HPC Toolkit [13].
Concerning the choice of the Graph 500 benchmark as representative for the
graph analytics space, we are aware that other frameworks and implementations
exist. Some of the notable examples are Giraph [1] and Graphlab [19]. Giraph
is an iterative graph processing framework built on top of Apache Hadoop. It is
written mostly in Java and uses sockets for communication. GraphLab is a dis-
tributed framework that allows easy implementation of asynchronous, dynamic
and graph-parallel algorithms, built on top of an RPC library. We have nonethe-
less, for the purposes of this paper, limited our analysis to Graph 500, because
our current tools are implemented on top of MPI and as such are incompatible
with the other two frameworks. As future work, we are looking into adapting the
same methodology for TCP socket communication as well as extending it beyond
the data analytics space by applying it to other relevant HPC benchmarks, e.g.,
SPEC MPI2007 [5].
Last but not least, within the Graph 500 benchmark, several implementations
are supplied, but we have chosen to focus on the MPI-simple implementation.
This is because, while the other implementations are expected to behave better
than the simple approach in a small-scale environment with few nodes, they do
not (as shown for example by Ueno et al. [25]) scale to the system sizes that are
now usual in practice in datacenter and HPC systems.
9
Conclusions
The goal of this work was to perform an in-depth characterization and data
motion analysis of a representative application in the graph-based analytics
space. To this end, we chose to analyze the Graph 500 benchmark suite, which
is widely considered a key application in assessing the performance of super-
computing systems on data-intensive applications. In particular, we focused on
the most scalable implementation of the reference benchmark, namely the MPI-
simple code.
Initial attempts of characterization using standard proﬁling exposed several
limitations, mainly a high spatial and temporal overhead and a lack of support
for data dependencies. Using a custom approach, we addressed these issues and
were able to target larger problem sizes and degrees of parallelism (up to scale
26 and 256 concurrent processes), while improving the accuracy of the charac-
terization. We were able to quantify the time spent in enabling communication
as making up to 80 % of the total execution time of each BFS iteration, clearly
indicating the main performance bottleneck of this application. Furthermore, we
quantiﬁed how the characterization changes with the problem size and the num-
ber of concurrent processes. Most importantly we identiﬁed that communication
becomes less dominant with the increase of the scale, but signiﬁcantly more
dominant as more computational resources (concurrent processes) are assigned
to the application. Moreover, we managed to separately quantify waiting times

486
A. Anghel et al.
due to data dependencies and show that they can become important for the high
levels of parallelism characteristic of HPC systems. Last but not least, analyzing
the spatial and temporal distribution of the data exchange, we identiﬁed that
the dominating communication pattern of the benchmark is uniform all-to-all,
opening avenues for further workload-speciﬁc data motion optimization of the
Graph 500 benchmark.
Acknowledgments. This work is partially conducted in the context of the joint
ASTRON and IBM DOME project and is funded by the Netherlands Organisation for
Scientiﬁc Research (NWO), the Dutch Ministry of EL&I, and the Province of Drenthe.
We would like to thank the Barcelona Supercomputing Center for providing support
and access to the MareNostrum III supercomputing cluster.
References
1. Apache Giraph. http://giraph.apache.org/
2. Barcelona Supercomputing Center (BSC) Marenostrum supercomputer. http://
www.bsc.es/marenostrum-support-services/mn3
3. Extrae instrumentation package. http://www.bsc.es/computer-sciences/extrae
4. Graph 500 benchmark. http://www.graph500.org/
5. SPEC MPI2007. https://www.spec.org/mpi/
6. Top 500 list, November 2014. http://www.top500.org/list/2014/11/. Accessed 10
February 2015
7. Agarwal, V., Petrini, F., Pasetto, D., Bader, D.A.: Scalable graph exploration
on multicore processors. In: Proceedings of the 2010 ACM/IEEE International
Conference for High Performance Computing, Networking, Storage and Analysis,
SC 2010, pp. 1–11. IEEE Computer Society, Washington, DC (2010). http://dx.
doi.org/10.1109/SC.2010.46
8. Bader, D., Riedy, J., Meyerhenke, H.: Applications and challenges in large-scale
graph analysis. In: HPC Graph Analytics Workshop (2013)
9. Badia, R.M., Labarta, J., Gimenez, J., Escale, F.: DIMEMAS: predicting MPI
applications behavior in grid environments. In: Workshop on Grid Applications
and Programming Tools (GGF8), vol. 86, pp. 52–62 (2003)
10. Borkar, S., Chien, A.: The future of microprocessors. Commun. ACM 54(5), 67–77
(2011)
11. Chakrabarti, D., Zhan, Y., Faloutsos, C.: R-MAT: A recursive model for graph
mining. SIAM (2004)
12. Checconi, F., Petrini, F.: Massive data analytics: the graph 500 on IBM blue
gene/Q. IBM J. Res. Dev. 57(1/2), 10 (2013)
13. Chung, I.H., Walkup, R.E., Wen, H.F., Yu, H.: MPI performance analysis tools on
blue gene/L. In: Proceedings of the 2006 ACM/IEEE Conference on Supercom-
puting, SC 2006. ACM, New York (2006). http://doi.acm.org/10.1145/1188455.
1188583
14. Crovella, M.E., LeBlanc, T.J.: Parallel performance prediction using lost cycles
analysis. In: Proceedings of the 1994 ACM/IEEE Conference on Supercomputing,
Supercomputing 1994, pp. 600–609. IEEE Computer Society Press, Los Alamitos
(1994)

Quantifying Communication in Graph Analytics
487
15. Dally, B.: Power, programmability, and granularity: the challenges of exascale com-
puting. In: IEEE Parallel & Distributed Processing Symposium, pp. 878–878 (2011)
16. Jose, J., Potluri, S., Tomko, K., Panda, D.K.: Designing scalable graph500 bench-
mark with hybrid MPI+OpenSHMEM programming models. In: Kunkel, J.M.,
Ludwig, T., Meuer, H.W. (eds.) ISC 2013. LNCS, vol. 7905, pp. 109–124. Springer,
Heidelberg (2013). http://dx.doi.org/10.1007/978-3-642-38750-0 9
17. Knpfer,
A.,
et
al.:
The
vampir
performance
analysis
tool-set.
In:
Resch,
M.,
Keller,
R.,
Himmler,
V.,
Krammer,
B.,
Schulz,
A.
(eds.)
Tools
for
High
Performance
Computing,
pp.
139–155.
Springer,
Heidelberg
(2008).
http://dx.doi.org/10.1007/978-3-540-68564-7 9
18. Labarta, J., Girona, S., Pillet, V., Cortes, T., Gregoris, L.: DiP: a parallel pro-
gram. In: Fraigniaud, P., Mignotte, A., Robert, Y., Boug´e, L. (eds.) Euro-Par
1996. LNCS, vol. 1124, pp. 665–674. Springer, London (1996)
19. Low, Y., Bickson, D., Gonzalez, J., Guestrin, C., Kyrola, A., Hellerstein,
J.M.:
Distributed
graphlab:
a
framework
for
machine
learning
and
data
mining
in
the
cloud.
Proc.
VLDB
Endow.
5(8),
716–727
(2012).
http://dx.doi.org/10.14778/2212351.2212354
20. Murphy, R.C., Wheeler, K., Barrett, B., Ang, J.: Introducing the Graph 500. Cray
Users Group (CUG) (2010)
21. Satish, N., Kim, C., Chhugani, J., Dubey, P.: Large-scale energy-eﬃcient graph
traversal: a path to eﬃcient data-intensive supercomputing. In: Proceedings of the
International Conference on High Performance Computing, Networking, Storage
and Analysis, SC 2012, pp. 14:1–14:11 (2012)
22. Shende, S.S., Malony, A.D.: The Tau parallel performance system. Int. J. High
Perform. Comput. Appl. 20(2), 287–311 (2006). http://dx.doi.org/10.1177/109434
2006064482
23. Suzumura, T., Ueno, K., Sato, H., Fujisawa, K., Matsuoka, S.: Performance char-
acteristics of Graph500 on large-scale distributed environment. In: Proceedings of
the 2011 IEEE International Symposium on Workload Characterization, IISWC
2011, pp. 149–158 (2011)
24. Ueno, K., Suzumura, T.: 2D partitioning based graph search for the Graph500
benchmark. In: Proceedings of the 2012 IEEE 26th International Parallel and Dis-
tributed Processing Symposium Workshops & PhD Forum, IPDPSW 2012, pp.
1925–1931 (2012)
25. Ueno, K., Suzumura, T.: Highly scalable graph search for the graph500 benchmark.
In: Proceedings of the 21st International Symposium on High-Performance Parallel
and Distributed Computing, HPDC 2012, pp. 149–160. ACM, New York (2012).
http://doi.acm.org/10.1145/2287076.2287104

Formal Metrics for Large-Scale Parallel
Performance
Kenneth Moreland(B) and Ron Oldﬁeld
Sandia National Laboratories, Albuquerque, NM 87185, USA
{kmorel,raoldfi}@sandia.gov
Abstract. Performance measurement of parallel algorithms is well stud-
ied and well understood. However, a ﬂaw in traditional performance met-
rics is that they rely on comparisons to serial performance with the same
input. This comparison is convenient for theoretical complexity analysis
but impossible to perform in large-scale empirical studies with data sizes
far too large to run on a single serial computer. Consequently, scaling
studies currently rely on ad hoc methods that, although eﬀective, have no
grounded mathematical models. In this position paper we advocate using
a rate-based model that has a concrete meaning relative to speedup and
eﬃciency and that can be used to unify strong and weak scaling studies.
1
Introduction
Empirical scaling studies are an important component in the analysis of parallel
algorithms and systems. A scaling study tests the performance of a parallel algo-
rithm using diﬀerent numbers of processing elements and usually over diﬀerent
amounts of data. A good scaling study shows how eﬀectively additional process-
ing elements are used by the algorithm and through trends provides evidence of
behavior at future larger scales.
The practice of measuring scaling relies on the well studied models for the the-
ory of parallel performance. However, current theoretic models rely on compar-
isons with algorithm behavior on a single, serial processing element. Empirically
measuring serial behavior for suﬃciently large parallel problems is impractical.
1.1
Performance Analysis Theory
The following is a brief overview of parallel algorithm performance.
The speedup of a parallel algorithm is deﬁned as
S(n, p) = T ∗(n)
T(n, p)
(1)
where T(n, p) is the time it takes to run the parallel algorithm on p processing
elements with an input of size n, and T ∗(n) the time for the best serial algorithm
on the same input. The best possible serial algorithm may be diﬀerent than the
parallel algorithm although using the same algorithm is also common practice.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 488–496, 2015.
DOI: 10.1007/978-3-319-20119-1 34

Formal Metrics for Large-Scale Parallel Performance
489
In theory the best possible speedup achievable is S(n, p) = p [5] (although
superlinear measurements can occur in practice [8]). Thus, we measure the eﬃ-
ciency as the ratio of the observed speedup to the ideal speedup.
E(n, p) = S(n, p)
p
=
T ∗(n)
p T(n, p)
(2)
Amdahl [1] famously observes the limits of scaling any parallel algorithm
based on the fraction f of inherently serial computation that exists in any algo-
rithm. The equation derived from this observation is known as Amdahl’s law.
S(n, p) ≤
1
f + (1 −f)/p
(3)
Gustafson [7] observes that the serial fraction tends to go down for larger
data sizes in parallel algorithms, which justiﬁes the use of parallel computing for
large problems. The Gustafson-Barsis law reformulates speedup in terms of the
parallel execution rather than the serial execution.
S(n, p) ≤p + (1 −p) · s(n, p)
(4)
where s(n, p) is the fraction of time in the parallel execution performing sequen-
tial operations. This law shows that speedup can be increased indeﬁnitely as
long as the serial fraction drops commensurately with the processing element
increase, which can often be done by increasing the problem size. Grama et al.
[6] introduce an isoeﬃciency relation that determines how much a problem needs
to grow to maintain a desired level of eﬃciency. Given a desired eﬃciency Ed,
the following inequality must hold.
T(n, 1) ≥
Ed
1 −Ed
To(n, p)
(5)
where To(n, p) is the total overhead (redundant, idle, and extra computation plus
communication) for running the algorithm on p processing elements for data of
size n.
Performance analysis theory is reviewed in much more detail in many parallel
computing textbooks such as Quinn’s [14].
1.2
Limitations of Performance Analysis
Although our deﬁnition for speedup and its derived quantities work well for the-
oretical complexity analysis, they all rely in some way on knowing the serial
performance. With large-scale simulations today reaching orders of billions to
trillions of elements [2,3,9,15], directly measuring serial performance is often
impossible. The Gustafson-Barsis law needs only the serial fraction, but esti-
mates for serial fraction such as the Karp-Flatt metric [12] require knowing the
serial performance anyway.

490
K. Moreland and R. Oldﬁeld
Because of this issue, most studies attempt to assess scalability with a pair of
trends named strong scalability and weak scalability [11]. Strong scaling demon-
strates an algorithm’s behavior by measuring its run time on a particular data
set for various numbers of processing elements. Perfect strong scaling has a run-
ning time proportional to 1/p. As we shall see in examples later, it is diﬃcult to
compare the quality of plotted curves to this perfect hyperbolic on both linear
and log scales.
Per Amdahl’s law, there are inevitable limits to strong scaling. In contrast
weak scaling varies the problem size proportionally with the number of process-
ing elements. It may not always be possible to keep the problem size proportional
to the job size, which makes weak scaling more diﬃcult. The metrics advocated
in this paper simplify studying scalability by removing the dependence between
problem size and number of processing elements. We encourage using them to
sample the 2D parameter space of problem size and number of processing ele-
ments as widely as possible for a broader view of the scalability.
Some studies use an ad hoc version of speedup or eﬃciency that replaces
the immeasurable T ∗(n) with some arbitrarily chosen measurement, usually the
time run on the smallest number of processing elements. The problem with this
approach is that the absolute meaning of “speedup” and “eﬃciency” changes
between experiments in a study. Furthermore, the metric cannot be used in
weak scaling because the problem size is not held constant.
Finally, some studies use rate in terms of the size of input computed per unit
time rather than absolute run time to assess scalability [11]. Rate is formally
deﬁned as
R(n, p) =
n
T(n, p)
(6)
Some analysts have discovered that rate, being essentially a reciprocal of time,
provides a much better visual analysis of scaling, and it is an essential mechanism
advocated in this position paper.
This paper establishes a more pragmatic deﬁnition and eﬃciency that can
be easily measured empirically. Furthermore, we demonstrate how rate can be
used as a proxy for speedup and can unify strong and weak scaling to provide a
more complete analysis. These metrics are demonstrated using real performance
data.
2
Deriving Eﬃciency from Cost Analysis
In this section we will use cost, a metric that is simple to measure, to deﬁne
eﬃciency in lieu of the immeasurable speedup. Cost is intuitively the number of
processing elements used multiplied by the amount of time they are used.
C(n, p) = p T(n, p)
(7)
Cost is sometimes used in theoretical algorithm analysis [10] and is often used
for HPC allocations, which are typically measured in core-hours.

Formal Metrics for Large-Scale Parallel Performance
491
Clearly the most eﬃcient algorithm will be the one that costs the least to
run. Although we expect the cost to go up with the problem size, a perfectly
scaled algorithm on a ﬁxed input size will cost the same regardless of how many
processors are used. That is, adding processors reduces the time proportionally.
Given a strong scaling study on a problem of a particular size, we can identify
the best (minimal) cost, C∗(n), that uses p∗processing elements. With this best
cost we can redeﬁne eﬃciency as the ratio of this best cost to the actual cost.
E(n, p) = C∗(n)
C(n, p)
(8)
If we make the typical assumptions that the minimal cost is when the ser-
ial algorithm is run (C∗(n) = T ∗(n)), then we observe that Eq. 8 simpliﬁes to
Eq. 2, making this deﬁnition of eﬃciency equivalent but broader than the tradi-
tional deﬁnition. And unlike the traditional deﬁnition of eﬃciency, determining
eﬃciency from cost is straightforward at large scales.
3
Strong and Weak Scaling with Cost per Unit
Our previous deﬁnition of eﬃciency (Eq. 8) works well for strong scaling where
the data size is constant but cannot be compared across diﬀerent data sizes for
weak scaling. To describe eﬃciency under weak scaling we deﬁne the new metric
cost per unit, Cu. Cost per unit is the amortized computational cost for one unit
of data.
Cu(n, p) = C(n, p)
n
= p T(n, p)
n
=
p
R(n, p)
(9)
The important feature of cost per unit is that under perfect scaling the cost
per unit is constant under any number of processing elements or data sizes.
Thus, given multiple strong scaling studies over data of diﬀerent sizes, we can
ﬁnd the best cost per unit, C∗
u, that uses p∗processing elements operating on
data of size n∗. The best cost per unit is generally the minimum. We are not
considering, however, outlier experiments where data sizes are not representative
of practical runs. For example, it is well known that a suﬃciently small data size
could ﬁt in the cache of a suﬃciently large parallel job [8]. An experiment of this
nature would report a much lower cost per unit but would be of little relevance
to the scale of problems run in practice and therefore should be disqualiﬁed from
ideal.
With this best cost per unit we can adjust the eﬃciency to be comparable
across all possible conﬁgurations.
E(n, p) =
C∗
u
Cu(n, p)
(10)
This this deﬁnition of eﬃciency allows us to combine strong and weak scaling
studies into one uniﬁed analysis.

492
K. Moreland and R. Oldﬁeld
4
Rate as a Proxy for Speedup
Both eﬃciency and speedup are good metrics for parallel performance analysis.
However, many analysts prefer using speedup, particularly for visual (chart)
analysis. This is because good scaling shows an upward sloping speedup as jobs
get larger whereas even a good scaling algorithm will show a gradual drop-oﬀ
from a perfect eﬃciency of 1.
Although there is no way to compute the speedup at large scales, we can
show that rate (Eq. 6) is a valid proxy for speedup. If we substitute rate for time
in Eq. 1, we get the following.
S(n, p) = T ∗(n)
T(n, p) = T ∗(n)
n
R(n, p)
(11)
We can observe that for a given problem size (i.e. n held constant) the
speedup is proportional to the rate. This means that the rate curve will have
the exact same shape as the speedup curve, and visually they will be identical
with the appropriate scaling of the ordinate axis.
For a proper parallel performance analysis we need to compare our measured
metrics with the ideal metrics. These ideal values are implicit in the deﬁnition
of eﬃciency (Eideal(n, p) = 1) and speedup (Sideal(n, p) = p). The ideal rate can
be derived from Eq. 10 by substituting the cost per unit with the rate (Eq. 9)
and solving for rate when the eﬃciency is the optimal value of 1.
Rideal(n, p) = p
C∗u
(12)
Note that the curve for Rideal(n, p) is independent of n, which means we
can use the same ideal rate for both strong and weak scaling analysis and can
compare these rates with each other.
5
Examples
In the previous sections we provide mathematical derivations to show how to use
rate as a proxy for speedup and to use cost per unit to ﬁnd the eﬃciency and
ideal rate across all scales. In this section we demonstrate using these metrics
on real measured data. We can observe that the metrics of rate and eﬃciency
make it easier to visually identify the behavior and trends at diﬀerent scales of
processing elements.
5.1
Gordon Bell Finalist
Our ﬁrst data set comes from a study by Habib et al. [9], which is one of the
2013 Gordon Bell ﬁnalists. We choose this source because in addition to showing
impressive scaling, the authors make many measurements across many scales
and report the results completely enough to extract the information and continue

Formal Metrics for Large-Scale Parallel Performance
493
analysis. In particular, we look at the performance data for scaling the full HACC
code on Titan (Sect. 4.3.2 in the original paper).
Figure 1 on the left shows the performance data from the strong and weak
scaling studies using a traditional time plot. The curves for the data are very
similar for what we would expect for perfect scaling: a hyperbolic curve for strong
scaling and a horizontal line for weak scaling. Habib et al. also provide an ad hoc
metric of time over data size to unify the curve shape of the two plots, which is
also replicated in Fig. 1 on the right. Again, both curves appear close to perfect.
Fig. 1. Data from the Habib et al. [9] Gordon Bell ﬁnalist. The left chart shows the
data using a traditional time metric. The right chart replicates the presentation of
Fig. 3 in the original paper using an ad hoc metric and a log-log scale. Both charts
present the data in a way to suggest near perfect scaling for both scaling studies.
Fig. 2. Data from the Habib et al. [9] Gordon Bell ﬁnalist using the rate (left) and
eﬃciency (right) metrics advocated in this paper. These plots give a more realistic and
visually measurable representation of scaling than the charts in Fig. 1.
Figure 2 shows the same data using the rate and eﬃciency metrics advocated
in this paper. The weak scaling is shown to diverge from ideal by a measurable

494
K. Moreland and R. Oldﬁeld
fraction, which is to be expected when scaling over 3 orders of magnitude. The
strong scaling study is shown to diverge very far from ideal, which is not at all
apparent in the original charts.
5.2
Imperfect Scaling
Our second data set comes from a study by Oldﬁeld et al. [13]. In this study a
visualization algorithm has a high communication overhead and therefore has a
known limit on its scalability. The study shows how transferring data between
parallel jobs of diﬀerent sizes can sometimes be faster than combining both in
one large job.
Fig. 3. Data from the Oldﬁeld et al. [13] simulation-visualization integration study
where the visualization (shown here) has a high communication overhead. These plots
use the traditional method of showing the trend of run time using linear and log-log
scaling.
Fig. 4. Data from Oldﬁeld et al. [13] using rate (left) and eﬃciency (right) to reveal
that the communication overhead has a severe impact on the overall scalability.

Formal Metrics for Large-Scale Parallel Performance
495
A traditional plot of the time of the visualization component shown in Fig. 3
gives curves that suggest good scaling performance. However, when we show the
same data using the rate and eﬃciency metrics, shown in Fig. 4, we can clearly
see the eﬀect the communication overhead has on the scalability of the algorithm.
Without such a presentation, erroneous interpretation of the performance is sure
to occur.
6
Discussion
In this paper we discuss limitations of traditional parallel performance analysis
and problems with the current metrics often used. We provide derivations of rate
as a proxy for speedup, ideal rate, and eﬃciency and demonstrate with real data
how these provide accurate visual representations of scalability. As scientists,
we should demand this high level of transparency and honesty in performance
analysis.
With these observations, we provide the following recommendations for the
visual display of parallel performance.
– Do not rely on running time for performance analysis. Instead use rate, eﬃ-
ciency, or both.
– Avoid using log-log scaling on plot axes, which hides major ineﬃciencies. If
necessary, repeat linear plots at diﬀerent scales.
– Rather than performing separate weak and strong scaling studies, incorporate
them in one. Perform several strong scaling studies at diﬀerent scales of data
size. Then ﬁnd an overall minimal practical cost per unit and plot all the
measurements together as demonstrated in the ﬁgures in this paper.
As architectures continue to advance, many are beginning to advocate measuring
electrical power instead of or in addition to speed [4]. As future work we advocate
using a cost model for this as well. It would be interesting to derive eﬃciency in
terms of optimal watt-hours per data unit (although rate may lose meaning).
The use of rate or eﬃciency to measure parallel performance is not itself a
new technique. After all, the ubiquitous “FLOPS” measurement is itself a rate,
and measurements given in data unit per time unit can be found throughout
the literature. However, other metrics of varying eﬀectiveness are also found in
published material with little or no justiﬁcation for the choice. The decision for
scaling metrics should not be arbitrary; it can have enormous impact on the
viability of the analysis. Our intention is to show that the metric visualized does
matter, to provide a best practices for measuring scalability, and to explain why
these metrics work better than others.
Acknowledgment. This material is based in part upon work supported by the U.S.
Department of Energy, Oﬃce of Science, Oﬃce of Advanced Scientiﬁc Computing
Research, Scientiﬁc Discovery through Advanced Computing (SciDAC) program under
Award Number 12-015215.

496
K. Moreland and R. Oldﬁeld
Sandia National Laboratories is a multi-program laboratory managed and operated
by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation,
for the U.S. Department of Energy’s National Nuclear Security Administration under
contract DE-AC04-94AL85000.
SAND 2015-2890 C
References
1. Amdahl, G.M.: Validity of the single processor approach to achieving large scale
computing capabilities. In: Proceedings of the AFIPS 1967, pp. 483–485, April
1967. doi:10.1145/1465482.1465560
2. Bernaschi, M., Bisson, M., Fatica, M., Melchionna, S.: 20 Petaﬂops simulation
of proteins suspensions in crowding conditions. In: Proceedings of the SC 2013,
November 2013. doi:10.1145/2503210.2504563
3. Bussmann, M., et al.: Radiative signatures of the relativistic Kelvin-Helmholtz
instability. In: Proceedings of the SC 2013, November 2013. doi:10.1145/2503210.
2504564
4. Cameron, K.W., Ge, R.: Generalizing Amdahl’s law for power and energy. IEEE
Comput. 45(3), 75–77 (2012). doi:10.1109/MC.2012.92
5. Faber, V., Lubeck, O.M., White Jr., A.B.: Superlinear speedup of an eﬃcient
sequential algorithm is not possible. Parallel Comput. 3(3), 259–260 (1986). doi:10.
1016/0167-8191(86)90024-4
6. Grama, A.Y., Gupta, A., Kuma, V.: Isoeﬃciency: measuring the scalability of
parallel algorithms and architectures. IEEE Parallel Distrib. Technol.: Syst. Appl.
1(3), 12–21 (1993). doi:10.1109/88.242438
7. Gustafson, J.L.: Reevaluating Amdahl’s law. Commun. ACM 31(5), 532–533
(1988). doi:10.1145/42411.42415
8. Gustafson, J.L.: Fixed time, tiered memory, and superlinear speedup. In: Proceed-
ings of the Fifth Distributed Memory Computing Conference, pp. 1255–1260 April
1990. doi:10.1109/DMCC.1990.556383
9. Habib, S., et al.: HACC: Extreme scaling and performance across diverse archi-
tectures. In: Proceedings of the SC 2013, November 2013. doi:10.1145/2503210.
2504566
10. J´aJ´a, J.: An Introduction to Parallel Algorithms. Addison Wesley, Boston (1992).
ISBN 0-201-54856-9
11. Kaminsky, A.: Big CPU, Big Data: Solving the World’s Toughest Computational
Problems with Parallel Computing. Unpublished manuscript (2015), retrieved from
http://www.cs.rit.edu/ark/bcbd
12. Karp, A.H., Flatt, H.P.: Measuring parallel processor performance. Commun. ACM
33(5), 539–543 (1990). doi:10.1145/78607.78614
13. Oldﬁeld, R.A., Moreland, K., Fabian, N., Rogers, D.: Evaluation of methods to
integrate analysis into a large-scale shock physics code. In: Proceedings of the ICS
2014, pp. 83–92. June 2014. doi:10.1145/2597652.2597668
14. Quinn, M.J.: Parallel Programming in C with MPI and OpenMP. McGraw-Hill,
New York (2004). ISBN 978-0-07-282256-4
15. Rossinelli, D., et al.: 11 PFLOP/s simulations of cloud cavitation collapse. In:
Proceedings of the SC 2013, November 2013. doi:10.1145/2503210.2504565

Hunting Down Load Imbalance:
A Moving Target
Christoph Pospiech(B)
Lenovo Deutschland GmbH, Stuttgart, Germany
cpospiech@lenovo.com
Abstract. Load imbalance is known to be a major bottleneck to scala-
bility, particularly when aiming for large parallel partitions. Apart from
re-balancing the distribution of the input data, several cures have been
proposed. Pretty much all of them assume that the load imbalance is
coming from a ﬁxed source. This paper presents an investigation for the
climate modelling version CCLM of the weather forecast code COSMO
[4]. An adapted MPI trace library is used to collect information about
the load imbalance thus introducing a load imbalance measure. Using
the visualization software OpenDX [6], this information is correlated to
geography and weather forecast results. The resulting pictures show that
the locations of high computational load move in space and time. They
appear to be correlated to some weather phenomena. In principle this
correlation is known for many years [8], but now it has been made visible.
Keywords: Parallel computing · Load imbalance · Scalability · Weather
forecast model
1
Introduction
Load imbalance is deﬁned in [2, p. 3] as “processor cycles spent idling while
unﬁnished parallel work exists.” As parallel speed up results from distributing a
work load onto several processors and idling processors don’t contribute to the
solution, any load imbalance will necessarily prolong the time to solution. To pin
down the exact nature of this prolongation, we use a mathematical formalization
to reveal a close similarity to Gustavsons formulation [5, p. 532] of Amdahls law
[1]. The similarity is particularly close for the special case of a “spiked load
imbalance”.
The “spiked load imbalance” might be an instructive example, but the source
for the load imbalance is very localized and stationary. Switching to a “real
life” example of the climate modelling version CCLM of the weather forecast
code COSMO [4], we can demonstrate that the sources for load imbalance are
moving around. Using the visualization software OpenDX [6], this information is
correlated to geography and weather forecast results. This is done by color coding
wind speed arrows red or blue depending on the amount of load imbalance. The
resulting pictures show that the locations of the red wind speed arrows move in
space and time. They are not only gone by the wind, but seem to follow areas
of rain or snow fall.
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 497–505, 2015.
DOI: 10.1007/978-3-319-20119-1 35

498
C. Pospiech
2
General Load Imbalance Considerations
2.1
Deﬁnitions
We have a parallel application based on ﬁnite grids or ﬁnite elements in mind,
but are abstracting from that by deﬁning (atomic) work load entities W =
{0, 1, . . . , n −1}. Each entity has an associated time ti, i ∈W. Similarly we
abstract from processors, (SMT) threads or places in a partitioned global address
space (PGAS) by deﬁning N > 0 parallel entities P = {0, 1, . . . , N −1}.
The Workload W is parallelized by deﬁning a map f : W →P. The inverse
image Wj := f −1(j) for any j ∈f(W) denotes the work load entities handled by
the parallel entity j ∈P. This deﬁnes a split of W into pairwise disjoint subsets
Wj. The work load entities in Wj are executed serially by the same parallel
entity. Hence the time for executing the work entities in Wj is Tj deﬁned as
Tj(f) :=

i∈Wj
ti
All these Wj are executed in parallel. Hence the total elapsed time T(f) for map
f can be deﬁned as
T(f) := max
j∈f(W ) Tj(f) = max
j∈f(W )

i∈Wj
ti
We are interested in the scalability, i.e. how T(fN) changes with fN, if we increase
the size N of P.
2.2
Special Cases
As a special case, we have the serial execution, characterized by the map f0 :
W →{0}. In this case we have the following.
T0 := T(f0) = max
j∈{0} Tj(f0) =

i∈W
ti
Another less trivial special case can be called “spiked load imbalance”. It is
described by the following assumptions.
– There are only two diﬀerent time values t0 > t1. Without loss of generality,
the index 0 is chosen for t0.
ti :=

t0,
if i = 0;
t1,
if i > 0.
– The items in W are distributed equally to the processors,
card(Wj) =

m + 1,
if 0 ≤j < r;
m,
if j ≥r.
,
where m = ⌊N/n⌋∈N is the integer division of n by N (⌊⌋indicating the
ﬂoor of a real number) and r is the remainder of this division.

Hunting Down Load Imbalance: A Moving Target
499
By construction the maximum T(fN) of execution times Tj(fN) is attained at
the parallel entity that hosts the “spike”. Computing the speed up by divid-
ing T(f0) by T(fN) we can recover something close to Gustavsons formulation
[5, p. 532] of Amdahls law [1]. We only have to associate t0 to the serial time s
in Gustavsons paper, t1 ∗(n −1) to the parallel time p.
T(f0)
T(fN) = t0 + (n −1) ∗t1
t0 + ⌊n/N⌋∗t1
−→
N→∞
t0 + (n −1) ∗t1
t0
The only diﬀerence comes from integer division ⌊n/N⌋. We can even retain
the result for the asymptotic limit N →∞, that the total time for the “spiked
load imbalance” case approaches the serial time t0 and hence the speed up con-
verges to the reciprocal of the “serial fraction” t0/T0.
2.3
Maximal Speedup
Eventually, this asymptotic result can be extended to the general case, i.e. to
parallelization of an arbitrary workload W by general maps fN : W →P, where
N is the cardinality of P.
Theorem 1. Let W = {0, 1, . . . , n −1} a set of work entities with associated
times ti, i ∈W. Let T0 denote the sum and t0 be the maximum of all ti.
T0 =

i∈W
ti ,
t0 = max
i∈W ti
Then the maximum speedup over all P and possible maps f : W →P is given
by the following formula.
max
f:W →P
T0
T(f) = T0
t0
For the proof we need the following lemma.
Lemma 1. Let W, T0 and t0 as above. If fm is a map attaining the minimum
min
f:W →P T(f) = T(fm),
m ∈fm(W) a parallel entity and Wm = f −1
m (m) the corresponding inverse image
of fm attaining the maximum as follows
T(fm) =
max
j∈fm(W )

i∈Wj
ti =

i∈Wm
ti,
then at least one Wm thus deﬁned has cardinality 1.
Proof. Suppose, all of the Wm thus deﬁned have at least two elements. Then
we can get a speed up by splitting all these work loads and assigning them to
new parallel entities as needed - contradicting the assumption that fm is a map
attaining the minimum.

500
C. Pospiech
Proof (of Theorem 1). As we keep W and subsequently T0 constant for all maps
f, we only have to proof that
min
f:W →P T(f) = t0 .
We can restrict ourselves to the case where P has ﬁnite cardinality less or equal
to the cardinality of W. Employing more parallel entities than there is work to
do cannot be minimal. Therefore the minimum exists and must be attained by
a map fm. Applying Lemma 1, we ﬁnd a Wm = {k} of cardinality 1 such that
T(fm) = tk. If tk ̸= t0, then tk < t0 by deﬁnition of t0. Additionally, we ﬁnd
the index 0 in a set 0 ∈W0 ̸= Wm because Wm = {k}. But then we have the
following inequality.

i∈Wm
ti = tk < t0 ≤

i∈W0
ti
contradicting the deﬁnition that Wm should be the set where the maximum is
attained. Hence tk = t0, which proves the assertion.
3
Investigating CCLM Load Imbalance
3.1
CCLM Parallelization Schema
The climate modelling version CCLM of the weather forecast code COSMO [4]
runs a dynamic core and a physical model on rotated geographical coordinates
and a generalized terrain following height coordinates. The parallelization is
done by domain decomposition along horizontal coordinates. In more detail,
each MPI rank is allotted a rectangular subdomain and the README 5.0.htm
ﬁle, distributed with the COSMO RAPS 5.0 distribution, indicates placement
of MPI ranks in a two-dimensional processor grid. In Fig. 1 the MPI rank for an
8 × 4 processor grid is shown as an opaque colored step function, placed on top
of the computational domain.
For each time step, this could be rephrased in the above abstract notation.
The (atomic) work load entities would be deﬁned by the work executed for a
single horizontal grid point. The parallelization would map rectangular subsets
of these to the same MPI rank. If we knew the time for each horizontal grid
point, we could think of plotting the time rather than the MPI rank in Fig. 1.
This would show a “time mountain”, and guided by Theorem 1 we would focus
on the highest hill top t0, as this is limiting the maximum speedup.
Currently the parallelization of CCLM is MPI only and it is exhibiting load
imbalance. The question arises whether a hybrid parallelization would do better.
If we keep the work load entities as before (i.e. parallelize horizontal loops only)
and only change the mapping to the parallel entities by adding threads to the
MPI ranks, Theorem 1 indicates that this doesn’t change the upper limit for
speed up. Of course we have to be careful with a verdict like that, as Theorem 1
is based on a performance model that neither includes communication time nor
cache or memory eﬀects. But at least height of the hill top t0 is a fact, that can’t
be easily argued away.

Hunting Down Load Imbalance: A Moving Target
501
Fig. 1. MPI rank representation by height and color for an 8 × 4 processor grid (Color
ﬁgure online)
Even if a parallelization strategy - yet to be found - further divides the cur-
rent atomic workloads, we still have to take into account that OpenMP is a
shared memory parallelization and the mapping of workloads to parallel enti-
ties has to respect node boundaries. The current preferred placement of MPI
ranks to nodes tries to minimize nearest neighbour communication across node
boundaries by placing a rectangular sub-domain of the processor grid onto each
node. As a consequence, the horizontal grid points on the same node are also
exhibiting geographic proximity. If we are to hope for some load balancing with
the new parallelization strategy, we have to ﬁnd valleys in the “time mountain”
geographically close to the hill top t0. Otherwise we don’t ﬁnd horizontal grid
points with free cycles that can take over the work load for the horizontal grid
point under hill top t0.
Alternatively, we might deliberately take a performance hit by changing the
mapping of MPI ranks to nodes, bringing distant “time valleys” onto the same
node as the hill top. But then he have to know the location of these “time
valleys” in advance.
To shed some light on this question, the following graphic visualization maps
computation time data to geographic location and weather phenomena. It is
based on data from a run with 450∗438 horizontal grid points, 40 vertical layers
and a 16 ∗32 processor grid.
3.2
Importing NetCDF Data
CCLM writes binary data in NetCDF4 format. Per Fortran namelist parameter
the frequency of output can be speciﬁed. In the CCLM case under consideration,

502
C. Pospiech
NetCDF data are written every 3 forecast hours. Additionally, there is an extra
NetCDF ﬁle written at the start of the model run that contains “constant”
data (in the sense that they don’t change during the forecast). The data import
module of OpenDX [6] version 4.4.4 can read NetCDF4 out of the box. The
terrain following coordinates [4, p. 31] allow to extract a topographic map from
the NetCDF4 ﬁle of constant data. The tutorial [3] explains in detail how this can
be accomplished. Adding wind speeds as arrows and isobares is pretty straight
forward.
The NetCDF data also contain ﬁelds labeled “QR” (mass fraction of rain in
air) and “QS” (mass fraction of snow in air). Unlike NetCDF ﬁeld “QV” (speciﬁc
humidity), “QR” and “QS” show water content that is already condensated.
“QR” and “QS” are added to form mass fraction of precipitation in air. In
Figs. 2 and 3, areas of high mass fraction of precipitation in air are indicated as
opaque, turquoise colored clouds.
3.3
Load Imbalance Data
Load imbalance data come from a diﬀerent source and are not automatically
available for the grid as used in Fig. 2. Load imbalance is deﬁned in [2, p. 3]
as “processor cycles spent idling while unﬁnished parallel work exists.” Natu-
rally, these idling processor cycles pile up at a synchronization point such as a
MPI Barrier or any other global blocking collective MPI communication. Both,
by CCLM namelist settings and some additional MPI instrumentation, extra
calls to MPI Barrier are added to the code and timers around these barriers are
used to harvest and measure the idling cycles. Actually, the complement of the
sum of idling cycles and communication time, is taken. This is the computation
time, which better ﬁts to our previous reasoning about a “time mountain”. In
order to align these data to the temporal resolution of 3 forecast hours (as out-
lined in Subsect. 3.2), we only consider harvested data in the forecast time inter-
val 20 forecast minutes prior to the output time until output time. The value of
20 forecast minutes was pretty much chosen ad libitum. Further research should
test whether the results change if this value is varied.
By the above construction, for each output period, the values for commu-
nication time are given per MPI rank, not per geographical grid point. These
values are integrated into the geographical grid for the weather data by using a
step function as shown in Fig. 1. By construction this step function has a con-
stant computation time value for each horizontal grid point that is mapped to
the same MPI rank. This is kind of a “discretized” representation of our “time
mountain”.
3.4
Relating Load Imbalance to Weather Phenomena
We are now adding these load imbalance data by coloring the wind speed arrows.
This means that “red” arrows indicate areas with high computation time that
cause the other MPI ranks to wait at MPI Barriers. Blue arrows, however, indi-
cate a lightweight compute load.

Hunting Down Load Imbalance: A Moving Target
503
Fig. 2. Weather and load imbalance 1.1.2000, 03 h (Color ﬁgure online)
Fig. 3. Weather and load imbalance 1.1.2000, 12 h (Color ﬁgure online)

504
C. Pospiech
Looking at Figs. 2 and 3, the red and blue arrows appear in diﬀerent places.
It appears that “red” arrows only occur in (or close) to areas of precipitation.
That would mean that they are not stationary but move along with the rain or
show sheets. Particularly when creating more of those images and connecting
them to a short video [7], we can clearly see this movement of red arrows along
with the precipitation fronts.
In retrospect this might not be as surprising as it ﬁrst seems. Weather fore-
cast codes frequently use a ﬁxed domain decomposition along horizontal grid
points. As stated in [8, p. 112], “because atmospheric processes occur nonuni-
formly within the computational domain, e.g., active thunderstorms may occur
within only a few subdomains of the decomposed domain, the load imbalance
across processors can be signiﬁcant.” If atmospheric processes have already been
identiﬁed as source for load imbalance, these load imbalances will move along in
time.
This already ends all hopes for an explicit static schema to combine “time
valleys” with “time mountains” as they can’t be located in advance. Anticipating
the future location may be as complicated as forecasting the weather itself.
A second look at this data reveals that not all red arrows ﬁt to the above
observation. While the precipitation front crossing Norway and Sweden West to
East carries the red arrows along nicely, there are some “outplaced” red arrows
in Fig. 3 north of the Black Sea. It is not clear whether this is an artiﬁcial eﬀect
coming from the step function implementation as explained in Subsect. 3.3.
Turning to the question whether we can do local dynamic load balancing, it
appears that the rain front crossing Scandinavia is oﬀering blue arrows close to
a peaked area of red arrows. Here the prerequisites identiﬁed in Subsect. 3.1 are
satisﬁed. North of the Black Sea, however, we rather see a ﬂat “time mountain”
top, as the red arrows are ﬂanked by amber and yellow arrows. Here the success
of local dynamical load balancing might be limited. Even this singular example
shows that the stakes for local dynamic load balancing may vary with the precise
nature of the atmospheric processes that cause the load imbalance.
4
Summary
Load imbalance is known to be a major bottleneck to scalability, but it is hard
to ﬁnd a cure. We used a mathematical formalization to describe and formally
deﬁne load imbalance. This revealed a close similarity to Gustavsons formulation
[5, p. 532] of Amdahls law [1]. This similarity was particularly close for the special
case of a “spiked load imbalance”.
However, a “real life” example such as the climate modelling version CCLM
of the weather forecast code COSMO [4] showed a much more complicated load
imbalance behavior than a simple stationary spike. Both, by CCLM namelist set-
tings and some additional MPI instrumentation, load imbalance data were col-
lected per MPI rank and forecast interval of 3 h. Using the visualization software
OpenDX [6], this information is correlated to geography and weather forecast
results. The graphic representation indicated that sources for load imbalance are

Hunting Down Load Imbalance: A Moving Target
505
not stationary but move along with the rain or snow sheets. This excludes some
common strategies against load imbalance while limiting the hopes for others.
References
1. Amdahl, G.M.: Validity of the single processor approach to achieving large scale
computing capabilities. In: Proceedings of the Spring Joint Computer Conference,
18–20 April 1967, pp. 483–485. ACM (1967)
2. Crovella, M.E., LeBlanc, T.J.: Parallel performance prediction using lost cycles
analysis. In: Proceedings of the 1994 ACM/IEEE Conference on Supercomputing,
pp. 600–609. IEEE Computer Society Press (1994)
3. Dalhousie University, Department of Oceanography: Data explorer tutorials (1995).
http://www.phys.ocean.dal.ca/docs/DX tutorial.html. Accessed 13 February 2015
4. Doms, G.: A description of the nonhydrostatic regional cosmo model. part 1: Dynam-
ics and numerics. DWD, Oﬀenbach, Germany (2011). http://www.cosmo-model.
org/content/model/documentation/core/cosmoDyncsNumcs.pdf. Accessed 13 Feb-
ruary 2015
5. John, L.: Reevaluating Amdahl’s law. Commun. ACM 31(5), 532–533 (1988)
6. IBM OpenDX. Open visualization data explorer (2002). http://www.opendx.org/.
Accessed 13 February 2015
7. Pospiech, C.: Hunting down load imbalance: a moving target (2015). https://youtu.
be/wPIplbq8fDA. Accessed 05 April 2015
8. Xue, M., Droegemeier, K., Weber, D.: Numerical prediction of high-impact local
weather: a driver for petascale computing. In: Petascale Computing: Algorithms
and Applications, pp. 103–124 (2007)

Orchestrating Docker Containers
in the HPC Environment
Joshua Higgins(B), Violeta Holmes, and Colin Venters
The University of Huddersﬁeld, Queensgate, Huddersﬁeld, UK
{joshua.higgins,v.holmes,c.venters}@hud.ac.uk
Abstract. Linux container technology has more than proved itself use-
ful in cloud computing as a lightweight alternative to virtualisation,
whilst still oﬀering good enough resource isolation. Docker is emerg-
ing as a popular runtime for managing Linux containers, providing both
management tools and a simple ﬁle format. Research into the perfor-
mance of containers compared to traditional Virtual Machines and bare
metal shows that containers can achieve near native speeds in processing,
memory and network throughput. A technology born in the cloud, it is
making inroads into scientiﬁc computing both as a format for sharing
experimental applications and as a paradigm for cloud based execution.
However, it has unexplored uses in traditional cluster and grid comput-
ing. It provides a run time environment in which there is an opportunity
for typical cluster and parallel applications to execute at native speeds,
whilst being bundled with their own speciﬁc (or legacy) library versions
and support software. This oﬀers a solution to the Achilles heel of cluster
and grid computing that requires the user to hold intimate knowledge
of the local software infrastructure. Using Docker brings us a step closer
to more eﬀective job and resource management within the cluster by
providing both a common deﬁnition format and a repeatable execution
environment. In this paper we present the results of our work in deploy-
ing Docker containers in the cluster environment and an evaluation of
its suitability as a runtime for high performance parallel execution. Our
ﬁndings suggest that containers can be used to tailor the run time envi-
ronment for an MPI application without compromising performance, and
would provide better Quality of Service for users of scientiﬁc computing.
Keywords: Linux containers · Docker · Cluster · Grids · Run time
environment
1
Introduction
Cloud computing has been driven by the Virtual Machine (VM). They are widely
deployed to achieve performance and resource isolation for workloads by con-
straining the amount of virtual memory and processor cores available to a guest
system. This allows resource sharing on a massive scale; VMs can be provisioned
with any software environment and kept separate from other guests on the same
physical server [18].
c
⃝Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 506–513, 2015.
DOI: 10.1007/978-3-319-20119-1 36

Orchestrating Docker Containers in the HPC Environment
507
Linux container technology is classed as an operating system virtualisation
method. It allows the creation of separate userspace instances in which the same
kernel is shared. This provides functionality similar to a VM but with a lighter
footprint. The Docker project provides a management tool and its own library
for communicating with containment features in the OS kernel [2].
Resource isolation takes a back seat in HPC systems which generally execute
a user’s job within the same OS environment that runs directly on the hardware
to gain the best performance. This poses a problem for application portability,
especially in grid systems where a remote resource may lack the libraries or sup-
port software required by a job. This undermines eﬀorts by middleware vendors
to unify resources and provide a common format for accessing heterogeneous
systems [8]. In this respect some features of the cloud are desirable in cluster
computing.
Docker containers oﬀer an opportunity to create cloud-like ﬂexibility in the
cluster without incurring the performance limitations of a VM. This paper inves-
tigates the performance gains that can be achieved using Docker containers for
executing parallel applications when compared to the KVM hypervisor [5] in a
typical scientiﬁc cluster computing environment. We also propose a method of
executing an MPI job encapsulated in a Docker container through the cluster
resource manager.
In the sections of this paper, a short review of Docker containers versus
Virtual Machines will be conducted. The current work will be then discussed.
Section 4 describes the proposed Docker in the HPC cluster solution. The results
from the deployment of this implementation will be evaluated and future work
identiﬁed.
2
Docker vs KVM
2.1
Architecture
KVM is a popular hypervisor for Linux that introduces virtualisation support
into the Linux kernel. The hypervisor provides an illusion to the guest OS that
it is managing it’s own hardware resources [15]. A request from the guest must
be translated into a request to the underlying physical hardware; a process in
modern hypervisors that is highly optimised and transparent to the guest. This
allows the hypervisor to host a VM without modiﬁcations to the OS that has
been chosen.
Docker containers do not require a layer of translation. On Linux, they
are implemented using ‘cgroups’; a feature in the Linux kernel that allows the
resources (such as CPU, memory and network) consumed by a process to be
constrained [14]. The processes can then be isolated from each other using kernel
‘namespaces’ [13]. This fundamental diﬀerence requires that the guest system
processes are executed by the host kernel, restricting containers on a Linux host
to only other Linux ﬂavours. However, it means that an executable within the
container system essentially runs with no additional overhead compared to an
executable in the host OS. A container is not required to perform any system

508
J. Higgins et al.
initialisation - its process tree could contain just the program being run and any
other programs or services that it depends on.
2.2
Performance
A benchmark by IBM Research demonstrates that the performance of a Docker
container equals or exceeds KVM performance in CPU, memory and network
benchmarks [12]. The results show that both methods do not introduce a mea-
surable overhead for CPU and memory performance. However the Linpack per-
formance inside KVM is shown to be very poor; the hypervisor abstracts the
hardware and processor topology which does not allow tuning and optimisation
to take place.
They also suggest that the preferential scaling topology for Docker containers
is by processor socket. By not allowing a container to span cores distributed over
multiple processor sockets, it avoids expensive inter-processor bus communica-
tion. This is in line with the philosophy already ingrained in cluster computing
applications in which a process per core is executed on compute nodes. However,
the eﬀect may not be appreciable in distributed memory applications where the
bandwidth of the network interconnect may be many orders of magnitude slower.
2.3
Storage
A Virtual Machine is traditionally accompanied by a disk image which holds the
OS and run time applications. To create or modify this disk image would require
the user to hold the knowledge of installing the OS, or systems management
experience. The resulting image ﬁle may span several gigabytes. This places a
large burden on storage and may be inconvenient to transfer between systems.
The Docker project introduces a concept of a ‘Dockerﬁle’ which allows the
userspace of a container to be described as a list of directives in a text ﬁle that
construct the real image [3]. Each directive produces a layer of the ﬁnal system
image, which are combined at run time using a copy-on-write method to appear
to the container as a single uniﬁed image. This allows multiple containers to
share common image layers, potentially reducing the amount of data required to
transfer between systems. The Dockerﬁle itself is signiﬁcantly easier to customise
and can be easily version controlled or shared.
3
Current Work
The ﬂexibility of the cloud allows one to create multiple clusters where each
individual virtual cluster can have it’s own customised software environment.
This draws parallels with the development of the now defunct OSCAR-V mid-
dleware [17] and Dynamic Virtual Clustering [11] concepts, which are able to
provision a virtual cluster based on the requirements of a job at the time of
submission. These systems still incur the overhead of running a VM as the job
execution environment and inherit the associated performance limitations.

Orchestrating Docker Containers in the HPC Environment
509
The Agave API is a gateway platform that provides services typically found
in grid systems tailored for scientiﬁc users. It has a strong focus on web standards
and boasts support for running applications packaged in Docker containers in a
cloud [10]. However, Agave orchestrates a single container per job, which limits
the scope for running parallel applications [9].
HTCondor is a middleware for high-throughput computing that has support
for running jobs in parallel on dedicated machines, using ‘universes’ to distin-
guish between diﬀerent execution paradigms. HTCondor itself already supports
‘cgroups’ to constrain the resources available to a job on Linux hosts and a
universe is under development to provide support for the container format [16].
HTCondor has powerful resource discovery features but is the usefulness of a
container in not needing to know?
4
Docker in the HPC Cluster
To implement existing container and VM solutions in HPC systems requires
modiﬁcations to the software stack of the local HPC resources. The HPC sys-
tems would already have resource management and job scheduling in place. The
methodology should follow the concept of containers, that is to abstract the
application from the software stack of the resource. Modifying core components
of this stack to support containers introduces a portability problem. Any stan-
dard resource manager provides all the information required to orchestrate a
Docker container within a resource.
A resource manager such as Torque uses a script that is the core of the job
execution and is responsible for conﬁguring the environment and passing the
required information to a process starter, such as ‘mpirun’. We use both the
MPICH [6] and OpenMPI [7] launchers regularly in our systems, which support
SSH to enable remote process execution.
4.1
Choosing a Container Model
Since we concluded that running a container has no appreciable overhead over
running a normal executable, we propose two diﬀerent container models for
parallel execution. A single container could be started to mimic the worker node,
as shown in Fig. 1, which would hold all the processes assigned to that node.
Secondly, a container per process could also be orchestrated as shown in
Fig. 2. Whilst it is unlikely that processes within the same job would require
diﬀerent run time environments, this presents an interesting opportunity for
resource usage accounting and can oﬀer real time adjustment of the resource
constraints per process through ‘cgroups’. It can also enforce a process to be
mapped to a speciﬁc processor core if this functionality is missing from the
process launcher.

510
J. Higgins et al.
Fig. 1. A container per node that holds all respective processes
Fig. 2. A container per process is orchestrated on each node
4.2
Container Model Implementation
The resource manager will not be aware of any containers, so the script that
is central to the job execution will be used to prepare the nodes and start the
containers, before the process launcher starts the parallel job. An overview of
this process is described in Fig. 3. We cannot assume that the user is able to
SSH to the worker nodes to run the container preparation commands. In this
case the bootstrap script can also be invoked through the process launcher ﬁrst
and then the process launcher is called a second time to invoke the parallel job.
The overview of the script process supports both container models.
When the process launcher is called, the environment has been modiﬁed in
two ways: the container will randomise it’s SSH port and expose it on the same
interface as the host. The SSH conﬁguration is temporarily changed so that
the process launcher will use this port instead of the default, thereby launching
the process within the container and not on the node. However, if the process
launcher forks the process on the rank 0 node instead of using SSH, the process
will run outside the container. To avoid this condition, the script will write a
unique hostname alias for each node into the SSH conﬁguration that maps to
the container on that node. These are substituted into the list of nodes provided
by the resource manager before being passed to the process launcher.
5
Deployment of Container Model on HPC Cluster
Eridani is the general purpose, 136 core cluster within the QueensGate Grid at
the University of Huddersﬁeld. Like the other clusters within this campus grid, it

Orchestrating Docker Containers in the HPC Environment
511
Start PBS 
script
Process 
launcher
Stop 
containers 
on nodes
PBS script 
finished
SSH to next 
worker
Is Docker 
running?
Pull image and 
start container
Expose SSH 
on random port
Is last 
worker 
node?
Create 
container user
Yes
No
Start docker -d
No
Yes
Write SSH 
host config
Fig. 3. Overview of script process
uses the Torque resource manager running on CentOS 6.5 and accepts jobs in a
PBS script format. The Intel-optimised parallel LINPACK version 11.2.1.009 [4]
benchmark was used to measure the performance of 2 nodes with 4 cores each.
In order to validate the claim in [12] that for the non-optimised use case there
is no appreciable diﬀerence in the CPU execution performance between a Docker
container and a VM, the same benchmark was run using the reference BLAS
version 3.5.0 library without architecture speciﬁc optimisations [1].
The containers were orchestrated in both container per-core and container
per-node models, using the implementation described in the previous section.
The VMs were created to consume all available cores on the host and appropriate
memory to ﬁt the LINPACK problem size.
5.1
Results
Figure 4 shows the experimental results using both the Intel-optimised parallel
LINPACK and generic BLAS library comparing native, Docker container models
and KVM performance. The results have been obtained from 10 runs of each
conﬁguration. The peak performance observed per conﬁguration is shown.
5.2
Evaluation
The
LINPACK
experimental
results
echo
those
obtained
by
previous
research [12], showing that the performance of Docker containers has no appre-
ciable diﬀerence compared to running natively, whilst the VM achieves approx-
imately half the peak performance of the container. However, this work diﬀers
signiﬁcantly in that the parallel LINPACK uses a distributed memory model, not
shared memory, utilising a network interconnect for message passing to achieve
data sharing between processes.

512
J. Higgins et al.
Native
Per node
Per process
KVM
0
20
40
60
Gﬂops (peak)
Intel-optimised LINPACK
Native
Per node
Per process
KVM
0
1
2
3
4
Gﬂops (peak)
Generic BLAS LINPACK
Fig. 4. LINPACK benchmarking results
Without optimisation for the processor architecture, the performance of a
VM and container are mostly comparable. However, the overall peak perfor-
mance is considerably lower for this application. This suggests that the Docker
container is therefore a more appropriate execution method for high performance
parallel applications where we are likely to employ these types of optimisation.
There is no diﬀerence in performance between the two container orchestration
models proposed. This is expected given that a process within the container is
essentially a process running in the host kernel with ‘cgroup’ limitations.
6
Summary
One of the requirements of grid computing is to run a job transparently to the
user on any resource they desire without requiring knowledge of the local software
conﬁguration. Based on our research and experimental results conducted, it is
evident that Docker containers can facilitate this by abstracting the software
environment of the local HPC resource without compromising performance. This
improves Quality of Service for our users by
– Allowing parallel jobs to run on traditional PBS clusters with arbitrary run
time environments.
– Reducing the entry level of customising the run time environment to that of
the average user.
– Running jobs on resources within the grid that was previously not possible
due to software conﬁguration.
7
Future Work
The container per process model oﬀers many advantages by allowing us to apply
‘cgroup’ constraints to each process in a HPC job. This would allow resource

Orchestrating Docker Containers in the HPC Environment
513
management to be improved based on job requirements as more ﬁne grained
control can be achieved for network and disk I/O usage in addition to CPU
time [14]. It also provides scope for optimising the power consumption of a job
as limits can be changed in real time without restarting the process.
In our future work we will perform benchmarking to appreciate the impact of
Docker’s NAT networking on message passing. We will also investigate orches-
tration of Docker containers that contain parallel applications in the cloud envi-
ronment as opposed to traditional cluster computing.
Acknowledgments. The experimental results for this work could not have been
obtained without the resources and support provided by the QueensGate Grid (QGG)
at The University of Huddersﬁeld.
References
1. Blas (basic linear algebra subprograms). http://www.netlib.org/blas/
2. Docker. https://www.docker.com/
3. Dockerﬁle reference - docker documentation, Version 1.4. https://docs.docker.com/
reference/builder/
4. Intel math kernel library linpack download. https://software.intel.com/en-us/
articles/intel-math-kernel-library-linpack-download
5. Kernel based virtual machine. http://www.linux-kvm.org/
6. MPICH high performance portable MPI. http://www.mpich.org/
7. Open MPI: Open source high performance computing. http://www.open-mpi.org/
8. Charlie, C.: Standards for grid computing: global grid forum. J. Grid Comput.
1(1), 3–7 (2003)
9. Dooley, R.: Agave docker quickstart (2014). https://bitbucket.org/deardooley/
agave-docker-support/
10. Dooley, R., Vaughn, M., Stanzione, D., Terry, S., Skidmore, E.: Software-as-a-
service: the iplant foundation API. In: 5th IEEE Workshop on Many-Task Com-
puting on Grids and Supercomputers, November 2012
11. Emeneker, W., Stanzione, D.: Dynamic virtual clustering. In: 2007 IEEE Interna-
tional Conference on Cluster Computing, pp. 84–90, September 2007
12. Felter, W., Ferreira, A., Rajamony, R., Rubio, J.: An updated performance com-
parison of virtual machines and linux containers. Technology 28, 32 (2014)
13. Kerrisk, M.: Namespaces in operation, part 1: namespaces overview (2014). http://
lwn.net/Articles/531114/. Accessed 7 February 2015
14. Menage,
P.:
Cgroups.
https://www.kernel.org/doc/Documentation/cgroups/
cgroups.txt. Accessed 7 February 2015
15. Smith, J., Nair, R.: Virtual Machines: Versatile Platforms for Systems and
Processes. Morgan Kaufmann, San Francisco (2005)
16. Tannenbaum, T.: Htcondor and hep partnership and activities (2014). Presented
at the HEPiX Fall 2014 Workshop. University of Nebraska, Lincoln, 13–17 October
2014
17. Vallee, G., Naughton, T., Scott, S.L.: System management software for virtual
environments. In: Proceedings of the 4th International Conference on Computing
Frontiers, pp. 153–160. ACM (2007)
18. Weiss, A.: Computing in the clouds. netWorker 11(4), 16–25 (2007)

Performance and Scaling of WRF on Three
Different Parallel Supercomputers
Zaphiris Christidis(&)
Lenovo, Morrisville, USA
zchristidis@lenovo.com
Abstract. A WRF weather model conﬁguration with high I/O frequency was
tested on three IBM supercomputers for up to 6144 cores. Scalability, overall
performance, and I/O throughput was examined for a 12-h forecast with hourly
I/O intervals. The three parallel systems tested were the (a) POWER 775 cluster
(p7-IH, IBM POWER7, HFI interconnect), (b) the POWER 460 cluster (Pure-
ﬂex IBM POWER7, Dual QDR Inﬁniband interconnect) and (c) the iDataPlex
cluster (dx360M4 Intel Sandybridge, FDR14 interconnect). MPI traces were
obtained on all systems for runs with 128, 256, 512, 1024, 1536, 2048, 3072,
4096 and 6144 cores. I/O quilting was employed, and the number of MPI tasks
(including task grids), OpenMP threads and I/O quilting tasks was kept Identical
across all three systems. The effects of computation, communication, I/O
throughput, load imbalance and simultaneous multithreading (SMT), was
examined across all three systems and for all core counts. It was found that all
systems performed similarly, despite signiﬁcant differences in interconnect,
processor, I/O subsystems and software/compiler technologies.
Keywords: WRF  idataplex  Pureﬂex  Power 775  460  I/O  Performance
1
Introduction
Supercomputer systems at Major Operational Numerical Weather Prediction (NWP)
Centers have the primary goal to reliably ingest massive amounts of weather obser-
vation data from all over the world, run large weather forecasts on the collected data
using Regional and Global weather forecasting models and then release the generated
forecast products to the ﬁeld by the same time every day. The secondary goal is to
support a research and development effort that will further scientiﬁc advances in
Numerical Weather Forecast models. The best methods to do this are computationally
intensive and are limited by the power of available computer systems. Thus the value of
supercomputers in NWP Centers is measured in their overall utilization, decreased
system downtime, and efﬁciency in the delivery of their forecasting products which is
as important as performance and speed when millions of dollars are being spent
towards their purchase.
The computer systems installed in NWP Centers must be powerful enough to
handle the computationally expensive and complex models used to generate the
forecasting products. As such, they must be capable of solving a single large problem
very fast. Solving large numbers of small problems in parallel on a capacity
© Springer International Publishing Switzerland 2015
J.M. Kunkel and T. Ludwig (Eds.): ISC High Performance 2015, LNCS 9137, pp. 514–528, 2015.
DOI: 10.1007/978-3-319-20119-1_37

maximizing system is not sufﬁcient. The cost of delivering computational performance
as capability is usually higher than the cost of delivering a capacity optimized system.
Therefore, the capability requirements drive the total cost of the system. In addition, the
algorithms used to simulate the atmosphere and the algorithms used to ingest and
assimilate the data required for the weather models, are computationally demanding.
Accuracy of the simulation is determined by the available computer power and
available data processing capability. Improved accuracy has reduced weather and cli-
mate risk to life and property and further improvements will continue to reduce these
risks. NWP Centers need increased processing capability as soon as it is available to
achieve these reductions. Additional processing power allows more accurate weather
forecasts, severe weather risk assessments, storm track and intensity forecasting, and
many other beneﬁts.
Another portion of an NWP Center workload includes data assimilation and
analysis projects to better describe past states of the atmosphere. This portion of the
work can strain capacity resources more than capability. All of these tasks require very
high speed I/O for large amounts of data and extremely high data storage capacities in
order to save and analyze the past atmosphere states. I/O requirements (transaction rate,
bandwidth, and capacities) will scale near linearly with computational capabilities and
the future supercomputer system must satisfy all three I/O requirements, which are
driven by the increasing power of the simulation algorithms, (more and larger forecasts
and simulation outputs), and by rapid increases in data available for use by these
algorithms.
It is quite important for NWP Centers to evaluate the overall performance of
various supercomputers in order to examine how well they can serve activities that are
critical to their mission. In this paper, the performance of three supercomputers is
examined while benchmarking a very popular capability weather application, known as
the Weather Research and Forecasting model, WRF [1, 2].
1.1
Motivation
WRF is used today by a community of over 5,000 meteorologists conducting weather
and climate studies, and it runs operationally on a variety of supercomputer platforms.
Arnold et al. [16], Morton et al. [17] and Kerbyson et al. [18] have studied the
performance of WRF on a variety of computer architectures, while Porter and
Ashworth [19], investigated the performance of WRF on CRAY systems. The WRF
benchmark site at NCAR [3], is the only source of information on the performance of
standardized WRF test cases (2.5 km and 12 km CONUS) among various super-
computers. However, this site has not been updated since 2010. In addition, the site
does not offer information on I/O workloads and communication characteristics for the
CONUS test cases. The lack of a direct comparison of identical WRF conﬁgurations
with typical I/O on several supercomputer systems was used as motivation for this
study. It was initiated by a request issued by the China Meteorological Administration
(CMA), where an operational WRF test case was distributed to computer vendors for
benchmarking. As such, the CMA WRF was tested on the following parallel computer
systems.
Performance and Scaling of WRF
515

2
Tested Parallel Systems
2.1
The IBM 775 POWER Parallel System (p775)
IBM announced the p775 system in 2011 [5], as an extremely dense supercomputer
node that boasts 256 POWER7 processor cores in just 2U of rack space. The p775 is
the sixth generation of very high density compute nodes based on the POWER pro-
cessor. The p775 uses 8–core 3.836 GHz POWER7 processors packed four apiece into
Quad Chip Modules (QCM). Each core supports Simultaneous Multithreading (SMT),
which can enable up to four instruction threads to run simultaneously per core. The
system can support up to 256 processor cores in a slim 2U rack drawer. Each 2U
drawer (CEC) contains eight QCMs so that each node has 256 cores. These QCMs are
interconnected via copper switching technology. A maximum of twelve of these node
drawers can be housed in custom water cooled rack along with optional 4U disk
enclosures. The system is logically conﬁgured into octant nodes with each octant
containing thirty two (32) processor cores. Thus, each drawer contains 8 Octant nodes.
IBM has used parallel optics on the p775 system, driven by the so-called torrent
chip. The tested system was outﬁtted with the interconnection fabric known as HFI
(Host Fabric Interface). Four node drawers (32 node octants) were interconnected with
HFI L-links, comprising a so called Super-node, while Super-nodes were intercon-
nected with HFI D-links. A total of 192 diskless octant nodes were used for compu-
tations, with an additional four octant nodes acting as Generalized Parallel File system
(GPFS [8]) servers. A total of 3 disk enclosures were used for I/O. Each node of this
cluster contained 128 GB of memory, and the standard IBM AIX software stack (AIX
7.1, XLF14, VAC 12, GPFS, IBM Parallel Environment (PE), LoadLeveler, MPI
proﬁle and Mathematical Acceleration Subsystem Libraries - MASS [11]).
2.2
The IBM 460 POWER Parallel System (p460)
IBM announced the p460 [6] in 2012, a supercomputer node that boasts 32 POWER7
processor cores in just 2U of rack space. This compute node runs in IBM Flex System
Enterprise Chassis units to provide a high-density, high-performance compute node
environment. The p460 is a full wide, four socket node that uses four Dual Chip
Modules (DCM). Each DCM consists of eight POWER7 cores operating at 3.55 GHz.
DCMs are interconnected via copper switching technology. Each DCM has 4 MB L3
cache per core and an integrated memory controller with four memory channels. Each
core supports SMT4, which enables four instruction threads to run simultaneously per
core, just like in system p775. The tested system consisted of 224 total compute nodes.
Each standard computer rack holds a total of 28 nodes and it can be either air or water
cooled via rear door heat exchangers. Individual nodes were interconnected with a dual
QDR Inﬁniband network. The I/O subsystem of this cluster consisted of 8 p740
POWER 7 nodes operating as GPFS servers. A total of 8 DSC3700 storage devices
were directly attached to the GPFS servers via four dual-port ﬁber channel adaptors,
such that each DSC3700 was connected to two p740 in a dual ring conﬁguration. Each
p460 node of this cluster contained 128 GB of memory, and the standard IBM AIX
software stack.
516
Z. Christidis

2.3
The IBM iDataPlex Parallel System (dx360M4)
The IBM iDataPlex dx360M4 system [7] is a parallel supercomputer that is based on
the Intel processor technology and it is designed to optimize density and performance
within typical data center infrastructures. Each compute rack of the system can contain
up to 84 individual nodes, with each node consisting of 2-socket 8-core Intel E5 2560
Sandy-bridge processors operating at 2.6 GHz. Thus each rack can host up to 1344
cores. The Intel cores consist of 32 KB data and instruction Level 1 caches, and
256 KB Level 2 data caches, much like the POWER 7 cores in the p775 and p460
systems. However, each Sandy-bridge processor had 20 MB L3 cache that is shared
among the 8 cores within the processor. Each node of the system had 32 GB of
physical memory, one hard disk drive and one Inﬁniband Mellanox connect X – FDR
adapter. A total of 6912 compute nodes were interconnected with a Mellanox IB4X
FDR full-bisection Fat-Tree Inﬁniband network, while the system contained 10 Intel
3650 M4 storage nodes serving GPFS through 20 DSC3700 storage devices. The
system was outﬁtted with the RHEL6.2 Operating system, including the Intel Studio
with the FORTRAN 13 and C/C++ 11 compilers. In addition, the system included
GPFS, the IBM PE, with the Platform LSF batch queuing system.
Table 1 contains a summary of all three tested systems with their components.
3
The WRF Mesoscale Model
The Weather Research and Forecasting (WRF) Model is a next-generation Mesoscale
numerical weather prediction system designed to serve both operational forecasting and
atmospheric research needs. It features multiple dynamical cores, a 3-dimensional
Table 1. Tested systems with their hardware and software conﬁgurations.
System
p775
p460
dx360M4
Total cores used
6144
6144
6144
Total compute nodes
192
192
384
Total cores per node
32
32
16
Core frequency (GHz)
3.84
3.55
2.60
L1 cache/core (KB)
32
32
32
L2 cache/core (KB)
256
256
256
L3 cache/core (MB)
4
4
2.5
Memory (GB/core)
4
4
2
Core vector capability
VSX
VSX
AVX
Interconnect fabric
HFI
QDR Inﬁniband
FDR Inﬁniband
Operating system
AIX 7.1
AIX 7.1
RHEL 6.2
Compilers
XLF14, VAC12
XLF14, VAC12
IFORT13, IC11
Storage nodes (GPFS)
4 NSD
10 NSD
10 NSD
Parallel environment
IBM PE
IBM PE
IBM PE
Queueing system
LoadLeveler
LoadLeveler
LSF
Libraries
MPI trace, MASS
MPI trace, MASS
MPI trace
Performance and Scaling of WRF
517

variation (3DVAR) data assimilation system, and a software architecture allowing for
computational parallelism and system extensibility. WRF is suitable for a broad
spectrum of applications across scales ranging from meters to thousands of kilometers.
The effort to develop WRF has been a collaborative partnership, principally among the
National Center for Atmospheric Research (NCAR), the National Oceanic and
Atmospheric Administration (the National Centers for Environmental Prediction
(NCEP) and the Forecast Systems Laboratory (FSL), the Air Force Weather Agency
(AFWA), the Naval Research Laboratory, Oklahoma University, and the Federal
Aviation Administration (FAA). WRF allows researchers the ability to conduct sim-
ulations reﬂecting either real data or idealized conﬁgurations. WRF provides opera-
tional forecasting a model that is ﬂexible and efﬁcient computationally, while offering
the advances in physics, numerics, and data assimilation contributed by the research
community.
3.1
WRF Benchmark Test Case
For this study, the WRF version 3.2 was used, installed and compiled on all tested
systems. The objective of this study was to examine the scalability of a large domain
WRF test case with frequent I/O for up to 6,144 cores [4]. This is a typical weather
workload on capacity oriented parallel systems. Thus, the WRF conﬁguration consisted
of a 5 km horizontal grid with 2200 grid points in the east west direction and 1600 grid
points in the north south direction. A total of 28 vertical layers were conﬁgured for this
test, the time step was set to 6 s, while WRF was set to perform a 12-h forecast, with
output frequency every forecast hour. Boundary conditions were ingested every three
hours.
3.2
WRF Compilation
WRF was compiled to execute in mixed mode parallelism (MPI + OpenMP) on all
systems. Large ﬁle support was enabled during compilation as each output step was
close to 7.5 GB of storage. The objective was to compile WRF with the most
aggressive compiler options on all systems, in order to maximize performance. As such
the code was compiled on the POWER systems to account for the VSX SIMD vector
units [10]. It was found that the XL FORTRAN compiler option to enable VSX
(-qsimd) was not effective towards overall performance gains, and thus not included.
On the other hand, enablement of the Intel vector AVX was beneﬁcial towards per-
formance on the Intel E5-2670 cores, despite the fact they do not support compound
multiply and add instructions, like the IBM POWER7 cores. The VSX units on the
POWER7 processors are enabled when intrinsic mathematical functions (log, exp,
pow, etc.) are evaluated via the use of the scalar MASS libraries.
Table 2, shows the compiler options used on all 3 systems.
Compiling on the POWER7 system with the most aggressive optimization ﬂags
(-O3 –qhot), forces the compiler to automatically insert intrinsic functions in a
vector form whenever possible (within simple do-loop constructs).
518
Z. Christidis

3.3
WRF Load Imbalances
When executing WRF on parallel systems, computational performance depends largely
on the workload imbalance among participating MPI tasks. Workload imbalances can
be attributed to:
• Communication imbalances among participating MPI tasks.
– MPI tasks at the boundaries of a computational domain communicate less data.
– I/O is performed by a single MPI task (task 0) or by the designated I/O or
quilting tasks. In the case of I/O write operations by the quilting tasks, com-
putation and I/O can be overlapped, except when the compute tasks have to send
their I/O data to the quilting tasks. Parallel I/O read operations with pnetcdf can
scale up to a certain degree, as I/O contention can occur.
• Computational imbalances among participating MPI tasks.
– Areas covered by land, have more complex physical processes associated with
them as opposed to ocean areas, hence requiring more processing time towards
their computation. In ocean areas, planetary boundary layer processes are much
simpler, since sea surface temperatures do not change during simple runs.
– Storm movement within the computational grid, requires more complex
microphysics for the computations of the non-convective precipitation in the
grid areas covered by speciﬁc MPI tasks. As such, convective precipitation in
certain areas of the computational grid has a detrimental effect towards the
balance of the computations among participating MPI tasks.
– Uneven grid distribution among participating MPI tasks is one of the major
reasons for computational imbalances in WRF, as its parallel implementation
does not allow for a dynamic workload distribution scheme. In many cases, the
grid cannot be distributed as evenly as possible among participating tasks.
Hence some tasks will have less workload, and most likely they will need to
wait longer, in order to receive necessary data from MPI tasks that have more
workload. When the workload is distributed among a large number of MPI
tasks, the chance of a workload imbalance becomes smaller. The designated
method for performing computations in this case, is to enable OpenMP threads
in order to carry the workload within each node dynamically. In turn, MPI can
Table 2. Compiler options and libraries used on all systems.
System
p775
p460
dx360M4
FCOPTIM
-O3 –qhot
–qarch = pwr7
–qtune = pwr7
-O3 –xAVX -fp-model
fast = 2 –ip
FCBASEOPTS
-qsmp = omp
–qcache = auto
–qﬂoat = rsqrt
-ip -fno-alias -w -ftz
-no-prec-div -no-prec-sqrt
-align all –openmp
Libraries
netcdf, pnetcdf,
massp7_simd,
mpihpm
netcdf, pnetcdf, mpitrace
Performance and Scaling of WRF
519

be used across nodes in order to exchange necessary data with other nodes for
parallel computational consistency.
3.4
WRF Tunable Parameters
The WRF namelist input ﬁle contains several tunable parameters that can affect both
performance and accuracy. It was decided not to alter any of the parameters that affect
accuracy of the generated forecast results on all systems. In addition, it was decided to
ﬁx the namelist.input parameters that affect performance to the ones that yield
the best possible performance on the POWER systems.
WRF I/O: WRF performed a 12-h forecast and outputted a 7.5 GB ﬁle for each
forecast hour.. In order to handle this I/O requirement, I/O quilting via dedicated I/O
tasks was enabled by increasing the parameter nio_tasks_per_group in the
namelist.input for the namelist_quilt block. It is noted that the I/O write oper-
ations take place asynchronously by the quilting tasks, while the computing tasks
perform computations. The message in the rsl.out.0000 ﬁle (Timing for Writing
wrfout) does not represent the actual I/O time, but the time it takes for the compute
tasks to perform the point-to-point MPI operations by sending their data to the I/O tasks
participating in the computation. It is also noted that the read operations are performed
synchronously by MPI task 0. In this case, while MPI task 0 reads the initial and
boundary data, the remainder compute tasks have to wait for the read to occur until the
necessary data is broadcasted by MPI task 0. Since this WRF test case involved a high
amount of I/O for the initial data ingest (6.5 GB) as well as the boundary data (2.0 GB
each) it was decided to introduce the parallel version of Netcdf (pnetcdf [12, 13]) in
order to read the initial and boundary data. It was found that quilting for the write
operations was quite efﬁcient, so there was no need to introduce pnetcdf for the writing
to the disk subsystem. The only drawback in this approach was the writing of the last
output ﬁle, before the termination of the WRF execution. Since the compute tasks
exited the computation at the end of the integration, the quilting tasks had to perform
the last I/O step synchronously.
It was neces-
sary to test WRF for a proper choice of the parameters nproc_x, nproc_y and
numtiles, present in the &domains block of the namelist.input ﬁle, in order
to achieve the best performance for all scaling runs. This particular choice for the above
parameters was non-trivial at ﬁrst, and all possible combinations for participating MPI
tasks and OpenMP threads were tested, to achieve optimum performance. The
parameters nproc_x and nproc_y essentially “shrink” the original domain (patch) to
smaller sub-domains that reside within the MPI tasks (Fig. 1).
It was observed that signiﬁcant performance improvements can be obtained with a
proper choice of the above parameters on the IBM POWER7 p775 and p460 systems.
This can be attributed to better cache utilization, as smaller contiguous arrays
(computational sub-domains) ﬁt more efﬁciently in local caches, and consequently
they are computed faster, especially in the cases of “thin” decompositions
(nproc_x < nproc_y) which promote computations with minimal stride (minimal
cache misses). In addition, loading north-south MPI communication buffers is done out
520
Z. Christidis

of stride, so the thinner the decomposition, the smaller the out-of-stride vector copies
from the application to the communication buffers. As a result cache misses could be
minimized. The third reason which seems to increase performance in thin decomposition
conﬁgurations could be the application of vector MASS library calls by the compiler,
when the options –O3 –qhot are selected during compilation. Performance of MASS
vector is essential when computing large vectors. Thus, when the x-direction is
decomposed across many MPI tasks, MASS vector calls are performed with a few
elements only, hurting computational efﬁciency. Figure 1 depicts two possible decom-
positions among 16 computational and 2 I/O tasks (total of 18 MPI tasks), controlled by
the choice of nproc_x and nproc_y, as well as nio_tasks_ per_group.
It is possible that a choice of nproc_x, nproc_y might lead to uneven workload
imbalance and at the same time yield the best performance. Thus it was important to
conduct several tests with various nproc_x and nproc_y, in order to obtain the
maximum performance for a given number of MPI tasks, without worrying that the
particular task partitioning would exhibit less than optimal workload balance.
3.5
WRF Runtime Environment
During test runs, it was found that the p775 and p460 systems exhibited better com-
putational throughput for low core counts when using SMT. Thus runs with up to 2048
cores on the p775 and 512 cores on the p460 were performed with SMT. In both
scenarios, one extra OpenMP thread was allocated for each MPI task within compute
nodes, such that the total MPI and OpenMP threads per node was equal to 64. For
scaling runs beyond 2048 (p775) and 512 (p460) cores, it was found that 32 compu-
tational tasks per node was sufﬁcient. No such behavior was exhibited on the dx360M4
system, where Hyper-threading was detrimental to overall performance for all scaling
runs. For high core counts, and to exploit better scalability, fewer MPI tasks were
assigned on each node for all systems, with 2 or 4 OpenMP threads to each MPI task.
0
4
8
12
13
9
14
15
10
11
5
1
6
7
2
3
0
1
8
9
2
3
10
11
4
5
12
13
6
7
14
15
nproc_x = 4
nproc_y = 4
nproc_x = 2
nproc_y = 8
16
17
16
17
nio_groups=1
nio_tasks_per_group=2
Fig. 1. Possible MPI task partitioning for WRF with 16 MPI and 2 I/O tasks
Performance and Scaling of WRF
521

Various tests were performed for each scaling run, to determine the best combi-
nation of MPI tasks and OpenMP threads, as well as nproc_x, nproc_y and numtiles.
Also, the number of quilting tasks was selected carefully, such that each node fully
contributed in I/O or computations. In addition, the code was linked to an MPI proﬁle
library in order to assess the amount of communication performed during the com-
putations. LoadLeveler scripts were used for all scaling runs, and the executable wrf.
exe was bound to physical cores on the system.
Parallel Netcdf (read option 11) was used to ingest the initial conditions (one time)
and boundary data (four times). It was found that for the core counts tested, this was a
superior method to read the provided data. For outputting the meteorological results
serial Netcdf (read option 2) was used in combination with at least four I/O (quilting
tasks) for each scaling run. Thus all output steps (13) were performed in an asyn-
chronous manner, except the ﬁnal output step, which was performed synchronously,
before the termination of WRF execution. Table 3 shows the MPI and OpenMP task
arrangement for each of the scaling runs on all systems.
4
WRF Scaling Runs
Scaling WRF runs were produced for 128, 256, 512, 1024, 1536, 2048, 3072, 4096,
and 6144 cores and various components, such as computation, communication and I/O
were isolated for analysis to better understand its scalability characteristics.
In order to perform identical comparisons on all tested systems, it was decided to
ﬁx all the input parameters in the namelist.input ﬁle. It is realized that the
particular choices of the nproc_x, nproc_y parameters favored the p775 and p460
systems due to cache effects arising from the choice of “thin” decompositions. Table 4
reports the total elapsed time for 6144 cores (18 × 85 MPI task arrangement) on the
p775, p640 and dx360M4 systems. The last column includes the elapsed time on the
dx360M4 system for a 30 × 51 task arrangement, which is the best result on 6144 core
runs among all systems.
WRF internal timers report elapsed times for each compute and I/O time step. The
elapsed times for read or write operations are accumulated into the time step following
the I/O operation. The elapsed time to read the initial conditions is not summed over the
Table 3. Runtime parameters chosen for each scaling run.
Number of
cores
MPI
tasks
nproc_x ×
nproc_y
nio_groups ×
nio_tasks_per_group
OpenMP threads
numtiles
p775
p460
dx360M4
128
124
4 × 31
1 × 4
2
2
1
4
256
252
6 × 42
1 × 4
2
2
1
4
512
504
8 × 63
1 × 8
2
2
1
4
1024
506
11 × 46
1 × 6
4
2
2
4
1536
760
19 × 40
1 × 8
4
2
2
4
2048
1020
20 × 51
1 × 4
4
2
2
4
3072
760
19 × 40
1 × 8
4
4
4
4
4096
1020
17 × 60
1 × 4
4
4
4
4
6144
1530
18 × 85
1 × 6
4
4
4
4
522
Z. Christidis

computational time steps. Finally, the elapsed time due to the last I/O step is derived
from the total elapsed time, minus the sum of all computational time steps (which
include I/O) plus the ﬁrst I/O step to ingest the initial conditions. This is a conservative
estimate, since the total elapsed time includes loading of the executable, reading of
namelist.input ﬁle, initialization of parallel tasks, termination of parallel tasks
and vacation of the executable from the queue. In order to have a better understanding
of the WRF runtime parameters on the tested systems, further analysis was performed
with the aid of the included MPI proﬁling libraries.
1. The total runtime without I/O was extracted by summing the elapsed times of the
computational steps, removing the elapsed times due to the 12 asynchronous I/O
steps and the last synchronous I/O write step, and the reading of the boundary
conditions from the 4 boundary data ingests.
2. Communication was deﬁned as the minimum communication measured among all
the MPI tasks involved.
3. Load imbalance was deﬁned as the difference of the median and the minimum
communication time among all MPI tasks.
4. Total computation in item 1 above, was broken into pure computation plus com-
munication (item 2) plus Load imbalance (item 3).
5. Total I/O was computed as the time it takes to read the initial and boundary data
plus the time to write the 12 asynchronous and the last synchronous write step.
Figures 2, 3 and 4 display the total elapsed times with all the above components on the
p775, p460 and dx360M4 systems respectively.
Table 4. 6144-core Elapsed times for p775 p460 and dx360M4 systems
System
p775
p460
dx360M4
dx360M4
nproc_x × nproc_y
18 × 85
18 × 85
18 × 85
30 × 51
Elapsed time (s)
830.7
874.1
807.2
759.4
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
128
256
512
1024
1536
2048
3072
4096
6144
p775 Total Elapsed time (seconds)
Number of p775 Cores
Write I/O
Read I/O
Initialization+termination
Load imbalance
Communication
Computation
Fig. 2. Runtime components of WRF on the p775 system.
Performance and Scaling of WRF
523

It can be seen that the p775 exhibits superior performance for low core counts,
while the computation scales well in all three tested systems.
Figure 5 displays the most important components of the WRF runs on all systems.
It is observed that the communication time does not change signiﬁcantly for large
core counts on all systems, which is characteristic of grid-point weather models with
dominant near-neighbor communication. In addition, load imbalance while relatively
signiﬁcant for low core counts, tends to diminish with high core counts, as expected.
Figure 6 displays the average compute time per time step on all systems for the
scaling runs, the average write time per write step, and the average read time per
time step. It can be seen that the computation scales well on all systems, while both the
average read and write per time step are rather insigniﬁcant for the entire run, mainly
due to the effect of quilting for the writes, and parallel netcdf for the read operations
for ingesting the boundary conditions. It is noted that write I/O operations are more
efﬁcient when SMT is not employed. In addition, read I/O operations with the parallel
netcdf library are shown to rise when more cores are used, which can be attributed to
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
128
256
512
1024
1536
2048
3072
4096
6144
p460 Total Elapsed time 
(seconds)
Number of p460 Cores
Write I/O
Read I/O
Initialization+termination
Load imbalance
Communication
Computation
Fig. 3. Runtime components of WRF on the p460 system.
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
128
256
512
1024
1536
2048
3072
4096
6144
dx360M4 Total Elapsed time 
(seconds)
Number of dx360M4 Cores
Write I/O
Read I/O
Initialization+termination
Load imbalance
Communication
Computation
Fig. 4. Runtime components of WRF on the dx360M4 system.
524
Z. Christidis

declining parallel I/O scalability. In addition, the 3072-core run was conducted with 8
quilting tasks and hence demonstrates efﬁcient write I/O rates per each write time
step. Other runs were conducted with the best possible combination of compute and I/O
tasks, with focus to maximize overall performance for a given number of cores.
GFLOP rates were obtained for all scaling runs on the p775 system, by instru-
menting each run with the hardware performance monitor (HPM) of POWER7 [14].
The GFLOP rates with 128 cores show that WRF has achieved a 10 % performance off
peak. Peak performance is derived from the core frequency (3.84 GHz) times the
maximum ﬂoating point operations per cycle (8) times the total cores used (128). Thus
the peak performance of four nodes (128 cores) is 3932 GFLOPs. Similarly, the 6144
core run (peak performance 188744 GFLOPs) yielded 8140 GFLOPs, which is 4.3 %
off peak. The POWER7 processor incorporates special purpose registers for the HPM
which are initialized and get accumulated during each run with relevant ﬂoating point
operations. Hence, sustained GFLOP rates are computed as:
The sustained GFLOP rates for the p460 and dx360M4 systems are approximated as:
0
5000
10000
15000
20000
25000
128 256 512 102415362048307240966144
Elapsed Time (sec)
 
Number Of Cores 
p775
dx360M4
p460
0
100
200
300
400
500
600
700
800
900
1000
128 256 512 102415362048307240966144
Communication  Time (sec)
 
Number Of Cores 
p775
dx360M4
p460
0
500
1000
1500
2000
2500
128 256 512 102415362048307240966144
Load Imbalance  Time (sec)
 
Number Of Cores 
p775
dx360M4
p460
Fig. 5. Total computation, communication and Load imbalance times on all tested systems
0
1
1
2
2
3
3
128
256
512 1024 1536 2048 3072 4096 6144
Average compute time per time step (seconds)
 
Number Of Cores 
p775
dx360M4
p460
0
5
10
15
20
25
128 256 512 1024 1536 2048 3072 4096 6144
Average read time per read time step (seconds)
 
Number Of Cores 
p775
dx360M4
p460
0
1
2
3
4
5
6
128 256 512 102415362048307240966144
Average write time per write  time step (seconds)
 
Number Of Cores 
p775
dx360M4
p460
Fig. 6. Average times per time steps for computation, read and write operations on all systems.
Performance and Scaling of WRF
525

Figure 7a displays the sustained GFLOP rates for all systems for the WRF scaling
runs, while Fig. 7b displays the percentage off-peak on all tested systems. It can be seen
that the dx360M4 system delivers the best sustained performance on all systems, even
though its cores run at the lowest frequency. This can be clearly attributed to the Intel
compiler capability, which can deliver efﬁcient AVX vector instructions.
5
Summary and Conclusions
The speciﬁc WRF test case examined in this report scales quite well with the amount of
cores tested. It is expected that scaling will continue for more than 6144 cores, since the
communication and load imbalance effects diminish with an increased number of cores,
while the computation continues to scale well. The only limiting scalability component
is I/O, for both read and write operations and for many cores. This is indicative of the
increasing I/O rates as the number of cores increases.
The code sustained close to 14 % off peak on 8 dx360M4 nodes, and about to 6 %
off peak with 384 nodes, despite the fact this particular test case performed heavy I/O,
outputting thirteen 7.5-GB ﬁles for every integration hour, while reading compatible in
size initial data and boundary conditions.
The Intel FORTRAN compiler was quite effective in producing efﬁcient AVX
vector instructions on the E5-2670 Intel Sandy-bridge cores, which along with the
single-precision nature of the WRF code were the top reasons for the highest sustained
performance among all tested systems. The vector units on the POWER systems (VSX)
did not deliver additional performance when WRF was compiled with appropriate
SIMD options, and hence were not used. The compiler generated vector MASS calls
for the intrinsic functions on the POWER systems were quite effective and contributed
with up to 15 % in additional performance with 128 cores.
The proper choice of the runtime parameters nproc_x and nproc_y were critical
towards better performance on the p775 and p460 systems when thin rectangular
decompositions were selected (nproc_x > nproc_y). In contrast, performance on
the dx360M4 system was favored under square task decompositions. The runtime
choice of the parameter numtiles did not affect performance on the p775 and p460
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
128
256
512
1024 1536 2048 3072 4096 6144
GFLOPS  Sustained
 
Number Of Cores
 
p775 GFLOPS Sustained
dx360M4 GFLOPS Sustained
p460 GFLOPS Sustained
0
2
4
6
8
10
12
14
16
128
256
512
1024 1536 2048 3072 4096 6144
Percent (%) Sustained of Peak 
Performance 
Number Of Cores
 
p775
dx360M4
p460
Fig. 7. Sustained GFLOP rates (a) and percent sustained off peak (b) for the WRF scaling runs.
526
Z. Christidis

systems, while it had a positive effect in the dx360M4 system when it was set to twice
the number of OpenMP threads.
SMT was beneﬁcial to the systems with POWER processors, especially to the
p775, due to the cores having twice as many channels to memory as compared to
the p460 processors. Hyper-threading did not have a positive performance effect on the
dx360M4 system, and hence was not employed.
The interconnect fabric of all three systems performed effectively, mainly due to the
near-neighbor communication characteristics in the MPI implementation of WRF,
which did not stress extensively the communication subsystems on all tested
supercomputers.
Acknowledgements. The author wishes to acknowledge the China Meteorological Adminis-
tration and Ms. Wei Min, for conﬁguring WRF and preparing the Initial and boundary conditions
for the runs. Acknowledgement is extended to Mr. James Abeles of IBM for performing the
WRF scaling runs on the iDataPlex dx360M4 system.
References
1. Weather Research and Forecasting (WRF) model. http://www.wrf-model.org
2. Skamarock, W.C., Klemp, J.B., Dudhia, J., Gill, D.O., Barker, D.M., Duda, M., Huang,
X.-Y., Wang, W., Powers, J.G.: A Description of the Advanced Research WRFVersion 3,
NCAR Technical Note (2008)
3. WRF V3 Parallel Benchmark Page. http://www.mmm.ucar.edu/wrf/WG2/benchv3/
4. Michalakes, J., Hacker, J., Loft, R., McCracken, M.O., Snavely, A., Wright, N.J., Spelce, T.,
Gorda, B., Walkup, R.: WRF nature run. J. Physics: Conf. Series 125(1), 012022 (2008).
SciDAC 2008, 13–17 July 2008, Washington, USA, 2008
5. The IBM p775 System. http://www.redbooks.ibm.com/redbooks/pdfs/sg248003.pdf
6. The IBM p460 System. http://www.redbooks.ibm.com/redbooks/pdfs/sg247989.pdf
7. The IBM dx360M4 iDataPlex. http://www.redbooks.ibm.com/redbooks/pdfs/sg247629.pdf
8. The IBM GPFS. http://www.redbooks.ibm.com/redbooks/pdfs/sg247844.pdf
9. Langkamp, T., Bohner, J.: Inﬂuence of the compiler on multi-CPU performance of WRFv3.
Geosci. Model Dev. 4, 611–623 (2011)
10. https://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.
cluster.essl.v5r2.essl100.doc%2Fam5gr_servsx.htm
11. http://www-01.ibm.com/support/docview.wss?rs=2021&uid=swg27005374
12. NetCDF. http://www.unidata.ucar.edu/software/netcdf/
13. Jianwei L., Liao, W.-K., Choudhary, A., Ross, R., Thakur, R., Gropp, W., Latham, R.,
Siegel, A., Gallagher, B., Zingale, M.: Parallel netCDF: a scientiﬁc high-performance I/O
interface. In: Proceedings of the 15th Supercomputing Conference, Phoenix, AZ, November
2003
14. https://www.power.org/wp-content/uploads/2012/09/POWER7_PMU_Detailed_Event_
Description.pdf
15. Hoisie, A., Johnson, G., Kerbyson, D.J., Lang, M., Pakin, S.: A performance comparison
through benchmarking and modeling of three leading supercomputers: blue Gene/L, red
storm, and purple. In: Proceedings of IEEE/ACM Supercomputing (SC 2006), Tampa, FL
(2006)
Performance and Scaling of WRF
527

16. Arnold, D., Morton, D., Schicker, I., Jorba, O., Harrison, K., Zabloudil, J., Newby, G.,
Seibert, P.: WRF benchmark for regional applications. In: WRF User’s Workshop (2011)
17. Morton, D., Nudson, O., Stephenson, C.: Benchmarking and evaluation of the weather
research and forecasting (WRF) model on the Cray XT5. In: Proceedings of CUG (2009)
18. Kerbyson, D.J., Barker, K.J., Davis, K.: Analysis of the weather research and forecasting
(WRF) model on large-scale systems. In: Proceedings of the Conference on Parallel
Computing. Advances in Parallel Computing, vol. 15. IOS Press (2008). ISSN 0927-5452,
ISBN 978-1-58603-796-3
19. Porter, A.R., Ashworth, M.: Conﬁguring and optimizing the weather research and forecast
model on the CRAY XT. In: 2010 Cray User Group Proceedings
528
Z. Christidis

Author Index
Aithal, Shashi M.
87
Aluru, Srinivas
122
an Mey, Dieter
358
Anderson, Michael J.
48
Anghel, Andreea
472
Anzt, Hartwig
1
Auweter, Axel
376
Awan, Ammar Ahmad
434
Bach, Matthias
179
Bader, Michael
340
Bailey, Peter E.
394
Barrett, Dave
139
Bartz, Christopher
274
Baumeister, Paul F.
96
Bayik, Utku
409
Bayindir, Ismail Ugur
409
Bergman, Keren
454
Berzins, Martin
212
Betke, Eugen
257
Bientinesi, Paolo
155
Bode, Arndt
376
Boettiger, Hans
96
Breuer, Alexander
340
Brunheroto, José R.
96
Chakraborty, Sourav
434
Chalyshev, Vladimir
231
Chasapis, Konstantinos
274
Chen, Cheng
197
Chow, Edmond
1, 122
Christidis, Zaphiris
514
Cui, Yingbo
74
Das, Dipankar
48
de Supinski, Bronis R.
394
Dittmann, Gero
472
Dong, Tingxing Tim
31
Dongarra, Jack
1, 31, 58
Dubey, Pradeep
48
Freisleben, Bernd
307
Gao, Yuxiang
17
Gaudin, Wayne
139
Getmanskiy, Victor
231
Glick, Madeleine
454
Graubner, Pablo
307
Guney, Isa Ahmet
409
Haidar, Azzam
31, 58
Hamidouche, Khaled
434
Hammond, Simon D.
454
Harman, Todd
212
Hater, Thorsten
96
Heckmann, Patrick
307
Heinecke, Alexander
340
Higgins, Joshua
506
Holmes, Violeta
506
Hori, Atsushi
282
Humphrey, Alan
212
Iliev, Hristo
358
Ionkov, Latchesar
291
Ishikawa, Yutaka
282
Ismail, Ahmed E.
155
Kabir, Khairul
58
Khrenova, Maria
113
Kogge, Peter M.
323
Kranzlmüller, Dieter
422
Kryzhanovsky, Dmitriy
231
Kucuk, Gurhan
409
Kuhn, Michael
240, 274
Kunkel, Julian
257
Lang, Michael
291
Leksikov, Evgeny
231
Leong, Siew Hoon
422
Liao, Xiangke
74, 197
Lindenstruth, Volker
179
Liu, Jie
171
Liu, Xing
122
Lopatin, Igor
231
Lowenthal, David K.
394
Lu, Yutong
74, 171
Ludwig, Thomas
274
Luszczek, Piotr
31
Marathe, Aniruddha
394
Maurer, Thilo
96

Md., Vasimuddin
122
Minkenberg, Cyriel
472
Mironov, Vladimir
113
Misra, Sanchit
122
Moreland, Kenneth
488
Moskovsky, Alexander
113
Müller, Matthias S.
358
Nerge, Petra
274
Nešković, Gvozden
179
Nobile, Andrea
96
Oldﬁeld, Ron
488
Pamnany, Kiran
122
Panda, Dhabaleswar K.
434
Panourgias, Iakovos
139
Park, Jongsoo
48
Parsons, Mark
139
Patwary, Md. Mostofa Ali
48
Pekurovsky, Dmitry
434
Peng, Shaoliang
74, 171
Philipsen, Owe
179
Pinke, Christopher
179
Pirogov, Vadim O.
48
Pleiter, Dirk
96
Pospiech, Christoph
497
Prisacari, Bogdan
472
Pudov, Sergey G.
48
Quan, Zhe
197
Rannabauer, Leonhard
340
Rodrigues, Arun
454
Rodriguez, German
472
Rohr, David
179
Rountree, Barry
394
Rumley, Sébastien
454
Satish, Nadathur Rajagopalan
48
Schulz, Martin
394
Serdaroglu, Kemal Cagri
409
Shoukourian, Hayk
376
Smith, Phillip
212
Springer, Paul
155
Subramoni, Hari
434
Sundaram, Narayanan
48
Tang, Tao
197
Tomko, Karen
434
Tomov, Stanimire
31, 58
Tsujita, Yuichi
282
Turland, David
139
Vadlamudi, Satya Gautam
48
Venkatesh, Akshay
434
Venters, Colin
506
Wang, Bingqiang
74
Wang, Heng
171
Weiland, Michele
139
Wen, Jiajun
171
Wienke, Sandra
358
Wild, Stefan M.
87
Wilde, Torsten
376
Wu, Chengkun
74, 171
Yang, Canqun
74, 197
Yildiz, Abdullah
409
Zhang, Peng
17
Zhu, Xiaoqian
171
Zimmer, Michaela
257
530
Author Index


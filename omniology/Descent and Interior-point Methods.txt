Lars-Åke Lindahl
Descent and Interior-point
Methods
Convexity and Optimization – Part III
Download free books at

ii
 
LARS-ÅKE LINDAHL 
DESCENT AND 
INTERIOR-POINT 
METHODS 
CONVEXITY AND 
OPTIMIZATION – PART III
Download free eBooks at bookboon.com

iii
Descent and Interior-point Methods: Convexity and Optimization – Part III
1st edition
© 2016 Lars-Åke Lindahl & bookboon.com
ISBN 978-87-403-1384-0
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
iv
Contents
iv
CONTENTS
	
To see Part II, download: Linear and Convex Optimization: Convexity  
and Optimization – Part II
	
Part I. Convexity
1	
Preliminaries	
Part I
2	
Convex sets	
Part I
2.1	
Affine sets and affine maps	
Part I
2.2	
Convex sets	
Part I
2.3	
Convexity preserving operations	
Part I
2.4	
Convex hull	
Part I
2.5	
Topological properties	
Part I
2.6	
Cones	
Part I
2.7	
The recession cone	
Part I
	
Exercises	
Part I
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
v
Contents
3	
Separation	
Part I
3.1	
Separating hyperplanes	
Part I
3.2	
The dual cone	
Part I
3.3	
Solvability of systems of linear inequalities	
Part I
	
Exercises	
Part I
4	
More on convex sets	
Part I
4.1	
Extreme points and faces	
Part I
4.2	
Structure theorems for convex sets	
Part I
	
Exercises	
Part I
5	
Polyhedra	
Part I
5.1	
Extreme points and extreme rays	
Part I
5.2	
Polyhedral cones	
Part I
5.3	
The internal structure of polyhedra	
Part I
5.4	
Polyhedron preserving operations	
Part I
5.5	
Separation	
Part I
	
Exercises	
Part I
6	
Convex functions	
Part I
6.1	
Basic definitions	
Part I
6.2	
Operations that preserve convexity	
Part I
6.3	
Maximum and minimum	
Part I
6.4	
Some important inequalities	
Part I
6.5	
Solvability of systems of convex inequalities	
Part I
6.6	
Continuity	
Part I
6.7	
The recessive subspace of convex functions	
Part I
6.8	
Closed convex functions	
Part I
6.9	
The support function	
Part I
6.10	
The Minkowski functional	
Part I
	
Exercises	
Part I
7	
Smooth convex functions	
Part I
7.1	
Convex functions on R	
Part I
7.2	
Differentiable convex functions	
Part I
7.3	
Strong convexity	
Part I
7.4	
Convex functions with Lipschitz continuous derivatives	
Part I
	
Exercises	
Part I
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
vi
Contents
8	
The subdifferential	
Part I
8.1	
The subdifferential	
Part I
8.2	
Closed convex functions	
Part I
8.3	
The conjugate function	
Part I
8.4	
The direction derivative	
Part I
8.5	
Subdifferentiation rules	
Part I
	
Exercises	
Part I
	
Bibliografical and historical notices	
Part I
	
References	
Part I
	
Answers and solutions to the exercises	
Part I
	
Index	
Part I
	
Endnotes	
Part I
	
Part II. Linear and Convex Optimization
	
Preface	
Part II
	
List of symbols	
Part II
9	
Optimization	
Part II
9.1	
Optimization problems	
Part II
9.2	
Classification of optimization problems	
Part II
9.3	
Equivalent problem formulations	
Part II
9.4	
Some model examples	
Part II
	
Exercises	
Part II
10	
The Lagrange function	
Part II
10.1	
The Lagrange function and the dual problem	
Part II
10.2	
John’s theorem	
Part II
	
Exercises	
Part II
11	
Convex optimization	
Part II
11.1	
Strong duality	
Part II
11.2	
The Karush-Kuhn-Tucker theorem	
Part II
11.3	
The Lagrange multipliers	
Part II
	
Exercises	
Part II
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
vii
Contents
12	
Linear programming	
Part II
12.1	
Optimal solutions	
Part II
12.2	
Duality	
Part II
	
Exercises	
Part II
13	
The simplex algorithm	
Part II
13.1	
Standard form	
Part II
13.2	
Informal description of the simplex algorithm	
Part II
13.3	
Basic solutions	
Part II
13.4	
The simplex algorithm	
Part II
13.5	
Bland’s anti cycling rule	
Part II
13.6	
Phase 1 of the simplex algorithm	
Part II
13.7	
Sensitivity analysis	
Part II
13.8	
The dual simplex algorithm	
Part II
13.9	
Complexity	
Part II
	
Exercises	
Part II
	
Bibliografical and historical notices	
Part II
	
References	
Part II
	
Answers and solutions to the exercises	
Part II
	
Index	
Part II
	
Part III. Descent and Interior-point Methods	
	
Preface	
ix
	
List of symbols	
x
14	
Descent methods	
1
14.1	
General principles	
1
14.2	
The gradient descent method	
7
	
Exercises	
12
15	
Newton’s method	
13
15.1	
Newton decrement and Newton direction	
13
15.2	
Newton’s method	
22
15.3	
Equality constraints	
34
	
Exercises	
39
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
viii
Contents
16	
Self-concordant functions	
41
16.1	
Self-concordant functions	
42
16.2	
Closed self-concordant functions	
47
16.3	
Basic inequalities for the local seminorm	
51
16.4	
Minimization	
56
16.5	
Newton’s method for self-concordant functions	
61
	
Exercises	
67
	
Appendix	
68
17	
The path-following method	
73
17.1	
Barrier and central path	
74
17.2	
Path-following methods	
78
18	
The path-following method with self-concordant barrier	
83
18.1	
Self-concordant barriers	
83
18.2	
The path-following method	
94
18.3	
LP problems	
108
18.4	
Complexity	
114
	
Exercises	
125
	
Bibliografical and historical notices	
127
	
References	
128
	
Answers and solution to the exercises	
130
	
Index	
136
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
ix
Preface
Preface
This third and ﬁnal part of Convexity and Optimization discusses some opti-
mization methods which when carefully implemented are eﬃcient numerical
optimization algorithms.
We begin with a very brief general description of descent methods and
then proceed to a detailed study of Newton’s method. For a particular class
of functions, the so-called self-concordant functions, discovered by Yurii Nes-
terov and Arkadi Nemirovski, it is possible to describe the convergence rate
of Newton’s method with absolute constants, and we devote one chapter to
this important class.
Interior-point methods are algorithms for solving constrained optimiza-
tion problems. Contrary to the simplex algorithms, they reach the optimal
solution by traversing the interior of the feasible region. Any convex opti-
mization problem can be transformed into minimizing a linear function over
a convex set by converting to the epigraph form and with a self-concordant
function as barrier, and Nesterov and Nemirovski showed that the number
of iterations of the path-following algorithm is bounded by a polynomial in
the dimension of the problem and the accuracy of the solution. Their proof
is described in this book’s ﬁnal chapter.
Uppsala, April 2015
Lars-˚
Ake Lindahl
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
x
List of symbols
List of symbols
bdry X
boundary of X, see Part I
cl X
closure of X, see Part I
dim X
dimension of X, see Part I
dom f
the eﬀective domain of f: {x | −∞< f(x) < ∞}, see Part I
epi f
epigraph of f, see Part I
ext X
set of extreme points of X, see Part I
int X
interior of X, see Part I
lin X
recessive subspace of X, see Part I
recc X
recession cone of X, see Part I
ei
ith standard basis vector (0, . . . , 1, . . . , 0)
f ′
derivate or gradient of f, see Part I
f ′′
second derivative or hessian of f, see Part I
vmax, vmin
optimal values, see Part II
B(a; r)
open ball centered at a with radius r
B(a; r)
closed ball centered at a with radius r
Df(a)[v]
diﬀerential of f at a, see Part I
D2f(a)[u, v]
n
i,j=1
∂2f
∂xi∂xj (a)uivj, see Part I
D3f(a)[u, v, w]
n
i,j,k=1
∂3f
∂xi∂xj∂xk (a)uivjwk, see Part I
E(x; r)
ellipsoid {y | ∥y −x∥x ≤r}, p. 88
L
input length, p. 115
L(x, λ)
Lagrange function, see Part II
R+, R++
{x ∈R | x ≥0}, {x ∈R | x > 0}
R−
{x ∈R | x ≤0}
R, R, R
R ∪{∞}, R ∪{−∞}, R ∪{∞, −∞}
Sµ,L(X)
class of µ-strongly convex functions on X with
L-Lipschitz continuous derivative, see Part I
VarX(v)
supx∈X⟨v, x⟩−infx∈X⟨v, x⟩, p. 93
X+
dual cone of X, see Part I
1
the vector (1, 1, . . . , 1)
λ(f, x)
Newton decrement of f at x, p. 16
πy
translated Minkowski functional, p. 89
ρ(t)
−t −ln(1 −t), p. 51
∆xnt
Newton direction at x, p. 15
∇f
gradient of f
[x, y]
line segment between x and y
]x, y[
open line segment between x and y
∥·∥1, ∥·∥2, ∥·∥∞
ℓ1-norm, Euclidean norm, maximum norm, see Part I
∥·∥x
the seminorm

⟨· , f ′′(x)·⟩, p. 18
∥v∥∗
x
dual local seminorm sup∥w∥x≤1⟨v, w⟩, p. 92
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
1
Descent methods
Chapter 14
Descent methods
The most common numerical algorithms for minimization of diﬀerentiable
functions of several variables are so-called descent algorithms.
A descent
algorithm is an iterative algorithm that from a given starting point gener-
ates a sequence of points with decreasing function values, and the process is
stopped when one has obtained a function value that approximates the min-
imum value good enough according to some criterion. However, there is no
algorithm that works for arbitrary functions; special assumptions about the
function to be minimized are needed to ensure convergence towards the min-
imum point. Convexity is such an assumption, which makes it also possible
in many cases to determine the speed of convergence.
This chapter describes descent methods in general terms, and we exem-
plify with the simplest descent method, the gradient descent method.
14.1
General principles
We shall study the optimization problem
(P)
min f(x)
where f is a function which is deﬁned and diﬀerentiable on an open subset
Ωof Rn. We assume that the problem has a solution, i.e. that there is an
optimal point ˆx ∈Ω, and we denote the optimal value f(ˆx) as fmin. A con-
venient assumption which, according to Corollary 8.1.7 in Part I, guarantees
the existence of a (unique) optimal solution is that f is strongly convex and
has some closed nonempty sublevel set.
Our aim is to generate a sequence x1, x2, x3, . . . of points in Ωfrom a
given starting point x0 ∈Ω, with decreasing function values and with the
property that f(xk) →fmin as k →∞. In the iteration leading from the
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
2
Descent methods
point xk to the next point xk+1, except when xk is already optimal, one ﬁrst
selects a vector vk such that the one-variable function φk(t) = f(xk + tvk) is
strictly decreasing at t = 0. Then, a line search is performed along the half-
line xk + tvk, t > 0, and a point xk+1 = xk + hkvk satisfying f(xk+1) < f(xk)
is selected according to speciﬁc rules.
The vector vk is called the search direction, and the positive number
hk is called the step size. The algorithm is terminated when the diﬀerence
f(xk) −fmin is less than a given tolerance.
Schematically, we can describe a typical descent algorithm as follows:
Descent algorithm
Given a starting point x ∈Ω.
Repeat
1. Determine (if f ′(x) ̸= 0) a search direction v and a step size h > 0 such
that f(x + hv) < f(x).
2. Update: x:= x + hv.
until stopping criterion is satisﬁed.
Diﬀerent strategies for selecting the search direction, diﬀerent ways to
perform the line search, as well as diﬀerent stop criteria, give rise to diﬀerent
algorithms, of course.
Search direction
Permitted search directions in iteration k are vectors vk which satisfy the
inequality
⟨f ′(xk), vk⟩< 0,
because this ensures that the function φk(t) = f(xk + tvk) is decreasing at
the point t = 0, since φ′
k(0) = ⟨f ′(xk), vk⟩. We will study two ways to select
the search direction.
The gradient descent method selects vk = −f ′(xk), which is a permissible
choice since ⟨f ′(xk), vk⟩= −∥f ′(xk)∥2 < 0. Locally, this choice gives the
fastest decrease in function value.
Newton’s method assumes that the second derivative exists, and the search
direction at points xk where the second derivative is positive deﬁnite is
vk = −f ′′(xk)−1f ′(xk).
This choice is permissible since ⟨f ′(xk), vk⟩= −⟨f ′(xk), f ′′(xk)−1f ′(xk)⟩< 0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
3
Descent methods
3
Line search
Given the search direction vk there are several possible strategies for selecting
the step size hk.
1. Exact line search. The step size hk is determined by minimizing the one-
variable function t →f(xk +tvk). This method is used for theoretical studies
of algorithms but almost never in practice due to the computational cost of
performing the one-dimensional minimization.
2. The step size sequence (hk)∞
k=1 is given a priori, for example as hk = h or
as hk = h/
√
k + 1 for some positive constant h. This is a simple rule that is
often used in convex optimization.
3. The step size hk at the point xk is deﬁned as hk = ρ(xk) for some given
function ρ. This technique is used in the analysis of Newton’s method for
self-concordant functions.
4. Armijo’s rule. The step size hk at the point xk depends on two parameters
α, β ∈]0, 1[ and is deﬁned as
hk = βm,
where m is the smallest nonnegative integer such that the point xk + βmvk
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
4
Descent methods
lies in the domain of f and satisﬁes the inequality
(14.1)
f(xk + βmvk) ≤f(xk) + αβm⟨f ′(xk), vk⟩.
Such an m certainly exists, since βn →0 as n →∞and
lim
t→0
f(xk + tvk) −f(xk)
t
= ⟨f ′(xk), vk⟩< α ⟨f ′(xk), vk⟩.
The number m is determined by simple backtracking: Start with m = 0
and examine whether xk + βmvk belongs to the domain of f and inequality
(14.1) holds. If not, increase m by 1 and repeat until the conditions are
fulﬁlled. Figure 14.1 illustrates the process.
βm
β2
1
β
t
f(xk)
f(xk + tvk)
f(xk) + t⟨f′(xk), vk⟩
f(xk) + αt⟨f′(xk), vk⟩
Figure 14.1.
Armijo’s rule: The step size is hk = βm,
where m is the smallest nonnegative integer such that
f(xk + βmvk) ≤f(xk) + αβm⟨f′(xk), vk⟩.
The decrease in iteration k of function value per step size, i.e. the ratio
(f(xk)−f(xk+1))/hk, is for convex functions less than or equal to −⟨f ′(xk), vk⟩
for any choice of step size hk. With step size hk selected according to Armijo’s
rule the same ratio is also ≥−α⟨f ′(xk), vk⟩. With Armijo’s rule, the decrease
per step size is, in other words, at least α of what the maximum might be.
Typical values of α in practical applications lie in the range between 0.01
and 0.3.
The parameter β determines how many backtracking steps are needed.
The larger β, the more backtracking steps, i.e. the ﬁner the line search. The
parameter β is often chosen between 0.1 and 0.8.
Armijo’s rule exists in diﬀerent versions and is used in several practical
algorithms.
Stopping criteria
Since the optimum value is generally not known beforehand, it is not pos-
sible to formulate the stopping criterion directly in terms of the minimum.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
5
Descent methods
Intuitively, it seems reasonable that x should be close to the minimum point
if the derivative f ′(x) is comparatively small, and the next theorem shows
that this is indeed the case, under appropriate conditions on the objective
function.
Theorem 14.1.1. Suppose that the function f : Ω→R is diﬀerentiable, µ-
strongly convex and has a minimum at ˆx ∈Ω. Then, for all x ∈Ω
f(x) −f(ˆx) ≤1
2µ∥f ′(x)∥2
and
(i)
∥x −ˆx∥≤1
µ∥f ′(x)∥.
(ii)
Proof. Due to the convexity assumption,
(14.2)
f(y) ≥f(x) + ⟨f ′(x), y −x⟩+ 1
2µ∥y −x∥2
for all x, y ∈Ω. The right-hand side of inequality (14.2) is a convex quadratic
function in the variable y, which is minimized by y = x −µ−1f ′(x), and the
minimum is equal to f(x) −1
2µ−1∥f ′(x)∥2. Hence,
f(y) ≥f(x) −1
2µ−1∥f ′(x)∥2
for all y ∈Ω, and we obtain the inequality (i) by choosing y as the minimum
point ˆx.
Now, replace y with x and x with ˆx in inequality (14.2). Since f ′(ˆx) = 0,
the resulting inequality becomes
f(x) ≥f(ˆx) + 1
2µ∥x −ˆx∥2,
which combined with inequality (i) gives us inequality (ii).
We now return to the descent algorithm and our discussion of the the
stopping criterion. Let
S = {x ∈Ω| f(x) ≤f(x0)},
where x0 is the selected starting point, and assume that the sublevel set S
is convex and that the objective function f is µ-strongly convex on S. All
the points x1, x2, x3, . . . that are generated by the descent algorithm will of
course lie in S since the function values are decreasing. Therefore, it follows
from Theorem 14.1.1 that f(xk) < fmin + ϵ if ∥f ′(xk)∥< (2µϵ)1/2.
As a stopping criterion, we can thus use the condition
∥f ′(xk)∥≤η,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
6
Descent methods
6
which guarantees that f(xk) −fmin ≤η2/2µ and that ∥xk −ˆx∥≤η/µ. A
problem here is that the convexity constant µ is known only in rare cases.
So the stopping condition ∥f ′(xk)∥≤η can in general not be used to give
precise bounds on f(xk) −fmin. But Theorem 14.1.1 veriﬁes our intuitive
feeling that the diﬀerence between f(x) and fmin is small if the gradient of f
at x is small enough.
Convergence rate
Let us say that a convergent sequence x0, x1, x2, . . . of points with limit ˆx
converges at least linearly if there is a constant c < 1 such that
(14.3)
∥xk+1 −ˆx∥≤c∥xk −ˆx∥
for all k, and that the convergence is at least quadratic if there is a constant
C such that
(14.4)
∥xk+1 −ˆx∥≤C∥xk −ˆx∥2
for all k.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
7
Descent methods
We also say that the convergence is no better than linear and no better
than quadratic if
lim
k→∞
∥xk+1 −ˆx∥
∥xk −ˆx∥α > 0
for α = 1 and α = 2, respectively.
Note that inequality (14.3) implies that the sequence (xk)∞
0 converges to
ˆx, because it follows by induction that
∥xk −ˆx∥≤ck∥x0 −ˆx∥
for all k.
Similarly, inequality (14.4) implies that the sequence (xk)∞
0 convergences
to ˆx if the starting point x0 satisﬁes the condition ∥x0 −ˆx∥< C−1, because
we now have
∥xk −ˆx∥≤C−1
C∥x0 −ˆx∥
2k
for all k.
If an iterative method, when applied to functions in a given class of
functions, always generates sequences that are at least linearly (quadratic)
convergent and there is a sequence which does not converge better than
linearly (quadratic), then we say that the method is linearly (quadratic)
convergent for the function class in question.
14.2
The gradient descent method
In this section we analyze the gradient descent algorithm with constant step
size. The iterative formulation of the variant of the algorithm that we have
in mind looks like this:
Gradient descent algorithm with constant step size
Given a starting point x and a step size h.
Repeat
1. Compute the search direction v = −f ′(x).
2. Update: x:= x + hv.
until stopping criterion is satisﬁed.
The algorithm converges linearly to the minimum point for strongly con-
vex functions with Lipschitz continuous derivatives provided that the step
size is small enough and the starting point is chosen suﬃciently close to the
minimum point.
This is the main content of the following theorem (and
Example 14.2.1).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
8
Descent methods
Theorem 14.2.1. Let f be a function with a local minimum point ˆx, and
suppose that there is an open neighborhood U of ˆx such that the restriction f|U
of f to U is µ-strongly convex and diﬀerentiable with a Lipschitz continuous
derivative and Lipschitz constant L.
The gradient descent algorithm with
constant step size h then converges at least linearly to ˆx provided that the
step size is suﬃciently small and the starting point x0 lies suﬃciently close
to ˆx.
More precisely: If the ball centered at ˆx and with radius equal to ∥x0 −ˆx∥
lies in U and if h ≤µ/L2, and (xk)∞
0 is the sequence of points generated by
the algorithm, then xk lies in U and
∥xk+1 −ˆx∥≤c∥xk −ˆx∥,
for all k, where c =

1 −hµ.
Proof. Suppose inductively that the points x0, x1, . . . , xk lie in U and that
∥xk −ˆx∥≤∥x0 −ˆx∥. Since the restriction f|U is assumed to be µ-strongly
convex and since f ′(ˆx) = 0,
⟨f ′(xk), xk −ˆx⟩= ⟨f ′(xk) −f ′(ˆx), xk −ˆx⟩≥µ∥xk −ˆx∥2
according to Theorem 7.3.1 in Part I, and since the derivative is assumed to
be Lipschitz continuous, we also have the inequality
∥f ′(xk)∥= ∥f ′(xk) −f ′(ˆx)∥≤L∥xk −ˆx∥.
By combining these two inequalities, we obtain the inequality
⟨f ′(xk), xk −ˆx⟩≥µ∥xk −ˆx∥2 = µ
2∥xk −ˆx∥2 + µ
2∥xk −ˆx∥2
≥µ
2∥xk −ˆx∥2 +
µ
2L2∥f ′(xk)∥2.
Our next point xk+1 = xk −hf ′(xk) therefore satisﬁes the inequality
∥xk+1 −ˆx∥2 = ∥xk −hf ′(xk) −ˆx∥2 = ∥(xk −ˆx) −hf ′(xk)∥2
= ∥xk −ˆx∥2 −2h⟨f ′(xk), xk −ˆx⟩+ h2∥f ′(xk)∥2
≤∥xk −ˆx∥2 −hµ∥xk −ˆx∥2 −h µ
L2∥f ′(xk)∥2 + h2∥f ′(xk)∥2
= (1 −hµ)∥xk −ˆx∥2 + h

h −µ
L2

∥f ′(xk)∥2.
Hence, h ≤µ/L2 implies that ∥xk+1 −ˆx∥2 ≤(1 −hµ)∥xk −ˆx∥2, and
this proves that the inequality of the theorem holds with c =

1 −hµ < 1,
and that the induction hypothesis is satisﬁed by the point xk+1, too, since
it lies closer to ˆx than the point xk does. So the gradient descent algorithm
converges at least linearly for f under the given conditions on h and x0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
9
Descent methods
9
We can obtain a slightly sharper result for µ-strongly convex functions
that are deﬁned on the whole Rn and have a Lipschitz continuous derivative.
Theorem 14.2.2. Let f be a function in the class Sµ,L(Rn). The gradient
descent method, with arbitrary starting point x0 and constant step size h,
generates a sequence (xk)∞
0
of points that converges at least linearly to the
function’s minimum point ˆx, if
0 < h ≤
2
µ + L.
More precisely,
(14.5)
∥xk −ˆx∥≤

1 −2hµL
µ + L
k/2
∥x0 −ˆx∥.
Moreover, if h =
2
µ + L then
∥xk −ˆx∥≤
Q −1
Q + 1
k
∥x0 −ˆx∥
and
(14.6)
f(xk) −fmin ≤L
2
Q −1
Q + 1
2k
∥x0 −ˆx∥2,
(14.7)
where Q = L/µ is the condition number of the function class Sµ,L(Rn).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
10
Descent methods
Proof. The function f has a unique minimum point ˆx, according to Corollary
8.1.7 in Part I, and
∥xk+1 −ˆx∥2 = ∥xk −ˆx∥2 −2h⟨f ′(xk), xk −ˆx⟩+ h2∥f ′(xk)∥2,
just as in the proof of Theorem 14.2.1. Since f ′(ˆx) = 0, it now follows from
Theorem 7.4.4 in Part I (with x = ˆx and v = xk −ˆx) that
⟨f ′(xk), xk −ˆx⟩≥
µL
µ + L∥xk −ˆx∥2 +
1
µ + L∥f ′(xk)∥2,
which inserted in the above equation results in the inequality
∥xk+1 −ˆx∥2 ≤

1 −2hµL
µ + L

∥xk −ˆx∥2 + h

h −
2
µ + L

∥f ′(xk)∥2.
So if h ≤2/(µ + L), then
∥xk+1 −ˆx∥≤

1 −2hµL
µ + L
1/2
∥xk −ˆx∥,
and inequality (14.5) now follows by iteration.
The particular choice of h = 2(µ + L)−1 in inequality (14.5) gives us
inequality (14.6), and the last inequality (14.7) follows from inequality (14.6)
and Theorem 1.1.2 in Part I, since f ′(ˆx) = 0.
The rate of convergence in Theorems 14.2.1 and 14.2.2 depends on the
condition number Q ≥1. The smaller the Q, the faster the convergence.
The constants µ and L, and hence the condition number Q, are of course
rarely known in practical examples, so the two theorems have a qualitative
character and can rarely be used to predict the number of iterations required
to achieve a certain precision.
Our next example shows that inequality (14.6) can not be sharpened.
Example 14.2.1. Consider the function
f(x) = 1
2(µx2
1 + Lx2
2),
where 0 < µ ≤L. This function belongs to the class Sµ,L(R2), f ′(x) =
(µx1, Lx2), and ˆx = (0, 0) is the minimum point.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
11
Descent methods
1
5
10
15
x1
1
x2
Figure 14.2. Some level curves for the function f(x) = 1
2(x2
1 + 16x2
2)
and the progression of the gradient descent algorithm with x(0) = (16, 1)
as starting point. The function’s condition number Q is equal to 16, so
the convergence to the minimum point (0, 0) is relatively slow.
The
distance from the generated point to the origin is improved by a factor
of 15/17 in each iteration.
The gradient descent algorithm with constant step size h = 2(µ + L)−1,
starting point x(0) = (L, µ), and α = Q−1
Q+1 proceeds as follows
x(0) = (L, µ)
f ′(x(0)) = (µL, µL)
x(1) = x(0) −hf ′(x(0)) = α(L, −µ)
f ′(x(1)) = α(µL, −µL)
x(2) = x(1) −hf ′(x(1)) = α2(L, µ)
...
x(k) = αk(L, (−1)kµ)
Consequently,
∥x(k) −ˆx∥= αk
L2 + µ2 = αk∥x(0) −ˆx∥,
so inequality (14.6) holds with equality in this case. Cf. with ﬁgure 14.2.
Finally, it is worth noting that 2(µ+L)−1 coincides with the step size that
we would obtain if we had used exact line search in each iteration step.
The gradient descent algorithm is not invariant under aﬃne coordinate
changes. The speed of convergence can thus be improved by ﬁrst making a
coordinate change that reduces the condition number.
Example 14.2.2. We continue with the function f(x) = 1
2(µx2
1 +Lx2
2) in the
previous example. Make the change of variables y1 = √µ x1, y2 =
√
L x2,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
12
Descent methods
and deﬁne the function g by
g(y) = f(x) = 1
2(y2
1 + y2
2).
The condition number Q of the function g is equal to 1, so the gradient
descent algorithm, started from an arbitrary point y(0), hits the minimum
point (0, 0) after just one iteration.
The gradient descent algorithm converges too slowly to be of practical use
in realistic problems. In the next chapter we shall therefore study in detail
a more eﬃcient method for optimization, Newton’s method.
Exercises
14.1 Perform three iterations of the gradient descent algorithm with (1, 1) as
starting point on the minimization problem
min x2
1 + 2x2
2.
14.2 Let X = {x ∈R2 | x1 > 1}, let x(0) = (2, 2), and let f : X →R be the
function deﬁned by f(x) = 1
2x2
1 + 1
2x2
2.
a) Show that the sublevel set {x ∈X | f(x) ≤f(x(0))} is not closed.
b) Obviously, fmin = inf f(x) =
1
2, but show that the gradient descent
method, with x(0) as starting point and with line search according to Armijo’s
rule with parameters α ≤1
2 and β < 1, generates a sequence x(k) = (ak, ak),
k = 0, 1, 2, . . . , of points that converges to the point (1, 1). So the function
values f(x(k)) converge to 1 and not to fmin.
[Hint: Show that ak+1 −1 ≤(1 −β)(ak −1) for all k.]
14.3 Suppose that the gradient descent algorithm with constant step size con-
verges to the point ˆx when applied to a continuously diﬀerentiable function
f. Prove that ˆx is a stationary point of f, i.e. that f′(ˆx) = 0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
13
Newton's method
Chapter 15
Newton’s method
In Newton’s method for minimizing a function f, the search direction at
a point x is determined by minimizing the function’s Taylor polynomial of
degree two, i.e. the polynomial
P(v) = f(x) + Df(x)[v] + 1
2D2f(x)[v, v] = f(x) + ⟨f ′(x), v⟩+ 1
2⟨v, f ′′(x)v⟩,
and since P ′(v) = f ′(x) + f ′′(x)v, we obtain the minimizing search vector as
a solution to the equation
f ′′(x)v = −f ′(x).
Each iteration is of course more laborious in Newton’s method than in
the gradient descent method, since we need to compute the second derivative
and solve a quadratic equation to determine the search vector. However, as
we shall see, this is more than compensated by a much faster convergence to
the minimum value.
15.1
Newton decrement and Newton direc-
tion
Since the search directions in Newton’s method are obtained by minimizing
quadratic polynomials, we start by examining when such polynomials have
minimum values, and since convexity is a necessary condition for quadratic
polynomials to be bounded below, we can restrict ourself to the study of
convex quadratic polynomials.
Theorem 15.1.1. A quadratic polynomial
P(v) = 1
2⟨v, Av⟩+ ⟨b, v⟩+ c
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
14
Newton's method
14
in n variables, where A is a positive semideﬁnite symmetric operator, is
bounded below on Rn if and only if the equation
(15.1)
Av = −b
has a solution.
The polynomial has a minimum if it is bounded below, and ˆv is a minimum
point if and only if Aˆv = −b.
If ˆv is a minimum point of the polynomial P, then
(15.2)
P(v) −P(ˆv) = 1
2⟨v −ˆv, A(v −ˆv)⟩
for all v ∈Rn.
If ˆv1 and ˆv2 are two minimum points, then ⟨ˆv1, Aˆv1⟩= ⟨ˆv2, Aˆv2⟩.
Remark. Another way to state that equation (15.1) has a solution is to say
that the vector −b, and of course also the vector b, belongs to the range of
the operator A. But the range of an operator on a ﬁnite dimensional space is
equal to the orthogonal complement of the null space of the operator. Hence,
equation (15.1) is solvable if and only if
Av = 0 ⇒⟨b, v⟩= 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
15
Newton's method
Proof. First suppose that equation (15.1) has no solution.
Then, by the
remark above there exists a vector v such that Av = 0 and ⟨b, v⟩̸= 0. It
follows that
P(tv) = 1
2⟨v, Av⟩t2 + ⟨b, v⟩t + c = ⟨b, v⟩t + c
for all t ∈R, and since the t-coeﬃcient is nonzero, we conclude that the
polynomial P(t) is unbounded below.
Next suppose that Aˆv = −b. Then
P(v) −P(ˆv) = 1
2(⟨v, Av⟩−⟨ˆv, Aˆv⟩) + ⟨b, v⟩−⟨b, ˆv⟩
= 1
2(⟨v, Av⟩−⟨ˆv, Aˆv⟩) −⟨Aˆv, v⟩+ ⟨Aˆv, ˆv⟩
= 1
2(⟨v, Av⟩+ ⟨ˆv, Aˆv⟩−⟨Aˆv, v⟩−⟨ˆv, Av⟩)
= 1
2⟨v −ˆv, A(v −ˆv)⟩≥0
for all v ∈Rn. This proves that the polynomial P(t) is bounded below, that
ˆv is a minimum point, and that the equality (15.2) holds.
Since every positive semideﬁnite symmetric operator A has a unique pos-
itive semideﬁnite symmetric square root A1/2, we can rewrite equality (15.2)
as follows:
P(v) = P(ˆv) + 1
2⟨A1/2(v −ˆv), A1/2(v −ˆv)⟩= P(ˆv) + 1
2∥A1/2(v −ˆv)∥2.
If v is another minimum point of P, then P(v) = P(ˆv), and it follows that
A1/2(v −ˆv) = 0.
Consequently, A(v −ˆv) = A1/2(A1/2(v −ˆv)) = 0, i.e. Av = Aˆv = −b. Hence,
every minimum point of P is obtained as a solution to equation (15.1).
Finally, if ˆv1 and ˆv2 are two minimum points of the polynomial, then
Aˆv1 = Aˆv2 (= −b), and it follows that ⟨ˆv1, Aˆv1⟩= ⟨ˆv1, Aˆv2⟩= ⟨Aˆv1, ˆv2⟩=
⟨Aˆv2, ˆv2⟩= ⟨ˆv2, Aˆv2⟩.
The problem to solve a convex quadratic optimization problem in Rn is
thus reduced to solving a quadratic system of linear equations in n variables
(with a positive semideﬁnite coeﬃcient matrix), which is a rather trivial
numerical problem that can be performed with O(n3) arithmetic operations.
We are now ready to deﬁne the main ingredients of Newton’s method.
Deﬁnition. Let f : X →R be a twice diﬀerentiable function with an open
subset X of Rn as domain, and let x ∈X be a point where the second
derivative f ′′(x) is positive semideﬁnite.
By a Newton direction ∆xnt of the function f at the point x we mean a
solution v to the equation
f ′′(x)v = −f ′(x).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
16
Newton's method
Remark. It follows from the remark after Theorem 15.1.1 that there exists a
Newton direction at x if and only if
f ′′(x)v = 0 ⇒⟨f ′(x), v⟩= 0.
The nonexistence of Newton directions at x is thus equivalent to the existence
of a vector w such that f ′′(x)w = 0 and ⟨f ′(x), w⟩= 1.
The Newton direction ∆xnt is of course uniquely determined as
∆xnt = −f ′′(x)−1f ′(x)
if the second derivative f ′′(x) is non-singular, i.e. positive deﬁnite.
A Newton direction ∆xnt is according to Theorem 15.1.1, whenever it
exists, a minimizing vector for the Taylor polynomial
P(v) = f(x) + ⟨f ′(x), v⟩+ 1
2⟨v, f ′′(x)v⟩,
and the diﬀerence P(0) −P(∆xnt) is given by
P(0) −P(∆xnt) = 1
2⟨0 −∆xnt, f ′′(x)(0 −∆xnt)⟩= 1
2⟨∆xnt, f ′′(x)∆xnt⟩.
Using the Taylor approximation f(x + v) ≈P(v), we conclude that
f(x) −f(x + ∆xnt) ≈P(0) −P(∆xnt) = 1
2⟨∆xnt, f ′′(x)∆xnt⟩.
Hence, 1
2⟨∆xnt, f ′′(x)∆xnt⟩is (for small ∆xnt) an approximation of the de-
crease in function value which is obtained by replacing f(x) with f(x+∆xnt).
This motivates our next deﬁnition.
Deﬁnition. The Newton decrement λ(f, x) of the function f at the point x
is a quantity deﬁned as
λ(f, x) =

⟨∆xnt, f ′′(x)∆xnt⟩
if f has a Newton direction ∆xnt at x, and as
λ(f, x) = +∞
if there is no Newton direction at x.
Note that the deﬁnition is independent of the choice of Newton direction
at x in case of nonuniqueness of Newton direction. This follows immediately
from the last statement in Theorem 15.1.1.
In terms of the Newton decrement, we thus have the following approxi-
mation
f(x) −f(x + ∆xnt) ≈1
2λ(f, x)2
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
17
Newton's method
17
for small values of ∆xnt.
By deﬁnition f ′′(x)∆xnt = −f ′(x), so it follows that the Newton decre-
ment, whenever ﬁnite, can be computed using the formula
λ(f, x) =

−⟨∆xnt, f ′(x)⟩.
In particular, if x is a point where the second derivative is positive deﬁnite,
then
λ(f, x) =

⟨f ′′(x)−1f ′(x), f ′(x)⟩.
Example 15.1.1. The convex one-variable function
f(x) = −ln x, x > 0
has Newton decrement
λ(f, x) =

⟨x2(−x−1), −x−1⟩=

(−x) · (−x−1) = 1
at all points x > 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
18
Newton's method
At points x with a Newton direction it is also possible to express the
Newton decrement in terms of the Euclidean norm ∥·∥as follows, by using
the fact that f ′′(x) har a positive deﬁnite symmetric square root:
λ(f, x) =

⟨f ′′(x)1/2∆xnt, f ′′(x)1/2∆xnt⟩= ∥f ′′(x)1/2∆xnt∥.
The improvement in function value obtained by taking a step in the Newton
direction ∆xnt is thus proportional to ∥f ′′(x)1/2∆xnt∥2 and not to ∥∆xnt∥2,
a fact which motivates our introduction of the following seminorm.
Deﬁnition. Let f : X →R be a twice diﬀerentiable function with an open
subset X of Rn as domain, and let x ∈X be a point where the second
derivative f ′′(x) is positive semideﬁnite.
The function ∥·∥x : Rn →R+,
deﬁned by
∥v∥x =

⟨v, f ′′(x)v⟩= ∥f ′′(x)1/2v∥
for all v ∈Rn, is called the local seminorm at x of the function f.
It is easily veriﬁed that ∥·∥x is indeed a seminorm on Rn. Since
{v ∈Rn | ∥v∥x = 0} = N(f ′′(x)),
where N(f ′′(x)) is the null space of f ′′(x), ∥·∥x is a norm if and only if the
positive deﬁnite second derivative f ′′(x) is nonsingular, i.e. positive deﬁnite.
At points x with a Newton direction, we now have the following simple
relation between direction and decrement:
λ(f, x) = ∥∆xnt∥x.
Example 15.1.2. Let us study the Newton decrement λ(f, x) when f is a
convex quadratic polynomial, i.e. a function of the form
f(x) = 1
2⟨x, Ax⟩+ ⟨b, x⟩+ c
with a positive semideﬁnite operator A. We have f ′(x) = Ax + b, f ′′(x) = A
and ∥v∥x =

⟨v, Av⟩, so the seminorms ∥·∥x are the same for all x ∈Rn.
If ∆xnt is a Newton direction of f at x, then
A∆xnt = −(Ax + b),
by deﬁnition, and it follows that A(x + ∆xnt) = −b. This implies that the
function f is bounded below, according to Theorem 15.1.1.
So if f is not bounded below, then there are no Newton directions at any
point x, which means that λ(f, x) = +∞for all x.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
19
Newton's method
Conversely, assume that f is bounded below. Then there exists a vector
v0 such that Av0 = −b, and it follows that
f ′′(x)(v0 −x) = Av0 −Ax = −b −Ax = −f ′(x).
The vector v0 −x is in other words a Newton direction of f at the point x,
which means that the Newton decrement λ(f, x) is ﬁnite at all points x and
is given by
λ(f, x) = ∥v0 −x∥x.
If f is bounded below without being constant, then necessarily A ̸= 0 and
we can choose a vector w such that ∥w∥x =

⟨w, Aw⟩= 1. Let xk = kw+v0,
where k is a positive number. Then
λ(f, xk) = ∥v0 −xk∥xk = k∥w∥xk = k,
and we conclude from this that supx∈Rn λ(f, x) = +∞.
For constant functions f, the case A = 0, b = 0, we have ∥v∥x = 0 for all
x and v, and consequently λ(f, x) = 0 for all x.
In summary, we have obtained the following result:
The Newton decrement of downwards unbounded convex quadratic func-
tions (which includes all non-constant aﬃne functions) is inﬁnite at all points.
The Newton decrement of downwards bounded convex quadratic functions
f is ﬁnite at all points, but supx λ(f, x) = ∞, unless the function is con-
stant.
We shall give an alternative characterization of the Newton decrement,
and for this purpose we need the following useful inequality.
Theorem 15.1.2. Suppose λ(f, x) < ∞. Then
|⟨f ′(x), v⟩| ≤λ(f, x)∥v∥x
for all v ∈Rn.
Proof. Since λ(f, x) is assumed to be ﬁnite, there exists a Newton direction
∆xnt at x, and by deﬁnition, f ′′(x)∆xnt = −f ′(x).
Using the Cauchy–
Schwarz inequality we now obtain:
|⟨f ′(x), v⟩| = |⟨f ′′(x)∆xnt, v⟩| = |⟨f ′′(x)1/2∆xnt, f ′′(x)1/2v⟩|
≤∥f ′′(x)1/2∆xnt∥∥f ′′(x)1/2v∥= λ(f, x)∥v∥x.
Theorem 15.1.3. Assume as before that x is a point where the second deriva-
tive f ′′(x) is positive semideﬁnite. Then
λ(f, x) = sup
∥v∥x≤1
⟨f ′(x), v⟩.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
20
Newton's method
20
Proof. First assume that λ(f, x) < ∞. Then
⟨f ′(x), v⟩≤λ(f, x)
for all vectors v such that ∥v∥x ≤1, according to Theorem 15.1.2. In the case
λ(f, x) = 0 the above inequality holds with equality for v = 0, so assume
that λ(f, x) > 0. For v = −λ(f, x)−1∆xnt we then have ∥v∥x = 1 and
⟨f ′(x), v⟩= −λ(f, x)−1⟨f ′(x), ∆xnt⟩= λ(f, x).
This proves that λ(f, x) = sup∥v∥x≤1⟨f ′(x), v⟩for ﬁnite Newton decrements
λ(f, x).
Next assume that λ(f, x) = +∞, i.e. that no Newton direction exists at
x. By the remark after the deﬁnition of Newton direction, there exists a
vector w such that f ′′(x)w = 0 and ⟨f ′(x), w⟩= 1. It follows that ∥tw∥x =
t∥w∥x = t

⟨w, f ′′(x)w⟩= 0 ≤1 and ⟨f ′(x), tw⟩= t for all positive numbers
t, and this implies that sup∥v∥x≤1⟨f ′(x), v⟩= +∞= λ(f, x).
We sometimes need to compare ∥∆xnt∥, ∥f ′(x)∥and λ(f, x), and we can
do so using the following theorem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
21
Newton's method
Theorem 15.1.4. Let λmin and λmax denote the smallest and the largest eigen-
value of the second derivative f ′′(x), assumed to be positive semideﬁnite, and
suppose that the Newton decrement λ(f, x) is ﬁnite. Then
λ1/2
min∥∆xnt∥≤λ(f, x) ≤λ1/2
max∥∆xnt∥
and
λ1/2
minλ(f, x) ≤∥f ′(x)∥≤λ1/2
maxλ(f, x).
Proof. Let A be an arbitrary positive semideﬁnite operator on Rn with small-
est and largest eigenvalue µmin and µmax respectively. Then
µmin∥v∥≤∥Av∥≤µmax∥v∥
for all vectors v.
Since λ1/2
min and λ1/2
max are the smallest and the largest eigenvalues of the
operator f ′′(x)1/2, we obtain the two inequalities of our theorem by applying
the general inequality to A = f ′′(x)1/2 and v = ∆xnt, and to A = f ′′(x)1/2
and v = f ′′(x)1/2∆xnt, noting that ∥f ′′(x)1/2∆xnt∥= λ(f, x) and that
∥f ′′(x)1/2(f ′′(x)1/2∆xnt)∥= ∥f ′′(x)∆xnt∥= ∥f ′(x)∥.
Theorem 15.1.4 is a local result, but if the function f is µ-strongly convex,
then λmin ≥µ, and if the norm of the second derivative is bounded by
some constant M, then λmax = ∥f ′′(x)∥≤M for all x in the domain of f.
Therefore, we get the following corollary to Theorem 15.1.4.
Corollary
15.1.5. If f : X →R is a twice diﬀerentiable µ-strongly convex
function, then
µ1/2∥∆xnt∥≤λ(f, x) ≤µ−1/2∥f ′(x)∥
for all x ∈X. If moreover ∥f ′′(x)∥≤M, then
M −1/2∥f ′(x)∥≤λ(f, x) ≤M 1/2∥∆xnt∥.
The distance from an arbitrary point to the minimum point of a strongly
convex function with bounded second derivative can be estimated using the
Newton decrement, because we have the following result.
Theorem 15.1.6. Let f : X →R be a µ-strongly convex function, and sup-
pose that f has a minimum at the point ˆx and that ∥f ′′(x)∥≤M for all
x ∈X. Then
f(x) −f(ˆx) ≤M
2µλ(f, x)2
and
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
22
Newton's method
∥x −ˆx∥≤
√
M
µ λ(f, x).
Proof. The theorem follows by combining Theorem 14.1.1 with the estimate
∥f ′(x)∥≤M 1/2λ(f, x) from Corollary 15.1.5.
The Newton decrement is invariant under surjective aﬃne coordinate
transformations. A slightly more general result is the following.
Theorem 15.1.7. Let f be a twice diﬀerentiable function whose domain Ωis
a subset of Rn, let A: Rm →Rn be an aﬃne map, and let g = f ◦A. Let
furthermore x = Ay be a point in Ω, and suppose that the second derivative
f ′′(x) is positive semideﬁnite. The second derivative g′′(y) is then positive
semideﬁnite, and the Newton decrements of the two functions g and f satisfy
the inequality
λ(g, y) ≤λ(f, x).
Equality holds if the aﬃne map A is surjective.
Proof. The aﬃne map can be written as Ay = Cy + b, where C is a linear
map and b is a vector, and the chain rule gives us the identities
⟨g′(y), w⟩= ⟨f ′(x), Cw⟩
and
⟨w, g′′(y)w⟩= ⟨Cw, f ′′(x)Cw⟩
for arbitrary vectors w in Rm. It follows from the latter identity that the
second derivative g′′(y) is positive semideﬁnite if f ′′(x) is so, and that
∥w∥y = ∥Cw∥x.
An application of Theorem 15.1.3 now gives
λ(g, y) = sup
∥w∥y≤1
⟨g′(y), w⟩=
sup
∥Cw∥x≤1
⟨f ′(x), Cw⟩≤sup
∥v∥x≤1
⟨f ′(x), v⟩= λ(f, x).
If the aﬃne map A is surjective, then C is a surjective linear map, and
hence v = Cw runs through all of Rn as w runs through Rm. In this case,
the only inequality in the above chain of equalities and inequalities becomes
an equality, which means that λ(g, y) = λ(f, x).
15.2
Newton’s method
The algorithm
Newton’s method for minimizing a twice diﬀerentiable function f is a descent
method, in which the search direction in each iteration is given by the Newton
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
23
Newton's method
23
direction ∆xnt at the current point. The stopping criterion is formulated in
terms of the Newton decrement; the algorithm stops when the decrement is
suﬃciently small. In short, therefore, the algorithm looks like this:
Newton’s method
Given a starting point x ∈dom f and a tolerance ϵ > 0.
Repeat
1. Compute a Newton direction ∆xnt and the Newton decrement λ(f, x)
at x.
2. Stopping criterion: stop if λ(f, x)2 ≤2ϵ.
3. Determine a step size h > 0.
4. Update: x:= x + h∆xnt.
The step size h is set equal to 1 in each iteration in the so-called pure
Newton method, while it is computed by line search with Armijo’s rule or
otherwise in damped Newton methods.
The stopping criterion is motivated by the fact that 1
2λ(f, x)2 is an ap-
proximation to the decrease f(x) −f(x + ∆xnt) in function value, and if this
decrease is small, it is not worthwhile to continue.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
24
Newton's method
Newton’s method generally works well for functions which are convex in
a neighborhood of the optimal point, but it breaks down, of course, if it hits
a point where the second derivative is singular and the Newton direction is
lacking. We shall show that the pure method, under appropriate conditions
on the objective function f, converges to the minimum point if the starting
point is suﬃciently close to the minimum point. To achieve convergence for
arbitrary starting points, it is necessary to use methods with damping.
Example 15.2.1. When applied to a downwards bounded convex quadratic
polynomial
f(x) = 1
2⟨x, Ax⟩+ ⟨b, x⟩+ c,
Newton’s pure method ﬁnds the optimal solution after just one iteration,
regardless of the choice of starting point x, because f ′(x) = Ax+b, f ′′(x) = A
and A∆xnt = −(Ax + b), so the update x+ = x + ∆xnt satisﬁes the equation
f ′(x+) = Ax+ + b = Ax + A∆xnt + b = 0,
which means that x+ is the optimal point.
Invariance under change of coordinates
Unlike the gradient descent method, Newton’s method is invariant under
aﬃne coordinate changes.
Theorem 15.2.1. Let f : X →R be a twice diﬀerentiable function with a
positive deﬁnite second derivative, and let (xk)∞
0
be the sequence generated
by Newton’s pure algorithm with x0 as starting point. Let further A: Y →X
be an aﬃne coordinate transformation, i.e. the restriction to Y of a bijective
aﬃne map. Newton’s pure algorithm applied to the function g = f ◦A with
y0 = A−1x0 as the starting point then generates a sequence (yk)∞
0
with the
property that Ayk = xk for each k.
The two sequences have identical Newton decrements in each iteration,
and they therefore satisfy the stopping condition during the same iteration.
Proof. The assertion about the Newton decrements follows from Theorem
15.1.7, and the relationship between the two sequences follows by induction
if we show that Ay = x implies that A(y + ∆ynt) = x + ∆xnt, where ∆xnt =
−f ′′(x)−1f ′(x) and ∆ynt = −g′′(y)−1g′(y) are the uniquely deﬁned Newton
directions at the points x and y of the respective functions.
The aﬃne map A can be written as Ay = Cy+b, where C is an invertible
linear map and b is a vector. If x = Ay, then g′(y) = CTf ′(x) and g′′(y) =
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
25
Newton's method
CTf ′′(x)C, by the chain rule. It follows that
C∆ynt = −Cg′′(y)−1g′(y) = −CC−1f ′′(x)−1(CT)−1CTf ′(x)
= −f ′′(x)−1f ′(x) = ∆xnt,
and hence
A(y+∆ynt) = C(y+∆ynt)+b = Cy+b+C∆ynt = Ay+∆xnt = x+∆xnt.
Local convergence
We will now study convergence properties for the Newton method, starting
with the pure method.
Theorem 15.2.2. Let f : X →R be a twice diﬀerentiable, µ-strongly convex
function with minimum point ˆx, and suppose that the second derivative f ′′ is
Lipschitz continuous with Lipschitz constant L. Let x be a point in X and
set
x+ = x + ∆xnt,
where ∆xnt is the Newton direction at x. Then
∥x+ −ˆx∥≤L
2µ∥x −ˆx∥2.
Moreover, if the point x+ lies in X then
∥f ′(x+)∥≤L
2µ2∥f ′(x)∥2.
Proof. The smallest eigenvalue of the second derivative f ′′(x) is greater than
or equal to µ by Theorem 7.3.2 in Part I. Hence, f ′′(x) is invertible and the
largest eigenvalue of f ′′(x)−1 is less than or equal to µ−1, and it follows that
(15.3)
∥f ′′(x)−1∥≤µ−1.
To estimate the norm of x+ −ˆx, we rewrite the diﬀerence as
x+ −ˆx = x + ∆xnt −ˆx = x −ˆx −f ′′(x)−1f ′(x)
(15.4)
= f ′′(x)−1
f ′′(x)(x −ˆx) −f ′(x)

= −f ′′(x)−1w
with
w = f ′(x) −f ′′(x)(x −ˆx).
For 0 ≤t ≤1 we then deﬁne the vektor w(t) as
w(t) = f ′(ˆx + t(x −ˆx)) −tf ′′(x)(x −ˆx),
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
26
Newton's method
26
and note that w = w(1) −w(0), since f ′(ˆx) = 0. By the chain rule,
w′(t) =

f ′′(ˆx + t(x −ˆx)) −f ′′(x)

(x −ˆx),
and by using the Lipschitz continuity of the second derivative, we obtain the
estimate
∥w′(t)∥≤∥f ′′(ˆx + t(x −ˆx)) −f ′′(x)∥∥x −ˆx∥
≤L∥ˆx + t(x −ˆx) −x∥∥x −ˆx∥= L(1 −t)∥x −ˆx∥2.
Now integrate the above inequality over the interval [0, 1]; this results in the
inequality
∥w∥=

 1
0
w′(t) dt
 ≤
 1
0
∥w′(t)∥dt ≤L∥x −ˆx∥2
 1
0
(1 −t) dt.
(15.5)
= 1
2L∥x −ˆx∥2.
By combining equality (15.4) with the inequalities (15.3) and (15.5) we
obtain the estimate
∥x+ −ˆx∥= ∥f ′′(x)−1w∥≤∥f ′′(x)−1∥∥w∥≤L
2µ∥x −ˆx∥2,
which is the ﬁrst claim of the theorem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
27
Newton's method
To prove the second claim, we assume that x+ lies in X and consider for
0 ≤t ≤1 the vectors
v(t) = f ′(x + t∆xnt) −tf ′′(x)∆xnt,
noting that
v(1) −v(0) = f ′(x+) −f ′′(x)∆xnt −f ′(x) = f ′(x+) + f ′(x) −f ′(x) = f ′(x+).
Since v′(t) =

f ′′(x + t∆xnt) −f ′′(x)

∆xnt, it follows from the Lipschitz
continuity that
∥v′(t)∥≤∥f ′′(x + t∆xnt) −f ′′(x)∥∥∆xnt∥≤L∥∆xnt∥2t,
and by integrating this inequality, we obtain the desired estimate
∥f ′(x+)∥=

 1
0
v′(t) dt
 ≤
 1
0
∥v′(t)∥dt ≤L
2 ∥∆xnt∥2 ≤L
2µ2∥f ′(x)∥2,
where the last inequality follows from Corollary 15.1.5.
One consequence of the previous theorem is that the pure Newton method
converges quadratically when applied to functions with a positive deﬁnite
second derivative that does not vary too rapidly in a neighborhood of the
minimum point, provided that the starting point is chosen suﬃciently close
to the minimum point. More precisely, the following holds:
Theorem 15.2.3. Let f : X →R be a twice diﬀerentiable, µ-strongly convex
function with minimum point ˆx, and suppose that the second derivative f ′′
is Lipschitz continuous with Lipschitz constant L. Let 0 < r ≤2µ/L and
suppose that the open ball B(ˆx; r) is included in X.
Newton’s pure method with starting point x0 ∈B(ˆx; r) will then generate
a sequence (xk)∞
0 of points in Ωsuch that
∥xk −ˆx∥≤2µ
L
 L
2µ∥x0 −ˆx∥
2k
for all k, and the sequence therefore converges to the minimum point ˆx as
k →∞.
The convergence is very rapid. For example,
∥xk −ˆx∥≤2µ
L 2−2k
if the initial point is chosen such that ∥x0 −ˆx∥≤µ/L, and this implies that
∥xk −ˆx∥≤10−19µ/L already for k = 6.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
28
Newton's method
Proof. We keep the notation of Theorem 15.2.2 and then have xk+1 = x+
k , so
if xk lies in the ball B(ˆx; r), then
(15.6)
∥xk+1 −ˆx∥≤L
2µ∥xk −ˆx∥2,
and this implies that ∥xk+1 −ˆx∥< Lr2/2µ ≤r, i.e. the point xk+1 lies in the
ball B(ˆx; r). By induction, all points in the sequence (xk)∞
0 lie in B(ˆx; r), and
we obtain the inequality of the theorem by repeated application of inequality
(15.6).
Global convergence
Newton’s damped method converges, under appropriate conditions on the
objective function, for arbitrary starting points. The damping is required
only during an initial phase, because the step size becomes 1 once the al-
gorithm has produced a point where the gradient is suﬃciently small. The
convergence is quadratic during this second stage.
The following theorem describes a convergence result for strongly convex
functions with Lipschitz continuous second derivative.
Theorem 15.2.4. Let f : X →R be a twice diﬀerentiable, strongly convex
function with a Lipschitz continuous second derivative. Let x0 be a point in
X and suppose that the sublevel set
S = {x ∈X | f(x) ≤f(x0)}
is closed.
Then, f has a unique minimum point ˆx, and Newton’s damped algorithm,
with x0 as initial point och with line search according to Armijo’s rule with
parameters 0 < α < 1
2 and 0 < β < 1, generates a sequence (xk)∞
0 of points
in S that converges towards the minimum point.
After an initial phase with damping, the algorithm passes into a quadrat-
ically convergent phase with step size 1.
Proof. The existence of a unique minimum point is a consequence of Corol-
lary 8.1.7 in Part I.
Suppose that f is µ-strongly convex and let L be the Lipschitz constant
of the second derivative. The sublevel set S is compact since it is bounded
according to Theorem 8.1.6. It follows that the distance from the set S to
the boundary of the open set X is positive. Fix a positive number r that is
less than this distance and also satisﬁes the inequality
r ≤µ/L.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
29
Newton's method
29
Given x ∈S we now deﬁne the point x+ by
x+ = x + h∆xnt,
where h is the step size according to Armijo’s rule. In particular, xk+1 = x+
k
for all k.
The core of the proof consists in showing that there are two positive
constants γ and η ≤µr such that the following two implications hold for all
x ∈S:
(i)
∥f ′(x)∥≥η ⇒f(x+) −f(x) ≤−γ;
(ii)
∥f ′(x)∥< η ⇒h = 1 & ∥f ′(x+)∥< η.
Suppose that we have managed to prove (i) and (ii). If ∥f ′(xk)∥≥η for
0 ≤k < m, then
fmin −f(x0) ≤f(xm) −f(x0) =
m−1

k=0
(f(x+
k ) −f(xk)) ≤−mγ,
because of property (i). This inequality can not hold for all m, and hence
there is a smallest integer k0 such that ∥f ′(xk0)∥< η, and this integer must
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
30
Newton's method
satisfy the inequality
k0 ≤

f(x0) −fmin

/γ.
It now follows by induction from (ii) that the step size h is equal to 1 for
all k ≥k0. The damped Newton algorithm is in other words a pure Newton
algorithm from iteration k0 and onwards. Because of Theorem 14.1.1,
∥xk0 −ˆx∥≤µ−1∥f ′(xk0)∥< µ−1η ≤r ≤µL−1,
so it follows from Theorem 15.2.3 that the sequence (xk)∞
0 converges to ˆx,
and more precisely, that the estimate
∥xk+k0 −ˆx∥≤2µ
L
 L
2µ∥xk0 −ˆx∥
2k
≤2µ
L 2−2k
holds for k ≥0.
It thus only remains to prove the existence of numbers η and γ with the
properties (i) and (ii). To this end, let
Sr = S + B(x; r);
the set Sr is a convex and compact subset of Ω, and the two continuous
functions f ′ and f ′′ are therefore bounded on Sr, i.e. there are constants K
and M such that
∥f ′(x)∥≤K
and
∥f ′′(x)∥≤M
for all x ∈Sr. It follows from Theorem 7.4.1 in Part I that the derivative f ′
is Lipschitz continuous on the set Sr with Lipschitz constant M, i.e.
∥f ′(y) −f ′(x)∥≤M∥y −x∥
for x, y ∈Sr.
We now deﬁne our numbers η and γ as
η = min
3(1 −2α)µ2
L
, µr

and
γ = αβcµ
M
η2, where c = min
 1
M , r
K

.
Let us ﬁrst estimate the stepsize at a given point x ∈S. Since
∥∆xnt∥≤µ−1∥f ′(x)∥≤µ−1K,
the point x + t∆xnt lies in i Sr and especially also in X if 0 ≤t ≤rµK−1.
The function
g(t) = f(x + t∆xnt)
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
31
Newton's method
is therefore deﬁned for these t-values, and since f is µ-strongly convex and
the derivative is Lipschitz continuous with constant M on Sr, it follows from
Theorem 1.1.2 in Part I and Corollary 15.1.5 that
f(x + t∆xnt) ≤f(x) + t⟨f ′(x), ∆xnt⟩+ 1
2M∥∆xnt∥2t2
≤f(x) + t⟨f ′(x), ∆xnt⟩+ 1
2Mµ−1λ(f, x)2t2
= f(x) + t

1 −1
2Mµ−1t

⟨f ′(x), ∆xnt⟩.
The number ˆt = cµ lies in the interval [0, rµK−1] and is less than or
equal to µM −1. Hence, 1 −1
2Mµ−1ˆt ≥1
2 ≥α, which inserted in the above
inequality gives
f(x + ˆt∆xnt) ≤f(x) + αˆt ⟨f ′(x), ∆xnt⟩.
Now, let h = βm be the step size given by Armijo’s rule, which means that
the Armijo algorithm terminates in iteration m. Since it does not terminate
in iteration m −1, we conclude that βm−1 > ˆt, i.e.
h ≥βˆt = βcµ,
and this gives us the following estimate for the point x+ = x + h∆xnt:
f(x+) −f(x) ≤αh⟨f ′(x), ∆xnt⟩= −αh λ(f, x)2
≤−αβcµ λ(f, x)2 ≤−αβcµM −1∥f ′(x)∥2 = −γη−2∥f ′(x)∥2.
So, if ∥f ′(x)∥≥η then f(x+) −f(x) ≤−γ, which is the content of implica-
tion (i).
To prove the remaining implication (ii), we return to the function g(t) =
f(x + t∆xnt), assuming that ∥f ′(x)∥< η. The function is well-deﬁned for
0 ≤t ≤1, since
∥∆xnt∥≤µ−1∥f ′(x)∥< µ−1η ≤r.
Moreover,
g′(t) = ⟨f ′(x + t∆xnt), ∆xnt⟩and g′′(t) = ⟨∆xnt, f ′′(x + t∆xnt)∆xnt⟩.
By Lipschitz continuity,
|g′′(t) −g′′(0)| = |⟨∆xnt, f ′′(x + t∆xnt)∆xnt⟩−⟨∆xnt, f ′′(x)∆xnt⟩|
≤∥f ′′(x + t∆xnt) −f ′′(x)∥∥∆xnt∥2 ≤tL∥∆xnt∥3,
and it follows, since g′′(0) = λ(f, x)2 and ∥∆xnt∥≤µ−1/2λ(f, x), that
g′′(t) ≤λ(f, x)2 + tL∥∆xnt∥3 ≤λ(f, x)2 + tLµ−3/2 λ(f, x)3.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
32
Newton's method
32
By integrating this inequality over the interval [0, t], we obtain the inequality
g′(t) −g′(0) ≤tλ(f, x)2 + 1
2t2Lµ−3/2λ(f, x)3.
But g′(0) = ⟨f ′(x), ∆xnt⟩= −λ(f, x)2, so it follows that
g′(t) ≤−λ(f, x)2 + tλ(f, x)2 + 1
2t2Lµ−3/2λ(f, x)3,
and further integration results in the inequality
g(t) −g(0) ≤−tλ(f, x)2 + 1
2t2λ(f, x)2 + 1
6t3Lµ−3/2λ(f, x)3.
Now, take t = 1 to obtain
f(x + ∆xnt) ≤f(x) −1
2λ(f, x)2 + 1
6Lµ−3/2λ(f, x)3
(15.7)
= f(x) −λ(f, x)2 1
2 −1
6Lµ−3/2λ(f, x)

= f(x) + ⟨f ′(x), ∆xnt⟩
 1
2 −1
6Lµ−3/2λ(f, x)

.
Our assumption ∥f ′(x)∥< η implies that
λ(f, x) ≤µ−1/2∥f ′(x)∥< µ−1/2η ≤µ−1/2·3(1−2α)µ2L−1 = 3(1−2α)µ3/2L−1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
33
Newton's method
We conclude that
1
2 −1
6Lµ−3/2λ(f, x) > α,
which inserted into inequality (15.7) gives us the inequality
f(x + ∆xnt) ≤f(x) + α⟨f ′(x), ∆xnt⟩,
which tells us that the step size h is equal to 1.
The iteration leading from x to x+ = x + h∆xnt is therefore performed
according to the pure Newton method. Due to the inequality
∥x −ˆx∥≤µ−1∥f ′(x)∥< µ−1η ≤r,
which holds by Theorem 14.1.1, x is a point in the ball B(ˆx; r), so it follows
from the local convergence Theorem 15.2.2 that
(15.8)
∥f ′(x+)∥≤L
2µ2∥f ′(x)∥2.
Since η ≤µr ≤µ2/L,
∥f ′(x+)∥< L
2µ2η2 ≤η
2 < η,
and the proof is now complete.
By iterating inequality (15.8), one obtains in fact the estimate
∥f ′(xk)∥≤2µ2
L
 L
2µ2∥f ′(xk0)∥
2k−k0
< 2µ2
L 2−2k−k0
for k ≥k0, and it now follows from Theorem 14.1.1 that
f(xk) −fmin < 2µ3
L2 2−2k−k0+1
for k ≥k0. Combining this estimate with the previously obtained bound
on k0, one obtains an upper bound on the number of iterations required to
estimate the minimum value fmin with a given accuracy. If
k > f(x0) −fmin
γ
+ log2 log2
2µ3
L2ϵ,
then surely f(xk)−fmin < ϵ. This estimate, however, is of no practical value,
because the constants γ, µ and L are rarely known in concrete cases.
Another shortcoming of the classical convergence analysis of Newton’s
method is that the convergence constants, unlike the algorithm itself, de-
pend on the coordinate system used.
For self-concordant functions, it is
however possible to carry out the convergence analysis without any unknown
constants, as we shall do in Chapter 16.5.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
34
Newton's method
15.3
Equality constraints
With only minor modiﬁcations, Newton’s algorithm also works well when
applied to convex optimization problems with constraints in the form of
aﬃne equalities.
Consider the convex optimization problem
(P)
min f(x)
s.t.
Ax = b
where f : Ω→R is a twice continuously diﬀerentiable convex function, Ωis
an open subset of Rn, and A is an m × n-matrix.
The problem’s Lagrange function L: Ω× Rm →R is given by
L(x, y) = f(x) + (Ax −b)Ty = f(x) + xTATy −bTy,
and according to the Karush–Kuhn–Tucker theorem (Theorem 11.2.1 in Part
II), a point ˆx in Ωis an optimal solution if and only if there is a vector ˆy ∈Rm
such that
(15.9)
f ′(ˆx) + ATˆy = 0
Aˆx
= b.
Therefore, the minimization problem (P) is equivalent to the problem of
solving the system (15.9) of linear equations.
Example 15.3.1. When f is a convex quadratic function of the form
f(x) = 1
2⟨x, Px⟩+ ⟨q, x⟩+ r,
the linear system (15.9) becomes
P ˆx + ATˆy = −q
Aˆx
=
b,
and this is a quadratic system of linear equations with a symmetric coeﬃcient
matrix of order m + n. The system has a unique solution if rank A = m and
N(A) ∩N(P) = {0}. See exercise 15.4. In particular, there is a unique
solution if the matrix P is positive deﬁnite and rank A = m.
We now return to the general convex minimization problem (P). Let X
denote the set of feasible points, so that
X = {x ∈Ω| Ax = b}.
In optimization problems without any constraints, the descent direction
∆xnt at the point x is a vector which miminizes the Taylor polynom of degree
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
35
Newton's method
35
two of the function f(x+v), and the minimization is over all vectors v in Rn.
As a new point x+ with function value less than f(x) we select x+ = x+h∆xnt
with a suitable step size h. In constrained problems, the new point x+ has
to be a feasible point, of course, and this requires that A∆xnt = 0. The
minimization of the Taylor polynomial is therefore restricted to vectors v
that satisfy the condition Av = 0, and this means that we have to modify
our previous deﬁnition of Newton direction and decrement as follows for
constrained optimization problems.
Deﬁnition. In the equality constrained minimization problem (P), a vector
∆xnt is called a Newton direction at the point x ∈X if there exists a vector
w ∈Rm such that
(15.10)
f ′′(x)∆xnt + ATw = −f ′(x)
A∆xnt
=
0.
The quantity
λ(f, x) =

⟨∆xnt, f ′′(x)∆xnt⟩
is called the Newton decrement.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
36
Newton's method
It follows from Example 15.3.1 that the Newton direction ∆xnt (if it
exists) is an optimal solution to the minimization problem
min f(x) + ⟨f ′(x), v⟩+ 1
2⟨v, f ′′(x)v⟩
s.t.
Av = 0.
And if (∆xnt, w) is a solution to the system (15.10), then
−⟨f ′(x), ∆xnt⟩= ⟨f ′′(x)∆xnt + ATw, ∆xnt⟩
= ⟨f ′′(x)∆xnt, ∆xnt⟩+ ⟨w, A∆xnt⟩
= ⟨f ′′(x)∆xnt, ∆xnt⟩+ ⟨w, 0⟩= ⟨∆xnt, f ′′(x)∆xnt⟩,
so it follows that
λ(f, x) =

−⟨f ′(x), ∆xnt⟩,
just as for unconstrained problems.
The objective function is decreasing in the Newton direction, because
d
dtf(x + t∆xnt)

t=0 = ⟨f ′(x), ∆xnt⟩= −λ(f, x)2 ≤0,
so ∆xnt is indeed a descent direction.
Let P(v) denote the Taylor polynomial of degree two of the function
f(x + v). Then
f(x) −f(x + ∆xnt) ≈P(0) −P(∆xnt)
= −⟨f ′(x), ∆xnt⟩−1
2⟨∆xnt, f ′′(x)∆xnt⟩= 1
2λ(f, x)2,
just as in the unconstrained case.
With our modiﬁed deﬁnition of the Newton direction, we can now copy
Newton’s method verbatim for convex minimization problem of the type
min f(x)
s.t.
Ax = b.
The algorithm looks like this:
Newton’s method
Given a starting point x ∈Ωsatisfying the constraint Ax = b, and a
tolerance ϵ > 0.
Repeat
1. Compute the Newton direction ∆xnt at x by solving the system of
equations (15.10), and compute the Newton decrement λ(f, x).
2. Stopping criterion: stop if λ(f, x)2 ≤2ϵ.
3. Compute a step size h > 0.
4. Update: x:= x + h∆xnt.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
37
Newton's method
Elimination of constraints
An alternative approach to the optimization problem
(P)
min f(x)
s.t.
Ax = b,
with x ∈Ωas implicit condition and r = rank A, is to solve the system of
equations Ax = b and to express r variables as linear combinations of the
remaining n −r variables. The former variables can then be eliminated from
the objective function, and we obtain in this way an optimization problem in
n −r variables without explicit constraints, a problem that can be attacked
with Newton’s method. We will describe this approach in more detail and
compare it with the method above.
Suppose that the set X of feasible points is nonempty, choose a point
a ∈X, and select an aﬃne parametrization
x = ξ(z),
z ∈˜Ω
of X with ξ(0) = a. Since {x ∈Rn | Ax = b} = a + N(A), we can write the
parametrization as
ξ(z) = a + Cz
where C : Rp →Rn is an injective linear map, whose range V(C) coincides
with the null space N(A) of the map A, and p = n −rank A. The domain
˜Ω= {z ∈Rp | a + Cz ∈Ω} is an open convex subset of Rp.
A practical way to construct the parametrization is of course to solve the
system Ax = b by Gaussian elimination.
Let us ﬁnally deﬁne the function ˜f : ˜Ω→R by setting ˜f(z) = f(ξ(z)).
The problem (P) is then equivalent to the convex optimization problem
(˜P)
min ˜f(z)
which has no explicit constraints.
Let ∆xnt be a Newton direction of the function f at the point x, i.e. a
vector that satisﬁes the system (15.10) for a suitably chosen vector w. We
will show that the function ˜f has a corresponding Newton direction ∆znt at
the point z = ξ−1(x), and that ∆xnt = C∆znt.
Since A∆xnt = 0 and N(A) = V(C), there is a unique vector v such that
∆xnt = Cv. By the chain rule, ˜f ′(z) = CTf ′(x) and ˜f ′′(z) = CTf ′′(x)C, so
it follows from the ﬁrst equation in the system (15.10) that
˜f ′′(z)v = CTf ′′(x)Cv = CTf ′′(x)∆xnt = −CTf ′(x) −CTATw
= −˜f ′(z) −CTATw.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
38
Newton's method
38
A general result from linear algebra tells us that N(S) = V(ST)⊥for
arbitrary linear maps S. Applying this result to the maps CT and A, and
using that V(C) = N(A), we obtain the equality
N(CT) = V(C)⊥= N(A)⊥= V(AT)⊥⊥= V(AT),
which implies that CTATw = 0. Hence,
˜f ′′(z)v = −˜f ′(z),
and v is thus a Newton direction of the function ˜f at the point z. So, ∆znt = v
is the direction vector we are looking for.
The iteration step z →z+ = z + h∆znt in Newton’s method for the
unconstrained problem (˜P) takes us from the point z = ξ−1(x) in ˜Ωto the
point z+ whose image in X is
ξ(z+) = ξ(z + h∆znt) = a + C(z + h∆znt) = a + Cz + hC(∆znt)
= ξ(z) + h∆xnt = x + h∆xnt,
and this is also the point we get by applying Newton’s method to the point
x in the constrained problem (P).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
39
Newton's method
Also note that the Newton decrements are the same at corresponding
points, because
λ( ˜f, z)2 = −⟨˜f ′(z), ∆znt⟩= −⟨CTf ′(x), ∆znt⟩= −⟨f ′(x), C∆znt⟩
= −⟨f ′(x), ∆xnt⟩= λ(f, x)2.
In summary, we have arrived at the following result.
Theorem 15.3.1. Let (xk)∞
0
be a sequence of points obtained by Newton’s
method applied to the constrained problem (P). Newton’s method applied to
the problem (˜P), obtained by elimination of the constraints and with ξ−1(x0)
as initial point, will then generate a sequence (zk)∞
0
with the property that
xk = ξ(zk) for all k.
Convergence analysis
No new convergence analysis is needed for the modiﬁed version of Newton’s
method, for we can, because of Theorem 15.3.1, apply the results of The-
orem 15.2.4. If the restriction of the function f : Ω→R to the set X of
feasible points is strongly convex and the second derivative is Lipschitz con-
tinuous, then the same also holds for the function ˜f : ˜Ω→R. (Cf. with
exercise 15.5.) Assuming x0 to be a feasible starting point and the sublevel
set {x ∈X | f(x) ≤f(x0)} to be closed, the damped Newton algorithm will
therefore converge to the minimum point when applied to the constrained
problem (P). Close enough to the minimum point, the step size h will also
be equal to 1, and the convergence will be quadratic.
Exercises
15.1 Determine the Newton direction, the Newton decrement and the local norm
at an arbitrary point x > 0 for the function f(x) = x ln x −x.
15.2 Let f be the function f(x1, x2) = −ln x1 −ln x2 −ln(4 −x1 −x2) with
X = {x ∈R2 | x1 > 0, x2 > 0, x1 + x2 < 4} as domain. Determine the
Newton direction, the Newton decrement and the local norm at the point x
when
a) x = (1, 1)
b) x = (1, 2).
15.3 Determine a Newton direction, the Newton decrement and the local norm
for the function f(x1, x2) = ex1+x2 + x1 + x2 at an arbitrary point x ∈R2.
15.4 Assume that P is a symmetric positive semideﬁnite n × n-matrix and that
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
40
Newton's method
40
A is an arbitrary m × n-matrix. Prove that the matrix
M =
P
AT
A
0

is invertible if and only if rank A = m and N(A) ∩N(P) = {0}.
15.5 Assume that the function f : Ω→R is twice diﬀerentiable and convex, let
x = ξ(z) = a + Cz be an aﬃne parametrization of the set
X = {x ∈Ω| Ax = b},
and deﬁne the function ˜f by ˜f(z) = f(ξ(z)), just as in Section 15.3. Let
further σ denote the smallest eigenvalue of the symmetric matrix CTC.
a) Prove that ˜f is µσ-strongly convex if the restriction of f to X is µ-strongly
convex.
b) Assume that the matrix A has full rank and that there are constants K
and M such that Ax = b implies

f ′′(x)
AT
A
0
−1 ≤K
and
∥f′′(x)∥≤M.
Show that ˜f is µ-strongly convex with convexity constant µ = σK−2M−1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
41
Self-concordant functions
Chapter 16
Self-concordant functions
Self-concordant functions were introduced by Nesterov and Nemirovski in
the late 1980s as a product of their analysis of the speed of convergence of
Newton’s method. Classic convergence results for two times continuously
diﬀerentiable functions assume that the second derivative is Lipschitz con-
tinuous, and the convergence rate depends on the Lipschitz constant. One
obvious weakness of these results is that the value of the Lipschitz constant,
unlike Newton’s method, is not invariant under aﬃne coordinate transfor-
mations.
Suppose that a function f, which is deﬁned on an open convex subset X
of Rn, has a Lipschitz continuous second derivative with Lipschitz constant
L, i.e. that
∥f ′′(y) −f ′′(x)∥≤L∥y −x∥
for all x, y ∈X. For the restriction φx,v(t) = f(x + tv) of f to a line through
x with direction vector v, this means that
|φ′′
x,v(t)−φ′′
x,v(0)| = |⟨v, (f ′′(x+tv)−f ′′(x))v⟩| ≤L∥x+tv−x∥∥v∥2 = L|t|∥v∥3.
So if the function f is three times diﬀerentiable, then consequently
|φ′′′
x,v(0)| = lim
t→0

φ′′
x,v(t) −φ′′
x,v(0)
t
 ≤L∥v∥3.
But
φ′′′
x,v(0) =
n

i,j,k=1
∂3f(x)
∂xi∂xj∂xk
vivjvk = D3f(x)[v, v, v],
so a necessary condition for a three times diﬀerentiable function f to have a
Lipschitz continuous second derivative with Lipschitz constant L is that
(16.1)
|D3f(x)[v, v, v]| ≤L∥v∥3
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
42
Self-concordant functions
for all x ∈X and all v ∈Rn, and it is easy to show this is also a suﬃcient
condition.
The reason why the value of the Lipschitz constant is not aﬃnely invariant
is that there is no natural connection between the Euclidean norm ∥·∥and the
function f. The analysis of a function’s behavior is simpliﬁed if we instead use
a norm that is adapted to the form of the level surfaces, and for functions
with a positive semideﬁnite second derivative f ′′(x), such a (semi)norm is
the local seminorm ∥·∥x, introduced in the previous chapter and deﬁned as
∥v∥x =

⟨v, f ′′(x)v⟩. Nesterov–Nemirovski’s stroke of genius consisted in
replacing ∥·∥with the local seminorm ∥·∥x in the inequality (16.1). For the
function class obtained in this way, it is possible to describe the convergence
rate of Newton’s method in an aﬃnely independent way and with absolute
constants.
16.1
Self-concordant functions
We are now ready for Nesterov–Nemirovski’s deﬁnition of self-concordance
and for a study of the basic properties of self-concordant functions.
Deﬁnition. Let f : X →R be a three times continuously diﬀerentiable func-
tion with an open convex subset X of Rn as domain. The function is called
self-concordant if it is convex, and the inequality
(16.2)
D3f(x)[v, v, v]
 ≤2

D2f(x)[v, v]
3/2
holds for all x ∈X and all v ∈Rn.
Since D2f(x)[v, v] = ∥v∥2
x, where ∥·∥x is the local seminorm deﬁned by
the function f at the point x, we can also write the deﬁning inequality (16.2)
as
D3f(x)[v, v, v]
 ≤2∥v∥3
x,
and it is this shorter version that we will prefer, when we work with a single
function f.
Remark 1. There is nothing special about the constant 2 in inequality (16.2).
If f satisﬁes the inequality
D3f(x)[v, v, v]
 ≤K∥v∥3
x, then the function
F = 1
4K2f, obtained from f by scaling, is self-concordant. The choice of 2
as the constant facilitates, however, the wording of a number of results.
Remark 2. For functions f deﬁned on subsets of the real axis and v ∈R,
∥v∥2
x = f ′′(x)v2 and D3f(x)[v, v, v] = f ′′′(x)v3. Hence, a convex function
f : X →R is self-concordant if and only if
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
43
Self-concordant functions
43
|f ′′′(x)| ≤2f ′′(x)3/2
for all x ∈X.
Remark 3. In terms of the restriction φx,v(t) = f(x+tv) of the function f to
the line through x with direction v, we can equivalently write the inequality
D3f(x + tv)[v, v, v]
 ≤2∥v∥3
x+tv
as |φ′′′
x,v(t)| ≤2φ′′
x,v(t)3/2. A three times continuously diﬀerentiable convex
function of several variables is therefore self-concordant if and only if all its
restrictions to lines are self-concordant.
Example 16.1.1. The convex function f(x) = −ln x is self-concordant on its
domain R++. Indeed, inequality (16.2) holds with equality for this function,
since f ′′(x) = x−2 and f ′′′(x) = −2x−3.
Example 16.1.2. Convex quadratic functions f(x) = 1
2⟨x, Ax⟩+ ⟨b, x⟩+ c
are self-concordant since D3f(x)[v, v, v] = 0 for all x and v.
Hence, aﬃne functions are self-concordant, and the function x →∥x∥2,
where ∥·∥is the Euclidean norm, is self-concordant.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
44
Self-concordant functions
The expression
D3f(x)[u, v, w] =
n

i,k,k=1
∂3f(x)
∂xi∂xj∂xk
uivjwk
is a symmetric trilinear form in the variables u, v, and w, if the function f is
three times continuously diﬀerentiable in a neighborhood of the point x. For
self-concordant functions we have the following generalization of inequality
(16.2) in the deﬁnition of self-concordance.
Theorem 16.1.1. Suppose f : X →R is a self-concordant function. Then,
D3f(x)[u, v, w]
 ≤2∥u∥x∥v∥x∥w∥x
for all x ∈X and all vectors u, v, w in Rn.
Proof. The proof is based on a general theorem on norms of symmetric tri-
linear forms, which is proven in an appendix to this chapter.
Assume ﬁrst that x is a point where the second derivative f ′′(x) is positive
deﬁnite. Then ∥·∥x is a norm with ⟨u, v⟩x = ⟨u, f ′′(x)v⟩as the corresponding
scalar product. We can therefore apply Theorem 1 of the appendix to the
symmetric trilinear form D3f(x)[u, v, w] with ∥·∥x as the underlying norm,
and it follows that
sup
u,v,w̸=0
D3f(x)[u, v, w]

∥u∥x∥v∥x∥w∥x
= sup
v̸=0
D3f(x)[v, v, v]

∥v∥3
x
≤2,
which is equivalent to the assertion of the theorem.
To cope with points where the second derivative is singular, we consider
for ϵ > 0 the scalar product
⟨u, v⟩x,ϵ = ⟨u, f ′′(x)v⟩+ ϵ⟨u, v⟩,
where ⟨· , ·⟩is the usual standard scalar product, and the corresponding norm
∥v∥x,ϵ =

⟨v, v⟩x,ϵ =

∥v∥2
x + ϵ∥v∥2.
Obviously, ∥v∥x ≤∥v∥x,ϵ for all vectors v, and hence
|D3f(x)[v, v, v]| ≤2∥v∥3
x,ϵ
for all v, since f is self-concordant. It now follows from Theorem 1 in the
appendix that
|D3f(x)[u, v, w]| ≤2∥u∥x,ϵ∥v∥x,ϵ∥w∥x,ϵ
= 2

(∥u∥2
x + ϵ∥u∥2)(∥v∥2
x + ϵ∥v∥2)(∥w∥2
x + ϵ∥u∥w2),
and we get the sought-after inequality by letting ϵ →0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
45
Self-concordant functions
Theorem 16.1.2. The second derivative f ′′(x) of a self-concordant function
f : X →R has the same null space N(f ′′(x)) at all points x ∈X.
Proof. We recall that N(f ′′(x)) = {v | ∥v∥x = 0}.
Let x and y be two points in X. For reasons of symmetry, we only have
to show the inclusion N(f ′′(x)) ⊆N(f ′′(y)).
Assume therefore that v ∈N(f ′′(x)) and let xt = x + t(y −x). Since X
is an open convex set, there is certainly a number a > 1 such that the points
xt lie in X for 0 ≤t ≤a, and we now deﬁne a function g: [0, a] →R by
setting
g(t) = D2f(xt)[v, v] = ∥v∥2
xt.
Then g(0) = ∥v∥2
x = 0 and g(t) ≥0 for 0 ≤t ≤a, and since
g′(t) = D3f(xt)[v, v, y −x],
it follows from Theorem 16.1.1 that
|g′(t)| ≤2∥v∥2
xt∥y −x∥xt = 2g(t)∥y −x∥xt.
But the seminorm
∥y −x∥xt =

D2f(xt)[y −x, y −x]
depends continuously on t, and it is therefore bounded above by some con-
stant C on the interval [0, a]. Hence,
|g′(t)| ≤2Cg(t)
for 0 ≤t ≤a. It now follows from Theorem 2 in the appendix to this chapter
that g(t) = 0 for all t, and in particular, g(1) = ∥v∥2
y = 0, which proves that
v ∈N(f ′′(y)). This proves the inclusion N(f ′′(x)) ⊆N(f ′′(y)).
Our next corollary is just a special case of Theorem 16.1.2, because f ′′(x)
is non-singular if and only if N(f ′′(x)) = {0}.
Corollary 16.1.3. The second derivative of a self-concordant function is ei-
ther non-singular at all points or singular at all points.
A self-concordant function will be called non-degenerate if its second
derivative is positive deﬁnite at all points, and by the above corollary, that
is the case if the second derivative is positive deﬁnite at one single point.
A non-degenerate self-concordant function is in particular strictly convex.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
46
Self-concordant functions
46
Operations that preserve self-concordance
Theorem 16.1.4. If f is a self-concordant function and α ≥1, then αf is
self-concordant.
Proof. If α ≥1, then α ≤α3/2, and it follows that
D3(αf)(x)[v, v, v]
 = α
D3f(x)[v, v, v]
 ≤2α

D2f(x)[v, v]
3/2
≤2

αD2f(x)[v, v]
3/2 = 2

D2(αf)(x)[v, v]
3/2.
Theorem 16.1.5. The sum f + g of two self-concordant functions f and g is
self-concordant on its domain.
Proof. We use the elementary inequality
a3/2 + b3/2 ≤(a + b)3/2,
which holds for all nonnegative numbers a, b (and is easily proven by squaring
both sides) and the triangle inequality to obtain
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
47
Self-concordant functions
D3(f + g)(x)[v, v, v]
 =
D3f(x)[v, v, v] + D3g(x)[v, v, v]

≤2

D2f(x)[v, v]
3/2 + 2

D2g(x)[v, v]
3/2
≤2

D2f(x)[v, v] + D2g(x)[v, v]
3/2
= 2

D2(f + g)(x)[v, v]
3/2.
Theorem 16.1.6. If the function f : X →R is self-concordant, where X
is an open convex subset of Rn, and A is an aﬃne map from Rm to Rn,
then the composition g = f ◦A is a self-concordant function on its domain
A−1(X).
Proof. The aﬃne map A can be written as Ay = Cy + b, where C is a linear
map and b is a vector. Let y be a point in A−1(X) and let u be a vector in
Rm, and write x = Ay och v = Cu. According to the chain rule,
D2g(y)[u, u] = D2f(Ay)[Cu, Cu] = D2f(x)[v, v]
and
D3g(y)[u, u, u] = D3f(Ay)[Cu, Cu, Cu] = D3f(x)[v, v, v],
so it follows that
D3g(y)[u, u, u]
 =
D3f(x)[v, v, v]
 ≤2

D2f(x)[v, v]
3/2
= 2

D2g(y)[u, u]
3/2.
Example 16.1.3. It follows from Example 16.1.1 and Theorem 16.1.6 that
the function f(x) = −ln(b −⟨c, x⟩) with domain {x ∈Rn | ⟨c, x⟩< b} is
self-concordant.
Example 16.1.4. Suppose that the polyhedron
X =
p
j=1
{x ∈Rn | ⟨cj, x⟩≤bj}
has nonempty interior. The function f(x) = −p
j=1 ln(bj −⟨cj, x⟩), with
int X as domain, is self-concordant.
16.2
Closed self-concordant functions
In Section 6.7 of Part I we studied the recessive subspace of arbitrary convex
functions. The properties of the recessive subspace of a closed self-concordant
function is given by the following theorem.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
48
Self-concordant functions
Theorem 16.2.1. Suppose that f : X →R is a closed self-concordant func-
tion.
The function’s recessive subspace Vf is then equal to the null space
N(f ′′(x) of the second derivative f ′′(x) at an arbitrary point x ∈X. More-
over,
(i)
X = X + Vf.
(ii)
f(x + v) = f(x) + Df(x)[v] for all vectors v ∈Vf.
(iii)
If λ(f, x) < ∞, then f(x + v) = f(x) for all v ∈Vf.
Proof. Assertions (i) and (ii) are true for the recessive subspace of an arbi-
trary diﬀerentiable convex function according to Theorem 6.7.1, so we only
have to prove the remaining assertions.
Let x be an arbitrary point in X and let v be an arbitrary vector in Rn,
and consider the restriction φx,v(t) = f(x + tv) of f to the line through x
with direction v. The domain of φx,v is an open interval I =]α, β[ around 0.
First suppose that v ∈Vf. Then
φx,v(t) = f(x) + tDf(x)[v]
for all t ∈I becuse of property (ii), and it follows that
∥v∥2
x = D2f(x)[v, v] = φ′′
x,v(0) = 0,
i.e. the vector v belongs to the null space of f ′′(x). This proves the inclusion
Vf ⊆N(f ′′(x)). Note that this inclusion holds for arbitrary twice diﬀeren-
tiable convex functions without any assumptions concerning self-concordance
and closedness.
To prove the converse inclusion N(f ′′(x)) ⊆Vf, we instead assume that
v is a vector in N(f ′′(x)). Since N(f ′′(x + tv)) = N(f ′′(x)) for all t ∈I due
to Theorem 16.1.2, we now have
φ′′
x,v(t) = D2f(x + tv)[v, v] = ∥v∥2
x+tv = 0
for all t ∈I, and it follows that
φx,v(t) = φx,v(0) + φ′
x,v(0)t = f(x) + Df(x)[v] t.
If β < ∞, then x + βv is a boundary point of X and limt→β φx,v(t) < ∞.
However, according to Corollary 8.2.2 in Part I this is a contradiction to f
being a closed function. Hence, β = ∞, and similarly, α = −∞. This means
that I =]−∞, ∞[, and in particular, I contains the number 1. We conclude
that the point x + v lies in X and that f(x + v) = φx,v(1) = f(x) + Df(x)[v]
for all x ∈X and all v ∈N(f ′′(x)), and Theorem 6.7.1 now provides us with
the inclusion N(f ′′(x)) ⊆Vf. Hence, Vf = N(f ′′(x)).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
49
Self-concordant functions
49
Finally, suppose that λ(f, x) < ∞. Then there exists, by deﬁnition, a
Newton direction at x, and this implies, according to the remark after the
deﬁnition of Newton direction, that the implication
f ′′(x)v = 0 ⇒Df(x)[v] = 0
holds. Since Vf = N(f ′′(x)), it now follows from assertion (ii) that f(x+v) =
f(x) for all v ∈Vf.
The problem of minimizing a degenerate closed self-concordant function
f : X →R with ﬁnite Newton decrement λ(f, x) at all points x ∈X can be
reduced to the problem of minimizing a non-degenerate closed self-concordant
function as follows.
Assume that the domain X is a subset of Rn, and let Vf denote the
recessive subspace of f.
Put m = dim V ⊥
f
and let A: Rm →Rn be an
arbitrary injective linear map onto V ⊥
f , and put X0 = A−1(X). The set X0
is then an open subset of Rm, and we obtain a function g: X0 →R by
deﬁning g(y) = f(Ay) for y ∈X0.
The function g is self-concordant according to Theorem 16.1.6, and since
(y, t) belongs to the epigraph of g if and only if (Ay, t) belongs to the epigraph
of f, it follows that g is also a closed function.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
50
Self-concordant functions
Suppose v ∈N(g′′(y)). Since g′′(y) = ATf ′′(Ay)A,
⟨Av, f ′′(Ay)Av⟩= ⟨v, ATf ′′(Ay)Av⟩= ⟨v, g′′(y)v⟩= 0,
which means that the vector Av belongs to N(f ′′(Ay)), i.e. to the recessive
subspace Vf. But Av also belongs to V ⊥
f , by deﬁnition, and Vf ∩V ⊥
f = {0}, so
it follows that Av = 0. Hence v = 0, since A is an injective map. This proves
that N(g′′(y)) = {0}, which means that g is a non-degenerate function.
Each vector x ∈X has a unique decomposition x = x1 +x2 with x1 ∈V ⊥
f
and x2 ∈Vf, and x1 (= x −x2) lies in X according to Theorem 16.2.1.
Consequently, there is a unique point y ∈X0 such that Ay = x1. Therefore,
g(y) = f(Ay) = f(x1) = f(x) by the same theorem.
The functions f and g thus have the same ranges, and ˆy is a minimum
point of g if and only if Aˆy is a minimum point of f, and thereby also all
points Aˆy + v with v ∈Vf are minimum points of f.
We also note for future use that
λ(g, y) ≤λ(f, Ay) = λ(f, Ay + v)
for all y ∈X0 and all v ∈Vf, according to Theorem 15.1.7. (In the present
case, the two Newton decrements are actually equal, which we leave as an
exercise to show.)
Corollary
16.2.2. A closed self-concordant function f : X →R is non-
degenerate if its domain X does not contain any line.
Proof. By Theorem 16.2.1, X = X + Vf. Hence, if f is degenerate, then
X contains all lines through points in X with directions given by nonzero
vectors in Vf. So the function must be non-degenerate if its domain does not
contain any lines.
Corollary 16.2.3. A closed self-concordant function is non-degenerate if and
only if it is strictly convex.
Proof. The second derivative f ′′(x) of a non-degenerate self-concordant func-
tion f is positive deﬁnit for all x in its domain, and this implies that f is
strictly convex.
The recessive subspace Vf of a degenerate function f is non-trivial, and
the restriction φx,v(t) = f(x + tv) of f to a line with a direction given by a
nonzero vector v ∈Vf is aﬃne, according to Theorem 16.2.1. This prevents
f from being strictly convex.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
51
Self-concordant functions
16.3
Basic inequalities for the local seminorm
The graph of a convex function f lies above its tangent planes, and the
vertical distance between the point (y, f(y)) on the graph and the tangent
plane through the point (x, f(x) is greater than or equal to 1
2µ∥y −x∥2 if f is
µ-strongly convex. The same distance is also bounded below if the function
is self-concordant, but now by an expression that is a function of the local
norm ∥y −x∥x. The actual function ρ is deﬁned in the following lemma,
which also describes all the properties of ρ that we will need.
Lemma 16.3.1. Let ρ: ]−∞, 1[→R be the function
ρ(t) = −t −ln(1 −t).
(i) The function ρ is convex, strictly decreasing in the interval ]−∞, 0],
and strictly increasing in the interval [0, 1[, and ρ(0) = 0.
(ii) For 0 ≤t < 1,
ρ(t) ≤
t2
2(1 −t).
In particular, ρ(t) ≤t2 if 0 ≤t ≤1
2.
(iii) If s < 1 and t < 1, then ρ(s) + ρ(t) ≥−st.
(iv) If s ≥0, 0 ≤t < 1 and ρ(−s) ≤ρ(t), then s ≤
t
1 −t.
Proof. Assertion (i) follows easily by considering the sign of the derivative,
and assertion (ii) follows from the Taylor series expansion, which gives
ρ(t) = 1
2t2 + 1
3t3 + 1
4t4 + · · · ≤1
2t2(1 + t + t2 + · · · ) = 1
2t2(1 −t)−1
for 0 ≤t < 1.
To prove (iii), we use the elementary inequality x −ln(1 + x) ≥0 and
take x = st −s −t. This gives
st + ρ(s) + ρ(t) = st −s −t −ln(1 −s) −ln(1 −t)
= st −s −t −ln(1 + st −s −t) ≥0.
Since ρ is strictly decreasing in the interval ]−∞, 0], assertion (iv) will
follow once we show that ρ(−s) ≥ρ(t) when s = t/(1 −t). To show this
inequality, let
g(t) = ρ

−
t
1 −t

−ρ(t)
for 0 ≤t < 1. We simplify and obtain
g(t) = t −1 + (1 −t)−1 + 2 ln(1 −t).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
52
Self-concordant functions
Since g(0) = 0 and g′(t) = 1 + (1 −t)−2 −2(1 −t)−1 = t2(1 −t)−2 ≥0,
we conclude that g(t) ≥0 for all t ∈[0, 1[, and this completes the proof of
assertion (iv).
The next theorem is used to estimate diﬀerences of the form ∥w∥y −∥w∥x,
Df(y)[w]−Df(x)[w], and f(y)−f(x)−Df(x)[y−x] in terms of ∥w∥x, ∥y−x∥x
and the function ρ.
Theorem 16.3.2. Let f : X →R be a closed self-concordant function, and
suppose that x is a point in X and that ∥y −x∥x < 1. Then, y is also a
point in X, and the following inequalities hold for the vector v = y −x and
arbitrary vectors w:
∥v∥x
1 + ∥v∥x
≤∥v∥y ≤
∥v∥x
1 −∥v∥x
(16.3)
∥v∥2
x
1 + ∥v∥x
≤Df(y)[v] −Df(x)[v] ≤
∥v∥2
x
1 −∥v∥x
(16.4)
ρ(−∥v∥x) ≤f(y) −f(x) −Df(x)[v] ≤ρ(∥v∥x)
(16.5)
(1 −∥v∥x)∥w∥x ≤∥w∥y ≤
∥w∥x
1 −∥v∥x
(16.6)
Df(y)[w] −Df(x)[w] ≤D2f(x)[v, w] + ∥v∥2
x∥w∥x
1 −∥v∥x
≤∥v∥x∥w∥x
1 −∥v∥x
.
(16.7)
The left parts of the three inequalities (16.3), (16.4) and (16.5) are also
satisﬁed with v = y −x for all y ∈X.
Proof. We leave the proof that y belongs to X to the end and start by showing
that the inequalities (16.3–16.7) hold under the additional assumption y ∈X.
I. We begin with inequality (16.6).
If ∥w∥x = 0, then ∥w∥z = 0 for all
z ∈X, according to Theorem 16.1.2. Hence, the inequality holds in this case.
Therefore, let w be an arbitrary vector with ∥w∥x ̸= 0, let xt = x + t(y −x),
and deﬁne the function ψ by
ψ(t) = ∥w∥−1
xt =

D2f(xt)[w, w]
−1/2.
The function ψ is deﬁned on an open interval that contains the interval [0, 1],
ψ(0) = ∥w∥−1
x
and ψ(1) = ∥w∥−1
y . It now follows, using Theorem 16.1.1, that
|ψ′(t)| = 1
2

D2f(xt)[w, w]
−3/2D3f(xt)[w, w, v]

(16.8)
= 1
2∥w∥−3
xt
D3f(xt)[w, w, v]
 ≤1
2∥w∥−3
xt · 2∥w∥2
xt∥v∥xt
= ∥w∥−1
xt ∥v∥xt = ψ(t)∥v∥xt.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
53
Self-concordant functions
53
If ∥v∥x = 0, then ∥v∥z = 0 for all z ∈X, and hence ψ′(t) = 0 for
0 ≤t ≤1. This implies that ψ(1) = ψ(0), i.e. that ∥w∥y = ∥w∥x. The
inequalities (16.3) and (16.6) are thus satisﬁed in the case ∥v∥x = 0.
Assume henceforth that ∥v∥x ̸= 0, and ﬁrst take w = v in the deﬁnition of
the function ψ. In this special case, inequality (16.8) simpliﬁes to |ψ′(t)| ≤1
for t ∈[0, 1], and hence ψ(0) −1 ≤ψ(1) ≤ψ(0) + 1, by the mean-value
theorem. The right part of this inequality means that ∥v∥−1
y
≤∥v∥−1
x
+ 1,
which after rearrangement gives the left part of inequality (16.3). Note, that
this is true even in the case ∥v∥x ≥1.
Correspondingly, the left part of the same inequality gives rise to the right
part of inequality (16.3), now under the assumption that ∥v∥x < 1.
To prove inequality (16.6), we return to the function ψ with a general w.
Since ∥tv∥x = t∥v∥x < 1 for 0 ≤t ≤1, it follows from the already proven
inequality (16.3) (with xt = x + tv instead of y) that
∥v∥xt = 1
t ∥tv∥xt ≤1
t ·
∥tv∥x
1 −∥tv∥x
=
∥v∥x
1 −t∥v∥x
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
54
Self-concordant functions
Insert this estimate into (16.8); this gives us the following inequality for the
derivative of the function ln ψ(t):
|(ln ψ(t))′| = |ψ′(t)|
ψ(t) = ∥v∥xt ≤
∥v∥x
1 −t∥v∥x
.
Let us now integrate this inequality over the interval [0, 1]; this results in the
estimate
ln ∥w∥y
∥w∥x
 =
ln ψ(0)
ψ(1)
 =
ln ψ(1) −ln ψ(0)
 =

 1
0
(ln ψ(t))′ dt

≤
 1
0
∥v∥x
1 −t∥v∥x
dt = −ln(1 −∥v∥x),
which after exponentiation yields
1 −∥v∥x ≤∥w∥y
∥w∥x
≤(1 −∥v∥x)−1,
and this is inequality (16.6).
II. To prove the inequality (16.4), we deﬁne
φ(t) = Df(xt)[v],
where xt = x + t(y −x), as before. Then
φ′(t) = D2f(xt)[v, v] = ∥v∥2
xt = t−2∥tv∥2
xt,
so by using inequality (16.3), we obtain the inequality
∥v∥2
x
(1 + t∥v∥x)2 = 1
t2
∥tv∥2
x
(1 + ∥tv∥x)2 ≤φ′(t) ≤1
t2
∥tv∥2
x
(1 −∥tv∥x)2 =
∥v∥2
x
(1 −t∥v∥x)2
for 0 ≤t ≤1. The left part of this inequality holds with v = y −x for all
y ∈X, and the right part holds if ∥v∥x < 1, and by integrating the inequality
over the interval [0,1], we arrive at inequality (16.4).
III. To prove inequality (16.5), we start with the function
Φ(t) = f(xt) −Df(x)[v] t,
noting that
Φ(1) −Φ(0) = f(y) −f(x) −Df(x)[v]
and that
Φ′(t) = Df(xt)[v] −Df(x)[v].
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
55
Self-concordant functions
By replacing y with xt in inequality (16.4) , we obtain the following inequality
t∥v∥2
x
1 + t∥v∥x
≤Φ′(t) ≤
t∥v∥2
x
1 −t∥v∥x
,
where the right part holds only if ∥v∥x < 1. By integrating the above in-
equality over the interval [0, 1], we obtain
ρ(−∥v∥x) =
 1
0
t∥v∥2
x
1 + t∥v∥x
dt ≤Φ(1) −Φ(0) ≤
 1
0
t∥v∥2
x
1 −t∥v∥x
dt = ρ(∥v∥x),
i.e. inequality (16.5).
IV. The proof of inequality (16.7) is analogous to the proof of inequality
(16.4), but this time our function φ is deﬁned as
φ(t) = Df(xt)[w].
Now, φ′(t) = D2f(xt)[w, v] and φ′′(t) = D3f(xt)[w, v, v], so it follows from
Theorem 16.1.1 and inequality (16.6) that
|φ′′(t)| ≤2∥w∥xt∥v∥2
xt ≤2 ∥w∥x∥v∥2
x
(1 −t∥v∥x)3.
By integrating this inequality over the interval [0, s], where s ≤1, we get the
estimate
φ′(s) −φ′(0) ≤
 s
0
|φ′′(t)| dt ≤2∥w∥x
 s
0
∥v∥2
x dt
(1 −t∥v∥x)3
= ∥w∥x

∥v∥x
(1 −s∥v∥x)2 −∥v∥x

,
and another integration over the interval [0, 1] results in the inequality
φ(1) −φ(0) −φ′(0) ≤
 1
0
(φ′(s) −φ′(0)) ds ≤∥w∥x∥v∥2
x
1 −∥v∥x
,
which is the left part of inequality (16.7).
By the Cauchy–Schwarz inequality,
D2f(x)[v, w] = ⟨v, f ′′(x)w⟩= ⟨f ′′(x)1/2v, f ′′(x)1/2w⟩
≤∥f ′′(x)1/2v∥∥f ′′(x)1/2w∥= ∥v∥x∥w∥x,
and we obtain the right part of inequality (16.7) by replacing D2f(x)[v, w]
with its majorant ∥v∥x∥w∥x.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
56
Self-concordant functions
56
V. It now only remains to prove that the condition ∥y −x∥x < 1 implies that
the point y lies in X.
Assume the contrary. i.e. that there is a point y outside X such that
∥y −x∥x < 1. The line segment [x, y] then intersects the boundary of X in
a point x + tv, where t is a number in the interval ]0, 1]. The function ρ is
increasing in the interval [0, 1[, and hence ρ(t∥v∥x) ≤ρ(∥v∥x) if 0 ≤t < t. It
therefore follows from inequality (16.5) that
f(x+tv) ≤f(x)+tDf(x)[v]+ρ(t∥v∥x) ≤f(x)+|Df(x)[v]|+ρ(∥v∥x) < +∞
for all t in the interval [0, t[.
However, this is a contradiction, because
limt→t f(x + tv) = +∞, since f is a closed function and x + tv is a boundary
point. Thus, y is a point in X.
16.4
Minimization
This section focuses on minimizing self-concordant functions, and the results
are largely based on the following theorem, which also plays a signiﬁcant role
in our study of Newton’s algorithm in the next section.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
57
Self-concordant functions
Theorem 16.4.1. Let f : X →R be a closed self-concordant function, sup-
pose that x ∈X is a point with ﬁnite Newton decrement λ = λ(f, x), let
∆xnt be a Newton direction at x, and deﬁne
x+ = x + (1 + λ)−1∆xnt.
The point x+ is then a point in X and
f(x+) ≤f(x) −ρ(−λ).
Remark. So a minimum point ˆx of f must satisfy the inequality
f(ˆx) ≤f(x) −ρ(−λ)
for all x ∈X with ﬁnite Newton decrement λ.
Proof. The vector v = (1 + λ)−1∆xnt has local seminorm
∥v∥x = (1 + λ)−1∥∆xnt∥x = λ(1 + λ)−1 < 1,
so it follows from Theorem 16.3.2 that the point x+ = x + v lies in X and
that
f(x+) ≤f(x) + Df(x)[v] + ρ(∥v∥x) = f(x) +
1
1 + λ⟨f ′(x), ∆xnt⟩+ ρ(
λ
1 + λ)
= f(x) −
λ2
1 + λ −
λ
1 + λ −ln
1
1 + λ = f(x) −λ + ln(1 + λ)
= f(x) −ρ(−λ).
Theorem 16.4.2. The Newton decrement λ(f, x) of a downwards bounded
closed self-concordant function f : X →R is ﬁnite at each point x ∈X and
infx∈X λ(f, x) = 0.
Proof. Let v be an arbitrary vector in the recessive subspace Vf = N(f ′′(x)).
Then
f(x + tv) = f(x) + t⟨f ′(x), v⟩
for all t ∈R according to Theorem 16.2.1, and since f is supposed to be
bounded below, this implies that ⟨f ′(x), v⟩= 0. This proves the implication
f ′′(x)v = 0 ⇒⟨f ′(x), v⟩= 0,
which means that there exists a Newton direction at the point x. Hence,
λ(f, x) is a ﬁnite number.
If there is a positive number δ such that λ(f, x) ≥δ for all x ∈X, then
repeated application of Theorem 16.4.1, with an arbitrary point x0 ∈X
as starting point, results in a sequence (xk)∞
0
of points in X, deﬁned as
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
58
Self-concordant functions
xk+1 = x+
k and satisfying the inequality f(xk) ≤f(x0) −kρ(−δ) for all k.
Since ρ(−δ) > 0, this contradicts our assumption that f is bounded below.
Thus, infx∈X λ(f, x) = 0.
Theorem 16.4.3. All sublevel sets of a non-degenerate closed self-concordant
function f : X →R are compact sets if λ(f, x0) < 1 for some point x0 ∈X.
Proof. The sublevel sets are closed since the function is closed, and to prove
that they are also bounded it is enough to prove that the particular sublevel
set S = {x ∈X | f(x) ≤f(x0)} is bounded, because of Theorem 6.8.3 in
Part I.
So, let x be an arbitrary point in S, and write r = ∥x −x0∥x0 and
λ0 = λ(f, x0) for short. Then
f(x) ≥f(x0) + Df(x0)[x −x0] + ρ(−r),
according to Theorem 16.3.2, and
Df(x0)[x −x0] = ⟨f ′(x0), x −x0⟩≥−λ(f, x0)∥x −x0∥x0 = −λ0r,
by Theorem 15.1.2. Combining these two inequalities we obtain the inequal-
ity
f(x0) ≥f(x) ≥f(x0) −λ0r + ρ(−r),
which simpliﬁes to
r −ln(1 + r) = ρ(−r) ≤λ0r.
Hence,
(1 −λ0)r ≤ln(1 + r)
and it follows that r ≤r0, r0 being the unique positive root of the equation
(1 −λ0)r = ln(1 + r). The sublevel set S is thus included in the ellipsoid
{x ∈Rn | ∥x −x0∥x0 ≤r0}, and it is therefore a bounded set.
Theorem 16.4.4. A closed self-concordant function f : X →R has a mini-
mum point if λ(f, x0) < 1 for some point x0 ∈X.
Proof. If in addition f is non-degenerate, then S = {x ∈X | f(x) ≤f(x0)}
is a compact set according to the previous theorem, so the restriction of
f to the sublevel set S attains a mininum, and this minimum is clearly a
global minimum of f. The minimum point is furthermore unique, since non-
degenerate self-concordant functions are strictly convex.
If f is degenerate, then there is a non-degenerate closed self-concordant
function g: X0 →R with the same range as f, according to the discussion
following Theorem 16.2.1. The relationship between the two functions has
the form g(y) = f(Ay + v), where A is an injective linear map and v is
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
59
Self-concordant functions
an arbitrary vector in the recessive subspace Vf.
To the point x0 there
corresponds a point y0 ∈X0 such that Ay0 + v = x0 for some v ∈Vf, and
λ(g, y0) ≤λ(f, x0) < 1. By the already proven part of the theorem, g has
a minimum point ˆy, and this implies that all points in the set Aˆy + Vf are
minimum points of f.
Theorem 16.4.5. Every downwards bounded closed self-concordant function
f : X →R has a minimum point.
Proof. It follows from Theorem 16.4.2 that there is a point x0 ∈X such that
λ(f, x0) < 1, so the theorem is a corollary of Theorem 16.4.4.
Our next theorem describes how well a given point approximates the
minimum point of a closed self-concordant function.
Theorem 16.4.6. Let f : X →R be a closed self-concordant function with
a minimum point ˆx. If x ∈X is an arbitrary point with Newton decrement
λ = λ(f, x) < 1, then
ρ(−λ) ≤f(x) −f(ˆx) ≤ρ(λ),
(16.9)
λ
1 + λ ≤∥x −ˆx∥x ≤
λ
1 −λ,
(16.10)
∥x −ˆx∥ˆx ≤
λ
1 −λ.
(16.11)
Remark. Since ρ(t) ≤t2 if t ≤1
2, we conclude from inequality (16.9) that
f(x) −fmin ≤λ(f, x)2
as soon as λ(f, x) ≤1
2.
Proof. To simplify the notation, let v = x −ˆx and r = ∥v∥x.
The left part of inequality (16.9) follows directly from the remark after
Theorem 16.4.1. To prove the right part of the same inequality, we recall the
inequality
(16.12)
⟨f ′(x), v⟩≤λ(f, x)∥v∥x = λr,
which we combine with the left part of inequality (16.5) in Theorem 16.3.2
and inequality (iii) in Lemma 16.3.1. This results in the following chain of
inequalities:
f(ˆx) = f(x −v) ≥f(x) + ⟨f ′(x), −v⟩+ ρ(−∥−v∥x)
= f(x) −⟨f ′(x), v⟩+ ρ(−r)
≥f(x) −λr + ρ(−r) ≥f(x) −ρ(λ),
and the proof of inequality (16.9) is now complete.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
60
Self-concordant functions
60
Since x −v = ˆx and f ′(ˆx) = 0, it follows from inequality (16.12) and the
left part of inequality (16.4) that
λr ≥⟨f ′(x), v⟩= ⟨f ′(x −v), −v⟩−⟨f ′(x), −v⟩≥
∥−v∥2
x
1 + ∥−v∥x
=
r2
1 + r,
and by solving the inequality above with respect to r, we obtain the right
part of inequality (16.10).
The left part of the same inequality obviously holds if r ≥1. So assume
that r < 1. Due to inequality (16.7),
⟨f ′(x), w⟩= ⟨f ′(x −v), −w⟩−⟨f ′(x), −w⟩≤∥−v∥x∥−w∥x
1 −∥−v∥x
=
r
1 −r∥w∥x,
and hence
λ =
sup
∥w∥x≤1
⟨f ′(x), w⟩≤
r
1 −r,
which gives the left part of inequality (16.10).
To prove the remaining inequality (16.11), we use the left part of inequal-
ity (16.5) with y replaced by x and x replaced by ˆx, which results in the
inequality
ρ(−∥x −ˆx∥ˆx) ≤f(x) −f(ˆx).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
61
Self-concordant functions
According to the already proven inequality (16.9), f(x)−f(ˆx) ≤ρ(λ), so
it follows that ρ(−∥x −ˆx∥ˆx) ≤ρ(λ), and by Lemma 16.3.1, this means that
∥x −ˆx∥ˆx ≤
λ
1 −λ.
Theorem 16.4.7. Let f be a closed self-concordant function whose domain
X is a subset of Rn, and suppose that
ν = sup{λ(f, x) | x ∈X} < 1.
Then X is equal to the whole space Rn, and f is a constant function.
Proof. It follows from Theorem 16.4.4 that f has a minimum point ˆx and
from inequality (16.9) in Theorem 16.4.6 that
ρ(−ν) ≤f(x) −f(ˆx) ≤ρ(ν)
for all x ∈X. Thus, f is a bounded function, and since f is closed, this
implies that X is a set without boundary points. Hence, X = Rn.
Let v be an arbitrary vector in Rn. By applying inequality (16.11) with
x = ˆx + tv, we obtain the inequality
t∥v∥ˆx = ∥x −ˆx∥ˆx ≤
λ(f, x)
1 −λ(f, x) ≤
ν
1 −ν
for all t > 0, and this implies that ∥v∥ˆx = 0. The recessive subspace Vf of
f is in other words equal to Rn, so f is a constant function according to
Theorem 16.2.1.
16.5
Newton’s method for self-concordant func-
tions
In this section we show that Newton’s method converges when the objec-
tive function f : X →R is closed, self-concordant and bounded below. We
shall also give an estimate of the number of iterations needed to obtain the
minimum with a given accuracy ϵ −an estimate that only depends on ϵ
and the diﬀerence between the minimum value and the function value at
the starting point. The algorithm starts with a damped phase, which re-
quires no line search as the step length at the point x can be chosen equal to
1/(1+λ(f, x)), and then enters into a pure phase with quadratic convergence,
when the Newton decrement is suﬃciently small.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
62
Self-concordant functions
The damped phase
During the damped phase, the points xk in Newton’s algorithm are generated
recursively by the equation
xk+1 = xk +
1
1 + λk
vk,
where λk = λ(f, xk) is the Newton decrement at xk and vk is a Newton
direction at the same point, i.e
f ′′(xk)vk = −f ′(xk).
According to Theorem 16.4.1, if the starting point x0 is a point in X, then
all generated points xk will lie in X and
f(xk+1) −f(xk) ≤ρ(−λk).
If δ > 0 and λk ≥δ, then ρ(−λk) ≥ρ(−δ), because the function ρ(t)
is decreasing for f¨or t < 0. So if xN is the ﬁrst point of the sequence that
satisﬁes the inequality λN = λ(f, xN) < δ, then
fmin −f(x0) ≤f(xN) −f(x0) =
N−1

k=0
(f(xk+1) −f(xk))
≤−
N−1

k=0
ρ(−λk) ≤−
N−1

k=0
ρ(−δ) = −Nρ(−δ),
which implies that att N ≤(f(x0) −fmin)/ρ(−δ). This proves the following
theorem.
Theorem 16.5.1. Let f : X →R be a closed, self-concordant and downwards
bounded function. Using Newton’s damped algorithm with step size as above,
we need at most
f(x0) −fmin
ρ(−δ)

iterations to generate a point x with Newton decrement λ(f, x) < δ from an
arbitrary starting point x0 in X.
Local convergence
We now turn to the study of Newton’s pure method for starting points that
are suﬃciently close to the minimum point ˆx. For a corresponding analysis
of Newton’s damped method we refer to exercise 16.6.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
63
Self-concordant functions
63
Theorem 16.5.2. Let f : X →R be a closed self-concordant function, and
suppose that x ∈X is a point with Newton decrement λ(f, x) < 1. Let ∆xnt
be a Newton direction at x, and let
x+ = x + ∆xnt.
Then, x+ is a point in X and
λ(f, x+) ≤

λ(f, x)
1 −λ(f, x)
2
.
Proof. The conclusion that x+ lies in X follows from Theorem 16.3.2, because
∥∆xnt∥x = λ(f, x) < 1. To prove the inequality for λ(f, x+), we ﬁrst use
inequality (16.7) of the same theorem with v = x+ −x = ∆xnt and obtain
⟨f ′(x+), w⟩≤⟨f ′(x), w⟩+ ⟨f ′′(x)∆xnt, w⟩+ λ(f, x)2∥w∥x
1 −λ(f, x)
= ⟨f ′(x), w⟩+ ⟨−f ′(x), w⟩+ λ(f, x)2∥w∥x
1 −λ(f, x)
= λ(f, x)2∥w∥x
1 −λ(f, x) .
But
∥w∥x ≤
∥w∥x+
1 −λ(f, x),
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
64
Self-concordant functions
by inequality (16.6), so it follows that
⟨f ′(x+), w⟩≤λ(f, x)2∥w∥x+
(1 −λ(f, x))2 ,
and this implies that
λ(f, x+) =
sup
∥w∥x+≤1
⟨f ′(x+), w⟩≤
λ(f, x)2
(1 −λ(f, x))2.
We are now able to prove the following convergence result for Newton’s
pure method.
Theorem 16.5.3. Suppose that f : X →R is a closed self-concordant func-
tion and that x0 is a point in X with Newton decrement
λ(f, x0) ≤δ < λ = 1
2(3 −
√
5) = 0.381966 . . . .
Let the sequence (xk)∞
0 be recursively deﬁned by
xk+1 = xk + vk,
where vk is a Newton direction at the point xk.
The sequence (f(xk))∞
0 converges to the minimum value fmin of the func-
tion f, and if ϵ > 0 then
f(xk) −fmin < ϵ
for k > A + log2(log2 B/ϵ), where A and B are constants that only depend
on δ.
Moreover, if f is a non-degenerate function, then (xk)∞
0 converges to the
unique minimum point of f.
Proof. The critical number λ is a root of the equation (1 −λ)2 = λ, and if
0 ≤λ < λ then λ < (1 −λ)2.
Let K(λ) = (1 −λ)−2; the function K is increasing in the interval [0, λ[
and K(λ)λ < 1. It therefore follows from Theorem 16.5.2 that the following
inequality is true for all points x ∈X with λ(f, x) ≤δ < λ:
λ(f, x+) ≤K(λ(f, x)) λ(f, x)2 ≤K(δ)λ(f, x)2 ≤K(δ)δλ(f, x) ≤λ(f, x) ≤δ.
Now, let λk = λ(f, xk). Due to the inequality above, it follows by induc-
tion that λk ≤δ and that
λk+1 ≤K(δ)λ2
k
for all k, and the latter inequality in turn implies that
λk ≤K(δ)−1
K(δ)λ0
2k
≤(1 −δ)2
K(δ)δ)2k.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
65
Self-concordant functions
Hence, λk tends to 0 as k →∞, because K(δ)δ < 1. By the remark
following Theorem 16.4.6,
f(xk) −fmin ≤λ2
k,
if λk ≤1
2, so we conclude that
lim
k→∞f(xk) = fmin.
To prove the remaining error estimate, we can without loss of generaliza-
tion assume that ϵ < δ2, because if ϵ > δ2 then already
f(x0) −fmin ≤λ(f, x0)2 ≤δ2 < ϵ.
Let A and B be the constants deﬁned by
A = −log2

−2 log2(K(δ)δ)

and
B = (1 −δ)4.
Then 0 < B ≤1, and log2(log2 B/ϵ) is a well-deﬁned number, since B/ϵ ≥
(1 −δ)4/δ2 = (K(δ)δ)−2 > 1. If k > A + log2(log2 B/ϵ), then
λ2
k ≤(1 −δ)4
K(δ)δ
2k+1
< ϵ,
and consequently f(xk) −fmin ≤λ2
k < ϵ.
If f is a non-degenerate function, then f has a unique minimum point ˆx,
and it follow from inequality (16.11) in Theorem 16.4.6 that
∥xk −ˆx∥ˆx ≤
λk
1 −λk
→0,
as k →∞.
Since ∥·∥ˆx is a proper norm, this means that xk →ˆx.
When δ = 1/3, the values of the constants in Theorem 16.5.3 are A =
0.268 . . . and B = 16/81, and A + log2(log2 B/ϵ) = 6.87 for ϵ = 10−30. So
with a starting point x0 satisfying λ(f, x0) < 1/3, Newton’s algorithm will
produce a function value that approximates the minimum value with an error
less than 10−30 after at most 7 iterations.
Newton’s method for self-concordant functions
By combining Newton’s damped method with 1/(1+λ(f, x)) as damping fac-
tor and Newton’s pure method, we arrive at the following variant of Newton’s
method.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
66
Self-concordant functions
66
Newton’s method
Given a positive number δ < 1
2(3 −
√
5), a starting point x0 ∈X, and a
tolerance ϵ > 0.
1. Initiate: x:= x0.
2. Compute the Newton decrement λ = λ(f, x).
3. Go to line 8 if λ < δ else continue.
4. Compute a Newton direction ∆xnt at the point x.
5. Update: x:= x + (1 + λ)−1∆xnt.
6. Go to line 2.
7. Compute the Newton decrement λ = λ(f, x).
8. Stopping criterion: stop if λ < √ϵ. x is an approximate optimal point.
9. Compute a Newton direction ∆xnt at the point x.
10. Update: x:= x + ∆xnt.
11. Go to line 7.
Assuming that f is closed, self-concordant and downwards bounded, the
damped phase of the algorithm, i.e. steps 2–6, continues during at most
⌊(f(x0) −fmin)/ρ(−δ)⌋
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
67
Self-concordant functions
iterations, and the pure phase 7–11 ends according to Theorem 16.5.3 after
at most ⌈A + log2(log2 B/ϵ)⌉iterations. Therefore, we have the following
result.
Theorem 16.5.4. If the function f is closed, self-concordant and bounded
below, then the above Newton method terminates at a point x satisfying
f(x) < fmin + ϵ after at most
⌊(f(x0) −fmin))/ρ(−δ)⌋+ ⌈A + log2(log2 B/ϵ)⌉
iterations, where A and B are the constants of Theorem 16.5.3.
In particular, 1/ρ(−δ) = 21.905 when δ = 1/3, and the second term can
be replaced by the number 7 when ϵ ≥10−30. Thus, at most
⌊22(f(x0) −fmin)⌋+ 7
iterations are required to ﬁnd an approximation to the minimum value that
meets all practical requirements by a wide margin.
Exercises
16.1 Show that the function f(x) = x ln x −ln x is self-concordant on R++.
16.2 Suppose fi : Xi →R are self-concordant functions for i = 1, 2, . . . , m, and
let X = X1 ×X2 ×· · ·×Xm. Prove that the function f : X →R, deﬁned by
f(x1, x2, . . . , xm) = f1(x1) + f2(x2) + · · · + fm(xm)
for x = (x1, x2, . . . , xm) ∈X, is self-concordant.
16.3 Suppose that f : R++ →R is a three times continuously diﬀerentiable,
convex function, and that
|f′′′(x)| ≤3f′′(x)
x
for all x.
a) Prove that the function
g(x) = −ln(−f(x)) −ln x,
with {x ∈R++ | f(x) < 0} as domain, is self-concordant.
[Hint: Use that 3a2b + 3a2c + 2b3 + 2c3 ≤2(a2 + b2 + c2)3/2 if a, b, c ≥0.]
b) Prove that the function
F(x, y) = −ln(y −f(x)) −ln x
is self-concordant on the set {(x, y) ∈R2 | x > 0, y > f(x)}.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
68
Self-concordant functions
16.4 Show that the following functions f satisfy the conditions of the previous
exercise:
a) f(x) = −ln x
b) f(x) = x ln x
c) f(x) = −xp, where 0 < p ≤1.
16.5 Let us write x′ for (x1, x2, . . . , xn−1) when x = (x1, x2, . . . , xn), and let ∥·∥
denote the Euclidean norm in Rn−1. Let X = {x ∈Rn | ∥x′∥< xn}, and
deﬁne the function f : X →R by f(x) = −ln(x2
n −∥x′∥2). Prove that the
following identity holds for all v ∈Rn:
D2f(x)[v, v] = 1
2

Df(x)[v]
2
+ 2(x2
n −∥x′∥2)(∥x′∥2∥v′∥2 −⟨x′, v′⟩2) + (vn∥x′∥2 −xn⟨x′, v′⟩)2
(x2n −∥x′∥2)2∥x′∥2
,
and use it to conclude that f is a convex function and that λ(f, x) = 2 for
all x ∈X.
16.6 Convergence for Newton’s damped method.
Suppose that the function f : X →R is closed and self-concordant, and
deﬁne for points x ∈X with ﬁnite Newton decrement the point x+ by
x+ = x +
1
1 + λ(f, x)∆xnt,
where ∆xnt is a Newton direction at x.
a) Then x+ is a point in X, according to Theorem 16.3.2. Show that
λ(f, x+) ≤2λ(f, x)2,
and hence that λ(f, x+) ≤λ(f, x) if λ(f, x) ≤1
2.
b) Suppose x0 is a point in X with Newton decrement λ(f, x0) ≤1
4, and
deﬁne the sequence (xk)∞
0 recursively by xk+1 = x+
k . Show that
f(xk) −fmin ≤1
4 ·
 1
2
2k+1
,
and hence that f(xk) converges quadratically to fmin.
Appendix
We begin with a result on tri-linear forms which was needed in the proof
of the fundamental inequality
D3f(x)[u, v, w]
 ≤2∥u∥x∥v∥x∥w∥x for self-
concordant functions.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
69
Self-concordant functions
69
Fix an arbitrary scalar product ⟨· , ·⟩on Rn and let ∥·∥denote the cor-
responding norm, i.e. ∥v∥= ⟨v, v⟩1/2. If φ(u, v, w) is a symmetric tri-linear
form on Rn × Rn × Rn, we deﬁne its norm ∥φ∥by
∥φ∥=
sup
u,v,w̸=0
|φ(u, v, w)|
∥u∥∥v∥∥w∥.
The numerator and the denominator in the expression for ∥φ∥are homoge-
neous of the same degree 3, hence
∥φ∥=
sup
(u,v,w)∈S3 |φ(u, v, w)|,
where S denotes the unit sphere in Rn with respect to the norm ∥·∥, i.e.
S = {u ∈Rn | ∥u∥= 1}.
It follows from the norm deﬁnition that
|φ(u, v, w)| ≤∥φ∥∥u∥∥v∥∥w∥
for all vectors u, v, w in Rn.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
70
Self-concordant functions
Since tri-linear forms are continuous and the unit sphere is compact, the
least upper bound ∥φ∥is attained at some point (u, v, w) ∈S3, and we will
show that the least upper bound is indeed attained at some point where
u = v = w. This is the meaning of the following theorem.
Theorem 1. Suppose that φ(u, v, w) is a symmetric tri-linear form. Then
∥φ∥=
sup
u,v,w̸=0
|φ(u, v, w)|
∥u∥∥v∥∥w∥= sup
v̸=0
|φ(v, v, v)|
∥v∥3
.
Remark. The theorem is a special case of the corresponding result for sym-
metric m-multilinear forms, but we only need the case m = 3. The general
case is proved by induction.
Proof. Let
∥φ∥′ = sup
v̸=0
|φ(v, v, v)|
∥v∥3
= sup
∥v∥=1
|φ(v, v, v)|.
We claim that ∥φ∥= ∥φ∥′. Obviously, ∥φ∥′ ≤∥φ∥, so we only have to prove
the converse inequality ∥φ∥≤∥φ∥′.
To prove this inequality, we need the corresponding result for symmetric
bilinear forms ψ(u, v). To such a form there is associated a symmetric linear
operator (matrix) A such that ψ(u, v) = ⟨Au, v⟩, and if e1, e2, . . . , en is an
ON-basis of eigenvectors of A and λ1, λ2, . . . , λn denote the corresponding
eigenvalues with λ1 as the one with the largest absolute value, and if u, v ∈S
are vectors with coordinates u1, u2, . . . , un and v1, v2, . . . , vn with respect to
the given ON-basis, then
|ψ(u, v)| = |
n

i=1
λiuivi| ≤
n

i=1
|λi||ui||vi| ≤|λ1|
n

i=1
|ui||vi|
≤|λ1|
 n

i=1
u2
i
1/2 n

i=1
v2
i
1/2
= |λ1| = |ψ(e1, e1)|,
which proves that sup(u,v)∈S2 |ψ(u, v)| = supv∈S |ψ(v, v)|.
We now return to the tri-linear form φ(u, v, w). Let (ˆu, ˆv, ˆw) be a point
in S3 where the least upper bound deﬁning ∥φ∥is attained, i.e.
∥φ∥= φ(ˆu, ˆv, ˆw),
and consider the function
ψ(u, v) = φ(u, v, ˆw);
this is a symmetric bilinear form on Rn × Rn and
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
71
Self-concordant functions
sup
(u,v)∈S2 |ψ(u, v)| = ∥φ∥.
But as already proven,
sup
(u,v)∈S2 |ψ(u, v)| = sup
v∈S
|ψ(v, v)|.
Therefore, we conclude that we can withour restriction assume that ˆu = ˆv.
We have in other words shown that the set
A = {(v, w) ∈S2 | |φ(v, v, w)| = ∥φ∥}
is nonempty. The set A is a closed subset of S2, and hence the number
α = max{⟨v, w⟩| (v, w) ∈A}
exists, and obviously 0 ≤α ≤1.
Due to tri-linearity,
φ(u + v, u + v, w) −φ(u −v, u −v, w) = 4φ(u, v, w).
So if u, v, w are arbitrary vectors in S, i.e. vectors with norm 1, then
4|φ(u, v, w)| ≤|φ(u + v, u + v, w)| + |φ(u −v, u −v, w)|
≤|φ(u + v, u + v, w)| + ∥φ∥∥u −v∥2∥w∥
= |φ(u + v, u + v, w)| −∥φ∥∥u + v∥2 + ∥φ∥(∥u + v∥2 + ∥u −v∥2)
= |φ(u + v, u + v, w)| −∥φ∥∥u + v∥2 + ∥φ∥(2∥u∥2 + 2∥v∥2)
= |φ(u + v, u + v, w)| −∥φ∥∥u + v∥2 + 4∥φ∥.
Now choose (v, w) ∈A such that ⟨v, w⟩= α. By the above inequality, we
then have
4∥φ∥= 4|φ(v, v, w)| = 4|φ(v, w, v)|
≤|φ(v + w, v + w, v)| −∥φ∥∥v + w∥2 + 4∥φ∥,
and it follows that
|φ(v + w, v + w, v)| ≥∥φ∥∥v + w∥2.
Note that ∥v + w∥2 = ∥v∥2 + ∥w∥2 + 2⟨v, w⟩= 2 + 2α > 0. Therefore, we
can form the vector z = (v + w)/∥v + w∥and write the above inequality as
|φ(z, z, v)| ≥∥φ∥,
which implies that
(16.13)
|φ(z, z, v)| = ∥φ∥
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
72
Self-concordant functions
since z and v are vectors in S. We conclude that the pair (z, v) is an element
of the set A, and hence
α ≥⟨z, v⟩= ⟨v, v⟩+ ⟨w, v⟩
∥v + w∥
=
1 + α
√2 + 2α =

1 + α
2
.
This inequality forces α to be greater than or equal to 1. Hence α = 1 and
⟨z, v⟩= 1 = ∥z∥∥v∥.
So Cauchy–Schwarz’s inequality holds with equality in this case, and this
implies that z = v.
By inserting this in equality (16.13), we obtain the
inequality
∥φ∥′ ≥φ(v, v, v) = ∥φ∥,
and the proof of the theorem is now complete.
Our second result in this appendix is a uniqueness theorem for functions
that satisfy a special diﬀerential inequality.
Theorem 2. Suppose that the function y(t) is continuously diﬀerentiable in
the interval I = [0, b[, that y(t) ≥0, y(0) = 0 and y′(t) ≤Cy(t)α for some
given constants C > 0 and α ≥1. Then, y(t) = 0 in the interval I.
Proof. Let a = sup{x ∈I | y(t) = 0 for 0 ≤t ≤x}. We will prove that a = b
by showing that the assumption a < b gives rise to a contradiction.
By continuity, y(a) = 0. Choose a point c ∈]a, b[ and let
M = max{y(t) | a ≤t ≤c}.
Then choose a point d such that a < d < c and d −a ≤1
2C−1M 1−α. The
maximum of the function y(t) on the interval [a, d] is attained at some point
e, and by the least upper bound deﬁnition of the point a, we have y(e) > 0.
Of course, we also have y(e) ≤M, so it follows that
y(e) = y(e) −y(a) =
 e
a
y′(t) dt ≤C
 e
a
y(t)α dt
≤C(d −a)y(e)α ≤C(d −a)M α−1y(e) ≤1
2y(e),
which is a contradiction.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
73
The path-following method
Chapter 17
The path-following method
In this chapter, we describe a method for solving the optimization problem
min f(x)
s.t.
x ∈X
when X is a closed subset of Rn with nonempty interior and f is a con-
tinuous function which is diﬀerentiable in the interior of X.
We assume
throughout that X = cl(int X). Pretty soon, we will restrict ourselves to
convex problems, i.e. assume that X is a convex set and f is a convex func-
tion, in which case, of course, automatically X = cl(int X) for all sets with
nonempty interior.
Descent methods require that the function f is diﬀerentiable in a neigh-
borhood of the optimal point, and if the optimal point lies on the boundary
of X, then we have a problem. One way to attack this problem is to choose
a function F : int X →R with the property that F(x) →+∞as x goes
to boundary of X and a parameter µ > 0, and to minimize the function
f(x) + µF(x) over int X. This function’s minimum point ˆx(µ) lies in the
interior of X, and since f(x) + µF(x) →f(x) as µ →0, we can hope that
the function value f(ˆx(µ)) should be close to the minimum value of f, if the
parameter µ is small enough. The function F acts as a barrier that prevents
the approximating minimum point from lying on the boundary.
The function µ−1f(x)+F(x) has of course the same minimum point ˆx(µ)
as f(x)+µF(x), and for technical reasons it works better to have the param-
eter in front of the objective function f than in front of the barrier function
F. Henceforth, we will therefore instead, with new notation, examine what
happens to the minimum point ˆx(t) of the function Ft(x) = tf(x) + F(x),
when the parameter t tends to +∞.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
74
The path-following method
74
17.1
Barrier and central path
Barrier
We begin with the formal deﬁnition of a barrier.
Deﬁnition. Let X be a closed convex set with nonempty interior. A barrier
to the set X is a diﬀerentiable function F : int X →R with the property that
limk→∞F(xk) = +∞for all sequences (xk)∞
1 that converge to a boundary
point of X.
If a barrier function has a unique minimum point, then this point is called
the analytic center of the set X (with respect to the barrier).
Remark 1. A convex function with an open domain goes to ∞at the bound-
ary if and only if it is a closed function. Hence, if F : int X →R is convex
and diﬀerentiable, then F is a barrier to X if and only if F is closed.
Remark 2. A strictly convex barrier function to a compact convex set has
a unique minimum point in the interior of the set. So compact convex sets
with nonempty interiors have analytic centers with respect to strictly convex
barriers.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
75
The path-following method
Now, let F be a barrier to the closed convex set X, and suppose that we
want to minimize a given function f : X →R. For each real number t ≥0
we deﬁne the function Ft : int X →R by
Ft(x) = tf(x) + F(x).
In particular, F0 = F. The following theorem is the basis for barrier-based
interior-point methods for minimization.
Theorem 17.1.1. Suppose that f : X →R is a continuous function, and let
F be a downwards bounded barrier to the set X. Suppose that the functions
Ft have minimum points ˆx(t) in the interior of X for each t > 0. Then,
lim
t→+∞f(ˆx(t)) = inf
x∈X f(x).
Proof. Let vmin = infx∈X f(x) and M = infx∈int X F(x). (We do not exclude
the possibility that vmin = −∞, but M is of course a ﬁnite number.)
Choose, given η > vmin, a point x∗∈int X such that f(x∗) < η. Then
vmin ≤f(ˆx(t)) ≤f(ˆx(t)) + t−1(F(ˆx(t)) −M) = t−1
Ft(ˆx(t)) −M

≤t−1
Ft(x∗) −M

= f(x∗) + t−1(F(x∗) −M).
Since the right hand side of this inequality tends to f(x∗) as t →+∞, it
follows that vmin ≤f(ˆx(t)) < η for all suﬃciently large numbers t, and this
proves the theorem.
In order to use the barrier method, one needs of course an appropriate
barrier to the given set. For sets of the type
X = {x ∈Ω| gi(x) ≤0, i = 1, 2, . . . , m}
we will use the logarithmic barrier function
(17.1)
F(x) = −
m

i=1
ln(−gi(x)).
Note that the barrier function F is convex if all functions gi : Ω→R are
convex. In this case, X is a convex set, and the interior of X is nonempty if
Slater’s condition is satisﬁed, i.e. if there is a point x ∈Ωsuch that gi(x) < 0
for all i.
Other examples of barriers are the exponential barrier function
F(x) =
m

i=1
e−1/gi(x)
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
76
The path-following method
and the power function barriers
F(x) =
m

i=1
(−gi(x))−p,
where p > 0.
Central path
Deﬁnition. Let F be a barrier to the set X and suppose that the functions
Ft have unique minimum points ˆx(t) ∈int X for all t ≥0.
The curve
{ˆx(t) | t ≥0} is called the central path for the problem minx∈X f(x).
Note that ˆx(0) is the analytic center of X with respect to the barrier F,
so the central path starts at the analytic center.
Since the gradient is zero at an optimal point, we have
(17.2)
tf ′(ˆx(t)) + F ′(ˆx(t)) = 0
for all points on the central path.
The converse is true if the objective
function f and the barrier function F are convex, i.e. ˆx(t) is a point on the
central path if and only if equation (17.2) is satisﬁed.
The logarithmic barrier F to X = {x ∈Ω| gi(x) ≤0, i = 1, 2, . . . , m}
has derivative
F ′(x) = −
m

i=1
1
gi(x)g′
i(x),
so the central path equation (17.2) has in this case the following form for
t > 0:
(17.3)
f ′(ˆx(t)) −1
t
m

i=1
1
gi(ˆx(t))g′
i(ˆx(t)) = 0.
Let us now consider a convex optimization problem of the following type:
(P)
min f(x)
s.t.
gi(x) ≤0, i = 1, 2, . . . , m
We assume that Slater’s condition is satisﬁed and that the problem has an
optimal solution ˆx.
The corresponding Lagrange function L is given by
L(x, λ) = f(x) +
m

i=1
λigi(x),
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
77
The path-following method
77
x1
x2
ˆx
Figure 17.1. The central path associated with the problem of mini-
mizing the function f(x) = x1ex1+x2 over X = {x ∈R2 | x2
1 + x2
2 ≤1}
with barrier function F(x) = (1 −x2
1 −x2
2)−1. The minimum point is
ˆx = (−0.5825, 0.8128).
and it follows from equation (17.3) that L′
x(ˆx(t), ˆλ) = 0, if ˆλ ∈Rm
+ is the
vector deﬁned by
ˆλi = −
1
tgi(ˆx(t)).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
78
The path-following method
x1
1
x2
ˆx
ˆxF
Figure 17.2. The central path for the LP problem minx∈X 2x1 −3x2
with X = {x ∈R2 | x2 ≥0, x2 ≤3x1, x2 ≤x1 + 1, x1 + x2 ≤4}
and logarithmic barrier. The point ˆxF is the analytic center of X, and
ˆx = (1.5, 2.5) is the optimal solution.
Since the Lagrange function is convex in the variable x, we conclude that
ˆx(t) is a minimum point for the function L(· , ˆλ).
The value at ˆλ of the
dual function φ: Rm
+ →R to our minimization problem (P) is therefore by
deﬁnition
φ(ˆλ) = L(ˆx(t), ˆλ) = f(ˆx(t)) −m/t.
By weak duality, φ(ˆλ) ≤f(ˆx), so it follows that
f(ˆx(t)) −m/t ≤f(ˆx).
We have thus arrived at the following approximation theorem, which for
convex problems with logarithmic barrier provides more precise information
than Theorem 17.1.1.
Theorem 17.1.2. The points ˆx(t) on the central path for the convex mini-
mization problem (P) with optimal solution ˆx and logarithmic barrier satisfy
the inequality
f(ˆx(t)) −f(ˆx) ≤m
t .
Note that the estimate of the theorem depends on the number of con-
straints but not on the dimension.
17.2
Path-following methods
A strategy for determining the optimal value of the convex optimization
problem
(P)
min f(x)
s.t.
gi(x) ≤0, i = 1, 2, . . . , m
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
79
The path-following method
for twice continuously diﬀerentiable objective and constraint functions with
an error that is less than or equal to ϵ, would in light of Theorem 17.1.2
be to solve the optimization problem min Ft(x) with logarithmic barrier F
for t = m/ϵ, using for example Newton’s method. The strategy works for
small problems and with moderate demands on accuracy, but better results
are obtained by solving the problems min Ft(x) for an increasing sequence of
t-values until t ≥m/ϵ.
A simple version of the barrier method or the path-following method, as
it is also called, therefore looks like this:
Path-following method
Given a starting point x = x0 ∈int X, a real number t = t0 > 0, an update
parameter α > 1 and a tolerance ϵ > 0.
Repeat
1. Compute ˆx(t) by minimizing Ft = tf + F with x as starting point
2. Update: x:= ˆx(t).
3. Stopping criterion: stop if m/t ≤ϵ.
4. Increase t: t:= αt.
Step 1 is called an outer iteration or a centering step because it is about
ﬁnding a point on the central path. To minimize the function Ft, Newton’s
method is used, and the iterations of Newton’s method to compute ˆx(t) with
x as the starting point are called inner iterations.
It is not necessary to compute ˆx(t) exactly in the outer iterations; the
central path serves no other function than to lead to the optimal point ˆx,
and good approximations to points on the central path will also give rise to
a sequence of points which converges to ˆx.
The computational cost of the method obviously depends on the total
number of outer iterations that have to be performed before the stopping
criterion is met, and on the number of inner iterations in each outer iteration.
The update parameter α
The parameter α (and the initial value t0) determines the number of outer
iterations required to reach the stopping criterion t ≥m/ϵ. If α is small, i.e.
close to 1, then many outer iterations are needed, but on the other hand,
each outer iteration requires few inner iterations since the minimum point
x = ˆx(t) of the function Ft is then a very good starting point in Newton’s
algorithm for the problem of minimizing the function Fαt.
For large α values the opposite is true; few outer iterations are needed,
but each outer iteration now requires more Newton steps as the starting point
ˆx(t) is farther from the minimum point ˆx(αt).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
80
The path-following method
80
From experience, it turns out, however, that the above two eﬀects tend
to oﬀset each other. The total number of Newton steps is roughly constant
over a wide range of α, and values of α between 10 and 20 usually work well.
The initial value t0
The choice of the starting value t0 is also signiﬁcant. A small value requires
many outer iterations before the stopping criterion is met. A large value,
on the other hand, requires many inner iterations in the ﬁrst outer iteration
before a suﬃciently good approximation to the point ˆx(t0) on the central
path has been found. Since f(ˆx(t0)) −f(ˆx) ≈m/t0, it may be reasonable
to choose t0 so that m/t0 is of the same magnitude as f(x0) −f(ˆx). The
problem, of course, is that the optimal value f(ˆx) is not known a priori, so
it is necessary to use a suitable estimate. If, for example, a feasible point λ
for the dual problem is known and φ is the dual function, then φ(λ) can be
used as an approximation of f(ˆx), and t0 = m/(f(x0) −φ(λ)) can be taken
as initial t-value.
The starting point x0
The starting point x0 must lie in the interior of X, i.e. it has to satisfy all
constraints with strict inequality. If such a point is not known in advance,
then one can use the barrier method on an artiﬁcial problem to compute such
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
81
The path-following method
a point, or to conclude that the original problem has no feasible points.
The procedure is called phase 1 of the path-following method and works as
follows.
Consider the inequalities
(17.4)
gi(x) ≤0,
i = 1, 2, . . . , m
and suppose that the functions gi : Ω→R are convex and twice continuously
diﬀerentiable. To determine a point that satisﬁes all inequalities strictly or
to determine that there is no such point, we form the optimization problem
(17.5)
min s
s.t.
gi(x) ≤s,
i = 1, 2, . . . , m
in the variables x and s. This problem has strictly feasible points, because we
can ﬁrst choose x0 ∈Ωarbitrarily and then choose s0 > maxi gi(x0), and we
obtain in this way a point (x0, s0) ∈Ω× R that satisﬁes the constraints with
strict inequalities. The functions (x, s) →gi(x) −s are obviously convex.
We can therefore use the path-following method on the problem (17.5), and
depending on the sign of the problem’s optimal value vmin, we get three cases.
vmin < 0: The system (17.4) has strictly feasible solutions. Indeed, if (x, s)
is a feasible point for the problem (17.5) with s < 0, then gi(x) < 0
for all i. This means that it is not necessary to solve the optimization
problem (17.5) with great accuracy. The algorithm can be stopped as
soon as it has generated a point (x, s) with s < 0.
vmin > 0: The system (17.4) is infeasible. Also in this case, it is not necessary
to solve the problem with great accuracy. We can stop as soon as we
have found a feasible point for the dual problem with a positive value
of the dual function, since this implies that vmin > 0.
vmin = 0: If the greatest lower bound vmin = 0 is attained, i.e. if there is
a point (ˆx, ˆs) with ˆs = 0, then the system (17.4) is feasible but not
strictly feasible. The system (17.4) is infeasible if vmin is not attained.
In practice, it is of course impossible to determine exactly that vmin = 0;
the algorithm terminates with the conclusion that |vmin| < ϵ for some
small positive number ϵ, and we can only be sure that the system
gi(x) < −ϵ is infeasible and that the system gi(x) ≤ϵ is feasible.
Convergence analysis
At the beginning of outer iteration number k, we have t = αk−1t0 . The
stopping criterion will be triggered as soon as m/(αk−1t0) ≤ϵ, i.e. when
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
82
The path-following method
82
k −1 ≥(log(m/(ϵt0))/ log α. The number of outer iterations is thus equal to
log(m/(ϵt0)
log α

+ 1
(for ϵ ≤m/t0).
The path-following method therefore works, provided that the minimiza-
tion problems
(17.6)
min tf(x) + F(x)
s.t.
x ∈int X
can be solved for t ≥t0. Using Newton’s method, this is true, for example,
if the objective functions satisfy the conditions of Theorem 15.2.4, i.e. if Ft
is strongly convex, has a Lipschitz continuous derivative and the sublevel set
corresponding to the starting point is closed.
A question that remains to be resolved is whether the problem (17.6)
gets harder and harder, that is requires more innner iterations, when t grows.
Practical experience shows that this is not so −in most problems, the number
of Newton steps seems to be roughly constant when t grows. For problems
with self-concordant objective and barrier functions, it is possible to obtain
exact estimates of the total number of iterations needed to solve the opti-
mization problem (P) with a given accuracy, and this will be the theme in
Chapter 18.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
83
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Chapter 18
The path-following method
with self-concordant barrier
18.1
Self-concordant barriers
Deﬁnition. Let X be a closed convex subset of Rn with nonempty interior
int X, and let ν be a nonnegative number. A function f : int X →R is called
a self-concordant barrier to X with parameter ν, or shorter a ν-self-concordant
barrier, if the function is closed, self-concordant and non-constant, and the
Newton decrement satisﬁes the inequality
(18.1)
λ(f, x) ≤ν1/2
for all x ∈int X.
It follows from Theorem 15.1.2 and Theorem 15.1.3 that inequality (18.1)
holds if and only if
|⟨f ′(x), v⟩| ≤ν1/2∥v∥x
for all vectors v ∈Rn, or equivalently, if and only if

Df(x)[v]
2 ≤ν D2f(x)[v, v]
for all v ∈Rn.
A closed self-concordant function f : Ω→R with the property that
supx∈Ωλ(f, x) < 1 is necessarily constant and the domain Ωis equal to Rn,
according to Theorem 16.4.7. The parameter ν of a self-concordant barrier
must thus be greater than or equal to 1.
Example 18.1.1. The function f(x) = −ln x is a 1-self-concordant barrier
to the interval [0, ∞[, because f is closed and self-concordant and λ(f, x) = 1
for all x > 0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
84
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Example 18.1.2. Convex quadratic functions
f(x) = 1
2⟨x, Ax⟩+ ⟨b, x⟩+ c
are self-concordant on Rn, but they do not function as self-concordant barri-
ers, because sup λ(f, x) = ∞for all non-constant convex quadratic functions
f, according to Example 15.1.2.
We will show later that only subsets of halfspaces can have self-concordant
barriers, so there is no self-concordant barrier to the whole Rn.
Example 18.1.3. Let g(x) be a non-constant convex, quadratic function.
The function f, deﬁned by
f(x) = −ln(−g(x)),
is a 1-self-concordant barrier to the set X = {x ∈Rn | g(x) ≤0}.
Proof. Let g(x) = 1
2⟨x, Ax⟩+ ⟨b, x⟩+ c, let v be an arbitrary vector in Rn,
and set
α = −1
g(x)Dg(x)[v]
and
β = −1
g(x)D2g(x)[v, v] = −1
g(x)⟨v, Av⟩,
where x is an arbitrary point in the interior of X. Note that β ≥0 and that
D3g(x)[v, v, v] = 0. It therefore follows from the diﬀerentiation rules that
Df(x)[v] = −1
g(x)Dg(x)[v] = α,
D2f(x)[v, v] =
1
g(x)2

Dg(x)[v]
2 −
1
g(x)D2g(x)[v, v] = α2 + β ≥0,
D3f(x)[v, v, v] = −
2
g(x)3

Dg(x)[v]
3 +
3
g(x)2D2g(x)[v, v]Dg(x)[v]
−
1
g(x)D3g(x)[v, v, v] = 2α3 + 3αβ.
The function f is convex since its second derivative is positive semidef-
inite, and it is closed since f(x) →+∞as g(x) →0. By squaring it is
easy to show that the inequality |2α3 + 3αβ| ≤2(α2 + β)3/2 holds for all
α ∈R and all β ∈R+, and obviously α2 ≤α2 + β.
This means that
D3f(x)[v, v, v]
 ≤2

D2f(x)[v, v]
3/2 and that (Df(x)[v])2 ≤D2f(x)[v, v].
So f is 1-self-concordant.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
85
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
85
The following three theorems show how to build new self-concordant bar-
riers from given ones.
Theorem 18.1.1. If f is a ν-self-concordant barrier to the set X and α ≥1,
then αf is an αν-self-concordant barrier to X.
Proof. The proof is left as a simple exercise.
Theorem 18.1.2. If f is a µ-self-concordant barrier to the set X and g is a
ν-self-concordant barrier to the set Y , then the sum f +g is a self-concordant
barrier with parameter µ + ν to the intersection X ∩Y . And f + c is a µ-
self-concordant barrier to X for each constant c.
Proof. The sum f + g is a closed convex function, and it is self-concordant
on the set int(X ∩Y ) according to Theorem 16.1.5. To prove that the sum
is a self-concordant barrier with parameter (µ + ν), we assume that v is an
arbitrary vector in Rn and write a = D2f(x)[v, v] and b = D2g(x)[v, v]. We
then have, by deﬁnition,

Df(x)[v]
2 ≤µa
and

Dg(x)[v]
2 ≤νb,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
86
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
and using the inequality 2√µνab ≤νa + µb between the geometric and the
arithmetic mean, we obtain the inequality

D(f + g)(x)[v]
2 =

Df(x)[v]
2 +

Dg(x)[v]
2 + 2Df(x)[v] · Dg(x)[v]
≤µa + νb + 2

µaνb ≤µa + νb + νa + µb
= (µ + ν)(a + b) = (µ + ν) D2(f + g)(x)[v, v],
which means that λ(f + g, x) ≤(µ + ν)1/2.
The assertion about the sum f + c is trivial, since λ(f, x) = λ(f + c, x)
for constants c.
Theorem 18.1.3. Suppose that A: Rm →Rn is an aﬃne map and that f is
a ν-self-concordant barrier to the subset X of Rn. The composition g = f ◦A
is then a ν-self-concordant barrier to the inverse image A−1(X).
Proof. The proof is left as an exercise.
Example 18.1.4. It follows from Example 18.1.1 and Theorems 18.1.2 and
18.1.3 that the function
f(x) = −
m

i=1
ln(bi −⟨ai, x⟩)
is an m-self-concordant barrier to the polyhedron
X = {x ∈Rn | ⟨ai, x⟩≤bi, i = 1, 2, . . . , m}.
Theorem 18.1.4. If f is a ν-self-concordant barrier to the set X, then
⟨f ′(x), y −x⟩≤ν
for all x ∈int X and all y ∈X.
Remark. It follows that a set with a self-concordant barrier must be a subset
of some halfspace. Indeed, a set X with a ν-self-concordant barrier is a subset
of the closed halfspace {y ∈Rn | ⟨c, y⟩≤ν + ⟨c, x0⟩}, where x0 ∈int X is an
arbitrary point with c = f ′(x0) ̸= 0.
Proof. Fix x ∈int X and y ∈X, let xt = x+t(y −x) and deﬁne the function
φ by setting φ(t) = f(xt). Then φ is certainly deﬁned on the open interval
]α, 1[ for some negative number α, since x is an iterior point. Moreover,
φ′(t) = Df(xt)[y −x],
and especially, φ′(0) = Df(x)[y −x] = ⟨f ′(x), y −x⟩. We will prove that
φ′(0) ≤ν.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
87
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
If φ′(0) ≤0, then we are done, so assume that φ′(0) > 0. By ν-self-
concordance,
φ′′(t) = D2f(xt)[y −x, y −x] ≥ν−1
Df(xt)[y −x]
2 = ν−1φ′(t)2 ≥0.
The derivative φ′ is thus increasing, and this implies that φ′(t) ≥φ′(0) > 0
for t ≥0. Furthermore,
d
dt

−
1
φ′(t)

= φ′′(t)
φ′(t)2 ≥1
ν
for all t in the interval [0, 1[, so by integrating the last mentioned inequality
over the interval [0, β], where β < 1, we obtain the inequality
1
φ′(0) >
1
φ′(0) −
1
φ′(β) =
 β
0
d
dt

−
1
φ′(t)

dt ≥β
ν .
Hence, φ′(0) < ν/β for all β < 1, which implies that φ′(0) ≤ν.
Theorem 18.1.5. Suppose that f is a ν-self-concordant barrier to the set X.
If x ∈int X, y ∈X and ⟨f ′(x), y −x⟩≥0, then
∥y −x∥x ≤ν + 2√ν.
Remark. If x ∈int X is a minimum point, then ⟨f ′(x), y −x⟩= 0 for all
points y ∈X, since f ′(x) = 0. Hence, ∥y −x∥x ≤ν + 2√ν for all y ∈X if x
is a minimum point.
Proof. Let r = ∥y−x∥x. If r ≤√ν, then there is nothing to prove, so assume
that r > √ν, and consider for α = √v/r the point z = x + α(y −x), which
lies in the interior of X since α < 1. By using Theorem 18.1.4 with z instead
of x, the assumption ⟨f ′(x), y −x⟩≥0, Theorem 16.3.2 and the equalities
y −z = (1 −α)(y −x) and z −x = α(y −x), we obtain the following chain
of inequalities and equalities:
ν ≥⟨f ′(z), y −z⟩= (1 −α)⟨f ′(z), y −x⟩≥(1 −α)⟨f ′(z) −f ′(x), y −x⟩
= 1 −α
α
⟨f ′(z) −f ′(x), z −x⟩≥1 −α
α
·
∥z −x∥2
x
1 + ∥z −x∥x
= (1 −α)α∥y −x∥2
x
1 + α∥y −x∥x
= r√ν −ν
1 + √ν .
The inequality between the extreme ends simpliﬁes to r ≤ν + 2√ν, which
is the desired inequality.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
88
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
88
Given a self-concordant funktion f with the corresponding local seminorm
∥·∥x, we set
E(x; r) = {y ∈Rn | ∥y −x∥x ≤r}.
If f is non-degenerate, then ∥·∥x is a norm at each point x ∈int X, and the
set E(x; r) is a closed ellipsoid in Rn with axis directions determined by the
eigenvectors of the second derivative f ′′(x).
For non-degenerate self-concordant barriers we now have the following
corollary to Theorem 18.1.5.
Theorem 18.1.6. Suppose that f is a non-degenerate ν-self-concordant bar-
rier to the closed convex set X. Then f attains a minimum if and only if X
is a bounded set. The minimum point ˆxf ∈int X is unique in that case, and
E(ˆxf; 1) ⊆X ⊆E(ˆxf; ν + 2√ν).
Remark. A closed self-concordant function whose domain does not contain
any line, is automatically non-degenerate, so it is not necessary to state
explicitly that a self-concordant barrier to a compact set should be non-
degenerate.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
89
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Proof. The sublevel sets of a closed convex function are closed, so if X is
a bounded set, then each sublevel set {x ∈int X | f(x) ≤α} is both closed
and bounded, and this implies that f has a minimum, and the minimum
point of a non-degenerate convex function is necessarily unique.
Conversely, assume that f has a minimum point ˆxf. Then by the remark
following Theorem 18.1.5, ∥y −ˆxf∥ˆxf ≤ν + 2√ν for all y ∈X, and this
amounts to the right inclusion in Theorem 18.1.6, which implies, of course,
that X is a bounded set.
The remaining left inclusion follows from Theorem 16.3.2, which implies
that the open ellipsoid {y ∈Rn | ∥y −x∥x < 1} is a subset of int X for each
choice of x ∈int X. The closure E(x; 1) is therefore a subset of X, and we
obtain the left inclusion by choosing x = ˆxf.
Given a self-concordant barrier to a set X we will need to compare the
local seminorms ∥v∥x and ∥v∥y of a vector at diﬀerent points x and y, and in
order to achieve this we need a measure for the distance from y to x relative
the distance from y to the boundary of X along the half-line from x through
x. The following deﬁnition provides us with the relevant measure.
Deﬁnition. Let X be a closed convex subset of Rn with nonempty interior.
For each y ∈int X we deﬁne a function πy : Rn →R+ by setting
πy(x) = inf{t > 0 | y + t−1(x −y) ∈X}.
Obviously, πy(y) = 0. To determine πy(x) if x ̸= y, we consider the half-
line from y through x; if the half-line intersects the boundary of X in a point
z, then πy(x) = ∥x −y∥/∥z −y∥(with respect to arbitrary norms), and if
the entire half-line lies in X, then πy(x) = 0. We note that πy(x) < 1 for
interior points x, that πy(x) = 1 for boundary points x, and that πy(x) > 1
for points outside X.
We could also have deﬁned the function πy in terms of the Minkowski
functional that was introduced in Section 6.10 of Part I, because
πy(x) = φ−y+X(x −y),
where φ−y+X is the Minkowski functional of the set −y + X.
The following simple estimate of πy(x) will be needed later on.
Theorem 18.1.7. Let X be a compact convex set, let x and y be points in
the interior of X, and suppose that
B(x, r) ⊆X ⊆B(0; R),
where the balls are given with respect to an arbitrary norm ∥·∥. Then
πy(x) ≤
2R
2R + r.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
90
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Proof. The inequality is trivially true if x = y, so suppose that x ̸= y. The
half-line from y through x intersects the boundary of X in a point z and
∥z −y∥= ∥z −x∥+ ∥x −y∥. Furthermore, ∥z −x∥≥r and ∥x −y∥≤2R,
so it follows that
πy(x) = ∥x −y∥
∥z −y∥=

1 + ∥z −x∥
∥x −y∥
−1
≤

1 + r
2R
−1
=
2R
2R + r.
The direction derivative ⟨f ′(x), v⟩of a ν-self-concordant barrier function
f is bounded by √ν∥v∥x, by deﬁnition. Our next theorem shows that the
same direction derivative is also bounded by a constant times ∥v∥y, if y is an
arbitrary point in the domain of f. The two local norms ∥v∥x and ∥v∥y are
also compared.
Theorem 18.1.8. Let f be a ν-self-concordant barrier to X, and let x and y
be two points in the interior of X. Then, for all vectors v
|⟨f ′(x), v⟩| ≤
ν
1 −πy(x) ∥v∥y
(18.2)
and
∥v∥x ≤ν + 2√ν
1 −πy(x)∥v∥y.
(18.3)
Proof. The two inequalities hold if y = x since
|⟨f ′(x), v⟩| ≤√ν∥v∥x ≤ν∥v∥x
and πx(x) = 0. They also hold if ∥v∥y = 0, i.e. if the vector v belongs to the
recessive subspace of f, because then ∥v∥x = 0 and ⟨f ′(x), v⟩= 0. Assume
henceforth that y ̸= x and that ∥v∥y ̸= 0.
First consider the case ∥v∥y = 1, and let s be an arbitrary number greater
than ν + 2√ν. Then, by Theorems 16.3.2 and 18.1.5, we conclude that
(i) The two points y ± v lie in X.
(ii) At least one of the two points x ±
s
∥v∥x
v lies outside X.
By the deﬁnition of πy(x) there is a vector z ∈X such that
x = y + πy(x)(z −y),
and since
x ± (1 −πy(x))v = πy(x)z + (1 −πy(x))(y ± v),
it follows from convexity that
(iii) The two points x ± (1 −πy(x))v lie in X.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
91
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
91
It now follows from (iii) and Theorem 18.1.4 that
⟨f ′(x), ±v⟩=
1
1 −πy(x)⟨f ′(x), x ± (1 −πy(x))v −x⟩≤
ν
1 −πy(x),
which means that
|⟨f ′(x), v⟩| ≤
ν
1 −πy(x).
This proves inequality (18.2) for vectors v with ∥v∥y = 1, and if v is an
arbitrary vector with ∥v∥y ̸= 0, we obtain inequality (18.2) by replacing v in
the inequality above with v/∥v∥y.
By combining the two assertions (ii) and (iii) we conclude that
1 −πy(x) <
s
∥v∥x
,
i.e. that
∥v∥x <
s
1 −πy(x) =
s
1 −πy(x)∥v∥y,
and since this holds for all s > ν + 2√ν, it follows that
∥v∥x ≤ν + 2√ν
1 −πy(x)∥v∥y.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
92
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
This proves inequality (18.3) in the case ∥v∥y = 1, and since the inequality
is homogeneous, it holds in general.
Deﬁnition. Let ∥·∥x be the local seminorm at x which is associated with the
two times diﬀerentiable convex function f : X →R, where X is a subset
of Rn. The corresponding dual local norm is the function ∥·∥∗
x : Rn →R,
which is deﬁned by
∥v∥∗
x =
sup
∥w∥x≤1
⟨v, w⟩
for all v ∈Rn.
The dual norm is easily veriﬁed to be subadditive and homogeneous, i.e.
∥v + w∥∗
x ≤∥v∥∗
x + ∥w∥∗
x and ∥λv∥∗
x = |λ|∥v∥∗
x for all v, w ∈Rn and all real
numbers λ, but ∥·∥∗
x is a proper norm on the whole of Rn only for points x
where the second derivative f ′′(x) is positive deﬁnite, because ∥v∥∗
x = ∞if v
is a nonzero vector in the null space N(f ′′(x)) since ∥tv∥x = 0 for all t ∈R
and ⟨v, tv⟩= t∥v∥2 →∞as t →∞. However, ∥·∥∗
x is always a proper norm
when restricted to the subspace N(f ′′(x))⊥. See exercise 18.2.
By Theorem 15.1.3, we have the following expression for the Newton
decrement λ(f, x) in terms of the dual local norm:
λ(f, x) = ∥f ′(x)∥∗
x.
The following variant of the Cauchy–Schwarz inequality holds f¨or the local
seminorm.
Theorem 18.1.9. Assume that ∥v∥∗
x < ∞. Then
|⟨v, w⟩| ≤∥v∥∗
x∥w∥x
for all vectors w.
Proof. If ∥w∥x ̸= 0 then ±w/∥w∥x are two vectors with local seminorm equal
to 1, so it follows from the deﬁnition of the dual norm that
±
1
∥w∥x
⟨v, w⟩= ⟨v, ±w/∥w∥x⟩≤∥v∥∗
x,
and we obtain the sought inequality after multiplication by ∥w∥x.
If instead ∥w∥x = 0, then ∥tw∥x = 0 for all real numbers t, and it follows
from the supremum deﬁnition that t⟨v, w⟩= ⟨v, tw⟩≤∥v∥∗
x < ∞for all t.
This being possible only if ⟨v, w⟩= 0, we conclude that the inequality applies
in this case, too.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
93
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Later we will need various estimates of ∥v∥∗
x. Our ﬁrst estimate is in
terms of the width in diﬀerent directions of the set X, and this motivates
our next deﬁnition.
Deﬁnition. Given a nonempty subset X of Rn, let VarX : Rn →R be the
function deﬁned by
VarX(v) = sup
x∈X
⟨v, x⟩−inf
x∈X⟨v, x⟩.
VarX(v) is obviously a ﬁnite number for each v ∈Rn if the set X is
bounded, and if v is a unit vector, then VarX(v) measures the width of the
set X in the direction of v.
Our next theorem shows how to estimate ∥·∥∗
x using VarX.
Theorem 18.1.10. Suppose that f : X →R is a closed self-concordant func-
tion with a bounded open convex subset X of Rn as domain, and let ∥·∥∗
x be
the dual local norm associated with the function f at the point x ∈X. Then
∥v∥∗
x ≤VarX(v)
for all v ∈Rn.
Proof. It follows from the previous theorem that y is a point in cl X if x is a
point in X and ∥y −x∥x ≤1. Hence,
∥v∥∗
x =
sup
∥w∥x≤1
⟨v, w⟩=
sup
∥y−x∥x≤1
⟨v, y −x⟩≤sup
y∈cl X
⟨v, y −x⟩= sup
y∈X
⟨v, y −x⟩
= sup
y∈X
⟨v, y⟩−⟨v, x⟩≤sup
y∈X
⟨v, y⟩−inf
y∈X⟨v, y⟩= VarX(v).
We have previously deﬁned the analytic center of a closed convex set X
with respect to a given barrier as the unique minimum point of the barrrier,
provided that there is one. According to Theorem 18.1.6, every compact
convex set with nonempty interior has an analytic center with respect to any
given ν-self-concordant barrier. We can now obtain an upper bound on the
dual local norm ∥v∥∗
x at an arbitrary point x in terms of the parameter ν and
the value of the dual norm at the analytic center.
Theorem 18.1.11. Let X be a compact convex set, and let ˆxf be the analytic
center of the set with respect to a ν-self-concordant barrier f. Then, for each
vector v ∈Rn and each x ∈int X,
∥v∥∗
x ≤(ν + 2√ν)∥v∥∗
ˆxf.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
94
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Proof. Let B1 = E(x; 1) and B2 = E(ˆxf; ν + 2√ν). Theorems 16.3.2 and
18.1.6 give us the inclusions B1 ⊆X ⊆B2, so it follows from the deﬁnition
of the dual local norm that
∥v∥∗
x =
sup
∥w∥x≤1
⟨v, w⟩= sup
y∈B1
⟨v, y −x⟩≤sup
y∈B2
⟨v, y −x⟩
= ⟨v, ˆxf −x⟩+ sup
y∈B2
⟨v, y −ˆxf⟩= ⟨v, ˆxf −x⟩+
sup
∥w∥ˆxf ≤ν+2√ν
⟨v, w⟩
= ⟨v, ˆxf −x⟩+ (ν + 2√ν)∥v∥∗
ˆxf.
Since ∥−v∥∗
x = ∥v∥∗
x, we may now without loss of generality assume that
⟨v, ˆxf −x⟩≤0, and this gives us the required inequality.
18.2
The path-following method
Standard form
Let us say that a convex optimization problem is in standard form if it is
presented in the form
min ⟨c, x⟩
s.t.
x ∈X
where X is a compact convex set with nonempty interior and X is equipped
with a ν-self-concordant barrier function F.
Remark. One can show that every compact convex set X has a barrier func-
tion, but for a barrier function to be useful in a practical optimization prob-
lem, it has to be explicitly given so that it is possible to eﬃciently calculate
its partial ﬁrst and second derivatives.
The assumption that the set X is bounded is not particularly restric-
tive for problems with ﬁnite optimal values, for we can always modify such
problems by adding artiﬁcial, very big bounds on the variables.
We also recall that an arbitrary convex problem can be transformed into
an equivalent convex problem with a linear objective function by an epigraph
formulation. (See Chapter 9.3 of Part II.)
Example 18.2.1. Each LP problem with ﬁnite optimal value can be written
in standard form after suitable transformations. By ﬁrst identifying the aﬃne
hull of the polyhedron of feasible points with Rn for an appropriate n, we
can without restriction assume that the polyhedron has a nonempty interior,
and by adding big bounds on the variables, if necessary, we can also assume
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
95
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
95
that our polyhedron X of feasible points is compact. And with X written in
the form
(18.4)
X = {x ∈Rn | ⟨ci, x⟩≤bi, i = 1, 2, . . . , m},
we get an m-self-concordant barrier F to X, by deﬁning
F(x) = −
m

i=1
ln(bi −⟨ci, x⟩)
Example 18.2.2. Convex quadratic optimization problems, i.e. problems of
the type
min g(x)
s.t.
x ∈X
where g is a convex quadratic function and X is a bounded polyhedron in Rn
with nonempty interior, can be transformed, using an epigraph formulation
and an artiﬁcial bound M on the new variable s, to problems of the form
min s
s.t.
(x, s) ∈Y
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
96
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
where Y = {(x, s) ∈Rn × R | x ∈X, g(x) ≤s ≤M} is a compact convex
set with nonempty interior. Now assume that the polyhedron X is given by
equation (18.4) as an intersection of closed halfspaces. Then the function
F(x, s) = −
m

i=1
ln(bi −⟨ci, x⟩) −ln(s −g(x)) −ln(M −s)
is an (m + 2)-self-concordant barrier to Y according to Example 18.1.3.
Central path
We will now study the path-following method for the standard problem
(SP)
min ⟨c, x⟩
s.t.
x ∈X
where X is a compact convex subset of Rn with nonempty interior, and F
is a ν-self-concordant barrier to X. The ﬁnite optimal value of the problem
is denoted by vmin.
For t ≥0 we deﬁne functions Ft : int X →R by
Ft(x) = t⟨c, x⟩+ F(x).
The functions Ft are closed and self-concordant, and since the set X is com-
pact, each function Ft has a unique minimum point ˆx(t). The central path
{ˆx(t) | t ≥0} is in other words well-deﬁned, and its points satisfy the equa-
tion
(18.5)
tc + F ′(ˆx(t)) = 0,
and the starting point ˆx(0) is by deﬁnition the analytic center ˆxF of X with
respect to the given barrier F.
We will use Newton’s method to determine the minimum point ˆx(t),
and for that reason we need to calculate the Newton step and the Newton
decrement with respect to the function Ft at points in the interior of X.
Since F ′′
t (x) = F ′′(x), the local norm ∥v∥x of a vector v with respect to
the function Ft is the same for all t ≥0, namely
∥v∥x =

⟨v, F ′′(x)v⟩.
In contrast, Newton steps and Newton decrements depend on t; the Newton
step at the point x is equal to −F ′′(x)−1F ′
t(x) for the function Ft, and the
decrement is given by
λ(Ft, x) =

⟨F ′
t(x), F ′′(x)−1F ′
t(x)⟩= ∥F ′′(x)−1F ′
t(x)∥x.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
97
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
The following theorem is used to formulate the stopping criterion in the
path-following method.
Theorem 18.2.1. (i) The points ˆx(t) on the central path of the optimization
problem (SP) satisfy the inequality
⟨c, ˆx(t)⟩−vmin ≤ν
t .
(ii) Moreover, the inequality
⟨c, x⟩−vmin ≤ν + κ(1 −κ)−1√ν
t
.
holds for t > 0 and all point x ∈int X satisfying the condition
λ(Ft, x) ≤κ < 1.
Proof. (i) Because of equation (18.5), c = −t−1F ′(ˆx(t)), and it therefore
follows from Theorem 18.1.4 that
⟨c, ˆx(t)⟩−⟨c, y⟩= 1
t ⟨F ′(ˆx(t)), y −ˆx(t)⟩≤ν
t
for all y ∈X. We obtain inequality (i) by choosing y as an optimal solution
to the problem (SP).
(ii) Since ⟨c, x⟩−vmin = (⟨c, x⟩−⟨c, ˆx(t)⟩) + (⟨c, ˆx(t)⟩−vmin), it suﬃces, due
to the already proven inequality, to show that
(18.6)
⟨c, x⟩−⟨c, ˆx(t)⟩≤
κ
1 −κ ·
√ν
t
if x ∈int X and λ(Ft, x) ≤κ < 1. But it follows from Theorem 16.4.6 that
∥x −ˆx(t)∥ˆx(t) ≤
λ(Ft, x)
1 −λ(Ft, x) ≤
κ
1 −κ,
so by using that tc = −F ′(ˆx(t)) and that F is ν-self-concordant, we get the
following chain of equalities and inequalities:
t(⟨c, x⟩−⟨c, ˆx(t)⟩) = −⟨F ′(ˆx(t)), x −ˆx(t)⟩≤∥F ′(ˆx(t))∥∗
ˆx(t)∥x −ˆx(t)∥ˆx(t)
= λ(F, ˆx(t))∥x −ˆx(t)∥ˆx(t) ≤√ν
κ
1 −κ,
which proves inequality (18.6).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
98
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
98
Algorithm
The path-following algorithm for solving the standard problem
(SP)
min ⟨c, x⟩
s.t.
x ∈X
works in brief as follows.
We start with a parameter value t0 > 0 and a point x0 ∈int X, which
is close enough to the point ˆx(t0) on the central path. ”Close enough” is
expressed in terms of the Newton decrement λ(Ft0, x0), which must be suﬃ-
ciently small.
Then we update the parameter t by deﬁning t1 = αt0 for a suitable α > 1
and minimize the function Ft1 using the damped Newton method with x0 as
the starting point. Newton’s method is terminated when it has reached a
point x1, which is suﬃciently close to the minimum point ˆx(t1) of Ft1.
The procedure is then repeated with t2 = αt1 as new parameter and with
x1 as starting point in Newton’s method for minimization of the function Ft2,
etc. As a result we obtain a sequence t0, x0, t1, x1, t2, x2, . . . of parameter
values and points, and the procedure is terminated when tk has become
suﬃciently large with xk as an approximate optimal point.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
99
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
From this sketchy description of the algorithm it is clear that we need two
parameters, one parameter α to describe the update of t, and one parameter
κ to deﬁne the stopping criterion in Newton’s method. We shall estimate the
total number of inner iterations, and the estimate will be the simplest and
most obvious if one writes the update parameter α in the form α = 1+γ/√ν.
The following precise formulation of the path-following algorithm there-
fore contains the parameters γ and κ. The addition ’phase 2’ is due to the
need for an additional phase to generate feasible initial values x0 and t0.
Path-following algorithm, phase 2
Given an update parameter γ > 0, a neighborhood parameter 0 < κ < 1, a
tolerance ϵ > 0, a starting point x0 ∈int X, and a starting value t0 > 0
such that λ(Ft0, x0) ≤κ.
1. Initiate: x:= x0 and t:= t0.
2. Stopping criterion: stop if ϵt ≥ν + κ(1 −κ)−1√ν.
3. Increase t: t:= (1 + γ/√ν)t.
4. Update x by using Newton’s damped method on the function Ft with the
current x as starting point:
(i) Compute the Newton decrement λ = λ(Ft, x).
(ii) quit Newton’s method if λ ≤κ, and go to line 2.
(iii) Compute the Newtonstep ∆xnt = −F ′′(x)−1F ′
t(x).
(iv) Uppdate: x:= x + (1 + λ)−1∆xnt
(v) Go to (i).
We can now show the following convergence result.
Theorem 18.2.2. Suppose that the above path-following algorithm is applied
to the standard problem (SP) with a ν-self-concordant barrier F. Then the
algorithm stops with a point x ∈int X which satisﬁes
⟨c, x⟩−vmin ≤ϵ.
For each outer iteration, the number of inner iterations in Newton’s al-
gorithm is bounded by a constant K, and the total number of inner iterations
in the path-following algorithm is bounded by
C√ν ln
 ν
t0ϵ + 1

,
where the constants K and C only depend on κ and γ.
Proof. Let us start by examining the inner loop 4 of the algorithm.
Each time the algorithm passes by line 2, it does so with a point x in
int X, which belongs to a t-value with Newton decrement λ(Ft, x) ≤κ.
In step 4, the function Fs, where s = (1 + γ/√ν)t, is then minimized
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
100
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
using Newton’s damped method with y0 = x as the starting point.
The
points yk, k = 1, 2, 3, . . . , generated by the method lie in int X accord-
ing to Theorem 16.3.2, and the stopping condition λ(Fs, yk) ≤κ implies,
according to Theorem 16.5.1, that the algorithm terminates after at most

Fs(x) −Fs(ˆx(s))

/ρ(−κ)

iterations, where ρ is the function
ρ(u) = −u −ln(1 −u).
We shall show that there is a constant K, which only depends on the param-
eters κ and γ, so that
Fs(x) −Fs(ˆx(s))
ρ(−κ)

≤K,
and for that reason we need to estimate the diﬀerence Fs(x)−Fs(ˆx(s)), which
we do in the next lemma.
Lemma 18.2.3. Suppose that λ(Ft, x) ≤κ < 1. Then, for all s > 0
Fs(x) −Fs(ˆx(s)) ≤ρ(κ) + κ√ν
1 −κ ·
s
t −1
 + ν ρ(1 −s/t).
Proof of the lemma. We start by writing
(18.7)
Fs(x) −Fs(ˆx(s)) =

Fs(x) −Fs(ˆx(t))

+

Fs(ˆx(t)) −Fs(ˆx(s))

.
By using the equality tc = −F ′(ˆx(t)) and the inequality
|⟨F ′(ˆx(t)), v⟩| ≤λ(F, ˆx(t))∥v∥ˆx(t) ≤√ν∥v∥ˆx(t),
we obtain the following estimate of the ﬁrst diﬀerence in the right-hand side
of (18.7):
Fs(x) −Fs(ˆx(t)) = Ft(x) −Ft(ˆx(t)) + (s −t)⟨c, x −ˆx(t)⟩
(18.8)
= Ft(x) −Ft(ˆx(t)) −(s/t −1)⟨F ′(ˆx(t)), x −ˆx(t)⟩
≤Ft(x) −Ft(ˆx(t)) + |s/t −1| √ν ∥x −ˆx(t)∥ˆx(t).
By Theorem 16.4.6,
Ft(x) −Ft(ˆx(t)) ≤ρ(λ(Ft, x)) ≤ρ(κ)
and
∥x −ˆx(t)∥ˆx(t) ≤
λ(Ft, x)
1 −λ(Ft, x) ≤
κ
1 −κ.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
101
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
101
Therefore, it follows from inequality (18.8) that
(18.9)
Fs(x) −Fs(ˆx(t)) ≤ρ(κ) +
s
t −1
 · κ√ν
1 −κ.
It remains to estimate the second diﬀerence
φ(s) = Fs(ˆx(t)) −Fs(ˆx(s))
(18.10)
= s⟨c, ˆx(t)⟩−s⟨c, ˆx(s)⟩+ F(ˆx(t)) −F(ˆx(s))
in the right-hand side of (18.7).
The function ˆx(s) is continuously diﬀerentiable. This follows from the
implicit function theorem, because ˆx(s) satisﬁes the equation
sc + F ′(ˆx(s)) = 0,
and the second derivative F ′′(x) is continuous and non-singular everywhere.
By implicit diﬀerentiation,
c + F ′′(ˆx(s))ˆx′(s) = 0,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
102
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
which means that
ˆx′(s) = −F ′′(ˆx(s))−1c.
It now follows from equation (18.10) that the diﬀerence φ(s) is continuously
diﬀerentiable with derivative
φ′(s) = ⟨c, ˆx(t)⟩−⟨c, ˆx(s)⟩−s⟨c, ˆx′(s)⟩−⟨F ′(ˆx(s), ˆx′(s)⟩
= ⟨c, ˆx(t) −ˆx(s)⟩−s⟨c, ˆx′(s)⟩+ s⟨c, ˆx′(s)⟩
= ⟨c, ˆx(t) −ˆx(s)⟩,
and a further diﬀerentiation gives
φ′′(s) = −⟨c, ˆx′(s)⟩= ⟨c, F ′′(ˆx(s))−1c⟩
= ⟨s−1F ′(ˆx(s)), s−1F ′′(ˆx(s))−1F ′(ˆx(s))⟩
= s−2⟨F ′(ˆx(s)), F ′′(ˆx(s))−1F ′(ˆx(s))⟩= s−2λ(F, ˆx(s))2 ≤νs−2.
Now note that φ(t) = φ′(t) = 0. By integrating the inequality for φ′′(s)
over the interval [t, u], we therefore obtain the following estimate for u ≥t:
φ′(u) = φ′(u) −φ′(t) ≤
 u
t
νs−2 ds = ν(t−1 −u−1).
Integrating once more over the interval [t, s] results in the inequality
Fs(ˆx(t)) −Fs(ˆx(s)) = φ(s) =
 s
t
φ′(u) du ≤ν
 s
t
(t−1 −u−1) du
(18.11)
= ν
s
t −1 −ln s
t

= ν ρ(1 −s/t)
for s ≥t. The same conclusion is also reached for s < t by ﬁrst integrating the
inequality for φ′′(s) over the interval [u, t], and then the resulting inequality
for φ′(u) over the interval [s, t].
The inequality in the lemma is now ﬁnally a consequence of equation
(18.7) and the estimates (18.9) and (18.11).
Continuation of the proof of Theorem 18.2.2. By using the lemma’s estimate
of the diﬀerence Fs(x) −Fs(ˆx(s)) when s = (1 + γ/√ν)t, we obtain the
inequality
Fs(x) −Fs(ˆx(s))
ρ(−κ)

≤
ρ(κ) + γκ(1 −κ)−1 + ν ρ(−γν−1/2)
ρ(−κ)

,
and ν ρ(−γν−1/2) ≤1
2γ2, because ρ(u) = −u −ln(1 −u) ≤1
2u2 for u < 0.
The number of inner iterations in each outer iteration is therefore bounded
by the constant
K =
ρ(κ) + γκ(1 −κ)−1 + 1
2γ2
ρ(−κ)

,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
103
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
which only depends on the parameters κ and γ. For example, K = 5 if
κ = 0.4 and γ = 0.32.
We now turn to the number of outer iterations. Set
β(κ) = ν + κ(1 −κ)−1√ν.
Suppose that the stopping condition ϵt ≥β(κ) is triggered during iteration
number k when t = (1 + γ/√ν)kt0. Because of Theorem 18.2.1, the current
point x then satisﬁes the condition
⟨c, x⟩−vmin ≤ϵ,
which shows that x approximates the minimum point with prescribed accu-
racy.
Since k is the least integer satisfying the inequality (1 + γ/√ν)k ≥
β(κ)/t0ϵ, we have
k =
 ln(β(κ)/t0ϵ)
ln(1 + γ/√ν)

.
To simplify the denominator, we use the fact that ln(1 + γx) is a concave
function. This implies that ln(1 + γx) ≥x ln(1 + γ) if 0 ≤x ≤1, and hence
ln(1 + γ/√ν) ≥ln(1 + γ)
√ν
.
Furthermore, β(κ) = ν +κ(1−κ)−1√ν ≤ν +κ(1−κ)−1ν = (1−κ)−1ν. This
gives us the estimate
k ≤
√ν ln((1 −κ)−1ν/t0ϵ)
ln(1 + γ)

≤K′√ν ln
 ν
t0ϵ + 1

for the number of outer iterations with a constant K′ that only depends
on κ and γ, and by multiplying this with the constant K we obtain the
corresponding estimate for the total number of inner iterations.
Phase 1
In order to use the path-following algorithm, we need a t0 > 0 and a point
x0 ∈int X with Newton decrement λ(Ft0, x0) ≤κ to start from. Since the
central path begins in the analytic center ˆxF of X and λ(F, ˆxF) = 0, it can
be expected that (x0, t0) is good enough as a starting pair if only x0 is close
enough to ˆxF and t0 > 0 is suﬃciently small. Indeed, this is true, and we
shall show that one can generate such a pair by solving an artiﬁcial problem,
given that one knows a point x ∈int X.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
104
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
104
Therefore, let Gt : int X →R, where 0 ≤t ≤1, be the functions deﬁned
by
Gt(x) = −t⟨F ′(x), x⟩+ F(x).
The functions Gt are closed and self-concordant, and they have unique min-
imum points x(t).
Note that G0 = F, and hence x(0) = ˆxF. Since G′
t(x) = −tF ′(x)+F ′(x),
G′
1(x) = 0, and this means that x is the minimum point of the function G1.
Hence, x(1) = x. The curve {x(t) | 0 ≤t ≤1} thus starts in the analytic
center ˆxF and ends in the given point x. By using the path-following method,
now following the curve backwards, we will therefore obtain a suitable starting
point for phase 2 of the algorithm.
We use Newton’s damped method to minimize Gt and note that G′′
t = F ′′
for all t, so the local norm with respect to the function Gt coincides with the
local norm with respect to the function F, and we can thus unambiguously
use the symbol ∥·∥x for the local norm at the point x.
The algorithm for determining a starting pair (x0, t0) now looks like this.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
105
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Path-following algorithm, phase 1
Given x ∈int X, and parameters 0 < γ < 1
2
√ν and 0 < κ < 1.
1. Initiate: x:= x and t:= 1.
2. Stopping criterion: stop if λ(F, x) < 3
4κ and set x0 = x.
3. Decrease t: t:= (1 −γ/√ν)t.
4. Update x by using Newton’s damped method on the function Gt with the
current x as starting point:
(i) Compute λ = λ(Gt, x).
(ii) quit Newton’s method if λ ≤κ/2, and go to line 2.
(iii) Compute the Newton step ∆xnt = −F ′′(x)−1G′
t(x).
(iv) Update: x:= x + (1 + λ)−1∆xnt.
(v) Go to (i).
When the algorithm has stopped with a point x0, we deﬁne t0 by setting
t0 = max{t | λ(Ft, x0) ≤κ}.
The number of iterations in phase 1 is given by the following theorem.
Theorem 18.2.4. Phase 1 of the path-following algorithm stops with a point
x0 ∈int X after at most
C√ν ln

ν
1 −πˆxF (x) + 1

inner iterations, where the constant C only depends on κ and γ, the number
t0 satisﬁes the conditions λ(Ft0, x0) ≤κ and t0 ≥κ/4 VarX(c).
Proof. We start by estimating the number of inner iterations in each outer
iteration; this number is bounded by the quotient
Gs(x) −Gs(x(s))
ρ(−κ/2)
,
where s = (1 −γ/√ν)t, and Lemma 18.2.3 gives us the majorant
ρ(κ/2) + κ√ν
2 −κ · γ
√ν + ν ρ(γ/√ν)
for the numerator of the quotient. By Lemma 16.3.1, νρ(γ/√ν) ≤γ2, so the
number of inner iterations in each outer iteration is bounded by the constant
ρ(κ/2) + κ(2 −κ)−1γ + γ2
ρ(−κ/2)
.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
106
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
We now consider the outer iterations. Since F ′ = G′
t + tF ′(x),
λ(F, x) = ∥F ′(x)∥∗
x = ∥G′
t(x) + tF ′(x)∥∗
x ≤∥G′
t(x)∥∗
x + t∥F ′(x)∥∗
x
(18.12)
= λ(Gt, x) + t∥F ′(x)∥∗
x.
It follows from Theorem 18.1.11 that
∥F ′(x)∥∗
x ≤(ν + 2√ν)∥F ′(x)∥∗
ˆxF ≤3ν∥F ′(x)∥∗
ˆxF ,
and from Theorem 18.1.8 that
∥F ′(x)∥∗
ˆxF =
sup
∥v∥ˆxF ≤1
⟨F ′(x), v⟩≤
ν
1 −πˆxF (x).
Hence
(18.13)
∥F ′(x)∥∗
x ≤
3ν2
1 −πˆxF (x).
During outer interation number k, we have t = (1 −γ/√ν)k and the point
x satisﬁes the condition λ(Gt, x) ≤κ/2 when Newton’s method stops. So
it follows from inequality (18.12) and the estimate (18.13) that the stopping
condition λ(F, x) < 3
4κ in line 2 of the algorithm is fulﬁlled if
1
2κ +
3ν2
1 −πˆxF (x)(1 −γ/√ν)k ≤3
4κ,
i.e. if
k ln(1 −γ/√ν) < −ln
 12κ−1ν2
1 −πˆxF (x

.
By using the inequality ln(1 −x) ≤−x, which holds for 0 < x < 1, we see
that the stopping condition is fulﬁlled for
k >
√ν
γ ln
 12κ−1ν2
1 −πˆxF (x

.
So the number of outer iterations is less than
K√ν ln

ν
1 −πˆxF (x) + 1

,
where the constant K only depends on κ and γ, and this proves the estimate
of the theorem, since the number of inner iterations in each outer iteration
is bounded by a constant, which only depends on κ and γ.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
107
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
107
The deﬁnition of t0 implies that κ = λ(Ft0, x0), so we get the following
inequalities with the aid of Theorem 18.1.10:
κ = λ(Ft, x0) = ∥F ′
t(x0)∥∗
x0 = ∥t0c + F ′(x0)∥∗
x0 ≤t0∥c∥∗
x0 + ∥F ′(x0)∥∗
x0
= t0∥c∥∗
x0 + λ(F, x0) ≤t0 VarX(c) + 3
4κ.
It follows that
t0 ≥
κ
4 VarX c.
The following complexity result is now obtained by combining the two
phases of the path-following algorithm.
Theorem 18.2.5. A standard problem (SP) with ν-self-concordant barrier,
tolerance level ϵ > 0 and starting point x ∈int X can be solved with at most
C√ν ln(νΦ/ϵ + 1)
Newton steps, where
Φ =
VarX(c)
1 −πˆxF (x)
and the constant C only depends on γ and κ.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
108
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Proof. Phase 1 provides a starting point x0 and an initial value t0 for phase 2,
satisfying the condition t0 ≥κ/(4 VarX(c)). The number of inner iterations
in phase 2 is therefore bounded by
O(1)√ν ln
4ν VarX(c)
κϵ
+ 1

= O(1)√ν ln
ν VarX(c)
ϵ
+ 1

.
So the total number of inner iterations in the two phases is
O(1)√ν ln

ν
1 −πˆxF (x) + 1

+O(1)√ν ln
ν VarX(c)
ϵ
+ 1

= O(1)√ν ln(νΦ/ϵ + 1).
Remark. The algorithms in this section provide nice theoretical complexity
results, but they are not suitable for practical use. The main limitation is
the low updating factor (1 + O(1)ν−1/2) of the penalty parameter t, which
implies that the total number of Newton steps will be proportional to √ν.
For an LP problem with n = 1000 variables and m = 10000 inequalities, one
would need to solve hundreds of linear equations with 1000 variables, which
requires far more time than what is needed by the simplex algorithm. In
the majority of outer iterations, one can, however, in practice increase the
penalty parameter much faster than what is needed for the theoretical worst
case analysis, without necessarily having to increase the number of Newton
steps to maintain proximity to the central path. There are good practical
implementations of the algorithm that use various dynamic strategies to con-
trol the penalty parameter t, and as a result only a moderate total number
of Newton steps is needed, regardless of the size of the problem.
18.3
LP problems
We now apply the algorithm in the previous section on LP problems. Con-
sider a problem of the type
(18.14)
min ⟨c, x⟩
s.t.
Ax ≤b
where A = [aij] is an m × n-matrix. We assume that the polyhedron
X = {x ∈Rn | Ax ≤b}
of feasible points is bounded and has a nonempty interior. The boundedness
assumption implies that m > n.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
109
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
The ith row of the matrix A is denoted by ai, that is ai = [ai1 ai2 . . . ain].
The matrix product aix is thus well-deﬁned.
As a barrier to the set X we use the m-self-concordant function
F(x) = −
m

i=1
ln(bi −aix).
The path-following algorithm started from an arbitrary point x ∈int X
results in an ϵ-solution, i.e. a point with a value of the objective function
that approximates the optimal value with an error less than ϵ, after at most
O(1)√m ln(mΦ/ϵ + 1)
inner iterations, where
Φ =
VarX(c)
1 −πˆxF (x).
We now estimate the number of arithmetic operations (additions, sub-
tractions, multiplications and divisions) that are required during phase 2 of
the algorithm to obtain this ϵ-solution.
For each inner iteration of the Newton algorithm, we ﬁrst have to compute
the gradient and the hessian of the barrier function at the current point x,
i.e.
F ′(x) =
m

i=1
a T
i
bi −aix
och
F ′′(x) =
m

i=1
a T
i ai
(bi −aix)2.
This can be done with O(mn2) arithmetic operations. The Newton direction
∆xnt at x is obtained as solution to the quadratic system
F ′′(x)∆xnt = −(tc + F ′(x))
of linear equations, and using Gaussian elimination, we ﬁnd the solution after
O(n3) arithmetic operations. Finally, O(n) additional arithmetic operations,
including one square root extraction, are needed to compute the Newton
decrement λ = λ(Ft, x) and the new point x+ = x + (1 + λ)−1∆xnt.
The corresponding estimate of the number of operations is also true for
phase 1 of the algorithm.
The gradient and hessian computation is the most costly of the above
computations since m > n. The total number of arithmetic operations in
each iteration is therefore O(mn2), and by multiplying with the number of
inner iterations, the overall arithmetic cost of the path-following algorithm
is estimated to be no more than O(m3/2n2) ln(mΦ/ϵ + 1) operations.
The resulting approximate minimum point ˆx(ϵ) is an interior point of the
polyhedron X, but the minimum is of course attained at an extreme point
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
110
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
110
on the the boundary of X.
However, there is a simple procedure, called
puriﬁcation and described below, which starting from ˆx(ϵ) ﬁnds an extreme
point ˆx of X after no more than O(mn2) arithmetic operations and with an
objective function value that does not exceed the value at ˆx(ϵ). This means
that we have the following result.
Theorem 18.3.1. For the LP problem (18.14) at most
O(m3/2n2) ln(mΦ/ϵ + 1)
arithmetic operations are needed to ﬁnd an extreme point ˆx of the polyhedron
of feasible points that approximates the minimum value with an error less
than ϵ.
Puriﬁcation
The proof of the following theorem describes an algorithm for how to generate
an extreme point with a value of the objective function that does not exceed
the value at a given interior point of the polyhedron of feasible points.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
111
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Theorem 18.3.2. Let
min ⟨c, x⟩
s.t.
Ax ≤b
be an LP problem with n variables and m constraints, and suppose that the
polyhedron X of feasible points is line-free and that the objective function
is bounded below on X. For each point of X we can generate an extreme
point of X with a value of the objective function that does not exceed the
value at the given point with an algorithm using at most O(mn2) arithmetic
operations.
Proof. The idea is very simple: Follow a half-line from the given point x(0)
with non-increasing function values until hitting upon a point x(1) in a face
F1 of the polyhedron X. Then follow a half-line in the face F1 with non-
increasing function values until hitting upon a point x(2) in the intersection
F1 ∩F2 of two faces, etc. After n steps, one has reached a point x(n) in the
intersection of n (independent) faces, i.e. an extreme point, with a function
value that is less than or equal to the value at the starting point.
To estimate the number of arithmetic operation we have to study the
above procedure in a little more detail.
We start by deﬁning v(1) = e1 if c1 < 0, v(1) = −e1 if c1 > 0, and
v(1) = ±e1 if c1 = 0, where the sign in the latter case should be chosen so
that the half-line x(0)+tv(1), t ≥0, intersects the boundary of the polyhedron;
this is possible since the polyhedron is assumed to be line-free. In the ﬁrst two
cases, the half-line also intersects the boundary of the polyhedron, because
⟨c, x(0) + tv(1)⟩= ⟨c, x(0)⟩−t|c1| tends to −∞as t tends to ∞and the
objective function is assumed to be bounded below on X. The intersection
point x(1) = x(0) + t1v(1) between the half-line and the boundary of X can be
computed with O(mn) arithmetic operations, since we only have to compute
the vectors b −Ax(0) and Av(1), and quotients between their coordinates in
order to ﬁnd the nonnegative parameter value t1.
After renumbering the equations, we may assume that the point x(1) lies
in the hyperplane a11x1 + a12x2 + · · · + a1nxn = b1. We now eliminate the
variable x1 from the constraints and the objective function, which results in
a system of the form
(18.15)









x1 + a′
12x2 + · · · + a′
1nxn = b′
1
A′


x2
...
xn

≤b′
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
112
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
where A′ is an (m −1) × (n −1)-matrix, and in a new objective function
c′
2x2 + · · · + c′
nxn + d′,
which is the restriction of the original objective function to the current face.
The number of operations required to perform the eliminations is O(mn).
After O(mn) operations we have thus managed to ﬁnd a point x(1) in
a face F1 of X with an objectiv function value ⟨c, x(1)⟩= ⟨c, x(0)⟩−t1|c1|
not exceeding ⟨c, x(0)⟩, and determined the equation of the face and the
restriction of the objective function to the face. We now have a problem of
lower dimension n −1 and with m −1 constraints.
We continue by choosing a descent vector v(2) for the objective function
that is parallel to the face F1, and we achieve this by deﬁning v(2) so that
v(2)
2
= ±1, v(2)
3
= · · · = v(2)
n
= 0 (and v(2)
1
= −a′
12v(2)
2 ), where the sign of v(2)
2
should be chosen so that the objective function is non-decreasing along the
half-line x(1) +tv(2), t ≥0, and the half-line instersects the relative boundary
of F1. This means that v(2)
2
= 1 if c′
2 < 0 and v(2)
2
= −1 if c′
2 > 0, while
the sign of v(2)
2
is determined by the requirement that the half-line should
intersect the boundary in the case c′
2 = 0.
We then determine the intersection between the half-line x(1)+tv(2), t ≥0,
and the relative boundary of F1, which occurs in one of the remaining hyper-
planes. If this hyperplane is the hyperplane a′
21x2 + · · · + a′
2nxn = b′
2, say, we
eliminate the variable x2 from the remaining constraints and the objective
function. All this can be done with at most O(mn) operations and results in
a point x(2) in the intersection of two faces, and the new value of the objective
function is ⟨c, x(2)⟩= ⟨c, x(1)⟩−t2|c′
2| ≤⟨c, x(1)⟩.
After n iterations, which together require at most nO(mn) = O(mn2)
arithmetic operations, we have reached an extreme point ˆx = x(n) with a
function value that does not exceed the value at the starting point x(0). The
coordinates of the extreme point are obtained by solving a triangular system
of equations, which only requires O(n2) operations. The total number of
operations is thus O(mn2).
Example 18.3.1. We exemplify the puriﬁcation algorithm with the LP prob-
lem
min
−2x1 + x2 + 3x3
s.t.







−x1 + 2x2 + x3 ≤4
−x1 + x2 + x3 ≤2
x1 −2x2
≤1
x1 −x2 −2x3 ≤1
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
113
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
113
Starting from the interior point x(0) = (1, 1, 1) with objectiv function
value cTx(0) = 2, we shall ﬁnd an extreme point with a lower value.
Since c1 = −2 < 0, we begin by choosing v(1) = (1, 0, 0) and by determin-
ing the point of intersection between the half-line x = x(0)+tv(1) = (1+t, 1, 1),
t ≥0, and the boundary of the polyhedron of feasible points. We ﬁnd that the
point x(1) = (3, 1, 1), corresponding to t = 2, satisﬁes all constraints and the
third constraint with equality. So x(1) lies in the face obtained by intersecting
the polyhedron X with the supporting hyperplane x1 −2x2 = 1. We elimi-
nate x1 from the objectiv function and from the remaining constraints using
the equation of this hyperplane, and consider the restriction of the objective
function to the corresponding face, i.e. the function f(x) = −3x2 + 3x3 −2
restricted to the polyhedron given by the system







x1 −2x2
= 1
x3 ≤5
−x2 + x3 ≤3
x2 −2x3 ≤0
The x2-coeﬃcient of our new objective function is negative, so we follow
the half-line x2 = 1 + t, x3 = 1, t ≥0, in the hyperplane x1 −2x2 = 1 until it
hits a new supporting hyperplane, which occurs for t = 1, when it intersects
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
114
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
the hyperplane x2 −2x3 = 0 in the point x(2) = (5, 2, 1). Elimination of x2
results in the objective function f(x) = −3x3 −2 and the system







x1 −2x2
= 1
x2 −2x3 = 0
x3 ≤5
−x3 ≤3
Our new half-line in the face F1∩F2 is given by the equation x3 = 1+t, t ≥0,
and the halﬂine intersects the third hyperplane x3 = 5 when t = 4, i.e. in a
point with x3-coordinate equal to 5. Back substitution gives x(3) = (21, 10, 5),
which is an extreme point with objective function value equal to −17.
18.4
Complexity
By the complexity of a problem we here mean the number of arithmetic
operations needed to solve it, and in this section we will study the complexity
of LP problems with rational coeﬃcients. The solution of an LP problem
consists by deﬁnition of the problem’s optimal value and, provided the value
is ﬁnite, of an optimal point. All known estimates of the complexity depend
not only on the number of variables and constraints, but also on the size of
the coeﬃcients, and an appropriate measure of the size of a problem is given
by the number of binary bits needed to represent all its coeﬃcients.
Deﬁnition. The input length of a vector x = (x1, x2, . . . , xn) in Rn is the
integer ℓ(x) deﬁned as
ℓ(x) =
n

j=1
⌈log2(|xj| + 1)⌉.
The number of digits in the binary expansion of a positive integer z is
equal to ⌈log2(|z| + 1)⌉.
The binary representation of a negative integer
z requires one bit more in order to take care of the sign, and so does the
representation of z = 0. The number of bits to represent an arbitrary vector
x in Rn with integer coordinates is therefore at most ℓ(x) + n.
The norm of a vector can be estimated using the input length, and we
shall need the following simple estimate in the two cases p = 1 and p = 2.
Lemma 18.4.1. ∥x∥p ≤2ℓ(x) for all x ∈Rn and all p ≥1.
Proof. The inequality is a consequence of the following trivial inequalities
n
j=1 aj ≤n
j=1(aj + 1), ap + 1 ≤(a + 1)p and log2(a + 1) ≤⌈log2(a + 1)⌉,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
115
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
which hold for nonnegative numbers a, aj, and imply that
∥x∥p
p =
n

j=1
|xj|p ≤
n

j=1
(|xj|p + 1) ≤
n

j=1
(|xj| + 1)p ≤2p ℓ(x).
We will now study LP problems of the type
(LP)
min ⟨c, x⟩
s.t.
Ax ≤b
where all coeﬃcients of the m × n-matrix A = [aij] and of the vectors b and
c are integers. Every LP problem with rational coeﬃcients can obviously be
replaced by an equivalent problem of this type after multiplication with a
suitable least common denominator. The polyhedron of feasible points will
be denoted by X so that
X = {x ∈Rn | Ax ≤b}.
Deﬁnition. The two integers
ℓ(X) = ℓ(A) + ℓ(b)
and
L = ℓ(X) + ℓ(c) + m + n,
where ℓ(A) denotes the input length of the matrix A, considered as a vector
in Rmn, are called the input length of the polyhedron X and the input length
of the given LP problem (LP), respectively.2
The main result of this section is the following theorem, which implies
that there is a solution algorithm that is polynomial in the input length of
the LP problem.
Theorem 18.4.2. There is an algorithm which solves the LP problem (LP)
with at most O((m + n)7/2L) arithmetic operations.
Proof. I. We begin by noting that we can without restriction assume that
the polyhedron X of feasible points is line-free. Indeed, we can, if necessary
replace the problem (LP) with the equivalent and line-free LP problem
min ⟨c, x+⟩−⟨c, x−⟩
s.t.



Ax+ −Ax−≤b
−x+ ≤0
−x−≤0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
116
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
116
This LP problem in n′ = 2n variables and with m′ = m + 2n constraints has
input length
L′ = 2ℓ(A) + 2n + ℓ(b) + 2ℓ(c) + m′ + n′
≤2(ℓ(A) + ℓ(b) + ℓ(c) + m + n) + 4n = 2L + 4n ≤6L,
so any algorithm that solves this problem with O((m′ + n′)7/2L′) operations
also solves problem (LP) with O((m + n)7/2L) operations since m′ + n′ ≤
4(m + n) and L′ ≤6L.
From now on, we therefore assume that X is a line-free polyhedron, and
for nonempty polyhedra X this implies that m ≥n and that X has at least
one extreme point.
The assertion of the theorem is also trivially true for LP problems with
only one variable, so we assume that m ≥n ≥2. Finally, we can naturally
assume that all the rows of the matrix A are nonzero, for if the kth row is
identically zero, then the corresponding constraint can be deleted if bk ≥0,
while the polyhedron X of feasible point is empty if bk < 0. In the future,
we can thus make use of the inequalities
ℓ(X) ≥ℓ(A) ≥m ≥n ≥2 and L ≥ℓ(X) + m + n ≥ℓ(X) + 4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
117
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
II. Under the above assumptions, we will prove the theorem by showing:
1. With O(m7/2L) operations, one can determine whether the optimal
value of the problem is +∞, −∞or ﬁnite, i.e. whether there are any
feasible points or not, and if there are feasible points whether the ob-
jective functions is bounded below or not.
2. Given that the optimal value is ﬁnite, one can then determine an opti-
mal solution with O(m3/2n2L) operations.
Since the proof of statement 1 uses the solution of an appropriate auxiliary
LP problem with ﬁnite value, we begin by showing statement 2.
III. As a ﬁrst building block we need a lemma that provides information
about the extreme points of the polyhedron X in terms of its input length.
Lemma 18.4.3. (i) Let ˆx be an extreme point of the polyhedron X. Then,
the following inequality holds for all nonzero coordinates ˆxj:
2−ℓ(X) ≤|ˆxj| ≤2ℓ(X).
Thus, all extreme points of X lie in the cube {x ∈Rn | ∥x∥∞≤2ℓ(X)}.
(ii) If ˆx and ˜x are two extreme points of X and ⟨c, ˆx⟩̸= ⟨c, ˜x⟩, then
|⟨c, ˆx⟩−⟨c, ˜x⟩| ≥4−ℓ(X).
Proof. To prove the lemma, we begin by recalling Hadamard’s inequality for
k × k-matrices C = [cij] with columns C∗1, C∗2, . . . , C∗k, and which reads as
follows:
|det C| ≤
k

j=1
∥C∗j∥2 =
k

j=1
 k

i=1
c2
ij
1/2.
The inequality is geometrically obvious −the left-hand side |det C| is the
volume of a (hyper)parallelepiped, spanned by the matrix columns, while the
right-hand side is the volume of a (hyper)cuboid whose edges are of the same
length as the edges of the parallelepiped.
By combining Hadamard’s inequality with Lemma 18.4.1, we obtain the
inequality
|det C| ≤
k

j=1
2ℓ(C∗j) = 2ℓ(C).
If C is a quadratic submatrix of the matrix

A
b

, then obviously ℓ(C) ≤
ℓ(A) + ℓ(b) = ℓ(X), and it follows from the above inequality that
(18.16)
|det C| ≤2ℓ(X).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
118
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Now let ˆx be an extreme point of the polyhedron X. According to Theo-
rem 5.1.1 in Part I, there is a set {i1, i2, . . . , in} of row indices such that the
extreme point ˆx is obtained as the unique solution to the equation system
n

j=1
aijxj = bi,
i = i1, i2, . . . , in.
By Cramer’s rule, we can write the solution in the form
ˆxj = ∆j
∆,
where ∆is the determinant of the coeﬃcient matrix and ∆j is the determi-
nant obtained by replacing column number j in ∆with the right-hand side
of the equation system. The determinants ∆and ∆j are integers, and their
absolute values are at most equal to 2ℓ(X), because of inequality (18.16). This
leads to the following estimates for all nonzero coordinates ˆxj, i.e. for all j
with ∆j ̸= 0:
|ˆxj| = |∆j|/|∆| ≤2ℓ(X)/1 = 2ℓ(X) and |ˆxj| = |∆j|/|∆| ≥1/2ℓ(X) = 2−ℓ(X),
which is assertion (i) of the lemma.
(ii) The value of the objective function at the extreme point ˆx is
⟨c, ˆx⟩=
 n

j=1
cj∆j

/∆= T/∆,
where the numerator T is an integer. If ˜x is another extreme point, then of
course we also have ⟨c, ˜x⟩= T ′/∆′ for some integer T ′ and determinant ∆′
with |∆′| ≤2ℓ(X). It follows that the diﬀerence
⟨c, ˜x⟩−⟨c, ˆx⟩= (T∆′ −T ′∆)/∆∆′
is either equal to zero or, if the numerator is nonzero, an integer with absolute
value ≥1/|∆∆′| ≥4−ℓ(X).
IV. We shall use the path-following method, but this assumes that the poly-
hedron of feasible points is bounded and that there is an inner point from
which to start phase 1. To get around this diﬃculty, we consider the following
auxiliary problems in n + 1 variables and m + 2 linear constraints:
(LPM)
min ⟨c, x⟩+ Mxn+1
s.t.



Ax + (b −1)xn+1 ≤b
xn+1 ≤2
−xn+1 ≤0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
119
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
119
Here, M is a positive integer, 1 denotes the vector (1, 1, . . . , 1) in Rm, and
x is as before the n-tuple (x1, x2, . . . , xn).
Let X′ denote the polyhedron of feasible points for the problem (LPM).
Since (x, xn+1) = (0, 1) satisﬁes all constraints with strict inequality, (0, 1) is
an inner point in X′.
We obtain the following estimates for the input length ℓ(X′) of the poly-
hedron X′ and the input lenght L(M) of problem (LPM):
ℓ(X′) = ℓ(A) +
m

i=1

log2

|bi −1| + 1

+ 1 + 1 + ℓ(b) + 2
(18.17)
≤ℓ(X) + 4 +
m

i=1

1 +

log2

1 + |bi

= ℓ(X) + 4 + m + ℓ(b) ≤2ℓ(X) + 4 ≤2L −4,
L(M) = ℓ(X′) + ℓ(c) + ⌈log2(M + 1)⌉+ m + n + 3
(18.18)
≤2ℓ(X) + 2ℓ(c) + ⌈log2 M⌉+ m + n + 8
= 2L + ⌈log2 M⌉−(m + n) + 8 ≤2L + ⌈log2 M⌉+ 4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
120
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
The reason for studying our auxiliary problem (LPM) is given in the
following lemma.
Lemma 18.4.4. Assume that problem (LP) has a ﬁnite value. Then:
(i) Problem (LPM) has a ﬁnite value for each integer M > 0.
(ii) If (ˆx, 0) is an optimal solution to problem (LPM), then ˆx is an optimal
solution to the original problem (LP).
(iii) Assume that M ≥24L and that the extreme point (ˆx, ˆxn+1) of X′ is an
optimal solution to problem (LPM). Then, ˆxn+1 = 0, so ˆx is an optimal
solution to problem (LP).
Proof. (i)
The assumption of ﬁnite value means that the polyhedron X is
nonempty and that the objective function ⟨c, x⟩is bounded below on X, and
by Theorem 12.1.1 in Part II, this implies that the vector c lies in the dual
cone of the recession cone recc X. Since
recc X′ = {(x, xn+1) | Ax + (b −1)xn+1 ≤0, xn+1 = 0}
= recc X × {0},
the dual cone of recc X′ is equal to (recc X)+ × R. We conclude that the
vector (c, M) lies in the dual cone (recc X′)+, which means that the objective
function of problem (LPM) is bounded below on the nonempty set X′. Hence,
our auxiliary problem has a ﬁnite value.
The polyhedron X′ is line-free, since
lin X′ = {(x, xn+1) | Ax + (b −1)xn+1 = 0, xn+1 = 0}
= lin X × {0} = {(0, 0)}.
(ii) The point (x, 0) is feasible for problem (LPM) if and only if x belongs
to X, i.e. is feasible for our original problem (LP). So if (ˆx, 0) is an optimal
solution to the auxiliary problem, then in particular
⟨c, ˆx⟩= ⟨c, ˆx⟩+ M · 0 ≤⟨c, x⟩+ M · 0 = ⟨c, x⟩
for all x ∈X, which shows that ˆx is an optimal solution to problem (LP).
(iii) Assume that (ˆx, ˆxn+1) is an extreme point of the polyhedron X′ and
an optimal solution to problem (LPM). By Lemma 18.4.3, applied to the
polyhedron X′, and the estimate (18.17), we then have the inequality
(18.19)
∥ˆx∥∞≤2ℓ(X′) ≤22ℓ(X)+4 ≤22L−4,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
121
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
so it follows by using Lemma 18.4.1 that
|⟨c, ˆx⟩| ≤
n

j=1
|cj||ˆxj| ≤∥c∥1∥ˆx∥∞≤2ℓ(c) · 22ℓ(X)+4 ≤22ℓ(X)+2ℓ(c)+4
≤22L−2m−2n+4 ≤22L−4.
Assume that ˆxn+1 ̸= 0.
Then ˆxn+1 ≥2−ℓ(X′) ≥2−2L, according to
Lemma 18.4.3. The optimal value ˆvM of the auxiliary problem (LPM) there-
fore satisﬁes the inequality
ˆvM = ⟨c, ˆx⟩+ M ˆxn+1 ≥M ˆxn+1 −|⟨c, ˆx⟩| ≥M · 2−2L −22L−4.
Let now x be an arbitrary extreme point of X. Since (x, 0) is a feasible point
for problem (LPM) and since ∥x∥∞≤2ℓ(X) by lemma 18.4.3, the optimal
value ˆvM must also satisfy the inequality
ˆvM ≤⟨c, x⟩+ M · 0 ≤|⟨c, x⟩| ≤∥c∥1 · ∥x∥∞≤2ℓ(c)+ℓ(X) = 2L−m−n ≤2L−4.
By combining the two inequalities for ˆvM, we obtain the inequality
2L−4 ≥M · 2−2L −22L−4,
which implies that
M ≤23L−4 + 24L−4 < 24L.
So if M ≥24L, then ˆxn+1 = 0.
V. We are now ready for the main step in the proof of Theorem 18.4.2.
Lemma 18.4.5. Suppose that problem (LP) has a ﬁnite value. The path-
following algorithm, applied to the problem (LPM) with ∥x∥∞≤22L as an
additional constraint, M = 24L, ϵ = 2−4L, and (0, 1) as starting point for
phase 1, and complemented with a subsequent puriﬁcation operation, gener-
ates an optimal solution to problem (LP) after at most O(m3/2n2L) aritmetic
operations.
Proof. It follows from the previous lemma and the estimate (18.19) that
the LP problem (LPM) has an optimal solution (ˆx, 0) which satisﬁes the
additional constraint ∥ˆx∥∞≤22L if M = 24L. The LP problem obtained
from (LPM) by adding the 2n constraints
xj ≤22L
and
−xj ≤22L,
j = 1, 2, . . . , n,
therefore has the same optimal value as (LPM).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
122
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
122
The extended problem has m+2n+2 = O(m) linear constraints, and the
point z = (x, xn+1) = (0, 1) is an interior point of the compact polyhedron
of feasible points, which we denote by Z. By Theorem 18.3.1, the path-
following algorithm with ϵ = 2−4L and z as the starting point therefore stops
after O((m+2n+2)3/2n2) ln((m+2n+2)Φ/ϵ+1) = O(m3/2n2) ln(m24LΦ+1)
arithmetic operations at a point in the polyhedron X′ and with a value of
the objective function that approximates the optimal value ˆvM with an error
less than 2−4L.
Puriﬁcation according to the method in Theorem 18.3.2 leads to an ex-
treme point (ˆx, ˆxn+1) of X′ with a value of the objective function less than
ˆvM + 2−4L, and since 2−4L = 4−2L < 4−ℓ(X′), it follows from Lemma 18.4.3
that (ˆx, ˆxn+1) is an optimal solution to (LPM). By Lemma 18.4.4, this implies
that ˆx is an optimal solution to the original problem (LP).
The puriﬁcation process requires O(mn2) arithmetic operations, so the
total arithmetic cost is
O(mn2) + O(m3/2n2) ln(m24LΦ + 1) = O(m3/2n2) ln(m24LΦ + 1)
operations. It thus only remains to prove that ln(m24LΦ + 1) = O(L), and
since m ≤L, this will follow if we show that ln Φ = O(L).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
123
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
By deﬁnition,
Φ = VarZ(c, M) ·
1
1 −πˆzF (z),
where ˆzF is the analytic center of Z with respect to the relevant logarithmic
barrier F. The absolute value of the objective function at an arbitrary point
(x, xn+1) ∈Z can be estimated by
|⟨c, x⟩+ Mxn+1| ≤∥c∥1∥x∥∞+ 2M ≤2ℓ(c)+2L + 2 · 24L ≤24L+2,
and the maximal variation of the function is at most twice this value. Hence,
VarZ(c, M) ≤24L+3.
The second component of Φ is estimated using Theorem 18.1.7.
Let
B∞(a, an+1; r) denote the closed ball of radius r in Rn+1 = Rn × R with
center at the point (a, an+1) and with distance given by the maximum norm,
i.e.
B∞(a, an+1; r) = {(x, xn+1) ∈Rn × R | ∥x −a∥∞≤r, |xn+1 −an+1| ≤r}.
The polyhedron Z is by deﬁnition included in the ball B∞(0, 0; 22L). On
the other hand, the tiny ball B∞(z; 2−L) is included in Z, for if ∥x∥∞≤2−L
and |xn+1 −1| ≤2−L, then
n

j=1
aijxj + (bi −1)xn+1 −bi =
n

j=1
aijxj + bi(xn+1 −1) −xn+1
≤
n

j=1
|aij||xj| + |bi||xn+1 −1| −xn+1 ≤2−L n

j=1
|aij| + |bi|

−(1 −2−L)
≤2−L+ℓ(X) + 2−L −1 ≤2−4 + 2−L −1 < 0,
which proves that the ith inequality of the system Ax+(b−1)xn+1 ≤b holds
with strict inequality for i = 1, 2, . . . , m, and the remaining inequalities that
deﬁne the polyhedron Z are obviously strictly satisﬁed.
It therefore follows from Theorem 18.1.7 that
πˆzF (z) ≤
2 · 22L
2 · 22L + 2−L,
and that consequently
1
1 −πˆzF (z) ≤2 · 23L + 1 < 23L+2.
This implies that Φ ≤24L+3 · 23L+2 = 27L+5. Hence, ln Φ = O(L), which
completes the proof of the lemma.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
124
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
VI. It remains to show that O(m7/2L) operations are suﬃcient to decide
whether the optimal value of the original problem (LP) is +∞, −∞or ﬁnite.
To decide whether the value is +∞or not, i.e. whether the polyhedron
X is empty or not, we consider the artiﬁcial LP problem
min xn+1
s.t.
Ax −1xn+1 ≤b
−xn+1 ≤0
This problem has feasible points since (0, t) satisﬁes all constraints for suf-
ﬁciently large positive numbers t. The optimal value of the problem is ap-
parently greater than or equal to zero, and it is equal to zero if and only if
X ̸= ∅.
So we can decide whether the polyhedron X is empty or not by deter-
mining an optimal solution to the artiﬁcial problem. The input length of
this problem is ℓ(X) + 2m + n + 4, and since this number is ≤2L, it fol-
lows from Lemma 18.4.5 that we can decide whether X is empty or not with
O(m3/2n2L) aritmethic operations.
Note that we do not need to solve the artiﬁcial problem exactly. If the
value is greater than zero, then, because of Lemma 18.4.3, it is namely greater
than or equal to 2−2L. It is therefore suﬃcient to determine a point that
approximates the value with an error of less than 2−2L to know if the value
is zero or not.
VII. If the polyhedron X is nonempty, we have as the next step to decide
whether the objective function is bounded below. This is the case if and
only if the dual problem to problem (LP) has feasible points, and this dual
maximization problem is equivalent to the minimization problem
min ⟨−b, y⟩
s.t.



ATy ≤
c
−ATy ≤−c
−y ≤
0,
which is a problem with m variables, 2n+m (= O(m)) constraints and input
length
2ℓ(A) + m + 2ℓ(c) + ℓ(b) + m + (2n + m) ≤2L + m ≤3L.
So it follows from step VI that we can decide whether the dual problem has
any feasible points with O(m7/2L) operations.
The proof of Theorem 18.4.2 is now complete.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
125
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
125
Exercises
18.1 Show that if the functions fi are νi-self-concordant barriers to the subsets
Xi of Rni, then f(x1, . . . , xm) = f1(x1) + · · · + fm(xm) is a (ν1 + · · · + νm)-
self-concordant barrier to the product set X1 × · · · × Xm.
18.2 Prove that the dual local norm ∥v∥∗
x that is associated with the function f
is ﬁnite if and only if v belongs to N(f′′(x))⊥, and that the restriction of
∥·∥∗
x to N(f′′(x))⊥is a proper norm.
18.3 Let X be a closed proper convex cone with nonempty interior, let ν ≥1 be
a real number, and suppose that the function f : int X →R is closed and
self-concordant and that f(tx) = f(x) −ν ln t for all x ∈int X and all t > 0.
Prove that
a) f ′(tx) = t−1f′(x)
b) f′(x) = −f′′(x)x
c)
λ(f, x) = √ν.
The function f is in other words a ν-self-concordant barrier to X.
18.4 Show that the nonnegative orthant X = Rn
+, ν = n and the logarithmic
barrier f(x) = −n
i=1 ln xi fulﬁll the assumptions of the previous exercise.
18.5 Let X = {(x, xn+1) ∈Rn × R | xn+1 ≥∥x∥2}.
a) Show that the function f(x) = −ln(x2
n+1 −(x2
1 + · · · + x2
n)) is self-
concordant on int X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
126
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
126
b) Show that X, ν = 2 and f fulﬁll the assumptions of exercise 18.3. The
function f is thus a 2-self-concordant barrier to X.
18.6 Suppose that the function f : R++ →R is convex, three times continuously
diﬀerentiable and that
|f′′′(x)| ≤3f′′(x)
x
for all x > 0. The function
F(x, y) = −ln(y −f(x)) −ln x
with X = {(x, y) ∈R2 | x > 0, y > f(x)} as domain is self-concordant
according to exercise 16.3. Show that F is a 2-self-concordant barrier to the
closure cl X.
18.7 Prove that the function
F(x, y) = −ln(y −x ln x) −ln x
is a 2-self-concordant barrier to the epigraph
{(x, y) ∈R2 | y ≥x ln x, x ≥0}.
18.8 Prove that the function
G(x, y) = −ln(ln y −x) −ln y
is a 2-self-concordant barrier to the epigraph {(x, y) ∈R2 | y ≥ex}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
127
Bibliogracal and historical notices
Bibliograﬁcal and historical
notices
Newton’s method is a classic iterative algorithm for ﬁnding critical points of
diﬀerentiable functions, and it was proven by Kantorovich [1] that the algo-
rithm converges quadratically when the function has a Lipschitz continuous,
positive deﬁnite second derivatives in a neighborhood of the critical point,
provided the starting point is selected close enough.
Barrier methods for solving nonlinear optimization problems were ﬁrst
used during the 1950s. The central path with logarithmic barriers was stud-
ied by Fiacco and McCormick, and their book on sequential minimization
techniques −Fiacco–McCormick [1], ﬁrst published in 1968 −is the stan-
dard work in the ﬁeld. The methods worked well in practice, for the most
part, but there were no theoretical complexity results. They lost in popularity
in the 1970s and then experienced a renaissance in the wake of Karmarkar’s
discovery.
Karmarkar’s [1] polynomial algorithm for linear programming proceeds
by mapping the polyhedron of feasible points and the current approximate
solution xk onto a new polyhedron and a new point x′
k which is located near
the center of the new polyhedron, using a projective scaling transformation.
Thereafter, a step is taken in the transformed space which results in a point
xk+1 with a lower objective function value. The progress is measured by
means of a logarithmic potential function.
It was soon noted that Karmarkar’s potential-reducing algorithm was
akin to previously studied path-following methods, and Renegar [1] and Gon-
zaga [1] managed to show that the path-following method with logarithmic
barrier is polynomial for LP problems.
A general introduction to linear programming and the algorithm devel-
opment in the area until the late 1980s (the ellipsoid method, Karmarkar’s
algorithm, etc.) is given by Goldfarb–Todd [1]. An overview of potential-
reducing algorithms is given by Todd [1], while Gonzaga [2] describes the
evolution of path-following algorithms until 1992.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
128
References
A breakthrough in convex optimization occurred in the late 1980s, when
Yurii Nesterov discovered that Gonzaga’s and Renegar’s proof only used two
properties of the logarithmic barrier function, namely, that it satisﬁes the two
diﬀerential inequalities, which with Nesterov’s terminology means that the
barrier is self-concordant with ﬁnite parameter ν. Since explicit computable
self-concordant barriers exist for a number of important types of convex
sets, the theoretical complexity results for linear programming could now be
extended to a large class of convex optimization problems, and Nemirovskii
together with Nesterov developed algorithms for convex optimization based
on self-concordant barriers. See Nesterov–Nesterovski [1].
A modern textbook on convex optimization, which in addition to theory
and algorithms also contains lots of interesting applications from a variety of
ﬁelds, is the book by Boyd–Vandenberghe [1].
References
Boyd, S. & Vandenberghe, L.
[1] Convex Optimization, Cambridge Univ. Press, Cambridge, UK, 2004.
Fiacco, A.V. & McCormick, G.P.
[1] Nonlinear Programming: Sequential Unconstrained Minimization Tech-
niques. Society for Industrial and Applied Mathematics, 1990. (First
published in 1968 by Research Analysis Corporation.)
Goldfarb, D.G. & Todd, M.J.
[1] Linear programming. Chapter 2 in Nemhauser, G.L. et al. (eds.), Hand-
books in Operations Research and Management Science, vol. 1: Opti-
mization, North-Holland, 1989.
Gonzaga, C.C.
[1] An algorithm for solving linear programming problems in O(n3L) oper-
ations. Pages 1–28 in Megiddo, N. (ed.), Progress in Mathematical Pro-
gramming: Interior-Point and Related Methods, Springer-Verlag, 1988.
[2] Path-Following Methods for Linear Programming, SIAM Rev. 34 (1992),
167–224.
Kantorovich, L.V.
[1] Functional Analysis and Applied Mathematics. National Bureau of Stan-
dards, 1952. (First published in Russian in 1948.)
Karmarkar, N.
[1] A new polynomial-time algorithm for linear programming, Combinator-
ica 4 (1984), 373–395.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
129
References
129
Nesterov, Y. & Nemirovskii, A.
[1] Interior-Point Polynomial Algorithms in Convex Programming. Society
for Industrial and Applied Mathematics, 1994.
Renegar, J.
[1] A polynomial-time algorithm based on Newton’s method for linear pro-
gramming, Math. Programm. 40 (1988), 59–94.
Todd, M.
[1] Potential-reduction methods in mathematical programming, Math. Pro-
gram. 76 (1997), 3–45.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
130
Answers and solution to the exercises
Answers and solution to the
exercises
Chapter 14
14.1 x1 = ( 4
9, −1
9), x2 = ( 2
27, 2
27), x3 = ( 8
243, −2
243).
14.3 hf ′(xk) = f(xk) −f(xk+1) →f(ˆx) −f(ˆx) = 0 and hf ′(xk) →hf ′(ˆx).
Hence, f ′(ˆx) = 0.
Chapter 15
15.1 ∆xnt = −x ln x, λ(f, x) = √x ln x, ∥v∥x = |v|/√x.
15.2 a) ∆xnt = ( 1
3, 1
3), λ(f, x) =

1
3, ∥v∥x = 1
2

5v2
1 + 2v1v2 + 5v2
2
b) ∆xnt = ( 1
3, −2
3), λ(f, x) =

1
3, ∥v∥x = 1
2

8v2
1 + 8v1v2 + 5v2
2.
15.3 ∆xnt = (v1, v2), where v1 + v2 = −1 −e−(x1+x2),
λ(f, x) = e(x1+x2)/2 + e−(x1+x2)/2, ∥v∥x = e(x1+x2)/2|v1 + v2|.
15.4 If rank A < m, then rank M < m + n, and if N(A) ∩N(P) contains
a nonzero vector x, then M
x
0

=
0
0

. Hence, the matrix M has no
inverse in these cases.
Conversely, suppose that rank A = m, i.e. that N(AT) = {0}, and
that N(A) ∩N(P) = {0}. We show that the coeﬃcient matrix M is
invertible by showing that the homogeneous system
Px + ATy = 0
Ax
= 0
has no other solutions than the trivial one, x = 0 and y = 0.
By multiplying the ﬁrst equation from the left by xT we obtain
0 = xTPx + xTATy = xTPx + (Ax)Ty = xTPx,
and since P is positive semideﬁnite, it follows that Px = 0. The ﬁrst
equation now gives ATy = 0. Hence, x ∈N(A)∩N(P) and y ∈N(AT),
which means that x = 0 and y = 0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
131
Answers and solution to the exercises
15.5 a) By assumption, ⟨v, f ′′(x)v⟩≥µ∥v∥2 if Av = 0. Since AC = 0, we
conclude that
⟨w, ˜f ′′(z)w⟩= ⟨w, CTf ′′(x)Cw⟩= ⟨Cw, f ′′(x)Cw⟩≥µ∥Cw∥2
= µ⟨w, CTCw⟩≥µσ∥w∥2
for all w ∈Rp, which shows that the function ˜f is µσ-strongly convex.
b) The assertion follows from a) if we show that the restriction of f to
X is a K−2M −1-strongly convex function. So assume that x ∈X and
that Av = 0. Then f ′′(x)
AT
A
0
 v
0

=
f ′′(x)v
0

and due to the bound on the norm of the inverse matrix, we conclude
that
∥v∥≤K∥f ′′(x)v∥.
The positive semideﬁnite second derivative f ′′(x) has a positive semidef-
inite square root f ′′(x)1/2 and ∥f ′′(x)1/2∥= ∥f ′′(x)∥1/2 ≤M 1/2.
It
follows that
∥f ′′(x)v∥2 = ∥f ′′(x)1/2f ′′(x)1/2v∥2 ≤∥f ′′(x)1/2∥2∥f ′′(x)1/2v∥2
≤M∥f ′′(x)1/2v∥2 = M⟨v, f ′′(x)v⟩,
which inserted in the above inequality results in the inequality
⟨v, f ′′(x)v⟩≥K−2M −1∥v∥2.
Chapter 16
16.2 Let Pi denote the projection of Rn1 × · · · × Rnm onto then ith factor
Rni. Then f(x) = m
i=1 fi(Pix), so it follows from Theorems 16.1.5
and 16.1.6 that f is self-concordant.
16.3 a) The function g is convex, since g′′(x) = f ′(x)2
f(x)2 −f ′′(x)
f(x) + 1
x2 ≥0.
g′′′(x) = −f ′′′(x)
f(x) + 3f ′(x)f ′′(x)
f(x)2
−2f ′(x)3
f(x)3 −2
x3 implies that
|g′′′(x)| ≤3 f ′′(x)
x|f(x)| + 3|f ′(x)|f ′′(x)
f(x)2
+ 2|f ′(x)|3
|f(x)|3 + 2 1
x3.
The inequality |g′′′(x)| ≤2g′′(x)3/2, which proves that the function g
is self-concordant, is now obtained by choosing a =

f ′′(x)/|f(x)|,
b = |f ′(x)|/|f(x)| and c = 1/x in the equality
3a2b + 3a2c + 2b3 + 2c3 ≤2(a2 + b2 + c2)3/2.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
132
Answers and solution to the exercises
132
To prove this inequality, we can due to homogeneity assume that
a2 + b2 + c2 = 1.
Inserting a2 = 1 −b2 −c2 into the inequality, we can rewrite it as
(b + c)(3 −(b + c)2) ≤2, which holds since x(3 −x2) ≤2 for x ≥0.
16.3 b) Let φ(t) = F(x0 +αt, y0 +βt) be the restriction of F to an arbitrary
line through the point (x0, y0) in dom F. We will prove that φ is self-
concordant, and we have to treat the cases α = 0 and α ̸= 0 separately.
If α = 0, then φ(t) = −ln(βt + a) + b, where a = y0 −f(x0) and
b = −ln x0, so φ is self-concordant in this case.
To prove the case α ̸= 0, we note that f(x) −Ax −B satisﬁes the
assumptions of the exercise for each choice of the constants A and
B, and hence h(x) = −ln(Ax + B −f(x)) −ln x is self-concordant
according to the result in a). But φ(t) = h(αt + x0), where A = β/α
and B = y0 −βx0/α. Thus, φ is self-concordant.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
133
Answers and solution to the exercises
16.6 a) Set λ = λ(f, x) and use the inequalities (16.7) and (16.6) in Theorem
16.3.2 with y = x+ and v = x+ −x = (1 + λ)−1∆xnt. This results in
the inequality
⟨f ′(x+), w⟩≤⟨f ′(x), w⟩+
1
1 + λ⟨f ′′(x)∆xnt, w⟩+
λ2∥w∥x
(1 + λ)2(1 −λ/(1 + λ))
= ⟨f ′(x), w⟩−
1
1 + λ⟨f ′(x), w⟩+
λ2
1 + λ∥w∥x
=
λ
1 + λ⟨f ′(x), w⟩+
λ2
1 + λ∥w∥x
≤
λ
1 + λλ∥w∥x +
λ2
1 + λ∥w∥x = 2λ2
1 + λ∥w∥x
≤
2λ2∥w∥x+
(1 + λ)(1 −λ/(1 + λ)) = 2λ2∥w∥x+
with λ(f, x+) ≤2λ2 as conclusion.
Chapter 18
18.1 Follows from Theorems 18.1.3 and 18.1.2.
18.2 To prove the implication ∥v∥∗
x < ∞⇒v ∈N(f ′′(x))⊥we write v
as v = v1 + v2 with v1 ∈N(f ′′(x)) and v2 ∈N(f ′′(c))⊥, noting that
∥v1∥x = 0. Hence ∥v∥2
1 = ⟨v1, v1⟩= ⟨v, v1⟩≤∥v∥∗
x∥v1∥x = 0, and we
conclude that v1 = 0. This proves that v belongs to N(f ′′(x))⊥.
Given v ∈N(f ′′(x))⊥there exists a vector u such that v = f ′′(x)u. We
shall prove that ∥v∥∗
x = ∥u∥x. From this follows that ∥v∥∗
x < ∞and
that ∥·∥∗
x is a norm on the subspace N(f ′′(x))⊥of Rn.
Let w ∈Rn be arbitrary. By Cauchy–Schwarz’s inequality,
⟨v, w⟩= ⟨f ′′(x)u, w⟩= ⟨f ′′(x)1/2u, f ′′(x)1/2w⟩
≤∥f ′′(x)1/2u∥∥f ′′(x)1/2w∥= ∥u∥x∥v∥x,
and this implies that ∥v∥∗
x ≤∥u∥x. Suppose v ̸= 0. Then u does not
belong to N(f ′′(x)), which means that ∥u∥x ̸= 0, and for w = u/∥u∥x
we get the identity
⟨v, w⟩= ∥u∥−1
x ⟨f ′′(x)1/2u, f ′′(x)1/2u⟩= ∥u∥−1
x ∥f ′′(x)1/2u∥2 = ∥u∥x,
which proves that ∥v∥∗
x = ∥u∥x. If on the other hand v = 0, then u is
a vector in N(f ′′(x)) so we have ∥v∥∗
x = ∥u∥x in this case, too.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
134
Answers and solution to the exercises
18.3 a) Diﬀerentiate the equality f(tx) = f(x) −ν ln t with respect to x.
b) Diﬀerentiate the equality obtained in a) with respect to t and then
take t = 1.
c) Since X does not contain any line, f is a non-degenerate self-concor-
dant function, and it follows from the result in b) that x is the unique
Newton direction of f at the point x. By diﬀerentiating the equality
f(tx) = f(x)−ν ln t with respect to t and then putting t = 1, we obtain
⟨f ′(x), x⟩= −ν. Hence
ν = −⟨f ′(x), x⟩= −⟨f ′(x), ∆xnt⟩= λ(f, x)2.
18.5 Deﬁne g(x, xn+1) = (x2
1 + · · · + x2
n) −x2
n+1 = ∥x∥2 −x2
n+1, so that
f(x) = −ln(−g(x, xn+1)),
and let w = (v, vn+1). Then
Dg = Dg(x, xn+1)[w] = 2(⟨v, x⟩−xn+1vn+1),
D2g = D2g(x, xn+1)[w, w] = 2(∥v∥2 −v2
n+1),
D3g = D3g(x, xn+1)[w, w, w] = 0,
Df = Df(x, xn+1)[w] = −1
gDg
D2f = D2f(x, xn+1)[w, w] = 1
g2

(Dg)2 −gD2g

,
D3f = D3f(x, xn+1)[w, w, w] = 1
g3

−2(Dg)3 + 3gDgD2g

.
Consider the diﬀerence
∆= (Dg)2−gD2g = 4(⟨x, v⟩−xn+1vn+1)2+2(x2
n+1−∥x∥2)(∥v∥2−v2
n+1).
Since xn+1 > ∥x∥, we have ∆≥0 if |vn+1| ≤∥v∥. So suppose that
|vn+1| > ∥v∥. Then
|xn+1vn+1 −⟨x, v⟩| ≥xn+1|vn+1| −|⟨x, v⟩|
≥xn+1|vn+1| −∥x∥∥v∥≥0,
and it follows that
∆≥4(xn+1|vn+1| −∥x∥∥v∥)2 + 2(x2
n+1 −∥x∥2)(∥v∥2 −v2
n+1)
= 2(xn+1|vn+1| −∥x∥∥v∥)2 + 2(xn+1∥v∥−∥x∥|vn+1|)2 ≥0.
This shows that D2f = ∆/g2 ≥0, so f is a convex function.
To prove that the function is self-concordant, we shall show that
4(D2f)3 −(D3f)2 ≥0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
135
Answers and solution to the exercises
After simpliﬁcation we obtain
4(D2f)3 −(D3f)2 = g−4(D2g)2(3(Dg)2 −4gD2g),
and the problem has now been reduced to showing that the diﬀerence
∆′ = 3(Dg)2 −4gD2g
= 12(⟨x, v⟩−xn+1vn+1)2 + 8(x2
n+1 −∥x∥2)(∥v∥2 −v2
n+1)
is nonnegative. This is obvious if |vn+1| ≤∥v∥, and if |vn+1| > ∥v∥then
we get in a similar way as above
∆′ ≥12(xn+1|vn+1| −∥x∥∥v∥)2 + 8(x2
n+1 −∥x∥2)(∥v∥2 −v2
n+1)
= 4(xn+1|vn+1| −∥x∥∥v∥)2 + 8(xn+1∥v∥−∥x∥|vn+1|)2 ≥0.
18.6 Let w = (u, v) be an arbitrary vector in R2. Writing a = 1/(y −f(x)),
b = −1/x, A = f ′(x) and B = f ′′(x) for short, where a > 0 and B ≥0,
we obtain
DF(x, y)[w] = (aA + b)u −av
D2F(x, y)[w, w] = (aB + a2A2 + b2)u2 −2a2Auv + a2v2,
and
2D2F(x, y)[w, w] −

DF(x, y)[w]
2
= a2A2u2 + b2u2 + a2v2 + 2abuv −2a2Auv −2abAu2 + 2aBu2
= (aAu −bu −av)2 + 2aBu2 ≥0.
So F is a 2-self-concordant function.
18.7 Use the previous exercise with f(x) = x ln x.
18.8 Taking f(x) = −ln x in exercise 18.5, we see that
F(x, y) = −ln(ln x + y) −ln x
is a 2-self-concordant barrier to the closure of the region −y < ln x.
Since G(x, y) = F(y, −x), it then follows from Theorem 18.1.3 that G
is a 2-self-concordant barrier to the region y ≥ex.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION – PART III
136
Index
Index
analytic center, 74
Armijo’s rule, 3
barrier, 74
central path, 76
convergence
linear, 6, 7
quadratic, 6, 7
damped Newton method, 23
descent algorithm, 1
dual local norm, 92
gradient descent method, 2, 7
inner iteration, 79
input length, 114
line search, 2
linear convergence, 6, 7
local seminorm, 18
logarithmic barrier, 75
ν-self-concordant barrier, 83
Newton
decrement, 16, 35
direction, 15, 35
method, 2, 23, 66
non-degenerate, 45
outer iteration, 79
path-following method, 79
phase 1, 81
pure Newton method, 23
puriﬁcation, 110
quadratic convergence, 6, 7
search direction, 2
self-concordant, 42
standard form, 94
step size, 2
Download free eBooks at bookboon.com


Lars-√Öke Lindahl
Descent and Interior-point
Methods
Convexity and Optimization ‚Äì Part III
Download free books at

ii
 
LARS-√ÖKE LINDAHL 
DESCENT AND 
INTERIOR-POINT 
METHODS 
CONVEXITY AND 
OPTIMIZATION ‚Äì PART III
Download free eBooks at bookboon.com

iii
Descent and Interior-point Methods: Convexity and Optimization ‚Äì Part III
1st edition
¬© 2016 Lars-√Öke Lindahl & bookboon.com
ISBN 978-87-403-1384-0
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
iv
Contents
iv
CONTENTS
	
To see Part II, download: Linear and Convex Optimization: Convexity  
and Optimization ‚Äì Part II
	
Part I. Convexity
1	
Preliminaries	
Part I
2	
Convex sets	
Part I
2.1	
Affine sets and affine maps	
Part I
2.2	
Convex sets	
Part I
2.3	
Convexity preserving operations	
Part I
2.4	
Convex hull	
Part I
2.5	
Topological properties	
Part I
2.6	
Cones	
Part I
2.7	
The recession cone	
Part I
	
Exercises	
Part I
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
v
Contents
3	
Separation	
Part I
3.1	
Separating hyperplanes	
Part I
3.2	
The dual cone	
Part I
3.3	
Solvability of systems of linear inequalities	
Part I
	
Exercises	
Part I
4	
More on convex sets	
Part I
4.1	
Extreme points and faces	
Part I
4.2	
Structure theorems for convex sets	
Part I
	
Exercises	
Part I
5	
Polyhedra	
Part I
5.1	
Extreme points and extreme rays	
Part I
5.2	
Polyhedral cones	
Part I
5.3	
The internal structure of polyhedra	
Part I
5.4	
Polyhedron preserving operations	
Part I
5.5	
Separation	
Part I
	
Exercises	
Part I
6	
Convex functions	
Part I
6.1	
Basic definitions	
Part I
6.2	
Operations that preserve convexity	
Part I
6.3	
Maximum and minimum	
Part I
6.4	
Some important inequalities	
Part I
6.5	
Solvability of systems of convex inequalities	
Part I
6.6	
Continuity	
Part I
6.7	
The recessive subspace of convex functions	
Part I
6.8	
Closed convex functions	
Part I
6.9	
The support function	
Part I
6.10	
The Minkowski functional	
Part I
	
Exercises	
Part I
7	
Smooth convex functions	
Part I
7.1	
Convex functions on R	
Part I
7.2	
Differentiable convex functions	
Part I
7.3	
Strong convexity	
Part I
7.4	
Convex functions with Lipschitz continuous derivatives	
Part I
	
Exercises	
Part I
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
vi
Contents
8	
The subdifferential	
Part I
8.1	
The subdifferential	
Part I
8.2	
Closed convex functions	
Part I
8.3	
The conjugate function	
Part I
8.4	
The direction derivative	
Part I
8.5	
Subdifferentiation rules	
Part I
	
Exercises	
Part I
	
Bibliografical and historical notices	
Part I
	
References	
Part I
	
Answers and solutions to the exercises	
Part I
	
Index	
Part I
	
Endnotes	
Part I
	
Part II. Linear and Convex Optimization
	
Preface	
Part II
	
List of symbols	
Part II
9	
Optimization	
Part II
9.1	
Optimization problems	
Part II
9.2	
Classification of optimization problems	
Part II
9.3	
Equivalent problem formulations	
Part II
9.4	
Some model examples	
Part II
	
Exercises	
Part II
10	
The Lagrange function	
Part II
10.1	
The Lagrange function and the dual problem	
Part II
10.2	
John‚Äôs theorem	
Part II
	
Exercises	
Part II
11	
Convex optimization	
Part II
11.1	
Strong duality	
Part II
11.2	
The Karush-Kuhn-Tucker theorem	
Part II
11.3	
The Lagrange multipliers	
Part II
	
Exercises	
Part II
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
vii
Contents
12	
Linear programming	
Part II
12.1	
Optimal solutions	
Part II
12.2	
Duality	
Part II
	
Exercises	
Part II
13	
The simplex algorithm	
Part II
13.1	
Standard form	
Part II
13.2	
Informal description of the simplex algorithm	
Part II
13.3	
Basic solutions	
Part II
13.4	
The simplex algorithm	
Part II
13.5	
Bland‚Äôs anti cycling rule	
Part II
13.6	
Phase 1 of the simplex algorithm	
Part II
13.7	
Sensitivity analysis	
Part II
13.8	
The dual simplex algorithm	
Part II
13.9	
Complexity	
Part II
	
Exercises	
Part II
	
Bibliografical and historical notices	
Part II
	
References	
Part II
	
Answers and solutions to the exercises	
Part II
	
Index	
Part II
	
Part III. Descent and Interior-point Methods	
	
Preface	
ix
	
List of symbols	
x
14	
Descent methods	
1
14.1	
General principles	
1
14.2	
The gradient descent method	
7
	
Exercises	
12
15	
Newton‚Äôs method	
13
15.1	
Newton decrement and Newton direction	
13
15.2	
Newton‚Äôs method	
22
15.3	
Equality constraints	
34
	
Exercises	
39
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
viii
Contents
16	
Self-concordant functions	
41
16.1	
Self-concordant functions	
42
16.2	
Closed self-concordant functions	
47
16.3	
Basic inequalities for the local seminorm	
51
16.4	
Minimization	
56
16.5	
Newton‚Äôs method for self-concordant functions	
61
	
Exercises	
67
	
Appendix	
68
17	
The path-following method	
73
17.1	
Barrier and central path	
74
17.2	
Path-following methods	
78
18	
The path-following method with self-concordant barrier	
83
18.1	
Self-concordant barriers	
83
18.2	
The path-following method	
94
18.3	
LP problems	
108
18.4	
Complexity	
114
	
Exercises	
125
	
Bibliografical and historical notices	
127
	
References	
128
	
Answers and solution to the exercises	
130
	
Index	
136
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
ix
Preface
Preface
This third and Ô¨Ånal part of Convexity and Optimization discusses some opti-
mization methods which when carefully implemented are eÔ¨Écient numerical
optimization algorithms.
We begin with a very brief general description of descent methods and
then proceed to a detailed study of Newton‚Äôs method. For a particular class
of functions, the so-called self-concordant functions, discovered by Yurii Nes-
terov and Arkadi Nemirovski, it is possible to describe the convergence rate
of Newton‚Äôs method with absolute constants, and we devote one chapter to
this important class.
Interior-point methods are algorithms for solving constrained optimiza-
tion problems. Contrary to the simplex algorithms, they reach the optimal
solution by traversing the interior of the feasible region. Any convex opti-
mization problem can be transformed into minimizing a linear function over
a convex set by converting to the epigraph form and with a self-concordant
function as barrier, and Nesterov and Nemirovski showed that the number
of iterations of the path-following algorithm is bounded by a polynomial in
the dimension of the problem and the accuracy of the solution. Their proof
is described in this book‚Äôs Ô¨Ånal chapter.
Uppsala, April 2015
Lars-Àö
Ake Lindahl
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
x
List of symbols
List of symbols
bdry X
boundary of X, see Part I
cl X
closure of X, see Part I
dim X
dimension of X, see Part I
dom f
the eÔ¨Äective domain of f: {x | ‚àí‚àû< f(x) < ‚àû}, see Part I
epi f
epigraph of f, see Part I
ext X
set of extreme points of X, see Part I
int X
interior of X, see Part I
lin X
recessive subspace of X, see Part I
recc X
recession cone of X, see Part I
ei
ith standard basis vector (0, . . . , 1, . . . , 0)
f ‚Ä≤
derivate or gradient of f, see Part I
f ‚Ä≤‚Ä≤
second derivative or hessian of f, see Part I
vmax, vmin
optimal values, see Part II
B(a; r)
open ball centered at a with radius r
B(a; r)
closed ball centered at a with radius r
Df(a)[v]
diÔ¨Äerential of f at a, see Part I
D2f(a)[u, v]
n
i,j=1
‚àÇ2f
‚àÇxi‚àÇxj (a)uivj, see Part I
D3f(a)[u, v, w]
n
i,j,k=1
‚àÇ3f
‚àÇxi‚àÇxj‚àÇxk (a)uivjwk, see Part I
E(x; r)
ellipsoid {y | ‚à•y ‚àíx‚à•x ‚â§r}, p. 88
L
input length, p. 115
L(x, Œª)
Lagrange function, see Part II
R+, R++
{x ‚ààR | x ‚â•0}, {x ‚ààR | x > 0}
R‚àí
{x ‚ààR | x ‚â§0}
R, R, R
R ‚à™{‚àû}, R ‚à™{‚àí‚àû}, R ‚à™{‚àû, ‚àí‚àû}
S¬µ,L(X)
class of ¬µ-strongly convex functions on X with
L-Lipschitz continuous derivative, see Part I
VarX(v)
supx‚ààX‚ü®v, x‚ü©‚àíinfx‚ààX‚ü®v, x‚ü©, p. 93
X+
dual cone of X, see Part I
1
the vector (1, 1, . . . , 1)
Œª(f, x)
Newton decrement of f at x, p. 16
œÄy
translated Minkowski functional, p. 89
œÅ(t)
‚àít ‚àíln(1 ‚àít), p. 51
‚àÜxnt
Newton direction at x, p. 15
‚àáf
gradient of f
[x, y]
line segment between x and y
]x, y[
open line segment between x and y
‚à•¬∑‚à•1, ‚à•¬∑‚à•2, ‚à•¬∑‚à•‚àû
‚Ñì1-norm, Euclidean norm, maximum norm, see Part I
‚à•¬∑‚à•x
the seminorm

‚ü®¬∑ , f ‚Ä≤‚Ä≤(x)¬∑‚ü©, p. 18
‚à•v‚à•‚àó
x
dual local seminorm sup‚à•w‚à•x‚â§1‚ü®v, w‚ü©, p. 92
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
1
Descent methods
Chapter 14
Descent methods
The most common numerical algorithms for minimization of diÔ¨Äerentiable
functions of several variables are so-called descent algorithms.
A descent
algorithm is an iterative algorithm that from a given starting point gener-
ates a sequence of points with decreasing function values, and the process is
stopped when one has obtained a function value that approximates the min-
imum value good enough according to some criterion. However, there is no
algorithm that works for arbitrary functions; special assumptions about the
function to be minimized are needed to ensure convergence towards the min-
imum point. Convexity is such an assumption, which makes it also possible
in many cases to determine the speed of convergence.
This chapter describes descent methods in general terms, and we exem-
plify with the simplest descent method, the gradient descent method.
14.1
General principles
We shall study the optimization problem
(P)
min f(x)
where f is a function which is deÔ¨Åned and diÔ¨Äerentiable on an open subset
‚Ñ¶of Rn. We assume that the problem has a solution, i.e. that there is an
optimal point ÀÜx ‚àà‚Ñ¶, and we denote the optimal value f(ÀÜx) as fmin. A con-
venient assumption which, according to Corollary 8.1.7 in Part I, guarantees
the existence of a (unique) optimal solution is that f is strongly convex and
has some closed nonempty sublevel set.
Our aim is to generate a sequence x1, x2, x3, . . . of points in ‚Ñ¶from a
given starting point x0 ‚àà‚Ñ¶, with decreasing function values and with the
property that f(xk) ‚Üífmin as k ‚Üí‚àû. In the iteration leading from the
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
2
Descent methods
point xk to the next point xk+1, except when xk is already optimal, one Ô¨Årst
selects a vector vk such that the one-variable function œÜk(t) = f(xk + tvk) is
strictly decreasing at t = 0. Then, a line search is performed along the half-
line xk + tvk, t > 0, and a point xk+1 = xk + hkvk satisfying f(xk+1) < f(xk)
is selected according to speciÔ¨Åc rules.
The vector vk is called the search direction, and the positive number
hk is called the step size. The algorithm is terminated when the diÔ¨Äerence
f(xk) ‚àífmin is less than a given tolerance.
Schematically, we can describe a typical descent algorithm as follows:
Descent algorithm
Given a starting point x ‚àà‚Ñ¶.
Repeat
1. Determine (if f ‚Ä≤(x) Ã∏= 0) a search direction v and a step size h > 0 such
that f(x + hv) < f(x).
2. Update: x:= x + hv.
until stopping criterion is satisÔ¨Åed.
DiÔ¨Äerent strategies for selecting the search direction, diÔ¨Äerent ways to
perform the line search, as well as diÔ¨Äerent stop criteria, give rise to diÔ¨Äerent
algorithms, of course.
Search direction
Permitted search directions in iteration k are vectors vk which satisfy the
inequality
‚ü®f ‚Ä≤(xk), vk‚ü©< 0,
because this ensures that the function œÜk(t) = f(xk + tvk) is decreasing at
the point t = 0, since œÜ‚Ä≤
k(0) = ‚ü®f ‚Ä≤(xk), vk‚ü©. We will study two ways to select
the search direction.
The gradient descent method selects vk = ‚àíf ‚Ä≤(xk), which is a permissible
choice since ‚ü®f ‚Ä≤(xk), vk‚ü©= ‚àí‚à•f ‚Ä≤(xk)‚à•2 < 0. Locally, this choice gives the
fastest decrease in function value.
Newton‚Äôs method assumes that the second derivative exists, and the search
direction at points xk where the second derivative is positive deÔ¨Ånite is
vk = ‚àíf ‚Ä≤‚Ä≤(xk)‚àí1f ‚Ä≤(xk).
This choice is permissible since ‚ü®f ‚Ä≤(xk), vk‚ü©= ‚àí‚ü®f ‚Ä≤(xk), f ‚Ä≤‚Ä≤(xk)‚àí1f ‚Ä≤(xk)‚ü©< 0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
3
Descent methods
3
Line search
Given the search direction vk there are several possible strategies for selecting
the step size hk.
1. Exact line search. The step size hk is determined by minimizing the one-
variable function t ‚Üíf(xk +tvk). This method is used for theoretical studies
of algorithms but almost never in practice due to the computational cost of
performing the one-dimensional minimization.
2. The step size sequence (hk)‚àû
k=1 is given a priori, for example as hk = h or
as hk = h/
‚àö
k + 1 for some positive constant h. This is a simple rule that is
often used in convex optimization.
3. The step size hk at the point xk is deÔ¨Åned as hk = œÅ(xk) for some given
function œÅ. This technique is used in the analysis of Newton‚Äôs method for
self-concordant functions.
4. Armijo‚Äôs rule. The step size hk at the point xk depends on two parameters
Œ±, Œ≤ ‚àà]0, 1[ and is deÔ¨Åned as
hk = Œ≤m,
where m is the smallest nonnegative integer such that the point xk + Œ≤mvk
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
4
Descent methods
lies in the domain of f and satisÔ¨Åes the inequality
(14.1)
f(xk + Œ≤mvk) ‚â§f(xk) + Œ±Œ≤m‚ü®f ‚Ä≤(xk), vk‚ü©.
Such an m certainly exists, since Œ≤n ‚Üí0 as n ‚Üí‚àûand
lim
t‚Üí0
f(xk + tvk) ‚àíf(xk)
t
= ‚ü®f ‚Ä≤(xk), vk‚ü©< Œ± ‚ü®f ‚Ä≤(xk), vk‚ü©.
The number m is determined by simple backtracking: Start with m = 0
and examine whether xk + Œ≤mvk belongs to the domain of f and inequality
(14.1) holds. If not, increase m by 1 and repeat until the conditions are
fulÔ¨Ålled. Figure 14.1 illustrates the process.
Œ≤m
Œ≤2
1
Œ≤
t
f(xk)
f(xk + tvk)
f(xk) + t‚ü®f‚Ä≤(xk), vk‚ü©
f(xk) + Œ±t‚ü®f‚Ä≤(xk), vk‚ü©
Figure 14.1.
Armijo‚Äôs rule: The step size is hk = Œ≤m,
where m is the smallest nonnegative integer such that
f(xk + Œ≤mvk) ‚â§f(xk) + Œ±Œ≤m‚ü®f‚Ä≤(xk), vk‚ü©.
The decrease in iteration k of function value per step size, i.e. the ratio
(f(xk)‚àíf(xk+1))/hk, is for convex functions less than or equal to ‚àí‚ü®f ‚Ä≤(xk), vk‚ü©
for any choice of step size hk. With step size hk selected according to Armijo‚Äôs
rule the same ratio is also ‚â•‚àíŒ±‚ü®f ‚Ä≤(xk), vk‚ü©. With Armijo‚Äôs rule, the decrease
per step size is, in other words, at least Œ± of what the maximum might be.
Typical values of Œ± in practical applications lie in the range between 0.01
and 0.3.
The parameter Œ≤ determines how many backtracking steps are needed.
The larger Œ≤, the more backtracking steps, i.e. the Ô¨Åner the line search. The
parameter Œ≤ is often chosen between 0.1 and 0.8.
Armijo‚Äôs rule exists in diÔ¨Äerent versions and is used in several practical
algorithms.
Stopping criteria
Since the optimum value is generally not known beforehand, it is not pos-
sible to formulate the stopping criterion directly in terms of the minimum.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
5
Descent methods
Intuitively, it seems reasonable that x should be close to the minimum point
if the derivative f ‚Ä≤(x) is comparatively small, and the next theorem shows
that this is indeed the case, under appropriate conditions on the objective
function.
Theorem 14.1.1. Suppose that the function f : ‚Ñ¶‚ÜíR is diÔ¨Äerentiable, ¬µ-
strongly convex and has a minimum at ÀÜx ‚àà‚Ñ¶. Then, for all x ‚àà‚Ñ¶
f(x) ‚àíf(ÀÜx) ‚â§1
2¬µ‚à•f ‚Ä≤(x)‚à•2
and
(i)
‚à•x ‚àíÀÜx‚à•‚â§1
¬µ‚à•f ‚Ä≤(x)‚à•.
(ii)
Proof. Due to the convexity assumption,
(14.2)
f(y) ‚â•f(x) + ‚ü®f ‚Ä≤(x), y ‚àíx‚ü©+ 1
2¬µ‚à•y ‚àíx‚à•2
for all x, y ‚àà‚Ñ¶. The right-hand side of inequality (14.2) is a convex quadratic
function in the variable y, which is minimized by y = x ‚àí¬µ‚àí1f ‚Ä≤(x), and the
minimum is equal to f(x) ‚àí1
2¬µ‚àí1‚à•f ‚Ä≤(x)‚à•2. Hence,
f(y) ‚â•f(x) ‚àí1
2¬µ‚àí1‚à•f ‚Ä≤(x)‚à•2
for all y ‚àà‚Ñ¶, and we obtain the inequality (i) by choosing y as the minimum
point ÀÜx.
Now, replace y with x and x with ÀÜx in inequality (14.2). Since f ‚Ä≤(ÀÜx) = 0,
the resulting inequality becomes
f(x) ‚â•f(ÀÜx) + 1
2¬µ‚à•x ‚àíÀÜx‚à•2,
which combined with inequality (i) gives us inequality (ii).
We now return to the descent algorithm and our discussion of the the
stopping criterion. Let
S = {x ‚àà‚Ñ¶| f(x) ‚â§f(x0)},
where x0 is the selected starting point, and assume that the sublevel set S
is convex and that the objective function f is ¬µ-strongly convex on S. All
the points x1, x2, x3, . . . that are generated by the descent algorithm will of
course lie in S since the function values are decreasing. Therefore, it follows
from Theorem 14.1.1 that f(xk) < fmin + œµ if ‚à•f ‚Ä≤(xk)‚à•< (2¬µœµ)1/2.
As a stopping criterion, we can thus use the condition
‚à•f ‚Ä≤(xk)‚à•‚â§Œ∑,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
6
Descent methods
6
which guarantees that f(xk) ‚àífmin ‚â§Œ∑2/2¬µ and that ‚à•xk ‚àíÀÜx‚à•‚â§Œ∑/¬µ. A
problem here is that the convexity constant ¬µ is known only in rare cases.
So the stopping condition ‚à•f ‚Ä≤(xk)‚à•‚â§Œ∑ can in general not be used to give
precise bounds on f(xk) ‚àífmin. But Theorem 14.1.1 veriÔ¨Åes our intuitive
feeling that the diÔ¨Äerence between f(x) and fmin is small if the gradient of f
at x is small enough.
Convergence rate
Let us say that a convergent sequence x0, x1, x2, . . . of points with limit ÀÜx
converges at least linearly if there is a constant c < 1 such that
(14.3)
‚à•xk+1 ‚àíÀÜx‚à•‚â§c‚à•xk ‚àíÀÜx‚à•
for all k, and that the convergence is at least quadratic if there is a constant
C such that
(14.4)
‚à•xk+1 ‚àíÀÜx‚à•‚â§C‚à•xk ‚àíÀÜx‚à•2
for all k.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
7
Descent methods
We also say that the convergence is no better than linear and no better
than quadratic if
lim
k‚Üí‚àû
‚à•xk+1 ‚àíÀÜx‚à•
‚à•xk ‚àíÀÜx‚à•Œ± > 0
for Œ± = 1 and Œ± = 2, respectively.
Note that inequality (14.3) implies that the sequence (xk)‚àû
0 converges to
ÀÜx, because it follows by induction that
‚à•xk ‚àíÀÜx‚à•‚â§ck‚à•x0 ‚àíÀÜx‚à•
for all k.
Similarly, inequality (14.4) implies that the sequence (xk)‚àû
0 convergences
to ÀÜx if the starting point x0 satisÔ¨Åes the condition ‚à•x0 ‚àíÀÜx‚à•< C‚àí1, because
we now have
‚à•xk ‚àíÀÜx‚à•‚â§C‚àí1
C‚à•x0 ‚àíÀÜx‚à•
2k
for all k.
If an iterative method, when applied to functions in a given class of
functions, always generates sequences that are at least linearly (quadratic)
convergent and there is a sequence which does not converge better than
linearly (quadratic), then we say that the method is linearly (quadratic)
convergent for the function class in question.
14.2
The gradient descent method
In this section we analyze the gradient descent algorithm with constant step
size. The iterative formulation of the variant of the algorithm that we have
in mind looks like this:
Gradient descent algorithm with constant step size
Given a starting point x and a step size h.
Repeat
1. Compute the search direction v = ‚àíf ‚Ä≤(x).
2. Update: x:= x + hv.
until stopping criterion is satisÔ¨Åed.
The algorithm converges linearly to the minimum point for strongly con-
vex functions with Lipschitz continuous derivatives provided that the step
size is small enough and the starting point is chosen suÔ¨Éciently close to the
minimum point.
This is the main content of the following theorem (and
Example 14.2.1).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
8
Descent methods
Theorem 14.2.1. Let f be a function with a local minimum point ÀÜx, and
suppose that there is an open neighborhood U of ÀÜx such that the restriction f|U
of f to U is ¬µ-strongly convex and diÔ¨Äerentiable with a Lipschitz continuous
derivative and Lipschitz constant L.
The gradient descent algorithm with
constant step size h then converges at least linearly to ÀÜx provided that the
step size is suÔ¨Éciently small and the starting point x0 lies suÔ¨Éciently close
to ÀÜx.
More precisely: If the ball centered at ÀÜx and with radius equal to ‚à•x0 ‚àíÀÜx‚à•
lies in U and if h ‚â§¬µ/L2, and (xk)‚àû
0 is the sequence of points generated by
the algorithm, then xk lies in U and
‚à•xk+1 ‚àíÀÜx‚à•‚â§c‚à•xk ‚àíÀÜx‚à•,
for all k, where c =

1 ‚àíh¬µ.
Proof. Suppose inductively that the points x0, x1, . . . , xk lie in U and that
‚à•xk ‚àíÀÜx‚à•‚â§‚à•x0 ‚àíÀÜx‚à•. Since the restriction f|U is assumed to be ¬µ-strongly
convex and since f ‚Ä≤(ÀÜx) = 0,
‚ü®f ‚Ä≤(xk), xk ‚àíÀÜx‚ü©= ‚ü®f ‚Ä≤(xk) ‚àíf ‚Ä≤(ÀÜx), xk ‚àíÀÜx‚ü©‚â•¬µ‚à•xk ‚àíÀÜx‚à•2
according to Theorem 7.3.1 in Part I, and since the derivative is assumed to
be Lipschitz continuous, we also have the inequality
‚à•f ‚Ä≤(xk)‚à•= ‚à•f ‚Ä≤(xk) ‚àíf ‚Ä≤(ÀÜx)‚à•‚â§L‚à•xk ‚àíÀÜx‚à•.
By combining these two inequalities, we obtain the inequality
‚ü®f ‚Ä≤(xk), xk ‚àíÀÜx‚ü©‚â•¬µ‚à•xk ‚àíÀÜx‚à•2 = ¬µ
2‚à•xk ‚àíÀÜx‚à•2 + ¬µ
2‚à•xk ‚àíÀÜx‚à•2
‚â•¬µ
2‚à•xk ‚àíÀÜx‚à•2 +
¬µ
2L2‚à•f ‚Ä≤(xk)‚à•2.
Our next point xk+1 = xk ‚àíhf ‚Ä≤(xk) therefore satisÔ¨Åes the inequality
‚à•xk+1 ‚àíÀÜx‚à•2 = ‚à•xk ‚àíhf ‚Ä≤(xk) ‚àíÀÜx‚à•2 = ‚à•(xk ‚àíÀÜx) ‚àíhf ‚Ä≤(xk)‚à•2
= ‚à•xk ‚àíÀÜx‚à•2 ‚àí2h‚ü®f ‚Ä≤(xk), xk ‚àíÀÜx‚ü©+ h2‚à•f ‚Ä≤(xk)‚à•2
‚â§‚à•xk ‚àíÀÜx‚à•2 ‚àíh¬µ‚à•xk ‚àíÀÜx‚à•2 ‚àíh ¬µ
L2‚à•f ‚Ä≤(xk)‚à•2 + h2‚à•f ‚Ä≤(xk)‚à•2
= (1 ‚àíh¬µ)‚à•xk ‚àíÀÜx‚à•2 + h

h ‚àí¬µ
L2

‚à•f ‚Ä≤(xk)‚à•2.
Hence, h ‚â§¬µ/L2 implies that ‚à•xk+1 ‚àíÀÜx‚à•2 ‚â§(1 ‚àíh¬µ)‚à•xk ‚àíÀÜx‚à•2, and
this proves that the inequality of the theorem holds with c =

1 ‚àíh¬µ < 1,
and that the induction hypothesis is satisÔ¨Åed by the point xk+1, too, since
it lies closer to ÀÜx than the point xk does. So the gradient descent algorithm
converges at least linearly for f under the given conditions on h and x0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
9
Descent methods
9
We can obtain a slightly sharper result for ¬µ-strongly convex functions
that are deÔ¨Åned on the whole Rn and have a Lipschitz continuous derivative.
Theorem 14.2.2. Let f be a function in the class S¬µ,L(Rn). The gradient
descent method, with arbitrary starting point x0 and constant step size h,
generates a sequence (xk)‚àû
0
of points that converges at least linearly to the
function‚Äôs minimum point ÀÜx, if
0 < h ‚â§
2
¬µ + L.
More precisely,
(14.5)
‚à•xk ‚àíÀÜx‚à•‚â§

1 ‚àí2h¬µL
¬µ + L
k/2
‚à•x0 ‚àíÀÜx‚à•.
Moreover, if h =
2
¬µ + L then
‚à•xk ‚àíÀÜx‚à•‚â§
Q ‚àí1
Q + 1
k
‚à•x0 ‚àíÀÜx‚à•
and
(14.6)
f(xk) ‚àífmin ‚â§L
2
Q ‚àí1
Q + 1
2k
‚à•x0 ‚àíÀÜx‚à•2,
(14.7)
where Q = L/¬µ is the condition number of the function class S¬µ,L(Rn).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
10
Descent methods
Proof. The function f has a unique minimum point ÀÜx, according to Corollary
8.1.7 in Part I, and
‚à•xk+1 ‚àíÀÜx‚à•2 = ‚à•xk ‚àíÀÜx‚à•2 ‚àí2h‚ü®f ‚Ä≤(xk), xk ‚àíÀÜx‚ü©+ h2‚à•f ‚Ä≤(xk)‚à•2,
just as in the proof of Theorem 14.2.1. Since f ‚Ä≤(ÀÜx) = 0, it now follows from
Theorem 7.4.4 in Part I (with x = ÀÜx and v = xk ‚àíÀÜx) that
‚ü®f ‚Ä≤(xk), xk ‚àíÀÜx‚ü©‚â•
¬µL
¬µ + L‚à•xk ‚àíÀÜx‚à•2 +
1
¬µ + L‚à•f ‚Ä≤(xk)‚à•2,
which inserted in the above equation results in the inequality
‚à•xk+1 ‚àíÀÜx‚à•2 ‚â§

1 ‚àí2h¬µL
¬µ + L

‚à•xk ‚àíÀÜx‚à•2 + h

h ‚àí
2
¬µ + L

‚à•f ‚Ä≤(xk)‚à•2.
So if h ‚â§2/(¬µ + L), then
‚à•xk+1 ‚àíÀÜx‚à•‚â§

1 ‚àí2h¬µL
¬µ + L
1/2
‚à•xk ‚àíÀÜx‚à•,
and inequality (14.5) now follows by iteration.
The particular choice of h = 2(¬µ + L)‚àí1 in inequality (14.5) gives us
inequality (14.6), and the last inequality (14.7) follows from inequality (14.6)
and Theorem 1.1.2 in Part I, since f ‚Ä≤(ÀÜx) = 0.
The rate of convergence in Theorems 14.2.1 and 14.2.2 depends on the
condition number Q ‚â•1. The smaller the Q, the faster the convergence.
The constants ¬µ and L, and hence the condition number Q, are of course
rarely known in practical examples, so the two theorems have a qualitative
character and can rarely be used to predict the number of iterations required
to achieve a certain precision.
Our next example shows that inequality (14.6) can not be sharpened.
Example 14.2.1. Consider the function
f(x) = 1
2(¬µx2
1 + Lx2
2),
where 0 < ¬µ ‚â§L. This function belongs to the class S¬µ,L(R2), f ‚Ä≤(x) =
(¬µx1, Lx2), and ÀÜx = (0, 0) is the minimum point.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
11
Descent methods
1
5
10
15
x1
1
x2
Figure 14.2. Some level curves for the function f(x) = 1
2(x2
1 + 16x2
2)
and the progression of the gradient descent algorithm with x(0) = (16, 1)
as starting point. The function‚Äôs condition number Q is equal to 16, so
the convergence to the minimum point (0, 0) is relatively slow.
The
distance from the generated point to the origin is improved by a factor
of 15/17 in each iteration.
The gradient descent algorithm with constant step size h = 2(¬µ + L)‚àí1,
starting point x(0) = (L, ¬µ), and Œ± = Q‚àí1
Q+1 proceeds as follows
x(0) = (L, ¬µ)
f ‚Ä≤(x(0)) = (¬µL, ¬µL)
x(1) = x(0) ‚àíhf ‚Ä≤(x(0)) = Œ±(L, ‚àí¬µ)
f ‚Ä≤(x(1)) = Œ±(¬µL, ‚àí¬µL)
x(2) = x(1) ‚àíhf ‚Ä≤(x(1)) = Œ±2(L, ¬µ)
...
x(k) = Œ±k(L, (‚àí1)k¬µ)
Consequently,
‚à•x(k) ‚àíÀÜx‚à•= Œ±k
L2 + ¬µ2 = Œ±k‚à•x(0) ‚àíÀÜx‚à•,
so inequality (14.6) holds with equality in this case. Cf. with Ô¨Ågure 14.2.
Finally, it is worth noting that 2(¬µ+L)‚àí1 coincides with the step size that
we would obtain if we had used exact line search in each iteration step.
The gradient descent algorithm is not invariant under aÔ¨Éne coordinate
changes. The speed of convergence can thus be improved by Ô¨Årst making a
coordinate change that reduces the condition number.
Example 14.2.2. We continue with the function f(x) = 1
2(¬µx2
1 +Lx2
2) in the
previous example. Make the change of variables y1 = ‚àö¬µ x1, y2 =
‚àö
L x2,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
12
Descent methods
and deÔ¨Åne the function g by
g(y) = f(x) = 1
2(y2
1 + y2
2).
The condition number Q of the function g is equal to 1, so the gradient
descent algorithm, started from an arbitrary point y(0), hits the minimum
point (0, 0) after just one iteration.
The gradient descent algorithm converges too slowly to be of practical use
in realistic problems. In the next chapter we shall therefore study in detail
a more eÔ¨Écient method for optimization, Newton‚Äôs method.
Exercises
14.1 Perform three iterations of the gradient descent algorithm with (1, 1) as
starting point on the minimization problem
min x2
1 + 2x2
2.
14.2 Let X = {x ‚ààR2 | x1 > 1}, let x(0) = (2, 2), and let f : X ‚ÜíR be the
function deÔ¨Åned by f(x) = 1
2x2
1 + 1
2x2
2.
a) Show that the sublevel set {x ‚ààX | f(x) ‚â§f(x(0))} is not closed.
b) Obviously, fmin = inf f(x) =
1
2, but show that the gradient descent
method, with x(0) as starting point and with line search according to Armijo‚Äôs
rule with parameters Œ± ‚â§1
2 and Œ≤ < 1, generates a sequence x(k) = (ak, ak),
k = 0, 1, 2, . . . , of points that converges to the point (1, 1). So the function
values f(x(k)) converge to 1 and not to fmin.
[Hint: Show that ak+1 ‚àí1 ‚â§(1 ‚àíŒ≤)(ak ‚àí1) for all k.]
14.3 Suppose that the gradient descent algorithm with constant step size con-
verges to the point ÀÜx when applied to a continuously diÔ¨Äerentiable function
f. Prove that ÀÜx is a stationary point of f, i.e. that f‚Ä≤(ÀÜx) = 0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
13
Newton's method
Chapter 15
Newton‚Äôs method
In Newton‚Äôs method for minimizing a function f, the search direction at
a point x is determined by minimizing the function‚Äôs Taylor polynomial of
degree two, i.e. the polynomial
P(v) = f(x) + Df(x)[v] + 1
2D2f(x)[v, v] = f(x) + ‚ü®f ‚Ä≤(x), v‚ü©+ 1
2‚ü®v, f ‚Ä≤‚Ä≤(x)v‚ü©,
and since P ‚Ä≤(v) = f ‚Ä≤(x) + f ‚Ä≤‚Ä≤(x)v, we obtain the minimizing search vector as
a solution to the equation
f ‚Ä≤‚Ä≤(x)v = ‚àíf ‚Ä≤(x).
Each iteration is of course more laborious in Newton‚Äôs method than in
the gradient descent method, since we need to compute the second derivative
and solve a quadratic equation to determine the search vector. However, as
we shall see, this is more than compensated by a much faster convergence to
the minimum value.
15.1
Newton decrement and Newton direc-
tion
Since the search directions in Newton‚Äôs method are obtained by minimizing
quadratic polynomials, we start by examining when such polynomials have
minimum values, and since convexity is a necessary condition for quadratic
polynomials to be bounded below, we can restrict ourself to the study of
convex quadratic polynomials.
Theorem 15.1.1. A quadratic polynomial
P(v) = 1
2‚ü®v, Av‚ü©+ ‚ü®b, v‚ü©+ c
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
14
Newton's method
14
in n variables, where A is a positive semideÔ¨Ånite symmetric operator, is
bounded below on Rn if and only if the equation
(15.1)
Av = ‚àíb
has a solution.
The polynomial has a minimum if it is bounded below, and ÀÜv is a minimum
point if and only if AÀÜv = ‚àíb.
If ÀÜv is a minimum point of the polynomial P, then
(15.2)
P(v) ‚àíP(ÀÜv) = 1
2‚ü®v ‚àíÀÜv, A(v ‚àíÀÜv)‚ü©
for all v ‚ààRn.
If ÀÜv1 and ÀÜv2 are two minimum points, then ‚ü®ÀÜv1, AÀÜv1‚ü©= ‚ü®ÀÜv2, AÀÜv2‚ü©.
Remark. Another way to state that equation (15.1) has a solution is to say
that the vector ‚àíb, and of course also the vector b, belongs to the range of
the operator A. But the range of an operator on a Ô¨Ånite dimensional space is
equal to the orthogonal complement of the null space of the operator. Hence,
equation (15.1) is solvable if and only if
Av = 0 ‚áí‚ü®b, v‚ü©= 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
15
Newton's method
Proof. First suppose that equation (15.1) has no solution.
Then, by the
remark above there exists a vector v such that Av = 0 and ‚ü®b, v‚ü©Ã∏= 0. It
follows that
P(tv) = 1
2‚ü®v, Av‚ü©t2 + ‚ü®b, v‚ü©t + c = ‚ü®b, v‚ü©t + c
for all t ‚ààR, and since the t-coeÔ¨Écient is nonzero, we conclude that the
polynomial P(t) is unbounded below.
Next suppose that AÀÜv = ‚àíb. Then
P(v) ‚àíP(ÀÜv) = 1
2(‚ü®v, Av‚ü©‚àí‚ü®ÀÜv, AÀÜv‚ü©) + ‚ü®b, v‚ü©‚àí‚ü®b, ÀÜv‚ü©
= 1
2(‚ü®v, Av‚ü©‚àí‚ü®ÀÜv, AÀÜv‚ü©) ‚àí‚ü®AÀÜv, v‚ü©+ ‚ü®AÀÜv, ÀÜv‚ü©
= 1
2(‚ü®v, Av‚ü©+ ‚ü®ÀÜv, AÀÜv‚ü©‚àí‚ü®AÀÜv, v‚ü©‚àí‚ü®ÀÜv, Av‚ü©)
= 1
2‚ü®v ‚àíÀÜv, A(v ‚àíÀÜv)‚ü©‚â•0
for all v ‚ààRn. This proves that the polynomial P(t) is bounded below, that
ÀÜv is a minimum point, and that the equality (15.2) holds.
Since every positive semideÔ¨Ånite symmetric operator A has a unique pos-
itive semideÔ¨Ånite symmetric square root A1/2, we can rewrite equality (15.2)
as follows:
P(v) = P(ÀÜv) + 1
2‚ü®A1/2(v ‚àíÀÜv), A1/2(v ‚àíÀÜv)‚ü©= P(ÀÜv) + 1
2‚à•A1/2(v ‚àíÀÜv)‚à•2.
If v is another minimum point of P, then P(v) = P(ÀÜv), and it follows that
A1/2(v ‚àíÀÜv) = 0.
Consequently, A(v ‚àíÀÜv) = A1/2(A1/2(v ‚àíÀÜv)) = 0, i.e. Av = AÀÜv = ‚àíb. Hence,
every minimum point of P is obtained as a solution to equation (15.1).
Finally, if ÀÜv1 and ÀÜv2 are two minimum points of the polynomial, then
AÀÜv1 = AÀÜv2 (= ‚àíb), and it follows that ‚ü®ÀÜv1, AÀÜv1‚ü©= ‚ü®ÀÜv1, AÀÜv2‚ü©= ‚ü®AÀÜv1, ÀÜv2‚ü©=
‚ü®AÀÜv2, ÀÜv2‚ü©= ‚ü®ÀÜv2, AÀÜv2‚ü©.
The problem to solve a convex quadratic optimization problem in Rn is
thus reduced to solving a quadratic system of linear equations in n variables
(with a positive semideÔ¨Ånite coeÔ¨Écient matrix), which is a rather trivial
numerical problem that can be performed with O(n3) arithmetic operations.
We are now ready to deÔ¨Åne the main ingredients of Newton‚Äôs method.
DeÔ¨Ånition. Let f : X ‚ÜíR be a twice diÔ¨Äerentiable function with an open
subset X of Rn as domain, and let x ‚ààX be a point where the second
derivative f ‚Ä≤‚Ä≤(x) is positive semideÔ¨Ånite.
By a Newton direction ‚àÜxnt of the function f at the point x we mean a
solution v to the equation
f ‚Ä≤‚Ä≤(x)v = ‚àíf ‚Ä≤(x).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
16
Newton's method
Remark. It follows from the remark after Theorem 15.1.1 that there exists a
Newton direction at x if and only if
f ‚Ä≤‚Ä≤(x)v = 0 ‚áí‚ü®f ‚Ä≤(x), v‚ü©= 0.
The nonexistence of Newton directions at x is thus equivalent to the existence
of a vector w such that f ‚Ä≤‚Ä≤(x)w = 0 and ‚ü®f ‚Ä≤(x), w‚ü©= 1.
The Newton direction ‚àÜxnt is of course uniquely determined as
‚àÜxnt = ‚àíf ‚Ä≤‚Ä≤(x)‚àí1f ‚Ä≤(x)
if the second derivative f ‚Ä≤‚Ä≤(x) is non-singular, i.e. positive deÔ¨Ånite.
A Newton direction ‚àÜxnt is according to Theorem 15.1.1, whenever it
exists, a minimizing vector for the Taylor polynomial
P(v) = f(x) + ‚ü®f ‚Ä≤(x), v‚ü©+ 1
2‚ü®v, f ‚Ä≤‚Ä≤(x)v‚ü©,
and the diÔ¨Äerence P(0) ‚àíP(‚àÜxnt) is given by
P(0) ‚àíP(‚àÜxnt) = 1
2‚ü®0 ‚àí‚àÜxnt, f ‚Ä≤‚Ä≤(x)(0 ‚àí‚àÜxnt)‚ü©= 1
2‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x)‚àÜxnt‚ü©.
Using the Taylor approximation f(x + v) ‚âàP(v), we conclude that
f(x) ‚àíf(x + ‚àÜxnt) ‚âàP(0) ‚àíP(‚àÜxnt) = 1
2‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x)‚àÜxnt‚ü©.
Hence, 1
2‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x)‚àÜxnt‚ü©is (for small ‚àÜxnt) an approximation of the de-
crease in function value which is obtained by replacing f(x) with f(x+‚àÜxnt).
This motivates our next deÔ¨Ånition.
DeÔ¨Ånition. The Newton decrement Œª(f, x) of the function f at the point x
is a quantity deÔ¨Åned as
Œª(f, x) =

‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x)‚àÜxnt‚ü©
if f has a Newton direction ‚àÜxnt at x, and as
Œª(f, x) = +‚àû
if there is no Newton direction at x.
Note that the deÔ¨Ånition is independent of the choice of Newton direction
at x in case of nonuniqueness of Newton direction. This follows immediately
from the last statement in Theorem 15.1.1.
In terms of the Newton decrement, we thus have the following approxi-
mation
f(x) ‚àíf(x + ‚àÜxnt) ‚âà1
2Œª(f, x)2
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
17
Newton's method
17
for small values of ‚àÜxnt.
By deÔ¨Ånition f ‚Ä≤‚Ä≤(x)‚àÜxnt = ‚àíf ‚Ä≤(x), so it follows that the Newton decre-
ment, whenever Ô¨Ånite, can be computed using the formula
Œª(f, x) =

‚àí‚ü®‚àÜxnt, f ‚Ä≤(x)‚ü©.
In particular, if x is a point where the second derivative is positive deÔ¨Ånite,
then
Œª(f, x) =

‚ü®f ‚Ä≤‚Ä≤(x)‚àí1f ‚Ä≤(x), f ‚Ä≤(x)‚ü©.
Example 15.1.1. The convex one-variable function
f(x) = ‚àíln x, x > 0
has Newton decrement
Œª(f, x) =

‚ü®x2(‚àíx‚àí1), ‚àíx‚àí1‚ü©=

(‚àíx) ¬∑ (‚àíx‚àí1) = 1
at all points x > 0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
18
Newton's method
At points x with a Newton direction it is also possible to express the
Newton decrement in terms of the Euclidean norm ‚à•¬∑‚à•as follows, by using
the fact that f ‚Ä≤‚Ä≤(x) har a positive deÔ¨Ånite symmetric square root:
Œª(f, x) =

‚ü®f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt, f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt‚ü©= ‚à•f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt‚à•.
The improvement in function value obtained by taking a step in the Newton
direction ‚àÜxnt is thus proportional to ‚à•f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt‚à•2 and not to ‚à•‚àÜxnt‚à•2,
a fact which motivates our introduction of the following seminorm.
DeÔ¨Ånition. Let f : X ‚ÜíR be a twice diÔ¨Äerentiable function with an open
subset X of Rn as domain, and let x ‚ààX be a point where the second
derivative f ‚Ä≤‚Ä≤(x) is positive semideÔ¨Ånite.
The function ‚à•¬∑‚à•x : Rn ‚ÜíR+,
deÔ¨Åned by
‚à•v‚à•x =

‚ü®v, f ‚Ä≤‚Ä≤(x)v‚ü©= ‚à•f ‚Ä≤‚Ä≤(x)1/2v‚à•
for all v ‚ààRn, is called the local seminorm at x of the function f.
It is easily veriÔ¨Åed that ‚à•¬∑‚à•x is indeed a seminorm on Rn. Since
{v ‚ààRn | ‚à•v‚à•x = 0} = N(f ‚Ä≤‚Ä≤(x)),
where N(f ‚Ä≤‚Ä≤(x)) is the null space of f ‚Ä≤‚Ä≤(x), ‚à•¬∑‚à•x is a norm if and only if the
positive deÔ¨Ånite second derivative f ‚Ä≤‚Ä≤(x) is nonsingular, i.e. positive deÔ¨Ånite.
At points x with a Newton direction, we now have the following simple
relation between direction and decrement:
Œª(f, x) = ‚à•‚àÜxnt‚à•x.
Example 15.1.2. Let us study the Newton decrement Œª(f, x) when f is a
convex quadratic polynomial, i.e. a function of the form
f(x) = 1
2‚ü®x, Ax‚ü©+ ‚ü®b, x‚ü©+ c
with a positive semideÔ¨Ånite operator A. We have f ‚Ä≤(x) = Ax + b, f ‚Ä≤‚Ä≤(x) = A
and ‚à•v‚à•x =

‚ü®v, Av‚ü©, so the seminorms ‚à•¬∑‚à•x are the same for all x ‚ààRn.
If ‚àÜxnt is a Newton direction of f at x, then
A‚àÜxnt = ‚àí(Ax + b),
by deÔ¨Ånition, and it follows that A(x + ‚àÜxnt) = ‚àíb. This implies that the
function f is bounded below, according to Theorem 15.1.1.
So if f is not bounded below, then there are no Newton directions at any
point x, which means that Œª(f, x) = +‚àûfor all x.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
19
Newton's method
Conversely, assume that f is bounded below. Then there exists a vector
v0 such that Av0 = ‚àíb, and it follows that
f ‚Ä≤‚Ä≤(x)(v0 ‚àíx) = Av0 ‚àíAx = ‚àíb ‚àíAx = ‚àíf ‚Ä≤(x).
The vector v0 ‚àíx is in other words a Newton direction of f at the point x,
which means that the Newton decrement Œª(f, x) is Ô¨Ånite at all points x and
is given by
Œª(f, x) = ‚à•v0 ‚àíx‚à•x.
If f is bounded below without being constant, then necessarily A Ã∏= 0 and
we can choose a vector w such that ‚à•w‚à•x =

‚ü®w, Aw‚ü©= 1. Let xk = kw+v0,
where k is a positive number. Then
Œª(f, xk) = ‚à•v0 ‚àíxk‚à•xk = k‚à•w‚à•xk = k,
and we conclude from this that supx‚ààRn Œª(f, x) = +‚àû.
For constant functions f, the case A = 0, b = 0, we have ‚à•v‚à•x = 0 for all
x and v, and consequently Œª(f, x) = 0 for all x.
In summary, we have obtained the following result:
The Newton decrement of downwards unbounded convex quadratic func-
tions (which includes all non-constant aÔ¨Éne functions) is inÔ¨Ånite at all points.
The Newton decrement of downwards bounded convex quadratic functions
f is Ô¨Ånite at all points, but supx Œª(f, x) = ‚àû, unless the function is con-
stant.
We shall give an alternative characterization of the Newton decrement,
and for this purpose we need the following useful inequality.
Theorem 15.1.2. Suppose Œª(f, x) < ‚àû. Then
|‚ü®f ‚Ä≤(x), v‚ü©| ‚â§Œª(f, x)‚à•v‚à•x
for all v ‚ààRn.
Proof. Since Œª(f, x) is assumed to be Ô¨Ånite, there exists a Newton direction
‚àÜxnt at x, and by deÔ¨Ånition, f ‚Ä≤‚Ä≤(x)‚àÜxnt = ‚àíf ‚Ä≤(x).
Using the Cauchy‚Äì
Schwarz inequality we now obtain:
|‚ü®f ‚Ä≤(x), v‚ü©| = |‚ü®f ‚Ä≤‚Ä≤(x)‚àÜxnt, v‚ü©| = |‚ü®f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt, f ‚Ä≤‚Ä≤(x)1/2v‚ü©|
‚â§‚à•f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt‚à•‚à•f ‚Ä≤‚Ä≤(x)1/2v‚à•= Œª(f, x)‚à•v‚à•x.
Theorem 15.1.3. Assume as before that x is a point where the second deriva-
tive f ‚Ä≤‚Ä≤(x) is positive semideÔ¨Ånite. Then
Œª(f, x) = sup
‚à•v‚à•x‚â§1
‚ü®f ‚Ä≤(x), v‚ü©.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
20
Newton's method
20
Proof. First assume that Œª(f, x) < ‚àû. Then
‚ü®f ‚Ä≤(x), v‚ü©‚â§Œª(f, x)
for all vectors v such that ‚à•v‚à•x ‚â§1, according to Theorem 15.1.2. In the case
Œª(f, x) = 0 the above inequality holds with equality for v = 0, so assume
that Œª(f, x) > 0. For v = ‚àíŒª(f, x)‚àí1‚àÜxnt we then have ‚à•v‚à•x = 1 and
‚ü®f ‚Ä≤(x), v‚ü©= ‚àíŒª(f, x)‚àí1‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©= Œª(f, x).
This proves that Œª(f, x) = sup‚à•v‚à•x‚â§1‚ü®f ‚Ä≤(x), v‚ü©for Ô¨Ånite Newton decrements
Œª(f, x).
Next assume that Œª(f, x) = +‚àû, i.e. that no Newton direction exists at
x. By the remark after the deÔ¨Ånition of Newton direction, there exists a
vector w such that f ‚Ä≤‚Ä≤(x)w = 0 and ‚ü®f ‚Ä≤(x), w‚ü©= 1. It follows that ‚à•tw‚à•x =
t‚à•w‚à•x = t

‚ü®w, f ‚Ä≤‚Ä≤(x)w‚ü©= 0 ‚â§1 and ‚ü®f ‚Ä≤(x), tw‚ü©= t for all positive numbers
t, and this implies that sup‚à•v‚à•x‚â§1‚ü®f ‚Ä≤(x), v‚ü©= +‚àû= Œª(f, x).
We sometimes need to compare ‚à•‚àÜxnt‚à•, ‚à•f ‚Ä≤(x)‚à•and Œª(f, x), and we can
do so using the following theorem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
21
Newton's method
Theorem 15.1.4. Let Œªmin and Œªmax denote the smallest and the largest eigen-
value of the second derivative f ‚Ä≤‚Ä≤(x), assumed to be positive semideÔ¨Ånite, and
suppose that the Newton decrement Œª(f, x) is Ô¨Ånite. Then
Œª1/2
min‚à•‚àÜxnt‚à•‚â§Œª(f, x) ‚â§Œª1/2
max‚à•‚àÜxnt‚à•
and
Œª1/2
minŒª(f, x) ‚â§‚à•f ‚Ä≤(x)‚à•‚â§Œª1/2
maxŒª(f, x).
Proof. Let A be an arbitrary positive semideÔ¨Ånite operator on Rn with small-
est and largest eigenvalue ¬µmin and ¬µmax respectively. Then
¬µmin‚à•v‚à•‚â§‚à•Av‚à•‚â§¬µmax‚à•v‚à•
for all vectors v.
Since Œª1/2
min and Œª1/2
max are the smallest and the largest eigenvalues of the
operator f ‚Ä≤‚Ä≤(x)1/2, we obtain the two inequalities of our theorem by applying
the general inequality to A = f ‚Ä≤‚Ä≤(x)1/2 and v = ‚àÜxnt, and to A = f ‚Ä≤‚Ä≤(x)1/2
and v = f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt, noting that ‚à•f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt‚à•= Œª(f, x) and that
‚à•f ‚Ä≤‚Ä≤(x)1/2(f ‚Ä≤‚Ä≤(x)1/2‚àÜxnt)‚à•= ‚à•f ‚Ä≤‚Ä≤(x)‚àÜxnt‚à•= ‚à•f ‚Ä≤(x)‚à•.
Theorem 15.1.4 is a local result, but if the function f is ¬µ-strongly convex,
then Œªmin ‚â•¬µ, and if the norm of the second derivative is bounded by
some constant M, then Œªmax = ‚à•f ‚Ä≤‚Ä≤(x)‚à•‚â§M for all x in the domain of f.
Therefore, we get the following corollary to Theorem 15.1.4.
Corollary
15.1.5. If f : X ‚ÜíR is a twice diÔ¨Äerentiable ¬µ-strongly convex
function, then
¬µ1/2‚à•‚àÜxnt‚à•‚â§Œª(f, x) ‚â§¬µ‚àí1/2‚à•f ‚Ä≤(x)‚à•
for all x ‚ààX. If moreover ‚à•f ‚Ä≤‚Ä≤(x)‚à•‚â§M, then
M ‚àí1/2‚à•f ‚Ä≤(x)‚à•‚â§Œª(f, x) ‚â§M 1/2‚à•‚àÜxnt‚à•.
The distance from an arbitrary point to the minimum point of a strongly
convex function with bounded second derivative can be estimated using the
Newton decrement, because we have the following result.
Theorem 15.1.6. Let f : X ‚ÜíR be a ¬µ-strongly convex function, and sup-
pose that f has a minimum at the point ÀÜx and that ‚à•f ‚Ä≤‚Ä≤(x)‚à•‚â§M for all
x ‚ààX. Then
f(x) ‚àíf(ÀÜx) ‚â§M
2¬µŒª(f, x)2
and
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
22
Newton's method
‚à•x ‚àíÀÜx‚à•‚â§
‚àö
M
¬µ Œª(f, x).
Proof. The theorem follows by combining Theorem 14.1.1 with the estimate
‚à•f ‚Ä≤(x)‚à•‚â§M 1/2Œª(f, x) from Corollary 15.1.5.
The Newton decrement is invariant under surjective aÔ¨Éne coordinate
transformations. A slightly more general result is the following.
Theorem 15.1.7. Let f be a twice diÔ¨Äerentiable function whose domain ‚Ñ¶is
a subset of Rn, let A: Rm ‚ÜíRn be an aÔ¨Éne map, and let g = f ‚ó¶A. Let
furthermore x = Ay be a point in ‚Ñ¶, and suppose that the second derivative
f ‚Ä≤‚Ä≤(x) is positive semideÔ¨Ånite. The second derivative g‚Ä≤‚Ä≤(y) is then positive
semideÔ¨Ånite, and the Newton decrements of the two functions g and f satisfy
the inequality
Œª(g, y) ‚â§Œª(f, x).
Equality holds if the aÔ¨Éne map A is surjective.
Proof. The aÔ¨Éne map can be written as Ay = Cy + b, where C is a linear
map and b is a vector, and the chain rule gives us the identities
‚ü®g‚Ä≤(y), w‚ü©= ‚ü®f ‚Ä≤(x), Cw‚ü©
and
‚ü®w, g‚Ä≤‚Ä≤(y)w‚ü©= ‚ü®Cw, f ‚Ä≤‚Ä≤(x)Cw‚ü©
for arbitrary vectors w in Rm. It follows from the latter identity that the
second derivative g‚Ä≤‚Ä≤(y) is positive semideÔ¨Ånite if f ‚Ä≤‚Ä≤(x) is so, and that
‚à•w‚à•y = ‚à•Cw‚à•x.
An application of Theorem 15.1.3 now gives
Œª(g, y) = sup
‚à•w‚à•y‚â§1
‚ü®g‚Ä≤(y), w‚ü©=
sup
‚à•Cw‚à•x‚â§1
‚ü®f ‚Ä≤(x), Cw‚ü©‚â§sup
‚à•v‚à•x‚â§1
‚ü®f ‚Ä≤(x), v‚ü©= Œª(f, x).
If the aÔ¨Éne map A is surjective, then C is a surjective linear map, and
hence v = Cw runs through all of Rn as w runs through Rm. In this case,
the only inequality in the above chain of equalities and inequalities becomes
an equality, which means that Œª(g, y) = Œª(f, x).
15.2
Newton‚Äôs method
The algorithm
Newton‚Äôs method for minimizing a twice diÔ¨Äerentiable function f is a descent
method, in which the search direction in each iteration is given by the Newton
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
23
Newton's method
23
direction ‚àÜxnt at the current point. The stopping criterion is formulated in
terms of the Newton decrement; the algorithm stops when the decrement is
suÔ¨Éciently small. In short, therefore, the algorithm looks like this:
Newton‚Äôs method
Given a starting point x ‚ààdom f and a tolerance œµ > 0.
Repeat
1. Compute a Newton direction ‚àÜxnt and the Newton decrement Œª(f, x)
at x.
2. Stopping criterion: stop if Œª(f, x)2 ‚â§2œµ.
3. Determine a step size h > 0.
4. Update: x:= x + h‚àÜxnt.
The step size h is set equal to 1 in each iteration in the so-called pure
Newton method, while it is computed by line search with Armijo‚Äôs rule or
otherwise in damped Newton methods.
The stopping criterion is motivated by the fact that 1
2Œª(f, x)2 is an ap-
proximation to the decrease f(x) ‚àíf(x + ‚àÜxnt) in function value, and if this
decrease is small, it is not worthwhile to continue.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
24
Newton's method
Newton‚Äôs method generally works well for functions which are convex in
a neighborhood of the optimal point, but it breaks down, of course, if it hits
a point where the second derivative is singular and the Newton direction is
lacking. We shall show that the pure method, under appropriate conditions
on the objective function f, converges to the minimum point if the starting
point is suÔ¨Éciently close to the minimum point. To achieve convergence for
arbitrary starting points, it is necessary to use methods with damping.
Example 15.2.1. When applied to a downwards bounded convex quadratic
polynomial
f(x) = 1
2‚ü®x, Ax‚ü©+ ‚ü®b, x‚ü©+ c,
Newton‚Äôs pure method Ô¨Ånds the optimal solution after just one iteration,
regardless of the choice of starting point x, because f ‚Ä≤(x) = Ax+b, f ‚Ä≤‚Ä≤(x) = A
and A‚àÜxnt = ‚àí(Ax + b), so the update x+ = x + ‚àÜxnt satisÔ¨Åes the equation
f ‚Ä≤(x+) = Ax+ + b = Ax + A‚àÜxnt + b = 0,
which means that x+ is the optimal point.
Invariance under change of coordinates
Unlike the gradient descent method, Newton‚Äôs method is invariant under
aÔ¨Éne coordinate changes.
Theorem 15.2.1. Let f : X ‚ÜíR be a twice diÔ¨Äerentiable function with a
positive deÔ¨Ånite second derivative, and let (xk)‚àû
0
be the sequence generated
by Newton‚Äôs pure algorithm with x0 as starting point. Let further A: Y ‚ÜíX
be an aÔ¨Éne coordinate transformation, i.e. the restriction to Y of a bijective
aÔ¨Éne map. Newton‚Äôs pure algorithm applied to the function g = f ‚ó¶A with
y0 = A‚àí1x0 as the starting point then generates a sequence (yk)‚àû
0
with the
property that Ayk = xk for each k.
The two sequences have identical Newton decrements in each iteration,
and they therefore satisfy the stopping condition during the same iteration.
Proof. The assertion about the Newton decrements follows from Theorem
15.1.7, and the relationship between the two sequences follows by induction
if we show that Ay = x implies that A(y + ‚àÜynt) = x + ‚àÜxnt, where ‚àÜxnt =
‚àíf ‚Ä≤‚Ä≤(x)‚àí1f ‚Ä≤(x) and ‚àÜynt = ‚àíg‚Ä≤‚Ä≤(y)‚àí1g‚Ä≤(y) are the uniquely deÔ¨Åned Newton
directions at the points x and y of the respective functions.
The aÔ¨Éne map A can be written as Ay = Cy+b, where C is an invertible
linear map and b is a vector. If x = Ay, then g‚Ä≤(y) = CTf ‚Ä≤(x) and g‚Ä≤‚Ä≤(y) =
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
25
Newton's method
CTf ‚Ä≤‚Ä≤(x)C, by the chain rule. It follows that
C‚àÜynt = ‚àíCg‚Ä≤‚Ä≤(y)‚àí1g‚Ä≤(y) = ‚àíCC‚àí1f ‚Ä≤‚Ä≤(x)‚àí1(CT)‚àí1CTf ‚Ä≤(x)
= ‚àíf ‚Ä≤‚Ä≤(x)‚àí1f ‚Ä≤(x) = ‚àÜxnt,
and hence
A(y+‚àÜynt) = C(y+‚àÜynt)+b = Cy+b+C‚àÜynt = Ay+‚àÜxnt = x+‚àÜxnt.
Local convergence
We will now study convergence properties for the Newton method, starting
with the pure method.
Theorem 15.2.2. Let f : X ‚ÜíR be a twice diÔ¨Äerentiable, ¬µ-strongly convex
function with minimum point ÀÜx, and suppose that the second derivative f ‚Ä≤‚Ä≤ is
Lipschitz continuous with Lipschitz constant L. Let x be a point in X and
set
x+ = x + ‚àÜxnt,
where ‚àÜxnt is the Newton direction at x. Then
‚à•x+ ‚àíÀÜx‚à•‚â§L
2¬µ‚à•x ‚àíÀÜx‚à•2.
Moreover, if the point x+ lies in X then
‚à•f ‚Ä≤(x+)‚à•‚â§L
2¬µ2‚à•f ‚Ä≤(x)‚à•2.
Proof. The smallest eigenvalue of the second derivative f ‚Ä≤‚Ä≤(x) is greater than
or equal to ¬µ by Theorem 7.3.2 in Part I. Hence, f ‚Ä≤‚Ä≤(x) is invertible and the
largest eigenvalue of f ‚Ä≤‚Ä≤(x)‚àí1 is less than or equal to ¬µ‚àí1, and it follows that
(15.3)
‚à•f ‚Ä≤‚Ä≤(x)‚àí1‚à•‚â§¬µ‚àí1.
To estimate the norm of x+ ‚àíÀÜx, we rewrite the diÔ¨Äerence as
x+ ‚àíÀÜx = x + ‚àÜxnt ‚àíÀÜx = x ‚àíÀÜx ‚àíf ‚Ä≤‚Ä≤(x)‚àí1f ‚Ä≤(x)
(15.4)
= f ‚Ä≤‚Ä≤(x)‚àí1
f ‚Ä≤‚Ä≤(x)(x ‚àíÀÜx) ‚àíf ‚Ä≤(x)

= ‚àíf ‚Ä≤‚Ä≤(x)‚àí1w
with
w = f ‚Ä≤(x) ‚àíf ‚Ä≤‚Ä≤(x)(x ‚àíÀÜx).
For 0 ‚â§t ‚â§1 we then deÔ¨Åne the vektor w(t) as
w(t) = f ‚Ä≤(ÀÜx + t(x ‚àíÀÜx)) ‚àítf ‚Ä≤‚Ä≤(x)(x ‚àíÀÜx),
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
26
Newton's method
26
and note that w = w(1) ‚àíw(0), since f ‚Ä≤(ÀÜx) = 0. By the chain rule,
w‚Ä≤(t) =

f ‚Ä≤‚Ä≤(ÀÜx + t(x ‚àíÀÜx)) ‚àíf ‚Ä≤‚Ä≤(x)

(x ‚àíÀÜx),
and by using the Lipschitz continuity of the second derivative, we obtain the
estimate
‚à•w‚Ä≤(t)‚à•‚â§‚à•f ‚Ä≤‚Ä≤(ÀÜx + t(x ‚àíÀÜx)) ‚àíf ‚Ä≤‚Ä≤(x)‚à•‚à•x ‚àíÀÜx‚à•
‚â§L‚à•ÀÜx + t(x ‚àíÀÜx) ‚àíx‚à•‚à•x ‚àíÀÜx‚à•= L(1 ‚àít)‚à•x ‚àíÀÜx‚à•2.
Now integrate the above inequality over the interval [0, 1]; this results in the
inequality
‚à•w‚à•=

 1
0
w‚Ä≤(t) dt
 ‚â§
 1
0
‚à•w‚Ä≤(t)‚à•dt ‚â§L‚à•x ‚àíÀÜx‚à•2
 1
0
(1 ‚àít) dt.
(15.5)
= 1
2L‚à•x ‚àíÀÜx‚à•2.
By combining equality (15.4) with the inequalities (15.3) and (15.5) we
obtain the estimate
‚à•x+ ‚àíÀÜx‚à•= ‚à•f ‚Ä≤‚Ä≤(x)‚àí1w‚à•‚â§‚à•f ‚Ä≤‚Ä≤(x)‚àí1‚à•‚à•w‚à•‚â§L
2¬µ‚à•x ‚àíÀÜx‚à•2,
which is the Ô¨Årst claim of the theorem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
‚ñ∂‚ñ∂enroll by September 30th, 2014 and 
‚ñ∂‚ñ∂save up to 16% on the tuition!
‚ñ∂‚ñ∂pay in 10 installments / 2 years
‚ñ∂‚ñ∂Interactive Online education
‚ñ∂‚ñ∂visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
27
Newton's method
To prove the second claim, we assume that x+ lies in X and consider for
0 ‚â§t ‚â§1 the vectors
v(t) = f ‚Ä≤(x + t‚àÜxnt) ‚àítf ‚Ä≤‚Ä≤(x)‚àÜxnt,
noting that
v(1) ‚àív(0) = f ‚Ä≤(x+) ‚àíf ‚Ä≤‚Ä≤(x)‚àÜxnt ‚àíf ‚Ä≤(x) = f ‚Ä≤(x+) + f ‚Ä≤(x) ‚àíf ‚Ä≤(x) = f ‚Ä≤(x+).
Since v‚Ä≤(t) =

f ‚Ä≤‚Ä≤(x + t‚àÜxnt) ‚àíf ‚Ä≤‚Ä≤(x)

‚àÜxnt, it follows from the Lipschitz
continuity that
‚à•v‚Ä≤(t)‚à•‚â§‚à•f ‚Ä≤‚Ä≤(x + t‚àÜxnt) ‚àíf ‚Ä≤‚Ä≤(x)‚à•‚à•‚àÜxnt‚à•‚â§L‚à•‚àÜxnt‚à•2t,
and by integrating this inequality, we obtain the desired estimate
‚à•f ‚Ä≤(x+)‚à•=

 1
0
v‚Ä≤(t) dt
 ‚â§
 1
0
‚à•v‚Ä≤(t)‚à•dt ‚â§L
2 ‚à•‚àÜxnt‚à•2 ‚â§L
2¬µ2‚à•f ‚Ä≤(x)‚à•2,
where the last inequality follows from Corollary 15.1.5.
One consequence of the previous theorem is that the pure Newton method
converges quadratically when applied to functions with a positive deÔ¨Ånite
second derivative that does not vary too rapidly in a neighborhood of the
minimum point, provided that the starting point is chosen suÔ¨Éciently close
to the minimum point. More precisely, the following holds:
Theorem 15.2.3. Let f : X ‚ÜíR be a twice diÔ¨Äerentiable, ¬µ-strongly convex
function with minimum point ÀÜx, and suppose that the second derivative f ‚Ä≤‚Ä≤
is Lipschitz continuous with Lipschitz constant L. Let 0 < r ‚â§2¬µ/L and
suppose that the open ball B(ÀÜx; r) is included in X.
Newton‚Äôs pure method with starting point x0 ‚ààB(ÀÜx; r) will then generate
a sequence (xk)‚àû
0 of points in ‚Ñ¶such that
‚à•xk ‚àíÀÜx‚à•‚â§2¬µ
L
 L
2¬µ‚à•x0 ‚àíÀÜx‚à•
2k
for all k, and the sequence therefore converges to the minimum point ÀÜx as
k ‚Üí‚àû.
The convergence is very rapid. For example,
‚à•xk ‚àíÀÜx‚à•‚â§2¬µ
L 2‚àí2k
if the initial point is chosen such that ‚à•x0 ‚àíÀÜx‚à•‚â§¬µ/L, and this implies that
‚à•xk ‚àíÀÜx‚à•‚â§10‚àí19¬µ/L already for k = 6.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
28
Newton's method
Proof. We keep the notation of Theorem 15.2.2 and then have xk+1 = x+
k , so
if xk lies in the ball B(ÀÜx; r), then
(15.6)
‚à•xk+1 ‚àíÀÜx‚à•‚â§L
2¬µ‚à•xk ‚àíÀÜx‚à•2,
and this implies that ‚à•xk+1 ‚àíÀÜx‚à•< Lr2/2¬µ ‚â§r, i.e. the point xk+1 lies in the
ball B(ÀÜx; r). By induction, all points in the sequence (xk)‚àû
0 lie in B(ÀÜx; r), and
we obtain the inequality of the theorem by repeated application of inequality
(15.6).
Global convergence
Newton‚Äôs damped method converges, under appropriate conditions on the
objective function, for arbitrary starting points. The damping is required
only during an initial phase, because the step size becomes 1 once the al-
gorithm has produced a point where the gradient is suÔ¨Éciently small. The
convergence is quadratic during this second stage.
The following theorem describes a convergence result for strongly convex
functions with Lipschitz continuous second derivative.
Theorem 15.2.4. Let f : X ‚ÜíR be a twice diÔ¨Äerentiable, strongly convex
function with a Lipschitz continuous second derivative. Let x0 be a point in
X and suppose that the sublevel set
S = {x ‚ààX | f(x) ‚â§f(x0)}
is closed.
Then, f has a unique minimum point ÀÜx, and Newton‚Äôs damped algorithm,
with x0 as initial point och with line search according to Armijo‚Äôs rule with
parameters 0 < Œ± < 1
2 and 0 < Œ≤ < 1, generates a sequence (xk)‚àû
0 of points
in S that converges towards the minimum point.
After an initial phase with damping, the algorithm passes into a quadrat-
ically convergent phase with step size 1.
Proof. The existence of a unique minimum point is a consequence of Corol-
lary 8.1.7 in Part I.
Suppose that f is ¬µ-strongly convex and let L be the Lipschitz constant
of the second derivative. The sublevel set S is compact since it is bounded
according to Theorem 8.1.6. It follows that the distance from the set S to
the boundary of the open set X is positive. Fix a positive number r that is
less than this distance and also satisÔ¨Åes the inequality
r ‚â§¬µ/L.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
29
Newton's method
29
Given x ‚ààS we now deÔ¨Åne the point x+ by
x+ = x + h‚àÜxnt,
where h is the step size according to Armijo‚Äôs rule. In particular, xk+1 = x+
k
for all k.
The core of the proof consists in showing that there are two positive
constants Œ≥ and Œ∑ ‚â§¬µr such that the following two implications hold for all
x ‚ààS:
(i)
‚à•f ‚Ä≤(x)‚à•‚â•Œ∑ ‚áíf(x+) ‚àíf(x) ‚â§‚àíŒ≥;
(ii)
‚à•f ‚Ä≤(x)‚à•< Œ∑ ‚áíh = 1 & ‚à•f ‚Ä≤(x+)‚à•< Œ∑.
Suppose that we have managed to prove (i) and (ii). If ‚à•f ‚Ä≤(xk)‚à•‚â•Œ∑ for
0 ‚â§k < m, then
fmin ‚àíf(x0) ‚â§f(xm) ‚àíf(x0) =
m‚àí1

k=0
(f(x+
k ) ‚àíf(xk)) ‚â§‚àímŒ≥,
because of property (i). This inequality can not hold for all m, and hence
there is a smallest integer k0 such that ‚à•f ‚Ä≤(xk0)‚à•< Œ∑, and this integer must
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
30
Newton's method
satisfy the inequality
k0 ‚â§

f(x0) ‚àífmin

/Œ≥.
It now follows by induction from (ii) that the step size h is equal to 1 for
all k ‚â•k0. The damped Newton algorithm is in other words a pure Newton
algorithm from iteration k0 and onwards. Because of Theorem 14.1.1,
‚à•xk0 ‚àíÀÜx‚à•‚â§¬µ‚àí1‚à•f ‚Ä≤(xk0)‚à•< ¬µ‚àí1Œ∑ ‚â§r ‚â§¬µL‚àí1,
so it follows from Theorem 15.2.3 that the sequence (xk)‚àû
0 converges to ÀÜx,
and more precisely, that the estimate
‚à•xk+k0 ‚àíÀÜx‚à•‚â§2¬µ
L
 L
2¬µ‚à•xk0 ‚àíÀÜx‚à•
2k
‚â§2¬µ
L 2‚àí2k
holds for k ‚â•0.
It thus only remains to prove the existence of numbers Œ∑ and Œ≥ with the
properties (i) and (ii). To this end, let
Sr = S + B(x; r);
the set Sr is a convex and compact subset of ‚Ñ¶, and the two continuous
functions f ‚Ä≤ and f ‚Ä≤‚Ä≤ are therefore bounded on Sr, i.e. there are constants K
and M such that
‚à•f ‚Ä≤(x)‚à•‚â§K
and
‚à•f ‚Ä≤‚Ä≤(x)‚à•‚â§M
for all x ‚ààSr. It follows from Theorem 7.4.1 in Part I that the derivative f ‚Ä≤
is Lipschitz continuous on the set Sr with Lipschitz constant M, i.e.
‚à•f ‚Ä≤(y) ‚àíf ‚Ä≤(x)‚à•‚â§M‚à•y ‚àíx‚à•
for x, y ‚ààSr.
We now deÔ¨Åne our numbers Œ∑ and Œ≥ as
Œ∑ = min
3(1 ‚àí2Œ±)¬µ2
L
, ¬µr

and
Œ≥ = Œ±Œ≤c¬µ
M
Œ∑2, where c = min
 1
M , r
K

.
Let us Ô¨Årst estimate the stepsize at a given point x ‚ààS. Since
‚à•‚àÜxnt‚à•‚â§¬µ‚àí1‚à•f ‚Ä≤(x)‚à•‚â§¬µ‚àí1K,
the point x + t‚àÜxnt lies in i Sr and especially also in X if 0 ‚â§t ‚â§r¬µK‚àí1.
The function
g(t) = f(x + t‚àÜxnt)
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
31
Newton's method
is therefore deÔ¨Åned for these t-values, and since f is ¬µ-strongly convex and
the derivative is Lipschitz continuous with constant M on Sr, it follows from
Theorem 1.1.2 in Part I and Corollary 15.1.5 that
f(x + t‚àÜxnt) ‚â§f(x) + t‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©+ 1
2M‚à•‚àÜxnt‚à•2t2
‚â§f(x) + t‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©+ 1
2M¬µ‚àí1Œª(f, x)2t2
= f(x) + t

1 ‚àí1
2M¬µ‚àí1t

‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©.
The number ÀÜt = c¬µ lies in the interval [0, r¬µK‚àí1] and is less than or
equal to ¬µM ‚àí1. Hence, 1 ‚àí1
2M¬µ‚àí1ÀÜt ‚â•1
2 ‚â•Œ±, which inserted in the above
inequality gives
f(x + ÀÜt‚àÜxnt) ‚â§f(x) + Œ±ÀÜt ‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©.
Now, let h = Œ≤m be the step size given by Armijo‚Äôs rule, which means that
the Armijo algorithm terminates in iteration m. Since it does not terminate
in iteration m ‚àí1, we conclude that Œ≤m‚àí1 > ÀÜt, i.e.
h ‚â•Œ≤ÀÜt = Œ≤c¬µ,
and this gives us the following estimate for the point x+ = x + h‚àÜxnt:
f(x+) ‚àíf(x) ‚â§Œ±h‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©= ‚àíŒ±h Œª(f, x)2
‚â§‚àíŒ±Œ≤c¬µ Œª(f, x)2 ‚â§‚àíŒ±Œ≤c¬µM ‚àí1‚à•f ‚Ä≤(x)‚à•2 = ‚àíŒ≥Œ∑‚àí2‚à•f ‚Ä≤(x)‚à•2.
So, if ‚à•f ‚Ä≤(x)‚à•‚â•Œ∑ then f(x+) ‚àíf(x) ‚â§‚àíŒ≥, which is the content of implica-
tion (i).
To prove the remaining implication (ii), we return to the function g(t) =
f(x + t‚àÜxnt), assuming that ‚à•f ‚Ä≤(x)‚à•< Œ∑. The function is well-deÔ¨Åned for
0 ‚â§t ‚â§1, since
‚à•‚àÜxnt‚à•‚â§¬µ‚àí1‚à•f ‚Ä≤(x)‚à•< ¬µ‚àí1Œ∑ ‚â§r.
Moreover,
g‚Ä≤(t) = ‚ü®f ‚Ä≤(x + t‚àÜxnt), ‚àÜxnt‚ü©and g‚Ä≤‚Ä≤(t) = ‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x + t‚àÜxnt)‚àÜxnt‚ü©.
By Lipschitz continuity,
|g‚Ä≤‚Ä≤(t) ‚àíg‚Ä≤‚Ä≤(0)| = |‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x + t‚àÜxnt)‚àÜxnt‚ü©‚àí‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x)‚àÜxnt‚ü©|
‚â§‚à•f ‚Ä≤‚Ä≤(x + t‚àÜxnt) ‚àíf ‚Ä≤‚Ä≤(x)‚à•‚à•‚àÜxnt‚à•2 ‚â§tL‚à•‚àÜxnt‚à•3,
and it follows, since g‚Ä≤‚Ä≤(0) = Œª(f, x)2 and ‚à•‚àÜxnt‚à•‚â§¬µ‚àí1/2Œª(f, x), that
g‚Ä≤‚Ä≤(t) ‚â§Œª(f, x)2 + tL‚à•‚àÜxnt‚à•3 ‚â§Œª(f, x)2 + tL¬µ‚àí3/2 Œª(f, x)3.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
32
Newton's method
32
By integrating this inequality over the interval [0, t], we obtain the inequality
g‚Ä≤(t) ‚àíg‚Ä≤(0) ‚â§tŒª(f, x)2 + 1
2t2L¬µ‚àí3/2Œª(f, x)3.
But g‚Ä≤(0) = ‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©= ‚àíŒª(f, x)2, so it follows that
g‚Ä≤(t) ‚â§‚àíŒª(f, x)2 + tŒª(f, x)2 + 1
2t2L¬µ‚àí3/2Œª(f, x)3,
and further integration results in the inequality
g(t) ‚àíg(0) ‚â§‚àítŒª(f, x)2 + 1
2t2Œª(f, x)2 + 1
6t3L¬µ‚àí3/2Œª(f, x)3.
Now, take t = 1 to obtain
f(x + ‚àÜxnt) ‚â§f(x) ‚àí1
2Œª(f, x)2 + 1
6L¬µ‚àí3/2Œª(f, x)3
(15.7)
= f(x) ‚àíŒª(f, x)2 1
2 ‚àí1
6L¬µ‚àí3/2Œª(f, x)

= f(x) + ‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©
 1
2 ‚àí1
6L¬µ‚àí3/2Œª(f, x)

.
Our assumption ‚à•f ‚Ä≤(x)‚à•< Œ∑ implies that
Œª(f, x) ‚â§¬µ‚àí1/2‚à•f ‚Ä≤(x)‚à•< ¬µ‚àí1/2Œ∑ ‚â§¬µ‚àí1/2¬∑3(1‚àí2Œ±)¬µ2L‚àí1 = 3(1‚àí2Œ±)¬µ3/2L‚àí1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
33
Newton's method
We conclude that
1
2 ‚àí1
6L¬µ‚àí3/2Œª(f, x) > Œ±,
which inserted into inequality (15.7) gives us the inequality
f(x + ‚àÜxnt) ‚â§f(x) + Œ±‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©,
which tells us that the step size h is equal to 1.
The iteration leading from x to x+ = x + h‚àÜxnt is therefore performed
according to the pure Newton method. Due to the inequality
‚à•x ‚àíÀÜx‚à•‚â§¬µ‚àí1‚à•f ‚Ä≤(x)‚à•< ¬µ‚àí1Œ∑ ‚â§r,
which holds by Theorem 14.1.1, x is a point in the ball B(ÀÜx; r), so it follows
from the local convergence Theorem 15.2.2 that
(15.8)
‚à•f ‚Ä≤(x+)‚à•‚â§L
2¬µ2‚à•f ‚Ä≤(x)‚à•2.
Since Œ∑ ‚â§¬µr ‚â§¬µ2/L,
‚à•f ‚Ä≤(x+)‚à•< L
2¬µ2Œ∑2 ‚â§Œ∑
2 < Œ∑,
and the proof is now complete.
By iterating inequality (15.8), one obtains in fact the estimate
‚à•f ‚Ä≤(xk)‚à•‚â§2¬µ2
L
 L
2¬µ2‚à•f ‚Ä≤(xk0)‚à•
2k‚àík0
< 2¬µ2
L 2‚àí2k‚àík0
for k ‚â•k0, and it now follows from Theorem 14.1.1 that
f(xk) ‚àífmin < 2¬µ3
L2 2‚àí2k‚àík0+1
for k ‚â•k0. Combining this estimate with the previously obtained bound
on k0, one obtains an upper bound on the number of iterations required to
estimate the minimum value fmin with a given accuracy. If
k > f(x0) ‚àífmin
Œ≥
+ log2 log2
2¬µ3
L2œµ,
then surely f(xk)‚àífmin < œµ. This estimate, however, is of no practical value,
because the constants Œ≥, ¬µ and L are rarely known in concrete cases.
Another shortcoming of the classical convergence analysis of Newton‚Äôs
method is that the convergence constants, unlike the algorithm itself, de-
pend on the coordinate system used.
For self-concordant functions, it is
however possible to carry out the convergence analysis without any unknown
constants, as we shall do in Chapter 16.5.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
34
Newton's method
15.3
Equality constraints
With only minor modiÔ¨Åcations, Newton‚Äôs algorithm also works well when
applied to convex optimization problems with constraints in the form of
aÔ¨Éne equalities.
Consider the convex optimization problem
(P)
min f(x)
s.t.
Ax = b
where f : ‚Ñ¶‚ÜíR is a twice continuously diÔ¨Äerentiable convex function, ‚Ñ¶is
an open subset of Rn, and A is an m √ó n-matrix.
The problem‚Äôs Lagrange function L: ‚Ñ¶√ó Rm ‚ÜíR is given by
L(x, y) = f(x) + (Ax ‚àíb)Ty = f(x) + xTATy ‚àíbTy,
and according to the Karush‚ÄìKuhn‚ÄìTucker theorem (Theorem 11.2.1 in Part
II), a point ÀÜx in ‚Ñ¶is an optimal solution if and only if there is a vector ÀÜy ‚ààRm
such that
(15.9)
f ‚Ä≤(ÀÜx) + ATÀÜy = 0
AÀÜx
= b.
Therefore, the minimization problem (P) is equivalent to the problem of
solving the system (15.9) of linear equations.
Example 15.3.1. When f is a convex quadratic function of the form
f(x) = 1
2‚ü®x, Px‚ü©+ ‚ü®q, x‚ü©+ r,
the linear system (15.9) becomes
P ÀÜx + ATÀÜy = ‚àíq
AÀÜx
=
b,
and this is a quadratic system of linear equations with a symmetric coeÔ¨Écient
matrix of order m + n. The system has a unique solution if rank A = m and
N(A) ‚à©N(P) = {0}. See exercise 15.4. In particular, there is a unique
solution if the matrix P is positive deÔ¨Ånite and rank A = m.
We now return to the general convex minimization problem (P). Let X
denote the set of feasible points, so that
X = {x ‚àà‚Ñ¶| Ax = b}.
In optimization problems without any constraints, the descent direction
‚àÜxnt at the point x is a vector which miminizes the Taylor polynom of degree
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
35
Newton's method
35
two of the function f(x+v), and the minimization is over all vectors v in Rn.
As a new point x+ with function value less than f(x) we select x+ = x+h‚àÜxnt
with a suitable step size h. In constrained problems, the new point x+ has
to be a feasible point, of course, and this requires that A‚àÜxnt = 0. The
minimization of the Taylor polynomial is therefore restricted to vectors v
that satisfy the condition Av = 0, and this means that we have to modify
our previous deÔ¨Ånition of Newton direction and decrement as follows for
constrained optimization problems.
DeÔ¨Ånition. In the equality constrained minimization problem (P), a vector
‚àÜxnt is called a Newton direction at the point x ‚ààX if there exists a vector
w ‚ààRm such that
(15.10)
f ‚Ä≤‚Ä≤(x)‚àÜxnt + ATw = ‚àíf ‚Ä≤(x)
A‚àÜxnt
=
0.
The quantity
Œª(f, x) =

‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x)‚àÜxnt‚ü©
is called the Newton decrement.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master‚Äôs Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master‚Äôs programmes
‚Ä¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
‚Ä¢ 1st place: MSc International Business
‚Ä¢ 1st place: MSc Financial Economics
‚Ä¢ 2nd place: MSc Management of Learning
‚Ä¢ 2nd place: MSc Economics
‚Ä¢ 2nd place: MSc Econometrics and Operations Research
‚Ä¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‚ÄòBeste Studies‚Äô ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
36
Newton's method
It follows from Example 15.3.1 that the Newton direction ‚àÜxnt (if it
exists) is an optimal solution to the minimization problem
min f(x) + ‚ü®f ‚Ä≤(x), v‚ü©+ 1
2‚ü®v, f ‚Ä≤‚Ä≤(x)v‚ü©
s.t.
Av = 0.
And if (‚àÜxnt, w) is a solution to the system (15.10), then
‚àí‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©= ‚ü®f ‚Ä≤‚Ä≤(x)‚àÜxnt + ATw, ‚àÜxnt‚ü©
= ‚ü®f ‚Ä≤‚Ä≤(x)‚àÜxnt, ‚àÜxnt‚ü©+ ‚ü®w, A‚àÜxnt‚ü©
= ‚ü®f ‚Ä≤‚Ä≤(x)‚àÜxnt, ‚àÜxnt‚ü©+ ‚ü®w, 0‚ü©= ‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x)‚àÜxnt‚ü©,
so it follows that
Œª(f, x) =

‚àí‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©,
just as for unconstrained problems.
The objective function is decreasing in the Newton direction, because
d
dtf(x + t‚àÜxnt)

t=0 = ‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©= ‚àíŒª(f, x)2 ‚â§0,
so ‚àÜxnt is indeed a descent direction.
Let P(v) denote the Taylor polynomial of degree two of the function
f(x + v). Then
f(x) ‚àíf(x + ‚àÜxnt) ‚âàP(0) ‚àíP(‚àÜxnt)
= ‚àí‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©‚àí1
2‚ü®‚àÜxnt, f ‚Ä≤‚Ä≤(x)‚àÜxnt‚ü©= 1
2Œª(f, x)2,
just as in the unconstrained case.
With our modiÔ¨Åed deÔ¨Ånition of the Newton direction, we can now copy
Newton‚Äôs method verbatim for convex minimization problem of the type
min f(x)
s.t.
Ax = b.
The algorithm looks like this:
Newton‚Äôs method
Given a starting point x ‚àà‚Ñ¶satisfying the constraint Ax = b, and a
tolerance œµ > 0.
Repeat
1. Compute the Newton direction ‚àÜxnt at x by solving the system of
equations (15.10), and compute the Newton decrement Œª(f, x).
2. Stopping criterion: stop if Œª(f, x)2 ‚â§2œµ.
3. Compute a step size h > 0.
4. Update: x:= x + h‚àÜxnt.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
37
Newton's method
Elimination of constraints
An alternative approach to the optimization problem
(P)
min f(x)
s.t.
Ax = b,
with x ‚àà‚Ñ¶as implicit condition and r = rank A, is to solve the system of
equations Ax = b and to express r variables as linear combinations of the
remaining n ‚àír variables. The former variables can then be eliminated from
the objective function, and we obtain in this way an optimization problem in
n ‚àír variables without explicit constraints, a problem that can be attacked
with Newton‚Äôs method. We will describe this approach in more detail and
compare it with the method above.
Suppose that the set X of feasible points is nonempty, choose a point
a ‚ààX, and select an aÔ¨Éne parametrization
x = Œæ(z),
z ‚ààÀú‚Ñ¶
of X with Œæ(0) = a. Since {x ‚ààRn | Ax = b} = a + N(A), we can write the
parametrization as
Œæ(z) = a + Cz
where C : Rp ‚ÜíRn is an injective linear map, whose range V(C) coincides
with the null space N(A) of the map A, and p = n ‚àírank A. The domain
Àú‚Ñ¶= {z ‚ààRp | a + Cz ‚àà‚Ñ¶} is an open convex subset of Rp.
A practical way to construct the parametrization is of course to solve the
system Ax = b by Gaussian elimination.
Let us Ô¨Ånally deÔ¨Åne the function Àúf : Àú‚Ñ¶‚ÜíR by setting Àúf(z) = f(Œæ(z)).
The problem (P) is then equivalent to the convex optimization problem
(ÀúP)
min Àúf(z)
which has no explicit constraints.
Let ‚àÜxnt be a Newton direction of the function f at the point x, i.e. a
vector that satisÔ¨Åes the system (15.10) for a suitably chosen vector w. We
will show that the function Àúf has a corresponding Newton direction ‚àÜznt at
the point z = Œæ‚àí1(x), and that ‚àÜxnt = C‚àÜznt.
Since A‚àÜxnt = 0 and N(A) = V(C), there is a unique vector v such that
‚àÜxnt = Cv. By the chain rule, Àúf ‚Ä≤(z) = CTf ‚Ä≤(x) and Àúf ‚Ä≤‚Ä≤(z) = CTf ‚Ä≤‚Ä≤(x)C, so
it follows from the Ô¨Årst equation in the system (15.10) that
Àúf ‚Ä≤‚Ä≤(z)v = CTf ‚Ä≤‚Ä≤(x)Cv = CTf ‚Ä≤‚Ä≤(x)‚àÜxnt = ‚àíCTf ‚Ä≤(x) ‚àíCTATw
= ‚àíÀúf ‚Ä≤(z) ‚àíCTATw.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
38
Newton's method
38
A general result from linear algebra tells us that N(S) = V(ST)‚ä•for
arbitrary linear maps S. Applying this result to the maps CT and A, and
using that V(C) = N(A), we obtain the equality
N(CT) = V(C)‚ä•= N(A)‚ä•= V(AT)‚ä•‚ä•= V(AT),
which implies that CTATw = 0. Hence,
Àúf ‚Ä≤‚Ä≤(z)v = ‚àíÀúf ‚Ä≤(z),
and v is thus a Newton direction of the function Àúf at the point z. So, ‚àÜznt = v
is the direction vector we are looking for.
The iteration step z ‚Üíz+ = z + h‚àÜznt in Newton‚Äôs method for the
unconstrained problem (ÀúP) takes us from the point z = Œæ‚àí1(x) in Àú‚Ñ¶to the
point z+ whose image in X is
Œæ(z+) = Œæ(z + h‚àÜznt) = a + C(z + h‚àÜznt) = a + Cz + hC(‚àÜznt)
= Œæ(z) + h‚àÜxnt = x + h‚àÜxnt,
and this is also the point we get by applying Newton‚Äôs method to the point
x in the constrained problem (P).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet‚Äôs 
electricity needs. Already today, SKF‚Äôs innovative know-
how is crucial to running a large proportion of the 
world‚Äôs wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
39
Newton's method
Also note that the Newton decrements are the same at corresponding
points, because
Œª( Àúf, z)2 = ‚àí‚ü®Àúf ‚Ä≤(z), ‚àÜznt‚ü©= ‚àí‚ü®CTf ‚Ä≤(x), ‚àÜznt‚ü©= ‚àí‚ü®f ‚Ä≤(x), C‚àÜznt‚ü©
= ‚àí‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©= Œª(f, x)2.
In summary, we have arrived at the following result.
Theorem 15.3.1. Let (xk)‚àû
0
be a sequence of points obtained by Newton‚Äôs
method applied to the constrained problem (P). Newton‚Äôs method applied to
the problem (ÀúP), obtained by elimination of the constraints and with Œæ‚àí1(x0)
as initial point, will then generate a sequence (zk)‚àû
0
with the property that
xk = Œæ(zk) for all k.
Convergence analysis
No new convergence analysis is needed for the modiÔ¨Åed version of Newton‚Äôs
method, for we can, because of Theorem 15.3.1, apply the results of The-
orem 15.2.4. If the restriction of the function f : ‚Ñ¶‚ÜíR to the set X of
feasible points is strongly convex and the second derivative is Lipschitz con-
tinuous, then the same also holds for the function Àúf : Àú‚Ñ¶‚ÜíR. (Cf. with
exercise 15.5.) Assuming x0 to be a feasible starting point and the sublevel
set {x ‚ààX | f(x) ‚â§f(x0)} to be closed, the damped Newton algorithm will
therefore converge to the minimum point when applied to the constrained
problem (P). Close enough to the minimum point, the step size h will also
be equal to 1, and the convergence will be quadratic.
Exercises
15.1 Determine the Newton direction, the Newton decrement and the local norm
at an arbitrary point x > 0 for the function f(x) = x ln x ‚àíx.
15.2 Let f be the function f(x1, x2) = ‚àíln x1 ‚àíln x2 ‚àíln(4 ‚àíx1 ‚àíx2) with
X = {x ‚ààR2 | x1 > 0, x2 > 0, x1 + x2 < 4} as domain. Determine the
Newton direction, the Newton decrement and the local norm at the point x
when
a) x = (1, 1)
b) x = (1, 2).
15.3 Determine a Newton direction, the Newton decrement and the local norm
for the function f(x1, x2) = ex1+x2 + x1 + x2 at an arbitrary point x ‚ààR2.
15.4 Assume that P is a symmetric positive semideÔ¨Ånite n √ó n-matrix and that
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
40
Newton's method
40
A is an arbitrary m √ó n-matrix. Prove that the matrix
M =
P
AT
A
0

is invertible if and only if rank A = m and N(A) ‚à©N(P) = {0}.
15.5 Assume that the function f : ‚Ñ¶‚ÜíR is twice diÔ¨Äerentiable and convex, let
x = Œæ(z) = a + Cz be an aÔ¨Éne parametrization of the set
X = {x ‚àà‚Ñ¶| Ax = b},
and deÔ¨Åne the function Àúf by Àúf(z) = f(Œæ(z)), just as in Section 15.3. Let
further œÉ denote the smallest eigenvalue of the symmetric matrix CTC.
a) Prove that Àúf is ¬µœÉ-strongly convex if the restriction of f to X is ¬µ-strongly
convex.
b) Assume that the matrix A has full rank and that there are constants K
and M such that Ax = b implies

f ‚Ä≤‚Ä≤(x)
AT
A
0
‚àí1 ‚â§K
and
‚à•f‚Ä≤‚Ä≤(x)‚à•‚â§M.
Show that Àúf is ¬µ-strongly convex with convexity constant ¬µ = œÉK‚àí2M‚àí1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
41
Self-concordant functions
Chapter 16
Self-concordant functions
Self-concordant functions were introduced by Nesterov and Nemirovski in
the late 1980s as a product of their analysis of the speed of convergence of
Newton‚Äôs method. Classic convergence results for two times continuously
diÔ¨Äerentiable functions assume that the second derivative is Lipschitz con-
tinuous, and the convergence rate depends on the Lipschitz constant. One
obvious weakness of these results is that the value of the Lipschitz constant,
unlike Newton‚Äôs method, is not invariant under aÔ¨Éne coordinate transfor-
mations.
Suppose that a function f, which is deÔ¨Åned on an open convex subset X
of Rn, has a Lipschitz continuous second derivative with Lipschitz constant
L, i.e. that
‚à•f ‚Ä≤‚Ä≤(y) ‚àíf ‚Ä≤‚Ä≤(x)‚à•‚â§L‚à•y ‚àíx‚à•
for all x, y ‚ààX. For the restriction œÜx,v(t) = f(x + tv) of f to a line through
x with direction vector v, this means that
|œÜ‚Ä≤‚Ä≤
x,v(t)‚àíœÜ‚Ä≤‚Ä≤
x,v(0)| = |‚ü®v, (f ‚Ä≤‚Ä≤(x+tv)‚àíf ‚Ä≤‚Ä≤(x))v‚ü©| ‚â§L‚à•x+tv‚àíx‚à•‚à•v‚à•2 = L|t|‚à•v‚à•3.
So if the function f is three times diÔ¨Äerentiable, then consequently
|œÜ‚Ä≤‚Ä≤‚Ä≤
x,v(0)| = lim
t‚Üí0

œÜ‚Ä≤‚Ä≤
x,v(t) ‚àíœÜ‚Ä≤‚Ä≤
x,v(0)
t
 ‚â§L‚à•v‚à•3.
But
œÜ‚Ä≤‚Ä≤‚Ä≤
x,v(0) =
n

i,j,k=1
‚àÇ3f(x)
‚àÇxi‚àÇxj‚àÇxk
vivjvk = D3f(x)[v, v, v],
so a necessary condition for a three times diÔ¨Äerentiable function f to have a
Lipschitz continuous second derivative with Lipschitz constant L is that
(16.1)
|D3f(x)[v, v, v]| ‚â§L‚à•v‚à•3
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
42
Self-concordant functions
for all x ‚ààX and all v ‚ààRn, and it is easy to show this is also a suÔ¨Écient
condition.
The reason why the value of the Lipschitz constant is not aÔ¨Énely invariant
is that there is no natural connection between the Euclidean norm ‚à•¬∑‚à•and the
function f. The analysis of a function‚Äôs behavior is simpliÔ¨Åed if we instead use
a norm that is adapted to the form of the level surfaces, and for functions
with a positive semideÔ¨Ånite second derivative f ‚Ä≤‚Ä≤(x), such a (semi)norm is
the local seminorm ‚à•¬∑‚à•x, introduced in the previous chapter and deÔ¨Åned as
‚à•v‚à•x =

‚ü®v, f ‚Ä≤‚Ä≤(x)v‚ü©. Nesterov‚ÄìNemirovski‚Äôs stroke of genius consisted in
replacing ‚à•¬∑‚à•with the local seminorm ‚à•¬∑‚à•x in the inequality (16.1). For the
function class obtained in this way, it is possible to describe the convergence
rate of Newton‚Äôs method in an aÔ¨Énely independent way and with absolute
constants.
16.1
Self-concordant functions
We are now ready for Nesterov‚ÄìNemirovski‚Äôs deÔ¨Ånition of self-concordance
and for a study of the basic properties of self-concordant functions.
DeÔ¨Ånition. Let f : X ‚ÜíR be a three times continuously diÔ¨Äerentiable func-
tion with an open convex subset X of Rn as domain. The function is called
self-concordant if it is convex, and the inequality
(16.2)
D3f(x)[v, v, v]
 ‚â§2

D2f(x)[v, v]
3/2
holds for all x ‚ààX and all v ‚ààRn.
Since D2f(x)[v, v] = ‚à•v‚à•2
x, where ‚à•¬∑‚à•x is the local seminorm deÔ¨Åned by
the function f at the point x, we can also write the deÔ¨Åning inequality (16.2)
as
D3f(x)[v, v, v]
 ‚â§2‚à•v‚à•3
x,
and it is this shorter version that we will prefer, when we work with a single
function f.
Remark 1. There is nothing special about the constant 2 in inequality (16.2).
If f satisÔ¨Åes the inequality
D3f(x)[v, v, v]
 ‚â§K‚à•v‚à•3
x, then the function
F = 1
4K2f, obtained from f by scaling, is self-concordant. The choice of 2
as the constant facilitates, however, the wording of a number of results.
Remark 2. For functions f deÔ¨Åned on subsets of the real axis and v ‚ààR,
‚à•v‚à•2
x = f ‚Ä≤‚Ä≤(x)v2 and D3f(x)[v, v, v] = f ‚Ä≤‚Ä≤‚Ä≤(x)v3. Hence, a convex function
f : X ‚ÜíR is self-concordant if and only if
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
43
Self-concordant functions
43
|f ‚Ä≤‚Ä≤‚Ä≤(x)| ‚â§2f ‚Ä≤‚Ä≤(x)3/2
for all x ‚ààX.
Remark 3. In terms of the restriction œÜx,v(t) = f(x+tv) of the function f to
the line through x with direction v, we can equivalently write the inequality
D3f(x + tv)[v, v, v]
 ‚â§2‚à•v‚à•3
x+tv
as |œÜ‚Ä≤‚Ä≤‚Ä≤
x,v(t)| ‚â§2œÜ‚Ä≤‚Ä≤
x,v(t)3/2. A three times continuously diÔ¨Äerentiable convex
function of several variables is therefore self-concordant if and only if all its
restrictions to lines are self-concordant.
Example 16.1.1. The convex function f(x) = ‚àíln x is self-concordant on its
domain R++. Indeed, inequality (16.2) holds with equality for this function,
since f ‚Ä≤‚Ä≤(x) = x‚àí2 and f ‚Ä≤‚Ä≤‚Ä≤(x) = ‚àí2x‚àí3.
Example 16.1.2. Convex quadratic functions f(x) = 1
2‚ü®x, Ax‚ü©+ ‚ü®b, x‚ü©+ c
are self-concordant since D3f(x)[v, v, v] = 0 for all x and v.
Hence, aÔ¨Éne functions are self-concordant, and the function x ‚Üí‚à•x‚à•2,
where ‚à•¬∑‚à•is the Euclidean norm, is self-concordant.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
44
Self-concordant functions
The expression
D3f(x)[u, v, w] =
n

i,k,k=1
‚àÇ3f(x)
‚àÇxi‚àÇxj‚àÇxk
uivjwk
is a symmetric trilinear form in the variables u, v, and w, if the function f is
three times continuously diÔ¨Äerentiable in a neighborhood of the point x. For
self-concordant functions we have the following generalization of inequality
(16.2) in the deÔ¨Ånition of self-concordance.
Theorem 16.1.1. Suppose f : X ‚ÜíR is a self-concordant function. Then,
D3f(x)[u, v, w]
 ‚â§2‚à•u‚à•x‚à•v‚à•x‚à•w‚à•x
for all x ‚ààX and all vectors u, v, w in Rn.
Proof. The proof is based on a general theorem on norms of symmetric tri-
linear forms, which is proven in an appendix to this chapter.
Assume Ô¨Årst that x is a point where the second derivative f ‚Ä≤‚Ä≤(x) is positive
deÔ¨Ånite. Then ‚à•¬∑‚à•x is a norm with ‚ü®u, v‚ü©x = ‚ü®u, f ‚Ä≤‚Ä≤(x)v‚ü©as the corresponding
scalar product. We can therefore apply Theorem 1 of the appendix to the
symmetric trilinear form D3f(x)[u, v, w] with ‚à•¬∑‚à•x as the underlying norm,
and it follows that
sup
u,v,wÃ∏=0
D3f(x)[u, v, w]

‚à•u‚à•x‚à•v‚à•x‚à•w‚à•x
= sup
vÃ∏=0
D3f(x)[v, v, v]

‚à•v‚à•3
x
‚â§2,
which is equivalent to the assertion of the theorem.
To cope with points where the second derivative is singular, we consider
for œµ > 0 the scalar product
‚ü®u, v‚ü©x,œµ = ‚ü®u, f ‚Ä≤‚Ä≤(x)v‚ü©+ œµ‚ü®u, v‚ü©,
where ‚ü®¬∑ , ¬∑‚ü©is the usual standard scalar product, and the corresponding norm
‚à•v‚à•x,œµ =

‚ü®v, v‚ü©x,œµ =

‚à•v‚à•2
x + œµ‚à•v‚à•2.
Obviously, ‚à•v‚à•x ‚â§‚à•v‚à•x,œµ for all vectors v, and hence
|D3f(x)[v, v, v]| ‚â§2‚à•v‚à•3
x,œµ
for all v, since f is self-concordant. It now follows from Theorem 1 in the
appendix that
|D3f(x)[u, v, w]| ‚â§2‚à•u‚à•x,œµ‚à•v‚à•x,œµ‚à•w‚à•x,œµ
= 2

(‚à•u‚à•2
x + œµ‚à•u‚à•2)(‚à•v‚à•2
x + œµ‚à•v‚à•2)(‚à•w‚à•2
x + œµ‚à•u‚à•w2),
and we get the sought-after inequality by letting œµ ‚Üí0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
45
Self-concordant functions
Theorem 16.1.2. The second derivative f ‚Ä≤‚Ä≤(x) of a self-concordant function
f : X ‚ÜíR has the same null space N(f ‚Ä≤‚Ä≤(x)) at all points x ‚ààX.
Proof. We recall that N(f ‚Ä≤‚Ä≤(x)) = {v | ‚à•v‚à•x = 0}.
Let x and y be two points in X. For reasons of symmetry, we only have
to show the inclusion N(f ‚Ä≤‚Ä≤(x)) ‚äÜN(f ‚Ä≤‚Ä≤(y)).
Assume therefore that v ‚ààN(f ‚Ä≤‚Ä≤(x)) and let xt = x + t(y ‚àíx). Since X
is an open convex set, there is certainly a number a > 1 such that the points
xt lie in X for 0 ‚â§t ‚â§a, and we now deÔ¨Åne a function g: [0, a] ‚ÜíR by
setting
g(t) = D2f(xt)[v, v] = ‚à•v‚à•2
xt.
Then g(0) = ‚à•v‚à•2
x = 0 and g(t) ‚â•0 for 0 ‚â§t ‚â§a, and since
g‚Ä≤(t) = D3f(xt)[v, v, y ‚àíx],
it follows from Theorem 16.1.1 that
|g‚Ä≤(t)| ‚â§2‚à•v‚à•2
xt‚à•y ‚àíx‚à•xt = 2g(t)‚à•y ‚àíx‚à•xt.
But the seminorm
‚à•y ‚àíx‚à•xt =

D2f(xt)[y ‚àíx, y ‚àíx]
depends continuously on t, and it is therefore bounded above by some con-
stant C on the interval [0, a]. Hence,
|g‚Ä≤(t)| ‚â§2Cg(t)
for 0 ‚â§t ‚â§a. It now follows from Theorem 2 in the appendix to this chapter
that g(t) = 0 for all t, and in particular, g(1) = ‚à•v‚à•2
y = 0, which proves that
v ‚ààN(f ‚Ä≤‚Ä≤(y)). This proves the inclusion N(f ‚Ä≤‚Ä≤(x)) ‚äÜN(f ‚Ä≤‚Ä≤(y)).
Our next corollary is just a special case of Theorem 16.1.2, because f ‚Ä≤‚Ä≤(x)
is non-singular if and only if N(f ‚Ä≤‚Ä≤(x)) = {0}.
Corollary 16.1.3. The second derivative of a self-concordant function is ei-
ther non-singular at all points or singular at all points.
A self-concordant function will be called non-degenerate if its second
derivative is positive deÔ¨Ånite at all points, and by the above corollary, that
is the case if the second derivative is positive deÔ¨Ånite at one single point.
A non-degenerate self-concordant function is in particular strictly convex.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
46
Self-concordant functions
46
Operations that preserve self-concordance
Theorem 16.1.4. If f is a self-concordant function and Œ± ‚â•1, then Œ±f is
self-concordant.
Proof. If Œ± ‚â•1, then Œ± ‚â§Œ±3/2, and it follows that
D3(Œ±f)(x)[v, v, v]
 = Œ±
D3f(x)[v, v, v]
 ‚â§2Œ±

D2f(x)[v, v]
3/2
‚â§2

Œ±D2f(x)[v, v]
3/2 = 2

D2(Œ±f)(x)[v, v]
3/2.
Theorem 16.1.5. The sum f + g of two self-concordant functions f and g is
self-concordant on its domain.
Proof. We use the elementary inequality
a3/2 + b3/2 ‚â§(a + b)3/2,
which holds for all nonnegative numbers a, b (and is easily proven by squaring
both sides) and the triangle inequality to obtain
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT‚Ä¶
     RUN FASTER.
          RUN LONGER..
                RUN EASIER‚Ä¶
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
47
Self-concordant functions
D3(f + g)(x)[v, v, v]
 =
D3f(x)[v, v, v] + D3g(x)[v, v, v]

‚â§2

D2f(x)[v, v]
3/2 + 2

D2g(x)[v, v]
3/2
‚â§2

D2f(x)[v, v] + D2g(x)[v, v]
3/2
= 2

D2(f + g)(x)[v, v]
3/2.
Theorem 16.1.6. If the function f : X ‚ÜíR is self-concordant, where X
is an open convex subset of Rn, and A is an aÔ¨Éne map from Rm to Rn,
then the composition g = f ‚ó¶A is a self-concordant function on its domain
A‚àí1(X).
Proof. The aÔ¨Éne map A can be written as Ay = Cy + b, where C is a linear
map and b is a vector. Let y be a point in A‚àí1(X) and let u be a vector in
Rm, and write x = Ay och v = Cu. According to the chain rule,
D2g(y)[u, u] = D2f(Ay)[Cu, Cu] = D2f(x)[v, v]
and
D3g(y)[u, u, u] = D3f(Ay)[Cu, Cu, Cu] = D3f(x)[v, v, v],
so it follows that
D3g(y)[u, u, u]
 =
D3f(x)[v, v, v]
 ‚â§2

D2f(x)[v, v]
3/2
= 2

D2g(y)[u, u]
3/2.
Example 16.1.3. It follows from Example 16.1.1 and Theorem 16.1.6 that
the function f(x) = ‚àíln(b ‚àí‚ü®c, x‚ü©) with domain {x ‚ààRn | ‚ü®c, x‚ü©< b} is
self-concordant.
Example 16.1.4. Suppose that the polyhedron
X =
p
j=1
{x ‚ààRn | ‚ü®cj, x‚ü©‚â§bj}
has nonempty interior. The function f(x) = ‚àíp
j=1 ln(bj ‚àí‚ü®cj, x‚ü©), with
int X as domain, is self-concordant.
16.2
Closed self-concordant functions
In Section 6.7 of Part I we studied the recessive subspace of arbitrary convex
functions. The properties of the recessive subspace of a closed self-concordant
function is given by the following theorem.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
48
Self-concordant functions
Theorem 16.2.1. Suppose that f : X ‚ÜíR is a closed self-concordant func-
tion.
The function‚Äôs recessive subspace Vf is then equal to the null space
N(f ‚Ä≤‚Ä≤(x) of the second derivative f ‚Ä≤‚Ä≤(x) at an arbitrary point x ‚ààX. More-
over,
(i)
X = X + Vf.
(ii)
f(x + v) = f(x) + Df(x)[v] for all vectors v ‚ààVf.
(iii)
If Œª(f, x) < ‚àû, then f(x + v) = f(x) for all v ‚ààVf.
Proof. Assertions (i) and (ii) are true for the recessive subspace of an arbi-
trary diÔ¨Äerentiable convex function according to Theorem 6.7.1, so we only
have to prove the remaining assertions.
Let x be an arbitrary point in X and let v be an arbitrary vector in Rn,
and consider the restriction œÜx,v(t) = f(x + tv) of f to the line through x
with direction v. The domain of œÜx,v is an open interval I =]Œ±, Œ≤[ around 0.
First suppose that v ‚ààVf. Then
œÜx,v(t) = f(x) + tDf(x)[v]
for all t ‚ààI becuse of property (ii), and it follows that
‚à•v‚à•2
x = D2f(x)[v, v] = œÜ‚Ä≤‚Ä≤
x,v(0) = 0,
i.e. the vector v belongs to the null space of f ‚Ä≤‚Ä≤(x). This proves the inclusion
Vf ‚äÜN(f ‚Ä≤‚Ä≤(x)). Note that this inclusion holds for arbitrary twice diÔ¨Äeren-
tiable convex functions without any assumptions concerning self-concordance
and closedness.
To prove the converse inclusion N(f ‚Ä≤‚Ä≤(x)) ‚äÜVf, we instead assume that
v is a vector in N(f ‚Ä≤‚Ä≤(x)). Since N(f ‚Ä≤‚Ä≤(x + tv)) = N(f ‚Ä≤‚Ä≤(x)) for all t ‚ààI due
to Theorem 16.1.2, we now have
œÜ‚Ä≤‚Ä≤
x,v(t) = D2f(x + tv)[v, v] = ‚à•v‚à•2
x+tv = 0
for all t ‚ààI, and it follows that
œÜx,v(t) = œÜx,v(0) + œÜ‚Ä≤
x,v(0)t = f(x) + Df(x)[v] t.
If Œ≤ < ‚àû, then x + Œ≤v is a boundary point of X and limt‚ÜíŒ≤ œÜx,v(t) < ‚àû.
However, according to Corollary 8.2.2 in Part I this is a contradiction to f
being a closed function. Hence, Œ≤ = ‚àû, and similarly, Œ± = ‚àí‚àû. This means
that I =]‚àí‚àû, ‚àû[, and in particular, I contains the number 1. We conclude
that the point x + v lies in X and that f(x + v) = œÜx,v(1) = f(x) + Df(x)[v]
for all x ‚ààX and all v ‚ààN(f ‚Ä≤‚Ä≤(x)), and Theorem 6.7.1 now provides us with
the inclusion N(f ‚Ä≤‚Ä≤(x)) ‚äÜVf. Hence, Vf = N(f ‚Ä≤‚Ä≤(x)).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
49
Self-concordant functions
49
Finally, suppose that Œª(f, x) < ‚àû. Then there exists, by deÔ¨Ånition, a
Newton direction at x, and this implies, according to the remark after the
deÔ¨Ånition of Newton direction, that the implication
f ‚Ä≤‚Ä≤(x)v = 0 ‚áíDf(x)[v] = 0
holds. Since Vf = N(f ‚Ä≤‚Ä≤(x)), it now follows from assertion (ii) that f(x+v) =
f(x) for all v ‚ààVf.
The problem of minimizing a degenerate closed self-concordant function
f : X ‚ÜíR with Ô¨Ånite Newton decrement Œª(f, x) at all points x ‚ààX can be
reduced to the problem of minimizing a non-degenerate closed self-concordant
function as follows.
Assume that the domain X is a subset of Rn, and let Vf denote the
recessive subspace of f.
Put m = dim V ‚ä•
f
and let A: Rm ‚ÜíRn be an
arbitrary injective linear map onto V ‚ä•
f , and put X0 = A‚àí1(X). The set X0
is then an open subset of Rm, and we obtain a function g: X0 ‚ÜíR by
deÔ¨Åning g(y) = f(Ay) for y ‚ààX0.
The function g is self-concordant according to Theorem 16.1.6, and since
(y, t) belongs to the epigraph of g if and only if (Ay, t) belongs to the epigraph
of f, it follows that g is also a closed function.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
50
Self-concordant functions
Suppose v ‚ààN(g‚Ä≤‚Ä≤(y)). Since g‚Ä≤‚Ä≤(y) = ATf ‚Ä≤‚Ä≤(Ay)A,
‚ü®Av, f ‚Ä≤‚Ä≤(Ay)Av‚ü©= ‚ü®v, ATf ‚Ä≤‚Ä≤(Ay)Av‚ü©= ‚ü®v, g‚Ä≤‚Ä≤(y)v‚ü©= 0,
which means that the vector Av belongs to N(f ‚Ä≤‚Ä≤(Ay)), i.e. to the recessive
subspace Vf. But Av also belongs to V ‚ä•
f , by deÔ¨Ånition, and Vf ‚à©V ‚ä•
f = {0}, so
it follows that Av = 0. Hence v = 0, since A is an injective map. This proves
that N(g‚Ä≤‚Ä≤(y)) = {0}, which means that g is a non-degenerate function.
Each vector x ‚ààX has a unique decomposition x = x1 +x2 with x1 ‚ààV ‚ä•
f
and x2 ‚ààVf, and x1 (= x ‚àíx2) lies in X according to Theorem 16.2.1.
Consequently, there is a unique point y ‚ààX0 such that Ay = x1. Therefore,
g(y) = f(Ay) = f(x1) = f(x) by the same theorem.
The functions f and g thus have the same ranges, and ÀÜy is a minimum
point of g if and only if AÀÜy is a minimum point of f, and thereby also all
points AÀÜy + v with v ‚ààVf are minimum points of f.
We also note for future use that
Œª(g, y) ‚â§Œª(f, Ay) = Œª(f, Ay + v)
for all y ‚ààX0 and all v ‚ààVf, according to Theorem 15.1.7. (In the present
case, the two Newton decrements are actually equal, which we leave as an
exercise to show.)
Corollary
16.2.2. A closed self-concordant function f : X ‚ÜíR is non-
degenerate if its domain X does not contain any line.
Proof. By Theorem 16.2.1, X = X + Vf. Hence, if f is degenerate, then
X contains all lines through points in X with directions given by nonzero
vectors in Vf. So the function must be non-degenerate if its domain does not
contain any lines.
Corollary 16.2.3. A closed self-concordant function is non-degenerate if and
only if it is strictly convex.
Proof. The second derivative f ‚Ä≤‚Ä≤(x) of a non-degenerate self-concordant func-
tion f is positive deÔ¨Ånit for all x in its domain, and this implies that f is
strictly convex.
The recessive subspace Vf of a degenerate function f is non-trivial, and
the restriction œÜx,v(t) = f(x + tv) of f to a line with a direction given by a
nonzero vector v ‚ààVf is aÔ¨Éne, according to Theorem 16.2.1. This prevents
f from being strictly convex.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
51
Self-concordant functions
16.3
Basic inequalities for the local seminorm
The graph of a convex function f lies above its tangent planes, and the
vertical distance between the point (y, f(y)) on the graph and the tangent
plane through the point (x, f(x) is greater than or equal to 1
2¬µ‚à•y ‚àíx‚à•2 if f is
¬µ-strongly convex. The same distance is also bounded below if the function
is self-concordant, but now by an expression that is a function of the local
norm ‚à•y ‚àíx‚à•x. The actual function œÅ is deÔ¨Åned in the following lemma,
which also describes all the properties of œÅ that we will need.
Lemma 16.3.1. Let œÅ: ]‚àí‚àû, 1[‚ÜíR be the function
œÅ(t) = ‚àít ‚àíln(1 ‚àít).
(i) The function œÅ is convex, strictly decreasing in the interval ]‚àí‚àû, 0],
and strictly increasing in the interval [0, 1[, and œÅ(0) = 0.
(ii) For 0 ‚â§t < 1,
œÅ(t) ‚â§
t2
2(1 ‚àít).
In particular, œÅ(t) ‚â§t2 if 0 ‚â§t ‚â§1
2.
(iii) If s < 1 and t < 1, then œÅ(s) + œÅ(t) ‚â•‚àíst.
(iv) If s ‚â•0, 0 ‚â§t < 1 and œÅ(‚àís) ‚â§œÅ(t), then s ‚â§
t
1 ‚àít.
Proof. Assertion (i) follows easily by considering the sign of the derivative,
and assertion (ii) follows from the Taylor series expansion, which gives
œÅ(t) = 1
2t2 + 1
3t3 + 1
4t4 + ¬∑ ¬∑ ¬∑ ‚â§1
2t2(1 + t + t2 + ¬∑ ¬∑ ¬∑ ) = 1
2t2(1 ‚àít)‚àí1
for 0 ‚â§t < 1.
To prove (iii), we use the elementary inequality x ‚àíln(1 + x) ‚â•0 and
take x = st ‚àís ‚àít. This gives
st + œÅ(s) + œÅ(t) = st ‚àís ‚àít ‚àíln(1 ‚àís) ‚àíln(1 ‚àít)
= st ‚àís ‚àít ‚àíln(1 + st ‚àís ‚àít) ‚â•0.
Since œÅ is strictly decreasing in the interval ]‚àí‚àû, 0], assertion (iv) will
follow once we show that œÅ(‚àís) ‚â•œÅ(t) when s = t/(1 ‚àít). To show this
inequality, let
g(t) = œÅ

‚àí
t
1 ‚àít

‚àíœÅ(t)
for 0 ‚â§t < 1. We simplify and obtain
g(t) = t ‚àí1 + (1 ‚àít)‚àí1 + 2 ln(1 ‚àít).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
52
Self-concordant functions
Since g(0) = 0 and g‚Ä≤(t) = 1 + (1 ‚àít)‚àí2 ‚àí2(1 ‚àít)‚àí1 = t2(1 ‚àít)‚àí2 ‚â•0,
we conclude that g(t) ‚â•0 for all t ‚àà[0, 1[, and this completes the proof of
assertion (iv).
The next theorem is used to estimate diÔ¨Äerences of the form ‚à•w‚à•y ‚àí‚à•w‚à•x,
Df(y)[w]‚àíDf(x)[w], and f(y)‚àíf(x)‚àíDf(x)[y‚àíx] in terms of ‚à•w‚à•x, ‚à•y‚àíx‚à•x
and the function œÅ.
Theorem 16.3.2. Let f : X ‚ÜíR be a closed self-concordant function, and
suppose that x is a point in X and that ‚à•y ‚àíx‚à•x < 1. Then, y is also a
point in X, and the following inequalities hold for the vector v = y ‚àíx and
arbitrary vectors w:
‚à•v‚à•x
1 + ‚à•v‚à•x
‚â§‚à•v‚à•y ‚â§
‚à•v‚à•x
1 ‚àí‚à•v‚à•x
(16.3)
‚à•v‚à•2
x
1 + ‚à•v‚à•x
‚â§Df(y)[v] ‚àíDf(x)[v] ‚â§
‚à•v‚à•2
x
1 ‚àí‚à•v‚à•x
(16.4)
œÅ(‚àí‚à•v‚à•x) ‚â§f(y) ‚àíf(x) ‚àíDf(x)[v] ‚â§œÅ(‚à•v‚à•x)
(16.5)
(1 ‚àí‚à•v‚à•x)‚à•w‚à•x ‚â§‚à•w‚à•y ‚â§
‚à•w‚à•x
1 ‚àí‚à•v‚à•x
(16.6)
Df(y)[w] ‚àíDf(x)[w] ‚â§D2f(x)[v, w] + ‚à•v‚à•2
x‚à•w‚à•x
1 ‚àí‚à•v‚à•x
‚â§‚à•v‚à•x‚à•w‚à•x
1 ‚àí‚à•v‚à•x
.
(16.7)
The left parts of the three inequalities (16.3), (16.4) and (16.5) are also
satisÔ¨Åed with v = y ‚àíx for all y ‚ààX.
Proof. We leave the proof that y belongs to X to the end and start by showing
that the inequalities (16.3‚Äì16.7) hold under the additional assumption y ‚ààX.
I. We begin with inequality (16.6).
If ‚à•w‚à•x = 0, then ‚à•w‚à•z = 0 for all
z ‚ààX, according to Theorem 16.1.2. Hence, the inequality holds in this case.
Therefore, let w be an arbitrary vector with ‚à•w‚à•x Ã∏= 0, let xt = x + t(y ‚àíx),
and deÔ¨Åne the function œà by
œà(t) = ‚à•w‚à•‚àí1
xt =

D2f(xt)[w, w]
‚àí1/2.
The function œà is deÔ¨Åned on an open interval that contains the interval [0, 1],
œà(0) = ‚à•w‚à•‚àí1
x
and œà(1) = ‚à•w‚à•‚àí1
y . It now follows, using Theorem 16.1.1, that
|œà‚Ä≤(t)| = 1
2

D2f(xt)[w, w]
‚àí3/2D3f(xt)[w, w, v]

(16.8)
= 1
2‚à•w‚à•‚àí3
xt
D3f(xt)[w, w, v]
 ‚â§1
2‚à•w‚à•‚àí3
xt ¬∑ 2‚à•w‚à•2
xt‚à•v‚à•xt
= ‚à•w‚à•‚àí1
xt ‚à•v‚à•xt = œà(t)‚à•v‚à•xt.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
53
Self-concordant functions
53
If ‚à•v‚à•x = 0, then ‚à•v‚à•z = 0 for all z ‚ààX, and hence œà‚Ä≤(t) = 0 for
0 ‚â§t ‚â§1. This implies that œà(1) = œà(0), i.e. that ‚à•w‚à•y = ‚à•w‚à•x. The
inequalities (16.3) and (16.6) are thus satisÔ¨Åed in the case ‚à•v‚à•x = 0.
Assume henceforth that ‚à•v‚à•x Ã∏= 0, and Ô¨Årst take w = v in the deÔ¨Ånition of
the function œà. In this special case, inequality (16.8) simpliÔ¨Åes to |œà‚Ä≤(t)| ‚â§1
for t ‚àà[0, 1], and hence œà(0) ‚àí1 ‚â§œà(1) ‚â§œà(0) + 1, by the mean-value
theorem. The right part of this inequality means that ‚à•v‚à•‚àí1
y
‚â§‚à•v‚à•‚àí1
x
+ 1,
which after rearrangement gives the left part of inequality (16.3). Note, that
this is true even in the case ‚à•v‚à•x ‚â•1.
Correspondingly, the left part of the same inequality gives rise to the right
part of inequality (16.3), now under the assumption that ‚à•v‚à•x < 1.
To prove inequality (16.6), we return to the function œà with a general w.
Since ‚à•tv‚à•x = t‚à•v‚à•x < 1 for 0 ‚â§t ‚â§1, it follows from the already proven
inequality (16.3) (with xt = x + tv instead of y) that
‚à•v‚à•xt = 1
t ‚à•tv‚à•xt ‚â§1
t ¬∑
‚à•tv‚à•x
1 ‚àí‚à•tv‚à•x
=
‚à•v‚à•x
1 ‚àít‚à•v‚à•x
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
54
Self-concordant functions
Insert this estimate into (16.8); this gives us the following inequality for the
derivative of the function ln œà(t):
|(ln œà(t))‚Ä≤| = |œà‚Ä≤(t)|
œà(t) = ‚à•v‚à•xt ‚â§
‚à•v‚à•x
1 ‚àít‚à•v‚à•x
.
Let us now integrate this inequality over the interval [0, 1]; this results in the
estimate
ln ‚à•w‚à•y
‚à•w‚à•x
 =
ln œà(0)
œà(1)
 =
ln œà(1) ‚àíln œà(0)
 =

 1
0
(ln œà(t))‚Ä≤ dt

‚â§
 1
0
‚à•v‚à•x
1 ‚àít‚à•v‚à•x
dt = ‚àíln(1 ‚àí‚à•v‚à•x),
which after exponentiation yields
1 ‚àí‚à•v‚à•x ‚â§‚à•w‚à•y
‚à•w‚à•x
‚â§(1 ‚àí‚à•v‚à•x)‚àí1,
and this is inequality (16.6).
II. To prove the inequality (16.4), we deÔ¨Åne
œÜ(t) = Df(xt)[v],
where xt = x + t(y ‚àíx), as before. Then
œÜ‚Ä≤(t) = D2f(xt)[v, v] = ‚à•v‚à•2
xt = t‚àí2‚à•tv‚à•2
xt,
so by using inequality (16.3), we obtain the inequality
‚à•v‚à•2
x
(1 + t‚à•v‚à•x)2 = 1
t2
‚à•tv‚à•2
x
(1 + ‚à•tv‚à•x)2 ‚â§œÜ‚Ä≤(t) ‚â§1
t2
‚à•tv‚à•2
x
(1 ‚àí‚à•tv‚à•x)2 =
‚à•v‚à•2
x
(1 ‚àít‚à•v‚à•x)2
for 0 ‚â§t ‚â§1. The left part of this inequality holds with v = y ‚àíx for all
y ‚ààX, and the right part holds if ‚à•v‚à•x < 1, and by integrating the inequality
over the interval [0,1], we arrive at inequality (16.4).
III. To prove inequality (16.5), we start with the function
Œ¶(t) = f(xt) ‚àíDf(x)[v] t,
noting that
Œ¶(1) ‚àíŒ¶(0) = f(y) ‚àíf(x) ‚àíDf(x)[v]
and that
Œ¶‚Ä≤(t) = Df(xt)[v] ‚àíDf(x)[v].
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
55
Self-concordant functions
By replacing y with xt in inequality (16.4) , we obtain the following inequality
t‚à•v‚à•2
x
1 + t‚à•v‚à•x
‚â§Œ¶‚Ä≤(t) ‚â§
t‚à•v‚à•2
x
1 ‚àít‚à•v‚à•x
,
where the right part holds only if ‚à•v‚à•x < 1. By integrating the above in-
equality over the interval [0, 1], we obtain
œÅ(‚àí‚à•v‚à•x) =
 1
0
t‚à•v‚à•2
x
1 + t‚à•v‚à•x
dt ‚â§Œ¶(1) ‚àíŒ¶(0) ‚â§
 1
0
t‚à•v‚à•2
x
1 ‚àít‚à•v‚à•x
dt = œÅ(‚à•v‚à•x),
i.e. inequality (16.5).
IV. The proof of inequality (16.7) is analogous to the proof of inequality
(16.4), but this time our function œÜ is deÔ¨Åned as
œÜ(t) = Df(xt)[w].
Now, œÜ‚Ä≤(t) = D2f(xt)[w, v] and œÜ‚Ä≤‚Ä≤(t) = D3f(xt)[w, v, v], so it follows from
Theorem 16.1.1 and inequality (16.6) that
|œÜ‚Ä≤‚Ä≤(t)| ‚â§2‚à•w‚à•xt‚à•v‚à•2
xt ‚â§2 ‚à•w‚à•x‚à•v‚à•2
x
(1 ‚àít‚à•v‚à•x)3.
By integrating this inequality over the interval [0, s], where s ‚â§1, we get the
estimate
œÜ‚Ä≤(s) ‚àíœÜ‚Ä≤(0) ‚â§
 s
0
|œÜ‚Ä≤‚Ä≤(t)| dt ‚â§2‚à•w‚à•x
 s
0
‚à•v‚à•2
x dt
(1 ‚àít‚à•v‚à•x)3
= ‚à•w‚à•x

‚à•v‚à•x
(1 ‚àís‚à•v‚à•x)2 ‚àí‚à•v‚à•x

,
and another integration over the interval [0, 1] results in the inequality
œÜ(1) ‚àíœÜ(0) ‚àíœÜ‚Ä≤(0) ‚â§
 1
0
(œÜ‚Ä≤(s) ‚àíœÜ‚Ä≤(0)) ds ‚â§‚à•w‚à•x‚à•v‚à•2
x
1 ‚àí‚à•v‚à•x
,
which is the left part of inequality (16.7).
By the Cauchy‚ÄìSchwarz inequality,
D2f(x)[v, w] = ‚ü®v, f ‚Ä≤‚Ä≤(x)w‚ü©= ‚ü®f ‚Ä≤‚Ä≤(x)1/2v, f ‚Ä≤‚Ä≤(x)1/2w‚ü©
‚â§‚à•f ‚Ä≤‚Ä≤(x)1/2v‚à•‚à•f ‚Ä≤‚Ä≤(x)1/2w‚à•= ‚à•v‚à•x‚à•w‚à•x,
and we obtain the right part of inequality (16.7) by replacing D2f(x)[v, w]
with its majorant ‚à•v‚à•x‚à•w‚à•x.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
56
Self-concordant functions
56
V. It now only remains to prove that the condition ‚à•y ‚àíx‚à•x < 1 implies that
the point y lies in X.
Assume the contrary. i.e. that there is a point y outside X such that
‚à•y ‚àíx‚à•x < 1. The line segment [x, y] then intersects the boundary of X in
a point x + tv, where t is a number in the interval ]0, 1]. The function œÅ is
increasing in the interval [0, 1[, and hence œÅ(t‚à•v‚à•x) ‚â§œÅ(‚à•v‚à•x) if 0 ‚â§t < t. It
therefore follows from inequality (16.5) that
f(x+tv) ‚â§f(x)+tDf(x)[v]+œÅ(t‚à•v‚à•x) ‚â§f(x)+|Df(x)[v]|+œÅ(‚à•v‚à•x) < +‚àû
for all t in the interval [0, t[.
However, this is a contradiction, because
limt‚Üít f(x + tv) = +‚àû, since f is a closed function and x + tv is a boundary
point. Thus, y is a point in X.
16.4
Minimization
This section focuses on minimizing self-concordant functions, and the results
are largely based on the following theorem, which also plays a signiÔ¨Åcant role
in our study of Newton‚Äôs algorithm in the next section.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
57
Self-concordant functions
Theorem 16.4.1. Let f : X ‚ÜíR be a closed self-concordant function, sup-
pose that x ‚ààX is a point with Ô¨Ånite Newton decrement Œª = Œª(f, x), let
‚àÜxnt be a Newton direction at x, and deÔ¨Åne
x+ = x + (1 + Œª)‚àí1‚àÜxnt.
The point x+ is then a point in X and
f(x+) ‚â§f(x) ‚àíœÅ(‚àíŒª).
Remark. So a minimum point ÀÜx of f must satisfy the inequality
f(ÀÜx) ‚â§f(x) ‚àíœÅ(‚àíŒª)
for all x ‚ààX with Ô¨Ånite Newton decrement Œª.
Proof. The vector v = (1 + Œª)‚àí1‚àÜxnt has local seminorm
‚à•v‚à•x = (1 + Œª)‚àí1‚à•‚àÜxnt‚à•x = Œª(1 + Œª)‚àí1 < 1,
so it follows from Theorem 16.3.2 that the point x+ = x + v lies in X and
that
f(x+) ‚â§f(x) + Df(x)[v] + œÅ(‚à•v‚à•x) = f(x) +
1
1 + Œª‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©+ œÅ(
Œª
1 + Œª)
= f(x) ‚àí
Œª2
1 + Œª ‚àí
Œª
1 + Œª ‚àíln
1
1 + Œª = f(x) ‚àíŒª + ln(1 + Œª)
= f(x) ‚àíœÅ(‚àíŒª).
Theorem 16.4.2. The Newton decrement Œª(f, x) of a downwards bounded
closed self-concordant function f : X ‚ÜíR is Ô¨Ånite at each point x ‚ààX and
infx‚ààX Œª(f, x) = 0.
Proof. Let v be an arbitrary vector in the recessive subspace Vf = N(f ‚Ä≤‚Ä≤(x)).
Then
f(x + tv) = f(x) + t‚ü®f ‚Ä≤(x), v‚ü©
for all t ‚ààR according to Theorem 16.2.1, and since f is supposed to be
bounded below, this implies that ‚ü®f ‚Ä≤(x), v‚ü©= 0. This proves the implication
f ‚Ä≤‚Ä≤(x)v = 0 ‚áí‚ü®f ‚Ä≤(x), v‚ü©= 0,
which means that there exists a Newton direction at the point x. Hence,
Œª(f, x) is a Ô¨Ånite number.
If there is a positive number Œ¥ such that Œª(f, x) ‚â•Œ¥ for all x ‚ààX, then
repeated application of Theorem 16.4.1, with an arbitrary point x0 ‚ààX
as starting point, results in a sequence (xk)‚àû
0
of points in X, deÔ¨Åned as
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
58
Self-concordant functions
xk+1 = x+
k and satisfying the inequality f(xk) ‚â§f(x0) ‚àíkœÅ(‚àíŒ¥) for all k.
Since œÅ(‚àíŒ¥) > 0, this contradicts our assumption that f is bounded below.
Thus, infx‚ààX Œª(f, x) = 0.
Theorem 16.4.3. All sublevel sets of a non-degenerate closed self-concordant
function f : X ‚ÜíR are compact sets if Œª(f, x0) < 1 for some point x0 ‚ààX.
Proof. The sublevel sets are closed since the function is closed, and to prove
that they are also bounded it is enough to prove that the particular sublevel
set S = {x ‚ààX | f(x) ‚â§f(x0)} is bounded, because of Theorem 6.8.3 in
Part I.
So, let x be an arbitrary point in S, and write r = ‚à•x ‚àíx0‚à•x0 and
Œª0 = Œª(f, x0) for short. Then
f(x) ‚â•f(x0) + Df(x0)[x ‚àíx0] + œÅ(‚àír),
according to Theorem 16.3.2, and
Df(x0)[x ‚àíx0] = ‚ü®f ‚Ä≤(x0), x ‚àíx0‚ü©‚â•‚àíŒª(f, x0)‚à•x ‚àíx0‚à•x0 = ‚àíŒª0r,
by Theorem 15.1.2. Combining these two inequalities we obtain the inequal-
ity
f(x0) ‚â•f(x) ‚â•f(x0) ‚àíŒª0r + œÅ(‚àír),
which simpliÔ¨Åes to
r ‚àíln(1 + r) = œÅ(‚àír) ‚â§Œª0r.
Hence,
(1 ‚àíŒª0)r ‚â§ln(1 + r)
and it follows that r ‚â§r0, r0 being the unique positive root of the equation
(1 ‚àíŒª0)r = ln(1 + r). The sublevel set S is thus included in the ellipsoid
{x ‚ààRn | ‚à•x ‚àíx0‚à•x0 ‚â§r0}, and it is therefore a bounded set.
Theorem 16.4.4. A closed self-concordant function f : X ‚ÜíR has a mini-
mum point if Œª(f, x0) < 1 for some point x0 ‚ààX.
Proof. If in addition f is non-degenerate, then S = {x ‚ààX | f(x) ‚â§f(x0)}
is a compact set according to the previous theorem, so the restriction of
f to the sublevel set S attains a mininum, and this minimum is clearly a
global minimum of f. The minimum point is furthermore unique, since non-
degenerate self-concordant functions are strictly convex.
If f is degenerate, then there is a non-degenerate closed self-concordant
function g: X0 ‚ÜíR with the same range as f, according to the discussion
following Theorem 16.2.1. The relationship between the two functions has
the form g(y) = f(Ay + v), where A is an injective linear map and v is
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
59
Self-concordant functions
an arbitrary vector in the recessive subspace Vf.
To the point x0 there
corresponds a point y0 ‚ààX0 such that Ay0 + v = x0 for some v ‚ààVf, and
Œª(g, y0) ‚â§Œª(f, x0) < 1. By the already proven part of the theorem, g has
a minimum point ÀÜy, and this implies that all points in the set AÀÜy + Vf are
minimum points of f.
Theorem 16.4.5. Every downwards bounded closed self-concordant function
f : X ‚ÜíR has a minimum point.
Proof. It follows from Theorem 16.4.2 that there is a point x0 ‚ààX such that
Œª(f, x0) < 1, so the theorem is a corollary of Theorem 16.4.4.
Our next theorem describes how well a given point approximates the
minimum point of a closed self-concordant function.
Theorem 16.4.6. Let f : X ‚ÜíR be a closed self-concordant function with
a minimum point ÀÜx. If x ‚ààX is an arbitrary point with Newton decrement
Œª = Œª(f, x) < 1, then
œÅ(‚àíŒª) ‚â§f(x) ‚àíf(ÀÜx) ‚â§œÅ(Œª),
(16.9)
Œª
1 + Œª ‚â§‚à•x ‚àíÀÜx‚à•x ‚â§
Œª
1 ‚àíŒª,
(16.10)
‚à•x ‚àíÀÜx‚à•ÀÜx ‚â§
Œª
1 ‚àíŒª.
(16.11)
Remark. Since œÅ(t) ‚â§t2 if t ‚â§1
2, we conclude from inequality (16.9) that
f(x) ‚àífmin ‚â§Œª(f, x)2
as soon as Œª(f, x) ‚â§1
2.
Proof. To simplify the notation, let v = x ‚àíÀÜx and r = ‚à•v‚à•x.
The left part of inequality (16.9) follows directly from the remark after
Theorem 16.4.1. To prove the right part of the same inequality, we recall the
inequality
(16.12)
‚ü®f ‚Ä≤(x), v‚ü©‚â§Œª(f, x)‚à•v‚à•x = Œªr,
which we combine with the left part of inequality (16.5) in Theorem 16.3.2
and inequality (iii) in Lemma 16.3.1. This results in the following chain of
inequalities:
f(ÀÜx) = f(x ‚àív) ‚â•f(x) + ‚ü®f ‚Ä≤(x), ‚àív‚ü©+ œÅ(‚àí‚à•‚àív‚à•x)
= f(x) ‚àí‚ü®f ‚Ä≤(x), v‚ü©+ œÅ(‚àír)
‚â•f(x) ‚àíŒªr + œÅ(‚àír) ‚â•f(x) ‚àíœÅ(Œª),
and the proof of inequality (16.9) is now complete.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
60
Self-concordant functions
60
Since x ‚àív = ÀÜx and f ‚Ä≤(ÀÜx) = 0, it follows from inequality (16.12) and the
left part of inequality (16.4) that
Œªr ‚â•‚ü®f ‚Ä≤(x), v‚ü©= ‚ü®f ‚Ä≤(x ‚àív), ‚àív‚ü©‚àí‚ü®f ‚Ä≤(x), ‚àív‚ü©‚â•
‚à•‚àív‚à•2
x
1 + ‚à•‚àív‚à•x
=
r2
1 + r,
and by solving the inequality above with respect to r, we obtain the right
part of inequality (16.10).
The left part of the same inequality obviously holds if r ‚â•1. So assume
that r < 1. Due to inequality (16.7),
‚ü®f ‚Ä≤(x), w‚ü©= ‚ü®f ‚Ä≤(x ‚àív), ‚àíw‚ü©‚àí‚ü®f ‚Ä≤(x), ‚àíw‚ü©‚â§‚à•‚àív‚à•x‚à•‚àíw‚à•x
1 ‚àí‚à•‚àív‚à•x
=
r
1 ‚àír‚à•w‚à•x,
and hence
Œª =
sup
‚à•w‚à•x‚â§1
‚ü®f ‚Ä≤(x), w‚ü©‚â§
r
1 ‚àír,
which gives the left part of inequality (16.10).
To prove the remaining inequality (16.11), we use the left part of inequal-
ity (16.5) with y replaced by x and x replaced by ÀÜx, which results in the
inequality
œÅ(‚àí‚à•x ‚àíÀÜx‚à•ÀÜx) ‚â§f(x) ‚àíf(ÀÜx).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
61
Self-concordant functions
According to the already proven inequality (16.9), f(x)‚àíf(ÀÜx) ‚â§œÅ(Œª), so
it follows that œÅ(‚àí‚à•x ‚àíÀÜx‚à•ÀÜx) ‚â§œÅ(Œª), and by Lemma 16.3.1, this means that
‚à•x ‚àíÀÜx‚à•ÀÜx ‚â§
Œª
1 ‚àíŒª.
Theorem 16.4.7. Let f be a closed self-concordant function whose domain
X is a subset of Rn, and suppose that
ŒΩ = sup{Œª(f, x) | x ‚ààX} < 1.
Then X is equal to the whole space Rn, and f is a constant function.
Proof. It follows from Theorem 16.4.4 that f has a minimum point ÀÜx and
from inequality (16.9) in Theorem 16.4.6 that
œÅ(‚àíŒΩ) ‚â§f(x) ‚àíf(ÀÜx) ‚â§œÅ(ŒΩ)
for all x ‚ààX. Thus, f is a bounded function, and since f is closed, this
implies that X is a set without boundary points. Hence, X = Rn.
Let v be an arbitrary vector in Rn. By applying inequality (16.11) with
x = ÀÜx + tv, we obtain the inequality
t‚à•v‚à•ÀÜx = ‚à•x ‚àíÀÜx‚à•ÀÜx ‚â§
Œª(f, x)
1 ‚àíŒª(f, x) ‚â§
ŒΩ
1 ‚àíŒΩ
for all t > 0, and this implies that ‚à•v‚à•ÀÜx = 0. The recessive subspace Vf of
f is in other words equal to Rn, so f is a constant function according to
Theorem 16.2.1.
16.5
Newton‚Äôs method for self-concordant func-
tions
In this section we show that Newton‚Äôs method converges when the objec-
tive function f : X ‚ÜíR is closed, self-concordant and bounded below. We
shall also give an estimate of the number of iterations needed to obtain the
minimum with a given accuracy œµ ‚àían estimate that only depends on œµ
and the diÔ¨Äerence between the minimum value and the function value at
the starting point. The algorithm starts with a damped phase, which re-
quires no line search as the step length at the point x can be chosen equal to
1/(1+Œª(f, x)), and then enters into a pure phase with quadratic convergence,
when the Newton decrement is suÔ¨Éciently small.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
62
Self-concordant functions
The damped phase
During the damped phase, the points xk in Newton‚Äôs algorithm are generated
recursively by the equation
xk+1 = xk +
1
1 + Œªk
vk,
where Œªk = Œª(f, xk) is the Newton decrement at xk and vk is a Newton
direction at the same point, i.e
f ‚Ä≤‚Ä≤(xk)vk = ‚àíf ‚Ä≤(xk).
According to Theorem 16.4.1, if the starting point x0 is a point in X, then
all generated points xk will lie in X and
f(xk+1) ‚àíf(xk) ‚â§œÅ(‚àíŒªk).
If Œ¥ > 0 and Œªk ‚â•Œ¥, then œÅ(‚àíŒªk) ‚â•œÅ(‚àíŒ¥), because the function œÅ(t)
is decreasing for f¬®or t < 0. So if xN is the Ô¨Årst point of the sequence that
satisÔ¨Åes the inequality ŒªN = Œª(f, xN) < Œ¥, then
fmin ‚àíf(x0) ‚â§f(xN) ‚àíf(x0) =
N‚àí1

k=0
(f(xk+1) ‚àíf(xk))
‚â§‚àí
N‚àí1

k=0
œÅ(‚àíŒªk) ‚â§‚àí
N‚àí1

k=0
œÅ(‚àíŒ¥) = ‚àíNœÅ(‚àíŒ¥),
which implies that att N ‚â§(f(x0) ‚àífmin)/œÅ(‚àíŒ¥). This proves the following
theorem.
Theorem 16.5.1. Let f : X ‚ÜíR be a closed, self-concordant and downwards
bounded function. Using Newton‚Äôs damped algorithm with step size as above,
we need at most
f(x0) ‚àífmin
œÅ(‚àíŒ¥)

iterations to generate a point x with Newton decrement Œª(f, x) < Œ¥ from an
arbitrary starting point x0 in X.
Local convergence
We now turn to the study of Newton‚Äôs pure method for starting points that
are suÔ¨Éciently close to the minimum point ÀÜx. For a corresponding analysis
of Newton‚Äôs damped method we refer to exercise 16.6.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
63
Self-concordant functions
63
Theorem 16.5.2. Let f : X ‚ÜíR be a closed self-concordant function, and
suppose that x ‚ààX is a point with Newton decrement Œª(f, x) < 1. Let ‚àÜxnt
be a Newton direction at x, and let
x+ = x + ‚àÜxnt.
Then, x+ is a point in X and
Œª(f, x+) ‚â§

Œª(f, x)
1 ‚àíŒª(f, x)
2
.
Proof. The conclusion that x+ lies in X follows from Theorem 16.3.2, because
‚à•‚àÜxnt‚à•x = Œª(f, x) < 1. To prove the inequality for Œª(f, x+), we Ô¨Årst use
inequality (16.7) of the same theorem with v = x+ ‚àíx = ‚àÜxnt and obtain
‚ü®f ‚Ä≤(x+), w‚ü©‚â§‚ü®f ‚Ä≤(x), w‚ü©+ ‚ü®f ‚Ä≤‚Ä≤(x)‚àÜxnt, w‚ü©+ Œª(f, x)2‚à•w‚à•x
1 ‚àíŒª(f, x)
= ‚ü®f ‚Ä≤(x), w‚ü©+ ‚ü®‚àíf ‚Ä≤(x), w‚ü©+ Œª(f, x)2‚à•w‚à•x
1 ‚àíŒª(f, x)
= Œª(f, x)2‚à•w‚à•x
1 ‚àíŒª(f, x) .
But
‚à•w‚à•x ‚â§
‚à•w‚à•x+
1 ‚àíŒª(f, x),
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
64
Self-concordant functions
by inequality (16.6), so it follows that
‚ü®f ‚Ä≤(x+), w‚ü©‚â§Œª(f, x)2‚à•w‚à•x+
(1 ‚àíŒª(f, x))2 ,
and this implies that
Œª(f, x+) =
sup
‚à•w‚à•x+‚â§1
‚ü®f ‚Ä≤(x+), w‚ü©‚â§
Œª(f, x)2
(1 ‚àíŒª(f, x))2.
We are now able to prove the following convergence result for Newton‚Äôs
pure method.
Theorem 16.5.3. Suppose that f : X ‚ÜíR is a closed self-concordant func-
tion and that x0 is a point in X with Newton decrement
Œª(f, x0) ‚â§Œ¥ < Œª = 1
2(3 ‚àí
‚àö
5) = 0.381966 . . . .
Let the sequence (xk)‚àû
0 be recursively deÔ¨Åned by
xk+1 = xk + vk,
where vk is a Newton direction at the point xk.
The sequence (f(xk))‚àû
0 converges to the minimum value fmin of the func-
tion f, and if œµ > 0 then
f(xk) ‚àífmin < œµ
for k > A + log2(log2 B/œµ), where A and B are constants that only depend
on Œ¥.
Moreover, if f is a non-degenerate function, then (xk)‚àû
0 converges to the
unique minimum point of f.
Proof. The critical number Œª is a root of the equation (1 ‚àíŒª)2 = Œª, and if
0 ‚â§Œª < Œª then Œª < (1 ‚àíŒª)2.
Let K(Œª) = (1 ‚àíŒª)‚àí2; the function K is increasing in the interval [0, Œª[
and K(Œª)Œª < 1. It therefore follows from Theorem 16.5.2 that the following
inequality is true for all points x ‚ààX with Œª(f, x) ‚â§Œ¥ < Œª:
Œª(f, x+) ‚â§K(Œª(f, x)) Œª(f, x)2 ‚â§K(Œ¥)Œª(f, x)2 ‚â§K(Œ¥)Œ¥Œª(f, x) ‚â§Œª(f, x) ‚â§Œ¥.
Now, let Œªk = Œª(f, xk). Due to the inequality above, it follows by induc-
tion that Œªk ‚â§Œ¥ and that
Œªk+1 ‚â§K(Œ¥)Œª2
k
for all k, and the latter inequality in turn implies that
Œªk ‚â§K(Œ¥)‚àí1
K(Œ¥)Œª0
2k
‚â§(1 ‚àíŒ¥)2
K(Œ¥)Œ¥)2k.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
65
Self-concordant functions
Hence, Œªk tends to 0 as k ‚Üí‚àû, because K(Œ¥)Œ¥ < 1. By the remark
following Theorem 16.4.6,
f(xk) ‚àífmin ‚â§Œª2
k,
if Œªk ‚â§1
2, so we conclude that
lim
k‚Üí‚àûf(xk) = fmin.
To prove the remaining error estimate, we can without loss of generaliza-
tion assume that œµ < Œ¥2, because if œµ > Œ¥2 then already
f(x0) ‚àífmin ‚â§Œª(f, x0)2 ‚â§Œ¥2 < œµ.
Let A and B be the constants deÔ¨Åned by
A = ‚àílog2

‚àí2 log2(K(Œ¥)Œ¥)

and
B = (1 ‚àíŒ¥)4.
Then 0 < B ‚â§1, and log2(log2 B/œµ) is a well-deÔ¨Åned number, since B/œµ ‚â•
(1 ‚àíŒ¥)4/Œ¥2 = (K(Œ¥)Œ¥)‚àí2 > 1. If k > A + log2(log2 B/œµ), then
Œª2
k ‚â§(1 ‚àíŒ¥)4
K(Œ¥)Œ¥
2k+1
< œµ,
and consequently f(xk) ‚àífmin ‚â§Œª2
k < œµ.
If f is a non-degenerate function, then f has a unique minimum point ÀÜx,
and it follow from inequality (16.11) in Theorem 16.4.6 that
‚à•xk ‚àíÀÜx‚à•ÀÜx ‚â§
Œªk
1 ‚àíŒªk
‚Üí0,
as k ‚Üí‚àû.
Since ‚à•¬∑‚à•ÀÜx is a proper norm, this means that xk ‚ÜíÀÜx.
When Œ¥ = 1/3, the values of the constants in Theorem 16.5.3 are A =
0.268 . . . and B = 16/81, and A + log2(log2 B/œµ) = 6.87 for œµ = 10‚àí30. So
with a starting point x0 satisfying Œª(f, x0) < 1/3, Newton‚Äôs algorithm will
produce a function value that approximates the minimum value with an error
less than 10‚àí30 after at most 7 iterations.
Newton‚Äôs method for self-concordant functions
By combining Newton‚Äôs damped method with 1/(1+Œª(f, x)) as damping fac-
tor and Newton‚Äôs pure method, we arrive at the following variant of Newton‚Äôs
method.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
66
Self-concordant functions
66
Newton‚Äôs method
Given a positive number Œ¥ < 1
2(3 ‚àí
‚àö
5), a starting point x0 ‚ààX, and a
tolerance œµ > 0.
1. Initiate: x:= x0.
2. Compute the Newton decrement Œª = Œª(f, x).
3. Go to line 8 if Œª < Œ¥ else continue.
4. Compute a Newton direction ‚àÜxnt at the point x.
5. Update: x:= x + (1 + Œª)‚àí1‚àÜxnt.
6. Go to line 2.
7. Compute the Newton decrement Œª = Œª(f, x).
8. Stopping criterion: stop if Œª < ‚àöœµ. x is an approximate optimal point.
9. Compute a Newton direction ‚àÜxnt at the point x.
10. Update: x:= x + ‚àÜxnt.
11. Go to line 7.
Assuming that f is closed, self-concordant and downwards bounded, the
damped phase of the algorithm, i.e. steps 2‚Äì6, continues during at most
‚åä(f(x0) ‚àífmin)/œÅ(‚àíŒ¥)‚åã
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
67
Self-concordant functions
iterations, and the pure phase 7‚Äì11 ends according to Theorem 16.5.3 after
at most ‚åàA + log2(log2 B/œµ)‚åâiterations. Therefore, we have the following
result.
Theorem 16.5.4. If the function f is closed, self-concordant and bounded
below, then the above Newton method terminates at a point x satisfying
f(x) < fmin + œµ after at most
‚åä(f(x0) ‚àífmin))/œÅ(‚àíŒ¥)‚åã+ ‚åàA + log2(log2 B/œµ)‚åâ
iterations, where A and B are the constants of Theorem 16.5.3.
In particular, 1/œÅ(‚àíŒ¥) = 21.905 when Œ¥ = 1/3, and the second term can
be replaced by the number 7 when œµ ‚â•10‚àí30. Thus, at most
‚åä22(f(x0) ‚àífmin)‚åã+ 7
iterations are required to Ô¨Ånd an approximation to the minimum value that
meets all practical requirements by a wide margin.
Exercises
16.1 Show that the function f(x) = x ln x ‚àíln x is self-concordant on R++.
16.2 Suppose fi : Xi ‚ÜíR are self-concordant functions for i = 1, 2, . . . , m, and
let X = X1 √óX2 √ó¬∑ ¬∑ ¬∑√óXm. Prove that the function f : X ‚ÜíR, deÔ¨Åned by
f(x1, x2, . . . , xm) = f1(x1) + f2(x2) + ¬∑ ¬∑ ¬∑ + fm(xm)
for x = (x1, x2, . . . , xm) ‚ààX, is self-concordant.
16.3 Suppose that f : R++ ‚ÜíR is a three times continuously diÔ¨Äerentiable,
convex function, and that
|f‚Ä≤‚Ä≤‚Ä≤(x)| ‚â§3f‚Ä≤‚Ä≤(x)
x
for all x.
a) Prove that the function
g(x) = ‚àíln(‚àíf(x)) ‚àíln x,
with {x ‚ààR++ | f(x) < 0} as domain, is self-concordant.
[Hint: Use that 3a2b + 3a2c + 2b3 + 2c3 ‚â§2(a2 + b2 + c2)3/2 if a, b, c ‚â•0.]
b) Prove that the function
F(x, y) = ‚àíln(y ‚àíf(x)) ‚àíln x
is self-concordant on the set {(x, y) ‚ààR2 | x > 0, y > f(x)}.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
68
Self-concordant functions
16.4 Show that the following functions f satisfy the conditions of the previous
exercise:
a) f(x) = ‚àíln x
b) f(x) = x ln x
c) f(x) = ‚àíxp, where 0 < p ‚â§1.
16.5 Let us write x‚Ä≤ for (x1, x2, . . . , xn‚àí1) when x = (x1, x2, . . . , xn), and let ‚à•¬∑‚à•
denote the Euclidean norm in Rn‚àí1. Let X = {x ‚ààRn | ‚à•x‚Ä≤‚à•< xn}, and
deÔ¨Åne the function f : X ‚ÜíR by f(x) = ‚àíln(x2
n ‚àí‚à•x‚Ä≤‚à•2). Prove that the
following identity holds for all v ‚ààRn:
D2f(x)[v, v] = 1
2

Df(x)[v]
2
+ 2(x2
n ‚àí‚à•x‚Ä≤‚à•2)(‚à•x‚Ä≤‚à•2‚à•v‚Ä≤‚à•2 ‚àí‚ü®x‚Ä≤, v‚Ä≤‚ü©2) + (vn‚à•x‚Ä≤‚à•2 ‚àíxn‚ü®x‚Ä≤, v‚Ä≤‚ü©)2
(x2n ‚àí‚à•x‚Ä≤‚à•2)2‚à•x‚Ä≤‚à•2
,
and use it to conclude that f is a convex function and that Œª(f, x) = 2 for
all x ‚ààX.
16.6 Convergence for Newton‚Äôs damped method.
Suppose that the function f : X ‚ÜíR is closed and self-concordant, and
deÔ¨Åne for points x ‚ààX with Ô¨Ånite Newton decrement the point x+ by
x+ = x +
1
1 + Œª(f, x)‚àÜxnt,
where ‚àÜxnt is a Newton direction at x.
a) Then x+ is a point in X, according to Theorem 16.3.2. Show that
Œª(f, x+) ‚â§2Œª(f, x)2,
and hence that Œª(f, x+) ‚â§Œª(f, x) if Œª(f, x) ‚â§1
2.
b) Suppose x0 is a point in X with Newton decrement Œª(f, x0) ‚â§1
4, and
deÔ¨Åne the sequence (xk)‚àû
0 recursively by xk+1 = x+
k . Show that
f(xk) ‚àífmin ‚â§1
4 ¬∑
 1
2
2k+1
,
and hence that f(xk) converges quadratically to fmin.
Appendix
We begin with a result on tri-linear forms which was needed in the proof
of the fundamental inequality
D3f(x)[u, v, w]
 ‚â§2‚à•u‚à•x‚à•v‚à•x‚à•w‚à•x for self-
concordant functions.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
69
Self-concordant functions
69
Fix an arbitrary scalar product ‚ü®¬∑ , ¬∑‚ü©on Rn and let ‚à•¬∑‚à•denote the cor-
responding norm, i.e. ‚à•v‚à•= ‚ü®v, v‚ü©1/2. If œÜ(u, v, w) is a symmetric tri-linear
form on Rn √ó Rn √ó Rn, we deÔ¨Åne its norm ‚à•œÜ‚à•by
‚à•œÜ‚à•=
sup
u,v,wÃ∏=0
|œÜ(u, v, w)|
‚à•u‚à•‚à•v‚à•‚à•w‚à•.
The numerator and the denominator in the expression for ‚à•œÜ‚à•are homoge-
neous of the same degree 3, hence
‚à•œÜ‚à•=
sup
(u,v,w)‚ààS3 |œÜ(u, v, w)|,
where S denotes the unit sphere in Rn with respect to the norm ‚à•¬∑‚à•, i.e.
S = {u ‚ààRn | ‚à•u‚à•= 1}.
It follows from the norm deÔ¨Ånition that
|œÜ(u, v, w)| ‚â§‚à•œÜ‚à•‚à•u‚à•‚à•v‚à•‚à•w‚à•
for all vectors u, v, w in Rn.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
70
Self-concordant functions
Since tri-linear forms are continuous and the unit sphere is compact, the
least upper bound ‚à•œÜ‚à•is attained at some point (u, v, w) ‚ààS3, and we will
show that the least upper bound is indeed attained at some point where
u = v = w. This is the meaning of the following theorem.
Theorem 1. Suppose that œÜ(u, v, w) is a symmetric tri-linear form. Then
‚à•œÜ‚à•=
sup
u,v,wÃ∏=0
|œÜ(u, v, w)|
‚à•u‚à•‚à•v‚à•‚à•w‚à•= sup
vÃ∏=0
|œÜ(v, v, v)|
‚à•v‚à•3
.
Remark. The theorem is a special case of the corresponding result for sym-
metric m-multilinear forms, but we only need the case m = 3. The general
case is proved by induction.
Proof. Let
‚à•œÜ‚à•‚Ä≤ = sup
vÃ∏=0
|œÜ(v, v, v)|
‚à•v‚à•3
= sup
‚à•v‚à•=1
|œÜ(v, v, v)|.
We claim that ‚à•œÜ‚à•= ‚à•œÜ‚à•‚Ä≤. Obviously, ‚à•œÜ‚à•‚Ä≤ ‚â§‚à•œÜ‚à•, so we only have to prove
the converse inequality ‚à•œÜ‚à•‚â§‚à•œÜ‚à•‚Ä≤.
To prove this inequality, we need the corresponding result for symmetric
bilinear forms œà(u, v). To such a form there is associated a symmetric linear
operator (matrix) A such that œà(u, v) = ‚ü®Au, v‚ü©, and if e1, e2, . . . , en is an
ON-basis of eigenvectors of A and Œª1, Œª2, . . . , Œªn denote the corresponding
eigenvalues with Œª1 as the one with the largest absolute value, and if u, v ‚ààS
are vectors with coordinates u1, u2, . . . , un and v1, v2, . . . , vn with respect to
the given ON-basis, then
|œà(u, v)| = |
n

i=1
Œªiuivi| ‚â§
n

i=1
|Œªi||ui||vi| ‚â§|Œª1|
n

i=1
|ui||vi|
‚â§|Œª1|
 n

i=1
u2
i
1/2 n

i=1
v2
i
1/2
= |Œª1| = |œà(e1, e1)|,
which proves that sup(u,v)‚ààS2 |œà(u, v)| = supv‚ààS |œà(v, v)|.
We now return to the tri-linear form œÜ(u, v, w). Let (ÀÜu, ÀÜv, ÀÜw) be a point
in S3 where the least upper bound deÔ¨Åning ‚à•œÜ‚à•is attained, i.e.
‚à•œÜ‚à•= œÜ(ÀÜu, ÀÜv, ÀÜw),
and consider the function
œà(u, v) = œÜ(u, v, ÀÜw);
this is a symmetric bilinear form on Rn √ó Rn and
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
71
Self-concordant functions
sup
(u,v)‚ààS2 |œà(u, v)| = ‚à•œÜ‚à•.
But as already proven,
sup
(u,v)‚ààS2 |œà(u, v)| = sup
v‚ààS
|œà(v, v)|.
Therefore, we conclude that we can withour restriction assume that ÀÜu = ÀÜv.
We have in other words shown that the set
A = {(v, w) ‚ààS2 | |œÜ(v, v, w)| = ‚à•œÜ‚à•}
is nonempty. The set A is a closed subset of S2, and hence the number
Œ± = max{‚ü®v, w‚ü©| (v, w) ‚ààA}
exists, and obviously 0 ‚â§Œ± ‚â§1.
Due to tri-linearity,
œÜ(u + v, u + v, w) ‚àíœÜ(u ‚àív, u ‚àív, w) = 4œÜ(u, v, w).
So if u, v, w are arbitrary vectors in S, i.e. vectors with norm 1, then
4|œÜ(u, v, w)| ‚â§|œÜ(u + v, u + v, w)| + |œÜ(u ‚àív, u ‚àív, w)|
‚â§|œÜ(u + v, u + v, w)| + ‚à•œÜ‚à•‚à•u ‚àív‚à•2‚à•w‚à•
= |œÜ(u + v, u + v, w)| ‚àí‚à•œÜ‚à•‚à•u + v‚à•2 + ‚à•œÜ‚à•(‚à•u + v‚à•2 + ‚à•u ‚àív‚à•2)
= |œÜ(u + v, u + v, w)| ‚àí‚à•œÜ‚à•‚à•u + v‚à•2 + ‚à•œÜ‚à•(2‚à•u‚à•2 + 2‚à•v‚à•2)
= |œÜ(u + v, u + v, w)| ‚àí‚à•œÜ‚à•‚à•u + v‚à•2 + 4‚à•œÜ‚à•.
Now choose (v, w) ‚ààA such that ‚ü®v, w‚ü©= Œ±. By the above inequality, we
then have
4‚à•œÜ‚à•= 4|œÜ(v, v, w)| = 4|œÜ(v, w, v)|
‚â§|œÜ(v + w, v + w, v)| ‚àí‚à•œÜ‚à•‚à•v + w‚à•2 + 4‚à•œÜ‚à•,
and it follows that
|œÜ(v + w, v + w, v)| ‚â•‚à•œÜ‚à•‚à•v + w‚à•2.
Note that ‚à•v + w‚à•2 = ‚à•v‚à•2 + ‚à•w‚à•2 + 2‚ü®v, w‚ü©= 2 + 2Œ± > 0. Therefore, we
can form the vector z = (v + w)/‚à•v + w‚à•and write the above inequality as
|œÜ(z, z, v)| ‚â•‚à•œÜ‚à•,
which implies that
(16.13)
|œÜ(z, z, v)| = ‚à•œÜ‚à•
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
72
Self-concordant functions
since z and v are vectors in S. We conclude that the pair (z, v) is an element
of the set A, and hence
Œ± ‚â•‚ü®z, v‚ü©= ‚ü®v, v‚ü©+ ‚ü®w, v‚ü©
‚à•v + w‚à•
=
1 + Œ±
‚àö2 + 2Œ± =

1 + Œ±
2
.
This inequality forces Œ± to be greater than or equal to 1. Hence Œ± = 1 and
‚ü®z, v‚ü©= 1 = ‚à•z‚à•‚à•v‚à•.
So Cauchy‚ÄìSchwarz‚Äôs inequality holds with equality in this case, and this
implies that z = v.
By inserting this in equality (16.13), we obtain the
inequality
‚à•œÜ‚à•‚Ä≤ ‚â•œÜ(v, v, v) = ‚à•œÜ‚à•,
and the proof of the theorem is now complete.
Our second result in this appendix is a uniqueness theorem for functions
that satisfy a special diÔ¨Äerential inequality.
Theorem 2. Suppose that the function y(t) is continuously diÔ¨Äerentiable in
the interval I = [0, b[, that y(t) ‚â•0, y(0) = 0 and y‚Ä≤(t) ‚â§Cy(t)Œ± for some
given constants C > 0 and Œ± ‚â•1. Then, y(t) = 0 in the interval I.
Proof. Let a = sup{x ‚ààI | y(t) = 0 for 0 ‚â§t ‚â§x}. We will prove that a = b
by showing that the assumption a < b gives rise to a contradiction.
By continuity, y(a) = 0. Choose a point c ‚àà]a, b[ and let
M = max{y(t) | a ‚â§t ‚â§c}.
Then choose a point d such that a < d < c and d ‚àía ‚â§1
2C‚àí1M 1‚àíŒ±. The
maximum of the function y(t) on the interval [a, d] is attained at some point
e, and by the least upper bound deÔ¨Ånition of the point a, we have y(e) > 0.
Of course, we also have y(e) ‚â§M, so it follows that
y(e) = y(e) ‚àíy(a) =
 e
a
y‚Ä≤(t) dt ‚â§C
 e
a
y(t)Œ± dt
‚â§C(d ‚àía)y(e)Œ± ‚â§C(d ‚àía)M Œ±‚àí1y(e) ‚â§1
2y(e),
which is a contradiction.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
73
The path-following method
Chapter 17
The path-following method
In this chapter, we describe a method for solving the optimization problem
min f(x)
s.t.
x ‚ààX
when X is a closed subset of Rn with nonempty interior and f is a con-
tinuous function which is diÔ¨Äerentiable in the interior of X.
We assume
throughout that X = cl(int X). Pretty soon, we will restrict ourselves to
convex problems, i.e. assume that X is a convex set and f is a convex func-
tion, in which case, of course, automatically X = cl(int X) for all sets with
nonempty interior.
Descent methods require that the function f is diÔ¨Äerentiable in a neigh-
borhood of the optimal point, and if the optimal point lies on the boundary
of X, then we have a problem. One way to attack this problem is to choose
a function F : int X ‚ÜíR with the property that F(x) ‚Üí+‚àûas x goes
to boundary of X and a parameter ¬µ > 0, and to minimize the function
f(x) + ¬µF(x) over int X. This function‚Äôs minimum point ÀÜx(¬µ) lies in the
interior of X, and since f(x) + ¬µF(x) ‚Üíf(x) as ¬µ ‚Üí0, we can hope that
the function value f(ÀÜx(¬µ)) should be close to the minimum value of f, if the
parameter ¬µ is small enough. The function F acts as a barrier that prevents
the approximating minimum point from lying on the boundary.
The function ¬µ‚àí1f(x)+F(x) has of course the same minimum point ÀÜx(¬µ)
as f(x)+¬µF(x), and for technical reasons it works better to have the param-
eter in front of the objective function f than in front of the barrier function
F. Henceforth, we will therefore instead, with new notation, examine what
happens to the minimum point ÀÜx(t) of the function Ft(x) = tf(x) + F(x),
when the parameter t tends to +‚àû.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
74
The path-following method
74
17.1
Barrier and central path
Barrier
We begin with the formal deÔ¨Ånition of a barrier.
DeÔ¨Ånition. Let X be a closed convex set with nonempty interior. A barrier
to the set X is a diÔ¨Äerentiable function F : int X ‚ÜíR with the property that
limk‚Üí‚àûF(xk) = +‚àûfor all sequences (xk)‚àû
1 that converge to a boundary
point of X.
If a barrier function has a unique minimum point, then this point is called
the analytic center of the set X (with respect to the barrier).
Remark 1. A convex function with an open domain goes to ‚àûat the bound-
ary if and only if it is a closed function. Hence, if F : int X ‚ÜíR is convex
and diÔ¨Äerentiable, then F is a barrier to X if and only if F is closed.
Remark 2. A strictly convex barrier function to a compact convex set has
a unique minimum point in the interior of the set. So compact convex sets
with nonempty interiors have analytic centers with respect to strictly convex
barriers.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
75
The path-following method
Now, let F be a barrier to the closed convex set X, and suppose that we
want to minimize a given function f : X ‚ÜíR. For each real number t ‚â•0
we deÔ¨Åne the function Ft : int X ‚ÜíR by
Ft(x) = tf(x) + F(x).
In particular, F0 = F. The following theorem is the basis for barrier-based
interior-point methods for minimization.
Theorem 17.1.1. Suppose that f : X ‚ÜíR is a continuous function, and let
F be a downwards bounded barrier to the set X. Suppose that the functions
Ft have minimum points ÀÜx(t) in the interior of X for each t > 0. Then,
lim
t‚Üí+‚àûf(ÀÜx(t)) = inf
x‚ààX f(x).
Proof. Let vmin = infx‚ààX f(x) and M = infx‚ààint X F(x). (We do not exclude
the possibility that vmin = ‚àí‚àû, but M is of course a Ô¨Ånite number.)
Choose, given Œ∑ > vmin, a point x‚àó‚ààint X such that f(x‚àó) < Œ∑. Then
vmin ‚â§f(ÀÜx(t)) ‚â§f(ÀÜx(t)) + t‚àí1(F(ÀÜx(t)) ‚àíM) = t‚àí1
Ft(ÀÜx(t)) ‚àíM

‚â§t‚àí1
Ft(x‚àó) ‚àíM

= f(x‚àó) + t‚àí1(F(x‚àó) ‚àíM).
Since the right hand side of this inequality tends to f(x‚àó) as t ‚Üí+‚àû, it
follows that vmin ‚â§f(ÀÜx(t)) < Œ∑ for all suÔ¨Éciently large numbers t, and this
proves the theorem.
In order to use the barrier method, one needs of course an appropriate
barrier to the given set. For sets of the type
X = {x ‚àà‚Ñ¶| gi(x) ‚â§0, i = 1, 2, . . . , m}
we will use the logarithmic barrier function
(17.1)
F(x) = ‚àí
m

i=1
ln(‚àígi(x)).
Note that the barrier function F is convex if all functions gi : ‚Ñ¶‚ÜíR are
convex. In this case, X is a convex set, and the interior of X is nonempty if
Slater‚Äôs condition is satisÔ¨Åed, i.e. if there is a point x ‚àà‚Ñ¶such that gi(x) < 0
for all i.
Other examples of barriers are the exponential barrier function
F(x) =
m

i=1
e‚àí1/gi(x)
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
76
The path-following method
and the power function barriers
F(x) =
m

i=1
(‚àígi(x))‚àíp,
where p > 0.
Central path
DeÔ¨Ånition. Let F be a barrier to the set X and suppose that the functions
Ft have unique minimum points ÀÜx(t) ‚ààint X for all t ‚â•0.
The curve
{ÀÜx(t) | t ‚â•0} is called the central path for the problem minx‚ààX f(x).
Note that ÀÜx(0) is the analytic center of X with respect to the barrier F,
so the central path starts at the analytic center.
Since the gradient is zero at an optimal point, we have
(17.2)
tf ‚Ä≤(ÀÜx(t)) + F ‚Ä≤(ÀÜx(t)) = 0
for all points on the central path.
The converse is true if the objective
function f and the barrier function F are convex, i.e. ÀÜx(t) is a point on the
central path if and only if equation (17.2) is satisÔ¨Åed.
The logarithmic barrier F to X = {x ‚àà‚Ñ¶| gi(x) ‚â§0, i = 1, 2, . . . , m}
has derivative
F ‚Ä≤(x) = ‚àí
m

i=1
1
gi(x)g‚Ä≤
i(x),
so the central path equation (17.2) has in this case the following form for
t > 0:
(17.3)
f ‚Ä≤(ÀÜx(t)) ‚àí1
t
m

i=1
1
gi(ÀÜx(t))g‚Ä≤
i(ÀÜx(t)) = 0.
Let us now consider a convex optimization problem of the following type:
(P)
min f(x)
s.t.
gi(x) ‚â§0, i = 1, 2, . . . , m
We assume that Slater‚Äôs condition is satisÔ¨Åed and that the problem has an
optimal solution ÀÜx.
The corresponding Lagrange function L is given by
L(x, Œª) = f(x) +
m

i=1
Œªigi(x),
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
77
The path-following method
77
x1
x2
ÀÜx
Figure 17.1. The central path associated with the problem of mini-
mizing the function f(x) = x1ex1+x2 over X = {x ‚ààR2 | x2
1 + x2
2 ‚â§1}
with barrier function F(x) = (1 ‚àíx2
1 ‚àíx2
2)‚àí1. The minimum point is
ÀÜx = (‚àí0.5825, 0.8128).
and it follows from equation (17.3) that L‚Ä≤
x(ÀÜx(t), ÀÜŒª) = 0, if ÀÜŒª ‚ààRm
+ is the
vector deÔ¨Åned by
ÀÜŒªi = ‚àí
1
tgi(ÀÜx(t)).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
78
The path-following method
x1
1
x2
ÀÜx
ÀÜxF
Figure 17.2. The central path for the LP problem minx‚ààX 2x1 ‚àí3x2
with X = {x ‚ààR2 | x2 ‚â•0, x2 ‚â§3x1, x2 ‚â§x1 + 1, x1 + x2 ‚â§4}
and logarithmic barrier. The point ÀÜxF is the analytic center of X, and
ÀÜx = (1.5, 2.5) is the optimal solution.
Since the Lagrange function is convex in the variable x, we conclude that
ÀÜx(t) is a minimum point for the function L(¬∑ , ÀÜŒª).
The value at ÀÜŒª of the
dual function œÜ: Rm
+ ‚ÜíR to our minimization problem (P) is therefore by
deÔ¨Ånition
œÜ(ÀÜŒª) = L(ÀÜx(t), ÀÜŒª) = f(ÀÜx(t)) ‚àím/t.
By weak duality, œÜ(ÀÜŒª) ‚â§f(ÀÜx), so it follows that
f(ÀÜx(t)) ‚àím/t ‚â§f(ÀÜx).
We have thus arrived at the following approximation theorem, which for
convex problems with logarithmic barrier provides more precise information
than Theorem 17.1.1.
Theorem 17.1.2. The points ÀÜx(t) on the central path for the convex mini-
mization problem (P) with optimal solution ÀÜx and logarithmic barrier satisfy
the inequality
f(ÀÜx(t)) ‚àíf(ÀÜx) ‚â§m
t .
Note that the estimate of the theorem depends on the number of con-
straints but not on the dimension.
17.2
Path-following methods
A strategy for determining the optimal value of the convex optimization
problem
(P)
min f(x)
s.t.
gi(x) ‚â§0, i = 1, 2, . . . , m
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
79
The path-following method
for twice continuously diÔ¨Äerentiable objective and constraint functions with
an error that is less than or equal to œµ, would in light of Theorem 17.1.2
be to solve the optimization problem min Ft(x) with logarithmic barrier F
for t = m/œµ, using for example Newton‚Äôs method. The strategy works for
small problems and with moderate demands on accuracy, but better results
are obtained by solving the problems min Ft(x) for an increasing sequence of
t-values until t ‚â•m/œµ.
A simple version of the barrier method or the path-following method, as
it is also called, therefore looks like this:
Path-following method
Given a starting point x = x0 ‚ààint X, a real number t = t0 > 0, an update
parameter Œ± > 1 and a tolerance œµ > 0.
Repeat
1. Compute ÀÜx(t) by minimizing Ft = tf + F with x as starting point
2. Update: x:= ÀÜx(t).
3. Stopping criterion: stop if m/t ‚â§œµ.
4. Increase t: t:= Œ±t.
Step 1 is called an outer iteration or a centering step because it is about
Ô¨Ånding a point on the central path. To minimize the function Ft, Newton‚Äôs
method is used, and the iterations of Newton‚Äôs method to compute ÀÜx(t) with
x as the starting point are called inner iterations.
It is not necessary to compute ÀÜx(t) exactly in the outer iterations; the
central path serves no other function than to lead to the optimal point ÀÜx,
and good approximations to points on the central path will also give rise to
a sequence of points which converges to ÀÜx.
The computational cost of the method obviously depends on the total
number of outer iterations that have to be performed before the stopping
criterion is met, and on the number of inner iterations in each outer iteration.
The update parameter Œ±
The parameter Œ± (and the initial value t0) determines the number of outer
iterations required to reach the stopping criterion t ‚â•m/œµ. If Œ± is small, i.e.
close to 1, then many outer iterations are needed, but on the other hand,
each outer iteration requires few inner iterations since the minimum point
x = ÀÜx(t) of the function Ft is then a very good starting point in Newton‚Äôs
algorithm for the problem of minimizing the function FŒ±t.
For large Œ± values the opposite is true; few outer iterations are needed,
but each outer iteration now requires more Newton steps as the starting point
ÀÜx(t) is farther from the minimum point ÀÜx(Œ±t).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
80
The path-following method
80
From experience, it turns out, however, that the above two eÔ¨Äects tend
to oÔ¨Äset each other. The total number of Newton steps is roughly constant
over a wide range of Œ±, and values of Œ± between 10 and 20 usually work well.
The initial value t0
The choice of the starting value t0 is also signiÔ¨Åcant. A small value requires
many outer iterations before the stopping criterion is met. A large value,
on the other hand, requires many inner iterations in the Ô¨Årst outer iteration
before a suÔ¨Éciently good approximation to the point ÀÜx(t0) on the central
path has been found. Since f(ÀÜx(t0)) ‚àíf(ÀÜx) ‚âàm/t0, it may be reasonable
to choose t0 so that m/t0 is of the same magnitude as f(x0) ‚àíf(ÀÜx). The
problem, of course, is that the optimal value f(ÀÜx) is not known a priori, so
it is necessary to use a suitable estimate. If, for example, a feasible point Œª
for the dual problem is known and œÜ is the dual function, then œÜ(Œª) can be
used as an approximation of f(ÀÜx), and t0 = m/(f(x0) ‚àíœÜ(Œª)) can be taken
as initial t-value.
The starting point x0
The starting point x0 must lie in the interior of X, i.e. it has to satisfy all
constraints with strict inequality. If such a point is not known in advance,
then one can use the barrier method on an artiÔ¨Åcial problem to compute such
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
‚ñ∂‚ñ∂enroll by September 30th, 2014 and 
‚ñ∂‚ñ∂save up to 16% on the tuition!
‚ñ∂‚ñ∂pay in 10 installments / 2 years
‚ñ∂‚ñ∂Interactive Online education
‚ñ∂‚ñ∂visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
81
The path-following method
a point, or to conclude that the original problem has no feasible points.
The procedure is called phase 1 of the path-following method and works as
follows.
Consider the inequalities
(17.4)
gi(x) ‚â§0,
i = 1, 2, . . . , m
and suppose that the functions gi : ‚Ñ¶‚ÜíR are convex and twice continuously
diÔ¨Äerentiable. To determine a point that satisÔ¨Åes all inequalities strictly or
to determine that there is no such point, we form the optimization problem
(17.5)
min s
s.t.
gi(x) ‚â§s,
i = 1, 2, . . . , m
in the variables x and s. This problem has strictly feasible points, because we
can Ô¨Årst choose x0 ‚àà‚Ñ¶arbitrarily and then choose s0 > maxi gi(x0), and we
obtain in this way a point (x0, s0) ‚àà‚Ñ¶√ó R that satisÔ¨Åes the constraints with
strict inequalities. The functions (x, s) ‚Üígi(x) ‚àís are obviously convex.
We can therefore use the path-following method on the problem (17.5), and
depending on the sign of the problem‚Äôs optimal value vmin, we get three cases.
vmin < 0: The system (17.4) has strictly feasible solutions. Indeed, if (x, s)
is a feasible point for the problem (17.5) with s < 0, then gi(x) < 0
for all i. This means that it is not necessary to solve the optimization
problem (17.5) with great accuracy. The algorithm can be stopped as
soon as it has generated a point (x, s) with s < 0.
vmin > 0: The system (17.4) is infeasible. Also in this case, it is not necessary
to solve the problem with great accuracy. We can stop as soon as we
have found a feasible point for the dual problem with a positive value
of the dual function, since this implies that vmin > 0.
vmin = 0: If the greatest lower bound vmin = 0 is attained, i.e. if there is
a point (ÀÜx, ÀÜs) with ÀÜs = 0, then the system (17.4) is feasible but not
strictly feasible. The system (17.4) is infeasible if vmin is not attained.
In practice, it is of course impossible to determine exactly that vmin = 0;
the algorithm terminates with the conclusion that |vmin| < œµ for some
small positive number œµ, and we can only be sure that the system
gi(x) < ‚àíœµ is infeasible and that the system gi(x) ‚â§œµ is feasible.
Convergence analysis
At the beginning of outer iteration number k, we have t = Œ±k‚àí1t0 . The
stopping criterion will be triggered as soon as m/(Œ±k‚àí1t0) ‚â§œµ, i.e. when
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
82
The path-following method
82
k ‚àí1 ‚â•(log(m/(œµt0))/ log Œ±. The number of outer iterations is thus equal to
log(m/(œµt0)
log Œ±

+ 1
(for œµ ‚â§m/t0).
The path-following method therefore works, provided that the minimiza-
tion problems
(17.6)
min tf(x) + F(x)
s.t.
x ‚ààint X
can be solved for t ‚â•t0. Using Newton‚Äôs method, this is true, for example,
if the objective functions satisfy the conditions of Theorem 15.2.4, i.e. if Ft
is strongly convex, has a Lipschitz continuous derivative and the sublevel set
corresponding to the starting point is closed.
A question that remains to be resolved is whether the problem (17.6)
gets harder and harder, that is requires more innner iterations, when t grows.
Practical experience shows that this is not so ‚àíin most problems, the number
of Newton steps seems to be roughly constant when t grows. For problems
with self-concordant objective and barrier functions, it is possible to obtain
exact estimates of the total number of iterations needed to solve the opti-
mization problem (P) with a given accuracy, and this will be the theme in
Chapter 18.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
83
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Chapter 18
The path-following method
with self-concordant barrier
18.1
Self-concordant barriers
DeÔ¨Ånition. Let X be a closed convex subset of Rn with nonempty interior
int X, and let ŒΩ be a nonnegative number. A function f : int X ‚ÜíR is called
a self-concordant barrier to X with parameter ŒΩ, or shorter a ŒΩ-self-concordant
barrier, if the function is closed, self-concordant and non-constant, and the
Newton decrement satisÔ¨Åes the inequality
(18.1)
Œª(f, x) ‚â§ŒΩ1/2
for all x ‚ààint X.
It follows from Theorem 15.1.2 and Theorem 15.1.3 that inequality (18.1)
holds if and only if
|‚ü®f ‚Ä≤(x), v‚ü©| ‚â§ŒΩ1/2‚à•v‚à•x
for all vectors v ‚ààRn, or equivalently, if and only if

Df(x)[v]
2 ‚â§ŒΩ D2f(x)[v, v]
for all v ‚ààRn.
A closed self-concordant function f : ‚Ñ¶‚ÜíR with the property that
supx‚àà‚Ñ¶Œª(f, x) < 1 is necessarily constant and the domain ‚Ñ¶is equal to Rn,
according to Theorem 16.4.7. The parameter ŒΩ of a self-concordant barrier
must thus be greater than or equal to 1.
Example 18.1.1. The function f(x) = ‚àíln x is a 1-self-concordant barrier
to the interval [0, ‚àû[, because f is closed and self-concordant and Œª(f, x) = 1
for all x > 0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
84
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Example 18.1.2. Convex quadratic functions
f(x) = 1
2‚ü®x, Ax‚ü©+ ‚ü®b, x‚ü©+ c
are self-concordant on Rn, but they do not function as self-concordant barri-
ers, because sup Œª(f, x) = ‚àûfor all non-constant convex quadratic functions
f, according to Example 15.1.2.
We will show later that only subsets of halfspaces can have self-concordant
barriers, so there is no self-concordant barrier to the whole Rn.
Example 18.1.3. Let g(x) be a non-constant convex, quadratic function.
The function f, deÔ¨Åned by
f(x) = ‚àíln(‚àíg(x)),
is a 1-self-concordant barrier to the set X = {x ‚ààRn | g(x) ‚â§0}.
Proof. Let g(x) = 1
2‚ü®x, Ax‚ü©+ ‚ü®b, x‚ü©+ c, let v be an arbitrary vector in Rn,
and set
Œ± = ‚àí1
g(x)Dg(x)[v]
and
Œ≤ = ‚àí1
g(x)D2g(x)[v, v] = ‚àí1
g(x)‚ü®v, Av‚ü©,
where x is an arbitrary point in the interior of X. Note that Œ≤ ‚â•0 and that
D3g(x)[v, v, v] = 0. It therefore follows from the diÔ¨Äerentiation rules that
Df(x)[v] = ‚àí1
g(x)Dg(x)[v] = Œ±,
D2f(x)[v, v] =
1
g(x)2

Dg(x)[v]
2 ‚àí
1
g(x)D2g(x)[v, v] = Œ±2 + Œ≤ ‚â•0,
D3f(x)[v, v, v] = ‚àí
2
g(x)3

Dg(x)[v]
3 +
3
g(x)2D2g(x)[v, v]Dg(x)[v]
‚àí
1
g(x)D3g(x)[v, v, v] = 2Œ±3 + 3Œ±Œ≤.
The function f is convex since its second derivative is positive semidef-
inite, and it is closed since f(x) ‚Üí+‚àûas g(x) ‚Üí0. By squaring it is
easy to show that the inequality |2Œ±3 + 3Œ±Œ≤| ‚â§2(Œ±2 + Œ≤)3/2 holds for all
Œ± ‚ààR and all Œ≤ ‚ààR+, and obviously Œ±2 ‚â§Œ±2 + Œ≤.
This means that
D3f(x)[v, v, v]
 ‚â§2

D2f(x)[v, v]
3/2 and that (Df(x)[v])2 ‚â§D2f(x)[v, v].
So f is 1-self-concordant.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
85
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
85
The following three theorems show how to build new self-concordant bar-
riers from given ones.
Theorem 18.1.1. If f is a ŒΩ-self-concordant barrier to the set X and Œ± ‚â•1,
then Œ±f is an Œ±ŒΩ-self-concordant barrier to X.
Proof. The proof is left as a simple exercise.
Theorem 18.1.2. If f is a ¬µ-self-concordant barrier to the set X and g is a
ŒΩ-self-concordant barrier to the set Y , then the sum f +g is a self-concordant
barrier with parameter ¬µ + ŒΩ to the intersection X ‚à©Y . And f + c is a ¬µ-
self-concordant barrier to X for each constant c.
Proof. The sum f + g is a closed convex function, and it is self-concordant
on the set int(X ‚à©Y ) according to Theorem 16.1.5. To prove that the sum
is a self-concordant barrier with parameter (¬µ + ŒΩ), we assume that v is an
arbitrary vector in Rn and write a = D2f(x)[v, v] and b = D2g(x)[v, v]. We
then have, by deÔ¨Ånition,

Df(x)[v]
2 ‚â§¬µa
and

Dg(x)[v]
2 ‚â§ŒΩb,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
86
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
and using the inequality 2‚àö¬µŒΩab ‚â§ŒΩa + ¬µb between the geometric and the
arithmetic mean, we obtain the inequality

D(f + g)(x)[v]
2 =

Df(x)[v]
2 +

Dg(x)[v]
2 + 2Df(x)[v] ¬∑ Dg(x)[v]
‚â§¬µa + ŒΩb + 2

¬µaŒΩb ‚â§¬µa + ŒΩb + ŒΩa + ¬µb
= (¬µ + ŒΩ)(a + b) = (¬µ + ŒΩ) D2(f + g)(x)[v, v],
which means that Œª(f + g, x) ‚â§(¬µ + ŒΩ)1/2.
The assertion about the sum f + c is trivial, since Œª(f, x) = Œª(f + c, x)
for constants c.
Theorem 18.1.3. Suppose that A: Rm ‚ÜíRn is an aÔ¨Éne map and that f is
a ŒΩ-self-concordant barrier to the subset X of Rn. The composition g = f ‚ó¶A
is then a ŒΩ-self-concordant barrier to the inverse image A‚àí1(X).
Proof. The proof is left as an exercise.
Example 18.1.4. It follows from Example 18.1.1 and Theorems 18.1.2 and
18.1.3 that the function
f(x) = ‚àí
m

i=1
ln(bi ‚àí‚ü®ai, x‚ü©)
is an m-self-concordant barrier to the polyhedron
X = {x ‚ààRn | ‚ü®ai, x‚ü©‚â§bi, i = 1, 2, . . . , m}.
Theorem 18.1.4. If f is a ŒΩ-self-concordant barrier to the set X, then
‚ü®f ‚Ä≤(x), y ‚àíx‚ü©‚â§ŒΩ
for all x ‚ààint X and all y ‚ààX.
Remark. It follows that a set with a self-concordant barrier must be a subset
of some halfspace. Indeed, a set X with a ŒΩ-self-concordant barrier is a subset
of the closed halfspace {y ‚ààRn | ‚ü®c, y‚ü©‚â§ŒΩ + ‚ü®c, x0‚ü©}, where x0 ‚ààint X is an
arbitrary point with c = f ‚Ä≤(x0) Ã∏= 0.
Proof. Fix x ‚ààint X and y ‚ààX, let xt = x+t(y ‚àíx) and deÔ¨Åne the function
œÜ by setting œÜ(t) = f(xt). Then œÜ is certainly deÔ¨Åned on the open interval
]Œ±, 1[ for some negative number Œ±, since x is an iterior point. Moreover,
œÜ‚Ä≤(t) = Df(xt)[y ‚àíx],
and especially, œÜ‚Ä≤(0) = Df(x)[y ‚àíx] = ‚ü®f ‚Ä≤(x), y ‚àíx‚ü©. We will prove that
œÜ‚Ä≤(0) ‚â§ŒΩ.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
87
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
If œÜ‚Ä≤(0) ‚â§0, then we are done, so assume that œÜ‚Ä≤(0) > 0. By ŒΩ-self-
concordance,
œÜ‚Ä≤‚Ä≤(t) = D2f(xt)[y ‚àíx, y ‚àíx] ‚â•ŒΩ‚àí1
Df(xt)[y ‚àíx]
2 = ŒΩ‚àí1œÜ‚Ä≤(t)2 ‚â•0.
The derivative œÜ‚Ä≤ is thus increasing, and this implies that œÜ‚Ä≤(t) ‚â•œÜ‚Ä≤(0) > 0
for t ‚â•0. Furthermore,
d
dt

‚àí
1
œÜ‚Ä≤(t)

= œÜ‚Ä≤‚Ä≤(t)
œÜ‚Ä≤(t)2 ‚â•1
ŒΩ
for all t in the interval [0, 1[, so by integrating the last mentioned inequality
over the interval [0, Œ≤], where Œ≤ < 1, we obtain the inequality
1
œÜ‚Ä≤(0) >
1
œÜ‚Ä≤(0) ‚àí
1
œÜ‚Ä≤(Œ≤) =
 Œ≤
0
d
dt

‚àí
1
œÜ‚Ä≤(t)

dt ‚â•Œ≤
ŒΩ .
Hence, œÜ‚Ä≤(0) < ŒΩ/Œ≤ for all Œ≤ < 1, which implies that œÜ‚Ä≤(0) ‚â§ŒΩ.
Theorem 18.1.5. Suppose that f is a ŒΩ-self-concordant barrier to the set X.
If x ‚ààint X, y ‚ààX and ‚ü®f ‚Ä≤(x), y ‚àíx‚ü©‚â•0, then
‚à•y ‚àíx‚à•x ‚â§ŒΩ + 2‚àöŒΩ.
Remark. If x ‚ààint X is a minimum point, then ‚ü®f ‚Ä≤(x), y ‚àíx‚ü©= 0 for all
points y ‚ààX, since f ‚Ä≤(x) = 0. Hence, ‚à•y ‚àíx‚à•x ‚â§ŒΩ + 2‚àöŒΩ for all y ‚ààX if x
is a minimum point.
Proof. Let r = ‚à•y‚àíx‚à•x. If r ‚â§‚àöŒΩ, then there is nothing to prove, so assume
that r > ‚àöŒΩ, and consider for Œ± = ‚àöv/r the point z = x + Œ±(y ‚àíx), which
lies in the interior of X since Œ± < 1. By using Theorem 18.1.4 with z instead
of x, the assumption ‚ü®f ‚Ä≤(x), y ‚àíx‚ü©‚â•0, Theorem 16.3.2 and the equalities
y ‚àíz = (1 ‚àíŒ±)(y ‚àíx) and z ‚àíx = Œ±(y ‚àíx), we obtain the following chain
of inequalities and equalities:
ŒΩ ‚â•‚ü®f ‚Ä≤(z), y ‚àíz‚ü©= (1 ‚àíŒ±)‚ü®f ‚Ä≤(z), y ‚àíx‚ü©‚â•(1 ‚àíŒ±)‚ü®f ‚Ä≤(z) ‚àíf ‚Ä≤(x), y ‚àíx‚ü©
= 1 ‚àíŒ±
Œ±
‚ü®f ‚Ä≤(z) ‚àíf ‚Ä≤(x), z ‚àíx‚ü©‚â•1 ‚àíŒ±
Œ±
¬∑
‚à•z ‚àíx‚à•2
x
1 + ‚à•z ‚àíx‚à•x
= (1 ‚àíŒ±)Œ±‚à•y ‚àíx‚à•2
x
1 + Œ±‚à•y ‚àíx‚à•x
= r‚àöŒΩ ‚àíŒΩ
1 + ‚àöŒΩ .
The inequality between the extreme ends simpliÔ¨Åes to r ‚â§ŒΩ + 2‚àöŒΩ, which
is the desired inequality.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
88
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
88
Given a self-concordant funktion f with the corresponding local seminorm
‚à•¬∑‚à•x, we set
E(x; r) = {y ‚ààRn | ‚à•y ‚àíx‚à•x ‚â§r}.
If f is non-degenerate, then ‚à•¬∑‚à•x is a norm at each point x ‚ààint X, and the
set E(x; r) is a closed ellipsoid in Rn with axis directions determined by the
eigenvectors of the second derivative f ‚Ä≤‚Ä≤(x).
For non-degenerate self-concordant barriers we now have the following
corollary to Theorem 18.1.5.
Theorem 18.1.6. Suppose that f is a non-degenerate ŒΩ-self-concordant bar-
rier to the closed convex set X. Then f attains a minimum if and only if X
is a bounded set. The minimum point ÀÜxf ‚ààint X is unique in that case, and
E(ÀÜxf; 1) ‚äÜX ‚äÜE(ÀÜxf; ŒΩ + 2‚àöŒΩ).
Remark. A closed self-concordant function whose domain does not contain
any line, is automatically non-degenerate, so it is not necessary to state
explicitly that a self-concordant barrier to a compact set should be non-
degenerate.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master‚Äôs Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master‚Äôs programmes
‚Ä¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
‚Ä¢ 1st place: MSc International Business
‚Ä¢ 1st place: MSc Financial Economics
‚Ä¢ 2nd place: MSc Management of Learning
‚Ä¢ 2nd place: MSc Economics
‚Ä¢ 2nd place: MSc Econometrics and Operations Research
‚Ä¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‚ÄòBeste Studies‚Äô ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
89
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Proof. The sublevel sets of a closed convex function are closed, so if X is
a bounded set, then each sublevel set {x ‚ààint X | f(x) ‚â§Œ±} is both closed
and bounded, and this implies that f has a minimum, and the minimum
point of a non-degenerate convex function is necessarily unique.
Conversely, assume that f has a minimum point ÀÜxf. Then by the remark
following Theorem 18.1.5, ‚à•y ‚àíÀÜxf‚à•ÀÜxf ‚â§ŒΩ + 2‚àöŒΩ for all y ‚ààX, and this
amounts to the right inclusion in Theorem 18.1.6, which implies, of course,
that X is a bounded set.
The remaining left inclusion follows from Theorem 16.3.2, which implies
that the open ellipsoid {y ‚ààRn | ‚à•y ‚àíx‚à•x < 1} is a subset of int X for each
choice of x ‚ààint X. The closure E(x; 1) is therefore a subset of X, and we
obtain the left inclusion by choosing x = ÀÜxf.
Given a self-concordant barrier to a set X we will need to compare the
local seminorms ‚à•v‚à•x and ‚à•v‚à•y of a vector at diÔ¨Äerent points x and y, and in
order to achieve this we need a measure for the distance from y to x relative
the distance from y to the boundary of X along the half-line from x through
x. The following deÔ¨Ånition provides us with the relevant measure.
DeÔ¨Ånition. Let X be a closed convex subset of Rn with nonempty interior.
For each y ‚ààint X we deÔ¨Åne a function œÄy : Rn ‚ÜíR+ by setting
œÄy(x) = inf{t > 0 | y + t‚àí1(x ‚àíy) ‚ààX}.
Obviously, œÄy(y) = 0. To determine œÄy(x) if x Ã∏= y, we consider the half-
line from y through x; if the half-line intersects the boundary of X in a point
z, then œÄy(x) = ‚à•x ‚àíy‚à•/‚à•z ‚àíy‚à•(with respect to arbitrary norms), and if
the entire half-line lies in X, then œÄy(x) = 0. We note that œÄy(x) < 1 for
interior points x, that œÄy(x) = 1 for boundary points x, and that œÄy(x) > 1
for points outside X.
We could also have deÔ¨Åned the function œÄy in terms of the Minkowski
functional that was introduced in Section 6.10 of Part I, because
œÄy(x) = œÜ‚àíy+X(x ‚àíy),
where œÜ‚àíy+X is the Minkowski functional of the set ‚àíy + X.
The following simple estimate of œÄy(x) will be needed later on.
Theorem 18.1.7. Let X be a compact convex set, let x and y be points in
the interior of X, and suppose that
B(x, r) ‚äÜX ‚äÜB(0; R),
where the balls are given with respect to an arbitrary norm ‚à•¬∑‚à•. Then
œÄy(x) ‚â§
2R
2R + r.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
90
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Proof. The inequality is trivially true if x = y, so suppose that x Ã∏= y. The
half-line from y through x intersects the boundary of X in a point z and
‚à•z ‚àíy‚à•= ‚à•z ‚àíx‚à•+ ‚à•x ‚àíy‚à•. Furthermore, ‚à•z ‚àíx‚à•‚â•r and ‚à•x ‚àíy‚à•‚â§2R,
so it follows that
œÄy(x) = ‚à•x ‚àíy‚à•
‚à•z ‚àíy‚à•=

1 + ‚à•z ‚àíx‚à•
‚à•x ‚àíy‚à•
‚àí1
‚â§

1 + r
2R
‚àí1
=
2R
2R + r.
The direction derivative ‚ü®f ‚Ä≤(x), v‚ü©of a ŒΩ-self-concordant barrier function
f is bounded by ‚àöŒΩ‚à•v‚à•x, by deÔ¨Ånition. Our next theorem shows that the
same direction derivative is also bounded by a constant times ‚à•v‚à•y, if y is an
arbitrary point in the domain of f. The two local norms ‚à•v‚à•x and ‚à•v‚à•y are
also compared.
Theorem 18.1.8. Let f be a ŒΩ-self-concordant barrier to X, and let x and y
be two points in the interior of X. Then, for all vectors v
|‚ü®f ‚Ä≤(x), v‚ü©| ‚â§
ŒΩ
1 ‚àíœÄy(x) ‚à•v‚à•y
(18.2)
and
‚à•v‚à•x ‚â§ŒΩ + 2‚àöŒΩ
1 ‚àíœÄy(x)‚à•v‚à•y.
(18.3)
Proof. The two inequalities hold if y = x since
|‚ü®f ‚Ä≤(x), v‚ü©| ‚â§‚àöŒΩ‚à•v‚à•x ‚â§ŒΩ‚à•v‚à•x
and œÄx(x) = 0. They also hold if ‚à•v‚à•y = 0, i.e. if the vector v belongs to the
recessive subspace of f, because then ‚à•v‚à•x = 0 and ‚ü®f ‚Ä≤(x), v‚ü©= 0. Assume
henceforth that y Ã∏= x and that ‚à•v‚à•y Ã∏= 0.
First consider the case ‚à•v‚à•y = 1, and let s be an arbitrary number greater
than ŒΩ + 2‚àöŒΩ. Then, by Theorems 16.3.2 and 18.1.5, we conclude that
(i) The two points y ¬± v lie in X.
(ii) At least one of the two points x ¬±
s
‚à•v‚à•x
v lies outside X.
By the deÔ¨Ånition of œÄy(x) there is a vector z ‚ààX such that
x = y + œÄy(x)(z ‚àíy),
and since
x ¬± (1 ‚àíœÄy(x))v = œÄy(x)z + (1 ‚àíœÄy(x))(y ¬± v),
it follows from convexity that
(iii) The two points x ¬± (1 ‚àíœÄy(x))v lie in X.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
91
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
91
It now follows from (iii) and Theorem 18.1.4 that
‚ü®f ‚Ä≤(x), ¬±v‚ü©=
1
1 ‚àíœÄy(x)‚ü®f ‚Ä≤(x), x ¬± (1 ‚àíœÄy(x))v ‚àíx‚ü©‚â§
ŒΩ
1 ‚àíœÄy(x),
which means that
|‚ü®f ‚Ä≤(x), v‚ü©| ‚â§
ŒΩ
1 ‚àíœÄy(x).
This proves inequality (18.2) for vectors v with ‚à•v‚à•y = 1, and if v is an
arbitrary vector with ‚à•v‚à•y Ã∏= 0, we obtain inequality (18.2) by replacing v in
the inequality above with v/‚à•v‚à•y.
By combining the two assertions (ii) and (iii) we conclude that
1 ‚àíœÄy(x) <
s
‚à•v‚à•x
,
i.e. that
‚à•v‚à•x <
s
1 ‚àíœÄy(x) =
s
1 ‚àíœÄy(x)‚à•v‚à•y,
and since this holds for all s > ŒΩ + 2‚àöŒΩ, it follows that
‚à•v‚à•x ‚â§ŒΩ + 2‚àöŒΩ
1 ‚àíœÄy(x)‚à•v‚à•y.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
92
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
This proves inequality (18.3) in the case ‚à•v‚à•y = 1, and since the inequality
is homogeneous, it holds in general.
DeÔ¨Ånition. Let ‚à•¬∑‚à•x be the local seminorm at x which is associated with the
two times diÔ¨Äerentiable convex function f : X ‚ÜíR, where X is a subset
of Rn. The corresponding dual local norm is the function ‚à•¬∑‚à•‚àó
x : Rn ‚ÜíR,
which is deÔ¨Åned by
‚à•v‚à•‚àó
x =
sup
‚à•w‚à•x‚â§1
‚ü®v, w‚ü©
for all v ‚ààRn.
The dual norm is easily veriÔ¨Åed to be subadditive and homogeneous, i.e.
‚à•v + w‚à•‚àó
x ‚â§‚à•v‚à•‚àó
x + ‚à•w‚à•‚àó
x and ‚à•Œªv‚à•‚àó
x = |Œª|‚à•v‚à•‚àó
x for all v, w ‚ààRn and all real
numbers Œª, but ‚à•¬∑‚à•‚àó
x is a proper norm on the whole of Rn only for points x
where the second derivative f ‚Ä≤‚Ä≤(x) is positive deÔ¨Ånite, because ‚à•v‚à•‚àó
x = ‚àûif v
is a nonzero vector in the null space N(f ‚Ä≤‚Ä≤(x)) since ‚à•tv‚à•x = 0 for all t ‚ààR
and ‚ü®v, tv‚ü©= t‚à•v‚à•2 ‚Üí‚àûas t ‚Üí‚àû. However, ‚à•¬∑‚à•‚àó
x is always a proper norm
when restricted to the subspace N(f ‚Ä≤‚Ä≤(x))‚ä•. See exercise 18.2.
By Theorem 15.1.3, we have the following expression for the Newton
decrement Œª(f, x) in terms of the dual local norm:
Œª(f, x) = ‚à•f ‚Ä≤(x)‚à•‚àó
x.
The following variant of the Cauchy‚ÄìSchwarz inequality holds f¬®or the local
seminorm.
Theorem 18.1.9. Assume that ‚à•v‚à•‚àó
x < ‚àû. Then
|‚ü®v, w‚ü©| ‚â§‚à•v‚à•‚àó
x‚à•w‚à•x
for all vectors w.
Proof. If ‚à•w‚à•x Ã∏= 0 then ¬±w/‚à•w‚à•x are two vectors with local seminorm equal
to 1, so it follows from the deÔ¨Ånition of the dual norm that
¬±
1
‚à•w‚à•x
‚ü®v, w‚ü©= ‚ü®v, ¬±w/‚à•w‚à•x‚ü©‚â§‚à•v‚à•‚àó
x,
and we obtain the sought inequality after multiplication by ‚à•w‚à•x.
If instead ‚à•w‚à•x = 0, then ‚à•tw‚à•x = 0 for all real numbers t, and it follows
from the supremum deÔ¨Ånition that t‚ü®v, w‚ü©= ‚ü®v, tw‚ü©‚â§‚à•v‚à•‚àó
x < ‚àûfor all t.
This being possible only if ‚ü®v, w‚ü©= 0, we conclude that the inequality applies
in this case, too.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
93
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Later we will need various estimates of ‚à•v‚à•‚àó
x. Our Ô¨Årst estimate is in
terms of the width in diÔ¨Äerent directions of the set X, and this motivates
our next deÔ¨Ånition.
DeÔ¨Ånition. Given a nonempty subset X of Rn, let VarX : Rn ‚ÜíR be the
function deÔ¨Åned by
VarX(v) = sup
x‚ààX
‚ü®v, x‚ü©‚àíinf
x‚ààX‚ü®v, x‚ü©.
VarX(v) is obviously a Ô¨Ånite number for each v ‚ààRn if the set X is
bounded, and if v is a unit vector, then VarX(v) measures the width of the
set X in the direction of v.
Our next theorem shows how to estimate ‚à•¬∑‚à•‚àó
x using VarX.
Theorem 18.1.10. Suppose that f : X ‚ÜíR is a closed self-concordant func-
tion with a bounded open convex subset X of Rn as domain, and let ‚à•¬∑‚à•‚àó
x be
the dual local norm associated with the function f at the point x ‚ààX. Then
‚à•v‚à•‚àó
x ‚â§VarX(v)
for all v ‚ààRn.
Proof. It follows from the previous theorem that y is a point in cl X if x is a
point in X and ‚à•y ‚àíx‚à•x ‚â§1. Hence,
‚à•v‚à•‚àó
x =
sup
‚à•w‚à•x‚â§1
‚ü®v, w‚ü©=
sup
‚à•y‚àíx‚à•x‚â§1
‚ü®v, y ‚àíx‚ü©‚â§sup
y‚ààcl X
‚ü®v, y ‚àíx‚ü©= sup
y‚ààX
‚ü®v, y ‚àíx‚ü©
= sup
y‚ààX
‚ü®v, y‚ü©‚àí‚ü®v, x‚ü©‚â§sup
y‚ààX
‚ü®v, y‚ü©‚àíinf
y‚ààX‚ü®v, y‚ü©= VarX(v).
We have previously deÔ¨Åned the analytic center of a closed convex set X
with respect to a given barrier as the unique minimum point of the barrrier,
provided that there is one. According to Theorem 18.1.6, every compact
convex set with nonempty interior has an analytic center with respect to any
given ŒΩ-self-concordant barrier. We can now obtain an upper bound on the
dual local norm ‚à•v‚à•‚àó
x at an arbitrary point x in terms of the parameter ŒΩ and
the value of the dual norm at the analytic center.
Theorem 18.1.11. Let X be a compact convex set, and let ÀÜxf be the analytic
center of the set with respect to a ŒΩ-self-concordant barrier f. Then, for each
vector v ‚ààRn and each x ‚ààint X,
‚à•v‚à•‚àó
x ‚â§(ŒΩ + 2‚àöŒΩ)‚à•v‚à•‚àó
ÀÜxf.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
94
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Proof. Let B1 = E(x; 1) and B2 = E(ÀÜxf; ŒΩ + 2‚àöŒΩ). Theorems 16.3.2 and
18.1.6 give us the inclusions B1 ‚äÜX ‚äÜB2, so it follows from the deÔ¨Ånition
of the dual local norm that
‚à•v‚à•‚àó
x =
sup
‚à•w‚à•x‚â§1
‚ü®v, w‚ü©= sup
y‚ààB1
‚ü®v, y ‚àíx‚ü©‚â§sup
y‚ààB2
‚ü®v, y ‚àíx‚ü©
= ‚ü®v, ÀÜxf ‚àíx‚ü©+ sup
y‚ààB2
‚ü®v, y ‚àíÀÜxf‚ü©= ‚ü®v, ÀÜxf ‚àíx‚ü©+
sup
‚à•w‚à•ÀÜxf ‚â§ŒΩ+2‚àöŒΩ
‚ü®v, w‚ü©
= ‚ü®v, ÀÜxf ‚àíx‚ü©+ (ŒΩ + 2‚àöŒΩ)‚à•v‚à•‚àó
ÀÜxf.
Since ‚à•‚àív‚à•‚àó
x = ‚à•v‚à•‚àó
x, we may now without loss of generality assume that
‚ü®v, ÀÜxf ‚àíx‚ü©‚â§0, and this gives us the required inequality.
18.2
The path-following method
Standard form
Let us say that a convex optimization problem is in standard form if it is
presented in the form
min ‚ü®c, x‚ü©
s.t.
x ‚ààX
where X is a compact convex set with nonempty interior and X is equipped
with a ŒΩ-self-concordant barrier function F.
Remark. One can show that every compact convex set X has a barrier func-
tion, but for a barrier function to be useful in a practical optimization prob-
lem, it has to be explicitly given so that it is possible to eÔ¨Éciently calculate
its partial Ô¨Årst and second derivatives.
The assumption that the set X is bounded is not particularly restric-
tive for problems with Ô¨Ånite optimal values, for we can always modify such
problems by adding artiÔ¨Åcial, very big bounds on the variables.
We also recall that an arbitrary convex problem can be transformed into
an equivalent convex problem with a linear objective function by an epigraph
formulation. (See Chapter 9.3 of Part II.)
Example 18.2.1. Each LP problem with Ô¨Ånite optimal value can be written
in standard form after suitable transformations. By Ô¨Årst identifying the aÔ¨Éne
hull of the polyhedron of feasible points with Rn for an appropriate n, we
can without restriction assume that the polyhedron has a nonempty interior,
and by adding big bounds on the variables, if necessary, we can also assume
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
95
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
95
that our polyhedron X of feasible points is compact. And with X written in
the form
(18.4)
X = {x ‚ààRn | ‚ü®ci, x‚ü©‚â§bi, i = 1, 2, . . . , m},
we get an m-self-concordant barrier F to X, by deÔ¨Åning
F(x) = ‚àí
m

i=1
ln(bi ‚àí‚ü®ci, x‚ü©)
Example 18.2.2. Convex quadratic optimization problems, i.e. problems of
the type
min g(x)
s.t.
x ‚ààX
where g is a convex quadratic function and X is a bounded polyhedron in Rn
with nonempty interior, can be transformed, using an epigraph formulation
and an artiÔ¨Åcial bound M on the new variable s, to problems of the form
min s
s.t.
(x, s) ‚ààY
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
96
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
where Y = {(x, s) ‚ààRn √ó R | x ‚ààX, g(x) ‚â§s ‚â§M} is a compact convex
set with nonempty interior. Now assume that the polyhedron X is given by
equation (18.4) as an intersection of closed halfspaces. Then the function
F(x, s) = ‚àí
m

i=1
ln(bi ‚àí‚ü®ci, x‚ü©) ‚àíln(s ‚àíg(x)) ‚àíln(M ‚àís)
is an (m + 2)-self-concordant barrier to Y according to Example 18.1.3.
Central path
We will now study the path-following method for the standard problem
(SP)
min ‚ü®c, x‚ü©
s.t.
x ‚ààX
where X is a compact convex subset of Rn with nonempty interior, and F
is a ŒΩ-self-concordant barrier to X. The Ô¨Ånite optimal value of the problem
is denoted by vmin.
For t ‚â•0 we deÔ¨Åne functions Ft : int X ‚ÜíR by
Ft(x) = t‚ü®c, x‚ü©+ F(x).
The functions Ft are closed and self-concordant, and since the set X is com-
pact, each function Ft has a unique minimum point ÀÜx(t). The central path
{ÀÜx(t) | t ‚â•0} is in other words well-deÔ¨Åned, and its points satisfy the equa-
tion
(18.5)
tc + F ‚Ä≤(ÀÜx(t)) = 0,
and the starting point ÀÜx(0) is by deÔ¨Ånition the analytic center ÀÜxF of X with
respect to the given barrier F.
We will use Newton‚Äôs method to determine the minimum point ÀÜx(t),
and for that reason we need to calculate the Newton step and the Newton
decrement with respect to the function Ft at points in the interior of X.
Since F ‚Ä≤‚Ä≤
t (x) = F ‚Ä≤‚Ä≤(x), the local norm ‚à•v‚à•x of a vector v with respect to
the function Ft is the same for all t ‚â•0, namely
‚à•v‚à•x =

‚ü®v, F ‚Ä≤‚Ä≤(x)v‚ü©.
In contrast, Newton steps and Newton decrements depend on t; the Newton
step at the point x is equal to ‚àíF ‚Ä≤‚Ä≤(x)‚àí1F ‚Ä≤
t(x) for the function Ft, and the
decrement is given by
Œª(Ft, x) =

‚ü®F ‚Ä≤
t(x), F ‚Ä≤‚Ä≤(x)‚àí1F ‚Ä≤
t(x)‚ü©= ‚à•F ‚Ä≤‚Ä≤(x)‚àí1F ‚Ä≤
t(x)‚à•x.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
97
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
The following theorem is used to formulate the stopping criterion in the
path-following method.
Theorem 18.2.1. (i) The points ÀÜx(t) on the central path of the optimization
problem (SP) satisfy the inequality
‚ü®c, ÀÜx(t)‚ü©‚àívmin ‚â§ŒΩ
t .
(ii) Moreover, the inequality
‚ü®c, x‚ü©‚àívmin ‚â§ŒΩ + Œ∫(1 ‚àíŒ∫)‚àí1‚àöŒΩ
t
.
holds for t > 0 and all point x ‚ààint X satisfying the condition
Œª(Ft, x) ‚â§Œ∫ < 1.
Proof. (i) Because of equation (18.5), c = ‚àít‚àí1F ‚Ä≤(ÀÜx(t)), and it therefore
follows from Theorem 18.1.4 that
‚ü®c, ÀÜx(t)‚ü©‚àí‚ü®c, y‚ü©= 1
t ‚ü®F ‚Ä≤(ÀÜx(t)), y ‚àíÀÜx(t)‚ü©‚â§ŒΩ
t
for all y ‚ààX. We obtain inequality (i) by choosing y as an optimal solution
to the problem (SP).
(ii) Since ‚ü®c, x‚ü©‚àívmin = (‚ü®c, x‚ü©‚àí‚ü®c, ÀÜx(t)‚ü©) + (‚ü®c, ÀÜx(t)‚ü©‚àívmin), it suÔ¨Éces, due
to the already proven inequality, to show that
(18.6)
‚ü®c, x‚ü©‚àí‚ü®c, ÀÜx(t)‚ü©‚â§
Œ∫
1 ‚àíŒ∫ ¬∑
‚àöŒΩ
t
if x ‚ààint X and Œª(Ft, x) ‚â§Œ∫ < 1. But it follows from Theorem 16.4.6 that
‚à•x ‚àíÀÜx(t)‚à•ÀÜx(t) ‚â§
Œª(Ft, x)
1 ‚àíŒª(Ft, x) ‚â§
Œ∫
1 ‚àíŒ∫,
so by using that tc = ‚àíF ‚Ä≤(ÀÜx(t)) and that F is ŒΩ-self-concordant, we get the
following chain of equalities and inequalities:
t(‚ü®c, x‚ü©‚àí‚ü®c, ÀÜx(t)‚ü©) = ‚àí‚ü®F ‚Ä≤(ÀÜx(t)), x ‚àíÀÜx(t)‚ü©‚â§‚à•F ‚Ä≤(ÀÜx(t))‚à•‚àó
ÀÜx(t)‚à•x ‚àíÀÜx(t)‚à•ÀÜx(t)
= Œª(F, ÀÜx(t))‚à•x ‚àíÀÜx(t)‚à•ÀÜx(t) ‚â§‚àöŒΩ
Œ∫
1 ‚àíŒ∫,
which proves inequality (18.6).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
98
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
98
Algorithm
The path-following algorithm for solving the standard problem
(SP)
min ‚ü®c, x‚ü©
s.t.
x ‚ààX
works in brief as follows.
We start with a parameter value t0 > 0 and a point x0 ‚ààint X, which
is close enough to the point ÀÜx(t0) on the central path. ‚ÄùClose enough‚Äù is
expressed in terms of the Newton decrement Œª(Ft0, x0), which must be suÔ¨É-
ciently small.
Then we update the parameter t by deÔ¨Åning t1 = Œ±t0 for a suitable Œ± > 1
and minimize the function Ft1 using the damped Newton method with x0 as
the starting point. Newton‚Äôs method is terminated when it has reached a
point x1, which is suÔ¨Éciently close to the minimum point ÀÜx(t1) of Ft1.
The procedure is then repeated with t2 = Œ±t1 as new parameter and with
x1 as starting point in Newton‚Äôs method for minimization of the function Ft2,
etc. As a result we obtain a sequence t0, x0, t1, x1, t2, x2, . . . of parameter
values and points, and the procedure is terminated when tk has become
suÔ¨Éciently large with xk as an approximate optimal point.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet‚Äôs 
electricity needs. Already today, SKF‚Äôs innovative know-
how is crucial to running a large proportion of the 
world‚Äôs wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
99
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
From this sketchy description of the algorithm it is clear that we need two
parameters, one parameter Œ± to describe the update of t, and one parameter
Œ∫ to deÔ¨Åne the stopping criterion in Newton‚Äôs method. We shall estimate the
total number of inner iterations, and the estimate will be the simplest and
most obvious if one writes the update parameter Œ± in the form Œ± = 1+Œ≥/‚àöŒΩ.
The following precise formulation of the path-following algorithm there-
fore contains the parameters Œ≥ and Œ∫. The addition ‚Äôphase 2‚Äô is due to the
need for an additional phase to generate feasible initial values x0 and t0.
Path-following algorithm, phase 2
Given an update parameter Œ≥ > 0, a neighborhood parameter 0 < Œ∫ < 1, a
tolerance œµ > 0, a starting point x0 ‚ààint X, and a starting value t0 > 0
such that Œª(Ft0, x0) ‚â§Œ∫.
1. Initiate: x:= x0 and t:= t0.
2. Stopping criterion: stop if œµt ‚â•ŒΩ + Œ∫(1 ‚àíŒ∫)‚àí1‚àöŒΩ.
3. Increase t: t:= (1 + Œ≥/‚àöŒΩ)t.
4. Update x by using Newton‚Äôs damped method on the function Ft with the
current x as starting point:
(i) Compute the Newton decrement Œª = Œª(Ft, x).
(ii) quit Newton‚Äôs method if Œª ‚â§Œ∫, and go to line 2.
(iii) Compute the Newtonstep ‚àÜxnt = ‚àíF ‚Ä≤‚Ä≤(x)‚àí1F ‚Ä≤
t(x).
(iv) Uppdate: x:= x + (1 + Œª)‚àí1‚àÜxnt
(v) Go to (i).
We can now show the following convergence result.
Theorem 18.2.2. Suppose that the above path-following algorithm is applied
to the standard problem (SP) with a ŒΩ-self-concordant barrier F. Then the
algorithm stops with a point x ‚ààint X which satisÔ¨Åes
‚ü®c, x‚ü©‚àívmin ‚â§œµ.
For each outer iteration, the number of inner iterations in Newton‚Äôs al-
gorithm is bounded by a constant K, and the total number of inner iterations
in the path-following algorithm is bounded by
C‚àöŒΩ ln
 ŒΩ
t0œµ + 1

,
where the constants K and C only depend on Œ∫ and Œ≥.
Proof. Let us start by examining the inner loop 4 of the algorithm.
Each time the algorithm passes by line 2, it does so with a point x in
int X, which belongs to a t-value with Newton decrement Œª(Ft, x) ‚â§Œ∫.
In step 4, the function Fs, where s = (1 + Œ≥/‚àöŒΩ)t, is then minimized
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
100
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
using Newton‚Äôs damped method with y0 = x as the starting point.
The
points yk, k = 1, 2, 3, . . . , generated by the method lie in int X accord-
ing to Theorem 16.3.2, and the stopping condition Œª(Fs, yk) ‚â§Œ∫ implies,
according to Theorem 16.5.1, that the algorithm terminates after at most

Fs(x) ‚àíFs(ÀÜx(s))

/œÅ(‚àíŒ∫)

iterations, where œÅ is the function
œÅ(u) = ‚àíu ‚àíln(1 ‚àíu).
We shall show that there is a constant K, which only depends on the param-
eters Œ∫ and Œ≥, so that
Fs(x) ‚àíFs(ÀÜx(s))
œÅ(‚àíŒ∫)

‚â§K,
and for that reason we need to estimate the diÔ¨Äerence Fs(x)‚àíFs(ÀÜx(s)), which
we do in the next lemma.
Lemma 18.2.3. Suppose that Œª(Ft, x) ‚â§Œ∫ < 1. Then, for all s > 0
Fs(x) ‚àíFs(ÀÜx(s)) ‚â§œÅ(Œ∫) + Œ∫‚àöŒΩ
1 ‚àíŒ∫ ¬∑
s
t ‚àí1
 + ŒΩ œÅ(1 ‚àís/t).
Proof of the lemma. We start by writing
(18.7)
Fs(x) ‚àíFs(ÀÜx(s)) =

Fs(x) ‚àíFs(ÀÜx(t))

+

Fs(ÀÜx(t)) ‚àíFs(ÀÜx(s))

.
By using the equality tc = ‚àíF ‚Ä≤(ÀÜx(t)) and the inequality
|‚ü®F ‚Ä≤(ÀÜx(t)), v‚ü©| ‚â§Œª(F, ÀÜx(t))‚à•v‚à•ÀÜx(t) ‚â§‚àöŒΩ‚à•v‚à•ÀÜx(t),
we obtain the following estimate of the Ô¨Årst diÔ¨Äerence in the right-hand side
of (18.7):
Fs(x) ‚àíFs(ÀÜx(t)) = Ft(x) ‚àíFt(ÀÜx(t)) + (s ‚àít)‚ü®c, x ‚àíÀÜx(t)‚ü©
(18.8)
= Ft(x) ‚àíFt(ÀÜx(t)) ‚àí(s/t ‚àí1)‚ü®F ‚Ä≤(ÀÜx(t)), x ‚àíÀÜx(t)‚ü©
‚â§Ft(x) ‚àíFt(ÀÜx(t)) + |s/t ‚àí1| ‚àöŒΩ ‚à•x ‚àíÀÜx(t)‚à•ÀÜx(t).
By Theorem 16.4.6,
Ft(x) ‚àíFt(ÀÜx(t)) ‚â§œÅ(Œª(Ft, x)) ‚â§œÅ(Œ∫)
and
‚à•x ‚àíÀÜx(t)‚à•ÀÜx(t) ‚â§
Œª(Ft, x)
1 ‚àíŒª(Ft, x) ‚â§
Œ∫
1 ‚àíŒ∫.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
101
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
101
Therefore, it follows from inequality (18.8) that
(18.9)
Fs(x) ‚àíFs(ÀÜx(t)) ‚â§œÅ(Œ∫) +
s
t ‚àí1
 ¬∑ Œ∫‚àöŒΩ
1 ‚àíŒ∫.
It remains to estimate the second diÔ¨Äerence
œÜ(s) = Fs(ÀÜx(t)) ‚àíFs(ÀÜx(s))
(18.10)
= s‚ü®c, ÀÜx(t)‚ü©‚àís‚ü®c, ÀÜx(s)‚ü©+ F(ÀÜx(t)) ‚àíF(ÀÜx(s))
in the right-hand side of (18.7).
The function ÀÜx(s) is continuously diÔ¨Äerentiable. This follows from the
implicit function theorem, because ÀÜx(s) satisÔ¨Åes the equation
sc + F ‚Ä≤(ÀÜx(s)) = 0,
and the second derivative F ‚Ä≤‚Ä≤(x) is continuous and non-singular everywhere.
By implicit diÔ¨Äerentiation,
c + F ‚Ä≤‚Ä≤(ÀÜx(s))ÀÜx‚Ä≤(s) = 0,
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
102
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
which means that
ÀÜx‚Ä≤(s) = ‚àíF ‚Ä≤‚Ä≤(ÀÜx(s))‚àí1c.
It now follows from equation (18.10) that the diÔ¨Äerence œÜ(s) is continuously
diÔ¨Äerentiable with derivative
œÜ‚Ä≤(s) = ‚ü®c, ÀÜx(t)‚ü©‚àí‚ü®c, ÀÜx(s)‚ü©‚àís‚ü®c, ÀÜx‚Ä≤(s)‚ü©‚àí‚ü®F ‚Ä≤(ÀÜx(s), ÀÜx‚Ä≤(s)‚ü©
= ‚ü®c, ÀÜx(t) ‚àíÀÜx(s)‚ü©‚àís‚ü®c, ÀÜx‚Ä≤(s)‚ü©+ s‚ü®c, ÀÜx‚Ä≤(s)‚ü©
= ‚ü®c, ÀÜx(t) ‚àíÀÜx(s)‚ü©,
and a further diÔ¨Äerentiation gives
œÜ‚Ä≤‚Ä≤(s) = ‚àí‚ü®c, ÀÜx‚Ä≤(s)‚ü©= ‚ü®c, F ‚Ä≤‚Ä≤(ÀÜx(s))‚àí1c‚ü©
= ‚ü®s‚àí1F ‚Ä≤(ÀÜx(s)), s‚àí1F ‚Ä≤‚Ä≤(ÀÜx(s))‚àí1F ‚Ä≤(ÀÜx(s))‚ü©
= s‚àí2‚ü®F ‚Ä≤(ÀÜx(s)), F ‚Ä≤‚Ä≤(ÀÜx(s))‚àí1F ‚Ä≤(ÀÜx(s))‚ü©= s‚àí2Œª(F, ÀÜx(s))2 ‚â§ŒΩs‚àí2.
Now note that œÜ(t) = œÜ‚Ä≤(t) = 0. By integrating the inequality for œÜ‚Ä≤‚Ä≤(s)
over the interval [t, u], we therefore obtain the following estimate for u ‚â•t:
œÜ‚Ä≤(u) = œÜ‚Ä≤(u) ‚àíœÜ‚Ä≤(t) ‚â§
 u
t
ŒΩs‚àí2 ds = ŒΩ(t‚àí1 ‚àíu‚àí1).
Integrating once more over the interval [t, s] results in the inequality
Fs(ÀÜx(t)) ‚àíFs(ÀÜx(s)) = œÜ(s) =
 s
t
œÜ‚Ä≤(u) du ‚â§ŒΩ
 s
t
(t‚àí1 ‚àíu‚àí1) du
(18.11)
= ŒΩ
s
t ‚àí1 ‚àíln s
t

= ŒΩ œÅ(1 ‚àís/t)
for s ‚â•t. The same conclusion is also reached for s < t by Ô¨Årst integrating the
inequality for œÜ‚Ä≤‚Ä≤(s) over the interval [u, t], and then the resulting inequality
for œÜ‚Ä≤(u) over the interval [s, t].
The inequality in the lemma is now Ô¨Ånally a consequence of equation
(18.7) and the estimates (18.9) and (18.11).
Continuation of the proof of Theorem 18.2.2. By using the lemma‚Äôs estimate
of the diÔ¨Äerence Fs(x) ‚àíFs(ÀÜx(s)) when s = (1 + Œ≥/‚àöŒΩ)t, we obtain the
inequality
Fs(x) ‚àíFs(ÀÜx(s))
œÅ(‚àíŒ∫)

‚â§
œÅ(Œ∫) + Œ≥Œ∫(1 ‚àíŒ∫)‚àí1 + ŒΩ œÅ(‚àíŒ≥ŒΩ‚àí1/2)
œÅ(‚àíŒ∫)

,
and ŒΩ œÅ(‚àíŒ≥ŒΩ‚àí1/2) ‚â§1
2Œ≥2, because œÅ(u) = ‚àíu ‚àíln(1 ‚àíu) ‚â§1
2u2 for u < 0.
The number of inner iterations in each outer iteration is therefore bounded
by the constant
K =
œÅ(Œ∫) + Œ≥Œ∫(1 ‚àíŒ∫)‚àí1 + 1
2Œ≥2
œÅ(‚àíŒ∫)

,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
103
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
which only depends on the parameters Œ∫ and Œ≥. For example, K = 5 if
Œ∫ = 0.4 and Œ≥ = 0.32.
We now turn to the number of outer iterations. Set
Œ≤(Œ∫) = ŒΩ + Œ∫(1 ‚àíŒ∫)‚àí1‚àöŒΩ.
Suppose that the stopping condition œµt ‚â•Œ≤(Œ∫) is triggered during iteration
number k when t = (1 + Œ≥/‚àöŒΩ)kt0. Because of Theorem 18.2.1, the current
point x then satisÔ¨Åes the condition
‚ü®c, x‚ü©‚àívmin ‚â§œµ,
which shows that x approximates the minimum point with prescribed accu-
racy.
Since k is the least integer satisfying the inequality (1 + Œ≥/‚àöŒΩ)k ‚â•
Œ≤(Œ∫)/t0œµ, we have
k =
 ln(Œ≤(Œ∫)/t0œµ)
ln(1 + Œ≥/‚àöŒΩ)

.
To simplify the denominator, we use the fact that ln(1 + Œ≥x) is a concave
function. This implies that ln(1 + Œ≥x) ‚â•x ln(1 + Œ≥) if 0 ‚â§x ‚â§1, and hence
ln(1 + Œ≥/‚àöŒΩ) ‚â•ln(1 + Œ≥)
‚àöŒΩ
.
Furthermore, Œ≤(Œ∫) = ŒΩ +Œ∫(1‚àíŒ∫)‚àí1‚àöŒΩ ‚â§ŒΩ +Œ∫(1‚àíŒ∫)‚àí1ŒΩ = (1‚àíŒ∫)‚àí1ŒΩ. This
gives us the estimate
k ‚â§
‚àöŒΩ ln((1 ‚àíŒ∫)‚àí1ŒΩ/t0œµ)
ln(1 + Œ≥)

‚â§K‚Ä≤‚àöŒΩ ln
 ŒΩ
t0œµ + 1

for the number of outer iterations with a constant K‚Ä≤ that only depends
on Œ∫ and Œ≥, and by multiplying this with the constant K we obtain the
corresponding estimate for the total number of inner iterations.
Phase 1
In order to use the path-following algorithm, we need a t0 > 0 and a point
x0 ‚ààint X with Newton decrement Œª(Ft0, x0) ‚â§Œ∫ to start from. Since the
central path begins in the analytic center ÀÜxF of X and Œª(F, ÀÜxF) = 0, it can
be expected that (x0, t0) is good enough as a starting pair if only x0 is close
enough to ÀÜxF and t0 > 0 is suÔ¨Éciently small. Indeed, this is true, and we
shall show that one can generate such a pair by solving an artiÔ¨Åcial problem,
given that one knows a point x ‚ààint X.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
104
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
104
Therefore, let Gt : int X ‚ÜíR, where 0 ‚â§t ‚â§1, be the functions deÔ¨Åned
by
Gt(x) = ‚àít‚ü®F ‚Ä≤(x), x‚ü©+ F(x).
The functions Gt are closed and self-concordant, and they have unique min-
imum points x(t).
Note that G0 = F, and hence x(0) = ÀÜxF. Since G‚Ä≤
t(x) = ‚àítF ‚Ä≤(x)+F ‚Ä≤(x),
G‚Ä≤
1(x) = 0, and this means that x is the minimum point of the function G1.
Hence, x(1) = x. The curve {x(t) | 0 ‚â§t ‚â§1} thus starts in the analytic
center ÀÜxF and ends in the given point x. By using the path-following method,
now following the curve backwards, we will therefore obtain a suitable starting
point for phase 2 of the algorithm.
We use Newton‚Äôs damped method to minimize Gt and note that G‚Ä≤‚Ä≤
t = F ‚Ä≤‚Ä≤
for all t, so the local norm with respect to the function Gt coincides with the
local norm with respect to the function F, and we can thus unambiguously
use the symbol ‚à•¬∑‚à•x for the local norm at the point x.
The algorithm for determining a starting pair (x0, t0) now looks like this.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
105
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Path-following algorithm, phase 1
Given x ‚ààint X, and parameters 0 < Œ≥ < 1
2
‚àöŒΩ and 0 < Œ∫ < 1.
1. Initiate: x:= x and t:= 1.
2. Stopping criterion: stop if Œª(F, x) < 3
4Œ∫ and set x0 = x.
3. Decrease t: t:= (1 ‚àíŒ≥/‚àöŒΩ)t.
4. Update x by using Newton‚Äôs damped method on the function Gt with the
current x as starting point:
(i) Compute Œª = Œª(Gt, x).
(ii) quit Newton‚Äôs method if Œª ‚â§Œ∫/2, and go to line 2.
(iii) Compute the Newton step ‚àÜxnt = ‚àíF ‚Ä≤‚Ä≤(x)‚àí1G‚Ä≤
t(x).
(iv) Update: x:= x + (1 + Œª)‚àí1‚àÜxnt.
(v) Go to (i).
When the algorithm has stopped with a point x0, we deÔ¨Åne t0 by setting
t0 = max{t | Œª(Ft, x0) ‚â§Œ∫}.
The number of iterations in phase 1 is given by the following theorem.
Theorem 18.2.4. Phase 1 of the path-following algorithm stops with a point
x0 ‚ààint X after at most
C‚àöŒΩ ln

ŒΩ
1 ‚àíœÄÀÜxF (x) + 1

inner iterations, where the constant C only depends on Œ∫ and Œ≥, the number
t0 satisÔ¨Åes the conditions Œª(Ft0, x0) ‚â§Œ∫ and t0 ‚â•Œ∫/4 VarX(c).
Proof. We start by estimating the number of inner iterations in each outer
iteration; this number is bounded by the quotient
Gs(x) ‚àíGs(x(s))
œÅ(‚àíŒ∫/2)
,
where s = (1 ‚àíŒ≥/‚àöŒΩ)t, and Lemma 18.2.3 gives us the majorant
œÅ(Œ∫/2) + Œ∫‚àöŒΩ
2 ‚àíŒ∫ ¬∑ Œ≥
‚àöŒΩ + ŒΩ œÅ(Œ≥/‚àöŒΩ)
for the numerator of the quotient. By Lemma 16.3.1, ŒΩœÅ(Œ≥/‚àöŒΩ) ‚â§Œ≥2, so the
number of inner iterations in each outer iteration is bounded by the constant
œÅ(Œ∫/2) + Œ∫(2 ‚àíŒ∫)‚àí1Œ≥ + Œ≥2
œÅ(‚àíŒ∫/2)
.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
106
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
We now consider the outer iterations. Since F ‚Ä≤ = G‚Ä≤
t + tF ‚Ä≤(x),
Œª(F, x) = ‚à•F ‚Ä≤(x)‚à•‚àó
x = ‚à•G‚Ä≤
t(x) + tF ‚Ä≤(x)‚à•‚àó
x ‚â§‚à•G‚Ä≤
t(x)‚à•‚àó
x + t‚à•F ‚Ä≤(x)‚à•‚àó
x
(18.12)
= Œª(Gt, x) + t‚à•F ‚Ä≤(x)‚à•‚àó
x.
It follows from Theorem 18.1.11 that
‚à•F ‚Ä≤(x)‚à•‚àó
x ‚â§(ŒΩ + 2‚àöŒΩ)‚à•F ‚Ä≤(x)‚à•‚àó
ÀÜxF ‚â§3ŒΩ‚à•F ‚Ä≤(x)‚à•‚àó
ÀÜxF ,
and from Theorem 18.1.8 that
‚à•F ‚Ä≤(x)‚à•‚àó
ÀÜxF =
sup
‚à•v‚à•ÀÜxF ‚â§1
‚ü®F ‚Ä≤(x), v‚ü©‚â§
ŒΩ
1 ‚àíœÄÀÜxF (x).
Hence
(18.13)
‚à•F ‚Ä≤(x)‚à•‚àó
x ‚â§
3ŒΩ2
1 ‚àíœÄÀÜxF (x).
During outer interation number k, we have t = (1 ‚àíŒ≥/‚àöŒΩ)k and the point
x satisÔ¨Åes the condition Œª(Gt, x) ‚â§Œ∫/2 when Newton‚Äôs method stops. So
it follows from inequality (18.12) and the estimate (18.13) that the stopping
condition Œª(F, x) < 3
4Œ∫ in line 2 of the algorithm is fulÔ¨Ålled if
1
2Œ∫ +
3ŒΩ2
1 ‚àíœÄÀÜxF (x)(1 ‚àíŒ≥/‚àöŒΩ)k ‚â§3
4Œ∫,
i.e. if
k ln(1 ‚àíŒ≥/‚àöŒΩ) < ‚àíln
 12Œ∫‚àí1ŒΩ2
1 ‚àíœÄÀÜxF (x

.
By using the inequality ln(1 ‚àíx) ‚â§‚àíx, which holds for 0 < x < 1, we see
that the stopping condition is fulÔ¨Ålled for
k >
‚àöŒΩ
Œ≥ ln
 12Œ∫‚àí1ŒΩ2
1 ‚àíœÄÀÜxF (x

.
So the number of outer iterations is less than
K‚àöŒΩ ln

ŒΩ
1 ‚àíœÄÀÜxF (x) + 1

,
where the constant K only depends on Œ∫ and Œ≥, and this proves the estimate
of the theorem, since the number of inner iterations in each outer iteration
is bounded by a constant, which only depends on Œ∫ and Œ≥.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
107
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
107
The deÔ¨Ånition of t0 implies that Œ∫ = Œª(Ft0, x0), so we get the following
inequalities with the aid of Theorem 18.1.10:
Œ∫ = Œª(Ft, x0) = ‚à•F ‚Ä≤
t(x0)‚à•‚àó
x0 = ‚à•t0c + F ‚Ä≤(x0)‚à•‚àó
x0 ‚â§t0‚à•c‚à•‚àó
x0 + ‚à•F ‚Ä≤(x0)‚à•‚àó
x0
= t0‚à•c‚à•‚àó
x0 + Œª(F, x0) ‚â§t0 VarX(c) + 3
4Œ∫.
It follows that
t0 ‚â•
Œ∫
4 VarX c.
The following complexity result is now obtained by combining the two
phases of the path-following algorithm.
Theorem 18.2.5. A standard problem (SP) with ŒΩ-self-concordant barrier,
tolerance level œµ > 0 and starting point x ‚ààint X can be solved with at most
C‚àöŒΩ ln(ŒΩŒ¶/œµ + 1)
Newton steps, where
Œ¶ =
VarX(c)
1 ‚àíœÄÀÜxF (x)
and the constant C only depends on Œ≥ and Œ∫.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT‚Ä¶
     RUN FASTER.
          RUN LONGER..
                RUN EASIER‚Ä¶
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
108
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Proof. Phase 1 provides a starting point x0 and an initial value t0 for phase 2,
satisfying the condition t0 ‚â•Œ∫/(4 VarX(c)). The number of inner iterations
in phase 2 is therefore bounded by
O(1)‚àöŒΩ ln
4ŒΩ VarX(c)
Œ∫œµ
+ 1

= O(1)‚àöŒΩ ln
ŒΩ VarX(c)
œµ
+ 1

.
So the total number of inner iterations in the two phases is
O(1)‚àöŒΩ ln

ŒΩ
1 ‚àíœÄÀÜxF (x) + 1

+O(1)‚àöŒΩ ln
ŒΩ VarX(c)
œµ
+ 1

= O(1)‚àöŒΩ ln(ŒΩŒ¶/œµ + 1).
Remark. The algorithms in this section provide nice theoretical complexity
results, but they are not suitable for practical use. The main limitation is
the low updating factor (1 + O(1)ŒΩ‚àí1/2) of the penalty parameter t, which
implies that the total number of Newton steps will be proportional to ‚àöŒΩ.
For an LP problem with n = 1000 variables and m = 10000 inequalities, one
would need to solve hundreds of linear equations with 1000 variables, which
requires far more time than what is needed by the simplex algorithm. In
the majority of outer iterations, one can, however, in practice increase the
penalty parameter much faster than what is needed for the theoretical worst
case analysis, without necessarily having to increase the number of Newton
steps to maintain proximity to the central path. There are good practical
implementations of the algorithm that use various dynamic strategies to con-
trol the penalty parameter t, and as a result only a moderate total number
of Newton steps is needed, regardless of the size of the problem.
18.3
LP problems
We now apply the algorithm in the previous section on LP problems. Con-
sider a problem of the type
(18.14)
min ‚ü®c, x‚ü©
s.t.
Ax ‚â§b
where A = [aij] is an m √ó n-matrix. We assume that the polyhedron
X = {x ‚ààRn | Ax ‚â§b}
of feasible points is bounded and has a nonempty interior. The boundedness
assumption implies that m > n.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
109
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
The ith row of the matrix A is denoted by ai, that is ai = [ai1 ai2 . . . ain].
The matrix product aix is thus well-deÔ¨Åned.
As a barrier to the set X we use the m-self-concordant function
F(x) = ‚àí
m

i=1
ln(bi ‚àíaix).
The path-following algorithm started from an arbitrary point x ‚ààint X
results in an œµ-solution, i.e. a point with a value of the objective function
that approximates the optimal value with an error less than œµ, after at most
O(1)‚àöm ln(mŒ¶/œµ + 1)
inner iterations, where
Œ¶ =
VarX(c)
1 ‚àíœÄÀÜxF (x).
We now estimate the number of arithmetic operations (additions, sub-
tractions, multiplications and divisions) that are required during phase 2 of
the algorithm to obtain this œµ-solution.
For each inner iteration of the Newton algorithm, we Ô¨Årst have to compute
the gradient and the hessian of the barrier function at the current point x,
i.e.
F ‚Ä≤(x) =
m

i=1
a T
i
bi ‚àíaix
och
F ‚Ä≤‚Ä≤(x) =
m

i=1
a T
i ai
(bi ‚àíaix)2.
This can be done with O(mn2) arithmetic operations. The Newton direction
‚àÜxnt at x is obtained as solution to the quadratic system
F ‚Ä≤‚Ä≤(x)‚àÜxnt = ‚àí(tc + F ‚Ä≤(x))
of linear equations, and using Gaussian elimination, we Ô¨Ånd the solution after
O(n3) arithmetic operations. Finally, O(n) additional arithmetic operations,
including one square root extraction, are needed to compute the Newton
decrement Œª = Œª(Ft, x) and the new point x+ = x + (1 + Œª)‚àí1‚àÜxnt.
The corresponding estimate of the number of operations is also true for
phase 1 of the algorithm.
The gradient and hessian computation is the most costly of the above
computations since m > n. The total number of arithmetic operations in
each iteration is therefore O(mn2), and by multiplying with the number of
inner iterations, the overall arithmetic cost of the path-following algorithm
is estimated to be no more than O(m3/2n2) ln(mŒ¶/œµ + 1) operations.
The resulting approximate minimum point ÀÜx(œµ) is an interior point of the
polyhedron X, but the minimum is of course attained at an extreme point
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
110
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
110
on the the boundary of X.
However, there is a simple procedure, called
puriÔ¨Åcation and described below, which starting from ÀÜx(œµ) Ô¨Ånds an extreme
point ÀÜx of X after no more than O(mn2) arithmetic operations and with an
objective function value that does not exceed the value at ÀÜx(œµ). This means
that we have the following result.
Theorem 18.3.1. For the LP problem (18.14) at most
O(m3/2n2) ln(mŒ¶/œµ + 1)
arithmetic operations are needed to Ô¨Ånd an extreme point ÀÜx of the polyhedron
of feasible points that approximates the minimum value with an error less
than œµ.
PuriÔ¨Åcation
The proof of the following theorem describes an algorithm for how to generate
an extreme point with a value of the objective function that does not exceed
the value at a given interior point of the polyhedron of feasible points.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
111
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Theorem 18.3.2. Let
min ‚ü®c, x‚ü©
s.t.
Ax ‚â§b
be an LP problem with n variables and m constraints, and suppose that the
polyhedron X of feasible points is line-free and that the objective function
is bounded below on X. For each point of X we can generate an extreme
point of X with a value of the objective function that does not exceed the
value at the given point with an algorithm using at most O(mn2) arithmetic
operations.
Proof. The idea is very simple: Follow a half-line from the given point x(0)
with non-increasing function values until hitting upon a point x(1) in a face
F1 of the polyhedron X. Then follow a half-line in the face F1 with non-
increasing function values until hitting upon a point x(2) in the intersection
F1 ‚à©F2 of two faces, etc. After n steps, one has reached a point x(n) in the
intersection of n (independent) faces, i.e. an extreme point, with a function
value that is less than or equal to the value at the starting point.
To estimate the number of arithmetic operation we have to study the
above procedure in a little more detail.
We start by deÔ¨Åning v(1) = e1 if c1 < 0, v(1) = ‚àíe1 if c1 > 0, and
v(1) = ¬±e1 if c1 = 0, where the sign in the latter case should be chosen so
that the half-line x(0)+tv(1), t ‚â•0, intersects the boundary of the polyhedron;
this is possible since the polyhedron is assumed to be line-free. In the Ô¨Årst two
cases, the half-line also intersects the boundary of the polyhedron, because
‚ü®c, x(0) + tv(1)‚ü©= ‚ü®c, x(0)‚ü©‚àít|c1| tends to ‚àí‚àûas t tends to ‚àûand the
objective function is assumed to be bounded below on X. The intersection
point x(1) = x(0) + t1v(1) between the half-line and the boundary of X can be
computed with O(mn) arithmetic operations, since we only have to compute
the vectors b ‚àíAx(0) and Av(1), and quotients between their coordinates in
order to Ô¨Ånd the nonnegative parameter value t1.
After renumbering the equations, we may assume that the point x(1) lies
in the hyperplane a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn = b1. We now eliminate the
variable x1 from the constraints and the objective function, which results in
a system of the form
(18.15)
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
x1 + a‚Ä≤
12x2 + ¬∑ ¬∑ ¬∑ + a‚Ä≤
1nxn = b‚Ä≤
1
A‚Ä≤
Ô£Æ
Ô£ØÔ£∞
x2
...
xn
Ô£π
Ô£∫Ô£ª‚â§b‚Ä≤
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
112
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
where A‚Ä≤ is an (m ‚àí1) √ó (n ‚àí1)-matrix, and in a new objective function
c‚Ä≤
2x2 + ¬∑ ¬∑ ¬∑ + c‚Ä≤
nxn + d‚Ä≤,
which is the restriction of the original objective function to the current face.
The number of operations required to perform the eliminations is O(mn).
After O(mn) operations we have thus managed to Ô¨Ånd a point x(1) in
a face F1 of X with an objectiv function value ‚ü®c, x(1)‚ü©= ‚ü®c, x(0)‚ü©‚àít1|c1|
not exceeding ‚ü®c, x(0)‚ü©, and determined the equation of the face and the
restriction of the objective function to the face. We now have a problem of
lower dimension n ‚àí1 and with m ‚àí1 constraints.
We continue by choosing a descent vector v(2) for the objective function
that is parallel to the face F1, and we achieve this by deÔ¨Åning v(2) so that
v(2)
2
= ¬±1, v(2)
3
= ¬∑ ¬∑ ¬∑ = v(2)
n
= 0 (and v(2)
1
= ‚àía‚Ä≤
12v(2)
2 ), where the sign of v(2)
2
should be chosen so that the objective function is non-decreasing along the
half-line x(1) +tv(2), t ‚â•0, and the half-line instersects the relative boundary
of F1. This means that v(2)
2
= 1 if c‚Ä≤
2 < 0 and v(2)
2
= ‚àí1 if c‚Ä≤
2 > 0, while
the sign of v(2)
2
is determined by the requirement that the half-line should
intersect the boundary in the case c‚Ä≤
2 = 0.
We then determine the intersection between the half-line x(1)+tv(2), t ‚â•0,
and the relative boundary of F1, which occurs in one of the remaining hyper-
planes. If this hyperplane is the hyperplane a‚Ä≤
21x2 + ¬∑ ¬∑ ¬∑ + a‚Ä≤
2nxn = b‚Ä≤
2, say, we
eliminate the variable x2 from the remaining constraints and the objective
function. All this can be done with at most O(mn) operations and results in
a point x(2) in the intersection of two faces, and the new value of the objective
function is ‚ü®c, x(2)‚ü©= ‚ü®c, x(1)‚ü©‚àít2|c‚Ä≤
2| ‚â§‚ü®c, x(1)‚ü©.
After n iterations, which together require at most nO(mn) = O(mn2)
arithmetic operations, we have reached an extreme point ÀÜx = x(n) with a
function value that does not exceed the value at the starting point x(0). The
coordinates of the extreme point are obtained by solving a triangular system
of equations, which only requires O(n2) operations. The total number of
operations is thus O(mn2).
Example 18.3.1. We exemplify the puriÔ¨Åcation algorithm with the LP prob-
lem
min
‚àí2x1 + x2 + 3x3
s.t.
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
‚àíx1 + 2x2 + x3 ‚â§4
‚àíx1 + x2 + x3 ‚â§2
x1 ‚àí2x2
‚â§1
x1 ‚àíx2 ‚àí2x3 ‚â§1
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
113
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
113
Starting from the interior point x(0) = (1, 1, 1) with objectiv function
value cTx(0) = 2, we shall Ô¨Ånd an extreme point with a lower value.
Since c1 = ‚àí2 < 0, we begin by choosing v(1) = (1, 0, 0) and by determin-
ing the point of intersection between the half-line x = x(0)+tv(1) = (1+t, 1, 1),
t ‚â•0, and the boundary of the polyhedron of feasible points. We Ô¨Ånd that the
point x(1) = (3, 1, 1), corresponding to t = 2, satisÔ¨Åes all constraints and the
third constraint with equality. So x(1) lies in the face obtained by intersecting
the polyhedron X with the supporting hyperplane x1 ‚àí2x2 = 1. We elimi-
nate x1 from the objectiv function and from the remaining constraints using
the equation of this hyperplane, and consider the restriction of the objective
function to the corresponding face, i.e. the function f(x) = ‚àí3x2 + 3x3 ‚àí2
restricted to the polyhedron given by the system
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
x1 ‚àí2x2
= 1
x3 ‚â§5
‚àíx2 + x3 ‚â§3
x2 ‚àí2x3 ‚â§0
The x2-coeÔ¨Écient of our new objective function is negative, so we follow
the half-line x2 = 1 + t, x3 = 1, t ‚â•0, in the hyperplane x1 ‚àí2x2 = 1 until it
hits a new supporting hyperplane, which occurs for t = 1, when it intersects
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
114
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
the hyperplane x2 ‚àí2x3 = 0 in the point x(2) = (5, 2, 1). Elimination of x2
results in the objective function f(x) = ‚àí3x3 ‚àí2 and the system
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
x1 ‚àí2x2
= 1
x2 ‚àí2x3 = 0
x3 ‚â§5
‚àíx3 ‚â§3
Our new half-line in the face F1‚à©F2 is given by the equation x3 = 1+t, t ‚â•0,
and the halÔ¨Çine intersects the third hyperplane x3 = 5 when t = 4, i.e. in a
point with x3-coordinate equal to 5. Back substitution gives x(3) = (21, 10, 5),
which is an extreme point with objective function value equal to ‚àí17.
18.4
Complexity
By the complexity of a problem we here mean the number of arithmetic
operations needed to solve it, and in this section we will study the complexity
of LP problems with rational coeÔ¨Écients. The solution of an LP problem
consists by deÔ¨Ånition of the problem‚Äôs optimal value and, provided the value
is Ô¨Ånite, of an optimal point. All known estimates of the complexity depend
not only on the number of variables and constraints, but also on the size of
the coeÔ¨Écients, and an appropriate measure of the size of a problem is given
by the number of binary bits needed to represent all its coeÔ¨Écients.
DeÔ¨Ånition. The input length of a vector x = (x1, x2, . . . , xn) in Rn is the
integer ‚Ñì(x) deÔ¨Åned as
‚Ñì(x) =
n

j=1
‚åàlog2(|xj| + 1)‚åâ.
The number of digits in the binary expansion of a positive integer z is
equal to ‚åàlog2(|z| + 1)‚åâ.
The binary representation of a negative integer
z requires one bit more in order to take care of the sign, and so does the
representation of z = 0. The number of bits to represent an arbitrary vector
x in Rn with integer coordinates is therefore at most ‚Ñì(x) + n.
The norm of a vector can be estimated using the input length, and we
shall need the following simple estimate in the two cases p = 1 and p = 2.
Lemma 18.4.1. ‚à•x‚à•p ‚â§2‚Ñì(x) for all x ‚ààRn and all p ‚â•1.
Proof. The inequality is a consequence of the following trivial inequalities
n
j=1 aj ‚â§n
j=1(aj + 1), ap + 1 ‚â§(a + 1)p and log2(a + 1) ‚â§‚åàlog2(a + 1)‚åâ,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
115
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
which hold for nonnegative numbers a, aj, and imply that
‚à•x‚à•p
p =
n

j=1
|xj|p ‚â§
n

j=1
(|xj|p + 1) ‚â§
n

j=1
(|xj| + 1)p ‚â§2p ‚Ñì(x).
We will now study LP problems of the type
(LP)
min ‚ü®c, x‚ü©
s.t.
Ax ‚â§b
where all coeÔ¨Écients of the m √ó n-matrix A = [aij] and of the vectors b and
c are integers. Every LP problem with rational coeÔ¨Écients can obviously be
replaced by an equivalent problem of this type after multiplication with a
suitable least common denominator. The polyhedron of feasible points will
be denoted by X so that
X = {x ‚ààRn | Ax ‚â§b}.
DeÔ¨Ånition. The two integers
‚Ñì(X) = ‚Ñì(A) + ‚Ñì(b)
and
L = ‚Ñì(X) + ‚Ñì(c) + m + n,
where ‚Ñì(A) denotes the input length of the matrix A, considered as a vector
in Rmn, are called the input length of the polyhedron X and the input length
of the given LP problem (LP), respectively.2
The main result of this section is the following theorem, which implies
that there is a solution algorithm that is polynomial in the input length of
the LP problem.
Theorem 18.4.2. There is an algorithm which solves the LP problem (LP)
with at most O((m + n)7/2L) arithmetic operations.
Proof. I. We begin by noting that we can without restriction assume that
the polyhedron X of feasible points is line-free. Indeed, we can, if necessary
replace the problem (LP) with the equivalent and line-free LP problem
min ‚ü®c, x+‚ü©‚àí‚ü®c, x‚àí‚ü©
s.t.
Ô£±
Ô£≤
Ô£≥
Ax+ ‚àíAx‚àí‚â§b
‚àíx+ ‚â§0
‚àíx‚àí‚â§0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
116
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
116
This LP problem in n‚Ä≤ = 2n variables and with m‚Ä≤ = m + 2n constraints has
input length
L‚Ä≤ = 2‚Ñì(A) + 2n + ‚Ñì(b) + 2‚Ñì(c) + m‚Ä≤ + n‚Ä≤
‚â§2(‚Ñì(A) + ‚Ñì(b) + ‚Ñì(c) + m + n) + 4n = 2L + 4n ‚â§6L,
so any algorithm that solves this problem with O((m‚Ä≤ + n‚Ä≤)7/2L‚Ä≤) operations
also solves problem (LP) with O((m + n)7/2L) operations since m‚Ä≤ + n‚Ä≤ ‚â§
4(m + n) and L‚Ä≤ ‚â§6L.
From now on, we therefore assume that X is a line-free polyhedron, and
for nonempty polyhedra X this implies that m ‚â•n and that X has at least
one extreme point.
The assertion of the theorem is also trivially true for LP problems with
only one variable, so we assume that m ‚â•n ‚â•2. Finally, we can naturally
assume that all the rows of the matrix A are nonzero, for if the kth row is
identically zero, then the corresponding constraint can be deleted if bk ‚â•0,
while the polyhedron X of feasible point is empty if bk < 0. In the future,
we can thus make use of the inequalities
‚Ñì(X) ‚â•‚Ñì(A) ‚â•m ‚â•n ‚â•2 and L ‚â•‚Ñì(X) + m + n ‚â•‚Ñì(X) + 4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
117
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
II. Under the above assumptions, we will prove the theorem by showing:
1. With O(m7/2L) operations, one can determine whether the optimal
value of the problem is +‚àû, ‚àí‚àûor Ô¨Ånite, i.e. whether there are any
feasible points or not, and if there are feasible points whether the ob-
jective functions is bounded below or not.
2. Given that the optimal value is Ô¨Ånite, one can then determine an opti-
mal solution with O(m3/2n2L) operations.
Since the proof of statement 1 uses the solution of an appropriate auxiliary
LP problem with Ô¨Ånite value, we begin by showing statement 2.
III. As a Ô¨Årst building block we need a lemma that provides information
about the extreme points of the polyhedron X in terms of its input length.
Lemma 18.4.3. (i) Let ÀÜx be an extreme point of the polyhedron X. Then,
the following inequality holds for all nonzero coordinates ÀÜxj:
2‚àí‚Ñì(X) ‚â§|ÀÜxj| ‚â§2‚Ñì(X).
Thus, all extreme points of X lie in the cube {x ‚ààRn | ‚à•x‚à•‚àû‚â§2‚Ñì(X)}.
(ii) If ÀÜx and Àúx are two extreme points of X and ‚ü®c, ÀÜx‚ü©Ã∏= ‚ü®c, Àúx‚ü©, then
|‚ü®c, ÀÜx‚ü©‚àí‚ü®c, Àúx‚ü©| ‚â•4‚àí‚Ñì(X).
Proof. To prove the lemma, we begin by recalling Hadamard‚Äôs inequality for
k √ó k-matrices C = [cij] with columns C‚àó1, C‚àó2, . . . , C‚àók, and which reads as
follows:
|det C| ‚â§
k

j=1
‚à•C‚àój‚à•2 =
k

j=1
 k

i=1
c2
ij
1/2.
The inequality is geometrically obvious ‚àíthe left-hand side |det C| is the
volume of a (hyper)parallelepiped, spanned by the matrix columns, while the
right-hand side is the volume of a (hyper)cuboid whose edges are of the same
length as the edges of the parallelepiped.
By combining Hadamard‚Äôs inequality with Lemma 18.4.1, we obtain the
inequality
|det C| ‚â§
k

j=1
2‚Ñì(C‚àój) = 2‚Ñì(C).
If C is a quadratic submatrix of the matrix

A
b

, then obviously ‚Ñì(C) ‚â§
‚Ñì(A) + ‚Ñì(b) = ‚Ñì(X), and it follows from the above inequality that
(18.16)
|det C| ‚â§2‚Ñì(X).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
118
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
Now let ÀÜx be an extreme point of the polyhedron X. According to Theo-
rem 5.1.1 in Part I, there is a set {i1, i2, . . . , in} of row indices such that the
extreme point ÀÜx is obtained as the unique solution to the equation system
n

j=1
aijxj = bi,
i = i1, i2, . . . , in.
By Cramer‚Äôs rule, we can write the solution in the form
ÀÜxj = ‚àÜj
‚àÜ,
where ‚àÜis the determinant of the coeÔ¨Écient matrix and ‚àÜj is the determi-
nant obtained by replacing column number j in ‚àÜwith the right-hand side
of the equation system. The determinants ‚àÜand ‚àÜj are integers, and their
absolute values are at most equal to 2‚Ñì(X), because of inequality (18.16). This
leads to the following estimates for all nonzero coordinates ÀÜxj, i.e. for all j
with ‚àÜj Ã∏= 0:
|ÀÜxj| = |‚àÜj|/|‚àÜ| ‚â§2‚Ñì(X)/1 = 2‚Ñì(X) and |ÀÜxj| = |‚àÜj|/|‚àÜ| ‚â•1/2‚Ñì(X) = 2‚àí‚Ñì(X),
which is assertion (i) of the lemma.
(ii) The value of the objective function at the extreme point ÀÜx is
‚ü®c, ÀÜx‚ü©=
 n

j=1
cj‚àÜj

/‚àÜ= T/‚àÜ,
where the numerator T is an integer. If Àúx is another extreme point, then of
course we also have ‚ü®c, Àúx‚ü©= T ‚Ä≤/‚àÜ‚Ä≤ for some integer T ‚Ä≤ and determinant ‚àÜ‚Ä≤
with |‚àÜ‚Ä≤| ‚â§2‚Ñì(X). It follows that the diÔ¨Äerence
‚ü®c, Àúx‚ü©‚àí‚ü®c, ÀÜx‚ü©= (T‚àÜ‚Ä≤ ‚àíT ‚Ä≤‚àÜ)/‚àÜ‚àÜ‚Ä≤
is either equal to zero or, if the numerator is nonzero, an integer with absolute
value ‚â•1/|‚àÜ‚àÜ‚Ä≤| ‚â•4‚àí‚Ñì(X).
IV. We shall use the path-following method, but this assumes that the poly-
hedron of feasible points is bounded and that there is an inner point from
which to start phase 1. To get around this diÔ¨Éculty, we consider the following
auxiliary problems in n + 1 variables and m + 2 linear constraints:
(LPM)
min ‚ü®c, x‚ü©+ Mxn+1
s.t.
Ô£±
Ô£≤
Ô£≥
Ax + (b ‚àí1)xn+1 ‚â§b
xn+1 ‚â§2
‚àíxn+1 ‚â§0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
119
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
119
Here, M is a positive integer, 1 denotes the vector (1, 1, . . . , 1) in Rm, and
x is as before the n-tuple (x1, x2, . . . , xn).
Let X‚Ä≤ denote the polyhedron of feasible points for the problem (LPM).
Since (x, xn+1) = (0, 1) satisÔ¨Åes all constraints with strict inequality, (0, 1) is
an inner point in X‚Ä≤.
We obtain the following estimates for the input length ‚Ñì(X‚Ä≤) of the poly-
hedron X‚Ä≤ and the input lenght L(M) of problem (LPM):
‚Ñì(X‚Ä≤) = ‚Ñì(A) +
m

i=1

log2

|bi ‚àí1| + 1

+ 1 + 1 + ‚Ñì(b) + 2
(18.17)
‚â§‚Ñì(X) + 4 +
m

i=1

1 +

log2

1 + |bi

= ‚Ñì(X) + 4 + m + ‚Ñì(b) ‚â§2‚Ñì(X) + 4 ‚â§2L ‚àí4,
L(M) = ‚Ñì(X‚Ä≤) + ‚Ñì(c) + ‚åàlog2(M + 1)‚åâ+ m + n + 3
(18.18)
‚â§2‚Ñì(X) + 2‚Ñì(c) + ‚åàlog2 M‚åâ+ m + n + 8
= 2L + ‚åàlog2 M‚åâ‚àí(m + n) + 8 ‚â§2L + ‚åàlog2 M‚åâ+ 4.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
120
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
The reason for studying our auxiliary problem (LPM) is given in the
following lemma.
Lemma 18.4.4. Assume that problem (LP) has a Ô¨Ånite value. Then:
(i) Problem (LPM) has a Ô¨Ånite value for each integer M > 0.
(ii) If (ÀÜx, 0) is an optimal solution to problem (LPM), then ÀÜx is an optimal
solution to the original problem (LP).
(iii) Assume that M ‚â•24L and that the extreme point (ÀÜx, ÀÜxn+1) of X‚Ä≤ is an
optimal solution to problem (LPM). Then, ÀÜxn+1 = 0, so ÀÜx is an optimal
solution to problem (LP).
Proof. (i)
The assumption of Ô¨Ånite value means that the polyhedron X is
nonempty and that the objective function ‚ü®c, x‚ü©is bounded below on X, and
by Theorem 12.1.1 in Part II, this implies that the vector c lies in the dual
cone of the recession cone recc X. Since
recc X‚Ä≤ = {(x, xn+1) | Ax + (b ‚àí1)xn+1 ‚â§0, xn+1 = 0}
= recc X √ó {0},
the dual cone of recc X‚Ä≤ is equal to (recc X)+ √ó R. We conclude that the
vector (c, M) lies in the dual cone (recc X‚Ä≤)+, which means that the objective
function of problem (LPM) is bounded below on the nonempty set X‚Ä≤. Hence,
our auxiliary problem has a Ô¨Ånite value.
The polyhedron X‚Ä≤ is line-free, since
lin X‚Ä≤ = {(x, xn+1) | Ax + (b ‚àí1)xn+1 = 0, xn+1 = 0}
= lin X √ó {0} = {(0, 0)}.
(ii) The point (x, 0) is feasible for problem (LPM) if and only if x belongs
to X, i.e. is feasible for our original problem (LP). So if (ÀÜx, 0) is an optimal
solution to the auxiliary problem, then in particular
‚ü®c, ÀÜx‚ü©= ‚ü®c, ÀÜx‚ü©+ M ¬∑ 0 ‚â§‚ü®c, x‚ü©+ M ¬∑ 0 = ‚ü®c, x‚ü©
for all x ‚ààX, which shows that ÀÜx is an optimal solution to problem (LP).
(iii) Assume that (ÀÜx, ÀÜxn+1) is an extreme point of the polyhedron X‚Ä≤ and
an optimal solution to problem (LPM). By Lemma 18.4.3, applied to the
polyhedron X‚Ä≤, and the estimate (18.17), we then have the inequality
(18.19)
‚à•ÀÜx‚à•‚àû‚â§2‚Ñì(X‚Ä≤) ‚â§22‚Ñì(X)+4 ‚â§22L‚àí4,
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
121
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
so it follows by using Lemma 18.4.1 that
|‚ü®c, ÀÜx‚ü©| ‚â§
n

j=1
|cj||ÀÜxj| ‚â§‚à•c‚à•1‚à•ÀÜx‚à•‚àû‚â§2‚Ñì(c) ¬∑ 22‚Ñì(X)+4 ‚â§22‚Ñì(X)+2‚Ñì(c)+4
‚â§22L‚àí2m‚àí2n+4 ‚â§22L‚àí4.
Assume that ÀÜxn+1 Ã∏= 0.
Then ÀÜxn+1 ‚â•2‚àí‚Ñì(X‚Ä≤) ‚â•2‚àí2L, according to
Lemma 18.4.3. The optimal value ÀÜvM of the auxiliary problem (LPM) there-
fore satisÔ¨Åes the inequality
ÀÜvM = ‚ü®c, ÀÜx‚ü©+ M ÀÜxn+1 ‚â•M ÀÜxn+1 ‚àí|‚ü®c, ÀÜx‚ü©| ‚â•M ¬∑ 2‚àí2L ‚àí22L‚àí4.
Let now x be an arbitrary extreme point of X. Since (x, 0) is a feasible point
for problem (LPM) and since ‚à•x‚à•‚àû‚â§2‚Ñì(X) by lemma 18.4.3, the optimal
value ÀÜvM must also satisfy the inequality
ÀÜvM ‚â§‚ü®c, x‚ü©+ M ¬∑ 0 ‚â§|‚ü®c, x‚ü©| ‚â§‚à•c‚à•1 ¬∑ ‚à•x‚à•‚àû‚â§2‚Ñì(c)+‚Ñì(X) = 2L‚àím‚àín ‚â§2L‚àí4.
By combining the two inequalities for ÀÜvM, we obtain the inequality
2L‚àí4 ‚â•M ¬∑ 2‚àí2L ‚àí22L‚àí4,
which implies that
M ‚â§23L‚àí4 + 24L‚àí4 < 24L.
So if M ‚â•24L, then ÀÜxn+1 = 0.
V. We are now ready for the main step in the proof of Theorem 18.4.2.
Lemma 18.4.5. Suppose that problem (LP) has a Ô¨Ånite value. The path-
following algorithm, applied to the problem (LPM) with ‚à•x‚à•‚àû‚â§22L as an
additional constraint, M = 24L, œµ = 2‚àí4L, and (0, 1) as starting point for
phase 1, and complemented with a subsequent puriÔ¨Åcation operation, gener-
ates an optimal solution to problem (LP) after at most O(m3/2n2L) aritmetic
operations.
Proof. It follows from the previous lemma and the estimate (18.19) that
the LP problem (LPM) has an optimal solution (ÀÜx, 0) which satisÔ¨Åes the
additional constraint ‚à•ÀÜx‚à•‚àû‚â§22L if M = 24L. The LP problem obtained
from (LPM) by adding the 2n constraints
xj ‚â§22L
and
‚àíxj ‚â§22L,
j = 1, 2, . . . , n,
therefore has the same optimal value as (LPM).
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
122
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
122
The extended problem has m+2n+2 = O(m) linear constraints, and the
point z = (x, xn+1) = (0, 1) is an interior point of the compact polyhedron
of feasible points, which we denote by Z. By Theorem 18.3.1, the path-
following algorithm with œµ = 2‚àí4L and z as the starting point therefore stops
after O((m+2n+2)3/2n2) ln((m+2n+2)Œ¶/œµ+1) = O(m3/2n2) ln(m24LŒ¶+1)
arithmetic operations at a point in the polyhedron X‚Ä≤ and with a value of
the objective function that approximates the optimal value ÀÜvM with an error
less than 2‚àí4L.
PuriÔ¨Åcation according to the method in Theorem 18.3.2 leads to an ex-
treme point (ÀÜx, ÀÜxn+1) of X‚Ä≤ with a value of the objective function less than
ÀÜvM + 2‚àí4L, and since 2‚àí4L = 4‚àí2L < 4‚àí‚Ñì(X‚Ä≤), it follows from Lemma 18.4.3
that (ÀÜx, ÀÜxn+1) is an optimal solution to (LPM). By Lemma 18.4.4, this implies
that ÀÜx is an optimal solution to the original problem (LP).
The puriÔ¨Åcation process requires O(mn2) arithmetic operations, so the
total arithmetic cost is
O(mn2) + O(m3/2n2) ln(m24LŒ¶ + 1) = O(m3/2n2) ln(m24LŒ¶ + 1)
operations. It thus only remains to prove that ln(m24LŒ¶ + 1) = O(L), and
since m ‚â§L, this will follow if we show that ln Œ¶ = O(L).
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
123
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
By deÔ¨Ånition,
Œ¶ = VarZ(c, M) ¬∑
1
1 ‚àíœÄÀÜzF (z),
where ÀÜzF is the analytic center of Z with respect to the relevant logarithmic
barrier F. The absolute value of the objective function at an arbitrary point
(x, xn+1) ‚ààZ can be estimated by
|‚ü®c, x‚ü©+ Mxn+1| ‚â§‚à•c‚à•1‚à•x‚à•‚àû+ 2M ‚â§2‚Ñì(c)+2L + 2 ¬∑ 24L ‚â§24L+2,
and the maximal variation of the function is at most twice this value. Hence,
VarZ(c, M) ‚â§24L+3.
The second component of Œ¶ is estimated using Theorem 18.1.7.
Let
B‚àû(a, an+1; r) denote the closed ball of radius r in Rn+1 = Rn √ó R with
center at the point (a, an+1) and with distance given by the maximum norm,
i.e.
B‚àû(a, an+1; r) = {(x, xn+1) ‚ààRn √ó R | ‚à•x ‚àía‚à•‚àû‚â§r, |xn+1 ‚àían+1| ‚â§r}.
The polyhedron Z is by deÔ¨Ånition included in the ball B‚àû(0, 0; 22L). On
the other hand, the tiny ball B‚àû(z; 2‚àíL) is included in Z, for if ‚à•x‚à•‚àû‚â§2‚àíL
and |xn+1 ‚àí1| ‚â§2‚àíL, then
n

j=1
aijxj + (bi ‚àí1)xn+1 ‚àíbi =
n

j=1
aijxj + bi(xn+1 ‚àí1) ‚àíxn+1
‚â§
n

j=1
|aij||xj| + |bi||xn+1 ‚àí1| ‚àíxn+1 ‚â§2‚àíL n

j=1
|aij| + |bi|

‚àí(1 ‚àí2‚àíL)
‚â§2‚àíL+‚Ñì(X) + 2‚àíL ‚àí1 ‚â§2‚àí4 + 2‚àíL ‚àí1 < 0,
which proves that the ith inequality of the system Ax+(b‚àí1)xn+1 ‚â§b holds
with strict inequality for i = 1, 2, . . . , m, and the remaining inequalities that
deÔ¨Åne the polyhedron Z are obviously strictly satisÔ¨Åed.
It therefore follows from Theorem 18.1.7 that
œÄÀÜzF (z) ‚â§
2 ¬∑ 22L
2 ¬∑ 22L + 2‚àíL,
and that consequently
1
1 ‚àíœÄÀÜzF (z) ‚â§2 ¬∑ 23L + 1 < 23L+2.
This implies that Œ¶ ‚â§24L+3 ¬∑ 23L+2 = 27L+5. Hence, ln Œ¶ = O(L), which
completes the proof of the lemma.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
124
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
VI. It remains to show that O(m7/2L) operations are suÔ¨Écient to decide
whether the optimal value of the original problem (LP) is +‚àû, ‚àí‚àûor Ô¨Ånite.
To decide whether the value is +‚àûor not, i.e. whether the polyhedron
X is empty or not, we consider the artiÔ¨Åcial LP problem
min xn+1
s.t.
Ax ‚àí1xn+1 ‚â§b
‚àíxn+1 ‚â§0
This problem has feasible points since (0, t) satisÔ¨Åes all constraints for suf-
Ô¨Åciently large positive numbers t. The optimal value of the problem is ap-
parently greater than or equal to zero, and it is equal to zero if and only if
X Ã∏= ‚àÖ.
So we can decide whether the polyhedron X is empty or not by deter-
mining an optimal solution to the artiÔ¨Åcial problem. The input length of
this problem is ‚Ñì(X) + 2m + n + 4, and since this number is ‚â§2L, it fol-
lows from Lemma 18.4.5 that we can decide whether X is empty or not with
O(m3/2n2L) aritmethic operations.
Note that we do not need to solve the artiÔ¨Åcial problem exactly. If the
value is greater than zero, then, because of Lemma 18.4.3, it is namely greater
than or equal to 2‚àí2L. It is therefore suÔ¨Écient to determine a point that
approximates the value with an error of less than 2‚àí2L to know if the value
is zero or not.
VII. If the polyhedron X is nonempty, we have as the next step to decide
whether the objective function is bounded below. This is the case if and
only if the dual problem to problem (LP) has feasible points, and this dual
maximization problem is equivalent to the minimization problem
min ‚ü®‚àíb, y‚ü©
s.t.
Ô£±
Ô£≤
Ô£≥
ATy ‚â§
c
‚àíATy ‚â§‚àíc
‚àíy ‚â§
0,
which is a problem with m variables, 2n+m (= O(m)) constraints and input
length
2‚Ñì(A) + m + 2‚Ñì(c) + ‚Ñì(b) + m + (2n + m) ‚â§2L + m ‚â§3L.
So it follows from step VI that we can decide whether the dual problem has
any feasible points with O(m7/2L) operations.
The proof of Theorem 18.4.2 is now complete.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
125
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
125
Exercises
18.1 Show that if the functions fi are ŒΩi-self-concordant barriers to the subsets
Xi of Rni, then f(x1, . . . , xm) = f1(x1) + ¬∑ ¬∑ ¬∑ + fm(xm) is a (ŒΩ1 + ¬∑ ¬∑ ¬∑ + ŒΩm)-
self-concordant barrier to the product set X1 √ó ¬∑ ¬∑ ¬∑ √ó Xm.
18.2 Prove that the dual local norm ‚à•v‚à•‚àó
x that is associated with the function f
is Ô¨Ånite if and only if v belongs to N(f‚Ä≤‚Ä≤(x))‚ä•, and that the restriction of
‚à•¬∑‚à•‚àó
x to N(f‚Ä≤‚Ä≤(x))‚ä•is a proper norm.
18.3 Let X be a closed proper convex cone with nonempty interior, let ŒΩ ‚â•1 be
a real number, and suppose that the function f : int X ‚ÜíR is closed and
self-concordant and that f(tx) = f(x) ‚àíŒΩ ln t for all x ‚ààint X and all t > 0.
Prove that
a) f ‚Ä≤(tx) = t‚àí1f‚Ä≤(x)
b) f‚Ä≤(x) = ‚àíf‚Ä≤‚Ä≤(x)x
c)
Œª(f, x) = ‚àöŒΩ.
The function f is in other words a ŒΩ-self-concordant barrier to X.
18.4 Show that the nonnegative orthant X = Rn
+, ŒΩ = n and the logarithmic
barrier f(x) = ‚àín
i=1 ln xi fulÔ¨Åll the assumptions of the previous exercise.
18.5 Let X = {(x, xn+1) ‚ààRn √ó R | xn+1 ‚â•‚à•x‚à•2}.
a) Show that the function f(x) = ‚àíln(x2
n+1 ‚àí(x2
1 + ¬∑ ¬∑ ¬∑ + x2
n)) is self-
concordant on int X.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
126
THE PATH-FOLLOWING METHOD
WITH SELF-CONCORDANT BARRIER
126
b) Show that X, ŒΩ = 2 and f fulÔ¨Åll the assumptions of exercise 18.3. The
function f is thus a 2-self-concordant barrier to X.
18.6 Suppose that the function f : R++ ‚ÜíR is convex, three times continuously
diÔ¨Äerentiable and that
|f‚Ä≤‚Ä≤‚Ä≤(x)| ‚â§3f‚Ä≤‚Ä≤(x)
x
for all x > 0. The function
F(x, y) = ‚àíln(y ‚àíf(x)) ‚àíln x
with X = {(x, y) ‚ààR2 | x > 0, y > f(x)} as domain is self-concordant
according to exercise 16.3. Show that F is a 2-self-concordant barrier to the
closure cl X.
18.7 Prove that the function
F(x, y) = ‚àíln(y ‚àíx ln x) ‚àíln x
is a 2-self-concordant barrier to the epigraph
{(x, y) ‚ààR2 | y ‚â•x ln x, x ‚â•0}.
18.8 Prove that the function
G(x, y) = ‚àíln(ln y ‚àíx) ‚àíln y
is a 2-self-concordant barrier to the epigraph {(x, y) ‚ààR2 | y ‚â•ex}.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
127
Bibliogracal and historical notices
BibliograÔ¨Åcal and historical
notices
Newton‚Äôs method is a classic iterative algorithm for Ô¨Ånding critical points of
diÔ¨Äerentiable functions, and it was proven by Kantorovich [1] that the algo-
rithm converges quadratically when the function has a Lipschitz continuous,
positive deÔ¨Ånite second derivatives in a neighborhood of the critical point,
provided the starting point is selected close enough.
Barrier methods for solving nonlinear optimization problems were Ô¨Årst
used during the 1950s. The central path with logarithmic barriers was stud-
ied by Fiacco and McCormick, and their book on sequential minimization
techniques ‚àíFiacco‚ÄìMcCormick [1], Ô¨Årst published in 1968 ‚àíis the stan-
dard work in the Ô¨Åeld. The methods worked well in practice, for the most
part, but there were no theoretical complexity results. They lost in popularity
in the 1970s and then experienced a renaissance in the wake of Karmarkar‚Äôs
discovery.
Karmarkar‚Äôs [1] polynomial algorithm for linear programming proceeds
by mapping the polyhedron of feasible points and the current approximate
solution xk onto a new polyhedron and a new point x‚Ä≤
k which is located near
the center of the new polyhedron, using a projective scaling transformation.
Thereafter, a step is taken in the transformed space which results in a point
xk+1 with a lower objective function value. The progress is measured by
means of a logarithmic potential function.
It was soon noted that Karmarkar‚Äôs potential-reducing algorithm was
akin to previously studied path-following methods, and Renegar [1] and Gon-
zaga [1] managed to show that the path-following method with logarithmic
barrier is polynomial for LP problems.
A general introduction to linear programming and the algorithm devel-
opment in the area until the late 1980s (the ellipsoid method, Karmarkar‚Äôs
algorithm, etc.) is given by Goldfarb‚ÄìTodd [1]. An overview of potential-
reducing algorithms is given by Todd [1], while Gonzaga [2] describes the
evolution of path-following algorithms until 1992.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
128
References
A breakthrough in convex optimization occurred in the late 1980s, when
Yurii Nesterov discovered that Gonzaga‚Äôs and Renegar‚Äôs proof only used two
properties of the logarithmic barrier function, namely, that it satisÔ¨Åes the two
diÔ¨Äerential inequalities, which with Nesterov‚Äôs terminology means that the
barrier is self-concordant with Ô¨Ånite parameter ŒΩ. Since explicit computable
self-concordant barriers exist for a number of important types of convex
sets, the theoretical complexity results for linear programming could now be
extended to a large class of convex optimization problems, and Nemirovskii
together with Nesterov developed algorithms for convex optimization based
on self-concordant barriers. See Nesterov‚ÄìNesterovski [1].
A modern textbook on convex optimization, which in addition to theory
and algorithms also contains lots of interesting applications from a variety of
Ô¨Åelds, is the book by Boyd‚ÄìVandenberghe [1].
References
Boyd, S. & Vandenberghe, L.
[1] Convex Optimization, Cambridge Univ. Press, Cambridge, UK, 2004.
Fiacco, A.V. & McCormick, G.P.
[1] Nonlinear Programming: Sequential Unconstrained Minimization Tech-
niques. Society for Industrial and Applied Mathematics, 1990. (First
published in 1968 by Research Analysis Corporation.)
Goldfarb, D.G. & Todd, M.J.
[1] Linear programming. Chapter 2 in Nemhauser, G.L. et al. (eds.), Hand-
books in Operations Research and Management Science, vol. 1: Opti-
mization, North-Holland, 1989.
Gonzaga, C.C.
[1] An algorithm for solving linear programming problems in O(n3L) oper-
ations. Pages 1‚Äì28 in Megiddo, N. (ed.), Progress in Mathematical Pro-
gramming: Interior-Point and Related Methods, Springer-Verlag, 1988.
[2] Path-Following Methods for Linear Programming, SIAM Rev. 34 (1992),
167‚Äì224.
Kantorovich, L.V.
[1] Functional Analysis and Applied Mathematics. National Bureau of Stan-
dards, 1952. (First published in Russian in 1948.)
Karmarkar, N.
[1] A new polynomial-time algorithm for linear programming, Combinator-
ica 4 (1984), 373‚Äì395.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
129
References
129
Nesterov, Y. & Nemirovskii, A.
[1] Interior-Point Polynomial Algorithms in Convex Programming. Society
for Industrial and Applied Mathematics, 1994.
Renegar, J.
[1] A polynomial-time algorithm based on Newton‚Äôs method for linear pro-
gramming, Math. Programm. 40 (1988), 59‚Äì94.
Todd, M.
[1] Potential-reduction methods in mathematical programming, Math. Pro-
gram. 76 (1997), 3‚Äì45.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
130
Answers and solution to the exercises
Answers and solution to the
exercises
Chapter 14
14.1 x1 = ( 4
9, ‚àí1
9), x2 = ( 2
27, 2
27), x3 = ( 8
243, ‚àí2
243).
14.3 hf ‚Ä≤(xk) = f(xk) ‚àíf(xk+1) ‚Üíf(ÀÜx) ‚àíf(ÀÜx) = 0 and hf ‚Ä≤(xk) ‚Üíhf ‚Ä≤(ÀÜx).
Hence, f ‚Ä≤(ÀÜx) = 0.
Chapter 15
15.1 ‚àÜxnt = ‚àíx ln x, Œª(f, x) = ‚àöx ln x, ‚à•v‚à•x = |v|/‚àöx.
15.2 a) ‚àÜxnt = ( 1
3, 1
3), Œª(f, x) =

1
3, ‚à•v‚à•x = 1
2

5v2
1 + 2v1v2 + 5v2
2
b) ‚àÜxnt = ( 1
3, ‚àí2
3), Œª(f, x) =

1
3, ‚à•v‚à•x = 1
2

8v2
1 + 8v1v2 + 5v2
2.
15.3 ‚àÜxnt = (v1, v2), where v1 + v2 = ‚àí1 ‚àíe‚àí(x1+x2),
Œª(f, x) = e(x1+x2)/2 + e‚àí(x1+x2)/2, ‚à•v‚à•x = e(x1+x2)/2|v1 + v2|.
15.4 If rank A < m, then rank M < m + n, and if N(A) ‚à©N(P) contains
a nonzero vector x, then M
x
0

=
0
0

. Hence, the matrix M has no
inverse in these cases.
Conversely, suppose that rank A = m, i.e. that N(AT) = {0}, and
that N(A) ‚à©N(P) = {0}. We show that the coeÔ¨Écient matrix M is
invertible by showing that the homogeneous system
Px + ATy = 0
Ax
= 0
has no other solutions than the trivial one, x = 0 and y = 0.
By multiplying the Ô¨Årst equation from the left by xT we obtain
0 = xTPx + xTATy = xTPx + (Ax)Ty = xTPx,
and since P is positive semideÔ¨Ånite, it follows that Px = 0. The Ô¨Årst
equation now gives ATy = 0. Hence, x ‚ààN(A)‚à©N(P) and y ‚ààN(AT),
which means that x = 0 and y = 0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
131
Answers and solution to the exercises
15.5 a) By assumption, ‚ü®v, f ‚Ä≤‚Ä≤(x)v‚ü©‚â•¬µ‚à•v‚à•2 if Av = 0. Since AC = 0, we
conclude that
‚ü®w, Àúf ‚Ä≤‚Ä≤(z)w‚ü©= ‚ü®w, CTf ‚Ä≤‚Ä≤(x)Cw‚ü©= ‚ü®Cw, f ‚Ä≤‚Ä≤(x)Cw‚ü©‚â•¬µ‚à•Cw‚à•2
= ¬µ‚ü®w, CTCw‚ü©‚â•¬µœÉ‚à•w‚à•2
for all w ‚ààRp, which shows that the function Àúf is ¬µœÉ-strongly convex.
b) The assertion follows from a) if we show that the restriction of f to
X is a K‚àí2M ‚àí1-strongly convex function. So assume that x ‚ààX and
that Av = 0. Then f ‚Ä≤‚Ä≤(x)
AT
A
0
 v
0

=
f ‚Ä≤‚Ä≤(x)v
0

and due to the bound on the norm of the inverse matrix, we conclude
that
‚à•v‚à•‚â§K‚à•f ‚Ä≤‚Ä≤(x)v‚à•.
The positive semideÔ¨Ånite second derivative f ‚Ä≤‚Ä≤(x) has a positive semidef-
inite square root f ‚Ä≤‚Ä≤(x)1/2 and ‚à•f ‚Ä≤‚Ä≤(x)1/2‚à•= ‚à•f ‚Ä≤‚Ä≤(x)‚à•1/2 ‚â§M 1/2.
It
follows that
‚à•f ‚Ä≤‚Ä≤(x)v‚à•2 = ‚à•f ‚Ä≤‚Ä≤(x)1/2f ‚Ä≤‚Ä≤(x)1/2v‚à•2 ‚â§‚à•f ‚Ä≤‚Ä≤(x)1/2‚à•2‚à•f ‚Ä≤‚Ä≤(x)1/2v‚à•2
‚â§M‚à•f ‚Ä≤‚Ä≤(x)1/2v‚à•2 = M‚ü®v, f ‚Ä≤‚Ä≤(x)v‚ü©,
which inserted in the above inequality results in the inequality
‚ü®v, f ‚Ä≤‚Ä≤(x)v‚ü©‚â•K‚àí2M ‚àí1‚à•v‚à•2.
Chapter 16
16.2 Let Pi denote the projection of Rn1 √ó ¬∑ ¬∑ ¬∑ √ó Rnm onto then ith factor
Rni. Then f(x) = m
i=1 fi(Pix), so it follows from Theorems 16.1.5
and 16.1.6 that f is self-concordant.
16.3 a) The function g is convex, since g‚Ä≤‚Ä≤(x) = f ‚Ä≤(x)2
f(x)2 ‚àíf ‚Ä≤‚Ä≤(x)
f(x) + 1
x2 ‚â•0.
g‚Ä≤‚Ä≤‚Ä≤(x) = ‚àíf ‚Ä≤‚Ä≤‚Ä≤(x)
f(x) + 3f ‚Ä≤(x)f ‚Ä≤‚Ä≤(x)
f(x)2
‚àí2f ‚Ä≤(x)3
f(x)3 ‚àí2
x3 implies that
|g‚Ä≤‚Ä≤‚Ä≤(x)| ‚â§3 f ‚Ä≤‚Ä≤(x)
x|f(x)| + 3|f ‚Ä≤(x)|f ‚Ä≤‚Ä≤(x)
f(x)2
+ 2|f ‚Ä≤(x)|3
|f(x)|3 + 2 1
x3.
The inequality |g‚Ä≤‚Ä≤‚Ä≤(x)| ‚â§2g‚Ä≤‚Ä≤(x)3/2, which proves that the function g
is self-concordant, is now obtained by choosing a =

f ‚Ä≤‚Ä≤(x)/|f(x)|,
b = |f ‚Ä≤(x)|/|f(x)| and c = 1/x in the equality
3a2b + 3a2c + 2b3 + 2c3 ‚â§2(a2 + b2 + c2)3/2.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
132
Answers and solution to the exercises
132
To prove this inequality, we can due to homogeneity assume that
a2 + b2 + c2 = 1.
Inserting a2 = 1 ‚àíb2 ‚àíc2 into the inequality, we can rewrite it as
(b + c)(3 ‚àí(b + c)2) ‚â§2, which holds since x(3 ‚àíx2) ‚â§2 for x ‚â•0.
16.3 b) Let œÜ(t) = F(x0 +Œ±t, y0 +Œ≤t) be the restriction of F to an arbitrary
line through the point (x0, y0) in dom F. We will prove that œÜ is self-
concordant, and we have to treat the cases Œ± = 0 and Œ± Ã∏= 0 separately.
If Œ± = 0, then œÜ(t) = ‚àíln(Œ≤t + a) + b, where a = y0 ‚àíf(x0) and
b = ‚àíln x0, so œÜ is self-concordant in this case.
To prove the case Œ± Ã∏= 0, we note that f(x) ‚àíAx ‚àíB satisÔ¨Åes the
assumptions of the exercise for each choice of the constants A and
B, and hence h(x) = ‚àíln(Ax + B ‚àíf(x)) ‚àíln x is self-concordant
according to the result in a). But œÜ(t) = h(Œ±t + x0), where A = Œ≤/Œ±
and B = y0 ‚àíŒ≤x0/Œ±. Thus, œÜ is self-concordant.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
133
Answers and solution to the exercises
16.6 a) Set Œª = Œª(f, x) and use the inequalities (16.7) and (16.6) in Theorem
16.3.2 with y = x+ and v = x+ ‚àíx = (1 + Œª)‚àí1‚àÜxnt. This results in
the inequality
‚ü®f ‚Ä≤(x+), w‚ü©‚â§‚ü®f ‚Ä≤(x), w‚ü©+
1
1 + Œª‚ü®f ‚Ä≤‚Ä≤(x)‚àÜxnt, w‚ü©+
Œª2‚à•w‚à•x
(1 + Œª)2(1 ‚àíŒª/(1 + Œª))
= ‚ü®f ‚Ä≤(x), w‚ü©‚àí
1
1 + Œª‚ü®f ‚Ä≤(x), w‚ü©+
Œª2
1 + Œª‚à•w‚à•x
=
Œª
1 + Œª‚ü®f ‚Ä≤(x), w‚ü©+
Œª2
1 + Œª‚à•w‚à•x
‚â§
Œª
1 + ŒªŒª‚à•w‚à•x +
Œª2
1 + Œª‚à•w‚à•x = 2Œª2
1 + Œª‚à•w‚à•x
‚â§
2Œª2‚à•w‚à•x+
(1 + Œª)(1 ‚àíŒª/(1 + Œª)) = 2Œª2‚à•w‚à•x+
with Œª(f, x+) ‚â§2Œª2 as conclusion.
Chapter 18
18.1 Follows from Theorems 18.1.3 and 18.1.2.
18.2 To prove the implication ‚à•v‚à•‚àó
x < ‚àû‚áív ‚ààN(f ‚Ä≤‚Ä≤(x))‚ä•we write v
as v = v1 + v2 with v1 ‚ààN(f ‚Ä≤‚Ä≤(x)) and v2 ‚ààN(f ‚Ä≤‚Ä≤(c))‚ä•, noting that
‚à•v1‚à•x = 0. Hence ‚à•v‚à•2
1 = ‚ü®v1, v1‚ü©= ‚ü®v, v1‚ü©‚â§‚à•v‚à•‚àó
x‚à•v1‚à•x = 0, and we
conclude that v1 = 0. This proves that v belongs to N(f ‚Ä≤‚Ä≤(x))‚ä•.
Given v ‚ààN(f ‚Ä≤‚Ä≤(x))‚ä•there exists a vector u such that v = f ‚Ä≤‚Ä≤(x)u. We
shall prove that ‚à•v‚à•‚àó
x = ‚à•u‚à•x. From this follows that ‚à•v‚à•‚àó
x < ‚àûand
that ‚à•¬∑‚à•‚àó
x is a norm on the subspace N(f ‚Ä≤‚Ä≤(x))‚ä•of Rn.
Let w ‚ààRn be arbitrary. By Cauchy‚ÄìSchwarz‚Äôs inequality,
‚ü®v, w‚ü©= ‚ü®f ‚Ä≤‚Ä≤(x)u, w‚ü©= ‚ü®f ‚Ä≤‚Ä≤(x)1/2u, f ‚Ä≤‚Ä≤(x)1/2w‚ü©
‚â§‚à•f ‚Ä≤‚Ä≤(x)1/2u‚à•‚à•f ‚Ä≤‚Ä≤(x)1/2w‚à•= ‚à•u‚à•x‚à•v‚à•x,
and this implies that ‚à•v‚à•‚àó
x ‚â§‚à•u‚à•x. Suppose v Ã∏= 0. Then u does not
belong to N(f ‚Ä≤‚Ä≤(x)), which means that ‚à•u‚à•x Ã∏= 0, and for w = u/‚à•u‚à•x
we get the identity
‚ü®v, w‚ü©= ‚à•u‚à•‚àí1
x ‚ü®f ‚Ä≤‚Ä≤(x)1/2u, f ‚Ä≤‚Ä≤(x)1/2u‚ü©= ‚à•u‚à•‚àí1
x ‚à•f ‚Ä≤‚Ä≤(x)1/2u‚à•2 = ‚à•u‚à•x,
which proves that ‚à•v‚à•‚àó
x = ‚à•u‚à•x. If on the other hand v = 0, then u is
a vector in N(f ‚Ä≤‚Ä≤(x)) so we have ‚à•v‚à•‚àó
x = ‚à•u‚à•x in this case, too.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
134
Answers and solution to the exercises
18.3 a) DiÔ¨Äerentiate the equality f(tx) = f(x) ‚àíŒΩ ln t with respect to x.
b) DiÔ¨Äerentiate the equality obtained in a) with respect to t and then
take t = 1.
c) Since X does not contain any line, f is a non-degenerate self-concor-
dant function, and it follows from the result in b) that x is the unique
Newton direction of f at the point x. By diÔ¨Äerentiating the equality
f(tx) = f(x)‚àíŒΩ ln t with respect to t and then putting t = 1, we obtain
‚ü®f ‚Ä≤(x), x‚ü©= ‚àíŒΩ. Hence
ŒΩ = ‚àí‚ü®f ‚Ä≤(x), x‚ü©= ‚àí‚ü®f ‚Ä≤(x), ‚àÜxnt‚ü©= Œª(f, x)2.
18.5 DeÔ¨Åne g(x, xn+1) = (x2
1 + ¬∑ ¬∑ ¬∑ + x2
n) ‚àíx2
n+1 = ‚à•x‚à•2 ‚àíx2
n+1, so that
f(x) = ‚àíln(‚àíg(x, xn+1)),
and let w = (v, vn+1). Then
Dg = Dg(x, xn+1)[w] = 2(‚ü®v, x‚ü©‚àíxn+1vn+1),
D2g = D2g(x, xn+1)[w, w] = 2(‚à•v‚à•2 ‚àív2
n+1),
D3g = D3g(x, xn+1)[w, w, w] = 0,
Df = Df(x, xn+1)[w] = ‚àí1
gDg
D2f = D2f(x, xn+1)[w, w] = 1
g2

(Dg)2 ‚àígD2g

,
D3f = D3f(x, xn+1)[w, w, w] = 1
g3

‚àí2(Dg)3 + 3gDgD2g

.
Consider the diÔ¨Äerence
‚àÜ= (Dg)2‚àígD2g = 4(‚ü®x, v‚ü©‚àíxn+1vn+1)2+2(x2
n+1‚àí‚à•x‚à•2)(‚à•v‚à•2‚àív2
n+1).
Since xn+1 > ‚à•x‚à•, we have ‚àÜ‚â•0 if |vn+1| ‚â§‚à•v‚à•. So suppose that
|vn+1| > ‚à•v‚à•. Then
|xn+1vn+1 ‚àí‚ü®x, v‚ü©| ‚â•xn+1|vn+1| ‚àí|‚ü®x, v‚ü©|
‚â•xn+1|vn+1| ‚àí‚à•x‚à•‚à•v‚à•‚â•0,
and it follows that
‚àÜ‚â•4(xn+1|vn+1| ‚àí‚à•x‚à•‚à•v‚à•)2 + 2(x2
n+1 ‚àí‚à•x‚à•2)(‚à•v‚à•2 ‚àív2
n+1)
= 2(xn+1|vn+1| ‚àí‚à•x‚à•‚à•v‚à•)2 + 2(xn+1‚à•v‚à•‚àí‚à•x‚à•|vn+1|)2 ‚â•0.
This shows that D2f = ‚àÜ/g2 ‚â•0, so f is a convex function.
To prove that the function is self-concordant, we shall show that
4(D2f)3 ‚àí(D3f)2 ‚â•0.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
135
Answers and solution to the exercises
After simpliÔ¨Åcation we obtain
4(D2f)3 ‚àí(D3f)2 = g‚àí4(D2g)2(3(Dg)2 ‚àí4gD2g),
and the problem has now been reduced to showing that the diÔ¨Äerence
‚àÜ‚Ä≤ = 3(Dg)2 ‚àí4gD2g
= 12(‚ü®x, v‚ü©‚àíxn+1vn+1)2 + 8(x2
n+1 ‚àí‚à•x‚à•2)(‚à•v‚à•2 ‚àív2
n+1)
is nonnegative. This is obvious if |vn+1| ‚â§‚à•v‚à•, and if |vn+1| > ‚à•v‚à•then
we get in a similar way as above
‚àÜ‚Ä≤ ‚â•12(xn+1|vn+1| ‚àí‚à•x‚à•‚à•v‚à•)2 + 8(x2
n+1 ‚àí‚à•x‚à•2)(‚à•v‚à•2 ‚àív2
n+1)
= 4(xn+1|vn+1| ‚àí‚à•x‚à•‚à•v‚à•)2 + 8(xn+1‚à•v‚à•‚àí‚à•x‚à•|vn+1|)2 ‚â•0.
18.6 Let w = (u, v) be an arbitrary vector in R2. Writing a = 1/(y ‚àíf(x)),
b = ‚àí1/x, A = f ‚Ä≤(x) and B = f ‚Ä≤‚Ä≤(x) for short, where a > 0 and B ‚â•0,
we obtain
DF(x, y)[w] = (aA + b)u ‚àíav
D2F(x, y)[w, w] = (aB + a2A2 + b2)u2 ‚àí2a2Auv + a2v2,
and
2D2F(x, y)[w, w] ‚àí

DF(x, y)[w]
2
= a2A2u2 + b2u2 + a2v2 + 2abuv ‚àí2a2Auv ‚àí2abAu2 + 2aBu2
= (aAu ‚àíbu ‚àíav)2 + 2aBu2 ‚â•0.
So F is a 2-self-concordant function.
18.7 Use the previous exercise with f(x) = x ln x.
18.8 Taking f(x) = ‚àíln x in exercise 18.5, we see that
F(x, y) = ‚àíln(ln x + y) ‚àíln x
is a 2-self-concordant barrier to the closure of the region ‚àíy < ln x.
Since G(x, y) = F(y, ‚àíx), it then follows from Theorem 18.1.3 that G
is a 2-self-concordant barrier to the region y ‚â•ex.
Download free eBooks at bookboon.com

DESCENT AND INTERIOR-POINT METHODS: 
CONVEXITY AND OPTIMIZATION ‚Äì PART III
136
Index
Index
analytic center, 74
Armijo‚Äôs rule, 3
barrier, 74
central path, 76
convergence
linear, 6, 7
quadratic, 6, 7
damped Newton method, 23
descent algorithm, 1
dual local norm, 92
gradient descent method, 2, 7
inner iteration, 79
input length, 114
line search, 2
linear convergence, 6, 7
local seminorm, 18
logarithmic barrier, 75
ŒΩ-self-concordant barrier, 83
Newton
decrement, 16, 35
direction, 15, 35
method, 2, 23, 66
non-degenerate, 45
outer iteration, 79
path-following method, 79
phase 1, 81
pure Newton method, 23
puriÔ¨Åcation, 110
quadratic convergence, 6, 7
search direction, 2
self-concordant, 42
standard form, 94
step size, 2
Download free eBooks at bookboon.com


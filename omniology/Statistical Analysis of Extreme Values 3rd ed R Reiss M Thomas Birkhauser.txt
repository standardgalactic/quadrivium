

R.-D. Reiss
M. Thomas
Statistical Analysis of 
Extreme Values
with Applications to Insurance, Finance, 
Hydrology and Other Fields
Third Edition
Birkhäuser
Basel · Boston · Berlin

2000 Mathematics Subject Classiﬁcation 60G70, 62P05, 62P99, 90A09, 62N05, 68N15, 62-07, 
62-09, 62F10, 62G07, 62G09, 62E25, 62G30, 62M10
Library of Congress Control Number: 2007924804
Bibliographic information published by Die Deutsche Bibliothek
Die Deutsche Bibliothek lists this publication in the Deutsche Nationalbibliograﬁe; 
detailed bibliographic data is available in the Internet at http://dnb.ddb.de
ISBN 978-3-7643-7230-9 Birkhäuser Verlag, Basel - Boston - Berlin
This work is subject to copyright. All rights are reserved, whether the whole or part of the 
material is concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, 
recitation, broadcasting, reproduction on microﬁlms or in other ways, and storage in data 
banks. For any kind of use permission of the copyright owner must be obtained. 
First edition 1997
Second edition 2001
© 2007 Birkhäuser Verlag AG, P.O. Box 133, CH-4010 Basel, Switzerland
Part of Springer Science+Business Media
Printed on acid-free paper produced from chlorine-free pulp. TCF f
Cover design: Alexander Faust, Basel, Switzerland
Printed in Germany
ISBN 978-3-7643-7230-9 
 
 
 
 
e-ISBN 978-3-7643-7399-3
9 8 7 6 5 4 3 2 1 
 
 
 
 
 
www.birkhauser.ch
Rolf-Dieter Reiss
Michael Thomas
FB Mathematik
Universität Siegen 
Walter-Flex-Str. 3 
57068 Siegen
Germany
e-mail: reiss@stat.math.uni-siegen.de
                michael@stat.math.uni-siegen.de

Preface to the Third Edition
First of all we would like to thank all those who showed a steady, extraordinary in-
terest in this book about extreme value analysis. This gave the motivation, courage
and opportunity to provide an update of the book and to work out new topics,
which primarily focus on dependencies, conditional analysis based on serial and
covariate information, predictions, and the multivariate modeling of extremes.
As mentioned by Chris Heyde1 while reviewing the second edition: “There is a
considerable statistical content in the book, quite apart from its focus on extremes.
... The authors are seeking, quite properly, to embed the analysis of extreme values
into the mainstream of applied statistical investigations.” This strategy has been
continued and strengthened in the new edition.
With each new edition there are complex questions as well as complex solu-
tions. In that context, the cooperation with distinguished experts becomes more
and more important.
Parts I–III about the basic extreme value methodology remained unchanged
to some larger extent, yet notable are new or extended sections about
• Testing Extreme Value Conditions with Applications related to goodness–of–
ﬁt tests, co–authored by J. H¨usler and D. Li;
• The Log–Pareto Model and other Pareto–Extensions with a view towards
super–heavy tailed distributions;
• An Overview of Reduced–Bias Estimation, an approach related to the jack-
knife method, co–authored by M.I. Gomes;
• The Spectral Decomposition Methodology which reduces multivariate ques-
tions to univariate ones to some extent,
• About Tail Independence with testing tail dependence against certain degrees
of tail independence, co–authored by M. Frick.
1Heyde, C. (2002). Book Review: Statistical Analysis ... Australian & New Zealand J.
Statist. 44, 247–248.

vi
Preface to the Third Edition
Of central importance are the new chapters entitled
• Extreme Value Statistics of Dependent Random Variables co–authored by
H. Drees;
• Conditional Extremal Analysis which provides the necessary technical sup-
port for certain applications;
• Elliptical and Related Distributions with special emphasis on multivariate
Student and sum–stable distributions.
Other new topics are collected within
Part IV:
Topics in Hydrology and Environmental Sciences;
Part V:
Topics in Finance and Insurance,
Part VI:
Topics in Material and Life Sciences.
Within these parts one may ﬁnd
• a new chapter about Environmental Sciences, co–authored by R.W. Katz,
with a detailed description of the concepts of cycles, trends and covariates;
• a new section about Predicting the Serial Conditional VaR, co–authored by
A. Kozek and C.S. Wehn, including remarks about the model validation;
• a new section about Stereology of Extremes, co–authored by E. Kaufmann,
with remarks about modeling and estimation.
The entire text has been thoroughly updated and rearranged to meet the
requirements. The new results and topics are elaborated on about 120 pages.
The book includes the statistical MS Windows application Academic Xtremes
4.1 and StatPascal on CD. The STABLE package for sum–stable distributions is
no longer included. The major diﬀerence of the academic version compared to
the professional one is a restriction of the executable sample sizes. Consequently,
not all of the numerical examples in the book can be executed with the academic
version.
To keep the book at a reasonable size, the former sections Implementation in
Xtremes and the separate Case Studies in Extreme Value Analysis were omitted.
The Appendix about Xtremes and StatPascal of the second edition is considerably
shortened. The full description of Xtremes and StatPascal—also including the
former sections Implementation in Xtremes—may be found in the Xtremes User
Manual which is enclosed as a pdf–ﬁle on the CD.
It is a pleasure to thank Th. Hempﬂing who showed great eﬃciency in editing
this book.
Siegen, Germany
Rolf–Dieter Reiss
Michael Thomas

Preface to the Second Edition
With the second edition we continue a project concerning extreme value analysis
in combination with the interactive statistical software Xtremes which started
in the late 1980’s. An early publication was Chapter 6 together with the User’s
Guide to Xtremes in [16], besides tutorials for the statistical software Xtremes in
1993 and 1995 which had a wider circulation within the extreme value community.
These eﬀorts culminated in the ﬁrst edition of the present book which has found
a favorable reception from the side of practitioners and in academic circles.
The new highlights of this extended edition, elaborated on about 160 pages,
include
• the statistical modeling of tails in conjunction with the global modeling of
distributions with special emphasis laid on heavy–tailed distributions such
as sum–stable and Student distributions;
• the Bayesian methodology with applications to regional ﬂood frequency anal-
ysis and credibility estimation in reinsurance business;
• von Mises type upper bounds on remainder terms in the exceedance process
approximation and a thorough theoretical and practical treatment of the
phenomenon of penultimate distributions;
• a section about conditional extremes;
• an extension of the chapter about multivariate extreme value models, espe-
cially for the Gumbel–McFadden model with an application to the theory of
economic choice behavior;
• a chapter about the bivariate peaks–over–threshold method;
• risk assessment of ﬁnancial assets and portfolios in the presence of fat and
heavy–tailed distributions by means of the Value–at–Risk (VaR);
• VaR under the Black–Scholes pricing and for general derivative contracts;
• sections about corrosion analysis and oldest–old questions.

viii
Preface to the Second Edition
The “analysis of extreme values must be embedded in various other ap-
proaches of main stream statistics” as mentioned in the ﬁrst edition. In that con-
text, M. Ivette Gomes2 remarks “its scope is much broader, and I would rather
consider it a welcome addition to the reference works in applied statistics ... though
there is a unifying basis provided by extreme value theory.” For the second edition,
it is our declared aim to enforce this characteristic of providing a broad statistical
background in the book.
In Part V there is a continuation of the successful program concerning self–
contained “Case Studies in Extreme Value Analysis” of other authors. The case
studies in the ﬁrst edition are replaced by new ones with emphasis laid on envi-
ronmental extreme value statistics. We thank Humberto Vaquera, Jos´e Villase˜nor,
Stuart Coles, J¨urg H¨usler, Daniel Dietrich, Dietmar Pfeifer, Pieter van Gelder and
Dan Lungu for the new contributions.
It was a pleasure to cooperate with several distinguished experts in vari-
ous ﬁelds, namely, with John Nolan (sum–stable distributions), Edgar Kaufmann
(rates of convergence and longevity of humans), Michael Falk (multivariate peaks–
over–threshold), Jon Hosking (ﬂood frequency analysis), Michael Radtke (insur-
ance) and Casper de Vries and Silvia Caserta (ﬁnance).
The present statistical software environment is much more than an update of
Xtremes, Version 2.1. As a consequence of our intention to establish a book about
applied statistics—with a unifying basis provided by extreme value statistics—the
Xtremes package becomes more and more applicable to various statistical ﬁelds.
Further introductory remarks may be found in
• Xtremes: Overview and the Hierarchy
• Xtremes and StatPascal Within the Computing Environment RiskTec
after the Prefaces and at the beginning of the Appendix.
We would like to thank colleagues, readers and users of the ﬁrst edition and
Xtremes for their comments, questions and suggestions; among others, Claudio
Baraldi, Arthur B¨oshans, Holger Drees, Harry Harper, Sylvia Haßmann, Claudia
Kl¨uppelberg, Elson Lee, Frank Marohn, Alexander McNeil, Richard Smith, Q.J.
Wang, Carsten Wehn.
Siegen, Germany
Rolf–Dieter Reiss
Michael Thomas
2Gomes, M.I. (1999). Book Review: Statistical Analysis ... Extremes 2, 111–113.

Preface to the First Edition
This textbook deals with the statistical modeling and analysis of extremes. The
restriction of the statistical analysis to this special ﬁeld is justiﬁed by the fact
that the extreme part of the sample can be of outstanding importance. It may
exhibit a larger risk potential of random scenery such as ﬂoods, hurricanes, high
concentration of air pollutants, extreme claim sizes, price shocks, incomes, life
spans, etc. The fact that the likelihood of a future catastrophe is not negligible
may initiate reactions which will help to prevent a greater disaster. Less spectacular
yet important, the statistical insight gained from extremes can be decisive in daily
business life or for the solution to ecological or technical problems.
Although extreme value analysis has its peculiarities, it cannot be looked at
in an isolated manner. Therefore, the analysis of extreme values must be embed-
ded in other various approaches of main stream statistics such as data analysis,
nonparametric curve estimation, survival analysis, time series analysis, regression
analysis, robust statistics and parametric inference.
The book is divided into
Part I:
Modeling and Data Analysis;
Part II:
Statistical Inference in Parametric Models;
Part III:
Elements of Multivariate Analysis;
Part IV:
Topics in Insurance, Finance and Hydrology;
Part V:
Case Studies in Extreme Value Analysis,
Appendix: An Introduction to XTREMES.
Whenever problems involving extreme values arise, statisticians in many
ﬁelds of modern science and in engineering or the insurance industry may proﬁtably
employ this textbook and the included software system XTREMES. This book is
helpful for various teaching purposes at colleges and universities on the undergrad-
uate and graduate levels. In larger parts of the book, it is merely presumed that
the reader has some knowledge of basic statistics. Yet more and more statistical

x
Preface to the First Edition
prerequisites are needed in the course of reading this book. Several paragraphs and
subsections about statistical concepts are intended to ﬁll gaps or may be regarded
as shorter refresher units. Parts I and II (with the exception of Chapter 6) are
elementary yet basic for the statistical analysis of extreme values. It is likely that
a more profound statistical background is helpful for a thorough understanding of
the advanced topics in Chapter 6 and the multivariate analysis in Part III.
Part I sets out the statistical background required for the modeling of extreme
values. The basic parametric models are introduced and theoretically justiﬁed in
Chapter 1. The nonparametric tools introduced in Chapter 2 are most impor-
tant for our approach to analyzing extreme values. In this context the included
statistical software system is helpful to
• get a ﬁrst insight into the data by means of visualizations;
• employ a data–based parametric modeling and assess the adequacy;
• draw statistical conclusions in a subjective manner;
• carry out the statistical inference in an objective (automatic) manner, and
• control results of parametric inference by nonparametric procedures.
Part II deals with statistical procedures in the parametric extreme value (EV)
and generalized Pareto (GP) models. Yet, at the beginning, we start with the sta-
tistical inference in normal and Poisson models (Chapter 3) in order to give an
outline of our approach to statistical questions within a setting which is familiar
to a larger readership. From our viewpoint, the Gaussian model is merely relevant
for the center of a distribution and, thus, not for extreme values. Chapters 4 and
5 develop the statistical methodology that is necessary for dealing with extremes.
Applied questions are addressed in various examples. These examples also include
critical examinations of case studies in publications which are occasionally very
ambitious. We will approach extreme value analysis from a practical viewpoint,
yet references are given to the theoretical background (as developed in the books
[24], [20], [39], [42] and [16]). Applied contributions to extreme value analysis can
be found in several journals. It is recommended to have a look at the J. Hydrology,
Insurance: Mathematics and Economics, J. Econometrics, J. Royal Statistical So-
ciety B and, particularly, at the forthcoming journal Extremes, Statistical Theory
and Applications in Science, Engineering and Economics. Other valuable sources
for applied work are the recent Gaithersburg proceeding volumes [15] and the
hydrology proceedings [13].
Part III contains supplementary material about the analysis of multivariate
data and auxiliary results for multivariate distributions applied in Part II. Initially,
a textbook for univariate extremes was scheduled. Yet, it is evident that a time–
scale must be included in conjunction with time series phenomena, for example,
exceedance times and exceedances are jointly visualized in a scatterplot. This was

Preface to the First Edition
xi
our ﬁrst step towards multivariate data. Further extensions of our initial frame-
work followed so that, ﬁnally, we decided to include some procedures concerning
multivariate extremes.
We also want to learn in which manner the methodology provided by the pre-
vious parts can be made applicable in certain areas. Part IV deals with important
questions in
• insurance (coauthored by Michael Radtke),
• ﬁnance (coauthored by Casper G. de Vries),
• hydrology.
We hope that the explanations are also of interest for non–specialists.
Part IV has a certain continuation in Part V which contains several case
studies in extreme value analysis. The case studies are written in the form of self–
contained articles, which facilitated the inclusion of studies of other authors. One
basic requirement for any case study is that the underlying data must be publicly
accessible because, otherwise, the hypotheses and conclusions of an analyst can-
not be critically examined and further improved by others, which would strongly
violate scientiﬁc principles. We would like to thank Ana M. Ferreira, Edgar Kauf-
mann, Cornelia Hillg¨artner, Tailen Hsing and J¨urg H¨usler for their contributions.
The appendix is a manual for the included statistical software XTREMES. The
menu–driven part of XTREMES allows us to surf through data sets and statistical
models and reduces the “start up” costs of working in this area. For special prob-
lems one may employ the integrated programming language XPL. A short overview
of the hierarchy of XTREMES is given after this preface. We believe that an expe-
rienced reader can partially handle XTREMES after having read the overview. A
further link between the book and the statistical software is provided by sections
entitled “Implementation in XTREMES” at the end of the chapters.
We will not make any attempt to give exhaustive references to the extreme
value literature. Several footnotes provide hints to papers and books which are
important from the historical viewpoint or may be helpful to get a more thorough
understanding of special questions. The bibliography merely consists of references
to monographs and proceeding volumes that are suggested for further reading or
cited several times within the text.
We are grateful to several colleagues for valuable suggestions and stimu-
lating discussions, especially, Sandor Cs¨org˝o, Richard A. Davis, Paul Embrechts,
Michael Falk, Laurens de Haan, J¨urg H¨usler, Edgar Kaufmann, Alex Koning, Ross
Leadbetter, Wolfgang Merzenich, Wolfgang Wefelmeyer. We would like to express
particular gratitude to the coauthors of Chapters 9 and 10 who had a constructive
impact on our work. Very special thanks are due to Sylvia Haßmann for collabora-
tion on the MS–DOS version of XTREMES (documented in [16]) and for assistance
in writing a ﬁrst draft of Section 6.1.

xii
Preface to the First Edition
Several generations of students were exposed to the development of this book
and the included software system; we acknowledge warmly the assistance of Si-
mon Budig (ﬁnal version of the UserFormula facility), Andreas Heimel (help sys-
tem), Jens Olejak (minimum distance estimators), Claudia Schmidt (expert for
the Moselle data) and Karsten Tambor (previous version of the multivariate mode
of XTREMES). The technical assistance of Sarah Schultz and Maximilian Reiss was
very helpful.
Part of the work of the ﬁrst author was done as a guest professor at the
Tinbergen Institute, Rotterdam, and visiting the Center for Stochastic Processes,
Chapel Hill. Thanks are due to Laurens de Haan and Ross Leadbetter for their
hospitality. The stimulating atmospheres of these institutions had a greater impact
on the course of this work. The stay at the Tinbergen Institute also enabled the
cooperation with Casper de Vries.
Siegen, Germany
Rolf–Dieter Reiss
Michael Thomas

Contents
Preface to the Third Edition . . . . . . . . . . . . . . . . . . . . . .
v
Preface to the Second Edition . . . . . . . . . . . . . . . . . . . . .
vii
Preface to the First Edition . . . . . . . . . . . . . . . . . . . . . . .
ix
List of Special Symbols
. . . . . . . . . . . . . . . . . . . . . . . . .
xviii
I
Modeling and Data Analysis
1
1
Parametric Modeling
3
1.1
Applications of Extreme Value Analysis . . . . . . . . . . . . . . .
3
1.2
Observing Exceedances and Maxima . . . . . . . . . . . . . . . . .
7
1.3
Modeling by Extreme Value Distributions . . . . . . . . . . . . . .
14
1.4
Modeling by Generalized Pareto Distributions . . . . . . . . . . . .
23
1.5
Heavy and Fat–Tailed Distributions
. . . . . . . . . . . . . . . . .
30
1.6
Quantiles, Transformations and Simulations . . . . . . . . . . . . .
35
2
Diagnostic Tools
39
2.1
Visualization of Data . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.2
Excess and Hazard Functions . . . . . . . . . . . . . . . . . . . . .
49
2.3
Fitting Parametric Distributions to Data . . . . . . . . . . . . . . .
56
2.4
Q–Q and P–P Plots
. . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.5
Trends, Seasonality and Autocorrelation . . . . . . . . . . . . . . .
64
2.6
The Tail Dependence Parameter
. . . . . . . . . . . . . . . . . . .
74
2.7
Clustering of Exceedances . . . . . . . . . . . . . . . . . . . . . . .
76
II
Statistical Inference in Parametric Models
81
3
An Introduction to Parametric Inference
83
3.1
Estimation in Exponential and Gaussian Models
. . . . . . . . . .
84
3.2
Conﬁdence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . .
90
3.3
Test Procedures and p–Values . . . . . . . . . . . . . . . . . . . . .
93
3.4
Inference in Poisson and Mixed Poisson Models . . . . . . . . . . .
96
xiii

xiv
Contents
3.5
The Bayesian Estimation Principle . . . . . . . . . . . . . . . . . .
102
4
Extreme Value Models
107
4.1
Estimation in Extremes Value Models
. . . . . . . . . . . . . . . .
107
4.2
Testing within Extreme Value Models
. . . . . . . . . . . . . . . .
118
4.3
Extended Extreme Value Models and Related Models
. . . . . . .
120
5
Generalized Pareto Models
127
5.1
Estimation in Generalized Pareto Models
. . . . . . . . . . . . . .
127
5.2
Testing Within Generalized Pareto Models . . . . . . . . . . . . . .
143
5.3
Testing Extreme Value Conditions with Applications
(co–authored by J. H¨usler and D. Li) . . . . . . . . . . . . . . . . .
144
5.4
Statistics in Poisson–GP Models
. . . . . . . . . . . . . . . . . . .
152
5.5
The Log–Pareto Model and Other Pareto–Extensions . . . . . . . .
154
6
Advanced Statistical Analysis
159
6.1
Non–Random and Random Censoring
. . . . . . . . . . . . . . . .
159
6.2
Models of Time Series, the Extremal Index
. . . . . . . . . . . . .
164
6.3
Statistics for Student Distributions . . . . . . . . . . . . . . . . . .
170
6.4
Statistics for Sum–Stable Distributions
(co–authored by J.P. Nolan) . . . . . . . . . . . . . . . . . . . . . .
172
6.5
Ultimate and Penultimate GP Approximation
(co–authored by E. Kaufmann) . . . . . . . . . . . . . . . . . . . .
182
6.6
An Overview of Reduced–Bias Estimation
(co–authored by M.I. Gomes) . . . . . . . . . . . . . . . . . . . . .
190
7
Statistics of Dependent Variables
(coauthored by H. Drees)
207
7.1
The Impact of Serial Dependence . . . . . . . . . . . . . . . . . . .
208
7.2
Estimating the Extreme Value Index . . . . . . . . . . . . . . . . .
209
7.3
Extreme Quantile Estimation . . . . . . . . . . . . . . . . . . . . .
215
7.4
A Time Series Approach . . . . . . . . . . . . . . . . . . . . . . . .
219
8
Conditional Extremal Analysis
227
8.1
Interpretations and Technical Preparations
. . . . . . . . . . . . .
227
8.2
Conditional Extremes: a Nonparametric Approach . . . . . . . . .
238
8.3
Maxima Under Covariate Information
. . . . . . . . . . . . . . . .
240
8.4
The Bayesian Estimation Principle, Revisited . . . . . . . . . . . .
242
9
Statistical Models for Exceedance Processes
247
9.1
Modeling Exceedances by Poisson Processes:
the Homogeneous Case . . . . . . . . . . . . . . . . . . . . . . . . .
247
9.2
Mean and Median T –Year Return Levels . . . . . . . . . . . . . . .
250
9.3
ML and Bayesian Estimation in Models of Poisson Processes
. . .
252
9.4
GP Process Approximations (co–authored by E. Kaufmann) . . . .
256

Contents
xv
9.5
Inhomogeneous Poisson Processes,
Exceedances Under Covariate Information . . . . . . . . . . . . . .
258
III
Elements of Multivariate Statistical Analysis
263
10 Basic Multivariate Concepts and Visualization
265
10.1 An Introduction to Basic Multivariate Concepts . . . . . . . . . . .
265
10.2 Visualizing Multivariate Data . . . . . . . . . . . . . . . . . . . . .
270
10.3 Decompositions of Multivariate Distributions . . . . . . . . . . . .
275
11 Elliptical and Related Distributions
279
11.1 Multivariate Gaussian Models . . . . . . . . . . . . . . . . . . . . .
279
11.2 Spherical and Elliptical Distributions . . . . . . . . . . . . . . . . .
281
11.3 Multivariate Student Distributions . . . . . . . . . . . . . . . . . .
283
11.4 Multivariate Sum–Stable Distributions
(co–authored by J.P. Nolan) . . . . . . . . . . . . . . . . . . . . . .
285
12 Multivariate Maxima
291
12.1 Nonparametric and Parametric Extreme Value Models . . . . . . .
291
12.2 The Gumbel–McFadden Model . . . . . . . . . . . . . . . . . . . .
301
12.3 Estimation in Extreme Value Models . . . . . . . . . . . . . . . . .
305
12.4 A Spectral Decomposition Methodology . . . . . . . . . . . . . . .
309
13 Multivariate Peaks Over Threshold
(co–authored by M. Falk)
313
13.1 Nonparametric and Parametric Generalized Pareto Models . . . . .
313
13.2 Estimation of the Canonical Dependence Function
. . . . . . . . .
318
13.3 About Tail Independence (co–authored by M. Frick) . . . . . . . .
321
13.4 The Point Process Approach to the Multivariate POT Method
. .
333
IV
Topics in Hydrology and Environmental Sciences
335
14 Flood Frequency Analysis
(co–authored by J.R.M. Hosking)
337
14.1 Analyzing Annual Flood Series . . . . . . . . . . . . . . . . . . . .
337
14.2 Analyzing Partial Duration Series . . . . . . . . . . . . . . . . . . .
338
14.3 Regional Flood Frequency Analysis . . . . . . . . . . . . . . . . . .
343
14.4 The L–Moment Estimation Method
. . . . . . . . . . . . . . . . .
345
14.5 A Bayesian Approach to Regional Estimation . . . . . . . . . . . .
349

xvi
Contents
15 Environmental Sciences
(co–authored by R.W. Katz)
353
15.1 Environmental Extremes . . . . . . . . . . . . . . . . . . . . . . . .
353
15.2 Inclusion of Covariates . . . . . . . . . . . . . . . . . . . . . . . . .
356
15.3 Example of Trend
. . . . . . . . . . . . . . . . . . . . . . . . . . .
359
15.4 Example of Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361
15.5 Example of Covariate
. . . . . . . . . . . . . . . . . . . . . . . . .
364
15.6 Numerical Methods and Software . . . . . . . . . . . . . . . . . . .
367
V
Topics in Finance and Insurance
369
16 Extreme Returns in Asset Prices
(co–authored by C.G. de Vries and S. Caserta)
371
16.1 Stylized Facts and Historical Remarks . . . . . . . . . . . . . . . .
372
16.2 Empirical Evidence in Returns Series . . . . . . . . . . . . . . . . .
375
16.3 Parametric Estimation of the Tails of Returns . . . . . . . . . . . .
378
16.4 The Proﬁt/Loss Variable and Risk Parameters
. . . . . . . . . . .
382
16.5 Evaluating the Value–at–Risk (VaR) . . . . . . . . . . . . . . . . .
386
16.6 The VaR for a Single Derivative Contract . . . . . . . . . . . . . .
392
16.7 GARCH and Stochastic Volatility Structures
. . . . . . . . . . . .
395
16.8 Predicting the Serial Conditional VaR
(co–authored by A. Kozek and C.S. Wehn)
. . . . . . . . . . . . .
401
17 The Impact of Large Claims on Actuarial Decisions
(co–authored by M. Radtke)
411
17.1 Numbers of Claims and the Total Claim Amount . . . . . . . . . .
412
17.2 Estimation of the Net Premium . . . . . . . . . . . . . . . . . . . .
415
17.3 Segmentation According to the Probable Maximum Loss . . . . . .
419
17.4 The Risk Process and the T –Year Initial Reserve . . . . . . . . . .
426
17.5 Elements of Ruin Theory
. . . . . . . . . . . . . . . . . . . . . . .
432
17.6 Credibility (Bayesian) Estimation of the Net Premium . . . . . . .
434
VI
Topics in Material and Life Sciences
439
18 Material Sciences
441
18.1 Extremal Corrosion Engineering
. . . . . . . . . . . . . . . . . . .
441
18.2 Stereology of Extremes (co–authored by E. Kaufmann) . . . . . . .
445
19 Life Science
(co–authored by E. Kaufmann)
453
19.1 About the Longevity of Humans
. . . . . . . . . . . . . . . . . . .
453

Contents
xvii
19.2 Extrapolating Life Tables To Extreme Life Spans:
A Regression Approach
. . . . . . . . . . . . . . . . . . . . . . . .
458
Appendix: First Steps towards
Xtremes and StatPascal
465
A The Menu System
467
A.1 Installation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
467
A.2 Overview and the Hierarchy . . . . . . . . . . . . . . . . . . . . . .
467
A.3 Becoming Acquainted with the Menu System . . . . . . . . . . . .
470
A.4 Technical Aspects of Xtremes . . . . . . . . . . . . . . . . . . . . .
476
A.5 The UserFormula (UFO) Facilities
. . . . . . . . . . . . . . . . . .
481
B The StatPascal Programming Language
485
B.1
Programming with StatPascal: First Steps . . . . . . . . . . . . . .
486
B.2
Plotting Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
490
B.3
Generating and Accessing Data . . . . . . . . . . . . . . . . . . . .
492
Author Index
495
Subject Index
501
Bibliography
509

List of Special Symbols
F
distribution function (df); likewise we use symbols G, W etc. for dfs
Fµ,σ
df F with added location and scale parameters µ and σ; p. 16
F
survivor function of df F; pages 11 and 266
f
density of df F with  x
∞f(y) dy = F(x); likewise, we use g, w etc.
F −1
quantile function (qf) pertaining to the df F; p. 40
F [u]
exceedance df at u (truncation of df F left of u); p. 12
F (u)
excess df at u (residual life df at age u) pertaining to the df F; p. 49
Gi,α
standard extreme value (EV) df for maximum (ith submodel); p. 15
Gγ
standard extreme value (EV) df for maximum (uniﬁed model); p. 16
Wi,α
standard generalized Pareto (GP) df (ith submodel); p. 24
Wγ
standard generalized Pareto (GP) df (uniﬁed model); p. 25
Φ
standard normal (Gaussian) df; p. 31
F m
mth power of df F (with F m(t) := (F(t))m); p. 10
F m∗
mth convolution of df F; p. 30
xi:n
ith ordered value of data x1, . . . , xn (in non–decreasing order); p. 42
Fn
sample df; occasionally written Fn(x; ·) to indicate the dependence on
x = (x1, . . . , xn); p. 39
F −1
n
sample qf; p. 42
α(F)
left endpoint of df F; p. 12
ω(F)
right endpoint of df F; p. 11
I(y ≤x)
indicator function with I(y ≤x) = 1 of y ≤x and 0, otherwise
E(X)
expectation of a random variable X
V (X)
variance of a random variable X
Cov(X,Y) covariance of random variables X and Y ; p. 71 and 267
¯x
sample mean 1
n

i≤n xi (with i running from 1 to n); p. 41
s2
sample variance
1
n−1

i≤n(xi −¯x)2; p. 41
si,j
sample covariance between ith and jth component; p. 71 and 270
X
d= Y
equality in distribution of X and Y ; p. 170

Part I
Modeling and Data Analysis

Chapter 1
Parametric Modeling
Chapter 1 is basic for the understanding of the main subjects treated in this book.
It is assumed that the given data are generated according to a random mechanism
that can be linked to some parametric statistical model.
In this chapter, the parametric models will be justiﬁed by means of mathe-
matical arguments, namely by limit theorems. In this manner, extreme value (EV)
and generalized Pareto (GP) models are introduced that are central for the sta-
tistical analysis of maxima or minima and of exceedances over a higher or lower
threshold, see Sections 1.3 and 1.4. Yet, we do not forget to mention other distri-
butions used in practice for the modeling of extremes. In Section 1.5 we especially
deal with fat and heavy–tailed distributions such as, e.g., log–normal and Student
distributions.
1.1
Applications of Extreme Value Analysis
At the beginning, the relevance of extreme value analysis to ﬂood frequency analy-
sis, environmental sciences, ﬁnance and insurance (these areas are especially dealt
with in Chapters 14–17) and other important topics is indicated. We start with
ﬂood frequency analysis primarily because of historical reasons.
Flood Frequency Analysis
The ultimate interest of ﬂood frequency analysis is the estimation of the T –year
ﬂood discharge (water level), which is the discharge once exceeded on the average in
a period of T years. Usually, the time span of 100 years is taken, yet the estimation
is carried out on the basis of ﬂood discharges for a shorter period. Consequences
of ﬂoods exceeding such a level can be disastrous. For example, 100–year ﬂood
levels were exceeded by the great American ﬂood of 1993 that caused widespread
devastations in the American Midwest (as pointed out in [13], Preface).

4
1. Parametric Modeling
Under standard conditions, the T –year level is a higher quantile of the distri-
bution of discharges. Thus, one is primarily interested in a parameter determined
by the upper tail of the distribution. Because ﬂood discharges of such a magnitude
were rarely observed or are not recorded at all, a parametric statistical modeling is
necessary to capture relevant tail probabilities. A careful checking of the validity
of the parametric modeling is essential.
If the statistical inference is based on annual maxima of discharges, then
our favorite model is the Gumbel or the uniﬁed extreme value model, yet we also
mention other models employed by hydrologists. Alternatively, the inference is
based on a partial duration series, which is the series of exceedances over a certain
higher threshold. In that case, our standard model for the ﬂood magnitudes is
the generalized Pareto model or certain submodels suggested by the well–known
index ﬂood procedure in regional ﬂood frequency analysis. We mention alternative
models. A modeling of the exceedance times must be included.
The regional ﬂood frequency provides the possibility to include information
from data recorded at nearby gauging stations, respectively, sites having a similar
characteristic.
The Actuary’s Interest in Extremes
In recent years, actuaries have become more and more aware of the potential risk
inherent in very large claim sizes due to catastrophic events. An insurer must com-
pensate the losses—due to the payments for claims of policy holders—by means of
an appropriate premium. Thereby, an actuary is ﬁrst of all interested in estimating
the net premium which is the mean of the total claim amount for an individual or
a portfolio (collection) of risks.
The total claim amount depends on the size of claims and the frequency of
claims within a given period. These two ingredients can be dealt with separately
from the statistical viewpoint.
Claim Sizes
Claim Number
Total Claim Amount
Net Premium
?
?
In conjunction with excess–of–loss (XL) reinsurance, when the reinsurer pays
for the excess of a higher ﬁxed limit for individual claim sizes, a parametric mod-
eling and estimation of the upper tail of the claim size distribution is of genuine
interest to evaluate the net premium.

1.1. Applications of Extreme Value Analysis
5
Our favorite model is again the generalized Pareto model, yet we also study
other distributions employed by actuaries (such as Benktander II and truncated
Weibull distributions).
For assessing the total premium, the actuary must also consider other param-
eters of the random losses besides the net premium. We will focus our attention
on the interdependence between the ruin probability of a portfolio within a ﬁnite
time horizon and the choice of an initial capital (reserve).
Extremes in Financial Time Series, Value at Risk
Insurance and ﬁnancial data can be investigated from the viewpoint of risk analysis
(as initiated in the books by H.L. Seal1 and H. B¨uhlmann2). Therefore, the insight
gained from insurance data can also be helpful for the understanding of ﬁnancial
risks (and vice versa).
Financial time series consist of daily or weekly reported speculative prices
of assets such as stocks, foreign currencies or commodities such as corn, cocoa,
coﬀee, sugar, etc. Risk management at a commercial bank is interested in guarding
against the risk of high losses due to the fall in prices of ﬁnancial assets held or
issued by the bank. It turns out that daily or weekly returns—relative diﬀerences
of consecutive prices or diﬀerences of log–prices (log–returns)—are the appropriate
quantities which must be investigated.
Most importantly, there is empirical evidence that distributions of returns
can possess fat or heavy tails so that a careful analysis of returns is required. In
this context, we deal with the upper tail of loss/proﬁt distributions and, especially,
with parameters, which summarize the potential risk to some extent, such as
• the Value–at–Risk (VaR) as the limit which is exceeded by the loss (measured
as a positive value) of a given speculative asset or a portfolio with a speciﬁed
low probability,
• the Capital–at–Risk (CaR) as the amount which may be invested with the
possible consequence that the loss exceeds a given limit with a speciﬁed low
probability.
These two concepts are closely related to each other: we ﬁx either the invested
capital (market value) or the limit and, then, compute the other variable.
A special feature of return series is the alternation between periods of tran-
quility and volatility. Therefore, the VaR, which is essentially the q–quantile of the
distribution of log–returns, should vary in time.
The following illustration3 concerns the log–returns of the Standard & Poors
1Seal, H.L. (1969). Stochastic Theory of a Risk Business. Wiley, New York.
2B¨uhlmann, H. (1970). Mathematical Methods in Risk Theory. Springer, Berlin.
3Illustrations are produced with the Frame Size (Print/EPS) facility in the local menus
of the plot windows of Xtremes.

6
1. Parametric Modeling
500 market index (stored in fm–poors.dat4).
time
S&P500 log-returns
1962
1968
1974
1980
1986
0.0
0.02
0.04
Fig. 1.1. Scatterplot of the log–returns (with changed signs) of the S&P500 index from
July 1962 to Dec 1987 and a moving sample 95%–quantile based on the preceding 90
days.
One recognizes the periods of higher volatility which are reﬂected by periods
of higher sample q–quantiles and, thus, a higher VaR.
Material Sciences and Other Important Areas
In most parts of the book, we do not focus our attention on special areas. Such a
theoretical approach is justiﬁed by the fact that, to some extent, the basic statisti-
cal questions have a common feature for all the potential applications. We brieﬂy
mention further questions for which extreme value analysis is of interest:
• (Corrosion Analysis.) Pitting corrosion can lead to the failure of metal struc-
tures such as tanks and tubes. Extreme value analysis becomes applicable
because pits of a larger depth are of primary interest. In analogy to the
T –year discharge in hydrology one may estimate the T –unit depths which
is the level once exceeded (within a given time span) on the average, if T
units or an area of size T are exposed to corrosion. The role of time in other
applications (e.g., in hydrology) is now played by the location of the pits,
see Section 18.1.
4Data sets *.dat can be found in the subdirectory xtremes\dat, if xtremes is taken
as the working directory of Xtremes.

1.2. Observing Exceedances and Maxima
7
• (Telecommunication.) There exist several research papers about teletraﬃc
data in conjunction with a discussion of the concepts of aggregation and
self–similarity, see page 169.
• (Strength of Material.) A sheet of metal breaks at its weakest point so that a
minimum strength determines the quality of the entire sheet. Related topics
are strength of bundles of threads and ﬁrst failure of equipments with many
components, see [20], pages 188–196.
• (Longevity of Human Life.) The insight gained from experiments led to state-
ments such as “the studies are consistently failing to show any evidence for
pre–programmed limit to human life span” which found their way to daily
news papers5. We will study such questions from the viewpoint of extreme
value analysis, see Chapter 19.
• (Environmental Sciences.) Higher concentration of certain ecological quanti-
ties, like concentration of ozone, acid rain or sulfur dioxide (SO2) in the air,
are of greater interest due to the negative response on humans and, generally,
on the biological system, see Chapter 15.
Other important areas of applications include geology, meteorology and seis-
mic risk analysis. It is self–evident that our list is by no means complete.
The T–Year Level and Related Parameters
The T –year level (e.g., for discharges in ﬂood frequency analysis) plays an impor-
tant role in our analysis. This parameter is dealt with, for example, in Sections 1.2
and 14.2. The same methodology can be applied to the T –unit depths in corrosion
analysis, see Section 18.1. In a similar way, we deﬁne a T –year initial reserve in
insurance (page 430) and a T –day capital in ﬁnance (page 390). These parameters
are connected to certain small or large quantiles and must be estimated by the
methods presented in this book.
1.2
Observing Exceedances and Maxima
It is understood that the reader is familiar with the concepts of a distribution
function (df) F, a density f as the derivative of the df, and independent and
identically distributed (iid) random variables X1, . . . , Xn. The df of a real–valued
random variable X is given by F(x) = P{X ≤x}. The expectation and variance
of a random variable X are denoted by E(X) and V (X). These quantities are the
mean and the variance of the distribution of X.
5Kolata, G. New views on life spans alter forecasts on elderly. The New York Times,
Nov. 16. 1992.

8
1. Parametric Modeling
Number of Exceedances over a Threshold
One method of extracting upper extremes from a set of data x1, . . . , xn is to take
the exceedances over a predetermined, high threshold u. Exceedances yj over u
(peaks–over–threshold (pot)) are those xi with xi > u taken in the original order
of their outcome or in any other order. The values yj −u are the excesses over u.
Subsequently, the number of exceedances over u will be denoted by k or,
occasionally, by K to emphasize the randomness of this number. In many cases,
the values below u are not recorded or cannot be observed by the statistician.
Given random variables X1, . . . , Xn, we may write K = 
i≤n I(Xi > u), where
I(Xi > u) is an indicator function with I(Xi > u) = 1 if Xi > u holds and zero,
otherwise. If the Xi are iid random variables with common df F, then
P{K = k} =
n
k

pk(1 −p)n−k =: Bn,p{k},
k = 0, . . . , n,
(1.1)
where Bn,p is the well–known binomial distribution with parameters n and p =
1 −F(u). The mean number of exceedances over u is
Ψn,F(u) = np = n(1 −F(u)),
(1.2)
which deﬁnes a decreasing mean value function.
The Poisson Approximation of Binomial Distributions
We give an outline of the well–known fact that the binomial distribution Bn,p can
be replaced by a Poisson distribution and, thus, the number K of exceedances may
be regarded as a Poisson random variable.
Let X1, . . . , Xn be iid random variables attaining the values 1 and 0 with
probability p and, respectively, 1 −p. In conjunction with the number of ex-
ceedances, these random variables are of the special form I(Xi ≥u). We have
P{X1 + · · · + Xn = k} = Bn,p{k},
k = 0, . . . , n.
This formula can be rephrased by saying that success occurs with probability p
and failure with probability 1 −p, then the total number of successes is governed
by Bn,p. Note that np is the expected number of successes, and np(1 −p) is the
variance.
Poisson distributions can be ﬁtted to binomial distributions under certain
conditions. If np(n) →λ as n →∞, then
Bn,p(n){k} →Pλ{k},
n →∞,
(1.3)
where
Pλ{k} = λk
k! e−λ,
k = 0, 1, 2, 3, . . .

1.2. Observing Exceedances and Maxima
9
deﬁnes the Poisson distribution with parameter λ.
In Fig. 1.2, the binomial and Poisson histograms Bn,p{k} and Pnp{k}, k =
0, . . . , 20, for the parameters n = 40 and p = 0.25 are displayed, see also the Demos
A.1 and A.2.
10
20
0.1
Fig. 1.2. Fitting the Poisson distribu-
tion P10 (dark) to a binomial distribu-
tion B(40, .25) (light).
If X is a Poisson random variable with parameter λ, then E(X) = λ. In
addition, the variance of X is V (X) = E(X −λ)2 = λ. We see that the expectation
and the variance of a Poisson random variable are equal.
It is well known that the inequality
|Bn,p(A) −Pnp(A)| ≤p
(1.4)
holds for each set A of nonnegative integers. Notice that the right–hand side of
the inequality does not depend on n. Thus, a Poisson distribution can be ﬁtted to
a binomial distribution whenever p is suﬃciently small.
The iid condition, under which the Poisson approximation was formulated,
can be weakened considerably, yet we do not go into details6. In Section 3.4,
we also introduce mixtures of Poisson distributions as, e.g., negative binomial
distributions.
Maxima
Assume that the given data yi are maxima, that is,
yi = max{xi,1, . . . , xi,m},
i = 1, . . . , n,
(1.5)
where the xi,j may not be observable.
If the xi,j in (1.5) can be observed by the statistician, then taking maxima
out of blocks is another possibility of extracting upper extreme values from a set of
6We refer to Haight, F.A. (1967). Handbook of the Poisson Distribution. Wiley, New
York; for a more recent monograph see Barbour, A.D., Holst, L. and Janson, S. (1992).
Poisson Approximation. Oxford Studies in Probability. Oxford University Press.

10
1. Parametric Modeling
data (besides taking exceedances). This method is called annual maxima, blocks or
Gumbel method. For example, one takes the maximum values—such as maximum
temperatures, water discharges, wind speeds, ozone concentrations, etc.—within a
month or a year.
For iid random variables X1, . . . , Xm with common df F, one may easily
compute the df of maxima; we have
P

max
i≤m Xi ≤x

= P{X1 ≤x, . . . , Xm ≤x} = F m(x).
(1.6)
Thus, the yi’s in (1.5) are governed by F m if the xi,j’s are governed by F.
If the iid condition fails, then a df of the form F m may still be an accurate
approximation of the actual df of the maximum. For independent, yet heteroge-
neous random variables Xj with df Fj, (1.6) holds with F m replaced by 
j≤m Fj.
A df F m can be ﬁtted to 
j≤m Fj if the deviations of the Fj from each other can
be neglected.
Likewise, if there is a slight dependence in the data, a df of the form F m may
still serve as an approximation of the actual df of the maximum. It is plausible to
employ this approach in more complex situations as well. In Section 1.3, we go one
step further and replace F m by an extreme value (EV) df G. The EV distributions
constitute a parametric model which will be introduced in Section 1.3.
We mention a special EV df, namely the Gumbel df G0(x) = exp(−e−x),
which plays a central role within the extreme value analysis.
Example 1.2.1. (Annual Wind–Speed Maxima at Vancouver.) We present annual max-
ima of daily measurements of wind speeds taken from [21], page 122 (stored in the ﬁle
em–cwind.dat).
Table 1.1. Annual wind–speed maxima in km/h from 1947 to 1984 at Vancouver.
year speed year speed year speed year speed year speed year speed year speed
47
79.5
53
64.8
59
64.8
65
61.0
71
70.3
77
48.1
83
51.8
48
68.4
54
59.2
60
88.8
66
51.8
72
68.4
78
53.6
84
48.1
49
74.0
55
79.5
61
88.8
67
62.9
73
55.5
79
55.5
50
59.2
56
62.9
62
75.8
68
64.8
74
64.8
80
62.9
51
74.0
57
59.2
63
68.4
69
61.0
75
77.7
81
61.0
52
64.8
58
68.2
64
68.4
70
61.0
76
57.3
82
61.0
Such data can be visualized by means of nonparametric tools such as, e.g., kernel
densities (see Section 2.1). One may try to ﬁt a parametric density—e.g., a Gumbel
density with location and scale parameters µ and σ—to the sample density.
In Fig. 1.3, one can recognize a reasonable ﬁt of a Gumbel density to the data
(represented by a sample density). For a continuation, see Example 2.1.3.

1.2. Observing Exceedances and Maxima
11
wind-speed in km/h
50
75
100
0.0
0.02
0.04
Fig. 1.3. Representing the wind–
speed data by a ﬁtted Gumbel
density (µ = 60.3 and σ = 8.3)
and a kernel density.
Minima
Generally, results for minima can be deduced from corresponding results for max-
ima by writing
min
i≤n Xi = −max
i≤n (−Xi).
In conjunction with minima, it can be useful to present results in terms of
the survivor function
F = 1 −F.
(1.7)
Utilizing the same arguments as in (1.6), one may compute the survivor function
of the minimum of iid random variables Xi with common df F. We have
P

min
i≤m Xi > x

= (1 −F(x))m = F
m(x).
(1.8)
Therefore, the df of the minimum is
P

min
i≤m Xi ≤x

= 1 −(1 −F(x))m.
(1.9)
Actual dfs of minima will be replaced by converse EV dfs in Section 1.3 (see page
22). As an example of a converse EV df, we mention the converse Gumbel df
	G0(x) = 1 −G0(−x) = 1 −exp(−ex),
(1.10)
which is also called Gompertz df.
Magnitudes of Exceedances and Upper Order Statistics
Let the xi be governed again by a df F and let the threshold u be smaller than the
right endpoint ω(F) := sup{x : F(x) < 1} of the support of F. We speak of a high

12
1. Parametric Modeling
threshold u, if u is close to the right endpoint ω(F). In that case, p = 1 −F(u)
is small and the number k of exceedances may be regarded as a Poisson random
variable. Subsequently, we deal with the magnitudes (sizes) of the exceedances.
Exceedances occur conditioned on the event that an observation is larger
than the threshold u. The pertaining conditional df F [u] is called exceedance df at
u. If X denotes a random variable with df F, then
F [u](x)
=
P(X ≤x|X > u)
=
P{X ≤x, X > u}/P{X > u}
=
F(x) −F(u)
1 −F(u)
,
x ≥u.
(1.11)
In terms of survivor functions we may write
F [u](x) = F(x)/F(u),
x ≥u.
(1.12)
One should keep in mind that the left endpoint α(F [u]) = inf{x : F [u](x) > 0}
of F [u] is equal to u. The relationship between exceedances and the exceedance df
will be examined more closely in (2.4). Moreover, generalized Pareto (GP) dfs will
be ﬁtted to exceedance dfs F [u] in Section 1.4.
Another closely related approach of extracting extreme values from the data
is to take the k largest values xn−k+1:n ≤· · · ≤xn:n of the xi, where the number
k is predetermined. Notice that xn:n is the maximum. Within this approach, the
(k + 1)th largest observation xn−k:n may be regarded as a random threshold.
The T–Year Level
One of the major objectives of extreme value analysis is the estimation of the
T –year level. We introduce the T –year level u(T ) as the threshold u(T ) such that
the mean number of exceedances over u(T ) within the time span of length T is
equal to 1. In this context, it is understood that for each year (or any other period
like day, month or season), there is one observation.
Let X1, X2, . . . , XT be random variables with common df F. Then, u(T ) is
the solution to the equation
E
 
i≤T
I(Xi ≥u)

= 1.
(1.13)
Apparently,
u(T ) = F −1(1 −1/T )
(1.14)
which is the (1 −1/T )–quantile of the df F (also see Section 1.6). We have
P{X1 > u(T )} = 1 −F(u(T )) = 1
T
and, thus, the T –year level u(T ) is exceeded by the observation in the given year
(period) with probability 1/T .

1.2. Observing Exceedances and Maxima
13
Exceedance Times and the T–Year Return Level
The primary aim of the following lines is to outline why the T –year level u(T ) =
F −1(1 −1/T ) is also called T –year return level. In this context, we also get a
diﬀerent interpretation of the T –year level.
Besides the number and sizes of exceedances, one is interested in the times
at which the exceedances occur. Given x1, . . . , xn, let xi(1), . . . , xi(k) be the ex-
ceedances over a predetermined threshold u. Let i(1) ≤i(2) ≤· · · ≤i(k) be the
ordered exceedance times.
In conjunction with exceedance times, we are primarily interested in the fu-
ture occurrence of the next exceedance at a certain higher threshold u for an inﬁnite
time horizon. Therefore, one must consider an inﬁnite sequence X1, X2, X3, . . . of
random variables with common df F. The ﬁrst exceedance time at u is
τ1 = min{m : Xm > u},
(1.15)
whereby it is understood that the threshold u is smaller than the right endpoint
ω(F) of F. We also deal with the random ordered exceedance times
τ1 < τ2 < τ3 < · · ·
at the threshold u. Occasionally, we also write τr,u instead of τr for the rth ex-
ceedance time to emphasize the dependence on the threshold u. The exceedance
times may be deﬁned recursively by
τr = min{m > τr−1 : Xm > u},
r > 1.
(1.16)
The sequence of exceedance times has independent, geometrically distributed
increments called return periods or interarrival times, see (1.17) and (1.21). Note
that
P{τ1 = k}
=
P{X1 ≤u, . . . , Xk−1 ≤u, Xk > u}
=
p(1 −p)k−1,
k = 1, 2, 3, . . . ,
(1.17)
with p = 1 −F(u), where the latter equality holds for iid random variables. Thus,
the ﬁrst exceedance time τ1 at u is distributed according to a geometric distribution
with parameter p. Consequently, the mean of the ﬁrst exceedance time—also called
mean return period—at u is E(τ1,u) = 1/p.
A threshold u such that the mean ﬁrst exceedance time is equal to T is the
T –year return level. Thus, the T –year return level is the solution to the equation
E(τ1,u) =
1
1 −F(u) = T.
(1.18)
This equation has the same solution as equation (1.13), namely
u(T ) = F −1(1 −1/T ).
(1.19)

14
1. Parametric Modeling
This means that the T –year level in (1.14) and the T –year return level coincide
under the present conditions.
The ﬁrst exceedance time τ1,u(T ) at the T –year level u(T ) is a geomet-
ric random variable with parameter 1/T . By the deﬁnition of u(T ), we have
E(τ1,u(T )) = T . In addition, the variance is V (τ1,u(T )) = (T −1)T.
A justiﬁcation for the names “mean return period” and “return level” is
provided by the fact that the return periods
τ1, τ2 −τ1, τ3 −τ2, . . .
(1.20)
are iid random variables. This yields that the mean return periods E(τr+1−τr) are
equal to the mean ﬁrst exceedance time E(τ1). Also, the threshold u such that the
mean return period is equal to T is the T –year return level in (1.14) and (1.19).
The iid property can be veriﬁed easily. For positive integers k(1), . . . , k(r),
we have
P{τ1 = k(1), τ2 −τ1 = k(2), . . . , τr −τr−1 = k(r)}
=
P{τ1 = k(1), τ2 = k(1) + k(2), . . . , τr = k(1) + · · · + k(r)}
=
P{X1 ≤u, . . . , Xk(1)−1 ≤u, Xk(1) > u,
Xk(1)+1 ≤u, . . . , Xk(1)+···+k(r) > u}
=

j≤r
P{τ1 = k(j)}.
(1.21)
Generally, one may verify that
P{τr = k} =
k −1
r −1

pr(1 −p)k−r
(1.22)
for k = r, r + 1, . . . and, hence, the rth exceedance time is a negative binomial
random variable with parameters r and p = 1 −F(u) shifted to the right by the
amount r.
1.3
Modeling by Extreme Value Distributions
The actual df of a maximum will be replaced by an extreme value (EV) df. First, we
give a list of the standard EV dfs Gi,α and Gγ within three submodels and a uniﬁed
model, where α and γ are certain shape parameters. The pertaining densities are
denoted by gi,α and gγ. One must include location and scale parameters µ and σ
to build the full statistical models.
At the end of this section, we will also mention corresponding questions
concerning minima.

1.3. Modeling by Extreme Value Distributions
15
The α–Parameterization
Here is a list of the three diﬀerent submodels by writing down the standard EV
dfs for the diﬀerent shape parameters α:
Gumbel (EV0):
G0(x) = exp(−e−x),
for all x;
Fr´echet (EV1), α > 0:
G1,α(x) = exp(−x−α),
x ≥0;
Weibull (EV2), α < 0:
G2,α(x) = exp(−(−x)−α),
x ≤0.
The Fr´echet df G1,α is equal to zero if x < 0; the Weibull df G2,α is equal
to one if x > 0. We see that each real parameter α determines a standard EV
distribution. Notice that G2,−1 is the exponential df on the negative half–line.
Occasionally, the standard Gumbel df G0 is also denoted by G0,α.
Warning! Our parameterization for Weibull dfs diﬀers from the stan-
dard one used in statistical literature, where Weibull dfs with positive
shape parameters are taken.
We also note the pertaining densities g = G
′ of the standard EV dfs:
Gumbel (EV0):
g0(x) = G0(x)e−x,
for all x;
Fr´echet (EV1), α > 0:
g1,α(x) = αG1,α(x)x−(1+α),
x ≥0;
Weibull (EV2), α < 0:
g2,α(x) = |α|G2,α(x)(−x)−(1+α),
x ≤0.
EV densities are unimodal. Remember that a distribution and the pertaining
density f are called unimodal if the density is non–decreasing left of some point u
and non–increasing right of u. Then, u is called a mode. Fr´echet densities and the
Gumbel density are skewed to the right.
Weibull dfs provide a very rich family of unimodal dfs. Weibull densities
• are skewed to the left if α is larger than −3.6 and have a pole at zero if
α > −1,
• look symmetrical if α is close to −3.6,
• are skewed to the right—such as Fr´echet and Gumbel densities—if α is
smaller than −3.6.

16
1. Parametric Modeling
Location and Scale Parameters
If a random variable X has the df F, then µ+σX has the df Fµ,σ(x) = F((x−µ)/σ),
where µ and σ > 0 are the location and scale parameters. Full EV models are
obtained by adding location and scale parameters µ and σ > 0. By
G0,µ,σ(x) = exp(−e−(x−µ)/σ)
and
Gi,α,µ,σ(x) = Gi,α
x −µ
σ

,
i = 1, 2,
one obtains Gumbel, Fr´echet and Weibull dfs with location and scale parameters
µ and σ.
We see that the location parameter µ is the left endpoint of the Fr´echet df
G1,α,µ,σ and the right endpoint of the Weibull df G2,α,µ,σ. Furthermore,
g0,µ,σ(x) = 1
σ e−(x−µ)/σ exp(−e−(x−µ)/σ)
and
gi,α,µ,σ(x) = 1
σ gi,α
x −µ
σ

,
i = 1, 2,
are the densities of the Gumbel df G0,µ,σ and the Fr´echet and Weibull dfs Gi,α,µ,σ.
The γ–Parameterization
Up to now, the three diﬀerent EV models are separated from each other. Yet a
visualization of these dfs—or of the pertaining densities—shows that Fr´echet and
Weibull dfs attain the shape of a Gumbel df when the shape parameter α goes to
∞and, respectively, to −∞.
By taking the reparameterization γ = 1/α—due to von Mises7 (also fre-
quently attributed to Jenkinson8)—of EV dfs Gi,α one obtains a continuous, uni-
ﬁed model. In this representation, the Gumbel df again has the parameter γ equal
to zero. The standard versions in the γ–parameterization are deﬁned such that
Gγ(x) →G0(x),
γ →0.
(1.23)
This is achieved by employing appropriate location and scale parameters in
addition to the reparameterization γ = 1/α. Thus, EV dfs with certain location
7 Mises von, R. (1936). La distribution de la plus grande de n valeurs. Rev. Math.
Union Interbalcanique 1, 141–160. Reproduced in Selected Papers of Richard von Mises,
Amer. Math. Soc. 2 (1964), 271–294.
8 Jenkinson, A.F. (1955). The frequency distribution of annual maximum (or mini-
mum) values of meteorological elements. Quart. J. Roy. Meteorol. Soc. 81, 158–171.

1.3. Modeling by Extreme Value Distributions
17
and scale parameters unequal to zero and one are taken as the standard EV dfs
in the γ–parameterization if γ ̸= 0. We have
G0(x) = exp(−e−x),
for all x,
(1.24)
and
Gγ(x) = exp

−(1 + γx)−1/γ
,
1 + γx > 0,
γ ̸= 0.
(1.25)
By applying the well–known formula (1 + γx)1/γ →exp(x) as γ →0, one
may verify that (1.23) holds. Notice that
• G0 is the standard Gumbel df;
• Gγ is a Fr´echet df if γ > 0,
• Gγ is a Weibull df if γ < 0.
The right endpoint of a Weibull df Gγ is equal to 1/|γ| and the left endpoint of
the Fr´echet df Gγ is equal to −1/γ. For γ = 1/α ̸= 0, one obtains the representation
Gγ = Gi,α,−α,|α|
(1.26)
with i = 1 if γ > 0 and i = 2 if γ < 0. The pertaining densities are
g0(x) = G0(x)e−x,
for all x,
and
gγ(x) = Gγ(x)(1 + γx)−(1+1/γ),
1 + γx > 0,
γ ̸= 0.
Also, check that gγ(x) →g0(x) as γ →0. Some EV densities around the
Gumbel density are displayed in Fig. 1.4.
-2
2
4
0.2
-2
2
4
0.2
0.4
Fig. 1.4. (left.) Gumbel density (dashed) and two Fr´echet (solid) densities for parameters
γ = .28, .56. (right.) Gumbel density (dashed) and two Weibull (solid) densities for
parameters γ = −.28, −.56.
On the right–hand side, we see the Weibull density with shape parameter γ =
−.28 (that is, α = −3.6) that looks symmetrical and can hardly be distinguished
from a normal (Gaussian) density. Recall that a Weibull density gγ has the ﬁnite
right endpoint 1/|γ|. We have ω(G−.28) = 3.6 and ω(G−.56) = 1.8.

18
1. Parametric Modeling
Limiting Distributions of Maxima
Recall from (1.6) that maxi≤n Xi has the df F n, if X1, . . . , Xn are iid random
variables with common df F. The choice of Fr´echet, Weibull and Gumbel dfs for
modeling dfs of maxima becomes plausible from the subsequent remarks:
(Fisher–Tippett9.) If F n(bn + anx) has a non–degenerate limiting df as n →∞
for constants bn and an > 0, then
|F n(x) −G ((x −µn)/σn))| →0,
n →∞,
(1.27)
for some EV df G ∈{G0, G1,α, G2,α} or G = Gγ and location and scale parameters
µn and σn > 0.
If (1.27) holds, then F belongs to the max–domain of attraction of the EV
df G; in short, F ∈D(G). For lists of suitable constants bn and an > 0, we refer to
(1.31) and (2.33), whereby the constants in (2.33) are applicable if a certain von
Mises condition is satisﬁed.
Additionally, convergence also holds in (1.27) for the pertaining densities if
F is one of the usual continuous textbook dfs. The illustrations in Fig. 1.5 concern
the Gumbel approximation of the maximum of exponential random variables for
a sample of size n = 30.
2
4
6
8
0.2
0.4
2
4
6
8
-0.02
0.02
Fig. 1.5. (left.) Density f of df

1 −e−x30 (solid) and Gumbel density g0,µ,σ (dotted)
for parameters µ = 3.4 and σ = .94. (right.) Plot of the diﬀerence f −g0,µ,σ.
It can be easily veriﬁed that (1.27) holds if, and only if,
nF(bn + anx) →−log G(x),
n →∞,
(1.28)
for every x in the support of G, where again F = 1 −F is the survivor func-
tion. According (1.2), this relation concerns the asymptotic behavior of the mean
number of exceedances over the threshold u(n) = µn + σnx.
9Fisher, R.A. and Tippett, L.H.C. (1928). Limiting forms of the frequency distribution
of the largest or smallest member of a sample. Proc. Camb. Phil. Soc. 24, 180–190.

1.3. Modeling by Extreme Value Distributions
19
More reﬁned necessary and suﬃcient conditions for (1.27) are due to Gne-
denko and de Haan, see, e.g., [16], Theorem 2.1.1 and Section 6.510. A discussion
about the accuracy of such approximations may also be found in Section 6.5. If
condition (1.27) does not hold or it merely holds with a low rate, then the situa-
tion is classiﬁed as the “horror case” by some authors. We simply suggest to use
a diﬀerent modeling.
The Max–Stability
EV dfs are characterized by their max–stability. A df F is max–stable if
F n(bn + anx) = F(x)
(1.30)
for a suitable choice of constants bn and an > 0. Thus, the standardized maximum
under the df F is distributed according to F.
We give a list of these constants when the max–stable df is one of the standard
EV dfs.
Gumbel
G0:
bn = log n,
an = 1,
Fr´echet
G1,α, α > 0:
bn = 0,
an = n1/α,
Weibull
G2,α, α < 0:
bn = 0,
an = n1/α.
(1.31)
For example, Gn
2,−1(x/n) = G2,−1(x) for the exponential df G2,−1 on the
negative half–line. The constants in (1.31) will also be used in Section 6.5 to show
the convergence of F n(bn + anx) to an EV df for a larger class of dfs F.
Moments and Modes of Extreme Value Distributions
The jth moment E(Xj) of a Fr´echet or a Weibull random variable X can be
written in terms of the gamma function
Γ(λ) =
 ∞
0
xλ−1e−x dx,
λ > 0.
(1.32)
Recall that Γ(λ + 1) = λΓ(λ) and Γ(1) = 1. If X has the df G and density g = G
′,
then the jth moment can be written as
mj,G := E(Xj) =

xj dG(x) =

xjg(x) dx.
(1.33)
10For example, a df F belongs to the max–domain of attraction of the EV df Gγ with
γ > 0 if, and only if, ω(F) = ∞and
F(tx)/F(t) →t→∞x−1/γ,
x > 0.
(1.29)
The standardizing constants may be chosen as bn = 0 and an = F −1(1 −1/n).

20
1. Parametric Modeling
Occasionally, the index G will be omitted. We also write mG = m1,G for the mean
of G. By applying the substitution rule, one obtains
mj,G1,α = Γ(1 −j/α),
if α > j,
and
mj,G2,α = (−1)jΓ(1 −j/α).
The jth moment of the Fr´echet df G1,α is inﬁnite if α ≤j. This is due to the
fact that the upper tail of the Fr´echet density g1,α(x) is approximately equal to
αx−(1+α). In that context, one speaks of heavy upper tails of Fr´echet distributions.
For the special case j = 1, one obtains the mean values of Fr´echet and Weibull
dfs. We have
mG1,α = Γ(1 −1/α),
if α > 1,
and
mG2,α = −Γ(1 −1/α).
Especially the ﬁrst moment is inﬁnite whenever 0 < α ≤1. In contrast to the
usual textbook analysis, we are also interested in such distributions.
If the actual distribution is of that type, then a single extreme observation
may dominate all the other observations and destroy the performance of a statistic
connected to the mean. This is one of the reasons why we are also interested in
other functional parameters of GP dfs such as q–quantiles with the median as a
special case, cf. Section 1.6.
For centered moments one obtains in analogy to (1.33),
E

(X −E(X))j
=

(x −m1)j dG(x) =

(x −m1)jg(x) dx,
where m1 = m1,G. For j = 2 one obtains the variance.
The variances varGi,α of Fr´echet and Weibull dfs can be deduced easily, be-
cause var = m2 −m2
1. We have
varGi,α = Γ(1 −2/α) −Γ2(1 −1/α),
if 1/α < 1/2.
In the γ–parameterization, the mean and variance of an EV df is given by
mGγ = (Γ(1 −γ) −1)/γ,
if γ < 1,
and
varGγ =

Γ(1 −2γ) −Γ2(1 −γ)

/γ2,
if γ < 1/2.
The case γ = 0 is included in the latter formulas by considering limits with γ
tending to zero. We have
mG0 = lim
γ→0 mGγ =
 ∞
0
(−log x)e−x dx = λ,
(1.34)

1.3. Modeling by Extreme Value Distributions
21
where λ = 0.577216 . . . is Euler’s constant. Moreover,
varG0 = lim
γ→0 varGγ = π2/6,
(1.35)
and
m2,G0 = λ2 + π2/6.
Normalizing centered moments by the standard deviation var1/2 one obtains
E

X −EX
var1/2
G
j
= E(X −EX)j
varj/2
G
.
(1.36)
For j = 1, 2, the normalized, centered moments are equal to zero and one, respec-
tively.
For j = 3, one obtains the skewness coeﬃcient of G
skewG = E(X −EX)3
var3/2
G
,
(1.37)
if α > 3 or γ < 1/3, and X is a random variable with df G. Note that skew =
(m3 −3m1m2 +2m3
1)/σ3. The 4th normalized, centered moment is the kurtosis, cf.
page 31. Check that the normalized, centered moments are independent of location
and scale parameters.
In Figure 1.6, the skewness coeﬃcient of EV dfs Gγ is plotted against γ.
The skewness coeﬃcient is equal to zero at γ0 = −.2776 . . . in accordance with
the remark on page 15 that the density of the Weibull df with shape parameter
γ = −.28 (or α = −3.6) looks symmetrical. We have skewGγ > 0, if γ > γ0, and
skewGγ < 0, if γ < γ0. Furthermore, skewG0 = 1.1395 . . ..
shape parameter
-0.4
-0.2
0.2
5
10
Fig. 1.6. Plotting the skewness
coeﬃcients skewGγ against γ <
1/3.
Finally, we deal with the unique modes, cf. page 15, of EV dfs G which will
be denoted by modG. We have
modG1,α = (α/(1 + α))1/α

22
1. Parametric Modeling
and, thus, modG1,α →0 as α →0 and modG1,α →1 as α →∞. In addition,
modG2,α = 0, if −1 ≤α < 0, and
modG2,α = −(α/(1 + α))1/α,
α < −1.
Thus, modG2,α →0 as α →−1 and modG2,α →−1 as α →−∞. In the γ–
parameterization there is
modGγ =

(1 + γ)−γ −1

/γ,
γ ̸= 0,
and the mode of the standard Gumbel distribution G0 is zero.
Limiting Distributions of Minima
There is a one–to–one relationship between limiting dfs of maxima and minima. If
P

max
i≤n (−Xi) ≤bn + anz

→G(z),
n →∞,
then
P

min
i≤n Xi ≤dn + cnz

→1 −G(−z),
n →∞,
with cn = an and dn = −bn (and vice versa). This yields that the limiting dfs of
sample minima are the converse Gumbel, Fr´echet and Weibull dfs
	Gi,α(x) = 1 −Gi,α(−x)
(1.38)
in the α–parameterization, and
	Gγ(x) = 1 −Gγ(−x)
(1.39)
in the γ–parameterization. Moreover, if Gi,α,µ,σ or Gγ,µ,σ is the df of −Yi, then
	Gi,α,−µ,σ or 	Gγ,−µ,σ is the df of Yi.
The limiting dfs 	G of minima are also called extreme value dfs or, if necessary,
converse extreme value dfs. Here are three special cases:
• the converse Gumbel df 	G0 is also called Gompertz df; this is the df that
satisﬁes the famous Gompertz law, see page 54;
• 	G2,−1 is the exponential df on the positive half–line,
• 	G2,−2 is the Rayleigh df that is also of interest in other statistical appli-
cations. Note the following relationship: if the areas of random circles are
exponentially distributed, then the diameters have a Rayleigh df.

1.4. Modeling by Generalized Pareto Distributions
23
The Min–Stability
The limiting dfs of minima—these are the converse EV dfs—are characterized by
the min–stability. A df F is min–stable if
P

min
i≤n Xi ≤dn + cnx

= 1 −(1 −F(dn + cnx))n = F(x)
(1.40)
for a certain choice of constants dn and cn > 0, where X1, . . . , Xn are iid random
variables with common df F.
The min–stability can be expressed in terms of the survivor function F =
1 −F of F. We have
P

min
i≤n Xi > dn + cnx

= F
n(dn + cnx) = F(x).
(1.41)
Some Historical Remarks
We compare the symbols employed for EV dfs in this textbook with others also
used in relevant statistical publications:
Gumbel
EV0
G0
Type I
Λ
Fr´echet
EV1
G1,α
Type II
Φα
Weibull
EV2
G2,α
Type III
Ψα
In this book—as in many statistical textbooks—the normal (Gaussian) df
is denoted by Φ and, therefore, the same symbol cannot be used for the Fr´echet
df. By combining the numbering of EV dfs in [20] with the letter G used in [39]
(presumably, in honor of B.V. Gnedenko), one obtains the symbol Gi,α for EV dfs
also taken in [42] and [16]. In contrast to the latter representation, the Gumbel
df also gets the parameter α = 0 in the present α–parameterization. The books
[20] by J. Galambos and [39] by M.R. Leadbetter, G. Lindgren and H. Rootz´en,
together with the booklet [25] by L. de Haan, laid the probabilistic foundations of
modern extreme value theory. We refer to [20] and [42] for a more detailed account
of the history.
1.4
Modeling by
Generalized Pareto Distributions
The standard generalized Pareto (GP) dfs Wi,α and Wγ are the adequate para-
metric dfs for exceedances, cf. also (1.11). The pertaining densities are denoted by
wi,α and wγ. There is the simple analytical relationship
W(x) = 1 + log G(x),
if
log G(x) > −1,
between GP dfs W and EV dfs G.

24
1. Parametric Modeling
The α–Parameterization
First we introduce a representation for GP dfs within three submodels correspond-
ing to that for EV dfs.
Exponential (GP0):
W0(x) = 1 −e−x,
x ≥0,
Pareto (GP1), α > 0:
W1,α(x) = 1 −x−α,
x ≥1,
Beta (GP2), α < 0:
W2,α(x) = 1 −(−x)−α,
−1 ≤x ≤0.
The exponential df W0 is equal to zero for x < 0; the Pareto dfs W1,α are
equal to zero for x < 1; the beta dfs W2,α are equal to zero for x < −1 and equal
to 1 for x > 0.
Note that W2,−1 is the uniform df on the interval [−1, 0]. One should be
aware that the dfs W2,α constitute a subclass of the usual family of beta dfs.
Subsequently, when we speak of beta dfs only dfs W2,α are addressed (except of
those in Section 4.3, where we mention the full beta family).
Warning! Our parameterization for beta dfs diﬀers from the stan-
dard one used in the statistical literature, where beta dfs with positive
shape parameters are taken.
Once more, we also note the pertaining densities w = W
′:
Exponential (GP0):
w0(x) = e−x,
x ≥0,
Pareto (GP1), α > 0:
w1,α(x) = αx−(1+α),
x ≥1,
Beta (GP2), α < 0:
w2,α(x) = |α|(−x)−(1+α),
−1 ≤x < 0.
The Pareto and exponential densities are decreasing on their supports. This
property is shared by beta densities with shape parameter α < −1. For α = −1,
one gets the uniform density on [−1, 0] as mentioned above. Finally, the beta
densities with shape parameter α > −1 are increasing, having a pole at zero.
We see that a GP density wi,α possesses an upper tail similar (asymptotically
equivalent) to that of an EV density gi,α. Roughly speaking, the EV density is a
GP density tied down near the left endpoint.
Again, one must add location and scale parameters µ and σ > 0 in order to
obtain the full statistical families of GP dfs. Notice that the left endpoint of the
Pareto df W1,α,µ,σ(x) = W1,α((x −µ)/σ) is equal to µ + σ.

1.4. Modeling by Generalized Pareto Distributions
25
The γ–Parameterization
The uniﬁed GP model is obtained by applying a representation that corresponds
to that for EV dfs in Section 1.3. The standard versions are deﬁned such that
Wγ(x) →W0(x),
γ →0.
(1.42)
By putting γ = 1/α and choosing location and scale parameters as in Section 1.3,
the GP dfs in the γ–parameterization are W0(x) = 1 −e−x for x > 0, and
Wγ(x) = 1 −(1 + γx)−1/γ
for
⎧
⎨
⎩
0 < x,
γ > 0;
if
0 < x < 1/|γ|,
γ < 0.
It is straightforward to verify that (1.42) holds for this choice of standard GP
distributions. Note that W0 is the standard exponential df again. Corresponding
to (1.26), we have for γ = 1/α ̸= 0,
Wγ = Wi,α,−α,|α|
with i = 1 if γ > 0 and i = 2 if γ < 0. The left endpoint of Wγ is equal to zero
for all γ. Therefore, the γ–parameterization has the additional advantage that the
location parameter is always the left endpoint of the support of the distribution.
The pertaining densities wγ = W
′
γ are w0(x) = e−x, x > 0, if γ = 0, and
wγ(x) = (1 + γx)−(1+1/γ)
for
⎧
⎨
⎩
0 ≤x,
γ > 0;
if
0 ≤x < 1/|γ|,
γ < 0.
The convergence wγ(x) →w0(x) as γ →0 holds again. In Fig. 1.7, Pareto and
beta densities around the exponential density are displayed.
One can recognize that the left endpoint of a standard GP distribution Wγ is
equal to zero; the right endpoint is ﬁnite in the case of beta (GP2) distributions.
The POT–Stability
GP dfs are the only continuous dfs F such that for a certain choice of constants
bu and au,
F [u](bu + aux) = F(x),
where F [u](x) = (F(x)−F(u))/(1−F(u)) is again the exceedance df at u pertaining
to F as introduced in (1.11). This property is the pot–stability of GP dfs. The
following three examples are of particular interest:
• for exponential dfs with left endpoint equal to zero,
W [u]
0,0,σ = W0,u,σ;
(1.43)

26
1. Parametric Modeling
2
4
0.5
1
2
4
0.5
1
Fig. 1.7. (left.) Exponential (dashed) and Pareto (solid) densities for parameters γ =
.5, 1.0. (right.) Exponential (dashed) and beta (solid) densities for parameters γ =
−.3, −.5.
• for Pareto dfs W1,α,µ,σ in the α–parameterization with µ + σ < u,
W [u]
1,α,µ,σ = W1,α,µ,u−µ,
(1.44)
• for GP dfs Wγ,µ,σ in the γ–parameterization with µ < u and σ+γ(u−µ) > 0,
W [u]
γ,µ,σ = Wγ,u,σ+γ(u−µ),
(1.45)
whereby (1.43) is a special case.
According to these relations the truncated version of a GP distribution re-
mains in the same model, a property that exhibits the outstanding importance of
GP distributions for our purposes. A beta df, truncated left of 1, is displayed in
Fig. 1.8.
1
2
3
0.5
1
Fig. 1.8. Beta df W−0.3 (solid) and
a pertaining exceedance df W [1]
−0.3 =
W−0.3, 1, 0.7 (dashed).
In Fig. 1.8, one recognizes that the truncated beta df has a left endpoint
equal to the truncation point 1 and the same right endpoint 1/|γ| = 10/3 as the
original beta df.

1.4. Modeling by Generalized Pareto Distributions
27
Limiting Distributions of Exceedances
The parametric modeling of exceedance dfs F [u] by GP dfs is based on a limit
theorem again. Recall that our intention is the modeling of exceedances dfs at
high thresholds and, hence, one should consider thresholds tending to the right
endpoint of the actual df F.
(Balkema–de Haan–Pickands11.) If F [u](bu +aux) has a continuous12 limiting
df as u goes to the right endpoint ω(F) of F, then
|F [u](x) −Wγ,u,σu(x)| →0,
u →ω(F),
(1.46)
for some GP df with shape, location and scale parameters γ, u and σu > 0.
Note that the exceedance df F [u] and the approximating GP df possess the
same left endpoint u. If (1.46) holds, then F belongs to the pot–domain of at-
traction of the GP df Wγ. It is evident that this limit theorem can be formulated
likewise in terms of GP dfs in the α–parameterization. It is evident that (1.46) is
closely related to (1.28).
One can easily prove that every EV df Gγ belongs to the pot–domain of
attraction of Wγ. For this particular case, we also compute a rate of convergence.
By employing a Taylor expansion of the exponential function around zero, one
should ﬁrst verify that the relation
Gγ(x) = Wγ(x)

1 + O

Wγ(x)

(1.47)
holds for the survivor functions Gγ and Wγ. Then, deduce
|G[u]
γ (x) −W [u]
γ (x)| = O

Wγ(u)

.
(1.48)
Now, (1.46) follows from (1.45), whereby σu = 1+γu. Moreover, if one of the usual
textbook dfs satisﬁes (1.46), then the convergence also holds for the pertaining
densities, also see Section 6.5.
Fig. 1.9 provides two illustrations in terms of densities:
The ﬁrst example treats the standard Weibull and beta densities gγ and wγ with
shape parameter γ = −.3. The second example concerns the standard Cauchy
density f(x) = 1/(π(1 + x2)) and a Pareto density with shape parameter α = 1.
In Fig. 1.9, one can see that the GP (beta and Pareto) densities ﬁt to the
upper tail of the Weibull and Cauchy densities. Elsewhere, the densities are sub-
stantially diﬀerent. The preceding Pareto density with µ = −.25 replaced by µ = 0
also ﬁts to the upper tail of the Cauchy density.
11Balkema, A.A. and de Haan, L. (1974). Residual life time at great age. Ann. Probab.
2, 792–804. Parallel work was done by J. Pickands (1975). Statistical inference using
extreme value order statistics. Ann. Statist. 3, 119–131.
12An example of a discrete limiting df is, e.g., the geometric df Hp(k) = 1 −(1 −p)k,
k ≥1, mentioned on page 13.

28
1. Parametric Modeling
1
2
3
0.5
1
1
2
3
4
0.2
0.4
Fig. 1.9. (left.) Weibull density gγ (solid) and beta density wγ (dashed) with shape
parameter γ = −.3. (right.) Standard Cauchy density (solid) and Pareto density w1,α,µ,σ
(dashed) with α = 1, µ = −.25 and σ = 1/π.
Fitting a Generalized Pareto Distribution to the Upper Tail
As suggested by the limiting relation (1.46), we assume that a GP df Wγ,u,σ
can be ﬁtted to the exceedance df F [u] for certain parameters γ and σ (in short:
F [u] ≈Wγ,u,σ). This implies that a certain GP df can be ﬁtted to the original df
F. More precisely, if F(u) is known, we deduce from a relation F [u] ≈Wγ,u,σ that
F(x) ≈Wγ,˜µ(x),˜σ,
x ≥u,
(1.49)
for certain parameters ˜µ and ˜σ. According to the deﬁnition (1.11) of exceedance
dfs we have
F(x)
=
(1 −F(u))F [u](x) + F(u)
≈
(1 −F(u))Wγ,µ,σ(x) + F(u)
=
Wγ,˜µ,˜σ(x),
x ≥u,
(1.50)
where the latter equality holds for the parameters
˜σ = σ/

1 + γW −1
γ (F(u))

and
˜µ = u −˜σW −1
γ (F(u)).
(1.51)
There is a unique relation between the two pairs of parameters µ, σ and ˜µ, ˜σ
determined by the equations
W [u]
γ,˜µ,˜σ = Wγ,u,σ
and
Wγ,˜µ,˜σ(u) = F(u).
(1.52)
In terms of survivor functions, (1.50) can be written F(x) = F(u)F [u](x) ≈
F(u)Wγ,u,σ(x) = Wγ,˜µ,˜σ(x), x ≥u.
Related conclusions hold in the α–parametrization: if the Pareto df W1,α,µ,σ
with left endpoint µ + σ = u ﬁts to the exceedance df F [u], then the Pareto df

1.4. Modeling by Generalized Pareto Distributions
29
W1,α,˜µ,˜σ determined by W [u]
1,α,˜µ,˜σ = W1,α,µσ and W1,α,˜µ,˜σ(u) = F(u) ﬁts to the
upper tail of the df F. We have ˜µ = µ and ˜σ = σ(1 −F(u))1/α.
Note that the ﬁrst line in (1.50) can be extended to the representation
F(x) = F(u)P(X ≤x|X ≤u) + (1 −F(u))P(X ≤x|X > u),
x real,
(1.53)
where X is a random variable with df F.
For a continuation of this topic we refer to page 58, where relations between
GP and sample dfs are investigated.
Maxima of Generalized Pareto Random Variables
It is a simple exercise to deduce from (1.47) that W n
0 (x + log n) and W n
i,α(anx)—
with an as in (1.31)—are EV dfs G0 and Gi,α in the limit as n →∞. The rate of
convergence is O(n−1).
There is another interesting non–asymptotic relationship between maxima of
GP random variables and EV distributions. Generally, let X1, X2, X3, . . . be non–
negative iid random variables with common df F which has the left endpoint zero.
Let N be a Poisson random variable with parameter λ > 0 which is independent
of the Xi. Then, with F 0(x) = 1 we have
P

max
i≤N Xi ≤x

=
∞

n=0
P

max
i≤N Xi ≤x, N = n

=
∞

n=0
P{N = n}F n(x)
=
exp(−λ(1 −F(x))
(1.54)
for x ≥0 and zero otherwise. Notice that there is a jump at zero. Plugging in a
GP df one obtains a df that equals an EV df right of zero. If λ is suﬃciently large,
then the jump is negligible and the df in (1.54) can be replaced by a GP df.
Moments of Generalized Pareto Distributions
The jth moment mj,W of a Pareto and beta dfs W can be computed in a straight-
forward way. Recollect that
mj,W = E(Xj) =

xj dW(x) =

xjw(x) dx,
where w = W
′ is the density of W and X is a random variable distributed accord-
ing to W. We have
mj,W1,α = α/(α −j),
if α > j,
and
mj,W2,α = (−1)jα/(α −j).

30
1. Parametric Modeling
The jth moment of a Pareto df W1,α is inﬁnite if α ≤j. For the special case of
j = 1, one obtains the mean values mW = m1,W of Pareto and beta dfs W. For
i = 1, 2 we have
mWi,α = |α|/(α −1),
(1.55)
if 1/α < 1. If 1/α < 1/2, the variances are
varWi,α = α/((α −1)2(α −2)).
(1.56)
The mean values and variances of the GP dfs Wγ are
mWγ = 1/(1 −γ),
if γ < 1,
(1.57)
and
varWγ = 1/((1 −γ)2(1 −2γ)),
if γ < 1/2.
In the special case of γ = 0, one gets the well–known result that the mean value
and variance of the standard exponential distribution are both equal to 1.
1.5
Heavy and Fat–Tailed Distributions
There are several diﬀerent concepts of fat or heavy–tailedness of a distribution. We
say that a df F has a heavy upper tail if a jth moment
 ∞
0
xj dF(x)—evaluated
over the positive half–line—is equal to inﬁnity for some positive integer j. There-
fore, all Pareto dfs are heavy–tailed. In addition, all the dfs in the pot–domain
of attraction of a Pareto df are heavy–tailed. Likewise one may speak of a heavy
lower tail.
Fat–tailedness may be introduced by comparing the kurtosis to that of the
normal distribution.
The Normal (Gaussian) Model
Let Φ(x) =  x
−∞ϕ(y) dy denote the standard normal df, and
ϕ(y) = exp(−y2/2)/
√
2π
the pertaining normal density.
We introduce the mth convolution of a df F, which is given by
F m∗(x) = P{X1 + · · · + Xm ≤x},
where X1, . . . , Xm are iid random variables with common df F. A parametric
modeling of convolutions by means of normal dfs is adequate, because under the
conditions of the central limit theorem, we know that
sup
x |F m∗(x) −Φ ((x −µm)/σm)| →0,
m →∞,
(1.58)

1.5. Heavy and Fat–Tailed Distributions
31
-2
2
0.5
1
-2
2
0.2
0.4
Fig. 1.10. (left.) Standard normal df Φ. (right.) Standard normal density ϕ.
where µm and σm > 0 are location and scale parameters.
It is noteworthy that the normal df Φ is sum–stable because Φm∗(m1/2x) =
Φ(x). The Cauchy df F is another sum–stable df with F m∗(mx) = F(x). This
topic will be further pursued in Section 6.4 within the framework of sum–stable
distributions.
The Kurtosis and a Concept of a Fat–Tailedness
The kurtosis of a df F is another moment ratio which is deﬁned by

(x −mF )4 dF(x)

var2
F ,
(1.59)
where mF and varF are the mean and the variance of F. If the density has a high
central peak and long tails, then the kurtosis is typically large. Notice that the
kurtosis is independent of location and scale parameters. A df with kurtosis larger
than 3, which is the kurtosis of normal dfs, is fat–tailed or leptokurtic. We also
speak of fat upper and lower tails in this context.
It is easily proven that the mixture of two Gaussian dfs—see (1.61) and
Example 2.3.2—with equal location parameters and unequal scale parameters is
leptokurtic.
Log–Normal Distributions
The property of fat–tailedness is also discussed in conjunction with the log–normal
distribution which has a short (ﬁnite) lower tail, yet one can speak of a fat upper
tail.
If one has a strong preference for the normal model, yet the positive data
x1, . . . , xn indicate a fat or heavy upper tail of the underlying distribution, then
one may try to ﬁt a normal df Φµ,σ to the transformed data T (x1), . . . , T(xn) of

32
1. Parametric Modeling
a reduced magnitude, cf. also page 37. The ﬁrst choice of such a transformation is
T (x) = log x.
If a normal df Φµ,σ can be ﬁtted to log(x1), . . . , log(xn), then the log–normal
df
F(µ,σ)(x) = Φµ,σ(log(x)),
x > 0,
(1.60)
can be ﬁtted to the original data x1, . . . , xn. Conversely, exp–transformed normal
data are log–normal which justiﬁes to say that the log–normal df has a fatter upper
tail than the normal one.
We have F(0,σ)(1 + σx) →Φ(x) as σ →0. Therefore, instead of employing
a log–transformation to the data, one may also try to ﬁt a log–normal df to the
original data.
The log–normal df is not heavy–tailed in our terminology, cf. page 30, because
all moments are ﬁnite. Note that normal as well as log–normal dfs belong to the
pot–domain of attraction of the exponential df , cf. [20] on pages 67–68.
Notice that F(µ,σ)(x) = Φ0,σ(log(x/ exp(µ))) and, hence, exp(µ) is a scale
parameter of the log–normal df. Log–normal distributions are skewed to the right.
1
2
3
0.5
1
1.5
Fig. 1.11.
Plots of log–normal
densities f(0,1) (solid) and f(0, .3)
(dashed).
The preceding illustration concerns the standard log–normal density
f(0,σ)(x) =
1
√
2πσx exp

−(log(x))2/2σ2
,
x > 0,
with the shape parameter σ.
Mixtures of Gaussian and Heavy–Tailed Distributions
To enlarge a given model one may take mixtures
F = (1 −d) F0 + d F1,
(1.61)
where F0 and F1 are taken from an initial parametric model and the mixing pa-
rameter d ranges between 0 and 1. An important example is a mixture of two
normal distributions.

1.5. Heavy and Fat–Tailed Distributions
33
Actual dfs are mixtures, if data are generated according to random mech-
anism with dfs F0 and F1 with a probability 1 −d and, respectively, d. A more
detailed interpretation of mixtures is given in Section 8.1 in conjunction with con-
ditional distributions.
Subsequently, we deal with mixtures (1 −d)Φ + d F, where F is another df
that may be regarded as a contamination of the ideal normal model and 0 ≤d ≤1.
We consider two special cases of a densities f which may serve as a contamination
of the normal density.
• (Student Distributions.) For α > 0, we introduce densities
fα(x) = c(α)

1 + x2
α
−(1+α)/2
(1.62)
where c(α) = Γ((1+α)/2)/(Γ(α/2)Γ(1/2)α1/2). For positive integers α this is
the Student distribution with α degrees of freedom. The variance is α/(α−2)
for α > 2. In (6.16), such distributions are introduced as mixtures of normal
distributions.
• (Generalized Cauchy Distributions.) The density of the standard generalized
Cauchy df with shape parameter α > 0 is13
fα(x) = c(α)/

1 + |x|1+α
,
(1.63)
where c(α) = ((1 + α)/(2π)) sin(π/(1 + α)).
In both cases one obtains the standard Cauchy density, cf. page 27, for α = 1.
In addition, these densities are unimodal with mode equal to zero and possess
heavy tails like the Pareto dfs with shape parameter α.
The mixture density (1−d)ϕ(x)+d fα(x), with d being close to zero, has the
shape of a normal density in the center, yet possesses heavier tails. It is advisable
to apply trimmed estimators of µ and σ to reach an estimate of the central normal
part of the distribution.
The following illustrations concern mixtures between the standard normal
distribution and Cauchy and Weibull distributions.
In Fig. 1.12 (left), one can clearly observe the heavier tails of the mixture,
although the fraction of the Cauchy distribution is only ten percent. The center
of the mixture can hardly be distinguished from a normal density.
Remarks About Robust Statistics
The contamination of a normal distribution by a heavy–tailed distribution (such as
the Cauchy distribution) is one of the favorite topics in robust statistics, because
13Drees, H. and Reiss, R.–D. (1992). Tail behavior in Wicksell’s corpuscle problem. In:
Probability Theory and Applications, J. Galambos and I. K´atai (eds.), 205–220, Kluwer,
Dortrecht.

34
1. Parametric Modeling
1
2
3
0.2
0.4
1
2
3
0.2
0.4
Fig. 1.12. Normal density ϕ (solid) and mixture densities .9ϕ + .1f1 (left) and 0.9ϕ +
0.1g2, −1.6, 3, 1 (right) with Cauchy and Weibull components.
observations under the heavy tail—frequently regarded as outliers—destroy the
performance of classical statistical procedures such as the sample mean.
A primary aim of robust statistics may be formulated in the following manner
(cf. [27]): “Describe the structure best ﬁtting to the bulk of the data: we tentatively
assume a parametric model ... taking explicitly into account that the model may be
distorted.” For this (central) bulk of the data the normal model is the dominating
parametric model.
A Converse View Towards Robust Statistics
In the preceding lines, a fat or heavy–tailed distribution was regarded as a con-
tamination of the normal df. A converse view can be expressed in the following
manner14:
“In many applications extreme data do not ﬁt to the ideology of normal sam-
ples and, therefore, extremes are omitted from the data set or one uses statistical
procedures upon which extremes have a bounded inﬂuence. The converse attitude
is to regard extremes as the important part of the data set, yet one must still
follow certain principles of robust statistics and check the validity of the statistical
model selection.”
The decisive step was the ﬁtting of certain dfs—such as GP dfs—to extreme
data. One must be aware that
• the actual df deviates from any of the chosen parametric ones, and
• there can be gross errors in the measurements which may heavily inﬂuence
the performance of statistical procedures.
14 Reiss, R.–D. (1989). Robust statistics: A converse view. 23rd Semester on Robustness
and Nonparametric Statistics. Stefan Banach Center, Warsaw, Abstracts of Lectures,
Part II, 183–184.

1.6. Quantiles, Transformations and Simulations
35
The diﬃculties arising from the fact that the actual distribution deviates
from any EV or GP distribution is the central topic in most research papers on
extreme value theory. This question is brieﬂy touched upon in this book:
• we already mentioned limit theorems for maxima and exceedance dfs (cf.
Sections 1.3 and 1.4);
• a von Mises condition will be introduced in Section 2.1 in conjunction with
reciprocal hazard functions, and
• the concept of a δ–neighborhood of a GP distribution and related weaker
conditions are studied in Section 6.5.
Less is known about robustifying statistical procedures in the extreme value
setting against gross errors. The explanations in the Sections 3.1 and 5.1 about
M–estimates should be regarded as a ﬁrst step.
1.6
Quantiles, Transformations
and Simulations
The concept of q–quantiles and quantile functions (qfs) is introduced in greater
generality and in a more rigorous manner compared to our remarks about quantiles
on page 12. We also deal with technical questions that can be best described by
means of quantile functions.
Quantiles
In conjunction with the T –year level, we already introduced the q–quantile of a df
F as the value z such that F(z) = q. Thus, the q–quantile z is a value along the
measurement scale with the property that the fraction q of the distribution is left
of z.
In Fig. 1.13, this will be illustrated for the Gumbel df G0(x) = exp(−e−x),
where z = −log(−log(q)).
If the q–quantile is not unique, one may take the smallest z such that F(z) =
q. For discrete dfs and, generally, for discontinuous dfs, it may happen that there
is no value z such that F(z) = q. Then, the q–quantile is the smallest value x such
that F(x) ≥q. The 1/2–quantile is the median of the df F.
Quantile Functions
If the df F is continuous and strictly increasing on its support (α(F), ω(F)), then
the quantile function (qf) F −1 is the usual inverse of F.
Usually, the quantile function (qf) F −1 is a “generalized inverse”
F −1(q) := inf{x : F(x) ≥q},
0 < q < 1.
(1.64)

36
1. Parametric Modeling
z
q
-2
2
4
1
z
-2
2
4
0.2
0.4
Fig. 1.13. (left.) The Gumbel df G0 evaluated at the q–quantile z. (right.) The area under
the Gumbel density g0(x) = e−x exp(−e−x) left of the q–quantile z is equal to q.
Notice that F −1(q) is the q–quantile of F as introduced in the preceding
lines. If Fµ,σ is a df with location and scale parameters µ and σ, then
F −1
µ,σ = µ + σF −1,
(1.65)
where F = F0,1 is the standard df.
0.5
1
-2
2
4
Fig. 1.14. Plot of the Gumbel
qf G−1
0 (q) = −log(−log(q)).
We state the explicit form of EV and GP qfs in both parameterizations
starting with EV qfs in their α–parameterization.
Gumbel (EV0):
G−1
0 (q) = −log(−log(q));
Fr´echet (EV1), α > 0:
G−1
1,α(q) = (−log(q))−1/α;
Weibull (EV2), α < 0:
G−1
2,α(q) = −(−log(q))−1/α.
Next, we present EV qfs in their γ–representation:

1.6. Quantiles, Transformations and Simulations
37
EV, γ ̸= 0:
G−1
γ (q) = ((−log(q))−γ −1)/γ,
and for γ = 0, we have the Gumbel qf G−1
0
again.
We continue with GP qfs in the α–parameterization:
Exponential (GP0):
W −1
0
(q) = −log(1 −q);
Pareto (GP1), α > 0:
W −1
1,α(q) = (1 −q)−1/α;
Beta (GP2), α < 0:
W −1
2,α(q) = −(1 −q)−1/α.
Finally, there is the γ–representation of GP dfs:
GP, γ ̸= 0:
W −1
γ (q) = ((1 −q)−γ −1)/γ,
and again W −1
0
(q) = −log(1 −q).
Recall that the medians of these distributions are obtained by plugging in
the value q = 1/2. Verify, by means of
(xy −1)/y →log x,
y →0,
(1.66)
that EV and GP qfs in the γ–parameterization again built a continuous family.
Transformations
If a statistician is preoccupied with certain parametric dfs, say Fϑ, yet none of
these dfs ﬁts to the data x1, . . . , xn, one may deal with the transformed data
T (x1), . . . , T(xn). If Fϑ ﬁts to T (x1), . . . , T(xn) for some parameter ϑ, then the
df Fϑ(T ) ﬁts to the original data x1, . . . , xn. For example, the transformation
T (x) = log(x) reduces the magnitude of positive data. The df Fϑ(log(x)) is called
the log–Fϑ df. We may say that the log–df has a fatter upper tail than the original
df in view of the exp–transformation of the data.
• (Exponential Model as Log–Gompertz Model.) If X is an exponential random
variable with scale parameter σ, then log X is a Gompertz random variable
with location parameter µ = log σ.
• (Pareto Model as Log–Exponential Model.) If X is a Pareto random variable
with shape and scale parameters α and σ, then log X is an exponential
random variable with location and scale parameters log σ and 1/α.
• (Fr´echet Model as Log–Gumbel Model.) A corresponding relationship holds
between a Fr´echet random variable with shape and scale parameters α and
σ and a Gumbel random variable with location and scale parameters log σ
and 1/α.

38
1. Parametric Modeling
• (Pareto and Beta Model.) If X is once again a Pareto random variable with
shape and scale parameters α and σ, then −1/X is a beta random variable
with shape and scale parameters −α and 1/σ.
• (Fr´echet and Weibull Model.) A corresponding relationship holds between
a Fr´echet random variable with shape and scale parameters α and σ and a
Weibull random variable with shape and scale parameters −α and 1/σ.
Such transformations are suggested by the quantile and probability transfor-
mations:
1. Quantile Transformation: if U is a (0, 1)–uniformly distributed random
variable, then F −1(U) has the df F,
2. Probability Transformation: conversely, if X is a random variable with
continuous df F, then F(X) is (0, 1)–uniformly distributed.
We mention further examples of transformed random variables: if X is a
converse exponential random variable, then
T (X) = G−1
2,α(G2,−1(X)) = −(−X)−1/α
(1.67)
is a Weibull random variable with shape parameter α. Likewise,
T (X) = −(−X)−1/α
(1.68)
is a beta random variable with shape parameter α if X is (−1, 0)–uniformly dis-
tributed.
Generation of Data
According to our preceding remarks, one can easily generate data under EV and
GP dfs: if u1, . . . , un are governed by the uniform df on the interval (0, 1), the
transformed values F −1(u1), . . . , F −1(un) are governed by the df F. For example,
the quantile transformation technique can be applied to a Cauchy random variable
which possesses the qf F −1
1
(q) = tan(π(q −0.5)).
If location and scale parameters µ and σ are included, then the transformation
can be carried out in two steps because F −1
µ,σ(ui) = µ + σF −1(ui).
We mention two other simulation techniques15:
• the normal qf is not feasible in an analytical form and the quantile transfor-
mation is not applicable. In this case, the polar method can be utilized,
• the interpretation of mixture distributions within a two–step experiment
stimulates a technique to carry out simulations, cf. page 33 and Section 8.1.
15 For details see, e.g., Devroye, L. (1986). Non–Uniform Random Variate Generation.
Springer, New York.

Chapter 2
Diagnostic Tools
In this chapter, we catch a glimpse of the real world in the condensed form of
data. Our primary aim is to ﬁt extreme value (EV) and generalized Pareto (GP)
distributions, which were introduced in the foregoing chapter by means of limit
theorems, to the data.
For that purpose, fairly simple nonparametric techniques for visualizing data
are introduced. Those tools are especially selected which are of high relevance to
extreme values. For example, sample excess and sample reciprocal hazard functions
should be close to a straight line if a GP hypothesis is valid. Of course, Q–Q plots
are also on the agenda. We discuss Q–Q plots for location and scale parameter
families as well as for EV and GP models. Finally, certain tools are introduced
which are relevant in conjunction with time series phenomena.
2.1
Visualization of Data
First, we describe visualization techniques such as the sample df, the sample qf,
histograms and kernel densities. In contrast to Chapter 1, we do not assume that
the data are maxima, exceedances or sums.
The Sample Distribution Function
The sample df Fn(x) at x for a series of univariate data x1, . . . , xn is the relative
number of the xi that are smaller or equal to x. Thus,
Fn(x) := 1
n

i≤n
I(xi ≤x),
(2.1)
where the indicator function is deﬁned by I(y ≤x) = 1 if y ≤x and 0, elsewhere;
furthermore, the summation runs over i = 1, . . . , n. Sample dfs are particularly
useful for representing samples of a smaller size.

40
2. Diagnostic Tools
The data x1, . . . , xn ordered from the smallest to the largest are denoted by
x1:n ≤· · · ≤xn:n.
We have Fn(xi:n) = i/n if xi:n is not a multiple point. Notice that Fn is
constant between consecutive ordered values. The ordered values can be recaptured
from the sample df and, thus, there is a one–to–one correspondence between the
sample df and the ordered values. One may take a linear interpolation between
consecutive points as well (also see pages 42 to 47 for other modiﬁcations).
Occasionally, we write Fn(x; x) in place of Fn(x) to indicate the dependence
on the vector of data x = (x1, . . . , xn). We will primarily deal with situations
where each of the xi is generated under a common df F and the sample df Fn is
approximately equal to F. This relationship will be brieﬂy written
Fn(x) ≈F(x).
(2.2)
In view of (2.2), we say that Fn(x) is an estimate of F(x).
-2
2
4
0.5
1
Fig. 2.1. Gumbel df (dotted) and
sample df of a Gumbel data set
with 50 points.
To a larger extent, our statistical arguments are based on the relationship
(2.2) between the sample df Fn and the underlying df F so that, apparently,
the statistical prerequisites for understanding this textbook are of an elementary
nature.
The Sample Exceedance Distribution Function
Let Fn(x; · ) be the sample df based on the data xi as in the preceding lines. Recall
that the exceedances yi over the threshold u are those xi with xi > u taken in
the original order of their outcome. The sample df Fk(y; · ) based on the vector

2.1. Visualization of Data
41
y = (y1, . . . , yk) of exceedances is the sample exceedance df. We have
Fk(y; · ) =

Fn(x; · )
[u]
.
(2.3)
Thus, the sample exceedance df is equal to the exceedance df of the sample df.
From the fact that the sample df Fn(x; · ) is an estimate of the underlying
df F, as pointed out in (2.2), one may conclude that
Fk(y; · ) ≈F [u],
x ≥u;
(2.4)
that is, the sample exceedance df is an estimate of the exceedance df.
Sample Moments
Now we introduce sample versions of the mean, variance and skewness coeﬃcient.
These sample functionals can be deduced from the sample moments
mj,n = 1
n

i≤n
xj
i .
(2.5)
Subsequently, assume that x1, . . . , xn are governed by the df F. The representation
mj,n =

xj dFn(x) and (2.2) make it plausible that the jth sample moment mj,n
is an estimate of the jth moment mj =

xj dF(x) of F if this moment exists.
We also write
¯xn = 1
n

i≤n
xi
for the sample mean. The sample variance is
s2
n =
1
n −1

i≤n
(xi −¯xn)2.
(2.6)
A factor 1/(n −1) is employed instead of 1/n in the deﬁnition of s2
n to obtain an
unbiased estimator (cf. page 89) of the variance of F. Note that s2
n = m2,n −m2
1,n
if the factor 1/n is taken. We also write ¯x and s2 instead of ¯xn and s2
n.
The sample skewness coeﬃcient can be written
1
n

i≤n
xi −¯xn
sn
3
= (m3,n −3m1,nm2,n + 2m3
1,n)/s3
n.
(2.7)
The sample skewness coeﬃcient is also a natural estimate of the skewness coeﬃ-
cient skewF of F.

42
2. Diagnostic Tools
Sample Quantiles, Sample Quantile Functions
We believe that qfs and densities are more useful than dfs for the visual discrimina-
tion between distributions. In the subsequent lines we present the sample versions
of qfs and densities.
The sample qf F −1
n
may be introduced by
F −1
n

i
n + 1

:= xi:n,
(2.8)
where x1:n ≤· · · ≤xn:n are the data arranged in increasing order. In statistical
publications, it is also suggested to employ plotting positions (i −0.5)/n in place
of i/(n + 1) in (2.8). In addition, let F −1
n
be constant between consecutive points
i/(n + 1). Likewise, one may employ a linear interpolation. Note that the sample
qf remains constant in case of multiple points.
The sample qf F −1
n
is the qf—in the sense of (1.64)—of the sample df Fn
if the sample qf is taken left–continuous. Also, F −1
n (q) is the sample q–quantile.
Usually, special constructions are employed for the sample median.
Conclude from the basic relationship (2.2) for dfs that
xi:n = F −1
n

i
n + 1

≈F −1

i
n + 1

,
(2.9)
where F −1 is the qf pertaining to the df F, cf. (1.64). We also have x[nq]:n ≈
F −1(q).
In conjunction with grouped data and the kernel method, we will also present
smoothed versions of the sample qf.
Linearly Interpolated Sample Distributions
If the underlying df is continuous, then it is plausible to estimate this df by means
of a continuous sample df. Such a df can be constructed by linearly interpolating
the sample df Fn in (2.1) over bins (tj, tj+1], where the tj < tj+1 constitute a grid
on the real line. One gets the continuous sample df
Fn(x)
=
Fn(tj) +
x −tj
tj+1 −tj
 Fn(tj+1) −Fn(tj)

=
Fn(tj) + (x −tj)nj
tj+1 −tj
,
for tj < x ≤tj+1,
(2.10)
where nj is the frequency of data x1, . . . , xn in the bin (tj, tj+1]. Thus, the sample
df Fn only depends on the data in a grouped form.
Example 2.1.1. (Sample Distribution and Quantile Functions of Grouped Fire Claim
Data.) The sample df and sample qf of grouped claim data are displayed. From the

2.1. Visualization of Data
43
original data set of UK ﬁre claim data, observed during a four year period, we just take
the 387 claim sizes over £51,200 (these data are taken from [10] and stored in the ﬁle
ig–ﬁre1.dat).
Table 2.1. Number n(j) of claim sizes within class limits in £1000
lower class limit
51.2 72.41 102.4 250 500 750 1000 2000 3000
number of claims n(j)
108
88
117
47
12
4
8
3
0
The sample df and sample qf pertaining to the histogram are plotted in Fig. 2.2.
claim size
1000
2000
3000
0.5
1
claim size
0.5
1
1000
2000
3000
Fig. 2.2. Sample df (left) and sample qf (right) for ﬁre claim data over £51,200 .
The ﬁrst overall impression is that the sample qf is a reﬂection of the sample df at
the diagonal (as it should be because the sample qf is a generalized inverse of the sample
df). It is evident that the sample df and qf provide the same information about the data
although the viewpoints are diﬀerent.
The sample df based on grouped data is piecewise continuously diﬀerentiable
and, therefore, it has a density in the form of a histogram. We will also deal with
histograms for discrete data and with kernel densities for continuous data. The
latter concept can be regarded as a modiﬁcation of histograms for grouped data.
Histograms for Grouped Data
Again let nj be the frequency of data in the bin (tj, tj+1]. Taking the derivative
of the preceding sample df Fn based on grouped data as given in (2.10), one gets
the probability density
fn(x) =
n(j)
n(tj+1 −tj),
tj < x ≤tj+1.
(2.11)

44
2. Diagnostic Tools
It is very natural to visualize frequencies by means of such a histogram. It is
apparent that this histogram is an appropriate estimate of the density f of F. The
histogram may also be addressed as sample density.
Practitioners use histograms because of their simplicity in representing data,
even if the data are given in a continuous form. One disadvantage of a histogram is
that one must choose the location of the grid. Later we will introduce an alternative
method for representing data, namely by means of a kernel density.
Histograms for Discrete Data
In the case of integer–valued data, a sample histogram is given by
pn(j) = n(j)/n,
where n(j) is the number of data x1, . . . , xn equal to the integer j. In analogy to
(2.2), we have
pn(j) ≈P{j},
(2.12)
where P is the underlying discrete distribution (under which the xi were gener-
ated). Note that the discrete values xi—ordered according to their magnitudes—
can be recaptured from the histogram. Histograms for binomial and Poisson dis-
tributions are given in Fig. 1.2, also see Fig. 3.2.
Kernel Densities
Starting with continuous data x1, . . . , xn, the histogram for grouped data may be
constructed in the following manner. Replace each point xi in the bin (tj, tj+1] by
the constant function
g(x, xi) =
1
n(tj+1 −tj),
tj < x ≤tj+1,
(2.13)
with weight 1/n. In summing up the single terms g(x, xi), one gets the histogram
for grouped data in the representation fn(x) = 
i≤n g(x, xi). If continuous data
are given, then the choice of the grid is crucial for the performance of the histogram.
We represent an alternative construction of a sample density. In contrast to
(2.13), replace xi by the function
gb(x, xi) = 1
nbk
x −xi
b

,
where k is a function (kernel) such that  k(y) dy = 1 and b > 0 is a chosen
bandwidth. If k ≥0, then k((x−xi)/b)/b may be regarded as a probability density
with location and scale parameters xi and b > 0. The function gb(·, xi) again
possesses the weight 1/n.

2.1. Visualization of Data
45
In summing up the single terms, one gets the kernel density
fn,b(x) :=

i≤n
gb(x, xi) = 1
nb

i≤n
k
x −xi
b

(2.14)
which is a probability density if k ≥0.
In subsequent illustrations, the Epanechnikov kernel
k(x) = 3
4(1 −x2)I(−1 ≤x ≤1)
is taken which is the optimal kernel under a certain criterion. A kernel related
directly to the terms g(x, xi) in the histogram is the “naive” one
k(x) = 0.5 × I(−1 ≤x < 1).
Other interesting choices of kernels are
k(x) = 1
8(9 −15x2)I(−1 ≤x ≤1)
(2.15)
or
k(x) = 15
32(3 −10z2 + 7z4)I(−1 ≤x ≤1),
(2.16)
because these kernels satisfy the additional condition

x2k(x) dx = 0.
In analogy to the choice of the grid for the histogram—particularly of the
bin–width—the choice of an appropriate bandwidth b is crucial for the performance
of the kernel density.
-2
2
0.2
Fig. 2.3. Plots of normal density
(solid) and kernel densities for the
bandwidths b = 1.1 (dashed) and
b = 0.3 (dotted) based on 50
Gaussian data.
If the bandwidth b is small, which is related to a small scale parameter, then
one can still recognize terms gb(x, xi) representing the single data. If b is large,
then an oversmoothing of the data may prevent the detection of certain clues in
the data.

46
2. Diagnostic Tools
An Automatic Bandwidth Selection
An automatic bandwidth selection is provided by cross–validation (see, e.g., the
review article by Marron1 or [50]). For ﬁnite sample sizes, the automatic choice
of the bandwidth must be regarded as a ﬁrst crude choice of the bandwidth. It is
useful to vary the bandwidth around the automatically selected parameter; e.g.,
decrease the bandwidth until the graph of the kernel density becomes bumpy.
Example 2.1.2. (TV Watching in Hours per Week.) We analyze 135 data of TV watching
in hours per week stored in the ﬁle su–tvcon.dat. The bulk of the data is below 20 hours.
The lower 100 observations range from 1.68 to 19.5. A list of the 35 data exceeding 20
hours is given.
Table 2.2. Hours per week over 20.
20
20.5
23
24
26
27.5
28.5
31.5
45
20
22
23
24.75
26
27.5
29
33
49
20
22
23
25
27
28
29.5
37
63
20.5
22
23.9
25
27
28
30
40
These data are represented by a kernel density in Fig. 2.4. The automatic bandwidth
selection by means of cross–validation leads to the choice b = 12.5.
hours per week
40
80
0.02
0.04
Fig. 2.4. Two kernel densities for
bandwidths b = 12.5 (solid) and
b = 5 (dotted) for TV data.
The cross–validation with the selected bandwidth b = 12.5 seems to oversmooth the
data. The bandwidth b = 5 is small enough so that a more detailed structure becomes
visible in the upper part of the distribution which might indicate a subpopulation. Yet,
this bandwidth is suﬃciently large so that the kernel density does not become bumpy.
1Marron, J.S. (1988). Automatic smoothing parameter selection: A survey. Empirical
Economics 13, 187–208.

2.1. Visualization of Data
47
It is likely that in the classical statistical analysis—from a methodological
viewpoint one may specify this as the work done before R.A. Fisher2—certain
observations, such as the data point 63, are regarded as outliers and are omitted
from the sample.
Kernel Distribution and Quantile Functions
By taking the df pertaining to the kernel density fn,b, one obtains a competitor
Fn,b of the sample df Fn. We have
Fn,b(x) := 1
n

i≤n
K
x −xi
b

,
(2.17)
where
K(x) =
 x
−∞
k(y) dy.
By taking the qf pertaining to Fn,b, one obtains a competitor of the sample qf.
Another version of the sample qf is constructed by directly smoothing the
sample qf by a kernel k. Let
F −1
n,b(q) = 1
b
 1
0
k
q −y
b

F −1
n (y) dy.
One must apply a variable bandwidth selection around the boundary points.
Kernel Densities With Bounded Support
If it is known that none of the observations is below or, respectively, above a
speciﬁc threshold—e.g., life spans are non–negative or exceedances over a certain
threshold t exceed t—then the foregoing smoothing of data should not result in
shifting weight below or above such thresholds (also compare the kernel densities
in Fig. 2.4 and Fig. 2.10).
One may reﬂect the sample at the minimum and/or maximum value and
apply the preceding method or, alternatively, take bandwidths that vary with the
location.
If one realizes that there is a mode at a boundary point—as in the case of
the exponential density at zero—then one should employ less smoothing around
this point.
2who discussed the problem of outliers : “... the rejection of observations is too crude
to be defended: and unless there are other reasons for rejection than the mere diver-
gences from the majority, it would be more philosophical to accept these extremes, not
as gross errors, but as indications that the distribution of errors is not normal” on page
322, respectively, page 289 in Fisher, R.A. (1922). On the mathematical foundation of
theoretical statistics. Phil. Trans. Roy. Soc. A 222, 309–368, and Collected Papers of R.A.
Fisher, Vol. I, J.H. Bennett, ed., pp. 274–335. University of Adelaide, 1971.

48
2. Diagnostic Tools
0.5
1
0.5
1
-1
1
2
3
4
0.5
1
Fig. 2.5. (left.) [0, 1]–uniform (solid), kernel (dashed) and bounded kernel density (dot-
ted) for b = .2 based on 400 uniform data. (right.) Exponential (solid), kernel (dashed)
and left bounded kernel density (dotted) for b = 1.1 based on 50 exponential data.
Critical Remarks About Sample Distribution Functions
Visualizing data by the sample df yields a severe regularization of the data in so
far that
• there is a severe averaging, and
• one gets a monotone function approaching one (zero) in the upper (lower)
tail.
This makes the sample df particularly applicable for small sample sizes. Yet for
moderate and large sample sizes, the sample qf and, to some extent, sample densi-
ties such as the histogram and the kernel densities are more useful. This discussion
will be continued in Section 2.4.
The Scatterplot
Points (i, x(i)) or, generally, (t(i), x(i)) for 1 ≤i ≤n are plotted, thus resulting
in a scatterplot. It is evident that such a scatterplot is also useful for plotting
a function. The scatterplot is an indispensable tool for visualizing time series
phenomena (see Section 2.5).
Example 2.1.3. (Continuation of Example 1.2.1 about Annual Wind–Speed Maxima at
Vancouver.) In Fig. 2.6 the annual wind–speed maximum is plotted against the year of
occurrence.
The scatterplot exhibits a certain decreasing tendency in the data which is captured
by a least squares line, cf. Section 2.5, given by s(x) = 938.49−0.44x. Such a decrease in
the annual wind–speed maxima may be due to changes in the climate or an urbanization
near the gauging station.

2.2. Excess and Hazard Functions
49
year
km/h
1950
1960
1970
1980
60
80
Fig. 2.6. Annual wind–speed
maxima measured in km/h
plotted against years.
This part of the book only concerns univariate extremes. Whenever bivariate
data are treated, then the ﬁrst component is regarded as a time–scale. In extreme
value theory, it is customary to replace the time i by i/n in order to obtain an
elegant formulation of asymptotic results. We are particularly interested in scat-
terplots of points where the second component x(i) exceeds a selected threshold
u. Then, the values i/n locate the exceedance times, cf. page 13.
For a continuation of this topic we refer to Sections 2.5 to 2.7.
2.2
Excess and Hazard Functions
Mean and median excess functions are important to extreme value analysis from
a diagnostic viewpoint, because excess functions of GP dfs are straight lines. The
pertaining sample versions provide further useful techniques for visualizing data.
Another diagnostic tool, taken from survival analysis, is the hazard function.
Yet, we are primarily interested in the reciprocal hazard function which is a straight
line for GP dfs.
Excess Distribution Functions
As introduced in Section 1.2, the excesses
y′
i := yi −u
over the threshold u are a variant of the exceedances yi. The excesses are the
exceedances shifted to the left by the amount u. The pertaining excess df F (u) at
u is
F (u)(x) = F [u](x + u) = F(x + u) −F(u)
1 −F(u)
,
x ≥0,
(2.18)
where F [u] is the exceedance df. Notice that the left endpoint of F (u) is equal to
zero.

50
2. Diagnostic Tools
The excess df F (u) can be introduced as a conditional df: if X is a random
variable with df F, then
F (u)(x) = P(X −u ≤x|X > u)
(2.19)
is the conditional df of X −u given X > u.
The excess df F (u) is alternatively called residual life df at age u, where
F (u)(x) is the probability that the remaining life time is smaller or equal to x
given survival at age u.
In this section, we are especially concerned with functional parameters of the
excess df F (u) as a function in u. For example, the mean excess function describes
the expected remaining life given survival at age u.
Mean Excess Functions
The mean excess function eF of a df F (respectively, of a random variable X) is
given by the conditional expectation of X −u given X > u. We have
eF (u) = E(X −u|X > u) =

x dF (u)(x),
u < ω(F).
(2.20)
It is evident that eF(u) is the mean of the excess df at u. The mean excess function
eF is also called the mean residual life function, see, e.g., [28]. Note that
eFµ,σ(u) = σeF ((u −µ)/σ) ,
(2.21)
where µ and σ are the location and scale parameters.
In conjunction with the visualization of data, we are expressly interested in
dfs, where the mean excess function is a straight line. It is well known that the
GP dfs are the only dfs where this goal is achieved.
We also deal with converse Weibull dfs
	G2,α(x) = 1 −exp(−x−α),
x > 0,
with parameter α < 0 (introduced in (1.38)). Note that 	G2,−1 = W0 is the expo-
nential df on the positive half–line.
Here is a list of mean excess functions for GP and converse Weibull dfs.
Exponential (GP0):
eW0(u) = 1,
u > 0,
Pareto (GP1), α > 1:
eW1,α(u) = u/(α −1),
u > 1,
Beta (GP2), α < 0:
eW2,α(u) = u/(α −1),
−1 ≤u ≤0,
GP:
eWγ(u) = 1+γu
1−γ
for
⎧
⎨
⎩
0 < u,
0 ≤γ < 1,
if
0 < u < −1/γ,
γ < 0,

2.2. Excess and Hazard Functions
51
Converse Weibull, α < 0:
e 	G2,α(u) = |α|−1 u1+α(1 + O(uα)).
We see that the mean excess function eWγ,µ,σ has the slope β1 = γ/(1 −γ)
and the intercept β0 = (1 −γµ)/(1 −γ). Therefore, the slopes of the mean excess
functions of GP dfs Wγ,µ,σ are increasing in γ.
γ = −0.5
γ = 0.5
γ = 0
0.5
1
1.5
1
2
3
4
Fig. 2.7. Mean excess functions
of GP dfs for parameters γ =
−0.5, 0, 0.5.
If the excess df F (s) is close to a GP df W, then it is obvious from (2.20) that
eF (u), u ≥s, is close to the corresponding straight line determined by W. Mean
excess functions do not exist for Pareto dfs with a shape parameter α ≤1.
We also refer to (5.34), where Benktander II dfs are constructed with mean
excess functions equal to |α|−1u1+α.
We now mention another representation of the mean excess function. First
notice that
eF (u) = E((X −u)I(X > u))
P{X > u}
,
u < ω(F).
Secondly, one gets
E((X −u)I(X > u))
=

I(u < x)(x −u) dF(x)
=
 ∞
u
(1 −F(x)) dx,
(2.22)
where the second equation can be veriﬁed by writing
x −u =

I(u ≤y)I(y < x) dy,
x ≥u,
and by interchanging the order of integration (applying Fubini’s theorem). There-
fore, the mean excess function can also be written as
eF(u) =
 ∞
u (1 −F(x)) dx
1 −F(u)
,
u < ω(F).
(2.23)

52
2. Diagnostic Tools
The expectations
E(max{X −u, 0}) =
 ∞
u
(1 −F(x)) dx
(2.24)
are also called “tail probabilities”.
Sample Mean Excess Functions
Let eF again be the mean excess function of the df F. By plugging in the sample
df Fn based on x1, . . . , xn, one obtains by
en(u) := e Fn(u) =

i≤n(xi −u) I(xi > u)

i≤n I(xi > u)
,
x1:n < u < xn:n,
(2.25)
an estimate of eF . Note that en(u) is the sample mean of the excesses over the
threshold u.
Taking the mean or the median of the excesses over u entails a certain smooth-
ing (regularization) of the data, yet one should be aware that excess functions are
neither increasing or decreasing nor do they approach a constant at the upper end
of the range.
Sample mean excess functions will be one of our basic tools to verify the
validity of a GP hypothesis in the upper tail of a distribution. The applicability of
this approach is conﬁrmed by the relationship to the reciprocal hazard function,
cf. (2.27), in conjunction with the von Mises condition (2.32).
Trimmed Mean Excess Functions
Because the mean excess function does not exist for Pareto dfs with shape param-
eter α < 1 and due to the fact that en is an inaccurate estimate of eF if α > 1 is
close to 1 (for a discussion see [16], page 150), a trimmed version
eF,p(u) = 1
p
 (F (u))−1(p)
−∞
x dF (u)(x)
is of interest, where 0 < p < 1. There is no trimming if p = 1. Trimmed mean
excess functions of GP dfs again form a straight line3. In the Pareto case we have
eW1,α,p(u) =
1
p
 1
1−p
y−1/αdy −1

u,
u > 1.
Moreover, eW2,α,p is strictly decreasing and eW0,1,p is a constant. A sample
version is also obtained by plugging in the sample df.
3Drees, H. and Reiss, R.–D. (1996). Residual life functionals at great age. Commun.
Statist.–Theory Meth. 25, 823–835.

2.2. Excess and Hazard Functions
53
Median Excess Functions
The median excess function is deﬁned by the medians of excess dfs. We have
mF (u) := (F (u))−1(1/2).
The median excess function has properties corresponding to those of the trimmed
mean excess function. For standard Pareto dfs W1,α, we have
mW1,α(u) = (21/α −1)u,
u > 1.
Once again, a sample version is obtained by plugging in the sample df. A formula
corresponding to (2.21) also holds for trimmed mean and median excess functions.
Hazard Functions
The hazard function hF of a df F with density f is
hF (t) = f(t)/(1 −F(t)),
t < ω(F).
Note that hF is the derivative of the cumulative hazard function
HF (t) = −log(1 −F(t)),
t < ω(F).
The value hF (t) is also called hazard rate or mortality rate at age t. It is the
right–hand derivative taken at zero of the residual life df F (t) at age t. We have
F (t)(x) ≈hF (t)x
for small x. Recall that F (t)(x) is the probability that the remaining life time is less
than the instant x given survival at age t. Therefore, one gets the interpretation
that the mortality rate is approximately equal to the probability that the remaining
life is less than 1 given survival at age t. Check that
hFµ,σ(t) = hF ((t −µ)/σ) /σ,
where µ and σ denote the location and scale parameters.
We include a list of the hazard functions of GP, converse Weibull and converse
Gumbel dfs.
Exponential (GP0):
hW0(t) = 1,
t > 0;
Pareto (GP1), α > 1:
hW1,α(t) = α/t,
t > 1;
Beta (GP2), α < 0:
hW2,α(t) = α/t,
−1 ≤t ≤0.
In the γ–representation, we have:

54
2. Diagnostic Tools
GP:
hWγ(t) =
1
1+γt
for
⎧
⎨
⎩
0 < t,
0 ≤γ,
if
0 < t < 1/|γ|,
γ < 0;
Converse Weibull, α < 0:
h 	G2,α(t) = |α|t−(1+α),
t > 0;
Converse Gumbel:
h 	G0(t) = et.
γ = 0
γ = 1
γ = −1
1
2
1
2
3
4
Fig. 2.8. Hazard functions of GP
dfs for γ = −1, 0, 1.
The hazard function of the converse Weibull df 	G2,α is a straight, increasing
line if α = −2. Recall that 	G2,−2 is the Rayleigh df. Moreover, one can easily
check that the converse Gumbel dfs 	G0,µ,σ are the only ones that satisfy the
famous Gompertz law postulating a mortality rate of the form4
h(x) = aebx.
Hence 	G0 is also called Gompertz df. Because
1 −F(t) = exp

−
 t
α(F )
hF (x) dx

,
α(F) < t < ω(F),
(2.26)
we see that the survivor function and, thus, also the df F can be regained from
the hazard function.
The hazard function and, therefore, the survivor function can be written
in terms of the mean excess function. By multiplying both sides in (2.23) with
1 −F(u) and taking derivatives, one obtains
hF (t) = 1 + e′
F (t)
eF (t)
,
α(F) < t < ω(F).
(2.27)
4Gompertz, B. (1825). On the nature of the function expressive of the law of human
mortality etc. Phil. Trans. Roy. Soc. 115, 513–585.

2.2. Excess and Hazard Functions
55
Combining this with (2.26), we have
1 −F(t) = exp

−
 t
α(F )
1 + e′
F (x)
eF(x)
dx

,
α(F) < t < ω(F).
(2.28)
Sample Hazard Functions
The sample hazard function
hn,b(t) =
fn,b(t)
1 −Fn,b(t)
,
t < xn:n,
is an estimator of the hazard function hF , where fn,b is the kernel density in (2.14),
and Fn,b is the kernel estimator of the df in (2.17). Note that Fn,b may be replaced
by the sample df Fn. The quality of the sample hazard function as an estimate of
the hazard function heavily depends on the choice of the bandwidth b.
It can be advantageous to consider left or right–bounded versions of the
sample hazard function as it was done for the kernel density itself.
For grouped data, again with frequencies n(j) in cells [tj, tj+1), the sample
hazard function is deﬁned by means of the histogram. Consequently,
hn(t) =
n(j)
(tj+1 −tj) 
i≥j n(i),
tj ≤t < tj+1.
Another version of the sample hazard function may be obtained by taking
the derivative of a smoothed sample cumulative hazard function log(1 −Fn).
Reciprocal Hazard Functions, a Von Mises Condition
The reciprocal 1/hW of the hazard function of a GP df W is a straight line which
is clearly of interest for visual investigations. Moreover, this observation leads to a
condition (due to von Mises) that guarantees that a df belongs to the max and the
pot–domain of an EV and GP distribution. Thus, the reciprocal hazard function
is also of theoretical interest.
Reciprocal hazard and mean excess functions are related to each other. We
have
eWi,α = 1/((α −1)αhWi,α),
(2.29)
if i = 1 and α > 1 or i = 2, and
eWγ = 1/((1 −γ)hWγ),
(2.30)
if γ < 1. Note that eW0 = hW0 = 1.
The reciprocal hazard function was also mentioned due to technical reasons.
Observe that the reciprocal hazard function of a GP df Wγ,µ,σ satisﬁes
1
hWγ,µ,σ(t) = 1 −Wγ,µ,σ(t)
wγ,µ,σ(t)
= σ + γ(t −µ).

56
2. Diagnostic Tools
Therefore, the ﬁrst derivative of the reciprocal hazard function is equal to γ. We
have
1 −Wγ,µ,σ
wγ,µ,σ
′
= γ
(2.31)
on the support of Wγ,µ,σ. From (2.26) deduce that GP dfs are the only dfs which
possess this property.
If this condition is approximately satisﬁed for large t by some df F, then F
belongs to the max and pot–domain of attraction of the EV and GP df with the
given parameter γ. More precisely, the
von Mises condition:
lim
x→ω(F )
1 −F
f
′
(x) = γ
(2.32)
is suﬃcient for the relations in (1.27) and (1.46), namely
• Gγ is the limiting df of F n(bn + anx) as n →∞, and
• Wγ is the limiting df of F [t](bt + atx) as t →ω(F)
for certain normalizing constants.
Check, for example, that the normal df Φ satisﬁes condition (2.32) for γ = 0
and, hence, the Gumbel df is the limiting df of maxima of normal random variables.
Under condition (2.32), one may take the following constants an and bn if
the standard EV dfs in the α–parameterization are taken as limiting dfs.
G0:
bn = F −1(1 −1/n),
an = 1/(nf(bn));
G1,α:
bn = 0,
an = F −1(1 −1/n),
G2,α:
bn = ω(F),
an = ω(F) −F −1(1 −1/n).
(2.33)
For iid random variables X1, . . . , Xn with common df F, the expected number
of exceedances over F −1(1 −1/n) is equal to one. For GP dfs F = Wi,α, one
obtains in (2.33) the standardizing constants under which the max–stability of the
standard EV dfs Gi,α holds, cf. also (1.31).
2.3
Fitting Parametric Distributions to Data
In this section we visualize data by parametric dfs or densities. Thereby, one
may also visually control the validity of a parametric model. This idea will be
exempliﬁed for EV, GP and, in addition, for Poisson and normal distributions.

2.3. Fitting Parametric Distributions to Data
57
Fitting an Extreme Value Distribution to Maxima
Let x = (x1, . . . , xn) be the vector of maxima xi of blocks of size m as in (1.5).
Recall from (2.2) that the sample df Fn(x; · ) based on x is approximately equal
to the underlying df F m if the number n of maxima is suﬃciently large.
Combining (2.2) and (1.27) one gets
Fn(x; · ) ≈F m ≈Gγ,µm,σm,
if n and m are suﬃciently large. Therefore, an EV df can be ﬁtted to the sample
df based on maxima.
It is likely that one of our basic assumptions is violated if none of the EV dfs
ﬁts to the sample df. Correspondingly, ﬁt an EV density to the histogram or kernel
density fn based on the xi. An eﬃcient, interactive software package is needed to
carry out a visual selection of the parameters.
Of course, the selection of the parameters is done in some subjective manner,
yet one may follow certain principles.
• If the selection is based on the sample df, one may choose the parametric df
by minimizing the maximum deviation between the curves.
• If the selection is based on a kernel density, then we suggest to single out a
parametric density so that the area between both curves is minimized; this
corresponds to minimizing the L1–distance.
• There is a strong dependence on the smoothing parameter in the latter case
if the selection is based on the maximum deviation between the parametric
and the sample curve.
Fitting a Generalized Pareto Distribution to Exceedances
Let Fk(y; · ) be the sample exceedance df, cf. page 41, based on the exceedances
y1, . . . , yk over the threshold u. Combining (2.4) and (1.46) one gets
Fk(y; · ) ≈F [u] ≈Wγ,u,σ,
(2.34)
if k and u are suﬃciently large. Therefore, a GP df can be ﬁtted to the sample
exceedance df. Once again, it is likely that one of our basic assumptions is violated
if none of the GP dfs ﬁts to the sample exceedance df.
If excesses y′
i = yi −u are taken in place of the exceedances yi, then a GP df
with location parameter (left endpoint) equal to zero must be ﬁtted to the data.
Likewise, one may ﬁt a Pareto df W1,α,µ,σ with left endpoint µ + σ = u to the
sample exceedance df Fk(y; · ).

58
2. Diagnostic Tools
Fitting a Generalized Pareto Distribution to the Original Data
In the preceding lines, a GP df was ﬁtted to the sample exceedance df. By changing
the location and scale parameters a GP df can be ﬁtted to the upper tail of the
original sample df and, therefore, to the original data.
This is an application of (1.49) with F [u] and F replaced by the sample
exceedance df Fk(y; · ) and and sample df Fn(x; · ). If (2.34) is valid, then (1.49)
yields
Wγ,˜µ,˜σ(x) ≈Fn(x; x ),
x ≥u,
(2.35)
where
˜σ = σ(k/n)γ
(2.36)
and
˜µ = u −σ

1 −(k/n)γ
/γ .
(2.37)
Notice that Fn(x; u) = 1 −k/n and k is the number of exceedances.
There is a unique relation between the two pairs of parameters µ, σ and ˜µ,
˜σ determined by the equations W [u]
γ,˜µ,˜σ = Wγ,u,σ and Wγ,˜µ,˜σ(u) = 1 −k/n.
Fig. 2.9 illustrates the two diﬀerent approaches of ﬁtting a density to the
exceedances and to the original data. The illustration on the right–hand side es-
sentially shows a magniﬁcation of the curves on the left–hand side right of u.
Fig. 2.9. (left.) Fitting a GP density (solid) to the upper tail of a kernel density (dotted)
based on the original xi. (right.) Fitting a GP density (solid) to a kernel density (dotted)
based on exceedances yi over u.
Let W1,α,µ,σ be a Pareto df with µ + σ = u which is the left endpoint. If
this Pareto df ﬁts to the sample exceedance df Fk(y; · ), then W1,α,˜µ,˜σ, determined
by W [u]
1,α,˜µ,˜σ = W1,α,µ,σ and W1,α,˜µ,˜σ(u) = 1 −k/n, ﬁts to the original sample df
Fn(x; · ). Moreover, ˜µ = µ and ˜σ = σ(k/n)1/α.

2.3. Fitting Parametric Distributions to Data
59
Fitting a Poisson Distribution to Discrete Data
Let x1, . . . , xn be governed by a Poisson distribution Pλ. Remember from (2.12)
that pn(j) = n(j)/n ≈Pλ{j}, where n(j) is again the number of data being equal
to j. Therefore, a Poisson hypothesis should be rejected if none of the Poisson
histograms Pλ{j} visually ﬁts to the sample histogram pn(j).
Local Fitting of a Normal Distribution
The preceding arguments can also be applied in the Gaussian case, if (1.27) is
replaced by (1.58). Select location and scale parameters µ and σ so that Φµ,σ ≈
Fn(y; · ) in order to get a visualization of sums y1, . . . , yn via a normal df.
The procedure of ﬁtting a GP df to upper extremes may be regarded as a
local statistical modeling. Such a local approach can be employed in the Gaussian
setting as well. For example, ﬁt the main component of the normal mixture in the
subsequent Fig. 2.10 to the central observations (visualized by a kernel density).
To some extent, the local ﬁtting of parametric distributions to data corre-
sponds to nonparametric density estimation, where the estimate of the density at
a ﬁxed point x is just based on data around x.
Mixtures of Normal Distributions
If a parametric model as dealt with above is untenable, then a modiﬁcation of this
approach, such as dealing with a mixture, can be beneﬁcial.
Example 2.3.1. (Mixture of Normal Distributions.) We continue the analysis of 135
data of TV watching in hours per week, cf. Example 2.1.2. From the form of the kernel
density (Epanechnikov kernel and bandwidth b = 5) in Fig. 2.4, we already know that a
normal density cannot be ﬁtted to the data. From Fig. 2.10, we see that a better ﬁt is
obtained by a normal mixture. This modeling is still unsatisfactory in the upper tail of
the distribution.
hours per week
20
40
60
0.02
0.04
Fig. 2.10.
Normal mixture (dot-
ted) 0.875ϕ1 + 0.125ϕ2, where
ϕ1(x) = ϕ((x −12.4)/6.2)/6.2,
ϕ2(x) = ϕ((x −27)/3.4)/3.4,
and a left–bounded kernel density.

60
2. Diagnostic Tools
If the means of the two normal components are slightly closer together, then the
heterogeneity of the data becomes less visible. Then, a representation of the data by
means of a right–skewed distribution, such as the Gumbel distribution (cf. Fig. 1.3),
would also be acceptable.
Conversely, if a Gumbel modeling for certain data seems to be adequate, then
a mixture distribution cannot be excluded. It depends on the posed question or the
additional information about the data as to which type of modeling (in other words,
hypothesis formulation) is preferable.
It is likely that mixtures of normal distributions having identical means are
of higher interest. Recall from page 31 that such distributions are fat–tailed, that
is, the kurtosis is lager than the kurtosis 3 of a normal distribution.
Example 2.3.2. (Fitting a Mixture of Two Normal Distributions to Financial Data.)
Parametric densities are ﬁtted to a kernel density—with bandwidth b = 0.002—based on
the centered ﬁnancial data which are displayed in Fig. 1.1.
A normal mixture—a maximum likelihood estimate—with mixing parameter d =
0.362 and scale parameters σ1 = 0.00285 and σ2 = 0.00898 is hardly distinguishable from
the kernel density (both represented by a solid line).
-0.02
-0.01
0.01
0.02
50
100
Fig. 2.11. Fitting normal (dot-
ted, dashed) and a mixture of
two normal densities (solid) to ﬁ-
nancial data.
Two normal densities with σ = 0.0039 (dotted) and σ = 0.0065 (dashed) are
included. The ﬁrst one ﬁts well to the kernel density in the center, yet there is a signiﬁcant
deviation in the tails. The second one—determined by the sample standard deviation—
strongly deviates from the kernel density in the center.
In Chapter 16, page 379, we also a apply maximum likelihood estimators in
statistical models of Student and sum–stable distributions, cf. Sections 6.3 and
6.4, to these data.

2.4. Q–Q and P–P Plots
61
The Art of Statistical Modeling
One major goal of this book is to select distributions which simultaneously ﬁt to
the central as well as to the extreme data. The appropriate insight can be gained
by using the available tools such as Q–Q plots (subsequent Section 2.4) and sample
mean excess functions (Section 2.2).
A parametric modeling can be useful in reducing the variance of an estimation
procedure. If the parametric model is incorrect, then one must take a bias into
account. An optimal choice of the parametric model is achieved if there is a certain
balance between the variance and the bias. In Section 6.6 much eﬀort is invested
to reduce the bias of estimators within the extreme value setting by introducing
higher order conditions.
We believe that visual procedures are preferable to automatic ones in many
situations. In that context, we also cite an interesting argument (translated from
German) in Pruscha5, page 62: “For larger sample sizes n, visual diagnostic tools
can be preferable to goodness–of–ﬁt tests. A parametric hypothesis will be rejected
for larger n, even if the deviation of this hypothesis is negligible (from a practical
viewpoint) due to the high power of test procedures.”
2.4
Q–Q and P–P Plots
Q–Q plots are usually deﬁned for location and scale parameter families and, there-
fore, we ﬁrst review the main ideas for such models. An extension to EV and GP
models is obtained by employing estimators of the shape parameter.
Q–Q Plots in Location and Scale Parameter Families
Assume that the data x1, . . . , xn are governed by a df
Fµ,σ(x) = F((x −µ)/σ)
with location and scale parameters µ and σ > 0. Thus, F = F0,1 is the standard
version. Values F −1
n (q) of the sample qf will be plotted against F −1(q). More
precisely, one plots the points

F −1(qi), F −1
n (qi)

,
i = 1, . . . , n,
where qi = i/(n+1). Notice that location and scale parameters need not be selected
in advance when a Q–Q plot is applied to the data. Because F −1
n (qi) = xi:n, the
relationship (2.9) between the sample qf and the underlying qf yields
F −1
n (qi) ≈F −1
µ,σ(qi) = µ + σF −1(qi),
(2.38)
5 Pruscha, H. (1989). Angewandte Methoden der Mathematischen Statistik. Teubner,
Stuttgart.

62
2. Diagnostic Tools
and, hence, the Q–Q plot of points

F −1(qi), xi:n

,
i = 1, . . . , n,
(2.39)
is close to the graph (x, µ + σx). The Q–Q plot can be visualized by a scatterplot,
whereby a linear interpolation may be added. Apparently, the intercept and the
slope of the Q–Q plot provide estimates of µ and σ. Another frequently taken
choice of qi is (i −0.5)/n.
The selected location/scale parameter family is untenable if the deviation of
the Q–Q plot from a straight line is too strong.
Example 2.4.1. (Continuation of Example 2.1.2 about TV Data.) A greater problem in
Example 2.1.2 was the selection of an appropriate bandwidth parameter. In conjunction
with Q–Q plots, one must not choose a bandwidth, but a parametric model. Subsequently,
we selected the normal location and scale parameter family. A straight line will be ﬁtted
to that part of the Q–Q plot which represents the bulk of the data.
hours
-3
-2
-1
1
2
3
20
40
Fig. 2.12. Normal Q–Q plot based on
135 TV data and a straight line visu-
ally ﬁtted to the Q–Q plot.
The normal Q–Q plot is suﬃciently close to a straight line below the quantity of
18 hours, yet there is a stronger deviation above this threshold.
Of course, one may also check whether the data were generated under a
speciﬁc df F. This is just the case when the actual df is the standard df in the
preceding considerations. Then, the Q–Q plot must be close to the main diagonal
in the plane. The disadvantage of Q–Q plots is that the shape of the selected
parametric distribution is no longer visible.
Q–Q Plots in Extreme Value and Generalized Pareto Models
In EV and GP models, one must keep in mind that there is an additional pa-
rameter, namely the shape parameter, besides the location and scale parameters.
We suggest applying a Q–Q plot with the unknown shape parameter having been

2.4. Q–Q and P–P Plots
63
replaced by an estimate. If there is a stronger deviation of the Q–Q plot from a
straight line, then either the estimate of the shape parameter is inaccurate or the
model selection is untenable.
P–P Plots
We introduce the P–P plot in conjunction with a location and scale parameter
family of dfs Fµ,σ. The P–P plot is given by

qi, F

(xi:n −µn)/σn

,
i = 1, . . . , n,
where µn and σn are estimates of the location and scale parameters (such estimates
will be presented in the next section).
Because
F

(xi:n −µn)/σn

= Fµn,σn
 F −1
n (qi)

,
a strong deviation of the P–P plot from the main diagonal in the unit square
indicates that the given model is incorrect (or the estimates of the location and
scale parameters are inaccurate). The values of the P–P plot will be close to one
(or zero) and, thus, close to the diagonal in the upper (or lower) tail, even if the
choice of the model is wrong.
Further Remarks About Regularization and Smoothing
Let us continue our permanent discussion about the usefulness of sample dfs, qfs,
densities, etc. for the visualization of data. A remark made about regularization
in Section 2.1 is also relevant to Q–Q and P–P plots.
By applying the sample qf, sample excess functions, the sample hazard func-
tion or the Q–Q plot, one is able to extract the information inherent in the data in
a suitably way by achieving a compromise between the following two requirements:
• the random ﬂuctuation of the data must be reduced, and
• special features and clues contained in the data must be exhibited.
We believe that this goal is not achieved in the same manner by dfs and related
tools such as the P–P plot because there is an oversmoothing particularly in the
upper and lower tails of the distribution.
Exaggerating a bit, one may say that one should apply the sample df Fn (or,
likewise, the survivor function 1 −Fn) and the P–P plot if one wants to justify
a hypothesis visually. The other tools are preferable whenever a critical attitude
towards the modeling is adopted.

64
2. Diagnostic Tools
2.5
Trends, Seasonality and Autocorrelation
This section concerns several aspects of exploratory time series analysis. Many
observations recorded at speciﬁed times exhibit a dependence on time. This de-
pendence may be caused, for example, by a certain tendency in the climate, an
increasing population, inﬂation, or seasonal eﬀects.
We collect and discuss some statistical procedures known from regression and
time series analysis for measuring and removing a trend or a seasonal component in
a series of data. The autocorrelation function is also on the agenda. After having
removed a trend or a seasonal component from the data, one obtains residuals
which may be dealt with by the tools provided in foregoing sections.
The Linear Least Squares Method
A trend is a long–term change of a series of data. First let a linear tendency be
visible, cf. Example 2.1.3, that will be captured and removed by a least squares
line. Thus, a straight line is ﬁtted to the points (t1, y1), . . . , (tn, yn) in a scatter-
plot by applying the least squares method. The ti need not be integer–valued or
equidistant.
The least squares line
s(t; β0, β1) = β0 + β1t
with regression slope β1 and intercept β0 is chosen such that the cumulated squared
distances

i≤n
(yi −β0 −β1ti)2
(2.40)
between the values yi and s(ti; β0, β1) at ti are minimal. The well–known solutions
are the estimates
β1,n =

i≤n(yi −¯y)(ti −¯t)

i≤n(ti −¯t)2
of the regression slope, and
β0,n = ¯y −β1,n¯t
of the intercept, where ¯y and ¯t are again the averages of the yi and ti.
One gets a decomposition
yi = s(ti; β0,n, β1,n) + xi
(2.41)
where s(ti; β0,n, β1,n) = β0,n + β1,nti represents a linear part in the data and the
xi are the residuals which ﬂuctuate irregularly around zero.

2.5. Trends, Seasonality and Autocorrelation
65
Nonlinear (Polynomial) Least Squares Methods
If one recognizes a nonlinear trend in the scatterplot of points (ti, yi), one may em-
ploy parametric trend functions s(t; β0, . . . , βp), where the parameters β0, . . . , βp
are selected such that

i≤n

yi −s(ti; β0, . . . , βp)
2
(2.42)
is minimal. There is a greater variety of parametric trend functions. For example,
the linear approach can be extended to polynomials
s(t; β0, . . . , βp) = β0 +

j≤p
βjtj
of degree p. For p = 0 one gets the sample mean which determines a straight line.
Explicit solutions β0,n, . . . , βp,n to the least squares minimization for polynomials
can be obtained within the bounds of multiple, linear regression.
Parametric Regression for a Fixed Design
We reformulate the preceding considerations within a stochastic framework. Let
Yi = s(ti; β0, . . . , βp) + εi,
where the random variable Yi is observable at time (at the position) ti, and εi is
a random residual with expectation Eεi = 0. Thus, Yi is a random variable with
expectation
EYi = s(ti; β0, . . . , βp).
Notice that the yi and xi in (2.41) may be regarded as realizations of Yi and εi.
The least squares solutions βj,n—within the polynomial framework—provide
unbiased estimators of the unknown parameters βj and, therefore,
mn(i) = s(ti; β0,n, . . . , βp,n)
(2.43)
is an unbiased estimator of the expectation E(Yi).
It suggests itself also to employ certain averages to estimate such expecta-
tions. Below, we deal with moving averages of the Yj pertaining to adjacent points
tj of ti. These moving averages provide nonparametric estimates of a parametric
or nonparametric trend function.
Parametric regression for a random design will be studied in Section 8.1.
Moving Averages, Nonparametric Estimation of a Trend
To eliminate—or, at least, to reduce—the irregular ﬂuctuation of measurements
yi in a nonparametric manner one may average those yj pertaining to adjacent
points tj of ti. These averages capture a trend in the yi.

66
2. Diagnostic Tools
For example, take the Nadaraya–Watson moving average
mn(i) =
1
Kn

j≤n
I(ti −b ≤tj ≤ti + b)yj
(2.44)
at the position ti, where
Kn =

j≤n
I(ti −b ≤tj ≤ti + b)
is the number of points tj in the interval of length 2b around ti. The value b > 0
is called a bandwidth.
Generally, averages can be expressed by
mn(i) =

j≤n
k
tj −ti
b

yj
 
j≤n
k
tj −ti
b

(2.45)
at the positions ti, where k is a kernel such that

k(t) dt = 1 and b > 0 is a
bandwidth (cf. also page 44, where, e.g., the Epanechnikov kernel is introduced).
Usually, k is taken symmetrical around zero. In (2.44) there is the uniform kernel
k(t) = 0.5 × I(−1 ≤t ≤1).
Again one gets a decomposition
yi = mn(i) + xi,
where the xi are the residuals which ﬂuctuate irregularly around zero.
Special kernels (e.g., truncated versions) must be taken at corner points. We
suggest to employ truncated versions of the given kernel.
Subsequently, we assume that the positions ti are arranged in increasing order
and, in addition, the ti are equidistant, with ti = i or ti = i/n as special cases.
Then, (2.45) can be written
mn(i) =

j
ajyi+j,
(2.46)
for certain weights aj satisfying aj = a−j and 
j aj = 1. In the case of the uniform
kernel k(t) = 0.5 × I(−1 ≤t ≤1), one obtains the moving average
mn(i) =
1
2v + 1

|j|≤v
yi+j.
(2.47)
A slightly modiﬁed version is
mn(i) = 1
2v
1
2yi−v +

|j|≤v−1
yi+j + 1
2yi+v

.
(2.48)
There is a trade–oﬀbetween the two requirements that

2.5. Trends, Seasonality and Autocorrelation
67
• the irregular ﬂuctuation should be reduced,
• the long term variation in the data such as a quadratic or cubic tendency
should not be distorted by oversmoothing the data.
A balance between these two requirements can be gained by an appropriate
choice of the bandwidth b or of the numbers v in (2.47) and (2.48).
In addition, it can be useful to employ kernels as in (2.15) or (2.16) with

k(t)t2 dt = 0
in order to preserve a quadratic or cubic tendency in the data. This corresponds
to the condition 
j ajj2 = 0 for the weights aj in (2.47), if x(i) −x(i −1) = 1.
An example is provided by Spencer’s 15 point moving average, where the weights
are
(a0, a1, . . . , a7) = (74, 67, 46, 21, 3, −5, −6, −3)/320.
In Fig. 2.13 (right), Spencer’s moving average is applied to 200 iid standard normal
data.
200
-2
2
200
-2
2
Fig. 2.13. (left.) Scatterplot of 200 standard normal data. (right.) Spencer’s 15 point
moving average of these data.
The strong ﬂuctuation in the normal data cannot be smoothed appropriately
by Spencer’s moving average. Apparently, the choice of the number of points is
more important than the selection of the kernel.
Example 2.5.1. (Maximum Daily Temperature at Death Valley.) We consider the maxi-
mum daily temperatures at Furnace Creek, Death Valley National Park, from Aug. 1994
to Aug. 1995, by which the measurements for Dec. 1994 were not available.
For the missing data, we ﬁlled in values deduced from a quadratic least squares
procedure applied to points around the gap. In Fig. 2.14 (left), the maximum daily
temperature is plotted against the day of the measurement.

68
2. Diagnostic Tools
time (days)
temperature
0
100
200
300
400
50
75
100
125
time (days)
temperature
0
100
200
300
400
50
75
100
125
Fig. 2.14. (left.) Scatterplot of maximum daily temperature measured in Fahrenheit.
(right.) Moving average (41 points) employing the Epanechnikov kernel.
Relative to the given time span of 13 months, we observe a long term variation (a
trend) in the data that can be removed by means of a moving average. This data set is
stored in the ﬁle et–deval.dat.
Given a time span of several years, the variation in the preceding temperature
data must be interpreted as a seasonal eﬀect (dealt with below). Thus, the time
horizon greatly inﬂuences the methodology of handling a series of data.
Our general references for this section are the mathematically oriented book
[5] and [26] for economic time series, yet the application of kernels and the question
of an appropriate choice of the number of points are not dealt with in these books.
Modiﬁcations of Moving Averages
In certain applications, it is desirable to employ modiﬁed versions of the averages
as introduced in the preceding lines. Other characteristics of the sample, such as
medians and quantiles, are also also of interest. We give some details.
• (One–Sided Moving Averages.) This is the construction employed for the
ordinary moving average at the upper corner point.
• (Moving Averages With a Random Bandwidth Chosen by Nearest Neigh-
bors.) If there is not a grid of equidistant points, as, e.g., a full grid of
integers, then it can be advisable to employ a nearest neighbor method. A
random bandwidth b is determined in the following manner. Given a non-
negative integer r, let b be the minimum of the distances between the ﬁxed
point ti, where the moving average is evaluated, and its rth upper and, re-
spectively, rth lower neighbor tj. Near to the upper or lower corner point,
one merely evaluates the distance to the rth lower or, respectively, the rth
upper neighbor. Thus, one is averaging over r + 1 to 2r + 1 values yj.

2.5. Trends, Seasonality and Autocorrelation
69
• (Local Weighted Regression.) As before choose neighboring tj of a ﬁxed value
ti and carry out a weighted local least squares (lowess) procedure based on
these neighbors, cf. the book [8] by Cleveland.
• (Moving Medians.) Alternatively, one may use moving medians of the form

ti, med|j|≤k yi+j

to reduce the ﬂuctuation of a time series. This can be
necessary when the observations come from a heavy–tailed distribution.
• (Moving Quantiles.) Later we will be particularly interested in moving higher
q–quantiles in conjunction with series of log–returns of ﬁnancial data (cf. Fig.
1.1 and Chapter 16).
In the latter context, we also use the parametric approach for estimating a
higher quantile.
The Seasonal Component
If the moving average exhibits a variation that is annual in period—in other words,
seasonal—then a reﬁned decomposition of the measurements yi is suggested. For
simplicity, let ti = i for i = 1, . . . , n, where n = lp and l, p are the number and
length of periods.
Now we also single out a periodic component sn, i = 1, . . . , p, satisfying
sn(i + jp) = sn(i),
j = 0, . . . , l −1; i = 1, . . . , p,
(2.49)
and 
i≤p sn(i) = 0. Thus, we have a decomposition in mind
yi = mn(i) + sn(i) + xi,
(2.50)
where the mn(i) represent the smooth trend components and the xi are the resid-
uals. This is done in three steps.
• (Preliminary Determination of a Trend Component.) By applying moving
averages as introduced in (2.47) or (2.48) with p = 2v +1 or p = 2v, one gets
a preliminary trend component 	mn(i) that is not aﬀected by any periodic
component.
• (Determination of a Period (Cycle).) From the residuals yi −	mn(i), single
out a periodic component determined by
˜sn(i) = 1
l
l−1

j=0

yi+jp −	mn(i + jp)

,
i = 1, . . . , p,
or
sn(i) = ˜sn(i) −1
p

j≤p
˜sn(j),
i = 1, . . . , p,

70
2. Diagnostic Tools
whereby the second version is preferable because it satisﬁes the additional
requirement 
i≤p sn(i) = 0.
If the number of periods is small, yet one can postulate a smooth periodic
component, then it is plausible to apply the preceding operations to a slightly
smoothed version of the yi −	mn(i).
• (Final Determination of the Trend Component.) Finally, compute a moving
average or a parametric trend function mn(i) based on the deseasonalized
data yi −sn(i). One gets the residuals
xi = yi −sn(i) −mn(i)
(2.51)
by combining these steps,
If the detrended and deseasonalized data x1, . . . , xn are realizations of random
variables X1, . . . , Xn with expectation EXi = 0, and X1, . . . , Xn are uncorrelated
or independent, then standard statistical procedures become applicable.
Example 2.5.2. (Water Levels of the Moselle River.) We examine the water levels (in
meters) of the Moselle River measured in Zeltingen from Nov. 1964 to Dec. 1977 and
from Jan. 1981 to Jan. 1996 (stored in the ﬁle ht–mosel.dat). The measurements from
the years 1978–1980 are missing. Since 1988, the measurements are daily maxima. Before
1988, there was one measurement each day.
Fig. 2.15. Scatterplot of the Moselle River levels from Nov. 1964 to Jan. 1996, with a
gap of the years 1978–1980 due to missing data.
Of course, the missing data from the years 1978 to 1980 caused an additional prob-
lem. To simplify the matter, this gap was ﬁlled by corresponding neighboring measure-
ments. The gap from Jan. 1978 to June 1979 was ﬁlled with the values from Jan. 1976 to
June 1977, and, likewise, the gap from July 1979 to Dec. 1980 was ﬁlled with the values
from July 1981 to Dec. 1982 (this completed data set is stored in the ﬁle ht–moﬁl.dat).

2.5. Trends, Seasonality and Autocorrelation
71
In Fig. 2.16, we see the estimated seasonal component with and without smoothing.
The smoothing reduces the irregular ﬂuctuation to some extent.
time (days)
100
300
500
-1
1
time (days)
100
300
500
-1
1
Fig. 2.16. (left.) Seasonal component without smoothing. (right.) Seasonal component
with an additional 25 points smoothing.
The seasonal component attains its maximum values within the period from the
end of December to mid–February. There is another remarkable peak in April.
The Moselle River data will be analyzed more intensively in Chapter 14.
One must cope with the facts that these data are serial correlated and seasonally
varying in the variance (heterosketastic) and in higher moments.
Serial Analysis of Stationary Data: the Autocovariance Function
For random variables X, Y with EX2, EY 2 < ∞the covariance is
Cov(X, Y ) = E

(X −EX)(Y −EY )

.
(2.52)
Loosely speaking, there is a tendency that the random variables X and Y simulta-
neously exceed or fall below their expectations EX and EY , if there is a positive
covariance.
The random variables X and Y are uncorrelated if Cov(X, Y ) = 0. Recall
that independent random variables are uncorrelated, yet the converse conclusion
is not valid.
The pertaining sample covariance is
sx,y,n =
1
n −1

i≤n
(xi −¯x)(yi −¯y),
(2.53)
where ¯x and ¯y are the sample means of the data x1, . . . , xn and y1, . . . , yn.
Subsequently, we study the serial dependence structure of a detrended and
deseasonalized time series by means of the autocovariance function. A sequence

72
2. Diagnostic Tools
X1, . . . , Xn is stationary if EXi = EX1, EX2
i < ∞and the covariances
Cov(Xi, Xi+h) = Cov(X1, X1+h) =: r(h),
i + h ≤n,
(2.54)
merely depend on the time lag h. One also speaks of a weakly or covariance station-
ary series. The function r, with the time lag h as a variable, is the autocovariance
function. It is particularly assumed that the expectations and variances are con-
stant. The autocovariance function r(h) can be estimated by the sample version
ˆrn(h) = 1
n

i≤n−h
(xi −¯x)(xi+h −¯x),
(2.55)
where ¯x = 
i≤n xi/n is the sample mean. The estimation is accurate if n −h is
suﬃciently large. Notice that ˆrn(0) is the sample variance with the factor 1/(n−1)
replaced by 1/n to reduce the random ﬂuctuation for larger lags h.
The ratio
ρ(h) = r(h)/r(0)
(2.56)
is the autocorrelation function. The sample autocorrelation function ρ is
ρn(h) = ˆrn(h)/ˆrn(0).
(2.57)
In Fig. 2.17, the sample autocorrelations are represented by bars.
time lag
10
30
0.0
1
Fig. 2.17. Sample autocorrela-
tions of 200 iid standard ex-
ponential data for lags h =
0, . . . , 40.
Notice that ρn(0) = ρ(0) = 1 and the autocorrelation function attains values
between −1 and 1 under the stationarity condition.
In the case of detrended and deseasonalized data, we may assume that the
expectation is equal to zero. An estimate of the autocovariance function r is
ˆrn(h) = 1
n

i≤n−h
xixi+h .

2.5. Trends, Seasonality and Autocorrelation
73
If ˆrn(h) is suﬃciently close to zero for h ≥q, it is legitimate to assume
that the residuals x1, x1+q, x1+2q, . . . are realizations of uncorrelated or indepen-
dent random variables. Then, standard statistical procedures for uncorrelated or
independent random variables become applicable to the subsequence.
In conjunction with a non–trivial autocorrelation function, one speaks of a
time series. As a theoretical example of such a series, we mention a Gaussian AR(1)
time series {Xi}.
Example 2.5.3. (Gaussian AR(1) Series.) For 0 ≤d ≤1, let
X1
=
Y1,
Xk
=
d Xk−1 + (1 −d2)1/2 Yk,
k > 1,
(2.58)
where the Yi are iid standard Gaussian random variables (normal random variables are
often called Gaussian in the time series context).
This is the usual construction when deﬁning standard Gaussian random variables
X1 and X2 with correlation d. Verify that {Xk} is a stationary sequence of standard
Gaussian random variables with autocorrelation function
ρ(h) = E(X1 X1+h) = dh,
h ≥0.
Thus, we have a geometrically decreasing autocorrelation function. Notice that indepen-
dence holds for d = 0 and total dependence for d = 1.
time lag
0
5
10
15
0
1
Fig. 2.18. Theoretical autocor-
relations of Gaussian AR(1) se-
ries with d = 0.8 (solid line)
and sample autocorrelations of
200 Gaussian AR(1) data under
the parameter d = 0.8 (bars) for
time lags h = 0, . . . , 15.
Next we want to illustrate the sample behavior of AR(1) series for uncorrelated
(independent) and strongly correlated random variables. In Fig. 2.19, Gaussian AR(1)
data series of size 200 are plotted which were generated under the correlation parameters
d = 0 and d = 0.95.
The data randomly ﬂuctuate around the x–axis on the left–hand side, yet seem to
exhibit a certain trend on the right–hand side although there is a stationary series.
The discussion about AR(1) series and related time series will be continued
in the Sections 6.2, 16.7 and 16.8.

74
2. Diagnostic Tools
100
200
-2
-1
1
2
100
200
-2
-1
1
2
3
4
Fig. 2.19. Gaussian AR(1) data generated under d = 0 (left) and d = 0.95 (right).
2.6
The Tail Dependence Parameter
The following remarks should be regarded as a preliminary technical introduction
to the concept of tail dependence and tail independence which concerns a certain
property of the bivariate survivor function in the upper tail.
Later on, namely in Section 12.1, tail independence is interpreted as the
property of the upper tail of a bivariate distribution which entails that the com-
ponentwise taken maxima are asymptotically independent. In Section 13.3 there
is also a detailed discussion of other tail independence parameters which measure
the rate at which the tail independence is attained.
At the beginning we introduce a certain tail dependence parameter by means
of the bivariate survivor function. The deﬁnition of a auto–tail–dependence func-
tion ist added which is related to the autocovariance function. In addition, certain
sample versions of the tail dependence parameter and of the auto–tail–dependence
function are suggested.
An Introduction to Tail Dependence
Let X and Y be random variables with the joint df F and univariate marginal dfs
FX and FY . The dependence in the upper tail region of the distribution may be
expressed by the conditional probability
P(Y > y|X > x) = P{X > x, Y > y}
P{X > x}
(2.59)
of Y > y given X > x. Such conditional probabilities were studied by Sibuya6 and
other authors in conjunction with the asymptotic independence of componentwise
taken maxima, see (12.8).
6Sibuya, M. (1960). Bivariate extreme statistics. Ann. Inst. Math. Statist. 11, 195–210.

2.6. The Tail Dependence Parameter
75
The conditional probability in (2.59) is independent of the marginal dfs FX
and FY when x and y are replaced by the quantiles F −1
X (u) and F −1
Y (v).
In the sequel, let u = v. The tail dependence parameter χ(u) of X and Y at
the level u is
χ(u)
=
P

Y > F −1
Y (u)
X > F −1
X (u)

=
P(V > u|U > u),
(2.60)
where
(U, V ) = (FX(X), FY (Y ))
is the pertaining copula random vector with (0, 1)–uniformly distributed marginal
random variables U and V , if FX and FY are continuous (according to the prob-
ability transformation).
It is always understood that u is close to 1, that is, we are dealing with
probabilities in the upper tail region of the joint distribution of X and Y or,
respectively, of U and V . The term
χ = lim
u→1 χ(u)
(2.61)
is addressed as tail dependence parameter. We have tail independence, if χ = 0,
and total tail dependence if χ = 1.
We list some properties of the tail dependence parameters:
• χ(u) and χ are symmetric in X and Y ;
• χ(u) and χ range between zero and one;
• if X and Y are stochastically independent (in the usual sense), then χ(u) =
1 −u and χ = 0; therefore, independence implies tail independence,
• if X = Y , then χ(u) = 1 and χ = 1.
However, one should be aware that tail independence does not imply inde-
pendence. Let
W(x, y) = 1 + x + y
for x, y ≤0 and x + y ≥−1. Then, χ(u) = 0 if u ≥1/2 and, therefore, also χ = 0.
Thus, we have tail independence. Yet, W is the joint df of the totally dependent
rvs Z and −(1 + Z) where Z is on (−1, 0)–uniformly distributed.
The sample version pertaining to χ(u) and χ, based on data (xi, yi) under
the df F, is
χn(u) =
1
n(1 −u)

i≤n
I(xi > x[nu]:n, yi > y[nu]:n).
(2.62)
For a continuation of this topic we refer to the Chapters 12 and 13 which con-
cern multivariate extreme value and multivariate generalized Pareto models. For

76
2. Diagnostic Tools
such multivariate distributions, the dependence structure is of central importance
(besides of the univariate marginals). We particularly refer to Section 12.1, where
we describe the relationship of the tail dependence parameter χ to the Pickands
dependence function D, and to Section 13.3 for the deﬁnition of other tail depen-
dence parameters ¯χ and β which determine the rate at which the tail independence
is attained.
The Auto–Tail–Dependence Function
Next, let X1, . . . , Xn be a series of identically distributed random variables with
common df F. Assume, in addition, that the series has stationary dependencies in
the upper tail in the sense that for i ≤n −h,
P

Xi+h > F −1(u)
Xi > F −1(u)

=
P

X1+h > F −1(u)
X1 > F −1(u)

=:
ρ(u, h)
(2.63)
which deﬁnes the auto–tail–dependence function at the level u. Likewise one deﬁnes
an auto–tail–dependence function ρ(h) by
ρ(h) = lim
u→1 ρ(u, h)
(2.64)
as the limit of ρ(u, h) for u →1.
The auto–tail–dependence functions ρ(u, h) at the level u can be estimated
by the sample version
ρn(u, h) =
1
n(1 −u)

i≤n
I(min(xi, xi+h) > x[nu]:n)
(2.65)
based on data x1, . . . , xn.
It is clear that ρn(u, h) also serves as an estimate of ρ(h), where a suﬃciently
large u must be selected by the statistician. The level (threshold) u should be
suﬃciently large to reduce the bias, and small enough to reduce the variance of
the estimator.
Another auto–tail–dependence function ¯ρ will be introduced in Section 13.3
in conjunction with the tail dependence parameter ¯χ which measures the degree
of tail independence.
2.7
Clustering of Exceedances
In conjunction with extreme values, it has been implicitly assumed in foregoing
sections that the data x1, . . . , xn are generated independently from each other or,
at least, the dependence between the data is negligible.
In Section 2.6 we already started with a preliminary discussion about depen-
dence concepts in conjuction with tail dependence parameters. In that context,

2.7. Clustering of Exceedances
77
another feature will be captured by the phenomenon that stronger dependence
may lead to a clustering of extreme values. The clustering of data is also related to
another important parameter in extreme value theory, namely the extremal index
θ (as indicated in this section, see below, and further mentioned in Section 6.2).
Building Clusters by Runs, the Mean Cluster Size
Given x1, . . . , xn let xi(1), . . . , xi(k) again denote the exceedances over a predeter-
mined threshold u. For some choice of a positive integer r, called the run length,
deﬁne clusters of exceedance times i(j) in the following manner.
Any run of at least r consecutive observations xi below the threshold u sep-
arates two clusters. Hence, there is a minimal gap of length r between two consec-
utive clusters of exceedance times.
time
0.0
25
50
75
100
1
1.5
2
Fig. 2.20. Six clusters above u =
1 with respect to a run length r =
3 of a Gaussian AR(1) series with
d = 0.8 with cluster sizes between
1 and 5.
We introduce the mean cluster size that characterizes the clusters to some
extent. Let n(u, r) denote the number of clusters over u. The mean cluster size,
relative to u and the run length r, is
mcsize(u, r) = k/n(u, r).
(2.66)
The mean cluster size is a useful statistic for describing extreme data besides an
estimate of the tail index.
The Blocks Method
Clusters of exceedance times may also be built by the blocks method (also called
Gumbel method) as mentioned in Section 1.2. Each block containing at least one
exceedance time is treated as a cluster (cf. [16], pages 242–243, and the literature
cited therein for more details).

78
2. Diagnostic Tools
Declustering
To obtain data that correspond more closely to a model of iid random variables,
one may reduce the sample of exceedances to that of the cluster maxima. This
topic will be further pursued within the framework of Poisson counting processes,
see Section 9.2.
The Cluster Size Distribution
The following discussion merely concerns the run length deﬁnition of clusters.
Let |µ| denote the size of a cluster µ of exceedance times for the given ex-
ceedances xi(1), . . . , xi(k) over u according to the run length r. The relative number
of clusters with size j deﬁnes the (dicrete) cluster size distribution Pu,r on the set
1, . . . , k. We have
Pu,r({j}) := |{µ : |µ| = j}|
n(u, r)
,
1 ≤j ≤k.
(2.67)
The mean of the cluster size distribution Pu,r is the mean cluster size mcsize(u, r)
as introduced in (2.66) before.
The illustration in Fig. 2.21 concerns the exceedances over a threshold u = 1
of 4000 Gaussian AR(1) data generated under the correlation coeﬃcient d = 0.8.
The exceedances as well as the exceedance times are plotted. One clearly recognizes
the clustering in the exceedances xi(j) as well as in the exceedance times.
time i/n
0
1
0
2
4
Fig. 2.21. Scatterplot of exceedances over the threshold u = 1 of 4000 Gaussian AR(1)
data with correlation coeﬃcient d = 0.8.

2.7. Clustering of Exceedances
79
We remark that the number of exceedances over 1 (respectively, 2, 3) in Fig.
2.21 is 707 (respectively, 102, 12).
Next, the run length r is chosen equal to 1. In Fig. 2.22 (left), the cluster size
distribution for u = 1 and the (reciprocal) mean cluster sizes for varying u ≥1
are plotted.
cluster size
0
5
10
15
0
0.2
0.4
threshold u
mcsize
1
2
3
4
0
2
4
threshold u
rec. mcsize
1
2
3
4
0
0.5
1
Fig. 2.22. Cluster size distribution for u = 1 and r = 1 (left). Mean cluster sizes (right,
top) and reciprocal mean cluster sizes for u ≥1 and r = 1 (right, bottom).
The reciprocal mean cluster sizes 1/mcsize(u, r) will also be written θ(u, r).
The reciprocal mean cluster sizes θ(u, r) are related to the extremal index, denoted
by θ, which will be discussed in Section 6.2. In Fig. 2.23, θ(·, r) is plotted against
the threshold u for run lengths r = 1, 2, 5, 10.
threshold u
rec. mcsize
r=1
r=10
1
2
3
4
0
0.5
1
Fig. 2.23. Reciprocal mean
cluster sizes for u ≥1 and
r = 1, 2, 5, 10.
We also include plots of the reciprocal mean cluster sizes θ(·, r) of Gaussian

80
2. Diagnostic Tools
AR(1) data for the sample size n = 4000, corresponding to those in Fig. 2.23, for
d = 0.4 and d = 0.6.
Fig. 2.24. Reciprocal mean cluster sizes θ(u, r) for d = 0.4 and d = 0.6 and u ≥1,
r = 1, 2, 5, 10
For smaller d, that is the situation closer to independence, the reciprocal
mcsizes are closer to 1. Likewise, one may deal with mean cluster sizes and cluster
size distributions depending on the number k of exceedances as a parameter.

Part II
Statistical Inference in
Parametric Models

Chapter 3
An Introduction to
Parametric Inference
In the preceding chapters, we emphasized the visual viewpoint of representing data
and ﬁtting parametric distributions to the data. This is the exploratory approach
to analyzing data. In the present chapter, we add some parametric estimation and
test procedures which have been partially deduced from the visual ones.
This chapter gives us the opportunity to reinforce previous knowledge as
well as to ﬁll gaps. For example, we give a detailed description of the parametric
bootstrap in conjunction with conﬁdence intervals in Section 3.2. The p–value is
employed instead of a ﬁxed signiﬁcance level in testing problems.
We introduce some classical estimation procedures in the exponential, Gaus-
sian and Poisson models in order to give an outline of our approach within a
familiar setting. This is done in the Sections 3.1, 3.4 and 3.5. Recall that exponen-
tial distributions belong to the family of generalized Pareto (GP) distributions.
Gaussian distributions are on the agenda because we want to compare classical
statistical procedures in Gaussian models with those inﬂuenced by extreme value
analysis.
The Sections 3.2 and 3.3 are devoted to conﬁdence intervals and test pro-
cedures for parametric models. Poisson distributions are of interest in extreme
value analysis because the number of exceedances above a higher threshold can be
modeled by such distributions.
Bayesian analysis gains more and more importance in our investigations. In
Section 3.5, we give an introduction to the Bayesian estimation principle within
a decision theoretic framework and collect some relevant examples of Bayesian
estimators in continuous and discrete models.

84
3. An Introduction to Parametric Inference
3.1
Estimation in Exponential and
Gaussian Models
In the preceding chapter, a normal df was visually ﬁtted to the sample df. Subse-
quently, automatic procedures are provided using estimators of the location and
scale parameters. We present some prominent estimators of the mean µ and the
standard deviation σ within the normal (Gaussian) model {Φµ,σ : µ real, σ > 0}.
Keep in mind that the normal dfs and densities will not be represented by the
variance σ2, but by the standard deviation (scale parameter) σ.
We start with a likelihood–based estimator, namely, the maximum likelihood
estimator in exponential and Gaussian models. Other likelihood–based estimators
are the Bayesian estimators.
Maximum Likelihood Estimation in the Exponential Model
We compute the maximum likelihood estimate (MLE) for the model of exponential
densities gϑ(x) = ϑ exp(−ϑx), x > 0, where ϑ > 0 is the unknown reciprocal scale
parameter.
Remember that the joint density of iid exponential variables X1, . . . , Xn with
parameter ϑ is
g(x|ϑ) =

i≤n
gϑ(xi),
x = (x1, . . . , xn),
cf. also (10.10) in the multivariate part of this book. The MLE ˆϑn maximizes the
likelihood function
L(ϑ) =

i≤n
gϑ(xi)
for the given sample x = (x1, . . . , xn).
Now compute the MLE by taking the derivative of the log–likelihood function
log L(ϑ) and solving the likelihood equation
(log L)′(ϑ) = 0.
The solution is the reciprocal sample mean ˆϑn = 1/¯x = n
 
i≤n xi.
Likewise, the sample mean is the MLE of the scale parameter in the expo-
nential model.
Maximum Likelihood Estimation in the Gaussian Model
The joint density of iid normal random variables X1, . . . , Xn with mean and stan-
dard deviation (location and scale parameters) µ and σ is
ϕ(x|µ, σ) =

i≤n
ϕµ,σ(xi),
x = (x1, . . . , xn),

3.1. Estimation in Exponential and Gaussian Models
85
where the density of Φµ,σ is denoted by ϕµ,σ again.
The MLEs of µ and σ in the normal model are the sample mean and the
sample standard deviation
ˆµn = ¯x
and
ˆσn =
 1
n

i≤n
(xi −ˆµn)2
1/2
.
(3.1)
The estimate (ˆµn, ˆσn) maximizes the likelihood function L(µ, σ) = ϕ(x|µ, σ)
for the given sample x1, . . . , xn. The values ˆµn and ˆσn may be computed as the
the solutions to the likelihood equations
∂
∂µ log L(µ, σ) = 0
and
∂
∂σ log L(µ, σ) = 0
obtained by the partial derivatives of the log–likelihood function.
Example 3.1.1. (Michelson’s Determination of the Velocity of Light in the Air from
1879.) Using a reﬁnement of Foucault’s method, Michelson obtained n = 100 measure-
ments of the velocity of light in the air1. The values in Table 3.1 plus 299,000 in km/sec
are Michelson’s determinations of the light speed in the air (stored in the order of the
outcome in the ﬁle nu–miche.dat).
Table 3.1. Michelson’s 1879 measurements of the velocity of light in the air.
620
760
800
810
840
850
870
880
930
960
650
760
800
810
840
850
870
880
930
960
720
760
800
810
840
850
870
890
940
970
720
760
800
810
840
850
880
890
940
980
720
770
800
810
840
850
880
880
940
980
740
780
810
820
840
860
880
900
950
980
740
780
810
820
840
860
880
900
950
1000
740
790
810
830
850
860
880
910
950
1000
750
790
810
830
850
870
880
910
960
1000
760
790
810
840
850
870
880
920
960
1070
The Q–Q plot in Fig. 3.1 conﬁrms a normal modeling for the measurements. The
MLEs are ˆµ = 852.4 and ˆσ = 78.6. In addition, the plots of the estimated parametric
density and a kernel density are suﬃciently close to one another. Thus, 299,841.7 km/sec
is a parametric estimate of the light speed in the air.
The universally–accepted light speed in a vacuum is about 299,792.5 km/sec. To
obtain the light speed in the air, this value must be multiplied by a correction factor which
depends on atmospheric humidity, pressure and temperature. By employing the correction
factor taken by Michelson, one arrives at the value 299,734.5; other reasonable choices of
1See also Andrews, D.F. and Herzberg, A.H. (1985). Data. Springer, New York.

86
3. An Introduction to Parametric Inference
km/sec
-2
-1
0
1
2
650
950
km/sec
600
800
1000
1200
0
0.002
0.004
Fig. 3.1. (left.) Normal Q–Q plot for Michelson’s data. (right.) Kernel density (dotted)
with b = 70 and normal density (solid) with parameters given by the MLE.
the correction factor would lead to a similar conclusion. Thus, due to a systematic error
in the experiment, the correct normal modeling of Michelson’s data and the skillfully
deﬁned MLE are merely of limited relevance to evaluating the target parameter, namely
the “true” light speed. For a continuation, see Example 3.2.1, where a conﬁdence interval
is presented.
It is likely that some of the readers are disappointed or even frustrated by
the foregoing example, yet it was deliberately chosen to enforce a critical attitude
and to exhibit certain limitations of statistical inference.
The Moment Estimation Method
The MLEs ˆµn and ˆσn in the normal model may be classiﬁed as moment estimates,
because ˆµn and ˆσ2
n are the sample mean and sample variance, and µ and σ2 are
the mean and the variance of the normal df Φµ,σ.
Generally, moment estimates are obtained by equating the sample moments
with the pertaining moments of parametric distributions. A similar method is
introduced in Section 14.5, where ordinary moments are replaced by L–moments.
The Quantile Estimation Method, L–Statistics
Recall from (2.9) that x[nq]:n is an estimate of the q–quantile. In a location and
scale parameter family, we have x[nq]:n ≈µ + σF −1(q). Such a relation can be
employed to estimate µ and σ (and further parameters if necessary). In the normal
case, the median x[n/2]:n is an estimate of µ. By taking diﬀerences, one eliminates
the location parameter and ﬁnds an estimate of σ. For example, the interquartile

3.1. Estimation in Exponential and Gaussian Models
87
range leads to the well–known robust estimate
(x[3n/4]:n −x[n/4]:n)/(2Φ−1(3/4))
of σ in the normal location and scale parameter family. Estimators of this type
may be classiﬁed as quick or systematic estimators.
Example 3.1.2. (Fitting a Normal Distribution When Fat Tails are Correct.) To data
x1, . . . , xn, which are governed by a symmetric distribution F with fat or heavy tails,
one may ﬁt a normal df with location parameter µ = 0 and scale parameter given by the
sample standard deviation, or quick estimators such as
ˆσn = x[nq]:n

Φ−1(q)
(3.2)
based on the xi, or, under a condition of symmetry imposed on F,
˜σn = y[nq]:n

Φ−1((1 + q)/2)
(3.3)
based on yi = |xi|.
By the deﬁnition of ˆσn, the normal df Φ0,ˆσn has the q–quantile x[nq]:n and, therefore,
the weight of Φ0,ˆσn and the number of data beyond the point x[nq]:n correspond to each
other. Typically, ˆσn > sn and ˜σn > sn and the normal dfs pertaining to the quick
estimates are more appropriate as estimators of the tails of F.
More generally, one may deal with L–statistics (linear combination of or-
der statistics) of the form 
i≤n ci,nxi:n which provide a rich class of statistics
for estimating parameters (see, e.g., [48]). Special L–statistics will by utilized in
conjunction with the L–moment estimation method which will be introduced in
Section 14.5.
The Least Squares Estimation Method
Location and scale parameters µ and σ may be estimated by a least squares
method.
The points of a normal Q–Q plot in (2.39) are close to the straight line
(x, σx+ µ) and, therefore, also close to the least squares line (x, ax+ b) as deﬁned
by (2.40). Consequently, the two straight lines (x, σx+ µ) and (x, ax+ b) are close
together. This yields that the intercept b and the slope a of the least squares line
provide plausible estimates of the location and scale parameters µ and σ.
It is advisable to use a trimmed version of the least squares procedure. Thus,
ﬁrst omit a certain number of upper and lower extremes from the sample, and then
apply the least squares method. This least squares method for estimating location
and scale parameters can be utilized for any df F in place of the normal df Φ.

88
3. An Introduction to Parametric Inference
The Minimum Distance Method
By visually ﬁtting a normal df or density to the sample df or, respectively, to a
kernel density or histogram, one is essentially applying a minimum distance (MD)
method.
Let d be a distance on the family of dfs. Then, (ˆµn, ˆσn) is an MDE2, if
d( Fn, Φˆµn,ˆσn) = inf
µ,σ d( Fn, Φµ,σ),
where Fn again denotes the sample df. The distance may also be based on a
distance between normal densities ϕµ,σ and sample densities fn. One must apply
the Hellinger distance
H(ϕµ,σ, fn) =
  
ϕ1/2
µ,σ(x) −f 1/2
n
(x)
2
dx
1/2
(3.4)
to obtain asymptotically eﬃcient estimators3 (a property which is shared by the
MLE). The Hellinger or L2 distances also possess computational advantages, be-
cause, then, the distances d(ϕµ,σ, fn) are diﬀerentiable in µ and σ.
The M–Estimation Method
In view of an application in Section 5.1 we deal with M–estimators in scale pa-
rameter models {Fσ : σ > 0} with special emphasis laid on the exponential scale
parameter model. Let f be the density of F = F1. The MLE of the scale parameter
is a special M–estimator as the solution to the likelihood equation

i≤n
ψML(xi/σ) = 0,
where the M–function is ψML(x) = −xf ′(x)/f(x) −1.
In the exponential model, where f(x) = e−x, x ≥0, we have
ψ∗
ML(x) = x −1,
(3.5)
and the MLE of the scale parameter is the sample mean.
Generally, M–estimates of σ are solutions to M–equations

i≤n
ψ(xi/σ) = 0,
(3.6)
where ψ is the M–function with

ψ(x)f(x)dx = 0. In addition,
2 The L2 distance between normal densities and histograms was used in Brown, L.D.
and Gene Hwang, J.T. (1993). How to approximate a histogram by a normal density.
The American Statistician 47, 251–255.
3Beran, R.J. (1977). Minimum Hellinger distance estimates for parametric models.
Ann. Statist. 5, 445–463.

3.1. Estimation in Exponential and Gaussian Models
89
• boundedness of ψ is required to achieve robustness against gross errors,
• ψ should be close to ψML in order to gain eﬃciency of the estimator.
An estimator satisfying both requirements is obtained by truncating ψML (see [27],
page 122). Such M–functions go back to P.J. Huber [30]).
For the exponential model, we use M–functions of the form
ψ∗
b (x) = −exp(−x/b) + b/(1 + b).
(3.7)
Check that ψ∗
b is bounded and bψ∗
b(x) →ψ∗
ML(x) as b →∞(apply (1.66)). Because
ψ∗
ML is diﬀerentiable, one can apply the Newton–Raphson iteration procedure to
solve the M–equation (3.6).
Covering Probabilities
The accuracy of an estimator can also be measured directly by the df of the
estimator: the probability that the absolute deviation of the sample mean X from
the true mean mF is smaller or equal to some t > 0 has the representation
P{|X −mF | ≤t} = P{X ≤mF + t} −P{X < mF −t}.
(3.8)
If F = Φµ,σ and the Xi are independent, this yields
P

|X −µ| ≤tσ/n1/2
= 2Φ(t) −1.
(3.9)
In greater generality, we have
P

|X −mF | ≤tsF /n1/2
≈2Φ(t) −1,
(3.10)
for every df F which possesses a ﬁnite standard deviation sF .
These two formulas lead to exact and asymptotic conﬁdence intervals for the
mean of a distribution (cf. Section 3.2).
The Bias and the Mean Squared Error (MSE) of an Estimator
Let us deal with the special case of estimating the location parameter µ in the
normal model and, generally, with estimating the mean mF =

x dF(x) of a df
F. The natural estimator of mF is the sample mean X = 1
n

i≤n Xi, where the
Xi are identically distributed with common df F. We have EX = mF . Hence, the
bias EX−mF is equal to zero, and X is an unbiased estimator of mF . For such an
unbiased estimator, the variance E(X −mF )2 is an appropriate measure for the
accuracy.
Generally, the performance of an estimator ˆϑn of a parameter ϑ can be mea-
sured by the mean squared error (MSE) which is the expected squared deviation
of the estimator from the target parameter. We have
E(ˆϑn −ϑ)2
=
E
ˆϑn −E(ˆϑn)
2 +

E(ˆϑn) −ϑ
2
=:
V (ˆϑn) + Bias2(ˆϑn).
(3.11)

90
3. An Introduction to Parametric Inference
For unbiased estimators, the MSE is the variance. The bias that occurs when the
model is incorrect is essential for the performance of an estimator.
The MSE plays a central role in the deﬁnition of the Bayes estimator which
will be dealt with in the subsequent lines.
3.2
Conﬁdence Intervals
Intervals based on the data are constructed so that the true parameter falls into
such an interval in (1−α)×100% of the trials. One obtains 95% or 99% conﬁdence
intervals, if α = 0.05 or α = 0.01.
We mostly deal with conﬁdence intervals based on estimators. Such a con-
ﬁdence interval also provides a measure for the accuracy of the estimator. First,
we will employ (3.9) and (3.10) to construct conﬁdence intervals for the mean of
a distribution.
Conﬁdence Intervals for the Mean
First assume that the data x1, . . . , xn are governed by a normal df Φµ,σ0 with µ
unknown and σ0 ﬁxed. Let u(α) = Φ−1(1 −α) denote the (1 −α)–quantile of Φ.
(3.9) yields that, with probability 1 −α, a sample x1, . . . , xn is drawn such that
the interval
[¯x −σ0u(α/2)/n1/2, ¯x + σ0u(α/2)/n1/2]
(3.12)
covers µ. Thus, we gain a (1 −α) conﬁdence interval for the mean.
If σ0 in (3.12) is unknown, then the interval bounds are also unknown to the
statistician. Yet, if σ0 is replaced by the sample standard deviation sn, one gets
an interval which is merely based on the data. This leads to a conﬁdence interval
that approximately attains the level 1 −α.
To obtain a conﬁdence interval such that the level 1 −α is attained exactly,
one must adopt quantiles of the t–df with n −1 degrees of freedom, see (1.62).
Example 3.2.1. (Continuation of Example 3.1.1 about Michelson’s 1879 Determination of
the Velocity of Light in the Air.) The conﬁdence interval of level .99 for the unknown mean
value µ∗is [¯x −2.58snn−1/2, ¯x + 2.58snn−1/2] = [832.1, 872.7]. Thus, when Michelson’s
experiment is repeated for several times, then µ∗would fall into such intervals in 99 % of
the trials. Whereas the value of 299,841.7 km/sec determined by Michelson might be an
acceptable estimate of the light–speed in the air—there is a deviation of 107.2 km/sec—
the measuring of the accuracy of the estimate by means of the conﬁdence interval must
be regarded as a failure.
From (3.10), we know that an asymptotic conﬁdence interval for the mean is
still obtained by the preceding construction if the normal distribution is replaced

3.2. Conﬁdence Intervals
91
by any df which has a ﬁnite variance. This construction can be extended to other
functional parameters.
Asymptotic Conﬁdence Intervals for Functional Parameters
We deal with the fairly general case of two–sided conﬁdence intervals for a func-
tional parameter T (ϑ) of a parametric df Fϑ with ϑ ∈Θ. We assume that a
consistent estimator ˆϑn of the parameter ϑ is available.
In analogy to (3.10), assume that Tn(X) is an estimator of T (ϑ) such that
P
 Tn(X) −T (ϑ)
 ≤ts(ϑ)/n1/2
≈2Φ(t) −1,
(3.13)
where X = (X1, . . . , Xn) is a vector of n iid random variables Xi with common df
Fϑ, and s(ϑ) are normalizing constants varying continuously in ϑ.
Let x be the vector of data xi governed by Fϑ. If ˆϑn(x) ≈ϑ, then s(ˆϑn(x)) ≈
s(ϑ). Therefore, corresponding to (3.12), the vectors x = (x1, . . . , xn) are drawn,
with probability approximately equal to 1 −α, such that the intervals

Tn(x) −s(ˆϑn(x))u(α/2)
n1/2
, Tn(x) + s(ˆϑn(x))u(α/2)
n1/2

(3.14)
cover the functional parameter T (ϑ).
For the preceding construction, one needs a precise knowledge of the asymp-
totic behavior of the estimator of the functional parameter T (ϑ). In particular,
the normalizing constants s(ϑ) must be known to the statistician. Otherwise, con-
ﬁdence intervals may be constructed by means of the bootstrap4 approach.
Parametric Bootstrap Conﬁdence Intervals
We will brieﬂy explain the bootstrap approach within a parametric framework
(also see [49] and the review paper by Manteiga and S´anchez5). Let Tn(x) be an
estimate of the functional parameter T (ϑ) based on x = (x1, . . . , xn). Assume that
ˆϑn is a consistent estimator of ϑ. Usually, we have Tn(x) = T (ˆϑn(x))
The parametric bootstrap df based on the vector x of data is given by
Bn(x; t) = P
 Tn(Z) −T (ˆϑn(x))
 ≤t

,
(3.15)
where Z = (Z1, . . . , Zn) is a vector of iid random variables with common df Fˆϑn(x).
Notice that Bn(x; ·) is known to the statistician (at least, theoretically). Denote
the (1 −α)–quantile of the bootstrap df Bn(x; ·) by
cn,α(x) = B−1
n (x; 1 −α).
(3.16)
4A term coined in Efron, B. (1979). Bootstrap methods: another look at the jackknife.
Ann. Statist. 7, 1–26.
5Manteiga, W.G. and S´anchez, J.M.P. (1994). The bootstrap—a review. Comp.
Statist. 9, 165–205

92
3. An Introduction to Parametric Inference
Under certain regularity conditions, the intervals in (3.14) can be replaced by the
bootstrap conﬁdence intervals
 Tn(x) −cn,α(x), Tn(x) + cn,α(x)

.
(3.17)
A justiﬁcation of the bootstrap approach will be provided at the end of this
section. First, the value cn,α(x) will be computed by employing the Monte Carlo
method.
Simulating the Bootstrap Conﬁdence Bounds
Generate data z1,j, . . . , zn,j, j = 1, . . . , N, according to the df Fˆϑn(x). Notice that
the bootstrap sample
bj = | Tn(z1,j, . . . , zn,j) −T (ˆϑn(x))|,
j = 1, . . . , N,
(3.18)
is governed by the bootstrap df Bn(x; ·). According to (2.9), the bootstrap sample
(1 −α)–quantile b[(1−α)N]+1:N satisﬁes the relation
b[(1−α)N]+1:N ≈B−1
n (x; 1 −α)
(3.19)
which yields that ˜cn,α(x) = b[(1−α)N]+1:N may be taken in (3.17) in place of
cn,α(x).
In order to obtain a suﬃciently accurate estimate of the bootstrap (1 −α)–
quantile in (3.19) for typical values α = .01 and α = .05, we suggest taking
N = max(n, 2000) as the sample size for the simulation.
A Justiﬁcation of the Bootstrap Approach
One must verify that
P
 Tn(X) −T (ϑ0)
 ≤cn,α(X)

≈1 −α
(3.20)
where X = (X1, . . . , Xn) is a vector of iid random variables with common df Fϑ0.
If (3.13) holds uniformly in a neighborhood of ϑ0 and s(ˆϑn(x)) ≈s(ϑ0), then the
bootstrap df is an estimate of the centered distribution of the estimator Tn(X),
that is,
Bn(x; t) ≈P
 Tn(X) −T (ϑ0)
 ≤t

.
(3.21)
Moreover, from (3.13) we know that
P
 Tn(X) −T (ϑ0)
 ≤dn,α

≈1 −α,
(3.22)
where dn,α = n1/2Φ−1(1 −α/2)/s(ϑ0). Now, deduce from (3.21) and (3.22) that
dn,α/cn,α(x) ≈1 which implies that dn,α can be replaced by cn,α(X) in (3.22).
Thus, (3.20) holds true.

3.3. Test Procedures and p–Values
93
Extensions
In many applications, a framework more general than that of replicates x1, . . . , xn
is required. For example, one may deal with measurements xi at time ti governed
by a df Fβ0+β1ti; the joint experiment is determined by the parameter vector
(β0, β1).
Likewise, one may deal with one–sided bootstrap conﬁdence intervals. For
that purpose take a bootstrap sample with Tn(z1,j, . . . , zn,j) −T (ˆϑn(x)) instead
of bj in (3.18).
3.3
Test Procedures and p–Values
In this section we brieﬂy discuss critical regions of a signiﬁcance level α and their
representation by means of a p–value. In statistical software packages, the output
of a test usually consists of the p–value, whereby it is understood that the user
has some signiﬁcance level in mind. The major advantage of the p–value is that
the signiﬁcance level must not be speciﬁed in advance.
Test Statistics and Critical Values
Let {Fϑ} be a family of dfs, where ϑ varies over a parameter space Θ. Let Θ0
and Θ1 be the null hypothesis and the alternative, where Θ0 and Θ1 constitute a
partition of Θ. A decision for or against the null hypothesis may be based on a
critical region C. If x = (x1, . . . , xn) belongs to the critical region C, then the null
hypothesis is rejected.
Throughout, we consider critical regions of the special form
Cα = {x : T (x) ≥G−1(1 −α)}
(3.23)
of a signiﬁcance level α, where T is a test statistic and G is a df. The usual
signiﬁcance levels are α = .05 or α = .01. Thus, the null hypothesis is rejected
whenever T (x) exceeds the critical value G−1(1 −α).
Let X = (X1, . . . , Xk) be a vector of iid random variables with common df
Fϑ, where ϑ belongs to the null hypothesis. Usually, G is the exact or asymptotic
df of the test statistic T (X) for a certain parameter ϑ in the null hypothesis. We
have
P{T (X) ≥G−1(1 −α)} ≤α.
(3.24)
The null hypothesis is rejected, although it is true with a probability bounded by
α.
The p–Value
We introduce the p–value in conjunction with critical regions as given in (3.23).
Let T be the test statistic and let G be the exact or approximate df of T (X) under

94
3. An Introduction to Parametric Inference
some appropriate parameter in the null hypothesis. Notice that
{x : T (x) ≥G−1(1 −α)}
=
{x : G(T (x)) ≥1 −α}
=
{x : p(x) ≤α},
where
p(x) = 1 −G(T (x)).
(3.25)
Now the decision will be based on the p–value p(x). If one has a signiﬁcance
level α in mind, then the null hypothesis is rejected whenever p(x) ≤α. The
p–value is also called the sample signiﬁcance level, because p(x) is the smallest
signiﬁcance level α such that the null hypothesis is rejected6.
According to the probability transformation (cf. page 38), the random vari-
able p(X) is uniformly distributed on [0, 1] if G is the distribution of T (X) under
the null hypothesis.
Evaluation of p–Values for the Normal Model
We mention three simple, well–known testing problems in the normal model and
specify the p–values.
• (One–Sided Test of the Mean with Known Variance.) The testing of the null
hypothesis H0 : µ ≤µ0 against the alternative H1 : µ > µ0, with σ = σ0
known, is based on the critical region
Cα =

x : ¯x −µ0
σ0/n1/2 ≥Φ−1(1 −α)

,
where ¯x is the sample mean again. Then, the p–value is
p(x) = 1 −Φ
 ¯x −µ0
σ0/n1/2

.
• (One–Sided t–Test of the Mean with Unknown Variance.) When testing the
same hypotheses as before with σ being unknown, let
Cα =

x : ¯x −µ0
s/n1/2 ≥t−1
n−1(1 −α)

,
where tn−1 is the Student df with n −1 degrees of freedom, and s2 is the
sample variance again. The explicit form of the Student density may be found
in (1.62) and (6.14). The p–value is
p(x) = 1 −tn−1
 ¯x −µ0
s/n1/2

.
6See, e.g., Rice, J.A. (1988). Mathematical Statistics and Data Analysis. Wadsworth
& Brooks, Paciﬁc Grove.

3.3. Test Procedures and p–Values
95
• (One–Sided Testing of the Mean with Asymptotic p–Value.) Because the
Student distribution is asymptotically normal as n →∞, we know that tn−1
can be replaced by the standard normal df Φ for larger sample sizes n. The
p–value is
p(x) = 1 −Φ
 ¯x −µ0
s/n1/2

.
This p–value can be generally employed for testing the mean of a distribution
under the conditions of the central limit theorem.
We started with a likelihood ratio (LR) test statistic for simple hypotheses
and replaced the unknown standard deviation by the corresponding sample coef-
ﬁcient. The latter test statistic is based on the ﬁrst and second sample moments.
Asymptotic p–Values for the Multinomial Model,
χ2 and Likelihood Ratio Statistics
We mention the χ2 and LR–statistics for the multinomial model and specify the
pertaining p–values. Let X1, . . . , Xn be iid random variables and let B0, . . . , Bm
be a partition of the real line (or, generally, of the Euclidean d–space). Then, the
joint distribution of the random numbers
Ni =

j≤n
I(Xj ∈Bi),
i = 0, . . . , m,
is a multinomial distribution Mn,p with parameter vector p = (p0, . . . , pm), where
pi = P{X1 ∈Bi}. For n = (n0, . . . , nm) with m
j=0 nj = n, we have
Mn,p({n})
=
P{N0 = n0, . . . , Nm = nm}
=
n!
n0! · · · nm!
m

j=0
pnj
j .
Notice that the parameter vector belongs to the space
Km =
 
p : pj ≥0,
m

j=0
pj = 1
!
.
We are testing a composite null hypothesis K0 which is a subspace of Km.
• (The χ2–Statistic.) The χ2–statistic is
Tχ2(n) =
m

j=0
(nj/n −ˆpj,0(n))2
ˆpj,0(n)/n
,
(3.26)
where ˆp0(n) = (ˆp0,0(n), . . . , ˆpm,0(n)) is an MLE for K0. Under the null
hypothesis, the asymptotic df of the χ2–statistic is a χ2–df χ2
k with k degrees

96
3. An Introduction to Parametric Inference
of freedom (see Section 4.3), where k is the diﬀerence of the dimension m of
Km and the dimension of the null hypothesis K0. Therefore, the (asymptotic)
p–value is
pχ2(n) = 1 −χ2
k(Tχ2(n)).
If the null hypothesis is simple, say, K0 = {p0}, then ˆp0 = p0 and k = m.
The χ2–statistic in (3.26) is the Pearson χ2–statistic.
• (The Likelihood Ratio (LR) Statistic.) The LR statistic for the preceding
testing problem is
TLR(n)
=
2 log supp∈Km Mn,p({n})
supp∈K0 Mn,p({n})
=
2n
m

j=0
nj
n log
nj
nˆpj,0(n),
(3.27)
where ˆp(n) = (n0/n, . . ., nm/n) is the MLE for the full parameter space Km
and ˆp0(n) = (ˆp0,0(n), . . . , ˆpm,0(n)) is the MLE again for K0. Under the null
hypothesis, the LR statistic has the same asymptotic df as the preceding
χ2–statistic. Therefore, the p–value is
pLR(n) = 1 −χ2
k(TLR(n)).
Examples of χ2 and LR–tests are given in the subsequent section, where a
goodness–of–ﬁt test is employed to the Poisson model.
3.4
Inference in Poisson
and Mixed Poisson Models
In this section, estimating and testing within the Poisson model and the negative
binomial model will be studied, whereby the latter model consists of mixed Poisson
distributions. The Poisson model is relevant to extreme value analysis—as pointed
out in Section 1.2—because the number of exceedances may be regarded as a
Poisson random variable.
Estimation in the Poisson Model
Let x1, . . . , xn be nonnegative integers governed by a Poisson distribution Pλ with
unknown parameter λ > 0. Recall that λ is the mean and variance of Pλ.
The sample mean ˆλn = ¯x is the natural estimate of the unknown parameter
λ. Note that the mean ˆλn is also the MLE for the Poisson model {Pλ : λ > 0}.

3.4. Inference in Poisson and Mixed Poisson Models
97
Statistics for such discrete data can be expressed in terms of the number n(j)
of data xi equal to j. For example, we may write
ˆλn =

j
jn(j)
 
j
n(j)

for the sample mean.
There are three diﬀerent approaches for estimating a Poisson distribution:
• (Nonparametric Approach.) Take the sample histogram pn(j) = n(j)/n as
an estimate of the Poisson histogram Pλ{j} (see page 59);
• (Parametric Approach: Poisson Model) Compute the MLE ˆλn in the Poisson
model and display Pˆλn{j};
• (Parametric Approach: Normal Model.) Compute the MLE (ˆµn, ˆσn) in the
normal model (or any related estimator) based on x1, . . . , xn and take the
normal density ϕˆµn,ˆσn as an estimate of the Poisson histogram Pλ{j}, where
this procedure is merely accurate if λ is suﬃciently large.
Another estimate of the parameter λ is the sample variance s2
n, due to the fact
that the mean and the variance of a Poisson distribution are equal. One obtains
the representation
s2
n =

j
j2n(j)
 
j
n(j)

−ˆλ2
n.
A strong deviation of the sample mean from the sample variance indicates that
the Poisson assumption is violated.
Goodness–of–Fit Test for the Poisson Model
The null hypothesis—that the data are generated under a Poisson distribution
Pλ with unknown parameter λ—is tested against any other distribution on the
nonnegative integers. The likelihood ratio and ˆχ2–test in the multinomial model
will be made applicable by grouping the data.
Under the null hypothesis, let X1, . . . , Xn be iid random variables with com-
mon Poisson distribution Pλ where λ is unknown. Let Bj = {j} for j = 0, . . . , m−1
and Bm = {m, m + 1, m + 2, . . .}. Thus, we are observing n = (n0, . . . , nm), where
nj is the frequency of the observations xi in the cell Bj.
The null hypothesis is
K0 =

(p0(λ), . . . , pm(λ)) : λ > 0

,
where
pj(λ) = Pλ{j} = λj
j! e−λ,
j = 0, . . . , m −1,

98
3. An Introduction to Parametric Inference
and
pm(λ) = 1 −
m−1

j=0
pj(λ).
The null hypothesis K0 is a parameter space of dimension 1 and, therefore,
the limiting df for the likelihood ratio (LR) and χ2–statistics is the χ2–df with
k = m −1 degrees of freedom. To make (3.27) and (3.26) applicable one must
compute the MLE
p(ˆλ) =

p0(ˆλ), . . . , pm(ˆλ)

for K0. For that purpose, one must ﬁnd the solution to the likelihood equation
∂
∂λ log Mn,p(λ){n} = 0
which is equivalent to the equation
λ = 1
n
⎛
⎝
m−1

j=0
jnj +
nm
pm(λ)
∞

j=m
jPλ{j}
⎞
⎠.
If nm = 0, then ˆλ(n) is the sample mean which is the MLE in the Poisson model.
The likelihood ratio (LR) and the χ2–statistics for the present problem are
TLR(n) = 2n
m

j=0
nj
n log
nj
npj(ˆλ(n))
(3.28)
and
ˆχ2(n) =
m

j=0
(nj −npj(ˆλ(n)))2
npj(ˆλ(n))
.
(3.29)
The pertaining p–values for testing the Poisson hypothesis are
pLR(n) = 1 −χ2
m−1(TLR(n))
and
pχ2(n) = 1 −χ2
m−1(ˆχ2(n)).
Mixtures of Poisson Distributions, Negative Binomial Distributions
Remember that the mean and variance of a Poisson distribution are equal. The
Poisson modeling is untenable if the sample mean and sample variance deviate
signiﬁcantly from each other. Therefore, we also deal with mixed Poisson distri-
butions Q given by
Q{k} =
 ∞
0
Pλ{k}f(λ) dλ,
k = 0, 1, 2, . . . ,
(3.30)

3.4. Inference in Poisson and Mixed Poisson Models
99
where f is a mixing density. Subsequently, the mixing will be done with respect
to a gamma density.
When the mixing is carried out with respect to a gamma density (see (3.42))
with shape parameter r > 0 and reciprocal scale parameter d, one ﬁnds the nega-
tive binomial distribution
B−
r,p{k} =
Γ(r + k)
Γ(r)Γ(k + 1)pr(1 −p)k,
k = 0, 1, 2, . . . ,
(3.31)
with parameters r > 0 and p = d/(1 + d). Notice that B−
1,p is a geometric distri-
bution (also see page 13).
The mean and the variance of the negative binomial distribution B−
r,p are
r(1 −p)/p and r(1 −p)/p2. We see that the variance is larger than the mean
(a property which is shared by all mixed Poisson distributions that diﬀer from a
Poisson distribution).
If r(1 −p(r)) →λ as r →∞, then the mean and variance of the negative
binomial distribution tend to λ. Moreover, one ends up with a Poisson distribution
with parameter λ in the limit. We have
B−
r,p(r){k} →Pλ{k},
k = 0, 1, 2, . . . .
In similarity to the Poisson approximation of binomial distributions, a much
stronger inequality holds. We have
|B−
r,p(A) −Pr(1−p)/p(A)| ≤1 −p
√
2p
(3.32)
for each set A of nonnegative integers7. Note that the negative binomial and the
approximating Poisson distribution in (3.32) have identical mean values. Thus,
a modeling by means of a Poisson as well as a negative binomial distribution is
justiﬁed, if p is suﬃciently close to 1.
Estimation in the Negative Binomial Model
From the mean and variance of negative binomial distributions, one may easily
deduce moment estimators for that model. We also mention MLEs.
• Moment Estimator: Given a sample of nonnegative integers xi, the moment
estimates ˆrn and ˆpn of the parameters r and p are the solutions to the
equations
r(1 −p)/p = ¯x
and
r(1 −p)/p2 = s2,
where ¯x and s2 again denote the sample mean and sample variance. We have
ˆpn = ¯x/s2
and
ˆrn = ¯x2/(s2 −¯x).
7Matsunawa, T. (1982). Some strong ϵ–equivalence of random variables. Ann. Inst.
Math. Statist. 34, 209–224.

100
3. An Introduction to Parametric Inference
If s2 ≤¯x, take some value for ˆpn close to 1 and ˆrn = ¯xˆpn/(1 −ˆpn); another
plausible choice would be a Poisson distribution with parameter ¯x.
• Maximum Likelihood Estimator: One must compute a solution to the likeli-
hood equations
∂
∂r log L(r, p) = 0
and
∂
∂p log L(r, p) = 0,
(3.33)
where
L(r, p) =

i≤n
B−
r,p{xi}
denotes the likelihood function. Deduce from the 2nd equation in (3.33) that
p = r/(r + ¯x).
By adopting the formula Γ(r + 1) = rΓ(r) and inserting the value for p in
the 1st equation (3.33), one obtains
1
n

i≤n

j≤xi
1
r + j −1 = log(1 + ¯x/r)
(3.34)
with 
j≤0 = 0. This equation must be solved numerically, whereby the
moment estimate for r may serve as an initial value of an iteration procedure.
For that purpose, it is advisable to write the left–hand side of (3.34) as

j≤max{xi}
 
i≤n
I(xi ≥j)

(r + j −1).
In the subsequent example, Poisson and negative binomial distributions are
ﬁtted to a data set of numbers of car accidents.
Example 3.4.1. (Number of Car Accidents.) We deal with the classical problem of mod-
eling the number of car accidents caused by a single driver within a given period. The
present data set records the number of accidents over the period from Nov. 1959 to Feb.
1968 for 7842 drivers in the state of California (stored in the ﬁle id–cars2.dat).
Table 3.2. Frequencies n(j) of number of accidents equal to j.
j
0
1
2
3
4
5
6
7
8
9
10
11
n(j)
5147
1859
595
167
54
14
5
0
0
0
0
1
Since the sample mean and sample variance are 0.49 and 0.68, one hesitates to ﬁt a
Poisson distribution to the data. This critical attitude is conﬁrmed by plots of sample and

3.4. Inference in Poisson and Mixed Poisson Models
101
number j of car accidents
0
1
2
3
4
5
0
0.25
0.5
Fig. 3.2. From left to right: Sample his-
togram; MLE and Moment histograms
in negative binomial model; MLE his-
togram in Poisson model.
Poisson histograms (cf. Figure 3.2) and a χ2–goodness–of–ﬁt test. The usual suggestion
is to ﬁt a negative binomial distribution to such numbers.
The sample histogram and the negative binomial histograms pertaining to the MLE
(r = 1.34, p = 0.73) and Moment estimates (r = 1.31, p = 0.73) cannot be distinguished
visually (as far as the height of the bars is concerned); there is a remarkable deviation of
the Poisson histogram from the other histograms.
The usual explanation for the failure of ﬁtting a Poisson distribution to
data—such as those in Example 3.4.1—is that the number of accidents of a sin-
gle driver may ﬁt to a Poisson distribution Pλ, yet the parameter λ varies from
one driver to another. Therefore, one deals with the following two–step stochastic
experiment:
1st Step.
A driver with an individual accident characteristic λ is randomly
drawn according to a certain mixing distribution.
2nd Step.
The ﬁnal outcome (namely the number of claims for this driver) is
generated under the Poisson distribution Pλ.
For the parametric modeling, one may take the mixture of Poisson distri-
butions Pλ with respect to a gamma distribution with shape and reciprocal scale
parameters r and d, which is a negative binomial distribution with parameters r
and p = d/(1 + d), cf. (3.31).
The data in Example 3.4.1 can be regarded as the outcome of 7842 indepen-
dent repetitions of such a two–step stochastic experiment.
Bayesian inference in Poisson models will be dealt with at the end of the
subsequent section.

102
3. An Introduction to Parametric Inference
3.5
The Bayesian Estimation Principle
Bayesian estimation is another likelihood–based method besides the maximum
likelihood (ML) method. In addition to the statistical model, the statistician must
specify a prior distribution.
In this section, we introduce Bayes estimators within a decision–theoretical
framework. We estimate a real–valued functional parameter T (ϑ), where
ϑ = (ϑ1, . . . , ϑm)
is a parameter vector. This includes, e.g., the estimation
• of the jth component ϑj of ϑ if T (ϑ) = ϑj,
• of the mean T (ϑ) =

x dFϑ(x) of the underlying df Fϑ represented by ϑ.
The latter problem will be studied in Section 17.6 in conjunction with estimating
the net premium of an individual risk.
The Prior Density, Minimizing the Bayes Risk
Let T(X) be an estimator of the functional parameter T (ϑ), where X is a random
variable having a distribution represented by ϑ. For example, X = (X1, . . . , Xn)
is a sample of size n, T (ϑ) is the mean of the common distribution and T(X) is
the sample mean X. The performance of T(X) as an estimator of T (ϑ) can be
measured by the mean squared error (MSE)
E
 T(X) −T (ϑ)
2ϑ

:= E
 T(X) −T (ϑ)
2,
cf. also page (3.11), where the left–hand side emphasizes the fact that the expec-
tation is taken under the parameter ϑ.
The performance of the estimator can be made independent of a special
parameter vector ϑ by means of a “prior” probability density p(ϑ) which may
be regarded as a weight function. Some prior knowledge about the parameter of
interest is included in the statistical modeling by means of the prior density p(ϑ).
The Bayes risk of the estimator T with respect to the prior p(ϑ) is the
integrated MSE
R(p, T) =

E
 T (X) −T (ϑ)
2ϑ

p(ϑ) dϑ1 · · · dϑm.
(3.35)
An estimator T , which minimizes the Bayes risk, is called Bayes estimator.

3.5. The Bayesian Estimation Principle
103
Computing the Bayes Estimator, the Posterior Density
We introduce the posterior density, which is determined by the prior density p(ϑ)
and the likelihood function, and deduce an explicit representation of the Bayes
estimator by means of the posterior density.
Let X = (X1, . . . , Xn) be a vector of iid random variables with common df
Fϑ and density fϑ, where again ϑ = (ϑ1, . . . , ϑm) is the parameter vector. As in
the discussion about the MLE on page 84, let
L(x|ϑ) =

i≤n
fϑ(xi)
(3.36)
be the likelihood function given the sample vector x = (x1, . . . , xn). Using the
likelihood function, one gets the representation
E
 T(X) −T (ϑ)
2|ϑ

=
  T(x) −T (ϑ)
2L(x|ϑ) dx1 · · · dxn
(3.37)
of the MSE.
We verify that the Bayes estimate can be written
T ∗(x) =

T (ϑ)p(ϑ|x) dϑ1 · · · dϑm,
(3.38)
where the function
p(ϑ|x) =
L(x|ϑ)p(ϑ)
 L(x|ϑ)p(ϑ) dϑ1 · · · dϑm
(3.39)
is the “posterior” density for a given sample vector x, whenever the denominator
is larger than zero. If ϑ is a one–dimensional parameter and T (ϑ) = ϑ, then the
Bayes estimate is the mean

ϑp(ϑ|x) dϑ of the posterior distribution according to
(3.38).
Thus, by means of the prior density p(ϑ) and the likelihood function L(x|ϑ)
one gets the posterior density p(ϑ|x). Notice that the posterior density in (3.39)
and the Bayes estimate can be computed whenever L(x|ϑ)p(ϑ), as a function in
ϑ, is known up to a constant. Writing g(ϑ) ∝f(ϑ), when functions g and f are
proportional, we have
p(ϑ|x) ∝L(x|ϑ)p(ϑ).
(3.40)
To prove the representation (3.38) of the Bayes estimate, combine (3.35) and
(3.37) and interchange the order of the integration. The Bayes risk with respect
to the prior p(ϑ) can be written
R(p, T)
=
  T(x) −T (ϑ)
2L(x|ϑ)p(ϑ) dx1 · · · dxn dϑ1 · · · dϑm
(3.41)
=
    T(x) −T (ϑ)
2p(ϑ|x) dϑ1 · · · dϑm

f(x) dx1 · · · dxn,

104
3. An Introduction to Parametric Inference
where f(x) =

L(x|ϑ)p(ϑ) dϑ1 · · · dϑm. To get the Bayes estimate, compute for
every x the value z which minimizes the integral  
z −T (ϑ)
2p(ϑ|x) dϑ1 · · · dϑm.
This value z can be computed by showing that the derivative
∂
∂z
 
z −T (ϑ)
2p(ϑ|x) dϑ1 · · · dϑm
=

2

z −T (ϑ)

p(ϑ|x) dϑ1 · · · dϑm
=
2

z −T ∗(x)

is equal to zero, if z is equal to the value T ∗(x) in (3.38).
In those cases where the posterior density p(ϑ|x) is of the same type as the
prior density p(ϑ) one speaks of a conjugate prior. An example of a conjugate
prior will be given for the exponential model in (3.44).
For a diﬀerent view toward the Bayesian principle we refer to Section 8.4. The
results of Section 3.5 are revisited within a Poisson process setting in Sections 9.3
and 9.5. The Sections 14.5 and 17.6 concern applications of Bayesian estimators
in regional ﬂood frequency analysis and reinsurance business.
The Gamma and Reciprocal Gamma Distributions
Our prior densities will be usually speciﬁed by means of gamma densities or modi-
ﬁcations of gamma densities. The gamma density with shape parameter s > 0 and
reciprocal scale parameter d > 0 is given by
hs,d(ϑ) =
d
Γ(s)(dϑ)s−1 exp(−dϑ),
ϑ > 0.
(3.42)
It is clear from the integral representation of the gamma function in (1.32) that
hs,d is a probability density. The gamma distribution in (3.42) has the mean s/d
and variance s/d2. It is the type III distribution in the Pearson system. We also
remark that h1,d is an exponential density.
We also introduce the reciprocal gamma distribution. If a random variable
X has the gamma density ha,b, then 1/X has an reciprocal gamma density
	ha,b(x) =
1
bΓ(a)(x/b)−(1+a) exp(−b/x),
x > 0.
(3.43)
A prominent example of a reciprocal gamma density is the sum–stable L´evy density
with index a = 1/2, also see (6.18). The mean, variance and coeﬃcient of variation
are b/(a−1), b2/

(a−1)2(a−2)

and (a−2)−1/2, if a > 1 and a > 2, respectively.
The mode is equal to b/(a + 1).

3.5. The Bayesian Estimation Principle
105
A First Example: Bayesian Estimators in the Exponential Model
We compute a Bayes estimator for the model of exponential densities
fϑ(x) = ϑ exp(−ϑx),
x > 0,
where ϑ > 0 is the unknown reciprocal scale parameter. The gamma density
p(ϑ) = hs,d(ϑ),
(3.44)
with parameters s, d > 0, is taken as a prior.
Writing the likelihood function as L(x|ϑ) = ϑn exp

−ϑ 
i≤n xi

and rep-
resenting the posterior density by
p(ϑ|x) ∝L(x|ϑ)hs,d(ϑ) ∝hs′,d′(ϑ),
with s′ = s + n and d′ = d + n¯x (¯x denoting the sample mean), one obtains the
representation
p(ϑ|x) = hs′,d′(ϑ)
(3.45)
of the posterior density which is again a gamma density. We see that gamma
densities are conjugate priors for the exponential model.
According to (3.38), the Bayes estimator ϑ∗
n of ϑ is the mean of the posterior
distribution. We have
ϑ∗
n(x) =

ϑp(ϑ|x) dϑ = s + n
d + n¯x.
(3.46)
Notice that the Bayes estimates ϑ∗
n(x) approach the reciprocal sample mean
1/¯x, which is the MLE in the exponential model, with a rate of order O(1/n).
The estimates can be written as the linear combination αs/d + (1 −α)/¯x with
α = 1/(1 + n¯x/d).
Bayesian estimation in Pareto models and Poisson process models with Pareto
marks is dealt with on the pages 129–133 and 254–245.
Bayesian Estimation in the Poisson Model
We also deal with the Bayes estimation in discrete models. For that purpose, the
likelihood function for continuous data, as utilized on page 102, must be replaced
by the likelihood function for discrete data. This will be illustrated for the Poisson
model with repect to two diﬀerent parameterizations.
• Bayes Estimator: we compute an explicit representation of the Bayes es-
timator within the Poisson model. The likelihood function for a sample
vector k = (k1, . . . , kn) of size n within this discrete model is given by
L(k|λ) = 
i≤n Pλ{ki}. The MSE of an estimator ˆλn of λ can be written
E

(ˆλn −λ)2|λ

=

(ˆλn(k) −λ)2L(k|λ),

106
3. An Introduction to Parametric Inference
where the summation runs over all vectors k = (k1, . . . , kn) of nonnegative
integers. In the subsequent lines, we just follow the arguments concerning the
Bayes risk and Bayes estimators as outlined at the beginning of this section
(or apply the general arguments in Section 8.4). The gamma density
p(λ) = hr,c(λ),
(3.47)
with parameters r and c, cf. (3.42), is chosen as a prior. Verify that the
posterior density p(λ|k) satisﬁes p(λ|k) ∝L(k|λ)hr,c(λ) ∝hr′,c′(λ) with
r′ = r + 
i≤n ki and c′ = c + n. Therefore, the posterior density
p(λ|k) = hr′,c′(λ)
(3.48)
is also a gamma density. We see that gamma densities are conjugate priors
for the Poisson model.
Because the Bayes estimate of λ is the mean of the posterior density, we have
λ∗
n(k) =

λp(λ|k) dλ = r + n¯k
c + n ,
(3.49)
where ¯k is the sample mean.
• Bayes Estimator in a Modiﬁed Poisson Model: for later purposes (cf. Section
9.3), we shortly mention the model of reparameterized Poisson distributions
PλT for parameters λ > 0, where T > 0 is some ﬁxed value. Moreover, let
n = 1. The likelihood function is
L(k|λ) =

(λT )k/k!

exp(−λT )
(3.50)
for every observed nonnegative integer k. Choosing again a gamma density
p0(λ) = hr,c(λ)
(3.51)
as a prior, one obtains the posterior density
p0(λ|k) = hr′,c′(λ),
(3.52)
which is the gamma density with parameters r′ = r + k and c′ = c + T .
Therefore, the Bayes estimate is
λ∗
k =

λp0(λ|k) dλ = r + k
c + T
(3.53)
for all nonnegative integers k.

Chapter 4
Extreme Value Models
This chapter is devoted to statistical procedures in parametric extreme value (EV)
models which are especially designed for maxima. It is worth recalling that minima
can be dealt with by changing the sign of the data.
In Section 4.1, we deal with estimators within the Gumbel (EV0), Fr´echet
(EV1), Weibull (EV2) and uniﬁed extreme value (EV) models. Q–Q plots based
on estimators of the shape parameter will be employed to check the validity of such
models. Tests are addressed in Section 4.2. Extensions of extreme value models
and certain alternative models for the modeling of maxima are brieﬂy discussed
in Section 4.3.
4.1
Estimation in Extremes Value Models
Consider data x1, . . . , xn generated under a df F m as in Section 1.2. Thus, each
xi is the maximum of m values that are governed by the df F. Think back to
Section 2.1, where we found that the df F m and the pertaining density or qf can
be estimated by the corresponding sample versions, namely the sample df Fn, the
histogram or kernel density fn or the sample qf F −1
n .
Remember that some extreme value (EV) df G can be ﬁtted to the df F m of
the maximum if the arguments in Section 1.2 are valid. This includes that, for a
while, the x1, . . . , xn may be regarded as observations governed by G. Therefore,
statistical procedures within a parametric model can be applied.
The Gumbel (EV0) Model
This is the traditional model in extreme value analysis which had about the same
status as the normal model in other applications (with just the same advantages
and negative consequences). The major advantage of the Gumbel model is that the

108
4. Extreme Value Models
distribution can be speciﬁed by location and scale parameters as in the Gaussian
case. Thus, the Gumbel model is an ordinary location and scale parameter model.
One of our declared aims is to overcome the potential dislike for an additional
parameter—besides the location and scale parameters—and to convince the reader
that an extension of the Gumbel (EV0) model to the EV model can be worthwhile.
Recall that the standard Gumbel df is given by
G0(x) = exp

−e−x).
By adding a location and scale parameters µ and σ, one gets the Gumbel (EV0)
model
EV0:
{G0,µ,σ : µ real, σ > 0}.
We mention the following estimators for µ and σ.
• MLE(EV0): the MLEs µn and σn of the location and scale parameters must
be evaluated numerically. First, compute σn as the solution to the equation
σ −n−1 
i≤n
xi +
 
i≤n
xi exp(−xi/σ)
 
i≤n
exp(−xi/σ)

= 0.
(4.1)
The least squares estimate for σ can be taken as an initial value of the
iteration procedure. Then,
µn = −σn log

n−1 
i≤n
exp(−xi/σn)

.
(4.2)
• Moment(EV0): estimators of µ and σ are deduced from the sample mean
and variance. From (1.34) and (1.35), conclude that µ + σλ and σ2π2/6 are
the mean and the variance of G0,µ,σ, where λ again denotes Euler’s constant.
Therefore,
σn = 61/2sn/π
and
µn = ¯x −σnλ
are the moment estimates of the location and scale parameters µ and σ in
the Gumbel model, where ¯x and s2
n are the sample mean and variance again.
If G0,µn,σn strongly deviates from Fn, then the modeling of F m by Gumbel
dfs G0,µ,σ is critical (or the estimates are inaccurate)! Likewise, one may compare
the estimated density g0,µn,σn with the kernel density fn,b for some choice of a
bandwidth b.

4.1. Estimation in Extreme Value Models
109
Example 4.1.1. (Annual Floods of the Feather River.) We partially repeat the analysis
of Benjamin and Cornell1 and Pericchi and Rodriguez–Iturbe2 about the annual ﬂoods
of the Feather River in Oroville, California, from 1902 to 1960. The following ﬂood data
are stored in the ﬁle ht–feath.dat.
Table 4.1. Annual maximum discharges of Feather River in ft3/sec from 1902 to 1960.
year
ﬂood
year
ﬂood
year
ﬂood
year
ﬂood
year
ﬂood
1907
230,000
1943
108,000
1911
75,400
1916
42,400
1934
20,300
1956
203,000
1958
102,000
1919
65,900
1924
42,400
1937
19,200
1928
185,000
1903
102,000
1925
64,300
1902
42,000
1913
16,800
1938
185,000
1927
94,000
1921
62,300
1948
36,700
1949
16,800
1940
152,000
1951
92,100
1945
60,100
1922
36,400
1912
16,400
1909
140,000
1936
85,400
1952
59,200
1959
34,500
1908
16,300
1960
135,000
1941
84,200
1935
58,600
1910
31,000
1929
14,000
1906
128,000
1957
83,100
1926
55,700
1918
28,200
1955
13,000
1914
122,000
1915
81,400
1954
54,800
1944
24,900
1931
11,600
1904
118,000
1905
81,000
1946
54,400
1920
23,400
1933
8,860
1953
113,000
1917
80,400
1950
46,400
1932
22,600
1939
8,080
1942
110,000
1930
80,100
1947
45,600
1923
22,400
By applying the MLE(EV0), one obtains µn = 47, 309 and σn = 37, 309. The
pertaining Gumbel distribution can be well ﬁtted to the upper part of the ﬂood data
(according to the Q–Q plot, etc.). Therefore, this parametric approach may be employed
to estimate T–year ﬂood levels (cf. (1.19)): the 50 and 100–year ﬂood levels are u(50) =
192, 887 and u(100) = 218, 937 ft3/sec. Notice that the estimated 100–year ﬂood level was
exceeded in the year 1907. The estimated Gumbel distribution has a remarkable 2.8% of
its weight on the negative half–line. Yet, this does not heavily inﬂuence the estimation
of parameters of the upper tail like T–year ﬂood levels (also see Example 5.2.1).
Pericchi and Rodriguez–Iturbe also select gamma (Pearson type III), log–
gamma (log–Pearson type III) and log–normal models (cf. (4.6), (4.12) and (1.60)),
yet there seems to be no clear conclusion which of these models is most preferable.
1 Benjamin, J.R. and Cornell, C.A. (1970). Probability, Statistics and Decisions for
Civil Engineers. McGraw–Hill, New York.
2Pericchi, L.R. and Rodriguez–Iturbe, I. (1985). On the statistical analysis of ﬂoods.
In: A Celebration of Statistics. The ISI Centenary Volume, A.C. Atkinson and S.E.
Fienberg (eds.), 511–541.

110
4. Extreme Value Models
Moreover, these authors suggest to use excess dfs and hazard functions in ﬂood
studies. In this context, these functions are called residual ﬂood df and ﬂood rate.
The following Fr´echet and Weibull models can be deduced from the Gumbel
model by means of the transformations T (x) = log(x) and T (x) = −1/x (cf. page
37). Thus, results and procedures (such as the MLE) for the Gumbel model can be
made applicable to these models (and vice versa). Finally, we deal with estimators
in the uniﬁed extreme value (EV) model.
The Fr´echet (EV1) Model
We deal with another submodel of extreme value dfs. Recollect that the standard
Fr´echet df with shape parameter α > 0 is given by G1,α(x) = exp(−x−α) for
x > 0. By adding a scale parameter σ, one gets the EV1 model
EV1:
{G1,α,0,σ : α, σ > 0}.
Keep in mind that the left endpoint of G1,α,0,σ is always equal to zero. If
all potential measurements are positive—e.g., due to physical reasons—and, in
addition, a left endpoint equal to zero is acceptable, then it can be worthwhile to
check the validity of the EV1 model.
When applying the maximum likelihood principle for this model, one should
make sure that the left endpoint of the actual df is larger than zero (at least this
should hold true for the data).
The Weibull (EV2) Model
The standard Weibull df with shape parameter α < 0 is given by G2,α(x) =
exp(−(−x)−α) for x ≤0. Recall from Section 1.3 that our parameterization for
Weibull dfs diﬀers from the standard one used in the statistical literature, where
a positive shape parameter is taken.
In the following model, a scale parameter is included:
EV2:
{G2,α,0,σ : α < 0, σ > 0}.
Notice that the upper endpoint of each of the dfs is equal to zero.
For many data which may be regarded as minima, as, e.g., data concerning
the strength of material, a converse Weibull df 	G2,α,0,σ(x) = 1 −G2,α,0,σ(−x) is a
plausible candidate of a distribution, also see (1.38) and (1.39).
The Uniﬁed Extreme Value Model
The uniﬁed EV model—in the γ–parameterization—is the most important one for
modeling maxima. By including scale and location parameters σ and µ, we obtain
the model
EV:
{Gγ,µ,σ : µ, γ real, σ > 0},

4.1. Estimation in Extreme Value Models
111
where Gγ are the standard versions as deﬁned on page 16.
Occasionally, the extreme value (EV) model is called the generalized extreme
value (abbreviated GEV) model in statistical references. The latter notion can be
a bit confusing because this model only consists of extreme value distributions.
Here is a list of estimation procedures.
• MLE(EV): The MLE in the EV model must be numerically evaluated as a
solution to the likelihood equations.
The MLE determines a local maximum of the likelihood function if the iter-
ated values remain in the region γ > −1. If γ varies in the region below −1,
then neither a global nor a local maximum of the likelihood function exists,
see R.L. Smith3.
• MDE(EV): Let d be a distance on the family of dfs. Then, (γn, µn, σn) is an
MDE (minimum distance estimator), if
d( Fn, Gγn,µn,σn) = inf
γ,µ,σ d( Fn, Gγ,µ,σ).
Likewise, an MDE may be based on distances between densities. This es-
timation principle is related to the visualization technique as dealt with in
preceding sections.
• LRSE(EV): This is a class of estimators that are linear combinations of ratios
of spacings (RS’s)
ˆr = x[nq2]:n −x[nq1]:n
x[nq1]:n −y[nq0]:n
,
where q0 < q1 < q2. Note that this statistic is independent of the location
and scale parameters in distribution (in other words, ˆr is invariant under
aﬃne transformations of the data). As a consequence of (2.38),
ˆr ≈G−1
γ (q2) −G−1
γ (q1)
G−1
γ (q1) −G−1
γ (q0) =
−log q2
−log q0
−γ/2
,
if q0, q1, q2 satisfy the equation (−log q1)2 = (−log q2)(−log q0). In this man-
ner, one obtains the estimate γn = 2 log(ˆr)/ log(log(q0)/ log(q1)) of γ. Such
an estimate was suggested in [31]—attributed to S.D. Dubey—for estimating
the shape parameter of Weibull distributions.
If q0 = q, q1 = qa, q2 = qa2 for some 0 < q, a < 1, then
γn = log(ˆr)/ log(1/a).
(4.3)
3Smith, R.L. (1985). Maximum likelihood estimation in a class of nonregular cases.
Biometrika 72, 67–90.

112
4. Extreme Value Models
The eﬃciency can be improved by taking a linear combination of such RSE’s
where each of the single terms is deﬁned for q = i/n.
The least squares method may be applied to obtain the additionally required
estimators of the location and scale parameters µ and σ.
The L–moment estimators within EV models will be dealt with in Section
14.5 in the context of ﬂood frequency analysis.
Simulating the Mean Squared Error (MSE)
Given an estimator γn and a df H, generate a set of estimates, say, γn,1, . . . , γn,N,
where N is the selected number of simulations. Our standard number of simulations
is N = 4000.
We simulate the MSE of estimators based on Gumbel data, maxima of m = 30
exponential data, and maxima of m = 30, 100 standard normal data (indicated by
the dfs G0, W 30 and Φm) for sample sizes n = 20, 100, 500.
Table 4.2. MSE of estimators γn of the shape parameter γ = 0 for block sizes m =
30, 100.
MSE
G0
W 30
0
sample size n
20
100
500
20
100
500
MLE(EV)
0.057
0.006
0.001
0.058
0.006
0.001
LRSE(EV)
0.165
0.022
0.004
0.167
0.023
0.004
Φ30
Φ100
sample size n
20
100
500
20
100
500
MLE(EV)
0.086
0.024
0.017
0.081
0.019
0.012
LRSE(EV)
0.156
0.041
0.023
0.163
0.034
0.016
For the sample size n = 500, the MSE is close to the squared bias. Based on
the γn,i one may also simulate the df or density of the estimator.
EV Q–Q Plots
Recall from page 62 that the unknown shape parameter γ must be replaced by an
estimator γn when a Q–Q plot is employed in the EV model. If there is a stronger
deviation of the EV Q–Q plot from a straight line, then either the estimator of
the shape parameter is inaccurate (perhaps try another one), or the EV model is
untenable.

4.1. Estimation in Extreme Value Models
113
Example 4.1.2. (De Bilt September Data.) The de Bilt (September) data consist of 133
monthly maxima of temperature in de Bilt measured during the years 1849–1981. Thus,
the background of the data suggests to use the EV model. These data are stored in the
ﬁle em–dbilt.dat jointly with the maxima of other months and the annual maxima.
The EV Q–Q plot (cf. Fig. 4.1 on the left–hand side) seems to conﬁrm the EV
assumption. The maximum temperature within the whole period, measured in the year
1949, is equal to 34.2 degrees Celsius. The values of the MLE in the EV model are γ =
−0.19, µ = 24.39 and σ = 2.76; the right endpoint of the estimated Weibull distribution is
equal to 38.76. In Fig. 4.1, on the right–hand side, the kernel density (with Epanechnikov
kernel and a bandwidth equal to 1.8) and the estimated Weibull density are displayed.
temperature
-2
-1
0
1
2
3
20
25
30
temperature
20
30
0.0
0.05
0.1
0.15
Fig. 4.1. (left.) EV Q–Q plot of the de Bilt (September) data with MLE(EV) γ =
−0.19. (right.) A kernel density (dotted) and the Weibull density (solid) pertaining to
the MLE(EV).
When a least squares line and a two–sided moving average are displayed in a scat-
terplot window, there is no signiﬁcant trend visible in the data. There is some evidence
of periods, yet we believe that this is negligible.
Notice that the estimated Weibull density in the preceding example is skewed
to the right. We remark that the shapes of the estimated Weibull distributions vary
from month to month. For example, the estimated Weibull density for the month
May is skewed to the left4.
Estimation of Parameters When a Trend Must be Dealt With
Next, we admit a certain trend in the data yi that are measurements at time
ti. Assume that yi is governed by an EV df Gγ,µ(ti;β0,...,βp),σ, where the location
parameter depends on the time ti and further parameters β0, . . . , βp. For example,
4Also see [42] and Buishand, T.A. (1989). Statistics of extremes in climatology. Statist.
Neerlandica 43, 1–30.

114
4. Extreme Value Models
the model is speciﬁed by the four parameters γ, β0, β1 and σ, if µ(t; β0, β1) =
β0 + β1t. The estimation in such a complex parametric model can be carried
out by using the ML or MD estimation methods. A particular challenge is to
theoretically evaluate the properties of such estimators and to implement such
estimation procedures in a statistical software package.
Subsequently, we handle this question in a more pragmatic way and suggest
an estimation method that can be carried out in three steps:
• estimate the trend in the data by the least squares method or in some non-
parametric manner (without taking the EV model into account);
• employ the preceding parametric estimators in the EV model to the residuals,
• deduce estimators of γ, µ(ti; β0, . . . , βp) and σ.
This approach will be exempliﬁed in conjunction with the least squares
method as introduced in Section 2.5. Let mn(i) be the least squares estimate
based on y1, . . . , yn of the mean of Gγ,µ(ti;β0,...,βp),σ as introduced in (2.43), where
we assume that γ < 1 so that the mean exists. Next, assume that the residuals
xi = yi −mn(i) are governed by an EV df Gγ,µ,σ, where µ is independent of i. Let
ˆγn, ˆµn and ˆσn be estimators of γ, µ and σ based on the residuals x1, . . . , xn. We
propose ˆγn, ˆµn + mn(i) and ˆσn as estimates of γ, µ(ui; β0, . . . , βp) and σ in the
original model.
Example 4.1.3. (Iceland Storm Data5.) We report preliminary investigations concerning
storm data for the years between 1912 and 1992, with the measurements from the years
1915 and 1939 missing (stored in em–storm.dat). The annual maxima resemble those of
the storm data at Vancouver given in Table 1.1.
Table 4.3. Iceland storm data.
year
1912
1913
1914
1916
· · ·
1989
1990
1991
1992
annual maxima
38
69
47
53
· · ·
60
68
89
63
In Fig. 4.2, the annual maxima are plotted against the years. One can recognize a
slight, positive trend. The least squares line f(x) = 48.05 + 0.12(x −1911) is added. A
quadratic least squares line leads to a similar trend function so that the hypothesis of a
linear trend seems to be justiﬁed.
Firstly, by applying the MLE(EV) to the annual maxima in Table 4.3, we get the
parameters α = −3.60, µ = 98.53 and σ = 50.59 (taken in the α–parameterization).
Thus, the estimated Weibull distribution is nearly symmetric around 53. Recall that
µ = 98.53 is also the upper endpoint of the distribution.
5cf. also Stoyan, D., Stoyan, H. and Jansen, U. (1997). Umweltstatistik. Teubner,
Stuttgart.

4.1. Estimation in Extreme Value Models
115
year
maximum windspeed
1910
1930
1950
1970
1990
30
50
70
90
Fig. 4.2. Scatterplot of storm
data and linear least squares
line.
Secondly, we take the slight, positive trend in the data into account. The MLEs
for the residuals (detrended data) are α = −3.09, µ = 38.85 and σ = 43.43. Thus, the
estimated Weibull distribution is now slightly skewed to the left. By adding the values
of the least squares line 48.05 + 0.12(x −1911) for each of the years x, one can estimate
the location parameter of the Weibull distribution for the year x by
µ(x) = 86.90 + 0.12(x −1911).
The estimates for α and σ remain unchanged. Moreover, the estimated upper endpoints
are equal to 86.90 and 96.62 for the years 1912 and 1992.
We estimate the T–year thresholds with and without a linear trend hypothesis.
In the latter case, equation (9.5) must be applied to Fk = Gα,µ(k),σ with α = −3.09,
µ(k) = 97.1+0.12k and σ = 43.43, when the ﬁrst year is 1997. Upper bootstrap conﬁdence
bounds are added.
Table 4.4. Estimated T–year thresholds (initial year 1997) with 95% upper bootstrap
conﬁdence bounds in brackets.
T–year thresholds u(T)
T
50
100
200
without trend
81.41 (85.8)
84.42 (89.8)
86.9 (103.5)
with trend
89.02 (89.7)
95.47 (109.7)
107.17 (134.7)
As mentioned at the beginning, these investigations were of a preliminary nature.
Although the analysis was carried out skillfully, according to the present state–of–the–
art, the results are partly of a limited relevance due to the fact that the measurements
do not directly concern wind speeds, but a certain index given by
number of stations measuring at least 9 on the Beaufort scale
total number of stations
× 100 .

116
4. Extreme Value Models
Thus, the values in the 2nd column of Table 4.4 concern the annual maximum of such
daily indices which are necessarily below 100. This implies that the estimated 200–year
threshold and some of the conﬁdence bounds in Table 4.4 are nonsense (sorry!).
If the estimation is carried out in the Weibull (EV2) model (without trend) with a
ﬁxed upper endpoint equal to 100, then the estimated parameters are α = −3.72, µ = 100
and σ = 52.13. Again, T–year thresholds and pertaining upper bootstrap conﬁdence
bounds may be calculated. Directions of the winds are also included in this data set.
This example shows that one has to work closely with people in applications
in order to carry out a correct analysis.
In conjunction with wind speeds it is of interest to include the direction of
the wind speeds in the analysis for design assessment of large buildings and other
structures6. We also refer to the case studies by Zwiers7 and Ross8; we believe
that the analysis of these wind–speed data would deserve particular attention, yet
the full data set seems to be unavailable to the scientiﬁc community.
The question of estimating a trend in the scale parameter of generalized
Pareto (GP) dfs is dealt with in Section 5.1, page 139.
Asymptotic Distributions of Estimators and
Conﬁdence Intervals in Extreme Value Models
We specify the asymptotic normal distributions of the MLE(EV0) and deduce
the asymptotic normality of the pertaining estimators of the T –year threshold by
means of (11.6). Detailed explanations about multivariate normal distributions are
postponed until Section 11.1. Asymptotic conﬁdence intervals can easily be estab-
lished by replacing unknown parameters in covering probabilities by estimators,
cf. Section 3.2.
The MLEs µn and σn of the location and scale parameters µ and σ in the
Gumbel EV0 model are jointly asymptotically normal with mean vector (µ, σ) and
covariance matrix Σ/n, where
Σ = 6σ2
π2
⎛
⎝
π2
6 + (1 −λ)2
(1 −λ)
(1 −λ)
1
⎞
⎠
and λ is Euler’s constant. Let gT (µ, σ) = G−1
0,µ,σ(1 −1/T ) = µ + σc(T ) denote the
6Coles, S.G. and Walshaw, D. (1994). Directional modeling of extreme wind speeds.
Appl. Statist. 43, 139–157.
7Zwiers, F.W. (1995). An extreme–value analysis of wind speeds at ﬁve Canadian
locations. In [21], pp. 124–134.
8Ross, W.H. (1995). A peaks–over–threshold analysis of extreme wind speeds. In [21],
pp. 135–142.

4.1. Estimation in Extreme Value Models
117
T –year threshold, where c(T ) = −log(−log(1 −1/T )). Because
D =
∂gT
∂µ (µ, σ), ∂gT
∂σ (µ, σ)

= (1, c(T )),
we know that the pertaining estimator gT(µn, σn) is asymptotically normal with
mean gT (µ, σ) and variance DΣDt/n, where
DΣDt = σ2
1 + 6(1 −λ + c(T ))2/π2
.
(4.4)
This is [24], 6.2.7, formula (2), in a paragraph written by B.F. Kimball.
For a discussion of the asymptotic normality of the MLE(EV) we refer to the
article by Smith cited on page 111 or [31], Chapters 20 and 21.
Minima
As mentioned before, results for minima can be deduced from those for maxima.
If x1, . . . , xn may be regarded as minima, then the yi = −xi may be regarded as
maxima. If the EV df Gi,α,µ,σ or Gγ,µ,σ can be ﬁtted to the yi, then the converse
EV df 	Gi,α,−µ,σ or 	Gγ,−µ,σ, see (1.38) and (1.39), can be ﬁtted to the original data
xi.
Example 4.1.4. (Tensile Strength of Sheet Steel.) We partly repeat the statistical analysis
in the book by D. Pfeifer9 concerning sheet steel with a cross–section of 20 × 0.7 mm2.
The tensile strength is measured as the elongation in % in a test according to DIN 50145.
Table 4.5. Tensile strength of sheet steel (elongation in %).
35.9
37.3
38.7
39.4
40.5
41.2
41.5
42.3
42.8
45.0
36.5
37.3
38.8
39.9
40.8
41.2
41.6
42.4
42.9
46.7
36.9
37.8
38.9
40.3
40.8
41.2
41.7
42.5
43.0
37.2
37.8
38.9
40.4
41.1
41.3
41.9
42.6
43.7
37.2
38.1
39.0
40.5
41.1
41.5
42.2
42.7
44.3
37.3
38.3
39.3
40.5
41.1
41.5
42.2
42.7
44.9
From P–P plots, Pfeifer deduced a converse Weibull df with parameters α = −3.34,
µ = 33.11 and σ = 8.38. Notice that µ = 33.11 is also the left endpoint of the distribution.
The MLE(EV) gives the parameters α = −2.87, µ = 34.53 and σ = 6.84, which are
comparable to the preceding ones.
One may argue that there is no plausible explanation for a left endpoint not equal
to zero. Therefore, we also applied the MLE in the EV2 model, where the left endpoint
is ﬁxed and equal to zero. The estimates are α = −18.1, µ = 0 and σ = 41.7, and there
is also a reasonable ﬁt to the data. The data are stored in gu–steel.dat.
9Pfeifer, D. (1989). Einf¨uhrung in die Extremwertstatistik. Teubner, Stuttgart.

118
4. Extreme Value Models
Estimation of Actual Functional Parameters
What do we estimate if the actual distribution is unequal yet close to an EV dis-
tribution? Let us return to our initial model, where the x1, . . . , xn are generated
under F m. Now, there is no one–to–one relationship between parameters and dis-
tributions and, therefore, parameters lose their operational meaning. There is a
close relationship between our question and a corresponding one in the book [27]
by Hampel et al., page 408: “What can actually be estimated?” There is a plain
answer in our case: by estimating the parameters γ, µ and σ, we also estimate
Gγ,µ,σ and, therefore, the actual df F m of the maximum, respectively, functional
parameters of F m.
4.2
Testing within Extreme Value Models
We may continue the discussion of the previous lines by asking: what are we
testing? We are primarily interested in the question whether the actual distribution
of the maximum is suﬃciently close to an EV distribution so that the actual
distribution can be replaced by an ideal one to facilitate the statistical inference.
Therefore, the test should not be too stringent for larger sample sizes because,
otherwise, small deviations from an EV df would be detected.
Of course, one may also test the domain of attraction as it was done, e.g., in
the paper by Castillo et al.10 Accepting the null–hypothesis does not necessarily
mean that the actual distribution is closer to one of the speciﬁc EV distributions
in the null–hypothesis.
Likelihood Ratio (LR) Test for the Gumbel (EV0) Model
Within the EV model, we are testing
H0 : γ = 0
against
H1 : γ ̸= 0
(4.5)
with unknown location and scale parameters. Thus, the Gumbel distributions are
tested against other EV distributions for a given vector x = (x1, . . . , xn) of data.
The likelihood ratio (LR) statistic is
TLR(x) = 2 log

i≤n gˆγ,ˆµ,ˆσ(xi)

i≤n g0,˜µ,˜σ(xi)
with (ˆγ, ˆµ, ˆσ) and (˜µ, ˜σ) denoting the MLEs in the EV and EV0 models. Because
the parameter sets have dimensions 3 and 2, we know that the LR–statistic is
10Castillo, E., Galambos, J. and Sarabia, J.M. (1989). The selection of the domain of
attraction of extreme value distribution from the set of data. In [14], pp. 181–190.

4.2. Testing within Extreme Value Models
119
asymptotically distributed according to the χ2–df χ2
1 with 1 degree of freedom
under the null–hypothesis. Consequently, the p–value is
pLR(x) = 1 −χ2
1(TLR(x)).
The signiﬁcance level is attained with a higher accuracy by employing the
Bartlett correction11 when the LR–statistic TLR is replaced by TLR/(1 + 2.8/n).
In this case the p–value is
p(x) = 1 −χ2
1

TLR(x)/(1 + 2.8/n)

.
There is a general device to replace an LR–statistic TLR by TLR/(1 + b/n) to
achieve a higher accuracy in the χ2–approximation12.
A Test–Statistic Suggested by LAN–Theory
Marohn13 applies the LAN–approach to deduce a test statistic for the testing
problem (4.5). Let
v1,µ,σ(x)
=
−(x −µ)/σ + ((x −µ)/σ)2 (1 −exp (−(x −µ)/σ)) /2,
v2,µ,σ(x)
=
−1/σ + ((x −µ)/σ) (1 −exp (−(x −µ)/σ)) /σ,
v3,µ,σ(x)
=
(1 + exp (−(x −µ)/σ))/σ .
The test–statistic can be approximately represented by
Tn,µ,σ(x) =
1.6449

i≤n
v1,µ,σ(xi) −σ × 0.5066

i≤n
v2,µ,σ(xi)
−σ × 0.8916

i≤n
v3,µ,σ(xi)


(n1/23.451).
The p–value is
p(x) = 2Φ(Tn,ˆµ(x),ˆσ(x)(x)/0.69) −1,
where ˆµ(x) and ˆσ(x) are the MLEs of µ and σ.
Other Test Procedures
In those cases where the null–hypothesis is a location and scale parameter family
it is desirable to use a test statistic that is invariant under location and scale pa-
rameters. This condition is satisﬁed by the sample skewness coeﬃcient (see (2.7)).
11 Hosking, J.R.M. (1984). Testing whether the shape is zero in the generalized extreme–
value distribution. Biometrika 71, 367–374.
12See, e.g, Barndorﬀ–Nielsen, O.E. and Cox, D.R. (1994). Inference and Asymptotics.
Chapmann & Hall, London.
13Marohn, F. (2000). Testing extreme value models. Extremes 3, 362–384.

120
4. Extreme Value Models
Tests based on the sample skewness coeﬃcient are well known in the statistical lit-
erature. It is evident that the null–hypothesis of a Gumbel distribution is accepted
for those distributions with skewness parameter close to 1.14, cf. page 21.
For tests based on spacings (diﬀerences of order statistics) we refer to Otten
and van Montfort14.
Goodness–of–ﬁt tests for the Gumbel or EV model are obtained by χ2–
test statistics as mentioned in Section 3.3. A class of goodness–of–ﬁt tests for
the Gumbel model was investigated by Stephens15. Test statistics of the form

i≤n

F(xi) −Fn(xi)
2 are utilized, where Fn is the sample df and F is the
Gumbel df with unknown parameters replaced by MLEs.
4.3
Extended Extreme Value Models
and Related Models
We present several distributions, such as Wakeby, two–component extreme value
and gamma distributions (also see [4], a book about the gamma and related dis-
tributions), which are also used for the modeling of maxima.
A Wakeby Construction
Distributions with a certain tail behavior may also be designed by using qfs. Note
that every real–valued, nondecreasing and left continuous function on the interval
(0, 1) deﬁnes a qf. Therefore,
Q(q) = a

1 −(1 −q)b
+ c

−1 + (1 −q)−d
+ e,
0 < q < 1
with a, b, c, d > 0, is a qf.
We see that the left and right tails of the distribution are related to those of
a beta and, respectively, a Pareto distribution16.
Two–Component Extreme Value Distributions
In cases where one observes a seasonal variation in the data, it is advisable to
apply a seasonal separation to obtain identically distributed data. This was done
14Otten, A. and van Montfort, M.A.J. (1978). The power of two tests on the type of
the distributions of extremes. J. Hydrology 37, 195–199.
15Stephens, M.A. (1977). Goodness of ﬁt for the extreme value distribution. Biometrika
64, 583–588.
16Houghton, J.C. (1978). Birth of a parent: The Wakeby distribution for modeling
ﬂows. Water Resour. Res. 14, 1105–1110, and
Hosking, J.R.M., Wallis, J.R. and
Wood, E.F. (1985). An appraisal of the regional
ﬂood frequency procedure in the UK Flood Studies Report. Hydrol. Sci. J. 30, 85–109.

4.3. Extended Extreme Value Models and Related Models
121
by Todorovic and Rousselle17 for ﬂood data (in addition see [42] and Davison
and Smith18). Then, the annual maximum can be regarded as the maximum of,
e.g., monthly or certain seasonal maxima. Subsequently, we merely deal with two–
component distributions. Then, the modeling by means of the product G1(x)G2(x)
of two EV dfs is adequate (called two–component EV distribution).
Such a modeling can also be used in conjunction with mixture distributions.
Before going into details we present an example of tropical and non–tropical wind
speeds (a data set of a similar type, without a classiﬁcation of extraordinarily large
data, consists of the annual ﬂoods of the Blackstone River in Woonsocket, Rhode
Island, from 1929 to 1965 (stored in the ﬁle ht–black.dat)).
Example 4.3.1. (Annual Maximum Wind Speeds for Jacksonville, Florida.) The data in
Table 4.6 (stored in the ﬁle em–jwind.dat) were recorded by M.J. Changery19 and further
discussed in [34]. By reviewing weather maps for the days on which the annual maxima
occurred, Changery identiﬁed storms of two types, namely tropical and non–tropical.
Table 4.6. Annual maximum wind speeds (in mph) from 1950 to 1979 with tropical
storms indicated by T.
year mph type year mph type year mph type year mph type year mph type
1950
65
T
1956
44
1962
49
1968
47
T
1974
48
1951
38
T
1957
42
1963
56
1969
53
1975
68
1952
51
1958
38
1964
74
T
1970
40
1976
46
1953
47
1959
34
1965
52
T
1971
51
1977
36
T
1954
42
1960
42
T
1966
44
T
1972
48
1978
43
1955
42
1961
44
1967
69
1973
53
1979
37
A preliminary statistical analysis is carried out separately for the two subsamples.
A Gumbel modeling can be acceptable for both subsamples. The MLE(EV0) is (µ, σ) =
(43.6, 6.7) for the tropical and (µ, σ) = (44.1, 9.0) for the non–tropical data. For a
continuation see Example 6.1.2.
Tropical and non–tropical winds are selected at random by nature so that a
modeling of a daily maximum wind speed by means of a mixture distribution is
17Todorovic, P. and Rousselle, J. (1971). Some problems of ﬂood analysis. Water Re-
sour. Res. 7, 1144–1150
18Davison, A.C. and Smith, R.L. (1990). Models for exceedances over high thresholds.
J. R. Statist. Soc. B 52, 393–442.
19Changery, M.J. (1982). Historical extreme winds for the United States—Atlantic and
Gulf of Mexico coastlines. U.S. Nuclear Regulatory Commission, NUREG/CR–2639.

122
4. Extreme Value Models
adequate. Next, we follow the line of arguments by Rossi et al.20 which suggests a
modeling of the annual maximum by means of a two–component EV distribution.
Assume that X1, . . . , XN(1) and Y1, . . . , YN(2) are maxima over a certain
threshold. Assume that the Xi and Yi are distributed according to F1 and F2.
Under independence conditions and the condition that N(1) and N(2) are Pois-
son random variables one can verify (e.g., using the superposition of two Poisson
processes) that both sequences can be jointly modeled by a sequence Z1, . . . , ZN,
where N is a Poisson random variable with parameter λ = λ1 + λ2 and the Zi
have the common mixture distribution F(x) = p1F1(x) + p2F2(x) with p1 = λ1/λ
and p2 = λ2/λ. According to (1.54), the maximum of the Zi has the df
exp

−λ(1 −F(x))

= exp

−λ1(1 −F1(x))

exp

−λ2(1 −F2(x))

for x ≥0. This df is close to a two–component EV distribution if λ1, λ2 are small
and F1, F2 are close to GP dfs.
Presently, we do not have direct access to statistical inference in the two–
component EV model (beyond a visual one). Some further consideration will be
made in Section 6.1, where the identiﬁcation of the single components will be
reconsidered within the framework of censored data.
Gamma (Pearson–Type III) and χ2 Distributions
The gamma (Pearson–type III) distribution with shape parameter r > 0 was al-
ready introduced in (3.42) as a prior for a Bayesian estimation procedure. Gamma
distributions may also be employed as mixing distributions in various cases and
for the direct modeling of distributions of actual maxima.
The density of the standard gamma distribution with shape parameter r > 0
is
Gamma:
hr(x) =
1
Γ(r)xr−1e−x,
x > 0.
(4.6)
The exponential density is a special case for r = 1. From the relation Γ(r +
1) = rΓ(r), it follows that the mean and variance of the gamma distribution are
both equal to r.
Gamma dfs Hr are sum–reproductive in so far as the convolution is of the
same type: we have Hm∗
r
= Hmr. Generally, the sum of m independent gamma
random variables with parameters ri is a gamma random variable with parameter
r1 + · · · + rm. It is apparent that gamma dfs approach the normal df as r →∞.
For positive integers r = n + 1, one obtains the distribution of the sum of n + 1
iid exponential random variables Xi. In this case, the gamma df can be written
Hn+1(x) = 1 −exp(−x)
n

i=0
xi
i! ,
x > 0.
(4.7)
20Rossi, F., Fiorenti, M. and Versace, P. (1984). Two–component extreme value distri-
bution for ﬂood frequency analysis. Water Resour. Res. 20, 847–856.

4.3. Extended Extreme Value Models and Related Models
123
The gamma df is also called the Erlang df in this special case.
It is noteworthy that χ2–distributions with n degrees of freedom are gamma
distributions with shape and scale parameters r = n/2 and σ = 2. If X1, . . . , Xn
are iid standard normal random variables, then 
i≤n X2
i is a χ2 random variable
with n degrees of freedom.
Logistic Distributions as Mixtures of Gumbel Distributions
If {Fϑ} is a family of dfs and h is a probability density, then
Fh(x) =

Fϑ(x)h(ϑ) dϑ
(4.8)
is another df (the mixture of the dfs Fϑ with respect to h). If fϑ is the density of
Fϑ, then
fh(x) =

fϑ(x)h(ϑ) dϑ
is the density of Fh. We refer to Section 8.1 for a detailed discussion of the heuristics
behind the concept of mixing.
We show that a certain mixture of Gumbel distributions is a generalized lo-
gistic distribution, if the mixing is done with respect to a gamma distribution (a
result due to Dubey21). Therefore, the generalized logistic model can be an appro-
priate model for distributions of maxima in heterogeneous populations. Likewise,
one may deal with minima.
A Gumbel df with location and scale parameters µ and σ > 0 can be repa-
rameterized by
G0,µ,σ(x)
=
exp(−exp(−(x −µ)/σ))
=
exp(−ϑ exp(−x/σ)),
where ϑ = exp(µ/σ).
Mixing over the parameter ϑ with respect to the gamma density hr,β, with
shape and scale parameters r, β > 0, one obtains the mixture df
Fβ,r,σ(x)
=
 ∞
0
exp(−ϑ exp(−x/σ))hr,β(ϑ) dϑ
=
(1 + β exp(−x/σ))−r
(4.9)
which is a generalized logistic df with two shape parameters β, r > 0 and the scale
parameter σ > 0. If r = 1 and β = 1 and, thus, the mixing is done with respect to
the standard exponential df, then the mixture is a logistic df
F1,1,σ(x) = (1 + exp(−x/σ))−1
(4.10)
21Dubey, S.D. (1969). A new derivation of the logistic distribution. Nav. Res. Logist.
Quart. 16, 37–40.

124
4. Extreme Value Models
with scale parameter σ > 0.
If the Gumbel df is replaced by the Gompertz df 	G0,µ,σ(x) = 1 −G0,µ,σ(−x)
and ϑ = exp(−µ/σ), then the mixture is the converse generalized logistic df
	Fβ,r,σ(x) = 1 −(1 + β exp(x/σ))−r.
(4.11)
If β = 1/r, then the Gumbel df G0,0,σ and, respectively, the Gompertz df
	G0,0,σ are the limits of the (converse) generalized logistic dfs in (4.9) and (4.11)
as r →∞.
Log–Gamma (Log–Pearson–Type III) and GP–Gamma Distributions
If log(X) has the gamma density hr(x/σ)/σ for some scale parameter σ > 0, then
X has the density
Log–Gamma:
f1,r,α(x) =
αr
Γ(r)(log x)r−1x−(1+α),
x > 1,
(4.12)
for α = 1/σ. We see that the log–gamma distribution has two shape parameters
r, α > 0. For r = 1, one obtains the standard Pareto density with shape parameter
α.
r = 0.5
r = 1
r = 1.5
1
2
3
4
0
0.5
1
Fig. 4.3.
Log–gamma densities
for α = 1 and r = 0.5, 1, 1.5.
Figure 4.3 indicates that log–gamma distributions have upper tails similar
to those of Pareto (GP1) distributions, yet the shape can be completely diﬀerent
near the left endpoint.
In addition, if X is a log–gamma random variable, then −1/X has the density
GP–Gamma2:
f2,r,α(x) = |α|r
Γ(r)(−log |x|)r−1(−x)−(1+α),
−1 < x < 0,
for parameters α < 0, where we again changed the sign of the shape parameter.
The standard beta (GP2) densities are special cases for r = 1.
Within this system, the gamma density may be denoted by f0,r. The following
diagram shows the relationships between the diﬀerent models.

4.3. Extended Extreme Value Models and Related Models
125
Gamma
Exponential (GP0)
GP–Gamma 2
Beta (GP2)
Log–Gamma
Pareto (GP1)

?
6
log(X)
−1/X
A uniﬁed model is achieved by using the von Mises approach. For r > 0 and
γ ̸= 0, we have
GP–Gamma:
fr,γ(x) =
1
Γ(r)

log(1+γx)
γ
r−1
(1 + γx)−(1+1/γ)
for 0 < x, if γ > 0, and 0 < x < 1/|γ|, if γ < 0. The gamma densities fr,0 are
obtained in the limit as γ →0.
Fitting Gamma and Log–Gamma Distributions to Maxima
If r < 1 and γ < −1, then GP–gamma densities have a pole at zero and a mono-
tonic decrease. Thus, due to the second property, there is a certain similarity to
GP densities.
If r > 1, there is a greater similarity to EV densities:
• gamma densities have an exponential type upper tail and are tied down near
zero by means of a factor xr−1,
• log–gamma densities with location parameter µ = −1 have a Pareto type
upper tail and are tied down near zero by
(log(1 + x))r−1 ≈xr−1.
Therefore, gamma and log–gamma densities have shapes that correspond visually
to Fr´echet densities if r > 1.
A further extension of the preceding models is achieved if we start with
generalized gamma instead of gamma distributions; e.g., logarithmic half–normal
distributions are included in such models. Yet, in the subsequent lines we deal
with a diﬀerent kind of extensions starting with converse gamma and converse
generalized gamma distributions.
Generalized Gamma Distributions
If X is a gamma random variable with parameter r > 0, then X1/β is a generalized
gamma random variable with density
˜hr,β(x) =
β
Γ(r)xβr−1 exp(−xβ),
x ≥0,

126
4. Extreme Value Models
for β > 0. This transformation corresponds to that on page 38, where Weibull
(EV2) dfs were deduced from converse exponential dfs. We see that the generalized
gamma model includes the converse Weibull model (for r = 1). For β = 2 and
r = 1/2 we have the half–normal distribution.
The normal distribution is included if we also take double generalized gamma
distributions with densities of the form ˜hr,β(|x|)/2.
EV–Gamma Distributions
We indicate certain extensions of EV models by a diagram with transformations
as in the EV family.
EV–Gamma 0
Gumbel (EV0)
Converse Gamma
Converse Exponential
EV–Gamma 1
Fr´echet (EV1)
Converse Generalized Gamma
Weibull (EV2)
-
?
6

log(X)
−(−X)−1/α
−1/X
Within the EV–Gamma 1 model one obtains the L´evy distribution (cf. (6.18))
which is a sum–stable density with index 1/2 and a certain skewness parameter. A
von Mises representation is again possible. The applicability of such models must
be still explored.
The Two–Parameter Beta Family
We mention also the beta distributions in the usual parameterization that are
given by
fa,b(x) = xa−1(1 −x)b−1
B(a, b)
,
0 ≤x ≤1,
(4.13)
for shape parameters a, b > 0, where
B(a, b) =
 1
0
xa−1(1 −x)b−1 dx
is the beta function. Notice that the beta densities in the GP2 model constitute
a special case; we have w1,α,1,1 = f1,−α. Recall that B(a, b) = Γ(a)Γ(b)/Γ(a + b),
where Γ is the gamma function.

Chapter 5
Generalized Pareto Models
This chapter deals once again with the central topic of this book, namely with
exceedances (in other words, peak–over–threshold values) over high thresholds
and upper order statistics. One may argue that this chapter is richer and more
exciting than the preceding one concerning maxima. The role of extreme value
(EV) dfs is played by generalized Pareto (GP) dfs.
5.1
Estimation in Generalized Pareto Models
Let x1, . . . , xn be the original data which are governed by a df F. We deal with
upper extremes which are either
• the exceedances y1, . . . , yk over a ﬁxed threshold u, or
• the k upper ordered values {y1, . . . , yk} = {xn−k+1:n, . . . , xn:n}, where k is
ﬁxed.
Recollect from (2.4) that a nonparametric estimate of the exceedance df F [u]
is given by the sample df Fk = Fk(y; ·) based on the exceedances y1, . . . , yk over
u. Likewise, the exceedance density or qf can be estimated by means of a sample
exceedance density fk or the sample exceedance qf F −1
k .
Now we want to pursue a parametric approach. We assume that the actual
exceedance df F [u] can be replaced by some GP df W with left endpoint u. For
example, we estimate the exceedance df F [u] by means of a GP df Wγk,u,σk (in
the γ–representation), where γk and σk are estimates of the shape and scale pa-
rameters. It is advisable to add the threshold u to the sample when a parametric
estimation based on exceedances is executed.
We also deal with likelihood–based estimator, namely MLEs and Bayesian
estimators, based on exceedances. For that purpose, exceedances are dealt with
like iid data. A justiﬁcation for this approach can be found in Section 8.1, page
234.

128
5. Generalized Pareto Models
Recall that the threshold value u is replaced by xn−k:n in the case of upper
ordered values. Another random threshold is obtained for a data–driven choice of
k, see page 137.
The Exponential Model GP0(u)
The statistical model for the exceedances over the threshold u consists of all ex-
ponential dfs with left endpoint u.
GP0(u):
{W0,u,σ : σ > 0}.
Estimators for the GP0(u) model:
• MLE(GP0): the MLE of the scale parameter is σk = 1
k

i≤k(yi −u). It is
apparent that the MLE is also a moment estimator.
• M–Estimator (GP0): an M–estimate of the scale parameter σ, which may be
regarded as a robustiﬁed MLE (see (3.7)), is obtained as the solution to

i≤k

exp

−yi −u
σb

−
b
1 + b

= 0.
For b →∞, the likelihood equation is attained.
We refer to Section 3.5 for Bayesian estimators in the exponential model.
Example 5.1.1. (Continuation of Example 4.1.1 about the Annual Floods of the Feather
River.) According to the insight gained from Example 4.1.1, we may already expect that
an exponential distribution can be ﬁtted to the upper part of the annual maximum ﬂood
(which is indeed conﬁrmed). From the MLE(GP0) with k = 25, we get the exponential
distribution with location and scale parameters µk = 36, 577 and σk = 45, 212. The 50
and 100–year ﬂood levels are u(50) = 213, 450 and u(100) = 244, 789. We see that the
100–year ﬂood level is exceeded by none of the 59 annual maximum discharges.
If one is not sure whether the exponential model is acceptable, one should
alternatively carry out the estimation within the Pareto model or the uniﬁed gen-
eralized Pareto model which are handled below.
The Restricted Pareto Model GP1(u, µ = 0)
This is just the exponential model GP0(u) transformed by T (x) = log x (as out-
lined on page 37). Therefore, the estimators correspond to those in the exponential
model. We consider a one–dimensional Pareto model parameterized by the shape
parameter α, namely,
GP1(u, µ = 0):
{W1,α,0,u : α > 0}.

5.1. Estimation in Generalized Pareto Models
129
This is an appropriate model for exceedances over the threshold u because the left
endpoints of these Pareto dfs are equal to u.
The shape parameter α can be represented as a functional parameter because

log(x/u) dW1,α,0,u(x) = 1/α .
(5.1)
Estimators of the shape parameter α within the model GP1(u, µ = 0):
• Hill1: this is the in the restricted Pareto model GP1(u, µ = 0). We have
αk = k
 
i≤k
log(yi/u).
(5.2)
Recall that the threshold u is replaced by xn−k:n in the case of upper ordered
values. Then, the estimate can be written
αk = k
 
i≤k
log(xn−i+1:n/xn−k:n)
(5.3)
which is the Hill estimator in the original form. The Hill estimator may be
introduced as a sample functional
αk = 1

log(x/u) d Fk(x) ,
where Fk is the sample exceedance df (replacing W1,α,0,u in equation (5.1)).
For a discussion about the performance of the Hill estimator we refer to the
subsequent page and the lines around the Figures 5.1 and 5.2.
• M–Estimator in the restricted Pareto model GP1(u, µ = 0): the estimate2 of
the shape parameter α that corresponds to the M–estimate of σ in the GP0
model is the solution to

i≤k
yi
u
−α/b
−
b
1 + b

= 0.
For b →∞, the Hill estimate is received.
• Bayes estimators for the model GP1(u, µ = 0)3: take the gamma density
p(α) = hs,d(α),
(5.4)
1Hill, B.M. (1975). A simple general approach to inference about the tail of a distri-
bution. Ann. Statist. 3, 1163–1174.
2Reiss, R.–D., Haßmann, S. and Thomas, M. (1994). XTREMES: Extreme value analysis
and robustness. In [15], Vol. 1, 175–187.
3Early references are Hill1 and Rytgaard, M. (1990). Estimation in the Pareto distri-
bution. ASTIN Bulletin 20, 201–216.

130
5. Generalized Pareto Models
cf. (3.42), as a prior for the shape parameter α. This Bayes estimator corre-
sponds to the one in the exponential model with unknown reciprocal scale
parameter, see page 102. The likelihood function satisﬁes
L(y|α) ∝αk exp

−α

i≤k
log(yi/u)

,
yi > u,
(5.5)
in α. Consequently, the posterior density satisﬁes p(α|y) ∝L(y|α)hs,d(α),
and one obtains
p(α|y) = hs′,d′(α)
(5.6)
for the posterior density which is the gamma density with parameters s′ =
s + k and d′ = d + 
i≤k log(yi/u). The Bayes estimate, as the mean of the
posterior density, is
α∗
k(y)
=

αp(α|y) dα
=
s + k
d + 
i≤k log(yi/u) .
(5.7)
We see that the Bayes estimate is close to the Hill estimate if s and d are
small or/and k is large.
The explicit representation of the Hill and the Bayes estimators facilitated
the development of an asymptotic theory. In addition, these estimators possess
an excellent performance if this one–dimensional model is adequate. Supposedly,
to check the performance of these estimators, simulations were usually run under
standard Pareto distributions with location parameter µ = 0.
Also, the Hill estimator has been applied to data in a lot of research papers.
Yet, this does not necessarily imply that the Hill estimator and related estimators
should be used in applications. We refer to the comments around Figures 5.1 and
5.2 for a continuation of this discussion.
What should be done if the diagnostic tools—like the sample mean or median
excess function—in Chapter 2 indicate that the GP1(u, µ = 0) model is signiﬁ-
cantly incorrect, as, e.g., in the case of ﬁnancial data in Chapter 16. It is likely
that one of these estimators was applied because a heavy–tailed or, speciﬁcally, a
Pareto–like tail of the underlying df was conjectured.
It suggests itself to carry out the statistical inference within the full model
GP1(u) of Pareto dfs with left endpoint u which are introduced in the next sub-
section.
For dfs in the full Pareto GP1(u) model we have
W1,α,µ,σ = W1,α,µ,u−µ ≈W1,α,0,u,
(5.8)
if u is suﬃciently large. Therefore, estimators especially tailored for the submodel
GP1(u, µ = 0) are of a certain relevance for the full Pareto model GP1(u) in

5.1. Estimation in Generalized Pareto Models
131
asymptotic considerations. Yet, for ﬁnite sample sizes the approximation error
in (5.8) may cause a larger bias and, thus, an unfavorable performance of the
estimator.
Also see Section 6.5, where theoretical results are indicated that the approx-
imation error is of a smaller order for small shape parameters α.
The Full Pareto GP1(u) Model: (α, µ, σ)–Parameterization
The model of all Pareto dfs (in the α–parameterization) with shape parameter
α > 0 and left endpoint u is given by
GP1(u):
{W1,α,µ,σ : α, σ > 0, µ real, µ + σ = u}.
It is apparent that the restricted Pareto model GP1(u, µ = 0) is a submodel of the
model GP1(u) with µ = 0.
The MLE in the GP1(u) model corresponds to the MLE in the generalized
Pareto model and does not exist in the present Pareto model with a certain positive
probability.
The Full Pareto GP1(u) Model: (α, η)–Parameterization
The GP1(u) model can be parameterized by two parameters α, η > 0. For every
df W1,α,µ,σ with µ + σ = u,
W1,α,µ,σ(x)
=
W1,α,µ,u−µ
=
W1,α,u(1−η),uη(x)
=
1 −

1 + (x −u)/u
η
−α
,
x ≥u,
(5.9)
where η = 1 −µ/u has the nice property of being a scale parameter for the
normalized excesses (x −u)/u.
One obtains a reparameterized full Pareto model (for exceedances over u)
(α, η)–parameterization:
{W1,α,u(1−η),uη : α, η > 0}.
(5.10)
In this representation, the restricted Pareto model GP1(u, µ = 0) is a submodel
with η = 1.
Within the reparameterized model of Pareto dfs one gets some closer insight
why the Hill estimator, and related estimators such as the Bayes estimator in (5.7),
are inaccurate even in such cases where a Paretian modeling is acceptable.
Notice that a larger scale parameter η puts more weight on the upper tail of
the df. If such a Pareto df is estimated, yet η = 1 is kept ﬁxed in the modeling, then
the heavier weight in the tail, due to a larger parameter η, must be compensated
by a smaller estimated shape parameter α.

132
5. Generalized Pareto Models
In the following illustration, we show simulated densities of the Hill estimator
based on k = 45 largest Pareto data out of n = 400. The number of simulations
is N = 4000. Whereas the shape and scale parameters are kept ﬁxed, we vary the
location parameter of the underlying Pareto distribution.
As expected, the density of the Hill estimator of α is nicely centered around
the true shape parameter α = 10 if µ = 0 because the underlying df belongs to
the restricted Pareto model GP1(u, µ = 0).
2
4
6
8
10
12
14
16
18
20
22
0.0
0.5
1
Fig. 5.1. Simulated densities of Hill
estimator under the Pareto distri-
butions with shape parameter α =
10, σ = 1 and µ = −1, 0, 1 from
left to right.
The Hill estimator has a bad performance for the other cases (for a continu-
ation see Fig. 5.2); e.g., for µ = −1 it centers around a value 3.
Bayes Estimation in the Full Pareto Model, (α, η)–Parameterization
Bayes estimators for the model GP1(u), in the (α, η)–parameterization, were dealt
with by Reiss and Thomas4. For a more recent article see Diebolt et al.5
As a prior density take
p(α, η) = hs,d(α)f(η),
(5.11)
where hs,d is a gamma density taken as a prior for α, and f, as a prior for η, is
another probability density which will be speciﬁed later. The likelihood function
for the present model is determined by
L(y|α, η) ∝(α/η)k exp

−(1 + α)

i≤k
log

1 + yi −u
ηu

,
yi > u.
(5.12)
4Reiss, R.-D. and Thomas, M. (1999). A new class of Bayesian estimators in Paretian
excess–of–loss reinsurance. ASTIN Bulletin 29, 339–349.
5Diebolt, J., El–Aroui, M.–A., Garrido, M. and Girard, S. (2005). Quasi–conjugate
Bayes estimates for GPD parameters and application to heavy tails modelling. Extremes
8, 57–78.

5.1. Estimation in Generalized Pareto Models
133
Deduce the posterior density
p(α, η|y) = hs′,d′(η)(α) ˜f(η),
(5.13)
where hs′,d′(η) is again a gamma density with parameters s′ = s + k and d′(η) =
d + Σi≤k log(1 + (yi −u)/(ηu)). The probability density ˜f is characterized by
˜f(η) ∝η−k(d′(η))−s′ exp

−

i≤k
log

1 + yi −u
ηu

f(η).
(5.14)
Simple calculations yield that the Bayes estimates of α and η are
α∗
k(y) =

αp(α, η|y) dαdη =

s′
d′(η)
˜f(η) dη,
(5.15)
and
η∗
k(y) =

ηp(α, η|y) dαdη =

η ˜f(η) dη.
(5.16)
We see that the Bayesian estimator of α is just the estimator in (5.7), if the
prior distribution—and, thus, also the posterior—is a point measure with mass
equal to one at σ = 1.
We remark that gamma priors were chosen for the parameter α in the re-
stricted model because they possess the nice property of being conjugate priors.
This property still holds in the full model in so far that the conditional posterior
for α is again a gamma distribution. Such a natural choice seems not to exist for
the scale parameter σ. As priors for η one may also take, e.g., reciprocal gamma
distributions as introduced in (3.43). For a continuation see Section 8.3.
If the Pareto modeling is not adequate for the actual fat or heavy–tailed df,
then one may choose one of the models introduced in Section 5.5.
The Beta Model (GP2)
We shortly mention the model of beta (GP2) distributions with upper endpoint
equal to zero and scale parameter u. The left endpoint of the distribution is −u.
Recall from Section 1.4 that these distributions form a submodel of the family of
beta distributions as dealt with in the statistical literature. In addition, a negative
shape parameter is taken.
GP2(u):
{W2,α,0,u : α < 0}.
The GP1 model can be transformed to the GP2 model by means of the transfor-
mation T (x) = −1/x (cf. page 37 again). Thus, an MLE corresponding to the Hill
estimator in the GP1 model, therefore called Hill(GP2) estimator, becomes appli-
cable. Shift the data below zero if an upper endpoint other than zero is plausible.

134
5. Generalized Pareto Models
The Generalized Pareto Model
The given model is
GP(u):
{Wγ,u,σ : γ real, σ > 0},
where u and σ are the location and scale parameters. Notice that the Pareto df
W1,α,µ,u−µ in (5.8) is equal to Wγ,u,σ with σ = γ(u −µ).
Estimators γk of γ include:
• MLE(GP): the MLE in the GP model must be evaluated by an iteration
procedure6. The remarks about the MLE(EV) also apply to the present
estimator.
• Moment(GP): the moment estimator7 takes the form γ1,k + γ2,k, where
γ1,k = 1/αk is the reciprocal of the Hill estimator. The second term γ2,k
is constructed by means of lj,k = 
i≤k(log(yi/u))j
k for j = 1, 2. We have
γ2,k = 1 −1/

2

1 −l2
1,k/l2,k

.
Roughly speaking, γ1,k (respectively, γ2,k) estimates the shape parameter γ
if γ ≥0 (if γ ≤0) and γ1,k (respectively, γ2,k) is close to zero if γ ≤0 (if
γ ≥0).
Warning! The moment estimator has an excellent performance in
general, yet it should not be applied to a full data set of exceedances.
In that case, one obtains irregular estimates.
• Drees–Pickands(GP): an LRSE of the shape parameter γ in the GP model
can be deﬁned in similarity to that in the EV model, cf. page 111. Let
γk = log(ˆr)/ log(1/a)
with q0 = 1 −q, q1 = 1 −aq and q3 = 1 −a2q, where 0 < q, a < 1. By
taking q = 4i/k and a = 1/2, one obtains the Pickands8 estimate, which
corresponds to a Dubey estimate (cf. page 111) in the EV model9. We have
γk,i = log
 yk−i:k −yk−2i:k
yk−2i:k −yk−4i:k
 
log 2
6Prescott, P. and Walden, A.T. (1980). Maximum likelihood estimation of the param-
eters of the generalized extreme–value distribution. Biometrika 67, 723–724.
7Dekkers, A.L.M., Einmahl, J.H.J. and de Haan, L. (1989). A moment estimator for
the index of an extreme–value distribution. Ann. Statist. 17, 1833–1855.
8Pickands, J. (1975). Statistical inference using extreme order statistics. Ann. Statist.
3, 119–131.
9A closely related estimator may be found in the article by Weiss (1971). Asymptotic
inference about a density function at an end of its range. Nav. Res. Logist. Quart. 18,
111–114.

5.1. Estimation in Generalized Pareto Models
135
for i ≤[k/4]. A linear combination 
i≤[k/4] ck,iγk,i with estimated optimal
scores ck,i was studied by H. Drees10. Estimates of γ based on two and,
respectively, ﬁve Pickands estimates were dealt with by M. Falk11 and T.T.
Pereira12.
• Slope(GP): the slope β1,k of the least squares line (see (2.40)), ﬁtted to the
mean excess function right of the k largest observations, is close to γ/(1−γ)
and, therefore, γk = β1,k/(1 + β1,k) is a plausible estimate of γ.
• L–Moment Approach: This method will be introduced in Section 14.4 in
conjunction with ﬂood frequency analysis.
In addition, σ may be estimated by a least squares estimator σk (a solution to
(2.40) for µ = u with Φ replaced by Wγk), where γk is the Moment, Drees–Pickands
or slope estimator.
If γ > −1/2, then one should apply the MLE(GP) or Moment(GP). The
Drees–Pickands estimator possesses remarkable asymptotic properties, yet the ﬁ-
nite sample size performance can be bad for γ > 0. Yet, we believe that the concept
of LRSEs has also the potential to provide accurate estimators for smaller sample
sizes in such cases. In addition, estimators of that type can be especially tailored
such that certain additional requirements are fulﬁlled. The Slope(GP) estimator
is only applicable if γ < 1.
Estimation Based on the Original Data
Let αk, γk and σk be the estimates of α, γ and σ based on
(a) the exceedances y1, . . . , yk over u, or
(b) the upper ordered values yi = xn−i+1:n,
as deﬁned in the preceding lines. Moreover, we assume that the original data
x1, . . . , xn are available. Put αk,n = αk and γk,n = γk. Derive µk,n and σk,n from
u and σk as described on page 58. Then, one may use Wγn,k,µn,k,σn,k as a parametric
estimate of the upper tail of the underlying df F. Recall that the sample df Fn(x; ·)
is a global estimator of F.
Likewise, use the pertaining parametric densities and qfs for estimating the
underlying density f and qf F −1 near the upper endpoints. Thereby, we obtain
parametric competitors of the kernel density and the sample qf.
10Drees, H. (1995). Reﬁned Pickands estimators of the extreme value index. Ann.
Statist. 23, 2059–2080.
11Falk, M. (1994). Eﬃciency of convex combinations of Pickands estimator of the ex-
treme value index. Nonparametric Statist. 4, 133–147.
12Pereira, T.T. (2000). A spacing estimator for the extreme value index. Preprint.

136
5. Generalized Pareto Models
Diagram of Estimates
The choice of the threshold u or, likewise, the number k of upper extremes corre-
sponds to the bandwidth selection problem for the kernel density in Section 2.1.
Such a choice can be supported visually by a diagram. Thereby, estimates γk,n,
µk,n and σk,n are plotted against the number k of upper ordered values.
The following properties become visible:
• if k is small, then there is a strong ﬂuctuation of the values γk,n for varying
k;
• for an intermediate number k of extremes, the values of the estimates γk,n
stabilize around the true value γ,
• ﬁnally, if k is large, then the model assumption may be strongly violated and
one observes a deviation of γk,n from γ.
In other words, if k is small, then the variance of the estimator is large and the
bias is small (and vice versa); in between, there is a balance between the variance
and the bias and a plateau becomes visible.
We start with a negative result: the Hill diagram for 400 Pareto data gen-
erated under the shape, location and scale parameters α = 10, µ = −1, σ = 1
provides a smooth curve, yet it is diﬃcult to detect the aforementioned plateau
in Fig. 5.2 (left). This is due to the fact that the Hill estimator is inaccurate for
larger α and µ ̸= 0, also see (6.38).
number k of extremes
Hill estimates
100
300
0.0
2
4
6
Pareto data
mean excess
0.2
0.4
0.0
0.2
Fig. 5.2. (left.) Hill diagram for 400 Pareto data generated under the shape, location and
scale parameters α = 10, µ = −1, σ = 1. (right.) Pareto mean excess function pertaining
to Hill (dashed) and MLE(GP) (dotted) estimates based on k = 45 upper extremes;
sample mean excess function (solid).
For real data, one may compare, e.g., the mean excess function, which belongs
to the estimated parameters, with the sample mean excess function in order to
check the adequacy of a parametric estimate. In Fig. 5.2 (right), one recognizes

5.1. Estimation in Generalized Pareto Models
137
again that the Hill estimate is not appropriate. The MLE(GP) with values α =
−20.23 (respectively, γ = −0.05) is acceptable.
In Fig. 5.2, the estimation was carried out within the ideal model for the
MLE(GP) and, hence, the performance of the MLE(GP) can be improved when
the number k is equal to the sample size n = 400. The situation changes for Fr´echet
data. Recall from (1.47) that a Fr´echet df is close to a Pareto df in the upper tail.
number k of extremes
MLE (GP) estimates
100
300
1
Fig. 5.3. MLE(GP) estimates αk,n
plotted against k = 1, . . . , 400 for
standard Fr´echet data under the
shape parameter α = 1.
In Fig. 5.3, a plateau with right endpoint k = 80 becomes visible. Therefore,
we suggest to base the estimation on this number of upper extremes.
Automatic Choice of the Number of Extremes
The selection of the right endpoint of a plateau may also be done in an auto-
matic manner. The following ad hoc procedure works reasonably well. Let γk,n
be estimates of the shape parameter γ based on the k upper extremes (likewise,
one may deal with estimates αk,n of α). Denote the median of γ1,n, . . . , γk,n by
med(γ1,n, . . . , γk,n). Choose k∗as the value that minimizes
1
k

i≤k
iβ|γi,n −med(γ1,n, . . . , γk,n)|
(5.17)
with 0 ≤β < 1/2. A slight smoothing of the series of estimates improves the
performance of the procedure for small and moderate sample sizes. Modiﬁcations
are obtained, for example, by taking squared deviations and γk,n in place of the
median.
Other selection procedures may be found in the papers by Beirlant et al.13
13Beirlant, J., Vynckier, P. and Teugels, J.L. (1996). Tail index estimation, Pareto
quantile plots, and regression diagnostics. JASA 91, 1659–1667.

138
5. Generalized Pareto Models
and by Drees and Kaufmann14 and in the literature cited therein. We also refer
to Cs¨org˝o et al.15 for the smoothing of tail index estimators.
Peaks–Over–Threshold and Annual Maxima Methods
Besides selecting exceedances or upper ordered values from the original data
x1, . . . , xn, there is another method of extracting extremes, namely by taking max-
ima out of blocks (see (1.5) and Chapter 4). Therefore, there are two diﬀerent
approaches to the estimation of the tail index α or γ of a df:
• based on exceedances or upper extremes as dealt with in this section (pot
method),
• based on maxima within certain subperiods (annual maxima method).
For the second approach, one must assume that the original data x1, . . . , xn
are observable. Let
{x(j−1)m+1, . . . , xjm},
j = 1, . . . , k,
(5.18)
be a partition of x1, . . . , xn, where n = mk, and take the maximum yj out of the
jth block.
Assume that the xi are governed by a df F. If the block size m is large, then
an EV df can be accurately ﬁtted to the actual df F m of the maximum. Yet, one
must cope with the disadvantage that the number k of maxima is small (and vice
versa).
According to foregoing remarks, the EV df G has the same shape parameter
as the GP df W corresponding to the exceedances. For discussions concerning the
eﬃciency of both approaches, we refer to [42], Section 9.6 and to Rasmussen et
al.16.
Random Thresholds
If the k largest values are selected, then xn−k:n may be regarded as a random
threshold. The index is also random either if the sample sizes are random or as in
the case of automatically selected thresholds (in (5.17)). Other choices are
• the smallest annual maximum,
14Drees, H. and Kaufmann, E. (1998). Selection of the optimal sample fraction in
univariate extreme value estimation. Stoch. Proc. Appl. 75, 149–172
15Cs¨org˝o, S., Deheuvels, P. and Mason, D.M. (1985). Kernel estimates of the tail index
of a distribution. Ann. Statist. 13, 1050–1078.
16Rasmussen, P.F., Ashkar, F., Rosbjerg, D. and Bob´ee, B. (1994). The pot method
for ﬂood estimation: a review, 15–26 (in [13]).

5.1. Estimation in Generalized Pareto Models
139
• the threshold such that, on the average, the number of selected extremes in
each year is equal to 3 or 4.
For the latter two proposals, we refer to a paper by Langbein17.
Extrapolation
In this section, a GP df was ﬁtted to the upper tail of the sample df, thus also to
the underlying df F in the region where the exceedances are observed. Using this
parametric approach, it is possible to construct estimates of certain functional
parameters of F with variances smaller than those of nonparametric estimates.
This advantage is possibly achieved at the cost of a larger bias if the parametric
modeling is inaccurate.
There will be a new maximum or observations larger than the present ones
in the near future. These observations are outside of the region where a GP df
was ﬁtted to the data. Statistical questions concerning future extremes should
be approached with the following pragmatic attitude. Of course we do not know
whether the insight gained from the data can be extrapolated to a region where
no observation has been sampled. Keep your ﬁngers crossed and evaluate the risk
of future extreme observations under the estimated df.
In the long run, more information will be available and the modeling should
be adjusted again.
Estimation of Parameters When a Trend Must be Dealt With
In Section 4.1, pages 113–116, we studied the question of estimating location pa-
rameters µ(ti; β0, . . . , βp) of EV dfs which depend on the time ti. Thus we assumed
that there is certain trend in the data yi that are measurements at time ti. For
that purpose we
• estimated the trend in the data by the least squares method;
• employed parametric estimators in the EV model to the residuals,
• deduced estimators of the parameters µ(ti; β0, . . . , βp).
Now, we assume that exceedances yi over a threshold u are observed within
a certain time period. The exceedances yi at the exceedance times ti follow GP
dfs Wγ,u,σ(ti;β0,...,βp). Thus, we allow a trend in the scale parameter σ.
The aim is to estimate γ and a trend function σ(t; β0, . . . , βp) in the scale
parameter. Let nj be the number of exceedances within a jth sub–period. We
implicitly assume stationarity for the exceedances in each sub–period. We proceed
in the following manner:
17Langbein, W.B. (1949). Annual ﬂoods and the partial duration ﬂood. Transactions
Geophysical Union 30, 879–881.

140
5. Generalized Pareto Models
• determine estimates ˆγj and ˆσj for each sub–period;
• take ˆγ = 
j njˆγj
 
j nj as an estimate of the stationary parameter γ,
• evaluate a trend function for the scale parameter by ﬁtting a regression line
to the σj.
Example 5.1.2. (Ozone Cluster Maxima18.) The data set used in this study has been
collected by the Mexico City automatic network for atmospheric monitoring (RAMA)
(from the south monitoring station called Pedregal). The data are the daily maxima
of ozone levels measured in parts per million from July 1987 to April 1999, stored in
mexozone.dat.
ozone ppm
year
1987
1990
1993
1996
1999
0.0
0.1
0.2
0.3
0.4
Fig. 5.4. Time series of daily maximum ozone levels from Pedregal Monitoring Station
from July 1987 to April 1999.
We take eleven twelve month periods from July to June and one last period from
July to April. The data yi are cluster maxima taken according to the run length deﬁnition
with a minimum gap of two days. In Table 5.1 we listed the estimates of the scale
parameters σj for the threshold u = 0.22.
Table 5.1. Scale parameters σ of estimated GP distributions for the threshold u = .22
and estimated shape parameter γ = −.35.
year
87
88
89
90
91
92
93
94
95
96
97
98
σ
.066
.061
.066
.082
.092
.068
.050
.051
.046
.041
.043
.028
18Villase˜nor, J.A., Vaquera, H. and Reiss, R.–D. (2001). Long–trend study for ground
level ozone data from Mexico City. In: 2nd ed. of this book, Part V, pages 353–358.

5.1. Estimation in Generalized Pareto Models
141
The regression curve ﬁtted to the σj is exp(−0.003t2 −0.34t −0.01). Similar calcu-
lations can be carried out within the exponential model where the shape parameter γ is
equal to zero.
This data set is also studied in Example 6.5.2 in conjunction with the question
of penultimate approximations.
Estimation Based on the Annual k Largest Order Statistics.
Up to now we investigated two diﬀerent methods of extracting upper extremes
from a set of data, namely, to take maxima out of blocks or to take exceedances
over a higher threshold, where the latter includes the selection of the largest k
values out of the data set. One may as well select the k largest values out of each
block. If k = 1, this reduces to the usual blocks method, whereas for larger k this
method is closer to the pot–method.
A parametric statistical model for the k largest order statistics is suggested
by limiting distributions as in the case of maxima and exceedances (see, e.g., [42],
Section 5.4). One may take submodels, which are the Gumbel, Fr´echet and Weibull
models if k = 1, or a uniﬁed model in the von Mises parameterization.
Example 5.1.3. (Exceptional Athletic Records.) The data (communicated by Richard
Smith), illustrated in Fig. 5.5 and stored in records.dat, concern women’s 1500m and
3000m best performances from 1972 to 1992. M.E. Robinson and J.A. Tawn19 and R.L.
Smith20 analyzed these data in view of an improvement of the 3000 m record from 502.62
to 486.11 seconds in the year 1993.
Conﬁdence intervals and, respectively, prediction intervals showed evidence that the
outstanding record in 1993 is inconsistent with the previous performances. Some insight
into the nature of the record in 1993 would be also gained by computing T–year records
based on the historical data.
From the scatterplots one recognizes trends and an “Olympic year” eﬀect in the
data. In view of the similar nature of both data sets and the small sample sizes, one could
take into account a pooling of the data, cf. Section 14.3.
Asymptotic Distributions of Estimators and
Conﬁdence Intervals in the Generalized Pareto Model
We ﬁrst deal with the case of iid random variables with common GP df.
19Robinson, M.E. and Tawn, J.A. (1995). Statistics for exceptional athletics records.
Appl. Statist. 44, 499–511.
20Smith, R.L. (1997). Statistics for exceptional athletics records: letter to the editor.
Appl. Statist. 46, 123–127.

142
5. Generalized Pareto Models
year
records 1500
1972
1976
1980
1984
1988
1992
235
240
245
250
year
records 3000
1972
1976
1980
1984
1988
1992
510
530
550
Fig. 5.5. Women’s best performances for 1500m (left) and 3000m (right).
• (The MLE in the GP1 Model.) We formulate the asymptotic normality of the
MLE ˆαk in the GP1 model in terms of the γ–parameterization to make the
result easily comparable to the performance of the MLE in the GP model.
For iid random variables with common df W1,α,0,σ, the reciprocal MLE 1/ˆαk
is asymptotically normal with mean γ = 1/α and variance γ2/k.
• (The MLE in the GP Model.) The MLE (ˆγk, ˆσk) of (γ, σ) in the GP model is
asymptotically normal with mean vector (γ, σ) and covariance matrix Σ/k,
where
Σ = (1 + γ)
⎛
⎝
(1 + γ)
σ
σ
2σ2
⎞
⎠
if γ > 1/2. In particular, the asymptotic variance of ˆγk is (1 + γ)2/k. This
result is taken from the paper by Davison and Smith mentioned on page 121.
We see that the asymptotic variance of 1/ˆαk is much smaller than that of ˆγk
for γ close to zero. Yet, as noted on page 136, the MLE in the GP1 model can
have a large bias if the special Pareto assumption is violated.
The estimation based on random exceedances or upper order statistics can
be reduced to the preceding problem concerning iid random variables by using the
conditioning devices in Section 8.1, page 234.
In the ﬁrst case, one must deal with a random number K of exceedances
over a threshold u: apply the preceding results conditioned on K = k. Then, the
asymptotic normality holds for the estimators 1/ˆαK and (ˆγK, ˆσK) and variances
and covariance matrices with k replaced by E(K). Secondly, u is replaced by the
order statistic Xn−k:n of n iid random variables Xi. The MLEs are computed
conditioned on Xn−k:n = u. The resulting estimators have the same performance
as 1/ˆαk and (ˆγk, ˆσk). Such calculations can be carried out for a larger class of
estimators.

5.2. Testing Within Generalized Pareto Models
143
We remark that 1/ˆαK and (ˆγK, ˆσK) are MLEs in a pertaining point process
model (see [43]). Moreover, these results remain valid under distributions which
are suﬃciently close (in the upper tail) to a GP df. We refer to (6.35) for an
appropriate technical condition.
Based on the asymptotic normality result for estimators it is straightforward
to construct conﬁdence intervals for the parameters (as it was done in Section
3.2). It would also be of interest to deal with conﬁdence bands and testing in
conjunction with reliability functions such as the mean excess function. Yet, this
goal is not further pursued in this book.
5.2
Testing Within Generalized Pareto Models
For testing an exponential upper tail of a distribution against other GP–type tails
one may again use the sample skewness coeﬃcient. Subsequently, we prefer the
sample coeﬃcient which leads to test statistic introduced by Hashofer and Wang21
We also include the LR test for the exponential model. The tests are either based
on exceedances over a threshold u or on the k + 1 largest order statistics.
Use the conditioning arguments given at the end of Section 8.1 to ﬁx the
critical values.
A Test Based on the Sample Coeﬃcient of Variation
The coeﬃcient of variation var1/2
F /mF is the standard deviation divided by the
mean. In the following we use the reciprocal squared coeﬃcient. This value is equal
to 1−2γ for GP distributions Wγ,0,σ with γ < 1/2. For exceedances y1, . . . , yk over
a threshold u one may take the scale invariant test statistic (¯yk −u)2/s2
k. Such a
test statistic was proposed by Gomes and van Montfort22.
Using ordered values one obtains the corresponding (location and scale in-
variant) test statistic
(¯xk −xn−k:n)2
1
k−1

i≤k

xn−i+1:n −¯xk
2 ,
(5.19)
where ¯xk = 1
k

i≤k xn−i+1:n. This is the test statistic introduced by Hashofer and
Wang for testing γ = 0 against γ ̸= 0. The asymptotic considerations by Hashofer
and Wang were based on results in a paper by Weissman23 which is of interest in
21Hashofer, A.M. and Wang, Z. (1992). A test for extreme value domain of attraction.
JASA 87, 171–177.
22Gomes, M.I. and van Montfort, M.A.J. (1986). Exponentiality versus generalized
Pareto, quick tests. Proc. 3rd Internat. Conf. Statist. Climatology, 185–195.
23Weissman, I. (1978). Estimation of parameters and large quantiles based on the k
largest observations. JASA 73, 812–815.

144
5. Generalized Pareto Models
its own right; alternatively, one may use results in [48], pages 136–137, and the
conditioning argument.
The local, asymptotic optimality of tests based on such test statistics was
proven by Marohn (article cited on page 119).
Likelihood Ratio (LR) Test for the Exponential (GP0) Model
Within the GP model, one tests H0 : γ = 0 against H1 : γ ̸= 0 with unknown scale
parameters σ > 0. Thus, the exponential hypothesis is tested against other GP
distributions. For a given vector y = (y1, . . . , yk) of exceedances over the threshold
u, the likelihood ratio (LR) statistic is
TLR(y) = 2 log
 
i≤k
wˆγ,u,ˆσ(yi)
 
i≤k
w0,u,˜σ(yi)

with (ˆγ, ˆσ) and ˜σ denoting the MLEs in the GP and GP0 models. The p–value is
pLR(y) = 1 −χ2
1(TLR(y)).
The p–value, modiﬁed with a Bartlett correction, is
p(y) = 1 −χ2
1

TLR(x)/(1 + b/k)

for b = 4.0. Recall that u is replaced by xn−k:n if the largest ordered values are
taken instead of exceedances.
5.3
Testing Extreme Value Conditions
with Applications
co–authored by J. H¨usler and D. Li24
We introduce two methods of testing one dimensional extreme value conditions
and apply them to two ﬁnancial data sets and a simulated sample.
Introduction and the Test Statistics
Extreme value theory (EVT) can be applied in many ﬁelds of interest as hydrology,
insurance, ﬁnance, if the underlying df F of the sample is in the max–domain of
attraction of an extreme value distribution Gγ, denoted by F ∈D(Gγ). We suppose
commonly that the random variables X1, X2, . . . , Xn are iid with df F such that
for some real γ there exist constants an > 0 and reals bn with
lim
n→∞F n(anx + bn) = Gγ(x) := exp

−(1 + γx)−1/γ
(5.20)
24both at the Department of Math. Statistics, University of Bern.

5.3. Testing Extreme Value Conditions with Applications
145
where 1 + γx > 0. In case of γ = 0, G0(x) is interpreted as exp(−e−x), cf. also
(1.25).
Under the extreme value condition (5.20), the common statistical approach
consists of estimating the extreme value index γ and the normalizing constants
bn and an which are the location and scale parameters. Then based on these
estimators, one can deduce quantiles, tail probabilities, conﬁdence intervals, etc.
The goodness–of–ﬁt is usually analyzed in a qualitative and subjective manner
by some graphical devices, as, e.g., by Q–Q plots and excess functions, which can
easily be drawn with Xtremes, cf. also the remarks about “the art of statistical
modeling” on page 61.
However the extreme value condition does not hold for all distributions. For
example, the Poisson distribution and the geometric distribution are not in D(Gγ)
for any real γ (see, e.g., C.W. Anderson25, or Leadbetter et al. [39]). Thus, before
applying extreme value procedures to maxima or exceedances one should check
the basic null–hypothesis
H0 :
F ∈D(Gγ)
for some real γ.
We are going to introduce two tests in (5.21) and, respectively, (5.22) which
we call test E and test T.
Dietrich et al.26 present the following test E assuming some additional second
order conditions. They propose to use the test statistic
En := k
 1
0
log Xn−[kt],n −log Xn−k,n
ˆγ+
−t−ˆγ−−1
ˆγ−
(1 −ˆγ−)
2
tη dt
(5.21)
for some η > 0. It is shown that En converges in distribution to
Eγ : =
 1
0

(1 −γ−)(t−γ−−1W(t) −W(1)) −(1 −γ−)2 t−γ−−1
γ−
P
+ t−γ−−1
γ−
R + (1 −γ−)R
 1
t
s−γ−−1 log s ds
2
tη dt,
where γ+ = max{γ, 0}, γ−= min{γ, 0}, W is Brownian motion, and the random
variables P and R are some integrals involving W (for details see Dietrich et al.).
The estimates ˆγ+ and ˆγ+ for γ+ and γ−, respectively, are ﬁxed to be the moment
estimator, cf. page 134.
Thus the limiting random variable Eγ depends in general on γ and η, only.
However, if γ ≥0, the limiting distribution does not depend on γ, since the terms
(t−γ−−1)/γ−are to be interpreted as −log t. Dietrich et al. state the result for
25Anderson, C.W. (1970). Extreme value theory for a class of discrete distributions
with application to some stochastic processes. J. Appl. Probab. 7, 99–113.
26Dietrich, D., de Haan, L. and H¨usler, J. (2002). Testing xtreme value conditions.
Extremes 5, 71–85.

146
5. Generalized Pareto Models
η = 2, but it is easy to extend the result to any η > 0. This approach is based
on sample qfs and of Gγ. They propose in addition another test statistic which
is valid for γ > 0. But it seems that this test statistic is no so good as the test
statistic En.
For testing H0, one needs to choose an η and to derive the quantiles of the
limiting random variable Eγ, which can be done by simulations for some γ, see
Table 5.2 below.
Table 5.2. Quantiles Qp,γ of the limiting random variable Eγ for the test E with η = 2.
p
γ
0.10
0.30
0.50
0.70
0.90
0.95
0.975
0.99
≥0
0.028
0.042
0.057
0.078
0.122
0.150
0.181
0.222
−0.1
0.027
0.041
0.054
0.074
0.116
0.144
0.174
0.213
−0.2
0.027
0.040
0.053
0.072
0.114
0.141
0.169
0.208
−0.3
0.027
0.040
0.054
0.073
0.113
0.140
0.168
0.206
−0.4
0.027
0.040
0.054
0.073
0.114
0.141
0.169
0.207
−0.5
0.027
0.040
0.054
0.073
0.115
0.141
0.169
0.208
−0.6
0.027
0.040
0.054
0.074
0.116
0.144
0.173
0.212
−0.7
0.028
0.041
0.055
0.074
0.118
0.147
0.176
0.218
Then in applications one continues as follows:
• First, estimate ˆγ+ and ˆγ−by the moment estimator and calculate the value
of the test statistic En.
• Secondly, determine the corresponding quantile Q1−α,ˆγ by linear interpola-
tion using the values of Table 5.2. Here ˆγ = ˆγ+ + ˆγ−and α is usually 0.05.
Moreover we use Q1−α,ˆγ = Q1−α,−0.7 if ˆγ < −0.7.
• Finally, compare the value of En with the quantile Q1−α,ˆγ. If En > Q1−α,ˆγ,
then reject H0 with nominal type I error α.
Drees et al.27 propose a test T restricted to γ > −1/2, assuming also some
additional second order conditions. For each η > 0, their test statistic
Tn := k
 1
0
n
k
¯Fn

ˆan/k
x−ˆγ −1
ˆγ
+ ˆbn/k

−x
2
xη−2 dx
(5.22)
converges in distribution to Tγ :=
 1
0

W(x)+Lγ(x)
2xη−2 dx, where W is Brown-
ian motion, and the process Lγ depends on the asymptotic distribution of (ˆγ, ˆa, ˆb),
27Drees, H., de Haan, L. and Li, D. (2006). Approximations to the tail empirical distri-
bution function with application to testing extreme value conditions. J. Statist. Plann.
Inf. 136, 3498–3538.

5.3. Testing Extreme Value Conditions with Applications
147
which is some
√
k–consistent estimator of (γ, a, b). Now proceed as in case of test
E based on the quantiles in Table 5.3.
Table 5.3. Quantiles Qp,γ of the limiting random variable Tγ for the test T with η = 1.0.
p
γ
0.10
0.30
0.50
0.70
0.90
0.95
0.975
0.99
4
0.086
0.123
0.161
0.212
0.322
0.393
0.462
0.558
3
0.085
0.120
0.156
0.205
0.307
0.372
0.440
0.532
2
0.083
0.116
0.150
0.195
0.286
0.344
0.402
0.489
1.5
0.082
0.115
0.148
0.192
0.282
0.340
0.400
0.480
1
0.082
0.114
0.146
0.189
0.276
0.330
0.388
0.466
0.5
0.083
0.116
0.149
0.194
0.285
0.343
0.404
0.481
0.25
0.085
0.119
0.153
0.120
0.295
0.355
0.415
0.499
0
0.089
0.126
0.163
0.213
0.319
0.388
0.455
0.542
−0.1
0.091
0.129
0.168
0.221
0.330
0.400
0.471
0.569
−0.2
0.093
0.133
0.174
0.231
0.350
0.425
0.500
0.604
−0.3
0.096
0.139
0.183
0.242
0.369
0.449
0.531
0.653
−0.4
0.100
0.145
0.192
0.256
0.393
0.484
0.576
0.690
−0.45
0.103
0.150
0.199
0.320
0.416
0.511
0.605
0.735
−0.499
0.107
0.157
0.210
0.338
0.439
0.546
0.652
0.799
The exact formulas of Lγ and Tγ are known for the MLE (for MLEs see
Smith, cf. page 111, and Drees et al.28) which depend on γ and η, only. Similar to
the test E, the test T can be applied for testing F ∈D(Gγ), but only if γ > −1/2.
Note that this second approach is based on the sample df.
H¨usler and Li29 discuss the tests E and T by extensive simulations. If F ̸∈
D(Gγ), then the power depends on how ’close’ F is to some F ∗∈D(Gγ). For
instance, if F = Poisson(λ) with λ small, the tests detect with high power the
alternative. However, if λ is large, so the Poisson df could be approximated by a
normal one, then the power is getting small. In summary, they suggest to
1. choose η = 2 for the test E and η = 1 (or maybe also η = 2) for the test T;
2. if the extreme value index γ seems to be positive, then both tests can be
applied equally well to test H0; otherwise the test E is preferable.
28Drees, H., Ferreira, A. and de Haan, L. (2004). On maximum likelihood estimation
of the extreme value index. Ann. Appl. Probab. 14, 1179–1201.
29H¨usler, J. and Li, D. (2006). On testing extreme value conditions. Extremes 9, 69–86.

148
5. Generalized Pareto Models
Data and Analysis
We apply the two tests to ﬁnancial data. First, two examples consider log–returns,
cf. Section 16.1, of daily equity over the period 1991–2003 for ABN ARMO and for
the ING bank with sample size n = 3283. The other data are log–returns of daily
exchange rates Dutch guilder–U.S. dollar and British pound–U.S. dollar during
the period from January 1, 1976 to April 28, 2000 with sample size n = 6346.
For each data mentioned, we ﬁrst calculate the maximum likelihood estima-
tor (MLE) and moment estimator (Mon.) of the extreme value index, which are
presented in the upper right plots in the ﬁgures for varying k. Secondly we derive
the test statistics Tn and En and their corresponding 0.95 quantiles, which are
shown in the two lower plots in each Fig. 5.6 to 5.9, also with varying k.
Fig. 5.6 and 5.7 show the data, the ML and the moment estimators for γ
depending on the chosen number k and the two tests E and T applied to the
largest k observations for the ING and the ABN AMRO example.
0
500
1000
1500
2000
2500
3000
−0.2
−0.1
0.0
0.1
0.2
Index
daily log return
0
100
200
300
400
500
−0.3
−0.2
−0.1
0.0
0.1
0.2
0.3
k
γ
MLE
Mon.
0
100
200
300
400
500
0.0
0.1
0.2
0.3
0.4
0.5
0.6
k
Tn
test statistic
0.95 quantile
0
100
200
300
400
500
0.00
0.05
0.10
0.15
0.20
0.25
0.30
k
En
test statistic
0.95 quantile
Fig. 5.6. Log–return of daily equity of ING bank (n = 3283).
It is evident that the test E is more sensitive to the selection of k; with large
k usually the null–hypothesis is rejected by the test E. Each test provides a hint
on the largest possible selection of k, preventing us to select a too large k. Note
that we should select k such that k/n →0 as n →∞. Thus, a k up to 200 or up
to 300 may be chosen in the ING and ABN ARMO examples, respectively. This is
either based on the visual inspection of the γ plots or much better by the behavior

5.3. Testing Extreme Value Conditions with Applications
149
of the tests shown in the test plots. In this perspective, the test E seems be more
sensitive. Since both Tn and En are much smaller than their corresponding 0.95
quantiles for a large range of k (k/n is from 3% to 10%), both tests tend not
to reject the null–hypothesis in both examples for the mentioned k’s. So we may
assume for each example that the underlying F ∈D(Gγ).
0
500
1000
1500
2000
2500
3000
−0.15
−0.10
−0.05
0.00
0.05
0.10
0.15
Index
daily log return
0
100
200
300
400
500
−0.3
−0.2
−0.1
0.0
0.1
0.2
0.3
k
γ
MLE
Mon.
0
100
200
300
400
500
0.0
0.1
0.2
0.3
0.4
0.5
0.6
k
Tn
test statistic
0.95 quantile
0
100
200
300
400
500
0.00
0.05
0.10
0.15
0.20
0.25
0.30
k
En
test statistic
0.95 quantile
Fig. 5.7. Log–return of daily equity of ABN ARMO bank (n = 3283).
The situation is similar for the daily exchange rate examples, see Fig. 5.8
and 5.9. Both tests reject the null–hypothesis for large k, with k/n > 10% in the
British pound–U.S. dollar exchange rate example. But again the test T is less
sensitive with respect to k. This test T does not reject the null–hypothesis even
for very large k in the Dutch guilder–U.S. dollar example.
We also apply the two tests to a simulated sample from Poisson(λ) distribu-
tion with sample size n = 3000 and λ = 10 and 100 (see Figures 5.10 and 5.11). As
mentioned before F = Poisson(10) or Poisson(100) are not in the max–domain and
are not close to a normal distribution. Again the data and the MLE and the mo-
ment estimators are given. For k not small, both tests reject the null–hypothesis,
so indicating that both Poisson(10) and Poisson(100) are not in the max–domain.
Note that the test E has a smoother behavior than the test T which may be
inﬂuenced by the strange behavior of the MLE for these data. Also we see that
the test E rejects the null–hypothesis already for smaller k than the test T .

150
5. Generalized Pareto Models
Fig. 5.8. Log–return of daily exchange rate British pound–U.S. dollar.
Fig. 5.9. Log–return of daily exchange rate Dutch guilder–U.S. dollar.

5.3. Testing Extreme Value Conditions with Applications
151
0
500
1000
1500
2000
2500
3000
0
5
10
15
20
25
Index
X
0
100
200
300
400
500
−3
−2
−1
0
1
2
3
k
γ
MLE
Mon.
0
100
200
300
400
500
0
20
40
60
80
100
120
k
Tn
test statistic
0.95 quantile
0
100
200
300
400
500
0
2
4
6
8
k
En
test statistic
0.95 quantile
Fig. 5.10. One sample from Poisson(10) distribution (n = 3000).
0
500
1000
1500
2000
2500
3000
60
80
100
120
140
Index
X
0
100
200
300
400
500
−1
0
1
2
3
4
5
k
γ
MLE
Mon.
0
100
200
300
400
500
0.0
0.5
1.0
1.5
2.0
2.5
3.0
k
Tn
test statistic
0.95 quantile
0
100
200
300
400
500
0.0
0.5
1.0
1.5
k
En
test statistic
0.95 quantile
Fig. 5.11. One sample from Poisson(100) distribution (n = 3000).

152
5. Generalized Pareto Models
5.4
Statistics in Poisson–GP Models
Assume that the original data x1, . . . , xn are governed by the df F. In contrast
to the preceding sections, the exceedances over a given threshold u are grouped
within certain cells.
Let u be a high threshold and consider a partition
u = u1 < u2 < · · · < um < um+1 = ∞
(5.23)
of the interval [u, ∞). The following considerations will be only based on the
frequencies n(i) of data in the m upper cells [ui, ui+1). Notice that k = 
i≤m ni
is the number of exceedances over u.
Note that the frequencies n(i) are part of a multinomial scheme with cell
probabilities pi := F(ui+1) −F(ui). If 1 −F(u) is suﬃciently small, so that a
Poisson approximation corresponding to (1.4) is applicable, then one may assume
that the n(1), . . . , n(m) are mutually independent and each n(i) is governed by a
Poisson distribution Pλi with parameter λi = npi.
We study the estimation within certain submodels which are deduced from
Pareto (GP1) or generalized Pareto (GP) dfs.
The Poisson–GP1 Model
Assume that F is suﬃciently close to some Pareto df W1,α,0,σ above the threshold
u and, hence, F [u] is close to W1,α,0,u. Then, the parameters λi = npi can be
replaced by
λi(α, σ) = nσα(u−α
i
−u−α
i+1),
i = 1, . . . , m,
where u−α
m+1 = 0.
The MLEs αm and σm of the parameters α and σ in the Poisson–GP1 model
must be evaluated numerically, see [16], page 140.
Example 5.4.1. (Increasing the Upper Limit in Liability Insurance.) Subsequently, we
consider the question that there is an upper limit of one million, which makes the whole
aﬀair relatively harmless, and it is intended to increase the covering to the amount of
four millions. Therefore, we are interested in the tail probabilities p1 = F(4) −F(1) and
p2 = 1−F(4), where F is the claim size distribution of those claim sizes over the priority
(threshold) of 0.03 millions. Our calculations will be based on a data set of grouped claim
sizes from the year 1983 (see Table 5.4; stored in im–claim.dat).
Table 5.4. Frequencies of claims within priorities in millions
priorities ui
0.03
0.05
0.1
0.2
0.3
0.4
0.5
1
frequencies n(i)
147
89
35
11
7
5
4
3

5.4. Statistics in Poisson–GP Models
153
According to the routine visualization of the data, it is plausible to accept the Pareto
assumption. By applying the MLE in the Poisson–GP1 model, one obtains the estimate
ˆα = 1.28. The pertaining estimates of the tail probabilities are ˆp1 = 0.0093 and ˆp2 =
0.0019. These considerations indicate that an increase of the premiums by a smaller
amount is suﬃcient. This calculation would lead to a disaster if the covering is removed,
because very large claim sizes may occur with a probability which cannot be neglected.
The MLE in the Poisson–GP1 model has the same deﬁciencies as the Hill
estimator. Therefore, we also compute the MLE in an enlarged model.
The Poisson–GP Model
This is a model deduced from generalized Pareto (GP) dfs Wγ,µ,σ with shape,
location and scale parameters γ, µ and σ. If F is suﬃciently close to Wγ,µ,σ, then
the truncated df F [u] is close to Wγ,u,η with η = σ + γ(u −µ) (cf. (1.45)). This
indicates that the problem of computing an MLE can be reduced to maximizing
a likelihood function in the parameters γ and η.
First, the parameters λi = npi are replaced by
λi(γ, µ, σ) = nσ1/γϕi(γ, η),
i = 1, . . . , m,
where
ϕi(γ, η) =

η + γ(ui −u)
−1/γ −

η + γ(ui+1 −u)
−1/γ,
i = 1, . . . , m −1,
and ϕm(γ, η) =

η + γ(um −u)
−1/γ. Because 
i≤m ϕi(γ, η) = η−1/γ, the log–
likelihood function as a function in γ, η and σ is
l(γ, η, σ) = k log σ
γ
+

i≤m
ni log ϕi(γ, η) −n
σ
η
1/γ
+ C,
(5.24)
where C is a constant. The solution to the likelihood equation (∂/∂σ)l = 0 is
σ = η(k/n)γ.
(5.25)
Furthermore, the parameter µ can be regained from γ and η by
µ = u −η

1 −(k/n)γ
/γ.
(5.26)
By inserting the right–hand side of (5.25) in (5.24), we see that γ and η must be
determined as the values maximizing
˜l(γ, η) = k(log η)/γ +

i≤m
ni log ϕi(γ, η).
(5.27)
This must be done numerically.

154
5. Generalized Pareto Models
5.5
The Log–Pareto Model and
Other Pareto–Extensions
First, we deal with mixtures of exponential, Pareto and converse Weibull distri-
butions with respect to the Gamma distribution. We obtain the log–Pareto and
the Burr model as an extension of the Pareto model. Secondly, the Benktander
II and the truncated converse Weibull models are introduced which include the
exponential and Pareto models.
Mixtures of Generalized Pareto Distributions
Recall from (4.8) that
Fh(x) =

Fϑ(x)h(ϑ) dϑ
is the mixture of the dfs Fϑ with respect to density h. We give two examples in
the present context:
• (Pareto Distributions as Mixtures of Exponential Distributions.) First, we
show that mixtures of exponential dfs
Fϑ(x) = 1 −e−ϑx,
x > 0,
with mean 1/ϑ > 0 are Pareto dfs, if the parameter ϑ is determined by
sampling with respect to a gamma density hα(x) = xα−1e−x/Γ(α), x > 0,
with shape parameter α > 0, cf. (3.42). The mixture
 ∞
0
Fϑ(x)hα(ϑ) dϑ = 1 −(1 + x)−α,
x > 0,
(5.28)
is a Pareto df with shape, location and scale parameters α, −1 and 1. For a
continuation of this topic we refer to (16.30).
• (Log–Pareto Distributions as Mixtures of Pareto Distributions.) The log–
Pareto df Lα,β with shape parameters α, β > 0 possesses a super–heavy
upper tail. The term “super–heavy” is characterized by the property that
the log–transformation leads to a df with a heavy tail. We have
Lα,β(x)
=
W1,α,−1,1/β(log x)
=
1 −(1 + β log x)−α,
x ≥1,
where W1,α,−1,1/β is the Pareto df with shape, location and scale parameters
α, −1 and 1/β.
The log–Pareto df Lα,β can be represented as a mixture of Pareto dfs with
respect to the gamma density hα,β with shape and scale parameters α, β > 0.

5.5. The Log–Pareto Model and Other Pareto–Extensions
155
Straightforward computations yield
 ∞
0
W1,z(x)hα,β(z)dz = Lα,β(x),
x ≥1.
Note that mixing with respect to a Dirac measure leads to a Pareto distri-
bution again.
Log–Pareto distributions within a generalized exponential power model are
studied by Desgagn´e and Angers 30. In a technical report31 associated to an article
by Diebolt et al., cited on page 132, these authors mention another mixture dis-
tribution, diﬀerent from the log–Pareto one, with super–heavy tails. Log–Pareto
random variables as innovations in an autoregressive process are studied by Zeevi
and Glynn32.
The Full Log–Pareto Model as an Extension
of the Pareto Model
We present another parametrization of log–Pareto distributions with left endpoint
of the support being equal to zero, and add a scale parameter, thus getting the
full log–Pareto model.
Let X be a random variable with df
&
Wα,β(x) = 1 −(1 + x/β)−α,
x > 0,
(5.29)
which is a Pareto df with shape and scale parameters α > 0 and β > 0.
Then, the transformed random variable (σ/β)

exp(X) −1

has the log–
Pareto df
˜Lα,β,σ(x) = 1 −

1 + 1
β log

1 + βx
σ
−α
,
x > 0,
(5.30)
with shape parameters α, β > 0 and scale parameter σ > 0. We have
˜Lα,β,σ(x) →β→0 &
Wα,σ(x),
x > 0,
(5.31)
and, therefore, the log–Pareto model can be regarded as an extension of the Pareto
model.
By repeating this procedure one gets models of loglog–Pareto dfs and, gener-
ally, iterated log–Pareto dfs with “mega–heavy” tails as extensions of the Pareto
model. Notice that log–moments and iterated log–moments of such distributions
are inﬁnite.
30Desgagn´e, A. and Angers, J.–F. (2005). Importance sampling with the generalized
exponential power density. Statist. Comp. 15, 189–195.
31see http://www.inria.fr/rrt/rr-4803.html
32Zeevi, A. and Glynn, P.W. (2004). Recurrence properties of autoregressive processes
with super–heavy–tailed innovations. J. Appl. Probab. 41, 639–653.

156
5. Generalized Pareto Models
The Burr Model as an Extension of the Pareto Model
Notice that X1/β is a standard Pareto random variable with shape parameter αβ,
if β > 0 and X is a standard Pareto random variable with shape parameter α.
Such a conclusion is no longer valid if a location parameter is added. The random
variable (X −1)1/β has a Burr df
Fα,β(x) = 1 −(1 + xβ)−α,
x ≥0,
(5.32)
with shape parameters α, β > 0. The density is
fα,β(x) = αβxβ−1(1 + xβ)−(1+α),
x ≥0.
Burr distributions can be represented as mixtures of converse Weibull dfs.
For β, ϑ > 0, let
	Gβ,ϑ(x) = 1 −exp(−ϑxβ),
x > 0,
be a Weibull df on the positive half–line. By mixing again with respect to the
gamma density hα over the parameter ϑ, one easily obtains
 ∞
0
	Gβ,ϑ(x)hα(ϑ) dϑ = 1 −(1 + xβ)−α = Fα,β(x),
x > 0.
(5.33)
For β = 1, one ﬁnds the Pareto df as a mixture of exponential distributions.
Burr distributions with parameter β > 1 have shapes that visually correspond
to Fr´echet densities.
Benktander II Distributions
We include two further families of distributions for modeling the tails of a distri-
bution, namely the Benktander II and truncated converse Weibull distributions.
Both types of distributions are well known within insurance mathematics and may
be of interest in general.
Formula (2.28) for mean excess functions can be employed to design a family
of dfs—called Benktander II dfs33—so that the mean excess functions are equal
to xb/a for a > 0 and −a ≤b < 1. Recall that such functions are attained by the
mean excess functions of converse Weibull dfs with shape parameter α = b −1 as
u →∞.
The standard Benktander II dfs with left endpoint equal to 1 are given by
Fa,b(x) = 1 −x−b exp

−
a
1 −b

x1−b −1

,
x ≥1,
(5.34)
where a > 0 and −a ≤b < 1 are two shape parameters.
33Benktander, G. (1970). Schadenverteilung nach Gr¨osse in der Nicht–Leben–Ver-
sicherung. Mitteil. Schweiz. Verein Versicherungsmath., 263–283

5.5. The Log–Pareto Model and Other Pareto–Extensions
157
Benktander II dfs have the interesting property that truncations F [u]
a,b are
Benktander II dfs again in the form Fau1−b,b,0,u with location and scale parameters
µ = 0 and σ = u. This is the pot–reproductivity of Benktander II distributions.
Also,
• for b = 0, the Benktander II df is the exponential df with location and scale
parameters µ = 1 and σ = 1/a, and
• for b →1, one reaches the standard Pareto df with shape parameter α = 1+a
in the limit.
Truncated Converse Weibull Distributions
It is analytically simpler to work with truncations of converse Weibull dfs them-
selves. These dfs have the additional advantage that we need not restrict our
attention to Pareto dfs with shape parameter α > 1.
Consider dfs
H1,α,β(x) = 1 −exp

−
α
1 −β

x1−β −1

,
x ≥1,
(5.35)
where α > 0 and β are two shape parameters.
By truncating converse Weibull dfs with shape and scale parameters ˜α < 0
and σ > 0 left of u = 1, one obtains dfs H1,α,β with β = 1 + ˜α and α = |˜α|σ ˜α.
Remember that converse Weibull densities have a pole at zero and a monotonic
decrease if −1 < ˜α < 0 (which is equivalent to 0 < β < 1).
The density and qf are
h1,α,β(x) = αx−β exp

−
α
1 −β

x1−β −1

,
x ≥1
(5.36)
and
H−1
1,α,β(q) =

1 −1 −β
α
log(1 −q)
1/(1−β)
.
(5.37)
In agreement with Benktander II dfs, the df H1,α,β is
• an exponential df with location and scale parameters µ = 1 and σ = 1/α, if
β = 0, and
• a standard Pareto df with shape parameter α in the limit as β →1.
In particular, it is understood that H1,α,1 is deﬁned as such a limit in (5.35).
Truncated converse Weibull dfs are pot–reproductive in so far as a truncation is
of the same type. We have H[u]
1,α,β = H1,αu1−β,β,0,u.
We present a model for exceedances yi over a threshold u:
{H1,α,β,0,u : α > 0, 0 ≤β ≤1}.
(5.38)

158
5. Generalized Pareto Models
For computing the MLE in that model, deduce from the ﬁrst likelihood equa-
tion for the parameters α and 0 < b < 1 that
1
α = 1
k

i≤k
(yi/u)1−β −1
1 −β
.
By inserting the value of α in the second likelihood equation, one ﬁnds
 
i≤k
log(yi/u)
1
k

i≤k
(yi/u)1−β −1
1 −β

+

i≤k
∂
∂β
(yi/u)1−β −1
1 −β
= 0.
This equation must be solved numerically.
In the ﬁrst equation, the results are that we gain
• the MLE for σ in the exponential model if β = 0, and
• the Hill estimate for α in the limit as β →1.
If the estimated β is suﬃciently close to 1, then a Pareto hypothesis is jus-
tiﬁed. The threshold u may be replaced by an upper ordered value xn−k:n again.
As in the GP case, a truncated converse Weibull distribution can be ﬁtted to the
upper tail of a distribution.
Including a Scale Parameter
in the Truncated Converse Weibull Model
A scale parameter σ > 0 should be included in the truncated converse Weibull
model. This will be done in conjunction with a representation as in the GP
case. Recall that the Pareto df Wγ in the γ–parameterization is the Pareto df
W1,α,−α,α, where γ = 1/α > 0. Likewise, let Hγ,β be a truncated converse Weibull
df H1,α,β,−α,α with location and scale parameters −α and α, where γ = 1/α > 0.
We have
Hγ,β(x) = 1 −exp

−
1
γ(1 −β)

(1 + γx)1−β −1

,
x ≥0,
for shape parameters γ > 0 and 0 ≤β ≤1. It is apparent that Hγ,β is
• the exponential df W0, if β = 0,
• the Pareto df Wγ in the limit as β →1, and
• W0 in the limit as γ →0.
Consider
{Hγ,β,u,σ : γ, σ > 0, 0 < β ≤1}
(5.39)
as a statistical model for exceedances over a threshold u. A pot–reproductivity
also holds in the extended framework.

Chapter 6
Advanced Statistical
Analysis
Section 6.1 provides a discussion about non–random and random censoring. Es-
pecially, the sample df is replaced by the Kaplan–Meier estimate in the case of
randomly censored data. In Section 6.2 we continue the discussion of Section 2.7
about the clustering of exceedances by introducing time series models and the ex-
tremal index. The insight gained from time series such as moving averages (MA),
autoregressive (AR) and ARMA series will also be helpful for the understanding
of time series like ARCH and GARCH which provide special models for ﬁnancial
time series, see Sections 16.7 and 16.8.
Student distributions provide further examples of heavy–tailed distributions
besides Fr´echet and Pareto distributions, see Section 6.3. Gaussian distributions
are limits of Student distributions when the shape parameter α goes to inﬁnity.
Further distributions of that kind are sum–stable distributions with index α < 2
which will be discussed in Section 6.4. Gaussian distributions are sum–stable with
index α = 2.
Rates of convergence towards the limiting distribution of exceedances and
the concept of penultimate distributions are dealt with in Section 6.5. We also
indicate a relationship between the concepts of pot–stability and regularly varying
functions.
Higher order asymptotics for extremes is adopted in Section 6.6 to establish
a bias reduction for estimators. This method is, e.g., useful to repair the Hill
estimator.
6.1
Non–Random and Random Censoring
This section deals with the nonparametric estimation of the df by means of the
Kaplan–Meier estimator (also called the product–limit estimator) and with esti-

160
6. Advanced Statistical Analysis
mators in EV and GP models based on randomly censored data. Smoothing the
Kaplan–Meier estimator by a kernel leads to nonparametric density estimators.
We start with remarks about the ﬁxed censoring.
Fixed Censoring
In the ﬁxed censorship model, one distinguishes two diﬀerent cases:
• (Type–I Censoring.) The data below and/or above a certain threshold u are
omitted from the sample.
• (Type–II Censoring.) Lower and/or upper order statistics are omitted from
the sample.
One also speaks of left/right censoring, if merely the lower/upper part of the
sample is omitted. Taking exceedances over a threshold or taking the k largest
ordered values is a left censoring (of Type–I or Type–II) of the sample. It would
be of interest to deal with such censorship models in conjunction with EV models1.
Fixed Right Censoring in Generalized Pareto Models
Let yi be the exceedances over a threshold u, yet those values above c > u are
censored. Thus, we just observe zi = min(yi, c). A typical situation was described
in Example 5.4.1, where c is the upper limit of an insurance policy.
This question can be dealt with in the following manner:
• if the number of yi ≥c is small and the statistical procedure for continuous
data—as described in the preceding sections—is robust against rounding, we
may just neglect the censoring,
• the number of censored data is the number of yi in the cell [c, ∞). If we take
a partition of [u, ∞) as in (5.23) with um = c, we may apply the procedures
already presented in Section 5.4.
Randomly Censored Data
First, let us emphasize that our main interest concerns the df Fs under which the
survival times x1, . . . , xn are generated. Yet, data (z1, δ1), . . . , (zn, δn) are merely
observed, where
• zi = min(xi, yi) is xi censored by some censoring time yi,
• δi = I(xi ≤yi) provides the information as to whether censoring took place,
where δi = 1 if xi is not censored, and δi = 0 if xi is censored by yi.
1For relevant results see Balakrishnan, N. and Cohen, A.C. (1991). Order Statistics
and Inference. Academic Press, Boston.

6.1. Non–Random and Random Censoring
161
Note that this procedure may be regarded as a right censoring with respect
to a random threshold. A left censoring, where zi = max(xi, yi), can be converted
to a right censoring by changing the signs. Then, we have −zi = min(−xi, −yi).
Example 6.1.1. (Stanford Heart Transplantation Data.) We provide an example of cen-
sored survival times referring to the Stanford heart transplantation data. These data
consist of the survival times (in days) of n = 184 patients who underwent a heart trans-
plantation during the period from t0 ≡Oct. 1, 1967 to t1 ≡Feb. 27, 1980 at Stanford.
We have
• the survival times x1, . . . , xn, which are of primary interest, and
• the censoring data yi = t1 −ui, where ui ∈[t0, t1] is the date of operation. The
survival time xi is censored by yi if the patient is alive at time t1.
Table 6.1. Survival times xi and censoring variables δi.
survival (in days)
0 1 1 1 2 3 · · · 2805 2878 2984 3021 3410 3695
censoring value
1 1 1 0 0 1 · · ·
0
1
0
0
0
0
The full data set, taken from the collection by Andrews and Herzberg (cf. page 85),
is stored in the ﬁle mc–heart.dat.
Subsequently, let the xi and yi be generated independently from each other
under the survival df Fs and the censoring df Fc. In the preceding example, one
may assume that Fc is the uniform df on the interval [0, t1 −t0]. Additionally, Fc
is a degenerate df in the ﬁxed censorship model. Our aim is to estimate the df Fs
based on the zi and δi. As in (1.6), check that the censored values zi are governed
by the df
H
=
1 −(1 −Fs)(1 −Fc)
=
Fs + (1 −Fs)Fc.
(6.1)
Note further that ω(H) = min(ω(Fs), ω(Fc)). One may distinguish the following
situations.
• The fraction of censored data is small and the censoring may be regarded
as some kind of a contamination that is negligible (such a case occurs when
ω(Fc) ≥ω(Fs) and Fc(x) is small for x < ω(Fs)). Yet, in this case, it can still
be desirable to adopt procedures according to the present state–of–the–art
censorship models.
• If the fraction of censored data is large, then the usual sample df Fn based
on the censored data zi will not accurately estimate the target df Fs. It is
necessary to apply procedures specially tailored for censored data.

162
6. Advanced Statistical Analysis
• When using the Kaplan–Meier estimate one can recognize from the data
whether ω(Fc) < ω(Fs). Then we have F(x) < 1 = H(x) for ω(Fc) ≤x <
ω(Fs). Notice that the observed zi and δi contain no information about the
form of the survival time df Fs beyond ω(Fc). One may try to estimate Fs(x)
for x ≤ω(Fc). Then, extrapolate this result to the region beyond ω(Fc) by
means of a parametric modeling.
The Kaplan–Meier Estimate
Assume again that the survival times x1, . . . , xn are generated under the df Fs.
Let z1:n ≤. . . ≤zn:n denote the ordered z1, . . . , zn. If there are ties in the zi’s, the
censored values are ranked ahead of the uncensored ones. Deﬁne the concomitant
δ[i:n] of zi:n by δ[i:n] := δj if zi:n = zj. The role of the usual sample df is played by
the Kaplan–Meier estimate F ∗
n. We have
F ∗
n(x)
=
1 −

zi:n≤x

1 −
δ[i:n]
n −i + 1

=

i≤n
wi,nI(zi:n ≤x),
(6.2)
where
wi,n =
δ[i:n]
n −i + 1

j≤i−1

n −j
n −j + 1
δ[j:n]
.
The wi,n are constructed in the following manner. At the beginning, each zi:n has
the weight 1/n. Let zi1:n ≤. . . ≤zim:n be the censored ordered values. Then, let
wi,n = 1/n for i = 1, . . . , i1 −1 and wi1,n = 0. The weight 1/n, initially belonging
to zi1:n, is uniformly distributed over zi1+1:n ≤zi1+2:n ≤. . . ≤zn:n. Each of the
remaining ordered values will then have the total weight 1/n + 1/(n(n −i1)). This
procedure is continued for i2, . . . , im. We see that 
i≤n wi,n = 1 if δ[n:n] = 1. The
weight

j≤n−1
((n −j)/(n −j + 1))δ[j:n]
eventually put on zn:n is omitted if im = n [which is equivalent to δ[n:n] = 0].
Then, 
i≤n wi,n < 1 yielding limx→∞F ∗
n(x) < 1.
From this construction of the Kaplan–Meier estimate, it is apparent which
part of the information contained in the data is still available. One can recapture
the ordered uncensored data and the number of censored data between consecutive
ordered uncensored data.
In analogy to (2.2), the Kaplan–Meier estimate F ∗
n(x) is approximately equal
to the survival df Fs(x), x < ω(H), for suﬃciently large sample sizes n, written

6.1. Non–Random and Random Censoring
163
brieﬂy2
F ∗
n(x) ≈Fs(x),
x < ω(H).
(6.3)
This relation still remains valid for x = ω(H) if Fs is continuous at this point.
Then, we also have F ∗
n(zn:n) ≈Fs(ω(H)), which settles the question as to whether
one can tell from the data that Fs(ω(H)) < 1. This entails that the zi and δi
contain no information about an upper part of the survival time df Fs.
The Kernel Density Based on Censored Data
A kernel estimate of the density f of Fs is obtained by a procedure corresponding
to that in Section 2.1 with weights 1/n replaced by wi,n. Let
fn,b(x) = 1
b

i≤n
wi,n k
x −zi:n
b

.
According to our preceding explanations, fn,b is an estimate of the survival time
density f for x < ω(H). It is advisable to employ a right–bounded version.
Example 6.1.2. (Continuation of Example 4.3.1 about Annual Maximum Wind Speeds
for Jacksonville, Florida.) We want to identify more closely the distribution of the tropical
maximum annual wind speeds dealt with in Example 4.3.1. These data can be regarded
as randomly left censored by the non–tropical wind speeds. As mentioned before the left
censoring can be converted to right censoring by changing signs (these data are stored
in the ﬁle ec–jwind.dat). Xtremes is applied to these negative data, yet our following
arguments concern the original wind speed data.
Plotting a kernel density only based on the tropical wind speeds and the kernel
density for censored data we see that the distribution is now shifted to the left. This
reﬂects the fact that we take into account tropical wind speeds which were censored by
non–tropical ones. Another eﬀect is that the distribution of tropical wind speeds can now
be better described by a Fr´echet density. Thus, the distribution is shifted to the left, yet
the (visual) estimate indicates a heavier upper tail. This may have serious consequences
for the forecast of catastrophic tropical storms.
Up to now there are no automatic procedures available that concern estima-
tion within EV models based on censored data. First steps were done in the GP
model.
2 See, e.g., Shorack, G.R. and Wellner, J.A. (1986). Empirical Processes with Appli-
cations to Statistics. Wiley, New York.

164
6. Advanced Statistical Analysis
Estimation in Generalized Pareto Models
Given (z1, δ1), . . . , (zn, δn), consider the exceedances ˜z1, . . . , ˜zk over a threshold u.
Denote the δ-values belonging to the ˜zi by ˜δ1, . . . , ˜δk. It is clear that the (˜zi, ˜δi)
only depend on the exceedances of the xi and yi over the threshold u and, hence,
the exceedance dfs F [u]
s
and F [u]
c
are merely relevant. Assume that F [u]
s
can be
substituted by some GP df W if u is suﬃciently large.
Maximize the likelihood function (made independent of the censoring df Fc)
(γ, σ) →log
 
i≤k
(wγ,σ(˜zi))
˜δi(1 −Wγ,σ(˜zi))1−˜δi

,
where wγ,σ is the density of Wγ,σ. The likelihood function is computed under the
condition that the xi’s and yi’s are independent replicates. Likewise, this can be
performed in the α–parameterization.
GP1 Model: The MLE in the GP1 model of α for censored data is given by
ˆαk =
 
i≤k
˜δi
 
i≤k
log(˜zi/u)

.
This estimator is related to the Hill estimator for non–censored
data.
GP Model:
The likelihood equations are solved numerically.
Extreme value analysis of randomly censored data is still in a research sta-
dium. The preceding considerations should be regarded as a ﬁrst step.
6.2
Models of Time Series, the Extremal Index
The purpose of discussing certain time series is twofold. Firstly, we want to pro-
vide examples for which the extremal index will be speciﬁed. Secondly, we make
some preparations for Chapter 16, where series of speculative asset prices will be
investigated within a time series framework.
Time series are based on white–noise series {εk} which are sequences of un-
correlated random variables εk satisfying the conditions
E(εk) = 0
and
E(ε2
k) = σ2.
Stochastic Linear Diﬀerence Equations
First, we deal with ﬁrst–order diﬀerence equations for random variables Yk starting
at time zero. The value of Yk is related to the value at the previous period by the
equation
Yk = φ0 + φ1Yk−1 + εk

6.2. Models of Time Series, the Extremal Index
165
for certain constants φ0, φ1 with |φ1| < 1 and innovations εk.
Notice that the random variables Yk satisfy these equations if, and only if,
the random variables Xk = Yk −φ0/(1 −φ1) satisfy
Xk = φ1Xk−1 + εk,
k = 1, 2, 3, . . . .
(6.4)
By recursive substitution, one obtains
Xk = φk
1X0 +
k−1

i=0
φi
1εk−i
as a solution to (6.4). If the innovations εk have zero–mean, then
EXk = φk
1EX0 →0,
k →∞,
and
V (Xk)
=
φ2k
1 V (X0) + σ2
φ2k
1 −1

φ2
1 −1

→
σ2
1 −φ2
1

,
k →∞,
(6.5)
if, in addition, X0, ε1, ε2, ε3, . . . are pairwise uncorrelated.
Weakly and Strictly Stationary Time Series
Besides weakly stationary processes—that are covariance–stationary processes (cf.
page 72)—we also work with strictly stationary sequences {Xk}, where k runs over
all integers or over the restricted time domain of nonnegative or positive integers.
For example, if in addition to the conditions speciﬁed around (6.5) the exact
relations EX0 = 0 and V (X0) = σ2
1−φ2
1

hold, then {Xk} is a weakly stationary
series with
Cov(X0, Xh) = φh
1V (X0).
(6.6)
We see that the solution of the stochastic linear equations converges to a stationary
series.
A sequence {Xk} is strictly stationary if the ﬁnite–dimensional marginal dis-
tributions are independent of the time lag h, that is, the joint distributions of
Xk1, . . . , Xkm and, respectively, Xk1+h, . . . , Xkm+h are equal for all k1, . . . , km and
h > 1. One speaks of a Gaussian time series if all ﬁnite–dimensional marginals are
Gaussian. For a Gaussian time series, weak and strict stationarity are equivalent.
Example 6.2.1. (Continuation of Example 2.5.3 about Gaussian AR(1) Series.) The
Gaussian time series Xk = φ1Xk−1 + εk in (2.58) is a special case with φ1 = d and
εk = (1 −d2)1/2Yk, where Y1, Y2, Y3, . . . are iid standard Gaussian random variables.
Because Xk−1 and εk are independent we know, see (8.13), that the conditional df of Xk
given Xk−1 = xk−1 is the df of dxk−1 + (1 −d2)1/2Yk. This yields that the conditional

166
6. Advanced Statistical Analysis
expectation E(Xk|xk−1) and conditional variance V (Xk|xk−1) are equal to dxk−1 and,
respectively, 1 −d2.
Example 6.2.1 should be regarded as a ﬁrst step towards the modeling of
ﬁnancial data by means of ARCH and GARCH series, see Sections 16.7 and 16.8.
AR(p) Time Series
Let the set of all integers k be the time domain. Given white noise {εk} and
constants φ1, . . . , φp, deﬁne an autoregressive (AR) time series {Xk} of order p as
a stationary solution to the AR equations
Xk = φ1Xk−1 + · · · + φpXk−p + εk.
(6.7)
The time series in (6.6) and, speciﬁcally, in Example 6.2.1 with time domain
restricted to the nonnegative or positive integers may also be addressed as AR(1)
series.
MA(q) Time Series
An MA(q) time series {Xk} for certain parameters θ1, . . . , θq is deﬁned by
Xk = εk + θ1εk−1 + · · · + θqεk−q.
(6.8)
This process is apparently covariance–stationary. The autocovariance function is
r(h) =
⎧
⎨
⎩
E

ε2
1
 q−h
i=0 θiθi+h
h ≤q,
for
0
h > q,
where θ0 = 1.
If the εk are independent, then {Xk} is strictly stationary. We mention two
examples.
• (Gaussian MA(q) Series.) One obtains a Gaussian time series in the case of
iid Gaussian random variables εk.
• (Cauchy MA(q) Series.) The construction in (6.8) can also be employed if the
expectations or variances of the innovations εk do not exist. Then, one does
not obtain an MA(q) process in the usual sense. Yet, the resulting process
may still be strictly stationary. For example, one gets a strictly stationary
sequence of Cauchy random variables with scale parameter σ q
i=0 θi, if the
εk are iid Cauchy random variables with scale parameter σ and θi > 0 for
i = 1, . . . , q, because the convolution of two Cauchy random variables with
scale parameters c1 and c2 is a Cauchy random variable with scale parameter
c1 + c2.
We remark that two values, namely 668.3 and 828.3 at 0.07 and 0.075, are
not visible in the illustration.

6.2. Models of Time Series, the Extremal Index
167
time i/n
0.5
1
200
400
Fig. 6.1. Scatterplot of n =
200 MA(5) data of standard
absolute Cauchy random vari-
ables with ci = i.
ARMA(p,q) Time Series
By combining (6.8) and (6.7), one obtains the ARMA equations
Xk −φ1Xk−1 −· · · −φpXk−p = εk + θ1εk−1 + · · · + θqεk−q.
(6.9)
This equation may be written φ(B)Xk = θ(B)εk, where BjXk = Xk−j is the
backshift operator, and θ(B) and φ(B) are deﬁned by means of the MA and AR
polynomials
θ(z) = 1 + θ1z + · · · + θqzq
and
φ(z) = 1 −φ1z −· · · −φpzp.
An ARMA(p,q) time series {Xk} is a stationary solution to these equations.
Notice that MA(q) and AR(p) processes are special ARMA processes.
If the AR polynomial φ satisﬁes φ(z) ̸= 0 for all complex numbers z with
|z| ≤1, then it is well known that a stationary solution to the ARMA equations
is found in the MA(∞) time series
Xk =
∞

j=0
ψjεk−j,
(6.10)
where the coeﬃcients ψj are determined by the equation
∞

j=0
ψjzj = θ(z)
φ(z),
|z| ≤1.
The ARMA(p, q) process is said to be causal. We have EXk = 0, and the autoco-
variance function is
r(h) = E

ε2
1
 ∞

j=0
ψjψj+|h|.

168
6. Advanced Statistical Analysis
A series {Yk} is an ARMA(p, q) process with mean µ, if Xk = Yk −µ is a causal
ARMA(p, q) process.
One obtains a Gaussian time series X1, X2, X3, . . . if the εk are iid Gaussian
random variables. For that purpose, check that (Xk,n)k≤m = (n
j=0 ψjεk−j)k≤m
is a Gaussian vector that converges pointwise to (Xk)k≤m, and the distribution of
(Xk,n)k≤m converges weakly to a Gaussian distribution as n goes to ∞. For the
simulation of such an ARMA process, one may use the innovation algorithm (cf.
[5], page 264).
Estimation in ARMA-Models
We shortly describe estimators in AR and ARMA models. We refer to time–series
books such as [26] and [5] for a more detailed description.
• (AR(p): Yule–Walker.) This is an estimator designed for the AR(p)–model.
The Yule–Walker estimator computes estimates of the coeﬃcients φ1, . . . , φp
of the AR(p) process and the variance σ2 of the white noise series. The
estimation is based on the Yule–Walker equations. Parameter estimates are
obtained by replacing the theoretical autocovariances in the Yule–Walker
equations by their sample counterparts.
• (ARMA(p, q): Hannan–Rissanen.) The Hannan–Rissanen algorithm uses lin-
ear regression to establish estimates for the parameters and the white noise
variance of an ARMA(p, q) process. For this purpose, estimates of the unob-
served white noise values ϵk, . . . , ϵk−q are computed.
• (ARMA(p, q): Innovations Algorithm.) One obtains estimates of the param-
eters and the white noise variance of a causal ARMA(p, q) process.
• (ARMA(p, q): MLE.) To compute the MLE of the parameters of a causal
ARMA(p, q) process, one must use one of the preceding estimators to get an
initial estimate.
The Extremal Index and the EV Modeling Revisited
The EV modeling for maxima can still be employed when the iid condition is con-
siderably relaxed. For a stationary sequence of random variables Xi with common
df F it is well known that (1.27) still holds under Leadbetter’s mixing conditions
D and D′ (see, e.g., [39]) which concern a long range and a local mixing of the
random variables.
If condition D′ is weakened, one may still have
P{max{X1, . . . , Xn} ≤x} ≈F θn(x)
(6.11)
with 0 ≤θ ≤1 for larger values x. The constant θ is the extremal index. This
index is equal to 1 for iid random variables. A condition D′′ which guarantees that

6.2. Models of Time Series, the Extremal Index
169
(6.11) holds was introduced by Leadbetter and Nandagopalan3 and weakened to
conditions D(k) by Chernick et al.4 If (6.11) holds, then an EV modeling for dfs
of maxima is still possible because Gθ is an EV df if G is an EV df. In (1.27), the
location and scale parameters are merely changing. As already noted in Section
2.7, the extremal index is also the limiting reciprocal mean cluster size.
In the statistical literature, the extremal index was calculated for various
stationary sequences. We mention only two examples concerning MA(q) and AR(1)
sequences.
• (MA(q) Sequence.) If the innovations εi in (6.8) have a Pareto type upper
tail with tail index α > 0, then the extremal index is equal to5
θ =

max
i≤q {θi}
α 
i≤q
θα
i .
• (AR(1) Sequence.) Examples of AR(1) sequences with Cauchy marginals and
θ < 1 are dealt with in the aforementioned paper by Chernick et al.
For the Gaussian AR(1) sequences dealt with in Section 2.5, condition D′
holds, and, therefore, the extremal index is equal to 1. This yields that max-
ima and exceedances over high thresholds asymptotically behave as those of iid
random variables. This asymptotic result somewhat contradicts the clustering of
exceedances in Fig. 2.21 for d = 0.8. Yet, this scatterplot only exhibits that the
rate of convergence in the asymptotics is exceedingly slow. The illustration would
not be drastically diﬀerent when the sample size is increased to n = 30, 000. An
asymptotic formulation for that question that represents the small sample behavior
of maxima (also that of the clustering) was provided by Hsing et al.6
Aggregation and Self–Similarity
Let {Zi} be a strictly stationary sequence of random variables. Consider the mov-
ing average
Z(m)
k
= 1
m
km

i=(k−1)m+1
Zi,
k = 1, 2, 3, . . .
(6.12)
3Leadbetter, M.R. and Nandagopalan, S. (1989). On exceedance point processes for
stationary sequences under mild oscillation restrictions. In [14], pp. 69–80.
4Chernick, M.R., Hsing, T. and McCormick, W.P. (1991). Calculating the extremal
index for a class of stationary sequences. Adv. Appl. Prob. 23, 835–850.
5Davis, R.A. and Resnick, S.I. (1985). Limit theory for moving averages of random
variables with regular varying tail probabilities. Ann. Prob. 13, 179–195.
6Hsing, T., H¨usler, J. and Reiss, R.–D. (1996). The extremes of a triangular array of
normal random variables. Ann. Appl. Prob. 6, 671–686.

170
6. Advanced Statistical Analysis
which is called aggregated sequence with level of aggregation m. It is evident
that the aggregated sequence is again strictly stationary. In that context, one also
speaks of self–similarity when the aggregated sequence is distributional of the same
type as the original one. For example, if the Xi are iid standard normal, then
m1/2X(m)
k
d= X1,
(6.13)
that is we have equality of both random variables in distribution. This can be
extended to all sum–stable random variables such as Cauchy and Levy random
variables, see Section 6.4. Another example is provided in the subsequent Section
6.3 about Student distributions.
6.3
Statistics for Student Distributions
In (1.62) we introduced the standard Student distribution with shape parameter
α > 0 and density
fα(x) = c(α)

1 + x2
α
−(1+α)/2
(6.14)
where c(α) = Γ((1 + α)/2)/(Γ(α/2)Γ(1/2)α1/2). Apparently, such a Student dis-
tribution has lower and upper tails with both tail indices equal to α. We see that
the Cauchy distribution is a special case for α = 1, cf. page 27.
For positive integers α = n, we obtain in (6.14) the well–known Student
distribution (t–distribution) with n degrees of freedom. This distribution can be
represented by
X

(Y/n)1/2,
(6.15)
where X and Y are independent random variables distributed according to the
standard normal df and the χ2–df with n degrees of freedom, cf. page 123. Recall
that a χ2 random variable with n degrees of freedom is a gamma random variable
with shape parameter r = n/2 and scale parameter σ = 2.
A Stochastic Representation of Student Distributions
Using gamma random variables one may ﬁnd a representation as in (6.15) for all
Student distributions.
Consider the ratio X/(Y/r)1/2, where again X and Y are independent, X is
standard normal and Y is a standard gamma random variable with parameter r,
cf. (4.6). The density is given by
gr(z)
=
 ∞
0

ϑϕ(ϑz)

2rϑhr(rϑ2)

dϑ
=
Γ((2r + 1)/2)
Γ(r)(2πr)1/2

1 + z2
2r
−(2r+1)/2
.
(6.16)

6.3. Statistics for Student Distributions
171
This is the Student density in (6.14), if r = α/2.
Notice that the Student density fα is the reciprocal scale mixture of normal
densities taken with respect to the distribution of (2Y/α)1/2, where Y is a gamma
random variable with parameter r = α/2. Alternatively, one may speak of a scale
mixture of normal densities with respect to the distribution of (2Y/α)−1/2.
Properties of Student Distributions
• (Asymptotic Normality.) Using the relation
(1 + x2/α)α/2 →exp(−x2/2)
as α →∞one may prove that fα is the standard Gaussian density in the
limit as α →∞.
• (Construction of a Self–Similar Sequence.) Let the {Xi} be iid standard
normal random variables, and let Y be a gamma random variable with pa-
rameter r = α/2 which is independent of the sequence {Xi}. Then, the
Zi = Xi/(2Y/α)1/2
are Student random variables with parameter α. Consider again moving
averages Z(m)
k
in (6.12). As a direct consequence of (6.13) one obtains
m1/2Z(m)
k
d= Z1
and, therefore, the Zi are self–similar.
Maximum Likelihood Estimation in the Student Model
This is one of the earliest examples of the use of the ML method (R.A. Fisher
(1922) in the article cited on page 47).
The likelihood equations have no explicit solution and must be numerically
computed. In such cases, we will apply a Newton–Raphson iteration procedure
by which the initial value of the iteration is an estimate determined by another
estimation method. A local maximum of the likelihood function is occasionally
computed instead of a global maximum.
Including a Skewness Parameter
Including a location parameter δ in the standard normal random variable X one
obtains a noncentral Student distribution with noncentrality parameter δ.
An extension to the multivariate framework may be found in Section 11.3.

172
6. Advanced Statistical Analysis
6.4
Statistics for Sum–Stable Distributions
co–authored by J.P. Nolan7
We add some statistical results for non–degenerate, sum–stable distributions which
are diﬀerent from the Gaussian ones. Sum–stable distributions are heavy–tailed,
like Student distributions, if the shape parameter α (index of stability) is smaller
than 2. All sum–stable distributions have unimodal densities.
Generally, a df F is sum–stable, if
F m∗(amx + bm) = F(x)
(6.17)
for a certain choice of constants am > 0 and bm. It is well known that am = m1/α
for some α with 0 < α ≤2, also see page 31.
Thus, we have α = 2 for the normal df and α = 1 for the Cauchy df. The
L´evy distribution is another example of a sum–stable distribution with α = 1/2,
for which a simple representation is feasible. The L´evy density is
f(x) = (2π)−1/2x−3/2 exp(−(2x)−1),
x ≥0,
(6.18)
which is a special reciprocal gamma density, see (3.43).
Check that
F(x) = 2

1 −Φ
'
1/x

is the pertaining df. In contrast to the normal and Cauchy distributions, the L´evy
distribution is not symmetric. The heavy tails and the possible asymmetry make
stable laws an attractive source of models.
The stability property (6.17) is closely related to the Generalized Central
Limit Theorem (GCLT). The classical Central Limit Theorem applies to terms
having ﬁnite variance: if X1, X2, . . . have ﬁnite variance, then
m−1/2(X1 + X2 + · · · + Xm) −m1/2EX1
converges in distribution to a normal distribution. If the terms X1, X2, . . . have
inﬁnite variance, then the normalized sums
cm(X1 + X2 + · · · + Xm) −dm
converge in distribution to a stable law, where the term cm must be of the form
m−1/α. Hence, stable laws are the only possible limiting distributions of normalized
sums of iid terms.
Until recently, stable distributions were inaccessible for practical problems
because of the lack of explicit formulas for the densities and distribution functions.
However, new computer programs make it feasible to use stable distributions in
applications.
7American University, Washington DC

6.4. Statistics for Sum–Stable Distributions
173
All computations with sum–stable distributions were done using a DLL–
version of STABLE8.
Some Basic Facts About Characteristic Functions
Since there are no explicit formulas for stable densities, they are usually described
through their characteristic functions or Fourier transforms. Let x+iy be the usual
representation of complex numbers and recall that
exp(ix) = cos(x) + i sin(x).
The expectation of a complex variable X + iY is deﬁned as
E(X + iY ) = E(X) + iE(Y ).
The characteristic function χX of a real–valued random variable X is deﬁned
as
χX(t) = E

exp(itX)

.
(6.19)
It is well known that there is a one–to–one relationship between the distri-
bution of X and the characteristic function χX. The importance of characteristic
functions becomes apparent from the fact that
χX+Y = χXχY
for independent random variables X and Y .
Because exp(u + w) = exp(u) exp(w) for complex numbers u and w we have
χµ+ηX(t)
=
E

exp(iηtX)

exp(iµt)
=
χX(ηt) exp(iµt)
(6.20)
for real numbers µ and η. Therefore, it suﬃces to specify the characteristic func-
tions of standard variables for the construction of statistical models.
Symmetric Sum–Stable Distributions
We ﬁrst introduce the standard sum–stable dfs S(α) with index of stability 0 <
α ≤2 which are symmetric about zero. These dfs can be represented by the
real–valued characteristic functions
χα(t) = exp(−|t|α).
(6.21)
The support of the df S(α) is the real line.
We use the following notation for
8The basic algorithms are described in Nolan, J.P. (1997). Numerical computation
of stable densities and distribution functions. Commun. Statist.–Stochastic Models 13,
759–774. Note that the STABLE package is no longer included in Xtremes.

174
6. Advanced Statistical Analysis
Symmetric Sum–Stable Distributions:
S(α, 0, σ, µ; 0),
if scale and location parameters σ > 0 and µ are included. At the second position
there is a skewness parameter which is equal to zero in the case of symmetric
sum–stable distributions. The zero after the semicolon indicates a certain type of
parameterization which will become important for skewed stable distributions.
Observe that the characteristic functions in (6.21) are continuous in the pa-
rameter α. The Fourier inverse formula for densities implies that this also holds
for the pertaining densities and dfs. This property remains valid when location
and scale parameters are added.
It is well known that S

2, 0, 1/
√
2, 0; 0

is the standard normal df Φ. Thus,
(6.20) yields
S(2) = S(2, 0, 1, 0; 0) = Φ0,
√
2
which is the normal df with location parameter zero and scale parameter σ =
√
2.
In addition, S(1, 0, 1, 0; 0) is the standard Cauchy df.
In Fig. 6.2 we plot some symmetric sum–stable densities varying between the
normal and the Cauchy density.
-5
-3
-1
1
3
5
0.0
0.1
0.2
0.3
Fig. 6.2. Plots of symmetric,
sum–stable densities with in-
dex of stability α equal to 2
(normal), 1.75, 1.5, 1.25 and
1 (Cauchy).
We see that the most signiﬁcant diﬀerence between the normal density and
other sum–stable symmetric densities is the weight in the tails. One could get a
greater similarity in the center of the distributions by varying the scale parameter.
The formulas become a bit more complicated when a skewness parameter is
included. In that context we deal with two diﬀerent parameterizations.
Sum–stable distributions have Pareto–like tails. Yet, it is well known that
estimators of the tail index α < 2 are inaccurate if α is close to 2. This phenomenon
will be clariﬁed by computing the remainder term in the Pareto approximations.
From Christoph and Wolf [7] we know that the expansion
fα(x) = 1
π

j≤m
(−1)j+1 Γ(1 + jα)
j!
sin(jαπ/2)|x|−(1+jα) + O

Am(x)

,
(6.22)

6.4. Statistics for Sum–Stable Distributions
175
holds for the density fα of S(α), where
Am(x) = O

|x|−(1+(m+1)α)
,
|x| →∞.
For m = 3, we have
fα(x) =
1
σ(α)w1,α

x
σ(α)
 
1 + C(α)

x
σ(α)
−α
+ O

x
σ(α)
−2α
(6.23)
for certain scale parameters σ(α) and constants C(α), where w1,α is the standard
Pareto density with shape parameter α. The constant C(α) in the second order
term satisﬁes
C(α) = 24/(2 −α) + O(1),
α →2,
(6.24)
which shows that these constants are very large for α close to 2.
Adding a Skewness Parameter, a Continuous Parameterization
To represent the family of all non–degenerate, sum–stable distributions one must
include a skewness parameter −1 ≤β ≤1 in addition to the index of stability
0 < α ≤2. For α = 2 one gets the normal df S(2) for each β.
We choose a parameterization so that the densities and dfs vary continuously
in the parameters. Such a property is indispensable for statistical inference and
visualization techniques.
The continuous location–scale parameterization introduced by Nolan9 is a
variant of the (M) parameterization of Zolotarev10. Let
χX(t) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
exp

−|t|α
1 + iβ tan
 απ
2

sign(t)

|t|1−α −1

α ̸= 1,
if
exp

−|t|

1 + iβ 2
πsign(t) log(|t|)

α = 1.
(6.25)
Using this parameterization one gets a family of characteristic functions which
is continuous in the parameters α and β. To prove the continuity at α = 1, notice
that tan(π/2 + x) = −1/x + o(x) and, therefore, according to (1.66)
tan
απ
2

|t|1−α −1

=
2
π
|t|1−α −1
1 −α
+ o(1 −α)
→
(2/π) log |t|,
α →1.
If β = 0, then one gets the family of characteristic functions in (6.21) for sym-
metric sum–stable distributions. The dfs pertaining to the characteristic functions
in (6.25) are denoted by S(α, β; 0). We write
9Nolan, J.P. (1998). Parameterizations and modes of stable distributions. Statist.
Probab. Letters 38, 187-195.
10Zolotarev, V.M. (1986). One–Dimensional Stable Distributions. Translations of Math-
ematical Monographs, Vol. 65, American Mathematical Society.

176
6. Advanced Statistical Analysis
Continuous Parameterization:
S(α, β, σ, µ; 0),
if scale and location parameters σ > 0 and µ are included. As in the case of
symmetric sum–stable distributions one obtains families of dfs and densities which
are continuous in all parameters.
-5
-3
-1
1
3
5
0.0
0.1
0.2
0.3
Fig. 6.3. Plots of sum–stable
densities with index of sta-
bility α equal to 2 (normal),
1.75, 1.5, 1.25 and 1 (Cauchy)
and skewness parameter β =
0.75.
Deduce from (6.20) that a random variable −X has the df S(α, −β; 0), if X
has the df S(α, β; 0). The support of S(α, β; 0) is the real line, if −1 < β < 1.
The dfs S(α, 1; 0) and S(α, −1; 0) have the supports (−tan(πα/2), ∞) and
(−∞, tan(πα/2)), respectively. Moreover, S(1/2, 1, 1, 1; 0) is the L´evy df, as deﬁned
by (6.18).
The tails of most stable laws are Pareto–like: when β ∈(−1, 1], the tail
probability and density satisfy as x →∞,
P(X > x)
∼
cα(1 + β)x−α,
f(x)
∼
αcα(1 + β)x−(α+1),
(6.26)
where cα = Γ(α) sin(πα/2)/π. (The β = −1 case decays faster than any power.)
This asymptotic power decay has been used to estimate stable parameters, but in
most cases one must go extremely far out on the tails to see this tail behavior,
e.g., Fofack and Nolan11 and Section 6.5, so this approach is of limited usefulness
in practice.
Adding a Skewness Parameter, the Conventional Parameterization
We also mention a parameterization which is more commonly used and which is
particularly convenient for mathematical proofs (see, e.g., [7] and [46]). Given a
11Fofack, H. and Nolan, J.P. (1999). Tail behavior, modes and other characteristics of
stable distributions. Extremes 2, 39–58.

6.4. Statistics for Sum–Stable Distributions
177
random variable X with df S(α, β; 0) let
Y =
⎧
⎨
⎩
γ

X + β tan(πα/2)

+ δ,
α ̸= 1,
if
γ

X + (2/π)β log(γ)

+ δ,
α = 1
(6.27)
for parameters γ > 0 and δ. The pertaining dfs are denoted by
Conventional Parameterization:
S(α, β, γ, δ; 1).
Notice that γ and δ are scale and location parameters if α ̸= 1. The parameter δ
is a location parameter for the dfs S(α, β, γ, 0; 1). If β = 0, then the continuous
and convential parameterizations coincide.
According to (6.20) and (6.25) the pertaining characteristic functions are
χY (t) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
exp

−γα|t|α
1 −iβ tan
 απ
2

sign(t)

+ iδt

α ̸= 1,
if
exp

−γ|t|

1 + iβ 2
πsign(t) log(|t|)

+ iδt

α = 1.
(6.28)
This parameterization is not continuous at α = 1 and, therefore, less useful for
statistical inference. The reader is cautioned that several other parameterizations
are used for historical and technical reasons.
Scale Mixtures of Normal Distributions
As in the case of Student distributions one may represent symmetric, sum-stable
distributions as scale mixtures of normal distributions. Now, the mixing df is the
sum–stable df with skewness parameter β = 1 and index α < 1.
Let X and Y be independent random variables, where X is standard normal
and Y has the df S(α/2, 1, 1, 0; 1) for some α < 2. Notice that Y has the support
(0, ∞). Then,
Y 1/2X
(6.29)
has the df S(α, 0, γ, 0; 1), where γ = 2−1/2sec(πα/4)1/α with sec(x) = 1/ cos(x)
denoting the secant function.
For example, if Y is the L´evy random variable, having the index α = 1/2,
then the scale mixture is the Cauchy df.
This construction will be of importance in the multivariate setup for the
characterization of multivariate sum–stable, spherically or elliptically contoured
dfs, see Section 11.4.
Estimating Stable Parameters
For estimating the four parameters in the S(α, β, σ, µ; 0)–parameterization several
methods are applicable. We mention

178
6. Advanced Statistical Analysis
• Quantile Method: McCulloch12 uses ﬁve sample quantiles (with q = 0.05,
0.25, 0.5, 0.75, 0.95) and matches the observed quantile spread with the
exact quantile spread in stable distributions.
• Empirical Characteristic Function Method: Kogon and Williams13 use the
known form of the characteristic function to estimate stable parameters.
The empirical/sample characteristic function is computed from the sample
X1, . . . , Xn by
ˆχ(t) =
 
j≤n
exp(itXj)

n.
The sample parameters are estimated by regression techniques from the exact
form for χ(t) given in (6.25).
• MLE: This method uses initial estimate of (α, β, σ, µ) from the quantile
method, and then maximizes the likelihood by a numerical search in the
4–dimensional parameter space. Large sample conﬁdence interval estimates
have been computed, see Nolan14.
The estimated parameters may be converted to the conventional parameter-
ization.
Diagnostics with Kernel Densities
In principle, it is not surprising that one can ﬁt a data set better with the 4
parameter stable model than with the 2 parameter normal model. The relevant
question is whether or not the stable ﬁt actually describes the data well.
The diagnostics we are about to discuss are an attempt to detect non–
stability. As with any other family of distributions, it is not possible to prove
that a given data set is or is not stable. The best we can do is to determine
whether or not the data are consistent with the hypothesis of stability. These tests
will fail if the departure from stability is small or occurs in an unobserved part of
the range.
The ﬁrst step is to do a smoothed density plot of the data. If there are clear
multiple modes or gaps in the support, then the data cannot come from a stable
distribution. For density plots, we smoothed the data with the Epanechnikov or a
Gaussian kernel with standard deviation given by a width parameter. In addition
to the automatic choice we used trial and error to ﬁnd a width parameter that was
12McCulloch, J.H. (1986). Simple consistent estimators of stable distribution parame-
ters. Commun. Statist. Simulations 15, 1109–1136.
13Kogon, S.M. and Williams, D.B. (1998). Characteristic function based estimation of
stable distribution parameters. In: A Practical Guide to Heavy Tails, R.J. Adler et all
(eds.), Birkh¨auser, Basel, 311-335.
14Nolan, J.P. (2001). Maximum likelihood estimation of stable parameters. In: L´evy
Processes, ed. by Barndorﬀ–Nielson, Mikosch, and Resnick, Birkh¨auser, Basel, 379–400.

6.4. Statistics for Sum–Stable Distributions
179
as small as possible without showing too much oscillations from individual data
points.
Example 6.4.1. (Distributions of Black Market Exchange Rate Returns.) We examine
two data sets of consecutive monthly log-returns (cf. Chapter 16) from Colombia and
Argentina15 of a relatively small small sample size of 119 studied by Akgiray et al.16 and
further investigated by Koedijk et al.17. Akgiray et al. estimated parameters of sum–
stable and Student distributions for several Latin American exchange rate series.
First one must note that these series exhibit the typical properties of ﬁnancial data,
namely a certain dependency which becomes visible by periods of tranquility and volatil-
ity. This entails that one cannot expect a good performance of estimation procedures.
For the Colombia data we estimated a sum–stable distribution S(α, β, σ, µ; 0) =
S(1.21, −0.14, 0.0059, 0.0096; 0) and a Student distribution with shape, location and scale
parameters α = 1.83, µ = 0.0094 and σ = 0.0072. It is remarkable that in both cases one
gets a shape parameter α less that 2.
In Fig. 6.4 (left) we plot the estimated sum–stable density and a kernel density
based on the Epanechnikov kernel and the width parameter b = 0.007. The estimated
Student density can hardly be distinguished from the sum–stable density and is, therefore,
omitted.
Colombia log-returns
-0.05
-0.03
-0.01
0.01
0.03
0.05
0.07
0.0
10
20
30
40
50
60
Argentina log-returns
-0.3
-0.1
0.1
0.3
0.5
0.0
1
2
3
4
5
6
7
8
9
Fig. 6.4. Kernel densities and estimated sum–stable densities for Colombia (left) and for
Argentina (right) exchange rate data.
For the Argentina exchange rate data the ML estimation procedures gives the
sum–stable distribution S(α, β, σ, µ; 0) = S(1.093, −0.44, 0.036, 0.0036; 0). This density is
strongly skewed to the right. For a continuation of this example see Example 6.4.2.
15Communicated by S. Caserta and C.G. de Vries and stored in blackmarket.dat.
16Akgiray, V., Booth, G.G. and Seifert, B. (1988). Distribution properties of Latin
American black market exchange rates. J. Int. Money and Finance 7, 37–48.
17Koedijk, K.G., Stork, P.A. and Vries, de C.G. (1992). Diﬀerences between foreign
exchange rate regimes: the view from the tail. J. Int. Money and Finance 11, 462-473.

180
6. Advanced Statistical Analysis
The density plots give a good sense of whether the ﬁt matches the data near
the mode of the distribution, but generally they are uninformative on the tails
where both the ﬁtted density and the smoothed data density are small.
Diagnostics with Q–Q Plots
If the smoothed density is plausibly stable, proceed with a stable ﬁt and compare
the ﬁtted distribution with the data using Q–Q and P–P plots and more reﬁned
statistical tools coming from extreme value analysis.
We observed practical problems with Q–Q plots for heavy tailed data. While
using Q–Q plots to compare simulated stable data sets with the exact correspond-
ing cumulative df, we routinely had two problems with extreme values: (1) most of
the data is visually compressed to a small region and (2) on the tails there seems
to be an unacceptably large amount of ﬂuctuation around the theoretical straight
line.
For heavy tailed stable distributions we should expect such ﬂuctuations: if
Xi:n is the ith order statistic from an iid stable sample of size n with common df
F, q = (i −1/2)/n and F −1(q) is the q–quantile, then for n large, the distribution
of Xi:n is approximately normal with expectation EXi:n ≈F −1(q) and variance
V (Xi:n) ≈q(1 −q)/nf(F −1(q))2.
The point is that Q–Q plots may appear non–linear on the tails, even when the
data set is stable.
Diagnostics with Trimmed Mean Excess Functions
One should be aware that the mean excess function does not exist, if the tail
index α of the underlying df is smaller than 1. In addition, the sample mean
excess function provides an inaccurate estimator, if α ≤1.5. Therefore, we employ
trimmed mean excess functions, cf. Section 2.2, in the subsequent analysis.
In conjunction with sum–stable dfs there is another diﬃculty, namely that
the constant in the remainder term of the Pareto approximation is large for α
close to 2, see (6.24). One cannot expect that linearity in the upper tail of the
underlying trimmed mean excess function becomes clearly visible in the sample
version.
In Fig. 6.5 we plot the underlying trimmed mean excess function with trim-
ming factor p = 0.8—simulated under the gigantic sample size of 100,000 data—for
standard, symmetric sum–stable dfs with parameters α = 1.5 and α = 1.8, and in
both cases three diﬀerent sample trimmed mean excess functions for sample sizes
n = 1000.
For the parameter α = 1.5 (respectively, α = 1.8) the linearity of the “theoret-
ical” trimmed mean excess function becomes visible beyond the threshold u = 1.5
(u = 2.5) which corresponds to the 85% quantile (95% quantile). This indicates

6.4. Statistics for Sum–Stable Distributions
181
threshold
trimmed mean excess
0
1
2
3
4
5
6
0.0
1
2
3
4
threshold
trimmed mean excess
0
1
2
3
4
5
6
0.0
1
2
3
4
Fig. 6.5. “Theoretical” trimmed mean excess functions (solid) and sample versions (dot-
ted) based on n = 1000 simulated data for the parameter α = 1.5 (left) and α = 1.8
(right).
why larger sample sizes are required to detect the linearity in the upper tail of the
trimmed mean excess function, if α is close to 2.
Example 6.4.2. (Continuation of Example 6.4.1.) The following considerations concern
the Argentina exchange rate data which were studied in Example 6.4.1. Recall that the
estimated sum-stable distribution was S(α, β, σ, µ; 0) = S(1.093, −0.44, 0.036, 0.0036; 0).
In Fig. 6.6 we plot the sample trimmed mean excess function with trimming factor
0.8 and the “theoretical” trimmed mean excess function simulated under the estimated
parameters (under the Monte Carlo sample size of 50,000).
threshold
trimmed mean excess
0
0.1
0.2
0.3
0.1
0.2
Fig. 6.6. Sample trimmed mean ex-
cess function based on Argentina ex-
change rate data and “theoretical”
trimmed mean excess function.
The number of data above the threshold u = 1.3 is k = 24. The MLE and moment
estimates in the full Pareto model based on the 24 largest data are around α = 2. This
also conﬁrms that there is a distribution with a very heavy tail.

182
6. Advanced Statistical Analysis
The overall impression from the analysis of data and of simulations is that
upper tails of sum–stable dfs can be successfully analyzed by sample trimmed
mean excess functions, if α is not too close to 2 and the sample size is suﬃciently
large.
6.5
Ultimate and Penultimate
GP Approximation
co–authored by E. Kaufmann18
From this section some theoretical insight can be gained why the modeling of dfs
of maxima or exceedances by means of EV or GP dfs leads to inaccurate results
in certain cases although the underlying df F belongs to the max or pot–domain
of attraction of an EV or GP df.
This question is handled by computing remainder terms in the limit theorems
(1.27) and (1.46) for dfs of maxima and exceedance dfs. This book is devoted
to exceedances to some larger extent and, therefore, we focus our attention on
the approximation to exceedance dfs in order not to overload this section. Our
arguments are closely related to those in [16], pages 11–45.
At the beginning we start with some explanations about the relationship
between the pot–stability of GP dfs and the concept of regularly varying functions.
In the second part of this section one may ﬁnd some results about von Mises bounds
for such approximations.
The POT–Stability and Regularly Varying Functions
In the following lines, we merely study the case of Pareto dfs W1,α,µ,σ in the α–
parametrization. Similar results hold for GP dfs in general. From (1.44) we know
that for exceedance dfs of Pareto dfs the relation
W [u]
1,α,µ,σ(x) = W1,α,µ,u−µ(x),
x ≥u,
holds for thresholds u > µ + σ, which is the pot–stability of Pareto dfs.
Recall from (1.12) that for any df F,
F [u](x) = F(x)/F(u),
x ≥u,
where F is the survivor function of F. Rewriting the pot–stability of Pareto dfs
F = W1,α,µ,σ in terms of survivor functions and replacing x by ux, one gets
F(ux)/F(u) = F [u](ux) =
x −µ/u
1 −µ/u
−α
→u→∞x−α,
x ≥1.
(6.30)
18University of Siegen.

6.5. Ultimate and Penultimate GP Approximation
183
In the special case of F = W1,α,0,σ—this is the case of a Pareto df with
location parameter µ = 0—we have equality in (6.30), that is,
F(ux)/F(u) = x−α,
x > 1,
(6.31)
holds. This indicates that pot–stability is closely related to the property that the
survivor function F is regularly varying at ∞.
Generally, a measurable function U : (0, ∞) →(0, ∞) is called regularly
varying at inﬁnity with index (exponent of variation) ρ, if
U(tx)/U(t) →t→∞xρ,
x > 0,
(6.32)
see, e.g., [44], Section 0.4. The canonical ρ–varying function is U(x) = xρ.
If ρ = 0, then U is called slowly varying. One speaks of regularly varying at
zero if (6.32) holds with t →∞replaced by t ↓0.
Recall from Section 1.3, page 19, that a df F belongs to the max–domain of
attraction of the EV df Gγ with γ > 0 if, and only if, ω(F) = ∞and the survivor
function F is regularly varying at ∞with index ρ = −1/γ.
If F = W1,α,µ,σ is a Pareto df with location parameter µ ̸= 0, then condition
(6.32) is satisﬁed for F with ρ = −α, yet the rate is exceedingly slow if α is a
larger index. In the following lines we make this remark more precise by computing
remainder terms.
Computing the Remainder Term in the GP Approximation
We start with a condition imposed on the survivor function F = 1 −F of a df F
under which the remainder term of the approximation in (1.46) of an exceedance
df F [u] by means of a GP df W can be calculated easily.
Our interest is primarily focused on GP dfs, yet the subsequent formulas
(6.33) to (6.37) are applicable to any df W as an approximate df. Assume that
F(x) = W(x)

1 + O

W
1/δ(x)

(6.33)
holds19 for some δ > 0. Thus, F is close to the df W in the upper tail. If this
condition holds, we say that a df F belongs to the δ–neighborhood of the df
W. Notice that the remainder term is small if δ is small. In addition, we have
ω(F) ≤ω(W) for the right–hand endpoints of the supports of F and W.
Condition (6.33), with W being a GP df, is satisﬁed for δ = 1 by many
of the distributions mentioned in this book such as Fr´echet, Weibull, Gumbel,
generalized Cauchy, Student, Burr and sum–stable distributions with α < 2 and
various mixtures.
Straightforward calculations show that the relation
F [u](x) −W [u](x)
 = O

W
1/δ(u)

,
u ≤x < ω(F),
(6.34)
19This condition can be attributed to L. Weiss (1971), see the article cited on page 134.

184
6. Advanced Statistical Analysis
holds uniformly over u for the exceedance dfs of F and W. A special case was
mentioned in (1.48). Recall that the exceedance df W [u] of a GP df W is again a
GP df (which is the pot–stability of GP dfs). Also, a df belongs to the pot–domain
of attraction of a GP df, if it belongs to the δ–neighborhood of a GP df.
Condition (6.33) reformulated in terms of densities is
f(x) = w(x)

1 + O

W
1/δ(x)

.
(6.35)
By integration one obtains that (6.35) implies (6.33). In addition, simple
calculations yield that
f [u](x) = w[u](x)

1 + O

W
1/δ(u)

,
u ≤x < ω(F),
(6.36)
holds for the exceedance densities f [u] and w[u] uniformly in the thresholds u.
In Section 9.4 we apply a reformulation of (6.36) in terms of the Hellinger
distance (introduced in (3.4)). (6.36) implies
H

f [u], w[u]
= O

W
1/δ(u)

,
(6.37)
where H is the Hellinger distance between f [u] and w[u].
Approximations to actual distributions of exceedances or maxima can still
be satisfactorily accurate for small sample sizes, if the constant in the remainder
term is small, with hardly any improvement when the sample size increases. For a
longer discussion see [42], pages 172–175.
Comparing the Tail Behavior of Certain Pareto Distributions
We also want to discuss the poor performance of the Hill estimator, as illustrated
in Figure 5.2 (left), by computing a reﬁnement of (6.33). The Hill estimator is
an MLE in the GP1(u, µ = 0) model, where the location parameter µ is equal to
zero. If µ ̸= 0, then it is well known that the Hill estimator is still consistent (as
outlined in Section 5.1).
We compare the tail behavior of Pareto dfs W1,α,µ,σ with location parameter
µ ̸= 0 to that for µ = 0. For that purpose, an expansion of length 2 is computed.
By applying a Taylor expansion one gets
W1,α,µ,σ(x) = W1,α,0,σ

1 + αµ
σ W
1/α
1,α,0,σ(x) + O

W
2/α
1,α,0,σ(x)

.
(6.38)
We see that Pareto dfs with µ ̸= 0 belong to the δ–neighborhood of a Pareto
df with µ = 0, yet δ = α can be large which yields a large remainder term in the
approximation.
This indicates that estimators of the shape parameter α, which are especially
tailored for the GP1(u, µ = 0) model such as the Hill estimator, are inaccurate, if
the parameters α is large. Of course, the constant µ/σ is important as well.

6.5. Ultimate and Penultimate GP Approximation
185
A stronger deviation of the actual distribution from the GP1(u, µ = 0) model
yields a larger bias of the Hill estimator a fact which was already observed by
Goldie and Smith20.
The Hall Condition
A reﬁnement of the Weiss condition (6.33), with the factor
1 + O

W
1/δ(x)

replaced by an expansion
1 + C(1 + o(1))W
1/δ(x),
(6.39)
is the Hall condition21. See (6.38) for an example. Another example is provided
by (6.23) which is an expansion for sum–stable distributions: the Hall condition
holds for γ = 1/α and δ = 1, yet the constant—hidden in the remainder term—is
very large if α is close to 2.
In terms of densities we get a
reﬁned δ–neighborhood:
f(x) = w(x)

1 + 	C(1 + o(1))W
1/δ(x)

,
(6.40)
where 	C = C(1 + 1/δ).
The expansion for sum–stable densities in the book by Christoph and Wolf
also includes those with a skewness parameter β ̸= 0. If α ̸= 1, 2, then the sum–
stable densities generally satisfy condition (6.35) with δ = 1; if α = 1 and β ̸= 0,
then this relation holds for δ = 1 + ε for every ε > 0.
Under such a condition the optimal number k of extremes was studied by Hall
and Welsh22. Early results on the bias–correction of estimators were established
under this condition. The latter topic will separately be studied in Section 6.6.
Such Hall type conditions are also of interest in conjunction with the auto-
matic choice of k (see page 137).
Von Mises Bounds for the Remainder Term
in the GP Approximation to Exceedance DFs
Conditions (6.33) and (6.35) are not satisﬁed by the normal, log–normal, gamma,
Gompertz and converse Weibull distributions, for example. Yet, by using the von
20See, e.g., Goldie, C.M. and Smith, R.L. (1987). Slow variation with remainder term:
A survey of the theory and its applications. Quart. J. Math. Oxford Ser. 38, 45–71.
21Hall, P. (1982). On estimating the endpoint of a distribution. Ann. Statist. 10, 556–
568.
22Hall, P. and Welsh, A.H. (1985). Adaptive estimates of parameters of regular varia-
tion. Ann. Statist. 13, 331–341.

186
6. Advanced Statistical Analysis
Mises condition (2.32), one may prove that these distributions belong to the pot–
domain of attraction of the exponential df (and, likewise, to the max–domain
of attraction of the Gumbel df). We refer to [20], pages 67–68, for the required
computations in the case of the log–normal distribution.
Subsequently, we make use of the von Mises condition (2.32), which is equiv-
alent to the following
von Mises condition:
M(t) :=
1 −F
f
′
(F −1(1 −t)) −γ →0,
t ↓0, (6.41)
for some real parameter γ.
An upper bound related to that in (6.37) will be determined by the remainder
function M. Note that M is independent of location and scale parameters of the
df F. Also,
• if M = 0, then F is a GP df,
• if
M(t) ∼ct1/δ
as t ↓0,
then f satisﬁes condition (6.35) for some GP density w (that is, f belongs
to the δ–neighborhood of a GP density w with shape parameter γ).
In addition, assume that
M(tx)
M(t) →xρ,
t ↓0,
x > 0,
(6.42)
holds for some ρ ≥0. This condition says that the remainder function M is
regularly varying at zero, if ρ > 0, and slowly varying at zero, if ρ = 0, see page
183. Note that, (6.42) implies (6.41), if ρ > 0.
The case ρ = 0 includes distributions for which poor rates of convergence are
achieved in (6.37). For the standard Gompertz df ˜G0 we have γ = 0 and
M(t) = −(log t)−1.
(6.43)
Such an order is also achieved by M for normal, log–normal, gamma and converse
Weibull dfs.
We compute an upper bound for the remainder term in the GP approximation
which is related to that in (6.37). Under the conditions (6.41) and (6.42) we have23
H(f [u], wγ,u,σ(u)) ∼C|M(F(u))|,
(6.44)
where
σ(u) =
⎧
⎨
⎩
(1 −F(u))/f(u)
γ ≥0,
if
(ω(F) −u)|γ|
γ < 0.
(6.45)
23Kaufmann, E. and Reiss, R.–D. (2002). An upper bound on the binomial process
approximation to the exceedance process. Extremes 5, 253–269.

6.5. Ultimate and Penultimate GP Approximation
187
For a continuation of this topic, within the framework of exceedance pro-
cesses, we refer to Section 9.4.
Penultimate Modeling
By computing remainder terms one is also able to discuss the concept of penul-
timate distributions. Within EV or GP models one may ﬁnd dfs which are more
accurate approximations of the actual df F m or F [u] than the limiting ones. Such
EV or GP dfs are called penultimate dfs.
Early results of that type are contained in the paper by Fisher and Tippett
cited on page 18. Further references concerning the penultimate EV approximation
are given at the end of this section.
The starting points are the conditions (6.41) and (6.42) which determine an
upper bound for the ultimate rate. Under these conditions a penultimate approx-
imation exists if, and only if, M is slowly varying24.
We assume that the von Mises condition (6.41) and condition24,
M(tx) −M(t)
A(t)
→c log x,
t ↓0,
x > 0,
(6.46)
hold, where A : (0, 1) →(0, ∞) is another auxiliary function and c = −1, 1.
This condition implies that M and A are slowly varying at 0. Hence, (6.42)
holds for M and A with ρ = 0. The rate of convergence in (6.42) is of order
A(t)/M(t) which shows that A is of smaller order than M.
We compute an upper bound for the remainder term in the penultimate GP
approximation which correspond to that in (6.44). Under the conditions (6.41) and
(6.46) we have24
H(f [u], wγ(u),u,σ(u)) ∼CA

F(u)

(6.47)
with σ(u) as in (6.45), and γ replaced by
γ(u) := γ + M(F(u)).
(6.48)
Recall that M(t) = −(log t)−1 for the Gompertz df. In addition, we have
A(t) = (log t)−2
for the second auxiliary function A in this case.
The existence of penultimate distributions can have serious consequences for
the estimation of GP distributions (and, likewise, for EV distributions) and the
interpretation of such results.
Example 6.5.1. (Extreme Life Spans.) Our analysis concerns extreme life spans of women
born before and around the year 1900 and later living in West Germany.
24Kaufmann, E. (2000). Penultimate approximations in extreme value theory. Extremes
3, 39–55.

188
6. Advanced Statistical Analysis
The given data are the ages of female persons at death in West–Germany in the
year 199325. None of the persons died at an age older than 111 years. The analysis is
based on the ages at death of 90 or older.
Table 6.2. Frequencies of life spans (in years).
life span
90
91
92
93
94
95
96
97
98
99
100
frequency
12079
10273
8349
6449
5221
3871
2880
1987
1383
940
579
life span
101
102
103
104
105
106
107
108
109
110
111
frequency
340
207
95
63
36
16
9
4
1
1
1
Gompertz dfs (see page 54) are one of the classical life span dfs. A Gompertz density
with location and scale parameters µ = 83.0 and σ = 9.8 ﬁts well to the histogram of
the life span data above 95 year. According to (6.43)–(6.45), an exponential df provides
an ultimate approximation. The penultimate approach, see (6.47) and (6.48), leads to a
beta dfs with γ(u) ↑0 as u →∞. For more details we refer to Section 19.1.
The possibility of a penultimate approximation must also be taken in account
when the shape parameter γ(u) varies with the threshold u.
Example 6.5.2. (Ozone Cluster Maxima.) We discuss one further aspect of the case
study mentioned in Example 5.1.2. In this case we do not have a classical distribution
for all of the data in mind. However, for the increasing thresholds u = −0.20, 0.22, 0.24
one obtains as estimates of the shape parameter γ the values ˆγ(.20) = −0.41, ˆγ(.22) =
−.35 and ˆγ(.24) = −.33. This indicates the possibility that we estimated penultimate
approximations to the actual distribution with the exponential distribution providing an
ultimate approximation.
This short discussion underlines the importance of penultimate approxima-
tion in extreme value theory. The situation is characterized by the facts that there
is a bad approximation rate by means of the ultimate distribution and a slight
improvement of the accuracy of the approximation by means of penultimate dis-
tributions.
The possibility of occurance of penultimate distributions should not be ig-
nored in statistically oriented applications, e.g., in conjunction with speculations
about the right endpoint of life–span distribution.
25Stored in the ﬁle um–lspge.dat (communicated by G.R. Heer, Federal Statistical
Oﬃce).

6.5. Ultimate and Penultimate GP Approximation
189
The Remainder Term in the EV Approximation
To obtain sharp bounds on the remainder term of the EV approximation to the
joint distribution of several maxima, one must compute the Hellinger distance
between such distributions, see [42] and [16]. Such bounds can be easily computed
if F belongs to a δ–neighborhood of a GP distribution.
In the literature about probability theory one ﬁnds bounds on the maxi-
mum deviation of dfs or, in exceptional cases, for the variational distance between
distributions in the case of a single maximum.
From the Falk–Marohn–Kaufmann theorem26, it is known that an algebraic
rate of convergence—that is a rate of order O(n−c) for some c > 0—can be achieved
if, and only if, condition (6.33) is satisﬁed. This clariﬁes why one gets inaccurate
results in certain cases. For example, the large MSEs in Table 4.2 for normal data
can be explained by the fact that the normal distribution does not satisfy condition
(6.33).
An upper bound, which involves the von Mises condition, was established
by Radtke27. A closely related upper bound may be found in an article by de
Haan and Resnick28. The condition formulated by de Haan and Resnick is better
adjusted to approximations of sample maxima.
First results concerning the penultimate approximation to distributions of
maxima are due to Fisher and Tippett (in the article cited on page 18). These
results were considerably extended by Gomes29. Exact penultimate rates were
recently established by Gomes and de Haan30 and in the afore mentioned article
by Kaufmann24.
26 Falk, M. and Marohn, F. (1993). Von Mises conditions revisited. Ann. Probab. 21,
1310–1328, and Kaufmann, E. (1995). Von Mises conditions, δ–neighborhoods and rates
of convergence for maxima. Statist. Probab. Letters 25, 63–70.
27Radtke, M. (1988). Konvergenzraten und Entwicklungen unter von Mises Bedingun-
gen der Extremwerttheorie. Ph.D. Thesis, University of Siegen, (also see, e.g., [42], page
199).
28de Haan, L. and Resnick, S.I. (1996). Second order regular variation and rates of
convergence in extreme value theory. Ann. Probab. 24, 97-124.
29Gomes, M.I. (1984). Penultimate limiting forms in extreme value theory. Ann. Inst.
Statist. Math. 36, Part A, 71–85, and Gomes, M.I. (1994). Penultimate behaviour of the
extremes. In [15], Vol. 1, 403–418.
30Gomes, M.I. and Haan, L. de (1999). Approximation by penultimate extreme value
distributions. Extremes 2, 71–85.

190
6. Advanced Statistical Analysis
6.6
An Overview of Reduced–Bias Estimation
co–authored by M.I. Gomes31
The semi–parametric estimators of ﬁrst order parameters of extreme or even rare
events—like the tail index, the extremal index, a high q–quantile, a return period
of a high level, and so on—may be based on the k top order statistics Xn−i+1:n,
i = 1, . . . , k, of iid random variables with common df F. Before dealing with second
order parameters, which are decisive for the bias reduction, we recall some basic
facts about the asymptotic theory for heavy tails in the ﬁrst order framework.
Next, we formulate a basic condition for the second order framework, and
• introduce the concept of bias reduction;
• provide details about the jackknife methodology;
• study an approximate maximum likelihood approach, together with the in-
troduction of simple bias–corrected Hill estimators,
• provide an application to data in the ﬁeld of ﬁnance.
This section is concluded with remarks about some recent literature on bias re-
duction.
The First Order Framework Revisited
The estimators for an extreme events’ parameter, say η, are consistent if the df F
is in the domain of attraction of an EV df Gγ, and k is intermediate, i.e.,
k = kn →∞
and
k = o(n) as n →∞.
(6.49)
We shall assume here that we are working with heavy tails and, thus, with shape
parameters γ > 0 in the uniﬁed model. Then, according to (1.29), a df F is in the
max–domain of attraction of Gγ if, and only if, for every x > 0,
F(tx)/F(t) →t→∞x−1/γ, or equivalently, U(tx)/U(t) →t→∞xγ,
(6.50)
where F = 1 −F is the survivor function, and U(t) = F −1(1 −1/t) for t ≥1.
Thus, according to (6.32), the survivor function is regularly varying at inﬁnity
with a negative index −1/γ, or equivalently, U is of regular variation with index
γ. We shall here concentrate on these Pareto–type distributions, for which (6.50)
holds.
31University of Lisbon, this research was partially supported by FCT/POCTI and
POCI/FEDER.

6.6. An Overview of Reduced–Bias Estimation
191
A Second Order Condition and
a Distributional Representation of the Estimators
Under the ﬁrst order condition (6.50), the asymptotic normality of estimators for
an extreme events’ parameter η is attained whenever we assume a second order
condition, i.e., when we assume to know the rate of convergence towards zero of,
for instance, log U(tx) −log U(t) −γ log x, as t →∞.
Such a second order condition may be written as
log U(tx) −log U(t) −γ log x
A(t)
→t→∞
xρ −1
ρ
,
(6.51)
where ρ ≤0 and A(t) →0 as t →∞. More precisely,
|A(tx)/A(t)| →t→∞xρ
for all x > 0;
that is, |A| is regularly varying at inﬁnity with index ρ.
For any classical semi–parametric estimator ηn,k, which is consistent for the
estimation of the parameter η, and under the second order condition (6.51), there
exists a function ϕ(k), converging towards zero as k →∞, such that the following
asymptotic distributional representation
ηn,k
d= η + σϕ(k)Pk + bρA(n/k) (1 + op(1)) ,
(6.52)
holds (thus, we have equality in distribution with one term converging to zero in
probability). Here the Pk are asymptotically standard normal random variables,
σ > 0, bρ is real and ̸= 0, and A(·) the function in (6.51).
We may thus provide approximations to the variance and the bias of ηn,k
given by (σϕ(k))2 and bρA(n/k), respectively. Consequently, these estimators ex-
hibit the same type of peculiarities:
• high variance for high thresholds Xn−k:n, i.e., for small values of k;
• high bias for low thresholds, i.e., for large values of k;
• a small region of stability of the sample path (plot of the estimates versus k
as it is done in Fig. 5.3), as a function of k, making problematic the adaptive
choice of the threshold, on the basis of any sample paths’ stability criterion;
• a “very peaked” mean squared error, making diﬃcult the choice of the value
k0 where the mean squared error function MSE(ηn,k) attains its minimum.
The Concept of Reduced–Bias Estimators
The preceding peculiarities have led researchers to consider the possibility of deal-
ing with the bias term in an appropriate manner, building new estimators ηR
n,k,
the so–called reduced–bias estimators.

192
6. Advanced Statistical Analysis
Under the second order condition (6.51) and for k intermediate, i.e., whenever
(6.49) holds, the statistic ηR
n,k, a consistent estimator of a functional of extreme
events η = η(F), based on the k top order statistics in a sample from a heavy
tailed df F, is said to be a reduced–bias semi–parametric estimator of η, whenever
ηR
n,k
d= η + σRϕ(k)P R
k + op(A(n/k)),
(6.53)
where the P R
k are asymptotically standard normal and σR > 0, with A(·) and ϕ(·)
being the functions in (6.51) and (6.52).
Notice that for the reduced–bias estimators in (6.53), we no longer have a
dominant component of bias of the order of A(n/k), as in (6.52). Therefore,
√
k

ηR
n,k −η

is asymptotically normal with null mean value not only when
√
k A(n/k) →0 (as
for the classical estimators), but also when
√
k A(n/k) →λ, ﬁnite and non–null.
Such a bias reduction provides usually a stable sample path for a wider region of
k–values and a reduction of the mean squared error at the optimal level.
Such an approach has been carried out in the most diversiﬁed manners, and
from now on we shall restrict ourselves to the tail index estimation, i.e., we shall
replace the generic parameter η by the tail index γ, in (6.50). As a consequence,
ϕ(k) = 1/
√
k. The key ideas are either to ﬁnd ways of getting rid of the dominant
component bρA(n/k) of bias in (6.52), or to go further into the second order
behavior of the basic statistics used for the estimation of γ, like the log–excesses
or the scaled log–spacings.
Historical Remarks
We mention some pre–2000 results about bias–corrected estimators in extreme
value theory. Such estimators may be dated back to Reiss32, Gomes33, Drees34
and Peng35, among others. Gomes uses the Generalized Jackknife methodology in
Gray and Schucany [23], and Peng deals with linear combinations of adequate tail
index estimators, in a spirit quite close to the one associated to the Generalized
Jackknife technique.
32Reiss, R.-D. (1989). Extreme value models and adaptive estimation of the tail index.
In [14], 156–165.
33Gomes, M.I. (1994). Metodologias Jackknife e Bootstrap em Estat´ıstica de Extremos.
In Mendes-Lopes et al. (eds.), Actas II Congresso S.P.E., 31–46.
34Drees, H. (1996). Reﬁned Pickands estimators with bias correction. Comm. Statist.
Theory and Meth. 25, 837–851.
35Peng, L. (1998). Asymptotically unbiased estimator for the extreme–value index.
Statist. Probab. Letters 38, 107–115.

6.6. An Overview of Reduced–Bias Estimation
193
The latter technique was also used in Martins et al.36, where convex mixtures
of two Hill estimators, computed at two diﬀerent levels, are considered. Within the
second order framework, Beirlant et al.37 and Feuerverger and Hall38 investigate
the accommodation of bias in the scaled log–spacings and derive approximate
“maximum likelihood” and “least squares” reduced–bias tail index estimators.
The Jackknife and Related Methodologies
First we provide some details about the Jackknife methodology, due to J. Tukey.
This is a nonparametric resampling technique, essentially in the ﬁeld of exploratory
data analysis, whose main objective is the reduction of bias of an estimator, by
means of the construction of an auxiliary estimator based on B. Quenouille’s re-
sampling technique, and the consideration of a suitable combination of the two
estimators.
The Generalized Jackknife statistics of Gray and Schucany [23] are more
generally based on two diﬀerent estimators of the same functional, with similar
bias properties. More precisely, and as a particular case of the Jackknife theory, if
we have two diﬀerent biased consistent estimators η(1)
n
and η(2)
n
of the functional
parameter η(F), such that E

η(1)
n

= η+ϕ(η) d1(n) and E

η(2)
n

= η+ϕ(η) d2(n),
then, denoting by
qn := BIAS

η(1)
n

BIAS

η(2)
n
 = d1(n)
d2(n),
the Generalized Jackknife statistic associated to

η(1)
n , η(2)
n

is
ηG
n

η(1)
n , η(2)
n

= η(1)
n
−qnη(2)
n
1 −qn
,
which is an unbiased consistent estimator of η(F), provided that qn ̸= 1 for all n.
Generalized Jackknife Estimators of the Tail Index
Whenever we are dealing with semi–parametric estimators of the tail index, or
even other parameters of extreme events, we have usually information about the
asymptotic bias of these estimators. We may thus choose estimators with similar
asymptotic properties, and build the associated Generalized Jackknife statistic.
36Martins, M.J., Gomes, M.I. and Neves, M. (1999). Some results on the behavior of
Hill estimator. J. Statist. Comput. and Simulation 63, 283–297.
37Beirlant, J., Dierckx, G., Gogebeur, Y. and Matthys, G. (1999). Tail index estimation
and an exponential regression model. Extremes 2, 177–200.
38Feuerverger, A. and Hall, P. (1999). Estimating a tail exponent by modelling depar-
ture from a Pareto distribution. Ann. Statist. 27, 760–781.

194
6. Advanced Statistical Analysis
This methodology has ﬁrst been used by Martins et al., as noted above,
and still later on, in Gomes et al.39. These authors suggest several Generalized
Jackknife estimators of the tail index γ. We shall here refer only to the one based
on the classical Hill estimator for γ, namely
γ(1)
n,k := 1
k

i≤k

log Xn−i+1:n −log Xn−k:n

,
(6.54)
cf. (5.3), and on the alternative estimator
γ(2)
n,k :=
M (2)
n,k
2M (1)
n,k
,
where, related to the lj,k with a ﬁxed threshold on page 134,
M (j)
n,k := 1
k

i≤k
(log Xn−i+1:n −log Xn−k:n)j ,
j ≥1.
(6.55)
Under the second order condition (6.51), and with P (1)
k
and P (2)
k
asymp-
totically standard normal random variables, we have individually and jointly the
validity of the distributional representations
γ(1)
n (k)
d=
γ + γP (1)
k
√
k
+ A(n/k)
1 −ρ
+ op(A(n/k)),
(6.56)
γ(2)
n (k)
d=
γ +
√
2 γP (2)
k
√
k
+ A(n/k)
(1 −ρ)2 + op(A(n/k)),
(6.57)
where one may choose
P (1)
k
=
√
k
 
i≤k
ηi/k −1

and
P (2)
k
=
√
2
2
√
k
2
 
i≤k
η2
i /k −2

−P (1)
k

,
with ηi, i ≥1 being iid standard exponential random variables. Consequently,
Cov

P (1)
k , P (2)
k

=
√
2/2.
The ratio between the dominant components of bias of γ(1)
n,k and γ(2)
n,k is equal
to 1 −ρ, and we thus get the Generalized Jackknife estimator
γGJ
ρ,k :=

γ(1)
n,k −(1 −ρ) γ(2)
n,k

/ρ,
(6.58)
39Gomes, M.I., Martins, M.J. and Neves, M. (2000). Alternatives to a semi–parametric
estimator of parameters of rare events—the Jackknife methodology. Extremes 3, 207–
229, and Gomes, M.I., Martins, M.J. and Neves, M. (2002). Generalized Jackknife semi–
parametric estimators of the tail index. Portugaliae Mathematica 59, 393–408.

6.6. An Overview of Reduced–Bias Estimation
195
where ρ must still be replaced by an estimator ˆρ. Note that this estimator is
exactly the estimator studied by Peng, cf. page 192, who claimed that no good
estimator for the second order parameter ρ was then available, and considered a
new ρ–estimator, alternative to the ones in Feuerverger and Hall, cf. page 193,
Beirlant et al.40 and Drees and Kaufmann, cf. page 138.
We formulate two asymptotic results for the estimator in (6.58).
• Under the second order condition (6.51) and k intermediate,
γGJ
ρ,k
d= γ + γP GJ
k
'
2ρ2 −2ρ + 1
|ρ|
√
k
+ op(A(n/k)),
(6.59)
where P GJ
k
is an asymptotically standard normal random variable. This re-
sult comes directly from the expression of γGJ
ρ,k , together with the distribu-
tional representations in (6.56) and (6.57).
• The result in (6.59) remains true for the Generalized Jackknife estimator
γGJ
ˆρ,k in (6.58) with ρ replaced by ˆρ, provided that ˆρ −ρ = op(1) for all k
on which we base the tail index estimation, i.e., whenever
√
k A(n/k) →λ,
ﬁnite. For these values of k, we have that
√
k

γGJ
ˆρ,k −γ

is asymptotically
normal with mean zero and variance
σ2
GJ = γ2
1 +
1 −ρ
ρ
2
.
(6.60)
This result comes from the fact that
dγGJ
ρ,k
dρ
=
γ(2)
n,k −γ(1)
n,k
ρ2
= Op

1/
√
k

+ Op

A(n/k)

,
and
γGJ
ˆρ,k
d= γGJ
ρ,k (k) +

ˆρ −ρ

Op

1/
√
k

+ Op

A(n/k)

(1 + op(1)).
(6.61)
A closer look at (6.61) reveals that it does not seem convenient to compute
ˆρ at the same level k we use for the tail index estimation. Indeed, if we do that,
and since we can have ˆρ−ρ = Op

1/
√
k A(n/k)

, we are going to have a change
in the asymptotic variance of the tail index estimator, because (ˆρ −ρ) A(n/k) is
then a term of the order of 1/
√
k.
Gomes et al., see the Extremes–article mentioned on page 194, have indeed
suggested the misspeciﬁcation of ρ at ρ = −1, and the consideration of the estima-
tor γGJ
n,k := 2 γ(2)
n,k −γ(1)
n,k which is a reduced–bias estimator, in the sense herewith
deﬁned, i.e., in the sense of (6.53), if and only if ρ = −1. This was essentially due
to the high bias and variance of the existing estimators of ρ at that time, together
with the idea of considering ˆρ = ˆρk.
40Beirlant, J., Vynckier, P. and Teugels, J.L. (1996). Excess function and estimation of
the extreme–value index. Bernoulli 2, 293–318.

196
6. Advanced Statistical Analysis
The Estimation of ρ
We shall consider here special members of the class of estimators of the second
order parameter ρ proposed by Fraga Alves et al.41 Under adequate general condi-
tions, they are semi–parametric asymptotically normal estimators of ρ, whenever
ρ < 0, which show highly stable sample paths as functions of k, the number of top
order statistics used, for a wide range of large k–values. Such a class of estimators
is parameterized by a tuning real parameter τ, and may be deﬁned as,
ˆρτ,k := −
3(T (τ)
n,k −1)/(T (τ)
n,k −3)
 ,
(6.62)
where
T (τ)
n,k :=
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩

M(1)
n,k
τ −

M(2)
n,k/2
τ/2

M(2)
n,k/2
 τ/2−

M(3)
n,k/6
τ/3
if
τ ̸= 0,
log

M(1)
n,k

−1
2 log

M(2)
n,k/2

1
2 log

M(2)
n,k/2

−1
3 log

M(3)
n,k/6

if
τ = 0,
with M (j)
n,k given in (6.55).
We shall here summarize a few results proved in Fraga Alves et al., now
related to the asymptotic behavior of the estimators of ρ in (6.62):
• For ρ < 0, if (6.49) and (6.51) hold, and if
√
k A(n/k) →∞, as n →∞, the
statistic ˆρτ,k in (6.62) converges in probability towards ρ, as k →∞, for any
real τ.
• Under additional restrictions on k related to a third order framework which
is not discussed here,
√
k A(n/k)

ˆρτ,k −ρ

is asymptotically normal with
mean zero and variance
σ2
ρ =
γ(1 −ρ)3
ρ
2 
2ρ2 −2ρ + 1

.
• For large levels k1, of the type k1 = [n1−ϵ] with ϵ > 0 small, and for a large
class of heavy tailed models, we can guarantee that,

ˆρτ,k1 −ρ

log n = op(1)
as n →∞.
41Fraga Alves, M.I., Gomes, M.I. and de Haan, L. (2003). A new class of semi–
parametric estimators of the second order parameter. Portugaliae Mathematica 60, 193–
213.

6.6. An Overview of Reduced–Bias Estimation
197
The Estimation of ρ in Action
The theoretical and simulated results in the above mentioned article by Fraga
Alves et al., as well as further results in Caeiro et al.42, lead to the proposal of the
following algorithm for the ρ–estimation:
Algorithm for ρ–estimation.
1. Consider any level
k1 = min

n −1, [n1−ϵ] + 1

,
with ϵ small, say ϵ = 0.05.
(6.63)
2. Given a sample (X1, X2, . . . , Xn), plot, for τ = 0 and τ = 1, the estimates
ˆρτ,k in (6.62), 1 ≤k < n.
3. Consider {ˆρτ,k}k∈K, for large k, say k ∈K =

n0.995
,

n0.999
, and com-
pute their median, denoted χτ. Next choose the tuning parameter
τ :=
⎧
⎨
⎩
0
if

k∈K (ˆρ0,k −χ0)2 ≤
k∈K (ˆρ1,k −χ1)2 ,
1
otherwise.
A few comments:
• Step 3 of the algorithm leads in almost all situations to the tuning parameter
τ = 0 whenever |ρ| ≤1 and τ = 1, otherwise. Such a expert’s guess usually
provides better results than a possibly “noisy” estimation of τ, and is highly
recommended in practice. For details on this and similar algorithms for the
ρ–estimation, see Gomes and Pestana43. The choice of the level k1 in (6.63),
and the ρ–estimator
ˆρτ1 := −

3

T (τ)
n,k1 −1

T (τ)
n,k1 −3

,
k1 =

n0.995
,
τ =
 0
if
ρ ≥−1,
1
if
ρ < −1,
(6.64)
is a sensible one.
• It is however possible to consider in Steps 2 and 3 of the algorithm a set T
of τ–values larger than the set T = {0, 1}, to draw sample paths of ˆρτ,k in
(6.62) for τ ∈T , as functions of k, selecting the value of τ which provides
higher stability for large k, by means of any stability criterion. A possible
choice on the lines of the algorithm is thus τ∗:= arg minτ

k∈K (ˆρτ,k −χτ)2.
42Caeiro, F., Gomes, M.I. and Pestana, D. (2005). Direct reduction of bias of the
classical Hill estimator. Revstat 3, 111–136.
43Gomes, M.I. and Pestana, D. (2004). A simple second order reduced–bias’ tail index
estimator. J. Statist. Comp. and Simulation, in press.

198
6. Advanced Statistical Analysis
Representations of Scaled Log–Spacings
Let us consider the representations of scaled log–spacings
Ui := i (log Xn−i+1:n −log Xn−i:n) ,
1 ≤i ≤k.
which are useful for the bias reduction and of interest in its own right.
Under the second order condition (6.51), and for ρ < 0, Beirlant et al., cf.
page 193, motivated the following approximation for the scaled log–spacings:
Ui ∼

γ + A(n/k) (i/k)−ρ
ηi,
1 ≤i ≤k,
(6.65)
where ηi, i ≥1, denotes again a sequence of iid standard exponential random
variables. In the same context, Feuerverger and Hall, cf. page 193, considered the
approximation,
Ui ∼γ exp

A(n/k) (i/k)−ρ /γ

ηi = γ exp (A(n/i)/γ) ηi,
1 ≤i ≤k.
(6.66)
The representation (6.65), or equivalently (6.66), has been made more precise, in
the asymptotic sense, in Beirlant et al.44, in a way quite close in spirit to the
approximations established by Kaufmann and Reiss45 and Drees et al.46.
The Hall Condition, Again
We shall here further assume just as in Feuerverger and Hall, cf. page 193, that
we are in Hall’s class of Pareto–type models, with a survivor function
F(x) = Cx−1/γ
1 + Dxρ/γ + o

xρ/γ
as
x →∞,
C > 0, D real, ρ < 0. Notice that this condition is (6.39) with W(x) = Cx−1/γ
and ρ = −1/δ. Then, (6.51) holds and we may choose
A(t) = αtρ =: γβtρ,
β real,
ρ < 0.
(6.67)
The Maximum Likelihood Estimation
Based on the Scaled Log–Spacings
The use of the approximation in (6.66) and the joint maximization in γ, β and ρ
of the approximate log–likelihood of the scaled log–spacings, i.e., of
log L(γ, β, ρ; Ui, 1 ≤i ≤k) = −k log γ −β

i≤k
(i/n)−ρ −1
γ

i≤k
e−β(i/n)−ρ Ui,
44Beirlant, J., Dierckx, G., Guillou, A. and Stˇaricˇa, C. (2002). On exponential repre-
sentations of log–spacings of extreme order statistics. Extremes 5, 157–180.
45Kaufmann, E. and Reiss, R.–D. (1998). Approximation of the Hill estimator process.
Statist. Probab. Letters 39, 347–354.
46Drees, H., de Haan, L. and Resnick, S.I. (2000). How to make a Hill plot. Ann. Statist.
28, 254–274.

6.6. An Overview of Reduced–Bias Estimation
199
led A. Feuerverger and P. Hall, cited on page 193, to an explicit expression for ˆγ,
as a function of ˆβ and ˆρ, given by
ˆγ = ˆγ
F H
ˆβ,ˆρ,k := 1
k

i≤k
e−ˆβ(i/n)−ˆ
ρ Ui.
(6.68)
Then, ˆβ = ˆβF H
k
and ˆρ = ˆρF H
k
are numerically obtained, through
(ˆβ, ˆρ) := arg min
(β,ρ)

log

1
k

i≤k
e−β(i/n)−ρ Ui

+ β

1
k

i≤k
(i/n)−ρ
.
(6.69)
If (6.49) and the second order condition (6.51) hold, it is possible to state
the following results.
• If we assume ρ to be known,
γF H
ˆβ,ρ,k
d= γ + γ
1 −ρ
ρ
 Γk
√
k
+ op(A(n/k)),
where Γk is asymptotically standard normal.
• If ρ is unknown as well as β, as usually happens, and they are both estimated
through (6.69), then
γF H
ˆβ,ˆρ,k
d= γ + γ
1 −ρ
ρ
2
Γ∗
k
√
k
+ op(A(n/k))
holds, where Γ∗
k is an asymptotically standard normal random variable.
• Consequently, even when
√
k A(n/k) →λ, non–null, we have an asymp-
totic normal behavior for the reduced–bias tail index estimator, with a null
asymptotic bias, but at the expenses of a large asymptotic variance, ruled
by σ2
F H = γ2 ((1 −ρ)/ρ)4 > σ2
GJ for |ρ| < 3.676, with σ2
GJ provided in (6.60).
Indeed,
√
k

γF H
ˆβ,ˆρ,k −γ

is asymptotically normal wit mean zero and variance
σ2
F H = γ21 −ρ
ρ
4
.
(6.70)
A Simpliﬁed Maximum Likelihood Tail Index Estimator
and the External Estimation of ρ
The ML estimators of β and ρ in (6.69) are the solution of the ML system of
equations
ˆb10 ˆB00 −ˆB10 = 0
and
ˆB11 −ˆb11 ˆB00 = 0,

200
6. Advanced Statistical Analysis
where for non–negative integers j and l,
ˆbjl ≡ˆbjl(ˆρ) := 1
k

i≤k
 i
n
−jˆρ
log i
n
l
=:
k
n
ˆρ
ˆcjl,
and
ˆBjl ≡ˆBjl(ˆρ, ˆβ) := 1
k

i≤k
 i
n
−jˆρ
log i
n
l
e−ˆβ(i/n)−ˆ
ρ Ui =:
k
n
ˆρ ˆCjl.
The ﬁrst ML equation may then be written as,

i≤k
i−ˆρ exp

−ˆβ

i/n
−ˆρ
Ui = ˆγ
 
i≤k
i−ˆρ
,
with ˆγ given in (6.68). The use of ex ∼1 + x as x →0, led Gomes and Martins47
to an explicit estimator for β, given by
ˆβGM
ˆρ,k :=
k
n
ˆρ ˆc10 ˆC00 −ˆC10
ˆc10 ˆC10 −ˆC20
,
ˆCj0 = ˆCj = 1
k

i≤k
 i
k
−jˆρ
Ui,
(6.71)
and the following approximate maximum likelihood estimator for the tail index γ,
ˆγGM
ˆρ,k := 1
k

i≤k
Ui −ˆβGM
ˆρ,k
n
k
ˆρ ˆC1,
(6.72)
based on an adequate, consistent estimator for ρ. The estimator in (6.72) is thus a
bias–corrected Hill estimator, i.e., the dominant component of the bias of the Hill
estimator, provided in (6.56) and equal to A(n/k)/(1 −ρ) = γβ(n/k)ρ/(1 −ρ) is
estimated though ˆβGM
ˆρ,k (n/k)ˆρ ˆC1, and directly removed from the Hill estimator in
(6.54), which can also be written as γH
n,k = 
i≤k Ui/k.
If the second order condition (6.51) holds, if k = kn is a sequence of in-
termediate positive integers, and
√
k A(n/k) →λ, ﬁnite, as n →∞, with ˆρ any
ρ–estimator such that ˆρ−ρ = op(1) for any k such that
√
kA(n/k) →λ, ﬁnite, one
obtains that
√
k

γGM
ˆρ,k −γ

is asymptotically normal with mean zero and variance
σ2
GM = γ2(1 −ρ)2
ρ2
.
(6.73)
47Gomes, M.I. and Martins, M.J. (2002). “Asymptotically unbiased” estimators of the
tail index based on external estimation of the second order parameter. Extremes 5, 5–31.

6.6. An Overview of Reduced–Bias Estimation
201
External Estimation of β and ρ
Gomes et al.48 suggest the computation of the β estimator ˆβGM
ˆρ,k at the level k1 in
(6.63), the level used for the estimation of ρ. With the notation ˆβ := ˆβGM
ˆρ,k1, they
suggest thus the replacement of the estimator in (6.72) by
ˆγM
ˆβ,ˆρ,k ≡M ˆβ,ˆρ,k := γH
n,k −ˆβ
n
k
ˆρ ˆC1,
(6.74)
where γH
n,k denotes the Hill estimator in (6.54), and (ˆβ, ˆρ) are adequate consistent
estimators of the second order parameters (β, ρ). With the same objectives, but
with a slightly simpler analytic expression, we shall here also consider the estimator
ˆγH
ˆβ,ˆρ,k ≡H ˆβ,ˆρ,k := γH
n,k

1 −ˆβ (n/k)ˆρ /(1 −ˆρ)

,
(6.75)
studied in Caeiro et al., cf. page 197. Notice that the dominant component of the
bias of the Hill estimator is estimated in (6.75) through γH
n,k ˆβ(n/k)ˆρ/(1 −ˆρ), and
directly removed from Hill’s classical tail index estimator.
The estimation of β and ρ at the level k1 in (6.63), of a higher order than the
level k used for the tail index estimation, enables the reduction of bias without
increasing the asymptotic variance, which is kept at the value γ2, the asymptotic
variance of Hill’s estimator.
Denoting by ˆγ•
ˆβ,ˆρ,k any of the estimators in (6.74) and (6.75), we now formu-
late two asymptotic results for these estimators.
• Under the second order condition (6.51) and k intermediate, further assuming
that A(·) can be chosen as in (6.67), we have
ˆγ•
β,ρ,k
d= γ + γP •
k
√
k
+ op(A(n/k)),
(6.76)
where P •
k is an asymptotically standard normal random variable. This is an
immediate consequence of the representation of ˆγ•
β,ρ,k.
• The result in (6.76) remains true for the reduced–bias tail index estima-
tors estimators in (6.74) and (6.75), provided that ˆβ −β = op(1) and (ˆρ −
ρ) log(n/k) = op(1) for all k on which we base the tail index estimation, i.e.,
whenever
√
k A(n/k) →λ, ﬁnite.
For these values of k, one gets that
√
k

ˆγ•
ˆβ,ˆρ,k −γ

is asymptotically normal
with mean zero and variance
σ2
• = γ2.
(6.77)
48Gomes, M.I., Martins, M.J. and Neves, M. (2005). Revisiting the second order re-
duced bias “maximum likelihood” extreme value index estimators. Notas e Comunica¸c˜oes
CEAUL 10/2005. Submitted.

202
6. Advanced Statistical Analysis
This is immediate from
ˆγ•
ˆβ,ˆρ,k −ˆγ•
β,ρ,k ∼−A(n/k)
1 −ρ
 ˆβ −β
β
+ (ˆρ −ρ) log(n/k)

.
In Fig. 6.7, we illustrate the diﬀerences between the sample paths of the
estimator in (6.75), denoted H ˆβ,ˆρ for the sake of simplicity. We have considered a
sample of size n = 10, 000 from a Fr´echet model, with γ = 1, when we compute ˆβ
and ˆρ at the same level k used for the estimation of the tail index γ (left), when
we compute only ˆβ at that same level k, being ˆρ computed at a larger k-value, let
us say an intermediate level k1 such that √k1 A(n/k1) →∞, as n →∞(center)
and when both ˆρ and ˆβ are computed at that high level k1 (right).
We have estimated β through ˆβ0,k = ˆβGM
ˆρ01,k in (6.71), computed at the level
k used for the estimation of the tail index, as well as computed at the level k1 =
[n0.995] in (6.63), the one used for the estimator ˆρ01 in (6.64). We use the notation
ˆβ01 = ˆβ0,k1. The estimates of β and ρ have been incorporated in the H–estimator,
leading to H ˆβ0,k,ˆρ0,k (left), H ˆβ0,k,ˆρ01 (center) and H ˆβ01,ˆρ01 (right).
0,8
1,0
1,2
0
5000
10000
0,8
1,0
1,2
0
5000
10000
0,8
1,0
1,2
0
5000
10000
H ˆβ 0,k , ˆρ 01
H ˆβ 0,k , ˆρ 0,k
Hˆβ 01, ˆρ 01
k
k
k
H
H
H
Fig. 6.7. External estimation of (β, ρ) at a larger ﬁxed level k1 (right) versus estimation
at the same level both for β and ρ (left) and only for β (center).
From the pictures in Fig. 6.7, as well as from the asymptotic results in (6.60),
(6.70), (6.73) and (6.77), we thus advise, in practice:
• The direct estimation of the dominant component of the bias of Hill’s es-
timator of a positive tail index γ. The second order parameters in the bias
should be computed at a ﬁxed level k1 of a larger order than that of the level
k at which we compute the Hill estimator.
• Such an estimated bias should then be directly removed from the classical
Hill estimator.
• Doing this, we are able to keep the asymptotic variance of the new reduced-
bias tail index estimator equal to γ2, the asymptotic variance of the Hill
estimator.

6.6. An Overview of Reduced–Bias Estimation
203
A Simulation Experiment
We have here implemented a simulation experiment, with 1000 runs, for an un-
derlying Burr parent,
F(x) = 1 −

1 + x−ρ/γ1/ρ, x ≥0,
with ρ = −0.5 and γ = 1. For these Burr models, β = γ for any ρ. We have
again estimated β through ˆβ0,k, computed at the level k used for the estimation
of the tail index, as well as computed at the level k1 in (6.63), the one used for
the estimator ˆρ01 in (6.64). We use the same notation as before. The simulations
show that the tail index estimator H ˆβ01,ˆρ01,k seems to work reasonably well, as
illustrated in Fig. 6.8.
0,4
0,6
0,8
1,0
1,2
1,4
1,6
0
200
400
600
800
1000
0,00
0,05
0,10
0
200
400
600
800
1000
H
k
H
k
H ˆ β 01, ˆ ρ 01
H β,ρ
H β,ρ
E[•]
MSE[•]
H ˆ β 01, ˆ ρ 01
GJ ˆ ρ 01
GJ ˆ ρ 01
H ˆ β 0,k , ˆ ρ 01
H ˆ β 0,k, ˆ ρ 01
Fig. 6.8. Mean values and mean squared errors of estimators the Hill and reduced–bias
estimators, for samples of size n = 1000, from a Burr parent with γ = 1 and ρ = −0.5
(β = 1).
The discrepancy between the behavior of the estimator H ˆβ01,ˆρ01 and the
random variable Hβ,ρ suggests that some improvement in the estimation of second
order parameters may be still welcome, but the behavior of the mean squared error
of the H–estimator is rather interesting: the new estimator H ˆβ01,ˆρ01,k is better than
the Hill estimator not only when both are considered at their optimal levels, but
also for every sub–optimal level k, and this contrarily to what happens with the
classical reduced–bias estimators GJˆρ01 and H ˆβ0,k,ˆρ01, as we may also see in Fig.
6.8.
Some overall conclusions
• If we estimate the ﬁrst order parameter at a level k, and use that same level k
for the estimation of the second order parameter ρ and β in A(t) = γ β tρ, we

204
6. Advanced Statistical Analysis
get a much higher asymptotic variance than when we compute ρ at a larger
level k1, computing β at the same level k. And we may still decrease the
asymptotic variance of the tail index reduced–bias estimators, if we estimate
both second order parameters, β and ρ, at a larger level k1 than the one used
for the estimation of the ﬁrst order parameter.
• The main advantage of the new reduced–bias estimators in (6.74) and (6.75)
lies on the fact that we can estimate β and ρ adequately through ˆβ and
ˆρ so that the MSE of the new estimator is smaller than the MSE of Hill’s
estimator for all k, even when |ρ| > 1, a region where has been diﬃcult to ﬁnd
alternatives for the Hill estimator. And this happens together with a higher
stability of the sample paths around the target value γ. The pioneering paper
on a new type of reduced–bias estimators of a positive tail index, i.e., the
ones with an asymptotic variance equal to γ2, is by Gomes et al.49, who
introduce a weighted Hill estimator, linear combination of the log–excesses.
• To obtain information on the asymptotic bias of these reduced–bias estima-
tors we should have gone further into a third order framework, specifying the
rate of convergence in the second order condition in (6.51). This is however
beyond the scope of this chapter. Interested readers may look into Gomes et
al.50
The Estimation of β and γ in Action
We go on with the following:
Algorithm (β and γ estimation).
4. Chosen τ in step 3, work then with (ˆρ, ˆβ) = (ˆρτ1, ˆβτ1) := (ˆρτ,k1, ˆβˆρτ,k1 ), ˆρτ,k,
k1 and ˆβˆρ,k given in (6.62), (6.63) and (6.71), respectively.
5. Estimate the optimal level for the estimation through the Hill, given by
ˆkH
0 =

(1 −ˆρ)n−ˆρ/(ˆβ √−2ˆρ)
2/(1−2ˆρ), compute kmin = ˆkH
0 /4, kmax = 4ˆkH
0
and plot the classical Hill estimates Hk, kmin ≤k ≤kmax.
6. Plot also, again for kmin ≤k ≤kmax, the reduced-bias tail index estimates
H ˆβ,ˆρ,k and M ˆβ,ˆρ,k, associated to the estimates (ˆρ, ˆβ) in step 4.
49Gomes, M.I., de Haan, L. and Rodrigues, L. (2004). Tail index estimation through
accommodation of bias in the weighted log–excesses. Notas e Comunica¸c˜oes CEAUL
14/2004. Submitted.
50Gomes, M.I., Caeiro, F. and Figueiredo, F. (2004). Bias reduction of a tail index
estimator through an external estimation of the second order parameter. Statistics 38,
497–510.

6.6. An Overview of Reduced–Bias Estimation
205
7. Obtain χH, χH and χM , the medians of Hk, H ˆβ,ˆρ,k and M ˆβ,ˆρ,k, respectively,
for kmin ≤k ≤kmax. Compute the indicators, IH := 
k1≤k≤k2

Hk −χH
2,
IH := 
k1≤k≤k2

H ˆβ,ˆρ,k −χH
2 and IM := 
k1≤k≤k2

M ˆβ,ˆρ,k −χM
2.
8. Let T be the estimator (among H, H and M) providing the smallest value
among IH, IH and IM . Consider ˆγT = χT as estimate of γ.
Financial Data Analysis
We now provide a data analysis of log–returns associated to the Euro–British
pound daily exchange rates, collected from January 4, 1999, until November 17,
2005.
The number of positive log–returns of these data is n0 = 385. The sample
paths of the ρ–estimates associated to the tuning parameter τ = 0 and τ = 1
lead us to choose, on the basis of any stability criterion for large k, like the one
suggested in step 3. of the Algorithm, the estimate associated to τ = 0. The
estimates obtained are

ˆρ0, ˆβ0

= (−0.686, 1.047) obtained at k1 = 808.
In Fig. 6.9, for the Euro–British pound data, we picture the sample paths
of the estimators of the second order parameters ρ (left) and the sample paths of
the classical Hill estimator H in (6.54), the second order reduced–bias tail index
estimators H0 = H ˆβ01,ˆρ01 and M0 = M ˆβ01,ˆρ01, provided in (6.74) and (6.75),
respectively (right). For this data set, the criterion in step 7. of the Algorithm led
to the choice of M0 and to the estimate ˆγM0 = 0.289 (associated to k = 132).
-3
-2
-1
0
0
400
800
ˆ ρ 0(k)
ˆ ρ 1(k)
k
ˆ ρ = −0.70
0,1
0,2
0,3
0,4
10
100
190
H
H 0
ˆ γ M 0 = 0.29
k
M 0
Fig. 6.9. Estimates of ρ, through ˆρτ,k in (6.62), τ = 0 and 1 (left), and of γ, through H,
M and H in (6.54), (6.74) and (6.75) (right), for the positive log–returns on Euro–British
pound data.

206
6. Advanced Statistical Analysis
Additional Literature on Reduced–Bias Tail Index Estimation
Other approaches to reduced–bias estimation of the tail index can be found in
Gomes and Martins51, Fraga Alves52 and Gomes et al.53 54, and the references
therein.
51Gomes, M.I. and Martins M.J. (2001). Alternatives to Hill’s estimator—asymptotic
versus ﬁnite sample behaviour. J. Statist. Planning and Inference 93, 161–180.
52Fraga Alves, M.I. (2001). A location invariant Hill–type estimator. Extremes 4, 199–
217.
53Gomes, M.I., Figueiredo, F. and Mendon¸ca, S. (2005). Asymptotically best linear
unbiased tail estimators under a second order regular variation condition. J. Statist.
Plann. Inf. 134, 409–433
54Gomes, M.I., Miranda, C. and Viseu, C. (2006). Reduced bias tail index estimation
and the Jackknife methodology. Statistica Neerlandica 60, 1–28.

Chapter 7
Statistics of
Dependent Variables
coauthored by H. Drees1
Classical extreme value statistics is dominated by the theory for independent
and identically distributed (iid) observations. In many applications, though, one
encounters a non–negligible serial (or spatial) dependence. For instance, returns
of an investment over successive periods are usually dependent, cf. Chapter 16,
and stable low pressure systems can lead to extreme amounts of rainfall over
several consecutive days. These examples demonstrate that a positive dependence
between extreme events is often particularly troublesome as the consequences,
which are already serious for each single event, may accumulate and ﬁnally result
in a devastating catastrophe.
In Section 7.1 we discuss the impact of serial dependence on the statistical
tail analysis. A more detailed analysis of the consequences for the estimation of
the extreme value index and extreme quantiles is given in Section 7.2 and Section
7.3, respectively, under mild conditions on the dependence structure. Section 7.4
deals with (semi–)parametric time series models where the dependence structure
is known up to a few parameters.
Throughout this chapter we assume that a strictly stationary univariate time
series Xi, 1 ≤i ≤n, with marginal df F is observed, cf. page 165.
1Universit¨at Hamburg.

208
7. Statistics of Dependent Variables
7.1
The Impact of Serial Dependence
The presence of serial dependence has two consequences for the modeling and the
statistical analysis of the data. Firstly, in contrast to the iid case, the stochastic
behavior is not fully determined by the df of a single observation, but the serial
dependence structure must be taken into account, e.g., when the distribution of
the sum of two successive observations is to be analyzed. Secondly, even if one is
interested only in the tail behavior of the marginal distribution, then often the
serial dependence strongly inﬂuences the accuracy of estimators or tests known
from classical extreme value statistics for iid data.
If one does not assume a (semi–)parametric time series model, then rela-
tively few statistical methods for the estimation of the dependence structure be-
tween extreme observations are available. One possible approach to the analysis of
the dependence is to apply multivariate extreme value statistics to the vectors of
successive observations. Although this approach has proved fruitful in probability
theory, it is of limited value in the statistical analysis because, due to the ‘curse of
dimensionality’, in practice only very few consecutive observations can be treated
that way.
Alternatively, estimators for certain parameters related to the serial depen-
dence have been proposed in the literature. Best known are estimators of the ex-
tremal index, that is, the reciprocal value of the mean cluster size, cf. Sections 2.7
and 6.2. Hsing2 also proposed estimators for the cluster size distribution. Unfortu-
nately, all these parameters bear limited information about the serial dependence
structure. For example, Gomes and de Haan3 proved that the probability of k
successive exceedances of an ARCH(1) time series over a high threshold does not
only depend on the extremal index but on the whole dependence structure of the
ARCH process. of the marginal parameters that of the aforementioned depen-
dence parameters. Since a more general approach to the statistical analysis of the
dependence structure is still an open problem, here we will focus on the second
aspect, that is, the inﬂuence of the serial dependence on estimators of marginal
tail parameters like the extreme value index or extreme quantiles.
Recall from (6.11) that under weak conditions one has
P

max
1≤i≤n Xi ≤x

≈P

max
1≤i≤[nθ]
˜Xi ≤x

for large values of n where ˜Xi denote iid random variables with the same df F as
X1 and θ ∈(0, 1] is the extremal index. Hence, as far as maxima are concerned,
the serial dependence reduces the eﬀective sample size by a factor θ. Likewise, one
may expect that a time series with a non–negligible serial dependence bears less
2Hsing, T. (1991). Estimating the parameters of rare events. Stoch. Processes Appl.
37, 117–139.
3Gomes, M.I. and de Haan, L. (2003). Joint exceedances of the ARCH process.
Preprint, Erasmus University Rotterdam.

7.2. Estimation of the Extreme Value Index
209
information on F than an iid sample of the same size, and that thus the estimation
error for marginal parameters will be higher. Indeed, we will see that under mild
conditions on the dependence structure the same estimators as in the classical
iid setting can be used, but that their distribution is less concentrated about the
true value. Generally, the application of standard estimation procedures relies on
ergodic theory4 5. However, if one does not account for this loss of accuracy, e.g.,
in ﬂood frequency analysis, then it is likely that safety margins are chosen too low
to prevent catastrophic events with the prescribed probability.
The construction of conﬁdence intervals based on dependent data is substan-
tially more complicated than in the iid setting, because the extent to which the
estimation error is increased by the serial dependence does not only depend on
a simple dependence parameter—like the extremal index—but on the whole de-
pendence structure in a complex manner. Therefore, a completely new approach
to the construction of conﬁdence intervals is needed, see Sections 7.2 and 7.3. As
a by–product, we will also obtain a new graphical tool for choosing the sample
fraction on which the estimation is based, that may be useful also in the standard
iid setting.
Instead of examining the accuracy of estimators of marginal parameters under
dependence, one may also ﬁrst try to decluster the observed time series and then to
apply the classical statistical theory to the nearly independent data thus obtained,
see also the end of Section 2.7. This approach, however, has two serious drawbacks.
Firstly, the declustering is often a delicate task that must be done manually by
subject–matter specialists. Secondly, although a cluster bears less information than
iid data of the same size, taking into account only one observation from each
cluster usually seems a gross waste of information which will often lead to even
larger estimation errors for the parameter of interest. For these reasons, in what
follows we will adopt are more reﬁned approach.
7.2
Estimating the Extreme Value Index
Assume that the df F belongs to the max–domain of attraction of an extreme value
distribution with index γ. Recall from (1.46) that then the conditional distribution
of an exceedance over a high threshold u can be approximated by a GP distribution
Wγ,u,σu. In Section 5.1 this approximation was used to derive estimators of the
extreme value index γ which use only the k largest observations from analogous
estimators in a parametric GP model. We will see that under mild conditions the
same estimators can be used for time series data.
4R´ev´esz, P. (1968). The Laws of Large Numbers. Academic Press, New York.
5Pantula, S.G. (1988). Estimation of autoregressive models with ARCH errors.
Sankhy¯a 50, 119–148.

210
7. Statistics of Dependent Variables
Conditions on the Dependence Structure
The trivial time series Xi = X1 for all i shows that, without further conditions, in
general this approach is not justiﬁed any longer if the data exhibit serial depen-
dence. To avoid problems of that type, we require that the dependence between
the data observed in two time periods separated by l time points vanishes as l
increases. More precisely, we assume that the so–called β–mixing coeﬃcients tend
to 0 suﬃciently fast. For technical details of the conditions and results mentioned
in this and the next section, we refer to a series of articles6 7 8. This mixing con-
dition is usually satisﬁed for linear time series like ARMA models, and also for
the GARCH models deﬁned by (16.29) and (16.44)—introduced in conjunction
with returns of random asset prices—and, more general, for Markovian time series
under mild conditions. However, time series models with long range dependence
are often ruled out.
Secondly, we need to assume that the serial dependence between exceedances
over a high threshold u stabilizes as u increases. This very mild condition will
be automatically satisﬁed if the vector (X1, X1+h) belongs to the max–domain
of attraction of a bivariate extreme value distribution, see (12.2), for all h > 0.
Finally, we assume that the sizes of clusters of extreme observations have a ﬁnite
variance.
Normal Approximations and Conﬁdence Intervals
Under these mild conditions, it can be shown that the k largest order statistics
Xn−k+1:n, . . . , Xn:n can be approximated by the order statistics of GP random
variables. More precisely, one can approximate the empirical tail quantile func-
tion pertaining to the observed order statistics by a GP quantile function plus a
stochastic error term of order k−1/2, provided k is suﬃciently small relative to the
sample size n, but not too small. The stochastic error term can be described in
terms of a centered Gaussian process whose covariance function is determined by
the tail dependence structure of the observed time series.
From this result, one can derive approximations to the distributions of the
estimators introduced in Section 5.1. For example, the Hill estimator (in the case
γ > 0) and the maximum likelihood estimator in the GP model based on the
k largest order statistics (if γ > −1/2) are approximately normally distributed
with mean γ and variance γ2σ2
0/k and (1 + γ)2σ2
0/k, respectively, where σ2
0 is
6Drees, H. (2000). Weighted approximations of tail processes for β–mixing random
variables. Ann. Appl. Probab. 10, 1274–1301.
7Drees, H. (2002). Tail empirical processes under mixing conditions. In: H.G. Dehling,
T. Mikosch und M. Sørensen (eds.), Empirical Process Techniques for Dependent Data,
325–342, Birkh¨auser, Boston.
8Drees, H. (2003). Extreme quantile estimation for dependent data with applications
to ﬁnance. Bernoulli 9, 617–657.

7.2. Estimation of the Extreme Value Index
211
determined by the dependence structure. If the data are independent, then σ2
0 = 1
while typically σ2
0 > 1 if serial dependence is present in the data. Recall that this
approximation is justiﬁed only if k is suﬃciently small relative to the length of
the time series, while for larger values of k the estimators exhibit a non–negligible
bias if F is not exactly equal to a GP df or, for the Hill estimator, the location
parameter in the GP1 model does not equal 0. Hence the Hill estimator need
not outperform the ML estimator, despite its smaller variance, cf. the discussion
around Fig. 5.1 and Fig. 5.2.
Unlike for iid samples, the estimators will not necessarily be asymptotically
normal if k grows too slowly as the sample size n increases. This new eﬀect can be
explained as follows. If k is very small relative to n, then all order statistics used by
the estimator stem from very few clusters of large observations. However, within
one cluster the behavior of large observations is not determined by the tail df only,
but it is also inﬂuenced by the speciﬁc serial dependence structure. In practice,
the lower bound on the rate at which k tends to inﬁnity does not cause major
problems, because usually the β–mixing coeﬃcients tend to 0 at an exponential
rate. In this case, it is suﬃcient that k is of larger order than log2+ε n for some
ε > 0, which is anyway needed to obtain accurate estimates.
More generally, a large class of estimators ˆγ(k)
n
that use only the k largest
order statistics are approximately normally distributed with mean γ and variance
σ2/k with a σ2 > 0 that is determined by the tail dependence structure of the
time series. While this approximation allows a comparison of the performance of
diﬀerent estimators for a given time series model, it cannot be used directly for
the construction of conﬁdence intervals, because σ2 is unknown.
To overcome this problem, we take advantage of the fact that the size of the
random ﬂuctuations of the estimates ˆγ(k)
n
as k varies are proportional to σ. More
concretely, one can show that, for a suitable chosen j, the estimator
ˆσ2
n :=
k

i=j
(ˆγ(i)
n −ˆγ(k)
n )2
k

i=j
(i−1/2 −k−1/2)2
(7.1)
is consistent for σ2. Simulation studies indicate that in practice one may choose j
equal to the smallest number such that ˆγ(j)
n
is well deﬁned, e.g., j = 2 for the Hill
estimator. Now approximative conﬁdence intervals for γ can be easily constructed.
For example,

ˆγ(k)
n
−Φ−1(1 −α/2)ˆσnk−1/2, ˆγ(k)
n
+ Φ−1(1 −α/2)ˆσnk−1/2
(7.2)
deﬁnes a two–sided conﬁdence interval with nominal coverage probability 1 −α.
Changes of Interest Rates—a Case Study, Part 1
Life insurance companies often invest a large proportion of the capital they manage
in low risk bonds. If they guarantee a minimal interest rate to their customers,

212
7. Statistics of Dependent Variables
they are exposed to the risk that the interest rates of the low risk bonds drop
below this level. On the other hand, a hike of the interest rates will let the value
of the portfolio decrease rapidly, which may cause problems if many customers
withdraw their capital from the company. In both cases a quick change of the
interest rate is particularly troublesome, as it gives the company little time to
adjust its investment strategy.
In this case study, we want to analyze the interest rates of the US treasury
bonds with maturity in 10 years observed on a monthly basis from 1957 to 1999.
The time series Ri, 1 ≤i ≤m, of length m = 515 is assumed stationary. Given
the long observational period, this assumption is somewhat questionable but no
natural alternative model is at hand. We are interested in extreme quantiles of
the change of the rate over one year. In order to not waste data, we analyze the
interest rate changes over overlapping one year periods, namely
Xi := Ri+12 −Ri,
1 ≤i ≤n := m −12 = 503,
which are clearly dependent, see Fig. 7.1.
100
200
300
400
500
-0.05
-0.04
-0.03
-0.02
-0.01
0.01
0.02
0.03
0.04
Fig. 7.1. Yearly changes of in-
terest rates of 10 year treasury
bonds.
As a ﬁrst step in the analysis of the upper tail, we estimate the extreme value
index. The left plot of Fig. 7.2 displays the Hill estimator, the ML estimator in the
GP model and the moment estimator, cf. Section 5.1, as a function of the number
k of largest order statistics. All three estimates are quite diﬀerent: the Hill plot
exhibits a clear (upward) trend almost from the beginning, while the curves of the
ML estimates and the moment estimates are nearly parallel for k between 50 and
200. As a rule of thumb, such a behavior indicates that a Pareto model with non–
vanishing location parameter ﬁts the upper tail well. Indeed, after a shift of the
data by 0.05, the ML estimator and the moment estimator yield almost identical
values for 80 ≤k ≤220 which are relatively stable for 100 ≤k ≤160, and the Hill
plot is very stable for 80 ≤k ≤160 with values close to the other estimates (Fig.
7.2, right plot). Again we see that the Hill estimator is particularly sensitive to
shifts of the data, whereas the moment estimator is less strongly inﬂuenced and

7.2. Estimation of the Extreme Value Index
213
the ML estimator is invariant under such shifts, cf. Fig. 5.1. Here, after the shift,
the Hill estimator with k = 160 seems a reasonable choice, that yields γ ≈0.111.
Other diagnostic tools, like a QQ–plot, conﬁrm that the corresponding pure Pareto
model ﬁts the tail well after the shift of the data.
k
50
100
150
200
250
300
350
400
450
500
-0.4
-0.2
0.2
0.4
0.6
0.8
1
k
50
100
150
200
250
300
350
400
450
500
-0.2
-0.15
-0.1
-0.05
0.05
0.1
0.15
0.2
0.25
Fig. 7.2. Hill estimator (solid), ML estimator (dashed) and moment estimator (dotted)
for the right tail of yearly interest rate changes (left) and the same data shifted by 0.05
(right).
Next we calculate the conﬁdence intervals (7.2). The estimates of the asymp-
totic variance according to (7.1) are plotted in the left graph of Fig. 7.3. Again the
curve is quite stable for k between 80 and 180. The right graph of Fig. 7.3 displays
the Hill estimator together with the 95%–conﬁdence intervals (7.2) as a function
of k. For comparison, also the conﬁdence intervals

ˆγ(k)
n (1 −Φ−1(1 −α/2)k−1/2), ˆγ(k)
n (1 + Φ−1(1 −α/2)k−1/2)

(7.3)
are plotted for α = 0.05, that would be appropriate if the data were independent
and hence the asymptotic variance was equal to γ2. As one expects, the conﬁdence
intervals which take the serial dependence into account, are considerably wider: for
k = 160 it equals [0.086, 0.136] compared with the interval [0.094, 0.128] derived
from the theory for iid data. (However, note that these conﬁdence intervals do
not account for the shift of the data by 0.05, which is motivated by the achieved
coincidence of the moment estimator and the ML estimator and is thus data driven.
Taking this into account would lead to even wider conﬁdence intervals.)
The analysis of the lower tail is more diﬃcult. After shifting the data by 0.2
one obtains the ML estimates and the moment estimates shown in the left plot of
Fig. 7.4. Here the true value may be negative, so that the Hill estimator cannot
be used. For k ≤120 both estimators are very unstable, but for k between 120
and 275 the curves are relatively stable and close together. A QQ–plot based on
the moment estimator with k = 270 shows that the tail is ﬁtted reasonably well,
although the most extreme observations deviate from the ideal line. Simulations,
however, show that for dependent data one must expect much bigger deviations
in the QQ–plot than one typically observes for iid data. Unfortunately, because

214
7. Statistics of Dependent Variables
k
50
100
150
200
250
300
350
0.02
0.04
0.06
0.08
0.1
0.12
0.14
k
50
100
150
200
250
300
350
0.05
0.1
0.15
0.2
0.25
0.3
Fig. 7.3. Estimated asymptotic variance according to (7.1) (left) and the Hill estimator
(solid) with 95%–conﬁdence intervals (7.2) (dashed) and 95%–conﬁdence intervals (7.3)
assuming independence (dotted) (right).
of the large ﬂuctuations of the estimators for small values of k, the estimates for
the asymptotic variance are unreliable, and no reasonable conﬁdence interval is
available.
k
50
100
150
200
250
300
350
400
-0.5
-0.45
-0.4
-0.35
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
0.05
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
0.2
0.205
0.21
0.215
0.22
0.225
0.23
0.235
0.24
0.245
Fig. 7.4. ML estimates (dashed) and moment estimates (dotted) for the left tail (left)
and QQ–plot (right).
Choice of the Sample Fraction Used for Estimation
The shape of the curve of variance estimates displayed in the left plot of Fig.
7.3 is quite common when the Hill estimator is used. Except for the very ﬁrst
values of k, the variance estimates are usually rather large when only few order
statistics are used, then the curve decreases until it stabilizes for intermediate
values of k, and ﬁnally at a certain point it sharply increases. Since the estimator
ˆσ2
n just measures an average squared diﬀerence of Hill estimators based on diﬀerent
numbers of order statistics, it cannot distinguish between random ﬂuctuations
and systematic changes due to an increasing bias. Hence, for larger values of k,
the growing bias leads to increasing variance estimates. Although in this range

7.3. Extreme Quantile Estimation
215
the variance estimates are clearly quite inaccurate, in practice this eﬀect is to be
welcomed for two reasons.
Firstly, much more clearly than the Hill plot, the sharp kink in the curve of
variance estimates indicates the point from which on the bias signiﬁcantly con-
tributes to the error of the Hill estimator. Therefore, the plot of the variance
estimates is a very useful tool for choosing the number of largest order statistics
to be used by the Hill estimator even for iid data.
Secondly, the conﬁdence intervals (7.2) start to widen again at the point
where the bias kicks in, whereas the conﬁdence intervals (7.3) motivated by the
theory for iid data usually shrink with increasing k. Hence the latter give a to-
tally false impression; indeed, in literature one can often ﬁnd plots of “conﬁdence
intervals” which are disjoint for diﬀerent values of k! In contrast, the conﬁdence
intervals derived from the theory for dependent data are qualitatively more rea-
sonable for large values of k (though they are too conservative) in that the growing
uncertainty due to the increasing bias is taken into account.
Unfortunately, as in the analysis of the lower tail in the case study, the
variance estimates (7.1) are often unreliable when the estimator of the extreme
value index are very unstable for small values of k. Sometimes it might help to
start with a larger number of order statistics, that is, to choose a larger value
for j, but this requires the number of observations to be large, which limits the
applicability of this approach. In this case, one might think of robustiﬁed versions
of the variance estimator. For example, the squared diﬀerence (ˆγ(i)
n −ˆγ(k)
n )2 could
be replaced with the absolute diﬀerence and, accordingly, (i−1/2 −k−1/2)2 with
|i−1/2 −k−1/2|, but it is not clear whether the resulting estimator is consistent for
σ2, too.
7.3
Extreme Quantile Estimation
In most applications, not the extreme value index (which is just a parameter in
the limit model for maxima or exceedances) but, e.g., extreme quantiles xp :=
F −1(1 −p) for small p > 0 are the main parameters of interest, see Chapter 16.
Estimation in the Restricted Pareto Model
The estimation of extreme quantiles is particularly simple if the largest order
statistics approximately behave as in the restricted Pareto model with location
parameter 0. Then xp can be estimated by the so–called Weissman estimator
ˆxn,p := ˆx(k)
n,p := Xn−k+1:n
np
k
−ˆγn
(7.4)
where ˆγn = ˆγ(k)
n
denotes an estimator of γ that is based on the k largest obser-
vations. Of course, this approach only makes sense if k is much bigger than np,
because else one could simply use the empirical quantile Xn−[np]+1:n.

216
7. Statistics of Dependent Variables
In the setting of Section 7.2, one has under very mild extra conditions on p
1
log(k/(np)) log ˆxn,p
xp
=
1
log(k/(np))
 ˆxn,p
xp
−1

(1 + oP (1))
=
(ˆγn −γ)(1 + oP (1)).
Hence, if ˆγn is approximately normal with mean γ and variance σ2/k, then both
log(ˆxn,p/xp) and ˆxn,p/xp −1 are approximately normal with mean zero and vari-
ance log2(k/(np))σ2/k. In particular if one uses the Hill estimator ˆγn, the normal
approximation is much better for log(ˆxn,p/xp) than for the relative estimation
error ˆxn,p/xp −1, because log ˆxn,p is a linear function of ˆγn, which in turn is a
linear statistic and hence is often well approximated by a normal random variable
(cf. Fig. 7.10). Therefore, conﬁdence intervals shall be based on the normal ap-
proximation for log ˆxn,p rather than the normal approximation for ˆxn,p. If ˆσ2
n is a
suitable estimator for σ2, then

ˆxn,p exp

−Φ−1(1 −α/2)ˆσnk−1/2 log(k/(np))

,
ˆxn,p exp

Φ−1(1 −α/2)ˆσnk−1/2 log(k/(np))

(7.5)
is a conﬁdence interval for xp with approximate coverage probability 1 −α.
Here one may use the variance estimator (7.1) deﬁned in terms of diﬀerences
of γ–estimates, but it is more natural to deﬁne an analogous estimator in terms
of logarithmic quantile estimators log ˆx(k)
n,p for diﬀerent numbers k of largest order
statistics:
ˆσ2
n,p :=
k

i=j
log(ˆx(i)
p /ˆx(k)
p )
log(i/(np))
2
k

i=j

i−1/2 −log(k/(np))
log(i/(np)) k−1/22
,
(7.6)
with j greater than np. Unlike (7.1), this estimator measures directly the ﬂuctu-
ations of log ˆx(k)
n,p as a function of k. Because the estimator (7.1), as an estimator
for the asymptotic variance of the quantile estimator, relies on the aforementioned
asymptotic relationship between the ﬂuctuations of ˆγn and of log ˆx(k)
n,p, one may
expect that the estimator ˆσ2
n,p performs better for moderate sample sizes when
this asymptotic approximation is inaccurate.
In an extensive simulation study9 it has been shown that for the Hill estimator
and several time series models, the conﬁdence intervals (7.5) with the variance
estimated by (7.6) are quite accurate, though a bit too conservative, because the
asymptotic variance is over–estimated. To correct for this over–estimation, one
may choose k ≥k0 such that ˆσ2
n,p (or the width of the conﬁdence intervals) is
minimized, when k0 is chosen such that the variance estimates seem reliable for
k ≥k0. This simple trick works surprisingly well for many time series models.
9Drees, H. (2003). Extreme quantile Estimation for Dependent Data with Applications
to Finance. Bernoulli 9, 617–657.

7.3. Extreme Quantile Estimation
217
Note that the asymptotic variance of the quantile estimators does not de-
pend on p. Hence one may use an exceedance probability ˜p in the deﬁnition (7.6)
of ˆσ2
n,˜p diﬀerent from the exceedance probability p of the extreme quantile one is
actually interested in. While this approach contradicts the general philosophy ex-
plained above (namely that one should use the same estimators for the estimation
of the variance and for the estimation of the parameter of interest), it improves
the performance of the conﬁdence intervals signiﬁcantly if p is so small such that
F −1(1 −p) lies far beyond the range of observations and thus the quantile es-
timators are quite inaccurate. In such a situation it is advisable to estimate the
variance by ˆσ2
n,˜p such that the corresponding quantile F −1(1−˜p) lies at the border
of the sample, e.g., ˜p = 1/n.
Changes of Interest Rates—a Case Study, Part 2
In the situation of the case study discussed above, we aim at estimating the max-
imal yearly change of interest rates of the 10 year US treasury bonds which is
exceeded with probability 1/2000, i.e., we want to estimate x0.0005. Again we use
the Hill estimator for the extreme value index applied to the observed yearly
changes shifted by 0.05.
The left plot in Fig. 7.5 displays the variance estimates (7.6) with j = 2. (The
estimates do not change much when one uses ˆσ2
n,˜p with ˜p = 1/500 and j = 3.)
As the variance estimates ˆσ2
n for the Hill estimator, see Fig. 7.3, the variance es-
timates ˆσ2
n,p are quite high if only few order statistics are used and they stabilize
for k around 150, but here for k = 160 one obtains an estimated variance of 0.038,
while for the Hill estimator one gets 0.027. This seems to contradict the asymp-
totic result, according to which these asymptotic variances are equal. However, a
closer inspection of the proof of the asymptotic normality of the quantile estimator
shows that the ﬁrst term ignored in the asymptotic expansion is just of the order
1/ log(k/(np)) smaller than the leading term. Thus for moderate sample sizes, the
asymptotic result can be quite misleading.
The quantile estimates and the conﬁdence intervals based on the variance
estimates discussed above are shown in the right plot of Fig. 7.5 together with
the conﬁdence intervals when the asymptotic variance is estimated by ˆγ2
n as it is
suggested by the asymptotic theory for iid data. (The estimates are corrected for
the shift by 0.05 of the observed interest rate changes.) Here, for k = 160, the
former interval [0.044, 0.089] is more than 4 times longer than the latter interval
[0.060, 0.070]! In such a situation, for two reasons it is questionable to use the
standard asymptotic theory: ﬁrstly, one ignores the loss of accuracy due to the
serial dependence, and secondly, the ﬁrst order asymptotic approximation of the
quantile estimator is rather crude for moderate sample sizes.
The point estimate for x0.0005 is 0.065. Note that the conﬁdence intervals
are not symmetric about this point estimate, because we have used the normal
approximation of log ˆxn,p rather than that of ˆxn,p to construct the conﬁdence
intervals.

218
7. Statistics of Dependent Variables
k
50
100
150
200
250
300
350
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
k
50
100
150
200
250
300
350
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Fig. 7.5. Variance estimator (7.6) (left) and the estimated quantile F −1(1 −1/2000)
(solid) with conﬁdence intervals (7.5) (dashed) and the conﬁdence intervals derived from
the asymptotic theory for iid data (dotted) (right).
Estimation in the Generalized Pareto Model
While in principle it is quite simple to calculate point estimates of extreme quan-
tiles in the full GP model, the construction of reliable conﬁdence intervals is a
diﬃcult task.
To estimate xp = F −1(1 −p), one ﬁts the GP model to the k largest order
statistics, as it is described in Section 5.1. To this end, one may use the maximum
likelihood estimator of the extreme value index and the scale parameter of the
approximating GP model (provided γ > −1/2), or the moment estimators
ˆγn := l1,n + 1 −1
2

1 −l2
1,n
l2,n
−1
,
ˆsn := 2l3
1,n
l2,n
with
lj,n :=
1
k −1
k−1

i=1
logj Xn−i+1:n
Xn−k+1:n
.
Then an estimator for xp is given by
˜xn,p := Xn−k+1:n + ˆsn
(np/k)−ˆγn −1
ˆγn
.
(7.7)
To construct conﬁdence intervals in an analogous way as in the restricted
Pareto model, one needs suitable approximations to the distribution of these quan-
tile estimators. Unfortunately, it turns out that the approximations have a quite
diﬀerent form for heavy tailed and light tailed distributions. If F is heavy tailed
(i.e., γ > 0), then the extreme quantiles are unbounded and the relative estima-
tion error ˆxn,p/xp −1 (or log(ˆxn,p/xp)) can be approximated by a normal random
variable as in the restricted Pareto model. Since for light tailed distributions (i.e.,
γ < 0) both the estimate and the extreme quantile to be estimated are close to the

7.4. A Time Series Approach
219
right endpoint of the distribution, the relative estimation error is of little inter-
est. In this case, a normal approximation can be established for the standardized
absolute estimation error.
More precisely, assume that the extreme value index γ < 0 and the scale
parameter of the GP approximation of the exceedances over Xn−k:n are estimated
by ˆγn = ˆγ(k)
n
and ˆsn = ˆs(k)
n , respectively, using the maximum likelihood approach
(if γ > −1/2) or the moment estimators mentioned above. Then
k1/2
n
ˆsn
(˜xn,p −xp)
is approximately normally distributed with mean 0 and a variance σ2 which again is
determined by the serial dependence in a complex manner.10 The limiting variance
can be consistently estimated by
ˆσ2
n :=
k

i=j
 ˜x(i)
n,p −˜x(k)
n,p
ˆs(i)
n
2
k

i=j
(i−1/2 −k−1/2)2
(7.8)
with ˆs(i)
n
denoting the estimator of the scale parameter of the GP approximation
of the exceedances over Xn−i:n.
Note that the estimators (7.6) and (7.8) for the approximative variances of the
quantile estimators look quite diﬀerent in the cases γ > 0 and γ < 0, respectively,
and likewise the resulting conﬁdence intervals do. In principle, one could ﬁrst test
for the sign of the extreme value index, and then construct the conﬁdence intervals
accordingly, but apparently this procedure does not work well in practice if γ is
close to 0. The problem of ﬁnding good estimators of the approximative variance is
aggravated by the large variability of many estimators of the extreme value index
based on a small number of order statistics (cf. part 1 of the case study discussed
above). Thus, the construction of reliable conﬁdence intervals remains an open
problem if γ is small in absolute value.
7.4
A Time Series Approach
Often parametric dependence structures are used to model time series, because
they facilitate a better interpretation of the dynamics of the time series than a non–
parametric model. Moreover, usually they allow for more eﬃcient estimators. The
best known examples are ARMA(p, q) models, where it is assumed that (Xt)1≤t≤n
is a stationary time series satisfying
Xt −
p

i=1
ϕiXt−i = Zt +
q

j=1
ϑjZt−j,
p + 1 ≤t ≤n,
(7.9)
10Drees, H. (2003). Extreme quantile stimation for dependent data with applications
to ﬁnance. Bernoulli 9, p. 652.

220
7. Statistics of Dependent Variables
with centered iid innovations Zt and, in addition, unknown coeﬃcients ϕ1, . . . , ϕp
and ϑ1, . . . , ϑq.
Suppose that the df FZ of the innovations has balanced heavy tails, that is,
FZ ∈D(Gγ) for some γ > 0 and (1 −FZ(x))/FZ(−x) →p/(1 −p) as x →∞for
some p ∈(0, 1). Then the tail of the df of the observed time series is related to the
tail of FZ as follows:
1 −FX(x) ∼cϕ,ϑ(1 −FZ(x)),
x →∞;
see, e.g., Datta and McCormick (1998)11. Here cϕ,ϑ is a constant depending on
(ϕ1, . . . , ϕp, ϑ1, . . . , ϑq). In particular, FX ∈D(Gγ) and
F −1
X (1 −p) ∼F −1
Z

1 −
p
cϕ,ϑ

,
p →0.
(7.10)
Therefore, one may adopt two diﬀerent approaches to the tail analysis of the
time series:
• Estimate γ and F −1
X (1−p) directly from the observed time series as discussed
in the Sections 7.2 and 7.3.
• First estimate ϕ1, . . . , ϕp and ϑ1, . . . , ϑq by some standard method for ARMA
models, cf. [5].
In the next step, calculate the residuals ˆZt based on the ﬁtted model; for
example, in an AR(p) model (i.e., if (7.9) holds with ϑ1 = · · · = ϑq = 0) let
ˆZt := Xt −
p

i=1
ˆϕiXt−i,
p + 1 ≤t ≤n.
Finally, apply the classical theory for iid observations to the residuals to
estimate γ and F −1
Z (1 −p/c ˆϕ, ˆϑ) (as an approximation to F −1
X (1 −p)).
For AR(p) time series, Resnick and St˘aric˘a12 demonstrated that the Hill esti-
mator performs better in the second approach. A similar result was recently proved
by Ling and Peng (2004)13. To the best of our knowledge, analogous results about
the corresponding quantile estimators are not known up to now. We expect that
the model based tail analysis using the residuals also leads to more accurate quan-
tile estimates, because the estimation error of the Hill estimator asymptotically
determines the performance of the quantile estimators.
11Datta, S. and McCormick, W.P. (1998). Inference for the tail parameters of a linear process
with heavy tail innovations. Ann. Inst. Statist. Math. 50, 337–359.
12Resnick, S.I. and Stˇaricˇa, C. (1997). Asymptotic behavior of Hill’s estimator for autoregres-
sive data. Comm. Statist. Stochastic Models 13, 703–721.
13Ling, S. and Peng, L. (2004). Hill’s estimator for the tail index of an ARMA model. J. Statist.
Plann. Inference 123, 279–293.

7.4. A Time Series Approach
221
However, the relationship (7.10) between the tails of FX and FZ heavily relies
on the model assumptions. Hence, even a moderate deviation from the assumed
model can lead to gross estimation errors. This drawback of the indirect model
based approach is aggravated by the fact that, due to the lack of data, model
deviations in the tails are particularly diﬃcult to detect statistically.
Quantile Estimation in AR(1) Models: a Simulation Study
Consider the AR(1) model
Xt = ϕXt−1 + Zt,
2 ≤t ≤n = 2000,
(7.11)
with 1 −FZ(x) = FZ(−x) = 0.5(1 + x)−1/γ, x ≥0, ϕ = 0.8, and γ = 1/2, i.e., the
positive and the negative part of the innovations have a Pareto distribution with
location parameter −1. Thus p = 1/2 and in this case cϕ,ϑ = 1/(2(1 −|ϕ1|)).
We aim at estimating the quantile F −1
X (1 −p) with p = 0.001, which usually
lies near the boundary of the range of observations. To calculate the true quantile
approximately, we simulate a time series of length 410 000 and determine the 400th
largest of the last 400 000 observations (i.e., the ﬁrst 10 000 simulated values are
used as a burn-in period to approach the stationary distribution). We do this
m = 10 000 times, and ﬁnally approximate the unknown true quantile by the
average of the m simulated order statistics, which yields F −1
X (1 −p) = 37.015
(with estimated standard deviation 0.02).
To estimate the quantile using the model based approach, we ﬁrst estimate
ϕ by the sample autocorrelation at lag 1, which is the Yule-Walker-type estimator
for AR(1) time series, cf. Section 6.2:
ˆϕ :=
n
t=2 Xt−1Xt
n
t=1 X2
t
.
(7.12)
Note that ˆϕ is a sensible estimator for ϕ even if the variance of Xt is inﬁnite and
hence the autocorrelation does not exist14.
Then we calculate the residuals
ˆZt := Xt −ˆϕXt−1,
2 ≤t ≤n.
(7.13)
Finally, estimate F −1
X (1 −p) by
ˆx(2)
n,p := ˆZn−k:n−1
(n −1)2(1 −| ˆϕ|)
k
−˜γn−1
with
˜γn−1 :=
1
k −1
k−1

i=1
log
ˆZn−i:n−1
ˆZn−k:n−1
14Davis, R.A. and Resnick, S.I. (1986). Limit theory for the sample covariance and correlation
functions of moving averages. Ann. Statist. 14, 533–558.

222
7. Statistics of Dependent Variables
denoting the Hill estimator based on the k largest order statistics of these residuals.
In addition, we estimate F −1
X (1 −p) directly using ˆxn,p deﬁned in (7.4).
The empirical root mean squared error (rmse) of these estimators (obtained in
10 000 simulations) are displayed in Fig. 7.6 as a function of k. Since these estimates
of the true rmse are somewhat unstable (i.e. they change from simulation to simu-
lation), we also show the corresponding simulated L1–errors E
ˆx(2)
n,p −F −1
X (1 −p)

and E
ˆxn,p −F −1
X (1 −p)
, which are more reliable approximations to the true
errors.
50
100
150
200
250
300
350
400
20
40
60
80
100
Fig. 7.6. Empirical root mean
squared error of
ˆxn,p
(solid
line) and ˆx(2)
n,p (dashed line)
and the corresponding empiri-
cal L1–errors (dotted line, resp.
dash-dotted line) in the AR(1)
model (7.11).
As expected, the estimator ˆx(2)
n,p based on the residuals is more accurate if k is
chosen appropriately. Both the rmse and the L1–error of the model based estimator
ˆx(2)
n,p is minimal for k = 50 with minimal values 15.8 and 11.2, respectively. In
contrast, the optimal k is much greater for the direct estimator ˆxn,p and the errors
are about 80% and 40%, respectively, larger. (The minimal rmse 28.3 is attained
for k = 184 and the minimal L1–error 15.3 for k = 159.)
The situation changes dramatically if we perturb the linear AR(1) model by
a logarithmic term:
Xt = ϕXt−1 + δ · sgn(Xt−1) log

max(|Xt−1|, 1)

+ Zt
(7.14)
with FZ as above and δ = 0.6. Then the true quantile F −1
X (1 −p) to be estimated
is approximately equal to 41.37 (with estimated standard deviation 0.02).
Note that the logarithmic perturbation of the linear dependence structure,
that aﬀects only the observations greater than 1 in absolute value, is diﬃcult to
distinguish from an increase of the autoregressive parameter ϕ. Indeed, in the
average the estimate ˆϕ deﬁned in (7.12) equals 0.92 in the perturbed model,
and the average relative diﬀerence between the perturbed autoregressive term
ϕXt−1+δ·sgn(Xt−1) log

max(|Xt−1|, 1)

and the ﬁtted AR(1) term ˆϕXt−1 is just
5.2%. Therefore, in the scatterplot of Xt versus Xt−1 displayed in Figure 7.7 for
one simulated sample of size n = 2 000 the deviation from the linear dependence
can hardly be seen. Moreover, the model deviation is also diﬃcult to detect by

7.4. A Time Series Approach
223
statistical tests. For example, the turning point test and the diﬀerence–sign test,
see [5], p. 37f., to the nominal size 5% reject the null hypothesis that the residuals
ˆZt deﬁned in (7.13) are iid with probability less than 6%. (Note that the more
popular tests for the linear AR(1) dependence structure which are based on the
sample autocorrelation function are not applicable in the present setting because
the variance may be inﬁnite.)
-40
-30
-20
-10
10
20
30
40
-40
-30
-20
-10
10
20
30
40
Fig. 7.7. Scatterplot of Xt ver-
sus Xt−1 and line through ori-
gin with slope ˆϕ.
Fig. 7.8 shows the rmse and L1–errors of the direct quantile estimator ˆxn,p
and the estimator ˆx(2)
n,p which is based on the (wrong) linear AR(1) model (7.11).
Now the model based estimator has minimal rmse 31.7 and minimal L1-error 22.8
(both attained at k = 2) while the simulated errors for the direct estimator equal
14.1 (for k = 282) and 10.0 (for k = 305), respectively. Thus, for the perturbed
time series, the estimation error of the model based estimator is more than double
as big as the error of the direct quantile estimator, which does not rely on a
speciﬁc time series model. Another disadvantage of the model based estimator is
the sensitivity of its performance to the choice of k. The smallest but one L1–error
is about 30% larger than the minimal value; it is attained for k = 18. Moreover,
for k > 50 the estimation error increases rapidly.
Fig. 7.9 displays the estimated density of the distribution of the quantile
estimators in the AR(1) model (7.11) (left plot) and the perturbed model (7.14)
(right plot). Here we use kernel estimates based on the Epanechnikov kernel with
bandwidth 2 applied to the simulated quantile estimates with minimal L1–error.
In the AR(1) model, the mode of the distribution of the model based estimator
ˆx(2)
n,p is very close to the true value, which is indicated by the vertical dotted line.
The distribution of ˆxn,p has a lower mode and is also a bit more spread out. In
contrast, in the perturbed model (7.14) the mode of the density of ˆx(2)
n,p is much
too low and the distribution is much more spread out than the distribution of ˆxn,p.
In addition, the estimated density of the model based estimator is also shown for
k = 18 (i.e. the value leading to the smallest but one L1–error). Here the mode
is approximately equal to the true value, but the distribution is even more spread

224
7. Statistics of Dependent Variables
50
100
150
200
250
300
350
400
20
40
60
80
100
Fig. 7.8. Empirical root mean
squared error of
ˆxn,p
(solid
line) and ˆx(2)
n,p (dashed line)
and the corresponding empiri-
cal L1–errors (dotted line, resp.
dash-dotted line) in the per-
turbed AR(1) model (7.14).
out.
50
100
150
200
0.01
0.02
0.03
0.04
0.05
50
100
150
200
0.01
0.02
0.03
0.04
0.05
Fig. 7.9. Kernel density estimates for ˆxn,p (solid line) and ˆx(2)
n,p (dashed line) with minimal
the L1–error in the AR(1) model (7.11) (left plot) and the perturbed model (7.14) (right
plot), where in addition, the estimated density is shown for ˆx(2)
n,p with the smallest but
one L1–error (dash-dotted line); the true values are indicated by the vertical lines.
From Fig. 7.9 it is also obvious that the distribution of the quantile estimator
ˆxn,p is strongly skewed to the right. Hence it cannot be well ﬁtted by a normal
distribution. Fig. 7.10 demonstrates that the normal ﬁt to the distribution of
log ˆxn,p is much better. Therefore, as explained above, one should use the normal
approximation to log ˆxn,p rather than a normal approximation to ˆxn,p to construct
conﬁdence intervals.
Non–linear Time Series
Non–linear time series models are particularly popular in ﬁnancial applications.
For example, various types of GARCH time series have been proposed to model
returns of risky assets, cf. Section 16.7. Here we consider GARCH(1,1) time series,
i.e., stationary solutions {Rt}, with t ranging over all natural numbers, of the

7.4. A Time Series Approach
225
50
100
150
200
0.005
0.01
0.015
0.02
0.025
0.03
0.035
2
2.5
3
3.5
4
4.5
5
5.5
6
0.2
0.4
0.6
0.8
1
1.2
Fig. 7.10. Kernel density estimates for ˆxn,p (solid line, left plot) and log ˆxn,p (solid line,
right plot) with minimal the L1–error in the AR(1) model (7.11) together with the
densities of the normal distribution ﬁtted by minimizing the Hellinger distance (cf. Section
3.1) (dotted lines).
stochastic recurrence equations
Rt
=
˜σtεt,
˜σ2
t
=
α0 + α1R2
t−1 + β1˜σ2
t−1,
with α0 > 0, α1, β1 ≥0 and iid innovations εt with mean 0 and variance 1. Under
mild conditions on the distribution of εt, the returns Rt are heavy–tailed with
extreme value index γ > 0 determined by the equation
E(α1ε2
1 + β1)1/(2γ) = 1,
cf. page 398.
If one ﬁxes the distribution of innovations, e.g., one assumes standard normal
innovations, then one might estimate the extreme value index from this relation-
ship after replacing the unknown parameters by suitable estimates, e.g., quasi
maximum likelihood estimates, cf. Section 16.7. If one does not ﬁx the distribu-
tion of the innovations, then ﬁrst one must ﬁt some model to the residuals, in
order to estimate γ this way. In any case, unlike for linear time series models, here
the extreme value index depends on the whole distribution of the innovations.
Thus any misspeciﬁcation of this distribution may lead to a serious estimation
error for γ. In an empirical study15, Stˇaricˇa and Pictet compared the estimates of
the extreme value index obtained from ﬁtting a GARCH(1,1) model with normal
innovations to the returns of several exchange rates aggregated over diﬀerent time
intervals with the Hill estimators applied directly to these data sets. It turned out
that for almost all levels of aggregation the approach based on the GARCH model
apparently underestimates the tail thickness drastically, an eﬀect which is most
15Stˇaricˇa, C. and Pictet, O. (1997). The tales the tails of GARCH processes tell.
Preprint, Chalmers University Gothenburg.

226
7. Statistics of Dependent Variables
likely caused by a misspeciﬁcation of the time series model (or by the choice of
the estimator).
However, even if the assumed GARCH model is correct, this does not help
much when it comes to extreme quantile estimation. While it is known that, for
symmetric innovations,
P{Rt > x} ∼cx−1/γ
as
x →∞,
no analytic expression in terms of α0, α1 and β1 and the distribution of εt is known
for the constant c > 0. Hence, in the present setting, we cannot copy the model
based approach for the estimation of extreme quantiles discussed above for ARMA
models. Of course, in principle, one can ﬁrst ﬁt the GARCH model and then obtain
an approximation to the extreme quantile by simulation, but the simulation results
obtained by Mikosch16 indicate that it can be quite computer intensive to obtain
accurate approximations of extreme quantiles this way even if the model would be
exactly known.
In view of these drawbacks of the time series approach, the general method
discussed in the Sections 7.2 and 7.3 seems the only feasible way to obtain reliable
estimates of extreme quantiles. In fact, this conclusion holds true for most non–
linear time series models, for which typically no simple asymptotic relationship
between the tails of the innovations and the tails of the time series is available.
16Mikosch, T. (2003). Modeling dependence and tails of ﬁnancial time series. In: Ex-
treme Value Theory and Applications, Finkenstadt, B. and Rootz´en, H. (eds.), Chapman
and Hall, 185–286.

Chapter 8
Conditional Extremal
Analysis
In this chapter we provide an overview of the conditioning concept including some
theoretical applications. After some heuristic arguments and technical preparations
in Section 8.1. we study conditional extremes in a nonparametric and, respectively,
in a parametric framework, see Sections 8.2 and 8.3.
The Bayesian view towards the Bayesian estimation principle is outlined in
Section 8.4. Especially, we give the standard Bayesian interpretation of the poste-
rior density as a conditional density.
8.1
Interpretations and Technical Preparations
This section lays out heuristic arguments about the conditioning concept and col-
lects some relevant auxiliary technical results. We make an attempt to clarify the
relationships between joint distributions, conditional distributions and mixtures
(shortly address as “roundabout”). In this section we also study the linear regres-
sion problem for a random design as a ﬁrst application.
Conditional Distribution Functions
In Section 1.1 we introduced elementary conditional dfs, namely exceedance dfs
P(Y ≤y|Y > u) and excess dfs P(Y −u ≤y|Y > u). In addition, the mean excess
function (in other words, the mean residual life function) was determined by the
means of excess dfs, cf. (2.20).
In the sequel, we consider conditional probabilities of a random variable Y
conditioned on the event that X = x. Generally, the conditional df F(y|x) of Y
given X = x is deﬁned by an integral equation, see (8.3). In the special case, where

228
8. Conditional Extremal Analysis
P{X = x} > 0, one may compute the conditional df by
F(y|x) = P(Y ≤y|X = x) = P{X = x, Y ≤y}
P{X = x}
.
(8.1)
The joint random experiment with outcome (x, y), described by the random
vector (X, Y ), can be regarded as a two–step experiment:
Step 1 (initial experiment): observe x as an outcome of the random
experiment governed by X.
Step 2 (conditional experiment): generate y under the conditional df
F(·|x) given x.
Keep in mind that y can be interpreted as the second outcome in the joint exper-
iment as well as the outcome of the conditional experiment.
If X has a discrete distribution, then the joint df F(x, y) of X and Y can be
regained from the distribution of X and the conditional df by
F(x, y) =

z≤x
F(y|z)P{X = z}.
(8.2)
If X does not have a discrete distribution, then the conditional df F(·|x) is
a solution of the integral equation
F(x, y) =
 x
−∞
F(y|z) dFX(z),
(8.3)
where FX, usually written F(x), denotes the df of X. The preceding interpretations
just remain the same. Notice that (8.2) can be written in the form of (8.3).
It is apparent that the df of Y is given by
F(y) =

F(y|z) dFX(z).
(8.4)
In addition, a density f(·|x) of F(·|x) is referred to as conditional density of
Y given X = x. Usually, we write f(y|x) in place of f(·|x), etc.
Conditional Distributions
The conditional distribution pertaining to the conditional df F(y|x) is denoted by
P(Y ∈·|X = x).
(8.5)
Apparently, P(Y ≤y|X = x) = F(y|x).
Generally, a conditional distribution P(Y ∈·|X = x) has the property
P{X ∈B, Y ∈C} =

B
P(Y ∈C|X = x) dL(X)(x),
(8.6)

8.1. Interpretations and Technical Preparations
229
and, hence, also
P{Y ∈C} =

P(Y ∈C|X = x) dL(X)(x)
(8.7)
which reduces to (8.3) and (8.4) in the special case of dfs.
Basic Technical Tools for Conditional Distributions
In the following lines we formulate some concepts and results for conditional dfs
and related conditional densities The experienced reader may elaborate these ideas
in greater generality, namely, in terms of conditional distributions and conditional
densities with respect to some dominating measure.
• (Computing the conditional density.) The conditional density of Y given
X = x can be written as
f(y|x) = f(x, y)
f(x) I(f(x) > 0).
(8.8)
Therefore, the conditional density can be deduced from the joint density
f(x, y) because
f(x) =

f(x, y) dy
and
f(y) =

f(x, y) dx.
(8.9)
• (Computing the joint density.) If f(x) is a density of F(x), then
f(x, y) = f(y|x)f(x)
(8.10)
is a density of F(x, y); by induction one also gets the representation
f(x1, . . . , xn) = f(x1)
n

i=2
f(xi|xi−1, . . . , x1),
(8.11)
where f(x1, . . . , xn) is the density of the df F(x1, . . . , xn). This indicates,
extending thereby Step 2 above, that the data may be sequentially generated
by conditional dfs.
• (Inducing conditional distributions.) For measurable maps g,
P(Y ∈g−1(B)|X = x) = P(g(Y ) ∈B|X = x).
(8.12)
• (Conditioning in the case of independent random variables.) If X and Y are
independent, one can prove in a direct manner that the conditional df of
g(X, Y ) given X = x is the df of g(x, Y ). Thus we have
P(g(X, Y ) ≤z|X = x) = P{g(x, Y ) ≤z}.
(8.13)
As a special case one gets that the conditional df of Y is the unconditional
df, if X and Y are independent.

230
8. Conditional Extremal Analysis
• (Conditioning in the case of random vectors.) Assume that (Xi, Yi) are iid
random vectors und put X = (X1, . . . , Xn) and Y = (Y1, . . . , Yn). Then the
conditional df of Y given X = x is
P(Y ≤y|X = x) =
n

i=1
P(Yi ≤yi|Xi = xi).
(8.14)
To prove this result, verify that the right–hand side satisﬁes the required
property of the conditional distribution1.
The Concept of Conditional Independence
The random variables Y1, . . . , Yn are called conditionally independent given X, if
P(Y ≤y|X = x) =
n

i=1
P(Yi ≤yi|X = x).
(8.15)
Assume that the random vectors (X1, Y1), . . . , (Xn, Yn) are iid. According to
(8.14),
P(Y ≤y|X = x) =
n

i=1
P(Yi ≤yi|Xi = xi)
(8.16)
which implies that
P(Yi ≤yi|X = x) = P(Yi ≤yi|Xi = xi).
(8.17)
Therefore, the Yi are conditionally independent with conditional distributions
P(Yi ≤yi|X = x) = P(Yi ≤yi|Xi = xi).
Conditional Expectation, Conditional Variance,
Conditional q–Quantile
Let Y be a real–valued random variable with E|Y | < ∞and X any further random
variable. Denote again by F(y|x) the conditional df of Y given X = x. We recall
some well–known functional parameters of the conditional df.
• The mean
E(Y |X = x) =

y dF(·|x)(y)
(8.18)
of the conditional df is called the conditional expectation of Y given X = x.
We shortly write E(Y |x) in place of E(Y |X = x) when no confusion can
arise.
Notice that the conditional expectation E(Y |x) is the mean within the con-
ditional experiment described by Step 2 on page 228.
1Presently, we do not know an appropriate reference to the stochastical literature.

8.1. Interpretations and Technical Preparations
231
• We mention another conditional functional parameter, namely the condi-
tional variance V (Y |x) of Y given X = x which is the variance of the con-
ditional df. Thus, V (Y |x) is the variance in the conditional experiment. We
have
V (Y |x) :=

(y −E(Y |x))2 dF(·|x)(y).
(8.19)
The conditional variance can be represented by means of conditional expec-
tations. With the help of (8.12) one gets
V (Y |x) = E(Y 2|x) −E(Y |x)2.
(8.20)
• Later on we will also be interested in the conditional q–quantile
q(Y |x) := F(·|x)−1(q)
(8.21)
of Y given X = x as the q–quantile of the conditional df F(y|x).
In addition, E(Y |X) := g(X) is the conditional expectation of Y given X,
where g(x) = E(Y |x). Notice that E(Y |X) is again a random variable. Likewise,
one may speak of the conditional variance V (Y |X) and conditional q–quantile
q(Y |X) of Y given X.
The Linear Regression Function for a Random Design
We characterize the linear regression model for a ﬁxed design as the conditional
model in the corresponding random design problem. It is pointed out that the
linear regression function is a conditional expectation.
Later on, corresponding questions will be studied in the Sections 8.3, 9.5, 16.8
and Chapter 15, where we specify parametric statistical models in the conditional
setup.
First we recall some basic facts from Section 2.5 about linear regression for
a ﬁxed design. Assume that the random variables Yt have the representation
Yt = α + βxt + εt,
t = 1, . . . , n,
(8.22)
with the residuals εi being independent random variables with expectations Eεt =
0, and the xt are predetermined values. Recall that g(x) = α + βx is the linear
regression function with intercept α and slope β.
This may be equivalently formulated in the following manner: let the Yt be
independent random variables with expectations
EYt = α + βxt,
t = 1, . . . , n.
(8.23)
In the subsequent example, the xt are also random.
Example 8.1.1. (Exercises and Tests.) The following illustration is based on the per-
centages xt of solved exercises in a tutorial and the scores yt in a written test (data set

232
8. Conditional Extremal Analysis
Table 8.1. percentages xt of solved exercises and scores yt in test
xt 41.1
42
24.5 50 56.1 50.7 56.1 82.5 90.8 ...
yt
0
8.5
6
3
9
5
15
11.5
17
...
exam96.dat). The maximum attainable score is 20. Some of the data are displayed in
Table 8.1.
In Fig. 8.1 we plot the scores yt against the percentages xt and include the estimated
least squares line.
percentage of solved exercises
score in examination
50
100
0.0
10
20
Fig. 8.1. Plotting scores in examination against percentages of solved exercises and ﬁtted
least squares line.
For the (academic) teacher it is very satisfactory that solving exercises has a positive
eﬀect for students on passing the test.
Subsequently, the xt are regarded as outcomes of random variables Xt. This
leads to the random design problem.
We assume that (X1, Y1), . . . , (Xn, Yn) are iid copies of a random vector
(X, Y ) which has the representation
Y = α + βX + ε,
(8.24)
where X, ε are independent and Eε = 0. Because X and ε are independent, we
get from (8.13) that
P(Y ∈· |X = x) = L(α + βx + ε).
(8.25)
Thus, the Yt conditioned on Xt = xt are distributed as in the ﬁxed design problem.

8.1. Interpretations and Technical Preparations
233
For the conditional expectation we have
E(Y |x) = α + βx
(8.26)
which is again called linear regression function.
Put again X = (X1, . . . , Xn) and Y = (Y1, . . . , Yn). From (8.16) and (8.26)
we know that the Yt, conditioned on X = x, are conditionally independent with
conditional expectations
E(Yt|xt) = E(Yt|Xt = xt) = α + βxt.
(8.27)
These properties correspond to those in the ﬁxed design problem. This is the
reason why results for the ﬁxed design regression carry over to the random design
regression.
Estimation and prediction in the linear regression model for a random design
will be studied at the end of this section.
The Roundabout: Joint, Conditional, and Mixture Distributions
We want to explain the relationships between joint, conditional and mixture dis-
tributions in a “roundabout”:
joint df
mixture
conditional df

1PPPPPPP
q

We may
(i) start with a joint df F(x, y) of random variables X and Y ;
(ii) decompose the joint df in the marginal df F(x) of X and the conditional df
F(y|x) of Y given X = x, see (8.3);
(iii) regard the conditional df and, respectively, the marginal df of X as param-
eterized dfs F(y|ϑ) and mixing df F(ϑ), replacing the x by ϑ, and, ﬁnally,
return to (i).
We provide some details concerning the last step: let F(·|ϑ) = Fϑ be a family
of dfs parameterized by ϑ, and let F be a mixing df. Then, the mixture of the Fϑ
with respect to F is
G(y) =

F(y|ϑ) dF(ϑ) =

F(y|ϑ)f(ϑ) dϑ,
(8.28)

234
8. Conditional Extremal Analysis
where the latter equality holds for a density f of F, see (4.8). If the integration in
(8.28) is carried out from −∞to x, then one obtains a bivariate df F(ϑ, y) as in
(8.3). Therefore, one may introduce random variables X and Y with common df
F(ϑ, y). Replacing ϑ by x we are in the initial situation (i) with the mixing df as
df of X and the mixture df as df of Y .
These arguments may be helpful to understand, e.g., the Bayesian view to-
wards Bayesian statistics, see page 243.
Conditional Distributions of Exceedances and Order Statistics
Further insight in the statistical modeling of exceedances over a threshold u and
upper order statistics may be gained by conditioning on the random number K of
exceedances and, respectively, on the (k + 1)st upper order statistic Xn−k:n:
• the random exceedances over u conditioned on K = k are distributed as k
iid random variables with common df F [u] (see [43] and [16]),
• the upper order statistics Xn−k+1:n ≤· · · ≤Xn:n conditioned on Xn−k:n = u
are distributed as the order statistics Y1:k ≤· · · ≤Yk:k of k iid random
variables Yi with common df F [u] (see [42]).
Consequently, the statistical analysis of exceedances and upper order statis-
tics can be reduced to the standard model of iid random variables.
Analyzing Dependent Data, the Rosenblatt Transformation
All the exploratory tools, like QQ–plots or sample excess functions, concern iid
data. In the case of independent, not necessarily identically distributed (innid)
data one my use the probability transformation to produce iid, (0, 1)–uniform
data.
In the case of dependent random variables one has to apply the Rosenblatt2
transformation to achieve such a result.
Let (X1, . . . , Xd) be a random vector. Let
Fi (·|xi−1, . . . , x1)
be the conditional df of Xi given Xi−1 = xi−1, . . . , X1 = x1. Assume that
P

ω ∈Ω: Fi

· |Xi−1(ω), . . . , X1(ω)

continuous, i = 1, . . . , d

= 1.
Then,
F1(X1), F2(X2|X1), . . . , Fd(Xd|Xd−1, . . . , X1)
(8.29)
are iid, (0, 1)–uniformly distributed random variables.
2Rosenblatt, M. (1952). Remarks on a multivariate transformation. Ann. Math.
Statist. 23, 470–472.

8.1. Interpretations and Technical Preparations
235
The following short proof of the Rosenblatt transformation (8.29) in its gen-
eral form is due to Edgar Kaufmann (personal communication), and it is worth-
while to indicate the basic idea: the joint integral is decomposed to an iterated
integral (by a Fubini theorem) and the probability transformation is applied in
the inner integral. First we mention these auxiliary results.
(a) Probability Transformation: according to the probability transformation, see
page 38, we have

I

F(x) ∈B)

dF(x) = P{U ∈B}
for continuous dfs F where U is (0, 1)–uniformly distributed
(b) Fubini Theorem3: for random variables X, Y and non–negative, measurable
functions f,

f(x, y) dL(X, Y )(x, y) =
  
f(x, y) P(Y ∈dy|X = x)

dL(X)(x)
which is the Fubini theorem for conditional distributions. If X and Y are
independent, then the conditional distribution is the distribution of Y and
one gets the usual Fubini theorem.
To prove the Rosenblatt transformation (8.29) it suﬃces to verify that X =
(Xd−1, . . . , X1) and Fd(Xd|X) are independent and Fd(Xd|X) is (0, 1)–uniformly
distributed. For measurable sets A and B one gets
P{X ∈A, Fd(Xd|X) ∈B}
=

I

x ∈A

I

Fd(x|x) ∈B

dL(X, Xd)(x, x)
(b)
=

I

x ∈A
 
I

Fd(x|x) ∈B

F(dx|x)

dL(X)(x)
(a)
=
P{X ∈A}P{U ∈B}
where U is (0, 1)-uniformly distributed. The proof is concluded.
Predicting a Random Variable
In the following lines, we recall some basic facts about the concept of predicting
(forecasting) a random variable. The aim is the prediction of a future value y of a
random experiment based on an observable value g(x), where x is a realization of
another experiment, and the function g describes a ﬁxed rule according to which
the prediction is carried out.
3E.g., G¨anssler, P. and Stute, W. (1977). Wahrscheinlichkeitstheorie. Springer, Berlin,
Satz 1.8.13.

236
8. Conditional Extremal Analysis
Clearly, one should be interested in the performance of the predictor. For that
purpose, let Y and X be random variables which represent the two experiments
with outcomes y and x. We predict Y by means of g(X). It is desirable that
• the predictor g(X) is unbiased; that is, E(Y −g(X)) = 0,
• the remainder term in the prediction is small, e.g., measured by the mean
squared loss E

(Y −g(X))2
.
It is well known that the conditional expectation E(Y |X) is the best predictor
of Y based on X. Recall from page 230 that E(Y |X) = g(X), where g(x) =
E(Y |x).
If g depends on an unknown parameter—this will usually be the case—one
has to replace these parameters by estimators in the prediction procedure. We
provide an example in conjunction with the linear regression model.
Estimation and Prediction in the
Linear Regression Model for a Random Design
First we study once again the linear regression problem as speciﬁed in (8.23).
Based on x = (x1, . . . , xn) and Y = (Y1, . . . , Yn) one gets the following least
squares estimators
ˆβx,Y
=
n
t=1(Yt −¯Y )(xt −¯x)
n
i=1(xt −¯x)2
,
ˆαx,Y
=
¯Y −ˆβx,Y ¯x
of the intercept α and the slope β. Plugging in these estimators into the regression
function, one also gets
ˆgx,Y (x) = ˆαx,Y + ˆβx,Y x
as an estimator of the linear regression function g(x) = α + βx.
In the random design problem, see (8.27), estimate the intercept α and the
slope β by the unbiased least squares estimators
ˆβX,Y
=
n
i=1(Yt −¯Y )(Xt −¯X)
n
i=1(Xt −¯X)2
,
ˆαX,Y
=
¯Y −ˆβX,Y ¯X
(8.30)
to get the unbiased estimator
ˆgX,Y (x) = ˆαX,Y + ˆβX,Y x
(8.31)
of the linear regression function g(x) = α + βx = E(Y |x).
Based on x we want to predict the future value y. As mentioned before the
conditional expectation
E(Y |X) = α + βX

8.1. Interpretations and Technical Preparations
237
provides the optimal predictor of Y , yet the parameters α and β are unknown.
Plugging in the estimators one gets by means of ˆgx,y(x) in (8.31) a reasonable
prediction of y based on the observed x, x and y .
Example 8.1.2. (Continuation of Example 8.1.1.) We want predict the future score y
in the test based on the known percentage x of solved exercises in the tutorial. As a
prediction use the pertaining value of the estimated regression line in Fig. 8.1.
Later on we will also write Eα,β(Y |x) and Eα,β(Y |X) to indicate the depen-
dence of the conditional expectation on the parameters α and β.
The Notion of a Predictive Distribution
Occasionally, a conditional distribution is called predictive distribution, see, e.g.,
the book by Fan and Yao [18], page 454. One could extend this notion and generally
address a random distribution as a predictive distribution. We shall use the term
“predictive distribution” in a narrower sense.
The conditional distribution usually includes unknown parameters, which
have to be estimated, so that one can deduce a prediction of a random functional
parameter such as the conditional expectation. We shall refer to this random dis-
tribution with the unknown parameters replaced by estimates—the “estimated
random distribution”—as predictive distribution.
The posterior distribution in the Bayesian framework, cf. Section 8.4, may
also be addressed as predictive distribution, if the prior distribution does not
depend on superparameters. A predictive distribution may also depend on the
model choice or certain approximations, cf. Section 16.8.
Example 8.1.3. (Continuation of Example 8.1.) In the linear regression set–up, where
Y = α + βX + ε with X, ε being independent, we get as predictive distribution in
the narrow sense (unknown parameters are replaced by the least squares estimators) by
distributions of the form
L(ˆαx,y + ˆβx,yx + ε).
By the way, this predictive distribution is the conditional distribution of
ˆαX,Y + ˆβX,Y X + ε
given
X = x, X = x, Y = y,
if the random vector (X, X, Y ) and the innovation ε are independent.
Notice that (X, X, Y ) represents the past which can be observed by the statistician
or hydrologist, etc. Therefore, the predictive distribution is observable, if, ε is speciﬁed,
e.g., as a standard normal random variable.
On page 236 the performance of a predictor g(X) of Y was measured by
the mean squared loss. A related measurement is possible within the framework of

238
8. Conditional Extremal Analysis
predictive distributions. The role of Y and g(X) is played by a random distribution
K(·|X) and a predictive distribution K∗(·|X, Z), where Z represents the additional
available information. Now, the performance is measured by the expected loss
E

d

K(·|X), K∗(·|X, Z)

,
where d is an appropriate distance.
Densities of predictive distributions are called predictive densities. Diebold et
al.4 address the actual conditional densities in (8.11) as “data generating process”,
and the pertaining predictive densities as a sequence of density forecasts (in accor-
dance with our terminology). Likewise one can speak of a sequence of predictive
distributions.
8.2
Conditional Extremes:
a Nonparametric Approach
One of the primary aims of extreme value analysis is to evaluate a higher quantile
(such as a T –year threshold) of a random variable Y . In the present section we
discuss such questions under the condition that the value x of a covariate X is
already known. Thus, we want to estimate, for example, a higher quantile of the
conditional df F(·|x) of Y given X = x (or some other parameter of the upper tail
of the conditional df).
The message from this section is that one may deal with conditional extremes
as if we had ordinary extremes from the conditional df. For that purpose we give
an outline of some technical results from the book [16], entitled Laws of Small
Numbers.
Let (xi, yi) be realizations of the random vector (X, Y ). The basic idea is
to base the estimation of the conditional df on those yi which belong to the xi
close to the speciﬁed value x. The maximum, exceedances over a threshold u, or
the k largest order statistics belonging to the selected yi may be addressed as
conditional extremes (conditional sample maximum, conditional exceedances or
conditional order statistics).
There are two diﬀerent possibilities of selecting the xi.
• (Nearest Neighbor Approach.) Take those xi which are the k closest neigh-
bors to x.
• (Fixed Bandwidth Approach.) Select those xi which are closer to x than a
predetermined bandwidth b.
It is understood that distances are measured with respect to the Euclidian dis-
tance, yet the subsequent results are valid in greater generality. For applications
in hydrology and insurance we refer to the Sections 14.2 and 17.3.
4Diebold, F.X., Gunther, T.A. and Tay, A.S. (1998). Evaluating density forecasts with appli-
cations to ﬁnancial risk management. Int. Economic Review 39, 863–883.

8.2. Conditional Extremes: a Nonparametric Approach
239
The Nearest Neighbor Approach
Let (X1, Y1), . . . , (Xn, Yn) be iid random vectors with common df F(x, y) and
let (x1, y1), . . . , (xn, yn) be realizations. Let y′
1, . . . , y′
k be the yi for which the
pertaining xi are the k closest neighbors to x. The y′
i are taken in the original
order of their outcome. Denote by Y
′
1, . . . , Y
′
k the pertaining random variables.
The Y
′
i can be replaced by iid random variables Y ∗
1 , . . . , Y ∗
k with common df
F(·|x), if
k = o

n4/5
(8.32)
and, if a certain technical condition holds for the density of (X, Y ) (for details we
refer to [16], Theorem 3.5.2, where the replacement is formulated in terms of the
Hellinger distance).
As a consequence we know that functionals like max(Y
′
1 , . . . , Y
′
k) can be re-
placed by max(Y ∗
1 , . . . , Y ∗
k ). Now one may construct statistical models etc. as in
the unconditional case.
Likewise, the exceedances of the Y
′
1, . . . , Y
′
k over a threshold u can be replaced
by the exceedances of the Y ∗
1 , . . . , Y ∗
k over u.
A result similar to that for the maxima was deduced by Gangopadhyay5
under a condition k = o

n2/3
which is stronger than (8.32). On the other hand,
Gangopadhyay merely requires a local condition on the density of (X, Y ) compared
to the overall condition imposed in [16], Theorem 3.5.2.
The Fixed Bandwidth Approach
Assume again that there are iid random vectors (X1, Y1), . . . , (Xn, Yn) with com-
mon df F(x, y). Let y′
1, . . . , y′
k be the yi such that |xi −x| ≤b, where b is pre-
determined. The y′
i are taken in the original order of their outcome. Denote by
Y
′
1, . . . , Y
′
K(n) the pertaining random variables, where K(n) is independent of the
Y
′
1, Y
′
2, Y
′
3 , . . . .
The Y
′
i can be replaced by random variables Y ∗
1 , . . . , Y ∗
K∗(n), where
• Y ∗
1 , Y ∗
2 , Y ∗
3 , . . . is a sequence of iid random variables with common df F(·|x),
• K∗(n) is a binomial random variable which is independent of the Y ∗
i ,
if
b = o

n−1/5
(8.33)
and, if a certain technical condition holds for the density of (X, Y ) (cf. [16], Corol-
lary 3.1.6, where the replacement is formulated in terms of the Hellinger distance).
5Gangopadhyay, A.K. (1995). A note on the asymptotic behavior of conditional ex-
tremes. Statist. Probab. Letters 25, 163–170.

240
8. Conditional Extremal Analysis
As a consequence we know that functionals like max(Y
′
1 , . . . , Y
′
K(n)) can be
replaced by max(Y ∗
1 , . . . , Y ∗
K∗(n)). Now one may construct statistical models etc.
as in the unconditional case.
One may also deal with exceedances over a higher threshold. In addition, the
binomial random variable K∗(n) can be replaced by a Poisson random variable
with the same expectation as K∗(n). Then, one arrives at a Poisson process as
introduced in Section 9.1.
8.3
Maxima Under Covariate Information
Contrary to the preceding section, we assume that the conditional distributions
belong to a parametric family. In addition, we do not necessarily assume a linear
regression framework as in Section 8.1. For example, we may assume, correspond-
ing to (8.24), that
Y = µ(X) + σ(X)ε
where ε is a standard normal random innovation which is independent of a covari-
ate (explanatory variable) X. This yields that the conditional df of Y given X = x
is normal with location and scale parameters µ(x) and σ(x). Thus, we have
F(y|x) = P(Y ≤y|X = x) = Φµ(x),σ(x)(y).
Such a modeling could be appropriate in Example 8.1.1.
Subsequently, this idea is further developed within the framework of EV
models in the present section and of GP models in Section 9.5. The use of such
models was initiated in the celebrated article by Davison and Smith (cited on page
121).
A Conditional Extreme Value Model
Assume that the conditional df of Y given X = x is an extreme value df, namely
F(y|x) = Gγ(x),µ(x),σ(x)(y)
(8.34)
where the shape, location and scale parameters γ(x) µ(x) and σ(x) depend on x.
To reduce the number of parameters we may assume that γ(x) does not
depend on x, and
µ(x)
=
µ0 + µ1x,
σ(x)
=
exp(σ0 + σ1x).
(8.35)
Therefore, we have to deal with the unknown parameters γ, µ0, µ1, σ0 and
σ1.

8.3. Maxima Under Covariate Information
241
Conditional Maximum Likelihood Estimation
Let again X = (X1, . . . , Xn) and Y = (Y1, . . . , Yn). We assume that the Yt are
conditionally independent conditioned on X with P(Yt ≤y|X = x) = P(Yt ≤
y|Xt = xt), see page 230.
The estimation of the unknown parameters γ, µ0, µ1, σ0 and σ1 may be
carried out by means of a conditional maximum likelihood (ML) method. The
conditional likelihood function is given by
L(γ, µ0, µ1, σ0, σ1) =
n

t=1
gγ,µ(xt),σ(xt)(yt).
Based on the conditional MLE’s ˆγx,y, ˆµ0,x,y, ˆµ1,x,y, ˆσ0,x,y, ˆσ1,x,y of the
parameters γ, µ0, µ1, σ0, σ1 one also gets the estimates
ˆµx,y(x)
=
ˆµ0,x,y + ˆµ1,x,yx
ˆσx,y(x)
=
exp(ˆσ0,x,y + ˆσ1,x,yx)
of µ(x) and σ(x). Now, replace the parameters in the EV df Gγ,µ(x),σ(x) by the
estimated parameters to get an estimate of the conditional df P(Y ≤y|X = x).
For related MLEs in the pot–framework we refer to Section 9.5.
Estimating Conditional Functional Parameters
In the subsequent lines we want to distinguish in a strict manner between es-
timation and prediction procedures. On the one hand, we estimate parameters,
which may be real–valued or functions, and on the other hand, we predict random
variables.
In that context, we merely consider the mean and the q–quantile as con-
ditional functional parameters. Another important parameter is, of course, the
variance.
Conditional Mean Function (Expectation):
Eγ,µ,σ(Y |x) =

zGγ,µ(x),σ(x)(dz).
(8.36)
Conditional q–Quantile Function:
qγ,µ,σ(Y |x) = G−1
γ,µ(x),σ(x)(q).
(8.37)
If the conditioning is based on a covariate we also speak of a covariate condi-
tional quantile or a covariate conditional expectation. Likewise within a time series
framework, one may speak of a serial conditional quantile or a serial conditional
expectation. It is apparent that the conditioning may also be based on covariate
as well as serial information (a case not treated in this book).

242
8. Conditional Extremal Analysis
Estimate the conditional functions by replacing the unknown parameters by
the conditional MLE’s based on x and y. For example,
qˆγx,y,ˆµx,y,ˆσx,y(Y |x)
is an estimate of the conditional q–quantile qγ,µ,σ(Y |x) for each x.
Predicting the Conditional q–Quantile
In place of the factorization of the conditional expectation Eγ,µ,σ(Y |x) of Y given
X = x consider the conditional expectation Eγ,µ,σ(Y |X) of Y given X.
Recall that the conditional expectation is the best prediction of Y based on
X with respect to the quadradic loss function. In the present context, we have to
replace the unknown parameters by estimates. By
EˆγX,Y ,ˆµX,Y ,ˆσX,Y (Y |X)
one gets a predictor of the random variables Eγ,µ,σ(Y |X) and Y based on X, Y
and X.
Likewise,
qˆγX,Y ,ˆµX,Y ,ˆσX,Y (Y |X)
(8.38)
is a predictor of the conditional q–quantile qγ,µ,σ(Y |X) based on X, Y and X.
In the same manner, predictions in the pot–framework, in conjunction with
Poisson processes of exceedances, are addressed in Section 9.5.
8.4
The Bayesian Estimation Principle, Revisited
The Bayesian estimation principle—cf. Sections 3.5 and 5.1—will be formulated
in a greater generality.
An Interpretation of the Posterior Density
In Section 3.5 we started with
• the prior density p(ϑ) as a mixing density, and
• densities L(·|ϑ) of distributions parameterized by ϑ and expressed by the
likelihood function.
Using these densities we may introduce the joint density L(x|ϑ)p(ϑ) of a random
vector (ξ, θ), see (8.8).
According to (8.8) and (8.10) one may interchange the role of ξ and θ. From
the joint distribution of (ξ, θ) one may deduce the conditional density of θ given
ξ = x which leads to the posterior density in (3.39) as well as in (8.41) below.

8.4. The Bayesian Estimation Principle, Revisited
243
The Bayesian Two–Step Experiment
The prior distribution for the parameter ϑ is regarded as the distribution of a ran-
dom variable θ with outcome ϑ. This random parameter θ describes the initial step
in a two–step experiment. The distributions, represented by ϑ, in the given statis-
tical model are regarded as conditional distributions of another random variable
ξ given θ = ϑ, cf. page 233 for the “roundabout” description.
Two special cases were dealt with in Section 3.5. In both cases a gamma
distribution with parameters s and d served as a prior distribution.
• P(ξ ∈·|θ = ϑ) is the common distribution of iid random variables X1, . . . , Xk
with df Fϑ and density fϑ, and likelihood function L(x|ϑ) = 
i≤k fϑ(xi).
• The Xi are iid Poisson random variables with parameter λ and likelihood
function L(x|λ) = 
i≤k Pλ{xi}.
• Both previous cases are combined in the present section within a Poisson
process setting with a likelihood function given in a product form.
Because the prior distribution and the family of conditional distributions are
speciﬁed by the statistician, the joint distribution L(ξ, θ) of ξ and θ is also known,
cf. (8.10). The joint density is L(x|ϑ)p(ϑ), where p(ϑ) is a density of the prior
distribution and L(x|ϑ) is the likelihood function for the given statistical model.
If a statistical model of prior distributions—instead of a ﬁxed one—is speci-
ﬁed by the statistician, then the present viewpoint also enables the estimation of
the prior distribution from data.
In the preceding lines we reformulated and extended a technical problem
discussed in Chapter 3 within a two–step stochastical procedure.
• In the initial stage there is a stochastic experiment governed by the prior
distribution L(θ) which generates an unobservable outcome, namely the pa-
rameter ϑ.
• Afterwards, an observation x is generated under ϑ. With respect to the total
two–step experiment, the value x is governed by L(ξ).
Although θ is unobservable, one gets knowledge of this random parameter
in an indirect manner, namely, by means of x. The information contained in x
is added to the initial information which is represented by the prior distribution
L(θ). As a result, one gets the updated information expressed by the posterior
distribution P(θ ∈·|ξ = x). This illuminates the importance of the posterior
distribution in its own right.
Computing Bayesian Estimates
We compute the Bayes estimate within a general framework. Let again L(θ) and
p(ϑ) denote the prior distribution and prior density, respectively. We repeat some
of the computations in Section 3.1 concerning the Bayes estimate.

244
8. Conditional Extremal Analysis
The MSE of an estimator T(X) of the functional parameter T (ϑ) can be
written as an integral with respect to the conditional distribution of ξ given θ = ϑ.
We have
E
 T(X) −T (ϑ)
2|ϑ

=

( T (x) −T (ϑ))2P(ξ ∈dx|θ = ϑ).
Therefore, the Bayes risk with respect to a prior distribution L(θ) can be written
as
R(p, T)
=
 
( T(x) −T (ϑ))2P(ξ ∈dx|θ = ϑ)

L(θ)(dϑ)
=

( T(x) −T (ϑ))2L(θ, ξ) (dϑdx)
=
 
( T(x) −T (ϑ))2P(θ ∈dϑ|ξ = x)

L(ξ)(dx) .
(8.39)
Now proceed as in (3.41) to get the Bayes estimate
T ∗(x) =

T (ϑ)P(θ ∈dϑ|ξ = x).
(8.40)
Thus, we get a representation of the Bayes estimate by means of the posterior
distribution P(θ ∈·|ξ = x). If ϑ is one–dimensional and T (ϑ) = ϑ, then T ∗=
E(θ|ξ = x) is the conditional expectation of θ given ξ = x.
In fact, (8.40) is an extension of (3.38). To see this, one must compute the
conditional density p(ϑ|x) of θ given ξ = x. Because L(x|ϑ)p(ϑ) is the joint density
of ξ and θ one obtains
p(ϑ|x) =
L(x|ϑ)p(ϑ)

L(x|ϑ)L(θ) (dϑ) ,
(8.41)
which is the posterior density given x (see also Section 8.1, where the “roundabout”
of joint, conditional and mixture dfs is discussed in detail).
Bayesian Estimation and Prediction
From (8.40) one recognizes that the Bayes estimate can be written as the condi-
tional expectation T ∗(x) = E(T (θ)|ξ = x). Moreover, the characteristic property
of the Bayes estimate of minimizing the Bayes risk can be reformulated (cf. second
line in (8.39)) as
E

(T ∗(ξ) −T (θ))2
= inf
T
E

( T(ξ) −T (θ))2
,
where the inf ranges over all estimators T. Of course, this is a well–known property
of conditional expectations.

8.4. The Bayesian Estimation Principle, Revisited
245
The property that T ∗is the Bayes estimator for the functional T can be
rephrased by saying that T ∗(ξ) = E(T (θ)|ξ) is the best predictor of the random
variable T (θ). This approach also allows the introduction of a linear Bayes esti-
mator by taking the best linear predictor of T (θ).
The conditional (posterior) distribution of T (θ) given ξ = x may be addressed
as predictive distribution (cf. Section 8.1, page 237) in the Bayesian framework.
This predictive distribution is known to the statistician as long as the prior dis-
tribution does not include superparameters, cf. Section 14.5.
Further Predictive Distributions in the Bayesian Framework
Let (ξ, θ) be a random vector where θ is distributed according to the prior density
p(ϑ). Outcomes of the random variables ξ and θ are x and the parameter ϑ. The
conditional density of ξ given θ = ϑ is p(x|ϑ). From (8.41) we know that the
posterior density (the conditional density of θ is given ξ = x) is given by
p(ϑ|x) = p(x|ϑ)p(ϑ)
 
p(x|ϑ)p(ϑ) dϑ .
(8.42)
Predictive distributions may be dealt with in the following extended frame-
work (as described in the book by Aitchison and Dunsmore6): Consider the random
vector (ξ, η, θ) where θ has the (prior) density p(ϑ), and ξ and η are conditional
independent (conditioned on θ = ϑ) with conditional densities p(x|ϑ) and p(y|ϑ).
Thus, the conditional density of (ξ, η) given θ = ϑ is p(x, y|ϑ) = p(x|ϑ)p(y|ϑ).
The predictive distribution is the conditional distribution of η given ξ = x
which has the density
p(y|x) =

p(y|ϑ)p(ϑ|x) dϑ ,
(8.43)
where p(ϑ|x) is the posterior density in (8.42). To establish (8.43) we make use
of the representation p(y|x) = p(x, y)/p(x) of the conditional density of η given
ξ = x, where p(x, y) and p(x) are the densities of (ξ, η) and ξ. We have
p(y|x)
=
p(x, y)/p(x)
=

p(x, y|ϑ)p(ϑ) dϑ

p(x)
=

p(x|ϑ)p(y|ϑ)p(ϑ) dϑ
 
p(x|ϑ)p(ϑ) dϑ
=

p(y|ϑ)p(ϑ|x) dϑ .
If p(ϑ) is a conjugate prior for both conditional densities p(x|ϑ) and p(y|ϑ),
then the last integrand is proportional to a density which is of the same type as
the prior p(ϑ) and the integrand can be analytically computed.
6Aitchison, J. and Dunsmore, I.R. (1975). Statistical Prediction Analysis. Cambridge
University Press, Cambridge.

246
8. Conditional Extremal Analysis
Example 8.4.1. (Predictive distributions for the restricted Pareto model.) We assume
that the statistical models related to the random variables ξ and η are both restricted
Pareto models with thresholds u and v. Let
p(x|α) =

i≤k
w1,α,0,u(xi)
and
p(y|α) = w1,α,0,v(y) .
As in Section 5.1 take the gamma density p(α) = hs,d(α) as a prior for the shape param-
eter α. Then the gamma density p(α|x) = hs′,d′(α) is the posterior with respect to the
p(x|α) where s′ = s + k and d′ = d + 
i≤k log(xi/u).
For computing the predictive density p(y|x) put s∗= s′ + 1 and d∗= d′ + log(y/v).
Writing the integrand as a gamma density we get
p(y|x)
=
 ∞
0
w1,α,0,v(y)hs′,d′(α) dα
(8.44)
=
d′s′Γ(s′)−1y−1
 ∞
0
αs∗−1 exp(−d∗α) dα
=
d′s′d∗−s∗
s′y−1
=
(s + k)

d +

i≤k
log xi
u
s+k
y−1
d +

i≤k
log xi
u + log y
v
−(s+k+1)
for y > v.
We note an extension where η is also a vector of iid Paretian random variables. If
p(y|α) is replaced by
p(y|α) =

i≤m
w1,α,0,v(yi) ,
then the predictive density is
p(y|x) = d′s′d∗−s∗ 
j≤m
yj
−1 
j≤m
(s′ + j −1)
with s∗= s′ + m and d∗= d′ + 
j≤m log(yj/v) for yj > v.
It would be desirable to give these ideas more scope within the extreme value
setting.

Chapter 9
Statistical Models for
Exceedance Processes
In this chapter, Poisson processes and related processes are studied. These pro-
cesses are essential for hydrological, environmental, ﬁnancial and actuarial studies
in Chapters 14 to 17. In Section 9.1 the basic concepts are introduced. We par-
ticularly mention the modeling of exceedances and exceedance times by means of
Poisson processes. Within the framework of Poisson processes, we reconsider the
concept of a T –year level in Section 9.2. The maximum likelihood and Bayesian
estimation within models of Poisson processes is addressed in Section 9.3. The ex-
planations about the GP approximation of exceedance dfs, cf. Section 6.5, will be
continued within the framework of binomial and Poisson processes in Section 9.4.
An extension of the modeling by Poisson processes from the homogeneous case to
the inhomogeneous one is investigated in Section 9.5.
9.1
Modeling Exceedances by Poisson Processes:
the Homogeneous Case
In this section, we deal with observations occurring at random times. Especially,
we have exceedance times modeled by Poisson processes. Let Ti denote the arrival
time of the ith random observation Xi, where necessarily 0 ≤T1 ≤T2 ≤T3 ≤· · · .
The Xi will be addressed as marks.
Homogeneous Poisson and Poisson(λ, F) Processes
The most prominent examples of arrival processes are homogeneous Poisson pro-
cesses with intensity λ, where the interarrival times T1, T2 −T1, T3 −T2, . . . are iid

248
9. Statistical Models for Exceedance Processes
random variables with common exponential df
F(x) = 1 −e−λx,
x ≥0,
with mean 1/λ. Under this condition, the arrival time Ti is a gamma random vari-
able. The numbers of observations occurring up to time t constitute the counting
process
N(t) =
∞

i=1
I(Ti ≤t),
t ≥0.
(9.1)
The arrival times T1 ≤T2 ≤T3 ≤· · · as well as the counting process N(t)
are addressed as a homogeneous Poisson process with intensity λ (in short, as a
Poisson(λ) process). Deduce from (4.7) that
P{N(t) = k}
=
P{Tk ≤t, Tk+1 > t}
=
P{Tk ≤t} −P{Tk+1 ≤t}
=
(λt)k
k!
e−λt.
Hence, the number N(t) of arrival times is a Poisson random variable with param-
eter λt. The mean value function—describing the mean number of observations up
to time t—is
Ψλ(t) = E(N(t)) = λt.
In addition, it is well known that the homogeneous Poisson process has in-
dependent and stationary increments, that is,
N(t1), N(t2) −N(t1), . . . , N(tn) −N(tn−1),
t1 < t2 < · · · < tn,
are independent, and N(tj) −N(tj−1) has the same distribution as N(tj −tj−1).
We see that the mean number of observations occurring within a time unit is
E

N(t + 1) −N(t)

= E(N(1)) = λ
which gives a convincing interpretation of the intensity λ of a homogeneous Poisson
process.
The sequence {(Ti, Xi)} of arrival times and marks is a Poisson(λ, F) process
if the following conditions are satisﬁed.
Poisson(λ, F) Conditions:
(a) the sequence of arrival times 0 ≤T1 ≤T2 ≤T3 ≤· · · is a Poisson(λ)
process;
(b) the marks X1, X2, X3, . . . are iid random variables with common df F,
(c) the sequences T1, T2, T3, . . . and X1, X2, X3, . . . are independent.

9.1. Modeling Exceedances by Poisson Processes: the Homogeneous Case
249
A justiﬁcation of such a process in ﬂood frequency studies with exponential
exceedances was given by Todorovic and Zelenhasic1. To capture the seasonality of
ﬂood discharges, an extension of the present framework to inhomogeneous Poisson
processes with time–dependent marks is desirable. Yet, in order not to overload
this section, the introduction of such processes is postponed until Section 9.5.
Poisson Approximation of Exceedances and Exceedance Times
In Section 1.2, the exceedance times τ1 ≤τ2 ≤τ3 ≤· · · of iid random variables Yi
over a threshold u were described in detail. Moreover, if F is the common df of the
Yi, then we know that the exceedances are distributed according to the exceedance
df F [u]. Exceedance times and exceedances are now regarded as arrival times and
marks. If 1−F(u) is suﬃciently small, then an adequate description of exceedance
times and exceedances is possible by means of a Poisson

1 −F(u), F [u]
process.
For further details see Section 9.5 and the monographs [44] and [43].
Exceedances for Poisson(λ, F) Processes
Now we go one step further and deal with exceedances and exceedance times
for a Poisson(λ, F) process of arrival times Ti and marks Xi. If only the ex-
ceedances of the Xi over a threshold v and the pertaining times are registered, then
one can prove that exceedance times and exceedances constitute a Poisson

λ(1 −
F(v)), F [v]
process. We refer to Section 9.5 for a generalization of this result.
Mixed Poisson Processes, P´olya–Lundberg Processes
We mention another class of arrival processes, namely mixed Poisson processes.
Our attention is focused on P´olya–Lundberg processes, where the mixing of ho-
mogeneous Poisson processes is done with respect to a gamma distribution. The
marginal random variables N(t) of a P´olya–Lundberg process are negative bino-
mial random variables.
Let us write N(t, λ), t ≥0, for a homogeneous Poisson (counting) process
with intensity λ > 0. By mixing such processes over the parameter λ with respect
to some density f, one obtains a mixed Poisson process, say, N(t), t ≥0. Recollect
that a mixed Poisson process represents the following two–step experiment: ﬁrstly,
a parameter λ is drawn according to the distribution represented by the density f
and, secondly, a path is drawn according to the homogeneous Poisson process with
intensity λ. The marginals N(t) are mixed Poisson random variables. We have
P{N(t) = n} =

Pλt{n}f(λ) dλ.
1Todorovic, P. and Zelenhasic, E. (1970). A stochastic model for ﬂood analysis. Water
Resour. Res. 6, 1641–1648.

250
9. Statistical Models for Exceedance Processes
In the special case where f = fα,σ is a gamma density with shape and scale
parameters α, σ > 0, one obtains a P´olya–Lundberg process with parameters α
and σ. Deduce from (3.31) that N(t) is a negative binomial random variable with
parameters α and p = 1/(1 + σt). In addition,
Ψα,σ(t) = E(N(t)) = ασt,
t ≥0,
(9.2)
is the increasing mean value function of a P´olya–Lundberg process with parameters
α and σ.
Arrival Times for Mixed Poisson Processes
Let N(t), t ≥0 be a counting process such as a mixed Poisson process. The ith
arrival time can be written
Ti = inf{t > 0 : N(t) ≥i},
i = 1, 2, 3, . . . .
The df of the ﬁrst arrival time T1 has the representation
P{T1 ≤t} = 1 −P{N(t) = 0}.
(9.3)
From (9.3) and (2.24) deduce
E(T1) =
 ∞
0
P{N(t) = 0} dt.
In the special case of a P´olya–Lundberg process with parameters α, σ > 0,
one must deal with negative binomial random variables N(t) for which
P{N(t) = 0} = (1 + σt)−α.
(9.4)
Hence, the ﬁrst arrival time T1 is a Pareto random variable with shape, location
and scale parameters α, −1/σ and 1/σ.
9.2
Mean and Median T –Year Return Levels
Recall that the T –year return level u(T ) of a sequence of random variables is that
threshold u such that the ﬁrst exceedance time τ1 = τ1,u at u is equal to T in the
mean, that is, u(T ) is the solution to the equation E(τ1,u) = T .
In Section 1.2, it was veriﬁed that the ﬁrst exceedance time of iid random
variables at a threshold u is a geometric random variable with parameter p =
1 −F(u) which yields u(T ) = F −1(1 −1/T ). In order to estimate u(T ), the
unknown df F is usually replaced by an estimated EV or GP df.

9.2. Mean and Median T –Year Return Levels
251
The T–Year Return Level for Heterogeneous Variables
Next, we assume that X1, X2, X3, . . . are independent, not necessarily identically
distributed random variables. Let Fi denote the df of Xi. The probability that the
ﬁrst exceedance time τ1 at the threshold u is equal to k is
P{τ1 = k} = (1 −Fk(u))

i≤k−1
Fi(u),
k = 1, 2, 3, . . . ,
according to the ﬁrst identity in (1.17). In the heterogeneous case, it may happen
that, with a positive probability, the threshold u is never exceeded. In this instance,
τ1 is put equal to ∞.
To evaluate the T –year return level u(T ), one must solve E(τ1,u) = T as an
equation in u, which can be written
∞

k=1
k(1 −Fk(u))

i≤k−1
Fi(u) = T.
(9.5)
This equation must be solved numerically by a Newton iteration procedure. We
remark that u(T ) ≥F −1(1−1/T ), if Fi ≤F. If E(τ1,u) = ∞, this approach is not
applicable.
The Median T–Year Return Level
One may consider a diﬀerent functional parameter of the ﬁrst exceedance time τ1,u
to ﬁx a T –year return level. To exemplify this idea, we compute a median of τ1,u
and determine the median T –year return level u(1/2, T ). The median of τ1,u is
med(τ1,u) = min
 
m :

k≤m
p(1 −p)k−1 ≥1/2
!
with p = 1 −F(u). Because 
k≤m zk−1 = (1 −zm)/(1 −z), one obtains
med(τ1,u) = ⟨log(1/2)/ log(F(u))⟩,
(9.6)
where ⟨x⟩is the smallest integer ≥x. The median T –year return level u(1/2, T )
is the solution to the equation med(τ1,u) = T. Approximately, one gets
u(1/2, T ) ≈F −1
2−1/T 
≈F −1(1 −(log 2)/T ).
(9.7)
Because log(2) = 0.693..., the median T –year return level is slightly larger than
the mean T –year return level in the iid case.
By generalizing this concept to q–quantiles of the ﬁrst exceedance time τ1,u
one obtains a T –year return level u(q, T ) according to the q–quantile criterion (see
also page 430).

252
9. Statistical Models for Exceedance Processes
Poisson Processes and T–Year Return Levels
When exceedances over u are modeled by a Poisson

1 −F(u), F [u]
process (cf.
page 249), then the ﬁrst exceedance time is an exponential random variable with
expectation 1/(1 −F(u)). Therefore, the expectation and the median of the ﬁrst
exceedance time are equal to a predetermined time span T for the thresholds
u = u(T ) = F −1(1 −1/T )
and
u = u(1/2, T ) = F −1(1 −(log 2)/T ).
Thus, when employing a Poisson approximation of the exceedances pertaining to
random variables Xi as in Section 1.2, one ﬁnds the same mean T –year return
level and, approximately, the median T –year return level given in (9.7).
If we already start with a Poisson(λ, F) process, then the mean and median
T –year return level are
u(T ) = F −1(1 −1/(λT ))
(9.8)
and
u(1/2, T ) = F −1(1 −(log 2)/(λT )).
(9.9)
This can be veriﬁed in the following manner: As mentioned in Section 9.1, the
marks exceeding v and the pertaining times constitute a Poisson

λ(1−F(v)), F [v]
process and, hence, the ﬁrst exceedance time is an exponential random variable
with mean 1/(λ(1 −F(v))). This implies the desired result.
This suggests the following approach for estimating the T –year return level.
Firstly, utilize a Poisson(λ, W) modeling for a suﬃciently large number of ex-
ceedances (over a moderately high threshold), where W is a GP df. Secondly,
estimate the intensity by ˆλ = N(t)/t and the GP df by )
W = Wγk,µk,σk. Then,
ˆu(T ) = )
W −1(1 −1/(ˆλT ))
(9.10)
and
ˆu(1/2, T ) = )
W −1(1 −(log 2)/(ˆλT ))
(9.11)
are estimates of the mean and median T –year return level.
In the case of clustered data, proceed in a similar manner. The T –year return
level of clustered data and of the pertaining cluster maxima (cf. page 78) are close
to each other and, therefore, one may reduce the analysis to the cluster maxima.
9.3
ML and Bayesian Estimation in Models
of Poisson Processes
In this section we study maximum likelihood (ML) and Bayes estimators for cer-
tain Poisson processes which provide a joint model for exceedance times and ex-

9.3. ML and Bayesian Estimation in Models of Poisson Processes
253
ceedances. The primary aim is to show that the estimators of Pareto and general-
ized Pareto (GP) parameters, as dealt with in Section 5.1, correspond to those in
the Poisson process setting.
Models of Poisson Processes
We observe a random scenery up to time T . In applications this usually concerns
the past T periods. For each parameter λ > 0 and each df F, let Poisson(λ, F, T )
denote the Poisson(λ, F) process—introduced in Section 9.1—restricted to the
time interval from 0 up to time T . Thus, there is a homogeneous Poisson process
with intensity λ in the time scale with marks which have the common df F.
Apparently, the number N(T ) of exceedances up to time T is a Poisson random
variable with parameter λT . Assume that F = Fϑ is a df with density fϑ. Thus,
the Poisson(λ, Fϑ, T ) process is represented by the parameter vector (λ, ϑ).
In the present context the outcome of such a Poisson process is a sequence of
pairs (ti, yi), for i = 1, . . . , k, consisting of exceedance times ti and the pertaining
exceedances yi over a threshold u. Notice that k is the outcome of the Poisson
random variable N(T ) with parameter λT . If k = 0, then there are no observations,
which happens with a positive probability.
The Likelihood Function for Poisson Process Models
By specifying a likelihood function, the ML and Bayesian estimation principles
become applicable. It suﬃces to determine a likelihood function up to a constant
to compute the ML and Bayes estimates.
One may prove (cf. [43], Theorem 3.1.1) that for every sample {(ti, yi)} the
likelihood function L({(ti, yi)}|λ, ϑ) satisﬁes
L({(ti, yi)}|λ, ϑ) ∝L1(k|λ)L2(y|ϑ)
(9.12)
(“∝” again denotes that both sides are proportional), where
• L1(k|λ) =

(λT )k/k!

exp(−λT ) is the likelihood function for the Poisson
model as given in (3.50),
• L2(y|ϑ) = 
i≤k fϑ(yi) is the likelihood function for the model of k iid ran-
dom variables with common density fϑ as dealt with, e.g., in (3.36).
One recognizes that likelihood–based estimators of λ and ϑ only depend on the
data by means of k and, respectively, y = (y1, . . . , yk).
Maximum Likelihood Estimators for Poisson Processes
with Generalized Pareto Marks
To get the MLE one must ﬁnd the parameters λ and ϑ which maximize the likeli-
hood function in (9.12). Due to the speciﬁc structure of the likelihood function in

254
9. Statistical Models for Exceedance Processes
(9.12), it is apparent that the ML procedure for Poisson processes splits up into
those which were separately dealt with in the Sections 3.1 and 3.4.
Because the ﬁrst factor L1(k|λ) is independent of ϑ it can be easily veriﬁed
that the MLE of λ is equal to
ˆλk = k/T.
(9.13)
We distinguish two diﬀerent models with respect to the parameter ϑ.
• (The Restricted Pareto Model GP1(u, µ = 0).) Let ϑ = α be the unknown
shape parameter in the restricted model GP1(u, µ = 0) of Pareto distribu-
tions (cf. page 116). Then, (ˆλk, ˆαk(y)) is the MLE in the present Poisson
process model, where ˆαk(y) denotes the Hill estimate (cf. (5.1); also see [43],
pages 142–143).
• (The Generalized Pareto Model GP(u).) Let ϑ = (γ, σ) be the parame-
ter vector in the generalized Pareto model as described on page 134. The
MLE in the pertaining Poisson process model is (ˆλk, ˆγk(y), ˆσk(y)), where
(ˆγk(y), ˆσk(y)) is the MLE in the GP(u) model of iid random variables of a
sample of size k.
Bayes Estimators for Poisson Processes with Pareto Marks
Next, we apply the concept of Bayes estimators, as outlined in Section 3.5 and
further developed in the Sections 5.1 and 7.3, to certain statistical models of
Poisson(λ, Fϑ, T ) processes, where Fϑ is a Pareto df represented by ϑ. An exten-
sion of that concept is required to cover the present questions, yet the calculations
remain just the same.
Recall from (9.12) that the likelihood function pertaining to the model of
Poisson(λ, Fϑ, T ) processes satisﬁes L({(ti, yi)}|λ, ϑ) ∝L1(k|λ)L2(y|ϑ). One must
estimate the unknown parameters λ and ϑ.
The Bayes estimate of the intensity parameter λ of the homogeneous Poisson
process of exceedance times can be computed independently of ϑ if the prior density
can be written as the product
p(λ, ϑ) = p1(λ)p2(ϑ).
(9.14)
Under this condition, the posterior density is
p(λ, ϑ|{(ti, yi)}) = p1(λ|k)p2(ϑ|y),
(9.15)
where p1(λ|k) ∝L1(k|λ)p1(λ) and p2(ϑ|y) ∝L2(y|ϑ)p2(ϑ).
The Bayes estimator for a functional parameter
T (λ, ϑ) = T1(λ)T2(ϑ)
(9.16)

9.3. ML and Bayesian Estimation in Models of Poisson Processes
255
can be represented by
T({ti, yi})
=

T1(λ)T2(ϑ)p1(λ|k)p2(ϑ|y) dλdϑ
=

T1(λ)p1(λ|k) dλ

T2(ϑ)p2(ϑ|y) dϑ .
(9.17)
We explicitly compute Bayes estimators of the intensity λ and the parameter
ϑ.
• If λ is estimated, then T1(λ) = λ, T2(ϑ) = 1 and

T2(ϑ)p2(ϑ|y) dϑ = 1. The
Bayes estimate of λ is
λ∗
k =

λp1(λ|k) dλ.
(9.18)
Speciﬁcally, if the prior p1(λ) is the gamma density with parameters r and
c, then we know from (3.53) that
λ∗
k = r + k
c + T
(9.19)
is the Bayes estimate of the intensity λ.
• Bayes estimation of ϑ: Again, we deal with two diﬀerent models, where ϑ = α
and, respectively, ϑ = (α, η) are the parameters of Paretian models.
– The restricted Pareto model GP1(u, µ = 0): Bayes estimators in this
Poisson–Pareto model were dealt with by Hesselager2. One gets the
Bayes estimate
α∗
k(y) =

αp2(α|y) dα
(9.20)
of the shape parameter α. If the prior p2(α) is a gamma density with
parameters s and d, then one obtains the Bayes estimate α∗
k(y) in (5.7).
– The full Pareto model GP(u) in the (α, η)–parameterization: Let ϑ =
(α, η) be the parameter vector in the model of Pareto distributions in
(5.10). The Bayes estimates of α and η can be written
α∗
k(y) =

αp2(α, η|y) dαdη
(9.21)
and
η∗
k(y) =

ηp2(α, η|y) dαdη
(9.22)
2Hesselager, O. (1993). A class of conjugate priors with applications to excess–of–loss
reinsurance. ASTIN Bulletin 23, 77–90.

256
9. Statistical Models for Exceedance Processes
with p2(α, η|y) as in (9.15). If
p2(α, η) = hs,d(α)f(η),
as in (5.11), then one obtains the Bayes estimates α∗
k(y) and η∗
k(y) of
α and η as in (5.15) and (5.16).
9.4
GP Process Approximations
co–authored by E. Kaufmann3
This section provides a link between the explanations in the Sections 1.2 and 5.1
about exceedances and exceedance dfs and Section 9.1, where exceedances and
exceedance times are represented by means of Poisson processes.
Recall from Section 1.2 that the number of exceedances, of a sample of size n
with common df F, over a threshold u is distributed according to a binomial dis-
tribution Bn,p with parameter p = F(u) = 1 −F(u). In addition, the exceedances
have the common df F [u] = (F(x) −F(u))/(1 −F(u)) for x ≥u. In Section 5.1
the actual exceedance df F [u] was replaced by a GP df W, and in Section 6.5
the remainder term in this approximation was computed under certain conditions
imposed on F. In the statistical context one must simultaneously deal with all
exceedances over the given threshold u.
Binomial Processes Representation of Exceedances Processes
First, the exceedances will be represented by a binomial process.
Binomial(n, p, F [u]) Conditions: Let
(a) Y1, Y2, Y3, . . . be iid random variables with common df F [u],
(b) K(n) be a binomial random variable with parameters n and p = F(u),
which is independent of the sequence Y1, Y2, Y3, . . . .
The actual exceedances can be distributionally represented by the sequence
Y1, Y2, Y3, . . . , YK(n)
(9.23)
which can be addressed as binomial process.
Binomial Process Approximation
Next, the actual binomial process in (9.23) will be replaced by another binomial
process
Z1, Z2, Z3, . . . , ZK(n)
(9.24)
3University of Siegen; co–authored the 2nd edition.

9.4. GP Process Approximations
257
where F [u] is replaced by W [u]. Recall that W [u] is again a GP df if W is a GP df.
Let F and W have the densities f and w. In addition, assume that ω(F) =
ω(W). According to [16], Corollary 1.2.4 (iv), such an approximation holds with
a remainder term bounded by
∆F (n, u) :=

nF(u)
1/2H

f [u], w[u]
,
(9.25)
where H is the Hellinger distance, cf. (3.4), between f [u] and w[u]. More precisely,
one gets in (9.25) a bound on the variational distance between the point processes
pertaining to the sequences in (9.23) and (9.24).
Under condition (6.35) we have
H

f [u], w[u]
= O

W
1/δ(u)

,
(9.26)
and, therefore, because (6.33) also holds,
∆F (n, u) = O

n1/2W
(2+δ)/2δ(u)

.
(9.27)
For example, if u is the (1 −k/n)–quantile of W—with k denoting the ex-
pected number of exceedances over u—then the right–hand side in (9.27) is of
order k1/2(k/n)1/δ.
Von Mises Bounds
We mention the required modiﬁcations if condition (6.35) is replaced by the con-
ditions (6.41) and (6.42).
The upper bound on the binomial process approximation, with w[u] replaced
by wγ,u,σ(u), is
∆F (n, u)
=

nF(u)
1/2H

f [u], wγ,u,σ(u)

=
O

nF(u)
1/2η

F(u)


(9.28)
in analogy to (9.27) with σ(u) as in (6.45).
Thus, the exceedances under the actual df F [u] can be replaced by GP random
variables with common df Wγ,u,σ(u) within the error bound in (9.28). For example,
if F is the Gompertz df and u = F −1(1 −k/n), then
∆F (n, u) = O

k−1/2/ log n

.
Penultimate Approximation
The corresponding upper bound for the penultimate approximation to the ex-
ceedances process, with wγ,u,σ(u) replaced by wγ(u),u,σ(u), is
∆F (n, u)
=

nF(u)
1/2H

f [u], wγ(u),u,σ(u)

=
O

nF(u)
1/2τ

F(u)

(9.29)

258
9. Statistical Models for Exceedance Processes
under the conditions (6.41) and (6.46).
A Poisson Process Approximation
One gets a Poisson process instead of a binomial process if the binomial random
variable with parameters n and p is replaced by a Poisson random variable with
parameter λ = np. The remainder term of such an approximation, in terms of the
variational distance, is bounded by p, see [43], Remark 1.4.1.
9.5
Inhomogeneous Poisson Processes,
Exceedances Under Covariate Information
In this section we model
• frequencies of occurance times by means of an inhomogeneous Poisson pro-
cess, and
• magnitudes by stochastically independent, time–dependent marks which are
distributed according to generalized Pareto (GP) dfs.
A parametric modeling for the marks is indispensable to achieve the usual
extrapolation to extraordinary large data.
Inhomogeneous Poisson and Poisson(Λ, F) Processes
We extend the concept of a Poisson(λ, F) process (cf. page 248) in two steps.
Firstly, the intensity λ is replaced by a mean value function Λ or an intensity
function (also denoted by λ) and, secondly, the df F of the marks is replaced by a
conditional df F = F(·|·).
Poisson(Λ, F) Conditions:
(a) (Poisson(Λ) Process.) Firstly, let 0 ≤T1 ≤T2 ≤T3 ≤· · · be a Pois-
son(1) process (a homogeneous Poisson process with intensity λ = 1,
cf. page 248). Secondly, let Λ be a nondecreasing, right–continuous
function deﬁned on the positive half–line [0, ∞) with Λ(0) = 0 and
limt→∞Λ(t) = ∞. Deﬁne the generalized inverse Λ−1 corresponding to
the concept of a qf, see (1.64).
Then, the series τi = Λ−1(Ti), i ≥1 or, equivalently, the counting
process
N(t) =
∞

i=1
I(τi ≤t), t ≥0,
can be addressed as an inhomogeneous Poisson process with mean value
function Λ.

9.5. Exceedances Under Covariate Information
259
(b) (Conditional Marks.) Given occurrence times ti consider random vari-
ables Xti with df F(·|ti). Thus, we have marks which distributionally
depend on the time at which they are observed.
One can verify that N(t) is a Poisson random variable with expectation Λ(t)
which justiﬁes the notion of Λ as a mean value function. Moreover notice that there
is a homogeneous Poisson process with intensity λ if Λ(t) = λt. If a representation
 b
a λ(x) dx = Λ(b) −Λ(a) holds, then λ(x) is called the intensity function of Λ.
This two–step random experiment constitutes a Poisson(Λ, F) process. The
reader is referred to [43], Corollary 7.2.2, where it is shown, in a more general
setting, that such a process is a Poisson point process with intensity measure
ν([0, t] × [0, y]) =
 t
0
F(y|z) dΛ(z).
(9.30)
One obtains a Poisson(λ, F) process as a special case if Λ(t) = λt and F(·|t) = F.
An Exceedance Process as an Inhomogeneous Poisson Process
Given a Poisson(Λ, F) process, the marks exceeding the threshold u and the per-
taining exceedance times form a Poisson(Λu, F [u]) process, where
Λu(t) =
 t
0
(1 −F(u|s)) dΛ(s)
(9.31)
is the mean value function of the exceedance times, and
F [u](w|t) =

F(w|t) −F(u|t)

1 −F(u|t)

,
w ≥u,
(9.32)
are the conditional dfs of the exceedances.
In terms of Poisson point processes, cf. end of this section, this can be for-
mulated in the following manner: if ν is the intensity measure pertaining to the
Poisson(Λ, F) process, then the truncation of ν outside of [0, ∞) × [u, ∞) is the
intensity measure pertaining to the Poisson(Λu, F [u]) process (of the truncated
process).
Densities of Poisson Processes
For the speciﬁcation of likelihood functions one requires densities of Poisson pro-
cesses with domain S in the Euclidean d–space.
Let N0 and N1 be Poisson processes on S with ﬁnite intensity measures ν0
and ν1; thus, ν0(S) < ∞and ν1(S) < ∞(cf. end of this section). If ν1 has the
ν0–density h, then L(N1) has the L(N0)–density g with
g({yi}) =

k

i=1
h(yi)

exp

ν0(S) −ν1(S)

,
(9.33)

260
9. Statistical Models for Exceedance Processes
where k is the number of points yi, see, e.g., [43], Theorem 3.1.1, for a general
formulation.
Statistical Modeling of the Exceedance Process
We start with independent random variables Yt with dfs F(y|t) and densities f(y|t)
for t = 1, . . . , n. We implicitly assume that the dfs depend on some unknown
parameter.
The discrete time points t = 1, . . . , n are replaced by a homogeneous Poisson
process on the interval [0, n] with intensity λ = 1 (this is motivated by weak
convergence results for empirical point processes). The random variables Yt are
regarded as marks at t. Combining the random time points and the marks on
gets a two–dimensional Poisson process with intensity measure ν([0, s] × [0, y]) =
 s
0 F(y|t) dt as in (9.30).
The exceedances above the threshold u and the pertaining exceedance times
form a Poisson(Λu, F [u]) process as speciﬁed in (9.31) and (9.32) with Λ(t) = t.
The intensity measure of the Poisson(Λu, F [u]) process is given by
νu([0, s] × [u, y]) =
 s
0
F [u](y|t) dΛu(t)
and, therefore, νu(S) =  n
0 (1−F(u|t)) dt where S = [0, n]×[u, ∞). One may check
that the Lebesgue density of νu is
hu(t, y) = f(y|t),
(t, y) ∈[0, n] × [u, ∞).
by changing the order of integration.
Such a model will be applied in Section 14.2 to compute the T –year ﬂood
level in ﬂood frequency analysis. Next we also specify a likelihood function for
such processes which enables, e.g., the computing of MLEs in an exceedance model
under covariate information.
A Likelihood Function for the Exceedance Process
The likelihood function of the Poisson(Λu, F [u]) process, as a function of the pa-
rameter which is suppressed in our notation, is given by
L({(yti, ti)}|·) ∝

k

i=1
f(yti|ti)

exp

−
 n
0
(1 −F(u|t)) dt

,
(9.34)
where k is the number of exceedances yti, and the ti are the pertaining exceedance
times4.
4To make (9.33) applicable, one has to use a density with respect to a ﬁnite measure
which is equivalent to the Lebesgue measure. This entails that a factor is included in
the representation of the density which does not depend on the given parameters and is,
therefore, negligible in (9.34).

9.5. Exceedances Under Covariate Information
261
The dfs F(y|t) are merely speciﬁed for t = 1, . . . , n and, therefore, the integral
 n
0 (1 −F(u|t)) dt in (9.34) is replaced by n
t=1(1 −F(u|t)). Notice that the term
1 −F(u|t) represents the exceedance probability at time t. Also notice that the
likelihood function L is merely evaluated for exceedance times ti ∈{1, . . . , n} in
conjunction with the pertaining exceedances yi.
An Application to Exceedances under Covariate Information
Corresponding to (8.34), where EV dfs are studied, assume that the conditional
df of Y given X = x is equal (or close to) a generalized Pareto (GP) df for values
y above a higher threshold u. More precisely, let
P(Y ≤y|X = x) = Wγ,µ(x),σ(x)(y),
y > u,
(9.35)
where the location and scale parameters µ(x) and σ(x) are given as in (8.35). We
take
µ(x)
=
µ0 + µ1x,
σ(x)
=
exp(σ0 + σ1x).
(9.36)
Also assume that the Yt are conditionally independent by conditioning on the
covariate vector X with
P(Yt ≤y|X = x)
=
P(Yt ≤y|Xt = xt)
=
Wγ,µ(xt),σ(xt)(y),
y > u,
(9.37)
cf. (8.15) to (8.17), where such a condition is deduced for iid random pairs (Xt, Yt).
Next the ML procedure in (9.34) is applied to dfs
F(y|t) = Wγ,µ(xt),σ(xt)(y),
y > u.
Put y = (yt1, . . . , ytk) and x = (x1, . . . , xn), where the yti are the exceedances
above the threshold u. As in Section 8.3, one obtains conditional MLEs ˆγx,y,
ˆµx,y(x), ˆσx,y(x) of the parameters γ, µ(x) and σ(x).
Other estimation procedures should be made applicable as well. We remind
on the estimation of a trend
• in the location parameter of EV dfs, see pages 113–116,
• in the scale parameter of GP dfs, see pages 139–141.
In this context one should also study moment or L–moment estimators. Such es-
timators may serve as initial values in the ML procedure.
For larger q, one gets an estimate of the covariate conditional q–quantile
qγ,µ,σ(Y |x) = W −1
γ,µ(x),σ(x)(q)
(9.38)
given X = x by means of qˆγx,y,ˆµx,y,ˆσx,y(Y |x).

262
9. Statistical Models for Exceedance Processes
Predicting the Conditional q–Quantile
We may as well deﬁne the covariate conditional q–quantile qγ,µ,σ(Y |X). Plugging
in the estimators of the unknown parameters as in (8.38) one gets a predictor of
qγ,µ,σ(Y |X).
A Poisson Process Modeling of Financial Data
The modeling of higher excesses of returns by means of Poisson processes under
covariate information (explanatory variables) was used by Tsay5 to investigate the
eﬀect of changes in U.S. daily interest rates on daily returns of the S&P 500 index,
and a covariate conditional Value–at–Risk (VaR). We refer to Section 16.8, where
a serial conditional VaR is studied within the framework of GARCH time series.
An application of the present Poisson process modeling under covariate in-
formation to environmental sciences may be found in Chapter 15.
The General Notion of a Poisson Process
In Section 9.1, we started with the notion of a homogeneous Poisson process,
denoted by Poisson(λ), with intensity λ. By adding marks one gets a Poisson(λ, F)
process. Using a transformation in the time scale by means of a mean value function
Λ one gets the inhomgeneous Poisson process Poisson(Λ, F). We also employed
the notion of an intensity measure which presents the mean number of points in a
measurable set.
Generally, one may deﬁne a Poisson process for every ﬁnite or σ–ﬁnite mea-
sure ν on a measurable space S, see [43]. This measure is again called intensity
measure. The Poisson process is denoted by Poisson(ν). In textbooks it is usually
assumed that the underlying space S is Polish or a locally compact Hausdorﬀspace
with a countable base, yet these conditions are superﬂuous for the deﬁnition of a
Poisson process.
5Tsay, R.S. (1999). Extreme value analysis of ﬁnancial data. Working paper, Graduate
School of Business, University of Chicago; also see Section 7.7 in Tsay, R.S. (2002).
Analysis of Financial Time Series. Wiley, New Jersey.

Part III
Elements of Multivariate
Statistical Analysis

Chapter 10
Basic Multivariate Concepts
and Visualization
This chapter provides a short introduction to several probabilistic and statistical
concepts in the multivariate setting such as, e.g., dfs, contour plots, covariance
matrices and densities (cf. Section 10.1), and the pertaining sample versions (cf.
Section 10.2) which may be helpful for analysing data.
Decomposition procedures of multivariate distributions by means of univari-
ate margins and multivariate dependence functions, such as copulas, are studied
in Section 10.3.
10.1
An Introduction to
Basic Multivariate Concepts
This section provides the multivariate versions of distribution functions (dfs), sur-
vivor functions and densities besides the notation.
Notation
Subsequently, operations and relations for vectors are understood componentwise.
Given row vectors a = (a1, . . . , ad) and b = (b1, . . . , bd), let
max{a, b} = (max{a1, b1}, . . . , max{adbd}),
min{a, b} = (min{a1, b1}, . . . , min{adbd}),
ab = (a1b1, . . . , adbd),
a + b = (a1 + b1, . . . , ad + bd),
a/b = (a1/b1, . . . , ad/bd),
if bi ̸= 0.

266
10. Basic Multivariate Concepts and Visualization
If a real value is added to or subtracted from a vector, then this is done compo-
nentwise. Likewise, let a/b = (a1/b, . . . , ad/b) for a real value b ̸= 0.
Furthermore, relations “≤”, “<”, “≥”, “>”, etc. hold for vectors if they are
valid componentwise; e.g., we have a ≤b, if aj ≤bj for j = 1, . . . , d or a < b, if
aj < bj for j = 1, . . . , d.
These summations and relations can be extended to matrices in a straight-
forward manner. Occasionally, we also use the multiplication of matrices that is
diﬀerent from the multiplication of row vectors as mentioned above: for a d1 × d2–
matrix A = (ai,j) and a d2×d3–matrix B =

bj,k

, we have AB =
 
j≤d2 ai,jbj,k

.
Especially, ab′ = 
j≤d ajbj for vectors a and b, where b′ is the transposed
vector of a, that is, b is written as a column vector. More generally, A′ denotes
the transposed of a matrix A.
Multivariate Distribution and Survivor Functions
The reader should be familiar with the notions of a d–variate df, a d–variate density
and the concept of iid random vectors X1, . . . , Xm with common df F.
The df of a random vector X = (X1, . . . , Xd) is
F(x) = P{X ≤x} = P{X1 ≤x1, . . . , Xd ≤xd}
for x = (x1, . . . , xd). If X1, . . . , Xd are independent, then
F(x) =

j≤d
F(j)(xj),
(10.1)
where F(j) is the df of Xj. If X1, . . . , Xd are identically distributed and totally
dependent—hence, the Xi are equal to X1 with probability one—then
F(x)
=
P{X1 ≤x1, . . . , X1 ≤xd}
=
F(1)(min{x1, . . . , xd}).
(10.2)
The d–variate survivor function corresponding to the df F is given by
F(x) = P{X > x} = P{X1 > x1, . . . , Xd > xd}.
Survivor functions will be of importance in multivariate extreme value anal-
ysis (as well as in the univariate setting, where a simpler representation of results
for minima was obtained by survivor functions).
Mean Vector and Covariance Matrix
For characterizing the df of a random vector X = (X1, . . . , Xd), the mean vector
m = (m1, . . . , md) = (EX1, . . . , EXd)

10.1. An Introduction to Basic Multivariate Concepts
267
and the covariance matrix Σ =

σj,k

are important, where the covariances are
given by
σj,k = Cov(Xj, Xk) = E

(Xj −mj)(Xk −mk)

,
whenever the covariance exists (cf. (2.52)). Notice that σj,j = σ2
j is the variance
of Xj. One gets the correlation coeﬃcients by
ρj,k = σj,k/(σjσk).
Note that the correlation coeﬃcient ρj,k ranges between −1 and 1. Random vari-
ables Xj and Xk are uncorrelated if ρj,k = 0. This condition holds if Xj and Xk
are independent and the second moments are ﬁnite. If Xj = Xk or Xj = −Xk
with probability 1, then ρj,k = 1 or ρj,k = −1.
Kendall’s τ
For characterizing the dependence structure of a random vector X = (X1, . . . , Xd)
one may also use Kendall’s τ in place of the covariance. One of the advantages of
Kendall’s τ is that the second moments need not be ﬁnite.
Let Y = (Y1, . . . , Yd) be another random vector which is distributional equal
to X, and let X and Y be independent. Then, deﬁne
τj,k = P{(Yj −Xj)(Yk −Xk) > 0} −P{(Yj −Xj)(Yk −Xk) < 0} .
(10.3)
Marginal Distribution and Survivor Functions
In (10.1) and (10.2), we already employed the jth margin df F(j) of a d–variate df
F. Generally, let
FK(x) = P{Xk ≤xk, k ∈K}
for any set K of indices between 1 and d. Thus, we have F(j)(xj) = F{j}(x).
Apparently, FK can be deduced from F by letting xj tend to inﬁnity for all indices
j not belonging to K. Likewise, the margins FK of a survivor function F are deﬁned
by
FK(x) = P{Xk > xk, k ∈K}.
One reason for the importance of survivor functions is the following repre-
sentation of a d–variate df F in terms of marginal survivor functions FK. We
have
1 −F(x) =

j≤d
(−1)j+1 
|K|=j
FK(x).
(10.4)
Likewise, we have
1 −F(x) =

j≤d
(−1)j+1 
|K|=j
FK(x).
(10.5)

268
10. Basic Multivariate Concepts and Visualization
To verify (10.4) and (10.5), one must apply an additivity formula (see, e.g.,
(4.4) in [16]). Finally, we remark that F is continuous if the univariate margins
F(j) are continuous. Deduce from (10.4) and (10.5) that
F(x1, x2) = F1(x1) + F2(x2) + F(x1, x2) −1
(10.6)
and
F(x1, x2) = F1(x1) + F2(x2) + F(x1, x2) −1
(10.7)
for bivariate dfs F with margins F1 and F2.
Multivariate Densities
If a df F has the representation
F(x) =
 x
−∞
f(x) dx =
 xd
−∞
· · ·
 x1
−∞
f(x1, . . . , xd) dx1 · · · dxd,
(10.8)
where f is nonnegative, then f is a (probability) density of F. A necessary and
suﬃcient condition that a nonnegative f is the density of a df is

f(x) dx :=
 ∞
−∞
f(x) dx = 1.
(10.9)
If the random variables X1, . . . , Xd are independent again and Xi possesses a
density fi, we see that
f(x) =

i≤d
fi(xi)
(10.10)
is a joint density of the X1, . . . , Xd.
Under certain conditions, a density can also be constructed from a given df
F by taking partial derivatives1: if the d–fold partial derivative
f(x) =
∂d
∂x1 · · · ∂xd
F(x)
(10.11)
is continuous, then f is a density of F. It is suﬃcient that the given condition
holds in an open rectangle (a, b) = 
j≤d(aj, bj)—or, generally, an open set U—
possessing the probability 1. Then, put f = 0 outside of (a, b) or U.
If the df F has a density f, then the survivor function F can be written
F(x) =
 ∞
x
f(x) dx =
 ∞
xd
· · ·
 ∞
x1
f(x1, . . . , xd) dx1 · · · dxd.
(10.12)
From this representation, as well as from (10.5), one realizes that the density
f can also be deduced from the survivor function by taking the d–fold partial
derivative.
1See Theorem A.2.2 in Bhattacharya, R.N. and Rao, R.R. (1976). Normal Approxi-
mation and Asymptotic Expansions. Wiley, New York.

10.1. An Introduction to Basic Multivariate Concepts
269
Contour Plots of Surfaces, Bivariate Quantiles
Another indispensable tool for the visualization of a bivariate function f is the
contour plot which consists of lines {(x, y) : f(x, y) = q} for certain values q.
The illustration in Fig. 10.1 concerns a bivariate normal density which will
be introduced in Section 11.1.
-3
0
3
-3
0
3
Fig. 10.1. Contour plot of the
bivariate normal density with
correlation coeﬃcient ρ = 0.5.
For a bivariate df F, the line {(x, y) : F(x, y) = q} may be addressed as the
q–quantile of F. Therefore, a contour plot of a df F displays a certain collection
of quantiles. Likewise, one may deal with upper p–quantiles {(x, y) : F(x, y) = p}
pertaining to the survivor function.
-2
-1
0
1
2
-2
-1
0.0
1
2
-2
-1
0
1
2
-2
-1
0.0
1
2
Fig. 10.2. Contour plots of the bivariate normal distribution with correlation coeﬃcient
ρ = 0.5. (left.) Quantiles of df for q = i/10 with i = 1, . . . , 9. (right.) Quantiles of survivor
function for p = i/10 with i = 1, . . . , 9.

270
10. Basic Multivariate Concepts and Visualization
10.2
Visualizing Multivariate Data
In this section we introduce the sample versions of multivariate dfs, covariances,
densities and also contour plots. Moreover, we display diﬀerent versions of scatter-
plots for trivariate data. Remarks about handling missing components in a vector
of data are added.
Multivariate Sample Distribution and Survivor Functions
For d–variate data x1, . . . , xn, the sample df is
Fn(x) = 1
n

i≤n
I(xi ≤x).
(10.13)
If x1, . . . , xn are governed by a d–variate df F, then the sample df Fn(x)
provides an estimate of F(x) as in the univariate case. An illustration of a bivariate
df may be found in Fig. 12.1.
A related remark holds for the sample survivor function which is deﬁned by
Fn(x) = 1
n

i≤n
I(xi > x).
(10.14)
The 2–dimensional sample df and survivor function must be plotted in a 3–D
plot. A simpliﬁed representation is achieved by using sample contour plots as dealt
with below.
Sample Mean Vector, Sample Covariances
The sample versions of the mean vector m and the covariance matrix (σj,k)—based
on d–variate data x1, . . . , xn with xi = (xi,1, . . . , xi,d)—are
• the vector mn of the componentwise taken sample means
mj,n = 1
n

i≤n
xi,j,
(10.15)
• the matrix of sample covariances (cf. also (2.53))
sj,k,n =
1
n −1

i≤n
(xi,j −mj,n)(xi,k −mk,n).
(10.16)
For the sample variances we also write s2
j,n in place of sj,j,n. In addition, the sample
correlation correlations
ρj,k,n = sj,k,n/(sj,j,nsk,k,n)1/2.
(10.17)

10.2. Visualizing Multivariate Data
271
Bivariate Sample Contour Plots
The contour plots of sample df Fn and sample survivor function Fn are estimates
of the contour plots of the underlying df F and survivor function F. Thus, in the
case of the sample survivor function one is computing lines of point t which admit
an approximately equal number of points xi > t.
x
y
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
Fig. 10.3. 2–dimensional data and
a contour plot of the sample sur-
vivor function.
As in the continuous case, one is evaluating the surface of the sample functions
on a rectangular grid, and the approximate contours are computed by interpola-
tion. For the details we refer to the book by Cleveland [8], pages 242–241.
Multivariate Kernel Density
Let k be a univariate kernel as presented in Section 2.1. Using such univariate
kernels as factors one gets a d–variate kernel by u(x) = 
j≤d k(xj). In analogy to
(2.14), deﬁne a d–variate kernel density fn,b at x by
fn,b(x) =
1
nbd

i≤n

j≤d
k
xj −yi,j
b

(10.18)
based on d–variate data yi = (yi,1, . . . , yi,d), where b > 0 is a smoothing parameter.
Note that (10.18) can be written
fn,b(x) =
1
nbd

i≤n
u
x −yi
b

.
(10.19)
In this formula, the special kernel
u(x) =

j≤d
k(xj)

272
10. Basic Multivariate Concepts and Visualization
-2
0
2
x
-2
0
2
y
0.1
0.2
z
-2
0
2
x
-2
0
2
y
0.1
0.2
z
Fig. 10.4. (left.) 3–D plot of bivariate normal density with correlation coeﬃcient ρ = 0.5.
(right.) 3–D plot of kernel density based on 100 bivariate normal data with the bandwidth
b = .5 (based on a bivariate normal kernel).
can be replaced by any d–variate probability density.
Modiﬁcations of this construction are necessary if the data exhibit a certain
direction. Then, it is useful to consider a kernel u that mimics the shape of the
data set. This question can also be dealt with in conjunction with data sphering
and principle component analysis2.
Bounded Kernel Density
Subsequently, we study bivariate data yi = (yi,1, yi,2) for i = 1, . . . , n. As in
the univariate case, we try to prevent a smoothing of the data beyond certain
boundaries. One may assume that the ﬁrst or second component is left or right–
bounded.
First, let us assume that the ﬁrst component is left bounded by a1, that
is, yi,1 ≥a1 for i = 1, . . . , n. For this case, a plausible kernel density may be
constructed in the following manner:
1. reﬂect the yi,1 in a1 obtaining values y′
i,1 = 2a1 −yi,1;
2. apply the preceding kernel density fn,b to the new enlarged data set consist-
ing of (yi,1, yi,2) and (y′
i,1, yi,2) for i = 1, . . . , n,
3. restrict the resulting curve to [a1, ∞) × (−∞, ∞).
If the ﬁrst component is also right–bounded by a2 > a1, reﬂect the yi1 also
in a2 and proceed as before. Of course, the order of these two steps can be in-
terchanged. If this is necessary, continue this procedure in the second component,
whereby one must start with the enlarged data set.
2Falk, M., Becker, R. and Marohn, F. (2002). Foundations of Statistical Analyses and
Applications with SAS. Birkh¨auser, Basel.

10.2. Visualizing Multivariate Data
273
The 3–D Scatterplot
As in the 2–dimensional case the 3–dimensional scatterplots provide an indispens-
able tool to get a ﬁrst impression of a data set.
We include two versions of a 3–dimensional scatterplot.
-2
0
2
x
-2
0
2
y
-2
0
2
z
Fig. 10.5. A scatterplot of
50 trivariate normal data.
The facility to rotate such a scatterplot around the z–axis makes it particu-
larly valuable.
Next, the 3–D scatterplot is once more applied to trivariate data where, how-
ever, the ﬁrst and second components describe the site of a spatial measurement.
-2
0
2
x
-2
0
2
y
Fig. 10.6. A scatterplot of
sites and measurements.
Contourplots, surface plots and scatterplots are available within the multi-
variate part of the menu system of Xtremes.

274
10. Basic Multivariate Concepts and Visualization
Missing Data
In conjunction with censored data, the question of handling missing data was
solved by deﬁning appropriate statistical models and by applying model–based
procedures.
Subsequently, we deal with the situation where one or several components
of an observation vector are missing (assuming that this happens in an irregular
manner). Simply discarding the incompletely recorded observation vector will en-
tail the loss of information, a consequence which should be avoided particularly
for samples of smaller sizes. The proposed ﬁlling–in procedure for missing values is
a nearest–neighbor method that is related to the stochastic regression imputation
discussed in the book by Little and Rubin3.
For simplicity, let us assume that xl = (xl,1, . . . , xl,d−1, ·) is a vector such that
only the dth component is missing. Put x(d)
l
= (xl,1, . . . , xl,d−1). Fix a number k.
From those vectors without a missing component, select xi1, . . . , xik such that
x(d)
i1 , . . . , x(d)
ik are the k nearest neighbors of x(d)
l
. Finally, a dth component xl,d is
added to xl which is randomly chosen from the dth components xi1,d, . . . , xik,d.
Notice that this is a sampling procedure according to a conditional sample df (cf.
[16], pages 96–98). Likewise, missing values in other components are ﬁlled in and
the completed data are analyzed by standard methods.
Example 10.2.1. (Annual Wind–Speed Maxima: Continuation of Examples 1.2.1 and
2.1.3.) We deal with the annual wind–speed maxima (in km/hr) based on hourly mea-
surements at ﬁve Canadian stations (located at Vancouver, Regina, Toronto, Montreal
and Shearwater) from 1947 to 1984. The wind–speeds at Shearwater from 1947 to 1949
are not recorded. Thus, if the incompletely recorded vectors are discarded, a sample of
size 35 remains.
The following table only contains the data for the years 1947 to 1951. The missing
values were generated by means of the nearest–neighborhood algorithm with k = 5.
Table 10.1. Annual wind–speed maxima from 1947 to 1951 with ﬁlled–in values.
Maximum wind speed (km/h)
Year
Vancouver
Regina
Toronto
Montreal
Shearwater
1947
79.5
88.8
79.5
120.2
(85.1)
1948
68.4
74.0
75.8
74.0
(79.5)
1949
74.0
79.5
88.8
64.8
(68.4)
1950
59.2
74.0
96.2
77.7
64.8
1951
74.0
79.5
72.1
61.0
79.5
3Little, R.J.A. and Rubin, D.B. (1987). Statistical Analysis with Missing Data. Wiley,
New York.

10.3. Decompositions of Multivariate Distributions
275
10.3
Decompositions of
Multivariate Distributions
Let X = (X1, . . . , Xd) be a random vector with df F. In this section we describe
some procedures in which manner the df F can be decomposed into certain uni-
variate dfs and a multivariate df where the latter df represents the dependence
structure.
Thereby, one may separately analyze and estimate certain univariate com-
ponents and the multivariate dependence structure. The piecing together of the
diﬀerent estimates yields an estimate of F.
Copulas
Assume that the univariate marginal dfs Fj are continuous (which implies that
F is continuous). By applying the probability transformation as introduced on
page 38, the random variables Xj can be transformed to (0, 1)–uniform random
variables. The df C of the vector Y of transformed random variables Yj = Fj(Xj)
is called the copula pertaining to F. We have,
C(u)
=
P{Y1 ≤u1, . . . , Yd ≤ud}
=
F

F −1
1
(u1), . . . , F −1
d
(ud)

,
(10.20)
where F −1
j
is the qf of Fj. In addition, Y may be addressed as copula random
vector pertaining to X.
Conversely, one may restore the original df F from the copula by applying
the quantile transformation. We ﬁnd
F(x) = C

F1(x1), . . . , Fd(xd)

,
(10.21)
which is the desired decomposition of F into the copula C and the univariate
margins Fj.
Likewise, the original univariate margins can be replaced by some other dfs.
Thus, we may design a multivariate df with a given multivariate dependence struc-
ture (as, e.g., a copula) and predetermined univariate margins.
For a shorter introduction to various families of copula functions we refer to a
paper by Joe4. There is a controversial discussion about the usefulness of copulas:
we refer to Mikosch5 and the attached discussion, as, e.g., the contribution by
Genest and R´emillard6.
4Joe, H. (1993). Parametric families of multivariate distributions with given marginals.
J. Mult. Analysis 46, 262–282.
5Mikosch, T. (2006). Copulas: tales and facts. Extremes 9, 3–20.
6Genest, C. and R´emillard, B. (2006). Discussion of “Copulas: tales and facts”, by
Thomas Mikosch. Extremes 9, 27–36.

276
10. Basic Multivariate Concepts and Visualization
Data–Based Transformations, Empirical Copulas
In application one may carry out the transformations with estimated paramet-
ric univariate and multivariate dfs and and piece the univariate and multivariate
components together, for details see page 389.
In the following example, the copula method is employed with estimated
Gaussian copula and Student margins.
Example 10.3.1. (Log-Returns Deutsche Bank against Commerzbank: Continuation of
Example 9.4.1.) The scatterplot in Figure 10.7 again consists of the log-returns (with
conversed signs) of the Deutsche Bank plotted against those of the Commerzbank on
trading days from Jan. 1, 1992 to Jan. 11, 2002.
In addition, the contour lines of the estimated density of a Gaussian copula and
univariate Student margins are plotted.
Commerzbank
Deutsche Bank
-0.1
0.0
0.1
-0.1
0.0
0.1
Fig. 10.7. Scatterplot of log-
returns (with changed signs)
Deutsche Bank against Com-
merzbank and contour plot of
estimated density with Gaus-
sian copula and univariate
Student margins.
The exotic “butterﬂy” contour plot does not ﬁt to the scatterplot which has an
elliptical shape. We conclude that the modeling by means of a Gaussian copula and
univariate Student margins is not correct.
A related illustration in conjunction with a scatterplot of daily diﬀerences of
zero–bond–discount–factors may be found in Wiedemann et al., page 757. These
examples show that the copula method should be employed with utmost care. We
are not aware whether there are general rules under which the copula method is
always appropriate.
7Wiedemann, A., Drosdzol, A., Reiss, R.–D. and Thomas, M. (2005). Statistische
Modellierung des Zins¨anderungsrisikos. Teil 2: Multivariate Verteilungen. In: Modernes
Risikomanagement (F. Romeike ed.). Wiley-VCH, Weinheim.

10.3. Decompositions of Multivariate Distributions
277
The transformations may also be based on kernel estimates of the dfs, see
(2.17) and (10.19). In this case, the data are still transformed by means of contin-
uous dfs.
In Section 13.3 we make use of univariate transformations of a diﬀerent type,
namely those by means of univariate sample dfs Fn based on the sample vector
x = (x1, . . . , xn), see (2.1). Thus, the original data xi in the margins are replaced
by yi = Fn(xi). Recall that the number of xj ≤xi is the rank of xi, denoted by
ri(x), see, e.g., H´ajek and ˇSidak, page 368. The value ri(x) is the rank of xi in
the ordered sample x1:n < . . . < xn:n. Therefore, the transformed data are relative
ranks; we have
yi = ri(x)/n.
(10.22)
In that context one loosely speaks of empirical copulas. Concerning stochastical
properties of order statistics and rank statistics we refer to the book by H´ajek and
ˇSidak.
Other Decompositions
The decomposition of the joint distribution of random variables X and Y in the
marginal distribution of X and the conditional distribution of Y given X = x
was the topic of Section 8.1. Of course, this is the most important decomposition
methodology for a random vector.
Notable is also the well–known decomposition of spherical random vectors
in two independent random variables, namely the radial and angular components,
see Section 11.2.
In conjunction with extreme values, we are interested in standard forms of
multivariate distributions diﬀerent from copulas, namely,
• the bivariate generalized Pareto (GP) dfs in their standard form, cf. Section
13.1, are closely related to copulas in so far that the margins are always
equal to the uniform df on the interval (−1, 0) which is the univariate GP df
W2,−1, thus, there is the slight modiﬁcation of a shifted uniform df;
• the bivariate extreme value (EV) dfs in Section 12.1 are described by the
Pickands dependence function and univariate margins equal to the reversed
exponential df on the negative half–line which is the EV df G2,−1 (by the
way, other authors prefer the Frech´et df G1,1 as univariate margin);
• representations of a diﬀerent type are employed in Section 12.4 where a
bivariate dfs H on the left, lower quadrant (−∞, 0]2 is represented by a
certain spectral decomposition. For that purpose one considers univariate
dfs of the form
Hz(c) = H(cz, c(1 −z))
8H´ajek, J. and ˇSidak, Z. (1967). Theory of Rank Tests. Academic Press, New York.

278
10. Basic Multivariate Concepts and Visualization
in the variable c for each z, where z and c may be regarded as the angular
and, respectively, radial component.
If H is an EV df or, respectively, a GP df, then Hz is a reversed exponential
df or, respectively, a uniform df depending on z,
• in analogy to the above mentioned decomposition for spherical random vec-
tors, bivariate GP random vectors can be decomposed in independent radial
and angular components, see Section 12.4.

Chapter 11
Elliptical and Related
Distributions
In Section 11.1 we ﬁrst recall some well–known, basic facts about multivariate
Gaussian and log–normal models. The Gaussian distributions will serve as ex-
amples of spherical and elliptical distributions in Section 11.2. An introduction
to multivariate Student and multivariate sum–stable distributions is separately
treated in the Sections 11.3 and 11.4.
Thus, we deal with multivariate distributions, where the univariate margins
have light, fat and heavy tails. Recall that Student distributions have Pareto–
type tails with α > 0, whereas the tail index of the non–Gaussian, sum–stable
distributions is restricted to α < 2.
11.1
Multivariate Gaussian Models
Corresponding to the univariate case, parametric models are built by starting with
certain standard dfs and, subsequently, adding location and scale parameters in
the single components. As examples we ﬁrst discuss the multivariate normal and
log–normal models.
In Section 12.1, the multivariate normal model will be converted into a mul-
tivariate EV model with univariate Gumbel margins.
Location and Scale Parameter Vectors
Let X = (X1, . . . , Xd) be a random vector with df F. Location and scale parameter
vectors can be added to this df by considering random variables Yj = µj + σjXj
for j = 1, . . . , d. The joint df of the Yj is given by
Fµ,σ(x)
=
P{X1 ≤(x1 −µ1)/σ1, . . . , Xd ≤(xd −µd)/σd}
=
F
x −µ
σ

,
(11.1)

280
11. Elliptical and Related Distributions
where µ = (µ1, . . . , µd) and σ = (σ1, . . . , σd) denote the location and scale vectors.
Multivariate Normal Distributions
The d–variate normal distribution serves as a ﬁrst example of a multivariate distri-
bution. Let X = (X1, . . . , Xd) be a vector of iid standard normal random variables.
Linear changes of X are again normal vectors. If A is a d × d–matrix (ai,j) with
determinant det A ̸= 0, then
Y
′ = AX
′ =
 
j≤d
a1,jXj, . . . ,

j≤d
ad,jXj
′
(11.2)
is a d–variate normal vector with mean vector zero and covariance matrix Σ =
(σi,j) = AA
′. The density of Y is given by
ϕΣ(y) =
1
(2π)d/2 det(Σ)1/2 exp

−1
2yΣ−1y′
,
(11.3)
where Σ−1 denotes the inverse of Σ. Let
ΦΣ(x) =
 x
−∞
ϕΣ(y) dy
(11.4)
be the df of Y . We write ϕµ,Σ and Φµ,Σ when a location vector µ is added.
The density ϕΣ can be deduced from the density ϕI of the original normal
vector X by applying the transformation theorem for densities (hereby, I denotes
the unit matrix with the elements equal to one in the main diagonal and the
elements are equal to zero otherwise). Moreover, marginal vectors (Yj1, . . . , Yjk)
with 1 ≤j1 < j2 < · · · < jk ≤d are normal again.
Notice that (11.2) is a special case of the following well–known result: if Y
is a d–variate normal vector with mean vector µ and covariance matrix Σ and A
is a k × d–matrix (with k rows and d columns and k ≤d) having rank k, then
Z
′ = AY
′ is a k–variate normal vector with mean vector Aµ′ and covariance
matrix AΣA
′.
Statistical Inference in the Multivariate Gaussian Model
The d–variate normal model is

Φµ,Σ : µ = (µ1, . . . , µd), Σ =

σi,j

,
(11.5)
where µ is a location (mean) vector and Σ is the covariance matrix. Estimators are
easily obtained by the sample mean vector µn and the sample covariance matrix
Σn, cf. (10.15) and (10.16).

11.2. Spherical and Elliptical Distributions
281
It is well known that the sample mean vector µn and the sample covariance
matrix Σn (with the factor 1/(n −1) replaced by 1/n) are also the MLEs in the
d–variate normal model.
Assume that (Y1, . . . , Yd) is a random normal vector with mean vector µ =
(µ1, . . . , µd) and covariance matrix Σ = (σi,j). Then Yj has the location and scale
parameters µj and σj = σ1/2
j,j . The correlation matrix is (ρi,j) = (σi,j/(σiσj)).
Therefore, a normal distribution may be alternatively represented by the the cor-
relation matrix (ρi,j) and the location and scale vectors µ = (µ1, . . . , µd) and
σ = (σ1, . . . , σd).
A Useful Result Concerning the Asymptotic Normality
The following result, which was applied in (4.4), is taken from [48], pages 118 and
122. Let
• b−1
n (Xn −µ) be distributed asymptotically according to ΦΣ (that is the
pointwise convergence of the dfs) where bn →0 as n →∞,
• gi be real–valued functions with partial derivatives ∂gi/∂xj ̸= 0 at µ for
i = 1, . . . , m and j = 1, . . . , d.
Let g = (g1, . . . , gm) and let D be the matrix of partial derivatives evaluated at
µ. Then,
b−1
n (g(Xn) −g(µ))
(11.6)
is distributed asymptotically according to ΦDΣD′.
Multivariate Log–Normal Distributions
If the positive data x1, . . . , xn indicate a heavier upper tail than that of a multi-
variate normal distribution Φµ,Σ, then one may try to ﬁt a multivariate log–normal
df
F(µ,Σ)(x) = Φµ,Σ(log(x1), . . . , log(xd)),
x > 0.
(11.7)
The values exp(µi) are scale parameters and Σ is a matrix of shape parame-
ters of the multivariate log–normal distribution. It is apparent that the univariate
margins are univariate log–normal distributions as set forth in Section 1.6.
11.2
Spherical and Elliptical Distributions
First, we study multivariate normal distributions from the viewpoint of spherical
and elliptical distributions.

282
11. Elliptical and Related Distributions
Multivariate Normal Distributions, Revisited
We describe multivariate normal distributions as spherical and elliptical ones. Let
again X = (X1, . . . , Xd) be a vector of iid standard normal random variables as
in Section 11.3. The pertaining density is
ϕI(x) =

j≤d
ϕ(xj) =
1
(2π)d/2 exp

−1
2xx′
,
(11.8)
where I is again the unit matrix.
Notice that |x| =
√
xx′ is the Euclidean norm of the vector x. It is apparent
that the contour lines {x : ϕI(x) = q} are spheres. Therefore, one speaks of
a spherical distribution. A random vector X with density ϕI has the following
property: we have
AX
d= X,
(11.9)
for any orthogonal matrix A; that is, for matrices A with AA′ = I. This can be
analytically veriﬁed by means of equation (11.3) because Σ−1 = (AA′)−1 = I
for orthogonal matrices A. (11.9) is also evident from the fact that an orthogonal
matrix causes the rotation of a distribution.
More generally, if A is a d × d–matrix with det A ̸= 0, then AX is a normal
vector with covariance matrix Σ = AA′ and density ϕΣ, cf. again (11.3). Now,
the contour line {y : ϕΣ(y) = q} is an ellipse for every q > 0, and one speaks of
an elliptical distribution.
Spherical and Elliptical Distributions
Generally, one speaks of a spherical random vector X if (11.9) is satisﬁed for any
orthogonal matrix A. Moreover, an elliptical random vector is the linear change
of a spherical random vector under a matrix A with det A ̸= 0.
In conjunction with multivariate Student and multivariate sum–stable distri-
butions we will make use of the following construction: if X is a spherical random
vector and Y is real–valued then XY is again a spherical random vector because
A(XY ) = (AX)Y
d= XY,
(11.10)
for any orthogonal matrix A.
A Decomposition of Spherical Distributions
It is well known, see, e.g., [19], that a spherical random vector X can be written
as
X = RU,
where R ≥0 is the radial component, U is uniformly distributed on the unit
sphere S = {u : |u| = 1} and R, U are independent. Here |x| =
√
xx′ is again the

11.3. Multivariate Student Distributions
283
Euclidean norm. We may call U the angular component (at least in the bivariate
case).
If P{X = 0} = 0, then we have |X|
d= R and X/|X|
d= U.
11.3
Multivariate Student Distributions
We introduce another class of elliptical distributions, namely certain multivariate
Student distributions, and deal with statistical questions. At the end of this section,
we also introduce non–elliptical Student distributions.
Multivariate Student Distribution with Common Denominators
Recall from Section 6.3 that Z = X/(2Y/α)1/2 is a standard Student random
variable with shape parameter α > 0, where X is standard normal and Y is a
gamma random variable with parameter r = α/2.
Next, X will be replaced by a random vector X = (X1, . . . , Xd) of iid stan-
dard normal random variables Xi. Because X is spherical, we know from the
preceding arguments that X/(2Y/α)1/2 is a spherical random vector. In addition,
if A is a matrix with det A ̸= 0, then a linear change yields the elliptical random
vector
Z = A

X/(2Y/α)1/2
= (AX)/(2Y/α)1/2,
(11.11)
where AX is a normal random vector with covariance matrix Σ = AA′.
This distribution of Z is the multivariate standard Student distribution with
parameter α > 0 and parameter matrix Σ = (σi,j). We list some properties of
these distributions.
• The density of Z = (Z1, . . . , Zd) is
fα,Σ(z) =
Γ((α + d)/2)
Γ(α/2)(απ)d/2(det Σ)1/2

1 + α−1zΣ−1z′−(α+d)/2 .
(11.12)
• According to (11.11), the marginal distributions are Student distributions
again. In particular, Zj is distributed according to the Student distribution
with shape parameter α > 0 and scale parameter σj = σ1/2
j,j .
• If α > 2, then
α
α−2Σ is the covariance matrix. For α ≤2 the covariances do
not exist.
Warning! Σ is not the covariance matrix of the Student
distribution with density fα,Σ for any α.
This was the reason why we spoke of a parameter matrix Σ instead of a
covariance matrix in that context. In the sequel, we still address Σ as “co-
variance matrix”—in quotation marks—for the sake of simplicity.

284
11. Elliptical and Related Distributions
Initial Estimation in the Student Model
The shape and scale parameters α and σj may be estimated within the univariate
marginal models. Take, e.g., the arithmetic mean of the estimates of α in the
marginal models as an estimate of α in the joint model. If α is known to be larger
than 2, then one may utilize sample variances to estimate the “covariance matrix”.
Otherwise, one may use an estimate based on Kendall’s τ, see (10.3), We
will estimate the “covariance matrix” Σ = (σi,j) by estimating the “correlation
matrix” (σi,j/σiσj), where σj = σ1/2
j,j again. The operational meaning of this pa-
rameterization becomes evident by generating the Student random vector Z in
(11.11) in two steps. Let D be the diagonal matrix with elements σj in the main
diagonal: we have Z = D ˜Z where ˜Z = (D−1AX)/(2Y/α)1/2 is the standard-
ized Student random vector with “covariance matrix” (σi,j/σiσj), and Z is the
pertaining random vector with scale parameter vector σ.
Maximum Likelihood Estimation in the Student Model
Based on initial estimates one may compute an MLE for the Student model using
the Newton–Raphson procedure. We continue Example 10.3.1 where the applica-
tion of the copula method produced a curious “butterﬂy” contour plot.
Example 11.3.1. (Continuation of Example 10.3.1.) The following considerations are
based on the daily stock indizes of the Commerzbank and the Deutsche Bank from Jan.
1, 1992 to Jan 11, 2002. The scatterplot in Figure 11.1 consists of the the pertaining
log-returns on trading days with conversed signs.
Commerzbank
Deutsche Bank
-0.1
0.0
0.1
-0.1
0.0
0.1
Fig. 11.1. Scatterplot of log-
returns (with changed signs)
Deutsche Bank against Com-
merzbank and contour plot of
the estimated Student den-
sity.
The estimated bivarate Student distribution provides a better ﬁt to the scatterplot
due to its elliptical contour.

11.4. Multivariate Sum–Stable Distributions
285
For a continuation we refer to page 390 where the Value–at–Risk is computed
within the multivariate Student model.
Extending the Concept of Multivariate Student Distributions
In (6.16) we introduced Student distributions as those of X/(Y/r)1/2, where X is
standard normal and Y is a gamma random variable with parameter r > 0. This
approach can be extended to the d–variate case dealing with random variables
Xi/(Yi/ri)1/2,
i = 1, . . . , d,
where X = (X1, . . . , Xd) is multivariate normal with mean vector zero and cor-
relation matrix Σ, and Y = (Y1, . . . , Yd) is a multivariate gamma vector. For
example, the Yi are totally dependent or independent (for details, see [32], Chap-
ter 37). It is apparent that the univariate margins are Student random variables
with parameter ri.
A further extension to skewed distributions is achieved when the mean vector
of X is unequal to zero. Then, the univariate margins are noncentral Student
distributions.
11.4
Multivariate Sum–Stable Distributions
co–authored by J.P. Nolan1
In this section we give a short introduction to multivariate sum–stable distribu-
tions. We emphasis the statistical diagnostic which reduces the multivariate case
to the univariate one which was outlined in Section 6.4.
Distributional Properties
A random vector X = (X1, X2, . . . , Xd) is stable if for all m = 2, 3, 4, . . .
X1 + · · · + Xm = amX + bm,
in distribution, where X1, X2, . . . are iid copies of X. The phrase “jointly stable”
is sometimes used to stress the fact that the deﬁnition forces all the components
Xj to be univariate sum–stable with the same α. Formally, if X is a stable random
vector, then every one dimensional projection uX
′ =  uiXi is a one dimensional
stable random variable with the same index α for every u, e.g.,
uX
′ ∼S(α, β(u), γ(u), δ(u); 1).
(11.13)
The converse of this statement is true if the projections are all symmetric,
or all strictly stable, or if α ≥1. Section 2.2 of [46] gives an example where α < 1
1American University, Washington DC.

286
11. Elliptical and Related Distributions
and all one dimensional projections are stable, but X is not jointly stable. In this
case, a technical condition must be added to get a converse.
One advantage of (11.13) is that it gives a way of parameterizing multivariate
stable distributions in terms of one dimensional projections. From (11.13) one gets
the (univariate) characteristic function of uX
′ for every u, and hence the joint
characteristic function of X. Therefore α and the functions β(·), γ(·) and δ(·)
completely characterize the joint distribution. In fact, knowing these functions on
the unit sphere S = {u : |u| = 1} in the Euclidean d–space characterizes the
distribution.
The functions β(·), γ(·) and δ(·) must satisfy certain regularity conditions.
The standard way of describing multivariate stable distributions is in terms of a
ﬁnite measure Λ on the sphere S, called the spectral measure. It is typical to use
the spectral measure to describe the joint characteristic function, we ﬁnd it more
convenient to relate it to the functions β(·), γ(·), and δ(·).
Let X = (X1, . . . , Xd) be jointly stable with a representation as in (11.13).
Then there exists a ﬁnite measure Λ on S and a d–variate vector δ with
γ(u)
=

S
|us′|α Λ(ds)
1/α
β(u)
=

S |us′|αsign (us′) Λ(ds)

S |us′|α Λ(ds)
δ(u)
=
⎧
⎨
⎩
δu′
α ̸= 1;
if
δu′ −2
π

S(us′) ln |us′| Λ(ds)
α = 1.
(11.14)
It is possible for X to be non–degenerate, but singular. X = (X1, 0) is for-
mally a two dimensional stable distribution if X1 is univariate stable, but it does
not have a density. It can be shown that the following conditions are equivalent:
(i) X is nonsingular, (ii) γ(u) > 0 for all u, (iii) the span of the support of Λ
is the Euclidean d–space. If these conditions hold, then a smooth density exists.
Relatively little is known about these densities—their support is a cone in general,
and they are likely to be unimodal.
There are a few special cases of multivariate stable distributions where quan-
tities of interest can be explicitly calculated. We concentrate on the bivariate case.
The Gaussian Case
The density function ϕµ,Σ of a multivariate Gaussian distribution was given in
(11.3). The joint characteristic function of such a distribution is
χ(u) = exp

−uΣu′/2 + uµ′
.
Hence the parameter functions are γ(u) = (uΣu′/2)1/2, β(u) = 0 and δ(u) = uµ′.

11.4. Multivariate Sum–Stable Distributions
287
The Independent Case
Let X1 ∼S(α, β1, γ1, δ1; 1) and X2 ∼S(α, β2, γ2, δ2; 1) be independent, then
X = (X1, X2) is bivariate stable. Since the components are independent, the
joint density is
f(x1, x2) = g(x1|α, β1, γ1, δ1; 1)g(x2|α, β2, γ2, δ2; 1),
where g(·| · · · ) are the one dimensional stable densities. When there is an explicit
form for a univariate density (Gaussian, Cauchy or L´evy cases), then there is an
explicit form for the bivariate density. For example, the standardized Cauchy with
independent components has density
f(x1, x2) = 1
π2
1
(1 + x2
1)(1 + x2
2).
(11.15)
For a general α the parameter functions are
β(u1, u2)
=
(sign u1)β1|u1γ1|α + (sign u2)β2|u2γ2|α
|u1γ1|α + |u2γ2|α
γ(u1, u2)
=
(|u1γ1|α + |u2γ2|α)1/α
δ(u1, u2)
=
 
u1δ1 + u2δ2,
α ̸= 1,
u1δ1 + u2δ2 −(2/π)(β1γ1u1 ln |u1| + β2γ2u2 ln |u2|),
α = 1.
Note that β(1, 0) = β1, γ(1, 0) = γ1, and δ(1, 0) = δ1, which corresponds to
(1, 0)X
′ = X1 and β(0, 1) = β2, γ(0, 1) = γ2, and δ(0, 1) = δ2 which corresponds
to (0, 1)X
′ = X2. The corresponding spectral measure is discrete with four point
masses γα
1 (1 + β1)/2 at (1, 0), γα
1 (1 −β1)/2 at (−1, 0), γα
2 (1 + β2)/2 at (0, 1),
γα
2 (1 −β2)/2 at (0, −1).
Radially Symmetric Stable Distributions
Next we deal with radially symmetric (spherical) distributions. When α = 2, they
are Gaussian distributions with independent components. When 0 < α < 2, the
components are univariate symmetric stable, but dependent. The Cauchy example
has an explicit formula for the density, namely,
f(x1, x2) = 1
2π
1
(1 + x2
1 + x2
2)3/2 .
(11.16)
See Fig. 11.2 for an illustration.
That is not the same as the independent components case (11.15). When
α ̸∈{1, 2}, there is not a closed form expression for the density. The parameter
functions are easy: β(u1, u2) = 0, γ(u1, u2) = c, and δ(u1, u2) = 0.
The spectral measure for a radially symmetric measure is a uniform measure
(a multiple of Lebesgue measure) on the unit circle. A linear change of variables
gives elliptically contoured stable distributions, which have heavier tails than their
Gaussian counterparts.

288
11. Elliptical and Related Distributions
-2
2
-2
2
-2
-1
0
1
2
x
-2
-1
0
1
2
y
0.05
0.1
0.15
z
Fig. 11.2. Contour plot (left) and surface plot (right) for a radially symmetric Cauchy
density.
Another Illustration
If the spectral measure has more mass in a certain arc, then the bivariate dis-
tribution bulges in that direction. This is illustrated in Fig. 11.3 for a bivari-
ate sum–stable distribution with α = 0.8, where the spectral measure has point
masses of weight 0.125 at (1, 0), and weights 0.250 at (cos(π/3), sin(π/3)) and
(cos(−π/3), sin(−π/3)).
1.8
-1.8
1.8
0
1
x
-2
-1
0
1
y
0
0.5
1
1.5
z
Fig. 11.3. Sum–stable density bulging in direction π/3 and −π/3 having also a smaller
bulge along the x–axis corresponding to diﬀerent masses.

11.4. Multivariate Sum–Stable Distributions
289
Multivariate Estimation
For conciseness, we stated the above results in the conventional S(α, β, γ, δ; 1)
parameterization. As in the univariate case, one should use the S(α, β, γ, δ; 0)
parameterization in all numerical and statistical work to avoid the discontinuity
at α = 1.
There are several methods of estimating for multivariate stable distributions;
in practice all involve some estimate of α and some discrete estimate of the spectral
measure ˆΛ = 
k≤m λk1{sk}, sk ∈S. Rachev and Xin2 and Cheng and Rachev3
use the fact that the directional tail behavior of multivariate stable distributions is
Pareto, and base an estimate of Λ on this. Nolan, Panorska and McCulloch4 deﬁne
two other estimates of Λ, one based on the joint sample characteristic function and
one based on one–dimensional projections of the data.
Another advantage of (11.14) is that it gives a way of assessing whether a
multivariate data set is stable by looking at just one dimensional projections of
the data. Fit projections in multiple directions using the univariate techniques
described earlier, and see if they are well described by a univariate stable ﬁt. If
so, and if the α’s are the same for every direction (if α < 1, another technical
condition holds), then a multivariate stable model is appropriate.
The parameters (α, β(u), γ(u), δ(u)) may be more useful than Λ itself when
two multivariate stable distributions are compared. This is because the distribution
of X depends more on how Λ distributes mass around the sphere than exactly
on the measure. Two spectral measures can be far away in the traditional total
variation norm (e.g., one can be discrete and the other continuous), but their
corresponding directional parameter functions and densities can be very close.
Indeed, (11.13) shows that the only way Λ enters into the joint distribution is
through the parameter functions.
Multivariate Diagnostics
The diagnostics suggested are:
• Project the data in a variety of directions u and use the univariate diagnos-
tics described in Section 6.4 on each of those distributions. Bad ﬁts in any
direction indicate that the data is not stable.
• For each direction u ∈S, estimate a value for the parameter functions α(u),
β(u), γ(u), δ(u) by ML estimation. The plot of α(u) should be a constant,
2Rachev, S.T. and Xin, H. (1993). Test for association of random variables in the
attraction of multivariate stable law. Probab. Math. Statist. 14, 125–141.
3Cheng, B.N. and Rachev, S.T. (1995). Multivariate stable future prices. Math. Fi-
nance 5, 133–153.
4Nolan, J.P., Panorska, A. and McCulloch, J.H. (2001). Estimation of stable spectral
measures. Mathematical and Computer Modelling 34, 1113–1122.

290
11. Elliptical and Related Distributions
signiﬁcant departures from this indicate that the data has diﬀerent decay
rates in diﬀerent directions and is not jointly stable. (Note that γ(·) will be
a constant iﬀthe distribution is isotropic.)
• Assess the goodness–of–ﬁt by computing a discrete ˆΛ by one of the methods
above. Substitute the discrete ˆΛ in (11.14) to compute parameter functions.
If it diﬀers from the one obtained above by projection, then either the data is
not jointly stable, or not enough points were chosen in the discrete spectral
measure approximation.

Chapter 12
Multivariate Maxima
Multivariate extreme value (EV) distributions are introduced as limiting distri-
butions of componentwise taken maxima. In contrast to the univariate case, the
resulting statistical model is a nonparametric one. Some basic properties and ﬁrst
examples of multivariate EV dfs are dealt with in Section 12.1.
In Section 12.2, we introduce the family of Gumbel–McFadden distributions
which are perhaps the most important multivariate EV distributions. As an ap-
plication, the question of economic choice behavior will be considered under dis-
turbances which are multivariate EV distributed. In the special case of Gumbel–
McFadden disturbances the choice probabilities are of a multinomial logit form.
Estimation in certain parametric EV submodels and will be investigated in
Section 12.3.
In Section 12.4, we give an outline of a new spectral decomposition method-
ology in extreme value theory where bivariate dfs are decomposed in univariate
dfs. For example, bivariate EV dfs are decomposed in univariate EV dfs.
12.1
Nonparametric and Parametric
Extreme Value Models
Because the univariate margins of multivariate EV distributions are EV distri-
butions, one may concentrate on the dependence structure of the multivariate
distribution. Therefore, we particularly, study EV dfs with reversed exponential
margins. In that context, the Pickands dependence function will be central for our
considerations.
Multivariate Extreme Value Distributions, Max–Stability
The maximum of vectors xi = (xi,1, . . . , xi,d) will be taken componentwise. We
have maxi≤m xi =

maxi≤m xi,1, . . . , maxi≤m xi,d

. If X1, . . . , Xm are iid random

292
12. Multivariate Maxima
vectors with common d–variate df F, then
P

max
i≤m Xi ≤x

= F m(x),
(12.1)
because maxi≤m Xi ≤x holds if, and only if, X1 ≤x, . . . , Xm ≤x.
Corresponding to the univariate case, the actual df F m will be replaced by a
limiting df. If
F m(bm + amx) →G(x),
m →∞,
(12.2)
for vectors bm and am > 0, then G is called a d–variate EV df and F is said to
belong to the max–domain of attraction of G.
One can prove that G is an EV df if, and only if, G is max–stable, that is,
Gm(bm + amx) = G(x)
(12.3)
for certain vectors bm and am > 0.
Let F(j) denote the jth margin of F. From (12.2), it follows that
F m
(j)(bm,j + am,jxj) →G(j)(xj),
m →∞,
(12.4)
where G(j) is the jth margin of G. Hence, the jth marginal df G(j) of a multi-
variate EV df is necessarily a univariate EV df. In addition, the vectors bm =
(bm,1, . . . , bm,d) and am = (am,1, . . . , am,d) are determined by the univariate con-
vergence. Another consequence is the continuity of the multivariate EV dfs. Yet, in
contrast to the univariate EV model, the d–variate one is a nonparametric model
for d ≥2.
Extreme Value Model for Independent Margins
If, in addition, the vectors Xi have independent margins, then the marginal max-
ima maxi≤m Xi,1, . . . , maxi≤m Xi,d are independent and, consequently,
F m(bm + amx) =

j≤d
F m
(j)(bm,j + am,jxj) →

j≤d
G(j)(xj)
(12.5)
as m →∞if (12.4) holds. For this special case, the limiting df G of maxi≤m Xi
in (12.2), taken in the γ–parameterization, is
G(x) =

j≤d
Gγj,µj,σj(xj).
(12.6)
It is evident that the estimation of the parameters γj, µj and σj must be based
on the data in the jth component.
The importance of the EV dfs as given in (12.6) comes from the fact that
an EV df of this form may also occur if the random vectors Xi have dependent
margins. We mention a prominent example.

12.1. Nonparametric and Parametric Extreme Value Models
293
(Tiago de Oliveira, Geﬀroy, Sibuya.) Let (Xi, Yi) be iid normal vectors with
correlation coeﬃcient ρ strictly between −1 and 1. Under this condition, the max-
ima maxi≤m Xi and maxi≤m Yi are asymptotically independent. We have
P

max
i≤m Xi ≤bm+amx, max
i≤m Yi ≤dm+cmy

→G0(x)G0(y),
m →∞, (12.7)
with standardizing constants such that the univariate convergence holds.
To prove this result, one may verify a simple condition imposed on conditional
probabilities which were already studied in Section 2.6. Let X and Y be random
variables with common df F such that F m(bm + amx) →G(x) as m →∞for all
x. The condition
P(Y > u|X > u) →0,
u ↑ω(F),
(12.8)
is equivalent to the asymptotic independence of the pertaining maxima; that is,
P{max
i≤m Xi ≤bm + amx, max
i≤m Yi ≤bm + amy} →G(x)G(y),
m →∞,
for iid copies (Xi, Yi) of (X, Y ), see Sibuya (1960) in the paper cited on page 74.
For a proof of this result we refer to [20], page 258, or [42], page 235.
The asymptotic independence of the marginal maxima may be regarded as
the intrinsic property of tail independence. It is equivalent to
χ =
lim
u↑ω(F ) P(Y > u|X > u) = 0
where χ is the tail dependence parameter introduced in Section 2.6.
Condition (12.8) also implies that the componentwise taken exceedances are
independent if the thresholds are chosen in such a manner that the expected num-
ber of exceedances remains bounded when the sample size goes to inﬁnity1.
Thus, before using a complex model, one should check whether the EV model
for independent margins or a related generalized Pareto model is applicable.
In addition, the pairwise asymptotic independence implies the joint asymp-
totic independence, see, e.g., [20] or [42].
Rates for the Asymptotic Independence of Maxima
Let F be a bivariate df with identical univariate margins F(j). Applying the rep-
resentation (10.7) of a df by means of survivor functions one gets (see (7.2.11) in
[42]) that
F n(x) = F n
1 (x1)F n
2 (x2) exp

nF(x)

+ O(n−1).
(12.9)
1Reiss, R.–D. (1990). Asymptotic independence of marginal point processes of ex-
ceedances. Statist. & Decisions 8, 153–165. See also [43], Section 6.2.

294
12. Multivariate Maxima
Therefore, the term nF(x) determines the rate at which the independence of the
marginal maxima is attained. For a general result of this type we refer to [16],
Lemma 4.1.3.
In the case of iid standard normal vectors with correlation coeﬃcient ρ with
−1 < ρ < 1 it was proven in [42] that
nF(bn + b−1
n x) = O

n−(1−ρ)/(1+ρ)(log n)−ρ/(1+ρ)
,
(12.10)
where bn = (bn, bn) and bn satisﬁes the condition bn = ϕ(bn). Thus, one obtains
again the above mentioned result of the asymptotic independence of maxima of
normal random variables. The rate of the asymptotic independence is slow when
ρ is close to 1. For a continuation of this topic we refer to Section 13.3.
The Marshall–Olkin Model
The standard Marshall–Olkin distributions Mλ are bivariate EV distributions with
exponential margins G2,−1 = exp(x), x < 0, where the dependence parameter
λ ranges from 0 to 1. For λ = 0 we have independence, and for λ = 1 total
dependence. We have
Mλ(x, y) = exp

(1 −λ)(x + y) + λ min{x, y}

,
x, y < 0.
(12.11)
The df Fλ is necessarily continuous, yet a density does not exist for λ > 0,
because standard Marshall–Olkin distributions have positive mass at the main
diagonal which becomes visible in Fig. 12.1.
Fig. 12.1. 3–D plot of Marshall–
Olkin df Mλ with λ = 0.5 .
For 0 ≤λ ≤1, check that
Mλ(x, y) = P
 
max
 Z2
1 −λ, Z1
λ
!
≤x, max
 Z3
1 −λ, Z1
λ
!
≤y
!
,
(12.12)

12.1. Nonparametric and Parametric Extreme Value Models
295
where Z1, Z2 and Z3 are iid random variables with common exponential df G2,−1.
This model can be extended to a d–variate one. Let Λ = (λi,j) be a m × d–
matrix such that λi,j ≥0 and

i≤m
λi,j = 1
for each j ≤d. Let Z1, . . . , Zm be iid random variables with common exponential
df G2,−1(x) = exp(x), x < 0. Then,
MΛ(x)
=
P

max
i≤m{Zi/λi,j} ≤xj, j = 1, . . . , d

=
exp
 
i≤m
min
j≤d(λi,jxj)

,
x < 0,
(12.13)
is a d–variate max–stable df with univariate margins G2,−1. In (12.12) there is a
special case for d = 2 with λ1,3 = λ2,2 = 0.
Such dfs are also of theoretical interest (see [16], page 111) for a general
representation of max–stable distributions.
H¨usler–Reiss Triangular Arrays
This model is constructed by taking limiting dfs of maxima of a triangular array
of standard normal random vectors. The univariate margins are Gumbel dfs. We
start with the bivariate case. Let Φ again denote the univariate standard normal
df. For positive parameters λ we have
Hλ(x, y)
=
exp

−e−x −
 ∞
y
Φ
 1
λ + λ(x −z)
2

e−z dz

(12.14)
=
exp

−Φ
 1
λ + λ(x −y)
2

e−y −Φ
 1
λ + λ(y −x)
2

e−x

,
where the second equality may be established by partial integration.
In addition, let
H0(x, y) = G0(x)G0(y)
and
H∞(x, y) = G0(min{x, y})
which are the dfs describing the cases of independence and total dependence. We
have
Hλ(x, y) →H0(x, y)
and
Hλ(x, y) →H∞(x, y),
λ →∞,
and, therefore, this model ranges continuously between independence and total
dependence with λ indicating the degree of dependence.

296
12. Multivariate Maxima
The bivariate EV dfs Hλ in (12.14) generally occur as limiting dfs of maxima
of iid random vectors (Xn,i, Yn,i), i = 1, . . . , n, where
(Xn,i, Yn,i)
d=

S1, ρnS1 +
'
1 −ρ2nS2

(12.15)
with ρn ↑1 at a certain rate as n →∞and (S1, S2) being a spherical random
vector, under the condition that the radius R =
'
S2
1 + S2
2 is in the max–domain
of attraction of the Gumbel df G02. Notice that the (Xn,i, Yn,i) are elliptically
distributed, see Section 11.2. Further related results may be found in another
article by Hashorva3.
3–D plots of the density of Hλ with parameter λ = 1 are displayed in Fig.
12.2 with views from two diﬀerent angles. On the left–hand side, one recognizes
that hλ is symmetric in x and y, cf. also (12.14). After a rotation of 90◦, the typical
contours of the marginal Gumbel densities become visible.
Fig. 12.2. (left.) 3–D plot with a view of the density hλ along the main diagonal in the
(x, y) plane for λ = 1. (right.) The density shown again after a rotation of 90◦about the
z–axis.
We compute the density hλ(x, y) and the conditional df Hλ(y|x) for param-
eters 0 < λ < ∞. The density can be computed by taking partial derivatives of
Hλ, cf. (10.11). By employing the identity
ϕ

λ−1 + λ(x −y)/2

e−y = ϕ

λ−1 + λ(y −x)/2

e−x,
one gets
hλ(x, y)
=
Hλ(x, y)

Φ

λ−1 + λ(x −y)/2

Φ

λ−1 + λ(x −y)/2

e−(x+y)
+ λϕ

λ−1 + λ(y −x)/2

e−x/2

.
(12.16)
2Hashorva, E. (2005). Elliptical triangular arrays in the max–domain of attraction of
H¨usler–Reiss distribution. Statist. Probab. Lett. 72, 125–135.
3Hashorva, E. (2006). On the max–domain of attraction of bivariate elliptical arrays.
Extremes 8, 225–233.

12.1. Nonparametric and Parametric Extreme Value Models
297
The conditional density is
hλ(y|x) = hλ(x, y)/g0(x) ,
cf. (8.10). By integration, one obtains the conditional df
Hλ(y|x) = Hλ(x, y)Φ (1/λ + λ(y −x)/2) /G0(x) .
(12.17)
The conditional density and df are useful for generating data in two steps:
ﬁrstly, generate x under the Gumbel df G0 and, secondly, generate y under the
conditional df Hλ(· |x).
There is a one–to–one relation between the dependence parameter λ and the
correlation coeﬃcient ρ of Hλ which is indicated in Fig. 12.3. We see that the
correlation coeﬃcient is practically zero when the shape parameter λ is smaller
than .3.
2
4
1
Fig. 12.3. Correlation coeﬃcient ρ
of Hλ plotted against the shape pa-
rameter λ.
Generally, let4
HΛ(x) = exp

−

k≤d
 ∞
xk
ΦΣ(k)

λ−1
i,k + λi,k(xi −z)/2
k−1
i=1

e−z dz

,
for a symmetric d × d–matrix Λ =

λi,j

with λi,j > 0 if i ̸= j and λi,i = 0, and
ΦΣ(k) is a (k −1)–variate normal df (with the convention that ΦΣ(1) = 1). The
mean vector of ΦΣ(k) is zero and Σ(k) =

σi,j(k)

is a correlation matrix given by
σi,j(k) =
⎧
⎨
⎩
λi,kλj,k

λ−2
i,k + λ−2
j,k −λ−2
i,j

/2
1 ≤i < j ≤k −1;
if
1
i ̸= j.
4Such a representation is given in Joe, H. (1994). Multivariate extreme–value distri-
butions with applications to environmental data. Canad. J. Statist. 22, 47–64. Replacing
ΦΣ(k) by survivor functions (cf. (10.4)), one obtains the original representation.

298
12. Multivariate Maxima
Each pair of marginal random variables determines one parameter of the limiting
df so that the dimension of the parameter vector is d(d −1)/2. For d = 2, the
matrix Λ is determined by λ = λ1,2 and one gets the bivariate case.
Extending and Modifying Standard Models
Let Z = (Z1, . . . , Zd) be a random vector distributed according to one of the
preceding multivariate EV dfs G. Denote by G0 the common univariate df of the
Zj. Thus, G0 is either an exponential or Gumbel df.
• (Location and Scale Vectors.) A ﬁrst natural extension of the models is
obtained by including location and scale parameters µj and σj. Thus, one
deals with random variables µj + σjZj with joint df 	G(x) = G ((x −µ)/σ)
(cf. page 163).
• (Exchanging Marginal Dfs.) Next the marginal df G0 is replaced by some
other univariate EV df G(j) in the jth component. First remember that the
Yj = G0(Zj) are (0, 1)–uniformly distributed with common df
C(y) = G

G−1
0 (y1), . . . , G−1
0 (yd)

which is the copula of G, cf. page 275.
Applying the quantile transformation one arrives at random variables Xj =
G−1
(j)(Yj) with marginal dfs G(j) and joint multivariate EV df
	G(x)
=
C

G(1)(x1), . . . , G(d)(xd)

=
G

G−1
0 (G(1)(x1)), . . . , G−1
0 (G(d)(xd))

.
(12.18)
Using the latter approach, one may build multivariate EV models with spec-
iﬁed univariate dfs such as, e.g., a H¨usler–Reiss model with Gumbel (EV 0) or EV
margins.
As an example, the bivariate, standard H¨usler–Reiss df Hλ is transformed
to a df 	Hλ with exponential margins G(j)(x) = G2,−1(x) = exp(x) for x < 0.
Because G0(x) = exp

−e−x
is the standard Gumbel df, we have G−1
0 (G(j)(x)) =
−log(−x) and, therefore,
	Hλ(x, y) = exp

Φ
 1
λ + λ
2 log
y
x

y + Φ
 1
λ + λ
2 log
x
y

x

(12.19)
for x, y < 0, with 	H0(x, y) = exp(x + y) and 	H∞(x, y) = exp(min{x, y}).
The Pickands Dependence Function for Distributions
A bivariate EV df with exponential marginals G2,−1(x) = exp(x), x < 0, has the
representation (cf. [20], 2nd edition, or [16]))
G(x, y) = exp

(x + y)D (x/(x + y))

,
x, y ≤0,
(12.20)

12.1. Nonparametric and Parametric Extreme Value Models
299
where D : [0, 1] →[0, 1] is the Pickands dependence function which is convex and
satisﬁes max(1−z, z) ≤D(z) ≤1 for 0 ≤z ≤1 with the properties of independence
and complete dependence being characterized by D = 1 and, respectively, D(z) =
max(1 −z, z).
We specify the explicit form of the Pickands dependence function for our
standard examples.
• Marshall–Olkin df Mλ:
Dλ(z) = 1 −λ min(z, 1 −z) .
(12.21)
• H¨usler–Reiss df 	Hλ in the version (12.19):
Dλ(z) = Φ
 1
λ + λ
2 log

z
1 −z

z + Φ
 1
λ + λ
2 log
1 −z
z

(1 −z).
(12.22)
Recall that the Pickands dependence functions are convex and satisﬁes
max(1 −z, z) ≤D(z) ≤1
for 0 ≤z ≤1.
The value D(1/2) of the Pickands dependence function will be used to deﬁne a
certain canonical representation.
Generally, a max–stable, d–variate df with univarite margins G−2,1 has the
representation
G(x) = exp
 
S
min
j≤d(yjxj) dµ(y)

,
x < 0,
(12.23)
where µ is a ﬁnite measure on the d–variate unit simplex S =

y : 
j≤d yj =
1, yj ≥1

such that

S yj dµ(y) = 1 for j ≤d.
The Canonical Parameterization
For any bivariate EV df G with standard exponential margins G2,−1, the Pickands
dependence function DG satisﬁes 1/2 ≤DG(1/2) ≤1. Particularly, DG(1/2) = 1
and DG(1/2) = 1/2, if independence and, respectively, complete dependence holds.
Deﬁne the functional (canonical) parameter
T (G) = 2

1 −Dλ(1/2)

(12.24)
which ranges between 0 and 1. Necessarily, T (G) = 0 and T (G) = 1, if indepen-
dence and, respectively, complete dependence holds.
Suppose that there is a parametric family of bivariate EV dfs Gλ with a one–
to–one relation between λ and Dλ(1/2). One may introduce another representation
of the family by
ϑ = T (Gλ) = 2

1 −Dλ(1/2)

.
(12.25)

300
12. Multivariate Maxima
This is the canonical parameterization of a bivariate EV family5 which will be
studied more closely on the subsequent pages. In special cases, we obtain the
following canonical parameters.
• Marshall–Olkin: ϑ = λ;
• H¨usler–Reiss: ϑ = 2(1 −Φ(1/λ)).
Notice that the original and the canonical parameterization are identical
for the Marshall–Olkin family (also see Section 12.2 for the Gumbel–McFadden
model).
Canonical and Tail Dependence Parameters
Let again G be a bivariate EV df with Pickands dependence function D and
canonical (functional) parameter T (G) = 2(1 −D(1/2)). An operational meaning
of the canonical parameter can be expressed by means of a property of the survivor
function G in the upper tail region. Applying (10.7) we get
G(u, u)
=
1 −2 exp(u) + exp(2uD(1/2))
=
T (G)|u| + O(u2),
u →0.
(12.26)
Let X and Y be random variables with common EV df G. For the tail de-
pendence parameter χ(q) at the level q, introduced in (2.60), one gets
χ(q)
=
P

Y > log q
X > log q

=
G(log q, log q)/(1 −q)
=
T (G) + O(1 −q),
q →1,
(12.27)
according to (12.26). Thus, the canonical parameter T (G) of a bivariate EV df is
the tail dependence parameter χ, cf. (2.61). This topic will be continued in Section
13.3.
Multivariate Minima, Limiting Distributions of Minima
and the Min–Stability
Corresponding to the maximum of vectors xi = (xi,1, . . . , xi,d), one may take
the componentwise minimum mini≤m xi =

mini≤m xi,1, . . . , mini≤m xi,d

. Let
X1, . . . , Xm be iid random vectors with common df F and survivor function F.
The survivor function of the minimum is given by
P

min
i≤m Xi > x

= F
m(x).
5Falk, M. and Reiss, R.–D. (2001). Estimation of the canonical dependence parameter
in a class of bivariate peaks–over–threshold models. Statist. Probab. Letters 52, 9–16

12.2. The Gumbel–McFadden Model
301
Moreover, the representation mini≤m Xi = −maxi≤m(−Xi) holds again. As
in the univariate case, cf. (1.38) and (1.38), there is a one–to–one relationship
between limiting dfs of maxima and minima. If
P

max
i≤m(−Xi) ≤bm + amx

→G(x),
m →∞,
then for cm = am and dm = −bm we have
P

min
i≤m Xi ≤dm + cmx

→G(−x),
m →∞.
We see that the limiting dfs 	G of sample minima are necessarily of the form
	G(x) = G(−x). Conversely, G(x) can be written as the survivor function of 	G
applied to −x.
Limiting dfs of minima can be characterized again by the min–stability. The
survivor function F (and, thus, the df F) is min–stable, if F
m(dm +cmx) = F(x)
for certain vectors dm and cm > 0.
12.2
The Gumbel–McFadden Model
We ﬁrst deal with certain bivariate dfs which are the Gumbel type II dfs as men-
tioned in [20], page 247, in the form of a min–stable df. D–variate versions are
introduced in (12.32) and (12.33). The section is concluded with an application to
utility–maximizing.
Bivariate Gumbel Type II Distributions
This is another model of EV dfs with exponential margins G2,−1(x) = exp(x), x <
0,. The bivariate Gumbel type II dfs Hλ are parameterized by a dependence pa-
rameter λ which ranges from 1 to inﬁnity. We have independence for λ = 1 and
total dependence for λ = ∞. Let
Hλ(x, y) = exp

−

(−x)λ + (−y)λ1/λ
,
x, y < 0.
(12.28)
Notice that H1(x, y) = exp(x) exp(y) and Hλ(x, y) →exp(min{x, y}) =:
H∞(x, y) for x, y < 0 as λ →∞. Taking partial derivatives one gets the density
hλ(x, y)
=
Hλ(x, y)(xy)λ−1
(−x)λ + (−y)λ2(1/λ−1)
+ (λ −1)

(−x)λ + (−y)λ1/λ−2
.
(12.29)
Several distributional properties of Hλ can be conveniently deduced from a
representation in terms of independent random variables (due to L. Lee6 and J.–C.
6Lee, L. (1979). Multivariate distributions having Weibull properties. J. Mult. Analysis
9, 267–277.

302
12. Multivariate Maxima
Lu and G.K. Bhattacharya7).
Let U, V1, V2 and Zλ be independent random variables, where U is uni-
formly distributed on [0, 1], the Vj have the common exponential df G2,−1(x) =
exp(x), x < 0, and Zλ is a discrete random variable with P{Zλ = 0} = 1 −1/λ
and P{Zλ = 1} = 1/λ. Let V = V1 + ZλV2.
Random variables X1 and X2 with joint df Hλ have the representation
(X1, X2) =

U 1/λV, (1 −U)1/λV

in distribution.
(12.30)
Moments of Gumbel–McFadden distributions can be easily deduced from
(12.30). In particular, there is a one–to–one relation between the dependence pa-
rameter λ and the correlation coeﬃcient ρ(λ) of Lλ. One gets
ρ(λ) = 2Γ2(1 + 1/λ)/Γ(1 + 2/λ) −1.
This function is plotted in Fig. 12.4.
shape parameter
correlation coefficient
1
2
3
4
5
0.0
0.5
1
Fig. 12.4. Correlation coeﬃcient ρ
of Hλ plotted against the shape pa-
rameter λ.
The Pickands dependence function of the Gumbel df Hλ is
Dλ(z) =

(1 −z)λ + zλ1/λ
.
(12.31)
The canonical parameter is given by ϑ = 2 −21/λ.
7Lu, J.–C. and Bhattacharya, G.K. (1991). Inference procedures for bivariate expo-
nential model of Gumbel. Statist. Probab. Letters 12, 37–50.

12.2. The Gumbel–McFadden Model
303
Gumbel–McFadden Distributions
Certain d–variate extensions of the Gumbel distributions are due to Daniel Mc-
Fadden8. We have
Hλ,d(x) = exp

−
 
i≤d
(−xi)λ1/λ
,
x < 0,
(12.32)
and as another extension
G(x) = exp

−

k≤m
ak
 
i
(−xi)λ(k)1/λ(k)
,
x < 0,
(12.33)
where i varies over Bk ⊂{1, . . . , d}, *
k≤m Bk = {1, . . ., d}, ak > 0, and λ(k) ≥1.
To get standard exponential margins in the second case, the additional con-
dition 
k≤m akI(i ∈Bk) = 1 must be satisﬁed for i = 1, . . . , d.
In the following lines it is shown that Gumbel–McFadden dfs Hλ,d lead to a
multinomial logit model in the utility–maximizing theory. In that application, EV
dfs are convenient because one gets an elegant representation for certain choice
probabilities.
An Application to Utility–Maximizing
It is the common situation that consumers can choose between diﬀerent alterna-
tives as, e.g., between diﬀerent
• residential locations8 described by certain conﬁgurations of attributes such
as accessibility, quality of public services, neighborhood, etc.
• residential end–use energy conﬁgurations9.
It is understood that a consumer chooses that conﬁguration which provides a
maximum utility.
The utility of a conﬁguration, labeled by the index i, will be expressed by
Ui = vi + Xi,
i = 1, . . . , d,
where vi is a function of the above mentioned attributes and of consumer’s char-
acteristics (family size, income), and Xi is an unobserved random variable.
8 McFadden, D. (1978). Modelling the choice of residential location. In: Spatial Inter-
action Theory and Planning Models, pp. 75–96, A. Karlquist et al. (eds.), North Holland,
Amsterdam. Reprinted in The Economics of Housing, Vol. I, pp. 531–552, J. Quigley (ed),
Edward Elgar, London, 1997.
9Cowing, T.G. and McFadden, D.L. (1984). Microeconomic Modeling and Policy Anal-
ysis. Academic Press, Orlando.

304
12. Multivariate Maxima
The probability of the choice of the conﬁguration with label i is
pi = P{Ui > Uj for j = 1, . . . , d with j ̸= i}
which can be expressed by means of the ith partial derivative of the joint df F of
the X1, . . . , Xd. We have
pi =

∂
∂xi
F

(vi + z −vj)j≤d

dz .
(12.34)
This formula will be applied to dfs F = G, where G are EV dfs with univariate
Gumbel margins.
Let G(x) = exp(−Ψ(x)) be an EV df with univariate exponential margins.
From (12.20) and (12.23) we know that the auxiliary function Ψ is homegeneous
of order 1, that is
Ψ(ay) = aΨ(y),
y < 0, a > 0,
(12.35)
which implies that the partial derivative is homogeneous of order 0, that is
∂
∂xi
Ψ(ay) =
∂
∂xi
Ψ(y),
y < 0, a > 0.
In addition, EV dfs with univariate Gumbel margins are of the form
G(x) = exp

−Ψ

(−e−xj)j≤d

,
cf. (12.18).
Using the homogeneity properties one gets
pi
=

(−e−z) ∂
∂xi
Ψ

(−e−(vi+z−vj))j≤d

exp

−Ψ

(−e−(vi+z−vj))j≤d

dz
=

(−e−z) ∂
∂xi
Ψ

(−evj)j≤d

exp

−e−(vi+z)Ψ

(−evj)j≤d

dz
=

−evi ∂
∂xi
Ψ

(−evj)j≤d

Ψ

(−evj)j≤d

.
(12.36)
In the special case of a Gumbel–McFadden df Hλ,d one gets the multinomial logit
form
pi = eviλ 
j≤d
evjλ
for the choice probabilities pi.
We have seen that the modeling of the random disturbances in utilities by
means of EV distributions leads to attractive formulas for the choice probabilities.

12.3. Estimation in Extreme Value Models
305
12.3
Estimation in Extreme Value Models
A multivariate EV distribution is determined by the parameters (shape, location
and scale parameter) in the single components and by the parameters of the copula
or by the copula itself in the nonparametric approach.
The ﬁrst class of parameters can be estimated by means of the methods
presented in Part II; new estimation procedures must be developed for the second
problem.
Estimation by Piecing–Together from Lower–Dimensional Margins
Let xi = (xi,1, . . . , xi,d), i = 1, . . . , k, be governed by the d–variate EV df G that
is determined by the location, scale and shape parameter vectors µ, σ and γ and,
in addition, by the copula C.
From (12.18) we know that G has the representation
G(x) = C

Gγj,µj,σj(xj)

j≤d

for vectors x = (x1, . . . , xd), where Gγj,µj,σj is the jth marginal df of G. If C ≡Cλ
is given in a parametric form, then we speak of a dependence parameter λ.
To estimate G one must construct estimates of µ, σ, γ and C (with C replaced
by λ in the parametric case). One can easily ﬁnd estimates of µ, σ and γ by taking
estimates in the single components. Notice that the data x1,j, . . . , xk,j in the jthe
component are governed by the jthe marginal df Gγj,µj,σj of G. Therefore, one
may take estimates of the parameters γj, µj and σj as introduced in Section 4.1.
The deﬁnition of the copula C suggests to base the estimation of C or the
unknown dependence parameter λ on the transformed vectors
zi =

Gˆγj,k,ˆµj,k,ˆσj,k(xi,j)

j≤d,
i = 1, . . . , k.
(12.37)
Notice that the zi only depend on the xi. Subsequently, the zi are regarded
as vectors that are governed by C. Let Ck be an estimate of C based on z1, . . . , zk
within the copula model. If C is of the parametric form Cλ, then construct an
estimate ˆλk of the parameter λ.
Thus, the piecing–together method yields estimates ˆγk, ˆµk, ˆσk and ˆλk of γ,
µ, σ and λ or, alternatively, the estimate
Gk(x) = Ck

Gˆγj,k,ˆµj,k,ˆσj,k(xj)

j≤d

of the EV df G, where Ck can be of the form Cˆλk
10.
10For supplementary results concerning multivariate EV models see [16] and
Tiago de Oliveira, J. (1989). Statistical decisions for bivariate extremes. In [14], pp.
246–261, or Smith, R.L., Tawn, J.A. and Yuen, H.K. (1990). Statistics of multivariate
extremes. ISI Review 58, 47–58.

306
12. Multivariate Maxima
The Pickands Estimator in the Marshall–Olkin Model
We give a simple estimate of the dependence parameter λ in the model of standard
Marshall–Olkin dfs, namely, for (x1, y1), . . . , (xk, yk),
ˆλk = 2 + k
 
i≤k
max{xi, yi}.
(12.38)
This estimate can be made plausible in the following way: from (12.11), de-
duce that (2 −λ) max{X, Y } is an exponential random variable with df G2,−1, if
(X, Y ) has the Marshall–Olkin df Mλ. Therefore,
1
k

i≤k
max{xi, yi} ≈E max{X, Y } = −
1
2 −λ
and ˆλk ≈λ. This yields estimates within an enlarged model.
Example 12.3.1. (American Football (NFL) Data.) We partially repeat the analysis by
Cs¨org˝o and Welsh11. The data were extracted from game summaries published in the
Washington Post newspaper during three consecutive weekends in 1986. Consider the
random game times to the ﬁrst
• ﬁeld goal (denoted by U);
• unconverted touchdown or safety (denoted by V ),
• point–after touchdown (denoted by W ).
Because the game time of the conversion attempt after a touchdown is zero, we have
X = min{U, W }:
game time to the ﬁrst kicking of the ball between the goalposts;
Y = min{V, W }:
game time to the ﬁrst moving of the ball into an endzone.
Notice that X = Y if the ﬁrst score is a point–after touchdown which happens with a
positive probability. Assuming for a while that the random variables U, V, W are expo-
nential it is reasonable to assume that (X, Y ) has a Marshall–Olkin distribution. Table
12.1 contains the ﬁrst and last three data vectors.
Table 12.1. Scoring times (minutes : seconds) from 42 American Football games.
x:
2:03
9:03
0:51
· · ·
19:39
17:50
10:51
y:
3:59
9:03
0:51
· · ·
10:42
17:50
38:04
The data (xi, yi), expressed in decimal minutes, are stored in the ﬁle football.dat.
Because we introduced the Marshall–Olkin distributions in the version for maxima, we
ﬁrst change the signs of the data. Estimation in the EV model shows that a Weibull
modeling for the single components is adequate, yet with α ̸= −1. The MLE(EV1)
procedure provides the parameters α1 = −1.39, µ1 = 0, σ1 = 9.92 and α2 = −1.18,
11Cs¨org˝o, S. and Welsh, A.H. (1989). Testing for exponential and Marshall–Olkin dis-
tributions. J. Statist. Plan. Inference 23, 287–300.

12.3. Estimation in Extreme Value Models
307
µ2 = 0, σ2 = 14.34 in the ﬁrst and second component. Thus, we have Weibull components,
yet we still assume a bivariate Marshall–Olkin structure.
Check that −(−X/σ)−α has the exponential df G2,−1 if X has the df G2,α,0,σ.
Therefore, a modeling by standard Marshall–Olkin dfs is more adequate for the trans-
formed data
(x′
i, y′
i) =

−(−xi/12)1.2, −(−yi/12)1.2
,
i = 1, . . . , 42.
(12.39)
Based on (x′
i, y′
i) one obtains the estimate ˆλ = 0.63 for the dependence parameter λ.
Finally, the converse transformations provide an estimated df for the original data.
An application of the ML method is a bit more complicated, because one
must deal with more sophisticated densities, namely densities with respect to the
sum of the 2–dimensional Lebesgue measure on the plane and the 1–dimensional
Lebesgue measure on the main diagonal.
Estimation in the Gumbel–McFadden and H¨usler–Reiss Models
First, specify a model for the univariate margins. We will primarily deal with PTEs
(piecing–together estimates) so that ﬁrst the parameters in the univariate margins
are estimated by means of one of the estimation methods in Chapter 4. After a
transformation, as explained in (12.37), we may assume that the data are governed
by the df Lλ or Hλ.
Thus, an estimate of the dependence parameter λ or the canonical parameter
ϑ must be constructed.
• Moment Method: because of the one–to–one relationship between the depen-
dence parameter λ and the correlation coeﬃcient ρ, one obtains an estimate
of λ based on the sample correlation coeﬃcient.
• Pickands Estimator: assume that the dfs G are given in the Pickands rep-
resentation (12.20). Deduce that ((2 −T (G)) max(X, Y ) is an exponential
random variable with exponential df G2,−1, where T (G) is the canonical
parameter. Therefore, generalizing (12.38) we conclude that
T(G)k = 2 + k
 
i≤k
max{xi, yi}
(12.40)
is an estimator of the functional parameter T (G), which also provides an
estimator of the canonical parameter ϑ within the parametric framework.
• Maximum Likelihood Method: from the density in (12.29) or (12.16), deduce
the likelihood equation and calculate the MLE numerically. The preceding
moment or Pickands estimate may serve as the initial value of the iteration
procedure.

308
12. Multivariate Maxima
Such PTEs may also serve as initial values of an iteration procedure to eval-
uate the MLE in the full bivariate model.
Example 12.3.2. (Ozone Concentration in the San Francisco Bay Area.) We partially
repeat the analysis of ozone concentration in the aforementioned article by Harry Joe4.
The data set (stored in the ﬁle cm–ozon1.dat) consists of 120 weekly maxima of hourly
averages of ozone concentrations measured in parts per hundred million for the years
1983–1987 for each of the ﬁve monitoring stations (Concord (cc), Pittsburg (pt), San
Jose (sj), Vallejo (va), Santa Rosa (st)). Weeks within the months April to September
are only taken, because there are smaller maxima in the winter months.
The ﬁrst 21 maxima are listed in Table 12.2 to give a ﬁrst impression of the data.
Table 12.2. The ﬁrst 21 maxima at 5 stations.
No.
cc
pt
sj
va
st
No.
cc
pt
sj
va
st
No.
cc
pt
sj
va
st
1.
7
6
6
6
5
8.
10
11
11
8
5
15.
13
8
15
8
7
2.
6
7
5
5
4
9.
7
7
6
4
4
16.
8
8
8
5
4
3.
6
7
8
5
6
10.
11
14
13
13
6
17.
7
8
6
3
4
4.
6
6
5
4
4
11.
12
10
12
8
5
18.
9
8
8
4
5
5.
6
6
5
4
4
12.
7
7
7
4
4
19.
13
11
14
12
7
6.
5
6
5
4
4
13.
8
8
8
5
5
20.
15
13
13
10
7
7.
9
9
7
7
5
14.
8
7
7
5
4
21.
5
6
6
4
4
Next, MLEs for the Gumbel (EV0) and the full EV model are calculated for the
single components. The estimates suggest to take a Gumbel modeling for the margins.
This is supported by a nonparametric visualization of the data. We have not analyzed
any other data in this book with a better ﬁt to a parametric model (perhaps with the
exception of Michelson’s data concerning the velocity of light in the air). Of course the
stronger discrete nature of the data becomes visible.
Table 12.3. MLE(EV0) and MLE(EV) for the single stations.
Univariate Parameters
cc
pt
sj
γ
µ
σ
γ
µ
σ
γ
µ
σ
MLE(EV0)
0
7.21
2.02
0
6.81
1.62
0
7.23
2.13
MLE(EV)
−0.06
7.28
2.06
0.06
6.76
1.58
0.00
7.23
2.13
va
st
γ
µ
σ
γ
µ
σ
MLE(EV0)
0
5.40
1.66
0
4.54
1.10
MLE(EV)
0.12
5.29
1.58
0.11
4.48
1.05
We also include in Table 12.4 the estimated pairwise dependence parameters λ at
the 5 diﬀerent stations.

12.4. A Spectral Decomposition Methodology
309
Dependence Parameter λ
cc
pt
sj
va
pt
2.7
sj
1.9
1.5
va
1.7
1.6
1.5
st
1.3
1.2
1.2
1.6
Table 12.4. MLEs of pairwise de-
pendence parameters λ.
As it was already reported in the article by H. Joe4 that the H¨usler–Reiss modeling
is adequate for the present data set. This judgement is supported by 3–D plots in Fig. 12.5
of a bivariate kernel density for the cc–pt data (38 distinct pairs with certain numbers
of multiplicities) and the estimated parametric density.
0
5
10
15
x
0
5
10
15
y
0
5
10
15
x
0
5
10
15
y
0
5
10
15
x
0
5
10
15
y
0
5
10
15
x
0
5
10
15
y
Fig. 12.5. Bivariate kernel density (left) and parametric density (right) for cc–pt data.
What has been achieved in the multivariate framework is promising yet this
approach is still in a research stadium.
12.4
A Spectral Decomposition Methodology
We merely state some basic properties and results about a certain spectral decom-
position in the special case of bivariate distributions. For a continuation we refer to
Section 13.3. A detailed account of this methodology, including the d–dimensional
case, is given in Falk et al. [17].
A Spectral Decomposition
Any vector (x, y) ∈(−∞, 0]2 with (x, y) ̸= (0, 0) can be uniquely represented by
means of the angular and radial components z and c with z = x/(x + y) ∈[0, 1]
and c = x + y ≤0. We have
(x, y) = (cz, c(1 −z)).
(12.41)

310
12. Multivariate Maxima
Any df H with support (−∞, 0]2 may be written in the form H(cz, c(1 −z)).
Putting
Hz(c) = H(cz, c(1 −z)),
z ∈[0, 1], c ≤0,
(12.42)
one gets a one–to–one representation of H by means of the family of univariate
dfs {Hz : z ∈[0, 1]} which we call spectral decomposition of H. Check that H0
and H1 are the marginal dfs of H in the 1st and 2nd component.
First, we provide two examples where H is the df of independent exponen-
tially and, respectively, uniformly distributed random variables:
(a) if H(x, y) = exp(x + y), x, y ≤0, then
Hz(c) = exp(c),
c ≤0,
(b) if H(x, y) = (1 + x)(1 + y), −1 ≤x, y ≤0, then
Hz(c) = 1 + c + c2z(1 −z),
c ≤0, 1 + c + c2z(1 −z) ≤0.
Next we extend (a) to bivariate EV dfs in general.
(c) A bivariate EV df G with Pickands dependence function D, cf. (12.20), has
the spectral dfs
Gz(c) = exp(cD(z)),
c ≤0.
Notice that Gz is an exponential df with reciprocal scale parameter D(z). In
particular, the spectral dfs are univariate EV dfs.
We also make use of the partial derivative
hz(c) = ∂
∂cHz(c).
(12.43)
In the preceding examples (a)–(c) we have
(a′) hz(c) = exp(c),
c ≤0;
(b′) hz(c) = 1 + 2cz(1 −z),
c ≤0, 1 + c + c2z(1 −z) ≤0,
(c′) gz(c) = D(z) exp(cD(z)),
c ≤0.
A Spectral Condition
Assume that H is a bivariate df with support (−∞, 0] such that
Hz(c) = 1 + cg(z)(1 + o(1)),
c ↑0, z ∈[−1, 0],
(12.44)
where g(0) = g(1) = 1. Then, the following assertions hold, cf. [17], Theorem 5.3.2,
which is formulated for the d–variate case:

12.4. A Spectral Decomposition Methodology
311
• g(z) = D(z) is a Pickands dependence function,
• H is in the domain of attraction of the EV df G with Pickands dependence
function; more precisely,
Hn x
n, y
n

→G(x, y),
n →∞.
It is apparent that condition (12.44) is satisﬁed with D(z) = 1 in the cases
(a) and (b), and for D(z) in the general case (c) of EV dfs. Because of the stan-
dardization D(0) = D(1) = 1 we get for the marginal dfs
H0(c) = 1 + c(1 + o(1))
and
H1(c) = 1 + c(1 + o(1)).
Therefore, it suggests itself to transform original data to (−1, 0)–uniform or expo-
nential data before applying results based on this condition.
Condition (12.44) can be veriﬁed by the following “diﬀerentiable” version: if
hz(c) > 0 for c close to zero, z ∈[0, 1], and
hz(c) = g(z)(1 + o(1)),
c ↑0, z ∈[0, 1],
(12.45)
where g(0) = g(1) = 1, then condition (12.44) holds.
It is evident that condition (12.45) is satisﬁed for the examples in (a′)–(c′) at
a certain rate. In Section 13.3 we formulate an expansion for hz(c) which speciﬁes
the rate at which (12.45) is attained. This expansion is applied to the testing of tail
dependence against tail independence. Section 13.3 also provides further examples
for which condition (12.45) is satisﬁed.
The Spectral Decomposition of a GP
Distribution Function
The spectral decomposition of a bivariate GP df W = 1 + log G, which will be
introduced in Section 13.1, consists of uniform dfs, we have
Wz(c) = 1 + cD(z),
−1/D(z) ≤c ≤0,
for 0 ≤z ≤1, where D is again the pertaining Pickands dependence function.
Thus, the univariate margins of bivariate GP dfs are again GP dfs and, more
general, the spectral dfs of bivariate GP dfs are GP dfs.
The Random Angular and Radial Components
Let (X, Y ) be a random vector with joint df H which has the support in (−∞, 0]2.
Then, the vector
(X/(X + Y ), X + Y )

312
12. Multivariate Maxima
may be called Pickands transform with the angular and radial component in the
1st and 2nd component (cf. Falk et al. [17], pages 150–153).
It turns out that the angular and radial components have remarkable, stochas-
tic properties. They are conditionally independent conditioned on X + Y > c
for GP dfs W under mild conditions. We have unconditional independence if
one takes a diﬀerent version of the GP df, namely that truncated outside of
{(x, y) ∈(−∞, 0]2 : x + y > c} with c > −1. Moreover, the asymptotic condi-
tional independence holds for dfs in a neighborhood of W.
This result for GP distributions is closely related to that for spherical distri-
butions, see Section 11.2. In that section, the radial component is the L2–norm of
the spherical random vector, whereas the radial component X + Y in the present
section is related to the L1–norm.
The testing of tail dependence against tail independence in Section 13.3 will
be based on the radial component.

Chapter 13
Multivariate Peaks Over
Threshold
co–authored by M. Falk1
We already realized in the univariate case that, from the conceptual viewpoint, the
peaks–over–threshold method is a bit more complicated than the annual maxima
method. One cannot expect that the questions are getting simpler in the multi-
variate setting. Subsequently, our attention is primarily restricted to the bivariate
case, this topic is fully worked out in [16], 2nd ed., for any dimension. A new result
about the testing of tail dependence is added in Section 13.3.
13.1
Nonparametric and Parametric
Generalized Pareto Models
We introduce bivariate generalized Pareto (GP) distributions and deal with the
concepts of canonical and tail–dependence parameters within this framework.
Introducing Bivariate GP Distributions
In analogy to the univariate case, deﬁne a bivariate GP distribution2 pertaining
to an EV df G by
W(x, y) = 1 + log G(x, y),
if log G(x, y) ≥−1.
(13.1)
1Katholische Universit¨at Eichst¨att; now at the University of W¨urzburg.
2Kaufmann, E. and Reiss, R.–D. (1995). Approximation rates for multivariate ex-
ceedances. J. Statist. Plan. Inf. 45, 235–245; see also the DMV Seminar Volume [16].

314
13. Multivariate Peaks Over Threshold
The univariate margins of the bivariate GP df W are just the univariate GP dfs
pertaining to the margins of the EV df G.
If G is an EV df such that the Pickands representation (12.20) holds, then
W(x, y) = 1 + (x + y)D

y
x + y

,
if (x + y)D

y
x + y

> −1,
(13.2)
where D is the Pickands dependence function of G. In this case, the univariate
margins of W are both the uniform df W2,−1 = 1 + x on [−1, 0].
Because D ≤1, we know that the condition log G(x, y) ≥−1 is satisﬁed if
x + y ≥−1. In the subsequent calculations, we assume that the latter condition is
satisﬁed to avoid technical complications.
If &
W is any bivariate GP df with margins &
W1 and &
W2, then
W(x, y) = &
W
&
W −1
1
(1 + x), &
W −1
2
(1 + y)

,
−1 < x, y < 0,
(13.3)
is of the standard form (13.2).
Tajvidi’s Deﬁnition of Bivariate GP Distributions
The deﬁnition of a bivariate GP df is not as self–evident as in the univariate case.
We mention the relation of the preceding deﬁnition of a GP df to another one.
Consider the map
m(u,v)(x, y) =

max(x, u), max(y, v)

(13.4)
which replaces marginal values x < u and, respectively, y < v by the marginal
thresholds u and v as illustrated in Fig. 13.1.
u
v
Fig. 13.1. Putting points (x, y) to
m(u,v)(x, y).
We refer to Section 13.3, where the map mu,v is once again utilized in the
context of a point process approximation.

13.1. Nonparametric and Parametric Generalized Pareto Models
315
Let (X, Y ) be a random vector with GP df W as introduced in (13.1) and
let u, v be thresholds such that log G(u, v) = −1. Then, the random vector
m(u,v)(X, Y ) has the df
&
W(x, y) =
⎧
⎨
⎩
W(x, y)
(x, y) ≥(u, v)
if
0
otherwise
(13.5)
which is a GP df as deﬁned by Tajvidi3.
Thus, both deﬁnitions are closely related to each other. We remark that
• both distributions are identical in the region {(x, y) : x > u, y > v},
• in contrast to the GP df W, the modiﬁcation &
W does not possess the property
that the margins are again GP dfs.
In view of the latter remark, we prefer to deal with GP dfs as introduced in
(13.1). Using such dfs also facilitates transformations in the univariate margins.
Canonical and Tail Dependence Parameters
Now, let (X, Y ) be a random vector with df bivariate GP df W as given in (13.2).
Applying (10.7) one gets for −1/2 ≤u < 0,
P(Y > u|X > u)
=
2|u| −2|u|DW(1/2)
|u|
=
2

1 −DW (1/2)

=:
T (W),
(13.6)
where DW is the Pickands dependence function pertaining to W. We see that the
conditional probabilities do not depend on the threshold u.
Notice that T (W) is just the canonical parameter introduced in (12.25) for
bivariate EV dfs. Thus, for a parametric family of GP dfs Wϑ, for which there
is a one–to–one relation between the original parameter λ and T (Wλ), we may
introduce another representation by taking the canonical parameter
ϑ = T (Wλ).
It is apparent from (13.6) that the survivor function Wϑ satisﬁes
W(u, u)/|u| = T (W),
−1/2 ≤u ≤0.
(13.7)
3Tajvidi, N. (1996). Characterization and some statistical aspects of univariate and
multivariate generalised Pareto distributions. PhD Thesis, Dept. of Mathematics, Uni-
versity of G¨oteborg.

316
13. Multivariate Peaks Over Threshold
Recall from (12.26) that such a relation approximately holds for the pertaining
EV df.
In addition, we have
T (W) = χ(q),
q ≥1/2,
(13.8)
where χ(q) is the tail dependence parameter at the level q introduced in (2.60).
Parametric Models of Bivariate GP Distributions
Explicit representations of Wϑ, where ϑ is the canonical parameter, are given for
the bivariate Marshall–Olkin, Gumbel–McFadden and H¨usler–Reiss dfs.
• (Marshall–Olkin–GP dfs.) Because ϑ = λ, we obtain from (13.1) and (12.11),
Wϑ(x, y) = 1 + (1 −ϑ)(x + y) + ϑ min{x, y},
(13.9)
whenever x, y ≤0 and the right–hand side exceeds zero.
• (Gumbel–McFadden–GP dfs.) Because ϑ = 2 −21/λ, we have
λ = (log 2)/ log(2 −ϑ).
(13.1) and (12.28) yield
Wϑ(x, y) = 1 −

(−x)
log 2
log(2−ϑ) + (−y)
log 2
log(2−ϑ)
 log(2−ϑ)
log 2
,
(13.10)
whenever x, y ≤0 and the right–hand side exceeds zero.
• (H¨usler–Reiss–GP dfs.) Because ϑ = 2(1 −Φ(1/λ)), we have λ = 1/Φ−1(1 −
ϑ/2). Deduce from (13.1) and (12.19) that
Wϑ(x, y) = 1 + ψϑ
y
x

y + ψϑ
x
y

x,
(13.11)
where the auxiliary function ψϑ is deﬁned by
ψϑ(z) = Φ

Φ−1
1 −ϑ
2

+
1
2Φ−1 
1 −ϑ
2
 log(z)

,
0 < z < ∞,
whenever x, y < 0 and the right–hand side in (13.11) exceeds zero.
In all three cases we have W0(x, y) = 1+(x+y) and W1(x, y) = 1+min{x, y}.
It is remarkable that W0 is the uniform distribution on the line {(x, y) : x, y ≤
0, x + y = −1}.

13.1. Nonparametric and Parametric Generalized Pareto Models
317
The Canonical Dependence Function
We deﬁne the canonical dependence function of a bivariate GP df W by
TW (z) = 2(1 −DW (z)),
(13.12)
where DW is again the Pickands dependence function pertaining to W.
In conjunction with EV distributions, Huang4 studied the stable tail depen-
dence function
l(x, y) = (x + y)D

y
x + y

which is the function −Ψ(x, y) on page 304. For obvious reasons we prefer to work
with a real–valued function which varies between zero and one.
The value of the canonical dependence function TW (1/2) at z = 1/2 is the
canonical parameter T (W). We also write Tϑ etc., if there is a parametric family of
GP dfs reresented in the canonical parameterization. Given the canonical param-
eterization we have T0(z) = 0 and T1(z) = min(2z, 2(1 −z)), if tail independence
and, respectively, total tail dependence hold.
In Fig. 13.2 we plot several canonical dependence functions Tϑ for Gumbel–
McFadden and Marshall–Olkin dfs ranging from the case of tail independence to
the one of total dependence.
0
1
0.0
0.25
0.5
0.75
1
Fig. 13.2. Canonical dependence func-
tions Tϑ for Gumbel–McFadden (solid)
and Marshall–Olkin (dashed) distribu-
tion with canonical parameters ϑ =
0, 0.25, 0.5, 0.75, 1.
We provide a representation of the canonical dependence function TW by
means of the survivor function W which extends that for the canonical parameter,
see (13.7).
4 Huang, X. (1992). Statistics of bivariate extreme values. PhD Thesis, Erasmus Uni-
versity Rotterdam.

318
13. Multivariate Peaks Over Threshold
Let W be a standard bivariate GP df as given in (13.2). From the represen-
tation (10.7) of a survivor function deduce that
2W(−d(1 −z), −dz)/d = TW (z)
(13.13)
for 0 < d ≤1 and 0 ≤z ≤1. Notice that the formula (13.7) for the canonical
parameter T (W) = TW (1/2) is a special case. In addition, one must take |u| = d/2.
Here, z and d may be regarded as direction and distance measures.
Therefore, by estimating the survivor function W in (13.13)—for certain se-
lected values d which may depend on z—one gets estimates of the canonical de-
pendence function TW and the canonical parameter T (W).
13.2
Estimation of the
Canonical Dependence Function
In this section we estimate the canonical dependence function TW by means of
a sample version of the survivor function. Consequently, the actual df F can be
estimated in the upper tail region by the piecing–together method. The results
presented here should be regarded as a preliminary introduction5.
Estimation Within the Standard GP Model
Let (xi, yi) be governed by the standard GP df W as given in (13.2). In view of
(13.13) one may take
Tn(z) = 2
nd

i≤n
I(xi > −d(1 −z), yi > −dz)
(13.14)
as an estimate of the canonical dependence function TW (z), where 0 < d ≤1 can
be selected by the statistician. Spezializing this to z = 1/2 one gets by Tn(1/2) an
estimate of the canonical parameter T (W).
This approach to estimating the canonical dependence function includes the
case where the modeling of an actual survivor function F by means of W is suﬃ-
ciently accurate for x, y > u ≥−1. In that case, one must take d = |u|.
Transforming Univariate GP Margins
In contrast to the preceding lines we consider the more general situation of bivari-
ate GP dfs &
W having the GP margins &
W1 and &
W2 which are not necessarily equal
to the uniform df on the interval [−1, 0].
5For further results see Falk, M. and Reiss, R.–D. (2003). Eﬃcient estimators and
LAN in canonical bivariate pot models. J. Mult. Analysis 84, 190–207.

13.2. Estimation of the Canonical Dependence Function
319
1. Use estimated univariate GP distributions to transform the original data to
the standard form of [−1, 0]–uniform data;
2. estimate the canonical dependence function (the canonical parameter) as in
(13.14) based on the transformed data,
3. use the estimated canonical dependence function and univariate GP dfs to
construct an estimate of the GP df &
W.
This approach can be also applied to the situation, where univariate GP dfs
&
W1 and &
W2 are ﬁtted to the upper tails of actual univariate margins F1 and F2
above the thresholds v(1) and v(2). Assume that the bivariate GP modeling of the
actual bivariate distribution is valid for x, y > u = max(u(1), u(2)), where the u(i)
are the transformed thresholds v(i).
Arbitrary Margins
Let X and Y be random variables having continuous dfs FX and FY . Put
p(d, z) := P

X > F −1
X (1 −d(1 −z)), Y > F −1
Y (1 −dz)

.
(13.15)
A natural estimator of the probability p(d, z) is
ˆpn(d, z) = 1
n

i≤n
I

xi > x[n(1−d(1−z))]:n, yi > y[n(1−dz)]:n

,
where (xi, yi) are realizations of (X, Y ).
Applying the quantile transformation one gets
p(d, z)
=
P

U > −d(1 −z), V > −dz

=
F

−d(1 −z), −dz

for 0 < d ≤1 and 0 ≤z ≤1, where U and V are [−1, 0]–uniform random
variables with common df F. In view of (13.13) it is plausible to assume that for
all 0 ≤z ≤1,
p(d, z) −W

−d(1 −z), −dz

d →0,
d →0,
(13.16)
where W is a standard GP df. For the probabilities in (13.15) one gets
2p(d, z)/d →TW (z),
d →0,
Thus, TW may be regarded as a limiting canonical dependence function. Putting
d = k/n, one arrives at the estimator
Tn,k(z) = 2
k

i≤n
I

xi > xn−[k(1−z)]:n, yi > yn−[kz]:n

(13.17)

320
13. Multivariate Peaks Over Threshold
of the limiting canonical dependence function. For the direction z = 1/2 one gets
again an estimator of the canonical parameter.
In Fig. 13.3 we plot the estimate Tn,k(z) of the canonical dependence function
based on n = 500 Gumbel–McFadden data generated under the parameter ϑ =
0.5 with k = 60. For z = 0.5 one gets an exceptional accuarate estimate of the
canonical parameter.
direction z
0.0
0.5
1
0.0
0.5
1
Fig. 13.3. Estimated canonical depen-
dence function generated under the
canonical parameter ϑ = 0.5.
A somewhat related estimator was examined by Deheuvels6 in the context of
“tail probabilities”. We refer to Huang4 and Drees and Huang7 for recent asymp-
totic results and a survey of the literature.
Estimating by Piecing–Together
Piecing together
• the estimated univariate GP dfs,
• the estimated parametric bivariate standard GP df
one gets an approximation to the original bivariate df in the upper tail region.
This approximation can be extrapolated to higher tail regions outside of the range
of the data
If this is done within the nonparametric framework—using the estimated
canonical dependence function—then one should be aware that the resulting df is
6Deheuvels, P. (1980). Some applications to the dependence functions in statistical
inference: nonparametric estimates of extreme value distributions, and a Kiefer type
universal bound for the uniform test of independence. In: Nonparametric Statistical In-
ference, pp. 183–201, B.V. Gnedenko et al. (eds), North Holland, Amsterdam.
7Drees, H. and Huang, X. (1998). Best attainable rates of convergence for estimators
of the stable tail dependence function. J. Mult. Analysis 64, 25–47.

13.3. About Tail Independence
321
not a bivariate GP df. This can be achieved by using a concave majorant of the
sample canonical dependence function.
We present once again the scatterplot and contour plot in Fig. 10.3 and add
an extrapolated contour plot.
x
y
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
x
y
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
Fig. 13.4. (left.) Contour plots of theoretical and sample survivor function.
(right.) Estimated parametric extrapolation to the upper tail region.
13.3
About Tail Independence
co–authored by M. Frick8
In this section we discuss some well–known measures of independence which char-
acterize diﬀerent degrees of tail independence and indicate the relationship of these
parameters to each other. Moreover, we study a test of tail dependence against
tail independence based on the radial component.
Recall from (12.10) that
¯Φρ(bn + b−1
n x) = O

n−2/(1+ρ)(log n)−ρ/(1+ρ)
,
where ¯Φρ(x, y) = P{X > x, Y > y} is the survivor function of a standard normal
vector (X, Y ) with correlation coeﬃcient ρ such that −1 < ρ < 1. Thus, according
to (12.9), the right–hand side determines the rate at which the random maxima in
each component become independent as the sample size goes to inﬁnity. The latter
property was addressed as the upper tail independence of the bivariate normal df.
8University of Siegen.

322
13. Multivariate Peaks Over Threshold
The Coeﬃcient of Tail Dependence for Normal Vectors
In an article by Ledford and Tawn9, relation (12.10) is reformulated and strength-
ened with (X, Y ) replaced by the pertaining copula random vector (U, V ) =
(Φ(X), Φ(Y )) where Φ is the univariate standard normal df.
These authors prove that
P{U > u, V > u} ∼c(ρ)(1 −u)2/(1+ρ)(−log(1 −u))−ρ/(1+ρ),
u →1,
(13.18)
where c(ρ) = (1 + ρ)3/2(1 −ρ)−1/2(4π)−ρ/(1+ρ).
In that context, Coles et al.10 introduced the coeﬃcient of tail dependence
at the level u, namely,
¯χ(u) =
2 log P{U > u}
log P{U > u, V > u} −1,
(13.19)
and the coeﬃcient of tail dependence
¯χ = lim
u→1 ¯χ(u).
(13.20)
It is easy to verify that
¯χ = ρ
(13.21)
for the particular case of (0, 1)–uniformly distributed random variables U and V
with normal dependence structure as in (13.18). As pointed out by Coles et al.,
the relation (13.21) “provides a useful benchmark for interpreting the magnitude
of ¯χ in general models.”
The reason for introducing the coeﬃcient of tail dependence as another tail
dependence parameter is the desire to distinguish between pairs of random vari-
ables which are both tail independent, that is
χ = lim
u→1 χ(u) := lim
u→1
P{U > u, V > u}
P{U > u}
= 0,
cf. (2.61), but have diﬀerent degrees of independence at an asymptotic level of
higher order.
The Coeﬃcient of Tail Dependence in General Models
For non–normal vectors, the coeﬃcients of tail dependence ¯χ(u) and ¯χ are deﬁned
in analogy to (13.19) and (13.20). We list some properties of ¯χ(u) and ¯χ:
9Ledford, A.W. und Tawn, J.A. (1996). Statistics for near independence in multivariate
extreme values. Biometrika 83, 169–187.
10Coles, S., Heﬀernan, J.E. and Tawn, J.A. (1999). Dependence measures for extreme
value analyses. Extremes 2, 339–365.

13.3. About Tail Independence
323
• ¯χ(u) and ¯χ are symmetric in U and V ;
• ¯χ(u) and ¯χ range between −1 and 1;
• if U = V , then ¯χ = 1.
The pair (χ, ¯χ) of tail dependence parameters may be employed to describe
the tail dependence structure of two random variables:
χ > 0, ¯χ = 1
tail dependence with χ determining the degree of
dependence.
χ = 0, ¯χ < 1
tail independence with ¯χ determining the degree
of dependence.
Another Coeﬃcient of Tail Dependence
We start again with a normal copula random vector (U, V ) pertaining to a standard
normal vector with correlation coeﬃcient ρ with −1 < ρ < 1.
Relation (13.18) can be written
P{U > u, V > u} ∼L(1 −u)(1 −u)1/η,
u →1,
(13.22)
where η = (1 + ρ)/2 and L(1 −u) = c(ρ)(−log(1 −u))−ρ/(1+ρ) with c(ρ) as in
(13.18). The term η is again called coeﬃcient of tail dependence.
Generally, if for a copula random vector (U, V ), a relation
P{U > u, V > u} ∼L(1 −u)(1 −u)1/η,
u →1,
(13.23)
holds, where L is a slowly varying function at 0 (i.e. as u →1), then η is called
coeﬃcient of tail dependence. It is evident that (13.22) is a special case. Notice
that 0 < η ≤1.
Example 13.3.1. (Morgenstern distributions.) The copula of the Morgenstern df with
parameter −1 < α ≤1 is given by
Cα(u, v) = uv[1 + α(1 −u)(1 −v)],
see, e.g., Heﬀernan11. The bivariate survivor function satisﬁes the relation
P{U > u, V > u} ∼(1 + α)(1 −u)2,
u →1,
so that η = 1/2 and L(1 −u) = 1 + α.
11Heﬀernan, J.E. (2000). A directory of coeﬃcients of tail independence. Extremes 3,
279–290.

324
13. Multivariate Peaks Over Threshold
For further examples we refer to the article by Heﬀernan. We indicate the
relationship between the diﬀerent dependence parameters χ, ¯χ and η.
¯χ
=
2η −1;
χ
=
⎧
⎨
⎩
c
if ¯χ = 1 and L(u)
u→1
−→c ≥0,
0
if ¯χ < 1.
(13.24)
In Ledford and Tawn12 one may ﬁnd related formulas for the case of unequal
thresholds u and v.
Estimating the Coeﬃcient of Tail Dependence
In analogy to (2.62), a sample version pertaining to ¯χ(u) based on data (xi, yi),
i = 1, . . . , n, is
¯χn(u) =
2 log(1 −u)
log

1
n

i≤n I(xi > x[nu]:n, yi > y[nu]:n)
 −1,
(13.25)
which is an estimator of ¯χ(u) as well as ¯χ.
Example 13.3.2. The illustration in Fig. 13.5 concerns a data set consisting of n = 2, 894
three–hourly–measurements of the surge and wave heights taken at Newlyn, a coastal
town in England, see Example 13.3.6. The sample version ¯χn(u) is plotted against the
level u. The plot was generated by using the source code written by Coles, Heﬀernan and
Tawn13. This plot suggests an estimate of ¯χn = 0.5 of ¯χ. Near u = 1 there is a larger
variation of ¯χn(u) due to the fact that the estimate is merely based on a smaller number
of extremes.
It would be desirable to get some theoretical results for this estimator (as
well as for the serial version which is mentioned in the following lines).
Another Auto–Tail–Dependence Function
In analogy to the auto–tail–dependence function ρ(u, h) for the parameter χ(u),
see (2.63), one may introduce a auto–tail–dependence function ¯ρ(u, h) for the tail–
independence parameter ¯χ(u) at the level u.
12Ledford, A.W. und Tawn, J.A. (1997). Modelling dependence within joint tail regions.
J.R. Statist. Soc. B 59, 475–499.
13www.maths.lancs.ac.uk/˜currie/Code/DependenceMeasuresForExtremes.S and -.txt

13.3. About Tail Independence
325
0.0
0.2
0.4
0.6
0.8
1.0
−1.0
−0.5
0.0
0.5
1.0
u
χn(u)
Fig. 13.5. The sample coeﬃcient of tail dependence ¯χn(u) at the level u plotted against
u for the wave and surge data.
Let again X1, . . . , Xn be a series of identically distributed random variables
with common df F. Then, for i ≤n −h, put
¯ρ(u, h)
=
2 log P{Xi > F −1(u)}
log P{Xi > F −1(u), Xi+h > F −1(u)} −1
=
2 log P{X1 > F −1(u)}
log P{X1 > F −1(u), X1+h > F −1(u)} −1,
(13.26)
where we implicitly assume stationarity in the dependencies.
Likewise deﬁne an auto–tail–dependence function ¯ρ(h) by
¯ρ(h) = lim
u→1 ¯ρ(u, h).
(13.27)
Again ¯ρ(u, h) and ¯ρ(h) can be estimated by the sample versions
¯ρn(u, h) =
2 log(1 −u)
log

1
n−h

i≤n−h I(min(xi, xi+h) > x[nu]:n)
 −1
(13.28)
based on the data x1, . . . , xn.
Example 13.3.3. (Gaussian AR(1) series.) We study once more a Gaussian AR(1) series
as in Example 2.5.3 with autocorrelation function ρ(h) = dh. It is evident that the
autocorrelation function ρ(h) and the auto–tail–dependence function ¯ρ(h) are identical.
Therefore, the sample versions estimate the same functions and should coincide to some
extent.

326
13. Multivariate Peaks Over Threshold
0
5
10
15
0.0
0.2
0.4
0.6
0.8
1.0
time lag
0
5
10
15
0.0
0.2
0.4
0.6
0.8
1.0
time lag
Fig. 13.6. Autocorrelation functions (left) and auto–tail–dependence functions (right).
In Fig. 13.6 we plot the sample autocorrelation function and sample auto–tail–
independence function for u = 0.8 based on 200 Gaussian AR(1) data under the param-
eter d = 0.8. The theoretical correlation function ρ(h) = dh is included.
The sample autocorrelation function, which is based on the whole data set,
provides a more accurate estimate of the theoretical curve than the sample auto-
tail-dependence function because the latter sample function is merely based on the
extremes.
A Spectral Expansion of Length 2
Next we make use of the spectral decomposition methodology, outlined in Section
12.4, to characterize the degree of tail independence.
First we state a reﬁnement of condition (12.45) given in Section 12.4. Let X
and Y be random variables having the joint bivariate df H with support (−∞, 0]2.
Assume that H satisﬁes a diﬀerentiable spectral expansion of length 2, i.e.,
hz(c) := ∂
∂cHz(c) = D(z) + B(c)A(z) + o(B(c)), as c ↑0,
(13.29)
where B is regularly varying with exponent of variation β > 0. We say that H
satisﬁes a diﬀerentiable spectral expansion of length 2 with exponent of variation
β. Notice that B satisﬁes limc↑0 B(c) = 0.
An important example is given by B(c) = |c|β. For this special case with
β = 1, condition (13.29) was introduced in Falk et al. [16], 2nd ed. The present
form is due to Frick et al.14
14Frick, M., Kaufmann, E. and Reiss, R.–D. (2006). Testing the tail–dependence based
on the radial component, submitted.

13.3. About Tail Independence
327
Because (13.29) implies (12.45) we know that the function D in the expansion
is necessarily a Pickands dependence function, cf. (12.20).
We provide two examples of distributions for which (13.29) is satisﬁed with
D(z) = 1 which is the Pickands dependence function for the case of independent
marginal maxima.
Example 13.3.4. (Bivariate Standard Normal Distribution) We consider again the bi-
variate standard normal distribution with correlation ρ ∈(0, 1). Let H be its df after
transformation to (−1, 0)–uniformly distributed margins. It satisﬁes the expansion
hz(c) = 1 + B(c)A(z) + o(B(c)),
as c ↑0,
(13.30)
with
B(c) = |c|2/(1+ρ)−1 ˜L(c),
where ˜L(c) := c(ρ)(−log |c|)−ρ/(1+ρ) with c(ρ) as in (13.18), and
A(z) =
2
1 + ρ(z(1 −z))1/(1+ρ).
The function ˜L is slowly varying so that the function B is regularly varying with exponent
of variation β = 2/(1 + ρ) −1. Substituting this result in equation (13.33) we receive
again the relationship ¯χ = ρ (see (13.21)).
Additionally,
L(1 −u) = (1 −u)−βB(−(1 −u))
and
η = (1 + β)−1
(13.31)
in (13.22).
In the following example, we extend the representation of hz(c) in the case
of independent, (−1, 0)–uniformly distributed random variables, cf. (c′) on page
310, to Morgenstern random vectors.
Example 13.3.5. (Morgenstern distributions.) A transformation of the Morgenstern df,
whose copula form was given in Example 13.3.1, to (−1, 0)–uniformly distributed margins
leads to the df
H(u, v) = (1 + u)(1 + v)(1 + αuv)
which satisﬁes the expansion
hz(c) = 1 + c(1 + α)2z(1 −z) + o(c),
c ↑0.
Therefore, D(z) = 1, A(z) = −2z(1 −z), and B(c) = −(1 + α)c, the latter being a
regularly varying function with exponent of variation β = 1. Recalling from Example
13.3.1 that η = 1/2, both (13.24) and the subsequent formula (13.33) imply that ¯χ = 0.

328
13. Multivariate Peaks Over Threshold
Relationships Between Measures of Dependence
Let again H be the df of a random vector (X, Y ) satisfying a diﬀerentiable spectral
expansion of length 2. According to (13.6) we have
χ = lim
c↑0 P(Y > c|X > c) = 2(1 −D(1/2)),
(13.32)
where D is again the Pickands dependence function, and the expression 2(1 −
D(1/2)) is the canonical parameter.
This implies the following characterization of tail dependence :
If D ̸= 1, we have tail dependence.
If D = 1, we have tail independence.
In addition, if D = 1 one obtains a relationship between the exponent of
variation β (in the spectral expansion of length 2) and the coeﬃcient of tail de-
pendence ¯χ. For that purpose let, in addition, B be absolutely continuous with
a monotone density. This is satisﬁed, e.g., for the standard case of a regularly
varying function B(c) = |c|β.
Under these conditions one can prove that
¯χ = 1 −β
1 + β .
(13.33)
Notice that ¯χ →1 if β →0, and ¯χ →−1 if β →∞.
Asymptotic Distributions of the Radial Component
We assume that the spectral expansion (13.29) is valid and the partial derivatives
hz(c) = ∂/∂c Hz(c) and ˜hc(z) = ∂/∂z Hz(c) are continuous.
Under these conditions, the conditional asymptotic distribution of the radial
component X + Y for increasing thresholds c is provided (a result due to Frick et
al. in the article cited on page 326, who extended a result in Falk et al. [16], 2nd
ed., from the special case of β = 1 to β > 0).
(i) D ̸= 1 implies
P(X + Y > ct|X + Y > c) −→t =: F0(t),
c ↑0,
uniformly for t ∈[0, 1],
(ii) D = 1 and β > 0 implies
P(X + Y > ct|X + Y > c) →t1+β =: Fβ(t),
c ↑0,
uniformly for t ∈[0, 1] provided that
(2 + β)

A(z) dz −A(0) −A(1) ̸= 0.
(13.34)

13.3. About Tail Independence
329
Condition (13.34) is satisﬁed, e.g., for normal and Morgenstern dfs. This con-
dition is not satisﬁed for the bivariate generalized Pareto df, special considerations
are required in this case (for a discussion see Falk et al. [16] and the above men-
tioned article by Frick et al.). This result will be applied to the testing of tail
dependence based on the radial component.
Selection of the Null–Hypothesis
We continue our discussion on page 293 about the selection of the model: “be-
fore using a more complex model, one should check whether the EV model for
independent margins or a related generalized Pareto model is applicable.” There-
fore, we wish to check whether the more complex model with unknown Pickands
dependence function D can be replaced by the simpler one where D = 1.
About the selection of the hypotheses, there is a well–known general advice
(e.g., in the booklet by J. Pfanzagl15, page 95, translated from German): “As
null–hypothesis select the opposite of that you want to prove and try to reject the
null–hypothesis.” In our special case we want to prove the tail independence and,
therefore, take tail dependence as the null–hypothesis.
We are well aware that statisticians often do not follow this advice primarily
due to technical reasons.
Testing Tail Dependence Against Tail Independence
We are testing a simple null–hypothesis H0, representing dependence, against a
composite alternative H1, representing the various degrees of independence.
For that purpose we merely have to deal with the asymptotic conditional
distributions of the test statistic X + Y . We are testing
H0 : F0(t) = t
against
H1 : Fβ(t) = t1+β, β > 0,
based on the the radial components
Ci = (Xi + Yi)/c,
i = 1, . . . , n,
with Ci < 1, i = 1, . . . , n. The testing procedure is carried out conditioned on the
random sample size of exceedances, see page 234. Denote by ˜Ci, i = 1, . . . , m, the
iid random variables in the conditional set–up.
The Neyman–Pearson test at the level α for testing F0 against the ﬁxed
alternative Fβ is independent of the parameter β > 0. Notice that this test is based
on the densities fβ(t) = (1+β)tβ, 0 ≤t ≤1. Therefore, one gets a uniformly most
powerful test for the testing against the composite alternative.
15Pfanzagl, J. (1974). Allgemeine Methodenlehre der Statistik II. Walter de Gruyter,
Berlin.

330
13. Multivariate Peaks Over Threshold
For iid random variables ˜Ci, i = 1, . . . , m, with common df F0, the Neyman–
Pearson test statistic m
i=1 log ˜Ci is distributed according to the gamma df
Hm(t) = exp(t)
m−1

i=0
(−t)i
i!
,
t ≤0,
on the negative half–line with parameter m. Therefore, the Neyman–Pearson test
at the level α is given by the critical region
Cm,α =
+ m

i=1
log ˜Ci > H−1
m (1 −α)
,
.
In the aforementioned article by Frick et al. one may also ﬁnd a bias–corrected
MLE for β.
Power Functions and P–Values
Evaluating P(Cm,α) under iid random variables Zi, which are distributed accord-
ing to Fβ, one gets the power function
ζm,α(β) = 1 −Hm

(1 + β)H−1
m (1 −α)

,
β ≥0,
for the level–α–test.
The p–value of the optimal test, ﬁnally, is given by
p(˜c) = 1 −Hm
 m

i=1
log ˜ci

≈Φ

−
m
i=1 log ˜ci + m
m1/2

,
(13.35)
with ˜c = (˜c1, · · · , ˜cm), according to the central limit theorem.
One may also derive an approximate representation of the power function by
ζm,α(β) ≈1 −Φ((1 + β)Φ−1(1 −α) −βm1/2).
Particularly, for m = 361 and α = 0.01 we have
ζ361,0.01(β) = 1 −Φ((1 + β)Φ−1(0.99) −19β).
This function together with some other power functions is displayed in Figure 13.7.
Data Transformation
Real data (v1, w1), . . . , (vn, wn) are independent realizations of a random vector
(V, W) with common unknown df. These pairs have to be transformed to the left

13.3. About Tail Independence
331
0.0
0.1
0.2
0.3
0.4
0.5
0.0
0.2
0.4
0.6
0.8
1.0
β
ζ361,0.01(β)
0.0
0.1
0.2
0.3
0.4
0.5
0.2
0.4
0.6
0.8
1.0
β
ζ361,0.05(β)
0.0
0.1
0.2
0.3
0.4
0.5
0.0
0.2
0.4
0.6
0.8
1.0
β
ζ290,0.01(β)
0.0
0.1
0.2
0.3
0.4
0.5
0.0
0.2
0.4
0.6
0.8
1.0
β
ζ434,0.01(β)
Fig. 13.7. Power functions for m = 361 and α = 0.01, 0.05 (above), and m = 290, 434
and α = 0.01 (below).
lower quadrant to make the preceding test procedure applicable. Such a transfor-
mation can be achieved as explained in Section 10.3. Subsequently, we make use
of transformations by means of the sample dfs Fn(v; ·) and Fn(w; ·) in the single
components. We have
xi = Fn(v; vi)
and
yi = Fn(w; wi),
i = 1, . . . , n.
Recall that the resulting data are relative ranks with values in the interval [0, 1]
or (0, 1) if the ranks are divided by n + 1 in place of n. Then the data are shifted
to (−1, 0). This procedure is illustrated in the following example.
Example 13.3.6. (Wave and Surge Heights at Newlyn, England.) The wave and surge
data set was originally recorded by Coles and Tawn. It consists of 2, 894 three–hourly–
measurements of the surge and wave heights taken at Newlyn, a coastal town in England.
Ledford and Tawn as well as Coles et al. have already analyzed the dependence structure
of these data with the motivation that ﬂooding is likely if both surge and wave heights
reach extreme levels.

332
13. Multivariate Peaks Over Threshold
Now we are going to test the extremal dependence of the two components by apply-
ing the uniformly most powerful test. In a ﬁrst step the marginal data are transformed by
means of the marginal univariate empirical dfs. Then they are shifted from (0, 1) × (0, 1)
to (−1, 0) × (−1, 0). The data set in its original form and after transformation to (0, 1)–
uniformly distributed margins is displayed in Figure 13.8.
Fig. 13.8. Wave heights and surge levels at Newlyn: original data (left) and transformed
data set with [0, 1]–uniformly distributed margins (right)
Testing the Tail Dependence for the Wave and Surge Heights
Next we proceed as in Falk et al. [17], page 189: ﬁx c < 0 and consider those
observations xi+yi exceeding the threshold c. These data are subsequently denoted
by c1, . . . , cm. The threshold c is chosen in such a way that the number m of
exceedances is about 10% to 15% of the total number n = 2, 894. Therefore, we
take values c from −0.46 to −0.35. It may be worthwhile to investigate more closely
the impact of the threshold c on the performance of the test. Figure 13.9 shows
the transformed full data set and the part near zero together with threshold lines.
Fig. 13.9. The transformed full data set, shifted to the negative quadrant (left) and data
above the threshold lines corresponding to c = −0.46 and c = −0.35 (right).

13.4. The Point Process Approach to the Multivariate POT Method
333
We assume that our results are applicable to the ci/c, i = 1, . . . , m. Substi-
tuting the xi in (13.35) by ci/c, i = 1, . . . , m, one gets approximate p–values. Table
1 provides some examples of p–values for diﬀerent thresholds c together with the
number m of exceedances.
Table 13.1. p–values for diﬀerent thresholds c.
c
−0.46
−0.45
−0.44
−0.43
−0.42
−0.41
m
431
414
400
390
376
361
p–value
0.00028
0.00085
0.00158
0.00162
0.00305
0.00665
c
−0.4
−0.39
−0.38
−0.37
−0.36
−0.35
m
353
338
325
313
300
294
p–value
0.00542
0.01179
0.01939
0.02816
0.04601
0.03291
Having in mind a signiﬁcance level of α = 0.01 the null–hypothesis is accepted
for the bigger values of c, up from −0.39. For lower thresholds the p–values are
rather low, suggesting a rejection of the null–hypothesis. Remembering that we are
considering limiting distributions as c ↑0 we may nevertheless decide to accept
the null–hypothesis entailing the acception of an extremal tail–dependence. Yet
we have to be aware that this conclusion is not straightforward and we must not
ignore that the null–hypothesis is always rejected if we choose a signiﬁcance level
of α = 0.05, for example.
13.4
The Point Process Approach
to the Multivariate POT Method
We shortly introduce the point process models related to the multivariate GP
models and possible estimation procedures within such models.
The Models
We shortly describe the relationship between
• the original statistical modeling by means of iid random vectors (Xi, Yi),
i = 1, . . . , n, with common bivariate df F,
• the model determined by bivariate GP dfs &
W as introduced in (13.5).
Let (xi, yi), i = 1, . . . , n, be generated according to F. Let (x′
i, y′
i), i =
1, . . . , k, be those (xi, yi) for which either xi > u or yi > v (taken in the orig-
inal order of the outcome). Let
(˜xi, ˜yi) := m(u,v)(x′
i, y′
i) =

max(x′
i, u), max(y′
i, v)


334
13. Multivariate Peaks Over Threshold
be the vectors introduced in (13.4). Within a certain error bound (see [16], Section
5.1), the (˜xi, ˜yi) can be regarded as realizations of random vectors
( 	
Xi, 	Yi),
i = 1, . . . , K(n),
(13.36)
where
• ( 	
X1, 	Y1), ( 	
X2, 	Y2), ( 	
X3, 	Y3), . . . are iid random vectors with common GP df
&
W(x, y) in the form described in (13.5),
• K(n) is a binomial or a Poisson random variable which is independent of the
( 	
Xi, 	Yi).
Thus, the series of random vectors in (13.36) constitute a binomial or a Poisson
process depending on the choice of K(n).
Estimation in Point Process Models
Of course, ﬁrst of all one may also use the estimators presented before within the
point process framework.
The ML approach was applied, for example, by Davison and Smith in the
article mentioned on page 121. To compute the likelihood function specify densities
of point processes as it was done in Section 9.3 (also see Section 9.5) with the help
of results in [43], Section 3.1.

Part IV
Topics in Hydrology and
Environmental Sciences

Chapter 14
Flood Frequency Analysis
co–authored by J.R.M. Hosking1
We ﬁrst summarize and supplement in Section 14.1 the at–site analysis done before
in conjunction with annual ﬂood series.
Section 14.2 deals with the partial duration series of daily discharges. Our
program was already outlined in Section 2.5, namely we want to handle the serial
correlation and seasonal variation in the data. Our primary interest still concerns
the calculation of the T –year threshold, yet also brieﬂy discuss the question of
seasonal variation in its own right. To catch the seasonal variation of discharges
over a higher level, we introduce inhomogeneous Poisson processes with marks
which distributionally depend on the time of their occurrence (Section 14.2). The
handling of such partial duration series is the main topic of this chapter. A trend
is included in Section 14.3.
In Sections 14.3–14.5, we also deal with the regional ﬂood frequency analysis
Our primary references are the paper by Hosking et al., cf. page 120, and the book
[29].
14.1
Analyzing Annual Flood Series
The traditional approach of dealing with ﬂoods is to use annual maxima. On the
one hand, one is avoiding the problems of serial correlation and seasonal variation;
on the other hand, one is losing information contained in the data. A ﬁrst modiﬁ-
cation of this approach is to base the inference on seasonal or monthly maxima.
1IBM Research Division, Thomas J. Watson Research Center; co–authored the 2nd
edition.

338
14. Flood Frequency Analysis
In Section 4.3 we introduced several distributions, such as Wakeby, two–
component and gamma distributions, which are used in the hydrological literature
besides EV distributions for the modeling of annual or seasonal ﬂoods. In this
chapter, we only employ EV distributions.
Estimation of the T–Year Flood Level Based on Annual Maxima
We repeat the analysis of annual ﬂoods as it was already done for the Feather River
discharges, see Examples 4.1.1 and 5.1.1. We computed 50 and 100–year discharge
levels based on the estimated Gumbel (EV0) and exponential (GP0) distributions.
To make the results in this and the subsequent section comparable, we repeat
the aforementioned analysis with respect to the Moselle River data.
Example 14.1.1. (Continuation of Example 2.5.2 about the Moselle River Data.) The
MLE(EV) will be applied to the annual maximum levels for water years running from
Nov. to Oct. of consecutive calendar years. Based on 31 annual maxima one obtains the
parameters γ = −0.40, µ = 8.00 and σ = 1.74. Moreover, the right endpoint of the
estimated Weibull distribution is 12.37. The 50 and 100–year levels are u(50) = 11.45
and u(100) = 11.67. According to these estimates, there was a 100–year ﬂood, with a
level of 11.73 meters, around Christmas time in the year 1993.
In view of the small number of data, we do not carry out a tail estimation for
the Moselle River within a GP model based on exceedances over a certain level.
Modiﬁcations of the Annual Maxima Approach
If the number of years is small, there is a greater need to extract more information
out of a sample of daily recorded discharges or water levels. The number of extreme
data can be increased by extracting monthly or seasonal maxima for the statistical
analysis, where independence may still be assumed, yet one is exposed to the
seasonal variation. We do not go into details but concentrate our attention to the
partial duration approach which will be outlined in the next section.
14.2
Analyzing Partial Duration Series
Series of ﬂood peaks are investigated under the fairly general conditions that the
frequency of occurrence and the magnitude of ﬂoods exhibit a seasonal dependency.
Modeling by an Inhomogeneous Poisson Process
After a declustering of the data, the modeling
• of frequencies by means of an inhomogeneous Poisson process, and

14.2. Analyzing Partial Duration Series
339
• of the magnitudes by stochastically independent, time–dependent marks
is adequate.
Subsequently, we use a nonparametric approach with the exception that the
marks are assumed to be distributed according to generalized Pareto (GP) dfs. A
parametric modeling for the marginal distributions of the marks is indispensable
when the usual extrapolation to extraordinary high ﬂood levels must be carried
out.
We model the frequencies and the marks by means of an inhomogeneous
Poisson process in the time scale with mean value function Λ, and marks with dfs
F(·|ti) at time ti. This constitutes a Poisson(Λ, F), cf. Section 9.5.
The ﬂood peaks are those marks exceeding a threshold v. According to (9.31)
and (9.32) this situation is described by a Poisson(Λv, F [v]) with mean value func-
tion
Λv(t) =
 t
0
(1 −F(v|s)) dΛ(s)
(14.1)
in the time scale, and marks distributed according to
F [v](w|t) =

F(w|t) −F(v|t)

1 −F(v|t)

,
w ≥v.
(14.2)
The T–Year Flood Level
We extend the computations on page 252 concerning the T –year return level to the
new framework. (14.1) and (14.2) yield that the ﬁrst exceedance time with respect
to the threshold v is the random variable τ1,v = Λ−1
v (X), where X is a standard
exponential random variable. Hence, the T –year return level is the solution to the
equation
E(τ1,v) = T.
(14.3)
An estimate of the T –year return level is obtained by plugging in estimates of the
mean value function Λ and of the conditional df F = F(·|·).
Subsequently, we assume that there is only a seasonal dependence of F(·|s)
and Λ(s). Then, according to (14.1),
Λv(T ) = T
 1
0
(1 −F(v|s)) dΛ(s) =: Tψ(v),
T = 1, 2, 3, . . . .
(14.4)
When the inhomogeneous mean value functions Λv(t) are replaced by the homo-
geneous mean value functions 	Λv(t) = tψ(v), then ψ(v) is the intensity in the time
domain, and the ﬁrst exceedance time is an exponential random variable with
expectation 1/ψ(v), see Section 9.2 for details. This expectation is equal to T , if
Tψ(v) = 1.
(14.5)
Therefore, one obtains a T –year return level ˜v(T ) as a solution to this equation.
This T –year return level may serve as an approximation to the one in the original
setting.

340
14. Flood Frequency Analysis
We also give an alternative interpretation of ˜v(T ): let Nv be the inhomoge-
neous Poisson counting process with mean value function Λv. Because
1 = Tψ(v) = 	Λ˜v(T )(T ) = E

N˜v(T )(T )

(14.6)
we know that ˜v(T ) is the threshold so that the mean number of exceedances up
to time T is equal to 1, also see page 12.
Estimation of the T–Year Level
Let l be the number of years for which the ﬂood measurements are available. Let
again N(t), t ≤l, be the counting process pertaining to the exceedance times
τi, i ≤N(l), of exceedances over u. It is apparent that
Nl(s) = 1
l
l−1

j=0

N(j + s) −N(j)

,
0 ≤s ≤1,
is an unbiased estimator of Λ(s) for 0 ≤s ≤1. Plugging in Nl for Λ in ψ(v) =
 1
0 (1 −F(v|s)) dΛ(s) one obtains by
 1
0
(1 −F(v|s)) dNl(s)
=
1
l

i≤N(l)

1 −F(v| j(i) + ηi)

=
1
l

i≤N(l)

1 −F(v|ηi)

an estimator of ψ(v), where j(i) + ηi = τi. Thus, we have ηi = τi modulo 1.
Next, F(·|ηi) will be replaced by a generalized Pareto (GP) df W. The ﬁnal
estimator of ψ(v) is given by
ψ(v) = 1
l

i≤N(l)

1 −Wˆγ(ηi),u,ˆσ(ηi)(v)

,
(14.7)
where the estimators ˆγ(ηi) and ˆσ(ηi) of the shape and scale parameters are con-
structed in the following manner: let Xj be the mark pertaining to the time τj
(and, thus, pertaining to ηj). Then, these estimators are based on those Xj for
which |ηj −ηi| ≤b, where b > 0 is a predetermined bandwidth. Alternatively, one
may employ a nearest neighbor method, that is, take the marks Xj pertaining to
the k values ηj closest to ηi, where k is predetermined (see Section 8.2).
Then, an estimator ˆv(T ) of the T –year level is obtained as the solution to
T ψ(v) = 1.
(14.8)
One gets again the estimator in the homogeneous case (see page 252) if the
parameters are independent of the time scale.

14.2. Analyzing Partial Duration Series
341
Example 14.2.1. (Continuation of Example 13.1.1 about the Moselle River Data.) After
taking the exceedances over a base level u = 5 and the selection of cluster maxima
(with run length r = 7) there are 131 observations within the 31 water years (starting in
November). In the hydrological literature, it is suggested to take a base level and clusters
such that there are three cluster maxima for each year on the average. Thus, we took a
slightly greater number of exceedances.
In Fig. 14.1 (left), the exceedances are plotted against the day of occurrence within
the water year. As one could expect the exceedances primarily occur in winter and spring
time. It is remarkable that higher ﬂood levels also occur in times with a lower ﬂood
frequency.
day in water year
exceedances
0
200
5
10
week in water year
frequency of exceedances
20
40
5
10
Fig. 14.1. (left.) Scatterplot of cluster maxima over u = 5 at days modulo 365. (right.)
Frequency of exceedances within weeks modulo 52.
We also included a histogram of the frequencies plotted against the week of occur-
rence within the water year (Fig. 14.1 (right)).
The GP parameters γ(i) and σ(i) of the ﬂood magnitudes are estimated by means
of the MLE(GP) using the nearest neighbor approach with k = 30. The use of the
Moment(GP) estimator would lead to a similar result. In Fig. 14.2, plots of the estimates
are provided for all days within the water year where an exceedance occurred.
This is our interpretation: the estimates reﬂect to some extent what can also be
seen in the scatterplot (Fig. 14.1 (left)). In winter time the cluster maxima are relatively
homogeneously scattered within the range between 5 and 12, whereas in spring time very
high ﬂoods are rare events. In the ﬁrst case, the wide range is captured by the larger scale
parameter, whereas the latter phenomenon is captured by the positive shape parameter.
Based on Monte Carlo simulations we obtain 12.1 meter as an estimate of the 100–year
threshold. Applying the Moment(GP) estimator one obtains higher T–year thresholds
than for the MLE(GP).
Statistical inference for such series (particularly, the estimation of the T –year
level) was carried out in the hydrological literature
• within the reduced setting of stationarity within certain seasons2,
2Ashkar, F. and Rousselle, J. (1981). Design discharge as a random variable: a risk

342
14. Flood Frequency Analysis
day in water year
shape parameter
200
-1
0.3
day in water year
scale parameter
200
2
4
Fig. 14.2. Estimated shape parameters γ(i) (left–hand side) and scale parameters σ(i)
(right–hand side) at days i.
• for a certain continuous modeling with trigonometric functions3.
Moreover, it was assumed that the marks (the ﬂood magnitudes) are exponentially
distributed. For further explanations and references see Davison and Smith (cited
on page 121), where also a decision is made for the exponential model according
to a likelihood ratio test. We believe that for our example of Moselle data the GP
modeling is adequate.
Flood Series with a Trend
The estimated T –year level may drastically increase in the presence of a slight
trend. If a linear trend is included in the modeling, then much higher T –year
levels were estimated for the Moselle river data4. We also refer to articles by R.L.
Smith5 and by H. Rootz´en and N. Tajvidi6.
study. Water Resour. Res. 17, 577–591.
Rasmussen, P.F. and Rosbjerg, D. (1991). Prediction uncertainty in seasonal partial
duration series. Water Resour. Res. 27, 2875–2883.
3North, M. (1980). Time–dependent stochastic model of ﬂoods. J. Hydraul. Div. Am.
Soc. Civ. Eng. 106, 649–665.
Nachtnebel, H.P. and Konecny, F. (1987). Risk analysis and time–dependent ﬂood
models. J. Hydrol. 91, 295–318.
4Reiss, R.–D. and Thomas, M. (2000). Extreme environmental data with a trend and a
seasonal component. In: Environmental Engineering and Health Sciences (ed. J.A. Raynal
et al.), 41–49, Water Resources Publications, Englewood.
5Smith, L.R. (1989). Extreme value analysis of environmental time series: an applica-
tion to trend detection in round–level ozone. Statistical Science 4, 367–381.
6Rootz´en, H. and Tajvidi, N. (1997). Extreme value statistics and wind storm losses:
a case study. Scand. Actuarial. J., 70–94.

14.3. Regional Flood Frequency Analysis
343
14.3
Regional Flood Frequency Analysis
We argued that the use of a partial duration series in place of an annual ﬂood series
is necessary to extract more information out of the available data. Another pos-
sibility is to include information from data recorded at nearby sites, respectively,
sites having similar characteristics.
Regional Estimation of Parameters
In the regional analysis it is assumed that data are available at diﬀerent sites with
data xj,i at site j. We deal with data yj,i which are
• annual maxima, or
• excesses (exceedances) over a threshold uj.
Subsequently, we primarily address the case of excesses.
Assume that the random excesses at site j are distributed according to the
GP df Wγ,0,σj. Apparently this is the situation, where the accesses have a common
shape parameter γ, yet the scale parameters are diﬀerent from each other.
If ˆγj is an estimate of γ at site j, then
ˆγR =

j
njˆγj
 
j
nj
(14.9)
is a regional estimate of γ, where the nj are the sample sizes (number of excesses
over the threshold uj) at site j (cf. [29], page 7, where further references to the
literature are provided). If the nj are of a similar size, then a pooling of the data
is appropriate.
An important question is the selection
• of the homogeneous region, that is, those sites which can be modeled by the
same shape parameter,
• of an appropriate submodel of GP distributions.
The Index Flood Procedure
We introduce a special at–site estimation procedure which is also useful in con-
junction with regional considerations.
Suppose that the excesses Yi of discharges over the threshold u are distributed
according to a GP df Wγ,0,σ with shape and scale parameters γ and σ. Recall from
(1.57) that Yi has the expectation
m = σ/(1 −γ),

344
14. Flood Frequency Analysis
which is the index ﬂood, under the condition γ < 1. Under this condition, the
statistical inference is done within a certain submodel of GP dfs, where dfs with
a very heavy upper tail are excluded.
The rescaled excesses Yi/m = Yi(1 −γ)/σ have means equal to 1 and are
distributed according to the GP df
Wγ,0,1−γ(z) = 1 −

1 +
γ
1 −γ z
−1/γ
for
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
0 < z,
0 ≤γ < 1,
if
0 < z < 1−γ
|γ| ,
γ < 0;
(14.10)
with density
wγ,0,1−γ(z) =
1
1 −γ

1+
γ
1 −γ z
−(1+1/γ)
for
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
0 < z,
0 ≤γ < 1,
if
0 < z < 1−γ
|γ| ,
γ < 0.
(14.11)
The unknown parameter γ < 1 may be estimated, for example, by the ML
estimator ˆγ in this submodel of GP dfs (using the Newton–Raphson iteration
procedure).
Within the original model of GP dfs with shape parameter γ < 1 and scale
parameter σ > 0 one gets the following two–step estimation procedure: let yi be
a realization of Yi and denote again by ¯y the sample mean of the yi. Then, the
excess degrees
zi = yi
¯y
may be regarded as realizations under the GP df Wγ,0,1−γ as given in (14.10). Let
ˆγ be the ML estimate (or any other estimate) of γ within the model of GP dfs

Wγ,0,1−γ : γ < 1

based on the excess degrees zi. Because ¯y is an estimate of the mean m = σ/(1−γ),
it is apparent that
ˆσ = (1 −ˆγ)¯y
is an estimate of the original scale parameter σ > 0.
The Sample Median Taken as Index Flood
If one takes the median in place of the mean, then the rescaled excesses have
medians equal to 1 and are distributed according to a GP df
W0,0,1/ log 2
γ = 0,
if
Wγ,0,γ/(2γ−1)
γ ̸= 0.

14.4. The L–Moment Estimation Method
345
For estimating the parameters γ and σ > 0 in the GP model of dfs Wγ,0,σ—without
any restriction on the shape parameter γ—we use excess degrees with the sample
mean replaced by the sample median.
Regional Estimation Using the Index Flood Procedure
Let again yj,i be the discharge excesses over a threshold uj at site j which are
governed by a GP df Wγ,0,σj.
Rescaling the GP dfs by the means mj = σj/(1−γ) as in the preceding lines,
we obtain the common regional frequency distribution Fγ = Wγ,0,1−γ. Recall that
mj is the index ﬂood for the site j. Notice that
Fγ,mj(y) = Fγ(y/mj) = Wγ,0,σj(y)
and, therefore, the index ﬂood mj is a scale parameter for the model of regional
frequency distributions.
Let again
zj,i = yj,i
¯yj
be the excess degrees at site j, where ¯yj is the sample mean of the excesses yj,i at
site j. Now, use the at–site estimates ˆγj of γ to deﬁne the regional estimate ˆγR in
(14.9). In addition,
ˆσj = (1 −ˆγ)¯yj
is an estimate of the original scale parameter σj = (1 −γ)mj.
Estimation of the T–Year Flood Level
At a given site, the T –year ﬂood level may be estimated as in (14.7) and (14.8)
with ˆγ and ˆγ(ηi) replaced by the corresponding regional estimates ˆγR and ˆγR(ηi).
14.4
The L–Moment Estimation Method
We give a short introduction to L–moments and the L–moment estimation method
following closely the explanations in the book by Hosking and Wallis [29] about
regional frequency analysis. L–moment estimators are obtained by equating sample
L–moments with the pertaining L–moments corresponding to the procedure for
ordinary moments, see page 86. As an application, L–moment estimators are dealt
with under the Paretian modeling.
L–moment estimators are extensively applied in ﬂood frequency in conjunc-
tion with the index ﬂood procedure analysis due to their appealing small sample
performance.

346
14. Flood Frequency Analysis
L–Moments
Recall from (1.33) that the ordinary jth moment is deﬁned by
mj =

xj dF(x) =
 1
0

F −1(u)
j du,
(14.12)
where the latter equality is a consequence of the quantile transformation, cf. page
38, and the transformation theorem for integrals.
The right–hand expressions will be replaced by certain probability weighted
moments
βr =
 1
0
F −1(u)ur du
(14.13)
for r = 0, 1, 2, . . . . Notice that β0 is the mean m1. Likewise, one could start with
βr replaced by αr =
 1
0 F −1(u)(1 −u)r du.
The next step corresponds to the step from moments to centered moments
to some extent. The ﬁrst L–moment λ1 is the mean, the second L–moment λ2
corresponds to the standard deviation, and for j ≥3 the jth L–moment is related
to the jth central moment. More precisely, let
λ1
=
β0
λ2
=
2β1 −β0
λ3
=
6β2 −6β1 + β0
λ4
=
20β3 −30β2 + 12β1 −β0
and, in general,
λj =
 1
0
F −1(u)Pj−1(u) du =
j−1

k=0
pj−1,kβk ,
(14.14)
where
Pr(u) =
r

k=0
pr,kuk
are Legendre polynomials of degree r shifted to the interval [0, 1] with the coeﬃ-
cients pr,k = (−1)r−k(r + k)!

(k!)2(r −k)!

for r = 0, 1, 2, . . . . It is evident that
all L–moments exist if the mean exists.
We have P0(x) = 1, Pr(1) = 1 and
 1
0 Pr(u)Ps(u) du = 0 for r ̸= s. In
particular,
 1
0 Pr(u) du = 0 for r ≥1 which yields that L–moments are independent
of a location parameter for j ≥1.
The representations
λ2
=
E

X2:2 −X1:2

2
λ3
=
E

X3:3 −X2:3 −

X2:3 −X1:3

3
λ4
=
E

X4:4 −X1:4 −3

X3:4 −X2:4

4

14.4. The L–Moment Estimation Method
347
reveal why L–moments exhibit characteristics of the distribution just as the stan-
dard deviation and the 3rd and 4th centered moments.
Also deﬁne the L–CV, the L–moment analogue of the coeﬃcient of variation,
by
τ = λ2/λ1
(14.15)
and the L–moment ratios
τj = λj/λ2
(14.16)
with the L–skewness and the L–kurtosis as special cases for j = 3, 4. Notice that
the L–CV is independent of a scale parameter, and the L–moment ratios are in-
dependent of location and scale parameters.
Sample L–Moments
One gets estimates of βr for r = 0, 1, 2, . . . by replacing the qf F −1 in (14.13) by
the sample qf F −1
n
as deﬁned in (2.8). A slight modiﬁcation leads to estimates
˜βr = 1
n

k≤n

k
n + 1
r
xk:n .
Yet, unbiased estimators are obtained by using another modiﬁcation, namely
ˆβ0
=
1
n
n

k=1
xk:n ,
ˆβ1
=
1
n
n

k=2
k −1
n −1xk:n ,
and, in general,
ˆβr = 1
n
n

k=r+1
(k −1) · · · (k −r)
(n −1) · · · (n −r)xk:n.
Unbiased estimates ˆλj of the L–moment λj are obtained by replacing the
probability weighted moments βk in (14.14) by the sample versions ˆβk. We have
ˆλj =
j−1

k=0
pj−1,k ˆβk ,
(14.17)
where, particularly, the ﬁrst sample L–moment ˆλ1 is the sample mean ¯x.
In this manner, on may also deﬁne the sample L–moment ratios ˆτj = ˆλj/ˆλ2
and the sample L–coeﬃcient of variation ˆτ = ˆλ2/ˆλ1.

348
14. Flood Frequency Analysis
L–Moment Estimation in the Pareto Model
We apply the L–moment estimation method
• to the Pareto model for excesses with unknown scale and shape parameters
σ and γ,
• to the reduced Pareto model in (14.10) for excess degrees in conjunction with
the index ﬂood procedure.
It turns out that both approaches are equivalent.
The ﬁrst two L–moments of a Pareto df Wγ,u,σ are
λ1
=
u + σ/(1 −γ),
λ2
=
σ/((1 −γ)(2 −γ)),
for γ < 1. Notice that u = 0, if we deal with excesses yi.
L–moments estimates of the parameters γ and σ are solutions to
ˆλ1(y)
=
u + σ/(1 −γ),
ˆλ2(y)
=
σ/((1 −γ)(2 −γ)),
where ˆλj(y) are the sample L–moments based on the excesses yi. The solutions
are
ˆγ(y)
=
2 −¯y/ˆλ2(y),
ˆσ(y)
=
(1 −ˆγ(y))¯y.
(14.18)
Within the model (14.10) for excess degrees we base the estimation of γ on
the 2nd L–moment λ2. One gets the estimate
˜γ(z) = 2 −1/ˆλ2(z),
where z is the sample of excess degrees zi = yi/¯y. It is apparent that
˜γ(z) = ˆγ(y)
(14.19)
and, therefore, the L–moment method applied to the excesses yi and, respectively,
the index ﬂood procedure in conjunction with the L–moment method applied to
the excess degrees zi leads to the same estimates of σ and γ within the Pareto
model for excesses.
Simulations show that the L–moment estimator of γ has a better performance
than the MLE in the model (14.10) for small and moderate sample sizes, if γ is
around zero.

14.5. A Bayesian Approach to Regional Estimation
349
L–Moment Estimation in the Extreme Value Model
The ﬁrst two L–moments and the L–skewness parameter of the EV df Gγ,µ,σ are
λ1
=
µ −σ(1 −Γ(1 −γ))/γ,
λ2
=
−σ

1 −2γ
Γ(1 −γ)/γ,
τ3
=
2

1 −3γ
/

1 −2γ
−3 .
(14.20)
If γ is known, then
σ =
−λ2γ

1 −2γ
Γ(1 −γ),
(14.21)
and
µ = λ1 + σ(1 −Γ(1 −γ))/γ .
(14.22)
Thus, to get the L–moment estimates of γ, µ, σ, replace λ1, λ2, τ3 by the sam-
ple versions and compute a numerical solution to the third equation in (14.20).
14.5
A Bayesian Approach to
Regional Estimation
In this section we give an outline of an approach to regional ﬂood frequency analysis
which originates in a paper by G. Kuczera7.
A Bayesian At–Site Analysis
Assume that the annual ﬂoods at a given site are distributed with common log–
normal df F(µ,σ)(x) = Φµ,σ(log(x)) as deﬁned in (1.60), where Φµ,σ is the normal
df with location and scale parameters µ and σ > 0. For such a distribution, the
T –year ﬂood (T –year level) is
u(T )
=
F −1
(µ,σ)(1 −1/T )
=
exp

µ + σΦ−1(1 −1/T )

.
(14.23)
The estimation of u(T ) will be carried out in the normal model. If z1, . . . , zn
are the annual ﬂoods then—according to the deﬁnition of the log–normal df—the
transformed data yi = log(zi) are governed by the normal df Φµ,σ. The T –year
ﬂood u(T ) will be estimated by replacing in (14.23) the parameters µ and σ by
the sample mean ¯y and a Bayesian estimate of σ2 based on y = (y1, . . . , yn).
In the sequel, the normal dfs are represented by the variance ϑ = σ2 and
the unknown mean µ is replaced by the sample mean ¯y. The latter corresponds to
7Kuczera, G. (1982). Combining site–speciﬁc and regional information: an empirical
Bayes approach. Water Resour. Res. 18, 306–314.

350
14. Flood Frequency Analysis
taking a non–informative prior for µ. For the Bayesian estimation of ϑ we take a
reciprocal gamma density
pa,b(ϑ) =
1
bΓ(a)(ϑ/b)−(1+a) exp(−b/ϑ),
ϑ > 0,
as a prior for ϑ, see (3.43). Such densities are conjugate priors for ϑ. The posterior
density is
pa,b(ϑ|y) = pa′,b′(ϑ)
with
a′ = a + n/2
and
b′ = b + ns2/2,
where s2 = (1/n) 
i≤n(yi −¯y)2.
The Bayesian estimate of ϑ = σ2—as the mean of the posterior distribution—
is
ˆσ2 =
b + ns2/2
a + n/2 −1 .
(14.24)
Therefore, one gets
ˆu(T ) = exp

¯y + ˆσΦ−1(1 −1/T )

as an estimate of the T –year ﬂood u(T ).
Regional Moment Estimation of the Superparameters
Next, we assume that data sets of size nj are recorded at k diﬀerent sites. We want
to use the regional information to estimate the superparameters a and b.
Let ˆϑj be estimates of the unknown parameter ϑj at site j. From the Bayesian
viewpoint (see page 243), the parameter ϑj was generated under a prior density pa,b
which is the density of a random parameter θ. One may also regard the estimates
ˆϑj as realizations under pa,b. It remains to estimate the superparameters a and b
from ˆϑj, j = 1, . . . , k.
This will be exempliﬁed by applying the moment estimation method (see
page 86) to the model of reciprocal gamma distributions. Because the mean and
the variance of the reciprocal gamma distribution are given by m(a, b) = b/(a −1)
and var(a, b) = b2/

(a −1)2(a −2)

(see page 104) we have
b = (a −1)m(a, b)
and
a = 2 + m(a, b)2/var(a, b)
for a > 2. Replacing m(a, b) and var(a, b) by the sample versions based on the ˆϑj,
j = 1, . . . , k, one gets estimates ˆak and ˆbk of the superparameters a and b. This
leads to a new estimator in (14.24) and a regional estimate of the T –year ﬂood.

14.5. A Bayesian Approach to Regional Estimation
351
Reﬁned Regional Approach
It is more realistic to assume that the random parameters θj at sites j are not
identically distributed. Suppose that
θj = xjβ′ + εj,
(14.25)
where xj represents the known characteristics at site j and the εj are residuals.
Thus, the prior distribution at site j has the parameters a(xj) and b(xj). In
addition, let the estimates at sites j have the representations
ˆϑj = ϑj + ηj
(14.26)
which yields that
ˆϑj = xjβ′ + εj + ηj .
(14.27)
Within this multiple, linear regression problem one may get estimates of the mean
and the variance of the prior distribution at site j and, therefore, of the superpa-
rameters a(xj) and b(xj) by using again the moment estimation method.
Further Applications of Empirical Bayes Estimation
in Regional Analysis
The Bayesian argument in Kuczera’s article was taken up by H.D. Fill and J.R.
Stedinger8 to justify forming a linear combination of two estimators with weights
inversely proportional to the estimators variance. This is done within the annual
maxima framework.
In this context, we also mention an article by H. Madsen and D. Rosbjerg9
who apply the regression technique in conjunction with empirical Bayes estimation
to regional partial duration series. Both articles contain exhaustive lists of relevant
articles.
8Fill, H.D. and Stedinger, J.R. (1998). Using regional regression within index ﬂood
procedures and an empirical Bayesian estimator. J. Hydrology 210, 128–14.
9Madsen, D. and Rosbjerg, D. (1997). Generalized least squares and empirical Bayes
estimation in regional partial duration series index–ﬂood modeling. Water Resour. Res.
33, 771–781.

Chapter 15
Environmental Sciences
co–authored by R.W. Katz1
This chapter deals with the application of the statistics of extremes in the envi-
ronmental sciences. Related chapters include those dealing with ﬂood frequency
analysis in hydrology (Chapter 14) and large claims in the insurance industry
(Chapter 16). Consideration of extreme events commonly arises in the regulatory
process related to the environment, particularly the selection of thresholds which
aid in determining compliance and eﬀectiveness. Statistical characteristics typical
of environmental extremes include annual and diurnal cycles, as well as trends
possibly attributable to anthropogenic activities.
One commonly unappreciated aspect in extreme value analysis for environ-
mental variables is the potential of making use of covariates, particularly geophys-
ical variables. Their incorporation into the analysis makes the resultant models
both more eﬃcient, e.g., in terms of quantile estimation, and more physically re-
alistic.
Another feature, beyond the scope of this treatment, is the spatial dependence
of extremes typically exhibited by ﬁelds of environmental data.
15.1
Environmental Extremes
In much of the environmental sciences, particularly impact assessment, extreme
events play an important role. For example, earthquakes (and related tsunamis),
ﬁres, ﬂoods, or hurricanes can have devastating impacts, ranging from disturbances
1Institute for Study of Society and Environment, National Center for Atmospheric
Research, Boulder, Colorado, USA. NCAR is sponsored by the National Science Foun-
dation.

354
15. Environmental Sciences
in ecosystems to economic impacts on society as well as loss of life. Such features
indicate that there should be plentiful applications of the statistics of extremes to
the environmental sciences.
Environmental Policy and Regulation
The implementation of environmental policy requires the development of regu-
lations, such as the setting of standards for environmental variables by govern-
ment agencies. In particular, these standards involve the selection of high (or low)
thresholds, deﬁning an extreme event. These circumstances imply that extreme
value analysis ought to be an integral part of this process. Such standards could
involve statistics as simple as whether the annual maxima exceeds a high thresh-
old, e.g., in the context of air pollution. Yet they sometimes involve much more
intricate quantities, whose motivation from a purely statistical perspective is lack-
ing; e.g., the ozone standard set by the U.S. Environmental Protection Agency
is deﬁned in terms of the statistic, the daily average of the maximum of 8–hour
running means, and is based on whether the fourth highest value of this quantity
over a given year exceeds a threshold.
The monitoring of environmental variables to detect any trend in extremes,
possibly attributable to human activity, provides important information for pol-
icy makers. Speciﬁcally, possible shifts in the frequency and intensity of extreme
weather and climate events are one of the primary concerns under global climate
change as part of the enhanced greenhouse eﬀect.
Cycles
That environmental variables often possess marked diurnal and annual cycles is
well established. Consequently, it is commonplace for statistical models of envi-
ronmental time series to include an annual (and/or diurnal) component for the
mean (as well as sometimes for the standard deviation). Yet in the statistics of
environmental extremes such cyclical behavior is usually neglected. Although not
necessarily identical in form to that for the mean, cyclical behavior in extremes
ought to be anticipated. It is an inherent aspect of the process by which envi-
ronmental extremes arise, so that taking into account the cyclical modulation of
extremal characteristics would be physically appealing.
Trends
The monitoring and detection of trends in environmental extremes is important
for a number of reasons. For one thing, any trends in extremes might serve as
an early indicator of broader change, a catalyst for public policy intervention.
In particular, it can be argued that the response of ecosystems is most sensitive
to extreme events, not average conditions. On the other hand, it may be that
regulations already have been imposed to “control” the environmental variable,

15.1. Environmental Extremes
355
with the hope of observing a diminished trend, i.e., as a measure of the eﬀectiveness
of environmental regulation. Typically, trend analysis of environmental variables
focuses on the mean (as well as occasionally on the standard deviation), not the
extremes per se. Yet there is no inherent reason why the form of trend in extremes
need be identical to that for the mean for an example in which there is a trend
in the extremes, but apparently none in the mean, see the article by R.L. Smith
cited on page 342). So, for completeness, any trend in extremes ought to be directly
modeled.
Covariates
Such cycles and trends can be incorporated into extreme value analysis as co-
variates, in the form of simple deterministic functions of time, e.g., a sum of sine
waves. Still a noteworthy feature of environmental variables is the inﬂuence of
covariates, especially those geophysical in nature, which are not simply determin-
istic functions, but rather random variables themselves. For example, air pollution
concentration is eﬀected by the prevailing meteorological conditions. This feature
is well appreciated for statistics like the mean level of an environmental time se-
ries. So it would be natural to anticipate that environmental extremes are likewise
inﬂuenced by geophysical covariates. Yet it is less common to incorporate such
covariates into environmental extreme value analysis, at least partly because of a
lack of awareness by practitioners that this extension of extreme value analysis is
feasible (a notable exception is the book by Coles [9], pages 107–114). In Chapters
8 and 9 we made some technical preparations.
Not only would the introduction of covariates result in increased precision in
estimating high quantiles, but it would serve to make the statistics of extremes
more realistic from an environmental science perspective.
Tail Behavior
The distribution of the impacts, in monetary terms, associated with extreme en-
vironmental events—e.g., a hurricane or a ﬂood—has a marked tendency to be
heavy–tailed. Yet the distribution of the underlying geophysical phenomenon may
not necessarily be heavy–tailed. Some geophysical variables, such as streamﬂow
or precipitation, are clearly heavy–tailed; whereas others, such as wind speed or
temperature, appear to have a light or bounded upper tail. Certain environmental
variables, such as air pollution concentration, also do not appear to be heavy–
tailed.
Because of the complex coupling among these processes (i.e., geophysical,
environmental, and economic impact), the determination of the origin of extremal
behavior in environmental impacts is non–trivial. In particular, the extent to which
this heavy–tailed behavior in environmental impacts is “inherited” from the tail
behavior of underlying geophysical and/or environmental variables is unclear. Re-
calling the origin of the Pareto distribution as a model for the distribution of

356
15. Environmental Sciences
income or wealth, the heavy tail behavior of environmental impacts could well
reﬂect primarily the aggregative nature of variables like income or wealth.
15.2
Inclusion of Covariates
In principle, the inclusion of covariates is feasible in any of the various approaches
to extreme value analysis covered in this book, including both block maxima and
peaks–over–threshold (pot). If the technique of maximum likelihood is adopted,
then parameter estimation remains straightforward. Nevertheless, although the sit-
uation closely resembles that of generalized linear models [41], the well developed
theory of estimation in that situation is not directly applicable. The extremal–
based approach used here would not necessarily diﬀer much from ordinary least
squares in terms of point estimates. But substantial discrepancies would be pos-
sible for standard errors (or conﬁdence intervals) for upper quantile estimators,
particularly when dealing with a variable possessing a heavy–tailed distribution.
Block Maxima
Consider the EV df Gγ,µ,σ as the approximate distribution for block maxima,
say corresponding to a random variable Y with observed data y1, . . . , yn (e.g.,
annual maximum of time series such as daily precipitation amount). Suppose that
a covariate X is also available, say with observed data, x1, . . . , xn, on the same
time scale t = 1, . . . , n as the block maxima. Given a value of covariate, say X = x,
the conditional distribution of the block maxima is assumed to remain the EV,
but now with parameters that possibly depend on x.
As an example, the location parameter µ and the logarithm of the scale pa-
rameter σ could be linear functions of x (applying the logarithmic transformation
to preserve non–negativity of scale), whereas for simplicity the shape parameter γ
might well be taken independent of the value x:
γ(x) = γ, µ(x) = µ0 + µ1x, log σ(x) = σ0 + σ1x .
(15.1)
Here µ0, µ1, σ0, σ1 and γ are unknown parameters to be estimated by maximum
likelihood. Now the parameters of the EV df in (15.1) depend on a covariate which
varies with time, writing γt = γ(xt), µt = µ(xt), and σt = σ(xt). For technical
details we refer to Section 8.4.
This approach amounts to ﬁtting non–identical EV dfs, whereas the conven-
tional diagnostic displays (e.g., Q–Q plots) are predicated upon identical distribu-
tions. This non–stationarity can be removed by using the relationship between an
arbitrary EV df and the standard Gumbel, i.e., EV0 df. Namely, if Yt has an EV
df with parameters γt, µt, and σt, then
εt = (1/γt) log(1 + γt(Yt −µt)/σt)
(15.2)

15.2. Inclusion of Covariates
357
has a G0,0,1 df (see Chapter 1). In practice, “residuals” can be obtained by substi-
tuting the time–dependent parameter estimates ˆγt, ˆµt, and ˆσt, e.g., as in (15.1),
along with the corresponding data (i.e., xt’s), into (15.2). Then a Q–Q plot, for
instance, can be constructed for a standard Gumbel distribution.
Peaks–Over–Threshold Approach
The potential advantages of the pot approach over the block maxima approach
become more readily apparent in the presence of covariates (for an early treatment
of that question see the article by Davison and Smith, mentioned on page 121,
in Section 3 about regression). For instance, allowing for an annual cycle in the
block maxima approach would not even be feasible, except in an ad hoc and non–
parsimonius manner such as taking block maxima over months or seasons. More
generally, many physically–based covariates would naturally be measured on a time
scale similar, if not identical, to that of the variables whose extremal behavior is
being modeled (e.g., covariates such as pressure readings on a daily time scale
in the case of modeling extreme high daily precipitation amounts). On the other
hand, additional complications arise in the pot approach with covariates, including
the need in some circumstances to permit the threshold to be time varying.
Poisson–GP (Two–Component) Model. By “two-component model” we refer
to the situation in which the two individual components, the Poisson process—
with rate parameter λt—for the occurrence of exceedances of the high threshold u
and the GP df (with shape parameter γt and scale parameter σt[u], which depends
on the threshold u unlike the scale parameter for the EV df) for the excess over
the threshold, are modeled separately. This approach was originally introduced
in hydrology, for instance, to incorporate annual cycles in one or both of the
components. This is the description of an exceedance process with an homogeneous
Poisson process, see Section 9.1, or an inhomogeneous Poisson process, see Section
9.5, in the time scale and GP marks for the exceedances themselves. Analogous to
the case of block maxima (15.1), it would be natural to express log λt, log σt[u],
and perhaps γt as functions of a covariate.
For the statistical validation of the model, the non–stationarity in the GP df
can be removed by using the relationship between an arbitrary GP distribution
and the standard exponential (i.e., GP0 df), corresponding to (15.2) for EV dfs.
Namely, if Yt has a GP df with parameters λt and σt[u], then
εt = (1/γt) log(1 + γtYt/σt[u])
(15.3)
has a W0,0,1 df (see Chapter 1). With marked dependence of the extremal charac-
teristics on a covariate (e.g., in the case of a substantial trend or cycle), it may be
that the threshold itself ought to vary with the covariate, i.e., with time t, writing
ut instead of u. For simplicity, we ignore this complication in the notation used
here.

358
15. Environmental Sciences
Point Process Approach. By “point process approach,” we refer here to the
treatment of the problem as a full–ﬂedged two–dimensional, non–homogeneous
point process, rather than modeling the two components separately2. Here the
one–dimensional Poisson process for the occurrence of exceedances and the GP
df for the excesses are modeled simultaneously, see Section 9.5, page 260. This
approach has the advantage treating all of the uncertainty in parameter estimation
in a uniﬁed fashion, e.g., making it more straightforward to determine conﬁdence
intervals for return levels.
Interpretation can be enhanced if the point process is parameterized in terms
of the EV df. To do this, it is convenient to specify a time scaling parameter, say
h > 0, e.g., h ≈1/365.25 for annual maxima of daily data. Then the parameters of
the point process, λt (i.e., rate parameter per unit time), γt, and σt[u], are related
to the parameters of the EV df, γt, µt, and σt, by
log λt = −(1/γt) log[1 + γt(u −µt)/σt)],
σt[u] = σt + γt(u −µt),
(15.4)
with the shape parameter γt being equivalent3. Now the model has at least a lim-
ited theoretical interpretation as arising as the limiting distribution which would
be obtained if many observations were available at a ﬁxed time t (instead of just
one as in practice) and the largest value were taken. The nonlinearity and interac-
tion in the relationships in (15.4) imply that expanding some of the parameters of
the point process components as simple functions of a covariate would not neces-
sarily correspond to nearly such a simple model in terms of the EV df parameters,
see Section 9.5 for technical details.
Another question concerns how to extend the concepts of return period and
level which originated in the context of stationarity, i.e., no covariates. One could
always think in terms of an “eﬀective” return period and level, consistent with
the theoretical interpretation just discussed. We defer the discussion of possible
extensions of these concepts until they arise in the context of speciﬁc examples
later in this chapter.
Finally, the choice of the time scaling constant h should be viewed as arbi-
trary. Speciﬁcally, δ = h/h∗denotes the ratio between two diﬀerent time scaling
parameters (e.g., annual versus monthly maxima), then the corresponding param-
eters of the EV df (say γ, µ, and σ for time scale h versus γ∗, µ∗, and σ∗for time
scale h∗) are related by4
γ∗= γ,
µ∗= µ + [σ∗(1 −δ−γ)]/γ,
σ∗= σδγ .
(15.5)
2Smith, R.L. (2001). Extreme value statistics in meteorology and the environment. In
Environmental Statistics, Chapter 8, 300–357 (NSF–CBMS conference notes). Available
at www.unc.edu/depts/statistics/postscript/rs/envnotes.pdf.
3Katz, R.W., Parlange, M.B. and Naveau, P. (2002). Statistics of extremes in hydrol-
ogy. Advances in Water Resources 25, 1287–1304.
4Katz, R.W., Brush, G.S. and Parlange, M.B. (2005). Statistics of extremes: Modeling
ecological disturbances. Ecology 86, 1124–1134.

15.3. Example of Trend
359
15.3
Example of Trend
Fitting a trend constitutes one of the simplest forms of covariate analysis. By
means of an example, we contrast the traditional approach of trend analysis to
that more in harmony with extreme value theory.
Traditional Approach
The traditional approach to trend analysis of environmental extremes would in-
volve ﬁtting a trend, ordinarily by the method of least squares, in the mean (and
possibly in the standard deviation) of the original time series. Then the extreme
value analysis is applied to residuals, presumed stationary, from this trend analysis
(see Chapter 4). There are several possible drawbacks to such an approach. For
one thing, it is in eﬀect assumed that removing a trend in the overall mean would
necessarily eliminate any trend in extremes as well. For another, such a two–stage
estimation approach makes it diﬃcult to account for all of the uncertainty in pa-
rameter estimation, with the error involved in constructing the residuals being
usually neglected.
Extreme Value Approach
In the extreme value approach, we simply expand the parameters of the EV df
as functions of time, e.g., as in (15.1). Of course, determining the appropriate
functional form of trend is not necessarily a routine matter. It should further be
recognized that such an analysis is not based on any statistical theory of extreme
values for non–stationary time series per se. But recall the general interpretation
for an EV df with time–dependent parameters as discussed earlier.
Example
In contrast to global warming, the “urban heat island” refers to a warming eﬀect
on the local climate attributable to urbanization. This phenomenon is well docu-
mented in terms of its eﬀects on mean daily minimum and maximum temperature.
Among other things, the warming trend is more substantial for minimum, rather
than maximum, temperature.
So we analyze the extremes for a time series of daily minimum temperature
during summer (i.e., July–August) at Phoenix, Arizona, USA, for the 43–year
time period 1948–19905. This area experience rapid urbanization during this time
period, with a substantial trend in the mean of the distribution of daily minimum
temperature being apparent. For simplicity, we adopt a block minima approach;
strictly speaking, ﬁtting the EV df for block maxima to negated block minima and
5Tarleton, L.F. and Katz, R.W. (1995). Statistical explanation for trends in extreme
temperatures at Phoenix, Arizona. Journal of Climate 8, 1704–1708.

360
15. Environmental Sciences
then converting the results back into the appropriate form for block minima, see
Chapter 1, with the time series of summer minima being shown in Fig. 15.1.
50
60
70
80
90
year
65
70
75
minimum temperature (oF)
median
95% limits
Fig. 15.1. Time series of summer
(July–August) minimum of daily
minimum temperature at Phoenix,
Arizona, USA, 1948–1990. Also in-
cludes trend in conditional median
of ﬁtted EV df (i.e., eﬀective two–
year return level) along with point-
wise approximate 95% conﬁdence
limits.
Table 15.1 summarizes the results of ﬁtting the EV df to block minima by
the method of maximum likelihood, with trends in the parameters of the form:
γt = γ,
µt = µ0 + µ1t,
log σt = σ0 + σ1t,
for t = 1, . . . , 43.
(15.6)
Three nested candidate models are compared:
(i) µ1 = σ1 = 0, i.e., no trend in either µ or σ;
(ii) µ1 ̸= 0, σ1 = 0, i.e., trend in µ but not in σ;
(iii) µ1 ̸= 0, σ1 ̸= 0, i.e., trends in both µ and σ.
The negative log–likelihoods (denoted by −log L in the table) indicate that model
(ii) with a trend in only the location parameter is the superior ﬁtting model.
Table 15.1. Maximum likelihood estimates of parameters (with standard errors of trend
parameters given in parentheses) of EV df ﬁtted by block minima approach for trend
analysis of extreme low summer (July–August) temperatures (◦F) at Phoenix, Arizona,
USA, 1948–1990.
model
ˆγ
ˆµ0
ˆµ1
ˆσ0
ˆσ1
−log L
µ1 = σ1 = 0
−0.184
70.88
0
1.346
0
121.52
µ1 ̸= 0, σ1 = 0
−0.204
66.05
0.202 (0.041)
1.135
0
111.52
µ1 ̸= 0, σ1 ̸= 0
−0.211
66.17
0.196 (0.041)
1.338
−0.009 (0.010)
111.11

15.4. Example of Cycle
361
The estimated slope of the trend in the location parameter is ˆµ1 = 0.202 ◦F
per year for model (ii), or an estimate of the urban warming eﬀect in terms of
extreme low temperatures. Fig. 15.1 shows the corresponding trend in the median,
i.e., eﬀective two–year return level, of the conditional EV df for the best ﬁtting
model (ii). Pointwise 95% local conﬁdence intervals for the conditional median are
also included in Fig. 15.1. Using the expression for the return level for the lowest
temperature during the tth summer as a function of the corresponding parameters
of the EV df (see Chapter 1), these intervals can be derived from the large–sample
standard errors for the maximum likelihood estimators.
A Q–Q plot for model (ii) based on (15.2) appears reasonably satisfactory
(not shown). Nevertheless, the pattern in Fig. 15.1 suggests that an alternative
form of trend in the location parameter, such as an abrupt shift, might be more
appropriate. The same conclusion concerning the existence of a trend in extreme
low summer temperatures is reached if the point process approach were applied
instead.
15.4
Example of Cycle
Fitting cycles constitutes another simple form of covariate analysis. By means of
an example, we contrast the traditional approach of seasonal analysis to that more
in harmony with extreme value theory, as well as indicating an inherent advantage
of the point process approach over block maxima.
Traditional Approach
The traditional approach to cyclic analysis of environmental extremes would in-
volve ﬁtting a cyclic function (e.g., sine wave), ordinarily by the method of least
squares, to the mean (and possibly to the standard deviation) of the original time
series. Then the extreme value analysis is applied to the residuals from the cyclic
analysis. The drawbacks to this approach are identical to those for traditional
trend analysis; namely, the possibility of a diﬀerent cyclic form for extremes than
that for the mean and standard deviation, as well as the lack of an integrated
treatment of uncertainty.
Extreme Value Approach
Like the extreme value approach for modeling trends, we simply expand the pa-
rameters of the EV df as functions, in this case periodic, of time. For instance, in
the following example, the location parameter and logarithm of the scale parame-
ter of the EV df are both modeled as sine waves, whereas the shape parameter is
assumed independent of time.

362
15. Environmental Sciences
Example
In many regions, daily precipitation exhibits marked seasonality. Such cycles are
well documented for both the probability of occurrence of precipitation and for
the conditional mean (or median) amount of precipitation given its occurrence.
Yet the seasonality in precipitation extremes is not typically explicitly taken into
account in extreme value analysis of precipitation (e.g., to estimate upper quantiles
of precipitation amount associated with ﬂooding).
So we analyze the extremes for a time series of daily precipitation amount at
Fort Collins, Colorado, USA, for the 100–year time period 1900–1999 (also see the
article by Katz et al. (2002) mentioned on page 358). For such a long time series,
it is convenient to summarize the data in terms of block maxima, with Fig. 15.2
showing the derived time series of annual maxima. Nevertheless, we will actually
apply the point process approach to model the extremal behavior with seasonality
being permitted. Although much year–to–year variation is present, no long–term
trend is evident in Fig. 15.2. Because of a ﬂood which occurred on 28 July 1997
(with an observed value of 4.63 inches), this data set is of special interest.
1900
1950
2000
year
0
2
4
precipitation (in)
Fig. 15.2. Time series of annual
maximum of daily precipitation
amount at Fort Collins, Colorado,
USA, 1900–1999.
Table 15.2 summarizes the results of ﬁtting the EV df indirectly through the
point process approach by the method of maximum likelihood. Annual cycles in
the form of sine waves are incorporated for the location parameter and logarithm
of the scale parameter:
γt
=
γ,
µt
=
µ0 + µ1 sin(2πt/T ) + µ2 cos(2πt/T ),
log σt
=
σ0 + σ1 sin(2πt/T ) + σ2 cos(2πt/T ),
(15.7)
for t = 1, . . . , 36524 with time scaling constant h = 1/T, T ≈365.25.

15.4. Example of Cycle
363
The threshold of u = 0.395 inches was chosen by trial and error, perhaps
a bit lower than optimal for wetter times of the year, a compromise to avoid
the complication of a time varying threshold. The parameter estimates are given
for the case of no declustering (with little evidence, in general, of the need for
declustering daily precipitation extremes), but quite similar results are obtained
when runs declustering with r = 1 is applied (see Chapter 2).
Three nested candidate models are compared in Table 15.2:
(i) µ1 = µ2 = σ1 = σ2 = 0, i.e., no cycle in either µ or σ;
(ii) µ1 ̸= 0, µ2 ̸= 0, σ1 = σ2 = 0, i.e., cycle in µ but not in σ;
(iii) µ1 ̸= 0, µ2 ̸= 0, σ1 ̸= 0, σ2 ̸= 0, i.e., cycles in both µ and σ.
The negative log–likelihoods (denoted by −log L in Table 15.2) indicate that model
(iii) with annual cycles in both the location parameter and the logarithm of the
scale parameter of the EV df is the superior ﬁtting model. A Q–Q plot for model
(iii) based on (15.2), comparing the observed annual maxima to the EV df ﬁtted
indirectly by the point process approach, appears reasonably satisfactory (not
shown). A more complex form of extremal model might include a sine wave for
the shape parameter as well, or a sum of more than one sine wave for the location
parameter and/or logarithm of the scale parameter.
Table 15.2. Maximum likelihood estimates of parameters of EV df (parameterized in
terms of annual maxima, time scaling constant h = 1/365.25) ﬁtted by point process
approach (threshold u = 0.395 inches) for annual cycle in extreme high daily precipitation
amount (inches) at Fort Collins, Colorado, USA, 1900-1999.
model
ˆγ
ˆµ0
ˆµ1
ˆµ2
ˆσ0
ˆσ1
ˆσ2
−log L
µ1 = µ2 = σ1 = σ2 = 0
0.212 1.383
0
0
−0.631
0
0
−1359.82
σ1 = σ2 = 0
0.103 1.306
0.082
−0.297 −0.762
0
0
−1521.45
no contraints
0.182 1.281 −0.085 −0.805 −0.847 −0.124 −0.602 −1604.29
For the best ﬁtting model (iii), Fig. 15.3 shows the eﬀective 100–year return
level, with the parameters of the EV df being rescaled for sake of comparison
using (15.5) to reﬂect the maximum of daily precipitation amount over a month
instead of a year (i.e., time scaling constant h∗= 12/365.25). These estimated
return levels range from a low in mid January of about 1.1 inches to a high in
mid July of about 4.3 inches. To give a rough feeling for the actual annual cycle
in extreme precipitation, the observed monthly maximum of daily precipitation
is also included in Fig. 15.3. Consistent with the eﬀective return levels for the
ﬁtted EV df, a marked tendency is evident toward higher precipitation extremes
in summer than in winter.
It is also of interest to estimate the return period for the high precipitation in
July 1997. With annual cycles in the parameters of the EV df, the determination

364
15. Environmental Sciences
0
100
200
300
day
0
1
2
3
4
5
precipitation (in)
Fig. 15.3. Annual cycle in eﬀective
100–year return level for ﬁtted EV
distribution for monthly maximum
of Fort Collins daily precipitation.
Observed values of monthly max-
imum of daily precipitation indi-
cated by circles.
of a return period involves a product of probabilities that diﬀer depending on
the day of year (i.e., of the form p1 × · · · × pT instead of simply pT , where pt
denotes the probability on the tth day of the year); for simplicity, treating the
daily precipitation amounts as independent. Reﬁtting only the data for the time
period before the ﬂood, i.e., 1900–1996, using model (iii), the estimated return
period for the observed daily amount of 4.63 inches is roughly 50.8 years.
15.5
Example of Covariate
Fitting covariates which are random variables themselves, such as geophysical
quantities, is no more involved than the examples of trends and cycles just pre-
sented. Yet such examples are more compelling, both in terms of the potential
predictability of extremes (e.g., reﬂected in terms of conﬁdence intervals for re-
turn levels whose length varies conditional on the covariate) and in terms of its
appeal from an environmental science perspective.
Traditional Approach
Even in ﬁelds such as hydrology with a rich tradition of reliance on extreme value
analysis, an inconsistency arises with extreme value theory typically being aban-
doned in favor of ordinary regression analysis, either for the entire data set or
for a subset consisting only of extremes, if covariates are treated. As an ad hoc
approach, sometimes an ordinary extremal analysis is conducted separately de-
pending on a few discretized categories of the covariate (e.g., “above average” or
“below average”). The disadvantages of such traditional approaches are somewhat
subtle, with the central issue being the lack of a robust treatment of extremes.

15.5. Example of Covariate
365
Extreme Value Approach
Just like the extreme value approach for modeling trends or cycles, we expand the
parameters of the EV df as functions of one or more physically based covariates.
But now the issue of how to specify the functional form of such relationships is
particularly challenging, ripe for input from environmental or geophysical scien-
tists.
Example
It is well understood that heavy precipitation is associated with particularly me-
teorological conditions, involving other variables such as pressure. To demonstrate
the viability of incorporating physically based covariates into extremal analysis, a
prototypical example is considered. This example is not intended to constitute a
realistic treatment of the various meteorological factors known to have an inﬂuence
on heavy precipitation.
So we analyze the extremes for a time series of daily precipitation amount at
Chico, California, USA, during the month of January over the time period 1907–
1988, with fours years have been eliminated from the data set because of missing
values (also see the paper by Katz et al. mentioned on page 362). As a covariate,
the January average sea level pressure at a single grid point (actually derived from
observations within a grid box) over the Paciﬁc Ocean adjacent to the California
coast is used, say a random variable denoted by Y . Fig. 15.4 shows a scatterplot of
the January maxima of daily precipitation amount versus the pressure covariate,
with at least a weak tendency for lower maximum precipitation when the average
pressure is higher being apparent.
Rather than using monthly block maxima (as, for simplicity, in Fig. 15.4),
we actually ﬁt the EV df indirectly through the point process approach by the
method of maximum likelihood, with the results being summarized in Table 15.3.
Given average pressure X = x, the location parameter and the logarithm of the
scale parameter of the conditional EV df are assumed linear functions of y:
γ(x) = x,
µ(x) = µ0 + µ1x,
log σ(x) = σ0 + σ1x .
(15.8)
These forms of functional relationship are intended solely for illustrative purposes,
not necessarily being the most physically plausible. Because daily time series for
the single month of January are being modeled, a time scaling constant of h = 1/31
is used. The threshold of u = 40 mm was selected by trial and error, with no
declustering being applied.
Three nested candidate models are compared in Table 15.3:
(i) µ1 = σ1 = 0, i.e., no variation with y;
(ii) µ1 ̸= 0, σ1 = 0, i.e., µ varies with y, but σ does not;
(iii) µ1 ̸= 0, σ1 ̸= 0, i.e., both µ and σ vary with y.

366
15. Environmental Sciences
The negative log–likelihoods (denoted by −log L in Table 15.3) suggest that model
(ii), with only location parameter of the conditional EV df depending on pressure,
is the superior ﬁtting model. In particular, a likelihood ratio test for model (ii)
versus model (iii) indicates only weak evidence that the scale parameter ought to
be varied as well (p–value ≈0.209). A Q–Q plot for model (ii) based on (15.2),
comparing the observed monthly maxima to the EV df ﬁtted indirectly by the
point process approach, appears reasonably satisfactory (not shown).
Table 15.3. Maximum likelihood estimates of parameters of EV df (parameterized in
terms of annual maximum, time scaling constant h = 1/31) ﬁtted by point process
approach (threshold u = 40 mm) to daily precipitation amount (mm) at Chico, California,
USA, conditional on pressure covariate (mb, with 1000 mb being subtracted), 1907–1988.
model
ˆγ
ˆµ0
ˆµ1
ˆσ0
ˆσ1
−log L
µ1 = σ1 = 0
0.198
35.49
0
2.226
0
244.10
µ1 ̸= 0, σ1 = 0
0.151
58.13
−1.361
2.315
0
235.27
µ1 ̸= 0, σ1 ̸= 0
0.199
58.15
−1.284
2.979
−0.045
234.49
The estimated slope parameter in model (ii) is ˆµ1 = −1.361 mm per mb, or
higher precipitation extremes being associated with lower pressure. For the best
ﬁtting model (ii), the conditional median of the ﬁtted EV df (i.e., eﬀective two–year
return level) is also included in Fig. 15.4. Another way of interpreting the ﬁtted
model is in terms of eﬀective return periods. For model (i) (i.e., no dependence
on pressure), the estimate of the conventional 20–year return level is 73.0 mm.
But this return level would correspond to eﬀective return periods, based on ﬁtted
model (ii), ranging from 8.1 years for the lowest observed pressure to 44.9 years
for the highest.
1010
1015
1020
1025
pressure (mb)
20
60
100
140
precipitation (mm)
Fig. 15.4. Scatterplot of January max-
imum of daily precipitation amount
at Chico, California, USA, versus Jan-
uary mean pressure. Also includes con-
ditional median of ﬁtted EV df (i.e.,
eﬀective 2–year return level).

15.6. Numerical Methods and Software
367
One natural extension of this example would be to allow the pressure covari-
ate to vary from one day to the next, in meteorological terminology considering
high frequency variations instead of just low frequency ones. The relationship be-
tween precipitation extremes and pressure could be simultaneously modeled over
the entire year, but such a model might well entail the complication of a diﬀerent
relationship depending on the time of year to be physically realistic.
15.6
Numerical Methods and Software
Although not diﬃcult in principle to program, most existing software routines
for extreme value analysis do not make any provision for the incorporation of
covariates, perhaps another explanation for alternatives to extreme value theory
still being prevalent in applications. So a brief description focused on techniques
for maximum likelihood estimation and on the available software is now provided.
Maximum Likelihood Estimation
Because the expansion of the parameters of extremal models in terms of one or
more covariates can be readily incorporated in maximum likelihood estimation, the
expressions for the likelihood function of the EV df, whether for the block maxima
or point process approaches, are not repeated here for this speciﬁc situation (but
see Chapter 5). It should be acknowledged that the presence of covariates makes
the routine reliance on iterative numerical methods more problematic, with the
possibility of multiple local maxima, etc.
Statistical Software
Software for extreme value analysis, which does make provision for covariates,
includes the suite of S functions provided as a companion to the text by Stuart
Coles [9]. These functions allow for covariates in ﬁtting the EV df to block maxima,
the GP df to excesses over a high threshold, and the EV df indirectly through the
point process approach. Based on these same S functions (ported into R), the
Extremes Toolkit provides a graphical interface so that users, particularly in the
environmental and geophysical sciences, unfamiliar with R or S can still make use
of the Coles software (gateway: www.issse.ucar.edu/extremevalues/extreme.html).

Part V
Topics in
Finance and Insurance

Chapter 16
Extreme Returns in
Asset Prices
co–authored by
C.G. de Vries1 and S. Caserta2
Throughout this chapter, we assume that speculative prices st like those per-
taining to stocks, foreign currencies, futures etc. are evaluated at discrete times
t = 0, 1, 2, . . ., where the periods can be days or weeks. Thus, if s0 is the price of an
investment at time t = 0, then the return—the diﬀerence of prices taken relatively
to the initial price—at time T is (sT −s0)/s0. Our primary interest concerns daily
returns under discrete compounding (arithmetic returns)
˜rt = st −st−1
st−1
or the daily returns under continuous compounding (log–returns)
rt = log(st) −log(st−1).
(16.1)
These quantities are close to each other if the ratio st/st−1 is close to 1. We will
focus on the latter concept. Log–returns are also called geometric returns in the
ﬁnancial literature.
The speculative return series generally exhibits two empirical properties. The
ﬁrst property is that the variability of the rt shows a certain clustering which makes
1Tinbergen Institute and Erasmus University Rotterdam; co–authored the 1st and 2nd
edition.
2Tinbergen Institute and Erasmus University Rotterdam; co–authored the 2nd edition.

372
16. Extreme Returns in Asset Prices
the variance somewhat predictable. The other feature is that the random log–prices
log St satisfy the martingale property. Both of these properties will be discussed in
greater detail. Keep in mind that we write Rt and St when stochastic properties
of random returns and prices are dealt with.
In Section 16.1, we collect stylized facts about ﬁnancial data and give refer-
ences to the history of statistical modeling of returns. In Section 16.2, daily stock
market and exchange rate returns are visualized to get a ﬁrst insight into ﬁnan-
cial data. The pot–methodology is applied in Section 16.3 to estimate the upper
and lower tails of log–returns and cumulated return dfs. The loss/proﬁt variable
is represented by means of return variables in Section 16.4. Such representations
make the statistical results of Section 16.3 applicable to the loss/proﬁt variable.
As an application, we estimate the q–quantile of the loss/proﬁt distribution in
Section 16.5 under diﬀerent statistical models for the returns. This is the so–called
Value–at–Risk (VaR) problem faced by ﬁnancial intermediaries like commercial
banks. A related question is to evaluate the amount of capital which can be invested
such that at time T the possible loss exceeds a certain limit l with probability q.
Section 16.6 deals with the VaR methodology for a single derivative contract.
Finally, we focus our attention on the modeling of returns by ARCH/GARCH and
stochastic volatility (SV) series in Section 16.7 and predict the conditional VaR in
Section 16.8.
16.1
Stylized Facts and Historical Remarks
In this section, we collect some basic facts about returns series of ﬁnancial data.
We compare the relationship between arithmetic and log–returns, deal with the
weekend and calendar eﬀects and discuss the concept of market eﬃciency in con-
junction with the martingale property.
Arithmetic Returns and Log–Returns
The subsequent explanations about the advantage of using log–returns rt compared
to arithmetic returns ˜rt heavily relies on a discussion given by Philippe Jorion [33].
• For the statistical modeling, e.g., by means of normal dfs, it is desirable that
the range of the returns is unbounded. This property is merely satisﬁed by
log–returns because the arithmetic returns are bounded from below by −1.
• Some economic facts can be related to each other in a simpler manner by
using log–returns. For example, let st be the exchange rate of the U.S. dollar
against the British pound and let rt = log(st/st−1) be the pertaining log–
return. Then, the exchange rate of the British pound against the U.S. dollar
is 1/st. This yields that the log–return, from the viewpoint of a British
investor, is just −rt = log((1/st)/(1/st−1)).

16.1. Stylized Facts and Historical Remarks
373
• It is apparent that the price sT at time T can be regained from the daily
returns r1, . . . , rT and the initial price s0 by
sT = s0 exp
 
t≤T
rt

.
(16.2)
Conversely, the T –day log–return log sT −log s0 of the period from 0 to T is
the sum of the daily log–returns which is a useful property in the statistical
context.
Weekend and Calendar Eﬀects
Before we can analyze any speculative return series, we must deal with the fact
that returns are computed at equally–spaced moments in time, but that business
time does not coincide with physical time. For example, stock markets are closed
at night and during weekends. This poses a problem for the deﬁnition of returns
at higher frequencies. The following ad hoc procedures can be employed to deal
with the weekend eﬀect.
1. (Returns with Respect to Trading Days.) Just take the prices for the given
trading days and compute the returns.
2. (Omitting Monday Returns.) Omit the days for which prices are not recorded
including the consecutive day. Thus, after a weekend, the Monday returns
are also omitted.
3. (Distributing Monday Returns.) The return registered after a gap (e.g., the
return recorded on Monday) is equally distributed over the relevant days
(e.g., if r is the return on Monday, then r/3 is taken as the return on Saturday,
Sunday and Monday).
Distributing or omitting the Monday returns is a very crude method; a thor-
ough analysis and a reﬁned treatment of the weekend eﬀect may be desirable for
certain questions. Of course, one may also take just the original returns with the
potential disadvantage that the Monday may exhibit the behavior of returns accu-
mulated over three days. In the book [54] by S. Taylor there is a detailed discussion
of further calendar eﬀects such as the dependence of the mean and standard devi-
ation of returns on the day in the week or the month in the year. Such eﬀects will
be neglected in this chapter.
The economic background of ﬁnancial time series is also well described in the
books [1] and [12].
Market Eﬃciency and the Martingale Property
The martingale property of the random return series {Rt} derives from economic
insight. The returns {Rt} are regarded as martingale innovations, a property which

374
16. Extreme Returns in Asset Prices
is characterized by the condition that the conditional expectation of the future re-
turn Rt given the past returns value rt−1, . . . , r0 with r0 = log s0 (or, equivalently,
given the prices st−1, . . . , s0) is equal to zero.
On the contrary, suppose that
E(Rt|st−1, . . . , s0) ̸= 0
holds. Let as assume that the conditional expectation is positive so that the price
is expected to rise. In speculative markets, lots of arbitrageurs quickly eliminate
unfair gambles: buying at todays low price st−1 is more than a fair gamble. If all
agents have this information, they will all want to buy at st−1 in order to resell
at the higher expected price tomorrow. Yet if many try to buy, this already drives
up the price today and the expected gain disappears.
If the martingale property of returns is accepted, then the returns are neces-
sarily uncorrelated.
Some Historical Remarks
The hypothesis that logarithmic speculative prices form a martingale series was
ﬁrst raised by Bachelier3. Speciﬁcally, he assumed normally distributed returns.
Writing
Rt = µ + σWt
for the random log–returns, where the Wt are iid standard normal random vari-
ables, µ is the drift parameter, and σ > 0 is the so–called volatility, one arrives in
(16.2) at a discrete version of the famous Black–Scholes4 model, namely,
ST = S0 exp

µT + σ

t≤T
Wt

.
(16.3)
Later, Benoit Mandelbrot5 discovered that the speculative return series, i.e.,
the innovations of the martingale, are fat–tailed distributed. He suggested the mod-
eling of speculative returns by means of non–normal, sum–stable random variables
(see also [40], [1], and the book by F.X. Diebold6). But this latter model conﬂicts
with the fact that return series generally exhibit bounded second moments, see,
e.g., Akgiray and Booth7 and the literature cited therein.
3Bachelier, L.J.B.A. (1900). Thˆeorie de la Speculation. Gauthier–Villars, Paris.
4Black, F. and Scholes, M. (1973). The pricing of options and corporate liabilities. J.
Political Economy 81, 637–659.
5Mandelbrot, B.B. (1963). The variation of certain speculative prices. J. Business 36,
394–419.
6Diebold, F.X. (1989). Empirical Modeling of Exchange Rate Dynamics. Lect. Notes
in Economics and Math. Systems 303, Springer, Berlin.
7Akgiray, V. and Booth, G.G. (1988). The stable-law model of stock returns. J. Busi-
ness & Economic Statistics 6, 51–57.

16.2. Empirical Evidence in Returns Series
375
16.2
Empirical Evidence in Returns Series
Certain properties of ﬁnancial series of stock market returns and exchange rate
returns will be illustrated by means of scatterplots. One recognizes a clustering of
higher volatility of returns which will also be captured by plots of sample variances
over moving windows in the time scale (related to that what is done by means of
moving averages).
Stock Market and Exchange Rate Data
Next, we partially repeat and extend the analysis made by Loretan and Phillips8
concerning
• the Standard & Poors 500 stock market index from July 1962 to Dec. 1987
(stored in fm–poors.dat), and
• exchange rates of the British pound, Swiss franc, French franc, Deutsche
mark, and Japanese yen measured relatively to the U.S. dollar from Dec.
1978 to Jan. 1991 (stored in fm–exchr.dat9).
We visualize the Standard & Poors 500 stock market data and the exchange
rates of the British pound measured relatively to the U.S. dollar. Stock market
and exchange rate data exhibit a similar behavior so that a joint presentation is
justiﬁed.
Fig. 16.1. (left.) Log–index log st of Standard & Poors 500 stock market. (right.) Log–
exchange rate log st of British pound relative to U.S. dollar.
8Loretan, M. and Phillips, P.C.B. (1994). Testing the covariance stationarity of heavy–
tailed time series. J. Empirical Finance 1, 211–248.
9Extended data sets are stored in fm–poor1.dat (from June 1952 to Dec. 1994) and in
fm–exch1.dat (from Jan. 1971 to Feb. 1994).

376
16. Extreme Returns in Asset Prices
Next, the equally (over the weekend) distributed returns are displayed in a
scatterplot. In Oct. 1987, there are extraordinarily high and low returns to the
stock market index which are not displayed in Fig. 16.2 (left).
Fig. 16.2. (left.) Daily returns rt to the Standard & Poors 500 stock market index.
(right.) Daily exchange rate returns rt to the British pound relative to U.S. dollar.
The scatterplot of the Standard & Poors log–returns (with changed signs)
was partly displayed in Fig. 1.1 together with a plot of a moving sample quantiles.
The Sample Autocorrelations of Log–Returns
The serial autocorrelations for the returns (based on trading days or with the
returns omitted after a gap) are practically equal to zero which supports the
martingale hypothesis. In particular, the expectation of returns conditioned on
the past is equal to zero. This is illustrated for the Standard & Poors market
indices in Fig. 16.3.
lags
correlation
1
2
3
4
5
6
0.1
0.2
0.3
0.4
0.5
Fig. 16.3.
Autocorrelation func-
tions for the Standard & Poors
log–returns w.r.t. the trading days
(solid) and with the Monday re-
turns distributed (dotted).

16.2. Empirical Evidence in Returns Series
377
There is a slightly larger positive correlation for the lag h = 1, if the Monday
returns are distributed over the weekend.
The Tranquility and Volatility of Returns
Series of daily squared returns r2
t from stock and foreign exchange markets are
visualized by a scatterplot.
time in days
squared returns
0
4000
8000
0
0.003
time in days
squared returns
0
2000
4000
0
0.002
Fig. 16.4. Squared daily returns r2
t to (left) Standard & Poors 500 stock market index,
and (right) exchange rates of the British pound measured relatively to the U.S. dollar.
The illustrations in Fig. 16.4 exhibit that there are periods of tranquility and
volatility of the return series in a particularly impressive manner.
The periods of volatility can also be expressed by moving sample variances
(1/m) t
i=t−m(ri −¯rt)2 based on the log–returns of the preceding m days.
time
1963
1969
1975
1981
1987
0.0
5e-08
1e-07
Fig. 16.5. Moving sample variance
over a time horizon of m = 250
days for the Standard & Poors
data.

378
16. Extreme Returns in Asset Prices
This shows that, while the return process is a fair game, risk is spread un-
evenly. Hence, the risk of an investment is clustered through time and somewhat
predictable. Supposedly, investors trade oﬀreturn against risk, and hence portfo-
lio management has to take this predictability into account. In technical terms,
we may assume that the returns are conditionally heterosketastic (the variance is
varying conditioned on the past).
Trend and Symmetry
For currencies no trend has to be eliminated. Also, currencies are naturally sym-
metric due to the fact that the return, e.g., for the Deutsche mark on the U.S.
dollar is the negative of the return of a Deutsche mark investment for U.S. dollars
(see also page 372). Therefore, for exchange rate returns, the tails will also be
estimated under the condition of symmetry. Tail estimators are also applied to
returns reﬂected in 0.
For stock prices, one must ﬁrst eliminate the positive trend, due to growth
of the economy, to get to the martingale model.
16.3
Parametric Estimation
of the Tails of Returns
Again, we may visualize returns by means of the usual nonparametric tools like
kernel densities, sample mean excess functions, etc., see Section 8.1 for a justiﬁca-
tion of classical extreme value statistics. The application of standard estimation
procedures may also be relied on ergodic theory, see page 209 for references to the
literature.
The overall impression of stock market and exchange rate data is that normal
distributions or similar symmetric distributions can be well ﬁtted to the central
data, yet there seem to be fat tails. So even in those cases where we estimate light
tails (exponential or beta tails) within the GP model, there seems to be a kurtosis
larger than that of the normal distribution.
We estimate the upper and lower tail indices of stock market and exchange
rate returns and compare our estimates with those of Loretan and Phillips (L&P)
who always employed the Hill estimator (presumably to the returns with respect
to the trading days).
Stock Market Returns
We start our investigations with the famous Standard & Poors stock market index
which has been frequently analyzed in the literature.
There is a certain trend in the returns that can be captured by a quadratic
least squares line to some extent. The trend is negligible, when GP parameters are
estimated, as long as there is a time horizon of less than about 10 years.

16.3. Parametric Estimation of the Tails of Returns
379
Example 16.3.1. (Estimating the Tail Indices of Returns to the Standard & Poors Index.)
In Table 16.1, Hill(GP1), Moment(GP) and MLE(GP) estimates of the lower and upper
tail index α, based on the daily returns to the Standard & Poors index, are displayed.
The number k of lower or upper extremes was chosen according to the insight gained
from the diagrams of the estimators.
We are also interested in the eﬀect of handling the daily returns in diﬀerent manners
(taking the returns for each trading day or distributing or omitting returns after gaps).
Table 16.1. Returns with respect to trading days and returns after gaps distributed or
omitted.
Tail Indices α of Standard & Poors Index Returns
Lower (k = 150)
Upper (k = 100)
trad.
distr.
omit.
L&P
trad.
distr.
omit.
L&P
Hill(GP1)
3.39
3.79
4.33
3.59
3.81
3.70
3.70
3.86
Moment(GP)
2.77
2.59
4.40
–
4.26
3.67
3.67
–
MLE(GP)
3.25
2.20
4.26
–
4.32
3.52
3.52
–
The remarkable diﬀerence between the Moment(GP) or MLE(GP) estimates for the lower
tail index—with returns after a gap distributed or omitted—is due to the drop from
282.70 to 224.84 of the Standard & Poors index during the well–known stock market
crash on Monday, Oct. 19, 1987.
Omitting this extraordinary event from the data yields an underestimation of the
risk entailed in negative returns.
The most important message from the preceding example is that for these
stock market returns, one must take heavy upper and lower tails with a tail index
α around 3 into account.
Related results are obtained for other stock market indices (as, e.g., for the
Lufthansa and Allianz data stored in fm-lh.dat and fm-allia.dat).
Global Modeling of a Distribution,
Local Modeling of the Tails
As already mentioned on page 61: one major goal of this book is to select distri-
butions which simultaneously ﬁt to the central as well as to the extreme data. The
extreme part is analyzed by means of the tools of extreme value analysis.
In Example 2.3.2, we successfully ﬁtted a mixture of two Gaussian distribu-
tions to the S&P500 data which were also dealt with in Example 16.3.1.
Applying the MLEs in the Student model as well as in the model of sum–
stable distributions, see Sections 6.3 and 6.4, we get the estimates as listed in
Table 16.2.

380
16. Extreme Returns in Asset Prices
Parameters
α
σ
Student
2.29
0.0034
sum–stable
1.34
0.0027
Table 16.2. MLEs of α and σ
based on S&P data.
If the pertaining densities are additionally plotted in Fig. 2.11, they are hardly
distinguishable from the Gaussian mixture and the kernel densities. Therefore,
there are three candidates for the modeling of the Standard & Poors, namely, a
Gaussian mixture, Student and sum-stable distribution. The insight gained from
Example 16.3.1 speaks in favor of the Student modeling.
Exchange Rate Returns
For exchange rate returns we ﬁnd light as well as heavy tails. The tail index is also
estimated under the condition of symmetry. Under that condition one may base
the estimation on a larger number of upper extremes.
Example 16.3.2. (Estimating the Tail Indices of Daily Exchange Rate Returns: British
Pound.) Our approach corresponds to that in Example 16.3.1.
Table 16.3. Estimating the tail indices.
Tail Indices α of British Pound Returns
Lower (k = 75)
Upper (k = 75)
Symm. (k = 150)
trad. distr. omit. L&P trad. distr. omit. L&P
distr.
Hill(GP1)
3.85
4.03
4.03
2.89
3.65
4.25
3.75
3.44
3.95
Moment(GP) 12.88
4.51
4.51
–
5.48
3.62
5.05
–
4.72
MLE(GP)
238.0
4.28
4.28
–
5.98
3.33
5.40
–
4.85
There is a greater variability in the Moment and MLE estimates based on returns
with respect to trading days. The diagrams of estimates provide more insight.
We remark that the Moment and ML estimates based on the symmetrized trading
day data are around α = 6. The overall picture is that a Paretian modeling of the lower
and upper tails is justiﬁed for the British daily exchange rate returns.
The insight gained from Example 16.3.2 suggest to intensify an exploratory
analysis of the data and to apply additionally the L–moment and related estima-
tors, see Chapter 14. The latter has not been done yet.
A completely diﬀerent modeling must be taken for the Swiss franc. It seems
to us that an exponential modeling for the tails of the distribution is justiﬁed.

16.3. Parametric Estimation of the Tails of Returns
381
If the estimation is carried out in the uniﬁed generalized Pareto (GP) model,
then the estimates of the shape parameter γ are close to zero in the present case.
Therefore, α = 1/γ attains large positive as well as small negative values. In such
cases, it is preferable to carry out the estimation in the γ–parameterization to
avoid bigger diﬀerences in the estimates of α for varying k.
It is remarkable that the Hill estimator is incorrect, a fact which becomes
apparent by comparing parametric and nonparametric plots, see also Fig. 5.2 (left)
and the expansion in (6.38).
Example 16.3.3. (Estimating the Tail Indices of Daily Exchange Rate Returns: Swiss
Franc.) For daily exchange rate returns of the Swiss franc relative to the U.S. dollar, we
obtain negative estimates of the lower tail index α. The estimates are based on equally
distributed returns. When omitting Monday returns, one ﬁnds similar results. The chosen
number k of extremes is 100.
Table 16.4. Estimating the tail indices α and γ of daily exchange rate returns of the
Swiss franc relative to the U.S. dollar with equally distributed returns (including Hill
estimates taken from Loretan and Phillips (L&P) in the α–parameterization).
Tail Indices α and γ of Swiss Franc Returns
Lower (k = 100)
Upper (k = 100)
Symmetric (k = 200)
α
γ
L&P
α
γ
L&P
α
γ
Hill(GP1)
3.61
0.28
3.10
4.16
0.24
2.77
3.66
0.27
Moment(GP)
−9.09
−0.11
–
9.67
0.10
–
−9.35
−0.11
MLE(GP)
−5.88
−0.17
–
18.19
0.05
–
−31.81
−0.03
Moment(GP) and MLE(GP) estimates γ for the lower tail index are negative yet
close to zero. When employing the armory of nonparametric tools such as sample excess
functions, Q–Q plots, sample qfs, etc. and then comparing the nonparametric and para-
metric curves, there is doubt that an application of the Hill estimator is correct. On the
basis of these results, we suggest a beta or an exponential modeling for the lower tail.
Moment(GP) and MLE(GP) estimates for the upper tail index can be negative for
other choices of the number k of extremes. We suggest an exponential modeling of the
upper tail in the case of Swiss exchange rate returns. The preceding remarks about the
Hill estimator are applicable again. The estimates for the symmetrized sample conﬁrm
the preceding conclusions.
There is a greater variability in the estimates of the shape parameter for vary-
ing numbers k of extremes which could be reduced, to some extent, by smoothing
these estimates over a moving window.
We shortly summarize results for the exchange rates of the French franc,
Deutsche mark and yen against the U.S. dollar. We give a list of Moment(GP)
and MLE(GP) estimates in the γ–parameterization.

382
16. Extreme Returns in Asset Prices
Table 16.5. Estimating the lower and upper tail indices of distributed daily returns
based on k = 75 extremes (k = 150 extremes for the symmetrized sample).
Estimating the Tail Index γ
Moment(GP)
MLE(GP)
Lower
Upper
Symmetric
Lower
Upper
Symmetric
French franc
−0.11
−0.05
−0.05
−0.09
−0.04
−0.05
Deutsche mark
−0.19
0.02
−0.05
−0.23
−0.04
−0.14
yen
0.06
0.08
0.007
−0.06
0.06
−0.03
We see that for all three cases an exponential modeling for the tails seems to
be adequate.
A detailed exploration of extreme returns of further exchange rates, stock
market indices and other speculative asset prices is desirable.
16.4
The Proﬁt/Loss Variable
and Risk Parameters
At the time of writing ﬁnancial institutions like commercial banks have to meet
new Capital Adequacy rules. A landmark is the Basle Committee accord of 1988
for credit risks which were supplemented by proposals for market risks starting
1993 (for details see the book by Jorion [33] and further supplements of the Basle
Committee accord10). These rules require that a bank must have suﬃcient capital
to meet losses on their exposures. In this context, we compute
• the Value–at–Risk (VaR) as the limit which is exceeded by the loss of a given
speculative asset or a portfolio with a speciﬁed low probability,
• the Capital–at–Risk (CaR) as the amount which may be invested so that the
loss exceeds a given limit with a speciﬁed low probability, see Section 16.5.
Thus, we ﬁx either the invested capital (market value) or the limit and, then,
compute the other variable. Further useful risk parameters of the loss/proﬁt dis-
tribution will be introduced at the end of this section.
The VaR is a quantile of the loss (more precisely, proﬁt/loss) distribution
which must be estimated from the data. This goal is achieved in the following
manner.
1. The loss variable is represented by means of the returns so that the results
from the preceding section become applicable. This is done in the subsequent
lines.
10Basle Committee on Banking Supervision (1996). Supplement to the Capital Accord
to Incorporate Market Risk. Basle Committee on Banking Supervision, Basle.

16.4. The Proﬁt/Loss Variable and Risk Parameters
383
2. We estimate the VaR based on diﬀerent statistical models for the returns,
particularly making use of the GP modeling as outlined in Section 16.3. This
step will be carried out in the next section.
In both steps one must distinguish between the case of a single asset and the one
of a portfolio.
Representing the Proﬁt/Loss Variable by Means of Returns
Let V0 and VT be the market values of a single speculative asset or a portfolio at
the times t = 0 and t = T . Usually, the time horizon is a day or a month (the
latter corresponds to 20 trading days or, alternatively, to 30 days if the Monday
returns are distributed over the weekend).
Losses and proﬁts within the given period of T –days will be expressed by the
loss (proﬁt/loss) variable as the diﬀerence of the market values VT and V0. We
have
LT = −(VT −V0).
(16.4)
Notice that losses are measured as positive values. Conversely, there is a proﬁt
if LT is negative. Under this convention, we may say that a loss is, for example,
smaller than the 99% Value–at–Risk, see (16.11), with a probability of 99%.
The T –day loss LT will be expressed by means of T –day returns
R(T ) =

t≤T
(−Rt)
(16.5)
taken with a changed sign. Next, we distinguish between representations of the
loss variable for a single asset and for a portfolio.
• (The Loss for a Single Asset.) The total market value Vt of an asset at time
t can be expressed by Vt = hSt, where h is the number of shares, which
are held within the given period, and St is the price at time t. From (16.2),
where prices are represented by means of the daily log–returns, we conclude
that the loss at time T is
LT
=
V0

1 −exp(−R(T ))

≈
V0R(T ).
(16.6)
• (The Portfolio Loss.) Let Vt,j = hjSt,j be the market value of the jth asset
in the given portfolio at time t, where hj are the numbers of shares which
are held within the given period, and St,j are the prices at time t. Notice
that
Vt =

j
Vt,j
(16.7)
is the market value of the portfolio at time t.

384
16. Extreme Returns in Asset Prices
We also introduce the vector of weights w = (w1, . . . , wd), where wj =
V0,j/V0. Notice that the wj sum to unity. The vector of weights determines
the market strategy of the investor at the beginning of the period. We have
Vt = V0

j
wjSt,j = V0wS
′
t
(16.8)
where S
′
t is the transposed vector of prices St = (St,1, . . . , St,d).
From (16.2) and (16.7) deduce that the loss LT = −(VT −V0) of the portfolio
at time T is
LT
=
V0

j
wj

1 −exp(−R(T,j))

≈
V0

j
wjR(T,j) = V0wR
′
(T ),
(16.9)
where R(T ) = (R(T,1), . . . , R(T,d)) is the vector of random T –day log–returns
(with changed sign) for the diﬀerent asset.
If the portfolio consists of a single asset, then the formulas in (16.9) reduce to
those in (16.6). The portfolio may also be treated like a single asset. The log–return
of the portfolio is given by
R∗
t = log Vt −log Vt−1
(16.10)
with Vt as in (16.7). Now (16.6) is applicable with R∗
t in place of Rt.
The Value–at–Risk (VaR)
The Value–at–Risk parameter VaR(T, q) is the q–quantile of the loss distribution.
The VaR at the probability q satisﬁes the equation
P{LT ≤VaR(T, q)} = q,
(16.11)
where LT is the loss variable in (16.4).
We also speak of a VaR at the 99% or 95% level, if q = 0.99 or q = 0.95. Thus,
for example, the loss is smaller than the VaR at the 99% level with a probability
of 99%.
If the VaR is computed to fulﬁll capital adequacy requirements, then it is
advisable to choose a higher percentage level, say 99% or 99.9%. To compare risks
across diﬀerent markets, a smaller level such as 95% can be appropriate.
The VaR depends on the distribution of the returns.
• (VaR of a Single Asset.) Let FT denote the df of the T –day log–return
R(T ) = 
t≤T (−Rt) with changed sign. Thus,
FT (x) = P{R(T ) ≤x}.
(16.12)

16.4. The Proﬁt/Loss Variable and Risk Parameters
385
Applying (16.6) we see that the VaR can be written
VaR(T, q)
=
V0

1 −exp

−F −1
T (q)

(16.13)
≈
V0F −1
T (q),
(16.14)
where V0 is the market value at time t = 0.
The VaR is overestimated in (16.14) in view of the inequality 1 −exp(−x) ≤
x, yet the error term is negligible if the q–quantile F −1
T (q) is not too large.
• (VaR of a Portfolio.) The Value–at–Risk VaR(T, q) is the q–quantile and,
respectively, the approximate q–quantile of the random variables in (16.9).
In Section 16.5, the VaR is computed in a more explicit manner based on
diﬀerent statistical models such as, e.g., the GP modeling for returns in the case
of a single asset.
Further Risk Parameters
Further risk parameters may be deﬁned as functional parameters of exceedance
dfs pertaining to the loss L. Let F denote the df of L, and denote by F [u] the
exceedance df at the threshold u (cf. (1.11)). An example of such a functional
parameter is the expected shortfall which is the mean of F [u] with u = VaR(q)
which has the representation
E

L
L > VaR(q)

=
 ∞
VaR(q)
x dF [VaR(q)](x)
=
1
1 −q
 1
q
VaR(x) dx,
(16.15)
where the second equation may be deduced with the help of the well–known for-
mula

x dF(x) =
 1
0 F −1(q) dq. We see that the expected shortfall at the level q
is an “average” of the Value–at–Risks with x ≥q.
Perhaps, it would be more reasonable to call
E

L −VaR(q)
L > VaR(q)

expected shortfall as the expected loss which is not covered by the allocated capital
of the bank.
In Section 16.3 we estimated the upper tail of the distribution of returns and,
thus, the exceedance df. Using a representation of the loss variable in Section 16.4
in terms of returns one can estimate the indicated risk parameters. Details will be
given for the VaR in the subsequent section.

386
16. Extreme Returns in Asset Prices
16.5
Evaluating the Value–at–Risk (VaR)
Next, we discuss the estimation of the VaR under certain models for the return
distribution. We start with the estimation of the VaR in the case of a single asset
based on the GP–modeling. The estimation by means of the sample q–quantile is
the second method. In the ﬁnancial literature this method runs under the label
Historical Simulation (HS).
Estimating the VaR for a Single Asset: the GP Modeling
We take representations of the VaR, given in (16.13) and (16.14). For simplicity,
let the initial market value V0 be equal to 1.
It remains to estimate the q–quantile F −1
T (q), where FT is the df of the T –day
log–return R(T ). Moreover, we assume that a GP df could be accurately ﬁtted to
the upper tail of FT as it was done in Section 15.3.
In Example 16.5.1, the VaR will be computed for T = 1 and T = 30 days
and several large probabilities q in the case of the Standard & Poors index with
the Monday returns distributed over the weekend.
Example 16.5.1. (VaR for the Standard & Poors Index.) The Value–at–Risk VaR(T, q)
is estimated for T = 1 and T = 30 based on the 9314 equally distributed, daily returns
to the Standard & Poors Index.
For T = 1 the q–quantile of the daily returns with changed sign is estimated in a
parametric manner by means of the Moment(GP) estimator and our standard estimators
of the location and scale parameters. The estimation is based on k = 150 lower extreme
daily returns as in Example 16.3.1.
To estimate VaR(30, q) the Moment(GP) estimator is applied to k = 30 extreme
30–days returns. The number of 30–days returns is 310. The estimated shape parameter
is α = 2.4.
In both cases, the upper tail of the qf F −1
T
is estimated by the Pareto qf belonging
to the estimated parameters. The VaR, according to (16.14), is listed in Table 16.6 for
several values of q.
Table 16.6. Estimating the Value–at–Risk VaR(T, q) for the Standard & Poors Index.
Value–at–Risk VaR(T, q)
probability q
0.99
0.995
0.999
0.9995
T = 1
0.017
0.020
0.033
0.041
T = 30
0.128
0.169
0.333
0.435
For example, the loss within T = 30 days is smaller than VaR(30, 0.99) = 0.128
with a 99%–probability, if V0 = 1 is invested at time t = 0.

16.5. Evaluating the Value–at–Risk (VaR)
387
We also give the exact values VaR∗= 1−exp(−VaR) of the Value–at–Risk according
to (16.13). As mentioned before, the values in Table 16.6 overestimate the exact values
VaR∗.
Table 16.7. Values VaR∗(T, q) = 1 −exp(−VaR(T, q)) for the Standard & Poors Index.
Value–at–Risk VaR∗(T, q)
probability q
0.99
0.995
0.999
0.9995
T = 1
0.017
0.020
0.032
0.040
T = 30
0.120
0.155
0.283
0.353
For T = 1 the diﬀerences are negligible, yet there are remarkable diﬀerences for
T = 30.
There is a remarkable coincidence between estimated parametric q–quantiles
and sample q–quantiles when the parametric q–quantile lies inside the range of the
sample.
Estimating the VaR: Historical Simulation
In Historical Simulation (HS), the loss/proﬁt df of a given asset or a portfolio over
a prescribed holding period of length T is simply given by the sample df of past
losses and gains. We then translate this into a loss/proﬁt qf and read of the VaR.
The advantage of HS relies on its simplicity and low implementation costs.
But this very simplicity is also the cause of problems. The main problem is that
extreme quantiles cannot be estimated, because extrapolation beyond past obser-
vation is impossible, and when estimation is possible the estimates are not reliable
due to the lack of suﬃcient observations in the tail area. Moreover, the quantile
estimators tend to be very volatile if a large observation enters the sample.
Estimating the VaR for a Portfolio:
the Variance–Covariance Method
This method is useful when we have to compute the VaR for a portfolio containing
many diﬀerent assets. In fact, this task becomes particularly easy when all asset
returns are assumed to be normally distributed and the loss variable of the portfolio
is a linear function of these. In this case, the VaR is a multiple of the portfolio
standard deviation and the latter is a linear function of individual variances and
covariances.

388
16. Extreme Returns in Asset Prices
We assume that the T –day log–return (with changed sign)
R(T,j) =

t≤T
(−Rt,j)
of the jth asset at time t is a Gaussian random variable with mean zero and
unknown variance σ2
T,j > 0. In addition, assume that the R(T,j), j ≤d, are jointly
Gaussian with a covariance matrix ΣT =

σT,i,j

as introduced on page 280.
Necessarily, σT,j,j = σ2
T,j.
Let V0,j be the market value of the jth asset in the portfolio at time t = 0.
According to (16.9), the loss LT of the portfolio at time t = T can be approximately
represented by
LT ≈V0wR
′
(T ),
(16.16)
where R(T ) is the vector of the T –day log–returns R(t,j). According to the well–
known results for Gaussian random vectors (cf. page 280), the term wR
′
(T ) in
(16.16) is a Gaussian random variable with mean zero and standard deviation
σT =
'
wΣT w′.
(16.17)
Combining (16.16) and (16.17) we get
VaR(T, q) = V0σT Φ−1(q),
where Φ−1 is the univariate standard Gaussian qf.
An estimate of the VaR is obtained by replacing the variances and covariances
σT,i,j in (16.17) by their sample versions ˆsT,i,j, cf. (2.6), based on historical T –day
log–returns.
If there is a single asset, or the portfolio is dealt with like a single asset, and
the daily log–returns are uncorrelated, then σT = V0T 1/2σ1 with σ1 denoting the
standard deviation of a daily log–return.
Estimating the VaR for a Portfolio:
the Copula Method
A considerable improvement, compared to the simple variance–covariance method,
is achieved when the univariate Gaussian margins are replaced by more realistic
dfs. This is the copula method (cf. page 275) as already successfully applied in the
Chapters 12 and 13 to estimate unknown multivariate EV and GP dfs.
We assume that the dependence structure of a multivariate Gaussian df Φ	Σ
(cf. (11.4)), with mean vector zero and covariance matrix 	Σ, is still valid, however
the univariate margins are unknown dfs Fj, say. Thus, we assume that the T –day
log-returns R(T,j) have the joint df
F(x) = ΦΣ

Φ−1(F1(x1)), . . . , Φ−1(Fd(xd))

,
(16.18)

16.5. Evaluating the Value–at–Risk (VaR)
389
where Σ is the correlation matrix pertaining to 	Σ and Φ−1 is the univariate stan-
dard Gaussian qf.
Notice that the preceding variance–covariance model is a special case with
Fj being a Gaussian df with scale parameters σj. We discuss further possibilities.
• De Raaij and Raunig11 report a successful implementation of mixtures of
two Gaussian dfs (also see pages 31 and 380).
• If one goes one step further and is continuously mixing Gaussian distributions
with respect to a gamma distribution, one arrives at a Student distribution
(cf. (6.16) and page 380). The careful univariate extreme value analysis in
Section 16.3 speaks in favor of such dfs which have Pareto–like tails.
• Further possibilities include sum–stable dfs yet, as mentioned before, this
conﬂicts with the empirical evidence that log–returns exhibit bounded second
moments.
The df in (16.18) can be estimated by applying the piecing–together method
(used to estimate multivariate EV and GP dfs).
1. Estimate the df Fj by means of Fj based on the univariate data xi,j in the
jth component;
2. transform the data xi,j to
yi,j = Φ−1 Fj(xi,j)

which may be regarded as data governed by ΦΣ;
3. estimate Σ by the sample correlation matrix Σ based on the transformed
data yi,j,
4. take
F(x) = ΦΣ

Φ−1( F1(x1)), . . . , Φ−1( Fd(xd))

(16.19)
as an estimate of F(x).
For an application we refer to Example 10.3.1 where the copula method led
to a curious modeling.
Using the tools from multivariate extreme value analysis, one should also
analyze the validity of the chosen copula function. Further candidates of copula
functions are, e.g., those pertaining to multivariate sum–stable or Student distri-
butions.
11Raaij, de G. and Raunig, B. (1999). Value at risk approaches in the case of fat–tailed
distributions of risk factors. Manuscript, Central Bank of Austria.

390
16. Extreme Returns in Asset Prices
Estimating the VaR for a Portfolio:
a Multivariate Student Modeling
We extend the variance–covariance method to multivariate Student distributions.
The loss LT of a portfolio at time T is again represented by V0wR
′
(T ) as in (16.16).
Now we assume that the T –day log–returns RT,j, j ≤d, are jointly Student
distributed with shape parameter α > 0 and parameter matrix Σ, see (11.11) and
(11.12). Therefore, the log–returns have the representations
RT,j =
Xj
(2Y/α)1/2 ,
where the Xj are jointly Gaussian with covariance matrix Σ. Consequently,
wR
′
(T ) =
X
(2Y/α)1/2 ,
where X is Gaussian with standard deviation σT = √wΣT w′ corresponding to
(16.17), and LT is a Student variable with shape parameter α > 0 and scale
parameter V0σT . Therefore, one gets the Value–at–Risk
VaR(T, q) = V0σT F −1
α (q),
where Fα denotes the standard Student df with shape parameter α.
One gets the variance–covariance result in the limit as α →∞. The extended
model is more ﬂexible compared to the Gaussian model and includes the case of
heavy–tailed distributions. There is still the restriction of equal shape parameters
in the single components.
Capital–at–Risk (CaR) for a Single Asset
In the preceding lines the VaR was computed as the limit l such that P{LT ≤
l} = q, where LT is the loss/proﬁt variable for the period t = 0 and t = T .
Conversely, one may ﬁx the limit l and compute the Capital–at–Risk CaR(T, q, l)
as the amount which can be invested such that the pertaining loss LT does not
exceed the limit l with a given probability q.
Using the representation (16.13) of the loss/proﬁt variable LT with V0 re-
placed by CaR(T, q, l), one gets
P{LT ≤l}
=
P

CaR(T, q, l)

1 −exp
 
t≤T
Rt

≤l

=
q,
if
CaR(T, q, l)
=
l/

1 −exp

−F −1
T (q)

≈
l/F −1
T (q).

16.5. Evaluating the Value–at–Risk (VaR)
391
Here, FT is again the df of the T –day return 
t≤T (−Rt) with changed sign (see
(16.12)). The ratio
c(T, q) = 1/F −1
T (q)
(16.20)
is the capital/loss coeﬃcient.
Example 16.5.2. (Capital/Loss Coeﬃcient for the Standard & Poors Index.) The capi-
tal/loss coeﬃcient c(T, q) is estimated for T = 1 and T = 30 based on the 9314 equally
distributed daily returns to the Standard & Poors Index. Apparently, one must estimate
the q–quantile of FT .
It is evident from (16.20) that the capital/loss coeﬃcients are just the reciprocals
of the values computed for the VaR in Table 16.6.
Table 16.8. Estimating the Capital/Loss Coeﬃcient for the Standard & Poors Index.
Capital/Loss Coeﬃcient c(T, q)
probability q
0.99
0.995
0.999
0.9995
T = 1
58.1
50.0
30.3
24.4
T = 30
7.8
5.9
3.0
2.3
It is diﬃcult, due to medium sample sizes, to extend the empirical approach to
estimating the capital/loss coeﬃcient beyond a period of a month.
Capital–at–Risk for a Portfolio
Let H0 be an initial market strategy which determines the proportions of the
diﬀerent assets to each other. Let again Vt,j = H0,jPt,j and Vt = 
j Vt,j be the
market values of the single assets and of the portfolio with respect to H0.
We compute a constant b such that the loss/proﬁt variable LT for the period
from t = 0 to t = T , belonging to the multiple bH0 of the initial market strategy,
fulﬁlls the equation
P{LT ≤l}
=
P

b

j
V0,j

1 −exp
 
t≤T
Rt,j

≤l

=
q.
This holds with b = l

F −1
T,H0(q), where F −1
T,H0 is the df of

j
V0,j

1 −exp
 
t≤T
Rt,j

.

392
16. Extreme Returns in Asset Prices
Therefore,
CaR(T, q, l) = bV0 = lV0

F −1
T,H0(q)
is the Capital–at–Risk at the probability q.
16.6
The VaR for a Single Derivative Contract
We start with a short introduction to derivative contracts with special empha-
sis laid on European call options, deduce the VaR for the call option under the
Black–Scholes pricing and conclude the section with remarks concerning the gen-
eral situation.
An Introduction to Derivative Contracts
A derivative contract is a contract, the value of which is dependent on the value
of another asset, called the underlying. Options are a special kind of derivative
contracts, because these depend non–linearly on the value of the underlying.
Options are traded on many underlyings such as single stocks, stock indices,
exchange rates, etc. We concentrate on an option for which the underlying is a
non–dividend paying stock. There are two basic types of options. A European call
option (respectively, a put option) gives the owner the right to buy (to sell) the
underlying asset by a certain future date T for a preﬁxed price X, where
• T is the expiration date or maturity,
• X is the strike price.
The prices and returns of the underlying are again denoted by St and Rt.
At maturity T the value of a European call option (called payoﬀ) will be the
amount
PF = max(ST −X, 0),
(16.21)
by which the stock price ST exceeds the strike price X. The value for the put
option is PF = max(X −ST , 0).
The Black–Scholes Prices for European Call Options
A call option becomes more valuable as the stock price increases. The opposite
holds for put options. The present value (at time t) of the option could be deter-
mined through discounting the payoﬀ. However, ST in (16.21) is unknown at time
t < T , and one has to use a diﬀerent market price.
Within the continuous–time version of the model presented in (16.3), in case
the underlying is not dividend paying, this is the Black–Scholes price
C(St, T −t)
:=
StΦ

d1(St, T −t)

−X exp

−r(T −t)

Φ

d2(St(T −t)

, (16.22)

16.6. The VaR for a Single Derivative Contract
393
where r is the annual risk–free rate of interest, T −t is the number of calendar
days until expiration,
d1(St, T −t) =
log St
X +

r + σ2
2

(T −t)
σ
√
T −t
and
d2(St, T −t) = d1 −σ
√
T −t.
We have St+1 = St exp(µ + σW), where W is a standard Gaussian random
variable. For simplicity, we assume that µ = 0. Therefore, σW is the log–return
for the given time period. More precisely, one must deal with the conditional
distribution of St+1 conditioned on st because the price st is known at time t, yet
this leads to the same result.
The proﬁt/loss (again with a conversed sign) for the period from t to t + 1 is
L = −

C(St+1, T −t −1) −C(St, T −t)

.
(16.23)
Therefore, the Value–at–Risk VaR(q) at the level q is the q-quantile of the distri-
bution of L. We have P{L ≤VaR(q)} = q corresponding to (16.11).
We compute the VaR by means of linearization and also show that a direct
computation is possible for the simple case of a call option.
Computation of the VaR by Linearization (Delta Method)
We deduce an approximate representation of the loss variable in (16.23) which is
linear in the log–return variable σW of the underlying.
From the fact that
sϕ

d1(s, T −t)

−X exp(−r(Tt))ϕ

d2(s, T −t)

= 0
one gets
∂
∂sC(s, T −t) = Φ

d1(s, T −t)

=: ∆(s, T −t)
which is called the ∆of the option.
Therefore, a Taylor expansion about St yields that
L
≈
∆(St, T −t)(St −St+1)
=
St∆(St, T −t)(1 −exp(σW))
≈
St∆(St, T −t)σW.
(16.24)
Using the last expression for the loss variable one gets
VaR(q) = St∆(St, T −t)σΦ−1(q)
(16.25)
for the one–day Value–at–Risk at the level q.
A more accurate linear approximation can be achieved when the linear term
depending on the partial derivative (∂/∂t)C(s, t) is added.
This kind of approach will underperform when more complicated kinds of
derivatives are considered or when the underlying is highly non–normal.

394
16. Extreme Returns in Asset Prices
Direct Computation of the VaR
The Black–Scholes formula for a call option can be also used to compute the VaR
directly, because the price of the option is strictly increasing in the price of the
underlying. The exact solution is
VaR(q) = C

St, T −t

−C

St exp(−σΦ−1(q), T −t −1)

.
(16.26)
We remark that (16.25) can also be deduced from this formula.
In the case of more complex derivatives, when the function C has no simple
analytic expression, such an approach can still be used, and the VaR can be found
by numerical approximation.
Computing the Black–Scholes Price
The preceding arguments heavily rely on the Black–Scholes pricing formula as
speciﬁed in (16.22). We indicate in which manner this price for the European
call option and other derivative contracts can be determined. This enables us to
compute the VaR as well for other derivative contracts.
Under the conditions of the Black–Scholes model (16.3) we have
ST = St exp

µ(T −t) + σ
T

k=t+1
Wk

with St being observed at time t.
Let r be the annual risk–free rate of interest. In addition, assume that
E(ST /St) = exp(r(T −t)).
(16.27)
Thus, the expected return of the speculative asset corresponds to the risk–free rate
of interest. Under this additional condition one obtains the equality
C(St, T −t) = E

max(ST −X, 0)

exp(−r(T −t))
between the discounted expected option payoﬀand the Black–Scholes price.
Without condition (16.27), the Black–Scholes price can still be justiﬁed as a
fair price by using the concept of hedge portfolios. Moreover, under certain regu-
larity conditions, the fair price of any derivative contract can be computed by cal-
culating the discounted expected value of the option payoﬀPF with respect to the
risk-neutral probability, also called the equivalent martingale measure. Thereby,
the original probability measure is replaced by another one under which the pro-
cess of payoﬀs becomes a martingale. Yet, both probability measures coincide, if
condition (16.27) is valid.

16.7. GARCH and Stochastic Volatility Structures
395
Value–at–Risk for General Derivative Contracts
For general derivative contracts we also obtain by
C(St, T −t) = E

PF

exp(−r(T −t))
(16.28)
the fair price, if the expectation is taken under the risk–neutral probability. The
proﬁt/loss with conversed sign is given in analogy to (16.23). One may try to apply
the delta method or to carry out a direct computation of the VaR.
Another possibility is to use intensive simulation procedures, such as Monte
Carlo or bootstraps, to compute the expected value of the payoﬀ. For details we
refer to Caserta et al.12.
16.7
GARCH and
Stochastic Volatility Structures
The aim of this section is to provide some theoretical insight in the tail–behavior of
return distributions and to start a discussion about a semiparametric estimation of
the VaR and the capital/loss coeﬃcient. We primarily deal with time series {Xk}
that possess an ARCH (autoregressive conditional heterosketastic)13 structure or,
by generalizing this concept, a GARCH14 structure.
We also mention stochastic volatility (SV) models which provide alternative
models for the volatility in returns series.
First, we review some basic facts of the conditioning concept as outlined in
Section 8.1. Recall that the mean and the variance of the conditional distribution
are the conditional expectation E(Y |x) and the conditional variance V (Y |x) of Y
given X = x.
Modeling the Conditional Heteroskedasticity
and Distributions of Returns
Assume that the random return Rt at time t can be expressed as
Rt = ˜σtεt,
(16.29)
where εt is a random innovation with E(εt) = 0 and V (εt) = 1, and ˜σt > 0 is a
random variable with ﬁnite expectation being independent of εt (and depending
on the past). Values of ˜σt are denoted by σt.
12Caserta, S., Dan´ıelsson, J. and de Vries, C.G. (1998). Abnormal returns, risk, and
options in large data sets. Statistica Neerlandica 52, 324–335.
13 Engle, R.F. (1982). Autoregressive conditional heteroscedasticity with estimates of
the variance of United Kingdom inﬂation. Econometrica 50, 987–1007.
14 Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. J.
Econometrics 31, 307–327.

396
16. Extreme Returns in Asset Prices
Because of the independence of ˜σt and εt, the conditional distribution of Rt
given ˜σt = σt is the distribution of σtεt, see (8.13). In the present context, ˜σ2
t is
also addressed as random variance or volatility.
Under the conditions above we have
(i) E(Rt|σt) = 0;
(ii) V (Rt|σt) = σ2
t .
Thus, the conditional variance is not a ﬁxed constant, a property addressed as
conditional heteroskedasticity. In addition,
(iii) E(Rt) = 0;
(iv) V (Rt) = E

˜σ2
t

.
Later on, further conditions will be imposed on ˜σt and εt so that {Rt} is a
martingale innovation scheme and, hence, a series of uncorrelated random vari-
ables.
Notice that the distribution of ˜σtεt is the mixture of the distributions σtεt
with respect to the distribution of ˜σt. We mention three examples of innovations
εt and random scale parameters ˜σt:
• (Normal Innovations.) If εt is standard normal and 1/˜σ2
t is a gamma random
variable with shape parameter r > 0 (see (4.6)), then Rt = ˜σtεt has the
density gr which is the Student density as deduced in (6.16).
• (Laplace Innovations.) If εt is a Laplace (double–exponential) random vari-
able with density f(x) = exp(−|x|)/2 and 1/˜σt is a gamma random variable
with shape parameter r, then Rt = ˜σtεt is a double–Pareto random variable
with density (cf. also (5.28))
gr(x)
=
 ∞
0
(ϑ/2) exp(−|x|)hr(ϑ) dϑ
=
r(1 + |x|)1+r/2.
(16.30)
• (Log–Normal Innovations.) If ˜σt and εt are log–normal, then Rt is log–
normal.
ARCH(1) Series
The following scheme captures both the martingale feature and the observed clus-
ters of volatility in speculative return series. These clusters are well described
analytically, but not well understood from the economic point of view15.
15See, e.g., Vries, de C.G. (1994). Stylized facts of nominal exchange rate returns. In:
The Handbook of International Economics, R. van der Ploeg (ed.), Blackwell, Oxford,
pp. 335–389.

16.7. GARCH and Stochastic Volatility Structures
397
Let {εt} be a white–noise process of iid random variables satisfying the con-
ditions E(εt) = 0 and E(ε2
t ) = 1. Let R0 be an initial random variable which is
independent of the innovations εt. Then, Rt = ˜σtεt with
˜σ2
t = α0 + α1R2
t−1,
t ≥1,
(16.31)
and α0, α1 > 0, is an ARCH(1) series. Notice that the conditions, speciﬁed in
(16.29), are valid.
We discuss some properties of the series {Rt}. Because the εt and Rt−i are
independent for each i ≥1, the Rt are uncorrelated with expectations E(Rt) = 0
and variances
V (Rt)
=
α0
 t−1

j=0
αj
1

+ αt
1V (R0)
=
α0
1 −αt
1
1 −α1
+ αt
1V (R0),
(16.32)
where one must assume that α1 ̸= 1 in the second representation.
If α1 < 1, then
V (Rt) →α0/(1 −α1),
t →∞,
and the ARCH series approximately satisﬁes the condition of weak stationarity. If,
in addition, V (R0) = α0/(1 −α1), then V (Rt) = V (R0) and {Rt} is a white–noise
process. We remark that white–noise processes of this type are taken to model
innovations in certain economic time series.
The special properties of the ARCH process are due to the fact that the Rt
are uncorrelated yet not independent. As a consequence of (8.13), the conditional
distribution of Rt given Rt−1 = rt−1 is the distribution of
σ(rt−1)εt,
(16.33)
where
σ(r) = (α0 + α1r2)1/2.
This is also the conditional distribution of Rt given the past Rt−1 = rt−1, . . . , R0 =
r0 up to time zero.
Especially, the property
E(Rt|rt−1, . . . , r0) = 0
of martingale innovations holds. In addition, it is evident that E(Rt|rt−1) = 0 and
V (Rt|rt−1) = σ2(rt−1).

398
16. Extreme Returns in Asset Prices
Extremal Properties of ARCH Series
The distributional properties of the ARCH series are best analyzed by viewing
the squared ARCH series R2
t as a stochastic diﬀerence equation. From (16.31) and
(16.29) we obtain
R2
t
=
α0ε2
t + α1ε2
tR2
t−1
=
Bt + AtR2
t−1,
(16.34)
say. This stochastic diﬀerence equation with iid pairs (Bt, At) is equivalent to the
ARCH series up to a coin ﬂip process for the sign. If E(log A1) < 0, and if there
is a κ such that E(Aκ
1) = 1, E(Aκ
1 log A1) < ∞, 0 < E(Bκ
1 ) < ∞and B1/(1 −A1)
is nondegenerate, then
R2
t →R2
∞=

j<∞
Bj

i≤j−1
Ai,
t →∞
(16.35)
in distribution.
Furthermore, R2
∞has a Pareto like tail with tail index κ, that is,
P{R2
∞> x} = (1 + o(1))cx−κ.
Note that this latter conclusion still follows if the innovations εt have a light tail.
For example, if the εt are iid standard normal, the tail index κ for R2
∞can be
computed from the condition that E(Aκ
1) = 1. We have
Γ(κ + 1/2) = π1/2(2α1)−κ.
For more details and references to the literature see de Haan et al.16 and Basrak
et al.17
Conditional Densities,
Quasi Maximum Likelihood Estimation
The question of estimating the tails of the unconditional distribution of Rt was
already dealt with in the preceding section. Presently, this distribution (more pre-
cisely, the sequence of distributions) is regarded as a nuisance parameter. The aim
is to estimate the parameters α0 and α1. We brieﬂy indicate that the maximum
likelihood principle is applicable by computing the joint density of the returns.
16Haan, de L., Resnick, S.I., Rootz´en, H. and Vries, de C.G. (1990). Extremal behavior
of solutions to a stochastic diﬀerence equation with applications to ARCH–processes.
Stoch. Proc. Appl. 32, 214–224.
17Basrak, B., Davis, R.A. and Mikosch, T. (2002). Regular variation of GARCH pro-
cesses. Stoch. Proc. Appl. 99, 95–115.

16.7. GARCH and Stochastic Volatility Structures
399
Let f be the density of the innovation εt, and let f0 be the density of the
initial random variable R0. According to (16.33), the conditional density of Rt
given Rt−1 = rt−1, . . . , R0 = r0 (which is also the conditional density of Rt given
Rt−1 = rt−1) is
ft(rt|rt−1, . . . , r0) = ft(rt|rt−1) =
1
σ(rt−1)f

rt
σ(rt−1)

,
(16.36)
where σ(r) = (α0 + α1r2)1/2. Therefore, the joint density of R0, . . . , Rt is
f(r0, . . . , rt) = f0(r0)

s≤t
fs(rs|rs−1)
(16.37)
according to (8.11).
It is well–known that consistent estimators of α0 and α1 are obtained by
maximizing the likelihood function based on (16.37), whereby the term f0(r0) can
be omitted; that is, one is maximizing a conditional likelihood function. Moreover,
the unknown density f of the innovations is replaced by the normal one.
The Extension to ARCH(p) Series
A ﬁrst extension of an ARCH(1) series is achieved, if the stochastic volatility is of
the form
˜σ2
t = α0 +

i≤p
αiR2
t−i
(16.38)
with innovations εt being independent of the past random variables R1−i, i =
1, . . . , p.
Repeating the arguments in (16.33) one may verify that the conditional dis-
tribution of Rt given Rt−1 = rt−1, . . . , Rt−p = rt−p is the distribution of
σ(rt−1, . . . , rt−p)εt,
(16.39)
where
σ(rt−1, . . . , rt−p) = (α0 +

i≤p
αir2
t−i)1/2.
In (16.39) one also gets the conditional distribution of Rt given the past Rt−1 =
rt−1, . . . , R1−p = r1−p up to time 1 −p.
Extending (16.36) and (16.37), one gets the conditional densities
ft(rt|rt−1, . . . , r1−p)
=
ft(rt|rt−1, . . . , rt−p)
(16.40)
=
1
σ(rt−1, . . . , rt−p)f

rt
σ(rt−1, . . . , rt−p)

,
and the joint density
f(r1−p, . . . , rt) = f0(r1−p, . . . , r0)

s≤t
fs(rs|rs−1, . . . , rs−p)
(16.41)
of the returns Rt.

400
16. Extreme Returns in Asset Prices
Stochastic Volatility (VS) Models
Let again Rt = ˜σtεt as in (16.29). In contrast to ARCH models we assume that
the series {˜σt} and {εt} are independent. For example, let {˜σt} be an AR series
of the form
˜σ2
t = β0 +

j≤q
βj˜σ2
t−j + ηt,
(16.42)
where {ηt} is another innovation series. This is the product series model as dealt
with in [54]. Another example is obtained if {˜σt} is deﬁned by means of an MA(∞)
series. Let
˜σ2
t = c exp
 ∞

j=0
ψjηt−j

.
(16.43)
Asymptotic results of extremes of such processes are obtained by F.J. Breidt and
R.A. Davis18.
The Extension to GARCH(p, q) Series
An extension of the concept of an ARCH series is achieved, if
˜σ2
t = α0 +

i≤p
αiR2
t−i +

j≤q
βj ˜σ2
t−j,
t ≥1.
(16.44)
Then, Rt = ˜σtεt, t ≥1, is a GARCH(p, q) (a generalized ARCH) series. Notice
that a GARCH(p, 0) series is an ARCH(p) series. Thus, the extension is related to
the step from an AR to an ARMA series.
The RiskMetricsT M (RM) method deals with GARCH(1, 1) series. RM is
concerned with the calculation of the Value–at–Risk for a portfolio consisting of
up to more than 450 assets. Market position, for example, can be entered through
the RM interface and, then, the VaR will be provided.
The basic idea used by RM is that daily log–returns of an asset have zero
mean and are generated according to a non–stationary GARCH(1, 1) series
Rt = ˜σtεt,
where the εt are iid standard normal innovations and
˜σ2
t = (1 −λ)R2
t−1 + λ˜σ2
t−1.
Using past log–returns on the asset it is possible to forecast the volatility of the
daily return for the next day which in turn can be used to compute the one–day
VaR.
18Breidt, F.J. and Davis, R.A. (1998). Extremes of stochastic volatility models. Ann.
Appl. Probab. 8, 664–675.

16.8. Predicting the Serial Conditional VaR
401
The RiskMetricsT M speciﬁcation for λ, called the decay factor, is 0.94 for
daily returns. This determination is based on minimization of the mean square
variance forecast error with respect to λ for a number of asset returns. The decay
factor is then a weighted average of individual optimal decay factors, see J.P.
Morgan Bank19 for details.
16.8
Predicting the Serial Conditional VaR
co–authored by A. Kozek20 and C.S. Wehn21
In the preceding sections, we primarily studied the estimation of the Value–at–
Risk (VaR) as a quantile of a stationary distribution of the loss variable Lt. In the
conditional set–up, the VaR is a conditional quantile, cf. Section 8.1, which should
be predicted based on observable quantities.
In the time series framework, we speak of a serial conditional quantile, cf.
Section 8.3, and, therefore, also of a serial conditional VaR. At the end of Section
9.5, we already mentioned a covariate conditional VaR. The present section focuses
on the serial conditioning within GARCH models. A more general formulation in
terms of both serial and covariate information is possible, yet not considered in
this book.
McNeil et al.22 argue that both approaches, the unconditional as well as the
conditional one, are relevant for risk management purposes. However, in view of
the fact that most empirical time series used in market risk modeling experience
non–stationarity and certain heteroskedastic properties, the conditional approach
is particularily relevant in market risk modeling.
The Serial Conditional VaR
Let again
Lt = −(Vt −Vt−1)
be the loss (proﬁt/loss) variable at time t pertaining to the market values Vt−1 =
hSt−1 and Vt = hSt, where h is the ﬁxed number of shares (the portfolio position),
and St is the price at time t. Denote again by Rt = log St −log St−1 = log Vt −
log Vt−1 the log–return at time t.
The serial conditional VaR at the level q is the conditional q–quantile
VaR(q; rt−1, rt−2, . . .) := q(Lt|rt−1, rt−2, . . .),
(16.45)
19J.P. Morgan (1996). RiskMetrics Technical Document (4th ed.). J.P. Morgan Bank,
New York.
20Macquarie University, Sydney
21DekaBank, Frankfurt am Main
22McNeil, A.J., Frey, R., Embrechts, P. (2005). Quantitative Risk Management—
Concepts, Techniques, Tools. Princeton University Press, Princeton.

402
16. Extreme Returns in Asset Prices
as mentioned in (8.21), of the loss variable Lt given the past returns rt−1, rt−2, . . .;
thus, q(Lt|rt−1, rt−2, . . .) is the q–quantile of the conditional distribution of Lt
given Rt−1 = rt−1, Rt−2 = rt−2, . . ..
As in (16.6) one gets the representation
Lt
=
Vt−1(1 −exp(Rt))
≈
−Vt−1Rt.
(16.46)
In view of (16.46), one may replace in (16.45) the loss variable Lt by the approx-
imate value −Vt−1Rt.
Notice that the past market value Vt−1 ≡Vt−1(Rt−1, Rt−2, . . .) is a non–
random function of the past returns Rt−i, and Vt−1 itself is known at time t −1.
To some larger extent, we suppress the dependency on some unknown initial values
or random variables in our notation when time series are studied.
In practical situations, the modeling of loss variables becomes even more
complicated. We mention two important extensions:
1. A trading portfolio usually consists of a larger number of assets. Therefore,
one has to consider the corresponding multivariate question.
2. The portfolio positions h = ht vary from one day to another, where ht is the
position in the asset Vt at time t. The assumption that the process ht merely
depends on the past returns Rt−1, Rt−2, . . . ensures that our analysis can be
easily extended to time varying portfolio weights.
GARCH Models for the Returns, Again
The statistical modeling of the loss distribution is again formulated in terms of
the log–returns Rt. As in (16.29) let
Rt = ˜σtεt,
t ≥1,
where the εt are iid random innovations, and εt is independent of the random
scale parameter ˜σt. Because we are merely interested in q–quantiles of conditional
distributions we do not necessarily impose any conditions on the moments of εt.
Therefore, we also speak of a random scale parameter instead of a stochastic
volatility. Speciﬁcally, we assume that ˜σt = σ(Rt−1, Rt−2, . . .) and, hence,
Rt = σ(Rt−1, Rt−2, . . .)εt,
t ≥1,
(16.47)
where εt is independent of the past returns Rt−1, Rt−2, . . ., and, consequently, the
innovation εt is independent of the random scale parameter.
For the ARCH(p) series in (16.38) we have
σ2(Rt−1, Rt−2, . . .) = α0 +

i≤p
αiR2
t−i.
(16.48)

16.8. Predicting the Serial Conditional VaR
403
For the GARCH(p, q) series in (16.44) the random scale parameter is recursively
deﬁned by
σ2(Rt−1, Rt−2, . . .) = α0 +

i≤p
αiR2
t−i +

j≤q
βjσ2(Rt−j−1, Rt−j−2, . . .).
(16.49)
Within the ARCH(p) and GARCH(p, q) series, the distributions of the returns
depend on the parameters αi and, respectively, the parameters αi and βj, and the
distribution of εt.
Representing GARCH as ARCH Models
By induction one gets a representation of ˜σ2
t in terms of the returns Rt, see, e.g.
Fan and Yao [18], (4.35). We have
˜σ2
t
=
α0
1 −q
j=1 βj
+
p

i=1
αiR2
t−i
=
+
p

i=1
αi

k≥1
q

j1=1
· · ·
q

jk=1
βj1 × · · · × βjkR2
t−i−j1−···−jk
(16.50)
with respect to an inﬁnite past. In practice, one has to use some initial values and
a truncation, e.g., by truncating all terms having a non–positive index.
As a special case we deduce the representation in the RiskMetricsT M (RM)
model of a GARCH(1,1) series, where
˜σ2
t = (1 −λ)R2
t−1 + λ˜σ2
t−1,
and the innovations εt are standard normal, cf. Section 16.7, page 400.
In that case, we have
˜σ2
t = (1 −λ)

i≥1
λi−1R2
t−i,
(16.51)
which can be addressed as a random scale parameter in an ARCH(∞) series. If
λ < 1 it makes sense to take the ﬁrst p terms in the sum, thus, getting an ARCH(p)
series, because the deleted returns Rt−p−1, Rt−p−2, . . . enter only with very small
cumulated weights 
i≥p+1 λi−1 = λp/(1 −λ).
The general strategy is to estimate a smaller number of parameters in a
GARCH series, yet to carry out further computations in a related (approximating)
ARCH(p) series.
Conditional Distributions in GARCH Series
It is evident from (16.47) that the conditional distribution of Rt given Rt−1 =
rt−1, Rt−2 = rt−2, . . . is the distribution of
σ(rt−1, rt−2, . . .)εt.
(16.52)

404
16. Extreme Returns in Asset Prices
Alternatively, the conditioning may simultaneously be based on certain past
returns and past scale parameters: the conditional distribution of Rt given Rt−1 =
rt−1, . . . , Rt−p = rt−p, ˜σt−1 = σt−1, . . . , ˜σt−q = σt−q is the distribution of
σtεt
(16.53)
with σt recursively deﬁned by
σ2
t = α0 +

i≤p
αir2
t−i +

j≤q
βjσ2
t−j.
The results in (16.52) and (16.53) are identical with σt = σ(rt−1, rt−2, . . .).
Based on these formulas one may write down conditional and joint densi-
ties and a likelihood function. In analogy to (16.40) and (16.41), one gets the
conditional densities
ft(rt|rt−1, rt−2, . . .) = 1
σt
f
 rt
σt

,
(16.54)
and the joint density
f(. . . , rt−1, rt) = f0(. . . , r−1, r0)
t
s=1
1
σs
f
 rs
σs

(16.55)
of the returns Rt, t ≥1, including some initial values.
Likewise the conditional density of R1, . . . Rt given the initial values is
f(r1, . . . , rt|r0, r−1, . . .) =
t
s=1
1
σs
f
 rs
σs

.
(16.56)
Now we may build a likelihood function based on the joint density or the
conditional likelihood function based on the conditional density (which leads to
the same expression if the term depending on f0 is omitted).
We provide two examples, namely, the cases where the innovation density f
is the standard normal density ϕ or a Student density with shape parameter α.
• (Normal innovations.) In Section 16.7 we mentioned the result that returns
have Pareto–like tails if the innovations εt are standard normal. Thus, nor-
mal innovations do not contradict our ﬁndings in the previous sections which
spoke in favor for return distributions with heavy tails such as Student dis-
tributions. From (16.56) one easily gets the conditional likelihood function.
For the conditional log–likelihood function one gets,
l(α, β)
=
−
t

s=1
log σs +
t

s=1
log ϕ(rs/σs)
=
−t
2 log(2π) −
t

s=1
log σs −1
2
t

s=1
r2
s
σ2s
,
(16.57)

16.8. Predicting the Serial Conditional VaR
405
where the right–hand side depends on the αi and βj via the σs; next one has
to compute the likelihood equations.
• (Student innovations.) In the recent ﬁnancial literature one observes a trend
to take Student innovations in the GARCH modeling. Simulation studies
indicate that this type of modeling for the innovations is compatible to
Pareto–distributed returns. More precisely, if the innovations are Student-
distributed, then one may conjecture that the returns are again Student–
distributed with a shape parameter smaller than the initial one (personal
communication by Petra Schupp).
We note the conditional log–likelihood function
l(α, β)
=
t log

Γ((α + 1)/2)
'
π(α −2)Γ(α/2)

−
t

s=1
log σs −α + 1
2
t

s=1
log

1 +
r2
s
σ2s(α −2)

, (16.58)
where we take standardized Student innovations with shape parameter α > 2
to make the result comparable to [18]. Recall from (1.62) that the standard
deviation is equal to
'
α/(α −2). We do not know any result for the case of
α ≤2.
The Serial Predictive Conditional VaR
We merely provide details in the special case of the RM model, see (16.51), where
the innovation df F is the standard normal df Φ.
Within the RM modeling let again ˆλ = 0.94 be the speciﬁcation of the pa-
rameter λ in the GARCH model as provided by RM (or some other estimate of
λ). The innovations εt are assumed to be distributed according to Φ. Then, one
gets the predictor
-
VaR(q; rt−1, rt−2, . . .) = vt−1ˆσtΦ−1(q),
(16.59)
with
ˆσt =
.
(1 −ˆλ)

i≥1
ˆλi−1r2
t−i,
(16.60)
of the conditional Value–at–Risk VaR(q; rt−1, rt−2, . . .) based on the observed mar-
ket value vt−1, and the past observed returns rt−i, where one only takes the ﬁrst
p terms in the sum.
It is worthwhile to reconsider all the steps which led to the prediction of
the serial conditional VaR (as the conditional q–quantile of the loss variable Lt
conditioned on the past returns):
1. replace the loss variable Lt by −Vt−1Rt

406
16. Extreme Returns in Asset Prices
2. take a model Rt = ˜σtεt with iid innovations εt for the returns;
3. specialize this model further to a GARCH model like the RM model;
4. replace unknown parameters like the λ by estimates based on observable
quantities (or by a value provided, e.g., by J.P. Morgan);
5. ﬁx a df for the innovation εt;
6. take an appropriate smaller number p of terms in the sum of the random
scale parameter which is the step from the GARCH model to a ARCH(p)
model,
7. take the conditional q–quantile within this ﬁnal model as a predictor of the
serial conditional VaR.
The conditional distribution in step 6, with the unknown paramters replaced
by estimates, may be addressed as a predictive distribution (as introduced in
Section 8.1, page 237). One may also speak of a serial predictive VaR as a predictor
of the serial conditional VaR.
In view of the longer list of conditions and approximations, one may ask after
the accuracy of the prediction by means of this predictive VaR. In this context,
the steps 2 to 5, which concern the statistical modeling and the estimation of
unknown parameters, are of particular interest. Later on, we indicate a validation
of the GARCH modeling by using the Rosenblatt transformation and, respectively,
certain residuals.
Predicting the Serial Conditional Expected Shortfall
The predictive df
F(l|rt−1, rt−2, . . .) = Φ(l/(vt−1 ˆσt)
(16.61)
in (16.59) for the loss variable Lt provides more than a predictor of the serial
conditional VaR.
For example, in place of a q–quantile we may use a diﬀerent functional pa-
rameter such as the expected shortfall (which itself is frequently called conditional
VaR in the literature; we hope that no confusion will arise due to this ambigu-
ity). The serial conditional expected shortfall may be predicted by plugging in the
predictive df. Other possible applications concern predictive intervals.
Validation for the GARCH Modeling
by Using the Rosenblatt Transformation
In the preceding sections, we employed exploratory tools to analyze ﬁnancial data
according to their postulated stationary distribution or their martingal (correla-
tion) structure. To make tests applicable (if dependencies cannot be neclected),

16.8. Predicting the Serial Conditional VaR
407
one needs complex theoretical results as, e.g., those developed in Chapter 7. Al-
ternatively, standard tests can be applied to transformed data.
If the data come from independent, not necessarily identically distributed
(innid) random variables, then the simple probability transformation helps to pro-
duce iid (0,1)–uniform data. Otherwise, as, e.g., in the GARCH case, one can use
the Rosenblatt transformation, see (8.29).
In the present context, the Rosenblatt transformation was applied by Diebold
et al. (in the article cited on page 238). We want to know whether the GARCH
modeling for the returns series Rt and the estimation procedures can be justiﬁed.
Let
Ft(·|rt−1, rt−2, . . .) = F(·/σ(rt−1, rt−2, . . .))
be the conditional df of Rt given Rt−1 = rt−1, Rt−2 = rt−2, . . ., where F is the
common df of the innovations εt, see (16.52). It is understood that unknown pa-
rameters in the conditional dfs are replaced by estimates based on the returns.
Thus, one is using the predictive dfs (the “estimated conditional dfs”) instead of
the actual conditional dfs in the Rosenblatt transformation. Diebold et al. also
compute the joint density of the transformed random variables when predictive
dfs in place of actual conditional dfs are applied in the transformation.
According to the Rosenblatt transformation, the
yt = Ft(rt|rt−1, rt−2, . . .)
may be regarded as iid (0, 1)–uniform data if the predictive dfs are suﬃciently
accurate.
Now, the distributional properties as well as the serial independence can be
analyzed. If, e.g., a test procedure rejects one of these properties then it is likely
that one of our basic conditions is violated (or our estimation procedures in the
GARCH model are not suﬃciently accurate).
Diebold et al. argue in favor of exploratory analysis (just in the spirit of larger
parts of the present book):
• ... when rejection occurs, the tests generally provide no guidance as to why.
• ... even if we know that rejection comes from violation of uniformity, we’d
like to know more: What, precisely, is the nature of violation ...
• Is the dependence strong and important, or is iid an adequate approximation,
even if strictly false?
They come to the conclusion: “The nonconstructive nature of tests of iid U(0,1)
behavior, and the nonconstructive nature of related separate tests of iid and U(0,1),
make us eager to adopt more revealing methods of exploratory data analysis.”
The authors analyze the transformed data by means of histograms and sample
autocorrelation functions. The latter tool is also applied to powers of centered data.

408
16. Extreme Returns in Asset Prices
In simulation studies, they generate the data according to a GARCH(1,1)
series (the data generating process) given by
˜σ2
t = 0.01 + 0.13 × R2
t−1 + 0.86 × ˜σ2
t−1
(16.62)
which is close to the RM model. Yet, in place of standard normal innovations,
these authors take standardized Student variables with shape parameter α = 6
(six degrees of freedom). For the standardization one has to use the standard
deviation which is equal to
'
3/2 for α = 6, see (1.62) and (16.58).
The Rosenblatt transformation is carried out under the following sequences
of predictive distributions:
• a sequence of iid standard normal or non–normal random variables;
• a GARCH(1,1) series with normal innovations,
• the correct model (with estimated parameters).
Diebold et al. conclude that “our density forecast evaluation procedures clearly
and correctly revealed the strength and weakness of the various density forecasts.”
A real data set of daily S&P returns is also analyzed within the framework
of MA(1)–GARCH(1,1) model (again with Student innovations).
Despite the arguments above, we mention some test procedures which are
employed for testing the simple null–hypothesis of iid (0, 1)–uniform random vari-
ables. One may apply goodness–of–ﬁt tests such as χ2 or Kolmogorov–Smirnov
tests. Likelihood–ratio tests may be applied to a binomially distributed number
of exceedances, see Kupiec23. The Kupiec test can be regarded as a two–sided
extension of the traﬃc light approach.
The traﬃc light approach is a binomial test based on the exceedances of the
serial conditional VaR, i.e., marks in time, where the observed losses exceed the
respective predicted VaR. By the Rosenblatt transformation it is ensured that these
exceedances are binomially distributed if the serial conditional df is appropriately
selected. This binomial test is especially relevant for regulatory purposes24.
Another useful reference in the context of testing the conditional modeling is
Berkowitz25.
By statistical tests or explorative means like QQ–plot, PP–plot, histograms or
autocorrelation functions, it is possible to iterate the diﬀerent steps 1–7 above that
23Kupiec, P.H. (1995). Techniques for verifying the accuracy of risk measurement mod-
els. J. Derivatives 2, 73–84.
24Basel Committee on Banking Supervision: Supervisory Framework for ‘Backtesting’
in conjunction with the Internal Models Approach to Market Risk Capital Requirements,
1996.
25Berkowitz, J. (2001). Testing density forecasts, with applications to risk management,
J. Business & Economic Statistics 19, 465–474.

16.8. Predicting the Serial Conditional VaR
409
led to the respective dfs26. If there is a signiﬁcant serial correlation identiﬁed (or a
signiﬁcant autocorrelation of the squared returns), the speciﬁed GARCH/ARCH–
model can be improved and, in addition, the estimated parameters should be
reviewed (steps 3, 4 and 6 of the loss variable speciﬁcation steps). If the distribu-
tional properties do not ﬁt well (which again is explored by statistical or graphical
means), the ﬁxed df for the innovations is questionable (step 5). This procedure is
called the “backtesting” of the risk model and should be conducted regularily to
improve stepwise and iteratively the chosen model and its assumptions.
These pragmatic procedures can as well be employed in the case of multi-
variate asset returns and for time varying portfolio weights ht (mentioned at the
beginning of this section).
Validation for the GARCH Modeling by Using Residuals
Fan and Yao [18] analyze daily S&P returns (diﬀerent from the one used by Diebold
et al.) with a modeling of the innovations by Student distributions. Applying the
conditional likelihood method these authors get the following GARCH(1,1) series
(with estimated parameters)
ˆσ2
t = 0.007 + 0.047 × r2
t−1 + 0.945 × ˆσ2
t−1
with an additional estimated shape parameter α = 7.41 of the Student distribution.
The validation for the GARCH modeling is based on the residuals
ˆεs = rs/ˆσs,
s ≥1.
(16.63)
One can expect that the residuals ˆεs have properties as the innovations εs to some
extent.
Fan and Yao apply tests as well as exploratory tools to the residuals and
conclude that Student modeling is more agreeable than the normal one. It is re-
ported that there is no signiﬁcant autocorrelation in the residual series and its
squared series. These authors also discuss the question of predicting the condi-
tional VaR based on the distribution of ˆRt = ˆσtˆεt,α, where ˆεt,α corresponds to a
Student–variable with estimated shape parameter α, and ˆσt is the predictive scale
parameter (with parameters estimated within the GARCH model).
Semiparametric Evaluation of the Conditional Df of Returns
We add some remarks about a predictive df of the return Rt within the GARCH
setup, where the parameters αi and βj are estimated in a correct manner, yet
the validity of the overall parametric modeling of the common innovation df F is
questionable. Then, the residuals ˆεs may still be regarded as observations under
26Wehn, C.S. (2005). Ans¨atze zur Validierung von Marktrisikomodellen—Systema-
tisierung, Anwendungsm¨oglichkeiten und Grenzen der Verfahren. Shaker, Aachen.

410
16. Extreme Returns in Asset Prices
F. Therefore, we may estimate F, based on the residuals, by means of the sample
df ˆFt−1 or a kernel df ˆFt−1,b with bandwidth b > 0 as deﬁned in (2.17). One may
as well apply the extreme value technique to estimate the upper tail of F.
By piecing together the predictive scale parameter ˆσt and the estimates ˆFt−1
or ˆFt−1,b of F, one gets the predictive dfs ˆFt−1(·/ˆσt) or ˆFt−1,b(·/ˆσt) of the return
Rt. In order to get a predictive VaR, one has to use quantiles of the predictive
dfs. The approach, using the kernel qf introduced below (2.17), would also be an
option. In conjunction with the kernel df approach, special reﬁned methods of
selecting the quantile are available27 28.
An Empirical Evaluation of the Conditional Df of Returns
Within the ARCH(p) setting we predicted the conditional df Ft(·|rt−1, . . . , rt−p) =
F(·/σ(rt−1, . . . , rt−p)) of the return Rt given Rt−1 = rt−1, . . . , Rt−p = rt−p, where
F is the common df of the innovations εt. A predictive version was provided by
replacing the unknown parameters αi by estimates ˆαi based on the past returns.
One may ask whether an empirical approach is possible by estimating the condi-
tional df in a direct manner under certain weak assumptions imposed on the time
series of returns. There is a positive answer if p is small, yet our answer is negative
due to the “curse of dimensionality” for the more interesting case of larger p. For
simplicity, we merely give details for p = 1.
In view of the corresponding property of an ARCH(1)–series, one may as-
sume that the returns Rt satisfy the technical condition of a Markov chain with
stationary transition df F(·|r). In the special case of an ARCH(1)–series we have
F(·|r) = F(·/σ(r)). Now we proceed as in Section 8.2 with the iid condition re-
placed by the Markov condition. Let rt(1), . . . , rt(k), 1 < t(j) < t, be the returns
in the past for which rt(j)−1 is close to the ﬁxed value r. Then, the rt(j) may be
regarded as observations under F(·|r), and the sample df Fk(·|r), based on the
rt(j), as an estimator of the conditional df F(·|r).
27Kozek, A. (2003). On M–estimators and normal quantiles. Ann. Statist. 31, 1170–
1185.
28Jaschke, S., Stahl, G. and Stehle, R. (2006). Value–at–risk forecasts under scrutiny—
the German experience. To appear in Quantitative Finance.

Chapter 17
The Impact of Large Claims
on Actuarial Decisions
co–authored by M. Radtke1
In this chapter, we elaborate on and develop some ideas which were already pre-
sented in Section 1.1. Recall that the expectation of the total claim amount de-
termines the net premium. Based on the net premium, the insurer determines the
total premium that must be paid by the policy holder. We start in Section 17.1
with the calculation of the df, expectation and variance of the total claim amount.
From our viewpoint, reinsurance is of particular interest, because the excesses
of large or catastrophic claim sizes over a certain higher threshold are covered by
the reinsurer. Special names for threshold are limit, priority or retentation level.
One may distinguish between the following reinsurance treaties:
• excess–of–loss (XL) reinsurance, when the reinsurer pays the excess of a
certain ﬁxed limit for individual claim sizes;
• stop–loss reinsurance or total loss insurance, when the reinsurer covers the
excess of a certain limit for the total claim amount of an insurer’s portfolio,
• ECOMOR reinsurance, which is a modiﬁcation of the XL treaty with the
kth largest individual claim size taken as a random limit (thus, the reinsurer
only pays excesses of the kth largest claim size).
The net premium for the next period can be estimated in a nonparametric
manner by means of the total claim amount of preceding periods. It is suggested
to also employ a parametric approach in the reinsurance business.
1K¨olnische R¨uckversicherung; co–authored the ﬁrst edition.

412
17. The Impact of Large Claims on Actuarial Decisions
We merely deal with risks in conjunction with the XL reinsurance treaty. The
required modiﬁcations for the ECOMOR treaty are apparent from our viewpoint2.
The restriction to the XL treaty is justiﬁed by the fact that the ECOMOR treaty
seems to have less practical relevance.
In XL reinsurance, the estimation of the net premium can be done again
within the generalized Pareto (GP) model (Section 17.2) or some other statistical
model such as the Benktander II or truncated converse Weibull model.
The segmentation of a portfolio with respect to the probable maximum loss
(PML) of single risks is dealt with in Section 17.3. The segmentation is necessary
to adjust the tariﬁcation to the risks of individual policy holders. We pursue an
empirical and a parametric statistical approach towards this important question.
The parametric one is based on the estimation of the mean of GP distributions.
Another important question is the choice of an adequate initial reserve (cap-
ital) for a portfolio. We will introduce a concept based on ﬁnite ruin theory (see,
e.g., the book by Gerber [22]) in which the initial reserve becomes a parameter
which can be estimated by the insurer. For that purpose, one must formulate a
certain criterion which determines the initial reserve in a unique manner: we sug-
gest using a T –year initial reserve for a q × 100% ruin probability. This is the
initial reserve for a portfolio such that ruin occurs within the next T years with
a probability of q × 100%. Reasonable quantities are a time horizon of T = 10
years and a predetermined ruin probability of 1% or 5%. These ideas are pursued
in Section 17.4 within the framework of risk processes. This chapter is concluded
with some remarks about asymptotic ruin theory (Section 17.5).
17.1
Numbers of Claims
and the Total Claim Amount
Let Sn = 
i≤n Xi denote the total (aggregate) claim amount of the ﬁrst n of
the random claims sizes X1, X2, X3, . . . . Then, the total claim amount for a given
period can be written
SN =

i≤N
Xi,
(17.1)
where N is the random number of claims occurring within this period. Remember
that the expectation E(SN) of the total claim amount is the net premium that
must be estimated by the actuary.
The Total Claims DF
The df, expectation and variance of the total claim amount SN will be computed
under the conditions of the homogeneous risk model.
2For details and further references, see, e.g., Kremer, E. (1992). The total claims
amount of largest claims reinsurance revisited. Bl¨atter DGVM 22, 431–439.

17.1. Numbers of Claims and the Total Claim Amount
413
(Homogeneous Risks Model): the claim sizes X1, X2, X3, . . . are iid random
variables with common df F. Additionally, the claim number N and the sequence
X1, X2, X3, . . . are independent.
With calculations similar to those in (1.54) we obtain for the total claims df
P{SN ≤x}
=
∞

n=0
P{SN ≤x, N = n}
=
∞

n=0
P{N = n}F n∗(x),
(17.2)
where S0 = 0, F 0∗(x) = I(0 ≤x) and F n∗(x) = P{Sn ≤x} is the n–fold
convolution, cf. page 30, of F again.
Next, the total claims df will be written in a more explicit form for binomial,
Poisson, negative binomial and geometric claim numbers3.
• (Compound Binomial.) If the number N of claims is a binomial random
variable with parameters n and p, then the total claims df is
P{SN ≤x} =
n

k=0
Bn,p{k}F k∗(x).
(17.3)
• (Compound Poisson.) If the number N of claims is a Poisson random variable
with parameter λ, then the total claims df is
P{SN ≤x} =
∞

k=0
Pλ{k}F k∗(x).
(17.4)
• (Compound Negative Binomial.) If the number N of claims is a negative
binomial random variable with parameters r and p, see (3.31), then the total
claims df is
P{SN ≤x} =
∞

k=0
B−
r,p{k}F k∗(x).
(17.5)
• (A Simpliﬁed Representation in the Geometric Case.) If r = 1 in the preced-
ing example, then N is geometric. Thus,
P{N = k}
=
B−
1,p{k}
=
p(1 −p)k,
k = 0, 1, 2, 3, . . . ,
3Also see Kuon, S., Radtke, M. and Reich, A. (1993). An appropriate way to switch
from the individual risk model to the collective one. Astin Bulletin 23, 23–54.

414
17. The Impact of Large Claims on Actuarial Decisions
for some p with 0 < p < 1, and
P{SN ≤x} = p
∞

k=0
(1 −p)kF k∗(x).
(17.6)
In addition, assume that the claim sizes are exponentially distributed with
common df F(x) = 1 −e−x/σ, x > 0. Recall from (4.7) that the convolution
F k∗is a gamma df with density
f k∗(x) = σ−1(x/σ)k−1 exp(−x/σ)/(k −1)!,
x > 0.
Check that
p
1 −p
∞

k=1
(1 −p)kf k∗(x) = σ−1pe−px/σ,
x ≥0,
and, hence, equality also holds for the pertaining dfs. By combining this with
(17.6), one obtains the representation
P{SN ≤x} = p + (1 −p)(1 −e−px/σ),
x ≥0,
(17.7)
of the total claims df.
Next, we verify that the net premium4—the expectation of the total claim
amount—within the given period is
E(SN) = E(X)E(N),
(17.8)
where X is a random variable with the same distribution as the Xi. Because the
random variables Sn and I(N = n) are independent, we have
E(SN)
=
∞

n=0
E

SNI(N = n)

=
∞

n=1
E(Sn)E(I(N = n))
=
E(X)
∞

n=1
nP{N = n}
=
E(X)E(N)
and, thus, (17.8) holds.
The variance of SN may be computed in a similar manner. One obtains
V (SN) = V (X)E(N) + (EX)2V (N).
(17.9)
4For an elementary introduction to premium principles see Straub, E. (1989). Non–Life
Insurance Mathematics. Springer, Berlin.

17.2. Estimation of the Net Premium
415
17.2
Estimation of the Net Premium
The net premium will be estimated in a nonparametric and a parametric manner
under various conditions.
Nonparametric Estimation of the Net Premium
Assume that the claim arrival times T1, T2, T3, . . . constitute a claim number pro-
cess N(t) = ∞
i=1 I(Ti ≤t), t ≥0, which has a constant arrival rate, that is,
E

N(t2) −N(t1)

= (t2 −t1)E(N(1)),
t1 < t2.
(17.10)
The estimation of the net premium E(SN) for the next period will be based on
the total claim amount SN(T ) of the past T years. Under the conditions of the
homogeneous risk model, one obtains according to (17.8),
E(SN) = E(N(1))E(X),
and, under condition (17.10),
E(SN(T )) = TE(N(1))E(X).
Therefore, SN(T )/T is an unbiased estimator of the net premium E(SN). The
variance of this estimator can easily be deduced from (17.9).
Example 17.2.1. (Large Norwegian Fire Claim Data.) A data set of large ﬁre claim data
(stored in the ﬁle it–ﬁre2.dat) was extensively analyzed by R. Schnieper5.
Table 17.1. Fire claims sizes over 22.0 million NKr from 1983 to 1992.
year
claim size
year
claim size
(in millions)
(in millions)
1983
42.719
23.208
1984
105.860
1990
37.772
1986
29.172
34.126
22.654
27.990
1987
61.992
1992
53.472
35.000
36.269
1988
26.891
31.088
1989
25.590
25.907
24.130
5Schnieper, R. (1993). Praktische Erfahrungen mit Grossschadenverteilungen. Mitteil.
Schweiz. Verein. Versicherungsmath., 149–165.

416
17. The Impact of Large Claims on Actuarial Decisions
The original ﬁre claim data over a priority of 10.0 million NKr (Norwegian crowns)
were corrected by means of a certain trend function. The original data are indexed “as if”
they occurred in 1993. Table 17.1 contains the corrected data. In addition, the original
data were made anonymous so that the portfolio cannot be identiﬁed (personal commu-
nication).
We estimate the net premium for the excess claim sizes over the given priority of
u = 22 million NKr and another one, namely u = 50 million NKr. In the ﬁrst case, the
estimation is based on the given 17 claim sizes; in the second case, only 3 claim sizes are
available and, thus, the estimate is less reliable. The nonparametric estimates of the net
premium (XL) are 26.98 and, respectively, 7.13 million NKr.
If the estimation of the net premium must be based on such a small number
of claim sizes as in the preceding example, then a parametric approach should be
employed.
Parametric Estimation of the Net Premium for Excess Claims
Assume that the arrival times and the claim sizes satisfy the conditions of a
Poisson(λ, F) process, cf. page 248. Notice that λ is the mean number of claims
in a period of unit length and F is the df of the excesses over the priority u. We
assume that F = Wγ,u,σ is a GP df. The mean claim size E(X) is the mean of the
GP df (in the γ–parameterization). The net premium for the next period is
E(SN) = λmWγ,u,σ,
where mF again denotes the mean of a df F. Thus, by estimating the parameters
λ, γ and σ one obtains an estimate of the net premium.
Notice that
λN(T ) = N(T )/T
is an estimator of λ = E(N). If γN(T ) and σN(T ) are estimators of the parameters
γ and σ, then the mean
mN(T ) := u +
σN(T )
1 + γN(T )
of the GP df WγN(T ),u,σN(T ) is an estimator of the mean claim size E(X) = mWγ,u,σ.
Therefore, λN(T )mN(T ) is an estimator of the net premium E(SN).
Example 17.2.2. (Continuation of Example 17.2.1.) Apparently, the estimated parameter
for the number of claims within a single year—the number of claims divided by the
number of years—is λ = 1.7.
The parametric estimation of the claim size df was carried out in the GP0, GP1 and
GP models with left endpoint u = 22.0. The estimated parameters are listed in Table
17.2. Necessarily, the nonparametric estimate and the MLE in the exponential model
(GP0) lead exactly to the same net premium.

17.2. Estimation of the Net Premium
417
Table 17.2. Parametric estimation of the mean claim size (excesses), the net premium
(XL) in millions of NKr and nonparametric estimate of net premium (XL).
γ–Parameterization
Mean Claim Size
Net Premium (XL)
γ
µ
σ
E(X)
E(SN)
nonparametric
–
–
–
26.98
MLE(GP0)
0.0
22.0
15.873
15.87
26.98
Hill(GP1)
0.451
22.0
9.915
18.06
30.70
MLE(GP)
0.254
22.0
11.948
16.02
27.23
Moment(GP)
0.293
22.0
12.060
17.06
29.00
In Fig. 17.1, the sample mean excess functions and the mean excess functions of GP
dfs pertaining to the parametric estimates are plotted. These functions are given within
the range of 20 to 35 millions NKr. First of all we are interested in the question as to
whether the sample mean excess function is suﬃciently close to a straight line so that
the GP modeling is acceptable.
claim sizes
25
30
35
15
20
25
Fig. 17.1. Sample mean excess
function and mean excess func-
tions for Hill(GP1) (solid), Mo-
ment(GP) (dashed), MLE(GP)
(dotted) and also MLE(GP0)
(dashed–dotted).
A comparison of the mean excess functions leads to the conclusion that a GP
modeling is acceptable. In addition, the plots speak in favor of the MLE(GP) which,
consequently, will be employed in the following computations.
For small sample sizes as in the preceding example it is desirable to include
further information in the statistical analysis. One possibility is to use Bayesian
analysis as it is outlined in Section 17.6. Another one is to pool data over diﬀerent
portfolios (as it was also done by Schnieper in the paper cited on page 415). Such
a pooling was also done in the regional frequency analysis (cf. Section 14.4).
Analyzing large Danish ﬁre claim data, McNeil6 justiﬁes the GP modeling for
the upper tail of the claim size distribution and obtains an ML estimate γ = 0.497.
6McNeil, A.J. (1997). Estimating the tails of loss severity distributions using extreme
value theory. ASTIN Bulletin 27, 117–137.

418
17. The Impact of Large Claims on Actuarial Decisions
Estimating the Net Premium for Extraordinarily High Priorities
We regard a priority v > u as extraordinarily high when the number of observed
excesses over v is very small (including the case that no claim size over v has been
observed). This is the typical situation when v is the predetermined retentation
level yet one wants to base the statistical inference on a larger number of claims.
Within the parametric framework, the preceding parametric estimates γN(T ) and
σN(T )—obtained for the priority u—can still be employed to estimate the net
premium for claim sizes over v > u.
Recall from page 249 that the claim arrival times and claim sizes over v
satisfy the conditions of a Poisson
˜λ, W [v]
γ,u,σ

process, where ˜λ = λ(1−Wγ,u,σ(v)).
We obtain
˜λN(T ) = N(T )
T

1 + γN(T )
v −u
σN(T )
−1/γN(T )
as an estimator of ˜λ = E(N). From (1.45), we found that
W [v]
γ,u,σ = Wγ,v,σ+γ(v−u).
Plugging in the estimators for γ and σ, one obtains by the mean
	mN(T ) = v + σN(T ) + γN(T )(v −u)
1 −γN(T )
an estimator of the mean claim size E(X). By combining these estimators, one
obtains ˜λN(T ) 	mN(T ) as an estimator of the net premium E(SN) = E(N)E(X) for
the excesses of the claim sizes over v.
Example 17.2.3. (Continuation of Example 17.2.1.) The estimated parameters of the
GP distribution (based on the 17 claim sizes) for the retentation level of 50 millions are
given in the following table.
Table 17.3. Parametric estimation of the mean claim size (excesses) and the net pre-
mium (XL) in millions of NKr (and nonparametric estimate of net premium (XL)) for
retentation level of 50 million.
γ–Parameterization
Mean Claim Size
Net Premium (XL)
γ
µ
σ
E(X)
E(SN)
nonparametric
–
–
–
7.13
MLE(GP)
0.254
50.0
19.06
25.55
6.90
Notice that the estimate of the shape parameter γ is identical to that of the reten-
tation level u = 22.

17.3. Segmentation According to the Probable Maximum Loss
419
Finally, we remark that there could be a positive trend in the frequency of
large claims. Yet, such phenomena will be only dealt with in conjunction with
partial duration ﬂood series (Section 14.2).
Combining the Nonparametric and Parametric Approach
Nonparametric and parametric calculations can be combined to estimate the claim
size df
• over a ﬁxed priority u in a parametric manner,
• below the priority in a nonparametric manner.
Let Fn(x; ·) be the sample df based on the claim sizes xi. Let Wγ,˜µ,˜σ be the
GP df ﬁtted to the upper tail of Fn(x; ·), cf. (2.35). Both dfs can be pieced together
smoothly because Wγ,˜µ,˜σ(u) = Fn(x; u). The df
Fn(x; x)I(x ≤u) + Wγ,˜µ,˜σ(x)I(x > u)
is such a nonparametric–parametric estimate of the claim size df. Such a procedure
must also be used when the parametric hypothesis is valid only for a threshold
larger than that predetermined by the XL treaty.
Let us review the justiﬁcation for such an approach. We want to utilize a
nontrivial model for the tail of the distribution, even in regions where possibly
no data are available. A parametric modeling seems to be the only reasonable
approach to that question. On the other hand, there is a greater bulk of data
available in the center of the distribution so that a nonparametric estimate can
have a higher accuracy than a parametric estimate if the parametric modeling is
incorrect.
17.3
Segmentation According to
the Probable Maximum Loss
In this section, we deal with the tariﬁcation of policies with respect to the probable
maximum loss (PML)7, especially in the property business with losses caused, e.g.,
by ﬁre, storm and earthquake. Besides a nonparametric estimation of the mean
claim sizes in the diﬀerent PML groups we pursue a parametric approaches within
GP models.
7Gerathewohl, K. (1976). R¨uckversicherung: Grundlagen und Praxis. Verlag Ver-
sicherungswirtschaft.

420
17. The Impact of Large Claims on Actuarial Decisions
Mean Claim Sizes in Dependence on the Probable Maximum Loss
It is evident that the loss potential is not homogeneous for all risks of a portfolio,
but it depends on the underlying exposure of the individual risk of a policy. A
particularly important ﬁgure is the PML of the individual risk that describes
the maximum single claim size covered by the policy. This value is estimated
by underwriting experts on the basis of additional information of the individual
risk and can be thought of as the upper endpoint of the individual claim size
distribution.
Thus, besides a claim size variable X for single policies, there is a covariate
Z, namely the PML. One is interested in the conditional expectation E(X|z) of
X given Z = z, where z is the PML for a given policy. The aim is to estimate the
conditional mean claim size E(X|z) as a function of the PML z (also see Section
6.6). It is apparent that the estimation of the conditional mean claim size is a
regression problem. The next steps are to estimate the mean claim number and
the mean total claim amount (net premium) in dependence on the PML.
In practice, this is done for each sub–portfolio deﬁned by a PML group of risks
within speciﬁed priorities. We introduce a greater variety of methods to estimate
the mean claim size within a PML group. We are particularly interested in PMLs
and claim sizes over a higher priority so that it is tempting to use also parametric
approaches besides empirical ones.
Estimating the Mean Claim Size for a PML Group
by Using Claim Degrees
The ith PML group is determined by those risks with PMLs z between boundaries
pi and pi+1, where
u = p0 < p1 < · · · < pm−1 < pm = ∞
is a predetermined partition. We write ni for the number of claims and xi,j for
the claim sizes belonging to the ith PML group. The PML pertaining to xi,j is
denoted by zi,j. Note that n−1
i

j≤ni xi,j is an estimate of the mean claim size
within the ith PML group.
In order to make the results in the diﬀerent PML groups comparable, we
introduce
• the claim degrees xi,j/zi,j, and
• the empirical mean degrees di = n−1
i

j≤ni xi,j/zi,j.
The mean claim size in the ith group can be estimated by
mi = qidi
where qi = (pi + pi+1)/2 are the centers of the PML intervals. The advantage of
using mean degrees di for estimating the mean claim sizes is that one may smooth
the variation of these values by using, e.g., a polynomial least squares function.

17.3. Segmentation According to the Probable Maximum Loss
421
Example 17.3.1. (A Segmented Fire–Industrial Portfolio.) We deal with a middle–sized
ﬁre–industrial portfolio segmented into 25 PML groups. It is indexed and coded. Each
recorded claim size exceeds u = 100 thousand currency units. A ﬁrst impression of the
data set (stored in im–pmlﬁ.dat) can be gained from Table 17.4.
Table 17.4. PMLs and claim sizes in ﬁre–industrial portfolio in thousands.
No. i
From pi
To pi+1
PML zi,j
Claim Size xi,j
Claim Degree xi,j/zi,j
1
100
250
.
.
.
2
250
500
434
123
0.28341
2
250
500
324
254
0.78395
3
500
750
727
534
0.73427
·
·
·
·
·
·
25
150000
200000
183186
176
0.00096
25
150000
200000
169666
607
0.00358
25
150000
200000
165994
161
0.00097
The largest claim size of over 37 millions occurred in the 24th PML group. No
claims are recorded in the ﬁrst PML group. An overview of the data is obtained by the
scatterplot in Fig. 17.2. One recognizes that the recorded claim sizes in the highest PML
group with PMLs between 150 and 200 millions are relatively small.
PML
Claim Size
0
100000
200000
0
40000
Fig. 17.2. Scatterplot of claim
sizes xj plotted against the
PMLs zj.
The special feature of the largest PML group may be caused by a systematic under-
writing eﬀect of these large scaled risks, i.e., the insurance company takes some individual
risk management measures in order to avoid heavy losses.
Another aspect is that these large PML risks normally are composed of a number
of diﬀerently located smaller risks—a greater industrial ﬁrm with diﬀerent locations and

422
17. The Impact of Large Claims on Actuarial Decisions
buildings—which produce independently small losses and, therefore, a total loss is less
probable.
In Fig. 17.3, the mean claim degrees di are plotted against the group number i. A
least squares line is added (the mean degrees of the second and third group are omitted
from that analysis).
mean claim degree
PML group
10
20
0.2
0.4
0.6
Fig. 17.3. Plotting the mean de-
grees di against the group num-
ber i including a least squares
line (dotted).
It is typical that the mean degrees di are decreasing in i. This is partially due to
the fact that for larger PMLs there is a smaller threshold for the claim degrees (therefore,
this evaluation of the mean degree is a length–biased estimation).
Up to now, it is not clariﬁed whether a plotting and smoothing should be
done against the centers of the PML intervals instead of the group numbers. Using
the smoothed mean degrees d′
i from the least squares line, one obtains the estimate
m′
i = qid′
i
(17.11)
of the mean claim size in the ith PML group.
Relative Frequencies of Segmented Portfolios
and Estimating the Net Premium in PML Groups
We introduce the relative claim frequencies
fi = ni/ri
where ni is again the number of claims and ri is the number of risks belonging to
the ith PML group.
Example 17.3.2. (Continuation of Example 17.3.1.) In Table 17.5, we list the number of
risks ri and the claim numbers ni for each of the PML groups.

17.3. Segmentation According to the Probable Maximum Loss
423
Table 17.5. Number of risks ri and number of claims ni in ith PML group.
Group Nr. i
2
3
4
5
6
7
8
9
10
11
12
13
ri
2049 1658 1673 2297 1732 2536 1709 1186 1749 1669 1349 726
ni
2
1
3
5
4
14
1
13
22
22
25
36
14
15
16
17
18
19
20
21
22
23
24
25
ri
719
254
194
123
76
61
34
36
24
26
15
28
ni
36
24
22
9
13
19
8
11
5
7
5
9
We also plot the relative frequencies fi = ni/ri of claim numbers against the group
number i and include a least squares line. The increase of the plot can also be explained
as an eﬀect of a length–biased estimation.
PML group
rel. claim frequency
10
20
0.2
0.4
Fig. 17.4. Plotting the relative frequencies fi = ni/ri of claim numbers against the group
number i (including a least squares (dotted) line).
Let m′
i be as in (17.11) and f ′
i the smoothed relative frequencies. Then,
besides ni
j=1 xi,j one obtains by
m′
if ′
iri
an estimate of the net premium for the ith PML group. Subsequently, m′
i will
be replaced by parametric estimates of the mean claim size. These estimates are
either based on claim degrees or on the original claim sizes. The smoothing by
least squares lines is done for the parameters of GP distributions.

424
17. The Impact of Large Claims on Actuarial Decisions
A Parametric Approach
The mean claim size in the ith PML group can be estimated in a parametric
manner. Specify a GP model for the claim degrees or the original claim sizes
in the ith PML group and use the mean of the estimated GP distribution for
estimating the mean claim degree or the mean claim size. For example, the claim
degree in the ith group are between ui = u/pi+1 and 1 and, thus, a beta (GP2)
modeling is possible (with unknown shape parameter αi). Note that the mean of
this beta df is
(1 + ui|αi|/(1 + |αi|).
Use, e.g., the Hill(GP2) estimator for evaluating αi.
A Parametric Nearest Neighbor Approach
The disadvantage of the previous method is that, for some of the PML groups, the
parametric estimation must be based on a very small number of claims. Recall that
the mean claim size within a PML group is related to the conditional expectation
E(X|z) of the claim size X conditioned on the PML Z = z.
The estimation of the conditional expectation E(X|z) can also be based on
the claim sizes yj, j = 1, . . . , k, pertaining to the k PMLs zj closest to z (for an
introduction to the nearest neighbor method see Section 7.2). Then,
mk = k−1 
j≤k
yj
is an empirical estimate of the conditional mean claim size E(X|z).
Likewise, the estimation can be carried out within a GP model. For that
purpose, compute estimates ˆγk(z) and ˆσk(z) of the shape and scale parameters
γ(z) and σ(z) based on the yj within a GP model and use the mean of the GP
distribution Wˆγk(z),u,ˆσk(z) as an estimate of E(X|z).
Example 17.3.3. (Continuation of Example 17.3.1.) For the given ﬁre–industrial claim
sizes, the estimates ˆγk(z) vary around the value 0.9 which corresponds to a Pareto mod-
eling. In the further calculation, we take the constant estimate ˆγk(z) = 0.88, cf. (17.13).
The scale parameters σ(z) of the GP distribution W0.88,u,σ(z) are estimated by means of
MLE’s for unknown scale parameters.
Motivated by Example 17.3.3, we use the simpliﬁed modeling of claim size
distributions conditioned on Z = z by means of GP dfs Wγ0,u,σ(z), where γ0 is a
predetermined shape parameter which is independent of z. If ˆσk(z) are estimates
of σ(z) (ﬁrst, a smoothing may be carried out for the ˆσk(z)), then the mean
mi,k = u + ˆσk(qi)/(1 −γ0)
(17.12)
of Wγ0,u,ˆσk(qi) is an estimate of the mean claim size in the ith PML group.

17.3. Segmentation According to the Probable Maximum Loss
425
Example 17.3.4. (Continuation of Example 17.3.3.) Our computations only concern the
10th to 24th PML group (thus, we have PMLs from 5 to 150 millions). We tabulate the
estimates m′
i and mi,k in (17.11) and (17.12) of the mean claim sizes in the diﬀerent
groups for k = 30. The smoothing was carried out with least squares lines of degree 2.
Table 17.6. Estimated mean claim sizes m′
i and mi,k in ith PML group.
Group Nr. i
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
m′
i
435
569
730
879
982
951
807
635
521
550
808 1380 2352 4747 10139
mi,k
1367 1334 1292 1248 1208 1203 1254 1361 1524 1743 2017 2348 2734 3418 4526
Notice the remarkable diﬀerences in the total amounts of the estimates as well as
the diﬀerent variations from group to group. Up to now we can not oﬀer a ﬁnal conclusion
which of the estimation methods is preferable for the given PML data set.
Further modiﬁcations of the presented approaches are possible. For example,
the smoothing of mean degrees, relative frequencies and GP parameters may be
done by ﬁtting least squares lines to the log–values or the estimated GP distribu-
tions may be truncated at the upper boundary of the PML segment.
A Collective Viewpoint
We clarify in which way we determined the mysterious shape parameter γ0 = 0.88
in Example 17.3.3. The collective claim size df for the portfolio (the unconditional
claim size df) is
Fγ,Z(x) =

Wγ,u,σ(z)(x) dFZ(z)
where FZ is the df of Z.
For γ close to 1 and large x, the conditional of Wγ,u,σ(z) can be replaced by
the Pareto df W1,1/γ,0,σ(z)/γ in the α–parameterization (cf. (6.38)). Therefore,
Fγ,Z(x) ≈

W1,1/γ,0,σ(z)/γ(x) dFZ(z) = W1,1/γ,0,σ0(x)
(17.13)
for larger x, where
σ0 =

σ(z)1/γ dFZ(z)
γ
/γ.
Thus, a Pareto modeling is adequate for the portfolio if this is justiﬁed in the seg-
ments. The value γ = 0.88 in Example 17.3.3 was estimated within this collective
approach.
The standard reference for collective risk theory is the book by B¨uhlmann
which was mentioned on page 5. Also see the aforementioned article by Kuon,
Radtke and Reich.

426
17. The Impact of Large Claims on Actuarial Decisions
17.4
The Risk Process
and the T –Year Initial Reserve
In Sections 17.1–17.3, the total claim amount SN was evaluated within a certain
ﬁxed period. When the time t varies, we write N(t) and S(t) in place of N and
SN, thereby obtaining the claim number and total claims processes.
Next, we deal with the risk process, which is primarily based on the initial
reserve, denoted by s, and the total claims process. The initial reserve is a variable
of the system which can be chosen by the insurer. Ruin occurs when the risk
process becomes negative.
The primary aim in risk theory is the calculation of the ultimate ruin proba-
bility8 or an upper bound (Lundberg inequality) of that quantity (also see Section
17.5). Knowledge about the ruin probability within a ﬁnite time horizon is pre-
ferred by practitioners.
For us the initial reserve is the central parameter that must be estimated by
the actuary. We estimated that initial reserve s such that ruin within a time span
of length T occurs with a probability of q × 100%.
Are Reserves for Single Portfolios of Practical Importance?
To measure the performance of a single portfolio from an economic viewpoint, the
insurer must take two aspects into account, namely
• the proﬁtability, i.e., the expected proﬁt from running the portfolio over a
certain period of time, and
• a certain ﬂuctuation potential
of a portfolio. The proﬁtability can be deduced from the estimated net premium
and the related premium income.
Yet, for an economic evaluation of a portfolio, it is also necessary to quantify
the ﬂuctuations of the results of a portfolio over time. A commonly used approach
is to consider the risk process and derive some initial reserve which is necessary to
avoid a technical ruin in a ﬁxed ﬁnite time horizon with a certain probability.
This reserve can be interpreted as the security capital the insurer needs to
carry the collective risk of the portfolio. It is a kind of ﬂuctuation reserve which
ensures that the company does not become ruined over time by possible deviations
from the expected results of the portfolio.
In this context, another important aspect is that risk processes allow compar-
ison of diﬀerent portfolios not only by their proﬁtability, but also by the required
initial reserves.
8See, e.g., Vylder, de F. (1997). Advanced Risk Theory. Editrans de l’Universit´e Brux-
elles.

17.4. The Risk Process and the T –Year Initial Reserve
427
For example, to compare a ﬁre portfolio, a portfolio consisting of natural
perils (like earthquakes and storms) and some type of motor insurance portfolio, it
is obviously necessary to deal with substantially diﬀerent reserves. By applying this
approach to all business over all lines, the insurance company is able to conduct the
technical solvency process. This also enables the supervisory authority to evaluate
a solvency margin for the liabilities of the company.
Net and Total Premiums
The net premium (at time t) is the expectation E(S(t)) of the total claim amount
S(t). Assuming the independence of the claim size process
X1, X2, X3, . . .
and the claim arrival process
T1, T2, T3, . . . ,
our basic condition in Section 17.1 is valid, namely the independence of the claim
size process X1, X2, X3, . . . and the claim numbers N(t).
We also assume that the claim sizes Xi are identically distributed. Denote
the mean claim size by E(X) again. Recall from (17.8) that the net premium can
be written
E(S(t)) = E(X)E(N(t)).
If the claim number process N(t), t ≥0 is a homogeneous Poisson process
with intensity λ, then
E(S(t)) = E(X)λt.
(17.14)
If N(t), t ≥0 is a P´olya–Lundberg process, then a corresponding formula holds
with λ replaced by ασ.
Assuming that the total premium c(t)—the total amount to be paid by the
policy holders to compensate the future losses—is a multiple of the net premium,
one may write
c(t) = (1 + ρ)E(S(t)),
(17.15)
where ρ is a constant called the safety loading.
The Risk Process
We introduce the risk (surplus, reserve) process in the form
U(t) = s + E(S(t)) + b(t) −S(t),
t ≥0,
(17.16)
where the single terms are
• the initial insurer’s reserve s = U(0) ≥0 for a given portfolio;
• the net premium E(S(t)) up to time t;

428
17. The Impact of Large Claims on Actuarial Decisions
• the safety function (mean surplus) b(t) at time t which is
– the diﬀerence ρE(S(t)) between the total and net premiums (with ρ
denoting the safety loading in (17.15))
– plus the interest income for the accumulated reserve
– minus expenses, taxes and dividends etc.,
• the total claim amount S(t) = SN(t) = 
i≤N(t) Xi up to time t.
The mean of the reserve variable U(t) is
E(U(t)) = s + b(t),
(17.17)
and, therefore, b(t) is the surplus that can be added (in the mean) to the reserve.
It is clear that the safety function b(t) must be nonnegative.
Special Safety Functions
Insurance mathematics primarily concerns safety functions b(t) = c(t) −E(S(t))
which are the diﬀerences between the total premium and the net premium. If c(t)
is given as in (17.15), then
b(t) = ρE(X)E(N(t)) = ρE(X)λt
for a homogeneous Poisson process with intensity λ.
If ρ > 0, then the mean reserve is linearly increasing according to (17.17);
on the other hand, the outstanding result of insurance mathematics is that ruin
occurs with a positive probability (see the next Section 17.5).
Accumulating reserves which are rapidly increasing is perhaps not a desirable
goal. We also suggest to consider safety functions of the form
b(t) = ρE(X)λtβ
(17.18)
for Poisson processes with intensity λ (and related functions for other claim number
processes).
Note that the safety exponent β = 1 is taken in the classical framework.
If β ≤1/2, then ruin occurs with probability one, if the claim sizes satisfy the
conditions of the law of the iterated logarithm.
Example 17.4.1. (Continuation of Example 17.2.1.) We again consider the claim sizes
over the retentation level of 22 million NKr. For the estimated parameters, several paths
of the risk process—with initial reserve s = 250, safety exponent β = 0.3, 1 and safety
loading ρ = 0.1—are generated and displayed in Fig. 17.5.
Let us summarize what has been achieved up to now:

17.4. The Risk Process and the T –Year Initial Reserve
429
Fig. 17.5. Paths of risk processes with β = 0.3 (left) and β = 1 (right).
• we estimated the unknown parameters γ, σ and λ, thereby getting an esti-
mate of the mean claim size E(X) = u + σ/(1 + γ) and of the net premium
λE(X),
• we ﬁxed a safety function ρE(X)λtβ, where the choice of the safety coef-
ﬁcients ρ and β is presumably dictated by the market and other external
conditions.
The knowledge of these constants enables us to simulate risk process paths. This
also yields that any functional parameter of the risk process can be estimated, and
the estimate can be computed by means of the simulation technique. This will be
exempliﬁed in conjunction with the T –year initial reserve for a given q × 100%
ruin probability.
Ruin Times and Ruin Probabilities
within an Inﬁnite und Finite Time Horizon
Ruin occurs at the time when the reserve variable U(t) becomes negative. Let τs
be the ruin time (with the convention that τs = ∞if no ruin occurs) of the risk
process U(t), t ≥0, starting with an initial reserve s = U(0). The ruin time can
be written
τs = inf{t : U(t) < 0}.
Consider the ruin time df
Hs(x) = P{τs ≤x}
which can be a defected df, since non–ruin may occur with a positive probability.
The ultimate ruin probability
ψ(s) = P{τs < ∞}
(17.19)
as a function of the initial reserve s will be dealt with in Section 17.5. This is the
quantity that is studied in ruin theory.

430
17. The Impact of Large Claims on Actuarial Decisions
Subsequently, we examine ruin within a certain ﬁnite time horizon T . The
probability of ruin up to time T is
ψT (s) = Hs(T ) = P{τs ≤T }.
We will calculate such ruin probabilities by simulations9. Apparently, the ultimate
ruin probability ψ(s) is the limit of ψT (s) as T goes to inﬁnity.
One may also deal with early warning times
τs,w = inf{t : U(t) < w},
(17.20)
where w > 0 is an early warning limit for the insurer. Because
U(t) = s + E(S(t)) + b(t) −S(t) < w
if, and only if,
(s −w) + E(S(t)) + b(t) −S(t) < 0,
we have τs,w = τ(s−w), so that this case can be treated within the previous frame-
work.
The T–Year Initial Reserve for a q × 100% Ruin Probability
Choose the initial reserve s such that ruin occurs with a probability q within a time
span of length T , where, e.g., q = 0.01 or q = 0.05 and T = 10 or T = 50 years.
The value s(q, T ) is is called a T –year initial reserve10. Apparently, it depends on
the underlying risk process of the speciﬁc portfolio.
Note that s(q, T ) is a solution to the equation ψT (s) = Hs(T ) = q which can
be written as
H−1
s (q) = T.
(17.21)
Thus, ﬁnd the initial reserve s such that the ruin time qf H−1
s
evaluated at q is
equal to T . Check that s(q, T ) is a T –year threshold according to the q–quantile
criterion (see page 251) for the process S(t) −(E(S(t)) + b(t)).
A closely related concept would be the mean T –year initial reserve as the
solution to Eτs = T, yet the mean seems to be inﬁnite even for ﬁnite ruin time
rvs (for a safety exponent β ≤1/2).
The Initial Risk Contour Plot
By employing the simulation technique, one can plot a sample qf as an estimate
of the ruin time qf H−1
s
for several initial reserves s. This plot is the initial risk
9Also see Vylder, de F. and Goovaerts, M.J. (1988). Recursive calculation of ﬁnite–
time ruin probabilities. Insurance: Mathematics and Economics 7, 1–7.
10Presented at the 35th ASTIN meeting (Cologne, 1996) of the DAV.

17.4. The Risk Process and the T –Year Initial Reserve
431
contour plot. Notice that one gets a sample version of a contour plot of the function
s(q, T ).
Now, by employing an iteration procedure, one can calculate the desired
estimate of the T –year initial reserve for a predetermined q×100% ruin probability.
Knowledge about the accuracy of this procedure can be gained by constructing a
parametric bootstrap conﬁdence interval, see Section 3.2.
Example 17.4.2. (Continuation of Example 17.2.1.) We again consider the claim sizes
over the retentation level of 22 million NKr. For the estimated parameters, we compute
the initial reserve contour lines for s = 100, 200, 300. The risk process is taken for the
safety exponent β = 1 and the safety loading ρ = 0.1.
s = 300
s = 200
s = 100
ruin probability
0.1
0.2
0.3
20
40
Fig. 17.6.
Initial risk con-
tour plot for the initial re-
serves s = 100, 200, 300 .
From the contour plot, one may deduce that, e.g., the 10–year initial reserve for
the 5% ruin probability is about 250. The estimate obtained by an iteration procedure
is ˆs(0.05, 10) = 225.
We also compute an upper bootstrap conﬁdence bound. First, let us recollect the
bootstrap procedure for this special application. The bootstrapping is based on the
Poisson

λ, Gγ,µ,σ

process as introduced on page 248, where λ = 1.7, γ = 0.254, µ = 0
and σ = 11.948. Moreover, the sampling time is T = 10. To obtain a bootstrap sample for
the initial reserve, generate the claim number and claim sizes according to the speciﬁed
Poisson process and estimate the parameters by means of the MLE(GP). Compute the
initial reserve according to the estimated parameters and store this value in a ﬁle. Repeat
this procedure according to the bootstrap sample size. We obtained the 80% and 90%
upper bootstrap conﬁdence bounds 334 and 508. These larger bounds are not surprising
in view of the small sample size of 17 claims (in the mean).
It is apparent that such contour lines can be utilized to simulate a T –year
initial reserve. This can be done by using a StatPascal program11.
11Supplementary details can be found in Reiss, R.–D., Radtke, M. and Thomas, M.

432
17. The Impact of Large Claims on Actuarial Decisions
17.5
Elements of Ruin Theory
Assume that the claim number process is a homogeneous Poisson process with
intensity λ > 0. Assume that the total premium is c(t) = (1 + ρ)E(X)λt, where
ρ > 0
(17.22)
is the safety loading (see (17.15)). We deal with risk processes
U(t) = s + (1 + ρ)E(X)λt −S(t).
Denote the ultimate ruin probability by ψ(s) again, given an initial reserve s.
We refer to the review article by Embrechts and Kl¨uppelberg12 and to [11]
for a detailed account of extremal ruin theory.
Exact Evaluation of Ruin Probabilities
Let F denote the claim size df again. Because 0 < E(X) < ∞, we know from
(2.22) that
 ∞
0 (1−F(y)) dy = E(X), and, hence, h(y) = (1−F(y))/E(X), y ≥0,
is a density of a df, say H. Because
1 −H(x) =
1
EX
 ∞
x
(1 −F(y)) dy
we see that the survivor function of H can be expressed by tail probabilities (cf.
(2.24)).
Under the present conditions, one obtains
1 −ψ(s) = p
∞

k=0
(1 −p)kHk∗(s),
s ≥0
(17.23)
for the probability of non–ruin13 where
p = ρ/(1 + ρ).
Therefore, one must compute a compound geometric df (cf. (17.6)) in order to
evaluate ruin probabilities.
(1997). The T–year initial reserve. Technical Report, Center for Stochastic Processes,
Chapel Hill.
12Embrechts, P. and Kl¨uppelberg, C. (1993). Some aspects of insurance mathematics.
Theory Probab. Appl. 38, 262–295.
13See, e.g., Hipp, C. and Michel, R. (1990). Risikotheorie: Stochastische Modelle und
Statistische Methoden. Verlag Versicherungswirtschaft, Karlsruhe.

17.5. Elements of Ruin Theory
433
Alternatively, the ruin probability function may be written in terms of the
survivor functions of Hk∗. We have
ψ(s) = p
∞

k=1
(1 −p)k(1 −Hk∗(s)),
s ≥0.
(17.24)
Deduce that ψ(s) tends to zero as s →∞.
We compute the auxiliary df H for two special cases. In addition, an explicit
representation of the ruin probability function ψ can be given for exponential claim
sizes (Erlang model).
• (Exponential Claim Sizes.) Assume that the claim sizes are exponentially
distributed. More precisely, let F(x) = 1 −e−x/σ, x ≥0. Hence, EX = σ.
Then, H(x) = F(x). From (17.7) and (17.23), one obtains
ψ(s) =
1
1 + ρ exp

−
ρs
(1 + ρ)E(X)

,
s ≥0.
(17.25)
Thus, the ruin probabilities decrease with an exponential rate to zero as the
initial capital s goes to inﬁnity.
• (Pareto Claim Sizes.) If X is a Pareto random variable with df W1,α,µ,σ with
lower endpoint µ + σ > 0 and α > 1, then E(X) = µ + σα/(α −1) and
1 −H(x) =
σ
E(X)(α −1)
x −µ
σ
−(α−1)
(17.26)
for x ≥µ + σ.
Lower Bounds of Ruin Probabilities
A simple lower bound of ψ(s) can be easily deduced from (17.24). Because Hk∗≤
H(s), one gets
ψ(s) ≥(1 −H(s))/(1 + ρ).
(17.27)
• (Exponential Claim Sizes.) Again, let F(x) = 1 −e−x/σ, x ≥0. We have
ψ(s) ≥exp(−E(X)/s)/(1 + ρ).
We see that the simple lower bound is close to the exact one in (17.25).
• (Pareto Claim Sizes.) From (17.26), we obtain the lower bound
ψ(s) ≥
σ
E(X)(α −1)(1 + ρ)
s −µ
σ
−(α−1)
(17.28)
for s ≥µ + σ. Thus, in contrast to the preceding examples, one gets an
arithmetic rate of the lower bound as s →∞.
We refer to the aforementioned book by Hipp and Michel for more reﬁned
lower bounds.

434
17. The Impact of Large Claims on Actuarial Decisions
Upper Bounds: The Lundberg Inequality
An upper bound for the ultimate ruin probability is provided by the Lundberg
inequality. Assume that R > 0 satisﬁes the equation
 ∞
0
eRy(1 −F(y)) dy = (1 + ρ)E(X).
(17.29)
Such a constant R is called an adjustment coeﬃcient. Then, the inequality
ψ(s) ≤exp(−Rs),
s ≥0,
(17.30)
holds.
• (Exponential Claim Sizes With Mean µ > 0.) We have
R =
ρ
(1 + ρ)E(X)
(17.31)
and, hence, the upper bound exceeds the exact value in (17.25) by a factor
1 + ρ.
• (Pareto Claim Sizes.) For Pareto claim size dfs the Lundberg inequality is
not satisﬁed according to (17.28). Notice that the left–hand side of (17.29)
is equal to inﬁnity.
Ruin Probabilities in the Pareto Case Revisited
Direct calculations lead to an upper bound of the ultimate ruin probability ψ(s)
for Pareto claim size dfs W1,α,µ,σ with α > 1. We have14
ψ(s) ≈
σ
E(X)(α −1)ρ
 s
σ
−(α−1)
,
s →∞.
(17.32)
We see that the lower bound in (17.28) is exact, except of a factor (ρ + 1)/ρ.
17.6
Credibility (Bayesian) Estimation
of the Net Premium
As in Section 17.1 we deal with an excess–of–loss (XL) reinsurance treaty. The
primary aim is to estimate the net premium which is the expectation of the total
14 Bahr von, B. (1975). Asymptotic ruin probabilities when exponential moments do
not exist. Scand. Actuarial J., 6–10.
In greater generality dealt with in Embrechts, P. and Veraverbeke, N. (1982). Esti-
mates for the probability of ruin with special emphasis on the probability of large claims.
Insurance: Mathematics and Economics 1, 55–72.

17.6. Credibility (Bayesian) Estimation of the Net Premium
435
claim amount
SN =

i≤N
(Xi −u)
for the next period, where N is the random number of claims occurring in the
period, and the Xi are the random claim exceedances over the priority u.
Throughout, we assume that the exceedance times and the exceedances can
be jointly modeled by a Poisson(λ, F) process. Thus, the exceedance times are
described by a homogeneous Poisson process with intensity λ > 0, and the ex-
ceedances have the common df F. The net premium is
E(SN) = λ(mF −u)
(17.33)
with mF denoting the mean of F. The estimate of the net premium is based on the
exceedance times and exceedances of the past T years. In contrast to Section 17.1,
we merely deal with the estimation of the net premium within certain Poisson–
Pareto models.
Our attention is focused on the Bayes estimation—that is, the exact credibil-
ity estimation—of the net premium. A justiﬁcation for a restriction to the exact
credibility estimation was formulated by S.A. Klugman [35], page 64: “Most of the
diﬃculties ... are due to actuaries having spent the past half–century seeking lin-
ear solutions to the estimation problems. At a time when the cost of computation
was high this was a valuable endeavor. The logical solution is to drop the linear
approximation and seek the true Bayesian solution to the problem. It will have
the smallest mse and provides a variety of other beneﬁcial properties”.
Recall that the mean of a Pareto distribution is equal to inﬁnity if the shape
parameter α is smaller or equal to 1, cf. (1.44). This yields that the support
of a prior distribution must be restricted to parameters α > 1, when the Bayes
estimation of the mean is dealt with. This is the reason that we introduce truncated
gamma distributions in the subsequent lines.
Truncated Gamma Distributions
A gamma distribution, truncated left of q ≥0, with shape and reciprocal scale
parameters s > 0 and d > 0, cf. (3.42), serves as a prior. The pertaining df and
density are denoted by Hs,d,q and hs,d,q. One gets gamma distributions if q = 0.
The gamma density hs,d,q, truncated left of q ≥0, is given by
hs,d,q(α) = hs,d(α)/

1 −Hs,d(q)

,
α > q.
(17.34)
It turns out that truncated gamma densities are conjugate priors for our models.
That is, the posterior densities are of the same type.
Straightforward calculations yield that such a truncated gamma distribution
has the mean
Ms,d,q =

s(1 −Hs+1,d(q))

d(1 −Hs,d(q))

(17.35)

436
17. The Impact of Large Claims on Actuarial Decisions
and the variance
Vs,d,q =
s

(s + 1)(1 −Hs+2,d(q))(1 −Hs,d(q)) −s(1 −Hs+1,d(q))2
d2(1 −Hs,d(q))2
.
(17.36)
These formulas are useful
• to specify a gamma prior with predetermined moments,
• to compute the numerical value of a Bayes estimate.
Estimation of the Net Premium for Poisson Processes
with Pareto GP1(u, µ = 0) Marks
For the modeling of exceedances over the priority u we use dfs within the restricted
Pareto GP1(u, µ = 0) model which are given by
W1,α,0,u(y) = 1 −(y/u)−α,
y ≥u,
(17.37)
where α > 0. These dfs are used for the modeling of exceedances over a priority u.
The pertaining excesses have the mean u/(α −1) and, therefore, the net
premium (cf. (17.33)) is given by
m(λ, α) =
λu
α −1,
α > 1.
(17.38)
This functional can be written in a product form as given in (9.16). We further ap-
ply the concept as developed in Section 9.3. The prior is p(λ, α) = hr,c(λ)hs,d,q(α)
with q > 1. The posterior with respect to the parameter α is p2(α|y) = hs′,d′,q(α)
with s′ and d′ as in (5.6).
One obtains the Bayes estimate
m({ti, yi}) = λ∗
km∗
k,q(y)
(17.39)
of the net premium m(λ, α) with λ∗
k as in (3.53) and
m∗
k,q(y) =
 ∞
q
u
α −1hs′,d′,q(α) dα.
(17.40)
The integral must be numerically evaluated. Notice that m∗
k,q(y) is a Bayes esti-
mate of the mean of excesses over u.
Estimation of the Net Premium for Poisson Processes
with Pareto GP1(u) Marks
In contrast to the preceding lines, we assume that the exceedances over the priority
u are distributed according to a Pareto df
Fα,η(y) = 1 −

1 + y −u
ηu
−α
,
y ≥u,
(17.41)

17.6. Credibility (Bayesian) Estimation of the Net Premium
437
where α, η > 0, cf. (5.9). Thus, we deal with the full Pareto model in the (α, η)–
parameterization. One obtains the restricted GP1(u, µ = 0) model if η = 1.
The excesses have the mean uη/(α −1) and, therefore, the net premium is
given by
m(λ, α, η) = λuη
α −1,
α > 1.
(17.42)
The prior is p(λ, α, η) = hr,c(λ)hs,d,q(α)f(η) with q > 1, and f is another probabil-
ity density on the positive half–line. The posterior with respect to the parameter
vector (α, η) is p2(α, η|y) = hs′,d′(η),q(α) ˜f(η) with s′, d′(η) and ˜f(η) as in (5.13)
and (5.14).
One obtains the Bayes estimate
	m({ti, yi}) = λ∗
km∗∗
k,q(y)
(17.43)
of the net premium m(λ, α, η) with λ∗
k as in (3.53) and
m∗∗
k,q(y) =
 ∞
0
η
  ∞
q
u
α −1hs′,d′(η),q(α) dα

˜f(η) dη.
(17.44)
Estimation of Parameters
We also shortly note the estimates of the parameters α and η if a prior hs,d,q is
taken for α.
• The restricted Pareto model GP1(u, µ = 0): from (17.35) deduce that the
Bayes estimate of α > q ≥0 is
α∗
k,q(y) = α∗
k(y)1 −Hs′+1,d′(q)
1 −Hs′,d′(q) ,
(17.45)
where α∗
k(y) is the Bayes estimate in the case of the gamma prior (with
q = 0; see (5.7)). Notice that α∗
k,0(y) = α∗
k(y).
• The full Pareto model GP1∗(u): extending the formulas in (5.15) and (5.16),
one gets
α∗
k,q(y) =

s′
d′(η)
(1 −Hs′+1,d′(η)(q))
(1 −Hs′,d′(η)(q))
˜f(η) dη,
q ≥0,
(17.46)
and, independently of q, η∗
k,q(y) = η∗
k(y) as Bayes estimates of α > q ≥0
and η > 0.

Part VI
Topics in
Material and Life Sciences

Chapter 18
Material Sciences
In Section 18.1 the blocks method (in other words, annual maxima or Gumbel
method) is applied to corrosion engineering. We are particularly interested in the
service life of items exposed to corrosion. Our primary sources are the book by
Kowaka et al., [37] and a review article by T. Shibata1.
Section 18.2 concerns a stereological problem in conjunction with extreme
value analysis. We are interested in extremes of 3–dimensional quantities of which
we merely observe a 2-dimensional image.
18.1
Extremal Corrosion Engineering
In corrosion analysis the role of time periods is played by several units such as
steel tanks. One can also deal with a single area which is divided into T subareas
of the same size. This corresponds to dividing a given period into T blocks.
For each of the units or subareas, which are exposed to corrosion, we observe
the maximum pit depth (or, e.g., a measurement concerning stress corrosion crack-
ing), and thus get a data set y1, . . . , yT of maxima. In the statistical modeling we
get random variables Y1, . . . , YT which are distributed according to a Gumbel df
G0,µ,σ or, in general, according to an extreme value df Gγ,µ,σ.
The uniﬁed extreme value model was introduced by Laycock et al.2 in the
ﬁeld of corrosion analysis.
Assume that there are T units or an area of size T exposed to corrosion. Our
ﬁnal goal is to determine the T –unit service life which is the period of x(T, l) years
such that the T –unit depth u(T ) for x(T, l) years is equal to an allowable margin
of thinning l.
1Shibata, T. Application of extreme value statistics to corrosion. In [15], Vol. II, 327–
336.
2Laycock, P.J., Cottis, R.A. and Scarf, P.A. (1990). Extrapolation of extreme pit
depths in space and time. J. Electrochem. Soc. 137, 64–69.

442
18. Material Sciences
Recall from page 6 that the T –unit pit depth u(T ) for a given time period of
length x is the level once exceeded on the average.
Estimation of a T–Unit Depth
Let Yi be the maximum pit depth of the ith unit within the given period of x
years. The T –unit depth u(T ) is the solution to the equation
E
 
i≤T
I(Yi ≥u)

= 1
which are the quantiles
u(T ) = G−1
0,µ,σ(1 −1/T )
or
u(T ) = G−1
γ,µ,σ(1 −1/T )
corresponding to (1.13).
It is evident that the T –unit depth also depends on the service time x. Occa-
sionally, we also write u(T, x) in place of u(T ). Necessarily, u(T, x) is an increasing
function in x.
The value u(T ) is evaluated by estimating the parameters µ, σ or γ, µ and σ
as it is done in Chapter 4 within the Gumbel EV0 model or the full extreme value
model EV.
Example 18.1.1. (Maximum Pit Depths of Steel Tanks.) The maximum pit depths (in
mm) of steel tanks exposed to cyclic dry/wet (by sea water) conditions are measured
after 4 years of service (stored in gu–pit04.dat). This data set is taken from Kowaka et
al., [37], Table 18.13.
These authors estimate a Gumbel distribution with location and scale parameters
µ = 1.03 and σ = 0.37 by visual inspection of a Gumbel probability paper. The 100–unit
depth u(100) is estimated as 2.76mm.
Table 18.1. Maximum pit depth measured in mm after 4 years of service.
0.3
0.7
0.8
0.9
1.1
1.2
1.3
1.4
1.5
1.6
0.7
0.7
0.8
1.0
1.1
1.2
1.3
1.4
1.6
2.0
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.6
2.1
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.6
2.3
0.7
0.8
0.9
1.0
1.1
1.3
1.4
1.5
1.6
2.5
3With reference given to Tsuge, H. (1983). Archive of 51st Corrosion Symposium,
JSCE, page 16.

18.1. Extremal Corrosion Engineering
443
This result is conﬁrmed by the MLE(EV0). One gets a Gumbel distribution with location
and scale parameters µ = 1.00 and σ = 0.36. The adequacy is also conﬁrmed by the
MLE(EV) and other diagnostic tools. The estimate for the 100–unit depth u(100) is
2.65mm.
Varying the Service Time
Subsequently, we compute the T –unit depths for diﬀerent years of service. We
write u(T, x) in place of u(T ) to emphasize the dependence of the T –unit depth
from the service time x.
In addition to the data analyzed in Example 18.1.1 for a service time of x = 4
years, we also deal with data for service times x = 6, 12.
Example 18.1.2. (Continuation of Example 18.1.1 About Maximum Pit Depths of Steel
Tanks.) We also analyze a sample of 32 maximum pit depths after a service time of 6
years (stored in gu–pit06.dat) and a sample of 30 maximum pit depths after a service
time of 12 years (stored in gu–pit12.dat). These data sets are also taken from Kowaka et
al. [37]. We compare the estimates obtained by the MLE(EV0) with those in [37].
In the subsequent table we compare the estimated Gumbel parameters µ, σ and the
pertaining 100–unit depths u(100) in [37] with the values obtained by the MLE(EV0). The
corresponding parameters of the MLE(EV) are added. Recall that Gumbel distributions
possess the shape parameter γ = 0.
Table 18.2. Estimated EV parameters and 100–unit pit depths.
Estimated EV0/EV parameters and 100–unit depths
Kowaka et al.
MLE(EV0)
MLE(EV)
x
4
6
12
4
6
12
4
6
12
γ(x)
0
0
0
0
0
0
−0.07
−0.07
0.38
µ(x)
1.03
1.16
1.43
1.00
1.17
1.58
1.01
1.19
1.40
σ(x)
0.37
0.49
1.23
0.36
0.41
0.94
0.36
0.42
0.79
u(100, x)
2.76
3.4
7.1
2.65
3.1
6.0
2.43
2.82
11.2
One recognizes a greater diﬀerence of the estimated parameters in the Gumbel
model for the service time of 12 years. Within the EV–model one gets a shape param-
eter γ = 0.38, which indicates a heavier upper tail of the distribution. Thus, the shape
parameter γ signiﬁcantly deviates from γ = 0 which is the shape parameter for Gumbel
distributions.
Whereas the Gumbel distribution—with the larger scale parameter σ = 1.23 of

444
18. Material Sciences
Kowaka et al.—catches the heavier upper tail it does not ﬁt to the lower data. Thus,
these authors locally ﬁtted a Gumbel distribution to the upper extremes of the maxima
y1, . . . , yT .
The Service Life Criterion
Based on diﬀerent T –unit depths u(T, xk) we select the service life x(T, l) which
is the number of years x such that the T –unit depth u(T, x) is equal to an allow-
able margin of thinning l. Thus, given T units exposed to corrosion there is one
maximum pit depths which will exceed the depth of the amount l within x(T, l)
years on the average.
Example 18.1.3. (Continuation of the Preceding Examples.) The margin of thinning is
taken equal to l = 5mm in the preceding example. By ﬁtting a linear and a quadratic
regression line to the 100–unit depths u(100, x) based on the MLE(EV) estimates for the
three diﬀerent years x = 4, 6, 12, one obtains estimates of ˆx(100, 5) = 6.9 and ˆx(100, 5) =
8.5 years for the service life compared to ˆx(100, 5) = 8.2 years which was estimated by
Kawata et al.
100-unit dept
year
4
5
6
7
8
9
10
11
12
0.0
5
10
Fig. 18.1. 100–unit depths plotted
against years, ﬁtted linear (dashed)
and quadratic (solid) least squares
lines, constant (dotted) line with
value equal to the margin of thin-
ning l = 5mm.
We merely dealt with the blocks method, yet one could also apply the peaks–
over–threshold (pot) method. Then, one should take care of a possible clustering
of larger pit depths. This question is discussed by Scarf and Laycock4. A global
modeling of pit depth by means of log–normal distributions may be found in a
paper by Komukai and Kasahara5.
4Scarf, P.A. and Laycock, P.J. (1994). Applications of extreme value theory in corro-
sion engineering. In: [15], Vol. II, 313–320.
5Komukai, S. and Kasahara, K. (1994). On the requirements for a reasonable extreme
value prediction of maximum pits on hot–water–supply copper tubing. In: [15], Vol. II,
321–326.

18.2 Stereology of Extremes
445
18.2
Stereology of Extremes
co–authored by E. Kaufmann6
This section is devoted to the well–known Wicksell7 “corpuscle problem” which
concerns the size and number of inclusions in a given material (such as particles
of oxides in a piece of steel). If we assume that the particles are spheres, then the
size is determined by the radius R which is regarded as a random variable. Besides
the df F of R one is interested in the number of particles.
Information about the spheres can be gained by those which become visible
on the surface area. On a plane surface one observes circles which are determined
by the radius S with a df which is denoted by W(F). This df is addressed as the
Wicksell transformation of F. Drawing conclusions about the initial 3–dimensional
objects from the observable 2–dimensional ones is a genuine problem in stereology.
Our attention will be focused on the modeling of the upper tails of F and W(F).
The Wicksell Transformation
At the beginning we compute the Wicksel transformation W(F) as a conditional
df of the radius variable S and provide diﬀerent representations.
A sphere in the Euclidean space is determined by the coordinates x, y, z of
the center and the radius r. We assume that 0 ≤z ≤T . The sphere intersects the
x, y–plane if r > z. In that case one observes a circle in the x, y–plane with radius
s =
√
r2 −z2.
(x,y,0)
(x,y,z)
r
s
Fig. 18.2. Computation of circle radius s.
Within the stochastical formulation we assume that the random radius R > 0
and the random coordinate Z are independent, R has the df F, and Z is uniformly
distributed on [0, T ]. Thus, S =
√
R2 −Z2 is the random radius of the circle in
the x, y–plane under the condition that S2 = R2 −Z2 > 0; that is, there is an
intersection with the x, y–plane.
6University of Siegen.
7Wicksell, S.D. (1925). The corpuscle problem I. Biometrika 17, 84–99, and Wicksell,
S.D. (1926). The corpuscle problem II. Biometrika 18, 152–172.

446
18. Material Sciences
We compute the survivor probability P{S > s} for s > 0 and the conditional
survivor function
WT (F)(s) = P(S > s|S > 0).
Applying results for conditional distributions, namely (8.4) and (8.13), one
gets
P{S > s}
=

P

R >
'
s2 + z2
dL(Z)(z)
=
1
T
 T
0
F
'
s2 + z2
dz,
where F is again the survivor function of F. Therefore,
WT (F)(s) =
 T
0 F
√
s2 + z2
dz
 T
0 F(z) dz
.
(18.1)
If
 ∞
0
F(z) dz < ∞, then the “tail probability” formula (2.24) yields
WT (F)(s)
→T →∞
 ∞
0
F
√
s2 + z2
dz
 ∞
0
z dF(z)
=:
H(s)
H(0) = W(F)(s),
(18.2)
where the df W(F) is the Wicksell transformation of F.
Alternative Representations of the Wicksell Transformation
We add alternative representations of the numerator H(s). Applying the substitu-
tion rule and, in a second step, writing F(z) as an integral with respect to F and
interchanging the order of integration one gets
H(s)
=
 ∞
s

z2 −s2−1/2zF(z) dz
(18.3)
=
 ∞
s
'
r2 −s2 dF(r).
(18.4)
With the help of (18.4) verify that H(s) =
 ∞
s
h(z) dz with
h(s) = s
 ∞
s

r2 −s2−1/2 dF(r).
(18.5)
Hence, h is a density pertaining to H. This also yields that w(F) := h/H(0) is a
density of the Wicksell transformation W(F).

18.2 Stereology of Extremes
447
Poisson Process Representations
In the preceding lines we merely considered the radii of the spheres and circles.
Next, we include the centers and the number of occurances. These quantities are
jointly described by a marked Poisson process. Thus, our 2–step approach resem-
bles that employed for exceedances, where we ﬁrst introduced the exceedance df
and, later on, the Poisson process of exceedances.
The centers of the spheres are regarded as points of a homogeneous Poisson
process with intensity c > 0. This is a Poisson process with intensity measure cλ3
(see the end of Section 9.5), where λ3 denotes the 3–dimensional Lebesgue measure.
Secondly, let F be the df of the sphere radii which are regarded as marks. Then, a
joint statistical model for the inclusions—for centers as well as radii—is given by
a Poisson(cλ3, F) process with unknown intensity c and unknown df F.
The intersections with the x, y–plane, that is, the centers and radii of the cir-
cles, constitute a marked homogeneous Poisson(2cH(0)λ2, W(F)) process, where
W(F) is the Wicksell transformation of F and H(0) is the constant in (18.2). The
new marked Poisson process is derived from the initial one by
• a truncation procedure, because only spheres contribute to the new process
that intersect the plane,
• a transformation, due to the fact that circle coordinates instead of the orig-
inal sphere coordinates are observed.
In practice, the surface area is a subset A in the x, y–plane with ﬁnite measure
λ2(A) < ∞. In addition, to analyze the size distribution of the circles, respectively,
of the spheres, one merely has to consider the marginal Poisson process of circle
radii. Under the postulated homogeneity, the circle centers provide no information
about the radius df.
The intensity measure ν of the marginal Poisson process pertaining to the
circle radii is given by the mean value representation
ν(s, ∞)
=
2cλ2(A)H(0)W(F)(s)
=
2cλ2(A)
 ∞
0
F(
'
s2 + z2) dz,
(18.6)
where the latter equation follows from (18.2).
Hence, the circle radii may be treated as a random number N of independent
and identically distributed random variables S1, S2, . . . with common df W(F),
where N is a Poisson random variable with parameter 2cλ2(A)H(0) which is in-
dependent of the Si.
The Art of Stereological Extreme Value Analysis
In the sequel, we study the relationship between the tails of radius distributions
of spheres and circles. The modeling of the upper tail of the observable circle radii

448
18. Material Sciences
by means of GP dfs can regarded as business as usual. Our primary aim is to ﬁnd
a theoretical justiﬁcation for a GP modeling for the sphere radii. This will be done
by comparing
• max/pot–domains of attractions,
• the validity of certain von Mises conditions.
In addition, a counterexample shows that a GP upper tail for the circle radii does
not necessarily justify such a modeling for the sphere radii.
Max–Domains of Attraction
We impose conditions on F which imply that the Wicksell transformation W(F)
belongs to the max–domain of attraction of an EV df G; in short W(F) ∈D(G).
First we provide some details and a proof in the special case of Fr´echet dfs
G1,α. Assume that F ∈D(G1,α). From Section 1.3 we know that this condition
holds if, and only if, the survivor function F of the sphere radius has a regularly
varying tail with index −α < 0; that is, for each t > 0 we have F(st)/F(s) →t−α
as s →∞.
By substitution and employing the letter property, one gets
 ∞
0
F(
'
s2 + z2) dz
=
s
 ∞
0
F(s
'
1 + y2) dy
∼
sF(s)
 ∞
0
(1 + y2)−α/2 dy
as s →∞, if α > 1 (otherwise, the latter integral is inﬁnite). Combining this with
(18.2) one gets that W(F) is a regularly varying function with index 1−α. Hence,
W(F) ∈D(G1,α−1), if α > 1.
Such questions were treated in the article by Dress and Reiss, cited on page
33, in greater generality, with the preceding result as a special case. We have
W(F) ∈D(Gi,β(α)), if F ∈D(Gi,α), for i = 1, 2, with the shape parameters β(α)
given by
β(α) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
α −1
i = 1, α > 1,
if
α −1/2
i = 2, α < 0.
(18.7)
In the Gumbel case we have W(F) ∈D(G0), if F ∈D(G0). It is remarkable that
we do not get any result for W(F) with −1/2 ≤β(α) < 0.

18.2 Stereology of Extremes
449
Von Mises Conditions
If the df F has a density f which is positive in a left neighborhood of ω(F), then
the following von Mises conditions are suﬃcient for F to belong to the domain of
attraction of an EV df Gi,α. We assume that
M(0) :
 ω(F )
t
(1 −F(u)) du < ∞for some t < ω(F),
lim
t↑ω(F ) f(t)
 ω(F )
t
(1 −F(u)) du/(1 −F(t))2 = 1;
M(1, α) :
ω(F) = ∞,
lim
t↑∞tf(t)/(1 −F(t)) = α;
M(2, α) :
ω(F) < ∞,
lim
t↑ω(F )((ω(F) −t)f(t)/(1 −F(t)) = −α.
We also write M(0, α) instead of M(0). For a diﬀerent von Mises condition, which
involves the 1st derivative of the density f, we refer to (2.32).
If F satisﬁes condition M(i, α), then F ∈D(Gi,α). The preceding conditions
also entail the convergence in terms of the variational and Hellinger distances, see
[42]. Under the restrictions on α given in (18.7), it was proved by Drees and Reiss
that W(F) fulﬁlls M(i, β(α)), if F fulﬁlls M(i, α).
Most important, from our viewpoint, is the following converse result: if β(α) <
−1/2 or β(α) > 0 and W(F) fulﬁlls M(i, β(α)), then F is in the domain of attrac-
tion of Gi,α. Therefore, if we come to the conclusion that the statistical modeling
of circle radii by means of a GP df with the shape parameter β(α) < −1/2 or
β(α) > 0 is appropriate, then we may assume that the upper tail of F is of GP
type with shape parameter α.
The preceding results were extended to ellipsoidal particles by Hlubinka8,
where the particles (except the positions) are determined by a bivariate random
vector.
A Counterexample
We focus our attention on the case i = 2 and β(α) = −1/2. We provide an
example of a Wicksell transformation W(F), where W(F) fulﬁlls the von Mises
condition M(2, −1/2), yet F is not in the max–domain of attraction of an EV
df. More precisely, we show that W(F) satisﬁes a Weiss condition (belongs to a
δ-neighborhood of a GP df), yet F is a df with the total mass concentrated in a
single point (and, therefore, does not belong to the domain of attraction of an EV
8Hlubinka, D. (2003). Stereology of extremes; shape factor of spheroids. Extremes 6,
5–24.

450
18. Material Sciences
df). Therefore, it is not self–evident to employ a GP modeling for the upper tail
of F, if such a modeling is accepted for W(F).
If the size of the inclusions is ﬁx, say s0 > 0, then one can deduce the
density of the Wicksell transformation W(F) from (18.5). Denote again by w(F)
the density of W(F). We have
w(F)(s)
=
h(s)
H(0)
=
s
s0
'
s2
0 −s2
=
w2,−1/2,µ,σ(s)

1 + O

W2,−1/2,µ,σ(s)
2
(18.8)
for 0 < s < s0, where µ = s0 and σ = s0/2.
Wicksell Transformation and Exceedance Dfs
There is a nice relationship
W(F)[u] = W(F [u])[u]
(18.9)
between exceedance dfs of F and the pertaining Wicksell transformations W(F),
due to Anderson and Coles9. The proof is straightforward; we have
W(F)[u](s)
=
1 −H(s)
H(u)
=
1 −
 ∞
s
√
r2 −s2 dF(r)
 ∞
u
√
r2 −u2 dF(r)
=
1 −
 ∞
s
√
r2 −s2 dF [u](r)
 ∞
u
√
r2 −u2 dF [u](r)
=
W(F [u])[u](s),
s ≥u.
This result was applied by Anderson and Coles to estimate the upper tail of the
initial df F based on exceedances of circle radii (see below).
Statistical Inference
In the article by Drees and Reiss, the estimation of the initial df F was treated
in several steps. Based on the exceedances of circle radii over some threshold, a
GP density with shape parameter ˆβ is ﬁtted to the upper tail of the circle radius
density w(F). This tail estimator is combined with a nonparametric estimator of
the central part of w(F) by the piecing together method. In a second step, an
estimate ˆf of the sphere radius density f is computed by applying a smoothed
9Anderson, C.W. and Coles, S.G. (2002). The largest inclusions in a piece of steel.
Extremes 5, 237–252.

18.2 Stereology of Extremes
451
expectation maximization (EMS) algorithm. Notice that the EMS algorithm is
one of the common methods within the framework of ill–posed inverse problems
such as the Wicksell corpuscle problem. Finally, the upper tail of the estimated
density ˆf is replaced by a GP density with shape parameter ˆα as established in
(18.7). Likewise, one could generally investigate extremes in inverse problems.
An alternative estimation procedure is employed by Anderson and Coles.
Again one assumes that a GP modeling for the upper tail of F is valid. In the
subsequent lines, we make use of the γ–parametrization of GP dfs because then
the left endpoint of the support is just the location parameter. Replacing F [u] by
a GP df Wγ,u,σ on the right–hand side of (18.9) one gets the truncated Wick-
sell transformation W(Wγ,u,σ)[u]. Notice that this df is the exceedance df for the
pertaining circle radii above the threshold u. Moreover, this df has the density
w(Wγ,u,σ)[u](s) = s
 ∞
s (r2 −s2)−1/2 dWγ,u,σ(r)
 ∞
u
√
r2 −u2 dWγ,u,σ(r)
,
s ≥u.
Now estimate the parameters γ and σ by applying the ML method to the circle
radii exceedances.
Additional Literature on Maximum Inclusion Sizes
Applied work about the “maximum size of inclusions” started with articles by
Murakami and Usuki10 and Murakami et al.11. In that context, we also refer to a
series of articles by Shi, Atkinson, Sellars and Anderson; see, e.g., the article by
J.R. Yates et al.12 and the references therein.
Further results about stereological extremes may be found in articles by Taka-
hashi and Sibuya13. Notable is also a recent, theoretically oriented review about
stereological particle analysis by K¨otzer14 which includes a longer list of references
to the stereological literature.
10Murakami, Y. and Usuki, H. (1989). Quantitative evaluation of eﬀects of non–metallic
inclusions on fatigue strength of high strength steel II: fatigue limit evaluation based on
statistics for extreme values of inclusion size. Int. J. Fatigue 11, 299–307.
11Murakami, Y., Uemura, Y. and Kawakami, K. (1989). Some problems in the applica-
tion of statistics extreme values to the estimation of the maximum size of non–metallic
inclusions in metals. Transactions Japan Soc. Mechan. Engineering 55, 58–62.
12Yates, J.R., Shi, G., Atkinson, H.V., Sellars, C.M. and Anderson, C.W. (2002). Fa-
tigue tolerant design of steel components based on the size of large inclusions. Fatigue
Fract. Engng. Mater. Struct. 25, 667–676.
13Takahashi, R. and Sibuya, M. (1996). The maximum size of the planar sections of
random sheres and its application to metallurgy. Ann. Inst. Statist. Math. 48, 127–144,
and Takahashi, R. and Sibuya, M. (1998). Prediction of the maximum size in Wicksell’s
corpuscle problem. Ann. Inst. Statist. Math. 50, 361–377.
14K¨otzer, S. (2006). Geometric identities in stereological particle analysis. Image Anal.
Stereol. 25, 63–74.

Chapter 19
Life Science
co–authored by E. Kaufmann1
In Section 19.1 we deal with the question whether human life spans are limited or
unlimited. The celebrated Gompertz law will be central for our considerations. We
particularly apply the results of Section 6.5, concerning penultimate distributions,
to this question. Section 19.2 concerns the prediction of life tables by adopting a
regression approach.
19.1
About the Longevity of Humans
We discuss the question whether the right endpoint of the life span df F of a pop-
ulation is inﬁnite, in other words, that the life span is unlimited. This is primarily
done within the framework of extreme value analysis, yet we also ﬁt Gompertz
and logistic distributions to extreme life spans.
Gompertz and Logistic Laws
The Gompertz distribution is the classical life span distribution. The Gompertz law
(called Gompertz–Makeham formula by Gumbel) postulates a hazard (mortality)
rate h(x) = aebx.
Recall from page 54 that the converse Gumbel df 	G0,µ,σ, called Gompertz df,
is the only df which satisﬁes the Gompertz law with µ = (log(b/a)/b and σ = 1/b.
A theoretical explanation for the adequacy of the Gompertz distribution, based
on the cellular aging phenomena of limited replicability and deceleration of the
1University of Siegen.

454
19. Life Science
process of mitosis, may be found in an article by Abernethy2.
Example 19.1.1. (Extreme Life Spans.) Our analysis concerns extreme life spans of
women born before and around the year 1900 and later living in West Germany.
The given data are the ages of female persons at death in West–Germany in the
year 19933. None of the persons died at an age older than 111 years. The analysis is
based on the ages at death of 90 or older.
Table 19.1. Frequencies of life spans (in years).
life span
90
91
92
93
94
95
96
97
98
99
100
frequency
12079
10273
8349
6449
5221
3871
2880
1987
1383
940
579
life span
101
102
103
104
105
106
107
108
109
110
111
frequency
340
207
95
63
36
16
9
4
1
1
1
Thus, our analysis is based on a subgroup of our target population described at
the beginning. Those who died before 1993 and were alive after 1993 are not included
in the study. Yet, the extreme life spans within the subgroup are governed by the same
distribution as those of the entire population under the condition that the birth rate
was nearly homogeneous during the relevant period. This idea can be made rigorous
by a thinning argument within the point process framework, see [43], pages 68 and 69.
For that purpose, consider uniformly distributed dates of birth Vi over the relevant time
period and add life spans Xi. The thinning random variable is deﬁned by Ui = I(1993 ≤
Vi + Xi < 1994).
A Gompertz density with location and scale parameters µ = 83.0 and σ = 9.8 ﬁts
well to the histogram of the life span data above 95 years, see Fig. 19.1.
If the Gompertz df is accepted, then life span is unlimited. This is in agree-
ment with recent experiments which showed (see, e.g., Carey et al.4 about Mediter-
ranean fruit ﬂies and the article by Kolata5) that the average life span and the
oldest ages strongly depend on the living conditions.
The insight gained from experiments and from a series of data sets may be
summarized by the statement that “the studies are consistently failing to show
any evidence for a pre–programmed limit to human life span,” as nicely expressed
2Abernethy, J. (1998). Gompertzian mortality originates in the winding–down of the
mitotic clock. J. Theoretical Biology 192, 419–435.
3Stored in the ﬁle um–lspge.dat (communicated by G.R. Heer, Federal Statistical
Oﬃce).
4Carey, J.R., Liedo, P., Orozco, D. and Vaupel, J.W. (1992). Slowing of mortality rates
at older ages in large medﬂy cohorts. Science 258, 457–461.
5Kolata, G., New views on life spans alter forecasts on elderly. The New York Times,
Nov. 16, 1992.

19.1. About the Longevity of Humans
455
by J.W. Vaupel (cited by Kolata).
Thatcher et al.6 suggest to ﬁt a logistic distribution to ages at death between
80 and 120. Such a modeling is very natural in view of the change in the population
which can be observed around 95 years, and the fact that the logistic distribution is
a certain mixture of Gompertz distributions with respect to a gamma distribution,
cf. (4.11). The hazard function of a (converse) logistic df is given by
h(x) = aebx/(1 + αebx)
with b = 1/σ, α = β and a = rβ/σ.
It was observed by Kannisto7 and Himes et al.8 that there is already a close
ﬁt to data in the special case where a = α which yields r = σ =: s. These
distributions can be represented by the dfs
Fs,µ(x) = 1 −(1 + exp((x −µ)/s))−s
which have the densities
fs,µ(x) = exp((x −µ)/s)/(1 + exp((x −µ)/s))1+s,
where µ is a location parameter. In Fig. 19.1 (right) we give an illustration based
on the same data as in Example 19.1.1.
life spans in years
90
100
110
0.0
0.01
0.02
0.03
0.04
0.05
life spans in years
90
100
110
0.0
0.01
0.02
0.03
0.04
0.05
Fig. 19.1. Histogram of life span data and (left) Gompertz density with parameters
µ = 83, σ = 9.8, (right) logistic density fs,µ with parameters s = 7.25, µ = 98.6.
6Thatcher, A.R., Kannisto, V. and Vaupel, J.W.. (1998). The Force of Mortality at
Ages 80 to 120. Odense University Press.
7Kannisto, V. (1992). Presentation at a workshop on old age mortality at Odense
University.
8Himes, C.L., Preston, S.H. and Condran, G.A. (1994). A relational model of mortality
at older ages in low mortality countries. Population Studies 48, 269–291.

456
19. Life Science
For a logistic df there is again an inﬁnite upper endpoint. In addition, these
dfs—just as the Gompertz dfs—belong to the pot domain of attraction of the
exponential df which is a special case of a generalized Pareto (GP) df. In the
subsequent lines, the statistical inference is done in the GP model.
Analyzing the Upper Tail of the Life Span Distribution
Within the Generalized Pareto Model
Aarssen and de Haan9 applied the standard pot–method to Dutch life span data
(stored in um–lspdu.dat) and estimated a negative shape parameter. This yields
that the estimated generalized Pareto (GP) df has a ﬁnite upper limit. These
authors go one step further and compute conﬁdence intervals for the upper limit.
In the subsequent example we also make use of the standard pot–method.
Example 19.1.2. (Continuation of Example 19.1.1.) The sample mean excess function
is close to a straight line for life spans exceeding 95 years; there is a stronger deviation
from this line below 95.
life spans in years
90
100
110
1
2
3
life spans in years
90
100
110
0.2
0.4
0.6
Fig. 19.2. (left.) Sample mean excess function. (right.) Sample hazard function.
The form of the mean excess function supports the conjecture that there is a long–
living subpopulation which becomes visible beyond the age of 95 years (cf., e.g., the article
of Perls (1995)). To a larger extent, our results merely concern this subpopulation.
It is clear that a beta df will be estimated within the GP model. Guided by sim-
ulations, we decided to base the estimation on 2500 extremes (just life spans ≥98 are
taken). The estimated shape parameter γ within the generalized Pareto (GP) model
9Aarssen, K. and de Haan, L. (1994). On the maximal life span of humans. Mathe-
matical Population Studies 4, 259–281.

19.1. About the Longevity of Humans
457
is equal to −0.08. The right endpoint of the estimated beta distribution is equal to 122
years. This estimate ﬁts well to the worldwide reported life span of Mrs Jeanne Calment
(122 years and 164 days) who was born in Arles (France) on Feb. 21, 1875 and died in
Arles on Aug. 4, 1997.
The estimation of a negative shape parameter is in accordance with the result
by Aarssen and de Haan. In contrast to these authors we do not conclude that
human life span has a ﬁnite upper limit. Therefore, we also make no attempt to
estimate such an upper limit in accordance with Gumbel10 who wisely refused to
discuss this question. He suggested to estimate the “return value” F −1(1 −1/n)
or the expected value E(Xn:n) of the maximum Xn:n of a sample of size n .
Using the concept of penultimate distributions one can show that an inﬁnite
upper limit is well compatible with extreme value theory although we estimated a
beta distribution. Another argument which speaks in favor of the exponential df
can be found in an article by Galambos and Macri11.
Gompertz Law and Penultimate Approximation
In Fig. 19.1 a Gompertz df 	G0,µ,σ with location and scale parameters µ = 83 and
σ = 9.8 was ﬁtted to the upper tail of the given data of life spans. The following
considerations are made under the hypothesis that this Gompertz df is the actual
life span df. We still utilize our standard modeling, that is, actual truncated dfs
are replaced by GP dfs. For that purpose we compute the parameters of ultimate
and penultimate GP dfs according to the theoretical results in Section 6.5.
The Gompertz df is truncated left of the threshold u = 98. According to
(6.44) and (6.45), the approximating exponential df has the location parameter
u = 98 and the scale parameter σ(u) = (1 −	G0,µ,σ(u))/˜g0,µ,σ(u) ≈2.12.
The penultimate approach, see (6.47) and (6.48), leads to a beta df with
the same location and scale parameters as in the ultimate case, and the shape
parameter
γ(u) = η(1 −	G0,µ,σ(u)) =

(1 −	G0,µ,σ)/˜g0,µ,σ
′(u) ≈−0.22.
Plots of the densities show that the beta density is much closer to the truncated
Gompertz density than the exponential density. Therefore, estimates based on ex-
ceedances of Gompertz data point to beta (GP2) dfs when the statistical inference
is carried out in the GP model. Yet, estimating a beta df within the uniﬁed GP
model does not necessarily yield that the actual upper endpoint (thus, the upper
limit of life spans) is ﬁnite.
10Gumbel, E.J. (1933). Das Alter des Methusalem. Z. Schweizerische Statistik und
Volkswirtschaft 69, 516–530.
11Galambos, J. and Macri, N. (2000). The life length of humans does not have a limit.
J. Appl. Statist. Science.

458
19. Life Science
In the case of a Gompertz df, the upper endpoint of the penultimate beta df is
given by u+σ, which is equal to 107.8 for the parameters found above. This reveals
that an approximation, measured in terms of the Hellinger distance, is not very
sensitive to higher extremes. One may also ﬁt a penultimate df to the truncation
of the Gompertz df beyond the threshold u = 111, the observed maximum life
span in our example. This procedure is related to the approximation of Gompertz
maxima by a penultimate EV df. Applying (6.47) again one gets a penultimate
beta df with shape parameter γ(111) ≈−0.06 and a right endpoint of 120.8.
The conclusion is that the penultimate approach leads to signiﬁcantly higher
accuracy than the approach by limiting dfs. In addition, the penultimate approxi-
mations reveal that a Gompertz hypothesis is compatible with estimated beta dfs
within the framework of GP dfs.
19.2
Extrapolating Life Tables To Extreme
Life Spans: A Regression Approach
Based on eleven German life tables from 1871 to 1986, we study the change of
mortality rates of women over the time for ﬁx ages and apply a regression approach
to predict life tables. In a second step, we extrapolate the tail of the life span
distribution by ﬁtting a Gompertz distribution.
Introduction
Life tables are used by actuaries to calculate the premium of an insurance policy
(life or health insurance, rental scheme etc.). The basic terms of interest are the
mortality rates that will be separately estimated for both sexes and each age up
to 100. In the 1986 life table, the estimator of the mortality rate of a x years
old women is based on female babies born around the year 1986 −x. Because
people become older and older due to better standard of life, health care etc., the
mortality rates of babies born today are overestimated.
In the following we study mortality rates of women over the time for ﬁx
ages and apply a regression approach to forecast life tables. In a second step, we
extrapolate the tail of the life span distribution by ﬁtting a Gompertz distribution.
For a general overview concerning the theory of life spans we refer to L.A. Gavrilov
and N.S. Gavrilova12.
For a women of x years born within the year t −x deﬁne the mortality rate
qx,t as the (conditional) probability of death within one year. Denote by Ft−x the
(continuous) life span distribution of a women born in the year t −x. Then, the
12Gavrilov, L.A. and Gavrilova, N.S. (1991). The Biology of Life Span: A Quantitative
Approach. Harwood Academic Publishers, Chur (Russian edition by Nauka, Moscow
(1986)).

19.2. Extrapolating Life Tables To Extreme Life Spans
459
theoretical mortality rate is
qx,t := Ft−x(x + 1) −Ft−x(x)
1 −Ft−x(x)
,
x = 0, 1, 2, . . .
(19.1)
In the statistical literature, the term mortality rate is often used as a synonym for
hazard rate. In our considerations these terms have a diﬀerent meaning. The link
between them will be given later.
A natural estimate of qx,t is given by the empirical estimate ˆqx,t which is
deﬁned by the number of women born in t −x and dying at age between x and
x + 1 divided by the total number of women born in t −x exceeding an age of x
years. In practice, one takes a small number of years to calculate the estimates.
The life span distribution of a women born within the year t can be derived
from the mortality rates in a simple manner. We have
1 −Ft(x) =
x−1

j=0
1 −Ft(j + 1)
1 −Ft(j)
=
x−1

j=0
(1 −qj,t+j),
x = 0, 1, 2, . . .
(19.2)
Because the calculation of an individual premium is based on the unknown mor-
tality rates in the future one has to ﬁnd a prediction procedure.
The Data
Our investigations are based on eleven German life tables13 (consisting of estimated
mortality rates ˆqx,t) from the year 1871 to 1986 dealing with ages up to 100. In
Table 19.2 we list the mortality rates ˆqx,t of women at ages x = 90, 100.
Table 19.2. Mortality rates ˆqx,t for women at ages x = 90, 100 taken from life tables at
the years t.
t
1871
1881
1891
1901
1910
1924
1932
1949
1960
1970
1986
ˆq90,t
.314
.306
.302
.296
.302
.263
.274
.259
.248
.234
.187
ˆq100,t
.518
.447
.446
.420
.476
.402
.476
415
.380
.405
.382
It should be noted that the life tables diﬀer from each other in methodology.
The main diﬀerences concern the method of estimating mortality rates, smoothing
techniques, the handling of special inﬂuences on mortality like the wave of inﬂuenza
in 1969/70, and the number of years a life table is based on (e.g., 10 years (1871)
and 3 years (1986)). In the following lines we ignore these diﬀerences and assume
that the life tables consist of the empirical estimates as speciﬁed above.
13Stored in the ﬁles lm–lifem.dat for men and lm–lifew.dat for women (communicated
by Winfried Hammes, Federal Statistical Oﬃce).

460
19. Life Science
Regression over Time
Scatterplots of the mortality rates ˆqx,t, for each ﬁxed age x plotted against the
years t, indicate a decrease of exponential or algebraic order (see also Table 1). We
assume a polynomial regression model for the log–mortality rates log ˆqx,t for ﬁxed
age x. Least squares polynomials of degree 2 ﬁt particularly well to the data.
We have
log ˆqx,t ≈ˆpx,0 + ˆpx,1t + ˆpx,2t2,
where ˆpx,0, ˆpx,1 and ˆpx,2 are the estimated parameters.
The following illustrations show plots of the log–mortality rates log ˆqx,t as
functions in t and the ﬁtted least squares polynomials of degree 2.
0
70
80
90
100
1900
1950
-4
-2
Fig. 19.3. Fitting least squares
polynomials of degree 2 (solid
lines) to log–transformed mor-
tality rates (dotted).
It is obvious from Fig. 19.2 that linear regression curves—as used by Bomsdorf
and Trimborn14 and L¨uhr15 do not adequately ﬁt the transformed data. The larger
reduction of mortality from the 1970 to the 1986 life table, which is also observed
in newer abriged life tables16 may serve as another argument for the non–linearity
of the log–mortality rates. We remark that the mortality itself decreases with a
slower rate for higher ages (in contrast to the log–mortality).
With the estimated polynomials px we estimate the theoretical mortality
rates qx,t and the life span distribution Ft that is related to a cohort life table for
women born in the year t.
14Bomsdorf, E. and Trimborn M. (1992). Sterbetafel 2000. Modellrechnungen der Ster-
betafel. Zeitschrift f¨ur die gesamte Versicherungswissenschaft 81, 457–485.
15L¨uhr, K.–H. (1986). Neue Sterbetafeln f¨ur die Rentenversicherung. Bl¨atter DGVM
XVII, 485–513.
16Schmithals, B. and Sch¨utz, E.U. (1995). Herleitung der DAV–Sterbetafel 1994 R f¨ur
Rentenversicherungen (English title: Development of table DAV 1994 R for annuities).
Bl¨atter DGVM XXII, 29–69.

19.2. Extrapolating Life Tables To Extreme Life Spans
461
1871
1991
50
100
0.5
1
1871
1991
60
80
100
0.2
0.4
Fig. 19.4. Estimated life span distributions (left) and estimated mortality rates (right) by
interpolation (solid) and extrapolation (dotted) for women born in 1871, 1891, . . . , 1991.
Estimated life span distributions by interpolation means that the empirical
estimators of the mortality rates are calculated within the period from 1871 to
1986 by interpolation.
Regression of Extreme Life Spans
In the following we are interested in extreme life spans. We apply again a regression
approach to ﬁt a parametric distribution to the mortality rates ˆqx,t of ages x
exceeding 90 (also see Section 19.1). Note that within the life tables diﬀerent
smoothing techniques in the center as well as in the tail were applied. We assume
that after the regression these manipulations are of no consequence for the general
structure of the extreme life spans.
Recall the relation between the theoretical mortality rates qx,t as deﬁned in
(19.1) and the hazard rate. The latter one is deﬁned by
ht(x) :=
ft(x)
1 −Ft(x)
where ft denotes the density of Ft. Then
qx,t+x = 1 −1 −Ft(x + 1)
1 −Ft(x)
= 1 −exp

−
 x+1
x
ht(s) ds

and, thus,
 x+1
x
ht(s) ds = −log(1 −qx,t+x).
To ﬁnd a reasonable distribution that ﬁts to the upper tail of the life span
distribution, we apply again visualization techniques. For 90 ≤x ≤100, a scatter-
plot of points (x, log(−log(1 −qx,t+x))) is close to a straight line. Notice that for

462
19. Life Science
a Gompertz distribution one has the hazard rate ht(x) = exp((t −µt)/σt)/σt and,
thus,
log
  x+1
x
ht(s) ds

= x
σt
−µt
σt
+ log

exp
 1
σt

−1

is a straight line. We ﬁt a Gompertz distribution to the upper tail by the least
square linear regression
log

−log(1 −ˆqx,t)

≈ˆatx + ˆbt,
x = 90, . . . , 100
(19.3)
and obtain mortality rates ˆqx,t for women of 101 years and older by extrapolation
in (19.3) and the formula
ˆqx,t = 1 −exp

−exp(ˆatx + ˆbt)

,
x = 101, 102, 103, . . .
The estimated mortality rates and the corresponding life span distribution
functions for high ages are plotted in Fig. 19.5.
1871
1991
80
100
0.5
1
1871
1991
80
100
120
0.5
1
Fig. 19.5. Estimated life span distributions (left) and estimated mortality rates (right)
with ﬁtted Gompertz tail by interpolation (solid) and extrapolation (dotted) for women
born in the years 1871, 1891, . . . , 1991.
We include tabulated values of three estimated survivor functions F t = 1−Ft
for t = 1901, 1921, 1991 given in Fig. 19.5 (left). Notice that the women born in
1901 achieved the age 96 in the year of writing this article17. We see, for example,
that the (estimated) probability of survival at the age of 96 years increases from
.018 to .046 for the women born in the years 1901 and 1921.
We do not believe that a reliable forecast can be given for longer periods by
using such a straightforward extrapolation technique. Forecasts for more than one
or two decades merely indicate the theoretical consequences of our modeling.
17Reference is given to Kaufmann, E. and Hillg¨artner, C. (1997). Extrapolating life
tables to extreme life spans: a regression approach using XPL. In: 1st ed. of this book,
Part V, pages 245–250.

19.2. Extrapolating Life Tables To Extreme Life Spans
463
Table 19.3. Estimated probability of survival F t := 1 −Ft for women born in the years
t = 1901, 1921, 1991.
age x
90
92
94
96
98
100
102
104
106
108
F 1901(x)
.088
.057
.034
.018
.009
.004
.001
<.001
<.001
<.001
F 1921(x)
.174
.121
.078
.046
.024
.011
.004
.001
<.001
<.001
F 1991(x)
.664
.573
.466
.349
.232
.131
.059
.019
.004
<.001
The behavior of the mortality rates in Fig. 19.5 (right) is somewhat surpris-
ing. Primarily, it is not a result of the Gompertz (or converse Weibull) ﬁt or the
regression approach of degree 2 instead of degree 1. It rather depends on the weaker
trend of reduction of mortality for elderly people. Fig. 19.5 (left) seems to conﬁrm
the ecological crisis hypothesis as reviewed (and rejected) in the above–mentioned
book by Gavrilov and Gavrilova that states a biological limit for human life spans
between 100 and 110 years. However, for a more detailed study it is essential, even
in view of the existence of a long–living subpopulation visible behind an age of
95 years, cf. Perls18 and Section 19.1, to take empirical mortality rates of women
exceeding 100 years into account.
Notice that it is also possible to ﬁt a converse Weibull distribution. In this
case the hazard rate is of the form atb. However, one obtains analogous illustrations
for the converse Weibull distribution.
18Perls, T.T. (1995). Vitale Hochbetagte. Spektrum der Wissenschaft, M¨arz 3, 72–77.

Appendix:
First Steps towards Xtremes
and StatPascal

Appendix A
The Menu System
A.1
Installation
Xtremes is a genuine Windows application so that any system running Windows
98 (and newer) or NT 4 (and newer) is also capable of executing Xtremes.
To facilitate the installation, the CD–ROM contains an installation program
that copies all required ﬁles to your computer. The installation program starts
automatically when you insert the CD–ROM. One may choose an installation
directory and select certain optional parts of the system. If you are unsure, we rec-
ommend to accept the default options. Please make sure that you are logged in as
an administrator when installing under Windows NT/2000/XP/2003 or Windows
Vista 32–Bit.
Xtremes is deinstalled in the usual manner; i.e., by executing the option
Software in the Windows Control Panel. Please consult the Xtremes User Manual
for further information; a pdf version is accessible from its menu entry in the
Xtremes section of the Windows start menu.
A.2
Overview and the Hierarchy
Xtremes is a statistical software system that possesses graphics facilities, a facility
to generate and load data, an arsenal of diagnostic tools and statistical procedures,
and a numerical part for Monte–Carlo simulations.
The development of Xtremes has been supported by Simon Budig (User-
Formula facility), Martin Elsner (HTML help), Andreas Gaumann (help system),
Sylvia Haßmann (censored data), Andreas Heimel (help system, ARMA estima-
tors), Jens Olejak (minimum distance estimators), Wolfgang Merzenich (consul-
tation on the StatPascal compiler), Reinhard Pfau (Linux port), Torsten Spill-
mann (XGPL plots), Karsten Tambor (early version of the multivatiate mode),
and Arthur B¨oshans, Carsten Wehn, Lars Fischer, Ralf Pollnow (MS Excel front-
end) whose help is gratefully acknowledged.

468
A. The Menu System
The illustration on the right–hand side shows the hierarchy of the Xtremes system:
• Univariate and Multivariate Modes: the system is partitioned into a univari-
ate and a multivariate mode that can be selected in the toolbar.
• Domains D(iscrete), SUM, MAX and POT: select certain domains in the
toolbar which correspond to diﬀerent parametric models (discrete, Gaussian,
extreme value (EV) or generalized Pareto (GP)) built for discrete data and
data that are sums, maxima or exceedances (peaks–over–threshold).
• The Menu–Bar: select menus for handling and visualizing data, plotting dis-
tributions by analytical curves, estimating parameters etc. The Visualize
menu and larger parts of the Data menu are independent of the diﬀerent
domains.
• Menus: commands of the system are available in diﬀerent menus.
• Dialog Boxes: if a menu command requires further parameters, a dialog box
is displayed.
• Graphics Windows: the result of a menu command is usually displayed in a
graphics window.
• Local Menus: options and commands that are speciﬁc to a particular window
or dialog box are provided by means of a local menu (available with a right–
click somewhere in the window or the dialog box).
• Special Facilities: in the toolbar, select tools to manipulate a graphic. These
tools change the action taking place when one drags the mouse in a graphics
window. For example, to change the coordinate system, you may click on the
coordinate tool
in the toolbar and, then, pull up a rectangle.
• Help System: a context–sensitive online help is available by pressing the F1–
key at any time besides the general help facility (option Index in the Help
menu).
• Act, $, Hyd Supplements: special options for insurance, ﬁnance and hydrol-
ogy (Chapters 14–17) are available.
• UFO: select the UserFormula facility to enter your own formulas to plot
curves, generate or to transform data (cf. Section A.5).
• StatPascal: the integrated programming language StatPascal is activated
by means of the SP button (cf. Appendix B and StatPascal User Manual).
Keep in mind that the options in the Visualize and Estimator menus must
be applied to an active data set (read from a ﬁle or generated in the Data menu).
Instructions about the installation of the system are given in Section A.1.

A.2. Overview and the Hierarchy
469

470
A. The Menu System
Professional Version
For details concerning the professional version of Xtremes we refer to the web–site
http://www.stat.math.uni-siegen.de/xtremes/.
Under this web–site one is also informed about the latest news about Xtremes
and possible updates.
A.3
Becoming Acquainted with the
Menu System
A further possibility is to work closely with the online–help facility (press the
F1–key or enter the Help... Index).
Plotting Histograms and Curves
Xtremes provides easy–to–use plotting facilities. The user can quickly plot a curve,
adjust the coordinate system, get a list of the curves displayed in a plot window
or export the picture via the clipboard. A special facility is available for producing
EPS–ﬁles (see Documenting Illustrations on page 478).
We partly repeat the operations which are required to plot the histograms
in Fig. 1.2. Make sure that the univariate D(iscrete) domain is active, that is, the
ﬁrst button in the toolbar shows a single bullet
and the button labeled
is
pressed.
Demo A.1. (Plotting Poisson Histograms.) (a) Select the option Distribution...
Poisson in the univariate D(iscrete) domain and choose the parameter λ = 10.
Execute the Histogram option.
(b) The
tool (activating the option mouse mode) may be employed to adjust the
positions of the plotted bars.
The user may change the parameters of a plotted histogram (or of a curve)
by means of sliders which are an indispensable tool to work interactively with
data. Select the parameter varying mouse mode
from the toolbar and click
onto a histogram or curve in a plot window to open a window with sliders for each
parameter.
Demo A.2. (Varying the Parameter of a Poisson Histogram.) First, plot a Pois-
son histogram with parameter λ = 1. Select the parameter varying mouse mode
from the toolbar and click on the histogram of the Poisson distribution. Vary the
parameter λ, and also change the boundaries for the possible parameter values. In Fig.
A.1 there is a Poisson histogram with the pertaining slider and, in addition, a sample
histogram.

A.3. Becoming Acquainted with the Menu System
471
Fig. A.1. Interactive ﬁtting of
a Poisson histogram (right) to
sample histogram (left).
Next, let us display a Pareto density on the screen. Now, activate the uni-
variate POT domain, that is, the ﬁrst button in the toolbar shows a single bullet
and the button labeled
is pressed.
Demo A.3. (Plotting a Pareto Density.) Select the menu option Distribution...
Pareto to open a dialog box asking for the parameters of the distribution The
dialog box for the Pareto distribution is displayed in Fig. A.2. Click on the Density button
to plot the Pareto density for the chosen parameters. A plot window opens showing the
graph of the density.
Fig. A.2. Dialog box POT... Dis-
tribution... Pareto in the univariate
mode. Enter the parameters of the
Pareto distribution and plot a curve
using the buttons.
The coordinate system can be adjusted by using the local menu of the win-
dow. Click inside the window with the right mouse button and select the option
Change coordinates. Xtremes stores the previous coordinates which may be re-
stored by executing the option Restore coordinates in the toolbar (click the
button) or by pressing the Backspace key.
A more direct, interactive facility to modify the coordinate system is avail-
able by selecting a certain mouse mode from the tool bar (explained in the next
subsection).
One can get an overview of the curves plotted in a window utilizing the

472
A. The Menu System
menu option Edit Curves and Labels. A list with all curves is displayed, and one
can delete some curves or change their options like color, line style, brushes used
to hatch histograms, etc. If one wants to know the parameters of a plotted curve,
select the information mouse mode tool
and click onto the curve.
Pictures may be exported. Click the option Print in the local menu to send
your window to the printer. You can also copy it to the Windows clipboard or
create EPS–ﬁles. Advanced options to format a plot are described on page 478.
Selecting a Mouse Mode
The mouse mode determines what happens if you click into a window. The default
mode just brings a window into the foreground.
Other mouse modes are employed to move or delete curves, change colors
and plot options, add text, etc. Detailed explanations of all modes are given in
Section A.4.
As an example, we describe the change of the coordinate system using a
speciﬁc mouse mode. Activate the coordinate changing mouse mode by clicking
the
tool in the toolbar. In this mode, a new coordinate system is selected by
pulling up a rectangle: click into the graphics window (left mouse key), hold down
the mouse key and move the cursor to the opposite corner. The rectangle may be
pulled outside the window to enlarge the coordinate system.
Reading and Generating Data
At the beginning, the user should restrict himself to handling data included in the
package or generated by Xtremes. To read a data set from the disk, execute the
menu option Data... Read Data. The ﬁle dialog box of Windows appears.
Proceed to the dat folder and select any ﬁle. Xtremes loads this ﬁle and
opens a window entitled Active Sample displaying information about the data
set. Read another data set and notice that the description of the active data set
changes.
Now, two data sets are kept in memory. One can choose the active data set
from the ones already loaded by executing the menu option Data... Choose Data.
A list of all data sets used in the current session is displayed and a new active data
set can be selected. Keep in mind that all visualization and estimation procedures
are based on the active data set.
If one needs to load multiple data sets from a diﬀerent subdirectory, the Drag
and Drop facility of Xtremes can be utilized. Just select the ﬁles in a directory
listing of Windows and drop them anywhere on the Xtremes window.
Xtremes also enables the user generating data sets. Use the menu option
Data... Generate Univariate Data and select a distribution from the menu (see
Demo A.4). A dialog box opens asking for parameters, the sample size and a
ﬁlename. Files are stored in the active directory of Xtremes.

A.3. Becoming Acquainted with the Menu System
473
After clicking OK the data set is generated and a short description appears
in the Active Sample window.
Visualization of Data
A simple way to display data is in the form of a text. Load a data set or generate
one using Data... Generate Data and select the menu option Data... List Data.
Then, Xtremes opens a text window showing your data set. You can use the scroll
bar to browse the data.
The Visualize menu contains options to display sample dfs, qfs, histograms,
scatterplots, mean and median excess functions, among others. Kernel Density
also provides options that reﬂect the data points at the right, left or both ends
of the support. The bandwidth can be chosen by the user, an automatic selection
(via cross–validation) is available.
The visualization options are also available in the local menu of a List Data
window. They are applied to the displayed data set (rather than the active one) if
selected from the local menu. An easy way to work with more than one data set
is therefore to list them, minimize the windows and work with the local menus.
Time series (see Section A.4 for a description of the diﬀerent types of data
sets used in Xtremes) are visualized by means of the scatterplot option. Note that
each scatterplot is displayed in a separate window. You can cut points from a
scatterplot using the point selection (scissors) mouse mode tool
. The option
Least Squares Polynomial in the local menu of the scatterplot window leads to a
dialog box for polynomial regression.
The scatterplot option is also applicable to multivariate data. Depending on
the active mode (univariate or multivariate), the user has to select two or three
components. In the latter case, the points are displayed using a 3–D dynamic plot.
Applying Estimators to the Active Data
The three chapters in Part II of this book correspond to four diﬀerent domains of
Xtremes called D(iscrete), SUM, MAX and POT. Each domain provides diﬀerent
distributions and estimators in the Data... Generate Univariate Data, Distribution
and Estimate menus. One may switch between the diﬀerent domains by means of
the buttons D(iscrete), POT, MAX and SUM in the toolbar.
In the following example, we focus on estimators in the POT domain because
it provides the richest facilities.

474
A. The Menu System
Demo A.4. (Estimation Using the Hill Estimator.) To start, let us apply the Hill
estimator to standard Pareto data.
(a) First, create a data set using Data... Generate Univariate Data... Pareto(GP1).
(b) Next, execute the option Estimating... Hill(GP1/GP2) . Recall that generalized
Pareto models are ﬁtted to the upper tail of the distribution. Therefore, the estima-
tor requires the number k of upper extremes to be used for the estimation. You can
change the number of extremes by clicking the up or down arrows in the estimator dialog
box.
(c) A plot of ˆαn,k or ˆσn,k as a function in k is obtained using the diagram option. Choose
the desired parameters before clicking the button.
Various parametric curves (plotted with the estimated parameter values)
can be selected from the estimator dialog box. Comparing these curves with the
corresponding nonparametric ones, the user is able to judge visually the quality of
the estimation.
Similar dialog boxes are provided within the MAX and SUM domains. One
can work with the other parts of Xtremes while estimator dialogs are open. It is
also possible to use two or more estimators at the same time to compare their
results.
The Toolbar
The toolbar below the main menu provides a quick access to frequently used op-
tions of Xtremes. The tools enable the user to select diﬀerent parametric distri-
butions in the main menu. They are also used to select a mouse mode. We start
with the tools already described in the Overview.
Switch menu bar from univariate to multivariate mode.
Switch menu bar from multivariate to univariate mode.
Activates pulldown menus for discrete models.
Activates pulldown menus for Gaussian models.
Activates pulldown menus for extreme value (EV) models.
Activates pulldown menus for generalized Pareto (GP) models.
Opens pulldown menu with options for hydrology data (Chapter 11).
Opens pulldown menu with options for insurances data (Chapter 12).

A.3. Becoming Acquainted with the Menu System
475
Opens pulldown menu with options for ﬁnance data (Chapter 13).
Opens pulldown menu providing UFO facilities.
Opens StatPascal Editor Window to enter and run StatPascal programs.
Next the tools are listed that are not described in the Overview.
Opens ASCII–editor window.
Opens the Windows ﬁle dialog box and loads a data set. The ﬁle dialog
box provides options to delete or copy ﬁles.
The active data set is displayed in a text window.
Restores coordinate system in active window to the size before the last
change.
The toolbar is also used to select a mouse mode. The mouse mode determines the
action taking place when the user clicks into a window or onto a curve1.
Standard mouse mode: no special actions occurs.
Option mouse mode*: changes display options of a curve (e.g., color, line
styles, number of supporting points, etc.). The actual dialog box depends
on the type of the curve.
Parameter varying mouse mode*: opens window with sliders for each
parameter of a curve. Parameters are changed dynamically while sliders
are dragged.
Clipboard mouse mode*:
• moves the curves to the Xtremes clipboard window. When this
mode is applied in the Xtremes clipboard window, the systems
asks for a destination window;
• a curve can be directly dragged to a diﬀerent plot window (also
to a scatterplot window), if the left mouse button is kept pressed
until the cursor is located in the destination window.
Deleting mouse mode*: deletes curves from a plot window.
1Mouse modes, where one must click onto a curve, are marked with *.

476
A. The Menu System
Information mouse mode*: displays parameters of curve.
Coordinate changing mouse mode: adjust the coordinate system by
pulling up a rectangle or, in the trivariate setup, rotate the coordinate
system.
Point selection mouse mode: use this mode to cut oﬀpoints in a bivariate
scatterplot. Options of the local menu of a scatterplot do not use the cut
points.
Line drawing mouse mode: adds straight lines to a plot window.
Label mouse mode: adds text labels to a plot. See page 478 for details.
Curve tabulating mouse mode*: the supporting points of a curve can be
tabulated by storing them into a bivariate data set.
For that purpose, adjust the coordinate system and the number of sup-
porting points (enter the Change Coordinates box to adjust the range of
the supporting points and use the option tool
to select the number
of supporting points).
A.4
Technical Aspects of Xtremes
This section discusses two technical aspects of Xtremes, namely the format of data
sets and the mechanisms provided to export graphics.
Format of Data Sets
Data sets are stored as plain ASCII ﬁles. Certain speciﬁcations can be given at
the top of the ﬁle, such as the type of the data set and the sample size. Moreover,
one may include a shorter and a more detailed description.
Data sets can be entered by utilizing any text editor available under MS–
DOS or Windows. It is possible to use the integrated editor, yet one should be
aware of the fact that Windows 95/98 and ME limits the text size of the editor to
64 KBytes. Under Windows NT/2000/XP and Vista 32–Bit, text ﬁles of arbitrary
size can be handled.
We start with an example showing the data entry using the integrated editor.
Suppose you want to create a univariate data set with the following values: 1, 3.5,
7, −4.
Start the editor by selecting the editor button in the toolbar and click on
the Header button in the toolbar of the editor window. A dialog box asking for
the type of the data set opens. Select Univariate Data to create a template of a
univariate data set and ﬁll in the following ﬁelds:

A.4. Technical Aspects of Xtremes
477
Xtremes Univariate Data
Type: Artificial example
\begin(description)
This is an artificial data set. It
was entered using the integrated editor.
\end(description)
Sample Size: 4
1
3.5
7
-4
17
The ﬁrst line deﬁnes the type of the data set—in the present case Xtremes
Univariate Data. A list of all types is given below. The second line starts with
Type: and provides a short description which will be shown in the list of loaded
data sets (Data... Choose Data). It is also added to the description of curves based
on this data set. The description must be restricted to one line.
Between the lines \begin(description) and \end(description), a longer
description may be added. It is displayed in the Active Sample window. The next
line determines the size of the data set.
Then, the data are listed, one value for each line. After having typed the
text, save it to a ﬁle (e.g., in the subdirectory \dat). Afterwards, your data set
becomes the active one. One may also simulate a data set of the desired type using
the option Generate... .
Xtremes particularly supports the following data types.
• Xtremes Univariate Data. Real data x1, . . . , xn in any order, as presented
above. Execute Data... Transform Data... Sort to sort these data according
to their magnitude.
• Xtremes Time Series. Pairs (i, xi) of integers i and reals xi as, e.g.,
1
17.5
2
−2
3
0.34
4
0.001
The discrete time must be given in increasing order. Some of the pairs (i, xi)
can be omitted (see, e.g., ct–sulco.dat), so that the entry Sample Size is not
necessarily the number of data points within the ﬁle. It may be larger than
the time of the last point if values were omitted at the end of the ﬁle.
• Xtremes Multivariate Data. Multivariate data (xi,1, . . . , xi,m) are stored
using m entries on a line.

478
A. The Menu System
Moreover, the line after Sample Size contains an entry deﬁning the dimen-
sion m of the data set. It is followed by m names surrounded by quotation
marks. They deﬁne the headers for the corresponding column, e.g.,
Sample Size: 12
Dimension: 4
“Month”
“SO2”
“NO”
“O3”
1
75.2
13.4
17.2
2
83.1
17.9
15.4
3
.
12.8
11.3
4
43.9
15.3
11.3
Missing values are indicated as a dot. It is possible to combine related uni-
variate data sets of diﬀerent length to one multivariate data set. The rows
containing a dot are ignored when the multivariate data set is transformed
or converted.
• Data Sets Without Header. Xtremes can also load plain ASCII ﬁles
containing just a matrix of data, without any headers. Such data sets are
treated as multivariate. Moreover, one can use decimal points or decimal
commas within a data set.
Discrete, grouped and censored data types are also available. Please consult
the Xtremes User Manual for details.
Data can be converted from one type into another by the option Data...
Convert to. All canonical conversions are available. There are also some special
conversions.
One can apply the UserFormula facility to perform transformations not cov-
ered by the menu system. More sophisticated conversions are accomplished by
means of StatPascal programs.
Documenting Illustrations
Xtremes provides various tools to change the outer appearance of a plot and to
export it to other systems. We start with a description of advanced plot options
(like diﬀerent colors and line styles) that are used to prepare pictures for exporting.
The following options are available:
• Coordinate System: the coordinate system is either displayed within the
window or on a rectangle around the actual plot area. These options are
controlled in the Change Coordinates box of the local menu. The portion of
the plot area may be changed to provide space for the attachment of labels
outside the frame using the Frame Size option.

A.4. Technical Aspects of Xtremes
479
• Line Styles and Colors: the option mouse mode tool
is used to change
the plot options of a curve. A left–click onto the curves opens a dialog box
(cf. Fig. A.3). The user may select
– predeﬁned line styles,
– deﬁne his own line style by specifying the length of curve segments and
gaps
as well as the thickness of the curve. For example, choose the values
– 1 and 1 to produce a dotted line,
– 4 and 4 to produce a dashed line.
These procedures lead to a better result on printed pages than the use of
predeﬁned line styles (except of the solid line).
Diﬀerent sizes and hatch styles are provided for histograms. The local menu
of a scatterplot window provides the Options entry to change the point size.
• Adding Text: select the label mouse mode tool
and click at the position
where you want to put your label. The font and position of the text may be
changed using the parameter varying
and option
mouse mode tools.
It is possible to display vertical text or to move a label to the edge of the
window. Labels are treated like curves, so they may be moved to another
window or deleted in the same way.
The box for curve options is presented in the following Fig. A.3. We suggest
to use the solid line option or the “line” and “gap” facility.
Fig. A.3. Specify your own line style
in the input ﬁelds “line” and “gap”.
The contents of an Xtremes plot window may be exported, either by printing
a window, saving it as an EPS (Encapsulated Postscript) ﬁle or storing it in the
Windows clipboard.

480
A. The Menu System
• Printing: ﬁrst, select the option Frame Size (Print/EPS) (cf. Fig. A.4) from
the local menu of the active window to deﬁne the size of the picture and
provide space for the frame (see the next Demo A.1). Then, select Print to
copy the contents to your printer. Printer Setup is utilized to change options
of the printer.
• Saving an EPS ﬁle: ﬁrst, set the size of the picture and frame, as in the
previous case. Then, select Save as EPS ﬁle to store the contents in the EPS
format. Xtremes asks for a ﬁlename.
• Copying to the clipboard (Option Copy to Clipboard in the local menu): the
contents of the active window are copied to the clipboard in the standard
bitmap format. It is possible to insert the contents of the clipboard in other
applications like painting programs or word processors.
Fig. A.4. Frame Size (Print/EPS) di-
alog box. The user selects the size of the
coordinate system and provides space
for text displayed outside the actual
picture.
In the following demo further explanations are provided about the dialog box
in Fig. A.4.
Demo A.5. (Printing a Graphics Window.) Select the Frame Size (Print/EPS)
option from the local menu and enter the size of your picture in the dialog box.
The values shown in Fig. A.4 entail a picture of the size 72mm × 52mm, the actual plot
area comprises 60mm × 40mm. After that, proceed with the Print option to copy the
active window to your printer or select Save as EPS ﬁle to create an EPS ﬁle that can
be included in other documents.
LATEX–users may employ the epsf macro package, which provides commands to in-
clude postscript ﬁles into LATEX documents (e.g., the commands \epsfxsize=72mm
\epsfbox{picture.eps} load picture.eps and scale it to a horizontal size of 72mm).

A.5. The UserFormula (UFO) Facilities
481
A.5
The UserFormula (UFO) Facilities
With UserFormula (UFO), the user can type in formulas that are used
• to evaluate expressions using a calculator;
• to plot univariate or bivariate curves;
• to generate data sets;
• to transform existing data sets.
The formulas are entered by using the notation of common programming lan-
guages.
We give an overview of the functions that are available in UserFormula ex-
pressions and describe the options of the UserFormula menu, which opens after
clicking the UFO button
in the toolbar. Operations that are too complicated
for UserFormula may be handled by using the integrated programming language
StatPascal, introduced in Appendix B.
Overview
One can access all distributions implemented in Xtremes by calls to predeﬁned
functions. There are three diﬀerent groups of predeﬁned functions.
• Standard mathematical functions like abs(x) (absolute value), exp(x) (ex-
ponential function), log(x) (natural logarithm) or sqrt(x) (square root),
among others.
• Function calls—partly including a shape parameter a—under which one may
generate data, such as betadata(a) or gumbeldata. The returned values are
independent for successive calls and governed by the respective distribution
in its standard form. In addition, [0, 1)–uniform data may be called by the
function random.
• Functions for densities, qfs and dfs (again partially including a shape param-
eter a) such as:
– betadensity(a,x), betadf(a,x), betaqf(a,x);
– gaussiandensity(x), gaussiandf(x), gaussianqf(x); etc.
The last curve plotted within an Xtremes plot window is available under the name
actualcurve.
The chapter Library Functions within the StatPascal User Manual gives a
detailed description of all predeﬁned functions that are available within the User-
Formula facility.

482
A. The Menu System
Calculator
The calculator allows the user to type in a formula and evaluate it. Fig. B.5 shows
the calculator dialog box.
Fig. A.5. Calculator dialog box.
Formulas typed in the upper edit
ﬁeld are evaluated. The lower edit
ﬁeld deﬁnes variables and func-
tions also available in other parts
of Xtremes.
In the lower part of the calculator window, you can deﬁne your own functions
and variables. Write your deﬁnitions in the edit ﬁeld User Deﬁned Functions and
click on the =–button. The deﬁnitions thus made are available within all dialog
boxes providing a UserFormula facility. They can also be used in all edit ﬁelds
where a real value is expected, e.g., in the dialog boxes used for plots of parametric
curves. For example, a Gaussian density including a location parameter is deﬁned
in the following way:
Pi:=3.1415
gauss(mu,x):=1/sqrt(2*Pi)*exp(-(x-mu)**2/2)
The formulas are stored in the ﬁle formula.txt within the working directory.
They are loaded again upon the next start of Xtremes.
Plotting Curves
The graph of a function x →f(x) or x →f(p, x) may be plotted in every graphics
window. The optional parameter vector p = (p1, p2, p3) is changed by using the
parameter varying mouse mode tool
. Instead of p1 one may also use p. Within
the multivariate mode, a surface plot of a function (x, y) →f(x, y) is performed.
Demo A.6. (Plotting Gaussian Densities with Varying Location Parameter.) Click
the UFO button
in the toolbar and select the option Plot curve from the popup
menu. Now, type the formula 1/sqrt(2*3.1415) * exp(-(x-p1)**2/2) in the edit ﬁeld
labeled f(x) or f(p,x) in the dialog box. If you have entered the deﬁnitions shown in the
Calculator box, you can also write gauss(p1, x).

A.5. The UserFormula (UFO) Facilities
483
Especially note the option for the destination window. Xtremes lists all open
windows, and you can also enter the name of a new window. Select OK to plot
the curve.
Generating Data
The UserFormula facility may be employed to generate univariate data sets. Click
the UFO button and select the option Generate Data... A dialog box similar to
the one used for plotting curves is utilized.
Now, the user must specify a quantile function (qf) Q(x) that is applied to
[0, 1)–uniform data. For example, use -log(x) to generate standard exponential
data.
Data distributed according to the distributions implemented in Xtremes is
available by means of the predeﬁned functions *data (where * is replaced by the
name of the distribution). For example, one might also write exponentialdata in
the above example.
Transforming Data
The UserFormula facility oﬀers the transformation of univariate or multivariate
data sets and time series. When you select the option Transform Data... in the
UFO menu, Xtremes asks for a transformation depending on the type of the active
data set.
• Univariate Data xi: specify a transformation T to generate the data T (xi).
• Time Series (ti, xi): specify two functions T1(t, x) and T2(t, x) to obtain the
time series values (T1(ti, xi), T2(ti, xi)). Note that real–valued times are al-
lowed.
• Multivariate Data (xi,1, . . . , xi,m): specify transformations Tj. The system
generates
(T1(xi,1, . . . , xi,m), . . . , Tk(xi,1, . . . , xi,m)).
In addition to the transformation, one must specify k names for the columns
of the transformed data set. See Demo B.8.
Demo A.7. (Smoothing a Data Set Using Polynomial Regression.) Convert the
data set to a time series, display a scatterplot and add a regression polynomial.
Now, the polynomial is available as actualcurve. Therefore, one can apply the transfor-
mation T1(t, x) = t and T2(t, x) = actualcurve(t) to store the values of the polynomial,
evaluated at the times t of the original time series.

484
A. The Menu System
Fig. A.6. Transform Data dialog
box for multivariate data. The trans-
formation in (12.39) is applied to
football.dat (with changed signs).
Demo A.8. (UFO Transformation of Multivariate Data.) Read football.dat (cf.
Example 8.2.1) and change the signs (Data... Transform Data... Change Signs).
Choose UFO and apply Transform Data. The dialog box Transform Data lists the column
names X1 and X2 of the current data set on the left–hand side (see Fig. B.6) together
with the variable names x1 and x2 assigned to the values in the columns. In the edit
ﬁeld on the right–hand side, the user must deﬁne the names of the ith column (using the
arrow button one may edit a template of the transformation T(x1, x2) = (x1, x2)). In our
example, we use the names Goalpost and Endzone and add the transformed variables
-(-x1/12)**1.2 and -(-x1/12)**1.2. Finally, press OK to execute the transformation.

Appendix B
The StatPascal
Programming Language
To enhance the ﬂexibility of the system beyond the possibilities of the UserFor-
mula facility, it is supplemented by the integrated Pascal–like programming lan-
guage StatPascal. StatPascal programs are handled in the StatPascal editor win-
dow which can be opened by selecting the
button.
Exaggerating a bit, one can say that the pull–down menu system serves as a
platform for learning the speciﬁc functions and procedures available in StatPascal.
Thus, many options in the menus and dialog boxes have their counterparts as
functions in StatPascal.
When a StatPascal program is executed, use the implemented
• dialog boxes,
• plot windows and a StatPascal window for the output.
One may also attach StatPascal programs to the menu bar which provides a facility
to extend the menu system. We assume that the reader has a working knowledge
of the Pascal language.
In contrast to other common statistical languages, StatPascal is a strongly
typed language which is compiled and executed by an abstract stack machine.
StatPascal is therefore usually faster than other systems.
The StatPascal User Manual contains a formal description of the syntax and
an alphabetical list of all library functions. The ﬁrst chapters of the StatPascal User
Manual also include an introduction to basic (Pascal) programming techniques.
The installation program copies a pdf version of the StatPascal User Manual
and Reference to the ﬁle spmanual.pdf in the sp subdirectory.

486
B. The StatPascal Programming Language
B.1
Programming with StatPascal: First Steps
This section enables the user to take the ﬁrst steps into the StatPascal environ-
ment. We mention the StatPascal editor and introduce some simple programs.
The StatPascal Editor
A StatPascal editor window is opened by selecting the
button within the
toolbar. It provides the usual editing facilities of Windows. Text blocks can be
exchanged by means of the clipboard utilizing the commands listed in Appendix
A. Under Windows 95, the maximum ﬁle size of the editor window is limited to
64 KBytes.
The toolbar enables the user to save and load text ﬁles. The Run option
is a short cut for the compilation and execution of a program. Fig. B.1 shows the
StatPascal editor window, where the following tools are available.
New: erases the text within the StatPascal editor window.
Load: opens the ﬁle dialog box and loads a StatPascal program.
Save: writes the text in the StatPascal editor window to a disk ﬁle. If no
ﬁlename has been provided, the Save as option is activated.
Save as: writes the text in the StatPascal editor window to a disk ﬁle
after asking for a ﬁle name.
Run: compiles and executes the program in the StatPascal editor win-
dow.
Compile: compiles a program and stores the resulting binary under a
ﬁlename, executes it or locates the position of a runtime error with the
source ﬁle.
Compiler Options: opens the Compiler Options dialog box, controlling
parameters of the compiler and runtime environment.
Help: opens the StatPascal online help.

B.1. Programming with StatPascal: First Steps
487
The “Hello, World” Program
We start with the traditional “Hello, world” program to demonstrate the tech-
niques of entering, compiling and running a StatPascal program. The user will
immediately recognize that the program looks like its Pascal equivalent.
program hello;
begin
writeln (’Hello, world!’)
end.
Type in the program, save it to a ﬁle (Save as,
and click the Run button
to execute the program. If the program contains no errors, Xtremes opens the
StatPascal window displaying the output.
The StatPascal Window
The procedures write, writeln and read of the Pascal language are provided to
perform input and output operations in the StatPascal window. In Fig. B.1, we
see a program in the StatPascal editor window and its output in the StatPascal
window.
Fig. B.1. StatPascal ed-
itor window with exam-
ple program (front) and
its output in the StatPas-
cal window (back).
The example program asks for real numbers t and displays the Gaussian den-
sity ϕ(t). The predeﬁned routines MessageBox, DialogBox and MenuBox (which
are described in the StatPascal manual) provide an alternative using dialog boxes.
Data Types and Structures
StatPascal provides most of the data types and data structures of the Pascal
language. The predeﬁned types

488
B. The StatPascal Programming Language
boolean, char, integer, real and string
are available, and the user can deﬁne new types using all data structures of Pascal
(with the exception of variant records). Our examples use only the predeﬁned data
types in conjunction with the standard Pascal data structure array and the new
data structure vector which is introduced below.
Elements of a StatPascal Program
A StatPascal program consists of up to eight diﬀerent sections. In the following
table, we give a short explanation of the diﬀerent sections of a program. As one
could see from the previous example programs, most of these sections are optional.
program name;
a StatPascal program starts with the reserved word
program, followed by the name of the program
uses ... ;
the optional uses lists libraries which are used by
the program
label ... ;
the optional label starts the declaration of the labels
const ... ;
the optional const deﬁnes constant values
type ... ;
the optional type is used to assign names to user
deﬁned data types (see page 489)
var ... ;
the optional var declares the variables used in a pro-
gram
procedure ... ;
function ... ;
an arbitrary number of functions and procedures
may be declared (see page 489)
begin
...
end.
the mandatory main program contains the instruc-
tions performed by the program
A tutorial on basic programming techniques with an introduction to standard
Pascal can be found in the StatPascal manual.
Vector and Matrix Types
StatPascal implements a new data structure vector which is similar to the array
structure. Yet, one does not have to specify the number of elements when declaring
a vector. A vector type is deﬁned using the declaration
vector of type.
We start with a simple example which shows the usage of a real vector. The
following program generates a Gaussian data set of size 100 under the location and
scale parameters 2 and 3, and stores the data in the vector x. Then the sample
mean and sample variance of the simulated data set are displayed.

B.1. Programming with StatPascal: First Steps
489
program example;
var x: vector of real;
begin
x := 2 + 3 * GaussianData (100);
writeln (mean (x), variance (x))
end.
Readers who are familiar with other statistical languages should note that
the usual arithmetic and logical expressions (with componentwise operations) as
well as index operations are supported.
Vectors can also be used as arguments and return types of functions. The
language provides implicit looping over the components of a vector if a function
operates on the base type of a vector structure.
The following program demonstrates further vector operations. We perform
a numerical integration of a real–valued function f, deﬁned on the interval [a, b],
using the approximation
 b
a
f(x) dx ≈
n

i=1
f(ci) + f(ci+1)
2
b −a
n
where ci = a + (i −1)(b −a)/n for i = 1, . . . , n + 1. The function f as well as the
parameters a, b and n are provided as arguments.
type realfunc = function (real): real;
function integrate (f: realfunc; a, b: real; n: integer): real;
var fc: vector of real;
begin
fc := f (realvect (a, b, n + 1));
return sum (fc [1..n] + fc [2..(n+1)]) * (b-a) / (2*n)
end;
We start with a type declaration for the functional parameter. The ﬁrst as-
signment within the function integrate calculates the values f(ci), i = 1, . . . , n + 1
and stores them in the variable fc.
Note that the call to the predeﬁned function realvect returns a real vector
with n + 1 equally spaced points between a and b, which is given as an argument
in the call of f.
In the second statement, we generate two integer vectors containing the values
from 1 to n and from 2 to n + 1, which serve as indices to fc. The index operation
yields two real vectors with the values (f(c1), . . . , f(cn)) and (f(c2), . . . , f(cn+1)).
The + operator adds these vectors componentwise, and the predeﬁned function
sum calculates the sum of the components of the resulting vector. Finally, the
value of the integral is returned.
Next, we deﬁne a function square and calculate its integral.

490
B. The StatPascal Programming Language
function square (x: real): real;
begin
return x * x
end;
begin
writeln (integrate (square, 0, 1, 100))
(* 0.33335 *)
end.
The data structure matrix represents two–dimensional arrays where the
number of rows and columns are determined at run time. The language provides
an implicit conversion from two–dimensional arrays to matrices. One can also con-
struct a matrix using the predeﬁned function MakeMatrix, which ﬁlls a matrix
with the components of a vector. Matrices can be used in arithmetic operations.
The multiplication of two matrices or of a matrix and a vector perform the usual
mathematical matrix operations. As an example, we show a program that prints
100 random variables simulated under a bivariate Gaussian distribution with co-
variance matrix
Σ =
⎛
⎝
1
0.2
0.2
1.5
⎞
⎠.
Note that chol (S) returns a matrix C such that S = CCt.
program bivgauss;
var S, C: matrix of real;
i: integer;
begin
S := MakeMatrix (combine (1.0, 0.2, 0.2, 1.5), 2, 2);
C := chol (S);
for i := 1 to 100 do
writeln (C * GaussianData (2))
end.
Consult the StatPascal Reference Manual for further information about vec-
tors and matrices.
B.2
Plotting Curves
StatPascal allows the user to open an Xtremes plot window and to display curves
and scatterplots in it. These windows and curves exactly act like the ones available
from the menu system.
In the following, we only discuss univariate curves and scatterplots. The sp
subdirectory of the Xtremes directory contains various example programs that
demonstrate the graphical facilities of StatPascal.

B.2. Plotting Curves
491
Univariate Curves
Xtremes provides a predeﬁned function plot which is utilized to plot univariate
curves. The function requires two vectors containing the points xi and values yi,
the destination window and a description of the curve. A linear interpolation of
the given points is displayed.
For example, the following program plots a Gaussian density in two Xtremes
plot windows.
program gaussplot;
const n = 100;
var x, y: vector of real;
begin
x := realvect (-3, 3, n);
y := gaussiandensity (x);
plot (x, y, ’Density 1’, ’Gaussian density’);
plot (x, y, ’Density 2’, ’Gaussian density’)
end.
Fig. B.2 shows the output of the program. Two Xtremes plot windows (Den-
sity 1 and Density 2) are opened by calls to plot. The curves are displayed as solid
black lines. One can change the plot options using the procedures listed at the end
of this section.
Fig. B.2. The program
gaussplot and its out-
put in two Xtremes plot
windows.
Scatterplots
The scatterplot procedure, displaying scatterplots, is similar to the plot procedure.
The routine requires three parameters: two arrays deﬁning the points and the
name of the scatterplot window. In the above example, the call to plot must be

492
B. The StatPascal Programming Language
replaced by scatterplot (x, y, ’Scatterplot’) to obtain a scatterplot of the
points (x1, y1), . . . , (xn, yn).
B.3
Generating and Accessing Data
An important facility of StatPascal is the implementation of routines for data gen-
eration and data transformation not covered by the menu system or UserFormula.
In this section, we introduce functions and procedures to exchange data between
StatPascal and Xtremes. Data stored in a StatPascal program (e.g., in a vector)
are not used directly within Xtremes, and data sets loaded in Xtremes are not
used by StatPascal automatically. Instead, all data transfer is accomplished by
calling predeﬁned functions and procedures. They give the user access to the ac-
tive data set from within a StatPascal program and allow to pass data collected
in a StatPascal vector to Xtremes, thus creating a new active data set.
We start with an example for generating standard Pareto data under the
shape parameter α = 1.
program Pareto;
const n = 100;
alpha = 1.0;
var x : vector of real;
begin
x := paretodata (alpha, n);
createunivariate (x, ’pareto.dat’, ’Description’)
end.
Here n Pareto data are generated independently by the function paretodata
and stored in the vector x. The call to createunivariate passes the data to Xtremes,
that is, the data set stored in x is saved to the ﬁle pareto.dat which is then the
active one. In addition, a short comment is added. After having run the program,
all options of the menu system can be applied to the new data set.
Passing Data from StatPascal to Xtremes
We now provide a more systematic description of the generation of data sets by
StatPascal. Four diﬀerent procedures are provided to pass data collected in a
vector from StatPascal to Xtremes. In the following examples, the data are saved
to ﬁlename.dat in the working directory. One may create data sets of the following
types.
Xtremes Univariate Data: data x1, . . . , xn are collected in a real vector given
as argument to the call of the predeﬁned procedure createunivariate.
var x: vector of real;
...

B.3. Generating and Accessing Data
493
createunivariate (x, ’filename.dat’, ’Description’);
Instead of a vector, a one–dimensional real array may be given as well.
Xtremes Time Series: in addition to the previous case, a vector containing the
times ti of the observations must be provided.
var
x : vector of real;
t : vector of integer;
...
createtimeseries (t, x, ’filename.dat’, ’Description’);
Xtremes Censored Data: besides a real vector containing the censored data,
there is an integer vector with the censoring information.
var
z : vector of real;
delta : vector of integer;
...
createcensored (z, delta, ’filename.dat’, ’Description’);
Xtremes Multivariate Data: the data xi,j are collected in a real matrix. In
addition, a string with the column names, separated by ’|’, must be provided.
var x: matrix of real;
h: string;
...
h := ’Day|Month|...’;
createmultivariate (x, h, ’filename.dat’, ’Description’);
Note that a two–dimensional array can be provided instead of a matrix type,
because the language supports an implicit type conversion from two–dimensional
arrays to matrix types.
As a result of such a procedure you will get an active data set of the type
as speciﬁed by the command create.... The Active Data window opens showing
the name of your data set and the description provided in the last argument.
Passing Data from Xtremes to StatPascal
Next, let us consider the case where active data are dealt with by StatPascal. The
active data set is accessed by means of the following functions:
samplesize
size of the active data set;
dimension
dimension of the active data of type Xtremes Multivariate
Data. This function can also be applied to univariate data or
a time series, yielding 1 or 2, respectively;

494
B. The StatPascal Programming Language
data(i)
xi:n if x1, . . . , xn are Xtremes Univariate Data. Use the func-
tion call data(i,1) to access the unsorted data;
data(i,j)
xi,j if (x1,1, x1,2), . . . , (xn,1, xn,2) is the active time series. Mul-
tivariate data are dealt with in the same way. If a grouped data
set is active, then data(i,1) returns the cell boundary ti and
data(i,2) the frequencies ni in cell [ti, ti+1). Moreover, censored
data are treated like multivariate data with the censored data
in the ﬁrst component, the censoring information in the second
and the weights of the Kaplan–Meier estimate in the third one;
columndata(i)
vector with the (unsorted) data in the ith column of the active
data set;
rowdata(i)
vector with the data in the ith row of the active data set;
columnname(i)
name of the ith column. This function yields an empty string
if not applied to a multivariate data set.
Demo B.1. (Translation of a Univariate Data Set.) We employ StatPascal to add
the value 5 to univariate data. Note that the vector structure allows us to deal
with data sets of any size.
program translation;
var x: vector of real;
begin
x := columndata (1);
createunivariate (x + 5, ’demo.dat’, ’’)
end.
We used the function call columndata (1) to access the unsorted data set.

Author Index
A
Aarssen, K., 456
Aitchison, J., 245
Akashi, M., 442
Akgiray, V., 179, 374
Anderson, C.W., 145, 450, 451
Andrews, D.F., 85, 161
Ashkar, F., 120, 138, 341
Atkinson, H.V., 451
B
B¨uhlmann, H., 5, 425
Bachelier, L.J.B.A., 374
Bahr von, B., 434
Baillie, R.T., 373
Balakrishnan, N., 160
Balkema, A.A., 27
Barbour, A.D., 9
Barndorﬀ–Nielsen, O.E., 119
Basrak, B., 398
Becker, R., 272
Beirlant, J., 137, 193, 195, 198
Benjamin, J.R., 109
Benktander, G., 156
Beran, R.J., 88
Berkowitz, J., 408
Bhattacharya, G.K., 302
Bhattacharya, R.N., 268
Black, F., 374
Bob´ee, B., 120, 138
Bollerslev, T., 395
Bomsdorf, E., 460
Booth, G.G., 179, 374
Breidt, F.J., 400
Brockwell, P.J., 68, 168, 220, 223
Brown, L.D., 88
Brush, G.S., 358
Buishand, T.A., 113
C
Caeiro, F., 197, 204
Carey, J.R., 454
Caserta, S., viii, 371, 395
Castillo, E., 118
Changery, M.J., 121
Cheng, B.N., 289
Chernick, M.R., 169
Christoph, G., 174
Cleveland, W.S., 69, 271
Cohen, A.C., 160
Coles, S.G., viii, 116, 322, 331, 450
Cornell, C.A., 109
Cottis, R.A., 441
Cowing, T.G., 303
Cox, D.R., 119
Cs¨org˝o, S., 138, 306
D
Dan´ıelsson, J., 395
Datta, S., 220
Davis, R.A., 68, 168, 169, 220, 221, 223,
398, 400
Davison, A.C., 121, 142, 240, 334, 342, 357
Daykin, C.D., 43
de Haan, L., 19, 23, 27, 134, 145–147, 189,
196, 198, 204, 208, 398, 456
Deheuvels, P., 138, 320
Dekkers, A.L.M., 134
Devroye, L., 38
Diebold, F.X., 238, 374, 407
Diebolt, J., 132, 155
Dierckx, G., 193, 198
Dietrich, D., viii, 145
Drees, H., vi, 33, 52, 135, 138, 146, 147, 192,
195, 198, 207, 210, 216, 219, 320,
448
Drosdzol, A., 276

496
Index
Dubey, S.D., 111, 123, 134
Dunsmore, I.R., 245
E
Efron, B., 91
Einmahl, J.H.J., 134
El–Aroui, M.–A., 132, 155
Embrechts, P., 401, 432, 434
Enders, W., 373
Engle, R.F., 395
F
Falk, M., viii, 23, 135, 189, 234, 238, 272,
300, 313, 332
Fan, J., 403, 409
Ferreira, A., 147
Feuerverger, A., 193, 198
Figueiredo, F., 204, 206
Fill, H.D., 351
Fiorenti, M., 122
Fisher, R.A., 18, 47, 171, 187
Fofack, H., 176
Fraga Alves, M.I., 196, 206
Frey, R., 401
Frick, I., v
Frick, M., 321, 326, 328
G
G¨anssler, P., 235
Galambos, J., 7, 23, 32, 118, 186, 293, 457
Gangopadhyay, A.K., 239
Garrido, M., 132, 155
Gavrilov, L.A., 458
Gavrilova, N.S., 458
Geﬀroy, J., 293
Gelder, van P., viii
Gene Hwang, J.T., 88
Genest, C., 275
Gentleman, J.F., 10
Gerber, H.–U., 412
Girard, S., 132, 155
Glynn, P.W., 155
Gnedenko, B.V., 19
Gogebeur, Y., 193
Goldie, C.M., 185
Gomes, M.I., v, viii, 143, 189, 190, 192–194,
196, 197, 201, 204, 206, 208
Gompertz, B., 54
Goovaerts, M.J., 430
Guillou, A., 198
Gumbel, E.J., 117, 457
Gunther, T.A., 238, 407
H
H¨usler, J., v, viii, 23, 144, 145, 147, 169,
234, 238
Haight, F.A., 9
Hall, P., 185, 193, 198
Hamilton, J.D., 68
Hampel, F.R., 34, 89, 118
Hashofer, A.M., 143
Hashorva, E., 296
Haßmann, S., 129
Heﬀernan, J.E., 322, 323, 331
Herzberg, A.H., 85, 161
Hesselager, O., 255
Heyde, C., v
Hill, B.M., 129
Hipel, K.W., x, 3
Hipp, C., 432
Hlubinka, D., 449
Hogg, R.V., 50
Holst, L., 9
Hosking, J.R.M., viii, 119, 120, 337
Houghton, J.C., 120
Hsing, T., 169, 208
Huang, X., 317, 320
Huber, P.J., 89
I
Ishimoto, H., 442
J
Jansen, U., 114
Janson, S., 9
Jenkinson, A.F., 16
Joe, H., 275, 297, 308
Johnson, N.L., 111, 117, 285
Jorion, P., 372, 382
K
K¨otzer, S., 451
Kannisto, V., 455
Kasahara, K., 444
Katz, R.W., vi, 353, 358, 359, 362, 365

Author Index
497
Kaufmann, E., vi, viii, 138, 182, 186, 189,
195, 198, 235, 256, 313, 326, 328,
445, 453
Kawakami, K., 451
Kimball, B.F., 117
Kinnison, R.P., 121
Kl¨uppelberg, C., 432
Klugman, S.A., 50, 435
Koedijk, K.G., 179
Kogon, S. M., 178
Kolata, G., 7, 454
Komukai, S., 444
Konecny, F., 342
Kotz, S., 111, 117, 285
Kowaka, M., 442
Kozek, A., vi, 401
Kremer, E., 412
Kuczera, G., 349
Kuon, S., 413, 425
Kupiec, P. H., 408
L
L¨uhr, K.–H., 460
Laycock, P.J., 441, 444
Leadbetter, M.R., 23, 145, 168, 169
Ledford, A.W., 322, 324, 331
Lee, L., 301
Li, D., v, 144, 146, 147
Liedo, P., 454
Lindgren, G., 23, 145
Ling, S., 220
Little, R.J.A., 274
Loretan, M., 375
Lu, J.–C., 301
Lungu, D., viii
M
Macri, N., 457
Madsen, H., 351
Mandelbrot, B.B., 374
Manteiga, W.G., 91
Marohn, F., 119, 144, 189, 272
Marron, J.S., 46
Martins, M.J., 193, 194, 201, 206
Masamura, K., 442
Mason, D.M., 138
Matsunawa, T., 99
Matthys, G., 193
McCormick, W.P., 169, 220
McCulloch, J.H., 178, 289
McFadden, D., 303
McMahon, P.C., 373
McNeil, A.J., 401, 417
Mendon¸ca, S., 206
Michel, R., 432
Mikosch, T., 226, 275, 398, 432
Miranda, C., 206
Mises von, R., 16
Montfort, van M.A.J., 120, 143
Murakami, Y., 451
N
Nachtnebel, H.P., 342
Nandagopalan, S., 169
Naveau, P., 358, 362, 365
Neves, M., 193, 201
Nolan, J.P., viii, 172, 175, 176, 178, 285,
289
North, M., 342
O
Orozco, D., 454
Otten, A., 120
P
Panorska, A., 289
Pantula, S.G., 209
Parlange, M.B., 358, 362, 365
Peng, L., 192, 220
Pentik¨ainen, T., 43
Pereira, T.T., 135
Pericchi, L.R., 109
Perls, T.T., 456
Pesonen, M., 43
Pestana, D., 197
Pfanzagl, J., 329
Pfeifer, D., viii, 117
Phillips, P.C.B., 375
Pickands, J., 27, 134
Pictet, O., 225
Prescott, P., 134
Pruscha, H., 61
R
R´emillard, B., 275
R´ev´esz, P., 209

498
Index
Raaij, de G., 389
Rachev, S.T., 289
Radtke, M., viii, 189, 411, 413, 425
Rao, R.R., 268
Rasmussen, P.F., 138, 342
Raunig, B., 389
Reich, A., 413, 425
Reiss, R.–D., 23, 33, 34, 52, 121, 129, 132,
138, 140, 169, 186, 192, 198, 234,
238, 276, 293, 300, 313, 326, 328,
332, 342, 448
Resnick, S.I., 169, 189, 198, 220, 221, 398
Rice, J.A., 94
Robinson, M.E., 141
Rodrigues, L., 204
Rodriguez–Iturbe, I., 109
Ronchetti, E.M., 34, 89, 118
Rootz´en, H., 23, 145, 342, 398
Rosbjerg, D., 138, 342, 351
Rosenblatt, M., 234
Ross, W.H., 116
Rossi, F., 122
Rousseeuw, P.J., 34, 89, 118
Rousselle, J., 121, 341
Rubin, D.B., 274
Rytgaard, M., 129
S
S´anchez, J.M.P., 91
Sarabia, J.M., 118
Scarf, P.A., 441, 444
Sch¨utz, E.U., 460
Schmithals, B., 460
Schnieper, R., 415
Scholes, M., 374
Schupp, P., 405
Seal, H.L., 5
Seifert, B., 179
Sellars, C.M., 451
Serﬂing, R.J., 87, 281
Shi, G., 451
Shibata, T., 441
Shorack, G.R., 163
Sibuya, M., 74, 293, 451
Simonoﬀ, J.S., 46
Smith, R.L., 111, 117, 121, 141, 142, 147,
185, 240, 305, 334, 342, 355, 357,
358
Stˇaricˇa, C., 198, 220, 225
Stahel, A.W., 34, 89, 118
Stedinger, J.R., 351
Stephens, M.A., 120
Stork, P.A., 179
Stoyan, D., 114
Stoyan, H., 114
Straub, E., 414
Stute, W., 235
T
Tajvidi, N., 315, 342
Takahashi, R., 451
Tarleton, L.F., 359
Tawn, J.A., 141, 305, 322, 324, 331
Tay, A.S., 238, 407
Taylor, S., 373, 400
Teugels, J.L., 137, 195
Thatcher, A.R., 455
Thomas, M., 129, 132, 276, 342
Tiago de Oliveira, J., 293, 305
Tippett, L.H.C., 18, 187
Todorovic, P., 121, 249
Trimborn, M., 460
Tsay, R.S., 262
Tsuge, H., 442
U
Uemura, Y., 451
Usuki, H., 451
V
Vaquera, H., viii, 140
Vaupel, J.W., 454, 455
Veraverbeke, N., 434
Versace, P., 122
Villase˜nor, J.A., viii, 140
Viseu, C., 206
Vries, de C.G., viii, 179, 371, 395, 396, 398
Vylder, de F., 426, 430
Vynckier, P., 137, 195
W
Walden, A.T., 134
Wallis, J.R., 120
Walshaw, D., 116
Wang, Z., 143
Wehn, C.S., vi, 401, 409

Author Index
499
Weiss, L., 134, 183
Weissman, I., 143
Wellner, J.A., 163
Welsh, A.H., 185, 306
Whitmore, G.A., 10
Wicksell, S. D., 445
Wiedemann, A., 276
Williams, D. B., 178
Wolf, W., 174
Wood, E.F., 120, 337
X
Xin, H., 289
Y
Yao, Q., 403, 409
Yates, J.R., 451
Yuen, H.K., 305
Z
Zeevi, A., 155
Zelenhasic, E., 249
Zolotarev, V.M., 175
Zwiers, F.W., 116

Subject Index
A
Aggregation, 170
Angular component, 283, 311
Annual maxima method, 10, 138
Approximation
EV, of maxima, 18
GP, of exceedance df, 27, 183
normal
of gamma distributions, 122
of sums, 30
penultimate, 187–189, 257
Poisson
in a multinomial scheme, 152
of binomial distribution, 8, 9
of exceedances, 249
of negative binomial distribution, 99
Asset prices, 5, 371
Auto–tail–dependence function, 76, 325
sample, 76, 325
Autocorrelation function, 72
Autocovariance function, 72, 166, 167
sample, 72
Automatic choice
of bandwidth, see Cross–validation
of number of extremes, 137
B
Bartlett correction
in EV model, 119
in GP model, 144
Bayes
risk, 102, 244
Beta function, 126
Bias, 61, 89
–reduction, 190
Black–Scholes
model, 374
price, 392
Blocks method, see Annual maxima method
Bootstrap
parametric, df, 91
sample, 92, 431
C
Calendar eﬀect, 373
Canonical
dependence function, 317
Censoring
ﬁxed, type–I, II, 160
random, 160
Central limit theorem, 30
Characteristic function, 173
Cluster size
distribution, 78, 208
mean, 77, 208
reciprocal, see Extremal index
Clustering of exceedances according to
blocks deﬁnition, 77
runs deﬁnition, 77
Condition
extreme value, 145
Hall, 185, 198
Poisson(Λ, F), 258
Poisson(λ,F), 248
von Mises, 56, 186, 449
Weiss, see δ–neighborhood of a GP df
Conditional
density, 228
posterior density as, 244
distribution, 12, 228
of exceedances, 234
of order statistics, 234
expectation, 50, 165, 230, 395
Bayesian estimator as, 244
covariate, 241
serial, 241
501

502
Index
independence, 230, 245, 312
mean function, 241
q–quantile, 231
covariate, 241, 261
function, 241
serial, 241
variance, 166, 231, 395
Conﬁdence interval, 90
bootstrap, 91, 115, 431
for functional parameter, 91
Contour plot, 269
Copula, 275
empirical, 277
Correlation coeﬃcient, 267
Covariance, 71, 165
matrix, 267
sample, 71, 270
Covariate, 238, 355
Critical region, 93
Cross–validation, 46
Cycle, 354
D
Data, 6
American football (NFL), 306
declustered, 78, 209, 252, 341
deseasonalized, 70
exceptional athletic records, 141
exchange rate
black market, 179
British pound vs U.S. dollar, 375
Swiss franc vs U.S. dollar, 381
ﬁre claim
Danish, 417
from UK, 42
Norwegian, 415
of industrial portfolio, 421
ﬂoods
of Feather River, 109
of Moselle River, 70, 338
generation of, by
building mixtures, 38
polar method, 38
quantile transformation, 38
grouped, 152
Iceland storm, 114
liability insurance, 152
maximum pit depth, 442
maximum temperature
at de Bilt, 113
at Furnace Creek, 67
maximum wind–speeds
at Jacksonville, 121, 163
at Vancouver, 10, 48
multivariate, 274
missing, 67, 70, 274
number of car accidents, 100
ozone
at Mexico City, 140
in San Francisco Bay Area, 308
pooling of, 343
spatial, 273
stock market, 375
tensile strength of sheet steel, 117
TV watching, 46, 59
velocity of light (Michelson), 85
δ–neighborhood of a GP df, 183
Density, 7
kernel, 45
for censored data, 163
Density, multivariate, 268
kernel, 271
representation of, by d–fold partial deriva-
tive, 268
Dependence
serial, 71, 208
Distance
Hellinger, 88, 184, 239, 257
L2–, 88
Distribution(s)
Benktander II, 156
beta (GP2), 24, 38, 124
beta, two–parameter, 126
binomial, 8
Burr, 156
as a mixture, see Distribution(s), mix-
ture of, converse Weibull
Cauchy, 27, 38, 170, 172, 174
χ2–, 123, 170
convolution of, 30
double–exponential, see Distribution(s),
Laplace
elliptical
symmetric, 287
endpoint of

Subject Index
503
left, 12
right, 11
Erlang, 123
exponential, 122
exponential (GP0), 24, 37
extreme value (EV)
ﬁtting, to maxima, 57
two–component, 121
fat–tailed, 31, 32
Fr´echet (EV1), 15, 37, 38
function (df), 7
marginal, 267
sample, 39
gamma, 104, 106, 122, 125
convolution of, 122
mean of, 104
reciprocal, 104, 172
variance of, 104
Gaussian, see Distribution, normal
generalized Cauchy, 33
generalized gamma, 125
double, 126
generalized logistic, 123
generalized Pareto (GP), 23
ﬁtting, to exceedances, 57
geometric, 13, 27, 99, 145
Gompertz, 11, 22, 37, 54, 124, 458, 462
GP–Gamma, 125
GP–Gamma 2, 124
Gumbel (EV0), 10, 15, 37, 124
half–normal, 126
heavy–tailed, 30, 69
super, 154
L´evy, 126, 172
Laplace, 396
leptokurtic, see Distribution(s), fat–tailed
limiting, of
exceedances, 27
maxima, 18
minima, 22
sums, see Central limit theorem
log–gamma, 124, 125
log–normal, 32, 186, 349, 396
log–Pareto, 154
logistic, 123, 455
mixture of, 32, 123, 154, 233
converse Weibull, 156
exponential, 154
Pareto, 154
Poisson, 98, 249
multinomial, 95
negative binomial, 14, 99, 250
normal, 17, 31, 172, 174
ﬁtting, to data, 59
kurtosis of, 31
mixture of, 31, 32, 59, 379, 389
of maximum, 10
of heterogeneous random variables,
10
of minimum, 11
Pareto (GP1), 24, 37, 38, 124
as a mixture, see Distribution(s), mix-
ture of, exponential
Pearson–type III, see Distribution(s),
gamma
penultimate, 187, 189
Poisson, 9
ﬁtting, to discrete data, 59
predictive, 245
proﬁt/loss, 384
Rayleigh, 22, 54
regional frequency, 345
Student, 33, 95, 170, 179, 379, 396
noncentral, 285
with n degrees of freedom, 94, 170
sum–stable, 172, 379
unimodal, 15
Wakeby, 120
Weibull (EV2), 15, 38
Distribution(s), multivariate
elliptical, 282, 296
function (df), 266
sample, 270
Gumbel type II, see Distribution(s),
multivariate, Gumbel–McFadden
Gumbel–McFadden, 301, 307
H¨usler–Reiss, 295, 307
log–normal, 281
Marshall–Olkin, 294, 306
normal (Gaussian), 280, 281
spherical, 282, 296, 312
Student, 283, 285, 390
sum–stable, 285
Domain of attraction

504
Index
max–, 18
pot–, 27, 30, 184
E
Equation
AR, 166
ARMA, 167
stochastic linear diﬀerence, see also Time
series, AR(p), 164
Ergodic theory, 209
Estimator
Bayes, 102, 103, 242
for exponential model, 105
for GP 1 model, 129
bias of, 89
diagram of, 136
Drees–Pickands (GP), 134
Hill
MLE in GP1(u, µ = 0), 129
bad performance of, 130, 132, 136,
184, 211, 212, 381
bias–reduced, 194, 203, 205
MLE in GP2(u), 133
in AR–model
Yule–Walker, 168
in ARMA–model
Hannan–Rissanen, 168
innovations algorithm, 168
MLE, 168
in GP model
for degrees, 344
Kaplan–Meier, 162
L–moment, 348
least squares, in normal model, 87
linear combination of ratios of spac-
ings (LRS)
in EV model, 111
in GP model, see Estimator, Drees–
Pickands (GP)
M–
for scale parameter, 88
in exponential (GP0) model, 89, 128
in Pareto (GP1) model, 129
maximum likelihood (ML)
in normal model, 85
for censored data, 164
in beta (GP2) model, 133
in EV model, 111
in exponential (GP0) model, 84, 128
in Fr´echet (EV1) model, 110
in GP model, 134
in Gumbel (EV0) model, 108
in multivariate normal model, 281
in negative binomial model, 100
in Poisson model, 96
mean squared error (MSE) of, 89, 102,
191
minimum distance (MD)
in EV model, 111
in normal model, 88
moment
in GP model, 134
in Gumbel (EV0) model, 108
in negative binomial model, 99
in normal model, 86
nonparametric density, see Density, ker-
nel
Pickands (2–dim EV), 306, 307
quick, in normal model, 87
reduced–bias, 190
unbiased, 89
Euclidean norm, 282
Euler’s constant, 21, 108
Exceedance(s), 8, 138
df, 12
for Poisson(λ, F) process, 249
number of, 8
time, see Time, arrival
Excess, 49
df, 49, 53
function
mean, 50
median, 53
sample mean, 52, 456
trimmed mean, 52, 180
Expected shortfall, 385
conditional, 406
Extremal index, 79, 168
Extreme value index, 208
Extremes
conditional, 238
F
Forecast, see Prediction
Fubini theorem, 235
for conditional distributions, 235

Subject Index
505
G
Gamma function, 19, 104
Gompertz law, 54, 453
Gumbel method, see Annual maxima method
H
Hazard
function, 53
cumulative, 53
reciprocal, 55
sample, 55, 456
rate, 53, 458
Histogram
of discrete distribution, 9, 470
Poisson, 97
sample, 44, 97
Homogeneity property, 304
Horror case, 19
I
Index ﬂood, 344, 345
Innovation algorithm, see Simulation, of ARMA
time series
Intensity, 248, 262, 447
function, 259
measure, 259, 262, 447
J
Jackknife method, 193
K
Kendall’s τ, 267, 284
Kurtosis, 31
L
L–CV, 347
L–kurtosis, 347
L–moment ratios, 347
L–moments, 86, 346
L–skewness, 347
L–statistic, 87
Lag, see Time, lag
Leadbetter’s mixing conditions, 168
Least squares method, 64, 65
Legendre polynomials, 346
Level, T–year, 12, 13
Likelihood
function, 84, 103
Location and scale parameters, 16, 36
vectors, 279
Lowess, see Regression, local weighted
M
Matrix
orthogonal, 282
transposed, 266
unit, 280, 282
Maxima, 9
of GP random variables, 29
McSize, see Cluster size, mean
Mean
number of exceedances, see Mean, value
function
of binomial distribution, 8
of EV distribution, 20
of gamma distribution, 122
of GP distribution, 30
of negative binomial distribution, 99
of Poisson distribution, 9
sample, 41
value function, 8, 18
of homogeneous Poisson process, 248
of inhomogeneous Poisson process,
258
of P´olya–Lundberg process, 250
vector, 266
sample, 270
Minima, 11, 110, 117
Mode(s), 15
of EV distributions, 21
Model
Poisson–GP, 153
Poisson–Pareto (Poisson–GP1), 152
Moments
centered, 20
of EV distributions, 19
of GP distributions, 29
sample, 41
Mortality rate, see Hazard, rate
Moving averages, 66
Nadaraya–Watson, 66
N
Newton–Raphson iteration, 171
O

506
Index
Order statistics, 12, 40, 141, 234
Outlier, 47
P
P–P plots, 63
p–value, 93, 95, 96, 330
Parameter
canonical, 299, 300, 307
tail dependence, 75, 300, 316
Parameterization
α–
of EV distributions, 15
of GP distributions, 24
γ–
of EV distributions, 16
of GP distributions, 25
Partial duration values, see Exceedances
Peaks–over–threshold, see Exceedance(s)
Pickands dependence function, 327
Polynomial, MA and AR, 167
Posterior
density, 103, 244
Power function, 330
Prediction, 235, 244
linear, 245
Predictive
density, 238
distribution, 237, 406
in Bayesian framework, 245
VaR, 406
Prior
conjugate, 104–106
density, 102
Probability weighted moments, 346
Probable maximum loss (PML), 419
Process
counting, 248, 415
P´olya–Lundberg, 250
point, 259
Poisson, 247, 262
homogeneous (Poisson(λ)), 248
inhomogeneous (Poisson(Λ)), 258
mixed, 249
white–noise, 164, 397
Q
Quantile(s), 35
bivariate, 269
extreme, 208
function (qf), 35, 120
sample, 42
sample, 42
Quantile–Quantile (Q–Q) plot, 61
EV, 62, 112
GP, 62
normal, 62
R
Radial component, 282, 311, 328
Random variables
uncorrelated, 71, 267
Regression
ﬁxed design, 65
local weighted, 69
slope, 64
Regularly varying, 182, 186
Reinsurance treaty
ECOMOR, 411
excess–of–loss (XL), 4, 411
stop–loss, 411
Reproductivity
pot–
of Benktander II distributions, 157
of converse truncated Weibull dis-
tributions, 157
sum–
of gamma distributions, 122
Residual life
df, see Excess, df
mean, function, see Excess, function,
mean
Residuals, 69
Return period, see Time, interarrival
Returns, 5, 371
volatility of, 377
Risk
contour plot, 430
process, 412
RiskMetrics, 400
Robust statistics, 34, 88, 128, 129
Rotation, 282
Run length, see Clustering, of exceedances
according to, run deﬁnition
S
Scatterplot, 48

Subject Index
507
3–D, 273
Seasonal component, 69
Self–similarity, 170, 171
Separation, seasonal, 120
Service
life, 441
Simulation
of ARMA time series, 168
Skewness coeﬃcient
of EV distributions, 21
sample, 41, 119
Slowly varying, 183, 186
Spacings, 120
Spectral expansion
diﬀerentiable, 326
Stability
max–, 19
min–, 23
pot–, 25, 182
sum–, 31, 126, 172, 174
STABLE, 173
Standard deviation, 21
Stationarity
covariance–, see Stationarity, weak
strict, 165, 207
of Gaussian time series, 165
weak, 72, 165, 397
StatPascal
accessing active data with, 493
editor, 486
generating data with, 492
plots, 490
Reference Manual, 485
Vector Types, 488
window, 487
Strength
of material, 7
of bundles of threads, 7
of material, 110, 117
tensile, 117
Survivor function, 11, 54
marginal, 267
multivariate, 266
T
T–day capital, 7
T–unit depths, 6, 7
T–year discharge, see Level, T–year
T–year initial reserve, 7, 430
Tail
behavior, 355
dependence
coeﬃcient of, 322
dependence/independence, 75, 328
testing for, 329
probability, 52
Test
χ2–
for EV models, 120
in multinomial model, 95, 98
goodness–of–ﬁt, 61
for EV models, 120
for Poisson model, 97
likelihood ratio (LR)
in EV model, 118
in GP model, 144
in multinomial model, 96, 98
selection of null–hypothesis, 329
t–, in normal model, 94
UMP, in normal model, 94
Theorem
Balkema–de Haan–Pickands, 27
Falk–Marohn–Kaufmann, 189
Fisher–Tippett, 18
Tiago de Oliveira–Geﬀroy–Sibuya, 292
Threshold, 8
random, 138
T–year, 4
for heterogeneous random variables,
251
given a Poisson process, 252
median, 251
Time
arrival, 13, 247
for mixed Poisson process, 250
Pareto, 250
early warning, 430
interarrival, 13
exponential, 247
lag, 72
ruin, 429
Time series
AR(p), 166, 221
ARCH(p), 397, 399
ARMA(p,q), 167, 210

508
Index
causal, 167
GARCH(p,q), 210, 224, 400, 401, 403
Gaussian AR(1), 73, 165, 325, 326
MA(∞), 167
MA(q), 166
Cauchy, 166
Gaussian, 166
Transformation
probability, 38, 234, 235, 407
quantile, 38
Rosenblatt, 234, 407
theorem for densities, 280
Wicksell, 446
Trend, 354
U
UFO, 481
calculator, 482
Utility
function, 303
maximizing, 304
V
Value–at–Risk (VaR), 384
conditional
covariate, 262
serial, 401, 409
Variance
of binomial distribution, 8
of EV distribution, 20
of gamma distribution, 122
of GP distribution, 30
of negative binomial distribution, 99
of Poisson distribution, 9
of Student distribution, 33
sample, 41
Varying function
regularly, 326
slowly, 323
Vector
transposed, 266
Volatility, 374
W
Wicksell’s corpuscle problem, 33, 445
X
Xtremes
clipboard, 475, 480
data
format, 476
generating, 472
missing, 478
reading, 472
types, 477
editor, 476
illustrations, 478
installation, 467
mouse mode, 475
coordinate changing, 468, 472
information, 472
label, 479
option, 470, 479
parameter varying, 470, 479, 482
point selection (scissors), 473
overview, 467
plots, 470
printing, 480
starting, 470
system requirements, 467
toolbar, 470, 471, 474

Bibliography
[1] Baillie, R.T. and McMahon, P.C. (1989). The Foreign Exchange Market: The-
ory and Econometric Evidence. Cambridge University Press, Cambridge.
[2] Beirlant, J., Teugels, J.L. and Vynckier, P. (1996). Practical Analysis of Ex-
treme Values. Leuven University Press, Leuven.
[3] Best, P. (1998). Implementing Value at Risk. Wiley, Chichester.
[4] Bob´ee, B. and Ashkar, F. (1991). The Gamma Family and Derived Distribu-
tions Applied in Hydrology. Water Resources Publications, Littleton.
[5] Brockwell, P.J. and Davis, R.A. (1987). Time Series: Theory and Methods.
Springer, New York.
[6] Castillo, E. (1988). Extreme Value Theory in Engineering. Academic Press,
Boston.
[7] Christoph, G. and Wolf, W. (1992). Convergence Theorems with a Stable
Limit Law. Akademie Verlag. Berlin.
[8] Cleveland, W.S. (1993). Visualizing Data. Hobart Press, New Jersey.
[9] Coles, S. (2001). An Introduction to the Statistical Modeling of Extreme
Values. Springer, London.
[10] Daykin, C.D., Pentik¨ainen, T. and Pesonen, M. (1994). Practical Risk Theory
for Actuaries. Chapman & Hall, London.
[11] Embrechts, P., Kl¨uppelberg, C. and Mikosch, T. (1997). Modelling Extremal
Events for Insurance and Finance. Springer, New York.
[12] Enders, W. (1995). Applied Econometric Time Series. Wiley, New York.
[13] Extreme Values: Floods and Droughts. (1994). Proceedings of International
Conference on Stochastic and Statistical Methods in Hydrology and Environ-
mental Engineering, Vol. 1, 1993, K.W. Hipel (ed.), Kluwer, Dordrecht.
[14] Extreme Value Theory. (1989). Proceedings of Oberwolfach Meeting, 1987,
J. H¨usler and R.–D. Reiss (eds.), Lect. Notes in Statistics 51, Springer, New
York.
[15] Extreme Value Theory and Applications. (1994). Proceedings of Gaithersburg
Conference, 1993, J. Galambos et al. (eds.), Vol. 1: Kluwer, Dortrecht, Vol. 2:
Journal Research NIST, Washington, Vol. 3: NIST Special Publication 866,
Washington.

510
Bibliography
[16] Falk, M., H¨usler, J. and Reiss, R.–D. (1994). Laws of Small Numbers: Ex-
tremes and Rare Events. DMV–Seminar Bd 23. Birkh¨auser, Basel.
[17] Falk, M., H¨usler, J. and Reiss, R.–D. (2004). Laws of Small Numbers: Ex-
tremes and Rare Events. 2nd ed., Birkh¨auser, Basel.
[18] Fan, J. and Yao, Q. (2003). Nonlinear Time Series. Springer, New York.
[19] Fang, K.–T., Kotz, S. and Ng, K.–W. (1990). Symmetric Multivariate and
Related Distributions. Chapman and Hall, London.
[20] Galambos, J. (1987). The Asymptotic Theory of Extreme Order Statistics.
2nd ed. Krieger: Malabar, Florida (1st ed., Wiley, New York, 1978).
[21] Gentleman J.F. and Whitmore G.A. (1995). Case Studies in Data Analysis.
Lect. Notes Statist. 94, Springer–Verlag, New York.
[22] Gerber, H.–U. (1979). An Introduction to Mathematical Risk Theory. Huebner
Foundation Monograph 8, Philadelphia.
[23] Gray, H.L. and Schucany, W.R. (1972). The Generalized Jackknife Statistic.
Marcel Dekker.
[24] Gumbel, E.J. (1958). Statistics of Extremes. Columbia Univ. Press, New York.
[25] Haan, L. de (1970). On Regular Variation and its Application to the Weak
Convergence of Sample Extremes. Math. Centre Tracts 32, Amsterdam.
[26] Hamilton, J.D. (1994). Time Series Analysis. Princeton University Press,
Princeton.
[27] Hampel, F.R., Ronchetti, E.M., Rousseeuw, P.J. and Stahel, A.W. (1986).
Robust Statistics. Wiley, New York.
[28] Hogg, R.V. and Klugman, S.A. (1984). Loss Distributions. Wiley, New York.
[29] Hosking, J.R.M. and Wallis, J.R. (1997). Regional Frequency Analysis. Cam-
bridge Univ. Press, Cambridge.
[30] Huber, P.J. (1981). Robust Statistics. Wiley, New York.
[31] Johnson, N.L. and Kotz, S. (1970). Distributions in Statistics: Continuous
Univariate Distributions–1, –2. Houghton Miﬄin, Boston.
[32] Johnson, N.L. and Kotz, S. (1972). Distributions in Statistics: Continuous
Multivariate Distributions. Wiley, New York.
[33] Jorion, P. (2001). Value at Risk: The New Benchmark for Controlling Market
Risk. (ﬁrst edition 1997), McGraw–Hill, New York.
[34] Kinnison, R.P. (1985). Applied Extreme Value Statistics. Battelle Press,
Columbus.
[35] Klugman, S.A. (1992). Bayesian Statistics in Actuarial Sciences. Kluwer, Dor-
drecht.
[36] Klugman, S.A., Panjer, H.H. and Willmot, G.E. (1998). Loss Models, From
Data to Decisions. Wiley, New York.
[37] Kowaka, M., Tsuge, H., Akashi, M., Masamura, K. and Ishimoto, H. (1994).
Introduction to Life Prediction of Industrial Plant Materials, Application of
the Extreme Value Statistical Method for Corrosion Analysis. Allerton Press
(Japanese version, 1984, Maruzen, Tokyo.)

511
[38] Kutoyants, Yu.A. (1998). Statistical Inference for Spatial Poisson Processes.
Lect. Notes in Statistics 134, Springer, New York.
[39] Leadbetter, M.R., Lindgren, G. and Rootz´en, H. (1983). Extremes and Related
Properties of Random Sequences and Processes. Springer, New York.
[40] Mandelbrot, B.B. (1997). Fractals and Scaling in Finance. Springer, New
York.
[41] McCullagh, P. and Nelder, J.A. (1989). Generalized Linear Models. Second
Edition. Chapman and Hall, London.
[42] Reiss, R.–D. (1989). Approximate Distributions of Order Statistics. Springer,
New York.
[43] Reiss, R.–D. (1993). A Course on Point Processes. Springer, New York.
[44] Resnick, S.I. (1987). Extreme Values, Regular Variation, and Point Processes.
Springer, New York.
[45] Rosbjerg, D. (1993). Partial Duration Series in Water Resources. Institute of
Hydrodynamics and Hydraulic Engineering. Technical University, Lyngby.
[46] Samorodnitsky, G. and Taqqu, M.S. (1994). Stable Non–Gaussian Random
Processes. Chapman & Hall, New York.
[47] SCOR Notes: Catastrophe Risks. 1993, J. Lemaire (ed.)
[48] Serﬂing, R.J. (1980). Approximation Theorems of Mathematical Statistics.
Wiley, New York.
[49] Shao, J. and Tu, D. (1995). The Jackknife and the Bootstrap. Springer, New
York.
[50] Simonoﬀ, J.S. (1996). Smoothing Methods in Statistics. Springer, New York.
[51] Statistics for the Environment 2, Water Related Issues. (1997). Proceedings
for the SPRUCE Conference, Rothamsted, 1993, V. Barnett and K.F. Turk-
man (eds.), Wiley, Chichester.
[52] Statistics for the Environment 3, Pollution Assessment and Control. (1997).
Proceedings for the SPRUCE Conference, Merida, 1995, V. Barnett and K.F.
Turkman (eds.), Wiley, Chichester.
[53] The Handbook of Risk Management and Analysis. (1996). C. Alexander (ed.)
Wiley, Chichester.
[54] Taylor, S. (1986). Modeling Financial Time Series. Wiley, Chichester.
[55] Tukey, J.W. (1977). Exploratory Data Analysis. Addison–Wesley, Reading.

